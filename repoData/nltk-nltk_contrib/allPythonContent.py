__FILENAME__ = align

# Natural Language Toolkit: Gale-Church Aligner
#
# Copyright (C) 2001-2011 NLTK Project
# Author: Chris Crowner <ccrowner@gmail.com>
# URL: <http://nltk.org/>
# For license information, see LICENSE.TXT

import sys
from itertools import izip

from nltk.metrics import scores

## --NLTK--
## Import the nltk.aligner module, which defines the aligner interface
from api import *

import distance_measures
import align_util

# Based on Gale & Church 1993, "A Program for Aligning Sentences in Bilingual Corpora"
# This is a Python version of the C implementation by Mike Riley presented in the appendix
# of the paper. The documentation in the C program is retained where applicable.

##//////////////////////////////////////////////////////
##  Alignment
##//////////////////////////////////////////////////////

class Alignment(object):
    def __init__(self): 
        self.x1 = 0
        self.y1 = 0
        self.x2 = 0
        self.y2 = 0
        self.d = 0  
        self.category = ''
                    
class AlignmentExtended(object):
    def __init__(self): 
        self.x1 = 0
        self.y1 = 0
        self.x2 = 0
        self.y2 = 0
        self.x3 = 0
        self.y3 = 0
        self.d = 0  
        self.category = ''
                    
##//////////////////////////////////////////////////////
##  GaleChurchAligner
##//////////////////////////////////////////////////////

class GaleChurchAligner(AlignerI):
    def __init__(self, dist_funct, alignment_type, output_format, print_flag=False):
        self.dist_funct = dist_funct
        # either 'original' or 'extended'
        self.alignment_type = alignment_type
        # either 'bead_objects' (the Alignment objects above - in a dict)
        # or 'text_tuples' (source text to target text mappings - in a list)
        # or 'index_tuples' (source indices to target indices - in a list)
        self.output_format = output_format
        # ideally, bead objects would be printed from the calling routines
        # unfortunately, the source and target text may be unavailable then for printing
        # (eg recursive_align) hence the print_flag, which is used in 'align' which is
        # common to both batch_align and recursive_align
        # note: in future, will post update that adds ability to output alignment files 
        # in formats such as ARCADE and TEI (also can read TEI files as input)
        # also coming later is printing in unicode - sorry about that
        self.print_flag = print_flag
    
    def get_delimited_regions(self, base_type, input_file1, input_file2, hard_delimiter, soft_delimiter):
        lines1 = align_util.readlines(input_file1)
        if (base_type == 'token'):
            hard_regions1 = align_util.get_regions(lines1, hard_delimiter, soft_delimiter)
        elif (base_type == 'sentence'):
            hard_regions1 = align_util.get_paragraphs_sentences(lines1, hard_delimiter, soft_delimiter)
            
        lines2 = align_util.readlines(input_file2)
        if (base_type == 'token'):
            hard_regions2 = align_util.get_regions(lines2, hard_delimiter, soft_delimiter)
        elif (base_type == 'sentence'):
            hard_regions2 = align_util.get_paragraphs_sentences(lines2, hard_delimiter, soft_delimiter)
                
        if (len(hard_regions1) != len(hard_regions2)):
            print "align_regions: input files do not contain the same number of hard regions" + '\n'
            print "%s" % hard_delimiter + '\n'
            print "%s has %d and %s has %d" % (input_file1, len(hard_regions1), \
                                               input_file2, len(hard_regions2) + '\n')
            return ([],[])            
        
        return (hard_regions1, hard_regions2)        
    
    def align(self, hard_region1, hard_region2):                        
        
        len1 = align_util.get_character_lengths(hard_region1)                
        number_of_soft_regions1 = len(hard_region1)
                        
        len2 = align_util.get_character_lengths(hard_region2)            
        number_of_soft_regions2 = len(hard_region2)
                
        if (self.alignment_type == 'original'):
            alignment = self._seq_align(len1, len2, number_of_soft_regions1, number_of_soft_regions2)
        elif (self.alignment_type == 'extended'):
            alignment = self._seq_align_extended(len1, len2, number_of_soft_regions1, number_of_soft_regions2)

        if (self.output_format == 'text_tuples'):      
            (output_alignment, indices_mapping) = \
                align_util.convert_bead_to_tuples(alignment, hard_region1, hard_region2)
            if (self.print_flag):
                align_util.print_alignment_text_mapping(output_alignment)
        elif (self.output_format == 'index_tuples'):      
            (text_mapping, output_alignment) = \
                align_util.convert_bead_to_tuples(alignment, hard_region1, hard_region2)
            if (self.print_flag):
                align_util.print_alignment_index_mapping(output_alignment)
        else: # the Gale-Church alignment "bead"  objects - a dictionary of objects
            output_alignment = alignment
            if (self.print_flag):
                align_util.print_alignments(output_alignment, hard_region1, hard_region2)
            
        return output_alignment
            
    def _seq_align(self, x, y, nx, ny):    
        """        
        Sequence alignment routine.
        This version allows for contraction/expansions.
        
        x and y are sequences of objects, represented as non-zero ints, to be aligned.
        
        dist_funct(x1, y1, x2, y2) is a distance function of 4 args:          
        
        dist_funct(x1, y1, 0, 0) gives cost of substitution of x1 by y1.
        dist_funct(x1, 0, 0, 0) gives cost of deletion of x1.
        dist_funct(0, y1, 0, 0) gives cost of insertion of y1.
        dist_funct(x1, y1, x2, 0) gives cost of contraction of (x1,x2) to y1.
        dist_funct(x1, y1, 0, y2) gives cost of expansion of x1 to (y1,y2).
        dist_funct(x1, y1, x2, y2) gives cost to match (x1,x2) to (y1,y2).
        
        align is the alignment, with (align[i].x1, align[i].x2) aligned
        with (align[i].y1, align[i].y2).  Zero in align[].x1 and align[].y1
        correspond to insertion and deletion, respectively.  Non-zero in
        align[].x2 and align[].y2 correspond to contraction and expansion,
        respectively.  align[].d gives the distance for that pairing.
                
        """
        distances = []
        path_x = []
        path_y = [] 
        
        first_len = nx + 1
        second_len = ny + 1
        
        distances = [[0] * second_len for c in range(first_len)]         
        path_x = [[0] * second_len for c in range(first_len)]              
        path_y = [[0] * second_len for c in range(first_len)]
                  
        d1 = d2 = d3 = d4 = d5 = d6 = sys.maxint
        
        for j in range(0, ny + 1):    
            for i in range(0, nx + 1):            
                if (i > 0 and j > 0):		
                    #/* substitution */
                    d1 = distances[i-1][j-1] + \
                        self.dist_funct(x[i-1], y[j-1], 0, 0)
                else:
                    d1 = sys.maxint
                    
                if (i > 0):	
                    #/* deletion */
                    d2 = distances[i-1][j] + \
                        self.dist_funct(x[i-1], 0, 0, 0)
                else:
                    d2 = sys.maxint
                    
                if (j > 0):		
                    #/* insertion */
                    d3 = distances[i][j-1] + \
                        self.dist_funct(0, y[j-1], 0, 0)
                else:
                    d3 = sys.maxint
                    
                if (i > 1 and j > 0):		
                    #/* contraction */
                    d4 = distances[i-2][j-1] + \
                        self.dist_funct(x[i-2], y[j-1], x[i-1], 0)
                else:
                    d4 = sys.maxint
                    
                if (i > 0 and j > 1):		
                    #/* expansion */
                    d5 = distances[i-1][j-2] + \
                        self.dist_funct(x[i-1], y[j-2], 0, y[j-1])
                else:
                    d5 = sys.maxint
                    
                if (i > 1 and j > 1):		
                    #/* melding */
                    d6 = distances[i-2][j-2] + \
                        self.dist_funct(x[i-2], y[j-2], x[i-1], y[j-1])
                else:
                    d6 = sys.maxint
     
                dmin = min(d1, d2, d3, d4, d5, d6)
                
                if (dmin == sys.maxint):
                    distances[i][j] = 0
                elif (dmin == d1):
                    distances[i][j] = d1                
                    path_x[i][j] = i - 1
                    path_y[i][j] = j - 1              
                elif (dmin == d2):
                    distances[i][j] = d2                
                    path_x[i][j] = i - 1
                    path_y[i][j] = j                
                elif (dmin == d3):
                    distances[i][j] = d3                
                    path_x[i][j] = i
                    path_y[i][j] = j - 1                
                elif (dmin == d4):
                    distances[i][j] = d4                
                    path_x[i][j] = i - 2
                    path_y[i][j] = j - 1                 
                elif (dmin == d5):
                    distances[i][j] = d5                
                    path_x[i][j] = i - 1
                    path_y[i][j] = j - 2
                else:			
                    # /* dmin == d6 */ {
                    distances[i][j] = d6                
                    path_x[i][j] = i - 2
                    path_y[i][j] = j - 2
        n = 0
        
        ralign_dict = {}
        
        i = nx
        j = ny
        while (i > 0 or j > 0):                
            oi = path_x[i][j]       
            oj = path_y[i][j]
            di = i - oi
            dj = j - oj
            
            ralign = Alignment()
                          
            if (di == 1 and dj == 1):
                #/* substitution */            
                ralign.x1 = x[i-1]
                ralign.y1 = y[j-1]
                ralign.x2 = 0
                ralign.y2 = 0
                ralign.d = distances[i][j] - distances[i-1][j-1] 
                ralign.category = '1 - 1'
                
            elif (di == 1 and dj == 0):
                #/* deletion */
                ralign.x1 = x[i-1]
                ralign.y1 = 0
                ralign.x2 = 0
                ralign.y2 = 0
                ralign.d = distances[i][j] - distances[i-1][j]
                ralign.category = '1 - 0'
            elif (di == 0 and dj == 1):
                #/* insertion */
                ralign.x1 = 0
                ralign.y1 = y[j-1]
                ralign.x2 = 0
                ralign.y2 = 0
                ralign.d = distances[i][j] - distances[i][j-1]        
                ralign.category = '0 - 1'
            elif (dj == 1):
                #/* contraction */
                ralign.x1 = x[i-2]
                ralign.y1 = y[j-1]
                ralign.x2 = x[i-1]
                ralign.y2 = 0
                ralign.d = distances[i][j] - distances[i-2][j-1]     
                ralign.category = '2 - 1'
            elif (di == 1):
                #/* expansion */
                ralign.x1 = x[i-1]
                ralign.y1 = y[j-2]
                ralign.x2 = 0
                ralign.y2 = y[j-1]
                ralign.d = distances[i][j] - distances[i-1][j-2]    
                ralign.category = '1 - 2'
            else: 
                #/* di == 2 and dj == 2 */ { /* melding */
                ralign.x1 = x[i-2]
                ralign.y1 = y[j-2]
                ralign.x2 = x[i-1]
                ralign.y2 = y[j-1]
                ralign.d = distances[i][j] - distances[i-2][j-2]
                ralign.category = '2 - 2'
                               
            ralign_dict[n] = ralign
            
            n = n + 1
            
            i = oi
            j = oj
           
        align_dict = {}
            
        for e in range(0, n):
            align_dict[n-e-1] = ralign_dict[e] 
               
        return align_dict
        
    def _seq_align_extended(self, x, y, nx, ny):    
        """        
        Sequence alignment routine.
        This version allows for contraction/expansions.
        
        x and y are sequences of objects, represented as non-zero ints, to be aligned.
        
        dist_funct(x1, y1, x2, y2) is a distance function of 4 args:          
        
        dist_funct(x1, y1, 0, 0) gives cost of substitution of x1 by y1.
        dist_funct(x1, 0, 0, 0) gives cost of deletion of x1.
        dist_funct(0, y1, 0, 0) gives cost of insertion of y1.
        dist_funct(x1, y1, x2, 0) gives cost of contraction of (x1,x2) to y1.
        dist_funct(x1, y1, 0, y2) gives cost of expansion of x1 to (y1,y2).
        dist_funct(x1, y1, x2, y2) gives cost to match (x1,x2) to (y1,y2).
        
        align is the alignment, with (align[i].x1, align[i].x2) aligned
        with (align[i].y1, align[i].y2).  Zero in align[].x1 and align[].y1
        correspond to insertion and deletion, respectively.  Non-zero in
        align[].x2 and align[].y2 correspond to contraction and expansion,
        respectively.  align[].d gives the distance for that pairing.
                
        """
        distances = []
        path_x = []
        path_y = [] 
        
        first_len = nx + 1
        second_len = ny + 1
        
        distances = [[0] * second_len for c in range(first_len)]         
        path_x = [[0] * second_len for c in range(first_len)]              
        path_y = [[0] * second_len for c in range(first_len)]
                  
        d1 = d2 = d3 = d4 = d5 = d6 = d7 = d8 = d9 = d10 = d11 = sys.maxint
        
        for j in range(0, ny + 1):    
            for i in range(0, nx + 1):            
                if (i > 0 and j > 0):		
                    #/* substitution */          /* 1-1 */
                    d1 = distances[i-1][j-1] + \
                        self.dist_funct(x[i-1], y[j-1], 0, 0, 0, 0)
                else:
                    d1 = sys.maxint
                    
                if (i > 0):	
                    #/* deletion */              /* 1-0 */
                    d2 = distances[i-1][j] + \
                        self.dist_funct(x[i-1], 0, 0, 0, 0, 0)
                else:
                    d2 = sys.maxint
                    
                if (j > 0):		
                    #/* insertion */             /* 0-1 */
                    d3 = distances[i][j-1] + \
                        self.dist_funct(0, y[j-1], 0, 0, 0, 0)
                else:
                    d3 = sys.maxint
                    
                if (i > 1 and j > 0):		
                    #/* contraction */           /* 2-1 */
                    d4 = distances[i-2][j-1] + \
                        self.dist_funct(x[i-2], y[j-1], x[i-1], 0, 0, 0)
                else:
                    d4 = sys.maxint
                    
                if (i > 0 and j > 1):		     
                    #/* expansion */             /* 1-2 */
                    d5 = distances[i-1][j-2] + \
                        self.dist_funct(x[i-1], y[j-2], 0, y[j-1], 0, 0)
                else:
                    d5 = sys.maxint
                    
                if (i > 1 and j > 1):		
                    #/* melding */               /* 2-2 */
                    d6 = distances[i-2][j-2] + \
                        self.dist_funct(x[i-2], y[j-2], x[i-1], y[j-1], 0, 0)
                else:
                    d6 = sys.maxint
                    
                if (i > 2 and j > 0):		
                    #/* contraction */           /* 3-1 */
                    d7 = distances[i-3][j-1] + \
                        self.dist_funct(x[i-3], y[j-1], x[i-2], 0, x[i-1], 0)
                else:
                    d7 = sys.maxint
                    
                if (i > 2 and j > 1):		
                    #/* contraction */           /* 3-2 */
                    d8 = distances[i-3][j-2] + \
                        self.dist_funct(x[i-3], y[j-1], x[i-2], y[j-2], x[i-1], 0)
                else:
                    d8 = sys.maxint
                    
                if (i > 0 and j > 2):		
                    #/* expansion */             /* 1-3 */
                    d9 = distances[i-1][j-3] + \
                        self.dist_funct(x[i-1], y[j-3], 0, y[j-2], 0, y[j-1])
                else:
                    d9 = sys.maxint
                    
                if (i > 1 and j > 2):		
                    #/* expansion */             /* 2-3 */
                    d10 = distances[i-2][j-3] + \
                        self.dist_funct(x[i-3], y[j-3], x[i-2], y[j-2], 0, y[j-1])
                else:
                    d10 = sys.maxint
                                                        
                if (i > 2 and j > 2):		
                    #/* melding */               /* 3-3 */
                    d11 = distances[i-3][j-3] + \
                        self.dist_funct(x[i-3], y[j-3], x[i-2], y[j-2], x[i-1], y[j-1])
                else:
                    d11 = sys.maxint
     
                dmin = min(d1, d2, d3, d4, d5, d6, d7, d8, d9, d10, d11)
                
                if (dmin == sys.maxint):
                    distances[i][j] = 0
                elif (dmin == d1):
                    distances[i][j] = d1                
                    path_x[i][j] = i - 1
                    path_y[i][j] = j - 1              
                elif (dmin == d2):
                    distances[i][j] = d2                
                    path_x[i][j] = i - 1
                    path_y[i][j] = j                
                elif (dmin == d3):
                    distances[i][j] = d3                
                    path_x[i][j] = i
                    path_y[i][j] = j - 1                
                elif (dmin == d4):
                    distances[i][j] = d4                
                    path_x[i][j] = i - 2
                    path_y[i][j] = j - 1                 
                elif (dmin == d5):
                    distances[i][j] = d5                
                    path_x[i][j] = i - 1
                    path_y[i][j] = j - 2
                elif (dmin == d6):			                   
                    distances[i][j] = d6                
                    path_x[i][j] = i - 2
                    path_y[i][j] = j - 2
                elif (dmin == d7):
                    distances[i][j] = d7                
                    path_x[i][j] = i - 3
                    path_y[i][j] = j - 1               
                elif (dmin == d8):
                    distances[i][j] = d8                
                    path_x[i][j] = i - 3
                    path_y[i][j] = j - 2                
                elif (dmin == d9):
                    distances[i][j] = d9                
                    path_x[i][j] = i - 1
                    path_y[i][j] = j - 3                 
                elif (dmin == d10):
                    distances[i][j] = d10                
                    path_x[i][j] = i - 2
                    path_y[i][j] = j - 3
                elif (dmin == d11):			                   
                    distances[i][j] = d11                
                    path_x[i][j] = i - 3
                    path_y[i][j] = j - 3
        n = 0
        
        ralign_dict = {}
        
        i = nx
        j = ny
        while (i > 0 or j > 0):                
            oi = path_x[i][j]       
            oj = path_y[i][j]
            di = i - oi
            dj = j - oj
            
            ralign = AlignmentExtended()
                          
            if (di == 1 and dj == 1):
                #/* substitution */            
                ralign.x1 = x[i-1]
                ralign.y1 = y[j-1]
                ralign.x2 = 0
                ralign.y2 = 0
                ralign.x3 = 0
                ralign.y3 = 0
                ralign.d = distances[i][j] - distances[i-1][j-1] 
                ralign.category = '1 - 1'                
            elif (di == 1 and dj == 0):
                #/* deletion */
                ralign.x1 = x[i-1]
                ralign.y1 = 0
                ralign.x2 = 0
                ralign.y2 = 0
                ralign.x3 = 0
                ralign.y3 = 0
                ralign.d = distances[i][j] - distances[i-1][j]
                ralign.category = '1 - 0'
            elif (di == 0 and dj == 1):
                #/* insertion */
                ralign.x1 = 0
                ralign.y1 = y[j-1]
                ralign.x2 = 0
                ralign.y2 = 0
                ralign.x3 = 0
                ralign.y3 = 0
                ralign.d = distances[i][j] - distances[i][j-1]        
                ralign.category = '0 - 1'
            elif (dj == 1):
                #/* contraction */
                ralign.x1 = x[i-2]
                ralign.y1 = y[j-1]
                ralign.x2 = x[i-1]
                ralign.y2 = 0
                ralign.x3 = 0
                ralign.y3 = 0
                ralign.d = distances[i][j] - distances[i-2][j-1]     
                ralign.category = '2 - 1'
            elif (di == 1):
                #/* expansion */
                ralign.x1 = x[i-1]
                ralign.y1 = y[j-2]
                ralign.x2 = 0
                ralign.y2 = y[j-1]
                ralign.x3 = 0
                ralign.y3 = 0
                ralign.d = distances[i][j] - distances[i-1][j-2]    
                ralign.category = '1 - 2'
            elif (di == 2 and dj == 2): 
                #/* di == 2 and dj == 2 */ { /* melding */
                ralign.x1 = x[i-2]
                ralign.y1 = y[j-2]
                ralign.x2 = x[i-1]
                ralign.y2 = y[j-1]
                ralign.x3 = 0
                ralign.y3 = 0
                ralign.d = distances[i][j] - distances[i-2][j-2]
                ralign.category = '2 - 2'
            elif (di == 3 and dj == 1):
                #/* deletion */
                ralign.x1 = x[i-3]
                ralign.y1 = y[j-1]
                ralign.x2 = x[i-2]
                ralign.y2 = 0
                ralign.x3 = x[i-1]
                ralign.y3 = 0
                ralign.d = distances[i][j] - distances[i-3][j-1]
                ralign.category = '3 - 1'
            elif (di == 3 and dj == 2):
                #/* insertion */
                ralign.x1 = x[i-3]
                ralign.y1 = y[j-1]
                ralign.x2 = x[i-2]
                ralign.y2 = y[j-2]
                ralign.x3 = x[i-1]
                ralign.y3 = 0
                ralign.d = distances[i][j] - distances[i-3][j-2]        
                ralign.category = '3 - 2'
            elif (di == 1 and dj == 3):
                #/* deletion */
                ralign.x1 = x[i-1]
                ralign.y1 = y[j-3]
                ralign.x2 = 0
                ralign.y2 = y[j-2]
                ralign.x3 = 0
                ralign.y3 = y[j-1]
                ralign.d = distances[i][j] - distances[i-1][j-3]
                ralign.category = '1 - 3'
            elif (di == 2 and dj == 3):
                #/* insertion */
                ralign.x1 = x[i-3]
                ralign.y1 = y[j-3]
                ralign.x2 = x[i-2]
                ralign.y2 = y[j-2]
                ralign.x3 = 0
                ralign.y3 = y[j-1]
                ralign.d = distances[i][j] - distances[i-2][j-3]        
                ralign.category = '2 - 3'
            elif (di == 3 and dj == 3): 
                ralign.x1 = x[i-3]
                ralign.y1 = y[j-3]
                ralign.x2 = x[i-2]
                ralign.y2 = y[j-2]
                ralign.x3 = x[i-1]
                ralign.y3 = y[j-1]
                ralign.d = distances[i][j] - distances[i-3][j-3]
                ralign.category = '3 - 3'
                               
            ralign_dict[n] = ralign
            
            n = n + 1
            
            i = oi
            j = oj
           
        align_dict = {}
            
        for e in range(0, n):
            align_dict[n-e-1] = ralign_dict[e] 
               
        return align_dict
        
    
##//////////////////////////////////////////////////////
##  Demonstration code
##//////////////////////////////////////////////////////

def demo_eval(alignments, gold_file):
    """
    
    """
    alignment_mappings = align_util2.get_alignment_links(alignments)
    
    print "Alignment mappings: %s" % alignment_mappings
    
    #test_values = align_util.get_test_values(alignments)
    
    reference_values = align_util2.get_reference_values(gold_file)
    
    print "Reference values: %s" % reference_values
         
    #accuracy = scores.accuracy(reference_values, test_values)
    
    #print "accuracy: %.2f" % accuracy   
                
def demo():
    """
    A demonstration for the C{Aligner} class.  
    """
        
    hard_delimiter = '.EOP'
    soft_delimiter = '.EOS'
    
    # demo 1
    input_file1 = 'data/turinen.tok'
    input_file2 = 'data/turinde.tok'
    gold_file = 'data/ground_truth.txt'
    
    gc = GaleChurchAligner(distance_measures2.two_side_distance, 'original', 
                            'bead_objects', print_flag=True)
    
    (regions1, regions2) = gc.get_delimited_regions('token',
                                                    input_file1, input_file2, 
                                                    hard_delimiter, soft_delimiter)
    
    gc_alignment = gc.batch_align(regions1, regions2)  
    
    print "Alignment0: %s" % gc_alignment
   
    demo_eval(gc_alignment, gold_file)    
        
    # demo 2
    
    hard_delimiter = '.EOP'
    soft_delimiter = '.EOS'
    
    input_file1 = 'data/bovaryen.tok'
    input_file2 = 'data/bovaryfr.tok'
    gold_file = 'data/ground_truth_bovary.txt'
    
    gc = GaleChurchAligner(distance_measures2.two_side_distance, 'original', 
                            'text_tuples', print_flag=True)
    
    (regions1, regions2) = gc.get_delimited_regions('token',
                                                    input_file1, input_file2, 
                                                    hard_delimiter, soft_delimiter)
                                                                
    gc_alignment = gc.batch_align(regions1, regions2)  
    
    print "Alignment1: %s" % gc_alignment
 
    demo_eval(gc_alignment, gold_file)
    
    # demo 3
    
    std = GaleChurchAligner(distance_measures2.two_side_distance, 'original', 
                            'text_tuples', print_flag=True)
    
    s_para_1 = [['asddd a rrg'],['hg']]
    s_para_2 = [['jk nkp'],['fg']]
    s2 = [s_para_1, s_para_2]
    
    t_para_1 = [['12345 6 78'],['910']]
    t_para_2 = [['45 67'],['89']]
    t2 = [t_para_1, t_para_2]
        
    standard_alignment2 = std.batch_align(s2, t2)
    
    print "Alignment2: %s" % standard_alignment2
    
    # demo 4
    
    s3 = [['asddd','a','rrg'],['hg']]
    t3 = [['12345','6','78'],['910']]
    
    standard_alignment3 = std.align(s3, t3)
    
    print "Alignment3: %s" % standard_alignment3
    
    # demo 5
    
    top_down_alignments = std.recursive_align(s3, t3)  
    
    for alignment in top_down_alignments:
        print "Top down align: %s" % alignment
    
if __name__=='__main__':
    demo()
        


########NEW FILE########
__FILENAME__ = alignment_util


# Utility Functions

def readlines(filename):  
    """ 
    Return an array of strings, one string for each line of the file
    set len_ptr to the number of lines in the file.
    
    @type filename: C{string}
    @param filename: 
        
    @return: lines
    @rtype: C{list}  
    
    @return: len_ptr
    @rtype: C{int}  
    
    """
    lines = []
    
    input_file = file(filename, "r")
    substrings = []
    file_text = input_file.read()
    
    raw_lines = file_text.split('\n')
    lines = [line for line in raw_lines if not(line.strip() == '')]
                
    len_ptr = len(lines)
    
    return (lines, len_ptr)
    
def get_test_values(alignments):
    """ 
    
    @type alignments: C{dict}
    @param alignments: 
        
    @return: 
    @rtype: C{list}    
    
    """
    test_values = []
    for hard_regions_index in alignments.keys():
        soft_regions_list = []
        for soft_regions_index in alignments[hard_regions_index].keys():
            soft_regions_list.extend(alignments[hard_regions_index][soft_regions_index].alignment_mappings) 
        soft_regions_list.reverse()
        test_values.extend(soft_regions_list)
        
    return test_values
    
def get_reference_values(filename):
    """ 
    
    @type filename: C{string}
    @param filename: 
        
    @return: 
    @rtype: C{list}    
    
    """
    
    input_file = file(filename, "r")
    reference_values = []
    
    raw_lines = input_file.read().split('\n')
    lines = [line for line in raw_lines if not(line.strip() == '')]
    
    for line in lines:                
        line_parts = line.split(',')        
        reference_values.append((int(line_parts[0]),int(line_parts[1]),int(line_parts[2])))
    
    return reference_values
    
    

########NEW FILE########
__FILENAME__ = align_regions

# Based on Gale & Church 1993, "A Program for Aligning Sentences in Bilingual Corpora"
# This is a Python version of the C implementation by Mike Riley presented in the appendix
# of the paper. The documentation in the C program is retained where applicable.

import sys

from nltk.metrics import scores

import distance_measures
import alignment_util

##//////////////////////////////////////////////////////
##  Alignment
##//////////////////////////////////////////////////////

class Alignment(object):
    """
    
    
    """
    def __init__(self): 
        """
        
        """
        self.x1 = 0
        self.y1 = 0
        self.x2 = 0
        self.y2 = 0
        self.d = 0  
        self.category = ''
        self.hard_regions_index = 0
        self.soft_regions_index = 0
        self.alignment_mappings = []
    
    def set_alignment_mappings(self):        
        """
        
        """
        if (self.category == '1 - 1'):
            align_triple = (self.hard_regions_index,
                            self.soft_regions_index,
                            self.soft_regions_index)
            self.alignment_mappings.append(align_triple)
        elif (self.category == '1 - 0'):
            align_triple = (self.hard_regions_index,
                            self.soft_regions_index,
                            -1)
            self.alignment_mappings.append(align_triple)
        elif (self.category == '0 - 1'):
            align_triple = (self.hard_regions_index,
                            -1,
                            self.soft_regions_index)
            self.alignment_mappings.append(align_triple)
        elif (self.category == '2 - 1'):
            align_triple = (self.hard_regions_index,
                            self.soft_regions_index + 1,
                            self.soft_regions_index)
            
            self.alignment_mappings.append(align_triple)
            align_triple = (self.hard_regions_index,
                            self.soft_regions_index,
                            self.soft_regions_index)
            self.alignment_mappings.append(align_triple)
        elif (self.category == '1 - 2'):
            align_triple = (self.hard_regions_index,
                            self.soft_regions_index,
                            self.soft_regions_index + 1)            
            self.alignment_mappings.append(align_triple)
            align_triple = (self.hard_regions_index,
                            self.soft_regions_index,
                            self.soft_regions_index)
            self.alignment_mappings.append(align_triple)
        elif (self.category == '2 - 2'):
            align_triple = (self.hard_regions_index,
                            self.soft_regions_index + 1,
                            self.soft_regions_index + 1)            
            self.alignment_mappings.append(align_triple)
            align_triple = (self.hard_regions_index,
                            self.soft_regions_index,
                            self.soft_regions_index)
            self.alignment_mappings.append(align_triple)
        else:
            print "not supported alignment type"

##//////////////////////////////////////////////////////
##  Aligner
##//////////////////////////////////////////////////////

class Aligner(object):
    """
    
      usage
    
      align -D '.EOP' -d '.EOS' <File1> <File2>
    
      outputs two files: <File11>.al & <File2>.al
    
      regions are delimited by the -D and -d args
    
      the program is allowed to delete -d delimiters as necessary in order
      align the files, but it cannot change -D delimiters.
      
    """
    
    def __init__(self, input_file1, input_file2, hard_delimiter, soft_delimiter):
        """
        
        """
        self.input_file1 = input_file1
        self.input_file2 = input_file2
        
        self.hard_delimiter = hard_delimiter
        self.soft_delimiter = soft_delimiter         
    
    def align_regions(self, dist_funct, debug=False, verbose=False):        
        """
        
        """
        alignments = {}
                
        (lines1, number_of_lines1) = alignment_util.readlines(self.input_file1)
        (lines2, number_of_lines2) = alignment_util.readlines(self.input_file2)
        
        tmp = Region(lines1, number_of_lines1)
        
        (hard_regions1, number_of_hard_regions1) = tmp.find_sub_regions(self.hard_delimiter)
        
        tmp.lines = lines2
        tmp.length = number_of_lines2
        
        (hard_regions2, number_of_hard_regions2) = tmp.find_sub_regions(self.hard_delimiter)        
        
        if (number_of_hard_regions1 != number_of_hard_regions2):
            print "align_regions: input files do not contain the same number of hard regions" + '\n'
            print "%s" % hard_delimiter + '\n'
            print "%s has %d and %s has %d" % (self.input_file1, number_of_hard_regions1, \
                                               self.input_file2, number_of_hard_regions2) + '\n'
            
            return
        
        hard_regions_index = 0        
        
        while (hard_regions_index < len(hard_regions1)):    
            (soft_regions1, number_of_soft_regions1) = \
                hard_regions1[hard_regions_index].find_sub_regions(self.soft_delimiter)            
            (soft_regions2, number_of_soft_regions2) = \
                hard_regions2[hard_regions_index].find_sub_regions(self.soft_delimiter)
                
            if (debug):
               out1.write("Text 1:number of soft regions=%d\n" % number_of_soft_regions1 + '\n')
               out1.write("Text 2:number of soft regions=%d\n" % number_of_soft_regions2 + '\n')
               out2.write("Text 1:number of soft regions=%d\n" % number_of_soft_regions1 + '\n')
               out2.write("Text 2:number of soft regions=%d\n" % number_of_soft_regions2 + '\n')
               
            len1 = []
            for reg in soft_regions1:
                len_lines = 0
                for li in reg.lines:
                    len_lines = len_lines + len(li)
                
                len1.append(len_lines)
                
            len2 = []
            for reg in soft_regions2:
                len_lines = 0
                for li in reg.lines:
                    len_lines = len_lines + len(li)
                
                len2.append(len_lines)                
                        
            (n, align) = self.seq_align(len1, 
                                        len2, 
                                        number_of_soft_regions1, 
                                        number_of_soft_regions2, 
                                        dist_funct,
                                        hard_regions_index)                                       
            
            alignments[hard_regions_index] = align
            
            self.output_alignment(n, align, 
                                  soft_regions1, soft_regions2,
                                  debug, verbose)
            
            hard_regions_index = hard_regions_index + 1
                        
        return alignments        
    
    def seq_align(self, x, y, nx, ny, dist_funct, hard_regions_index):    
        """
        
        Sequence alignment routine.
        This version allows for contraction/expansions.
        
        x and y are sequences of objects, represented as non-zero ints, to be aligned.
        
        dist_funct(x1, y1, x2, y2) is a distance function of 4 args:          
        
        dist_funct(x1, y1, 0, 0) gives cost of substitution of x1 by y1.
        dist_funct(x1, 0, 0, 0) gives cost of deletion of x1.
        dist_funct(0, y1, 0, 0) gives cost of insertion of y1.
        dist_funct(x1, y1, x2, 0) gives cost of contraction of (x1,x2) to y1.
        dist_funct(x1, y1, 0, y2) gives cost of expansion of x1 to (y1,y2).
        dist_funct(x1, y1, x2, y2) gives cost to match (x1,x2) to (y1,y2).
        
        align is the alignment, with (align[i].x1, align[i].x2) aligned
        with (align[i].y1, align[i].y2).  Zero in align[].x1 and align[].y1
        correspond to insertion and deletion, respectively.  Non-zero in
        align[].x2 and align[].y2 correspond to contraction and expansion,
        respectively.  align[].d gives the distance for that pairing.
        
        The function returns the length of the alignment.
        (The Python version also returns the alignment as a Python dictionary)
        
        """
        
        distances = []
        path_x = []
        path_y = [] 
        
        first_len = nx + 1
        second_len = ny + 1
        
        distances = [[0] * second_len for c in range(first_len)]         
        path_x = [[0] * second_len for c in range(first_len)]              
        path_y = [[0] * second_len for c in range(first_len)]
                  
        d1 = d2 = d3 = d4 = d5 = d6 = sys.maxint
        
        for j in range(0, ny + 1):    
            for i in range(0, nx + 1):            
                if (i > 0 and j > 0):		
                    #/* substitution */
                    d1 = distances[i-1][j-1] + \
                        dist_funct(x[i-1], y[j-1], 0, 0)
                else:
                    d1 = sys.maxint
                    
                if (i > 0):	
                    #/* deletion */
                    d2 = distances[i-1][j] + \
                        dist_funct(x[i-1], 0, 0, 0)
                else:
                    d2 = sys.maxint
                    
                if (j > 0):		
                    #/* insertion */
                    d3 = distances[i][j-1] + \
                        dist_funct(0, y[j-1], 0, 0)
                else:
                    d3 = sys.maxint
                    
                if (i > 1 and j > 0):		
                    #/* contraction */
                    d4 = distances[i-2][j-1] + \
                        dist_funct(x[i-2], y[j-1], x[i-1], 0)
                else:
                    d4 = sys.maxint
                    
                if (i > 0 and j > 1):		
                    #/* expansion */
                    d5 = distances[i-1][j-2] + \
                        dist_funct(x[i-1], y[j-2], 0, y[j-1])
                else:
                    d5 = sys.maxint
                    
                if (i > 1 and j > 1):		
                    #/* melding */
                    d6 = distances[i-2][j-2] + \
                        dist_funct(x[i-2], y[j-2], x[i-1], y[j-1])
                else:
                    d6 = sys.maxint
     
                dmin = min(d1, d2, d3, d4, d5, d6)
                
                if (dmin == sys.maxint):
                    distances[i][j] = 0
                elif (dmin == d1):
                    distances[i][j] = d1                
                    path_x[i][j] = i - 1
                    path_y[i][j] = j - 1              
                elif (dmin == d2):
                    distances[i][j] = d2                
                    path_x[i][j] = i - 1
                    path_y[i][j] = j                
                elif (dmin == d3):
                    distances[i][j] = d3                
                    path_x[i][j] = i
                    path_y[i][j] = j - 1                
                elif (dmin == d4):
                    distances[i][j] = d4                
                    path_x[i][j] = i - 2
                    path_y[i][j] = j - 1                 
                elif (dmin == d5):
                    distances[i][j] = d5                
                    path_x[i][j] = i - 1
                    path_y[i][j] = j - 2
                else:			
                    # /* dmin == d6 */ {
                    distances[i][j] = d6                
                    path_x[i][j] = i - 2
                    path_y[i][j] = j - 2
        n = 0
        
        ralign_dict = {}
        
        i = nx
        j = ny
        while (i > 0 or j > 0):                
            oi = path_x[i][j]       
            oj = path_y[i][j]
            di = i - oi
            dj = j - oj
            
            ralign = Alignment()
                          
            if (di == 1 and dj == 1):
                #/* substitution */            
                ralign.x1 = x[i-1]
                ralign.y1 = y[j-1]
                ralign.x2 = 0
                ralign.y2 = 0
                ralign.d = distances[i][j] - distances[i-1][j-1] 
                ralign.category = '1 - 1'
                
            elif (di == 1 and dj == 0):
                #/* deletion */
                ralign.x1 = x[i-1]
                ralign.y1 = 0
                ralign.x2 = 0
                ralign.y2 = 0
                ralign.d = distances[i][j] - distances[i-1][j]
                ralign.category = '1 - 0'
            elif (di == 0 and dj == 1):
                #/* insertion */
                ralign.x1 = 0
                ralign.y1 = y[j-1]
                ralign.x2 = 0
                ralign.y2 = 0
                ralign.d = distances[i][j] - distances[i][j-1]        
                ralign.category = '0 - 1'
            elif (dj == 1):
                #/* contraction */
                ralign.x1 = x[i-2]
                ralign.y1 = y[j-1]
                ralign.x2 = x[i-1]
                ralign.y2 = 0
                ralign.d = distances[i][j] - distances[i-2][j-1]     
                ralign.category = '2 - 1'
            elif (di == 1):
                #/* expansion */
                ralign.x1 = x[i-1]
                ralign.y1 = y[j-2]
                ralign.x2 = 0
                ralign.y2 = y[j-1]
                ralign.d = distances[i][j] - distances[i-1][j-2]    
                ralign.category = '1 - 2'
            else: 
                #/* di == 2 and dj == 2 */ { /* melding */
                ralign.x1 = x[i-2]
                ralign.y1 = y[j-2]
                ralign.x2 = x[i-1]
                ralign.y2 = y[j-1]
                ralign.d = distances[i][j] - distances[i-2][j-2]
                ralign.category = '2 - 2'
            
            ralign.hard_regions_index = hard_regions_index
            ralign.soft_regions_index = n
            ralign.set_alignment_mappings()
            
            ralign_dict[n] = ralign
            
            n = n + 1
            
            i = oi
            j = oj
           
        align_dict = {}
            
        for e in range(0, n):
            align_dict[n-e-1] = ralign_dict[e] 
               
        return (n, align_dict)
        
    def output_alignment(self, n, align, soft_regions1, soft_regions2, debug, verbose):
        """
        
        """
        out1 = open(self.input_file1 + '.al', 'w')
        out2 = open(self.input_file2 + '.al', 'w')
        
        prevx = 0
        prevy = 0
        ix = 0
        iy = 0
        
        for i in range(0,n):
            a = align[i]
            
            if (a.x2 > 0):
              ix = ix + 1
            elif(a.x1 == 0): 
              ix = ix - 1
              
            if (a.y2 > 0): 
              iy = iy + 1
            elif(a.y1 == 0):
              iy = iy - 1
                            
            if (a.x1 == 0 and a.y1 == 0 and a.x2 == 0 and a.y2 == 0):
              ix = ix + 1
              iy = iy + 1
                        
            ix = ix + 1
            iy = iy + 1
            
            if (debug):
                out1.write("Par nr %d:\n" % i+1 + '\n')
                out2.write("Par nr %d:\n" % i+1 + '\n')
      
            if (verbose):
                out1.write(".Score %d\n" % a.d + '\n')
                out2.write(".Score %d\n" % a.d + '\n')
                
            out1.write("*** Link: %s ***\n" % (a.category) + '\n')            
            out2.write("*** Link: %s ***\n" % (a.category) + '\n')            
            
            while (prevx < ix):
              if (debug):
                  out1.write("Text 1:ix=%d prevx=%d\n" % (ix, prevx) + '\n')
                  out2.write("Text 1:ix=%d prevx=%d\n" % (ix, prevx) + '\n')
              soft_regions1[prevx].print_region(out1, out2, a.d)                  
              prevx = prevx + 1
            
            while (prevy < iy):
              if (debug): 
                  out1.write("Text 2:ix=%d prevx=%d\n" % (iy, prevy) + '\n')
                  out2.write("Text 2:ix=%d prevx=%d\n" % (iy, prevy) + '\n')                      
              soft_regions2[prevy].print_region(out1, out2, a.d)                  
              prevy = prevy + 1        
        
# Functions for Manipulating Regions

##//////////////////////////////////////////////////////
##  Region
##//////////////////////////////////////////////////////

class Region(object):
    """
    
    
    """
    def __init__(self, lines, length): 
        """
        
        """
        self.lines = lines
        self.length = length
        
    def print_region(self, out1, out2, score):
        """
        
        """        
        sentence_text = " ".join(self.lines)    
        out1.write("score: %s %s" % (score, sentence_text) + '\n')
        out2.write("score: %s %s" % (score, sentence_text) + '\n')
    
    def find_sub_regions(self, delimiter):
        """
        
        """
        result = []  
        
        region_lines = []
        num_lines = 0
        
        for line in self.lines:          
          if delimiter and not(line.find(delimiter) == -1):
              result.append(Region(region_lines, num_lines))
              num_lines = 0
              region_lines = []   
          else:              
              region_lines.append(line)
              num_lines = num_lines + 1
        
        if (region_lines): 
          result.append(Region(region_lines, num_lines))
           
        return (result, len(result))

##//////////////////////////////////////////////////////
##  Demonstration code
##//////////////////////////////////////////////////////

def demo_eval(alignments, gold_file):
    """
    
    """
    test_values = alignment_util.get_test_values(alignments)
    
    reference_values = alignment_util.get_reference_values(gold_file)
         
    accuracy = scores.accuracy(reference_values, test_values)
    
    print "accuracy: %.2f" % accuracy   
                
def demo():
    """
    A demonstration for the C{Aligner} class.  
    """
    
    hard_delimiter = '.EOP'
    soft_delimiter = '.EOS'
    
    # demo 1
    input_file1 = 'data/turinen.tok'
    input_file2 = 'data/turinde.tok'
    gold_file = 'data/ground_truth.txt'
    aligner = Aligner(input_file1, input_file2, hard_delimiter, soft_delimiter)
    
    alignments = aligner.align_regions(distance_measures.two_side_distance)
    
    demo_eval(alignments, gold_file)    
    
    # demo 2
    input_file1 = 'data/bovaryen.tok'
    input_file2 = 'data/bovaryfr.tok'
    gold_file = 'data/ground_truth_bovary.txt'
    aligner = Aligner(input_file1, input_file2, hard_delimiter, soft_delimiter)
    
    alignments = aligner.align_regions(distance_measures.two_side_distance)
    
    demo_eval(alignments, gold_file)    
    
if __name__=='__main__':
    demo()
    
    """
    if len(sys.argv) > 1:
        source_file_name = sys.argv[1]
        target_file_name = sys.argv[2]               
    else:
        sys.exit('Usage: arg1 - source input filename arg2 - target input filename')
    """
    
    


########NEW FILE########
__FILENAME__ = align_util

# Natural Language Toolkit: Gale-Church Aligner Utilities
#
# Copyright (C) 2001-2011 NLTK Project
# Author: Chris Crowner <ccrowner@gmail.com>
# URL: <http://nltk.org/>
# For license information, see LICENSE.TXT

# Utility Functions

def readlines(filename):  
    lines = []
    
    input_file = file(filename, "r")
    substrings = []
    file_text = input_file.read()
    
    raw_lines = file_text.split('\n')
    lines = [line for line in raw_lines if not(line.strip() == '')]
                        
    return lines
    
def get_regions(lines, hard_delimiter, soft_delimiter):
    hard_regions = []          
    soft_regions = []
    soft_region = []        
    
    for line in lines:          
      if not(line.find(hard_delimiter) == -1) and (soft_regions):          
          hard_regions.append(soft_regions)           
          soft_regions = [] 
      elif not(line.find(soft_delimiter) == -1) and (soft_region):          
          soft_regions.append(soft_region)
          soft_region = []
      else:
          soft_region.append(line)
    
    if (soft_regions):
        hard_regions.append(soft_regions) 
    
    return hard_regions 
    
def get_paragraphs_sentences(lines, para_delimiter, sent_delimiter):
    paragraphs = []          
    sentences = []
    sentence = ''        
    
    for line in lines:          
      if not(line.find(para_delimiter) == -1) and (sentences):          
          paragraphs.append(sentences)           
          sentences = [] 
      elif not(line.find(sent_delimiter) == -1) and (sentence):          
          sentences.append(sentence)
          sentence = ''
      else:
          if sentence:
              sentence = sentence + ' ' + line
          else:
              sentence = line
    
    if (sentences):
        paragraphs.append(sentences) 
    
    return paragraphs      

def get_character_lengths(region):
    character_lengths = []    
    
    for soft_reg in region:
        len_lines = 0
        for line in soft_reg:
            len_lines = len_lines + len(line)
            
        character_lengths.append(len_lines)
        
    return character_lengths

def print_alignment_text_mapping(alignment_mapping):
    entry_num = 0
    for entry in alignment_mapping:
        print "--------------------------------"
        print "Entry: %d" % entry_num
        entry_num = entry_num + 1
        print "%s" % str(entry[0])
        print "%s" % str(entry[1])
        
def print_alignment_index_mapping(alignment_mapping_indices):
    entry_num = 0
    for entry in alignment_mapping_indices:
        print "--------------------------------"
        print "Indices Entry: %d" % entry_num
        entry_num = entry_num + 1
        source = entry[0]
        target = entry[1]
        print "%s" % str(source)
        print "%s" % str(target) 
        
def print_alignments(alignments, hard_region1, hard_region2):
    hard1_key = 0
    hard2_key = 0
    for soft_key in alignments.keys():            
        alignment = alignments[soft_key]        
        if (alignment.category == '1 - 1'):
            print "1-1: %s" % alignment.d
            print "--------------------------"
            print "%s" % hard_region1[hard1_key]
            print "%s" % hard_region2[hard2_key]
            print "--------------------------"                
            hard1_key = hard1_key + 1
            hard2_key = hard2_key + 1
        elif (alignment.category == '1 - 0'):
            print "1-0: %s" % alignment.d
            print "--------------------------"
            print "%s" % hard_region1[hard1_key]                    
            print "--------------------------"                
            hard1_key = hard1_key + 1            
        elif (alignment.category == '0 - 1'):
            print "0-1: %s" % alignment.d
            print "--------------------------"
            print "%s" % hard_region2[hard2_key]                    
            print "--------------------------"                              
            hard2_key = hard2_key + 1
        elif (alignment.category == '2 - 1'):
            print "2-1: %.2f" % alignment.d
            print "--------------------------"
            print "%s" % hard_region1[hard1_key]
            print "%s" % hard_region1[hard1_key + 1]
            print "%s" % hard_region2[hard2_key]
            print "--------------------------"  
            hard1_key = hard1_key + 2
            hard2_key = hard2_key + 1
        elif (alignment.category == '1 - 2'):
            print "1-2: %.2f" % alignment.d
            print "--------------------------"
            print "%s" % hard_region1[hard1_key]                    
            print "%s" % hard_region2[hard2_key]
            print "%s" % hard_region2[hard2_key + 1]
            print "--------------------------" 
            hard1_key = hard1_key + 1
            hard2_key = hard2_key + 2
        elif (alignment.category == '2 - 2'):
            print "2-2: %.2f" % alignment.d
            print "--------------------------"
            print "%s" % hard_region1[hard1_key]
            print "%s" % hard_region1[hard1_key + 1]
            print "%s" % hard_region2[hard2_key]
            print "%s" % hard_region2[hard2_key + 1]
            print "--------------------------"                    
            hard1_key = hard1_key + 2
            hard2_key = hard2_key + 2
        elif (alignment.category == '3 - 1'):
            print "3-1: %.2f" % alignment.d
            print "--------------------------"
            print "%s" % hard_region1[hard1_key]
            print "%s" % hard_region1[hard1_key + 1]
            print "%s" % hard_region1[hard1_key + 2]
            print "%s" % hard_region2[hard2_key]            
            print "--------------------------"                    
            hard1_key = hard1_key + 3
            hard2_key = hard2_key + 1
        elif (alignment.category == '3 - 2'):
            print "3-2: %.2f" % alignment.d
            print "--------------------------"
            print "%s" % hard_region1[hard1_key]
            print "%s" % hard_region1[hard1_key + 1]
            print "%s" % hard_region1[hard1_key + 2]
            print "%s" % hard_region2[hard2_key]
            print "%s" % hard_region2[hard2_key + 1]              
            print "--------------------------"                    
            hard1_key = hard1_key + 3
            hard2_key = hard2_key + 2
        elif (alignment.category == '1 - 3'):
            print "1-3: %.2f" % alignment.d
            print "--------------------------"
            print "%s" % hard_region1[hard1_key]
            print "%s" % hard_region2[hard2_key]
            print "%s" % hard_region2[hard2_key + 1]
            print "%s" % hard_region2[hard2_key + 2]                     
            print "--------------------------"                    
            hard1_key = hard1_key + 1
            hard2_key = hard2_key + 3
        elif (alignment.category == '2 - 3'):
            print "2-3: %.2f" % alignment.d
            print "--------------------------"
            print "%s" % hard_region1[hard1_key]
            print "%s" % hard_region1[hard1_key + 1]
            print "%s" % hard_region2[hard2_key]
            print "%s" % hard_region2[hard2_key + 1]
            print "%s" % hard_region2[hard2_key + 2]                     
            print "--------------------------"                    
            hard1_key = hard1_key + 2
            hard2_key = hard2_key + 3
        elif (alignment.category == '3 - 3'):
            print "3-3: %.2f" % alignment.d
            print "--------------------------"
            print "%s" % hard_region1[hard1_key]
            print "%s" % hard_region1[hard1_key + 1]
            print "%s" % hard_region1[hard1_key + 2]
            print "%s" % hard_region2[hard2_key]
            print "%s" % hard_region2[hard2_key + 1]
            print "%s" % hard_region2[hard2_key + 2]                     
            print "--------------------------"                    
            hard1_key = hard1_key + 3
            hard2_key = hard2_key + 3
        else:
            print "not supported alignment type"    

def list_to_str(input_list):
    return input_list
    #return ' '.join([item for item in input_list])
    
def convert_bead_to_tuples(alignments, hard_region1, hard_region2):
    alignment_mapping = []
    alignment_mapping_indices = []
    hard1_key = 0
    hard2_key = 0
    for soft_key in alignments.keys():
        alignment = alignments[soft_key]        
        if (alignment.category == '1 - 1'):            
            align_tuple = (list_to_str(hard_region1[hard1_key]), list_to_str(hard_region2[hard2_key]))
            alignment_mapping.append(align_tuple)
            alignment_mapping_indices.append(([hard1_key], [hard2_key]))
            hard1_key = hard1_key + 1
            hard2_key = hard2_key + 1
        elif (alignment.category == '1 - 0'):
            align_tuple = (list_to_str(hard_region1[hard1_key]), '')
            alignment_mapping.append(align_tuple)
            alignment_mapping_indices.append(([hard1_key], []))
            hard1_key = hard1_key + 1            
        elif (alignment.category == '0 - 1'):
            align_tuple = ('', list_to_str(hard_region2[hard2_key]))
            alignment_mapping.append(align_tuple)
            alignment_mapping_indices.append(([], [hard2_key]))            
            hard2_key = hard2_key + 1
        elif (alignment.category == '2 - 1'):
            align_tuple = (list_to_str(hard_region1[hard1_key]), list_to_str(hard_region2[hard2_key]))
            alignment_mapping.append(align_tuple)
            align_tuple = (list_to_str(hard_region1[hard1_key + 1]), list_to_str(hard_region2[hard2_key]))
            alignment_mapping.append(align_tuple)
            alignment_mapping_indices.append(([hard1_key, hard1_key + 1], [hard2_key]))
            hard1_key = hard1_key + 2
            hard2_key = hard2_key + 1
        elif (alignment.category == '1 - 2'):
            align_tuple = (list_to_str(hard_region1[hard1_key]), list_to_str(hard_region2[hard2_key]))
            alignment_mapping.append(align_tuple)
            align_tuple = (list_to_str(hard_region1[hard1_key]), list_to_str(hard_region2[hard2_key + 1]))
            alignment_mapping.append(align_tuple)
            alignment_mapping_indices.append(([hard1_key], [hard2_key, hard2_key + 1]))
            hard1_key = hard1_key + 1
            hard2_key = hard2_key + 2
        elif (alignment.category == '2 - 2'):
            align_tuple = (list_to_str(hard_region1[hard1_key]), list_to_str(hard_region2[hard2_key]))
            alignment_mapping.append(align_tuple)
            align_tuple = (list_to_str(hard_region1[hard1_key + 1]), list_to_str(hard_region2[hard2_key + 1]))
            alignment_mapping.append(align_tuple)
            alignment_mapping_indices.append(([hard1_key, hard1_key + 1], [hard2_key, hard2_key + 1]))
            hard1_key = hard1_key + 2
            hard2_key = hard2_key + 2
        elif (alignment.category == '3 - 1'):
            align_tuple = (list_to_str(hard_region1[hard1_key]), list_to_str(hard_region2[hard2_key]))
            alignment_mapping.append(align_tuple)
            align_tuple = (list_to_str(hard_region1[hard1_key + 1]), list_to_str(hard_region2[hard2_key]))
            alignment_mapping.append(align_tuple)
            align_tuple = (list_to_str(hard_region1[hard1_key + 2]), list_to_str(hard_region2[hard2_key]))
            alignment_mapping.append(align_tuple)
            alignment_mapping_indices.append(([hard1_key, hard1_key + 1, hard1_key + 2], [hard2_key]))
            hard1_key = hard1_key + 3
            hard2_key = hard2_key + 1
        elif (alignment.category == '3 - 2'):
            # note: this seems to select only one of the options 1->1, 2->2, 3->2
            # and not 1->1, 2->1, 3->2 (how is this done correctly?)
            align_tuple = (list_to_str(hard_region1[hard1_key]), list_to_str(hard_region2[hard2_key]))
            alignment_mapping.append(align_tuple)
            align_tuple = (list_to_str(hard_region1[hard1_key + 1]), list_to_str(hard_region2[hard2_key + 1]))
            alignment_mapping.append(align_tuple)
            align_tuple = (list_to_str(hard_region1[hard1_key + 2]), list_to_str(hard_region2[hard2_key + 1]))
            alignment_mapping.append(align_tuple)
            alignment_mapping_indices.append(([hard1_key, hard1_key + 1, hard1_key + 2], [hard2_key, hard2_key + 1]))
            hard1_key = hard1_key + 3
            hard2_key = hard2_key + 2
        elif (alignment.category == '1 - 3'):            
            align_tuple = (list_to_str(hard_region1[hard1_key]), list_to_str(hard_region2[hard2_key]))
            alignment_mapping.append(align_tuple)
            align_tuple = (list_to_str(hard_region1[hard1_key]), list_to_str(hard_region2[hard2_key + 1]))
            alignment_mapping.append(align_tuple)
            align_tuple = (list_to_str(hard_region1[hard1_key]), list_to_str(hard_region2[hard2_key + 2]))
            alignment_mapping.append(align_tuple)
            alignment_mapping_indices.append(([hard1_key], [hard2_key, hard2_key + 1, hard2_key + 2]))
            hard1_key = hard1_key + 1
            hard2_key = hard2_key + 3
        elif (alignment.category == '2 - 3'):
            # note: this seems to select only one of the options 1->1, 2->2, 2->3
            # and not 1->1, 1->2, 2->3 (how is this done correctly?)
            align_tuple = (list_to_str(hard_region1[hard1_key]), list_to_str(hard_region2[hard2_key]))
            alignment_mapping.append(align_tuple)
            align_tuple = (list_to_str(hard_region1[hard1_key + 1]), list_to_str(hard_region2[hard2_key + 1]))
            alignment_mapping.append(align_tuple)
            align_tuple = (list_to_str(hard_region1[hard1_key + 1]), list_to_str(hard_region2[hard2_key + 2]))
            alignment_mapping.append(align_tuple)
            alignment_mapping_indices.append(([hard1_key, hard1_key + 1], [hard2_key, hard2_key + 1, hard2_key + 2]))
            hard1_key = hard1_key + 2
            hard2_key = hard2_key + 3
        elif (alignment.category == '3 - 3'):            
            align_tuple = (list_to_str(hard_region1[hard1_key]), list_to_str(hard_region2[hard2_key]))
            alignment_mapping.append(align_tuple)
            align_tuple = (list_to_str(hard_region1[hard1_key + 1]), list_to_str(hard_region2[hard2_key + 1]))
            alignment_mapping.append(align_tuple)
            align_tuple = (list_to_str(hard_region1[hard1_key + 2]), list_to_str(hard_region2[hard2_key + 2]))
            alignment_mapping.append(align_tuple)
            alignment_mapping_indices.append(([hard1_key, hard1_key + 1, hard1_key + 2], [hard2_key, hard2_key + 1, hard2_key + 2]))
            hard1_key = hard1_key + 3
            hard2_key = hard2_key + 3
        else:
            print "not supported alignment type"
    
    return (alignment_mapping, alignment_mapping_indices)
    
def get_alignment_links(alignments):
    alignment_mappings = []
    hard_key = 0
    for hard_list in alignments:        
        for alignment_dict in hard_list:            
            for align_key in alignment_dict.keys():
                alignment = alignment_dict[align_key]
    
                if (alignment.category == '1 - 1'):
                    align_triple = (hard_key,
                                    align_key,
                                    align_key)
                    alignment_mappings.append(align_triple)
                elif (alignment.category == '1 - 0'):
                    align_triple = (hard_key,
                                    align_key,
                                    -1)
                    alignment_mappings.append(align_triple)
                elif (alignment.category == '0 - 1'):
                    align_triple = (hard_key,
                                    -1,
                                    align_key)
                    alignment_mappings.append(align_triple)
                elif (alignment.category == '2 - 1'):
                    align_triple = (hard_key,
                                    align_key + 1,
                                    align_key)        
                    alignment_mappings.append(align_triple)
                    align_triple = (hard_key,
                                    align_key,
                                    align_key)
                    alignment_mappings.append(align_triple)
                elif (alignment.category == '1 - 2'):
                    align_triple = (hard_key,
                                    align_key,
                                    align_key + 1)            
                    alignment_mappings.append(align_triple)
                    align_triple = (hard_key,
                                    align_key,
                                    align_key)
                    alignment_mappings.append(align_triple)
                elif (alignment.category == '2 - 2'):
                    align_triple = (hard_key,
                                    align_key + 1,
                                    align_key + 1)            
                    alignment_mappings.append(align_triple)
                    align_triple = (hard_key,
                                    align_key,
                                    align_key)
                    alignment_mappings.append(align_triple)
                else:
                    print "not supported alignment type"
                
    return alignment_mappings
    
def get_test_values(alignments):
    test_values = []
    for hard_regions_index in alignments.keys():
        soft_regions_list = []
        for soft_regions_index in alignments[hard_regions_index].keys():
            soft_regions_list.extend(alignments[hard_regions_index][soft_regions_index].alignment_mappings) 
        soft_regions_list.reverse()
        test_values.extend(soft_regions_list)
        
    return test_values
    
def get_reference_values(filename):
    input_file = file(filename, "r")
    reference_values = []
    
    raw_lines = input_file.read().split('\n')
    lines = [line for line in raw_lines if not(line.strip() == '')]
    
    for line in lines:                
        line_parts = line.split(',')        
        reference_values.append((int(line_parts[0]),int(line_parts[1]),int(line_parts[2])))
    
    return reference_values
    
    

########NEW FILE########
__FILENAME__ = api
# Natural Language Toolkit: Aligner Interface

"""
Interfaces for aligning bitexts

L{AlignerI} is a standard interface for X{bilingual alignment}

"""
from nltk.internals import deprecated, overridden
from itertools import izip

##//////////////////////////////////////////////////////
# Alignment Interfaces
##//////////////////////////////////////////////////////

class AlignerI(object):
    """
    A processing interface for I{aligning} two lists of text sections
    (i.e., mapping text sections from a source text to text sections from a target text)
    
    Subclasses must define:      
      - either L{align()} or L{batch_align()} (or both)
      
    Subclasses may define:
      - either L{prob_align()} or L{batch_prob_align()} (or both)
    """
    def align(self, source_text_sections, target_text_sections):
        """
        @return: the alignment of the two texts.
        @rtype: a C{list} of C{tuple} pairs where 
        1. the first element is a section of source text 
        2. the second element is the aligned section(s) of target text
           or 
        1. the first element is an identifier of a section of source text 
        2. the second element is identifier(s) of aligned section(s) of target text
        The second option is necessary for cases where crossing alignments are permitted, as
        in word alignment implementations.
        
        Both elements of the returned tuples are lists - either empty lists, 
        (in the case of ommitted/deleted text) or single or multiple element lists
        """
        if overridden(self.batch_align):
            return self.batch_align([source_text_sections], [target_text_sections])
        else:
            raise NotImplementedError()

    def batch_align(self, source, target):
        """
        Apply L{self.align()} to the elements of the C{source} and C{target}
            texts.  I.e.:

            >>> return [self.align(st, tt) for (st, tt) in izip(source, target)]

        @rtype: C{list} of I{alignments}
        """
        return [self.align(st, tt) for (st, tt) in izip(source, target)]
    
    def recursive_align(self, source, target, alignments):
        """
        Apply L{self.align()} to the elements of the C{source} and C{target}
            texts in a top-down manner

        @rtype: C{list} of I{alignments}
        """
        standard_alignment = self.align(source, target)
       
        alignments.append(standard_alignment)
                
        alignment_mapping = None
        if (self.output_format == 'text_tuples'):            
            alignment_mapping = standard_alignment
        
        import align_util
        
        if (self.output_format == 'bead_objects'):
            (alignment_mapping, alignment_mapping_indices) = align_util.convert_bead_to_tuples(standard_alignment, source, target)
             
        for entry in alignment_mapping:                            
            source_list = [item for item in entry[0]]                
            target_list = [item for item in entry[1]] 
            
            if len(source_list) == 0 or len(target_list) == 0:                
                break
            if not(isinstance(source_list[0], list)) or not(isinstance(target_list[0], list)):                
                break
                
            lower_align = self.recursive_align(source_list, target_list, alignments)                        
            
        return alignments
    
    def textfile_align(self, source_file, target_file):
        """
        Apply L{self.batch_align()} to the text of two files. This method will
        parse the input files into the list structures required
        (using an input routine? punkt.py?)            

        @rtype: C{list} of I{alignments}
        """
        pass
        
    def text_align(self, source_text, target_text):
        """
        Apply L{self.align()} to source and target text. This method will
        parse the input texts (using an input routine? punkt.py?).
        
        This method is for primarily for testing alignments using the Python interpreter.
        For example, cutting and pasting two texts from Google Translate.

        @rtype: C{list} of I{alignments}
        """
        pass
        
    def prob_align(self, featureset):
        """
        @return: a probability distribution over labels for the given
            featureset.
        @rtype: L{ProbDistI <nltk.probability.ProbDistI>}
        """
        if overridden(self.batch_prob_classify):
            return self.batch_prob_align([featureset])[0]
        else:
            raise NotImplementedError()
    

    def batch_prob_align(self, featuresets):
        """
        Apply L{self.prob_classify()} to each element of C{featuresets}.  I.e.:

            >>> return [self.prob_classify(fs) for fs in featuresets]

        @rtype: C{list} of L{ProbDistI <nltk.probability.ProbDistI>}
        """
        return [self.prob_classify(fs) for fs in featuresets]



########NEW FILE########
__FILENAME__ = distance_measures

# Natural Language Toolkit: Gale-Church Aligner Distance Functions
#
# Copyright (C) 2001-2011 NLTK Project
# Author: Chris Crowner <ccrowner@gmail.com>
# URL: <http://nltk.org/>
# For license information, see LICENSE.TXT

import math

BIG_DISTANCE = 2500

# Based on Gale & Church 1993, "A Program for Aligning Sentences in Bilingual Corpora"
# This is a Python version of the C implementation by Mike Riley presented in the appendix
# of the paper. The documentation in the C program is retained where applicable.
    
# Local Distance Function


def pnorm(z):
    """ 
    Returns the area under a normal distribution
    from -inf to z standard deviations 
    
    @type z: C{float}
    @param z: 
        
    @return: the area under a normal distribution
             from -inf to z standard deviations
    @rtype: C{float}    
        
    """
    t = 1/(1 + 0.2316419 * z)
    pd = 1 - 0.3989423 *  \
    math.exp(-z * z/2) * \
      ((((1.330274429 * t - 1.821255978) * t \
     + 1.781477937) * t - 0.356563782) * t + 0.319381530) * t
    # /* see Gradsteyn & Rhyzik, 26.2.17 p932 */
    return pd
   
def match(len1, len2):
    """
    Return -100 * log probability that an English sentence of length
    len1 is a translation of a foreign sentence of length len2.  The
    probability is based on two parameters, the mean and variance of
    number of foreign characters per English character.
        
    @type len1: C{int}
    @param len1: 
        
    @type len2: C{int}
    @param len2: 
        
    
    @return: the probability that an English sentence of length
             len1 is a translation of a foreign sentence of length len2.
    @rtype: C{int}    
   
    """
    #/* foreign characters per english character */
    foreign_chars_per_eng_char = 1
    
    #/* variance per english character */
    var_per_eng_char = 6.8 	
    
    if (len1==0 and len2==0): 
        return 0    
    
    try:
        mean = (len1 + len2/foreign_chars_per_eng_char)/2          
    
        z = (foreign_chars_per_eng_char * len1 - len2)/math.sqrt(var_per_eng_char * mean)
    except ZeroDivisionError:
        z = float(999999999999999999999)
    
    #/* Need to deal with both sides of the normal distribution */
    if (z < 0):
        z = -z
        
    pd = 2 * (1 - pnorm(z))
    
    if (pd > 0):
        return (-100 * math.log(pd))
    else:
        return (BIG_DISTANCE);

def two_side_distance(x1, y1, x2, y2):
    """
    Calculate a distance metric .
    
    @type x1: C{int}
    @param x1:
        
    @type y1: C{int}
    @param y1:
        
    @type x2: C{int}
    @param x2:
        
    @type y2: C{int}
    @param y2:    
    
    @return: 
    @rtype: C{int}    
   
    """
    penalty21 = 230		
    #/* -100 * log([prob of 2-1 match] / [prob of 1-1 match]) */
    
    penalty22 = 440
    #/* -100 * log([prob of 2-2 match] / [prob of 1-1 match]) */
    
    penalty01 = 450
    #/* -100 * log([prob of 0-1 match] / [prob of 1-1 match]) */
        
    if (x2 == 0 and y2 == 0):    
        if (x1 == 0):			
            # /* insertion */
            return (match(x1, y1) + penalty01)          
        elif(y1 == 0):		
            # /* deletion */
            return (match(x1, y1) + penalty01)    
        else: 
            #/* substitution */
            return (match(x1, y1))     
    elif (x2 == 0):		
        #/* expansion */
        return (match(x1, y1 + y2) + penalty21)    
    elif (y2 == 0):		
        #/* contraction */
        return (match(x1 + x2, y1) + penalty21)     
    else:				
        # /* melding */
        return (match(x1 + x2, y1 + y2) + penalty22)
    
        
def three_side_distance(x1, y1, x2, y2, x3, y3):
    """
    Calculate a distance metric .
    
    @type x1: C{int}
    @param x1:
        
    @type y1: C{int}
    @param y1:
        
    @type x2: C{int}
    @param x2:
        
    @type y2: C{int}
    @param y2:    
    
    @return: 
    @rtype: C{int}    
   
    """
    penalty21 = 230		
    #/* -100 * log([prob of 2-1 match] / [prob of 1-1 match]) */
    
    penalty22 = 440
    #/* -100 * log([prob of 2-2 match] / [prob of 1-1 match]) */
    
    penalty01 = 450
    #/* -100 * log([prob of 0-1 match] / [prob of 1-1 match]) */
    
    penalty31 = 230
    
    penalty13 = 230
    
    penalty32 = 600
    
    penalty23 = 600
    
    penalty33 = 650
    
    if (x3 == 0 and y3 == 0):
        if (x2 == 0 and y2 == 0):    
            if (x1 == 0):			
                # /* insertion */
                return (match(x1, y1) + penalty01)          
            elif(y1 == 0):		
                # /* deletion */
                return (match(x1, y1) + penalty01)    
            else: 
                #/* substitution */
                return (match(x1, y1))     
        elif (x2 == 0):		
            #/* expansion */
            return (match(x1, y1 + y2) + penalty21)    
        elif (y2 == 0):		
            #/* contraction */
            return (match(x1 + x2, y1) + penalty21)     
        else:				
            # /* melding */
            return (match(x1 + x2, y1 + y2) + penalty22)
    else:
        if (x3 == 0):
            if (x2 == 0):
                #/* expansion  1-3 */
                return (match(x1, y1 + y2 + y3) + penalty13)
            else:
                #/* expansion  2-3 */
                return (match(x1 + x2, y1 + y2 + y3) + penalty23)
        elif (y3 == 0):
            if (y2 == 0):
                #/* contraction  3-1 */
                return (match(x1 + x2 + x3, y1) + penalty31)
            else:
                #/* contraction  3-2 */
                return (match(x1 + x2 + x3, y1 + y2) + penalty32)                 
        else:				
            # /* melding */
            return (match(x1 + x2 + x3, y1 + y2 + y3) + penalty33)

########NEW FILE########
__FILENAME__ = gale_church
# -*- coding: utf-8 -*-

# Natural Language Toolkit: Gale-Church Aligner
#
# Copyright (C) 2001-2011 NLTK Project
# Author: Torsten Marek <marek@ifi.uzh.ch>
# URL: <http://nltk.org/>
# For license information, see LICENSE.TXT

from __future__ import division
import math

from util import *

# Based on Gale & Church 1993, 
# "A Program for Aligning Sentences in Bilingual Corpora"

infinity = float("inf")

def erfcc(x):
    """Complementary error function."""
    z = abs(x)
    t = 1 / (1 + 0.5 * z)
    r = t * math.exp(-z * z -
                     1.26551223 + t *
                     (1.00002368 + t *
                      (.37409196 + t *
                       (.09678418 + t *
                        (-.18628806 + t *
                         (.27886807 + t *
                          (-1.13520398 + t *
                           (1.48851587 + t *
                            (-.82215223 + t * .17087277)))))))))
    if (x >= 0.):
        return r
    else:
        return 2. - r


def norm_cdf(x):
    """Return the area under the normal distribution from M{-∞..x}."""
    return 1 - 0.5 * erfcc(x / math.sqrt(2))


class LanguageIndependent(object):
    # These are the language-independent probabilities and parameters
    # given in Gale & Church

    # for the computation, l_1 is always the language with less characters
    PRIORS = {
        (1, 0): 0.0099,
        (0, 1): 0.0099,
        (1, 1): 0.89,
        (2, 1): 0.089,
        (1, 2): 0.089,
        (2, 2): 0.011,
    }

    AVERAGE_CHARACTERS = 1
    VARIANCE_CHARACTERS = 6.8


def trace(backlinks, source, target):
    links = []
    pos = (len(source) - 1, len(target) - 1)

    while pos != (-1, -1):
        s, t = backlinks[pos]
        for i in range(s):
            for j in range(t):
                links.append((pos[0] - i, pos[1] - j))
        pos = (pos[0] - s, pos[1] - t)

    return links[::-1]


def align_probability(i, j, source_sentences, target_sentences, alignment, params):
    """Returns the probability of the two sentences C{source_sentences[i]}, C{target_sentences[j]}
    being aligned with a specific C{alignment}.

    @param i: The offset of the source sentence.
    @param j: The offset of the target sentence.
    @param source_sentences: The list of source sentence lengths.
    @param target_sentences: The list of target sentence lengths.
    @param alignment: The alignment type, a tuple of two integers.
    @param params: The sentence alignment parameters.

    @returns: The probability of a specific alignment between the two sentences, given the parameters.
    """
    l_s = sum(source_sentences[i - offset] for offset in range(alignment[0]))
    l_t = sum(target_sentences[j - offset] for offset in range(alignment[1]))
    try:
        # actually, the paper says l_s * params.VARIANCE_CHARACTERS, this is based on the C
        # reference implementation. With l_s in the denominator, insertions are impossible.
        m = (l_s + l_t / params.AVERAGE_CHARACTERS) / 2
        delta = (l_t - l_s * params.AVERAGE_CHARACTERS) / math.sqrt(m * params.VARIANCE_CHARACTERS)
    except ZeroDivisionError:
        delta = infinity

    return 2 * (1 - norm_cdf(delta)) * params.PRIORS[alignment]


def align_blocks(source_sentences, target_sentences, params = LanguageIndependent):
    """Creates the sentence alignment of two blocks of texts (usually paragraphs).

    @param source_sentences: The list of source sentence lengths.
    @param target_sentences: The list of target sentence lengths.
    @param params: the sentence alignment parameters.

    @return: The sentence alignments, a list of index pairs.
    """
    alignment_types = list(params.PRIORS.keys())

    # there are always three rows in the history (with the last of them being filled)
    # and the rows are always |target_text| + 2, so that we never have to do
    # boundary checks
    D = [(len(target_sentences) + 2) * [0] for x in range(2)]

    # for the first sentence, only substitution, insertion or deletion are
    # allowed, and they are all equally likely ( == 1)

    D.append([0, 1])
    D[-2][2] = 1
    D[-2][1] = 1

    backlinks = {}

    for i in range(len(source_sentences)):
        for j in range(len(target_sentences)):
            m = []
            for a in alignment_types:
                k = D[-(1 + a[0])][j + 2 - a[1]]
                if k > 0:
                    p = k * \
                      align_probability(i, j, source_sentences, target_sentences, a, params)
                    m.append((p, a))

            if len(m) > 0:
                v = max(m)
                backlinks[(i, j)] = v[1]
                D[-1].append(v[0])
            else:
                backlinks[(i, j)] = (1, 1)
                D[-1].append(0)

        D.pop(0)
        D.append([0, 0])

    return trace(backlinks, source_sentences, target_sentences)


def align_texts(source_blocks, target_blocks, params = LanguageIndependent):
    """Creates the sentence alignment of two texts.

    Texts can consist of several blocks. Block boundaries cannot be crossed by sentence 
    alignment links. 

    Each block consists of a list that contains the lengths (in characters) of the sentences
    in this block.
    
    @param source_blocks: The list of blocks in the source text.
    @param target_blocks: The list of blocks in the target text.
    @param params: the sentence alignment parameters.

    @returns: A list of sentence alignment lists
    """
    if len(source_blocks) != len(target_blocks):
        raise ValueError("Source and target texts do not have the same number of blocks.")
    
    return [align_blocks(source_block, target_block, params) 
            for source_block, target_block in zip(source_blocks, target_blocks)]


def split_at(it, split_value):
    """Splits an iterator C{it} at values of C{split_value}. 

    Each instance of C{split_value} is swallowed. The iterator produces
    subiterators which need to be consumed fully before the next subiterator
    can be used.
    """
    def _chunk_iterator(first):
        v = first
        while v != split_value:
            yield v
            v = it.next()
    
    while True:
        yield _chunk_iterator(it.next())
        

def parse_token_stream(stream, soft_delimiter, hard_delimiter):
    """Parses a stream of tokens and splits it into sentences (using C{soft_delimiter} tokens) 
    and blocks (using C{hard_delimiter} tokens) for use with the L{align_texts} function.
    """
    return [
        [sum(len(token) for token in sentence_it) 
         for sentence_it in split_at(block_it, soft_delimiter)]
        for block_it in split_at(stream, hard_delimiter)]


if __name__ == "__main__":
    import sys
    from contextlib import nested
    
    with nested(open(sys.argv[1], "r"), open(sys.argv[2], "r")) as (s, t):
        source = parse_token_stream((l.strip() for l in s), ".EOS", ".EOP")
        target = parse_token_stream((l.strip() for l in t), ".EOS", ".EOP")
        print align_texts(source, target)

########NEW FILE########
__FILENAME__ = test

import align_util
import align
import distance_measures

import sys

# import for madame_bovary_test()
from nltk.corpus.reader import plaintext, util
from nltk.text import Text
from nltk.data import *
from nltk.tokenize import *
import codecs

##//////////////////////////////////////////////////////
##  Demonstration code
##//////////////////////////////////////////////////////

"""
def demo_eval(alignments, gold_file):
    
    alignment_mappings = align_util.get_alignment_links(alignments)
    
    print "Alignment mappings: %s" % alignment_mappings
    
    #test_values = align_util.get_test_values(alignments)
    
    reference_values = align_util.get_reference_values(gold_file)
    
    print "Reference values: %s" % reference_values
         
    #accuracy = scores.accuracy(reference_values, test_values)
    
    #print "accuracy: %.2f" % accuracy
"""
                
def demo():
    """
    A demonstration for the C{Aligner} class.  
    """
        
    hard_delimiter = '.EOP'
    soft_delimiter = '.EOS'
    
    # demo 1
    input_file1 = 'data/turinen.tok'
    input_file2 = 'data/turinde.tok'
    gold_file = 'data/ground_truth.txt'
    
    gc = align.GaleChurchAligner(distance_measures.two_side_distance, 'original', 
                                    'bead_objects', print_flag=True)
    
    (regions1, regions2) = gc.get_delimited_regions('token',
                                                    input_file1, input_file2, 
                                                    hard_delimiter, soft_delimiter)
    
    gc_alignment = gc.batch_align(regions1, regions2)  
    
    print "Alignment0: %s" % gc_alignment
        
    #demo_eval(gc_alignment, gold_file)    
        
    # demo 2
    
    hard_delimiter = '.EOP'
    soft_delimiter = '.EOS'
    
    input_file1 = 'data/bovaryen.tok'
    input_file2 = 'data/bovaryfr.tok'
    gold_file = 'data/ground_truth_bovary.txt'
    
    gc = align.GaleChurchAligner(distance_measures.two_side_distance, 'original', 
                                    'text_tuples', print_flag=True)
    
    (regions1, regions2) = gc.get_delimited_regions('token',
                                                    input_file1, input_file2, 
                                                    hard_delimiter, soft_delimiter)
                                                                
    gc_alignment = gc.batch_align(regions1, regions2)  
    
    print "Alignment1: %s" % gc_alignment
        
    #demo_eval(gc_alignment, gold_file)
    
    # demo 3
    
    std = align.GaleChurchAligner(distance_measures.two_side_distance, 'original', 
                                    'text_tuples', print_flag=True)
    
    s_para_1 = [['asddd a rrg'],['hg']]
    s_para_2 = [['jk nkp'],['fg']]
    s2 = [s_para_1, s_para_2]
    
    t_para_1 = [['12345 6 78'],['910']]
    t_para_2 = [['45 67'],['89']]
    t2 = [t_para_1, t_para_2]
        
    standard_alignment2 = std.batch_align(s2, t2)
    
    print "Alignment2: %s" % standard_alignment2
    
    # demo 4
    
    #s3 = [['asddd','a','rrg'],['hg']]
    #t3 = [['12345','6','78'],['910']]
    
    s3 = [[['asddd','a','rrg'],['hg']],[['xxxxx','y','rrg'],['pp']]]
    t3 = [[['12345','6','78'],['910']],[['wally','i','am'],['dob']]]
    
    standard_alignment3 = std.align(s3, t3)
    
    print "Alignment3: %s" % standard_alignment3
    
    # demo 5
    
    top_down_alignments = std.recursive_align(s3, t3, [])  
    
    for alignment in top_down_alignments:
        print "Top down align: %s" % alignment

def madame_bovary_test(source_file, target_file, source_pickle_file, target_pickle_file):
    
    source_plaintext_reader = plaintext.PlaintextCorpusReader('', 
        [source_file],
        word_tokenizer=WhitespaceTokenizer(),
        sent_tokenizer=LazyLoader(source_pickle_file),              
        encoding='utf8')
        
    target_plaintext_reader = plaintext.PlaintextCorpusReader('', 
        [target_file],
        word_tokenizer=WhitespaceTokenizer(),
        sent_tokenizer=LazyLoader(target_pickle_file),              
        encoding='utf8')
 
    source_chapter = [source_para for source_para in source_plaintext_reader.paras()]
       
    target_chapter = [target_para for target_para in target_plaintext_reader.paras()]
      
    std = align.GaleChurchAligner(distance_measures.three_side_distance, 'extended', 'bead_objects', print_flag=True)
        
    source_paras = source_plaintext_reader.paras()
    target_paras = target_plaintext_reader.paras()
 
    top_down_alignments = std.recursive_align(source_chapter, target_chapter, [])  
        
if __name__=='__main__':
    demo()    
    
    # usage: python test2.py data/chapter1_madame_bovary_fr.txt data/chapter1_madame_bovary_en.txt fr en
    if len(sys.argv) > 1:
        source_file = sys.argv[1]
        target_file = sys.argv[2]
        source_lang = sys.argv[3]
        target_lang = sys.argv[4]
    else:
        sys.exit('Usage: arg1 - input filename arg2 - output filename arg3 - source language arg4 - target language')
    
        
    if (source_lang == "fr"):
        source_pickle_file = 'tokenizers/punkt/french.pickle'
    elif (source_lang == "en"):
        source_pickle_file = 'tokenizers/punkt/english.pickle'
    else:
        source_pickle_file = ''
   
    if (target_lang == "fr"):
        target_pickle_file = 'tokenizers/punkt/french.pickle'
    elif (target_lang == "en"):
        target_pickle_file = 'tokenizers/punkt/english.pickle'
    else:
        target_pickle_file = ''
    
        
    if (source_pickle_file) and (target_pickle_file):
        madame_bovary_test(source_file, target_file, source_pickle_file, target_pickle_file)
    


########NEW FILE########
__FILENAME__ = util
# Natural Language Toolkit: Aligner Utilities
#
# Copyright (C) 2001-2011 NLTK Project
# Author: 
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT


########NEW FILE########
__FILENAME__ = bioreader
"""
Classes to read and process MedLine XML record files, for use in processing modules.

They can handle the XML format used by PubMed and MedLine services, for example as returned by eutils online services

   >>> xml_records = urllib.urlopen('http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id='+list_of_pmids+'&retmode=xml').read()

When used, an object is created as a global repository, from which records (also objects) can be queried and extracted. These record-objects have properties like title, authors, abstracts that return their string values.

Somewhat long loading times can be shortened later by serializing objects using  cPickle module

   USAGE:
   >>> from BioReader import *
   >>> data = DataContainer('AllAbstracts.xml','pubmed')
   >>> data.howmany # len(data.dictRecords.keys())
   >>> data.keys    # data.dictRecords.keys()
   >>> record = data.Read('7024555')
   >>> record.title

           u'The birA gene of Escherichia coli encodes a biotin holoenzyme synthetase.'
           record +
                  - B{.title}
                  - B{.pmid}
                  - B{.Abs} I{(abstracts)}
                  - B{.year}
                  - B{.journal}
                  - B{.auth} I{(list of authors)}
                  - B{.m} I{(list of MeSH keywords, descriptors and qualifiers)}
                  - B{.MD} I{(MesH Descriptors)}
                  - B{.MQ} I{(MesH Qualifiers, if any)}
                  - B{.MDMay} I{(list of Mayor MesH Descriptors, if any)}
                  - B{.MQMay} I{(list of Mayor MesH Qualifiers, if any)}
                  - B{.paper} I{(full text flat file if exists in user-defined repository

The Search method inside the DataContainer class is not working well, and should be rewritten  using better XML techniques and methods

A class (CreateXML) has been added recently to create the pubmed XML file from a list of PubMed ids. Has not been fully integrated with the data container class

Another class shoud be able to query keywords directly to pubmed, to either get the pubmed ids or the xml directly, using either BioPython's PubMed modules of directly through Eutil's facilities

""" 
#__docformat__ = 'epytext en'

# General info
__version__ = '5.0'
__author__ = 'Carlos Rodriguez'
__url__ = 'http://www.cnio.es'
__license__ = 'GNU'

from xml.dom.minidom import parseString
import string
import re
import os

class BioReader:
    """
    Class BioReader for BioMedical files
    """
    def __init__(self, string, path=None):
        """
        Initialize class with XML string  and returns record data and body of text objects.
        
            >>> single_record = BioReader(record)

            >>> single_record.title
            u'The birA gene of Escherichia coli encodes a biotin holoenzyme synthetase.'
         
            >>> single_record.pmid
            u'7024555'
            
        single_record +
           - B{.title}
           - B{.pmid}
           - B{.Abs} I{(abstracts)}
           - B{.year}
           - B{.journal}
           - B{.auth} I{(list of authors)}
           - B{.m} I{(list of MeSH keywords, descriptors and qualifiers)}
           - B{.MD} I{(MesH Descriptors)}
           - B{.MQ} I{(MesH Qualifiers, if any)}
           - B{.MDMay} I{(list of Mayor MesH Descriptors, if any)}
           - B{.MQMay} I{(list of Mayor MesH Qualifiers, if any)}
           - B{.paper} I{(full text flat file if exists in user-defined repository [see notes below])}
                  
        If we use a repository with full text papers (with pmid+<pmidnumber>+txt format), 
        we can use the following, after specifying it in the Data Container we instantiated:
        
             >>> data.Repository("/repositorio/Regulontxt/")

             >>> record = data.dictRecords['9209026']

             >>> single_record = BioReader(record,data.repository)# or directly inputing path, if it was  not done\\
                  through the DataContainer class: single_record = BioReader(record,'/path/to/repository/')

             >>> single_record.paper

             'Aerobic Regulation of the sucABCD Genes of Escherichia coli,Which Encode \xef\xbf\xbd-Ketoglutarate Dehydrogenase andSuccinyl Coenzyme A Synthetase: Roles of ArcA,Fnr, and the Upstream sdhCDAB Promoter\n.....'
             
        """
        self.tags = re.compile("<.*?>")
        self.parsed = parseString(string)
        self.document = self.parsed.documentElement
        self.pmid = self.document.getElementsByTagName("PMID")[0].firstChild.data
        self.year = self.document.getElementsByTagName("DateCreated")[0].getElementsByTagName("Year")[0].firstChild.data
        self.journal = self.document.getElementsByTagName("MedlineJournalInfo")[0].getElementsByTagName("MedlineTA")[0].firstChild.data
        self.testAbs = self.document.getElementsByTagName("Abstract")
        if path != None:
            self.path = path
            self.paper = self.GetFullPaper()
        else:
            self.path = None
            self.paper = None
        try:
            self.year = self.document.getElementsByTagName("PubDate")[0].getElementsByTagName("Year")[0].firstChild.data
        except IndexError:
            self.year = self.document.getElementsByTagName("DateCreated")[0].getElementsByTagName("Year")[0].firstChild.data
        try:
            self.Abs = self.document.getElementsByTagName("Abstract")[0].getElementsByTagName("AbstractText")[0].firstChild.data
        except IndexError:
            self.Abs = "n/a"
        self.title = self.document.getElementsByTagName("ArticleTitle")[0].firstChild.data
        try:
            self.authorsList = self.document.getElementsByTagName("AuthorList")[0].getElementsByTagName("Author")
            self.Lista = [self.authorize(y.childNodes) for y in self.authorsList]
            s = ""
            for x in self.Lista:
                s = s + x + "\n"
            self.auth = s
        except AttributeError:
            self.auth = " "
        except IndexError:
            self.auth = " "
        try:
            self.meshes = self.document.getElementsByTagName("MeshHeadingList")[0].getElementsByTagName("MeshHeading")
            self.ListaMs = [self.Meshes(z.childNodes) for z in self.meshes]
            self.MD = []
            self.MQ = []
            self.MDMay = []
            self.MQMay = []
            for z in self.meshes:
                MD,MQ,MDMay,MQMay = self.MeshKeys(z)
                self.MD = MD + self.MD
                self.MQ = MQ + self.MQ
                self.MDMay = MDMay + self.MDMay
                self.MQMay = MQMay + self.MQMay
                
            self.m = ""
            for x in self.ListaMs:
                self.m = x+" \n "+self.m
            #self.p = None
        except IndexError:
            self.m = "n/a"            
            self.meshes = "n/a"
            self.MQ = None
            self.MD = None
            self.MDMay = None
            self.MQMay = None
            #self.p = None
        #from DataContainer import repository
        #self.authors = string.join( self.Lista )#[self.authorize(x)+"\n" for x in self.Lista]
    def __repr__(self):
        return "<BioReader record instance: pmid: "+self.pmid+" title: "+self.title+" abstract: "+self.Abs+">"
    
    def authorize(self, node):
        s = ""
        for z in node:
            f = z.toxml()
            f = re.sub(self.tags,"",f)
            f  = re.sub("\n","",f)
            f  = re.sub("\t"," ",f)
            f  = re.sub("  ","",f)
            s = s + f+" "
        return s

    def Meshes(self, node):
        s = ""
        for z in node:
            f = z.toxml()
            f = re.sub(self.tags,"",f)
            f  = re.sub("\n","",f)
            f  = re.sub("\t"," ",f)
            f  = re.sub("  ","",f)
            s = s + f+" "
        return s

    def MeshKeys(self,node):
        """
        Create sets of MesH Keywords, separating qualifiers and descriptors, as well as //
        MajorTopics for each one. returns Lists.
        """
        listDescriptors = node.getElementsByTagName("DescriptorName")
        listQualifiers =  node.getElementsByTagName("QualifierName")
        MD = [x.firstChild.data for x in listDescriptors]
        MQ = [x.firstChild.data for x in listQualifiers]
        MQMay = [q.firstChild.data for q in listQualifiers if (q.getAttribute("MajorTopicYN") == "Y")]
        MDMay = [q.firstChild.data for q in listDescriptors if (q.getAttribute("MajorTopicYN") == "Y")]
        return MD,MQ,MDMay,MQMay
    def GetFullPaper(self):
        """
        Gets the full paper from the path of an (optional) repository.
        The full papers must have the following format:
        pmid+<pmidnumber>+.txt (last extension optional)
        """
        pmidList = os.listdir(self.path)
        if pmidList[0][-4:] == '.txt':
            pmidList = [x[4:-4] for x in pmidList]
            formato = 1
        else:
            pmidList = [x[4:] for x in pmidList]
            formato = None
        if self.pmid in pmidList:
            if formato:
                self.paper = open(self.path+"pmid"+self.pmid+".txt").read()
                return self.paper
            else:
                self.paper = open(self.path+"pmid"+self.pmid).read()
                return self.paper
        else:
            self.paper = None
        
        
class DataContainer:
    """
    Data container for Pubmed and Medline XML files.
    The instance creates a dictionary object (dictRecords) of PMIDs, 
    referenced to string of record, which BioReader class can parse. 
    The method C{Read} creates a queryable object for each record  assoicated with a PMID:

        >>> from BioReader import *
        >>> data = DataContainer('AllAbs.xml','pubmed')
        >>> data.dictRecords.keys()[23]
        >>> u'7024555'
        >>> data.howmany
        >>> 14350

    1) Method One

       >>> record = data.Read('7024555')
       >>> record.title

           u'The birA gene of Escherichia coli encodes a biotin holoenzyme synthetase.'
           record +
                  - B{.title}
                  - B{.pmid}
                  - B{.Abs} I{(abstracts)}
                  - B{.year}
                  - B{.journal}
                  - B{.auth} I{(list of authors)}
                  - B{.m} I{(list of MeSH keywords, descriptors and qualifiers)}
                  - B{.MD} I{(MesH Descriptors)}
                  - B{.MQ} I{(MesH Qualifiers, if any)}
                  - B{.MDMay} I{(list of Mayor MesH Descriptors, if any)}
                  - B{.MQMay} I{(list of Mayor MesH Qualifiers, if any)}
                  - B{.paper} I{(full text flat file if exists in user-defined repository [see notes below])}
    If we use a repository with full text papers 
    (with pmid+<pmidnumber>+txt format (extension optional), 
    we can use the following, after specifying it in the DataContainer we instantiated:
        
    >>> data.Repository("/repositorio/Regulontxt/")
    >>> record.paper

        'Aerobic Regulation of the sucABCD Genes of Escherichia coli, Which Encode \xef\xbf\xbd-Ketoglutarate Dehydrogenase andSuccinyl Coenzyme A Synthetase: Roles of ArcA,Fnr, and the Upstream sdhCDAB Promoter\n.....       

    2) Method two
        
    >>> record = data.dictRecords['7024555']
    >>> single_record = BioReader(record)
    >>> single_record.title
    >>> u'The birA gene of Escherichia coli encodes a biotin holoenzyme synthetase.'   etc ...

    (See L{BioReader})
    """
    def __init__(self,file,format="medline"):
        """
        Initializes class and returns record data and body of text objects
        """
        import time
        tinicial = time.time()
        self.file = file
        whole = open(self.file).read()
        if format.lower() == "medline":
            self.rerecord = re.compile(r'\<MedlineCitation Owner="NLM" Status="MEDLINE"\>'r'(?P<record>.+?)'r'\</MedlineCitation\>',re.DOTALL)
        elif format.lower() == "pubmed":
            self.rerecord  = re.compile(r'\<PubmedArticle\>'r'(?P<record>.+?)'r'\</PubmedArticle\>',re.DOTALL)
        else:
            print "Unrecognized format"
        self.RecordsList = re.findall(self.rerecord,whole)
        whole = ""
        self.RecordsList =  ["<PubmedArticle>"+x.rstrip()+"</PubmedArticle>" for x in self.RecordsList]
        self.dictRecords = self.Createdict()
        self.RecordsList = []
        self.howmany = len(self.dictRecords.keys())
        self.keys = self.dictRecords.keys()
        tfinal = time.time()
        self.repository = None
        print "finished loading at ",time.ctime(tfinal)
        print "loaded in", tfinal-tinicial," seconds, or",((tfinal-tinicial)/60)," minutes"
    def __repr__(self):
        return "<BioReader Data Container Instance: source filename: "+self.file+" \nnumber of files: "+str(self.howmany)+">"

    def Repository(self,repository):
        """
        Establish path to a full text repository, in case you want to use that variable in the BioReader 
        """
        self.repository = repository
        return self.repository
    def Createdict(self):
        """
        Creates a dictionary with pmid number indexing record xml string
        """
        i = 0
        dictRecords = {}
        for p in self.RecordsList:
            r = BioReader(p)
            dictRecords[r.pmid] = self.RecordsList[i]
            i += 1
        return dictRecords

    def Read(self,pmid):
        if self.repository:
            self.record = BioReader(self.dictRecords[pmid],self.repository)
        else:
            self.record = BioReader(self.dictRecords[pmid])
        return self.record

    def Search(self,cadena,where=None):
        """
        This method is not working. Needs to be redone to comply with more up-to-date XML search methods
        Searches for "cadena" string inside the selected field, and returns a list of pmid where it was found.

        If not "where" field is provided, will search in all of the record.
        You can search in the following fields:
         - title
         - year
         - journal
         - auth or authors
         - 'abs' or 'Abs' or 'abstract'
         - paper or "full" (if full-text repository has been defined)
         - pmid

        With defined field search is very slow but much more accurate. See for comparison:
        
        >>> buscados = data.Search("Richard")
        
            Searched in 0.110424995422  seconds, or 0.00184041659037  minutes

            Found a total of  75  hits for your query, in all fields

            >>> buscados = data.Search("Richard","auth")

                Searched in 66.342936039  seconds, or 1.10571560065  minutes

                Found a total of  75  hits for your query, in the  auth  field
        """
        tinicial = time.time()
        resultlist = []
        if where:
            for cadapmid in self.dictRecords.keys():
                d = self.Read(cadapmid)
                if where == 'title':
                    tosearch = d.title
                elif where == 'year':
                    tosearch = d.year
                elif where == 'journal':
                    tosearch = d.journal
                elif where == ('auth' or 'authors'):
                    tosearch = d.auth
                elif where == ('m' or 'mesh'):
                    tosearch = d.m
                elif where == ('abs' or 'Abs' or 'abstract'):
                    tosearch = d.Abs
                elif where == ('paper' or 'full'):
                    tosearch = d.paper
                    if self.repository:
                        pass
                    else:
                        print "No full text repository has been defined...."
                        return None
                elif where == 'pmid':
                    tosearch = d.pmid
                hit = re.search(cadena,tosearch)
                if hit:
                    resultlist.append(d.pmid)
                else:
                    pass
            if len(resultlist)!= 0:
                tfinal = time.time()
                print "Searched in", tfinal-tinicial," seconds, or",((tfinal-tinicial)/60)," minutes"
                print "Found a total of ",str(len(resultlist))," hits for your query, in the ",where," field"
                return resultlist
            else:
                print "Searched in", tfinal-tinicial," seconds, or",((tfinal-tinicial)/60)," minutes"
                print "Query not found"
                return None
        else:
            tosearch = ''
            for cadapmid in self.dictRecords.keys():
                tosearch = self.dictRecords[cadapmid]
                hit = re.search(cadena,tosearch)
                if hit:
                    resultlist.append(cadapmid)
                else:
                    pass
        if len(resultlist)!= 0:
            tfinal = time.time()
            print "Searched in", tfinal-tinicial," seconds, or",((tfinal-tinicial)/60)," minutes"
            print "Found a total of ",str(len(resultlist))," hits for your query, in all fields"
            return resultlist
        else:
            tfinal = time.time()
            print "Searched in", tfinal-tinicial," seconds, or",((tfinal-tinicial)/60)," minutes"
            print "Query not found"
            return None
                

class CreateXML:
    
    """
    Class to generate PubMed XMLs from a list of ids (one per line), to use with BioRea.
    downloads in 100 batch.
    Usage:

    outputfile = "NuevosPDFRegulon.xml"
    inputfile = "/home/crodrigp/listaNuevos.txt"
       >>> XMLCreator = CreateXML()
       >>> XMLCreator.GenerateFile(inputfile,outputfile)
       >>> parseableString = XMLCreator.Generate2String(inputfile)

       or
       >>> XMLString = XMLCreator.Generate2String()

    """
    def __init__(self):
        #global urllib,time,string,random
        import urllib,time,string,random
 
    def getXml(self,s):
        pedir = urllib.urlopen("http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id="+s+"&retmode=xml")
        stringxml = pedir.read()
        self.salida.write(stringxml[:-20]+"\n")
        
    def getXmlString(self,s):
        pedir = urllib.urlopen("http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id="+s+"&retmode=xml")
        stringxml = pedir.read()
        return stringxml[:-20]+"\n"
    
    def listastring(self,list):
        suso = string.join(list,",")
        return suso

    def GenerateFile(self,inputfile,outputfile):
        self.outputfile = outputfile
        self.inputfile = inputfile
        self.salida = open(self.outputfile,"w")
        self.listaR = open(self.inputfile).readlines()
        self.listafin = [x.rstrip() for x in self.listaR]
        self.listacorr = []
        while self.listafin != []:
            if len(self.listafin) < 100:
                cientos = self.listafin[:]
                #self.listafin = []
            else:
                cientos = self.listafin[:100]


            print "new length self.listacorr", len(self.listafin)
            if len(self.listafin) <= 0:
                break
            else:
                #time.sleep(120)
                nueva = self.listastring(cientos)
                self.getXml(nueva)
            for c in cientos:
                print c
                self.listafin.remove(c)
        self.salida.close()

    def Generate2String(self,inputfile):
        self.inputfile = inputfile
        self.listaR = open(self.inputfile).readlines()
        self.AllXML = ''
        self.listafin = [x.rstrip() for x in self.listaR]
        self.listacorr = []
        while self.listafin != []:
            if len(self.listafin) < 100:
                cientos = self.listafin[:]
                #self.listafin = []
            else:
                cientos = self.listafin[:100]


            print "new length self.listacorr", len(self.listafin)
            if len(self.listafin) <= 0:
                break
            else:
                time.sleep(120)
                nueva = self.listastring(cientos)
                newX = self.getXmlString(nueva)
                self.AllXML = self.AllXML + newX
            for c in cientos:
                print c
                self.listafin.remove(c)
        return self.AllXML

########NEW FILE########
__FILENAME__ = attribute
# Natural Language Toolkit - Attribute
#  can extract the name and values from a line and operate on them
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT
from nltk_contrib.classifier.exceptions import systemerror as se
from nltk_contrib.classifier import autoclass as ac, cfile, decisionstump as ds
from nltk import probability as prob
import UserList

CONTINUOUS = 'continuous'
DISCRETE = 'discrete'

class Attribute:
    """
    Immutable object which represents an attribute/feature
    """
    def __init__(self, name, values, index):
        self.name = name
        self.values = values
        self.type = self.__get_type()
        self.index = index
    
    def __get_type(self):
        if len(self.values) == 1 and self.values[0] == CONTINUOUS:
            return CONTINUOUS
        return DISCRETE
        
    def has_value(self, to_test):
        return self.values.__contains__(to_test)
    
    def is_continuous(self):
        return self.type == CONTINUOUS
    
    def __eq__(self, other):
        if other is None: return False
        if self.__class__ != other.__class__: return False
        if self.name == other.name and \
           self.values == other.values and \
           self.index == other.index: 
            return True
        return False
    
    def __str__(self):
        return self.name +':[' + self.values_as_str() + '] index:' + str(self.index)
    
    def values_as_str(self):
        """
        Used to write contents back to file store
        """
        return ','.join([str(value) for value in self.values])
    
    def empty_freq_dists(self):
        return dict([(value, prob.FreqDist()) for value in self.values])
    
    def __hash__(self):
        return hash(self.name) + hash(self.index)        
            
class Attributes(UserList.UserList):
    def __init__(self, attributes = []):
        self.data = attributes

    def has_values(self, test_values):
        if len(test_values) != len(self): return False
        for i in range(len(test_values)):
            test_value = test_values[i]
            if self.data[i].is_continuous(): continue #do not test continuous attributes
            if not self.data[i].has_value(test_value): return False
        return True
    
    def has_continuous(self):
        for attribute in self.data:
            if attribute.is_continuous(): 
                return True
        return False
    
    def subset(self, indices):
        return [self.data[index] for index in indices]

    def discretise(self, discretised_attributes):
        for disc_attr in discretised_attributes:
            self.data[disc_attr.index] = disc_attr
            
    def empty_decision_stumps(self, ignore_attributes, klass):
        filtered = filter(lambda attribute: attribute not in ignore_attributes, self.data)
        return [ds.DecisionStump(attribute, klass) for attribute in filtered]

    def remove_attributes(self, attributes):
        for attribute in attributes:
            self.remove(attribute)
        self.reset_indices()
            
    def reset_indices(self):
        for i in range(len(self.data)):
            self.data[i].index = i
            
    def continuous_attribute_indices(self):
        return [atr.index for atr in self.data if atr.is_continuous()]
    
    def empty_freq_dists(self):
        return dict([(attribute, attribute.empty_freq_dists()) for attribute in self.data])
        
    def __str__(self):
        return '[' + ', '.join([each.__str__() for each in self]) + ']'
            
def fact(n):
    if n==0 or n==1: return 1
    return n * fact(n -1)

def ncr(n, r):
    return fact(n) / (fact(r) * fact(n -r))

########NEW FILE########
__FILENAME__ = autoclass
# Natural Language Toolkit - AutoClass
#  automatic class value generator
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT

class AutoClass:
    def __init__(self, name):
        self.name = name
        
    def next(self):
        base26 = self.base26()
        base26 += 1
        return AutoClass(string(base26))

    def base26(self):
        base26 = 0
        length = len(self.name)
        for index in range(length):
            numeric = ord(self.name[index]) - 97
            if (index == length - 1): base26 += numeric
            else: base26 += numeric * 26 * (length - index - 1)
        return base26
    
    def __eq__(self, other):
        if other is None: return False
        if self.__class__ != other.__class__: return False
        if self.name == other.name: return True
        return False
    
    def __hash__(self):
        if self.name == None: return id(self)
        return 3 * self.name + 7 
    
    def __str__(self):
        return self.name

FIRST = AutoClass('a')

def string(base26):
    strn = ''
    while (base26 /26 > 0):
        strn = chr((base26 % 26) + 97) + strn
        base26 = base26 / 26
    strn = chr((base26 % 26) + 97) + strn
    return strn

########NEW FILE########
__FILENAME__ = basicimports
from nltk_contrib.classifier.attribute import Attribute, Attributes
from nltk_contrib.classifier.confusionmatrix import ConfusionMatrix
from nltk_contrib.classifier.decisionstump import DecisionStump
from nltk_contrib.classifier.decisiontree import DecisionTree
from nltk_contrib.classifier.featureselect import FeatureSelection
from nltk_contrib.classifier.discretise import Discretiser
from nltk_contrib.classifier.instances import TrainingInstances, TestInstances, GoldInstances
from nltk_contrib.classifier.instance import TrainingInstance, TestInstance, GoldInstance
from nltk_contrib.classifier.knn import IB1
from nltk_contrib.classifier.naivebayes import NaiveBayes
from nltk_contrib.classifier.oner import OneR
from nltk_contrib.classifier.zeror import ZeroR
from nltk_contrib.classifier.format import c45

########NEW FILE########
__FILENAME__ = cfile
# Natural Language Toolkit - File
#  Understands operations on files and the various input files extensions
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT

from nltk_contrib.classifier.exceptions import filenotfounderror as fnf, invaliddataerror as inv
import os, os.path

DOT = '.'

class File:
    def __init__(self, path, extension):
        self.path = path + DOT + extension
        
    def for_each_line(self, method):
        self.__check_for_existence()
        fil = open(self.path, 'r')
        returned = []
        for line in fil:
            filtered = filter_comments(line)
            if len(filtered) == 0:
                continue
            returned.append(method(filtered))
        fil.close()
        return returned

    def __check_for_existence(self):
        if not os.path.isfile(self.path): 
            raise fnf.FileNotFoundError(self.path)
        
    def create(self, overwrite = False):
        if not overwrite and os.path.exists(self.path):
            raise inv.InvalidDataError('File or Directory exists at ' + self.path + ' and overwrite is set to false.')
        if os.path.exists(self.path): 
            if os.path.isfile(self.path):
                os.remove(self.path)
            else:
                raise inv.InvalidDataError('Cannot overwrite directory ' + self.path + '.')
        fil = open(self.path, 'w')
        fil.close()
        
    def write(self, lines):
        self.__check_for_existence()
        fil = open(self.path, 'w')
        for line in lines:
            fil.write(line)
            fil.write('\n')
        fil.close()
        
def filter_comments(line):
    index = line.find('|')
    if index == -1:
        return line.strip()
    return line[:index].strip()

def name_extension(file_name):
    dot_index = file_name.rfind(DOT)
    if dot_index == -1:
        return [file_name, '']
    return [file_name[:dot_index], file_name[dot_index+1:]]

########NEW FILE########
__FILENAME__ = classify
from nltk_contrib.classifier import commandline as cl
from nltk_contrib.classifier import oner, zeror, decisiontree, format, naivebayes, knn
import sys

a_help = "Selects the classification algorithm                  " \
        + "Options: 0R for Zero R, 1R for One R, DT for Decision" \
        + " Trees, NB for Naive Bayes, IB1 for Instance Based   " \
        + " Learner with one nearest neighbour.                " \
        + "Default: 0R."

f_help = "Specifies the base name of test, training or gold files." \
        + "By default it searches for training and test files, look at the verify option for more details."

v_help = "Used in conjunction with the files option to verify  " \
        + "the efficiency with a gold file instead of testing " \
        + "the classifier on a test file. Setting this option " \
        + "will mean that a gold file is present with the common" \
        + "name.                                               " \
        + "Options: True/False or yes/no."

t_help = "When the files option is not used this option is used " \
        + "to specify the path to the training file without the " \
        + "extension."

T_help = "When the files option is not used this option is used " \
        + "to specify the path to the test file without the " \
        + "extension."

g_help = "When the files option is not used this option is used " \
        + "to specify the path to the gold file without the " \
        + "extension."

A_help = "Used to disable calculation of Accuracy.              " \
        + "Options: True/False or yes/no.                       " \
        + "Default: False.                                      "

e_help = "Used to enable calculation of Error rate.             " \
        + "Options: True/False or yes/no.                       " \
        + "Default: False.                                      "

F_help = "Used to disable calculation of F-score.               " \
        + "Options: True/False or yes/no.                       " \
        + "Default: False.                                      "

p_help = "Used to enable calculation of Precision.              " \
        + "Options: True/False or yes/no.                       " \
        + "Default: False.                                      "

r_help = "Used to enable calculation of Recall.                 " \
        + "Options: True/False or yes/no.                       " \
        + "Default: False.                                      "

w_help = "Writes resulting gold file with a modified base name  " \
         "Is always true for test files.                        "
         
c_help = "Classify by using a cross validation dataset with the " \
         "specified fold.                                       "
         
o_help = "Classifier options                                    " \
         " Decision Tree: IG - Max Information Gain             " \
         "                GR - Max Gain Ratio                   "

ZERO_R = '0R'
ONE_R = '1R'
DECISION_TREE = 'DT'
NAIVE_BAYES = 'NB'
IB1 = 'IB1'

ALGORITHM_MAPPINGS = {ZERO_R:zeror.ZeroR, ONE_R:oner.OneR, DECISION_TREE:decisiontree.DecisionTree, NAIVE_BAYES:naivebayes.NaiveBayes, IB1:knn.IB1}
ALL_ALGORITHMS = ALGORITHM_MAPPINGS.keys()

VERIFY='verify'
ACCURACY='accuracy'
ERROR='error'
F_SCORE='fscore'
PRECISION='precision'
RECALL='recall'
WRITE='write'
CROSS_VALIDATION='cross_validation'

class Classify(cl.CommandLineInterface):    
    def __init__(self):
        cl.CommandLineInterface.__init__(self, ALGORITHM_MAPPINGS.keys(), ONE_R, a_help, f_help, t_help, T_help, g_help, o_help)
        self.add_option("-v", "--verify", dest=VERIFY, action="store_true", default=False, help=v_help)
        self.add_option("-A", "--accuracy", dest=ACCURACY, action="store_false", default=True, help=A_help)
        self.add_option("-e", "--error", dest=ERROR, action="store_true", default=False, help=e_help)
        self.add_option("-F", "--f-score", dest=F_SCORE, action="store_false", default=True, help=F_help)
        self.add_option("-p", "--precision", dest=PRECISION, action="store_true", default=False, help=p_help)
        self.add_option("-r", "--recall", dest=RECALL, action="store_true", default=False, help=r_help)
        self.add_option("-w", "--write", dest=WRITE, action="store_true", default=False, help=r_help)
        self.add_option("-c", "--cross-validation-fold", dest=CROSS_VALIDATION, type="string", help=c_help)
        
    def execute(self):
        cl.CommandLineInterface.execute(self)
        self.validate_basic_arguments_are_present()
        self.validate_files_arg_is_exclusive()
        cross_validation_fold = self.get_value(CROSS_VALIDATION)
        if cross_validation_fold is None and self.files is None and self.test_path is None and self.gold_path is None:
            self.required_arguments_not_present_error()
        if self.test_path is not None and self.gold_path is not None:
            self.error('Invalid arguments. Test and gold files are mutually exclusive.')
        if self.files is None and self.test_path is not None and self.get_value(VERIFY):
            self.error('Invalid arguments. Cannot verify classification for test data.')
        
        file_strategy = get_file_strategy(self.files, self.training_path, self.test_path, self.gold_path, self.get_value(VERIFY))
        self.training_path, self.test_path, self.gold_path = file_strategy.values()
        
        training, attributes, klass, test, gold = self.get_instances(self.training_path, self.test_path, self.gold_path, cross_validation_fold is not None)
        classifier = ALGORITHM_MAPPINGS[self.algorithm](training, attributes, klass)
        classification_strategy = self.get_classification_strategy(classifier, test, gold, training, cross_validation_fold, attributes, klass)
        classification_strategy.train()
        self.log_common_params('Classification')
        classification_strategy.classify()
        classification_strategy.print_results(self.log, self.get_value(ACCURACY), self.get_value(ERROR), self.get_value(F_SCORE), self.get_value(PRECISION), self.get_value(RECALL))
        classification_strategy.write(self.log, self.get_value(WRITE), self.data_format, '-c_' + self.algorithm)

    #ugh!! ugly!!!.. need to find a better way.. there are way too many params here! till then.. this stays
    def get_classification_strategy(self, classifier, test, gold, training, cross_validation_fold, attributes, klass):
        if self.algorithm == DECISION_TREE: 
            classifier_options = DecisionTreeOptions(self.options)
        else:
            classifier_options = NoOptions()
        
        if cross_validation_fold is not None:
            return CrossValidationStrategy(self.algorithm, attributes, klass, training, cross_validation_fold, self.training_path, classifier_options)
        if test is not None:
            return TestStrategy(classifier, test, self.test_path, classifier_options)
        return VerifyStrategy(classifier, gold, self.gold_path, classifier_options)

def get_file_strategy(files, training, test, gold, verify):
    if files is not None:
        return CommonBaseNameStrategy(files, verify)
    return ExplicitNamesStrategy(training, test, gold)    

class CrossValidationStrategy:
    def __init__(self, algorithm, attributes, klass, training, fold, training_path, classifier_options):
        self.algorithm = algorithm
        self.training = training
        self.fold = fold
        self.confusion_matrices = []
        self.gold_instances = []
        self.klass = klass
        self.attributes = attributes
        self.training_path = training_path
        self.classifier_options = classifier_options

    def classify(self):
        datasets = self.training.cross_validation_datasets(self.fold)
        for each in datasets:
            classifier = ALGORITHM_MAPPINGS[self.algorithm](each[0], self.attributes, self.klass)
            self.classifier_options.set_options(classifier)
            classifier.train()
            self.confusion_matrices.append(classifier.verify(each[1]))
            self.gold_instances.append(classifier.gold_instances)
        
    def print_results(self, log, accuracy, error, fscore, precision, recall):
        self.__print_value(log, accuracy, ACCURACY, 'Accuracy')
        self.__print_value(log, error, ERROR, 'Error')
        self.__print_value(log, fscore, F_SCORE, 'F-score')
        self.__print_value(log, precision, PRECISION, 'Precision')
        self.__print_value(log, recall, RECALL, 'Recall')
        
    def __print_value(self, log, is_true, attribute, str_repn):
        if is_true:
            total = 0
            for each in self.confusion_matrices:
                total += getattr(each, attribute)()
            print >>log, str_repn + ': ' + str(float(total)/len(self.confusion_matrices))
        
    def write(self, log, should_write, data_format, suffix):
        if should_write:
            for index in range(len(self.gold_instances)):
                new_path = self.training_path + str(index + 1) + suffix
                data_format.write_gold(self.gold_instances[index], new_path)
                print >>log, 'Gold classification written to ' + new_path + ' file.'
    
    def train(self):
        #do Nothing
        pass

class TestStrategy:
    def __init__(self, classifier, test, test_path, classifier_options):
        self.classifier = classifier
        self.test = test
        self.test_path = test_path
        classifier_options.set_options(self.classifier)
        
    def classify(self):
        self.classifier.test(self.test)
        
    def print_results(self, log, accuracy, error, fscore, precision, recall):
        """
        Nothing to print in tests
        """
        
    def write(self, log, should_write, data_format, suffix):
        """
        Will always write in the case of test files
        """
        data_format.write_test(self.test, self.test_path + suffix)
        print >>log, 'Test classification written to ' + self.test_path + suffix + ' file.'
        
    def train(self):
        self.classifier.train()
        
class VerifyStrategy:
    def __init__(self, classifier, gold, gold_path, classifier_options):
        self.classifier = classifier
        self.gold = gold
        self.gold_path = gold_path
        self.confusion_matrix = None
        classifier_options.set_options(self.classifier)
        
    def classify(self):
        self.confusion_matrix = self.classifier.verify(self.gold)
        
    def print_results(self, log, accuracy, error, fscore, precision, recall):
        self.__print_value(log, accuracy, ACCURACY, 'Accuracy')
        self.__print_value(log, error, ERROR, 'Error')
        self.__print_value(log, fscore, F_SCORE, 'F-score')
        self.__print_value(log, precision, PRECISION, 'Precision')
        self.__print_value(log, recall, RECALL, 'Recall')
        
    def __print_value(self, log, is_true, attribute, str_repn):
        if is_true: 
            print >>log, str_repn + ': ' + getattr(self.confusion_matrix, attribute)().__str__()
            
    def write(self, log, should_write, data_format, suffix):
        if should_write:
            data_format.write_gold(self.gold, self.gold_path + suffix)
            print >>log, 'Gold classification written to ' + self.gold_path + suffix + ' file.'
            
    def train(self):
        self.classifier.train()
    
class CommonBaseNameStrategy:
    def __init__(self, files, verify):
        self.files = files
        self.verify = verify
        
    def values(self):
        return [self.files] + self.__test_or_gold()
    
    def __test_or_gold(self):
        if self.verify:
            return [None, self.files]
        return [self.files, None]

class ExplicitNamesStrategy:
    def __init__(self, training, test, gold):
        self.training = training
        self.test = test
        self.gold = gold
        
    def values(self):
        return [self.training, self.test, self.gold]
                
class DecisionTreeOptions:
    VALID = {'IG': 'maximum_information_gain', 'GR': 'maximum_gain_ratio'}
    
    def __init__(self, options):
        self.options = options
        
    def valid(self):
        if self.options not in self.VALID:
            return False
        return True
        
    def set_options(self, classifier):
        if self.valid():
            classifier.set_options(self.VALID[self.options])
        
class NoOptions:
        
    def set_options(self, classifier):
        #do Nothing
        pass

if __name__ == "__main__":
    Classify().run(sys.argv[1:])

########NEW FILE########
__FILENAME__ = commandline
# Natural Language Toolkit CommandLine
#     understands the command line interaction
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT
from optparse import OptionParser
from nltk_contrib.classifier.exceptions import filenotfounderror as fnf, invaliddataerror as inv
from nltk_contrib.classifier import format
import time

D_help = "Used to specify the data format.                      " \
        + "Options: C45 for C4.5 format.                        " \
        + "Default: C45.                                        "
l_help = "Used to specify the log file.                         "


ALGORITHM = 'algorithm'
FILES = 'files'
TRAINING = 'training'
TEST = 'test'
GOLD = 'gold'
DATA_FORMAT = 'data_format'
LOG_FILE = 'log_file'
OPTIONS = 'options'

C45_FORMAT = 'c45' 

DATA_FORMAT_MAPPINGS = {C45_FORMAT: format.c45}

class CommandLineInterface(OptionParser):
    def __init__(self, alg_choices, alg_default, a_help, f_help, t_help, T_help, g_help, o_help):
        OptionParser.__init__(self)
        self.add_option("-a", "--algorithm", dest=ALGORITHM, type="choice", \
                        choices=alg_choices, default=alg_default, help= a_help)
        self.add_option("-f", "--files", dest=FILES, type="string", help=f_help)
        self.add_option("-t", "--training-file", dest=TRAINING, type="string", help=t_help)
        self.add_option("-T", "--test-file", dest=TEST, type="string", help=T_help)
        self.add_option("-g", "--gold-file", dest=GOLD, type="string", help=g_help)
        
        self.add_option("-D", "--data-format", dest=DATA_FORMAT, type="choice", choices=DATA_FORMAT_MAPPINGS.keys(), \
                default=C45_FORMAT, help=D_help)
        self.add_option("-l", "--log-file", dest=LOG_FILE, type="string", help=l_help)
        self.add_option("-o", "--options", dest=OPTIONS, type="string", help=o_help)
        
    def get_value(self, name):
        return self.values.ensure_value(name, None)
    
    def parse(self, args):
        """
        method to aid testing
        """
        self.parse_args(args, None)

    def execute(self):
        """
        Stores values from arguments which are common to all command line interfaces
        """
        self.algorithm = self.get_value(ALGORITHM)
        self.files = self.get_value(FILES)
        self.training_path = self.get_value(TRAINING)
        self.test_path = self.get_value(TEST)
        self.gold_path = self.get_value(GOLD)
        self.options = self.get_value(OPTIONS)
        self.data_format = DATA_FORMAT_MAPPINGS[self.get_value(DATA_FORMAT)]
        log_file = self.get_value(LOG_FILE)
        self.log = None
        if log_file is not None:
            self.log = open(log_file, 'a')
            print >>self.log, '-' * 40
            print >>self.log, 'DateTime: ' + time.strftime('%c', time.localtime())

    def run(self, args):
        """
        Main method which delegates all the work
        """
        self.parse(args)
        self.execute()
        if self.log is not None: self.log.close()
        
    def validate_basic_arguments_are_present(self):
        if self.algorithm is None or self.files is None and self.training_path is None : 
            self.required_arguments_not_present_error()
            
    def validate_files_arg_is_exclusive(self):
        if self.files is not None and (self.training_path is not None or self.test_path is not None or self.gold_path is not None):
            self.error("Invalid arguments. The files argument cannot exist with training, test or gold arguments.")

    def get_instances(self, training_path, test_path, gold_path, ignore_missing = False):
        test = gold = None
        training = self.data_format.training(training_path)
        attributes, klass = self.data_format.metadata(training_path)
        test = self.__get_instance(self.data_format.test, test_path, ignore_missing)
        gold = self.__get_instance(self.data_format.gold, gold_path, ignore_missing)
        return (training, attributes, klass, test, gold)
    
    def __get_instance(self, method, path, ignore_if_missing):
        if path is not None:
            if ignore_if_missing:
                try:
                    return method(path)
                except fnf.FileNotFoundError:
                    return None
            return method(path)
        return None

    def required_arguments_not_present_error(self):
        self.error("Invalid arguments. One or more required arguments are not present.")
        
    def write_to_file(self, suffix, training, attributes, klass, test, gold, include_classification = True):
        files_written = []
        files_written.append(self.data_format.write_training(training, self.training_path + suffix))
        if test is not None: files_written.append(self.data_format.write_test(test, self.test_path + suffix, include_classification))
        if gold is not None: files_written.append(self.data_format.write_gold(gold, self.gold_path + suffix, include_classification))
        files_written.append(self.data_format.write_metadata(attributes, klass, self.training_path + suffix))
        return files_written
    
    def log_common_params(self, name):
        if self.log is not None: 
            print >>self.log, 'Operation: ' + name
            print >>self.log, '\nAlgorithm: ' + str(self.algorithm) + '\nTraining: ' + str(self.training_path) + \
                    '\nTest: ' + str(self.test_path) + '\nGold: ' + str(self.gold_path) + '\nOptions: ' + str(self.options)
            
            
    def log_created_files(self, files_names, message):
        if self.log is None:
            print message
        else:
            print >>self.log, "NumberOfFilesCreated: " + str(len(files_names))
        count = 0
        for file_name in files_names:
            if self.log is None:
                print file_name
            else:
                print >>self.log, "CreatedFile" + str(count) + ": " + file_name
            count += 1


def as_integers(name, com_str):
    indices = []
    if com_str is not None:
        for element in com_str.split(','):
            try:
                indices.append(int(element.strip()))
            except ValueError:
                raise inv.InvalidDataError('Invalid Data. ' + name + ' should contain integers.')
    return indices


########NEW FILE########
__FILENAME__ = confusionmatrix
# Natural Language Toolkit - Confusion Matrix
#  Updates itself with each classification result and constructs a matrix 
#  Using the matrix it is capable of calculating several performance figures
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT

from nltk_contrib.classifier.exceptions import systemerror as se
class ConfusionMatrix:
    def __init__(self, klass):
        self.index, self.matrix = {}, []
        self.__num_class_vals = len(klass)
        for i in range(self.__num_class_vals): 
            self.index[klass[i]] = i
            self.matrix.append([0] * self.__num_class_vals)
        
    def count(self, actual, predicted):
        self.matrix[self.index[actual]][self.index[predicted]] += 1
        
    def accuracy(self, index = 0):
        return self.__div(self.tp(index) + self.tn(index), self.tp(index) + self.fp(index) + self.fn(index) + self.tn(index))
        
    def error(self, index=0):
        return 1 - self.accuracy(index)
    
    def tpr(self, index = 0):
        return self.__div(self.tp(index), self.tp(index) + self.fn(index))
    sensitivity = tpr
    
    def tnr(self, index = 0):
        return self.__div(self.tn(index), self.fp(index) + self.tn(index))
    specificity = tnr
    
    def fpr(self, index = 0):
        return self.__div(self.fp(index), self.fp(index) + self.tn(index))
    
    def precision(self, index = 0):
        return self.__div(self.tp(index), self.tp(index) + self.fp(index))
    
    def recall(self, index = 0):
        return self.__div(self.tp(index), self.tp(index) + self.fn(index))
    
    def fscore(self, beta = 1, index = 0):
        p, r = self.precision(index), self.recall(index)
        return (1 + beta * beta) * self.__div(p * r, r + beta * beta * p)
    
    def __div(self, num, den):
        if num == 0: return 0;
        if den == 0: raise se.SystemError('Divide by Zero Error')
        return float(num)/ den
    
    def tp(self, index = 0):
        return self.matrix[index][index]
        
    def tn(self, index = 0):
        return sum([self.matrix[i][j] for i in range(self.__num_class_vals) for j in range(self.__num_class_vals) if not (i == index or j == index)])
        
    def fp(self, index = 0):
        return sum([self.matrix[i][index] for i in range(self.__num_class_vals) if not i == index])
            
    def fn(self, index = 0):
        return sum([self.matrix[index][i] for i in range(self.__num_class_vals) if not i == index])

    def __str__(self):
        strn = '{'
        for i in range(self.__num_class_vals): 
            strn += ','.join([str(self.matrix[i][j]) for j in range(self.__num_class_vals)]) + '\n'
        strn += '}'
        return strn

########NEW FILE########
__FILENAME__ = decisionstump
# Natural Language Toolkit - Decision Stump
#  Understands the procedure of creating a decision stump and 
#     calculating the number of errors
#  Is generally created at the attribute level
#   ie. each attribute will have a decision stump of its own
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT

from math import log
from nltk.probability import FreqDist

class DecisionStump:
    """
    Decision Stump is a tree created for each attribute, with branches for
    each attribute value. It also stores the count for each attribute value
    """
    def __init__(self, attribute, klass):
        self.attribute = attribute
        self.__safe_default = None
        """
        counts is a dictionary in which 
        each key is an attribute value
        and each value is a dictionary of class frequencies for that attribute value
        """
        self.children = {} #it has children only in decision trees
        self.root = dictionary_of_values(klass)
        self.counts = {}
        for value in attribute.values:
            self.counts[value] = dictionary_of_values(klass)
            
    def update_count(self, instance):
        attr_value = instance.value(self.attribute)
        self.counts[attr_value][instance.klass_value] += 1
        self.root[instance.klass_value] += 1
    
    def error(self):
        count_for_each_attr_value = self.counts.values()
        total, errors = 0, 0
        for class_count in count_for_each_attr_value:
            subtotal, counts = 0, class_count.values()
            counts.sort()
            for count in counts: subtotal += count
            errors += (subtotal - counts[-1])
            total += subtotal
        return float(errors)/ total
    
    def klass(self, instance):
        attr_value = instance.value(self.attribute)
        if len(self.children) == 0 or not attr_value in self.children: 
            return self.majority_klass(attr_value)
        return self.children[attr_value].klass(instance)
    
    def majority_klass(self, attr_value):
        klass_values_with_count = self.counts[attr_value]
        _max, klass_value = 0, self.safe_default() # will consider safe default because at times the test will have an attribute value not present in the stump(can happen in cross validation as well)
        for klass, count in klass_values_with_count.items():
            if count > _max:
                _max, klass_value = count, klass
        return klass_value
    
    def safe_default(self):
        """
        Mimics Zero-R behavior by find the majority class in all the occurances at this stump level
        """
        if self.__safe_default == None:
            max_occurance, klass = -1, None
            for klass_element in self.root.keys():
                if self.root[klass_element] > max_occurance:
                    max_occurance = self.root[klass_element]
                    klass = klass_element
            self.__safe_default = klass
        return self.__safe_default
    
    def entropy(self, attr_value):
        """
        Returns the entropy of class disctribution for a particular attribute value
        """
        from nltk_contrib.classifier import entropy_of_key_counts
        return entropy_of_key_counts(self.counts[attr_value])
    
    def mean_information(self):
        total, total_num_of_instances = 0, 0
        for attr_value in self.attribute.values:
            instance_count = total_counts(self.counts[attr_value])
            if instance_count == 0: 
                continue
            total += (instance_count * self.entropy(attr_value))
            total_num_of_instances += instance_count
        return float(total) / total_num_of_instances
    
    def information_gain(self):
        from nltk_contrib.classifier import entropy_of_key_counts
        return entropy_of_key_counts(self.root) - self.mean_information()
    
    def gain_ratio(self):
        return float(self.information_gain()) / self.split_info()
    
    def split_info(self):
        instance_distrbn = FreqDist()
        for attribute_value in self.counts:
            instance_distrbn.inc(attribute_value)#laplacian smoothing
            class_values = self.counts[attribute_value]
            for class_value in class_values:
                instance_distrbn.inc(attribute_value, self.counts[attribute_value][class_value])
        from nltk_contrib.classifier import entropy_of_freq_dist
        return entropy_of_freq_dist(instance_distrbn)
    
    def __str__(self):
        _str = 'Decision stump for attribute ' + self.attribute.name
        for key, value in self.counts.items():
            _str += '\nAttr value: ' + key + '; counts: ' + value.__str__()
        for child in self.children:
            _str += child.__str__()
        return _str
        
def total_counts(dictionary_of_klass_freq):
    return sum([count for count in dictionary_of_klass_freq.values()])
        
def dictionary_of_values(klass):
    return dict([(value, 0) for value in klass])

########NEW FILE########
__FILENAME__ = decisiontree
# Natural Language Toolkit - Decision Tree
#  Creates a Decision Tree Classifier
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT

from nltk_contrib.classifier import oner

class DecisionTree(oner.OneR):
    DEFAULT_METRIC = 'maximum_information_gain'
    
    def __init__(self, training, attributes, klass):
        oner.OneR.__init__(self, training, attributes, klass)
        self.root = None
        
    def train(self):
        oner.OneR.train(self)
        self.root = self.build_tree(self.training, [])
        
    def build_tree(self, instances, used_attributes):
        decision_stump = self.best_decision_stump(instances, used_attributes, self.options or DecisionTree.DEFAULT_METRIC)
        if len(self.attributes) - len(used_attributes) == 1: return decision_stump
        used_attributes.append(decision_stump.attribute)
        for attr_value in decision_stump.attribute.values:
            if decision_stump.entropy(attr_value) == 0:
                continue
            new_instances = instances.filter(decision_stump.attribute, attr_value)
            new_child = self.build_tree(new_instances, used_attributes)
            if new_child is not None: decision_stump.children[attr_value] = new_child
        return decision_stump
    
    def classify(self, instances):
        for instance in instances:
            instance.classified_klass = self.root.klass(instance)
        
    def maximum_information_gain(self, decision_stumps):
        return self.higher_value_preferred(decision_stumps, lambda decision_stump: decision_stump.information_gain())
    
    def maximum_gain_ratio(self, decision_stumps):
        return self.higher_value_preferred(decision_stumps, lambda decision_stump: decision_stump.gain_ratio())
    
    def higher_value_preferred(self, decision_stumps, method):
        highest, max_stump = -1, None
        for decision_stump in decision_stumps:
            new = method(decision_stump)
            if new > highest: highest, max_stump = new, decision_stump
        return max_stump
    
    def is_trained(self):
        return self.root is not None
        

########NEW FILE########
__FILENAME__ = discretise
# Natural Language Toolkit - Discretise
#  The command line entry point to discretisers
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT
from nltk_contrib.classifier import split_ignore_space
from nltk_contrib.classifier import instances as ins, discretisedattribute as da, cfile as f, numrange as r, format, commandline as cl, util
from nltk_contrib.classifier.exceptions import filenotfounderror as fnf, invaliddataerror as inv
import sys

a_help = "Selects the discretisation algorithm                 " \
       + "Options: UEW for Unsupervised Equal Width            " \
       + "         UEF for Unsupervised Equal Frequency        " \
       + "         NS for Naive Supervised                     " \
       + "         NS1 for Naive Supervised version 1          " \
       + "         NS2 for Naive Supervised version 2          " \
       + "         ES for Entropy Based Supervised             " \
       + "Default: UEW."

f_help = "Base name of attribute, klass, training, test and gold" \
       + " files.                                              "

t_help = "Base name of training file for discretisation.       "

T_help = "Base name of test file to be discretised.            "

g_help = "Base name of gold file to be discretised.            "

A_help = "Comma separated list of attribute indices           " \
       + "Index numbering starts from 0 and not 1.                         "

o_help = "Algorithm specific options                           " \
       + "UEW: Comma separated list of number of parts in which" \
       + "     each attribute should be split.                 "

UNSUPERVISED_EQUAL_WIDTH = 'UEW'
UNSUPERVISED_EQUAL_FREQUENCY = 'UEF'
NAIVE_SUPERVISED = 'NS'
NAIVE_SUPERVISED_V1 = 'NS1'
NAIVE_SUPERVISED_V2 = 'NS2'
ENTROPY_BASED_SUPERVISED = 'ES'

ALGORITHM_MAPPINGS = {UNSUPERVISED_EQUAL_WIDTH : 'unsupervised_equal_width', \
                     UNSUPERVISED_EQUAL_FREQUENCY : 'unsupervised_equal_frequency', \
                     NAIVE_SUPERVISED : 'naive_supervised', \
                     NAIVE_SUPERVISED_V1 : 'naive_supervised_v1', \
                     NAIVE_SUPERVISED_V2 : 'naive_supervised_v2', \
                     ENTROPY_BASED_SUPERVISED : 'entropy_based_supervised'}


class Discretise(cl.CommandLineInterface):    
    def __init__(self):
        cl.CommandLineInterface.__init__(self, ALGORITHM_MAPPINGS.keys(), UNSUPERVISED_EQUAL_WIDTH, a_help, f_help, t_help, T_help, g_help, o_help)
        self.add_option("-A", "--attributes", dest="attributes", type="string", help=A_help)
        
    def execute(self):
        cl.CommandLineInterface.execute(self)
        self.attributes_indices = self.get_value('attributes')
        self.validate_basic_arguments_are_present()
        self.validate_files_arg_is_exclusive()
        
        if not self.algorithm == NAIVE_SUPERVISED and self.options is None: 
            self.error("Invalid arguments. One or more required arguments are not present.")
        self.discretise_and_write_to_file()
        
    def discretise_and_write_to_file(self):
        ignore_missing = False
        #duplicate code and not tested!!
        if self.files is not None:
            self.training_path, self.test_path, self.gold_path = [self.files] * 3
            ignore_missing = True
        training, attributes, klass, test, gold = self.get_instances(self.training_path, self.test_path, self.gold_path, ignore_missing)
        self.log_common_params('Discretisation')    
        disc = Discretiser(training, attributes, klass, test, gold, cl.as_integers('Attribute indices', self.attributes_indices), cl.as_integers('Options', self.options))
        getattr(disc, ALGORITHM_MAPPINGS[self.algorithm])()
        files_written = self.write_to_file(self.get_suffix(), training, attributes, klass, test, gold, False)
        self.log_created_files(files_written, 'The following files were created with discretised values...')
            
    def get_suffix(self):
        indices_str = ''
        indices = self.attributes_indices.split(',')
        for index in indices:
            indices_str += '_' + str(index.strip())
        return '-d' + '_' + self.algorithm + indices_str

class Discretiser:
    def __init__(self, training, attributes, klass, test, gold, attribute_indices, options = None):
        """
        Initializes the discretiser object
        self.subset contains the attributes which have to be discretised
        """
        self.training, self.attributes, self.klass, self.test, self.gold = training, attributes, klass, test, gold
        self.attribute_indices, self.options = attribute_indices, options
        self.__validate_attribute_indices()
        self.__validate_options()
        self.subset = self.attributes.subset(self.attribute_indices)

    def __validate_options(self):
        if self.options is None: return
        for option in self.options:
            if option == 0:
                raise inv.InvalidDataError('Option cannot be equal to zero.')

    def __validate_attribute_indices(self):
        for index in self.attribute_indices:
            if index < 0 or index >= len(self.attributes):
                raise inv.InvalidDataError('Attribute indices should be between 0 and ' + str(len(self.attributes) - 1) + ' both inclusive, but found ' + str(index))
            
    def unsupervised_equal_width(self):
        ranges = self.training.value_ranges(self.subset)
        disc_attrs = self.discretised_attributes(ranges)
        self.__discretise(disc_attrs)
    
    def __discretise(self, disc_attrs):
        self.training.discretise(disc_attrs)
        if self.test is not None: self.test.discretise(disc_attrs)
        if self.gold is not None: self.gold.discretise(disc_attrs)
        self.attributes.discretise(disc_attrs)
    
    def unsupervised_equal_frequency(self):
        values_array = self.training.values_grouped_by_attribute(self.subset)
        disc_attrs = []
        for index in range(len(self.subset)):
            values = values_array[index]
            values.sort()
            attribute = self.subset[index] 
            ranges = ranges_from_chunks(get_chunks_with_frequency(values, self.options[index]))
            disc_attrs.append(da.DiscretisedAttribute(attribute.name, ranges, attribute.index))
        self.__discretise(disc_attrs)
    
    def naive_supervised(self):
        self.__supervised_discretisation(lambda breakpoints, index: breakpoints.find_naive())

    def naive_supervised_v1(self):
        self.__supervised_discretisation(lambda breakpoints, index: breakpoints.find_naive_v1(self.options[index]))

    def naive_supervised_v2(self):
        self.__supervised_discretisation(lambda breakpoints, index: breakpoints.find_naive_v2(self.options[index]))
    
    def entropy_based_supervised(self):
        self.__supervised_discretisation(lambda breakpoints, index: breakpoints.find_entropy_based_max_depth(self.options[index]))
    
    def __supervised_discretisation(self, action):
        disc_attrs = []
        for index in range(len(self.subset)):
            attribute = self.subset[index]
            breakpoints = self.training.supervised_breakpoints(attribute)
            action(breakpoints, index)
            disc_attrs.append(da.DiscretisedAttribute(attribute.name, breakpoints.as_ranges(), attribute.index))
        self.__discretise(disc_attrs)

    def discretised_attributes(self, ranges):
        discretised_attributes = []
        for index in range(len(self.options)):
            _range, width, attribute = ranges[index], self.options[index], self.subset[index]
            discretised_attributes.append(da.DiscretisedAttribute(attribute.name, _range.split(width), attribute.index))
        return discretised_attributes
            
def get_chunks_with_frequency(values, freq):
    chunks = []
    while len(values) > 0:
        chunk = values[:freq]
        chunks.append(chunk)
        values = values[freq:]
        while len(values) > 0 and chunk[-1] == values[0]:
            values = values[1:]
    return chunks

def ranges_from_chunks(chunks):
    ranges = []
    if len(chunks) > 0: prev = chunks[0][0]
    for index in range(len(chunks) - 1):
        mid = float(chunks[index][-1] + chunks[index + 1][0]) / 2
        ranges.append(r.Range(prev, mid))
        prev = mid
    ranges.append(r.Range(prev, chunks[-1][-1], True))
    return ranges

def create_and_run(algorithm, path, indices, log_path, options):
    disc = Discretise()
    params = ['-a', algorithm, '-f', path, '-A', util.int_array_to_string(indices_string)]
    if options is not None:
        params.extend(['-o', options])
    if log_path is not None:
        params.extend(['-l', log_path])
    print "Params " + str(params)
    disc.run(params)
    return disc.get_suffix()

def batch_run(path, indices, log_path, options):
    created_file_suffixes = []
    for each in options:
        suffix = self.create_and_run(each, path, indices, log_path, options[each])
        created_file_suffixes.append(suffix)
    return created_file_suffixes

if __name__ == "__main__":
    Discretise().run(sys.argv[1:])


########NEW FILE########
__FILENAME__ = discretisedattribute
# Natural Language Toolkit Discretized attribute
#    Capable of mapping continuous values to discrete ones
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT
from nltk_contrib.classifier import attribute, autoclass
from nltk_contrib.classifier.exceptions import invaliddataerror as inv

class DiscretisedAttribute(attribute.Attribute):
    def __init__(self, name, ranges, index):
        self.name = name
        self.values, klass_value = [], autoclass.FIRST
        for i in range(len(ranges)):
            self.values.append(klass_value.name)
            klass_value = klass_value.next()
        self.index = index
        self.type = attribute.DISCRETE
        self.ranges = ranges
        
    def mapping(self, continuous_value):
        range_index = binary_search(self.ranges, continuous_value)
        if range_index == -1: 
            if continuous_value > self.ranges[-1].upper: return self.values[len(self.ranges) - 1]
            else: return self.values[0]
        return self.values[range_index]
    
    def __ranges_as_string(self):
        return str([str(_range) for _range in self.ranges])
    
    def __str__(self):
        return attribute.Attribute.__str__(self) + self.__ranges_as_string()

def binary_search(ranges, value):
    length = len(ranges)
    low, high = 0, length - 1
    mid = low + (high - low) / 2;
    while low <= high:
        if ranges[mid].includes(value):
            return mid
        elif ranges[mid].lower > value: # search lower half
            high = mid - 1
        else: # search upper half
            low = mid + 1
        mid = low + (high - low) / 2
    return -1
            

########NEW FILE########
__FILENAME__ = distancemetric
# Natural Language Toolkit - Distance Metric
#  distance metrics to be used with different types of attributes
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT
import math

def euclidean_distance(instance1, instance2, attributes):
    total = 0
    for attribute in attributes:
        d = distance(instance1.value(attribute), instance2.value(attribute), attribute.is_continuous())
        total += d * d
    return math.sqrt(total)
        
def hamiltonian_distance(instance1, instance2, attributes):
    return sum([distance(instance1.value(attribute), instance2.value(attribute), attribute.is_continuous()) for attribute in attributes])

def distance(value1, value2, is_continuous):
    if is_continuous:
        return abs(value1 - value2)    
    if value1 == value2:
        return 0
    return 1
    
    

########NEW FILE########
__FILENAME__ = filenotfounderror
# Natural Language Toolkit - FileNotFound Error
#  Is thrown when a file cannot be found at the path specified
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT

import os

class FileNotFoundError:
    def __init__(self, path):
        self.msg = 'cannot find file at ' + os.path.abspath(path)
    
    def __str__(self):
        return self.msg

########NEW FILE########
__FILENAME__ = illegalstateerror
# Natural Language Toolkit - Illegal state error 
#   Is thrown to show indicate an illegal state in the program
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT

from nltk_contrib.classifier.exceptions import SimpleError

class IllegalStateError(SimpleError):
    pass

########NEW FILE########
__FILENAME__ = invaliddataerror
# Natural Language Toolkit - InvalidDataError 
#  Is thrown when the input data entered by the user is invalid
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT
from nltk_contrib.classifier.exceptions import SimpleError

class InvalidDataError(SimpleError):
    pass

########NEW FILE########
__FILENAME__ = systemerror
# Natural Language Toolkit - System error 
#   Is thrown to show unusual behavior not caused by bad input from the user, 
#   but from a programming mistake.
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT
from nltk_contrib.classifier.exceptions import SimpleError

class SystemError(SimpleError):
    pass

########NEW FILE########
__FILENAME__ = featureselect
# Natural Language Toolkit - Feature Select
#  The command line entry point for feature selection
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT
from nltk_contrib.classifier import split_ignore_space
from nltk_contrib.classifier import format, cfile, commandline as cl, attribute as attr, classify as cy
from nltk_contrib.classifier.exceptions import invaliddataerror as inv
import copy

import sys

a_help = "Selects the feature selection algorithm                 " \
       + "Options: RNK for Ranking(Filter method of Feature       " \
       + "                         Selection)                     " \
       + "         FS for Forward Selection(Wrapper)              " \
       + "         BE for Backward Eliminiation(Wrapper)          " \
       + "Default: RNK.                                           "
       
f_help = "Base name of attribute, klass, training, test and gold  " \
       + " files.                                                 "

t_help = "Base name of training file for feature selection.       "

T_help = "Base name of test file for feature selection.           "

g_help = "Base name of gold file for feature selection.           "

o_help = "Algorithm specific options                              " \
       + "                                                        " \
       + "For RNK based feature selection the options should      " \
       + "include the method to calculate the rank:               " \
       + "  IG: for Information gain                              " \
       + "  GR: for Gain ratio                                    " \
       + "followed by a number which indicates the number of      " \
       + "attributes which should be chosen.                      " \
       + "                                                        " \
       + "For FS and BE based feature selection the options should" \
       + "compulsorily include the induction algorithm as the first" \
       + "parameter. The second and third parameters are the fold " \
       + "and delta respectively. The default value of fold is 10." \
       + "The default value of delta is 0.                        " 

R_help = "Ranking algorithm.                                      "
INFORMATION_GAIN = 'IG'
GAIN_RATIO = 'GR'
OPTION_MAPPINGS = {INFORMATION_GAIN: 'information_gain', GAIN_RATIO: 'gain_ratio'}

RANK='RNK'
FORWARD_SELECTION='FS'
BACKWARD_ELIMINATION='BE'

ALGORITHM_MAPPINGS = {RANK:'by_rank', FORWARD_SELECTION:'forward_selection', BACKWARD_ELIMINATION:'backward_elimination'}
DEFAULT_FOLD=10
MIN_FOLD=2

class FeatureSelect(cl.CommandLineInterface):
    def __init__(self):
        cl.CommandLineInterface.__init__(self, ALGORITHM_MAPPINGS.keys(), RANK, a_help, f_help, t_help, T_help, g_help, o_help)        
        
    def execute(self):
        cl.CommandLineInterface.execute(self)
        self.validate_basic_arguments_are_present()
        self.validate_files_arg_is_exclusive()
        if self.options is None:
            self.required_arguments_not_present_error()
        self.split_options = split_ignore_space(self.options)
        if OPTIONS_TEST[self.algorithm](self.split_options):
            self.error("Invalid options for Feature selection.")
        self.select_features_and_write_to_file()

    def select_features_and_write_to_file(self):
        ignore_missing = False
        #duplicate code and not tested!!
        if self.files is not None:
            self.training_path, self.test_path, self.gold_path = [self.files] * 3
            ignore_missing = True
        training, attributes, klass, test, gold = self.get_instances(self.training_path, self.test_path, self.gold_path, ignore_missing)
        self.log_common_params('FeatureSelection')
        feature_sel = FeatureSelection(training, attributes, klass, test, gold, self.split_options)
        getattr(feature_sel, ALGORITHM_MAPPINGS[self.algorithm])()
        
        files_written = self.write_to_file(self.get_suffix(), training, attributes, klass, test, gold, False)
        self.log_created_files(files_written, 'The following files were created after feature selection...')
            
    def get_suffix(self):
        if self.options is None: return '-' + self.algorithm
        suf = '-f_' + self.algorithm
        for option in self.split_options:
            suf += '_' + option.replace('.','-')
        return suf

class FeatureSelection:
    def __init__(self, training, attributes, klass, test, gold, options):
        self.training, self.attributes, self.klass, self.test, self.gold = training, attributes, klass, test, gold
        self.options = options
        
    def by_rank(self):
        if self.attributes.has_continuous():
            raise inv.InvalidDataError("Rank based feature selection cannot be performed on continuous attributes.")
        if rank_options_invalid(self.options):
            raise inv.InvalidDataError("Invalid options for Rank based Feature selection.")#Additional validation when not used from command prompt
        rem_attributes = self.find_attributes_by_ranking(OPTION_MAPPINGS[self.options[0]], int(self.options[1]))
        self.remove(rem_attributes)
    
    def find_attributes_by_ranking(self, method, number):
        decision_stumps = self.attributes.empty_decision_stumps([], self.klass)
        for decision_stump in decision_stumps:
            for instance in self.training:
                decision_stump.update_count(instance)
        decision_stumps.sort(lambda x, y: cmp(getattr(x, method)(), getattr(y, method)()))
        
        if number > len(decision_stumps): number = len(decision_stumps)
        to_remove = decision_stumps[:number * -1]
        return [stump.attribute for stump in to_remove]
    
    def forward_selection(self):
        if wrapper_options_invalid(self.options):
            raise inv.InvalidDataError("Invalid options for Forward Select Feature selection.")#Additional validation when not used from command prompt
        selected = self.__select_attributes(-1, [], self.attributes[:], self.get_delta())
        self.remove(self.invert_attribute_selection(selected))
        
    def backward_elimination(self):
        if wrapper_options_invalid(self.options):
            raise inv.InvalidDataError("Invalid options for Backward Select Feature selection.")
        fold = self.get_fold()
        avg_acc = self.avg_accuracy_by_cross_validation(self.training.cross_validation_datasets(fold), fold, self.attributes)
        selected = self.__eliminate_attributes(avg_acc, self.attributes[:], self.get_delta())
        self.remove(self.invert_attribute_selection(selected))
        
    def __eliminate_attributes(self, max, selected, delta):
        if selected is None or len(selected) == 0 or len(selected) == 1: return selected
        max_at_level, selections_with_max_acc, fold = -1, None, self.get_fold()
        datasets = self.training.cross_validation_datasets(fold)
        selected_for_iter = selected[:]
        for attribute in selected_for_iter:
            selected.remove(attribute)
            avg_accuracy = self.avg_accuracy_by_cross_validation(datasets, fold, attr.Attributes(selected))
            if avg_accuracy > max_at_level:
                max_at_level = avg_accuracy
                selections_with_max_acc = selected[:]
            selected.append(attribute)
        if max_at_level - max < delta: return selected
        return self.__eliminate_attributes(max_at_level, selections_with_max_acc, delta)
    
    def get_delta(self):
        if len(self.options) != 3:
            return 0
        return float(self.options[2])

    def invert_attribute_selection(self, selected):
        not_selected = []
        for attribute in self.attributes:
            if not selected.__contains__(attribute):
                not_selected.append(attribute)
        return not_selected
        
    def __select_attributes(self, max, selected, others, delta):
        if others is None or len(others) == 0: return selected
        max_at_level, attr_with_max_acc, fold = -1, None, self.get_fold()
        datasets = self.training.cross_validation_datasets(fold)
        for attribute in others:
            selected.append(attribute)
            avg_accuracy = self.avg_accuracy_by_cross_validation(datasets, fold, attr.Attributes(selected))
            if avg_accuracy > max_at_level:
                max_at_level = avg_accuracy
                attr_with_max_acc = attribute
            selected.remove(attribute)
        if max_at_level - max < delta: return selected
        
        selected.append(attr_with_max_acc)
        others.remove(attr_with_max_acc)
        return self.__select_attributes(max_at_level, selected, others, delta)
    
    def get_fold(self):
        if len(self.options) == 1:
            specified_fold = DEFAULT_FOLD
        else: specified_fold = int(self.options[1])
        if specified_fold >= len(self.training):
            specified_fold = len(self.training) / 2
        return specified_fold

    def avg_accuracy_by_cross_validation(self, datasets, fold, attributes):
        total_accuracy = 0
        for index in range(fold):
            training, gold = datasets[index]
            classifier = cy.ALGORITHM_MAPPINGS[self.options[0]](training, attributes, self.klass)
            classifier.do_not_validate = True
            classifier.train()
            cm = classifier.verify(gold)
            total_accuracy += cm.accuracy()
        return total_accuracy / len(datasets)
    
    def remove(self, attributes):
        self.training.remove_attributes(attributes)
        if self.test is not None: self.test.remove_attributes(attributes)
        if self.gold is not None: self.gold.remove_attributes(attributes)
        self.attributes.remove_attributes(attributes)

def rank_options_invalid(options):
    return len(options) != 2 or not options[0] in OPTION_MAPPINGS or not options[1].isdigit()

def wrapper_options_invalid(options):
    return (len(options) < 1 or len(options) > 3) \
           or \
           (not options[0] in cy.ALGORITHM_MAPPINGS \
                or \
                ( (len(options) == 2 or len(options) == 3) \
                     and \
                     (not options[1].isdigit() or int(options[1]) < MIN_FOLD)
                )
                or \
                len(options) == 3 and not isfloat(options[2])
           )

OPTIONS_TEST = {RANK : rank_options_invalid, FORWARD_SELECTION : wrapper_options_invalid, BACKWARD_ELIMINATION : wrapper_options_invalid}

def isfloat(stringval):
    try:
        float(stringval)
        return True
    except (ValueError, TypeError), e: return False 
                            
def batch_filter_select(base_path, suffixes, number_of_attributes, log_path, has_continuous):
    filter_suffixes = []
    for each in suffixes:
        for selection_criteria in [INFORMATION_GAIN, GAIN_RATIO]:
            feat_sel = FeatureSelect()
            params = ['-a', RANK, '-f', base_path + each, '-o', selection_criteria + ',' + str(number_of_attributes), '-l', log_path]
            print "Params " + str(params)
            feat_sel.run(params)
            filter_suffixes.append(each + feat_sel.get_suffix())
    return filter_suffixes

def batch_wrapper_select(base_path, suffixes, classifier, fold, delta, log_path):
    wrapper_suffixes = []
    for each in suffixes:
        for alg in [FORWARD_SELECTION, BACKWARD_ELIMINATION]:
            feat_sel = FeatureSelect()
            params = ['-a', alg, '-f', base_path + each, '-o', classifier + ',' + str(fold) + ',' + str(delta), '-l', log_path]
            print "Params " + str(params)
            feat_sel.run(params)
            wrapper_suffixes.append(each + feat_sel.get_suffix())
    return wrapper_suffixes

if __name__ == "__main__":
    FeatureSelect().run(sys.argv[1:])


########NEW FILE########
__FILENAME__ = format
from nltk_contrib.classifier import cfile, item, attribute as a, instance as ins, instances as inss
from nltk_contrib.classifier.exceptions import systemerror as se, filenotfounderror as fnf

class FormatI:    
    def __init__(self, name):
        self.name = name
        
    def metadata(self, file_path):
        """
        Returns a tuple containing attributes and class instances
        """
        return AssertionError()
    
    def training(self, file_path):
        """
        Returns training instances
        """
        return AssertionError()
    
    def test(self, file_path):
        """
        Returns test instances
        """
        return AssertionError()
    
    def gold(self, file_path):
        """
        Returns gold instances
        """
        return AssertionError()
        
    def write_training(self, training, file_path):
        """
        Writes training instances to file system
        """
        return AssertionError()
    
    def write_test(self, test, file_path, including_classification=True):
        """
        Writes test instances to file system
        """
        return AssertionError()

    def write_gold(self, gold, file_path, including_classification=True):
        """
        Writes gold instances to file system
        """
        return AssertionError()
        
    def write_metadata(self, attributes, klass, file_path):
        """
        Writes attributes and class instances to file system
        """
        return AssertionError()
    
class C45Format(FormatI):
    DATA = 'data'
    TEST = 'test'
    GOLD = 'gold'
    NAMES = 'names'

    def __init__(self):
        FormatI.__init__(self, "c45")
        
    def metadata(self, file_path):
        lines = self.__get_lines(file_path, self.NAMES)
        klass_values = item.NameItem(lines[0]).processed().split(',')
        index,attributes = 0, []
        for line in lines:
            nameitem = item.NameItem(line)      
            processed = nameitem.processed()
            if not len(processed) == 0 and nameitem.isAttribute():
                attributes.append(a.Attribute(self.get_name(processed), self.get_values(processed), index))
                index += 1
        return (a.Attributes(attributes), klass_values)
    
    def training(self, file_path):
        all_values = self.__get_all_values(file_path, self.DATA)
        return inss.TrainingInstances([ins.TrainingInstance(values[:-1], values[-1]) for values in all_values if values is not None])
    
    def test(self, file_path):
        all_values = self.__get_all_values(file_path, self.TEST)
        return inss.TestInstances([ins.TestInstance(values) for values in all_values if values is not None])
    
    def gold(self, file_path):
        all_values = self.__get_all_values(file_path, self.GOLD)
        return inss.GoldInstances([ins.GoldInstance(values[:-1], values[-1]) for values in all_values if values is not None])
    
    def __get_all_values(self, file_path, ext):
        lines = self.__get_lines(file_path, ext)
        return [self.__get_comma_sep_values(line) for line in lines]        
    
    def write_training(self, instances, file_path):
        return self.write_to_file(file_path, self.DATA, instances, lambda instance: instance.str_attrs() + ',' + str(instance.klass_value))
        
    def write_test(self, instances, file_path, including_classification=True):
        if not including_classification:
            return self.write_to_file(file_path, self.TEST, instances, lambda instance: instance.str_attrs())
        return self.write_to_file(file_path, self.TEST, instances, lambda instance: instance.str_attrs() + ',' + str(instance.classified_klass))

    def write_gold(self, instances, file_path, including_classification=True):
        if not including_classification:
            return self.write_to_file(file_path, self.GOLD, instances, lambda instance: instance.str_attrs() + ',' + str(instance.klass_value))
        return self.write_to_file(file_path, self.GOLD, instances, lambda instance: instance.str_attrs() + ',' + str(instance.klass_value) + ',' + str(instance.classified_klass))
        
    def write_metadata(self, attributes, klass, file_path):
        new_file = self.create_file(file_path, self.NAMES)
        lines = [','.join([str(value) for value in klass]) + '.']
        for attribute in attributes:
            lines.append(attribute.name + ':' + attribute.values_as_str() + '.')
        new_file.write(lines)
        return file_path + cfile.DOT + self.NAMES
        
    def write_to_file(self, file_path, extension, instances, method):
        new_file = self.create_file(file_path, extension)
        new_file.write([method(instance) for instance in instances])
        return file_path + cfile.DOT + extension
    
    def create_file(self, file_path, extension):
        new_file = cfile.File(file_path, extension)
        new_file.create(True)
        return new_file

    def __get_comma_sep_values(self, line):
        _line = item.Item(line).stripNewLineAndWhitespace()
        if not len(_line) == 0:
            return _line.split(',')
        return None
    
    def __get_lines(self, file_path, ext):
        if file_path is None:
            raise se.SystemError('Cannot open file. File name not specified.')
        return cfile.File(file_path, ext).for_each_line(lambda line: line)
    
    def get_name(self, line):
        return line[:self.__pos_of_colon(line)]
            
    def get_values(self, line):
        return line[self.__pos_of_colon(line) + 1:].split(',')
        
    def __pos_of_colon(self, line):
        return line.find(':')

c45 = C45Format()

########NEW FILE########
__FILENAME__ = instance
# Natural Language Toolkit - Instance
#  Understands the various operations that can be preformed on an instance
#     Each Instance inheriting from the main Instance is capable of operations
#     it can logically perform on that instance value eg: Test instance can 
#     set the Class where as the Training instance cannot
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT

from nltk_contrib.classifier.exceptions import systemerror as system, invaliddataerror as inv
import item, copy

class Instance:
    def __init__(self):
        self.klass_value, self.attrs, self.classified_klass = None, None, None
        
    def is_valid(self, klass, attributes):
        """
        Verifies if the instance contains valid attribute and class values
        """
        return attributes.has_values(self.attrs)
    
    def value(self, attribute):
        """
        Returns the value corresponding to @param:attribute
        """
        if attribute.is_continuous():
            return float(self.attrs[attribute.index])
        return self.attrs[attribute.index]
    
    def values(self, attributes):
        """
        Returns a list of attribute values corresponding to @param:attributes
        """
        return [self.attrs[attribute.index] for attribute in attributes]

    def discretise(self, discretised_attributes):
        """
        Set discretised values for continuous attributes
        """
        for discretised_attribute in discretised_attributes:
            index = discretised_attribute.index
            self.attrs[index] = discretised_attribute.mapping(float(self.attrs[index]))
    
    def remove_attributes(self, attributes):
        """
        Used when selecting features and @param:attributes are removed from instances
        @param:attributes is an array of attributes
        """
        to_be_removed = [attribute.index for attribute in attributes]
        to_be_removed.sort()
        to_be_removed.reverse()
        for r in to_be_removed:
            self.attrs.__delitem__(r)
            
    def convert_to_float(self, indices):
        """
        Converts attribute values at @param:indices to floats from numeric data represented as strings
        Will throw a value error if an attempt is made to convert anything other than numeric data
        """
        for index in indices:
            self.attrs[index] = float(self.attrs[index])
    
    def __eq__(self, other):
        if other is None: return False
        if self.__class__ != other.__class__: return False
        if self.klass_value == other.klass_value and self.attrs == other.attrs and self.classified_klass == other.classified_klass: return True
        return False
    
    def str_klassified_klass(self):
        """
        Returns the classified class as a string, will return <whitespace> if instance is not classified
        """
        return self.__check_none(self.classified_klass)

    def __check_none(self, var):
        if var is None: 
            return ' '
        return var.__str__()

    def str_class(self):
        """
        Returns the class as a string, will return <whitespace> in the case of a test instance
        """
        return self.__check_none(self.klass_value)

    def str_attrs(self):
        """
        Returns the a comma separated string of attribute values
        """
        return ','.join([self.__check_none(each) for each in self.attrs])
    
    def __str__(self):
        return '[' + ';'.join(self.as_str()) + ']'
    
    def as_str(self):
        """
        Helper method for __str__(self)
        """
        return [self.str_attrs()]
        
class TrainingInstance(Instance):
    def __init__(self, attr_values, klass_value):
        Instance.__init__(self)
        self.klass_value, self.attrs = klass_value, attr_values
        
    def is_valid(self, klass, attributes):
        """
        Verifies if the instance contains valid attribute and class values
        """
        return Instance.is_valid(self, klass, attributes) and klass.__contains__(self.klass_value)
    
    def as_gold(self):
        """
        Converts the training instance into a Gold instance(used in cross validation)
        """
        return GoldInstance(copy.copy(self.attrs), self.klass_value)
        
    def as_str(self):
        """
        Helper method for __str__(self)
        """
        _attrs = Instance.as_str(self)
        _attrs.append(self.str_class())
        return _attrs
    
class TestInstance(Instance):
    def __init__(self, attr_values):
        Instance.__init__(self)
        self.attrs = attr_values
        
    def set_klass(self, klass):
        self.classified_klass = klass
        
    def as_str(self):
        """
        Helper method for __str__(self)
        """
        _attrs = Instance.as_str(self)
        _attrs.append(self.str_klassified_klass())
        return _attrs
                
class GoldInstance(TrainingInstance, TestInstance):
    def __init__(self, attr_values, klass_value):
        TrainingInstance.__init__(self, attr_values, klass_value)
        
    def is_valid(self, klass, attributes):
        """
        Verifies if the instance contains valid attribute and class values
        """
        return TrainingInstance.is_valid(self, klass, attributes)
    
    def as_str(self):
        """
        Helper method for __str__(self)
        """
        _attrs = Instance.as_str(self)
        _attrs.append(self.str_class())
        _attrs.append(self.str_klassified_klass())
        return _attrs
    

########NEW FILE########
__FILENAME__ = instances
# Natural Language Toolkit - Instances
#  Understands the creation and validation of instances from input file path
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT

from nltk_contrib.classifier import instance as ins, item, cfile, confusionmatrix as cm, numrange as r, util
from nltk_contrib.classifier.exceptions import systemerror as system, invaliddataerror as inv
from nltk import probability as prob
import operator, UserList, UserDict, math

class Instances(UserList.UserList):
    def __init__(self, instances):
        UserList.UserList.__init__(self, instances)

    def are_valid(self, klass, attributes):
        for instance in self.data:
            if not instance.is_valid(klass, attributes):
                return False
        return True
    
    def discretise(self, discretised_attributes):
        for instance in self.data:
            instance.discretise(discretised_attributes)

    def remove_attributes(self, attributes):
        for instance in self.data:
            instance.remove_attributes(attributes)
            
    def convert_to_float(self, indices):
        for instance in self.data:
            instance.convert_to_float(indices)
            
    def __str__(self):
        return '[' + ', '.join([str(instance) for instance in self.data]) + ']'
            
class TrainingInstances(Instances):
    def __init__(self, instances):
        Instances.__init__(self, instances)
        self.prior_probabilities = None
            
    def filter(self, attribute, attr_value):
        return TrainingInstances([instance for instance in self.data if instance.value(attribute) == attr_value])
    
    def value_ranges(self, attributes):
        """
        Returns an array of range objects, in which each corresponds to the range of values an 
        attribute in the attributes parameter can take.
        len(returned range array) is equal to len(attributes)
        """
        ranges = []
        for attribute in attributes:
            if not attribute.is_continuous():
                raise inv.InvalidDataError('Cannot discretise non continuous attribute ' + attribute.name)
        values = self.values_grouped_by_attribute(attributes)
        for value in values: #each entry in values is the range of values for a particular attribute
            value.sort()
            ranges.append(r.Range(value[0], value[-1], True))
        return ranges
    
    def values_grouped_by_attribute(self, attributes):
        """
        Returns an array where each element is an array of attribute values for a particular attribute
        len(returned array) is equal to len(attributes)
        """
        values = []
        for attribute in attributes:
            _vals_in_attr = []
            for instance in self.data:
                if attribute.is_continuous():
                    _vals_in_attr.append(float(instance.value(attribute)))
                else:
                    _vals_in_attr.append(instance.value(attribute))
            values.append(_vals_in_attr)
        return values
        
    def __as_float(self, values):
        return [float(value) for value in values]
    
    def klass_values(self):
        return [instance.klass_value for instance in self.data]
    
    def supervised_breakpoints(self, attribute):
        self.sort_by(attribute)
        attr_values = self.attribute_values(attribute)
        return SupervisedBreakpoints(self.klass_values(), attr_values)
       
    def attribute_values(self, attribute):
        return [instance.value(attribute) for instance in self.data]
    
    def sort_by(self, attribute):
        self.data.sort(lambda x, y: cmp(x.value(attribute), y.value(attribute)))
        
    def cross_validation_datasets(self, fold):
        """
        Gold instances are completely new objects except for attribute value objects,
        we wont be changing the attribute value objects in the gold instances anyway 
        unless something really weird is happening!
        """
        if fold > len(self): fold = len(self) / 2
        stratified = self.stratified_bunches(fold)
        datasets = []
        for index in range(len(stratified)):
            gold = GoldInstances(training_as_gold(stratified[index]))
            rest = flatten(stratified[:index]) + flatten(stratified[index + 1:])
            training = TrainingInstances(rest)
            datasets.append((training, gold))
        return datasets
    
    def stratified_bunches(self, fold):
        stratified = [[] for each in range(fold)]
        self.data.sort(key=lambda instance: instance.klass_value)
        for index in range(len(self.data)): stratified[index % fold].append(self.data[index])
        return stratified
    
    def posterior_probablities(self, attributes, klass_values):
        freq_dists = attributes.empty_freq_dists()
        for attribute in attributes:
            for value in attribute.values:
                for klass_value in klass_values:
                    freq_dists[attribute][value].inc(klass_value) #Laplacian smoothing
        stat_list_values = {}
        cont_attrs = filter(lambda attr: attr.is_continuous(), attributes)
        if attributes.has_continuous():
            for attribute in cont_attrs:
                stat_list_values[attribute] = {}
                for klass_value in klass_values:
                    stat_list_values[attribute][klass_value] = util.StatList()
        for instance in self.data:
            for attribute in attributes:
                if attribute.is_continuous():
                    stat_list_values[attribute][instance.klass_value].append(instance.value(attribute))
                else:
                    freq_dists[attribute][instance.value(attribute)].inc(instance.klass_value)
        return PosteriorProbabilities(freq_dists, stat_list_values)
                
    def class_freq_dist(self):
        class_freq_dist = prob.FreqDist()
        for instance in self.data:
            class_freq_dist.inc(instance.klass_value)
        return class_freq_dist
            
#todo remove this      
class TestInstances(Instances):
    def __init__(self, instances):
        Instances.__init__(self, instances)
                    
class GoldInstances(Instances):
    def __init__(self, instances):
        Instances.__init__(self, instances)
            
    def confusion_matrix(self, klass):
        for i in self.data:
            if i.classified_klass == None: 
                raise system.SystemError('Cannot calculate accuracy as one or more instance(s) are not classified')
        matrix = cm.ConfusionMatrix(klass)
        for i in self.data:
            matrix.count(i.klass_value, i.classified_klass)
        return matrix
    
class SupervisedBreakpoints(UserList.UserList):
    """
    Used to find breakpoints for discretisation
    """
    def __init__(self, klass_values, attr_values):
        UserList.UserList.__init__(self, [])
        self.attr_values = attr_values
        self.klass_values = klass_values
        
    def find_naive(self):
        self.data[:] = self.breakpoints_in_class_membership()
        self.adjust_for_equal_values()

    def find_naive_v1(self, min_size):
        frequencies = prob.FreqDist()
        for index in range(len(self.klass_values) - 1):
            frequencies.inc(self.klass_values[index])
            if frequencies[frequencies.max()] >= min_size:
                self.append(index)
                frequencies = prob.FreqDist()
        
    def find_naive_v2(self, min_size):
        self.find_naive()
        self.adjust_for_min_freq(min_size)
        
    def find_entropy_based_max_depth(self, max_depth):
        self.max_depth = max_depth
        self.extend(self.__find_breakpoints(self.klass_values))
        
    def __find_breakpoints(self, klass_values, depth = 0):
        breakpoints = []
        if len(klass_values) <= 1: return breakpoints
        from nltk_contrib.classifier import min_entropy_breakpoint
        position, entropy = min_entropy_breakpoint(klass_values)
        if abs(entropy) == 0: return breakpoints
        breakpoints.append(position)
        first, second = klass_values[:position+1], klass_values[position+1:]
        if depth < self.max_depth:
            breakpoints.extend(self.__find_breakpoints(first, depth + 1))
            breakpoints.extend([position + 1 + x for x in self.__find_breakpoints(second, depth + 1)])
        return breakpoints
    
    def breakpoints_in_class_membership(self):
        """
        Returns an array of indices where the class membership changes from one value to another
        the indicies will always lie between 0 and one less than number of instance, both inclusive.
        """
        return [index for index in range(len(self.klass_values) - 1) if not self.klass_values[index] == self.klass_values[index + 1]]
    
    def adjust_for_min_freq(self, min_size):
        prev = -1
        self.sort()
        to_remove,frequencies = [], prob.FreqDist()
        for breakpoint in self.data:
            frequencies.inc(self.klass_values[breakpoint], breakpoint - prev)
            if frequencies[frequencies.max()] < min_size:
                to_remove.append(breakpoint)
            else:
                frequencies = prob.FreqDist()
            prev = breakpoint    
        for item in to_remove:
            self.remove(item)
    
    def adjust_for_equal_values(self):
        index = 0
        to_be_deleted = []
        while(index < len(self.data) - 1):
            if self.attr_values[self.data[index]] == self.attr_values[self.data[index + 1]]:
                to_be_deleted.append(index)
            else:
                while(self.data[index] < self.data[index + 1] and self.attr_values[self.data[index]] == self.attr_values[self.data[index] + 1]):
                    self.data[index] += 1
            index += 1
        to_be_deleted.sort()
        to_be_deleted.reverse()
        for index in to_be_deleted:
            self.data.__delitem__(index)
        last = self.data[-1]
        while (last < len(self.attr_values) - 1 and self.attr_values[last] == self.attr_values[last + 1]):
            self.data[-1] += 1
            last = self.data[-1]
        if last == len(self.attr_values) - 1: del self.data[-1]
    
    def as_ranges(self):
        ranges, lower = [], self.attr_values[0]
        self.sort()
        for breakpoint in self.data:
            mid = (self.attr_values[breakpoint] + self.attr_values[breakpoint + 1]) / 2.0
            ranges.append(r.Range(lower, mid))
            lower = mid
        ranges.append(r.Range(lower, self.attr_values[-1], True))
        return ranges

class PosteriorProbabilities(UserDict.UserDict):
    def __init__(self, freq_dists, stat_list_values):
        self.freq_dists = freq_dists
        self.stat_list_values = stat_list_values
        
    def value(self, attribute, value, klass_value):
        if attribute.is_continuous():
            stat_list = self.stat_list_values[attribute][klass_value]
            return calc_prob_based_on_distrbn(stat_list.mean(), stat_list.std_dev(), value)
        return self.freq_dists[attribute][value].freq(klass_value)
        
def calc_prob_based_on_distrbn(mean, sd, value):
    if sd == 0: 
        if value == mean:
            return 1
        else: return 0
    return (1.0 / math.sqrt(2 * math.pi * sd)) * math.exp(-pow((value - mean), 2)/ (2 * pow(sd, 2)))
        
def training_as_gold(instances):
    return [instance.as_gold() for instance in instances]

## Utility method
#  needs to be pulled out into a common utility class
def flatten(alist):
    if type(alist) == list:
        elements = []
        for each in alist:
            if type(each) == list:
                elements.extend(flatten(each))
            else:
                elements.append(each)
        return elements
    return None

########NEW FILE########
__FILENAME__ = item
# Natural Language Toolkit - Item
#  An item should be capable of operating on its string value
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT

import re

class Item:
    def __init__(self, line):
        self.line = line
    
    def stripNewLineAndWhitespace(self):
        nonewline = self.line.strip()
        return re.compile(' ').sub('', nonewline)

class NameItem(Item):
    def __init__(self, line):
        Item.__init__(self, line)
    
    def processed(self):
        return re.compile('\.$').sub('', self.stripNewLineAndWhitespace())
    
    def isAttribute(self):
        return self.line.find(':') != -1

########NEW FILE########
__FILENAME__ = knn
# Natural Language Toolkit - K nearest neighbour classifier
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT

from nltk_contrib.classifier import instances as ins, Classifier, distancemetric as dm
from nltk import probability as prob

class IB1(Classifier):
    def __init__(self, training, attributes, klass):
        Classifier.__init__(self, training, attributes, klass)
                
    def classify(self, instances):
        for each_test in instances:
            id = InstanceDistances()
            for each_training in self.training:
                dist = dm.euclidean_distance(each_test, each_training, self.attributes)
                id.distance(dist, each_training)
            each_test.classified_klass = id.klass(majority_klass_vote)
    
    @classmethod
    def can_handle_continuous_attributes(self):
        return True
    
    def is_trained(self):
        return True

class InstanceDistances:
    """
    Maps instances to the distance they are from a common test_instance
    """
    def __init__(self):
        self.distances = {}
        
    def distance(self, value, instance):
        if value in self.distances:
            self.distances[value].append(instance)
        else: 
            self.distances[value] = [instance]
            
    def minimum_distance_instances(self):
        keys = self.distances.keys()
        keys.sort()
        return self.distances[keys[0]]
    
    def klass(self, strategy):
        return strategy(self.minimum_distance_instances())
        
def majority_klass_vote(instances):
    fd = prob.FreqDist()
    for each in instances:
        fd.inc(each.klass_value)
    return fd.max()
 

########NEW FILE########
__FILENAME__ = naivebayes
# Natural Language Toolkit - NaiveBayes
#  Capable of classifying the test or gold data using Naive Bayes algorithm
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT

from nltk_contrib.classifier import instances as ins, decisionstump as ds, Classifier
from nltk_contrib.classifier.exceptions import invaliddataerror as inv

class NaiveBayes(Classifier):
    def __init__(self, training, attributes, klass):
        Classifier.__init__(self, training, attributes, klass)
        self.post_probs, self.class_freq_dist = None, None
        
    def train(self):
        Classifier.train(self)
        self.post_probs = self.training.posterior_probablities(self.attributes, self.klass)
        self.class_freq_dist = self.training.class_freq_dist()
        for klass_value in self.klass:
            self.class_freq_dist.inc(klass_value)#laplacian smoothing
            
    def classify(self, instances):
        for instance in instances:
            instance.classified_klass = self.estimate_klass(instance)

    def estimate_klass(self, instance):
        estimates_using_prob = {}
        for klass_value in self.klass:
            class_conditional_probability = self.class_conditional_probability(instance, klass_value)
            estimates_using_prob[class_conditional_probability] = klass_value
        keys = estimates_using_prob.keys()
        keys.sort()#find the one with max conditional prob
        return estimates_using_prob[keys[-1]]
    
    def prior_probability(self, klass_value):
        return self.class_freq_dist.freq(klass_value)
    
    def posterior_probability(self, attribute, attribute_value, klass_value):
        return self.post_probs.value(attribute, attribute_value, klass_value)
    
    def class_conditional_probability(self, instance, klass_value):
        class_cond_prob = 1.0
        for attribute in self.attributes:
            attr_value = instance.value(attribute)
            class_cond_prob *= self.posterior_probability(attribute, attr_value, klass_value)
        class_cond_prob *= self.prior_probability(klass_value)
        return class_cond_prob
    
    @classmethod
    def can_handle_continuous_attributes(self):
        return True
        
    def is_trained(self):
        return self.post_probs is not None and self.class_freq_dist is not None    
    

########NEW FILE########
__FILENAME__ = numrange
# Natural Language Toolkit - Range
#  Represents a range of numbers, not an immutable object and can be modified by include
#  Capable of performing operations on ranges
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT
from nltk_contrib.classifier.exceptions import systemerror as se
DELTA = 0.000001

class Range:
    def __init__(self, lower = 0, upper = 0, upper_includes_max=False):
        """
        any number within this range should be greater than or equal to self.lower and 
        less than (or less than equal to depending on whether it includes the max) self.upper
        """
        self.__delta_added = False
        if upper < lower: 
            raise se.SystemError('Lower limit ' + str(lower) + ' cannot be greater than the Upper limit ' + str(upper) + ' in a range')
        self.__uninitialized = False
        if upper == lower == 0: 
            self.__uninitialized = True
        self.lower, self.upper, self.__delta_added = lower, upper, False
        if upper_includes_max:
            self.upper += DELTA
            self.__delta_added = True
    
    def include(self, number):
        if self.__uninitialized:
            self.lower, self.upper = number, number
            self.__uninitialized = False
        if number >= self.upper:
            self.__delta_added = True
            self.upper = number + DELTA
        elif number < self.lower:
            self.lower = number
            
    def includes(self, number):
        return self.lower <= number and self.upper > number
    
    def split(self, parts):
        if self.lower == self.upper: return None
        size = self.upper - self.lower
        max_limit = self.upper
        if self.__delta_added:
            size -= DELTA
            max_limit -= DELTA
        each = size / parts
        if each < DELTA: 
            raise se.SystemError('Splitting of range resulted in elements smaller than delta ' + str(DELTA) + '.')
        lower, ranges = self.lower, []
        for i in range(parts - 1):
            ranges.append(Range(lower, lower + each))
            lower += each
        ranges.append(Range(lower, self.upper))
        return ranges

    def __eq__(self, other):
        if other is None: return False
        if self.__class__ != other.__class__ : return False
        if self.lower == other.lower and self.upper == other.upper: return True
        return False
    
    def __hash__(self):
        return hash(self.lower) + hash(self.upper)
    
    def __str__(self):
        return '[' + str(self.lower) + ',' + str(self.upper) + ']'

########NEW FILE########
__FILENAME__ = oner
# Natural Language Toolkit - OneR
#  Capable of classifying the test or gold data using the OneR algorithm
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT

from nltk_contrib.classifier import instances as ins, decisionstump as ds, Classifier
from nltk_contrib.classifier.exceptions import invaliddataerror as inv

class OneR(Classifier):
    
    def __init__(self, training, attributes, klass):
        Classifier.__init__(self, training, attributes, klass)
        self.__best_decision_stump = None
        
    def train(self):
        Classifier.train(self)
        self.__best_decision_stump = self.best_decision_stump(self.training)
        
    def classify(self, instances):
        for instance in instances:
            instance.classified_klass = self.__best_decision_stump.klass(instance)

    def best_decision_stump(self, instances, ignore_attributes = [], algorithm = 'minimum_error'):
        decision_stumps = self.possible_decision_stumps(ignore_attributes, instances)
        try:
            return getattr(self, algorithm)(decision_stumps)
        except AttributeError:
            raise inv.InvalidDataError('Invalid algorithm to find the best decision stump. ' + str(algorithm) + ' is not defined.')
    
    def possible_decision_stumps(self, ignore_attributes, instances):
        """
        Returns a list of decision stumps, one for each attribute ignoring the ones present in the
        ignore list. Each decision stump maintains a count of instances having particular attribute
        values.
        """
        decision_stumps = self.attributes.empty_decision_stumps(ignore_attributes, self.klass);
        for stump in decision_stumps:
            for instance in instances:
                stump.update_count(instance)
        return decision_stumps

        
    def minimum_error(self, decision_stumps):
        """
        Returns the decision stump with minimum error
        """
        error, min_error_stump = 1, None
        for decision_stump in decision_stumps:
            new_error = decision_stump.error()
            if new_error < error: 
                error = new_error
                min_error_stump = decision_stump
        return min_error_stump
    
    def is_trained(self):
        return self.__best_decision_stump is not None

########NEW FILE########
__FILENAME__ = util
# Natural Language Toolkit utilities used in classifier module, should be migrated to main utilities later
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT
import UserList, math

class StatList(UserList.UserList):
    def __init__(self, values=None):
        UserList.UserList.__init__(self, values)
        
    def mean(self):
        if len(self.data) == 0: return 0
        return float(sum([each for each in self.data])) / len(self.data)
    
    def variance(self):
        _mean = self.mean()
        if len(self.data) < 2: return 0
        return float(sum([pow((each - _mean), 2) for each in self.data])) / (len(self.data) - 1)
    
    def std_dev(self):
        return math.sqrt(self.variance())

def int_array_to_string(int_array):
    return ','.join([str(each) for each in int_array])

########NEW FILE########
__FILENAME__ = zeror
# Natural Language Toolkit - ZeroR
#  Capable of classifying the test or gold data using the ZeroR algorithm
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT

from nltk_contrib.classifier import instances as ins, Classifier

class ZeroR(Classifier):
    def __init__(self, training, attributes, klass):
        Classifier.__init__(self, training, attributes, klass)
        self.__majority_class = None
        self.__klassCount = {}
        
    def train(self):
        Classifier.train(self)
        self.__majority_class = self.majority_class()
        
    def classify(self, instances):
        for instance in instances:
            instance.classified_klass = self.__majority_class
        
    def majority_class(self):
        for instance in self.training:
            self.update_count(instance)
        return self.__max()
    
    def update_count(self, instance):
        klass_value = instance.klass_value
        if klass_value in self.__klassCount:
            self.__klassCount[klass_value] += 1
        else:
            self.__klassCount[klass_value] = 1
            
    def __max(self):
        max, klass_value = 0, None
        for key in self.__klassCount.keys():
            value = self.__klassCount[key]
            if value > max:
                max = value
                klass_value = key
        return klass_value
    
    @classmethod
    def can_handle_continuous_attributes(self):
        return True
    
    def is_trained(self):
        return self.__majority_class is not None
    

########NEW FILE########
__FILENAME__ = alltests
# Natural Language Toolkit
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT

import unittest
import re, os

def allTestsSuite():
    testfilenames = []
    for dn,d,f in os.walk('.'):
        if dn is not '.': continue
        testfilenames = [filename for filename in f if re.search('tests\.py$', filename) is not None]
    modulenames = map(lambda f: re.sub('\.py$', '', f), testfilenames)         
    modules = map(__import__, modulenames)                 
    load = unittest.defaultTestLoader.loadTestsFromModule  
    return unittest.TestSuite(map(load, modules))    

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(allTestsSuite())

########NEW FILE########
__FILENAME__ = attributestests
# Natural Language Toolkit
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT

from nltk_contrib.classifier import attribute as a, discretisedattribute as da, numrange as nr
from nltk_contrib.classifier_tests import *

class AttributesTestCase(unittest.TestCase):
    def setUp(self):
        self.attrs = attributes(datasetsDir(self) + 'test_phones' + SEP + 'phoney')    
    
    def test_counts_correct_number_of_attributes(self):
        self.assertEqual(5, len(self.attrs), 'there should be 5 attributes')
        
    def test_attributes_are_valid(self):
        self.assertTrue(self.attrs.has_values(['dual', 'big', 'symbian', 'y', 'y']))

    def test_attributes_are_in_order(self):
        self.assertEqual('band', self.attrs[0].name)
        self.assertEqual('size', self.attrs[1].name)
        self.assertEqual('os', self.attrs[2].name)
        self.assertEqual('pda', self.attrs[3].name)
        self.assertEqual('mp3', self.attrs[4].name)
        
    def test_attributes_contain_an_attribute(self):
        self.assertTrue(self.attrs.__contains__(a.Attribute('band', ['dual','tri','quad'], 0)))

    def test_attributes_are_equal(self):
        attrs = a.Attributes([a.Attribute('band', ['dual','tri','quad'], 0), a.Attribute('size', ['big','small','medium'], 1)])
        same = a.Attributes([a.Attribute('band', ['dual','tri','quad'], 0), a.Attribute('size', ['big','small','medium'], 1)])
        self.assertEqual(attrs, same, 'they should be the same')
        other = a.Attributes([a.Attribute('band', ['dual','tri','quad'], 0), a.Attribute('pda', ['y','n'], 1)])
        self.assertNotEqual(self.attrs, other, 'shouldnt be the same')

    def test_index_stored_in_attributes(self):
        for i in range(len(self.attrs)):
            self.assertEqual(i, self.attrs[i].index)

    def test_has_continuous_attibutes_returns_true_if_even_1_attr_is_cont(self):
        has_cont = attributes(datasetsDir(self) + 'numerical' + SEP + 'weather')
        self.assertTrue(has_cont.has_continuous())
        
        all_disc = attributes(datasetsDir(self) + 'test_phones' + SEP + 'phoney')
        self.assertFalse(all_disc.has_continuous())
        
    def test_does_not_check_continuous_attribute_for_validity(self):
        has_cont = attributes(datasetsDir(self) + 'numerical' + SEP + 'weather')
        self.assertTrue(has_cont.has_values(['sunny','21','normal','true']))
        
    def test_return_subset_as_requested_by_index_array(self):
        attrs = attributes(datasetsDir(self) + 'numerical' + SEP + 'person')
        subset = attrs.subset([2, 4, 5])
        self.assertEqual(3, len(subset))
        self.assertEqual(2, subset[0].index)
        self.assertEqual(4, subset[1].index)
        self.assertEqual(5, subset[2].index)

    def test_discretise_replaces_cont_attrs_in_args_with_disc_ones(self):
        attrs = attributes(datasetsDir(self) + 'numerical' + SEP + 'person')
        self.assertTrue(attrs[0].is_continuous())
        self.assertTrue(attrs[4].is_continuous())
        self.assertTrue(attrs[6].is_continuous())
        self.assertTrue(attrs[7].is_continuous())
        
        attrs.discretise([da.DiscretisedAttribute('dependents', nr.Range(0, 2, True).split(2), 4), \
                          da.DiscretisedAttribute('annualincome', nr.Range(0, 120000, True).split(5), 6)])
        
        self.assertFalse(attrs[4].is_continuous())
        self.assertFalse(attrs[6].is_continuous())
        
        self.assertTrue(attrs[0].is_continuous())
        self.assertTrue(attrs[7].is_continuous())
        
        self.assertEqual(['a', 'b'], attrs[4].values)
        self.assertEqual(['a', 'b', 'c', 'd', 'e'], attrs[6].values)

    def test_empty_decision_stumps(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'person'
        attrs, klass = metadata(path)

        decision_stumps = attrs.empty_decision_stumps([], klass)
        self.assertEqual(8, len(decision_stumps))
        
        decision_stumps = attrs.empty_decision_stumps([attrs[0], attrs[3]], klass)
        self.assertEqual(6, len(decision_stumps))
        
    def test_remove_attributes(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'person'
        attrs = attributes(path)
        
        self.assertEqual(8, len(attrs))
        attr1 = attrs[1]
        attrs.remove_attributes([attrs[0], attrs[6]])
        self.assertEqual(6, len(attrs))
        self.assertEqual(attr1, attrs[0])

    def test_continuous_indices(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'person'
        attrs = attributes(path)
        self.assertEqual([0, 1, 4, 5, 6, 7], attrs.continuous_attribute_indices())
        
    def test_empty_freq_dists(self):
        attr1 = a.Attribute("first", ['a','b','c'], 0)
        attr2 = a.Attribute("second", ['d','e'], 1)
        attrs = a.Attributes([attr1,attr2])
        freq_dists = attrs.empty_freq_dists()
        self.assertEqual(2, len(freq_dists))
        self.assertEqual(3, len(freq_dists[attr1]))
        self.assertEqual(2, len(freq_dists[attr2]))
        
    def test_to_string(self):
        attr1 = a.Attribute("first", ['a','b','c'], 0)
        attr2 = a.Attribute("second", ['d','e'], 1)
        attrs = a.Attributes([attr1,attr2])
        self.assertEqual('[first:[a,b,c] index:0, second:[d,e] index:1]', str(attrs))
        
if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(unittest.TestSuite(unittest.makeSuite(AttributesTestCase)))

########NEW FILE########
__FILENAME__ = attributetests
# Natural Language Toolkit
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT

from nltk_contrib.classifier import attribute as a
from nltk_contrib.classifier_tests import *
import math

class AttributeTestCase(unittest.TestCase):
    def test_attribute_creation(self):
        attr = a.Attribute('foo', ['a','b','c'], 0)
        self.assertEqual('foo', attr.name)
        self.assertEqual(['a', 'b', 'c'], attr.values)
    
    def test_returns_true_if_value_is_present(self):
        attr = a.Attribute('foo', ['a','b','c'], 0)
        self.assertTrue(attr.has_value('c'))
        self.assertFalse(attr.has_value('d'))
        
    def test_equality(self):
        attr = a.Attribute('foo', ['a','b','c'], 0)
        same = a.Attribute('foo', ['a','b','c'], 0)
        othername = a.Attribute('foobar', ['a','b','c'], 1)
        otherval = a.Attribute('foo',['a','b','c','d'], 0)
        self.assertEqual(attr, same, 'they should be equal')
        self.assertNotEqual(attr, othername, 'they are not equal')
        self.assertNotEqual(attr, otherval, 'they are not equal')
            
    def test_is_countinuous_returns_true_if_continuous(self):
        cont_attr = a.Attribute('temperature',['continuous'], 1)
        self.assertEqual(a.CONTINUOUS, cont_attr.type)
        self.assertTrue(cont_attr.is_continuous())
        
        disc_attr = a.Attribute('foo',['a','b','c'], 0)
        self.assertEqual(a.DISCRETE, disc_attr.type)
        self.assertFalse(disc_attr.is_continuous())
                
    def test_empty_freq_dists(self):
        attr = a.Attribute('foo', ['a','b','c'], 0)
        freq_dists = attr.empty_freq_dists()
        self.assertEqual(3, len(freq_dists))
        for each in attr.values:
            self.assertEqual(0, freq_dists[each].N())
        
    def test_values_as_str(self):
        attr = a.Attribute('foo', ['a','b','c'], 0)
        self.assertEqual('a,b,c', attr.values_as_str())
        
    def test_to_string(self):
        attr = a.Attribute('foo', ['a','b','c'], 0)
        self.assertEqual('foo:[a,b,c] index:0', str(attr))
        

########NEW FILE########
__FILENAME__ = autoclasstests
# Natural Language Toolkit - Attribute
#  can extract the name and values from a line and operate on them
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT

from nltk_contrib.classifier import autoclass
from nltk_contrib.classifier_tests import *

class AutoClassTestCase(unittest.TestCase):
    def test_base26(self):
        self.assertEqual(0, autoclass.FIRST.base26())
        self.assertEqual(25, autoclass.AutoClass('z').base26())
        self.assertEqual(26, autoclass.AutoClass('ba').base26())
        self.assertEqual(0, autoclass.AutoClass('aaa').base26())
        self.assertEqual(26 * 3, autoclass.AutoClass('baaa').base26())
        
    def test_string(self):
        self.assertEqual('a', autoclass.string(0))
        self.assertEqual('z', autoclass.string(25))
        self.assertEqual('ba', autoclass.string(26))
        self.assertEqual('bb', autoclass.string(27))
    
    def test_next(self):
        a = autoclass.FIRST
        b = a.next()
        self.assertEqual('b', str(b))    
        self.assertEqual('c', str(b.next()))    
        self.assertEqual('z', self.next('y'))    
        self.assertEqual('ba', self.next('z'))    
        self.assertEqual('bb', self.next('ba'))
        self.assertEqual('bc', self.next('bb'))    
        self.assertEqual('ca', self.next('bz'))
        self.assertEqual('baa', self.next('zz'))
        
    def next(self, current):
        return str(autoclass.AutoClass(current).next())

########NEW FILE########
__FILENAME__ = cfiletests
# Natural Language Toolkit
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT

from nltk_contrib.classifier import cfile, item, format
from nltk_contrib.classifier_tests import *

class FileTestCase(unittest.TestCase):
    def test_file_read_operation(self):
        self.contents = ""
        f = cfile.File(datasetsDir(self) + 'test_phones' + SEP + 'phoney', format.c45.NAMES)
        f.for_each_line(self.printline)
        
        verificationContents = ""
        check = open(datasetsDir(self) + 'test_phones' + SEP + 'phoney.names', 'r')
        for l in check:
            verificationContents += l
        self.assertEqual(verificationContents, self.contents)
        
    def test_name_extension(self):
        basename, extension = cfile.name_extension('/home/something.something/else/test_phones' + SEP + 'phoney.' + format.c45.NAMES)
        self.assertEqual('/home/something.something/else/test_phones/phoney', basename)
        self.assertEqual('names', extension)
        
    def test_filter_comments(self):
        f = cfile.File(datasetsDir(self) + 'test_phones' + SEP + 'phoney', format.c45.NAMES)
            
    def printline(self, l):
        self.contents += l + '\n' # the \n is to simulate a new line

########NEW FILE########
__FILENAME__ = classifytests
# Natural Language Toolkit
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT
from nltk_contrib.classifier import classify as c
from nltk_contrib.classifier_tests import *

class ClassifyTestCase(unittest.TestCase):
    def setUp(self):
        self.classify = c.Classify()
        
    def test_reads_classifier_name(self):
        self.classify.parse(['-a', '0R']) #Zero R classifier
        self.assertEqual('0R', self.classify.values.ensure_value('algorithm', None))
        
        self.classify.parse(['-a', '1R']) #One R classifier
        self.assertEqual('1R', self.classify.values.ensure_value('algorithm', None))
        
        self.classify.parse(['-a', 'DT']) #Decision Tree classifier
        self.assertEqual('DT', self.classify.values.ensure_value('algorithm', None))

    def test_accuracy_and_fscore_are_true_by_default(self):
        self.classify.parse(None)
        self.assertEqual(True, self.classify.values.ensure_value('accuracy', None))
        self.assertEqual(True, self.classify.values.ensure_value('fscore', None))
        
        self.classify.parse(["-A"])
        self.assertEqual(False, self.classify.values.ensure_value('accuracy', None))
        
        self.classify.parse(["-AF"])
        self.assertEqual(False, self.classify.values.ensure_value('accuracy', None))
        self.assertEqual(False, self.classify.values.ensure_value('fscore', None))

    def test_classifyDoesNotThrowErrorIfRequiredComponentsArePresent(self):
        path = datasetsDir(self) + 'minigolf' + SEP + 'weather'
        dns = DoNothingStrategy()
        
        classify = StubClassify(dns)
        self.assertFalse(dns.called)
        classify.run(['-a', '1R', '-t', path, '-T', path])
        self.assertTrue(dns.called)
        self.assertFalse(classify.errorCalled)
        
        dns = DoNothingStrategy()
        classify = StubClassify(dns)
        self.assertFalse(dns.called)
        classify.run(['-a', '1R', '-t', path, '-g', path])
        self.assertTrue(dns.called)
        self.assertFalse(classify.errorCalled)
        
    def test_classify_throws_error_if_neither_test_nor_gold_is_present(self):
        path = datasetsDir(self) + 'minigolf' + SEP + 'weather'
        dns = DoNothingStrategy()
        classify = StubClassify(dns)
        self.assertFalse(dns.called)
        classify.parse(['-a', '1R', '-t', path])
        try:
            classify.execute()
        except TypeError:
            pass
        self.assertTrue(classify.errorCalled)
        self.assertTrue(dns.called)#in reality it will never be called as it exits in the error method
        self.assertEqual('Invalid arguments. One or more required arguments are not present.', classify.message)
        
    def test_throws_error_if_both_files_and_other_options_are_present(self):
        path = datasetsDir(self) + 'minigolf' + SEP + 'weather'
        dns = DoNothingStrategy()
        classify = StubClassify(dns)
        self.assertFalse(dns.called)
        classify.parse(['-a', '1R', '-f', path, '-t', path])
        classify.execute()
        self.assertTrue(classify.errorCalled)
        self.assertEqual('Invalid arguments. The files argument cannot exist with training, test or gold arguments.', classify.message)

    def test_throws_error_if_both_test_and_gold_files_are_present(self):
        path = datasetsDir(self) + 'minigolf' + SEP + 'weather'
        dns = DoNothingStrategy()
        classify = StubClassify(dns)
        self.assertFalse(dns.called)
        classify.parse(['-a', '1R', '-t', path, '-T', path, '-g', path])
        classify.execute()
        self.assertTrue(classify.errorCalled)
        self.assertEqual('Invalid arguments. Test and gold files are mutually exclusive.', classify.message)

    def test_throws_error_if_verify_options_are_present_for_a_test_file(self):
        path = datasetsDir(self) + 'minigolf' + SEP + 'weather'
        dns = DoNothingStrategy()
        classify = StubClassify(dns)
        self.assertFalse(dns.called)
        classify.parse(['-a', '1R', '-v', '-t', path, '-T', path])
        classify.execute()
        self.assertTrue(classify.errorCalled)
        self.assertEqual('Invalid arguments. Cannot verify classification for test data.', classify.message)
        
    def test_does_not_throw_error_if_cross_validation_option_is_present_and_only_training_exists(self):
        path = datasetsDir(self) + 'minigolf' + SEP + 'weather'
        dns = DoNothingStrategy()
        classify = StubClassify(dns)
        self.assertFalse(dns.called)
        classify.parse(['-a', '1R', '-t', path, '-c', 5])
        classify.execute()
        self.assertFalse(classify.errorCalled)
        self.assertTrue(dns.called)

        dns = DoNothingStrategy()
        classify = StubClassify(dns)
        self.assertFalse(dns.called)
        classify.parse(['-a', '1R', '-f', path, '-c', 5])
        classify.execute()
        self.assertFalse(classify.errorCalled)
        self.assertTrue(dns.called)
        
    def test_does_not_throw_error_if_only_file_option_present(self):
        path = datasetsDir(self) + 'minigolf' + SEP + 'weather'
        dns = DoNothingStrategy()
        classify = StubClassify(dns)
        self.assertFalse(dns.called)
        classify.parse(['-a', '1R', '-f', path])
        classify.execute()
        self.assertFalse(classify.errorCalled)
        self.assertTrue(dns.called)
        
        
    def test_get_file_strategy(self):
        strategy = c.get_file_strategy('files', None, None, None, True)
        self.assertEqual(c.CommonBaseNameStrategy, strategy.__class__)
        values = strategy.values()
        self.assertEqual(values[0], 'files')
        self.assertEqual(values[1], None)
        self.assertEqual(values[2], 'files')
        
        strategy = c.get_file_strategy('files', None, None, None, False)
        self.assertEqual(c.CommonBaseNameStrategy, strategy.__class__)
        values = strategy.values()
        self.assertEqual(values[0], 'files')
        self.assertEqual(values[1], 'files')
        self.assertEqual(values[2], None)
        
        strategy = c.get_file_strategy(None, 'train', 'test', None, False)
        self.assertEqual(c.ExplicitNamesStrategy, strategy.__class__)
        values = strategy.values()
        self.assertEqual(values[0], 'train')
        self.assertEqual(values[1], 'test')
        self.assertEqual(values[2], None)

        strategy = c.get_file_strategy(None, 'train', None, 'gold', False)
        self.assertEqual(c.ExplicitNamesStrategy, strategy.__class__)
        values = strategy.values()
        self.assertEqual(values[0], 'train')
        self.assertEqual(values[1], None)
        self.assertEqual(values[2], 'gold')

    def test_valid_decision_tree_options(self):
        dto = c.DecisionTreeOptions('IG')
        dummy_classifier = ClassifierStub()
        self.assertTrue(dummy_classifier.options is None)
        dto.set_options(dummy_classifier)
        self.assertEqual('maximum_information_gain', dummy_classifier.options)

        dto = c.DecisionTreeOptions('GR')
        dummy_classifier = ClassifierStub()
        self.assertTrue(dummy_classifier.options is None)
        dto.set_options(dummy_classifier)
        self.assertEqual('maximum_gain_ratio', dummy_classifier.options)
        
    def test_invalid_decision_tree_option_results_in_no_setting(self):
        dto = c.DecisionTreeOptions('foo')
        dummy_classifier = ClassifierStub()
        self.assertTrue(dummy_classifier.options is None)
        dto.set_options(dummy_classifier)
        self.assertTrue(dummy_classifier.options is None)
        

        
class ClassifierStub:
    def __init__(self):
        self.options = None
    
    def set_options(self, options):
        self.options = options
        
class StubClassify(c.Classify):
    def __init__(self, strategy):
        c.Classify.__init__(self)
        self.errorCalled = False
        self.strategy = strategy
        
    def error(self, message):
        #in reality error will display usage and quit
        self.message = message
        self.errorCalled = True
        
    def get_classification_strategy(self, classifier, test, gold, training, cross_validation_fold, attributes, klass):
        return self.strategy
                    
class DoNothingStrategy:
    def __init__(self):
        self.called = False
        
    def classify(self):
        #do nothing
        self.called = True
    
    def print_results(self, log, accuracy, error, fscore, precision, recall):
        #do nothing
        pass
    
    def write(self, log, should_write, data_format, suffix):
        #do nothing
        pass
    
    def train(self):
        #do Nothing
        pass
        

########NEW FILE########
__FILENAME__ = commandlinetests
# Natural Language Toolkit CommandLine
#     understands the command line interaction
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT
from nltk_contrib.classifier_tests import *
from nltk_contrib.classifier import commandline as cl

class CommandLineTestCase(unittest.TestCase):
    def test_converts_an_array_into_integer_array(self):
        returned = cl.as_integers('Foo', None)
        self.assertEqual([], returned)
        
        returned = cl.as_integers('Foo', '3,5, 7, 9')
        self.assertEqual([3, 5, 7, 9], returned)
        

########NEW FILE########
__FILENAME__ = confusionmatrixtests
# Natural Language Toolkit
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT

from nltk_contrib.classifier import confusionmatrix as cm
from nltk_contrib.classifier.exceptions import systemerror as se
from nltk_contrib.classifier_tests import *

class ConfusionMatrixTestCase(unittest.TestCase):
    def setUp(self):
        self.c = cm.ConfusionMatrix(['yes', 'no'])
        self.pos = 'yes'
        self.neg = 'no'
        
    def test_initial_confusion_matrix_has_all_zero_counts(self):
        self.__assert_matrix(0, 0, 0, 0)
            
    def test_confusion_matrix_updates_on_each_count(self):
        self.__assert_matrix(0, 0, 0, 0)
        self.c.count(self.pos, self.pos)
        self.__assert_matrix(1, 0, 0, 0)
        self.c.count(self.pos, self.pos)
        self.__assert_matrix(2, 0, 0, 0)
        self.c.count(self.pos, self.neg)
        self.__assert_matrix(2, 1, 0, 0)
        self.c.count(self.neg, self.pos)
        self.__assert_matrix(2, 1, 1, 0)
        self.c.count(self.neg, self.neg)
        self.__assert_matrix(2, 1, 1, 1)
        
    def test_calculation_of_accuracy_and_error(self):
        self.c.count(self.pos, self.pos)
        self.c.count(self.pos, self.neg)
        self.c.count(self.neg, self.pos)
        self.c.count(self.neg, self.pos)
        self.assertEqual(0.25, self.c.accuracy())
        self.assertEqual(0.75, self.c.error())
        
    def test_true_positive_rate(self):
        self.c.count(self.pos, self.pos)
        self.c.count(self.pos, self.neg)
        self.c.count(self.neg, self.pos)
        self.assertEqual(0.5, self.c.tpr())
        self.assertEqual(0.5, self.c.sensitivity())
        
    def test_true_negative_rate(self):
        self.c.count(self.pos, self.neg)
        self.c.count(self.neg, self.pos)
        self.c.count(self.neg, self.neg)
        self.c.count(self.neg, self.neg)
        self.assertAlmostEqual(0.66666667, self.c.tnr(), 8)
        self.assertAlmostEqual(0.66666667, self.c.specificity(), 8)
        
    def test_false_positive_rate(self):
        self.c.count(self.neg, self.pos)
        self.c.count(self.neg, self.neg)
        self.c.count(self.neg, self.neg)
        self.assertAlmostEqual(0.33333333, self.c.fpr(), 8)
        
    def test_precision(self):
        self.c.count(self.pos, self.pos)
        self.c.count(self.pos, self.neg)
        self.c.count(self.neg, self.pos)
        self.c.count(self.neg, self.pos)
        self.c.count(self.neg, self.pos)
        self.assertEqual(0.25, self.c.precision())
        
    def test_recall(self):
        self.c.count(self.pos, self.pos)
        self.c.count(self.pos, self.pos)
        self.c.count(self.pos, self.neg)
        self.c.count(self.neg, self.pos)
        self.c.count(self.neg, self.neg)
        self.assertAlmostEqual(0.66666667, self.c.recall(), 8)
        
    def test_fscore(self):
        self.c.count(self.pos, self.pos)
        self.c.count(self.pos, self.neg)
        self.c.count(self.neg, self.pos)
        self.c.count(self.neg, self.pos)
        self.c.count(self.neg, self.pos)
        self.assertEqual(0.25, self.c.precision())
        self.assertAlmostEqual(0.5, self.c.recall(), 8)
        self.assertAlmostEqual(0.33333333, self.c.fscore(), 8)
    
    def test_no_divide_by_zero_error_when_numerators_are_zero(self):
        self.c.count(self.pos, self.neg)
        self.c.count(self.pos, self.neg)
        self.c.count(self.neg, self.neg)
        self.c.count(self.neg, self.neg)
        self.assertEqual(0, self.c.precision())
        self.assertEqual(0, self.c.recall())
        self.assertEqual(0, self.c.fscore())
        self.assertEqual(0.5, self.c.accuracy())
        
    def test_more_klass_values(self):
        c = cm.ConfusionMatrix(['1', '2', '3', '4', '5', 'U'])
        c.count('1', '2')
        c.count('2', '2')
        c.count('2', '3')
        c.count('3', '3')
        c.count('4', '2')
        c.count('5', '5')
        c.count('2', '2')
        c.count('3', '2')
        #matrix values for class '1'
        self.assertEqual(0, c.tp(0))
        self.assertEqual(7, c.tn(0))
        self.assertEqual(1, c.fn(0))
        self.assertEqual(0, c.fp(0))
        #matrix values for class 'U'
        self.assertEqual(0, c.tp(5))
        self.assertEqual(8, c.tn(5))
        self.assertEqual(0, c.fn(5))
        self.assertEqual(0, c.fp(5))
        
        
        self.assertEqual(0.875, c.accuracy())#accuracy for class '1'(default accuracy)
        self.assertEqual(1, c.accuracy(index=5))#accuracy for 'U'
        self.assertEqual(0, c.fscore())#f-score for '1'(default f-score)
        self.assertEqual(0, c.fscore(index=5))#f-score for 'U'
        self.assertEqual(0.5, c.fscore(index=1))#f-score for '2'
        
        
    def __assert_matrix(self, tp, fn, fp, tn):
        self.assertEqual(tp, self.c.tp())
        self.assertEqual(fn, self.c.fn())
        self.assertEqual(fp, self.c.fp())
        self.assertEqual(tn, self.c.tn())

########NEW FILE########
__FILENAME__ = decisionstumptests
# Natural Language Toolkit
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT

from nltk_contrib.classifier import decisionstump as ds, instances as ins, instance
from nltk_contrib.classifier_tests import *
import math

class DecisionStumpTestCase(unittest.TestCase):
    def setUp(self):
        self.attributes, self.klass = metadata(datasetsDir(self) + 'minigolf' + SEP + 'weather')
        self.outlook_attr = self.attributes[0]
        self.outlook_stump = ds.DecisionStump(self.outlook_attr, self.klass)
        self.instances = training(datasetsDir(self) + 'minigolf' + SEP + 'weather')
    
    def test_creates_count_map(self): 
        self.assertEqual(3, len(self.outlook_stump.counts))
        for attr_value in self.outlook_attr.values:
            for class_value in self.klass:
                self.assertEqual(0, self.outlook_stump.counts[attr_value][class_value])
    
    def test_updates_count_with_instance_values(self):
        self.outlook_stump.update_count(self.instances[0])
        for attr_value in self.outlook_attr.values:
            for class_value in self.klass:
                if attr_value == 'sunny' and class_value == 'no': continue
                self.assertEqual(0, self.outlook_stump.counts[attr_value][class_value])
        self.assertEqual(1, self.outlook_stump.counts['sunny']['no'])

    def test_error_count(self):
        self.__update_stump()
        self.assertAlmostEqual(0.2222222, self.outlook_stump.error())
        self.assertEqual('outlook', self.outlook_stump.attribute.name)
        
    def __update_stump(self):
        for instance in self.instances:
            self.outlook_stump.update_count(instance)
        
    def test_majority_class_for_attr_value(self):
        self.__update_stump()
        self.assertEqual('no', self.outlook_stump.majority_klass('sunny'))
        self.assertEqual('yes', self.outlook_stump.majority_klass('overcast'))
        self.assertEqual('yes', self.outlook_stump.majority_klass('rainy'))
        
    def test_classifies_instance_correctly(self):
        self.__update_stump()
        self.assertEqual('no', self.outlook_stump.klass(instance.GoldInstance(['sunny','mild','normal','true'],'yes')))
        self.assertEqual('yes', self.outlook_stump.klass(instance.GoldInstance(['overcast','mild','normal','true'],'yes')))
        self.assertEqual('yes', self.outlook_stump.klass(instance.GoldInstance(['rainy','mild','normal','true'],'yes')))
        self.assertEqual('no', self.outlook_stump.klass(instance.TestInstance(['sunny','mild','normal','true'])))
        self.assertEqual('yes', self.outlook_stump.klass(instance.TestInstance(['overcast','mild','normal','true'])))
        self.assertEqual('yes', self.outlook_stump.klass(instance.TestInstance(['rainy','mild','normal','true'])))
        
    def test_total_counts(self):
        dictionary_of_klass_counts = {}
        dictionary_of_klass_counts['yes'] = 2
        dictionary_of_klass_counts['no'] = 0
        self.assertEqual(2, ds.total_counts(dictionary_of_klass_counts))

        dictionary_of_klass_counts['yes'] = 9
        dictionary_of_klass_counts['no'] = 5
        self.assertEqual(14, ds.total_counts(dictionary_of_klass_counts))
        
    # root - yes 5
    #  |     no  4
    #  |
    #  |------sunny----- yes 1
    #  |                 no  3
    #  | 
    #  |------rainy------yes 2
    #  |                 no  1
    #  |
    #  |------overcast---yes 2
    #                    no  0
    #
    # mean info = 4.0/9 * (-(1.0/4 * log(1.0/4, 2)) + -(3.0/4 * log(3.0/4, 2))) + 3.0/9 * (-(2.0/3 * log(2.0/3, 2))  + -(1.0/3 * log(1.0/3, 2))) 
    def test_mean_information(self):
        self.__update_stump()
        expected = 4.0/9 * (-(1.0/4 * math.log(1.0/4, 2)) + -(3.0/4 * math.log(3.0/4, 2))) + 3.0/9 * (-(2.0/3 * math.log(2.0/3, 2))  + -(1.0/3 * math.log(1.0/3, 2))) 
        self.assertAlmostEqual(expected, self.outlook_stump.mean_information(), 6)

    # info_gain = entropy(root) - mean_information()
    # entropy(root) = -(5.0/9 * log(5.0/9, 2))  + -(4.0/9 * log(4.0/9, 2)) = 0.99107605983822222
    # mean_info = 0.666666666
    def test_information_gain(self):
        self.__update_stump()
        entropy = -(5.0/9 * math.log(5.0/9, 2))  + -(4.0/9 * math.log(4.0/9, 2))
        mean_info = 4.0/9 * (-(1.0/4 * math.log(1.0/4, 2)) + -(3.0/4 * math.log(3.0/4, 2))) + 3.0/9 * (-(2.0/3 * math.log(2.0/3, 2))  + -(1.0/3 * math.log(1.0/3, 2))) 
        expected = entropy - mean_info
        self.assertAlmostEqual(expected, self.outlook_stump.information_gain(), 6)
        
    def test_returns_entropy_for_each_attribute_value(self):
        self.__update_stump()

        # there are 4 training instances in all out of which 
        # 3 training instances have their class assigned as no and
        # 1 training instance has its class assigned as yes
        expected = -(1.0/4 * math.log(1.0/4, 2)) + -(3.0/4 * math.log(3.0/4, 2))
        self.assertAlmostEqual(expected, self.outlook_stump.entropy('sunny'), 6)
        
        expected = -(2.0/2 * math.log(2.0/2, 2)) + 0
        self.assertAlmostEqual(0, self.outlook_stump.entropy('overcast'))
        
        expected = -(2.0/3 * math.log(2.0/3, 2)) + -(1.0/3 * math.log(1.0/3, 2))
        self.assertAlmostEqual(expected, self.outlook_stump.entropy('rainy'))
        
    def test_dictionary_of_all_values_with_count_0(self):
        phoney = klass(datasetsDir(self) + 'test_phones' + SEP + 'phoney')
        values = ds.dictionary_of_values(phoney);
        self.assertEqual(3, len(values))
        for i in ['a', 'b', 'c']:
            self.assertTrue(values.has_key(i))
            self.assertEqual(0, values[i])
            
    def test_gain_ratio(self):
        self.__update_stump()
        
        entropy = -(5.0/9 * math.log(5.0/9, 2))  + -(4.0/9 * math.log(4.0/9, 2))
        mean_info = 4.0/9 * (-(1.0/4 * math.log(1.0/4, 2)) + -(3.0/4 * math.log(3.0/4, 2))) + 3.0/9 * (-(2.0/3 * math.log(2.0/3, 2))  + -(1.0/3 * math.log(1.0/3, 2))) 
        info_gain = entropy - mean_info
        split_info = -(5.0/12 * math.log(5.0/12, 2)) + -(3.0/12 * math.log(3.0/12, 2))+ -(4.0/12 * math.log(4.0/12, 2))#with smoothing, actual occurances are 4,2,3
        expected = float(info_gain) / split_info
        
        self.assertAlmostEqual(expected, self.outlook_stump.gain_ratio(), 6)
        
    def test_sorting_of_decision_stumps(self):
        stumps = []
        for attribute in self.attributes:
            stumps.append(ds.DecisionStump(attribute, self.klass))
        for instance in self.instances:
            for stump in stumps:
                stump.update_count(instance)
        
        self.assertAlmostEqual(0.324409, stumps[0].information_gain(), 6)
        self.assertAlmostEqual(0.102187, stumps[1].information_gain(), 6)
        self.assertAlmostEqual(0.091091, stumps[2].information_gain(), 6)
        self.assertAlmostEqual(0.072780, stumps[3].information_gain(), 6)

        stumps.sort(lambda x, y: cmp(getattr(x, 'information_gain'), getattr(y, 'information_gain')))

        self.assertAlmostEqual(0.324409, stumps[0].information_gain(), 6)
        self.assertAlmostEqual(0.102187, stumps[1].information_gain(), 6)
        self.assertAlmostEqual(0.091091, stumps[2].information_gain(), 6)
        self.assertAlmostEqual(0.072780, stumps[3].information_gain(), 6)
        
    def test_split_info_with_equal_distribution(self):
        self.outlook_stump.update_count(instance.TrainingInstance(['sunny','mild','normal','true'],'yes'))
        self.outlook_stump.update_count(instance.TrainingInstance(['overcast','mild','normal','true'],'no'))
        self.outlook_stump.update_count(instance.TrainingInstance(['sunny','hot','normal','true'],'yes'))
        self.outlook_stump.update_count(instance.TrainingInstance(['overcast','hot','normal','true'],'yes'))
        self.outlook_stump.update_count(instance.TrainingInstance(['rainy','mild','normal','true'],'yes'))
        self.outlook_stump.update_count(instance.TrainingInstance(['rainy','mild','normal','false'],'yes'))
        expected = -(3.0/9 * math.log(3.0/9, 2)) * 3 #3.0/9 and not 2.0/6 because of smoothing
        self.assertEqual(expected, self.outlook_stump.split_info())
        
    def test_split_info_greater_for_higher_arity_attributes(self):
        self.outlook_stump.update_count(instance.TrainingInstance(['sunny','mild','normal','true'],'yes'))
        self.outlook_stump.update_count(instance.TrainingInstance(['overcast','mild','normal','true'],'no'))
        self.outlook_stump.update_count(instance.TrainingInstance(['sunny','hot','normal','false'],'yes'))
        self.outlook_stump.update_count(instance.TrainingInstance(['overcast','hot','normal','false'],'yes'))
        self.outlook_stump.update_count(instance.TrainingInstance(['rainy','mild','normal','true'],'yes'))
        self.outlook_stump.update_count(instance.TrainingInstance(['rainy','mild','normal','false'],'yes'))
        
        windy_stump = ds.DecisionStump(self.attributes[3], self.klass)
        windy_stump.update_count(instance.TrainingInstance(['sunny','mild','normal','true'],'yes'))
        windy_stump.update_count(instance.TrainingInstance(['overcast','mild','normal','true'],'no'))
        windy_stump.update_count(instance.TrainingInstance(['sunny','hot','normal','false'],'yes'))
        windy_stump.update_count(instance.TrainingInstance(['overcast','hot','normal','false'],'yes'))
        windy_stump.update_count(instance.TrainingInstance(['rainy','mild','normal','true'],'yes'))
        windy_stump.update_count(instance.TrainingInstance(['rainy','mild','normal','false'],'yes'))

        self.assertTrue(self.outlook_stump.split_info() > windy_stump.split_info())

########NEW FILE########
__FILENAME__ = decisiontreetests
# Natural Language Toolkit
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT

from nltk_contrib.classifier_tests import *
from nltk_contrib.classifier import decisiontree, decisionstump as ds, instances as ins, attribute as attr
from nltk_contrib.classifier.exceptions import invaliddataerror as inv

class DecisionTreeTestCase(unittest.TestCase):
    def test_tree_creation(self):
        path = datasetsDir(self) + 'test_phones' + SEP + 'phoney'
        a, c = metadata(path)
        tree = decisiontree.DecisionTree(training(path), a, c)
        tree.train()
        self.assertNotEqual(None, tree)
        self.assertNotEqual(None, tree.root)
        self.assertEqual('band', tree.root.attribute.name)
        self.assertEqual(1, len(tree.root.children))
        self.assertEqual('size', tree.root.children['tri'].attribute.name)
        
    def test_filter_does_not_affect_the_original_training(self):
        path = datasetsDir(self) + 'minigolf' + SEP + 'weather'
        a, c = metadata(path)
        tree = decisiontree.DecisionTree(training(path), a, c)
        tree.train()
        outlook = tree.attributes[0]
        self.assertEqual(9, len(tree.training))
        filtered = tree.training.filter(outlook, 'sunny')
        self.assertEqual(9, len(tree.training))
        self.assertEqual(4, len(filtered))
        
    def test_maximum_information_gain_stump_is_selected(self):
        path = datasetsDir(self) + 'test_phones' + SEP + 'phoney'
        _training = training(path)
        a, c = metadata(path)
        tree = decisiontree.DecisionTree(_training, a, c)
        decision_stumps = tree.possible_decision_stumps([], _training)

        max_ig_stump = tree.maximum_information_gain(decision_stumps)
        self.assertEqual('band', max_ig_stump.attribute.name)
        
    def test_maximum_gain_raito_stump_is_selected(self):
        path = datasetsDir(self) + 'test_phones' + SEP + 'phoney'
        _training = training(path)
        a, c = metadata(path)
        tree = decisiontree.DecisionTree(_training, a, c)
        decision_stumps = tree.possible_decision_stumps([], _training)
        
        max_gr_stump = tree.maximum_gain_ratio(decision_stumps)
        self.assertEqual('pda', max_gr_stump.attribute.name)
        

        #                     outlook
        #               sunny  / | \ rainy
        #                     /  |  \
        #           temperature       windy
        #             
    def test_ignores_selected_attributes_in_next_recursive_iteration(self):
        path = datasetsDir(self) + 'minigolf' + SEP + 'weather'
        a, c = metadata(path)
        tree = decisiontree.DecisionTree(training(path), a, c)
        tree.train()
        self.assertEqual('outlook', tree.root.attribute.name)
        children = tree.root.children
        self.assertEqual(2, len(children))
        
        sunny = children['sunny']
        self.assertEqual('temperature', sunny.attribute.name)
        self.assertEqual(0, len(sunny.children))
        
        rainy = children['rainy']
        self.assertEqual('windy', rainy.attribute.name)
        self.assertEqual(0, len(rainy.children))

    def test_throws_error_if_conitinuous_atributes_are_present(self):
        try:
            path = datasetsDir(self) + 'numerical' + SEP + 'weather'
            a,c = metadata(path)
            dt = decisiontree.DecisionTree(training(path), a, c)
            dt.train()
            self.fail('should have thrown an error')
        except inv.InvalidDataError:
            pass
            
        

########NEW FILE########
__FILENAME__ = discretisedattributetests
# Natural Language Toolkit Discretized attribute tests
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT
from nltk_contrib.classifier import discretisedattribute as da, numrange as nr, attribute as attr
from nltk_contrib.classifier.exceptions import invaliddataerror as inv
from nltk_contrib.classifier_tests import *

class DiscretisedAttributeTestCase(unittest.TestCase):
    def test_binary_search(self):
        ranges = [nr.Range(2, 4), nr.Range(4, 6), nr.Range(6, 8), nr.Range(8, 10, True)]
        self.assertEqual(0, da.binary_search(ranges, 2))
        self.assertEqual(1, da.binary_search(ranges, 4))
        self.assertEqual(3, da.binary_search(ranges, 10))
        self.assertEqual(-1, da.binary_search(ranges, 1))
        self.assertEqual(-1, da.binary_search(ranges, 11))
        
        ranges = [nr.Range(2, 4), nr.Range(4, 6), nr.Range(6, 8, True)]
        self.assertEqual(-1, da.binary_search(ranges, 9))
        self.assertEqual(2, da.binary_search(ranges, 8))
        
        ranges = nr.Range(6, 32, True).split(3)
        self.assertEqual(0, da.binary_search(ranges, 12))
        
        ranges = nr.Range(0, 2, True).split(2)
        self.assertEqual(0, da.binary_search(ranges, 0))
        
    def test_creates_class_values_for_ranges(self):
        ranges = nr.Range(-10, 40, True).split(5)
        disc_attr = da.DiscretisedAttribute('temperature', ranges, 1)
        self.assertEqual('temperature', disc_attr.name)
        self.assertEqual(['a', 'b', 'c', 'd', 'e'], disc_attr.values)
        self.assertEqual(ranges, disc_attr.ranges)
        self.assertEqual(1, disc_attr.index)
        self.assertEqual(attr.DISCRETE, disc_attr.type)
        
    def test_maps_continuous_value_to_correct_discretised_equivalent(self):
        ranges = nr.Range(-10, 40, True).split(5)
        disc_attr = da.DiscretisedAttribute('temperature', ranges, 1)
        self.assertEqual('a', disc_attr.mapping(-10))
        self.assertEqual('b', disc_attr.mapping(0))
        self.assertEqual('b', disc_attr.mapping(1))
        self.assertEqual('c', disc_attr.mapping(10))
        self.assertEqual('e', disc_attr.mapping(40))
        
    def test_finding_mapping_for_value_out_of_range_returns_nearest_match(self):
        ranges = nr.Range(-10, 40, True).split(5)
        disc_attr = da.DiscretisedAttribute('temperature', ranges, 1)
        self.assertEqual('e', disc_attr.mapping(50))
        self.assertEqual('a', disc_attr.mapping(-20))
        


########NEW FILE########
__FILENAME__ = discretisetests
# Natural Language Toolkit - Discretise tests
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT
from nltk_contrib.classifier_tests import *
from nltk_contrib.classifier import discretise
from nltk_contrib.classifier import numrange as nr, instances as ins
from nltk_contrib.classifier.exceptions import invaliddataerror as inv


class DiscretiseTestCase(unittest.TestCase):    
    def test_decodes_algorithm_training_other_files_attributes_options(self):
        disc = discretise.Discretise()
        disc.parse(['-a', 'UEW', '-t', 'path', '-T', 'path1,path2', '-A', '3,4,5', '-o', '3,2,4'])
        _algorithm = disc.values.ensure_value('algorithm', None)
        _training = disc.values.ensure_value('training', None)
        _test = disc.values.ensure_value('test', None)
        attributes = disc.values.ensure_value('attributes', None)
        options = disc.values.ensure_value('options', None)
        
        self.assertEqual('UEW', _algorithm)
        self.assertEqual('path', _training)
        self.assertEqual('path1,path2', _test)
        self.assertEqual('3,4,5', attributes)
        self.assertEqual('3,2,4', options)
        
    def test_throws_error_when_any_of_the_attributes_are_missing(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'person'
        disc = DiscretiseStub()
        self.assertFalse(disc.error_called)
        disc.parse(['-a', 'UEW', '-t', path, '-T', path + '.test,' + path + 'extra.test', '-A', '3,4,5'])
        disc.execute()
        self.assertTrue(disc.error_called)
        self.assertEqual('Invalid arguments. One or more required arguments are not present.', disc.message)

    def test_options_are_optional_for_naive_supervised_algorithm(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'person'
        disc = DiscretiseStub()
        self.assertFalse(disc.error_called)
        
        disc.parse(['-a', 'NS', '-t', path, '-T', path + '.test,' + path + 'extra.test', '-A', '3,4,5'])
        disc.execute()
        
        self.assertFalse(disc.error_called)

    def test_instances_attributes_and_options_are_extracted_from_strings(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'person'
        _training, attributes, klass, _test, _gold = self.get_instances(path, True, False)
        disc = discretise.Discretiser(_training, attributes, klass, _test, _gold, [0,1,4,5,6,7], [2,3,2,3,4,2])
        self.assertEqual(6, len(disc.training))
        self.assertEqual(2, len(disc.test))
        self.assertEqual([0, 1, 4, 5, 6, 7], disc.attribute_indices)
        self.assertEqual([2, 3, 2, 3, 4, 2], disc.options)
        
    def test_unsupervised_equal_width_discretisation(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'person'
        _training, attributes, klass, _test, _gold = self.get_instances(path, True, False)
        disc = discretise.Discretiser(_training, attributes, klass, _test, _gold, [1,4,5,6,7], [3,2,3,4,2])
        self.assertTrue(disc.attributes[0].is_continuous())
        self.assertTrue(disc.attributes[1].is_continuous())
        self.assertTrue(disc.attributes[4].is_continuous())
        self.assertTrue(disc.attributes[5].is_continuous())
        self.assertTrue(disc.attributes[6].is_continuous())
        self.assertTrue(disc.attributes[7].is_continuous())
        self.assertEqual(25, disc.training[0].value(disc.attributes[1]))
        self.assertEqual(26, disc.test[0].value(disc.attributes[1]))
        disc.unsupervised_equal_width()
        self.assertTrue(disc.attributes[0].is_continuous())
        self.assertFalse(disc.attributes[1].is_continuous())
        self.assertFalse(disc.attributes[4].is_continuous())
        self.assertFalse(disc.attributes[5].is_continuous())
        self.assertFalse(disc.attributes[6].is_continuous())
        self.assertFalse(disc.attributes[7].is_continuous())
        self.assertEqual('a', disc.training[0].value(disc.attributes[1]))
        self.assertEqual('a', disc.test[0].value(disc.attributes[1]))
        
    def test_returns_array_of_discretised_attributes(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'person'
        _training, attributes, klass, _test, _gold = self.get_instances(path, True, False)
        disc = discretise.Discretiser(_training, attributes, klass, _test, _gold, [4,6], [2,4])
        disc_attrs = disc.discretised_attributes([nr.Range(0, 2), nr.Range(0, 120000)])
        self.assertEqual(2, len(disc_attrs))
        self.assertEqual(4, disc_attrs[0].index)
        self.assertEqual(2, len(disc_attrs[0].values))
        self.assertEqual(4, len(disc_attrs[1].values))
        
    def test_option_cannot_be_zero(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'person'
        try:
            _training, attributes, klass, _test, _gold = self.get_instances(path, True, False)
            disc = discretise.Discretiser(_training, attributes, klass, _test, _gold, [4,6], [2,0])
            self.fail('should raise error as an option is zero')
        except inv.InvalidDataError:
            pass
        
    def test_ranges_from_chunks(self):
        ranges = discretise.ranges_from_chunks([[6, 6, 7, 7, 8], [9, 10, 10, 13, 14], [15, 16, 16, 16, 19]])
        self.assertEqual(3, len(ranges))
        self.assertTrue(ranges[0].includes(6))
        self.assertTrue(ranges[0].includes(8))
        self.assertTrue(ranges[0].includes(8.49))
        self.assertFalse(ranges[0].includes(8.51))
        self.assertFalse(ranges[1].includes(8.49))
        self.assertTrue(ranges[1].includes(8.51))
        self.assertTrue(ranges[1].includes(9))
        self.assertTrue(ranges[1].includes(14))
        self.assertTrue(ranges[1].includes(14.49))
        self.assertFalse(ranges[1].includes(14.51))
        self.assertFalse(ranges[2].includes(14.49))
        self.assertTrue(ranges[2].includes(14.51))
        self.assertTrue(ranges[2].includes(15))
        self.assertTrue(ranges[2].includes(19))
        
    def test_get_chunks_with_frequency(self):
        chunks = discretise.get_chunks_with_frequency([6, 6, 7, 7, 8, 8, 8, 9, 10, 10, 13, 14, 14, 15, 16, 16, 16, 19], 5)
        self.assertEqual(3, len(chunks))
        self.assertEqual([[6, 6, 7, 7, 8], [9, 10, 10, 13, 14], [15, 16, 16, 16, 19]], chunks)

    def test_unsupervised_equal_frequency(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'weather'
        _training, attributes, klass, _test, _gold = self.get_instances(path)
        disc = discretise.Discretiser(_training, attributes, klass, _test, _gold, [1], [3])
        self.assertTrue(disc.attributes[1].is_continuous())
        self.assertEqual(27.5, disc.training[0].value(disc.attributes[1]))
        self.assertEqual(32, disc.training[2].value(disc.attributes[1]))
        self.assertEqual(25.4, disc.test[0].value(disc.attributes[1]))
        values = disc.training.values_grouped_by_attribute([disc.attributes[1]])
        values[0].sort()
        self.assertEqual([6.0, 9.0, 9.0, 10.699999999999999, 12.0, 12.0, 12.0, 14.1, 18.0, 27.5, 32.0, 33.100000000000001], values[0])
        
        disc.unsupervised_equal_frequency()
        
        self.assertFalse(disc.attributes[1].is_continuous())
        self.assertEqual(4, len(disc.attributes[1].values))
        self.assertEqual('c', disc.training[0].value(disc.attributes[1]))
        self.assertEqual('d', disc.training[2].value(disc.attributes[1]))
        self.assertEqual('c', disc.test[0].value(disc.attributes[1]))
        
    def test_naive_supervised_discretisation(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'person'
        _training, attributes, klass, _test, _gold = self.get_instances(path, True, False)
        disc = discretise.Discretiser(_training, attributes, klass, _test, _gold, [1])
        self.assertEqual(1, len(disc.attributes[1].values))
        
        disc.naive_supervised()
        
        self.assertEqual(3, len(disc.attributes[1].values))
        
    def test_stores_subset(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'person'
        _training, attributes, klass, _test, _gold = self.get_instances(path, True, False)
        disc = discretise.Discretiser(_training, attributes, klass, _test, _gold, [4,6], [2,2])
        self.assertEqual(2, len(disc.subset))
        self.assertEqual(4, disc.subset[0].index)
        self.assertEqual(6, disc.subset[1].index)
        
    def get_instances(self, path, get_test = True, get_gold = True):
        _test = _gold = None
        _training = training(path)
        attributes, klass = metadata(path)
        if get_test: _test = test(path)
        if get_gold: _gold = gold(path)
        return [_training, attributes, klass, _test, _gold]

        
class DiscretiseStub(discretise.Discretise):
    def __init__(self):
        discretise.Discretise.__init__(self)
        self.error_called = False
        self.message = None
        
    def error(self, message):
        #in reality error will display usage and quit
        self.message = message
        self.error_called = True
        
    def discretise_and_write_to_file(self):
        #do nothing
        pass

########NEW FILE########
__FILENAME__ = distancemetrictests
# Natural Language Toolkit - Distance Metric tests
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT
from nltk_contrib.classifier_tests import *
from nltk_contrib.classifier import distancemetric, instance as ins, attribute as attr
import math

class DistanceMetricTestCase(unittest.TestCase):    
    def test_integer_distance(self):
        self.assertEqual(1, distancemetric.distance(2, 1, True))
        self.assertEqual(1, distancemetric.distance(1, 2, True))
        
    def test_float_distance(self):
        self.assertEqual(0.345, distancemetric.distance(0.968, 0.623, True))
        self.assertEqual(0.345, distancemetric.distance(0.623, 0.968, True))
        
    def test_discrete_distance(self):
        self.assertEqual(1, distancemetric.distance('a', 'b', False))
        self.assertEqual(0, distancemetric.distance('a', 'a', False))
        
    def test_distance_with_one_value_as_string_performs_string_comparison(self):
        self.assertEqual(1, distancemetric.distance('a', 5, False))
        self.assertEqual(1, distancemetric.distance(5, 'b', False))
        
    def test_euclidean_distance(self):
        attributes = [attr.Attribute('A1', ['a','b'], 0), attr.Attribute('A2', ['continuous'], 1), 
                      attr.Attribute('A3', ['continuous'], 2), attr.Attribute('A4', ['g','h'], 3)]
        instance1 = ins.TrainingInstance(['a', 5, 3.4, 'g'], 'y')
        instance2 = ins.TestInstance(['a', 5, 3.4, 'g'])
        self.assertEqual(0, distancemetric.euclidean_distance(instance1, instance2, attributes))
        
        instance2 = ins.TestInstance(['b', 5, 3.4, 'g'])
        self.assertEqual(1, distancemetric.euclidean_distance(instance1, instance2, attributes))

        instance2 = ins.TestInstance(['b', 4, 3.4, 'h'])
        self.assertEqual(math.sqrt(3), distancemetric.euclidean_distance(instance1, instance2, attributes))

        instance2 = ins.TestInstance(['b', 4, 1.4, 'h'])
        self.assertEqual(math.sqrt(7), distancemetric.euclidean_distance(instance1, instance2, attributes))
    
    def test_hamilton_distance(self):
        attributes = [attr.Attribute('A1', ['a','b'], 0), attr.Attribute('A2', ['continuous'], 1), 
                      attr.Attribute('A3', ['continuous'], 2), attr.Attribute('A4', ['g','h'], 3)]
        instance1 = ins.TrainingInstance(['a', 5, 3.4, 'g'], 'y')
        instance2 = ins.TestInstance(['a', 5, 3.4, 'g'])
        self.assertEqual(0, distancemetric.hamiltonian_distance(instance1, instance2, attributes))
        
        instance2 = ins.TestInstance(['b', 5, 3.4, 'g'])
        self.assertEqual(1, distancemetric.hamiltonian_distance(instance1, instance2, attributes))

        instance2 = ins.TestInstance(['b', 4, 3.4, 'h'])
        self.assertEqual(3, distancemetric.hamiltonian_distance(instance1, instance2, attributes))

        instance2 = ins.TestInstance(['b', 4, 1.4, 'h'])
        self.assertEqual(5, distancemetric.hamiltonian_distance(instance1, instance2, attributes))

########NEW FILE########
__FILENAME__ = featureselecttests
# Natural Language Toolkit - Feature Select tests
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT
from nltk_contrib.classifier_tests import *
from nltk_contrib.classifier import featureselect as fs, decisionstump as ds, attribute as attr
from nltk_contrib.classifier.exceptions import invaliddataerror as inv
import copy

class FeatureSelectTestCase(unittest.TestCase):
    def test_decodes_parameters(self):
        feature_select = fs.FeatureSelect()
        feature_select.parse(['-a', 'RNK', '-t', 'path', '-T', 'path1,path2', '-o', 'IG,4'])
        algorithm = feature_select.values.ensure_value('algorithm', None)
        _training = feature_select.values.ensure_value('training', None)
        _test = feature_select.values.ensure_value('test', None)
        options = feature_select.values.ensure_value('options', None)
        
        self.assertEqual('RNK', algorithm)
        self.assertEqual('path', _training)
        self.assertEqual('path1,path2', _test)
        self.assertEqual('IG,4', options)

    def test_validates_algorithm(self):
        feat_sel = FeatureSelectStub()
        self.assertFalse(feat_sel.error_called)        
        feat_sel.parse(['-a', 'RNL', '-t', 'path', '-T', 'path1,path2', '-o', 'IG,4'])
        self.assertTrue(feat_sel.error_called)
        self.assertEqual('option -a: invalid choice: \'RNL\' (choose from \'RNK\', \'BE\', \'FS\')', feat_sel.message)

    def test_validates_required_arguments(self):
        feat_sel = FeatureSelectStub()
        self.assertFalse(feat_sel.error_called)        
        feat_sel.run(['-a', 'RNK', '-t', 'path', '-o', 'IG,4'])
        self.assertFalse(feat_sel.error_called) # should not throw error this situation can exist if there is only one dataset

        feat_sel = FeatureSelectStub()
        self.assertFalse(feat_sel.error_called)        
        feat_sel.run(['-a', 'RNK', '-T', 'path1,path2', '-o', 'IG,4'])
        self.assertTrue(feat_sel.error_called)
        self.assertEqual('Invalid arguments. One or more required arguments are not present.', feat_sel.message)

        #Takes in the default attribute
        feat_sel = FeatureSelectStub()
        self.assertFalse(feat_sel.error_called)        
        feat_sel.run(['-t', 'path', '-T', 'path1,path2', '-o', 'IG,4'])
        self.assertFalse(feat_sel.error_called)

        feat_sel = FeatureSelectStub()
        self.assertFalse(feat_sel.error_called)    
        try:    
            feat_sel.run(['-a', 'RNK', '-t', 'path', '-T', 'path1,path2'])
        except AttributeError:
            #When not running on the stub will return as soon as it encounters the error
            pass
        self.assertTrue(feat_sel.error_called)
        self.assertEqual('Invalid arguments. One or more required arguments are not present.', feat_sel.message)
        
    def test_cannot_perform_rank_select_on_cont_attrs(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'person'
        _training = training(path)
        attributes, klass = metadata(path)
        _test = test(path)
        feature_selection = fs.FeatureSelection(_training, attributes, klass, _test, None, ['IG','2'])
        try:
            feature_selection.by_rank()
            self.fail('should throw error as path points to continuous attributes')
        except inv.InvalidDataError:
            pass
    
    def test_find_attributes_by_ranking(self):
        path = datasetsDir(self) + 'minigolf' + SEP + 'weather'
        _training = training(path)
        attributes, klass = metadata(path)
        _test = test(path)
        _gold = gold(path)

        feature_selection = fs.FeatureSelection(_training, attributes, klass, _test, _gold, ['IG','3'])
        
        ig_for_attr1 = information_gain(attributes[0], klass, _training)
        self.assertAlmostEqual(0.324409, ig_for_attr1, 6)
        self.assertEqual('outlook', attributes[0].name)
        ig_for_attr2 = information_gain(attributes[1], klass, _training)
        self.assertAlmostEqual(0.102187, ig_for_attr2, 6)
        self.assertEqual('temperature', attributes[1].name)
        ig_for_attr3 = information_gain(attributes[2], klass, _training)
        self.assertAlmostEqual(0.091091, ig_for_attr3, 6)
        self.assertEqual('humidity', attributes[2].name)
        ig_for_attr4 = information_gain(attributes[3], klass, _training)
        self.assertAlmostEqual(0.072780, ig_for_attr4, 6)
        self.assertEqual('windy', attributes[3].name)
        attributes_to_remove = feature_selection.find_attributes_by_ranking('information_gain', 3)
        self.assertEqual(1, len(attributes_to_remove))
        self.assertEqual('windy', attributes_to_remove[0].name)
        
    def test_if_wrapper_options_is_invalid(self):
        self.assertTrue(fs.wrapper_options_invalid(['1R', '3', '5', '6']))
        self.assertTrue(fs.wrapper_options_invalid(['3', '1R']))
        self.assertTrue(fs.wrapper_options_invalid(['1R', 'a']))
        self.assertTrue(fs.wrapper_options_invalid(['1R', '1.0']))
        self.assertTrue(fs.wrapper_options_invalid(['1R', '1']))
        self.assertFalse(fs.wrapper_options_invalid(['1R', '2']))
        self.assertFalse(fs.wrapper_options_invalid(['1R', '2', '5']))
        self.assertTrue(fs.wrapper_options_invalid(['1R', '2', 'a']))
        self.assertFalse(fs.wrapper_options_invalid(['1R', '2', '0.1']))
        
    def test_get_delta(self):
        feature_selection = fs.FeatureSelection(None, None, None, None, None, ['IG','3', '5'])
        self.assertEqual(5, feature_selection.get_delta())
        feature_selection = fs.FeatureSelection(None, None, None, None, None, ['IG','3'])
        self.assertEqual(0, feature_selection.get_delta())
        
    def test_get_fold(self):
        feature_selection = fs.FeatureSelection(['trainingn'] * 4, None, None, None, None, ['IG','3', '5'])
        self.assertEqual(3, feature_selection.get_fold())
        
        feature_selection = fs.FeatureSelection(['trainingn'] * 14, None, None, None, None, ['IG'])
        self.assertEqual(10, feature_selection.get_fold())
        
    def test_invert_attrbute_selection(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'person'
        _attributes, _klass = metadata(path)
        feature_selection = fs.FeatureSelection(None, _attributes, None, None, None, ['IG'])
        unselected = feature_selection.invert_attribute_selection([_attributes[0], _attributes[1]])
        self.assertEqual(len(_attributes) - 2, len(unselected))
        self.assertEqual([_attributes[2], _attributes[3], _attributes[4], _attributes[5], _attributes[6], _attributes[7]], unselected)
        
    def test_is_float(self):
        self.assertTrue(fs.isfloat('1.2'))
        self.assertTrue(fs.isfloat('1'))
        self.assertTrue(fs.isfloat('0'))
        self.assertFalse(fs.isfloat('a'))
        
    ## calculations included by using verify variables and comments
    def test_forward_select(self):
        path = datasetsDir(self) + 'minigolf' + SEP + 'weather'
        _training = training(path)
        _attributes, _klass = metadata(path)
        _test = test(path)
        _gold = gold(path)
        
        verify_training = copy.deepcopy(_training)
        verify_attributes = copy.deepcopy(_attributes)

        feat_sel = fs.FeatureSelection(_training, _attributes, _klass, _test, _gold, ['1R', '4', '0.1'])
        feat_sel.forward_selection()
                
        self.assertEqual(1, len(_attributes))
        self.assertEqual('outlook', _attributes[0].name)
        self.verify_number_of_attributes(_training, 1)
        self.verify_number_of_attributes(_test, 1)
        self.verify_number_of_attributes(_gold, 1)

        #verification
        verification_cv_datasets = verify_training.cross_validation_datasets(4)
        accuracies = {}
        for attribute in verify_attributes:
            accuracies[attribute.name] = feat_sel.avg_accuracy_by_cross_validation(verification_cv_datasets, 4, attr.Attributes([attribute]))
        
        #'windy': 0.41666666666666663, 'outlook': 0.79166666666666663, 'temperature': 0.41666666666666663, 'humidity': 0.54166666666666663
        self.assertAlmostEqual(0.4166666, accuracies['windy'], 6)
        self.assertAlmostEqual(0.79166666, accuracies['outlook'], 6)
        self.assertAlmostEqual(0.4166666, accuracies['temperature'], 6)
        self.assertAlmostEqual(0.5416666, accuracies['humidity'], 6)
        
        #outlook selected
        accuracies = {}
        for each in verify_attributes:
            if each.name == 'outlook':
                outlook = each
        verify_attributes.remove(outlook)
        for attribute in verify_attributes:
            accuracies[('outlook',attribute.name)] = feat_sel.avg_accuracy_by_cross_validation(verification_cv_datasets, 4, attr.Attributes([outlook, attribute]))
        
        #{('outlook', 'humidity'): 0.79166666666666663, ('outlook', 'temperature'): 0.79166666666666663, ('outlook', 'windy'): 0.54166666666666663}
        self.assertAlmostEqual(0.7916666, accuracies[('outlook','humidity')], 6)
        self.assertAlmostEqual(0.7916666, accuracies['outlook', 'temperature'], 6)
        self.assertAlmostEqual(0.5416666, accuracies[('outlook','windy')], 6)

    ## calculations included by using verify variables and comments
    def test_backward_select(self):
        path = datasetsDir(self) + 'minigolf' + SEP + 'weather'
        _training = training(path)
        _attributes, _klass = metadata(path)
        _test = test(path)
        _gold = gold(path)
        
        verify_training = copy.deepcopy(_training)
        verify_attributes = copy.deepcopy(_attributes)

        feat_sel = fs.FeatureSelection(_training, _attributes, _klass, _test, _gold, ['1R', '4', '0.1'])
        feat_sel.backward_elimination()
                
        self.assertEqual(3, len(_attributes))
        self.verify_number_of_attributes(_training, 3)
        self.verify_number_of_attributes(_test, 3)
        self.verify_number_of_attributes(_gold, 3)

        #verification
        #level 0
        avg_acc = feat_sel.avg_accuracy_by_cross_validation(verify_training.cross_validation_datasets(4), 4, verify_attributes)
        self.assertAlmostEqual(0.5416666, avg_acc, 6)
        
        verification_cv_datasets = verify_training.cross_validation_datasets(4)
        accuracies = {}
        for attribute in verify_attributes:
            attributes = verify_attributes[:]
            attributes.remove(attribute)
            accuracies[(attributes[0].name,attributes[1].name,attributes[2].name)] = feat_sel.avg_accuracy_by_cross_validation(verification_cv_datasets, 4, attr.Attributes(attributes))
        
#        {('outlook', 'humidity', 'windy'): 0.54166666666666663, 
#        ('outlook', 'temperature', 'windy'): 0.54166666666666663, 
#        ('temperature', 'humidity', 'windy'): 0.29166666666666663, 
#        ('outlook', 'temperature', 'humidity'): 0.79166666666666663}

        self.assertAlmostEqual(0.5416666, accuracies[('outlook', 'humidity', 'windy')], 6)
        self.assertAlmostEqual(0.5416666, accuracies[('outlook', 'temperature', 'windy')], 6)
        self.assertAlmostEqual(0.2916666, accuracies[('temperature', 'humidity', 'windy')], 6)
        self.assertAlmostEqual(0.7916666, accuracies[('outlook', 'temperature', 'humidity')], 6)
#        
        #('outlook', 'temperature', 'humidity') selected
        accuracies = {}

        for each in verify_attributes:
            if each.name == 'windy':
                windy = each
        verify_attributes.remove(windy)
        for attribute in verify_attributes:
            attributes = verify_attributes[:]
            attributes.remove(attribute)
            accuracies[(attributes[0].name,attributes[1].name)] = feat_sel.avg_accuracy_by_cross_validation(verification_cv_datasets, 4, attr.Attributes(attributes))
        
        self.assertAlmostEqual(0.7916666, accuracies[('outlook','humidity')], 6)
        self.assertAlmostEqual(0.7916666, accuracies['outlook', 'temperature'], 6)
        self.assertAlmostEqual(0.4166666, accuracies[('temperature','humidity')], 6)
        
    def test_get_suffix_replaces_decimal_point_in_options_with_hyphen(self):
        feat_sel = FeatureSelectStub()
        feat_sel.run(['-a', 'RNK', '-f', 'path', '-o', 'IG,4'])
        self.assertEqual('-f_RNK_IG_4', feat_sel.get_suffix())

        feat_sel = FeatureSelectStub()
        feat_sel.run(['-a', 'FS', '-f', 'path', '-o', '0R,4,0.34'])
        self.assertEqual('-f_FS_0R_4_0-34', feat_sel.get_suffix())
        
    def verify_number_of_attributes(self, instances, number):
        for instance in instances:
            self.assertEqual(number, len(instance.attrs))
        
def information_gain(attribute, klass, instances):
    stump = ds.DecisionStump(attribute, klass)
    for instance in instances:
        stump.update_count(instance)
    return stump.information_gain()

class FeatureSelectStub(fs.FeatureSelect):
    def __init__(self):
        fs.FeatureSelect.__init__(self)
        self.error_called = False
        self.message = None
        self.log = open('test_log', 'w') # w to over write previous logs.. is append mode in code
        
    def error(self, message):
        #in reality error will display usage and quit
        self.message = message
        self.error_called = True
        
    def select_features_and_write_to_file(self):
        pass

########NEW FILE########
__FILENAME__ = formattests
# Natural Language Toolkit - Format tests
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT
from nltk_contrib.classifier_tests import *
from nltk_contrib.classifier import format, instance as ins, attribute as a

class FormatTestCase(unittest.TestCase):
    def test_get_c45_name(self):
        self.assertEqual('foo', format.c45.get_name('foo:a,b,c'))

    def test_get_c45_values(self):
        self.assertEqual(['a', 'b', 'c'], format.c45.get_values('foo:a,b,c'))
    
    def test_attribute_creation(self):
        _attributes = attributes(datasetsDir(self) + 'numerical' + SEP + 'person')
        self.assertEqual(8, len(_attributes), '8 attributes should be present')
        self.assertEqual(a.Attribute('id', ['continuous'], 0), _attributes[0])
        self.assertEqual(a.Attribute('creditrating', ['continuous'], 7), _attributes[7])

    def test_training_intances_creation(self):
        instances = training(datasetsDir(self) + 'numerical' + SEP + 'person')
        self.assertEqual(6, len(instances), '6 instances should be present')
        self.assertEqual(ins.TrainingInstance(['0','25','salaried','single','0','0','65000','3'],'yes'), instances[0])
        self.assertEqual(ins.TrainingInstance(['5','42','salaried','married','2','6','65000','6'],'no'), instances[5])

    def test_test_intances_creation(self):
        instances = test(datasetsDir(self) + 'numerical' + SEP + 'weather')
        self.assertEqual(1, len(instances), '1 instance should be present')
        self.assertEqual(ins.TestInstance(['overcast','25.4','high','true']), instances[0])

    def test_gold_intances_creation(self):
        instances = gold(datasetsDir(self) + 'numerical' + SEP + 'weather')
        self.assertEqual(4, len(instances), '4 instances should be present')
        self.assertEqual(ins.GoldInstance(['sunny','21','normal','true'],'yes'), instances[0])
        self.assertEqual(ins.GoldInstance(['rainy','17.9','high','true'],'no'), instances[3])
        
    def test_klass_creation(self):
        _klass = klass(datasetsDir(self) + 'numerical' + SEP + 'weather')
        self.assertEqual(2, len(_klass))
        self.assertEqual(['yes', 'no'], _klass)
        
    def test_classified_klass_in_gold_is_not_written_if_asked_not_to(self):
        _gold = gold(datasetsDir(self) + 'numerical' + SEP + 'weather')
        fmt = C45FormatStub()
        self.assertTrue(fmt.dummy_file is None)
        fmt.write_gold(_gold, '/dummy/path')
        self.assertFalse(fmt.dummy_file is None)
        self.assertEqual(len(_gold), len(fmt.dummy_file.lines_written))
        self.assertEqual(['sunny,21,normal,true,yes,None', 'overcast,18,high,true,yes,None', 'overcast,28.3,notmal,false,yes,None', 'rainy,17.9,high,true,no,None'], fmt.dummy_file.lines_written)
        
        fmt = C45FormatStub()
        fmt.write_gold(_gold, '/dummy/path', False)
        self.assertEqual(['sunny,21,normal,true,yes', 'overcast,18,high,true,yes', 'overcast,28.3,notmal,false,yes', 'rainy,17.9,high,true,no'], fmt.dummy_file.lines_written)

    def test_classified_klass_in_test_is_not_written_if_asked_not_to(self):
        _test = test(datasetsDir(self) + 'numerical' + SEP + 'weather')
        fmt = C45FormatStub()
        self.assertTrue(fmt.dummy_file is None)
        fmt.write_test(_test, '/dummy/path')
        self.assertFalse(fmt.dummy_file is None)
        self.assertEqual(len(_test), len(fmt.dummy_file.lines_written))
        self.assertEqual(['overcast,25.4,high,true,None'], fmt.dummy_file.lines_written)
        
        fmt = C45FormatStub()
        fmt.write_test(_test, '/dummy/path', False)
        self.assertEqual(['overcast,25.4,high,true'], fmt.dummy_file.lines_written)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(unittest.TestSuite(unittest.makeSuite(FormatTestCase)))

class C45FormatStub(format.C45Format):
    def __init__(self):
        self.dummy_file = None
        
    def create_file(self, path, extension):
        self.dummy_file = LinesFile()
        return self.dummy_file
        
class LinesFile:
    def __init__(self):
        self.lines_written = None
    
    def write(self, lines):
        self.lines_written = lines

########NEW FILE########
__FILENAME__ = inittests
# Natural Language Toolkit - Format tests
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT
from nltk_contrib.classifier_tests import *
from nltk_contrib.classifier import *
from nltk.probability import FreqDist

class InitTestCase(unittest.TestCase):
    def test_split_file_names(self):
        returnvalue = split_ignore_space('foo , bar, foobar')
        self.assertEqual(3, len(returnvalue))
        self.assertEqual(['foo', 'bar', 'foobar'], returnvalue)

    def test_entropy_of_elements(self):
        e = entropy(['yes', 'no', 'yes', 'yes', 'yes', 'no'])
        self.assertEqual(-1 * (4.0/6 * math.log( 4.0/6, 2) + 2.0/6 * math.log(2.0/6, 2)), e)
    
    def test_min_entropy_breakpoint(self):
        position, min_ent = min_entropy_breakpoint(['yes', 'no', 'yes', 'yes', 'yes', 'no'])
        self.assertEqual(4, position)
        self.assertEqual(-1 * (4.0/5 * math.log(4.0/5, 2) + 1.0/5 * math.log(1.0/5, 2)), min_ent)
        
    def test_entropy_function(self):
        dictionary_of_klass_counts = {}
        dictionary_of_klass_counts['yes'] = 2
        dictionary_of_klass_counts['no'] = 0
        self.assertEqual(0, entropy_of_key_counts(dictionary_of_klass_counts))
        
        dictionary_of_klass_counts['yes'] = 3
        dictionary_of_klass_counts['no'] = 3
        self.assertAlmostEqual(1, entropy_of_key_counts(dictionary_of_klass_counts))
        
        dictionary_of_klass_counts['yes'] = 9
        dictionary_of_klass_counts['no'] = 5
        self.assertAlmostEqual(0.94, entropy_of_key_counts(dictionary_of_klass_counts), 2)
        
        dictionary_of_klass_counts['yes'] = 1
        dictionary_of_klass_counts['no'] = 3
        expected = -(1.0/4 * math.log(1.0/4, 2)) + -(3.0/4 * math.log(3.0/4, 2))
        self.assertAlmostEqual(expected, entropy_of_key_counts(dictionary_of_klass_counts), 6)

        dictionary_of_klass_counts['yes'] = 2
        dictionary_of_klass_counts['no'] = 1
        expected = -(2.0/3 * math.log(2.0/3, 2))  + -(1.0/3 * math.log(1.0/3, 2))
        self.assertAlmostEqual(expected, entropy_of_key_counts(dictionary_of_klass_counts), 6)

    def test_entropy_of_freq_dist(self):
        fd = FreqDist()
        fd.inc('yes', 2)
        fd.inc('no', 1)
        expected = -(2.0/3 * math.log(2.0/3, 2))  + -(1.0/3 * math.log(1.0/3, 2))
        self.assertAlmostEqual(expected, entropy_of_freq_dist(fd), 6)

########NEW FILE########
__FILENAME__ = instancestests
# Natural Language Toolkit
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT

from nltk_contrib.classifier import instances as ins, instance, attribute as a, discretisedattribute as da, numrange as nr, util
from nltk_contrib.classifier.exceptions import systemerror as system, invaliddataerror as inv
from nltk_contrib.classifier_tests import *
import math

class InstancesTestCase(unittest.TestCase):
    def test_the_number_of_instances(self):
        instances = ins.TrainingInstances([instance.TrainingInstance(['foo', 'bar'], 'a'), instance.TrainingInstance(['foo', 'foobar'], 'b')])
        self.assertEqual(2, len(instances), '2 instances should be present')
        
    def test_validatiy_of_attribute_values(self):
        path = datasetsDir(self) + 'test_faulty' + SEP + 'invalid_attributes'
        instances = training(path)
        a, c = metadata(path)
        self.assertFalse(instances.are_valid(c, a))
        
    def test_equality(self):
        instances = training(datasetsDir(self) + 'test_phones' + SEP + 'phoney')
        same = training(datasetsDir(self) + 'test_phones' + SEP + 'phoney')
        self.assertEqual(instances, same, 'should be same')
        other = training(datasetsDir(self) + 'test_faulty' + SEP + 'invalid_attributes')
        self.assertNotEqual(instances, other, 'should not be same')
        
        instances = test(datasetsDir(self) + 'test_phones' + SEP + 'phoney')
        same = test(datasetsDir(self) + 'test_phones' + SEP + 'phoney')
        self.assertEqual(instances, same, 'should be same')
        other = training(datasetsDir(self) + 'test_phones' + SEP + 'phoney')
        self.assertNotEqual(instances, other, 'should not be same')
        
    def test_gold_instances_are_created_from_gold_files(self):
        _gold = gold(datasetsDir(self) + 'test_phones' + SEP + 'phoney')
        self.assertEqual(7, len(_gold))
        self.assertEqual(instance.GoldInstance, _gold[0].__class__)

    def test_gold_insts_thrws_system_error_if_confusion_matrix_is_invoked_bfore_classification(self):
        _gold = gold(datasetsDir(self) + 'test_phones' + SEP + 'phoney')
        try:
            _gold.confusion_matrix(None)
            self.fail('Should throw exception as it is not classified yet')
        except system.SystemError:
            pass

    def test_filtering_does_not_affect_existing_instances(self):
        path = datasetsDir(self) + 'test_phones' + SEP + 'phoney'
        _training = training(path)
        self.assertEqual(7, len(_training))
        _attributes = attributes(path)
        filtered = _training.filter(_attributes[1], 'big')
        self.assertEqual(3, len(filtered))
        self.assertEqual(7, len(_training))

    def test_ranges_of_attribute_values(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'weather'
        _training = training(path)
        _attributes = attributes(path)
        ranges = _training.value_ranges([_attributes[1]])
        self.assertEqual(1, len(ranges))
        self.assertEqual(6.0, ranges[0].lower)
        self.assertAlmostEqual(33.100001, ranges[0].upper, 6)
        
    def test_ranges_of_multiple_attribute_values(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'person'
        _training = training(path)
        _attributes = attributes(path)
        ranges = _training.value_ranges([_attributes[0], _attributes[1], _attributes[4], _attributes[5], _attributes[6]])
        self.assertEqual(5, len(ranges))
        self.assertEqual(0, ranges[0].lower)
        self.assertAlmostEqual(5.000001, ranges[0].upper)
        self.assertEqual(19, ranges[1].lower)
        self.assertAlmostEqual(42.000001, ranges[1].upper)
        self.assertEqual(0, ranges[2].lower)
        self.assertAlmostEqual(2.000001, ranges[2].upper)
        self.assertEqual(0, ranges[3].lower)
        self.assertAlmostEqual(6.000001, ranges[3].upper)
        self.assertEqual(0, ranges[4].lower)
        self.assertAlmostEqual(120000.000001, ranges[4].upper)

    def test_attempt_to_discretise_non_continuous_attribute_raises_error(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'weather'
        _training = training(path)
        try:
            _training.value_ranges([a.Attribute('outlook', ['sunny','overcast','rainy'], 0)])
            self.fail('should throw error')
        except inv.InvalidDataError:
            pass
        
    def test_discretise_using_discretised_attributes(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'person'
        _training = training(path)
        _attributes = attributes(path)
        self.assertEqual(0.0, _training[0].value(_attributes[4]))
        self.assertEqual(65000.0, _training[0].value(_attributes[6]))
        disc_dependents = da.DiscretisedAttribute('dependents', nr.Range(0, 2, True).split(2), 4)
        disc_annual_income = da.DiscretisedAttribute('annualincome', nr.Range(0, 120000, True).split(5), 6)
        _training.discretise([disc_dependents, disc_annual_income])
        
        self.assertEqual('a', _training[0].value(disc_dependents))
        self.assertEqual('c', _training[0].value(disc_annual_income))
        
    def test_values_grouped_by_attribute(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'weather'
        _training = training(path)
        _attributes = attributes(path)
        self.assertEqual([[27.5, 33.1, 32, 18, 12, 10.7, 6, 14.1, 9, 9, 12, 12]] ,_training.values_grouped_by_attribute([_attributes[1]]))
        
    def test_returns_array_of_all_klass_values(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'person'
        _training = training(path)
        klass_values = _training.klass_values()
        
        self.assertEqual(len(_training), len(klass_values))
        for index in range(len(klass_values)):
            self.assertEqual(klass_values[index], _training[index].klass_value)

    def test_sort_by_attribute(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'person'
        _training = training(path)
        _attributes = attributes(path)
        attr_values = _training.values_grouped_by_attribute([_attributes[1]])
        self.assertEqual([25.0, 19.0, 21.0, 34.0, 31.0, 42.0], attr_values[0])
        klass_values = _training.klass_values()
        self.assertEqual(['yes', 'no', 'yes', 'yes', 'yes', 'no'], klass_values)
        
        _training.sort_by(_attributes[1])
        attr_values = _training.values_grouped_by_attribute([_attributes[1]])
        self.assertEqual([19.0, 21.0, 25.0, 31.0, 34.0, 42.0], attr_values[0])
        klass_values = _training.klass_values()
        self.assertEqual(['no', 'yes', 'yes', 'yes', 'yes', 'no'], klass_values)
        
    def test_ranges_from_breakpoints(self):
        brkpts = ins.SupervisedBreakpoints(['no', 'yes', 'yes', 'yes', 'yes', 'no'], [19.0, 21.0, 25.0, 31.0, 34.0, 42.0])
        brkpts.find_naive()
        ranges = brkpts.as_ranges()
        self.assertEqual(3, len(ranges))
        self.assertEqual(19.0, ranges[0].lower)
        self.assertEqual(20.0, ranges[0].upper)
        self.assertEqual(20.0, ranges[1].lower)
        self.assertEqual(38.0, ranges[1].upper)
        self.assertEqual(38.0, ranges[2].lower)
        self.assertEqual(42.000001, ranges[2].upper)
        
    def test_simple_naive_breakpoints(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'person'
        _training = training(path)
        _attributes = attributes(path)
        
        breakpoints = _training.supervised_breakpoints(_attributes[1])
        breakpoints.find_naive()
        self.assertEqual(['no', 'yes', 'yes', 'yes', 'yes', 'no'], _training.klass_values())
        self.assertEqual([19.0, 21.0, 25.0, 31.0, 34.0, 42.0], _training.attribute_values(_attributes[1]))
        self.assertEqual(2, len(breakpoints))
        self.assertEqual([0,4], breakpoints)
        
    def test_naive_breakpoints_with_shifting(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'person'
        _attributes = attributes(path)
        _training = training(path)
        breakpoints = _training.supervised_breakpoints(_attributes[4])
        breakpoints.find_naive()
        
        self.assertEqual(['yes', 'no', 'yes', 'yes', 'yes', 'no'], _training.klass_values())
        self.assertEqual([0.0, 0.0, 0.0, 2.0, 2.0, 2.0], _training.attribute_values(_attributes[4]))
        self.assertEqual(1, len(breakpoints))
        self.assertEqual([2], breakpoints)

    def test_breakpoints_in_class_membership(self):
        breakpoints = ins.SupervisedBreakpoints(['yes', 'no', 'yes', 'yes', 'yes', 'no'], [19.0, 21.0, 25.0, 31.0, 34.0, 42.0])
        breakpoints = breakpoints.breakpoints_in_class_membership()
        self.assertEqual(3, len(breakpoints))
        self.assertEqual([0, 1, 4], breakpoints)
    
    def test_entropy_based_breakpoints(self):
        breakpoints = ins.SupervisedBreakpoints(['yes', 'no', 'yes', 'yes', 'yes', 'no'], [19.0, 21.0, 25.0, 31.0, 34.0, 42.0])
        breakpoints.find_entropy_based_max_depth(2)
        self.assertEqual(2, len(breakpoints))
        self.assertEqual([4,0], breakpoints.data)
        
    def test_adjust_for_min_freq(self):
        breakpoints = ins.SupervisedBreakpoints(['yes', 'no', 'yes', 'yes', 'yes', 'no', 'no', 'yes', 'yes'], [64, 65, 68, 69, 70, 71, 72, 72, 75])
        breakpoints.find_naive()
        self.assertEqual(4, len(breakpoints))
        self.assertEqual([0, 1, 4, 7], breakpoints)
        
        breakpoints.adjust_for_min_freq(4)
        self.assertEqual(1, len(breakpoints))
        self.assertEqual([4], breakpoints)
        
    def test_naive_discretisation_version1(self):
        breakpoints = ins.SupervisedBreakpoints(['yes', 'no', 'yes', 'yes', 'yes', 'no', 'no', 'yes', 'yes'], [64, 65, 68, 69, 70, 71, 72, 72, 75])
        breakpoints.find_naive_v1(3)
        self.assertEqual(1, len(breakpoints))
        self.assertEqual([3], breakpoints)
        
    def test_naive_discretisation_version2(self):
        breakpoints = ins.SupervisedBreakpoints(['yes', 'no', 'yes', 'yes', 'yes', 'no', 'no', 'yes', 'yes'], [64, 65, 68, 69, 70, 71, 72, 72, 75])
        breakpoints.find_naive_v2(3)
        self.assertEqual(2, len(breakpoints))
        self.assertEqual([4, 7], breakpoints)

    def test_remove_attributes(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'person'
        _attributes = attributes(path)
        _training = training(path)
        self.assertEqual(8, len(_training[0].attrs))
        self.assertEqual(8, len(_training[-1].attrs))
        _training.remove_attributes([_attributes[0], _attributes[3]])
        self.assertEqual(6, len(_training[0].attrs))
        self.assertEqual(6, len(_training[-1].attrs))

    def test_stratified_bunches(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'person'
        _training = training(path)
        self.assertEqual(6, len(_training))
        
        bunches = _training.stratified_bunches(3)
        self.assertEqual(3, len(bunches))
        #training is now sorted.. so content of bunches can be predicted
        self.assertEqual([_training[0], _training[3]], bunches[0])
        self.assertEqual([_training[1], _training[4]], bunches[1])
        self.assertEqual([_training[2], _training[5]], bunches[2])
        
    def test_training_as_gold(self):
        training1 = instance.TrainingInstance(['a','b','c'],'x')
        training2 = instance.TrainingInstance(['d','b','c'],'y')
        training3 = instance.TrainingInstance(['e','b','c'],'z')
        training_instances = [training1, training2, training3]
        gold_instances = ins.training_as_gold(training_instances)
        self.assertEqual(3, len(gold_instances))
        
        for i in [0,1,2]:
            self.assertEqual(training_instances[i].attrs, gold_instances[i].attrs)
            self.assertEqual(training_instances[i].klass_value, gold_instances[i].klass_value)

    def test_training_returns_datasets_for_cross_validation(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'person'
        _training = training(path)
        self.assertEqual(6, len(_training))
        datasets = _training.cross_validation_datasets(4)
        
        self.assertEqual(4, len(datasets))
        self.assertEqual(ins.TrainingInstances, datasets[0][0].__class__)
        self.assertEqual(ins.GoldInstances, datasets[0][1].__class__)
        self.assertEqual(4, len(datasets[0][0]))#first training has 4 instances
        self.assertEqual(2, len(datasets[0][1]))#first gold has 2 instances
        self.assertEqual(4, len(datasets[1][0]))#second training has 4 instances
        self.assertEqual(5, len(datasets[2][0]))#third training has 5 instances
        self.assertEqual(5, len(datasets[3][0]))#fourth training has 5 instances
        self.assertEqual(1, len(datasets[3][1]))#fourth gold has 1 instance
        
    def test_cross_validation_datasets_with_fold_greater_than_length_of_training(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'person'
        _training = training(path)
        self.assertEqual(6, len(_training))
        datasets = _training.cross_validation_datasets(8)
        
        self.assertEqual(3, len(datasets))
        self.assertEqual(ins.TrainingInstances, datasets[0][0].__class__)
        self.assertEqual(ins.GoldInstances, datasets[0][1].__class__)
        self.assertEqual(4, len(datasets[0][0]))#first training has 4 instances
        self.assertEqual(2, len(datasets[0][1]))#first gold has 2 instances
        self.assertEqual(4, len(datasets[1][0]))#second training has 4 instances
        self.assertEqual(4, len(datasets[2][0]))#third training has 5 instances

    def test_flatten(self):
        result = ins.flatten([[2,3],[4,5],[6,7]])
        self.assertEqual([2,3,4,5,6,7], result)
        
    def test_class_frequency_distribution(self): 
        path = datasetsDir(self) + 'numerical' + SEP + 'person'
        _training = training(path)
        class_freq_dist = _training.class_freq_dist()
        self.assertEqual(6, class_freq_dist.N())
        self.assertEqual(2, class_freq_dist.B())
        self.assertEqual(4, class_freq_dist['yes'])
        self.assertEqual(2, class_freq_dist['no'])
        
    def test_class_freq_dist_in_reverse_to_store_classes(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'person'
        _training = training(path)
        class_freq_dist = _training.class_freq_dist()
        self.assertEqual(['yes','no'], class_freq_dist.keys())
        
        
    def test_posterior_probablities_with_discrete_values(self):
        path = datasetsDir(self) + 'test_phones' + SEP + 'phoney'
        _training = training(path)
        _attributes, _klass = metadata(path)
        
        posterior_probabilities = _training.posterior_probablities(_attributes, _klass)
        self.assertAlmostEqual(0.2, posterior_probabilities.value(_attributes[0], 'dual', 'a'))
        self.assertAlmostEqual(0.2, posterior_probabilities.value(_attributes[0], 'dual', 'b'))
        self.assertAlmostEqual(0.6, posterior_probabilities.value(_attributes[0], 'dual', 'c'))
        
        self.assertEqual(len(_attributes), len(posterior_probabilities.freq_dists))
        self.assertEqual(len(_attributes[0].values), len(posterior_probabilities.freq_dists[_attributes[0]]))

    def test_posterior_probabilities_with_cont_values(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'person'
        _training = training(path)
        _attributes = attributes(path)
        _klass = klass(path)
        
        posterior_probabilities = _training.posterior_probablities(_attributes, _klass)
        #numerical verification
        values_for_class_yes = util.StatList([25,21,34,31])#from data set
        mean = values_for_class_yes.mean()
        sd = values_for_class_yes.std_dev()
        expected_value = (1.0 / math.sqrt(2 * math.pi * sd)) * math.exp(-pow((30 - mean), 2)/ (2 * pow(sd, 2)))
        self.assertEqual(expected_value, posterior_probabilities.value(_attributes[1], 30, 'yes'))
        
        self.assertTrue(posterior_probabilities.value(_attributes[1], 30, 'yes') > posterior_probabilities.value(_attributes[1], 30, 'no'))
        
    def test_prob_using_gaussian_dist(self):
        self.assertAlmostEqual(1.0 / math.sqrt(2 * math.pi), ins.calc_prob_based_on_distrbn(2, 1, 2))
        
    def test_adjust_for_equal_values(self):
        attr_values = [4.2999999999999998, 4.4000000000000004, 4.4000000000000004, 4.4000000000000004, 4.5, 4.5999999999999996, 4.5999999999999996, 4.5999999999999996, 4.5999999999999996, 4.7000000000000002, 4.7000000000000002, 4.7999999999999998, 4.7999999999999998, 4.7999999999999998, 4.7999999999999998, 4.7999999999999998, 4.9000000000000004, 4.9000000000000004, 4.9000000000000004, 4.9000000000000004, 4.9000000000000004, 4.9000000000000004, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0999999999999996, 5.0999999999999996, 5.0999999999999996, 5.0999999999999996, 5.0999999999999996, 5.0999999999999996, 5.0999999999999996, 5.0999999999999996, 5.0999999999999996, 5.2000000000000002, 5.2000000000000002, 5.2000000000000002, 5.2000000000000002, 5.2999999999999998, 5.4000000000000004, 5.4000000000000004, 5.4000000000000004, 5.4000000000000004, 5.4000000000000004, 5.4000000000000004, 5.5, 5.5, 5.5, 5.5, 5.5, 5.5, 5.5, 5.5999999999999996, 5.5999999999999996, 5.5999999999999996, 5.5999999999999996, 5.5999999999999996, 5.5999999999999996, 5.7000000000000002, 5.7000000000000002, 5.7000000000000002, 5.7000000000000002, 5.7000000000000002, 5.7000000000000002, 5.7000000000000002, 5.7000000000000002, 5.7999999999999998, 5.7999999999999998, 5.7999999999999998, 5.7999999999999998, 5.7999999999999998, 5.7999999999999998, 5.7999999999999998, 5.9000000000000004, 5.9000000000000004, 5.9000000000000004, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0999999999999996, 6.0999999999999996, 6.0999999999999996, 6.0999999999999996, 6.0999999999999996, 6.0999999999999996, 6.2000000000000002, 6.2000000000000002, 6.2000000000000002, 6.2000000000000002, 6.2999999999999998, 6.2999999999999998, 6.2999999999999998, 6.2999999999999998, 6.2999999999999998, 6.2999999999999998, 6.2999999999999998, 6.2999999999999998, 6.2999999999999998, 6.4000000000000004, 6.4000000000000004, 6.4000000000000004, 6.4000000000000004, 6.4000000000000004, 6.4000000000000004, 6.4000000000000004, 6.5, 6.5, 6.5, 6.5, 6.5, 6.5999999999999996, 6.5999999999999996, 6.7000000000000002, 6.7000000000000002, 6.7000000000000002, 6.7000000000000002, 6.7000000000000002, 6.7000000000000002, 6.7000000000000002, 6.7000000000000002, 6.7999999999999998, 6.7999999999999998, 6.7999999999999998, 6.9000000000000004, 6.9000000000000004, 6.9000000000000004, 6.9000000000000004, 7.0, 7.0999999999999996, 7.2000000000000002, 7.2000000000000002, 7.2000000000000002, 7.2999999999999998, 7.4000000000000004, 7.5999999999999996, 7.7000000000000002, 7.7000000000000002, 7.7000000000000002, 7.7000000000000002, 7.9000000000000004]
        klass_values = ['Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-versicolor', 'Iris-virginica', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-versicolor', 'Iris-versicolor', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-versicolor', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-versicolor', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-versicolor', 'Iris-setosa', 'Iris-setosa', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-virginica', 'Iris-setosa', 'Iris-setosa', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-virginica', 'Iris-setosa', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-versicolor', 'Iris-versicolor', 'Iris-virginica', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-virginica', 'Iris-virginica', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-virginica', 'Iris-virginica', 'Iris-versicolor', 'Iris-versicolor', 'Iris-virginica', 'Iris-virginica', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-versicolor', 'Iris-versicolor', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-versicolor', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-versicolor', 'Iris-virginica', 'Iris-virginica', 'Iris-versicolor', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-versicolor', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica', 'Iris-virginica']
        sb = ins.SupervisedBreakpoints(klass_values, attr_values)
        breakpoints = sb.breakpoints_in_class_membership()
        self.assertEqual([19, 20, 21, 29, 31, 39, 40, 43, 44, 50, 51, 53, 63, 64, 66, 71, 72, 73, 76, 79, 81, 82, 86, 88, 92, 94, 96, 98, 101, 107, 109, 114, 115, 119, 124, 129, 130, 132, 133, 136, 137], breakpoints)
        self.assertAlmostEqual(4.9, attr_values[19])
        self.assertAlmostEqual(4.9, attr_values[20])
        self.assertAlmostEqual(4.9, attr_values[21])
        self.assertAlmostEqual(5.0, attr_values[22])
        self.assertAlmostEqual(5.0, attr_values[29])
        self.assertAlmostEqual(5.0, attr_values[30])
        self.assertAlmostEqual(5.0, attr_values[31])
        self.assertAlmostEqual(5.1, attr_values[32])
        
        sb.find_naive()
        self.assertEqual([21, 31, 40, 44, 51, 58, 64, 72, 79, 82, 88, 94, 98, 107, 114, 119, 129, 132, 136, 137], sb.data)
        self.assertAlmostEqual(4.9, attr_values[sb[0]])
        self.assertAlmostEqual(5.0, attr_values[sb[1]])
        self.assertAlmostEqual(5.1, attr_values[sb[2]])
        
    def test_to_string(self):
        instances = ins.TrainingInstances([instance.TrainingInstance(['foo', 'bar'], 'a'), instance.TrainingInstance(['foo', 'foobar'], 'b')])
        self.assertEqual('[[foo,bar;a], [foo,foobar;b]]', str(instances))

        
if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(unittest.TestSuite(unittest.makeSuite(InstancesTestCase)))

########NEW FILE########
__FILENAME__ = instancetests
# Natural Language Toolkit
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT

from nltk_contrib.classifier import instance as ins, attribute, discretisedattribute as da, numrange as r
import unittest

class InstanceTestCase(unittest.TestCase):
    def setUp(self):
        self.a = 'a'
        self.b = 'b'
    
    def test_create_n_validate_instance(self):
        instance = ins.TestInstance(['bar','two','a'])
        self.assertEqual(3, len(instance.attrs))
        self.assertEqual('bar', instance.attrs[0])
        self.assertEqual('two', instance.attrs[1])
        self.assertEqual('a', instance.attrs[2])
        
    def test_training_instance_has_class_and_attributes(self):
        instance = ins.TrainingInstance(['bar','two'],'a')
        self.assertEqual(self.a, instance.klass_value)
        self.assertEqual(['bar', 'two'], instance.attrs)
        
    def test_test_instance_has_only_attributes_and_none_as_class(self):
        instance = ins.TestInstance(['bar','two'])
        self.assertEqual(None, instance.klass_value)
        self.assertEqual(['bar', 'two'], instance.attrs)
        
    def test_cannot_set_class_in_training_instance(self):
        instance = ins.TrainingInstance(['bar','two'],'a')
        try:
            getattr(instance, 'set_klass')(self.b)
            self.fail('should not be able to set a class on a Training Instance')
        except AttributeError:
            self.assertEqual(self.a, instance.klass_value, 'should not have changed the original class')
        
    def test_should_be_able_to_set_class_on_test_instance(self):
        instance = ins.TestInstance(['bar','two'])
        try:
            instance.set_klass('c')
            self.assertEqual('c', instance.classified_klass)
            self.assertEqual(None, instance.klass_value)
        except AttributeError:
            self.fail('should be able to set class in Test Instance')
        
    def test_gold_instance_creates_class(self):
        gold = ins.GoldInstance(['bar','two'],'a')
        self.assertEqual(2, len(gold.attrs))
        self.assertEqual('bar', gold.attrs[0])
        self.assertEqual('two', gold.attrs[1])
        self.assertEqual(self.a, gold.klass_value)
        self.assertEqual(None, gold.classified_klass)
        
    def test_classes_can_be_set_on_gold_instance(self):
        gold = ins.GoldInstance(['bar','two'],'a')
        self.assertEqual(self.a, gold.klass_value)
        self.assertEqual(None, gold.classified_klass)
        gold.set_klass(self.b)
        self.assertEqual(self.a, gold.klass_value)
        self.assertEqual(self.b, gold.classified_klass)
        
    def test_string_representation(self):
        instance = ins.TrainingInstance(['bar','two'],'a')
        self.assertEqual("[bar,two;a]", instance.__str__());
        
        instance = ins.TestInstance(['bar','two'])
        self.assertEqual("[bar,two; ]", instance.__str__());
        instance.set_klass('b')
        self.assertEqual("[bar,two;b]", instance.__str__());
                
        instance = ins.GoldInstance(['bar','two'],'a')
        self.assertEqual("[bar,two;a; ]", instance.__str__());
        instance.set_klass('b')
        self.assertEqual("[bar,two;a;b]", instance.__str__());
        
    def test_get_attribute_value_from_instance_using_attribute(self):
        instance = ins.TrainingInstance(['bar','two'],'a')
        attr = attribute.Attribute('second', ['two','duo'], 1)
        self.assertEqual('two', instance.value(attr))
        
        test = ins.TestInstance(['bar','two'])
        self.assertEqual('two', test.value(attr))

        gold = ins.GoldInstance(['bar','two'],'a')
        self.assertEqual('two', gold.value(attr))

    def test_discretise_using_discretised_attributes(self):
        dependents = attribute.Attribute('dependents',['continuous'], 4)
        annual_salary = attribute.Attribute('annualsalary', ['continuous'], 6)
        disc_dependents = da.DiscretisedAttribute('dependents', r.Range(0, 2, True).split(2), 4)
        disc_annual_salary = da.DiscretisedAttribute('annualsalary', r.Range(0, 120000, True).split(5), 6)
        discretised_attributes = [disc_dependents, disc_annual_salary]
        
        instance = ins.TrainingInstance(['3','34','self-employed','married','2','3','120000','2'],'yes')
        self.assertEqual(2, instance.value(dependents))
        self.assertEqual(120000, instance.value(annual_salary))
        instance.discretise(discretised_attributes)
        
        self.assertEqual('b', instance.value(disc_dependents))
        self.assertEqual('e', instance.value(disc_annual_salary))

    def test_values_of_atrributes(self):
        _training = ins.TrainingInstance(['3','34','self-employed','married','2','3','120000','2'],'yes')
        dependents = attribute.Attribute('dependents', ['continuous'], 4)
        annual_salary = attribute.Attribute('annualsalary', ['continuous'], 6)
        self.assertEqual(['2','120000'], _training.values([dependents, annual_salary]))
        
    def test_remove_attrbutes(self):
        _training = ins.TrainingInstance(['3','34','self-employed','married','2','3','120000','2'],'yes')
        id = attribute.Attribute('id', ['continuous'], 0)
        annual_salary = attribute.Attribute('annualsalary', ['continuous'], 6)
        _training.remove_attributes([id, annual_salary])
        self.assertEqual(6, len(_training.attrs))
        self.assertEqual('34', _training.attrs[0])
    
    def test_get_training_as_gold(self):
        _training = ins.TrainingInstance(['3','34','self-employed','married','2','3','120000','2'],'yes')
        gold = _training.as_gold()
        self.assertEqual(gold.attrs, _training.attrs)
        self.assertEqual(gold.klass_value, _training.klass_value)

########NEW FILE########
__FILENAME__ = itemtests
# Natural Language Toolkit
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT

from nltk_contrib.classifier import item
import unittest

class ItemTestCase(unittest.TestCase):
    def testRemovesNewLineAndWhitespace(self):
        i = item.Item('f,g,h\n')
        self.assertEqual('f,g,h', i.stripNewLineAndWhitespace())
        i = item.Item('f, g , h')
        self.assertEqual('f,g,h', i.stripNewLineAndWhitespace())
        i = item.Item('f, g, h\n')
        self.assertEqual('f,g,h', i.stripNewLineAndWhitespace())
        
    def testNameItemRemovesDotAndNewLine(self):
        i = item.NameItem('a,b,c.\n')
        self.assertEqual('a,b,c', i.processed(), 'dot and slash should be removed')

    def testNameItemSouldNotRemoveDotBetweenFloats(self):
        i = item.NameItem('foo: 1.0,2.0.\n')
        self.assertEqual('foo:1.0,2.0', i.processed(), 'should not remove dot in float')        
        i = item.NameItem('foo: 1.0,2.0\n')
        self.assertEqual('foo:1.0,2.0', i.processed(), 'should not remove dot in float')        

        
    def testIsAttributeReturnsFalseForClasses(self):
        i = item.NameItem('a,b,c.\n')
        self.assertFalse(i.isAttribute(), 'it is not an attribute')
        
    def testIsAttributeReturnsTrueForAttribute(self):
        i = item.NameItem('temp: high, low.\n')
        self.assertTrue(i.isAttribute(), 'it is an attribute')
        
    def testClassValueDoesNotIncludeSpace(self):
        i = item.NameItem('a ,b,c')
        self.assertEqual('a,b,c', i.processed(), 'should not have whitespaces in the string')
        
    def testAttributeValuesDoNotHaveWhitespace(self):
        i = item.NameItem('foo : a , b, c')
        self.assertEqual('foo:a,b,c', i.processed(), 'should not have whitespaces in the string')

########NEW FILE########
__FILENAME__ = knntests
# Natural Language Toolkit
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT

from nltk_contrib.classifier import knn, instance as ins, instances as inss
from nltk_contrib.classifier_tests import *
import unittest

class KNNTestCase(unittest.TestCase):
    def setUp(self):
        self.ins1 = ins.TrainingInstance(['bar','two'],'a')
        self.ins2 = ins.TrainingInstance(['foo','two'],'a')
        self.ins3 = ins.TrainingInstance(['baz','three'],'b')        
    
    def test_majority_klass_vote_with_no_training_instances(self):
        self.assertEqual(None, knn.majority_klass_vote([]))

    def test_majority_klass_vote_with_training_instances(self):
        self.assertEqual('a', knn.majority_klass_vote([self.ins1, self.ins2, self.ins3]))

    def setup_instance_distances_with_6_instances(self):
        ins4 = ins.TrainingInstance(['bar','one'],'a')
        ins5 = ins.TrainingInstance(['foo','one'],'a')
        ins6 = ins.TrainingInstance(['baz','four'],'b')

        id = knn.InstanceDistances()
        id.distance(1.0, self.ins1)
        id.distance(1.0, self.ins2)
        id.distance(1.0, self.ins3)
        id.distance(2.0, ins4)
        id.distance(3.0, ins5)
        id.distance(2.0, ins6)
        
        return id
    
    def test_instance_distances_min_dist_instances(self):
        id = self.setup_instance_distances_with_6_instances()
        self.assertEqual([self.ins1, self.ins2, self.ins3], id.minimum_distance_instances())
        
    def test_instance_distances_invokes_strategy_with_instances(self):
        id = self.setup_instance_distances_with_6_instances()
        self.assertEqual('foo', id.klass(self.stub_strategy))
    
    def stub_strategy(self, test_parameters):
        self.assertEqual(3, len(test_parameters))
        return 'foo'
        
    def test_ib1(self):
        path = datasetsDir(self) + 'minigolf' + SEP + 'weather'
        _training = training(path)
        _attributes, _klass = metadata(path)
        
        classifier = knn.IB1(_training, _attributes, _klass)
        test_instance = ins.TestInstance(['sunny','hot','high','false'])
        classifier.test(inss.TestInstances([test_instance]))
        self.assertEqual('no', test_instance.classified_klass)
        

########NEW FILE########
__FILENAME__ = naivebayestests
# Natural Language Toolkit
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT

from nltk_contrib.classifier import instances as ins, instance, attribute as a, naivebayes
from nltk_contrib.classifier_tests import *

class NaiveBayesTestCase(unittest.TestCase):
    def setUp(self):
        path = datasetsDir(self) + 'loan' + SEP + 'loan'
        self._training = training(path)
        self._attributes, self._klass = metadata(path)
        self._test = test(path)
        self.nb = naivebayes.NaiveBayes(self._training, self._attributes, self._klass)    
        
    def test_naive_bayes_classification(self):
        self.nb.train()
        self.nb.test(self._test)
        self.assertEqual('no', self._test[0].classified_klass)
        
        self.assertAlmostEqual(0.5555555, self.nb.post_probs.value(self._attributes[0], 'no', 'no'), 6)
        
    def test_prior_probability_calculations(self):
        self.nb.train()
        self.assertAlmostEqual((7.0 + 1.0)/(10 + 2), self.nb.prior_probability('no'), 6)
        self.assertAlmostEqual((3.0 + 1.0)/(10 + 2), self.nb.prior_probability('yes'), 6)
        
    def test_posterior_probability_calculations(self):
        self.nb.train()
        self.assertAlmostEqual((2.0 + 1.0)/(4 + 2), self.nb.posterior_probability(self._attributes[1], 'single', 'no'), 6)
        self.assertAlmostEqual((2.0 + 1.0)/(4 + 2), self.nb.posterior_probability(self._attributes[1], 'single', 'yes'), 6)
        self.assertAlmostEqual((4.0 + 1.0)/(4 + 2), self.nb.posterior_probability(self._attributes[1], 'married', 'no'), 6)
        self.assertAlmostEqual((0 + 1.0)/(4 + 2), self.nb.posterior_probability(self._attributes[1], 'married', 'yes'), 6)
        self.assertAlmostEqual((1.0 + 1.0)/(2 + 2), self.nb.posterior_probability(self._attributes[1], 'divorced', 'no'), 6)
        self.assertAlmostEqual((1.0 + 1.0)/(2 + 2), self.nb.posterior_probability(self._attributes[1], 'divorced', 'yes'), 6)
        
    def test_class_conditional_probability(self):
        #sunny,hot,high,false,no
        #sunny,hot,high,true,no
        #overcast,hot,high,false,yes
        #rainy,mild,high,false,yes
        #rainy,cool,normal,false,yes
        #rainy,cool,normal,true,no
        #overcast,cool,normal,true,yes
        #sunny,mild,high,false,no
        #sunny,cool,normal,false,yes
        nominal_path = datasetsDir(self) + 'minigolf' + SEP + 'weather'
        _training = training(nominal_path)
        _attributes = attributes(nominal_path)
        _klass = klass(nominal_path)
        _test = test(nominal_path)
        nb = naivebayes.NaiveBayes(_training, _attributes, _klass)    
        
        nb.train()
        #test[0] = overcast,mild,high,true
        expected_no = ((0 + 1.0)/(2 + 2)) * ((1.0 + 1.0)/(2 + 2)) * ((3.0 + 1.0)/(5 + 2)) * ((2.0 + 1.0)/(3 + 2)) * ((4.0 + 1.0)/(9 + 2))
        self.assertAlmostEqual(expected_no, nb.class_conditional_probability(_test[0], 'no'), 6)
        
        expected_yes = ((2.0 + 1.0)/(2 + 2)) * ((1.0 + 1.0)/(2 + 2)) * ((2.0 + 1.0)/(5 + 2)) * ((1.0 + 1.0)/(3 + 2)) * ((5.0 + 1.0)/(9 + 2))
        self.assertAlmostEqual(expected_yes, nb.class_conditional_probability(_test[0], 'yes'), 6)
                
    def test_expected_class(self):
        nominal_path = datasetsDir(self) + 'minigolf' + SEP + 'weather'
        _training = training(nominal_path)
        _attributes = attributes(nominal_path)
        _klass = klass(nominal_path)
        _test = test(nominal_path)
        nb = naivebayes.NaiveBayes(_training, _attributes, _klass)    

        nb.train()
        
        self.assertEqual('yes', nb.estimate_klass(_test[0]))

########NEW FILE########
__FILENAME__ = numrangetests
# Natural Language Toolkit - RangeTest
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
from nltk_contrib.classifier import numrange as r
from nltk_contrib.classifier_tests import *
from nltk_contrib.classifier.exceptions import systemerror as se

class RangeTestCase(unittest.TestCase):
    def test_within_range(self):
        _range = r.Range(0, 4)
        self.assertTrue(_range.includes(0))
        self.assertTrue(_range.includes(1))
        self.assertTrue(_range.includes(3))
        self.assertTrue(_range.includes(3.9999))
        self.assertFalse(_range.includes(4))
        self.assertFalse(_range.includes(4.1))
        
        _new_range = r.Range(0, 4, True)
        self.assertTrue(_new_range.includes(4))
        self.assertFalse(_range.includes(4.1))
                
    def test_range_equality(self):
        _range = r.Range(0, 4)
        _same = r.Range(0, 4)
        self.assertEqual(_range, _same)
        self.assertEqual(hash(_range), hash(_same))
        _other = r.Range(0, 4.1)
        self.assertNotEqual(_range, _other)
        
    def test_include_expands_range(self):
        _range = r.Range()
        _range.include(4)
        self.assertFalse(_range.includes(0))
        self.assertFalse(_range.includes(3.99999))
        self.assertTrue(_range.includes(4))
        self.assertFalse(_range.includes(4.000002))
        
        _range.include(0)
        self.assertTrue(_range.includes(0))
        self.assertTrue(_range.includes(1))
        self.assertTrue(_range.includes(4))
        
        _other = r.Range(0, 4)
        self.assertTrue(_range, _other)
        _same = r.Range(0, 4, True)
        self.assertTrue(_range, _same)
        
        _other.include(4)
        self.assertEqual(0, _other.lower)
        self.assertEqual(4.000001, _other.upper)
        
        _range.include(5)
        self.assertTrue(_range.includes(4.1))
        self.assertTrue(_range.includes(5))
        
    def test_split_returns_none_when_lower_eq_upper(self):
        _range = r.Range()
        self.assertEquals(None, _range.split(2))
        
    def test_split_returns_none_if_size_of_each_split_is_less_than_delta(self):
        try:
            _range = r.Range(0, 0.000005)
            _range.split(7)
        except (se.SystemError), e:
            self.assertEquals('Splitting of range resulted in elements smaller than delta 1e-06.', e.message)
        
    def test_split_includes_the_highest_and_lowest(self):
        _range = r.Range()
        _range.include(0)
        _range.include(4)
        splits = _range.split(4)
        self.assertEqual(0, splits[0].lower)
        self.assertEqual(1, splits[0].upper)
        self.assertEqual(1, splits[1].lower)
        self.assertEqual(2, splits[1].upper)
        self.assertEqual(2, splits[2].lower)
        self.assertEqual(3, splits[2].upper)
        self.assertEqual(3, splits[3].lower)
        self.assertEqual(4.000001, splits[3].upper)
        
        _range = r.Range()
        _range.include(2)    
        _range.include(8)
        splits = _range.split(2)
        self.assertEqual(2, splits[0].lower)
        self.assertEqual(5, splits[0].upper)
        self.assertEqual(5, splits[1].lower)
        self.assertAlmostEqual(8.000001, splits[1].upper, 6)
        
        
    def test_string_reprn(self):
        _range = r.Range()
        _range.include(0)
        _range.include(4)
        self.assertEqual('[0,4.000001]', str(_range))
        
    def test_include_adds_the_max(self):
        _range = r.Range(5,8.0)
        self.assertFalse(_range.includes(8))
        
        _range.include(8.0)
        self.assertTrue(_range.includes(8))
        self.assertAlmostEqual(8.000001, _range.upper)


if __name__ == '__main__':
        runner = unittest.TextTestRunner()
        runner.run(unittest.TestSuite(unittest.makeSuite(RangeTestCase)))

########NEW FILE########
__FILENAME__ = onertests
# Natural Language Toolkit
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT

from nltk_contrib.classifier import oner, instances as ins, format
from nltk_contrib.classifier_tests import *
from nltk_contrib.classifier.exceptions import invaliddataerror as inv

class OneRTestCase(unittest.TestCase):
    
    def setUp(self):
        path = self.WEATHER = datasetsDir(self) + 'minigolf' + SEP + 'weather'
        self.classifier = oner.OneR(training(path), attributes(path), klass(path))
        self.classifier.train()
                
    def test_best_decision_stump_returns_minimum_error_stump_by_default(self):
        minError = self.classifier.best_decision_stump(self.classifier.training)
        self.assertAlmostEqual(0.2222222, minError.error())
        
    def test_classifies_test_with_stump(self):
        self.classifier.test(test(self.WEATHER))
        self.assertTrue(self.classifier.test_instances[0].classified_klass is not None)
        self.assertEqual('yes', self.classifier.test_instances[0].classified_klass)
        
    def test_verifies_classification(self):
        cm = self.classifier.verify(gold(self.WEATHER))
        self.assertEqual(0.5, cm.accuracy())
        self.assertAlmostEqual(0.6666667, cm.fscore())
        
    def test_best_decision_stump_uses_the_passed_in_algorithm(self):
        path = self.WEATHER
        classifier = OneRStub(training(path), attributes(path), klass(path))
        self.assertEqual("dummy Best Decision stump", classifier.best_decision_stump(classifier.training, [], 'dummy_algorithm'))
        
    def test_throws_error_for_invalid_algorithm(self):
        try:
            self.classifier.best_decision_stump(self.classifier.training, [], 'invalid_algorithm')
            self.fail('should throw error')
        except inv.InvalidDataError:
            pass
    
    def test_throws_error_for_continuous_attributes(self):
        try:
            path = datasetsDir(self) + 'numerical' + SEP + 'weather'
            classifier = oner.OneR(training(path), attributes(path), klass(path))
            classifier.train()
            self.fail('should have thrown error')
        except inv.InvalidDataError:
            pass
        
        
class OneRStub(oner.OneR):
    def __init__(self, instances, attributes, klass):
        oner.OneR.__init__(self, instances, attributes, klass)
        
    def dummy_algorithm(self, decision_stumps):
        return "dummy Best Decision stump"

########NEW FILE########
__FILENAME__ = batchtest
# Natural Language Toolkit
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT


import os
import os.path
from nltk_contrib.classifier import format as fmt, discretise as d, featureselect as f, classify as c, util
import sys

def run(root_path, log_path):
    print('in run')
    for dir_name, dirs, files in os.walk(root_path):
        data = set([])
        print('Dir name ' + str(dir_name) + ' dirs ' + str(dirs) + ' files ' + str(files))
        for file in files:
            index = file.rfind('.')
            if index != -1:
                ext = file[index + 1:]
                if ext == fmt.c45.NAMES or ext == fmt.c45.DATA or ext == fmt.c45.TEST or ext == fmt.c45.GOLD:
                    data.add(dir_name + os.path.sep + file[:index])
        for each in data:
            process(each, log_path)
    
def process(path, log_path):
    attributes, klass = fmt.c45.metadata(path)
    training = fmt.c45.training(path)

    disc_suffixes, filter_inputs = [], []
    has_continuous = attributes.has_continuous()
    
    if has_continuous:
        indices = attributes.continuous_attribute_indices()
        
        uef_options = to_str_array(len(training) / 10, len(indices))
        uew_options = to_str_array(10, len(indices))
        ns_mod_options = to_str_array(len(training) / 15, len(indices))
        en_options = to_str_array(3, len(indices))# 3 will result in 8 classes(closest to 10)
        
        disc_suffixes = d.batch_run(path, indices, log_path, {d.UNSUPERVISED_EQUAL_FREQUENCY: uef_options, 
                                              d.UNSUPERVISED_EQUAL_WIDTH: uew_options,
                                              d.NAIVE_SUPERVISED: None,
                                              d.NAIVE_SUPERVISED_V1: ns_mod_options,
                                              d.NAIVE_SUPERVISED_V2: ns_mod_options,
                                              d.ENTROPY_BASED_SUPERVISED: en_options})
        filter_inputs = ['']
    filter_inputs.extend(disc_suffixes)
    filter_suffixes = f.batch_filter_select(path, filter_inputs, get_number_of_filter_attributes(len(attributes)), log_path, has_continuous)
        
    suffixes = ['']
    suffixes.extend(disc_suffixes)
    suffixes.extend(filter_suffixes)
    
    for algorithm in c.ALL_ALGORITHMS:
        wrapper_inputs, all = [''], suffixes[:]
        wrapper_inputs.extend(disc_suffixes)
        if has_continuous and not c.ALGORITHM_MAPPINGS[algorithm].can_handle_continuous_attributes():
            del wrapper_inputs[0]
            all.remove('')
        wrapper_suffixes = f.batch_wrapper_select(path, wrapper_inputs, algorithm, 25, 0.1, log_path)
        all.extend(wrapper_suffixes)
        
        for suffix in all:
            params = ['-a', algorithm, '-f', path + suffix, '-l', log_path, '-c', 5]
            print "Params " + str(params)
            c.Classify().run(params)    
            
def to_str_array(value, times):
    return util.int_array_to_string([value] * times)
        
def get_number_of_filter_attributes(len_attrs):
    if len_attrs <= 10:
        return len_attrs * 2 / 3
    if len_attrs <= 20:
        return len_attrs / 2
    return 10 + (len_attrs - 20) / 8 
                
def delete_generated_files(path):
    to_be_deleted = []
    for (dir_name, dirs, files) in os.walk(path):
        for f in files:
            if f.find('-') != -1 and f.find('_') != -1:
                to_be_deleted.append(dir_name + os.path.sep + f)
    for each in to_be_deleted:
        os.remove(each)
                
if __name__ == "__main__":
    resp = 0
    while(resp != 1 and resp != 2):
        try:
            resp = int(raw_input("Select one of following options:\n1. Run all tests\n2. Delete generated files\n"))
        except ValueError:
            pass
    if resp == 1:
        dir_tree_path = raw_input("Enter directory tree path")
        log_file = raw_input("Enter log file")
        run(dir_tree_path, log_file)
    elif resp == 2:
        dir_path = raw_input("Enter directory path")
        delete_generated_files(dir_path)

########NEW FILE########
__FILENAME__ = convert
# Natural Language Toolkit
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT


import re
from nltk_contrib.classifier import format, discretise as ds, featureselect as fs, classify, util
import os, os.path

DISC_METHODS = [ds.UNSUPERVISED_EQUAL_WIDTH, ds.UNSUPERVISED_EQUAL_FREQUENCY, ds.NAIVE_SUPERVISED, ds.NAIVE_SUPERVISED_V1, ds.NAIVE_SUPERVISED_V2, ds.ENTROPY_BASED_SUPERVISED]
FS_METHODS = [fs.RANK, fs.FORWARD_SELECTION, fs.BACKWARD_ELIMINATION]
FS_OPTIONS = [fs.INFORMATION_GAIN, fs.GAIN_RATIO]
CLASSIFIERS = [classify.ZERO_R, classify.ONE_R, classify.DECISION_TREE, classify.NAIVE_BAYES, classify.IB1]

def convert_and_shift(file_path, ext, suffix = 'conv', sep = ' '):
    """
    converts elements separated by a blank space into comma separated data
    also changes the position of the class element from the first element 
    to the last element
    """
    lines = []
    f = open(file_path, 'r')
    for line in f:        
        words = line.split(sep)
        words[-1] = words[-1].strip()
        words = words[1:] + [words[0]]
        lines.append(','.join(words))
    f.close()
    ext_dot_index = file_path.rindex('.')
    if ext_dot_index == -1:
        base_name = file_path
    else:
        base_name = file_path[:ext_dot_index]
    f = open((base_name + suffix) + '.' + ext, 'w')
    for line in lines:
        f.write(line + '\n')
    f.close()    
    
def values(file_path, index, sep = " "):
    """
    returns a comma separated list of all values that an 
    element at index 'index' can take in a file at 'file_path'
    """
    values = set([])
    f = open(file_path, 'r')
    for line in f:        
        words = line.split(sep)
        if not index < len(words):
            print "Warning! omitting line " + str(line)
            continue
        values.add(words[index])
    return ','.join(values)
    
def convert(path):
    """
    converts elements separated by a blank space into comma separated data
    """
    converted = []
    f = open(path, 'r')
    converted = [','.join(l.split()) for l in f]
    f.close()
    ind = path.rfind('.')
    if ind == -1: ind = len(path)
    nf = open(path[:ind] + 'conv' + path[ind:], 'w')
    for l in converted:print >>nf, l
    nf.close()
    
def convert_log_to_csv(path):
    classifications = get_classification_log_entries(path)
    
    csvf = open(path + '.csv', 'w')
    for each in classifications:
        print >>csvf, each.algorithm + ',' + each.training + ',' + each.test + ',' + each.gold + ',' + each.accuracy + ',' + each.f_score

def get_classification_log_entries(path):
    f = open(path)
    separator = '-' * 40
    classifications = []
    obj = None
    for each in f:
        if each.find(separator) != -1:
            if obj != None and obj.operation == 'Classification':
                classifications.append(obj)
            obj = LogEntry()            
        else:
            parts = each.split(":")
            if len(parts) < 2: continue
            name = parts[0].strip()
            name = name.lower()
            name = re.sub('-', '_', name)
            if len(parts) > 2: value = parts[1:]
            else: value = parts[1].strip()
            setattr(obj, name, value)
    return classifications

def convert_log_to_tex_tables(path):
    classifications = get_classification_log_entries(path)
    
    datasets = {}
    for each in classifications:
        dataset = get_dataset_name(each.training)
        if not dataset in datasets:
            datasets[dataset] = {}
            
        if not each.algorithm in datasets[dataset]:
            datasets[dataset][each.algorithm] = {classify.ACCURACY:{}, classify.F_SCORE:{}}
        pp = get_preprocessing(each.training)
        datasets[dataset][each.algorithm][classify.ACCURACY][pp] = float(each.accuracy)
        datasets[dataset][each.algorithm][classify.F_SCORE][pp] = float(each.f_score)
    
    cols = [str(None)]
    cols.extend(DISC_METHODS) 
    
    rows = [str(None)] + combine_methods(fs.RANK, FS_OPTIONS)
    
    accuracy_tables = []
    f_score_tables = []
    
    for dataset in datasets:
        accuracy_table = get_tex_table_header(dataset, classify.ACCURACY, cols)
        f_score_table = get_tex_table_header(dataset, classify.F_SCORE, cols)
        
        for alg in CLASSIFIERS:
            rows += [fs.FORWARD_SELECTION + '_' + alg, fs.BACKWARD_ELIMINATION + '_' + alg]
            accuracy_table += ' \\multirow{' + str(len(rows)) + '}{*}{' + alg + '}'
            f_score_table += ' \\multirow{' + str(len(rows)) + '}{*}{' + alg + '}'
            for index in range(len(rows)):
                sel = rows[index]
                accuracies, fscores = [], []

                for disc in cols:
                    if (disc, sel) in datasets[dataset][alg][classify.ACCURACY]:
                        if float == type(datasets[dataset][alg][classify.ACCURACY][(disc, sel)]) or int == type(datasets[dataset][alg][classify.ACCURACY][(disc, sel)]):
                            acc = datasets[dataset][alg][classify.ACCURACY][(disc, sel)]
                            fsc = datasets[dataset][alg][classify.F_SCORE][(disc, sel)]
                            accuracies.append('%.4f' % acc)
                            fscores.append('%.4f' % fsc)
                        else:
                            accuracies.append(datasets[dataset][alg][classify.ACCURACY][(disc, sel)])
                            fscores.append(datasets[dataset][alg][classify.F_SCORE][(disc, sel)])
                    else:
                        accuracies.append('')
                        fscores.append('')
                accuracy_table += ' & ' + get_fs_display_name(sel) + ' & ' + ' & '.join(accuracies) + '\\\\ \n'
                f_score_table += ' & ' + get_fs_display_name(sel) + ' & ' + ' & '.join(fscores) + '\\\\ \n'
                
            accuracy_table += '\\hline \n'
            f_score_table += '\\hline \n'            
            rows.remove(fs.FORWARD_SELECTION + '_' + alg)
            rows.remove(fs.BACKWARD_ELIMINATION + '_' + alg)

        
        accuracy_table += get_tex_table_footer()
        f_score_table += get_tex_table_footer()
        accuracy_tables.append(accuracy_table)
        f_score_tables.append(f_score_table)
    allrows = rows + combine_methods(fs.FORWARD_SELECTION, CLASSIFIERS) + combine_methods(fs.BACKWARD_ELIMINATION, CLASSIFIERS)
    
    mean_accuracy_tables = []
    for dataset in datasets:
        mean_accuracy_table = get_mean_tex_table_header(dataset, "Mean Accuracy", cols)   
        for row in allrows:
            mean_accuracy_table += get_fs_display_name(sel)
            for column in cols:
                stat_list = util.StatList()
                for alg in CLASSIFIERS:
                    if (column, row) in datasets[dataset][alg][classify.ACCURACY]:
                        val = datasets[dataset][alg][classify.ACCURACY][(column, row)]
                        if float == type(val) or int == type(val):
                            stat_list.append(val)
                mean_accuracy_table += ' & %.4f' %stat_list.mean()
            mean_accuracy_table += '\\\\ \n'
        mean_accuracy_table += '\\hline'
        mean_accuracy_table += get_tex_table_footer()
        mean_accuracy_tables.append(mean_accuracy_table)        
        
    mean_datasets = ''
    mean_datasets = get_tex_table_header("All Datasets", "Mean Accuracy", cols)   
    test_classifiers = CLASSIFIERS[:]
    test_classifiers.remove(classify.ZERO_R)
    for alg in test_classifiers:
        rows += [fs.FORWARD_SELECTION + '_' + alg, fs.BACKWARD_ELIMINATION + '_' + alg]
        mean_datasets += '\\hline \\multirow{' + str(len(rows) * 2) + '}{*}{' + alg + '}'
        for row in rows:
            mean_datasets += ' & \\multirow{2}{*}{' + get_fs_display_name(row) + '}'
            stat_list_col = {}
            for column in cols:
                stat_list = util.StatList()
                for dataset in datasets:
                    if (column, row) in datasets[dataset][alg][classify.ACCURACY]:
                        val = datasets[dataset][alg][classify.ACCURACY][(column, row)]
                        if float == type(val) or int == type(val):
                            if row.find(fs.FORWARD_SELECTION) != -1:
                                oner_row = fs.FORWARD_SELECTION + '_' + classify.ZERO_R
                            elif row.find(fs.BACKWARD_ELIMINATION) != -1:
                                oner_row = fs.BACKWARD_ELIMINATION + '_' + classify.ZERO_R
                            else:
                                oner_row = row
                            stat_list.append(val - datasets[dataset][classify.ZERO_R][classify.ACCURACY][(column, oner_row)])
                stat_list_col[column] = stat_list
            for column in cols:
                mean_datasets += ' & %.4f' %stat_list_col[column].mean()
            mean_datasets += '\\\\ \n &'
            for column in cols:
                mean_datasets += ' & ($\\pm$ %.4f)' %stat_list_col[column].std_dev()            
            mean_datasets += '\\\\ \n'
        rows.remove(fs.FORWARD_SELECTION + '_' + alg)
        rows.remove(fs.BACKWARD_ELIMINATION + '_' + alg)

        mean_datasets += '\\hline'
    mean_datasets += get_tex_table_footer()
        
    texf = open(path + '-acc.tex', 'w')
    for table in accuracy_tables:
        print >>texf, table
    texf = open(path + '-fs.tex', 'w')
    for table in f_score_tables:
        print >>texf, table
    texf = open(path + '-macc.tex', 'w')
    for table in mean_accuracy_tables:
        print >>texf, table
    texf = open(path + '-mdatasets.tex', 'w')
    print >>texf, mean_datasets

def get_stat_lists(cols):
    return dict([(each, util.StatList()) for each in cols])        

def get_fs_display_name(name):
    if name.find('_') == -1:
        return name
    parts = name.split('_')
    if parts[0] == fs.FORWARD_SELECTION or parts[0] == fs.BACKWARD_ELIMINATION:
        return parts[0]
    return ' '.join(parts)
        
def get_tex_table_header(dataset, measure, cols):
    return '\\begin{table*}\n' + \
        '\\centering\n' + \
        '\\caption{' + dataset.capitalize() + ' - ' + measure.capitalize() +'}\n' + \
        '\\label{'+ dataset + '_' + measure +'}\n' + \
        '\\begin{tabular}{cc|' + 'c' * len(cols) + '}\n' + \
        '\\hline \\textbf{Algorithm} & \\textbf{Feature selection} & \\multicolumn{7}{|c}{\\textbf{Discretization}}\\\\ \n' + \
        ' & & ' + ' & '.join(cols) + ' \\\\ \n' + \
        '\\hline'

def get_mean_tex_table_header(dataset, measure, cols):
    return '\\begin{table*}\n' + \
        '\\centering\n' + \
        '\\caption{' + dataset.capitalize() + ' - ' + measure.capitalize() +'}\n' + \
        '\\label{'+ dataset + '_' + measure +'}\n' + \
        '\\begin{tabular}{c|' + 'c' * len(cols) + '}\n' + \
        '\\hline \\textbf{Feature selection} & \\multicolumn{7}{|c}{\\textbf{Discretization}}\\\\ \n' + \
        ' & ' + ' & '.join(cols) + ' \\\\ \n' + \
        '\\hline'


def get_tex_table_footer():
    return '\\end{tabular} \n' + \
        '\\end{table*} \n'

def get_dataset_name(name):
    filename = name.split('-d_')[0]
    filename = filename.split('-f_')[0]
    return os.path.basename(filename)   

def get_preprocessing(name):
    discn = get_pp(name, 'd')
    fsn = get_pp(name, 'f')
    return (discn, fsn)

def combine_methods(base, options):
    return [base + '_' + each for each in options]

def get_pp(name, pp_method):
    parts = name.split(get_dataset_name(name))[-1].split('-')
    i = 1
    while (i < len(parts)):
        if parts[i].find(pp_method) != -1:
            elements = parts[i].split('_')
            to_be_removed = []
            for index in range(len(elements)):
                each = elements[index]
                if not DISC_METHODS.__contains__(each) and not FS_METHODS.__contains__(each) and not FS_OPTIONS.__contains__(each) and not CLASSIFIERS.__contains__(each):
                    to_be_removed.append(index)
            to_be_removed.sort()
            to_be_removed.reverse()
            for each in to_be_removed:
                elements.__delitem__(each)
            return '_'.join(elements)
        i+=1
    return str(None)

def create_CV_datasets(path, fold):
    training = format.C45_FORMAT.get_training_instances(path)
    datasets = training.cross_validation_datasets(fold)
    slash = path.rindex(os.path.sep)
    parent_path = path[:slash]
    name = path[slash + 1:]
    for i in range(len(datasets)):
        os.makedirs(parent_path + os.path.sep + str(i + 1))
        format.C45_FORMAT.write_training_to_file(datasets[i][0], parent_path + os.path.sep + str(i + 1) + os.path.sep + name)
        format.C45_FORMAT.write_gold_to_file(datasets[i][1], parent_path + os.path.sep + str(i + 1) + os.path.sep + name)
    
class LogEntry:
    pass

if __name__ == "__main__":
    convert_log_to_tex_tables('/home/sumukh/changedlog')


    

########NEW FILE########
__FILENAME__ = stats
# Natural Language Toolkit
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT
from nltk import probability
from nltk_contrib.classifier import format
import os

def class_distribution(base_path):
    training = format.C45_FORMAT.get_training_instances(base_path)
    freq_dist = probability.FreqDist()
    for each in training:
        freq_dist.inc(each.klass_value)
    return freq_dist

def disc_attribute_values(base_path):
    attributes = format.C45_FORMAT.get_attributes(base_path)
    indices = attributes.continuous_attribute_indices()
    
        

########NEW FILE########
__FILENAME__ = zerortests
# Natural Language Toolkit
#
# Author: Sumukh Ghodke <sumukh dot ghodke at gmail dot com>
#
# URL: <http://www.nltk.org/>
# This software is distributed under GPL, for license information see LICENSE.TXT

from nltk_contrib.classifier import zeror as z, instances as ins
from nltk_contrib.classifier.exceptions import invaliddataerror as inv
from nltk_contrib.classifier_tests import *

class ZeroRTestCase(unittest.TestCase):
    def test_zeroR_instance_is_created_with_training_data(self):
        path = datasetsDir(self) + 'test_phones' + SEP + 'phoney'
        classifier = z.ZeroR(training(path), attributes(path), klass(path))
        self.assertEqual(training(datasetsDir(self) + 'test_phones' + SEP + 'phoney'), classifier.training, 'should have created training instances')
    
    def test_zeroR_verifies_validity_of_training_data(self):
        try:
            path = datasetsDir(self) + 'test_faulty' + SEP + 'invalid_attributes'
            classifier = z.ZeroR(training(path), attributes(path), klass(path))
            classifier.train()
            self.fail('should throw invalid data error')
        except inv.InvalidDataError:
            pass
        
    def test_majority_class(self):
        path = datasetsDir(self) + 'test_phones' + SEP + 'phoney'
        classifier = z.ZeroR(training(path), attributes(path), klass(path))
        self.assertEqual('b', classifier.majority_class())
        
    def test_majority_class_is_set_on_test_instances(self):
        path = datasetsDir(self) + 'test_phones' + SEP + 'phoney'
        zeror = z.ZeroR(training(path), attributes(path), klass(path))
        zeror.train()
        zeror.test(test(path))
        i = 0
        for i in range(4):
            self.assertEqual('b', zeror.test_instances[i].classified_klass)
            self.assertEqual(None, zeror.test_instances[i].klass_value)
            
    def test_verify_returns_correct_confusion_matrix(self):
        path = datasetsDir(self) + 'minigolf' + SEP + 'weather'
        klasses = klass(path)
        zeror = z.ZeroR(training(path), attributes(path), klasses)
        zeror.train()
        confusion_matrix = zeror.verify(gold(path))
        
        self.assertEqual(0.75, confusion_matrix.accuracy())
        self.assertEqual(0.25, confusion_matrix.error())
        self.assertEqual(1, confusion_matrix.tpr())
        self.assertEqual(0, confusion_matrix.tnr())
        self.assertEqual(1, confusion_matrix.fpr())
        self.assertEqual(0.75, confusion_matrix.precision())
        self.assertEqual(1, confusion_matrix.recall())
        self.assertAlmostEqual(0.85714286, confusion_matrix.fscore(), 8)

    def test_can_classify_data_having_continuous_attributes(self):
        path = datasetsDir(self) + 'numerical' + SEP + 'weather'
        zeror = z.ZeroR(training(path), attributes(path), klass(path))
        zeror.train()
        zeror.verify(gold(path))
        
        
        

        

########NEW FILE########
__FILENAME__ = cosine
# Natural Language Toolkit: Cosine Classifier
#
# Copyright (C) 2001-2007 NLTK Project
# Author: Sam Huston <shuston@csse.unimelb.edu.au>
#         Steven Bird <sb@csse.unimelb.edu.au>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT
#

"""
Cosine Classifier -- Beta version
"""

from math import sqrt, pow
from nltk.probability import *
from nltk_contrib.classify import *

class Cosine(AbstractClassify):
    """
    The Cosine Classifier uses the cosine distance algorithm to compute
    the distance between the sample document and each of the specified classes.
    A cosine classifier needs to be trained with representative examples
    of each class. From these examples the classifier
    calculates the most probable classification of the sample.
  
                     C . S
    D(C|S) = -------------------------
             sqroot(C^2) * sqroot (S^2)
  
    Internal data structures:
    _feature_dectector:
        holds a feature detector function
    _classes:
        holds a list of classes supplied during training
    _cls_freq_dist:
        holds a dictionary of Frequency Distributions,
        this structure is defined in probabilty.py in nltk
        this structure is indexed by class names and feature types
        the frequency distributions are indexed by feature values

    """

    def __init__(self, feature_detector):
        """
        @param feature_detector: feature detector produced function, which takes
        a sample of object to be classified (eg: string or list of words) and returns
        a list of tuples (feature_type_name, list of values of this feature type)
        """
        self._feature_detector = feature_detector
   
    def train(self, gold):
        """     
        Train classifier using representative examples of classes;
        creates frequency distributions of these classes
            
        @param gold: dictionary mapping class names to representative examples
        """
        self._classes = []
        self._cls_freq_dist = {}
        for cls in gold:
            self._classes.append(cls)
            for (fname, fvals) in self._feature_detector(gold[cls]):
                self._cls_freq_dist[cls, fname] = FreqDist()
                for fval in fvals:
                    self._cls_freq_dist[cls, fname].inc(fval)



    def get_class_dict(self, sample):
        """
        @type sample: (any)
        @param sample: sample to be classified
        @return: Dictionary (class to probability)
        """
        return self._cosine(sample)

    def _cosine(self, sample):
        """
        @param sample: sample to be classified
        @return: Dictionary class to probability
            
            function uses sample to create a frequency distribution
            cosine distance is computed between each of the class distribustions
            and the sample's distribution
        """
        sample_vector_len = 0
        dot_prod = {}
        score = {}

        sample_dist = {}

        for (fname, fvals) in self._feature_detector(sample):
            sample_dist[fname] = FreqDist()
            for fval in fvals:
                sample_dist[fname].inc(fval)
         
        for cls in self._classes:
            dot_prod[cls] = 0

        for fname in sample_dist:
            for fval in sample_dist[fname].samples():
                #calculate the length of the sample vector
                sample_vector_len += pow(sample_dist[fname].count(fval), 2)

                for cls in self._classes:
                    if fval in self._cls_freq_dist[cls, fname].samples():
                        #calculate the dot product of the sample to each class
                        dot_prod[cls] += sample_dist[fname].count(fval) * self._cls_freq_dist[cls,fname].count(fval)


        for cls in self._classes:
            cls_vector_len = 0
            for fname in sample_dist:
                for fval in self._cls_freq_dist[cls, fname].samples():
                    #calculate the length of the class vector
                    cls_vector_len += pow(self._cls_freq_dist[cls, fname].count(fval), 2)
            
            #calculate the final score for this class 
            if sample_vector_len == 0 or cls_vector_len == 0:
                score[cls] = 0
            else :
                score[cls] = float(dot_prod[cls]) / (sqrt(sample_vector_len) * sqrt(cls_vector_len))
            
        return score

    def __repr__(self):
        return '<CosineClassifier: classes=%d>' % len(self._classes)  

##//////////////////////////////////////////////////////
##  Demonstration code
##//////////////////////////////////////////////////////

def demo():
    from nltk_contrib import classify
    from nltk import detect
    
    fd = detect.feature({"1-tup": lambda t: [t[n] for n in range(len(t))]})

    classifier = classify.cosine.Cosine(fd)
    training_data = {"class a": "aaaaaab",
                      "class b": "bbbbbba"}
    classifier.train(training_data)

    result = classifier.get_class_dict("a")

    for cls in result:
        print cls, ':', result[cls]
    
    """
    expected values:
    class a: 'a' = 6
             'b' = 1
         vector = 6^2 + 1^2 = 37
      b: 'a' = 1
         'b' = 6
         vector = 1^2 + 6^2 = 37
    sample: 'a' = 1
            vector = 1^2 = 1
    
    dot_prod a: 6*1
             b: 1*1

    score a: 6 / (sqrt(37) * sqrt(1)) = 0.98~
    score b: 1 / (sqrt(37) * sqrt(1)) =  0.16~
    """

   


def demo2():
    from nltk_contrib import classify
    from nltk import detect
  
    fd = detect.feature({"2-tup": lambda t: [t[n:n+2] for n in range(len(t)-1)]})

    classifier = classify.Cosine(fd)
    training_data = {"class a": "aaaaaab",
                      "class b": "bbbbbba"}
    classifier.train(training_data)

    result = classifier.get_class_dict("aaababb")

    for cls in result:
        print cls, ':', result[cls]
    """
    expected values:
    class a: 'aa' = 5
             'ab' = 1
         vector = 5^2 + 1^2 = 26
      b: 'bb' = 5
         'ba' = 1
         vector = 5^2 + 1^2 = 26
    sample: 'aa' = 2
            'ab' = 2
            'ba' = 1
            'bb' = 1
            vector = 2^2 + 2^2 + 1^2 + 1^2 = 10
    
    dot_prod a: 5*2 + 1*2
             b: 5*1 + 1*1

    score a: 12 / (sqrt(26) * sqrt(10)) = 0.74~
    score b: 6 / (sqrt(26) * sqrt(10))  = 0.37~
    """
    


def demo3():
    from nltk_contrib import classify
    from nltk import detect
  
    fd = detect.feature({"1-tup": lambda t: [t[n] for n in range(len(t))],
                          "2-tup": lambda t: [t[n:n+2] for n in range(len(t)-1)]})

    classifier = classify.Cosine(fd)
    training_data = {"class a": "aaaaaab",
                      "class b": "bbbbbba"}
    classifier.train(training_data)

    result = classifier.get_class_dict("aaababb")

    for cls in result:
        print cls, ':', result[cls]

    """
    expected values:
    class a: 'a' = 6
             'b' = 1
             'aa' = 5
             'ab' = 1
         vector = 6^2 + 5^2 + 1 + 1 = 63
      b: 'a' = 1
         'b' = 6
         'bb' = 5
         'ba' = 1
         vector = 6^2 + 5^2 + 1 + 1 = 63
    sample: 'a' = 4
            'b' = 3
            'aa' = 2
            'ab' = 2
            'ba' = 1
            'bb' = 1
            vector = 4^2 + 3^2 + 2^2 + 2^2 + 1 + 1 = 35
    
    dot_prod a: 4*6 + 3*1 + 5*2 + 2*1 = 39
             b: 4*1 + 3*6 + 5*1 + 1*1 = 28

    score a: 39 / (sqrt(63) * sqrt(35)) = 0.83~
    score b: 28 / (sqrt(63) * sqrt(35)) = 0.59~
    """


def demo4():
    from nltk_contrib import classify
    from nltk import detect

    from nltk.corpora import genesis
    from itertools import islice

    fd = detect.feature({"2-tup": lambda t: [' '.join(t)[n:n+2] for n in range(len(' '.join(t))-1)],
                     "words": lambda t: t})

    classifier = classify.Cosine(fd)
    training_data = {}
    training_data["english-kjv"] = list(islice(genesis.raw("english-kjv"), 0, 400))
    training_data["french"] = list(islice(genesis.raw("french"), 0, 400))
    training_data["finnish"] = list(islice(genesis.raw("finnish"), 0, 400))

    classifier.train(training_data)

    result = classifier.get_class_probs(list(islice(genesis.raw("english-kjv"), 150, 200)))

    print 'english-kjv :', result.prob('english-kjv')
    print 'french :', result.prob('french')
    print 'finnish :', result.prob('finnish')

  
if __name__ == '__main__':
    demo2()

########NEW FILE########
__FILENAME__ = naivebayes
# Natural Language Toolkit: Naive Bayes Classifier
#
# Copyright (C) 2001-2007 NLTK Project
# Author: Edward Loper <edloper@gradient.cis.upenn.edu>
#         Sam Huston <shuston@csse.unimelb.edu.au>
#         Steven Bird <sb@csse.unimelb.edu.au>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT
#

"""
Naive Bayes Classifier -- Beta version
"""

from operator import itemgetter
from nltk.probability import *
from nltk_contrib.classify import *

class NaiveBayes(AbstractClassify):
    """
    The Naive Bayes Classifier is a supervised classifier.
    It needs to be trained with representative examples of 
    each class. From these examples the classifier
    calculates the most probable classification of the sample.

  
                          P(class) * P(features|class)
    P(class|features) =    -------------------------
                                  P(features)
    
    Internal data structures:
    _feature_dectector:
        holds a feature detector function
    _classes:
        holds a list of classes supplied during training
    _cls_prob_dist:
        hols a Probability Distribution, namely GoodTuringProbDist
        this structure is defined in probabilty.py in nltk
        this structure is indexed by classnames
    _feat_prob_dist:
        holds Conditional Probability Distribution, conditions are 
        class name, and feature type name
        these probability distributions are indexed by feature values
        this structure is defined in probabilty.py in nltk
    """

    def __init__(self, feature_detector):
        """
        @param feature_detector: feature detector produced function, which takes
        a sample of object to be classified (eg: string or list of words) and returns
        a list of tuples (feature_type_name, list of values of this feature type)
        """
        self._feature_detector = feature_detector

    def train(self, gold):
        """
        @param gold: dictionary of class names to representative examples
            function takes representative examples of classes
            then creates frequency distributions of these classes
            these freqdists are used to create probability distributions
        """
        cls_freq_dist = FreqDist()
        feat_freq_dist = ConditionalFreqDist()
        self._classes = []
        feature_values = {}

        for cls in gold:
            self._classes.append(cls)
            for (fname, fvals) in self._feature_detector(gold[cls]):
                for fval in fvals:
                    #increment number of tokens found in a particular class
                    cls_freq_dist.inc(cls)

                    #increment number of features found in (class, feature type)
                    feat_freq_dist[cls, fname].inc(fval)

                    #record that fname can be associated with this feature 
                    if fname not in feature_values: feature_values[fname] = set()
                    feature_values[fname].add(fval)

        # convert the frequency distributions to probability distribution for classes
        self._cls_prob_dist = GoodTuringProbDist(cls_freq_dist, cls_freq_dist.B())
        
        # for features
        def make_probdist(freqdist, (cls, fname)):
            return GoodTuringProbDist(freqdist, len(feature_values[fname]))
        self._feat_prob_dist = ConditionalProbDist(feat_freq_dist, make_probdist, True)
        
    def get_class_dict(self, sample):
        """
        @param sample: sample to be classified
        @ret: Dictionary (class to probability)
        """
        return self._naivebayes(sample)

    def _naivebayes(self, sample):
        """
        @param sample: sample to be tested
        @ret: Dictionary (class to probability)
        
            naivebayes classifier:
            creates a probability distribution based on sample string

            sums the log probabilities of each feature value 
                for each class and feature type
                and with the probability of the resepective class
        """
        sample_feats = self._feature_detector(sample)

        logprob_dict = {}
        score = {}
        for cls in self._classes:
            # start with the probability of each class
            logprob_dict[cls] = self._cls_prob_dist.prob(cls)
    
        for fname, fvals in sample_feats:
            for cls in self._classes:
                probdist = self._feat_prob_dist[cls, fname]
                for fval in fvals:
                    if fval in probdist.samples():
                        logprob_dict[cls] += probdist.logprob(fval)

        dicttmp = DictionaryProbDist(logprob_dict, normalize=True, log=True)
        for sample in dicttmp.samples():
            score[sample] = dicttmp.prob(sample) 
            
        return score

    def __repr__(self):
        return '<NaiveBayesClassifier: classes=%d>' % len(self._classes)  


##//////////////////////////////////////////////////////
##  Demonstration code
##//////////////////////////////////////////////////////


def demo():
    from nltk_contrib import classify
    from nltk import detect
  
    fd = detect.feature({"1-tup": lambda t: list(t)})

    classifier = classify.NaiveBayes(fd)
    training_data = {"class a": "aaaaaab",
                     "class b": "bbbbbba"}
    classifier.train(training_data)

    result = classifier.get_class_dict("a")

    for cls in result:
        print cls, ':', result[cls]
    
    """
    expected values:
    class_probs a = 0.5
                b = 0.5
    class a: 'a' = 6/7
             'b' = 1/7
      b: 'a' = 1/7
         'b' = 6/7
    sample: 'a' = 1
    
    score a: 0.5 * 6/7 = 0.42~
    score b: 0.5 * 1/7 = 0.07~
    """   


def demo2():
    from nltk_contrib import classify
    from nltk import detect
 
    fd = detect.feature({"2-tup": lambda t: [t[n:n+2] for n in range(len(t))]})

    classifier = classify.NaiveBayes(fd)
    training_data = {"class a": "aaaaaab",
                      "class b": "bbbbbba"}
    classifier.train(training_data)

    result = classifier.get_class_dict("aababb")

    for cls in result:
        print cls, ':', result[cls]
    """
    expected values:
    class_probs a = 0.5
                b = 0.5
    class a: 'aa' = 5/6
             'ab' = 1/6
          b: 'bb' = 5/6
             'ba' = 1/6
    sample: 'aa' = 2
            'ab' = 2
            'ba' = 1
            'bb' = 1
    
    score a: 0.5 * 5/6 * 5/6 * 1/6 * 1/6 = 0.09~
    score b: 0.5 * 5/6 * 1/6 = 0.06~
    """
    


def demo3():
    from nltk_contrib import classify
    from nltk import detect
  
    fd = detect.feature({"1-tup": lambda t: [t[n] for n in range(len(t))],
                          "2-tup": lambda t: [t[n:n+2] for n in range(len(t))]})

    classifier = classify.NaiveBayes(fd)
    training_data = {"class a": "aaaaaab",
                      "class b": "bbbbbba"}
    classifier.train(training_data)

    result = classifier.get_class_dict("aaababb")

    for cls in result:
        print cls, ':', result[cls]

    """
    expected values:
    class_probs a = 0.5
                b = 0.5
    class a: 'a' = 6/7
             'b' = 1/7
             'aa' = 5/6
             'ab' = 1/6
      b: 'a' = 1/7
         'b' = 6/7
         'bb' = 5/6
         'ba' = 1/6
    sample: 'a' = 4
            'b' = 3
            'aa' = 2
            'ab' = 2
        'ba' = 1
        'bb' = 1
    
    score a: 0.5 * 6/7^4 * 1/7^3 * 5/6^2 * 1/6^2 = 1.5 e-5
    score b: 0.5 * 1/7^4 * 6/7^3 * 5/6 * 1/6 = 0.0014~
    """

def demo4():
    from nltk_contrib import classify
    from nltk import detect

    from nltk.corpora import genesis
    from itertools import islice
  
    fd = detect.feature({"2-tup": lambda t: [' '.join(t)[n:n+2] for n in range(len(' '.join(t))-1)],
                     "words": lambda t: t})

    classifier = classify.NaiveBayes(fd)
    training_data = {}
    training_data["english-kjv"] = list(islice(genesis.raw("english-kjv"), 0, 400))
    training_data["french"] = list(islice(genesis.raw("french"), 0, 400))
    training_data["finnish"] = list(islice(genesis.raw("finnish"), 0, 400))

    classifier.train(training_data)

    result = classifier.get_class_probs(list(islice(genesis.raw("english-kjv"), 150, 200)))

    print 'english-kjv :', result.prob('english-kjv')
    print 'french :', result.prob('french')
    print 'finnish :', result.prob('finnish')

if __name__ == '__main__':
    demo2()

########NEW FILE########
__FILENAME__ = spearman
# Natural Language Toolkit: Spearman Rank Correlation classifier
#
# Copyright (C) 2001-2007 NLTK Project
# Author: Sam Huston <shuston@csse.unimelb.edu.au>
#         Steven Bird <sb@csse.unimelb.edu.au>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT
#

"""
Spearman Classifier -- Beta version
"""

from math import pow
from nltk.probability import *
from nltk_contrib.classify import *

class Spearman(AbstractClassify):
    """
    The Spearman-rho Classifier is a non-parametric measure of correlation between two
    sets of ranked values
    Spearman-rho classification is a supervised classifier. It needs to be trained
    with representative examples of each class. From these examples the classifier
    calculates the most probable classification of the sample.
  
                    6 * sum((Ai - Bi)^2)
    p = 1   -     -------------------------
                       (n^3) - n
                       
    where A, B are the vectors of ranked objects
             n is the number of ranked objects being compared
  
    Internal data structures:
    _feature_dectector:
        holds a feature detector function
    _classes:
        holds a list of classes supplied during trainning
    _cls_rank:
        holds a dictionary of ordered lists,   
        the order of the list is deturnmined by:
        first ranked object is ordered first
        duplicate values are ordered in alphabetical order
    """

    def __init__(self, feature_detector, crop_data=100):
        """
        @param feature_detector: feature detector produced function
        @param crop_data: ranking beyond which features are ignored
                          this produces a maximum rank for large data sets
                          where the lower order values would offset the results
        """
        self._feature_detector = feature_detector
        self._crop_data = crop_data
   

    def train(self, gold):
        """
        trains the classifier
        @param classes: dictionary of class names to representative examples

            function takes representative examples of classes
            then creates ordered lists of ranked features
            indexed by class name and feature type
        """

        self._classes = []
        self._cls_rank = {}

        for cls in gold:
            self._classes.append(cls)
            for (fname, fvals) in self._feature_detector(gold[cls]):
                cls_freq_dist = FreqDist()
                for fval in fvals:
                    cls_freq_dist.inc(fval)
                self._cls_rank[cls, fname] = self._get_ranks(cls_freq_dist)


    def get_class_dict(self, sample):
        """
        @param text: sample to be classified
        @ret: Dictionary (class to probability)
        """
        return self._spearman_ranked(sample)


    def _spearman_ranked(self, sample):
        """
        @param text: sample to be classified
        @ret: Dictionary class to probability
            
            function uses sample to create an ordered list of ranked features
            the spearman-rho formula is then applied to produce a correlation
            
            a union operation is used to create two lists of the same length for the formula
            missing values are appended to each list in alphabetical order
        """

        rank_diff = {}
        score = {}
        sample_rank = {}
        totalfvals = {}
        
        for (fname, fvals) in self._feature_detector(sample):
            sample_dist = FreqDist()
            for fval in fvals:
                sample_dist.inc(fval)
            sample_rank[fname] = self._get_ranks(sample_dist)
        

        for cls in self._classes:
            rank_diff[cls] = 0
            totalfvals[cls] = 0
    
        for fname in sample_rank:
            for cls in self._classes:
                tmp_clslist = self._cls_rank[cls, fname]
                tmp_smplist = sample_rank[fname]
                
                # take the union of the the class and sample lists, append each in alphabetical order
                
                tmp_clslist.extend(list(set(tmp_smplist).difference(set(tmp_clslist))))
                tmp_smplist.extend(list(set(tmp_clslist).difference(set(tmp_smplist))))            
                
                totalfvals[cls] += len(tmp_clslist)
                
                for fval in tmp_smplist:
                    rank_diff[cls] += pow(tmp_smplist.index(fval) - tmp_clslist.index(fval), 2)

        for cls in self._classes:
            score[cls] = 1 - (float(6 * rank_diff[cls]) / (pow(totalfvals[cls], 3) - totalfvals[cls]))

        return score



    def _get_ranks(self, sample_dist):
        """
        @param sample_dist: sample frequency distribution
        @ret: ordered list of features.
        """

        samples = sample_dist.samples()
        ordered_list = [item for (freq, item) in sorted([(sample_dist.count(item),item) for item in samples], reverse=True)]      
        return ordered_list[:self._crop_data] 
        
    def __repr__(self):
        return '<SpearmanClassifier: classes=%d>' % len(self._classes)          

###########################################################

def demo():
    from nltk_contrib import classify
    from nltk import detect
    
    fd = detect.feature({"1-tup": lambda t: [t[n] for n in range(len(t))]})

    classifier = classify.spearman.Spearman(fd)
    trainning_data = {"class a": "aaaaaab",
                      "class b": "bbbbbba"}
    classifier.train(trainning_data)

    result = classifier.get_class_dict("a")

    for cls in result:
        print cls, ':', result[cls]
    """
    expected values:
    class a: 'a' = 1
             'b' = 2
          b: 'a' = 2
             'b' = 1
    sample: 'a' = 1
   
    score a: 6*(0^2) / 8-2= 0
    score b: 6*(1^2) / 8-2 = 1
    """


def demo2():
    from nltk_contrib import classify
    from nltk import detect
  
    fd = detect.feature({"2-tup": lambda t: [t[n:n+2] for n in range(len(t)-1)]})

    classifier = classify.spearman.Spearman(fd)
    trainning_data = {"class a": "aaaaaab",
                      "class b": "bbbbbba"}
    classifier.train(trainning_data)

    result = classifier.get_class_dict("aaababb")

    for cls in result:
        print cls, ':', result[cls]
    """
    expected values:
    class a: 'aa' = 1
             'ab' = 2
          b: 'bb' = 1
             'ba' = 2
    sample: 'aa' = 1
            'ab' = 1
            'ba' = 2
            'bb' = 2
    

    score a: 1/ 1+0+1+5+5 = 0.5
    score b: 1/ 1+1+0+6+6 = 0.5
    """
    


def demo3():
    from nltk_contrib import classify
    from nltk import detect
  
    fd = detect.feature({"1-tup": lambda t: [t[n] for n in range(len(t))],
                          "2-tup": lambda t: [t[n:n+2] for n in range(len(t)-1)]})

    classifier = classify.spearman.Spearman(fd)
    trainning_data = {"class a": "aaaaaab",
                      "class b": "bbbbbba"}
    classifier.train(trainning_data)

    result = classifier.get_class_dict("aaababb")

    for cls in result:
        print cls, ':', result[cls]

    """
    expected values:
    class a: 'a' = 1
             'b' = 3
             'aa' = 2
             'ab' = 3
          b: 'a' = 3
             'b' = 1
             'bb' = 2
             'ba' = 3
    sample: 'a' = 1
            'b' = 2
            'aa' = 3
            'ab' = 3
            'ba' = 4
            'bb' = 4

    score a: 1/ 1+0+1+1+0+3+3 = 1/ 9
    score b: 1/ 1+2+1+2+1+4+4 = 1/ 15
    """


def demo4():
    from nltk_contrib import classify
    from nltk import detect

    from nltk.corpora import genesis
    from itertools import islice

    fd = detect.feature({"2-tup": lambda t: [' '.join(t)[n:n+2] for n in range(len(' '.join(t))-1)],
                     "words": lambda t: t})

    classifier = classify.spearman.Spearman(fd)
    training_data = {}
    training_data["english-kjv"] = list(islice(genesis.raw("english-kjv"), 0, 400))
    training_data["french"] = list(islice(genesis.raw("french"), 0, 400))
    training_data["finnish"] = list(islice(genesis.raw("finnish"), 0, 400))

    classifier.train(training_data)

    result = classifier.get_class_probs(list(islice(genesis.raw("english-kjv"), 150, 200)))

    print 'english-kjv :', result.prob('english-kjv')
    print 'french :', result.prob('french')
    print 'finnish :', result.prob('finnish')

  
if __name__ == '__main__':
    demo2()

########NEW FILE########
__FILENAME__ = combined
import math
import os

# tagger importing
from nltk import tag
from nltk.tag import SequentialBackoff
# work-around while marshal is not moved into standard tree
from nltk_contrib.marshal import MarshalDefault ; Default = MarshalDefault
from nltk_contrib.marshal import MarshalUnigram ; Unigram = MarshalUnigram
from nltk_contrib.marshal import MarshalAffix   ; Affix   = MarshalAffix
from nltk_contrib.marshal import MarshalNgram   ; Ngram   = MarshalNgram
from nltk_contrib.marshalbrill import *

class CombinedTagger (SequentialBackoff):
    def __init__ (self):
        self._tagger = []
        self._brill = None

    def _append_default (self, default_tag, verbose=False):
        self._tagger.append( Default(default_tag) )

    def _append_affix (self, a_len, w_len, train_sents, verbose=False):
        self._tagger.append( Affix(a_len, w_len, backoff=self._tagger[-1]) )
        self._tagger[-1].train([train_sents], verbose)

    def _append_unigram (self, train_sents, verbose=False):
        self._tagger.append( Unigram(backoff=self._tagger[-1]) )
        self._tagger[-1].train(train_sents, verbose)

    def _append_ngram (self, size, train_sents, verbose=False, cutoff_value=0.001):
        cutoff = math.floor(len(train_sents)*cutoff_value)
        self._tagger.append( Ngram(size, cutoff=cutoff, backoff=self._tagger[-1]) )
        self._tagger[-1].train([train_sents], verbose)
        
    def _append_brill (self, train_sents, max_rules, min_score=2, trace=0):
        templates = [
            SymmetricProximateTokensTemplate(ProximateTagsRule,  ( 1,  1)        ),
            SymmetricProximateTokensTemplate(ProximateTagsRule,  ( 2,  2)        ),
            SymmetricProximateTokensTemplate(ProximateTagsRule,  ( 1,  2)        ),
            SymmetricProximateTokensTemplate(ProximateTagsRule,  ( 1,  3)        ),
            SymmetricProximateTokensTemplate(ProximateWordsRule, ( 1,  1)        ),
            SymmetricProximateTokensTemplate(ProximateWordsRule, ( 2,  2)        ),
            SymmetricProximateTokensTemplate(ProximateWordsRule, ( 1,  2)        ),
            SymmetricProximateTokensTemplate(ProximateWordsRule, ( 1,  3)        ),
            ProximateTokensTemplate         (ProximateTagsRule,  (-1, -1), (1,1) ),
            ProximateTokensTemplate         (ProximateWordsRule, (-1, -1), (1,1) ),
            ]

        trainer = BrillTrainer(self._tagger[-1], templates, trace)
        self._brill = trainer.train(train_sents, max_rules, min_score)
        
    def marshal (self, basepath):
        # create the model files, one for each tagger (*.mod) plus a general one
        handler = file(os.path.join(basepath, "model.mrs"), "w")

        for index in range(len(self._tagger)):
            filename = os.path.join(basepath, "tagger%02d.mod" % index)
            handler.write("%s %s\n" % (self._tagger[index]._classname, filename) )
            self._tagger[index].marshal(filename)

        filename = os.path.join(basepath, "tagger%02d.mod" % (index+1))
        handler.write("%s %s\n" % (self._brill._classname, filename) )
        self._brill.marshal(filename)

        handler.close()

    def unmarshal (self, basepath):
        # clear taggers
        self._tagger = []
        self._brill = None

        # read model's configuration
        filename = os.path.join(basepath, "model.mrs")
        handler = file(filename, "r")
        model = handler.readlines()
        handler.close()
        model = [line[:-1] for line in model] # remove "\n"s
        model = [line for line in model if len(line) > 0] # remove empty lines

        # tagger by tagger
        for tagger in model:
            tagger_type, tagger_file = tagger.split(" ")
            if   tagger_type == "DefaultTagger":
                self._tagger.append( Default("") )
                self._tagger[-1].unmarshal(tagger_file)
            elif tagger_type == "AffixTagger":
                self._tagger.append( Affix(1, 2, backoff=self._tagger[-1]) )
                self._tagger[-1].unmarshal(tagger_file)
            elif tagger_type == "UnigramTagger":
                self._tagger.append( Unigram(backoff=self._tagger[-1]) )
                self._tagger[-1].unmarshal(tagger_file)
            elif tagger_type == "NgramTagger":
                self._tagger.append( Ngram(2, backoff=self._tagger[-1]) )
                self._tagger[-1].unmarshal(tagger_file)
            elif tagger_type == "BrillTagger":
                self._brill = Brill(self._tagger[-1], [])
                self._brill.unmarshal(tagger_file)
            else:
                 print "error, tagger type not recognized."

    def exemple_train (self, train_sents, verbose=False):
        self._append_default("N")

        self._append_affix(-2, 6, train_sents, verbose)
        self._append_affix(-3, 7, train_sents, verbose)
        self._append_affix(-4, 8, train_sents, verbose)
        self._append_affix(-5, 9, train_sents, verbose)

        self._append_unigram(train_sents, verbose)
        
        self._append_ngram(2, train_sents, verbose)
        
        self._append_brill(train_sents, 1, 2, trace=3)

    def tag_one (self, token):
        return self._tagger[-1].tag_one(token)

    def tag (self, tokens, verbose=False):
        return self._tagger[-1].tag(tokens, verbose)

def create_tagger (train_sents):
    ct = CombinedTagger()
#    ct.example_train(train_sents, True)
    ct.unmarshal("tresoldi")
    
    tokens = "Mauro viu o livro sobre a mesa".split()
    print list(ct.tag(tokens))

    # tests
    acc = tag.accuracy(ct, [train_sents])
    print 'Accuracy = %4.2f%%' % (100 * acc)

########NEW FILE########
__FILENAME__ = concord
# Natural Language Toolkit: Concordance System
#
# Copyright (C) 2005 University of Melbourne
# Author: Peter Spiller
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

from nltk.corpus import brown
from math import *
import re, string
from nltk.probability import *

class SentencesIndex(object):
    """Class implementing an index of a collection of sentences.

    Given a list of sentences, where each sentence is a list of words,
    this class generates an index of the list. Each word should be a (word, POS
    tag) pair. The index is stored as a dictionary, with the hashable items as
    keys and a list of (sentence number, word number) tuples as values. This
    class also generates a list of sentence lengths. 
    """
    
    def __init__(self, sentences):
        """ Constructor. Takes the list of sentences to index.

        @type sentences:    list
        @param sentences:   List of sentences to index. Sentences should be
                            lists of (string, string) pairs.
        """

        sentenceCount = 0
        self.index = {}
        self.lengths = []

        # for each sentence:
        for sentence in sentences:
            # add the sentences length to the list of sentence lengths
            self.lengths.append(len(sentence))
            wordCount = 0
            for word in sentence:
                self.index[word] = self.index.get(word, []) + [(sentenceCount, wordCount)]
                wordCount += 1
            sentenceCount += 1

    def getIndex(self):
        """ Returns the index dictionary.

        @rtype:     dictionary
        @returns:   The dictionary containing the index.
        """
        return self.index

    def getSentenceLengths(self):
        """ Returns the list of sentence lengths.

        Element 0 is the length of the first sentence, element 1 the second,
        etc.

        @rtype:     list
        @returns:   List of lengths of sentences.
        """
        return self.lengths

class IndexConcordance(object):
    """ Class that generates concordances from a list of sentences.

    Uses an index for efficiency. If a SentencesIndex object is provided,
    it will be used, otherwise one will be constructed from the list of
    sentences. When generating a concordance, the supplied regular expression
    is used to filter the list of words in the index. Any that match are looked
    up in the index, and their lists of (sentence number, word number) pairs are
    used to extract the correct amount of context from the sentences.

    Although this class also allows regular expressions to be specified for the
    left and right context, they are not used on the index. If only left/right
    regexps are provided, the class will essentially generate a concordance for
    every word in the corpus, then filter it with the regexps. This will not be
    very efficient and requires very large amounts of memory.

    @cvar SORT_WORD:    Constant for sorting by target word.
    @cvar SORT_POS:     Constant for sorting by target word's POS tag.
    @cvar SORT_NUM:     Constant for sorting by sentence number.
    @cvar SORT_RIGHT_CONTEXT:    Constant for sorting by the first word of the
    right context.
    """

    # constants for different types of sort
    
    SORT_WORD = 0
    SORT_POS = 1
    SORT_NUM = 2
    SORT_RIGHT_CONTEXT = 3
    
    def __init__(self, sentences, index=None):
        """ Constructor.

        Arguments:
        @type sentences:    list
        @param sentences:   List of sentences to create a concordance for.
                            Sentences should be lists of (string, string) pairs.
        @type index:        SentencesIndex
        @param index:     SentencesIndex object to use as an index. If this is
                            not provided, one will be generated.
        """
        
        self.sentences = sentences
        self.index = index
        # generate an index if one wasn't provided
        if self.index == None:
            self.index = SentencesIndex(self.sentences)

    def formatted(self, leftRegexp=None, middleRegexp=".*", rightRegexp=None,
            leftContextLength=3, rightContextLength=3, contextInSentences=False,
            contextChars=50, maxKeyLength=0, showWord=True,
            sort=0, showPOS=True, flipWordAndPOS=False, verbose=False):
        """Generates and displays keyword-in-context formatted concordance data.

        This is a convenience method that combines raw() and display()'s
        options. Unless you need raw output, this is probably the most useful
        method.
        
        @type leftRegexp:       string
        @param leftRegexp:    Regular expression applied to the left context
                                to filter output. Defaults to None.
        @type middleRegexp:     string
        @param middleRegexp:  Regular expression applied to target word to
                                filter output. Defaults to ".*" (ie everything).
        @type rightRegexp:      string
        @param rightRegexp:   Regular expression applied to the right context
                                to filter output. Defaults to None.
        @type leftContextLength:        number
        @param leftContextLength:     Length of left context. Defaults to 3.
        @type rightContextLength:       number
        @param rightContextLength:    Length of right context. Defaults to 3.
        @type contextInSentences:       number
        @param contextInSentences:    Determines whether the context lengths
        arguments are in words or sentences. If false, the context lengths
        are in words - a rightContextLength argument of 2 results in two
        words of right context. If true, a rightContextLength argument of 2
        results in a right context consisting of the portion of the target
        word's sentence to the right of the target, plus the two sentences
        to the right of that sentence. Defaults to False.
        @type contextChars      number
        @param contextChars:  Amount of context to show. If set to less than
                                0, does not limit amount of context shown
                                (may look ugly). Defaults to 55.
        @type maxKeyLength:     number
        @param maxKeyLength:  Max number of characters to show for the
                                target word. If 0 or less, this value is
                                calculated so as to fully show all target
                                words. Defaults to 0.
        @type showWord:         boolean
        @param showWord:      Whether to show words. Defaults to True.
        @type sort:             integer
        @param sort: Should be set to one the provided SORT constants. If
        SORT_WORD, the output is sorted on the target word. If SORT_POS, the
        output is sorted on the target word's POS tag. If SORT_NUM, the
        output is sorted by sentence number. If SORT_RIGHT_CONTEXT, the
        output is sorted on the first word of the right context. Defaults to
        SORT_WORD.
        @type showPOS:          boolean
        @param showPOS:       Whether to show POS tags. Defaults to True.
        @type flipWordAndPOS:   boolean
        @param flipWordAndPOS: If true, displays POS tags first instead of
            words (ie prints 'cc/and' instead of 'and/cc'). Defaults to False.
        @type verbose:          boolean
        @param verbose:       Displays some extra status information. Defaults
                                to False.
        """
            
        self.format(self.raw(leftRegexp, middleRegexp, rightRegexp, leftContextLength,
                rightContextLength, contextInSentences, sort, verbose), contextChars,
                maxKeyLength, showWord, showPOS, flipWordAndPOS, verbose)

    def raw(self, leftRegexp=None, middleRegexp=".*", rightRegexp=None,
            leftContextLength=3, rightContextLength=3, contextInSentences=False,
            sort=0, verbose=False):
        """ Generates and returns raw concordance data.

        Regular expressions supplied are evaluated over the appropriate part of
        each line of the concordance. For the purposes of evaluating the regexps,
        the lists of (word, POS tag) tuples are flattened into a space-separated
        list of word/POS tokens (ie the word followed by '/' followed by the POS
        tag). A regexp like '^must/.*' matches the word 'must' with any POS tag,
        while one like '.*/nn$' matches any word with a POS tag of 'nn'. All
        regexps are evaluated over lowercase versions of the text.

        @type leftRegexp:       string
        @param leftRegexp:    Regular expression applied to the left context
                                to filter output. Defaults to None.
        @type middleRegexp:     string
        @param middleRegexp:  Regular expression applied to target word to
                                filter output. Defaults to ".*" (ie everything).
        @type rightRegexp:      string
        @param rightRegexp:   Regular expression applied to the right context
                                to filter output. Defaults to None.
        @type leftContextLength:        number
        @param leftContextLength:     Length of left context. Defaults to 3.
        @type rightContextLength:       number
        @param rightContextLength:    Length of right context. Defaults to 3.
        @type contextInSentences:       number
        @param contextInSentences:    Determines whether the context lengths
            arguments are in words or sentences. If false, the context lengths
            are in words - a rightContextLength argument of 2 results in two
            words of right context. If true, a rightContextLength argument of 2
            results in a right context consisting of the portion of the target
            word's sentence to the right of the target, plus the two sentences
            to the right of that sentence. Defaults to False.
        @type sort:             integer
        @param sort: Should be set to one the provided SORT constants. If
            SORT_WORD, the output is sorted on the target word. If SORT_POS, the
            output is sorted on the target word's POS tag. If SORT_NUM, the
            output is sorted by sentence number. If SORT_RIGHT_CONTEXT, the
            output is sorted on the first word of the right context. Defaults to
            SORT_WORD.
        @type verbose:          boolean
        @param verbose:       Displays some extra status information. Defaults
                                to False.
        @rtype:     list
        @return:    Raw concordance ouput. Returned as a list of
                    ([left context], target word, [right context], target word
                    sentence number) tuples.
        """
        # compile the middle regexp.
        reg = re.compile(middleRegexp)

        if verbose:
            print "Matching the following target words:"
        wordLocs = []
        # get list of (sentence, word) pairs to get context for
        for item in self.index.getIndex().iteritems():
            if reg.match("/".join([item[0][0].lower(), item[0][1]])):
                if verbose:
                    print "/".join(item[0])
                wordLocs.append(item[1])
                
        print ""

        items = []
        # if context lengths are specified in words:
        if contextInSentences == False:
            # for each list of (sentence, word offset in sentence) pairs:
            for wordList in wordLocs:
                # for each (sentence, word offset in sentence) pair:
                for sentenceNum, offset in wordList:
                    # set pointers to the left- and rightmost sentences to be
                    # looked at to the sentence the target word is in
                    leftCorpusIndex = sentenceNum
                    rightCorpusIndex = sentenceNum
                    # number of words to include in the left context is
                    # initially everything in the sentence up to the target
                    leftLength = offset
                    # number of words to include in the left context is
                    # initially everything in the sentence after the target
                    rightLength = self.index.getSentenceLengths()[sentenceNum] - offset - 1

                    # while the length of the left context is less than what we
                    # need, keep decreasing the left corpus index (ie adding
                    # sentences to the left context).
                    while leftLength < leftContextLength:
                        leftCorpusIndex -= 1
                        # if the new corpus index would fall off the end of the
                        # list, stop at 0
                        if(leftCorpusIndex < 0):
                            leftCorpusIndex = 0
                            break
                        # adjust length and offset
                        leftLength += self.index.getSentenceLengths()[leftCorpusIndex]
                        offset += self.index.getSentenceLengths()[leftCorpusIndex]

                    # while the length of the right context is less than what we
                    # need, keep increasing the right corpus index (ie adding
                    # sentences to the right context).
                    while rightLength < rightContextLength:
                        rightCorpusIndex += 1
                        try:
                            rightLength += self.index.getSentenceLengths()[rightCorpusIndex]
                        # if the new corpus index falls off the end of the list,
                        # stop at the end
                        except IndexError:
                            rightCorpusIndex -= 1
                            break

                    # grab all sentences from the left to right corpus indices,
                    # then flatten them into a single list of words
                    sents = self.sentences[leftCorpusIndex:rightCorpusIndex+1]
                    words = []
                    for sentence in sents:
                        for word in sentence:
                            words.append(word)

                    # select the appropriate sections of context from the list
                    # of words
                    left = words[offset-leftContextLength:offset]
                    target = words[offset]
                    right = words[offset+1:offset+1+rightContextLength]
                    items.append((left, target, right, sentenceNum))
        # if context lengths are specified in sentences:
        else:
            # for each list of (sentence, word offset in sentence) pairs:
            for wordList in wordLocs:
                # for each list of (sentence, word offset in sentence) pairs:
                for sentenceNum, offset in wordList:
                    # set pointers to the left- and rightmost sentences to be
                    # looked at to the sentence the target word is in
                    leftCorpusIndex = sentenceNum
                    rightCorpusIndex = sentenceNum
                    # number of words to include in the left context is
                    # initially everything in the sentence up to the target
                    leftLength = offset
                    # number of words to include in the left context is
                    # initially everything in the sentence after the target
                    rightLength = self.index.getSentenceLengths()[sentenceNum] - offset - 1
                    # keep track of the number of sentences included in the
                    # left/right context
                    leftSents = 0;
                    rightSents = 0;

                    # while we don't have enough sentences in the left context,
                    # keep decreasing the left corpus index
                    while leftSents < leftContextLength:
                        leftCorpusIndex -= 1
                        # if the new corpus index would fall off the end of the
                        # list, stop at 0
                        if(leftCorpusIndex < 0):
                            leftCorpusIndex = 0
                            break
                        leftLength += self.index.getSentenceLengths()[leftCorpusIndex]
                        offset += self.index.getSentenceLengths()[leftCorpusIndex]
                        leftSents += 1

                    # while we don't have enough sentences in the right context,
                    # keep increasing the right corpus index
                    while rightSents < rightContextLength:
                        rightCorpusIndex += 1
                        try:
                            rightLength += self.index.getSentenceLengths()[rightCorpusIndex]
                            rightSents += 1
                        # if the new corpus index falls off the end of the list,
                        # stop at the end
                        except IndexError:
                            rightCorpusIndex -= 1
                            break

                    # grab all sentences from the left to right corpus indices,
                    # then flatten them into a single list of words
                    sents = self.sentences[leftCorpusIndex:rightCorpusIndex+1]
                    words = []
                    for sentence in sents:
                        for word in sentence:
                            words.append(word)

                    # select the appropriate sections of context from the list
                    # of words
                    left = words[0:offset]
                    target = words[offset]
                    right = words[offset+1:]
                    items.append((left, target, right, sentenceNum))

        if verbose:
            print "Found %d matches for target word..." % len(items)

        # sort the concordance
        if sort == self.SORT_WORD:
            if verbose:
                print "Sorting by target word..."
            items.sort(key=lambda i:i[1][0].lower())
        elif sort == self.SORT_POS:
            if verbose:
                print "Sorting by target word POS tag..."
            items.sort(key=lambda i:i[1][1].lower())
        elif sort == self.SORT_NUM:
            if verbose:
                print "Sorting by sentence number..."
            items.sort(key=lambda i:i[3])
        elif sort == self.SORT_RIGHT_CONTEXT:
            if verbose:
                print "Sorting by first word of right context..."
            items.sort(key=lambda i:i[2][0][0])

        # if any regular expressions have been given for the context, filter
        # the concordance using them
        filtered = []
        filterBool = False
        if leftRegexp != None or rightRegexp != None:
            filterBool = True
        if filterBool:    

            leftRe=None
            rightRe=None
            if leftRegexp != None:
                if verbose:
                    print "Filtering on left context..."
                leftRe = re.compile(leftRegexp)
            if rightRegexp != None:
                if verbose:
                    print "Filtering on right context..."
                rightRe = re.compile(rightRegexp)
            
            for item in items:
                if self._matches(item, leftRe, rightRe):
                    filtered.append(item)
    
        if filterBool:
            source = filtered
        else:
            source = items

        return source

    def format(self, source, contextChars=55, maxKeyLength=0, showWord=True,
            showPOS=True, flipWordAndPOS=False, verbose=False):
        """Formats raw concordance output produced by raw().

        Displays a concordance in keyword-in-context style format.

        @type source:   list
        @param source:  Raw concordance output to format. Expects a list of
        ([left context], target word, [right context], target
        word sentence number) tuples.
        @type contextChars      number
        @param contextChars:  Amount of context to show. If set to less than
        0, does not limit amount of context shown (may look ugly). Defaults to 55.
        @type maxKeyLength:     number
        @param maxKeyLength:  Max number of characters to show for the
        target word. If 0 or less, this value is
        calculated so as to fully show all target
        words. Defaults to 0.
        @type showWord:         boolean
        @param showWord:      Whether to show words. Defaults to True.
        @type showPOS:          boolean
        @param showPOS:       Whether to show POS tags. Defaults to True.
        @type flipWordAndPOS:   boolean
        @param flipWordAndPOS: If true, displays POS tags first instead of
        words (ie prints 'cc/and' instead of 'and/cc'). Defaults to False.
        @type verbose:          boolean
        @param verbose:       Displays some extra status information. Defaults
        to False.
        """

        # flatten lists of tokens into strings
        lines = []
        maxMiddleLength = -1

        # generate intermediate list of string tuples        
        for line in source:
            # flatten left context tokens into a single string, joining words
            # and their POS tag with a '/' (if both are shown).
            left = ""
            for item in line[0]:
                if item[0] == "" and item[1] == "":
                    left = ""
                elif showWord and (not showPOS):
                    left += item[0] + " "
                elif (not showWord) and showPOS:
                    left += item[1] + " "
                elif flipWordAndPOS:
                    left += item[1] + "/" + item[0] + " "
                else:      
                    left += "/".join(item) + " "

            # flatten target word into a single string, joining the word and
            # its POS tag with a '/' (if both are shown).
            if showWord and (not showPOS):
                middle = line[1][0]
            elif (not showWord) and showPOS:
                middle = line[1][1]
            elif flipWordAndPOS:
                middle = line[1][1] + "/" + line[1][0] + " "
            else:      
                middle = "/".join(line[1])
            
            if len(middle) > maxMiddleLength:
                maxMiddleLength = len(middle)

            # flatten right context tokens into a single string, joining words
            # and their POS tag with a '/' (if both are shown).        
            right = ""
            for item in line[2]:
                if item[0] == "" and item[1] == "":
                    right = ""
                elif showWord and (not showPOS):
                    right += item[0] + " "
                elif (not showWord) and showPOS:
                    right += item[1] + " "
                elif flipWordAndPOS:
                    right += item[1] + "/" + item[0] + " "
                else:      
                    right += "/".join(item) + " "

            num = line[3]

            lines.append((middle, left, right, num))

        # crop and justify strings to generate KWIC-format output
        count = 0
        for middle, left, right, num in lines:
            # calculate amount of left padding needed
            leftPaddingLength = contextChars - len(left)
            if leftPaddingLength < 0:
                leftPaddingLength = 0
            if len(left) > contextChars and contextChars > -1:
                left = left[-contextChars:]
            left = " "*leftPaddingLength + left
            if contextChars > -1:
                right = right[0:contextChars]
            
            # add sentence numbers
            left = str(num) + ": " + left[len(str(num))+2 : ]

            # calculate amount of middle padding needed
            if maxKeyLength > 0:
                maxMiddleLength = maxKeyLength
            lPad = int(ceil(max(maxMiddleLength - len(middle), 0) / 2.0))
            rPad = int(floor(max(maxMiddleLength - len(middle), 0) / 2.0))
            middle = " "*lPad + middle + " "*rPad
            
            print left + "| " + middle + " | " + right + " "
            count += 1
        
        if verbose:    
            print "\n" + repr(count) + " lines"

    def _matches(self, item, leftRe, rightRe):
        """ Private method that runs the given regexps over a raw concordance
        item and returns whether they match it.
        """
        left = item[0]
        right = item[2]

        # flatten left and right contexts
        leftString = ""
        for token in left:
            leftString += "/".join(token) + " "
        rightString = ""
        for token in right:
            rightString += "/".join(token) + " "    

        # see if regexps match    
        ok = True
        if leftRe != None and leftRe.match(leftString) == None:
            ok = False
        if rightRe != None and rightRe.match(rightString) == None:
            ok = False
                       
        if ok:                
            return True
        else:
            return False

class Aggregator(object):
    """ Class for aggregating and summarising corpus concordance data.

    This class allows one or more sets of concordance data to be summarised and
    displayed. This is useful for corpus linguistic tasks like counting the
    number of occurences of a particular word and its different POS tags in a
    given corpus, or comparing these frequencies across different corpora. It
    creates a FreqDist for each set of concordance data, counting how often each
    unique entry appears in it.

    An example of how to use this class to show the frequency of the five most
    common digrams of the form "must/md X/Y" in the Brown Corpus sections a
    and g::
    
        concA = IndexConcordance(brown.tagged_sents('a'))
        rawA = concA.raw(middleRegexp="^must/md$", leftContextLength=0, rightContextLength=1)
        concG = IndexConcordance(brown.tagged_sents('g'))
        rawG = concG.raw(middleRegexp="^must/md$", leftContextLength=0, rightContextLength=1)
        agg = Aggregator()
        agg.add(rawA, "Brown Corpus A")
        agg.add(rawG, "Brown Corpus G")
        agg.formatted(showFirstX=5)

        Output:

        Brown Corpus A
        ------------------------------
         must/md be/be          17
         must/md have/hv        5
         must/md not/*          3
         must/md play/vb        2
         must/md ''/''          1

        Brown Corpus G
        ------------------------------
         must/md be/be          38
         must/md have/hv        21
         must/md ,/,            6
         must/md not/*          5
         must/md always/rb      3
    """

    # text for 'other' row in output tables
    _OTHER_TEXT = "<OTHER>"
    # text for 'total' row in output tables
    _TOTAL_TEXT = "<TOTAL>"
    
    def __init__(self, inputList=None):
        """ Constructor.

        @type inputList:    list
        @param inputList: List of (raw concordance data, name) tuples to be
                            entered into the aggregator. Defaults to None.
        """
        self._outputSets = []
        if inputList != None:
            for (item, n) in inputList:
                self.add(item, name=n)

    def add(self, raw, name):
        """ Adds the given set of raw concordance output to the aggregator.

        @type raw:  list
        @param raw: Raw concordance data (produced by IndexConcordance.raw()).
                    Expects a list of ([left context], target word,
                    [right context], target word sentence number) tuples.
        @type name:     string
        @param name:    Name to associate with the set of data.
        """
        self._outputSets.append((raw, name));

    def remove(self, name):
        """ Removes all sets of raw concordance output with the given name.

        @type name:     string
        @param name:    Name of data set to remove.
        """
        for item in self._outputSets:
            if item[1] == name:
                self._outputSets.remove(item)

    def formatted(self, useWord=True, usePOS=True, normalise=False,
                  threshold=-1, showFirstX=-1, decimalPlaces=4,
                  countOther=False, showTotal=False):
        """ Displays formatted concordance summary information.

        This is a convenience method that combines raw() and display()'s
        options. Unless you need raw output, this is probably the most useful
        method.

        @type useWord:      boolean
        @param useWord:   Include the words in the count. Defaults to True.
        @type usePOS:       boolean
        @param usePOS:    Include the POS tags in the count. Defaults to
                            False.
        @type normalise:    boolean
        @param normalise: If true, normalises the frequencies for each set
            of concordance output by dividing each key's frequency by the total
            number of samples in that concordances's FreqDist. Allows easier
            comparison of results between data sets.  Care must be taken when
            combining this option with the threshold option, as any threshold
            of 1 or more will prevent any output being displayed. Defaults to
            False.
        @type threshold:    number
        @param threshold: Frequency display threshold. Results below this
            frequency will not be displayed. If less than 0, everything will be
            displayed. Defaults to -1.
        @type showFirstX:       number
        @param showFirstX:    Only show this many results, starting with the
            most frequent. If less than 0, everything will be displayed.
            Defaults to -1.
        @type decimalPlaces:    integer
        @param decimalPlaces: Number of decimal places of accuracy to
            display. Used when displaying non-integers with the normalise
            option. Defaults to 4.
        @type countOther:       boolean
        @param countOther:    If true, any samples not shown (due to their
            frequency being below the given thershold or because they were
            after the number of results specified by the showFirstX argument)
            will be combined into one sample. This sample's frequency is the
            sum of all unshown sample's frequencies. Defaults to False.
        @type showTotal:    boolean
        @param showTotal: If true, prints the sum of all frequencies (of
            the entire FreqDist, not just of the samples displayed.) Defaults
            to False.
        """
        
        output, maxKeyLength = self.raw(useWord, usePOS)
        self.format(output, maxKeyLength, threshold, showFirstX,
                decimalPlaces, normalise, countOther, showTotal)

    def raw(self, useWord=True, usePOS=True):
        """ Generates raw summary information.

        Creates a FreqDist for each set of concordance output and uses it to
        count the frequency of each line in it. The concordance output is
        flattened from lists of tokens to strings, as lists cannot be hashed.
        The list of FreqDists is returned, as well as the length of the longest
        string (used for formatted display).

        @type useWord:      boolean
        @param useWord:   Include the words in the count. Defaults to True.
        @type usePOS:       boolean
        @param usePOS:    Include the POS tags in the count. Defaults to
                            False.
        @rtype:     list, number
        @returns:   A list of (FreqDist, name) pairs, and the length of the
                    longest key in all the FreqDists.
        """

        output = []
        maxKeyLength = 0

        # for each set of raw concordance data:
        for (rawConcOutput, name) in self._outputSets:
            # initialise a FreqDist
            dist = FreqDist()
            # for each item in the raw concordance output:
            for (left, middle, right, num) in rawConcOutput:
                # flatten the lists of tokens so they can be hashed in
                # the FreqDist
                leftList = []
                for word in left:
                    if usePOS == False and useWord == True:
                        leftList.append(word[0].lower())
                    elif usePOS == True and useWord == False:
                        leftList.append(word[1].lower())
                    else:
                        leftList.append(word[0].lower() + "/" + word[1].lower())
                try:
                    if usePOS == False and useWord == True:
                        midString = middle[0].lower()
                    elif usePOS == True and useWord == False:
                        midString = middle[1].lower()
                    else:
                        midString = middle[0].lower() + "/" + middle[1].lower()
                except IndexError:
                    midString = ""

                rightList = []
                for word in right:
                    if usePOS == False and useWord == True:
                        rightList.append(word[0].lower())
                    elif usePOS == True and useWord == False:
                        rightList.append(word[1].lower())
                    else:
                        rightList.append(word[0].lower() + "/" + word[1].lower())

                # join the tokens together to form a key string
                key = string.join(leftList) + " " + midString + " " + string.join(rightList)
                # keep track of the longest key length
                if len(key) > maxKeyLength:
                    maxKeyLength = len(key)
                # increment the FreqDist's count for this key
                dist.inc(key)

            # add this FreqDist and name to the output
            output.append((dist, name))

        # return the output and maximum key length
        return output, maxKeyLength

    def format(self, output, maxKeyLength=20, threshold=-1, showFirstX=-1,
                decimalPlaces=4, normalise=False, countOther=False,
                showTotal=False):
        """ Displays concordance summary information.

        Formats and displays information produced by raw().

        @type output:   list
        @param output:  List of (FreqDist, name) pairs (as produced by raw()).
        @type maxKeyLength:     number
        @param maxKeyLength:  Length of longest key. Defaults to 20.
        @type normalise:    boolean
        @param normalise: If true, normalises the frequencies for each set
            of concordance output by dividing each key's frequency by the total
            number of samples in that concordances's FreqDist. Allows easier
            comparison of results between data sets.  Care must be taken when
            combining this option with the threshold option, as any threshold
            of 1 or more will prevent any output being displayed. Defaults to
            False.
        @type threshold:    number
        @param threshold: Frequency display threshold. Results below this
            frequency will not be displayed. If less than 0, everything will be
            displayed. Defaults to -1.
        @type showFirstX:       number
        @param showFirstX:    Only show this many results, starting with the
            most frequent. If less than 0, everything will be displayed.
            Defaults to -1.
        @type decimalPlaces:    integer
        @param decimalPlaces: Number of decimal places of accuracy to
            display. Used when displaying non-integers with the normalise
            option. Defaults to 4.
        @type countOther:       boolean
        @param countOther:    If true, any samples not shown (due to their
            frequency being below the given thershold or because they were
            after the number of results specified by the showFirstX argument)
            will be combined into one sample. This sample's frequency is the
            sum of all unshown sample's frequencies. Defaults to False.
        @type showTotal:    boolean
        @param showTotal: If true, prints the sum of all frequencies (of
            the entire FreqDist, not just of the samples displayed.) Defaults
            to False.
        """

        # for each FreqDist:
        for (dist, name) in output:
            x = 0
            other = 0
            total = 0
            print name
            print "-"*(maxKeyLength + 7)
            # for each key:
            for key in dist.keys():
                # keep track of how many samples shown, if using the showFirstX
                # option
                #if showFirstX > 0 and x >= showFirstX:
                #   break

                # get and format the sample's frequency
                if normalise:
                    count = 1.0 * dist[key] / dist.N()
                    countString = str(count)[0:decimalPlaces + 2]
                else:
                    count = dist[key]
                    countString = str(count)

                total += count

                # if the count is less than the threshold value, or we've
                # already shown X samples, add this sample's frequency to the
                # 'other' bin
                if count < threshold or (showFirstX > 0 and x >= showFirstX):
                    other += count
                else:
                    print key + " "*(maxKeyLength - len(key) + 1) + countString
                x += 1

            if countOther:
                if normalise:
                    count = 1.0 * other
                    countString = str(count)[0:decimalPlaces + 2]
                else:
                    count = other
                    countString = str(count)
                print self._OTHER_TEXT + " "*(maxKeyLength - len(self._OTHER_TEXT) + 1) + countString
            if showTotal:
                if normalise:
                    count = 1.0 * total
                    countString = str(count)[0:decimalPlaces + 2]
                else:
                    count = total
                    countString = str(count)
                print self._TOTAL_TEXT + " "*(maxKeyLength - len(self._TOTAL_TEXT) + 1) + countString
            print ""
            
def demo():
    """
    Demonstrates how to use IndexConcordance and Aggregator.
    """
    print "Reading Brown Corpus into memory..."
    corpus = brown.tagged_sents('a')
    print "Generating index..."
    ic = IndexConcordance(corpus)
    print "Showing all occurences of 'plasma' in the Brown Corpus..."
    ic.formatted(middleRegexp="^plasma/.*", verbose=True)

    print "Investigating the collocates of 'deal' and derivatives..."
    agg = Aggregator()
    agg.add(ic.raw(middleRegexp="^deal", leftContextLength=1, rightContextLength=0,
    leftRegexp="^(\w|\s|/)*$"), "Brown Corpus 'deal' left collocates")
    agg.add(ic.raw(middleRegexp="^deal", leftContextLength=0, rightContextLength=1,
    rightRegexp="^(\w|\s|/)*$"), "Brown Corpus 'deal' right collocates")
    agg.formatted(showFirstX=5, usePOS=False)

if __name__ == '__main__':
    demo()    

########NEW FILE########
__FILENAME__ = ace2
# Natural Language Toolkit (NLTK) ACE-2 Corpus Reader
#
# Copyright (C) 2001-2011 NLTK Project 
# Author: Joseph Frazee <jfrazee@mail.utexas.edu>
# URL: http://nltk.org
# For license information, see LICENSE.TXT

"""
Corpus reader for the ACE-2 Corpus.

"""

from sgmllib import SGMLParser

from nltk.corpus.reader.api import *
from nltk.corpus.reader.util import *

from nltk.tokenize.punkt import *
from nltk.tokenize.regexp import *
from nltk.tokenize.simple import *


class ACE2CorpusReader(CorpusReader):
    """
    """

    def __init__(self, root, files):
        CorpusReader.__init__(self, root, files)

    def words(self, files=None):
        """
        """
        return concat([ACE2SourceDocument(filename).words()
                       for filename in self.abspaths(files)])

    def sents(self, files=None):
        """
        """
        return concat([ACE2SourceDocument(filename).sents()
                       for filename in self.abspaths(files)])


class ACE2SourceDocument(str):
    """
    """

    def __init__(self, path):
        """
        """
        str.__init__(self, path)
        self._sgmldoc_ = None

    def _sgmldoc(self):
        """
        """
        if self._sgmldoc_:
            return self._sgmldoc_
        self._sgmldoc_ = ACE2SGMLParser().parse(self)
        return self._sgmldoc_
        
    def _sent_tokenizer(self):
        """
        """
        if self.docsource() == 'broadcast news':
            return PunktSentenceTokenizer()
        if self.docsource() == 'newswire':
            return PunktSentenceTokenizer()
        if self.docsource() == 'newspaper':
            return PunktSentenceTokenizer()
        raise

    def _word_tokenizer(self):
        """
        """
        if self.docsource() == 'broadcast news':
            return PunktWordTokenizer()
        if self.docsource() == 'newswire':
            return PunktWordTokenizer()
        if self.docsource() == 'newspaper':
            return PunktWordTokenizer()
        raise

    def words(self):
        """
        """
        result = []
        for sent in self.sents():
            result.extend(sent)
        assert None not in result
        return result

    def sents(self):
        """
        """
        result = []
        for sent in self._sent_tokenizer().tokenize(self._sgmldoc().text()):
            result.append(self._word_tokenizer().tokenize(sent))
        assert None not in result
        return result

    def docno(self):
        """
        """
        result = self._sgmldoc().docno()
        assert result
        return result

    def doctype(self):
        """
        """
        result = self._sgmldoc().doctype()
        assert result
        return result

    def docsource(self):
        """
        """
        result = self._sgmldoc().docsource()
        assert result
        return result


class ACE2SGMLParser(SGMLParser):
    """
    """

    def __init__(self):
        """
        """
        SGMLParser.__init__(self)

    def reset(self):
        """
        """
        SGMLParser.reset(self)
        self._parsed = False
        self._text = None
        self._docno = None
        self._doctype = None
        self._docsource = None
        self._in = []

    def start_doc(self, attrs):
        """
        """
        self._in.insert(0, 'doc')

    def end_doc(self):
        """
        """
        self._in.remove('doc')

    def start_text(self, attrs):
        """
        """
        self._in.insert(0, 'text')
        self._text = ''

    def end_text(self):
        """
        """
        self._in.remove('text')
        self._text = self._text.strip()

    def start_docno(self, attrs):
        """
        """
        self._in.insert(0, 'docno')
        self._docno = ''

    def end_docno(self):
        """
        """
        self._in.remove('docno')
        self._docno = self._docno.strip()

    def start_doctype(self, attrs):
        """
        """
        self._in.insert(0, 'doctype')
        self._doctype = ''
        self._docsource = ''
        for k, v in attrs:
            if k == 'source':
                self._docsource = v 
                break

    def end_doctype(self):
        """
        """
        self._in.remove('doctype')
        self._doctype = self._doctype.strip()

    def handle_data(self, data):
        """
        """
        if self._in and self._in[0] == 'text':
            self._text += data
        if self._in and self._in[0] == 'docno':
            self._docno += data
        if self._in and self._in[0] == 'doctype':
            self._doctype += data

    def parse(self, filename):
        """
        """
        file = open(filename)
        for line in file:
            self.feed(line)
        file.close()
        self._parsed = True
        return self

    def text(self):
        """
        """
        assert self._parsed and self._text
        return self._text

    def docno(self):
        """
        """
        assert self._parsed and self._docno
        return self._docno

    def doctype(self):
        """
        """
        assert self._parsed and self._doctype
        return self._doctype

    def docsource(self):
        """
        """
        assert self._parsed and self._docsource
        return self._docsource


def _demo(root, file):
    """
    """
    from nltk_contrib.coref.ace2 import ACE2CorpusReader

    try:
        reader = ACE2CorpusReader(root, file)
        print 'Sentences for %s:' % (file)
        for sent in reader.sents():
            print '    %s' % (sent)
        print
        print 'Words for %s:' % (file)
        for word in reader.words():
            print '    %s' % (word)
        print
    except Exception, e:
        print 'Error encountered while running demo for %s: %s' % (file, e)
        print

def demo():
    """
    """
    import os
    import os.path

    try:
        ace2_dir = os.environ['ACE2_DIR']
    except KeyError:
        raise 'Demo requires ACE-2 Corpus, set ACE2_DIR environment variable!' 

    bnews_dir = os.path.join(ace2_dir, 'data/ace2_train/', 'bnews')
    _demo(bnews_dir, 'ABC19980106.1830.0029.sgm')

    npaper_dir = os.path.join(ace2_dir, 'data/ace2_train/', 'npaper')
    _demo(npaper_dir, '9801.139.sgm')

    nwire_dir = os.path.join(ace2_dir, 'data/ace2_train/', 'nwire')
    _demo(nwire_dir, 'APW19980213.1302.sgm')

if __name__ == '__main__':
    demo()

########NEW FILE########
__FILENAME__ = api
# Natural Language Toolkit (NLTK) Coreference API
#
# Copyright (C) 2001-2011 NLTK Project 
# Author: Joseph Frazee <jfrazee@mail.utexas.edu>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

import os

from nltk.corpus import CorpusReader
from nltk.tokenize.punkt import PunktWordTokenizer
from nltk.tag import TaggerI, ClassifierBasedTagger                                          
from nltk.tag.hmm import HiddenMarkovModelTaggerTransformI
from nltk.classify import ClassifierI
from nltk.util import LazyMap

from nltk.chunk import ChunkParserI, ChunkScore

# Location of coref data outside of NLTK_DATA
NLTK_COREF_DATA = os.path.dirname(__file__) + '/data'

class TrainableI(object):
    """
    An interface for trainable classes.
    
    Subclasses must define L{train()}.
    """
    def __init__(self):
        if self.__class__ == TrainableI:
            raise AssertionError, "Interfaces can't be instantiated" 

    def train(self, labeled_sequence, test_sequence=None, 
              unlabeled_sequence=None, **kwargs):
        """
        Train a subclass instance of C{TrainableI}.
        
        @return: a subclass instance of C{TrainableI}
        @rtype: C{TrainableI}
        @param labeled_sequence: a C{list} of labeled training instances.
        @type labeled_sequence: C{list}
        @param test_sequence: a C{list} of labeled test instances.
        @type test_sequence: C{list}
        @param unlabeled_sequence: a C{list} of unlabeled training instances.
            An unlabeled sequence is useful for EM-like unsupervised
            training algorithms.
        @type unlabeled_sequence: C{list}
        @param kwargs: additional arguments to L{train()}.
        @type kwargs: C{dict}
        """
        raise AssertionError()
        
        
class HiddenMarkovModelChunkTaggerTransformI(HiddenMarkovModelTaggerTransformI):
    # Inherit the superclass documentation.
    def __init__(self):
        if self.__class__ == HiddenMarkovModelChunkTaggerTransformI:
            raise AssertionError, "Interfaces can't be instantiated"
    
    def path2tags(self, path):
        """
        Transform a viterbi/tag sequence of (word, tag) into a list of tags.
        
        @return: a C{list} of tags.
        @rtype: C{list}
        @param path: a C{list} of (word, tag) pairs.
        @type path: C{list} of C{tuple}
        """
        raise AssertionError()


class CorpusReaderDecoratorI(CorpusReader):
    """
    An interface for C{CorpusReader} decorators.  Instances of
    C{CorpusReaderDecoratorI} are useful for providing C{CorpusReader}
    instances with additional features.  For example, a tagging
    C{CorpusReaderDecoratorI} could add tagged_words() or tagged_sents()
    methods to the C{CorpusReader} of an untagged corpus.
    """
    def __init__(self):
        if self.__class__ == CorpusReaderDecorator:
            raise AssertionError, "Interfaces can't be instantiated"

    def reader(self):
        """
        Return the underlying C{CorpusReader} instance.
        
        @return: the underlying C{CorpusReader} instance
        @rtype: C{CorpusReader}
        """
        raise AssertionError()


class CorpusReaderDecorator(CorpusReaderDecoratorI):
    def __init__(self, reader, **kwargs):
        self._reader = reader

    def __getattr__(self, name):
        if name != '_reader':
            return getattr(self._reader, name)

    def reader(self):
        wrapped_reader = self._reader
        while isinstance(wrapped_reader, CorpusReaderDecoratorI):
            wrapped_reader = wrapped_reader.reader()
        return wrapped_reader


class NamedEntityI(str):
    IN = 'I'
    OUT = 'O'
    BEGINS = 'B'

    def __new__(self, s, **kwargs):
        return str.__new__(self, s)
            
    def __init__(self, s, **kwargs):
        if self.__class__ == NamedEntityI:
            raise AssertionError, "Interfaces can't be instantiated"
        self._iob_tag = kwargs.get('iob_tag', self.BEGINS)
    
    def iob_in(self):
        return self._iob_tag == self.IN

    def iob_begins(self):
        return self._iob_tag == self.BEGINS
    
    def iob_out(self):
        return self._iob_tag == self.OUT
        
    def iob_tag(self):
        return self._iob_tag

    def ne_type(self):
        return self.__class__.__name__.upper()

    def ne_tag(self):
        return '%s-%s' % (self.iob_tag(), self.ne_type())
    
    def split(self, sep=None, maxsplit=-1):
        if not sep and maxsplit == -1:
            tokens = PunktWordTokenizer().tokenize(self)
        else:
            tokens = str.split(self, sep, maxsplit)
        result = []            
        for (index, token) in enumerate(tokens):
            if self.iob_out():
                iob_tag = self.OUT
            elif self.iob_begins() and index == 0:
                iob_tag = self.BEGINS
            else:
                iob_tag = self.IN
            result.append(self.__class__(token, iob_tag=iob_tag))            
        return result


class ChunkTaggerI(TaggerI):
    """
    An interface for a chunk tagger class.
    """
    def __init__(self):
        if self.__class__ == ChunkTaggerI:
            raise AssertionError, "Interfaces can't be instantiated"

        
      
class CorefResolverI(object):
    """
    An interface for coreference resolvers.  Coreference resolvers identify
    and label co-referring L{mentions()} from a list of sentences.  Instances 
    of C{CorefResolverI} must implement L{mentions()}, L{resolve_mention()},
    L{resolve_mentions()}, and L{resolve()}.
    """
    def __init__(self):
        if self.__class__ == CorefResolverI:
            raise AssertionError, "Interfaces can't be instantiated"
    
    def mentions(self, sentences):
        """
        Identify the mentions from a list of sentences.
        
        For a list of sentences consisting of words [[w1, w2, w3], [w4, w5, w6]]
        where w2, w4, and w6 are mentions, L{mentions()} will yield the indexed
        list [(w2, 1, 0, 2), (w4, 2, 1, 0), (w6, 3, 1, 2)] which contains 
        4-tuples of the form (mention, mention id, sentence index, chunk index).
        
        @return: a C{list} of mentions.
        @rtype: C{list} of C{tuple}
        @param sentences: a C{list} of C{list} corresponding to a list of
            sentences.
        @type sentences: C{list} of C{list} of C{str} or C{tuple}
        """
        raise AssertionError()
        
    def resolve_mention(self, mentions, index, history):
        """
        Identify the coreferent, if any, for a mention in the mentions list.
        
        For a list of mentions consisting of 4-tuples of the form
        (mention, mention id, sentence, index, chunk index), 
        L{resolve_mention()} will yield the single coreferent 4-tuple for the
        mention located at index in the mentions list.
        
        @return: a C{tuple} of mention, mention id, sentence id, and chunk id.
        @rtype: C{tuple}
        @param mentions: a C{list} of mentions.
        @type mentions: C{list} of C{tuple}
        @param index: the index of the mention to resolve.
        @type index: C{int}
        @param history: the C{list} of previous (or future) mentions that can
            serve as coreferents.
        @type history: C{list} of C{tuple}
        """
        raise AssertionError()
    
    def resolve_mentions(self, mentions):
        """
        Identify co-referring discourse mentions from an indexed list of
        mentions.
        
        For a list of mentions [(w2, 1, 0, 2), (w4, 2, 1, 0), (w6, 3, 1, 2)],
        L{resolve_mentions()} will yield 
        [(w2, 1, 0, 2), (w4, 2, 1, 0), (w6, 1, 1, 2)] 
        iff. w2 and w6 co-refer and w4 does not co-refer with either w2 or w6.
        In other words, L{resolve_mentions()} indexes the indexed list of
        mentions so that each co-referring mention contains a matching index
        in the second element of its 4-tuple and non-co-referring mentions.
        
        @return: a C{list} of resolved mentions.
        @rtype: C{list} of C{tuple}
        @param mentions: a C{list} of mentions.
        @type mentions: C{list} of C{tuple}
        """
        raise AssertionError()
    
    def resolve(self, sentences):
        """
        Identify and resolve co-referring discourse mentions from a discourse
        or sequence of text.
        
        For a discourse consisting of words [[w1, w2, w3], [w4, w5, w6]] where
        w2, w4, and w6 are mentions and w2 and w6 co-refer L{resolve()} will
        yield the indexed list 
        [[(w1, None), (w2, 1), (w3, None)], [(w4, None), (w5, None), (w6, 1)]].
        
        @return: a C{list} of C{list} corresponding to a list of coreference
            resolved sentences.
        @rtype: C{list} of C{tuple}
        @param sentences: a C{list} of C{list} corresponding to a list of 
            sentences.
        @type sentences: C{list} of C{list} of C{str} or C{tuple}
        """
        raise AssertionError()


class ChunkTaggerI(TaggerI, ChunkParserI):
    def __init__(self):
        if self.__class__ == ChunkTaggerI: 
            raise AssertionError, "Interfaces can't be instantiated"

    def tag(self, sent):
        """
        Returns an IOB tagged sentence.

        @return: a C{list} of IOB-tagged tokens.
        @rtype: C{list} or C{tuple}
        @param sent: a C{list} of tokens
        @type sent: C{list} of C{str} or C{tuple}
        """
        raise NotImplementedError()
        
    def parse(self, sent):
        raise NotImplementedError()    

    def chunk(self, sent):
        """
        Returns a chunked sentence.

        @return: a C{list} of chunked tokens.
        @rtype: C{list} of C{list} or C{tuple}
        @param sent: a C{list} of tokens
        @type sent: C{list} of C{str} or C{tuple}
        """
        raise NotImplementedError()
                
    def test(self, iob_sents):
        raise NotImplementedError()

    @classmethod
    def train(cls, iob_sents, **kwargs):
        raise NotImplementedError()


class AbstractClassifierBasedTagger(ClassifierBasedTagger, ClassifierI, 
                                    TrainableI):
    """
    An abstract classifier-based tagger that simplifies implementing
    task-specific taggers. Subclasses of C{AbstractClassifierBasedTagger}
    can be trained on arbitrary sequences of tokens. L{test()} must be
    provided by the subclass.
    """
    def __init__(self, feature_detector, labeled_sequence, classifier_builder):
        """
        @param feature_detector: the function or dictionary used to featurize
            the training data.
        @type feature_dector: C{dict} or C{function}
        @param labeled_sequence: the list of training tokens
        @type labeled_sequence: C{list} of C{list} of C{tuple}
        @param classifier_builder: the function used to initialize the
            classifier
        @type classifier_builder: C{function}
        """
        if self.__class__ == AbstractClassifierBasedTagger:
            raise AssertionError, "Interfaces can't be instantiated"
        ClassifierBasedTagger.__init__(self, feature_detector, 
                                       labeled_sequence, classifier_builder)
    
    def __getattr__(self, name):
        if name != '_classifier':
            return getattr(self._classifier, name)

    @classmethod
    def _flatten(cls, tokens):
        return [token[0] + (token[1],) for token in tokens]

    @classmethod
    def _unflatten(cls, tokens):
        return [(token[:-1], token[-1]) for token in tokens]

    def batch_classify(self, featuresets):
        return self._classifier.batch_classify(featuresets)

    def batch_prob_classify(self, featuresets):
        return self._classifier.batch_prob_classify(featuresets)    

    def labels(self):
        return self._classifier.labels()

    def test(self, test_sequence, **kwargs):
        """
        Test the classifier object against a list of test tokens.
        
        @param test_sequence: a list of test tokens.
        @type test_sequence: C{list} of C{tuple}
        """
        raise AssertionError()

    @classmethod
    def train(cls, labeled_sequence, test_sequence=None,
                   unlabeled_sequence=None, **kwargs):
        classifier = \
            cls(kwargs.get('feature_detector'), 
                LazyMap(cls._unflatten, labeled_sequence), 
                kwargs.get('classifier_builder'))
        if test_sequence:
            classifier.test(test_sequence)
        return classifier


########NEW FILE########
__FILENAME__ = chunk
# Natural Language Toolkit (NLTK) Coreference Chunking Utilities
#
# Copyright (C) 2001-2011 NLTK Project 
# Author: Joseph Frazee <jfrazee@mail.utexas.edu>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

import re
import os
import sys

import nltk
from nltk.util import LazyMap
from nltk.data import BufferedGzipFile
from nltk.tree import Tree
from nltk.classify import NaiveBayesClassifier, MaxentClassifier
from nltk.tag import ClassifierBasedTagger
from nltk.tag.crf import MalletCRF
from nltk.chunk import ChunkScore

from nltk.corpus import stopwords

from nltk_contrib.coref import ChunkTaggerI

STOPWORDS = set(stopwords.words())

RE_PUNCT = re.compile(r'[-!"#$%&\'\(\)\*\+,\./:;<=>^\?@\[\]\\\_`{\|}~]')
RE_NUMERIC = re.compile(r'(\d{1,3}(\,\d{3})*|\d+)(\.\d+)?')

class ChunkTaggerFeatureDetector(dict):
    """
    A simple feature detector for training a C{ChunkTagger}.
    """
    def __init__(self, tokens, index=0, history=None, **kwargs):
        """
        @param tokens: a list of tokens containing a token to featurize.
        @type tokens: C{list} of C{tuple}
        @param index: the list position of the token to featurize.
        @type index: C{int}
        @param history: the previous features and classifier predictions
        @type history: C{list} of C{dict}
        @kwparam window: the number of previous/next tokens to include in the
            features
        @type window: C{int}
        """
        dict.__init__(self)
        
        window = kwargs.get('window', 2)
        # TODO: This will tag (X, Y, Z) to ((X, Y, Z), W) as well as (X, Y) to
        # ((X, Y), W). Do we want this?
        spelling, pos = tokens[index][:2]
        
        self['spelling'] = spelling
        self['word'] = spelling.lower()
        if pos: self['pos'] = pos
        self['isupper'] = int(spelling.isupper())
        self['islower'] = int(spelling.islower())
        self['istitle'] = int(spelling.istitle())
        self['isalnum'] = int(spelling.isalnum())
        
        for i in range(2, 4):
            self['prefix_%d' % i] = spelling.lower()[:i]
            self['suffix_%d' % i] = spelling.lower()[-i:]
        
        self['ispunct'] = int(bool(RE_PUNCT.match(spelling)))
        self['isstopword'] = int(spelling.lower() in STOPWORDS)        
        self['isnumeric'] = int(bool(RE_NUMERIC.match(spelling)))       
        self['startofsent'] = int(index == 0)         
        self['endofsent'] = int(index == len(tokens) - 1)
                    
        if window > 0 and index > 0:
            prev_feats = \
                self.__class__(tokens, index - 1, history, window=window - 1)
            for key, val in prev_feats.items():
                if not key.startswith('next_') and key != 'word':
                    self['prev_%s' % key] = val
        
        if window > 0 and index < len(tokens) - 1:
            next_feats = self.__class__(tokens, index + 1, window=window - 1)
            for key, val in next_feats.items():
                if not key.startswith('prev_') and key != 'word':
                    self['next_%s' % key] = val
        
        if 'prev_pos' in self:
            self['prev_pos_pair'] = '%s/%s' % \
                (self.get('prev_pos'), self.get('pos'))
        
        if history is not None:
            if len(history) > 0 and index > 0:
                self['prev_tag'] = history[index - 1]
            else:
                self['prev_tag'] = 'O'


class AbstractChunkTagger(ChunkTaggerI):
    chunk_types = None
    
    def parse(self, sent):
        return self.__iob2tree(self.tag(sent))
        
    def batch_parse(self, sents):
        return map(self.__iob2tree, self.batch_tag(sents))
    
    def chunk(self, sent):
        return self.__tree2chunks(self.parse(sent))
        
    def batch_chunk(self, sents):
        return map(self.__tree2chunks, self.batch_parse(sents))
        
    def __iob2tree(self, tagged_sent):
        return tokens2tree(map(flatten, tagged_sent), self.chunk_types)
        
    def __tree2chunks(self, tree):
        chunks = []
        for chunk in tree:
            if isinstance(chunk, Tree):
                chunks.append(chunk.leaves())
            elif isinstance(chunk, tuple):
                chunks.append(chunk)
            else:
                raise
        return chunks    
        
    def test(self, iob_sents, **kwargs):
        return test_chunk_tagger(self, iob_sents,
            chunk_types=self.chunk_types,
            verbose=kwargs.get('verbose', False))


class NaiveBayesChunkTagger(ClassifierBasedTagger, AbstractChunkTagger):
    @classmethod
    def train(cls, iob_sents, **kwargs):  
        fd = kwargs.get('feature_detector', ChunkTaggerFeatureDetector)
        chunk_types = kwargs.get('chunk_types', _DEFAULT_CHUNK_TYPES)        
        train = LazyMap(lambda sent: map(unflatten, sent), iob_sents)
        chunker = cls(fd, train, NaiveBayesClassifier.train)
        chunker.chunk_types = chunk_types
        return chunker
        

class MaxentChunkTagger(ClassifierBasedTagger, AbstractChunkTagger):
    @classmethod
    def train(cls, iob_sents, **kwargs):
        fd = kwargs.get('feature_detector', ChunkTaggerFeatureDetector)        
        chunk_types = kwargs.get('chunk_types', _DEFAULT_CHUNK_TYPES)
                
        algorithm = kwargs.get('algorithm', 'megam')
        gaussian_prior_sigma = kwargs.get('gaussian_prior_sigma', 100)
        count_cutoff = kwargs.get('count_cutoff', 1)
        min_lldelta = kwargs.get('min_lldelta', 1e-7)
        trace = int(not kwargs.get('verbose', False) or 3)
        
        def __maxent_train(fs):
            return MaxentClassifier.train(fs, 
                algorithm=algorithm,
                gaussian_prior_sigma=gaussian_prior_sigma,
                count_cutoff=count_cutoff,
                min_lldelta=min_lldelta,
                trace=trace)
        train = LazyMap(lambda sent: map(unflatten, sent), iob_sents)
        chunker = cls(fd, train, __maxent_train)
        chunker.chunk_types = chunk_types
        return chunker


class CRFChunkTagger(MalletCRF, AbstractChunkTagger):
    def tag(self, sent):
        return self.batch_tag([sent])[0]
    
    @classmethod
    def train(cls, iob_sents, **kwargs):
        fd = kwargs.get('feature_detector', ChunkTaggerFeatureDetector)        
        chunk_types = kwargs.get('chunk_types', _DEFAULT_CHUNK_TYPES)
                
        gaussian_variance = kwargs.get('gaussian_variance', 10)
        default_label = kwargs.get('default_label', 'O')
        transduction_type = kwargs.get('transduction_type', 'VITERBI_FBEAMKL')
        if kwargs.get('trace'):
            trace = kwargs.get('trace', 2)
        elif kwargs.get('verbose'):
            trace = 3
        else:
            trace = 0
        
        train = LazyMap(lambda sent: map(unflatten, sent), iob_sents)

        mallet_home = os.environ.get('MALLET_HOME', '/usr/local/mallet-0.4')
        nltk.classify.mallet.config_mallet(mallet_home) 
               
        crf = MalletCRF.train(fd, train, 
            gaussian_variance=gaussian_variance, 
            default_label=default_label, 
            transduction_type=transduction_type, 
            trace=trace)

        crf = cls(crf.filename, crf.feature_detector)
        crf.chunk_types = chunk_types
        return crf

_DEFAULT_CHUNK_TYPES = ('NP', 'PP', 'VP')
# tokens2tree() is almost entirely based on nltk.chunk.util.conllstr2tree()
# but works for a list of tokens instead of a CoNLL string.
def tokens2tree(tokens, chunk_types=_DEFAULT_CHUNK_TYPES, top_node='S'):
    stack = [Tree(top_node, [])]
    
    for token in tokens:
        token, tag = unflatten(token)
        if isinstance(token, basestring):
            word = token
            pos = None
        elif isinstance(token, tuple):
            word, pos = token
        else:
            ValueError, 'invalid type for variable \'token\': %s' % type(token)
        state, chunk_type = re.match(r'([IOB])-?(\S+)?', tag).groups()
        
        # If it's a chunk type we don't care about, treat it as O.
        if (chunk_types is not None and
            chunk_type not in chunk_types):
            state = 'O'
        
        # For "Begin"/"Outside", finish any completed chunks -
        # also do so for "Inside" which don't match the previous token.
        mismatch_I = state == 'I' and chunk_type != stack[-1].node
        if state in 'BO' or mismatch_I:
            if len(stack) == 2: stack.pop()
        
        # For "Begin", start a new chunk.
        if state == 'B' or mismatch_I:
            chunk = Tree(chunk_type, [])
            stack[-1].append(chunk)
            stack.append(chunk)
        
        # Add the new word token.
        stack[-1].append((word, pos or ''))
    
    return stack[0]
    
def flatten(tokens):
    if not tokens:
        return ()
    if not getattr(tokens, '__iter__', False):
        return (tokens,)
    return flatten(tokens[0]) + flatten(tokens[1:])
    
def unflatten(token):
    if not token:
        return ()
    if not getattr(token, '__iter__', False):
        return (token,)
    if len(token) == 1 or len(token[:-1]) == 1:
        return tuple(token)
    return (token[:-1], token[-1])
    
def test_chunk_tagger(chunk_tagger, iob_sents, **kwargs):
    chunk_types = chunk_tagger.chunk_types
    correct = map(lambda sent: tokens2tree(sent, chunk_types), iob_sents)
    guesses = chunk_tagger.batch_parse(map(lambda c: c.leaves(), correct))
    
    chunkscore = ChunkScore()    
    for c, g in zip(correct, guesses):
        chunkscore.score(c, g)
    
    if kwargs.get('verbose'):
        guesses = chunk_tagger.batch_tag(map(lambda c: c.leaves(), correct))
        correct = iob_sents
        
        print
        for c, g in zip(correct, guesses):        
            for tokc, tokg in zip(map(flatten, c), map(flatten, g)):
                word = tokc[0]
                iobc = tokc[-1]
                iobg = tokg[-1]
                star = ''
                if iobg != iobc: star = '*'
                print '%3s %20s %20s %20s' % (star, word, iobc, iobg)
            print      
        
    print 'Precision: %.2f' % chunkscore.precision()
    print 'Recall:    %.2f' % chunkscore.recall()
    print 'Accuracy:  %.2f' % chunkscore.accuracy()                
    print 'F-measure: %.2f' % chunkscore.f_measure()
    
    return chunkscore
    
def unittest(verbose=False): 
    import doctest
    failed, tested = doctest.testfile('test/chunk.doctest', verbose)
    if not verbose:
        print '%d passed and %d failed.' % (tested - failed, failed)
        if failed == 0:
            print 'Test passed.'
        else:
            print '***Test Failed*** %d failures.' % failed
    return (tested - failed), failed

def demo():
    pass
    
_TRAINERS = ['NaiveBayesChunkTagger.train', 'MaxentChunkTagger.train',
             'CRFChunkTagger.train']
_CORPORA = ['nltk.corpus.conll2000']
if __name__ == '__main__':
    import optparse
    
    try:
        import cPickle as pickle
    except:
        import pickle    
    
    import nltk_contrib
    from nltk_contrib.coref.chunk import *
    
    try:
        parser = optparse.OptionParser()
        parser.add_option('-d', '--demo', action='store_true', dest='demo',
            default=False, help='run demo')
        parser.add_option('-t', '--trainer', metavar='TRAINER', 
            dest='trainer',
            type='choice', choices=_TRAINERS, default=_TRAINERS[0], 
            help='train model using TRAINER, e.g. %s' % ', '.join(_TRAINERS))
        parser.add_option('-n', '--num-sents', metavar='TRAIN,TEST',  
            dest='numsents', type=str, default=None,
            help='number of TRAIN and TEST sentences to train model with')
        parser.add_option('-c', '--corpus', metavar='CORPUS', dest='corpus',
            type=str, default=_CORPORA[0],
            help='train model using CORPUS, e.g. %s' % ', '.join(_CORPORA))
        parser.add_option('-m', '--model', metavar='MODEL',
            dest='model', type='str', default=None,
            help='save model file to MODEL')
        parser.add_option('-u', '--unit-test', action='store_true', 
            default=False, dest='unittest', help='run unit tests')
        parser.add_option('-v', '--verbose', action='store_true', 
            default=False, dest='verbose', help='verbose')
        (options, args) = parser.parse_args()
    
        if options.numsents:
            m = re.match('^(?P<train>\d+)\s*(\,\s*(?P<test>\d+))?$', 
                options.numsents)
            if m:
                num_train = int(m.group('train'))
                num_test = int(m.group('test') or 0)
                options.numsents = (num_train, num_test)
            else:
                raise ValueError, "malformed argument for option -n"
        else:
            options.numsents = (None, None)
            
    except ValueError, v:
        print 'error: %s' % v.message
        parser.print_help()            
    
    if options.unittest:
        failed, passed = unittest(options.verbose)
        sys.exit(int(bool(failed)))

    if options.demo:
        demo()
        sys.exit(0)
    
    if options.trainer:
        corpus = eval(options.corpus).iob_sents()

        num_train, num_test = options.numsents
        num_train = num_train or int(len(corpus) * 0.9)
        num_test = num_test or (len(corpus) - num_train)
        train = corpus[:num_train]
        test = corpus[num_train:num_train + num_test]        

        trainer = eval(options.trainer)        
        if options.verbose:
            print 'Training %s with %d sentences' % \
                (options.trainer, num_train)
        chunker = trainer(train, verbose=options.verbose)
        
        if options.model:
            options.model = os.path.abspath(options.model)
            try:
                if chunker.__class__ == CRFChunkTagger:
                    pass
                else:
                    if options.model.endswith('.gz'):
                        _open = BufferedGzipFile
                    else:
                        _open = open                    
                    stream = _open(options.model, 'w')
                    pickle.dump(chunker, stream)
                    stream.close()                    
                    chunker = pickle.load(_open(options.model, 'r'))
                if options.verbose:
                    print 'Model saved as %s' % options.model                    
            except Exception, e:
                print "error: %s" % e

        if test:
            if options.verbose:
                print 'Testing %s on %d sentences' % \
                    (options.trainer, num_test)
            chunker.test(test, verbose=options.verbose)
########NEW FILE########
__FILENAME__ = data
# Natural Language Toolkit (NLTK) Coreference Data Utilities
#
# Copyright (C) 2001-2011 NLTK Project 
# Author: Joseph Frazee <jfrazee@mail.utexas.edu>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

from zlib import Z_SYNC_FLUSH
from gzip import GzipFile, READ as GZ_READ, WRITE as GZ_WRITE

try:
    import cPickle as pickle
except:
    import pickle
    
try:
    from cStringIO import StringIO
except:
    from StringIO import StringIO

class BufferedGzipFile(GzipFile):
    """
    A C{GzipFile} subclass that buffers calls to L{read()} and L{write()}.
    This allows faster reads and writes of data to and from gzip-compressed 
    files at the cost of using more memory.
    
    The default buffer size is 2mb.
    
    C{BufferedGzipFile} is useful for loading large gzipped pickle objects
    as well as writing large encoded feature files for classifier training.
    """    
    SIZE = 2 * 2**20

    def __init__(self, filename=None, mode=None, compresslevel=9, **kwargs):
        """
        @return: a buffered gzip file object
        @rtype: C{BufferedGzipFile}
        @param filename: a filesystem path
        @type filename: C{str}
        @param mode: a file mode which can be any of 'r', 'rb', 'a', 'ab', 
            'w', or 'wb'
        @type mode: C{str}
        @param compresslevel: The compresslevel argument is an integer from 1
            to 9 controlling the level of compression; 1 is fastest and 
            produces the least compression, and 9 is slowest and produces the
            most compression. The default is 9.
        @type compresslevel: C{int}
        @kwparam size: number of bytes to buffer during calls to
            L{read()} and L{write()}
        @type size: C{int}
        """         
        GzipFile.__init__(self, filename, mode, compresslevel)
        self._size = kwargs.get('size', self.SIZE)
        self._buffer = StringIO()
        # cStringIO does not support len.
        self._len = 0
        
    def _reset_buffer(self):
        # For some reason calling StringIO.truncate() here will lead to 
        # inconsistent writes so just set _buffer to a new StringIO object.
        self._buffer = StringIO()
        self._len = 0
        
    def _write_buffer(self, data):
        # Simply write to the buffer and increment the buffer size.
        if data is not None:
            self._buffer.write(data)
            self._len += len(data)
    
    def _write_gzip(self, data):
        # Write the current buffer to the GzipFile.
        GzipFile.write(self, self._buffer.getvalue())
        # Then reset the buffer and write the new data to the buffer.
        self._reset_buffer()
        self._write_buffer(data)

    def close(self):
        # GzipFile.close() doesn't actuallly close anything.
        if self.mode == GZ_WRITE:
            self._write_gzip(None)
            self._reset_buffer()
        return GzipFile.close(self)

    def flush(self, lib_mode=Z_SYNC_FLUSH):
        self._buffer.flush()
        GzipFile.flush(self, lib_mode)

    def read(self, size=None):
        if not size: 
            size = self._size
        return GzipFile.read(self, size)

    def write(self, data, size=None):
        """
        @param data: C{str} to write to file or buffer
        @type data: C{str}
        @param size: buffer at least size bytes before writing to file
        @type size: C{int}
        """
        if not size: 
            size = self._size
        if self._len + len(data) <= size:
            self._write_buffer(data)
        else:
            self._write_gzip(data)
########NEW FILE########
__FILENAME__ = features
# Natural Language Toolkit (NLTK) Coreference Feature Functions
#
# Copyright (C) 2001-2011 NLTK Project 
# Author: Joseph Frazee <jfrazee@mail.utexas.edu>
#         Ewan Klein <ewan@inf.ed.ac.uk> (modifications)
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

import re
import math

from nltk.corpus import names, gazetteers

NUMBERS = ['one', 'two', 'three', 'four', 'five', 'six', 'seven',
           'eight', 'nine', 'ten', 'eleven', 'twelve', 'thirteen',
           'fourteen', 'fifteen', 'sixteen', 'seventeen', 'eighteen',
           'nineteen', 'twenty', 'thirty', 'fourty', 'fifty',
           'sixty', 'seventy', 'eighty', 'ninety', 'hundred',
           'thousand', 'million', 'billion', 'trillion']

ORDINALS = ['first', 'second', 'third', 'fourth', 'fifth', 'sixth', 
            'seventh', 'eighth', 'ninth', 'tenth', 'eleventh', 'twelfth']

DAYS = ['monday', 'tuesday', 'wednesday', 'thursday', 
        'friday', 'saturday', 'sunday']

MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',
          'august', 'september', 'october', 'november', 'december',
          'jan', 'feb', 'mar', 'apr', 'jun', 'jul', 'aug', 'sep', 'sept',
          'oct', 'nov', 'dec']

                     
NAMES = set([name.lower() for filename in ('male.txt', 'female.txt') for name
             in names.words(filename)])

USCITIES = set(gazetteers.words('uscities.txt'))

# [XX] contains some non-ascii chars
COUNTRIES = set([country for filename in ('isocountries.txt','countries.txt')
                 for country in gazetteers.words(filename)])

# States in North America
NA_STATES = set([state.lower() for filename in
                 ('usstates.txt','mexstates.txt','caprovinces.txt') for state in
                 gazetteers.words(filename)])
                     
US_STATE_ABBREVIATIONS = set(gazetteers.words('usstateabbrev.txt'))

NATIONALITIES = set(gazetteers.words('nationalities.txt'))
                     
PERSON_PREFIXES = ['mr', 'mrs', 'ms', 'miss', 'dr', 'rev', 'judge',
                   'justice', 'honorable', 'hon', 'rep', 'sen', 'sec',
                   'minister', 'chairman', 'succeeding', 'says', 'president']

PERSON_SUFFIXES = ['sr', 'jr', 'phd', 'md']

ORG_SUFFIXES = ['ltd', 'inc', 'co', 'corp', 'plc', 'llc', 'llp', 'gmbh',
                'corporation', 'associates', 'partners', 'committee',
                'institute', 'commission', 'university', 'college',
                'airlines', 'magazine']

CURRENCY_UNITS = ['dollar', 'cent', 'pound', 'euro']

ENGLISH_PRONOUNS = ['i', 'you', 'he', 'she', 'it', 'we', 'you', 'they']
                
RE_PUNCT = '[-!"#$%&\'\(\)\*\+,\./:;<=>^\?@\[\]\\\_`{\|}~]'

RE_NUMERIC = '(\d{1,3}(\,\d{3})*|\d+)(\.\d+)?'

RE_NUMBER = '(%s)(\s+(%s))*' % ('|'.join(NUMBERS), '|'.join(NUMBERS))

RE_QUOTE = '[\'"`]'

RE_ROMAN = 'M?M?M?(CM|CD|D?C?C?C?)(XC|XL|L?X?X?X?)(IX|IV|V?I?I?I?)'

RE_INITIAL = '[A-Z]\.'

RE_TLA = '([A-Z0-9][\.\-]?){2,}'

RE_ALPHA = '[A-Za-z]+'

RE_DATE = '\d+\/\d+(\/\d+)?'

RE_CURRENCY = '\$\s*(%s)?' % RE_NUMERIC

RE_PERCENT = '%s\s*' % RE_NUMERIC + '%'

RE_YEAR = '(\d{4}s?|\d{2}s)'

RE_TIME = '\d{1,2}(\:\d{2})?(\s*[aApP]\.?[mM]\.?)?'

def contains(is_method, s):
    for word in s.split():
        if is_method(word):
            return True
    return False

def startswith(is_method, s):
    return is_method(s.split()[0])

def endswith(is_method, s):
    return is_method(s.split()[-1])
    
def re_contains(regex, s):
    return bool(re.match(r'.*%s.*' % regex, s))

def re_is(regex, s):
    return bool(re.match(r'^%s$' % regex, s))

def re_startswith(regex, s):
    return bool(re.match(r'^%s' % regex, s))
    
def re_endswith(regex, s):
    return bool(re.match(r'%s$' % regex, s))

def contains_period(s):
    return '.' in s
    
def contains_hyphen(s):
    return '-' in s

def is_punct(s):
    return re_is(RE_PUNCT, s)

def contains_punct(s):
    return re_contains(RE_PUNCT, s)
    
def is_numeric(s):
    return re_is(RE_NUMERIC, s)

def contains_numeric(s):
    return re_contains(RE_NUMERIC, s)

def is_number(s):
    return re_is(RE_NUMBER, s)

def contains_number(s):
    return re_contains(RE_NUMBER, s)

def is_currency(s):
    if re_is(RE_CURRENCY, s):
        return True
    else:
        for cu in CURRENCY_UNITS:
            if cu in s.lower():
                return True
    return False

def contains_currency(s):
    return contains(is_currency, s)
    
def is_percent(s):
    return bool(re.match(r'^%s$' % RE_PERCENT, s))

def contains_percent(s):
    return contains(is_percent, s)

def is_quote(s):
    return bool(re.match(r'^%s$' % RE_QUOTE, s))

def contains_quote(s):
    return bool(re.match(r'.*%s.*' % RE_QUOTE, s))

def is_digit(s):
    return s.isdigit()

def is_upper_case(s):
    return s.isupper()

def is_lower_case(s):
    return s.islower()

def is_title_case(s):
    return s.istitle()
    
def contains_title_case_sequence(s):
    words = s.split()
    count = sum([int(is_title_case(word)) for word in words])
    return (count / len(words)) > 0.5

def is_mixed_case(s):
    return s.isalpha() and not \
        (is_lower_case(s) or is_upper_case(s) or is_title_case(s))

def is_alpha_numeric(s):
    return s.isalnum()

def is_roman_numeral(s):
    return re_is(RE_ROMAN, s)
    
def contains_roman_numeral(s):
    return contains(is_roman_numeral, s)

def is_initial(s):
    return re_is(RE_INITIAL, s)
    
def contains_initial(s):
    return re_contains(RE_INITIAL, s)

def is_tla(s):
    return re_is(RE_TLA, s) and re_contains(RE_ALPHA, s)
    
def contains_tla(s):
    return re_contains(RE_TLA, s)

def is_name(s):
    return s.lower() in NAMES

def contains_name(s):
    return contains(is_name, s)

def contains_name_sequence(s):
    count = 0.0
    words = s.split()
    for word in words:
        if is_person_prefix(word) or is_name(word) or is_initial(word) or \
           is_person_suffix(word) or is_roman_numeral(word):
            count += 1
    return (count / len(words)) > 0.5
        
def is_city(s):
    return s.lower() in USCITIES

def contains_city(s):
    if contains(is_city, s):
        return True
    for city in USCITIES:
        if city in s.lower():
            return True
    return False

def part_of_city(s):
    for city in USCITIES:
        if s.lower() in city:
            return True
    return False
    
def is_state(s):
    return s.lower() in NA_STATES or s in US_STATE_ABBREVIATIONS

def contains_state(s):
    if contains(is_state, s):
        return True
    for state in NA_STATES:
        if state in s.lower():
            return True
    return False

def is_person_prefix(s):
    return s.replace('.', '').lower() in PERSON_PREFIXES

def startswith_person_prefix(s):
    return startswith(is_person_prefix, s)

def contains_person_prefix(s):
    return contains(is_person_prefix, s)

def is_person_suffix(s):
    return s.replace('.', '').lower() in PERSON_SUFFIXES

def endswith_person_suffix(s):
    return endswith(is_person_suffix, s)
    
def contains_person_suffix(s):
    return contains(is_person_suffix, s)

def is_org_suffix(s):
    return s.replace('.', '').lower() in ORG_SUFFIXES
    
def endswith_org_suffix(s):
    return endswith(is_org_suffix, s)

def contains_org_suffix(s):
    return contains(is_org_suffix, s)

def is_day(s):
    return s.lower() in DAYS

def contains_day(s):
    return contains(is_day, s)

def is_month(s):
    return s.lower().replace('.', '') in MONTHS

def contains_month(s):
    return contains(is_month, s)

def is_date(s):
    return re_is(RE_DATE, s)

def contains_date(s):
    return re_contains(RE_DATE, s)

def is_ordinal(s):
    if s.lower() in ORDINALS or s.endswith('teenth'):
        return True
    elif (s.lower()[:4] in [n[:4] for n in NUMBERS] or s[:1].isdigit()) and \
        s[-2:] in ['st', 'nd', 'rd', 'th']:
        return True
    return False

def contains_ordinal(s):
    return contains(is_ordinal, s)

def is_prefix(s):
    return s.startswith('-')

def is_suffix(s):
    return s.endswith('-')

def is_country(s):
    def _country_name(s):
        stop_words = ['islands', 'saint', 'and', 'republic', 'virgin',
                      'united', 'south', 'of', 'new', 'the']                      
        words = []
        for word in s.split():
            word = re.sub(r'%s' % RE_PUNCT, '', word)            
            if word.lower() not in stop_words:
                words.append(word)
        return ' '.join(words) or s
        
    if s.lower() in COUNTRIES:
        return True
    else:
        country_name = _country_name(s)
        for country in COUNTRIES:
            if country_name.lower() == _country_name(country).lower():
                return True
    return False

def contains_country(s):
    if contains(is_country, s):
        return True
    for country in COUNTRIES:
        if country in s.lower():
            return True
    return False

def is_nationality(s):
    return s.lower() in NATIONALITIES or \
           s.lower()[:-1] in NATIONALITIES or \
           s.lower()[:-2] in NATIONALITIES

def contains_nationality(s):
    return contains(is_nationality, s)

def log_length(s):
    return int(math.log(len(s)))
    
def word_count(s):
    return len(s.split())

def contains_of_location(s):
    for words in re.split(r'\s+of\s+', s)[1:]:
        if is_city(words) or is_country(words) or is_state(words):
            return True
    return False
    
def is_location(s):
    return is_city(s) or is_country(s) or is_state(s)

def contains_location(s):
    if contains(is_location, s):
        return True
    for location in USCITIES.union(COUNTRIES).union(NA_STATES):
        if location in s.lower():
            return True
    return False

def is_year(s):
    return re_is(RE_YEAR, s)

def contains_year(s):
    return re_contains(RE_YEAR, s)
    
def is_time(s):
    return re_is(RE_TIME, s)

def contains_time(s):
    return re_contains(RE_TIME, s)

def is_pronoun(s):
    return s.lower() in ENGLISH_PRONOUNS
    
def word_type(word):
    if not word:
        return ()

    word_type = []
    if contains_person_prefix(word) or contains_person_suffix(word):
        word_type.append('PERSON')
    if contains_org_suffix(word):
        word_type.append('ORG')
    if contains_name(word):
        word_type.append('NAME')
    if contains_nationality(word):
        word_type.append('NATIONALITY')
    if contains_location(word):
        word_type.append('LOCATION')
    if contains_roman_numeral(word):
        word_type.append('ROMAN_NUMERAL')
    if contains_tla(word):
        word_type.append('TLA')
    if contains_initial(word):
        word_type.append('INITIAL')
    if contains_currency(word):
        word_type.append('CURRENCY')
    if contains_percent(word):
        word_type.append('PERCENT')
    if contains_numeric(word) or contains_number(word) or \
       contains_ordinal(word) or is_digit(word):
        word_type.append('NUMBER')
    if contains_day(word) or contains_month(word) or \
       contains_date(word) or contains_year(word):
        word_type.append('DATE')
    if contains_time(word):
        word_type.append('TIME')
    if is_suffix(word):
        word_type.append('SUFFIX')
    if is_prefix(word):
        word_type.append('PREFIX')
    if contains_title_case_sequence(word):
        word_type.append('TITLE_CASE')
    if contains_punct(word):
        word_type.append('PUNCT')

    return tuple(word_type[:3])

def demo():
    from nltk.corpus import treebank
    for word in treebank.words('wsj_0034.mrg'):
        wt = word_type(word)
        if len(wt) == 0: wt = None
        if '*' in word: continue
        print "%-20s\t%s" % (word, wt)
        
if __name__ == '__main__':
    demo()


########NEW FILE########
__FILENAME__ = freiburg
# Natural Language Toolkit (NLTK) Freiburg Corpora Reader
#
# Copyright (C) 2001-2011 NLTK Project 
# Author: Joseph Frazee <jfrazee@mail.utexas.edu>
# URL: http://nltk.org
# For license information, see LICENSE.TXT

"""
Corpus reader for the Freiburg Corpora.

"""

from sgmllib import SGMLParser

from nltk.corpus.reader.api import *
from nltk.corpus.reader.util import *

from nltk.tokenize.punkt import *
from nltk.tokenize.regexp import *
from nltk.tokenize.simple import *


class FreiburgCorpusReader(CorpusReader):
    """
    """

    def __init__(self, root, files):
        CorpusReader.__init__(self, root, files)

    def words(self, files=None):
        """
        """
        return concat([FreiburgDocument(filename).words()
                       for filename in self.abspaths(files)])

    def sents(self, files=None):
        """
        """
        return concat([FreiburgDocument(filename).sents()
                       for filename in self.abspaths(files)])

    def paras(self, files=None):
        """
        """
        return concat([FreiburgDocument(filename).paras()
                       for filename in self.abspaths(files)])


class FreiburgDocument(str):
    """
    """

    def __init__(self, path):
        """
        """
        str.__init__(self, path)
        self._sgmldoc_ = None

    def _sgmldoc(self):
        """
        """
        if self._sgmldoc_:
            return self._sgmldoc_
        self._sgmldoc_ = FreiburgSGMLParser().parse(self)
        return self._sgmldoc_

    def _sent_tokenizer(self):
        """
        """
        return PunktSentenceTokenizer()

    def _word_tokenizer(self):
        """
        """
        return PunktWordTokenizer()

    def words(self):
        """
        """
        result = []
        for sent in self.sents():
            result.extend(sent)
        assert None not in result
        return result

    def sents(self):
        """
        """
        result = []
        for p in self.paras():
            result.extend(p)
        assert None not in result
        return result

    def paras(self):
        """
        """
        result = []
        result = self._sgmldoc().paras()
        assert result
        return result


class FreiburgToken(str):
    """
    """

    def __init__(self, token):
        """
        """
        str.__init__(self, token)
        self._pos = None

    def set_pos(self, pos):
        """
        """
        self._pos = pos
        return self

    def pos(self):
        """
        """
        return self._pos


class FreiburgSGMLParser(SGMLParser):
    """
    """

    def __init__(self):
        """
        """
        SGMLParser.__init__(self)

    def reset(self):
        """
        """
        SGMLParser.reset(self)
        self._parsed = False
        self._s = None
        self._p = None
        self._current = None
        self._current_pos = None
        self._in = []

    def start_p(self, attrs):
        """
        """
        if 'p' in self._in:
            self.start_s(None)
            if self._s:
                self._p.append(self._s)
                self._s = None
            self._in.remove('p')
        self._in.insert(0, 'p')
        if not self._p:
            self._p = []
        self._s = []

    def start_head(self, attrs):
        """
        """
        self.start_p(None)

    def start_s(self, attrs):
        """
        """
        if 's' in self._in:
            if self._current:
                self._s.append(self._current) 
                self._current = None
            self._in.remove('s')
        self._in.insert(0, 's')
        if not self._s:
            self._s = []
        self._current = []

    def start_w(self, attrs):
        """
        """
        self._in.insert(0, 'w')
        if attrs:
            self._current_pos = attrs[0][1]

    def start_c(self, attrs):
        """
        """
        self._in.insert(0, 'c')
        if attrs:
            self._current_pos = attrs[0][1]

    def start_text(self, attrs):
        """
        """
        pass

    def end_text(self):
        """
        """
        self.start_p(None)

    def handle_data(self, data):
        """
        """
        if self._in and 'w' == self._in[0]:
            token = FreiburgToken(data.rstrip()).set_pos(self._current_pos)
            self._current.append(token)
            self._current_pos = None
            self._in.remove('w')
        if self._in and 'c' == self._in[0]:
            token = FreiburgToken(data.rstrip()).set_pos(self._current_pos)
            self._current.append(token)
            self._current_pos = None
            self._in.remove('c')

    def parse(self, filename):
        """
        """
        file = open(filename)
        for line in file:
            self.feed(line)
        file.close()
        self._parsed = True
        return self

    def paras(self):
        """
        """
        assert self._parsed and self._p
        return self._p


def _demo(root, file):
    """
    """
    import os.path
    from nltk_contrib.coref.ace2 import ACE2CorpusReader

    try:
        reader = FreiburgCorpusReader(root, file)
        print 'Paragraphs for %s:' % (file)
        for para in reader.paras():
            print '    %s' % (para)
            print
        print 'Sentences for %s:' % (file)
        for sent in reader.sents():
            print '    %s' % (sent)
        print
        print 'Words for %s:' % (file)
        for word in reader.words():
            print '    %s/%s' % (word, word.pos())
        print
    except Exception, e:
        print 'Error encountered while running demo for %s: %s' % (file, e)
        print

def demo():
    """
    """
    import os
    import os.path

    try:
        fb_dir = os.environ['FREIBURG_DIR']
    except KeyError:
        raise 'Demo requires Freiburg Corpus, set FREIBURG_DIR environment variable!' 

    brown_dir = os.path.join(fb_dir, 'BROWN')
    _demo(brown_dir, 'R.txt')

    frown_dir = os.path.join(fb_dir, 'FROWN')
    _demo(frown_dir, 'FROWN_R.txt')

    flob_dir = os.path.join(fb_dir, 'F-LOB')
    _demo(flob_dir, 'F-LOB_R.txt')

if __name__ == '__main__':
    demo()

########NEW FILE########
__FILENAME__ = muc
# Natural Language Toolkit (NLTK) MUC Corpus Reader
#
# Copyright (C) 2001-2011 NLTK Project 
# Author: Joseph Frazee <jfrazee@mail.utexas.edu>
#         Steven Bird <sb@csse.unimelb.edu.au> (original IEER Corpus Reader)
#         Edward Loper <edloper@gradient.cis.upenn.edu> (original IEER Corpus 
#         Reader)
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

# Adapted from nltk.corpus.reader.ieer.IEERCorpusReader

import re
import codecs

from itertools import chain

from nltk import Tree

from nltk.util import LazyMap, LazyConcatenation

from nltk.tokenize.treebank import TreebankWordTokenizer
from nltk.tokenize.punkt import PunktSentenceTokenizer

from nltk.corpus.util import LazyCorpusLoader

from nltk.corpus.reader.api import CorpusReader
from nltk.corpus.reader.util import concat, StreamBackedCorpusView

muc6_titles = {
    '891102-0189.ne.v1.3.sgm':'',
    '891102-0189.co.v2.0.sgm':'',
    '891101-0050.ne.v1.3.sgm':'',
}
muc6_documents = sorted(muc6_titles)

muc7_titles = {
    'dryrun01.muc7':'',
    'dryrun02.muc7':'',    
    'dryrun03.muc7':'',    
}
muc7_documents = sorted(muc7_titles)

_MUC_CHUNK_TYPES = [
    'DATE',
    'IDENT',
    'LOCATION',
    'MONEY',
    'ORGANIZATION',
    'PERCENT',
    'PERSON',
    'TIME'
]

_MUC6_DOC_RE = re.compile(
    r'\s*<DOC>\s*'
    r"""
     (\s*(<DOCNO>\s*(?P<docno>.+?)\s*</DOCNO>|
          <CODER>\s*.+?\s*</CODER>|
          <DD>\s*.+?\s*</DD>|
          <AN>\s*.+?\s*</AN>|
          <HL>\s*(?P<headline>.+?)\s*</HL>|
          <SO>\s*.+?\s*</SO>|
          <CO>\s*.+?\s*</CO>|
          <IN>\s*.+?\s*</IN>|
          <GV>\s*.+?\s*</GV>|
          <DATELINE>\s*(?P<dateline>.+?)\s*</DATELINE>)\s*)*
     """
    r'<TXT>\s*(?P<text>(<p>\s*(<s>\s*.+?\s*</s>)+\s*</p>)+)\s*</TXT>\s*'
    r'</DOC>\s*', re.DOTALL | re.I | re.VERBOSE)
_MUC6_PARA_RE = re.compile('(<p>\s*(?P<para>.+?)\s*</p>?)+', re.DOTALL | re.I)
_MUC6_SENT_RE = re.compile('(<s>\s*(?P<sent>.+?)\s*</s>)+', re.DOTALL | re.I)    

_MUC7_DOC_RE = re.compile(
    r'\s*<DOC>\s*'
    r"""
     (\s*(<DOCID>\s*(?P<docid>.+?)\s*</DOCID>|
           <STORYID\s+[^>]*?>\s*.+?\s*</STORYID>|      
           <SLUG\s+[^>]*?>\s*.+?\s*</SLUG>|          
           <DATE>\s*(?P<date>.+?)\s*</DATE>|         
           <NWORDS>\s*.+?\s*</NWORDS>|                    
           <PREAMBLE>\s*.+?\s*</PREAMBLE>)\s*)*
     """
    r'<TEXT>\s*(?P<text>.+?)\s*</TEXT>\s*'
    r'(<TRAILER>\s*(?P<trailer>.+?)\s*</TRAILER>\s*)?'
    r'</DOC>\s*', re.DOTALL | re.I | re.VERBOSE)
_MUC7_PARA_RE = re.compile(r'\s*<p>\s*.+?\s*(<p>\s*.+?\s*?)*\s*', re.DOTALL | re.I)
_MUC7_PARA_SPLIT_RE = re.compile(r'\s*<p>\s*', re.DOTALL | re.I)

_MUC_NE_B_RE = re.compile('<(ENAMEX|NUMEX|TIMEX)\s+[^>]*?TYPE="(?P<type>\w+)"', re.DOTALL | re.I)
_MUC_NE_E_RE = re.compile('</(ENAMEX|NUMEX|TIMEX)>', re.DOTALL | re.I)
_MUC_CO_B_RE = re.compile('<COREF\s+[^>]*?ID="(?P<id>\w+)"(\s+TYPE="(?P<type>\w+)")?(\s+REF="(?P<ref>\w+)")?', re.DOTALL | re.I)
_MUC_CO_E_RE = re.compile('</COREF>', re.DOTALL | re.I)

_WORD_TOKENIZER = TreebankWordTokenizer()
_SENT_TOKENIZER = PunktSentenceTokenizer()

class MUCDocument:
    # def __init__(self, text, docno=None, dateline=None, headline=''):
    def __init__(self, **text):
        self.text = None
        if isinstance(text, basestring):
            self.text = text
        elif isinstance(text, dict):
            for key, val in text.items():
                setattr(self, key, val)
        else:
            raise
        assert self.text
        
    def __repr__(self):
        if self.headline:
            headline = ' '.join(self.headline.leaves())
        else:
            headline = ' '.join([w for w in self.text.leaves()
                                 if w[:1] != '<'][:11])+'...'
        if self.docno is not None:            
            return '<MUCDocument %s: %r>' % (self.docno, headline)
        else:
            return '<MUCDocument: %r>' % headline

class MUCCorpusReader(CorpusReader):
    """
    A corpus reader for MUC SGML files.  Each file begins with a preamble
    of SGML-tagged metadata. The document text follows. The text of the 
    document is contained in <TXT> tags for MUC6 and <TEXT> tags for MUC7.
    Paragraphs are contained in <p> tags in both corpus formats. Sentences are
    contained in <s> tags in MUC6 only. For MUC7 corpus files L{sents()},
    L{chunked_sents()}, and L{iob_sents()} return sentences via tokenizing
    with C{PunktSentenceTokenizer}.
    
    Additionally named entities and coreference mentions may be marked within 
    the document text and document metadata. The MUC6 corpus provides
    named entity and coreference annotations in two separate sets of files.
    The MUC7 corpus contains coreference annotations only. Only one kind of 
    metadata will be returned depending on which kind of file is being read.
    
    Named entities are tagged as ENAMEX (name expressions), NUMEX 
    (number expressions), or TIMEX (time expressions), all of which include 
    TYPE attributes.  
    
    Coreference mentions are tagged as COREF and include ID, TYPE, REF, and 
    MIN attributes. ID is used to give each coreference mention a unique 
    numeric idenitifier. REF indicates the ID of the intended referent of the 
    coreference mention and is not required for first mentions. MIN contains 
    the minimum coreferential string of the coreference mention.
    """
    def raw(self, fileids=None):
        """
        @return: A list of corpus file contents.
        @rtype: C{list} of C{str}
        @param fileids: A list of corpus files.
        @type fileids: C{list} of C{str} or regular expression
        """
        if fileids is None:
            fileids = self._fileids
        elif isinstance(fileids, basestring): 
            fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])

    def docs(self, fileids=None):
        """
        @return: A list of corpus document strings.
        @rtype: C{list} of C{StreamBackedCorpusView}
        @param fileids: A list of corpus files.
        @type fileids: C{list} of C{str} or regular expression
        """
        return concat([StreamBackedCorpusView(fileid, 
                                              self._read_block,
                                              encoding=enc)
                       for (fileid, enc) in self.abspaths(fileids, True)])

    def parsed_docs(self, fileids=None):
        """
        @return: A list of parsed corpus documents.
        @rtype: C{list} of C{StreamBackedCorpusView}
        @param fileids: A list of corpus files.
        @type fileids: C{list} of C{str} or regular expression
        """        
        return concat([StreamBackedCorpusView(fileid,
                                              self._read_parsed_block,
                                              encoding=enc)
                       for (fileid, enc) in self.abspaths(fileids, True)])
        
    def paras(self, fileids=None, **kwargs):
        """
        @return: A list of paragraphs.
        @rtype: C{list} of C{list} of C{list} of C{str}
        @param fileids: A list of corpus files.
        @type fileids: C{list} of C{str} or regular expression.
        """
        def __para(para):
            return [sent.leaves() for sent in list(para)]
        return LazyMap(__para, self._paras(fileids))
    
    def sents(self, fileids=None):
        """
        @return: A list of sentences.
        @rtype: C{list} of C{list} of C{str}
        @param fileids: A list of corpus files.
        @type fileids: C{list} of C{str} or regular expression        
        """
        return LazyConcatenation(self.paras(fileids))
        
    def chunked_sents(self, fileids=None, **kwargs):
        """
        @return: A list of sentence chunks as tuples of string/tag pairs.
        @rtype: C{list} of C{list} of C{tuple}
        @param fileids: A list of corpus files.
        @type fileids: C{list} of C{str} or regular expression 
        @kwparam depth: Depth of chunk parsing for nested chunks.
        @type depth: C{int}       
        """
        def __chunked_sent(sent):
            chunks = []
            # Map each sentence subtree into a tuple.
            for token in map(tree2tuple, sent):
                # If the token's contents is a list of chunk pieces, append it
                # as a list of word/tag pairs.
                if isinstance(token[0], list):
                    chunks.append([(word, None) for word in token[0]])
                # If the token's contents is a string, append it as a 
                # word/tag tuple.
                elif isinstance(token[0], basestring):
                    chunks.append((token[0], None))
                # Something bad happened.
                else:
                    raise
            return chunks
        depth = kwargs.get('depth', 0)        
        sents = self._chunked_sents(self._sents(fileids, **kwargs), depth)
        return LazyMap(__chunked_sent, sents)
        
    def iob_sents(self, fileids=None, **kwargs):
        """
        @return: A list of sentences as iob word/iob/other tag pairs.
        @rtype: C{list} of C{list} of C{tuple}
        @param fileids: A list of corpus files.
        @type fileids: C{list} of C{str} or regular expression 
        @kwparam depth: Depth of chunk parsing for nested chunks.
        @type depth: C{int}      
        """
        def __iob_sent(sent):
            chunks = []
            # Map each sentence subtree into a tuple.
            for token in map(tree2tuple, sent):
                # If the token has a chunk type, parse the token contents.
                if token[1] is not None:
                    for index, word in enumerate(token[0]):
                        # The first word in a chunk B-egins the chunk.
                        if index == 0:
                            chunks.append((word, 'B-%s' % token[1:2]) + token[2:])
                        # All other words in a chunk are I-n the chunk.
                        else:
                            chunks.append((word, 'I-%s' % token[1:2]) + token[2:])
                # If the token doesn't have a chunk type, it's O-ut.
                else:
                    chunks.append((token[0], 'O'))
            return chunks
        depth = kwargs.get('depth', 0)        
        sents = self._chunked_sents(self._sents(fileids), depth)          
        return LazyMap(__iob_sent, sents)
        
    def words(self, fileids=None):
        """
        @return: A list of words.
        @rtype: C{list} of C{str}
        @param fileids: A list of corpus files.
        @type fileids: C{list} of C{str} or regular expression 
        @kwparam depth: Depth of chunk parsing for nested chunks.
        @type depth: C{int}        
        """
        # Concatenate the list of lists given by sents().
        return LazyConcatenation(self.sents(fileids))
    
    def iob_words(self, fileids=None, **kwargs):
        """
        @return: A list of word/iob/other tag tuples.
        @rtype: C{list} of C{tuple}
        @param fileids: A list of corpus files.
        @type fileids: C{list} of C{str} or regular expression 
        @kwparam depth: Depth of chunk parsing for nested chunks.
        @type depth: C{int}        
        """
        # Concatenate the list of lists given by iob_sents().
        return LazyConcatenation(self.iob_sents(fileids, **kwargs))
        
    def chunks(self, fileids=None, **kwargs):
        """
        @return: A list of chunked sents where chunks are multi-word strings.
        @rtype: C{list} of C{list} of C{str}
        @param fileids: A list of corpus files.
        @type fileids: C{list} of C{str} or regular expression 
        @kwparam depth: Depth of chunk parsing for nested chunks.
        @type depth: C{int}        
        @kwparam concat: Concatenate sentence lists into one list; works like
            itertools.chain()
        @type concat: C{bool}
        """
        def __chunks(sent):
            chunks = []
            for token in sent:
                # If the token is a list of chunk pieces, append the piece's
                # contents as a string.                
                if isinstance(token, list):
                    # TODO: Better if able to reverse Treebank-style
                    # tokenization. The join leaves some weird whitespace.                    
                    chunks.append(' '.join([word[0] for word in token]))
                # If the token is a tuple, append the token's contents.
                elif isinstance(token, tuple):
                    chunks.append(token[0])
                # Something bad happened.
                else:
                    raise
            return chunks
        sents = self.chunked_sents(fileids, **kwargs) 
        # Concatenate the lists.          
        if kwargs.get('concat'):
            return LazyConcatenation(LazyMap(__chunks, sents))
        # Or not.
        else:
            return LazyMap(__chunks, sents)
        
    def mentions(self, fileids=None, **kwargs):
        """
        @return: A list of mentions as the tuple of 
            ([words...], id, referent, type)
        @rtype: C{list} of C{list} of C{tuple}
        @param fileids: A list of corpus files.
        @type fileids: C{list} of C{str} or regular expression 
        @kwparam depth: Depth of chunk parsing for nested chunks.
        @type depth: C{int}  
        @kwparam concat: Concatenate sentence lists into one list; works like
            itertools.chain(). Defaults to False.
        @type concat: C{bool}
        @kwparam nonmentions: Return nonmentions as well as mentions. Defaults
            to False.
        @type nonmentions: C{bool}              
        """
        def __mentions(sent):
            mentions = []            
            # Map each sentence subtree into a tuple.            
            for token in map(tree2tuple, sent):
                # If the token type is COREF then append the token contents
                # and everything but the token type.
                if token[1] == 'COREF':
                    mentions.append(token[:1] + token[2:])
                # If including nonmentions, append the token contents only.
                elif kwargs.get('nonmentions'):
                    mentions.append(token[:1])
            return mentions
        # TODO: Is depth doing what it's expected to?                
        depth = kwargs.get('depth', 0)        
        sents = self._chunked_sents(self._sents(fileids), depth)                
        # Concatenate the lists.
        if kwargs.get('concat'):
            return LazyConcatenation(LazyMap(__mentions, sents))
        # Or not.
        else:
            return LazyMap(__mentions, sents)
            
    def _paras(self, fileids=None):
        """
        @return: A list of paragraphs.
        @rtype: C{list} of C{Tree}
        @param fileids: A list of corpus files.
        @type fileids: C{list} of C{str} or regular expression.
        """
        def __para(doc):
            return list(doc.text)
        return LazyConcatenation(LazyMap(__para, self.parsed_docs(fileids)))

    def _sents(self, fileids=None):
        """
        @return: A list of sentence trees.
        @rtype: C{list} of C{list} of C{Tree}
        @param fileids: A list of corpus files.
        @type fileids: C{list} of C{str} or regular expression      
        """
        def __sents(para):
            return list(para)
        # Flatten this because it's a list of list of trees for each doc. It
        # doesn't matter which doc the list is from so chain them together.
        return LazyConcatenation(LazyMap(__sents, self._paras(fileids)))
        
    def _chunked_sents(self, sents, depth=0):
        """
        @return: A list of sentence chunk trees which are flatter than the
            original trees.
        @rtype: C{list} of C{list} of C{Tree}
        @param sents: A list of sentence trees.
        @type sents: C{list} of C{list} of C{Tree}
        @param depth: How deep to read nested chunks off of the trees. If
            depth is None, all possible chunk substrees are returned, 
            otherwise, chunks are returned starting at the highest level 0,
            then the next highest 1, etc.
        @type depth: C{int}
        """        
        def __chunked_sent(sent):
            for chunk in sent:
                # If the chunk is a Tree, append it's immediate subtrees.
                if isinstance(chunk, Tree):
                    return list(chunk)
                # If the chunk is not a tree, append it.
                else:
                    return chunk
        # If depth is None, return all possible subtrees 
        if depth is None:
            return LazyMap(lambda sent: sent.subtrees(), sents)
        # If depth is too small, no need to recurse and read further.
        if not depth - 1 >= 0:
            return sents
        # Otherwise, apply __chunked_sent() and recurse.
        return self._chunked_sents(LazyConcatenation(LazyMap(__chunked_sent, sents)), depth - 1)

    def _read_parsed_block(self, stream):
        # TODO: LazyMap but StreamBackedCorpusView doesn't support
        # AbstractLazySequence currently.
        return map(self._parse, self._read_block(stream))
  
    def _parse(self, doc):
        """
        @return: A parsed MUC document.
        @rtype: C{MUCDocument}
        @param doc: The string contents of a MUC document.
        @type doc: C{str}
        """
        tree = mucstr2tree(doc, top_node='DOC')
        if isinstance(tree, dict):
            return MUCDocument(**tree)
        else:
            return MUCDocument(tree)

    def _read_block(self, stream):
        return ['\n'.join(stream.readlines())]


def mucstr2tree(s, chunk_types=_MUC_CHUNK_TYPES, top_node='S'):
    """
    Convert MUC document contents into a tree.
    
    @return: A MUC document as a tree.
    @rtype: C{Tree}
    @param s: Contents of a MUC document.
    @type s: C{str}
    @param chunk_types: Chunk types to extract from the MUC document.
    @type chunk_types: C{list} of C{str}
    @param top_node: Label to assign to the root of the tree.
    @type top_node: C{str}
    """
    tree = None
    match = _MUC6_DOC_RE.match(s)
    if match:
        # If the MUC document is valid, read the document element groups off its
        # contents and return a dictionary of each part.
        if match:
            tree = {
                'text': _muc_read_text(match.group('text'), top_node),
                'docno': match.group('docno'),
                # Capture named entities/mentions in the front-matter too.            
                'dateline': _muc_read_text(match.group('dateline'), top_node),           
                'headline': _muc_read_text(match.group('headline'), top_node),
            }
    else:
        match = _MUC7_DOC_RE.match(s)
        if match:
            tree = {
                'text': _muc_read_text(match.group('text'), top_node),
                'docid': match.group('docid'),
                # Capture named entities/mentions in the front-matter too.            
                'date': _muc_read_text(match.group('date'), top_node),
            }
    assert tree
    return tree
        
def tree2tuple(tree):
    """
    Convert a tree or string into a flat tuple of leaves and a label.
    
    @return: A tuple of tree leaves and their parent's label.
    @rtype: C{tuple}
    @param tree: A tree.
    @type tree: C{Tree}
    """
    result = ()
    # If the tree is a tree then create a tuple out of the leaves and label.
    if isinstance(tree, Tree):
        # Get the leaves.
        s = (tree.leaves(),)
        # Get the label
        if isinstance(tree.node, basestring):
            node = (tree.node,)
        elif isinstance(tree.node, tuple):
            node = tree.node
        else:
            raise
        # Merge the leaves and the label.
        return s + node
    # If the tree is a string just convert it to a tuple.
    elif isinstance(tree, basestring):
        return (tree, None)
    # Something bad happened.
    else:
        raise

def _muc_read_text(s, top_node):
    # The tokenizer sometimes splits within coref tags.
    def __fix_tokenization(sents):
        for index in range(len(sents)):
            next = 1
            while sents[index].count('<COREF') != sents[index].count('</COREF>'):
                sents[index] += ' '
                sents[index] += sents[index + next]
                sents[index + next] = ''
                next += 1
        sents = filter(None, sents)
        return sents
    if s:
        tree = Tree(top_node, [])        
        if _MUC6_PARA_RE.match(s):
            for para in _MUC6_PARA_RE.findall(s):
                if para and para[0] and para[0].strip():
                    tree.append(Tree('P', []))
                    for sent in _MUC6_SENT_RE.findall(para[0]):
                        words = _MUC6_SENT_RE.match(sent[0]).group('sent').strip()
                        # There are empty sentences <s></s> in the MUC6 corpus.
                        if words:
                            tree[-1].append(_muc_read_words(words, 'S'))                
        elif _MUC7_PARA_RE.match(s):
            for para in _MUC7_PARA_SPLIT_RE.split(s):
                if para and para.strip():
                    tree.append(Tree('P', []))
                    for sent in __fix_tokenization(_SENT_TOKENIZER.tokenize(para)):
                        tree[-1].append(_muc_read_words(sent, 'S'))
        return tree

def _muc_read_words(s, top_node):
    if not s: return []
    stack = [Tree(top_node, [])]
    for word in re.findall('<[^>]+>|[^\s<]+', s):
        ne_match = _MUC_NE_B_RE.match(word)
        co_match = _MUC_CO_B_RE.match(word)
        if ne_match:
            chunk = Tree(ne_match.group('type'), [])
            stack[-1].append(chunk)
            stack.append(chunk)
        elif co_match:
            chunk = Tree(('COREF', co_match.group('id'), 
                          co_match.group('ref'), co_match.group('type')), [])
            stack[-1].append(chunk)
            stack.append(chunk)
        elif _MUC_NE_E_RE.match(word) or _MUC_CO_E_RE.match(word):
            stack.pop()
        else:
            stack[-1].extend(_WORD_TOKENIZER.tokenize(word))
    if len(stack) != 1:
        print stack
    assert len(stack) == 1
    return stack[0]

def demo(**kwargs):
    import nltk
    from nltk_contrib.coref import NLTK_COREF_DATA    
    from nltk_contrib.coref.muc import muc6_documents, muc7_documents
    from nltk_contrib.coref.muc import MUCCorpusReader
    nltk.data.path.insert(0, NLTK_COREF_DATA)   
    muc6 = LazyCorpusLoader('muc6/', MUCCorpusReader, muc6_documents)
    for sent in muc6.iob_sents()[:]:
        for word in sent:
            print word
        print
    print
    for sent in muc6.mentions(depth=None):
        for mention in sent:
            print mention
        if sent: print
    print
    muc7 = LazyCorpusLoader('muc7/', MUCCorpusReader, muc7_documents)
    for sent in muc7.iob_sents()[:]:
        for word in sent:
            print word
        print
    print
    for sent in muc7.mentions(depth=None):
        for mention in sent:
            print mention
        if sent: print
    print
    
if __name__ == '__main__':
    demo()

########NEW FILE########
__FILENAME__ = muc7
# Natural Language Toolkit (NLTK) MUC-7 Corpus Reader
#
# Copyright (C) 2001-2011 NLTK Project 
# Author: Joseph Frazee <jfrazee@mail.utexas.edu>
# URL: http://nltk.org
# For license information, see LICENSE.TXT

"""
Corpus reader for the MUC-7 Corpus.

"""

from sgmllib import SGMLParser

from nltk.corpus.reader.api import *
from nltk.corpus.reader.util import *

from nltk.tokenize.punkt import *
from nltk.tokenize.regexp import *
from nltk.tokenize.simple import *


class MUC7CorpusReader(CorpusReader):
    """
    """

    def __init__(self, root, files):
        CorpusReader.__init__(self, root, files)

    def words(self, files=None):
        """
        """
        return concat([MUC7Document(filename).words()
                       for filename in self.abspaths(files)])

    def sents(self, files=None):
        """
        """
        return concat([MUC7Document(filename).sents()
                       for filename in self.abspaths(files)])

    def paras(self, files=None):
        """
        """
        return concat([MUC7Document(filename).paras()
                       for filename in self.abspaths(files)])


class MUC7Document(str):
    """
    """

    def __init__(self, path):
        """
        """
        str.__init__(self, path)
        self._sgmldoc_ = None

    def _sgmldoc(self):
        """
        """
        if self._sgmldoc_:
            return self._sgmldoc_
        self._sgmldoc_ = MUC7SGMLParser().parse(self)
        return self._sgmldoc_

    def _sent_tokenizer(self):
        """
        """
        return PunktSentenceTokenizer()

    def _word_tokenizer(self):
        """
        """
        return PunktWordTokenizer()

    def words(self):
        """
        """
        result = []
        for sent in self.sents():
            result.extend(sent)
        assert None not in result
        return result

    def sents(self):
        """
        """
        result = []
        for p in self.paras():
            result.extend(p)
        assert None not in result
        return result

    def paras(self):
        """
        """
        result = []
        for p in self._sgmldoc().text():
            sent = []
            for s in self._sent_tokenizer().tokenize(p):
                sent.append(self._word_tokenizer().tokenize(s))
            result.append(sent)
        assert None not in result
        return result

    def docid(self):
        """
        """
        result = self._sgmldoc().docid()
        assert result
        return result

    def preamble(self):
        """
        """
        result = self._sgmldoc().preamble()
        assert result
        return result

    def trailer(self):
        """
        """
        result = self._sgmldoc().trailer()
        assert result
        return result


class MUC7SGMLParser(SGMLParser):
    """
    """

    def __init__(self):
        """
        """
        SGMLParser.__init__(self)

    def reset(self):
        """
        """
        SGMLParser.reset(self)
        self._parsed = False
        self._docid = None
        self._preamb = None
        self._p = None
        self._trailer = None
        self._current = None
        self._in = []

    def start_doc(self, attrs):
        """
        """
        self._in.insert(0, 'doc')

    def end_doc(self):
        """
        """
        self._in.remove('doc')

    def start_text(self, attrs):
        """
        """
        self._in.insert(0, 'text')

    def end_text(self):
        """
        """
        if self._in and self._in[0] == 'p':
            self._p.append(self._current)
            self._current = None
            self._in.remove('p')
        self._in.remove('text')

    def start_docid(self, attrs):
        """
        """
        self._in.insert(0, 'docid')
        self._docid = ''

    def end_docid(self):
        """
        """
        self._docid = self._docid.strip()
        self._in.remove('docid')

    def start_preamble(self, attrs):
        """
        """
        self._in.insert(0, 'preamble')
        self._preamb = ''

    def end_preamble(self):
        """
        """
        self._preamb = self._preamb.strip() 
        self._in.remove('preamble')

    def start_p(self, attrs):
        """
        """
        if self._in and self._in[0] == 'p':
            self._p.append(self._current)
            self._current = None
            self._in.remove('p')
        self._in.insert(0, 'p')
        if self._p == None:
            self._p = []
        self._current = ''

    def start_trailer(self, attrs):
        """
        """
        self._in.insert(0, 'trailer')
        self._trailer = ''

    def end_trailer(self):
        """
        """
        self._trailer = self._trailer.strip()
        self._in.remove('trailer')

    def handle_data(self, data):
        """
        """
        if self._in and self._in[0] == 'docid':
            self._docid += data
        if self._in and self._in[0] == 'preamble':
            self._preamb += data.replace('\n', '')
        if self._in and self._in[0] == 'p':
            self._current += data.replace('\n', '')
        if self._in and self._in[0] == 'trailer':
            self._trailer += data.replace('\n', '')

    def parse(self, filename):
        """
        """
        file = open(filename)
        for line in file:
            self.feed(line)
        file.close()
        self._parsed = True
        return self

    def text(self):
        """
        """
        assert self._parsed and self._p
        return self._p

    def docid(self):
        """
        """
        assert self._parsed and self._docid
        return self._docid

    def preamble(self):
        """
        """
        assert self._parsed and self._preamb
        return self._preamb

    def trailer(self):
        """
        """
        assert self._parsed and self._trailer
        return self._trailer

def _demo(root, file):
    """
    """
    import os.path
    from nltk_contrib.coref.ace2 import ACE2CorpusReader

    try:
        reader = MUC7CorpusReader(root, file)
        print 'Paragraphs for %s:' % (file)
        for para in reader.paras():
            print '    %s' % (para)
            print
        print 'Sentences for %s:' % (file)
        for sent in reader.sents():
            print '    %s' % (sent)
        print
        print 'Words for %s:' % (file)
        for word in reader.words():
            print '    %s' % (word)
        print
    except Exception, e:
        print 'Error encountered while running demo for %s: %s' % (file, e)
        print

def demo():
    """
    """
    import os
    import os.path

    try:
        muc7_dir = os.environ['MUC7_DIR']
    except KeyError:
        raise 'Demo requires MUC-7 Corpus, set MUC7_DIR environment variable!' 

    _demo(muc7_dir, 'dryrun01.muc7')

if __name__ == '__main__':
    demo()

########NEW FILE########
__FILENAME__ = ne
# Natural Language Toolkit (NLTK) Coreference Named Entity Components
#
# Copyright (C) 2001-2011 NLTK Project 
# Author: Joseph Frazee <jfrazee@mail.utexas.edu>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

import re
import os
import sys
import optparse

from tempfile import mkstemp

import nltk
from nltk.util import LazyMap, LazyZip
from nltk.data import BufferedGzipFile
from nltk.tree import Tree
from nltk.classify import NaiveBayesClassifier, MaxentClassifier
from nltk.tag import ClassifierBasedTagger
from nltk.tag.crf import MalletCRF
from nltk.chunk import ChunkScore
from nltk.corpus import names, gazetteers, stopwords

from nltk_contrib.coref.chunk import NaiveBayesChunkTagger, MaxentChunkTagger
from nltk_contrib.coref.tag import MXPostTaggerCorpusReader

NUMBERS = ['one', 'two', 'three', 'four', 'five', 'six', 'seven',
           'eight', 'nine', 'ten', 'eleven', 'twelve', 'thirteen',
           'fourteen', 'fifteen', 'sixteen', 'seventeen', 'eighteen',
           'nineteen', 'twenty', 'thirty', 'fourty', 'fifty',
           'sixty', 'seventy', 'eighty', 'ninety', 'hundred',
           'thousand', 'million', 'billion', 'trillion']

ORDINALS = ['first', 'second', 'third', 'fourth', 'fifth', 'sixth', 
            'seventh', 'eighth', 'ninth', 'tenth', 'eleventh', 'twelfth']

DAYS = ['monday', 'tuesday', 'wednesday', 'thursday', 
        'friday', 'saturday', 'sunday']

MONTHS = ['january', 'february', 'march', 'april', 'may', 'june', 'july',
          'august', 'september', 'october', 'november', 'december',
          'jan', 'feb', 'mar', 'apr', 'jun', 'jul', 'aug', 'sep', 'sept',
          'oct', 'nov', 'dec']

NAMES = set([name.lower() for filename in ('male.txt', 'female.txt') for name
             in names.words(filename)])

US_CITIES = set([city.lower() for city in gazetteers.words('uscities.txt')])

# [XX] contains some non-ascii chars
COUNTRIES = set([country.lower() for filename in ('isocountries.txt','countries.txt')
                 for country in gazetteers.words(filename)])

# States in North America
NA_STATES = set([state.lower() for filename in
    ('usstates.txt','mexstates.txt','caprovinces.txt') for state in
    gazetteers.words(filename)])
                     
US_STATE_ABBREVIATIONS = set([state.lower() for state in 
    gazetteers.words('usstateabbrev.txt')])

NATIONALITIES = set([nat.lower() for nat in 
    gazetteers.words('nationalities.txt')])
                     
PERSON_PREFIXES = ['mr', 'mrs', 'ms', 'miss', 'dr', 'rev', 'judge',
                   'justice', 'honorable', 'hon', 'rep', 'sen', 'sec',
                   'minister', 'chairman', 'succeeding', 'says', 'president']

PERSON_SUFFIXES = ['sr', 'jr', 'phd', 'md']

ORG_SUFFIXES = ['ltd', 'inc', 'co', 'corp', 'plc', 'llc', 'llp', 'gmbh',
                'corporation', 'associates', 'partners', 'committee',
                'institute', 'commission', 'university', 'college',
                'airlines', 'magazine', 'association', 'staff', 'family',
                'administration']

PERSON_ORG_ACTIONS = ['says', 'said', 'call', 'called', 'ask', 'asks',
                      'give', 'gave', 'who']

CURRENCY_UNITS = ['dollar', 'cent', 'pound', 'euro']

ENGLISH_PRONOUNS = ['i', 'you', 'he', 'she', 'it', 'we', 'you', 'they']

STOPWORDS = set(stopwords.words())

NUMERIC = r'(\d{1,3}(\,\d{3})*|\d+)(\.\d+)?'
RE_PUNCT = re.compile(r'[-!"#$%&\'\(\)\*\+,\./:;<=>^\?@\[\]\\\_`{\|}~]')
RE_NUMERIC = re.compile(NUMERIC)
RE_NUMBER = re.compile(r'(%s)(\s+(%s))*' % ('|'.join(NUMBERS), 
    '|'.join(NUMBERS)), re.I)
RE_QUOTE = re.compile(r'[\'"`]', re.I)
RE_ROMAN = re.compile(r'M?M?M?(CM|CD|D?C?C?C?)(XC|XL|L?X?X?X?)(IX|IV|V?I?I?I?)', re.I)
RE_INITIAL = re.compile(r'[A-Z]\.', re.I)
RE_TLA = re.compile(r'([A-Z0-9][\.\-]?){2,}', re.I)
RE_ALPHA = re.compile(r'[A-Za-z]+', re.I)
RE_DATE = re.compile(r'\d+\/\d+(\/\d+)?')
RE_CURRENCY = re.compile(r'\$\s*(%s)?' % NUMERIC)
RE_CURRENCY_UNIT = re.compile(r'%s' % ('|'.join(CURRENCY_UNITS)), re.I)
RE_PERCENT = re.compile(r'%s\s*' % NUMERIC + '%')
RE_YEAR = re.compile(r'(\d{4}s?|\d{2}s)')
RE_TIME = re.compile(r'\d{1,2}(\:\d{2})?(\s*[aApP]\.?[mM]\.?)?', re.I)
RE_ORDINAL = re.compile(r'%s' % ('|'.join(ORDINALS)))
RE_DAY = re.compile(r'%s' % ('|'.join(DAYS)), re.I)
RE_MONTH = re.compile(r'%s' % ('|'.join(MONTHS)), re.I)
RE_PERSON_PREFIX = re.compile(r'%s' % ('|'.join(PERSON_PREFIXES)), re.I)
RE_PERSON_SUFFIX = re.compile(r'%s' % ('|'.join(PERSON_SUFFIXES)), re.I)
RE_ORG_SUFFIX = re.compile(r'%s' % ('|'.join(ORG_SUFFIXES)), re.I)

class NERChunkTaggerFeatureDetector(dict):
    def __init__(self, tokens, index=0, history=None, **kwargs):
        dict.__init__(self)
        window = kwargs.get('window', 3)
        
        spelling, pos = tokens[index][:2]

        self['spelling'] = spelling
        self['word'] = spelling.lower()
        self['wordlen'] = len(spelling)
        if pos: self['pos'] = pos
        self['isupper'] = int(spelling.isupper())
        self['islower'] = int(spelling.islower())
        self['istitle'] = int(spelling.istitle())
        self['isalnum'] = int(spelling.isalnum())
        
        for i in range(2, 4):
            self['prefix_%d' % i] = spelling.lower()[:i]
            self['suffix_%d' % i] = spelling.lower()[-i:]
        
        self['ispunct'] = int(bool(RE_PUNCT.match(spelling)))
        self['isstopword'] = int(spelling.lower() in STOPWORDS)
        self['ispercent'] = int(bool(RE_PERCENT.match(spelling)))
        self['isnumber'] = int(bool(RE_PERCENT.match(spelling)))
        self['isnumeric'] = int(bool(RE_NUMERIC.match(spelling)))
        self['isquote'] = int(bool(RE_QUOTE.match(spelling)))
        self['isroman'] = int(bool(RE_ROMAN.match(spelling)))
        self['isinitial'] = int(bool(RE_INITIAL.match(spelling)))
        self['istla'] = int(bool(RE_TLA.match(spelling)))
        self['isdate'] = int(bool(RE_DATE.match(spelling)))
        self['iscurrency'] = int(bool(RE_CURRENCY.match(spelling)))
        self['iscurrencyunit'] = int(bool(RE_CURRENCY_UNIT.match(spelling)))
        self['isyear'] = int(bool(RE_YEAR.match(spelling)))
        self['istime'] = int(bool(RE_TIME.match(spelling)))
        self['isordinal'] = int(bool(RE_ORDINAL.match(spelling)))
        self['isday'] = int(bool(RE_DAY.match(spelling)))
        self['ismonth'] = int(bool(RE_MONTH.match(spelling)))        
        self['isname'] = int(spelling.lower() in NAMES)
        self['iscity'] = int(spelling.lower() in US_CITIES)
        self['isstateabbrev'] = spelling.lower() in US_STATE_ABBREVIATIONS
        self['isnastate'] = int(spelling.lower() in NA_STATES)
        self['isnationality'] = int(spelling.lower() in NATIONALITIES)
        self['personprefix'] = int(bool(RE_PERSON_PREFIX.match(spelling)))
        self['personsuffix'] = int(bool(RE_PERSON_PREFIX.match(spelling)))                                                                      
        self['orgsuffix'] = int(bool(RE_ORG_SUFFIX.match(spelling)))
        self['personorgaction'] = int(bool(spelling.lower() in PERSON_ORG_ACTIONS))
        self['endofsent'] = int(index == len(tokens) - 1)
        self['startofsent'] = int(index == 0)

        if window > 0 and index > 0:
            prev_feats = \
                self.__class__(tokens, index - 1, history, window=window - 1)
            for key, val in prev_feats.items():
                if not key.startswith('next_') and not key == 'word':
                    self['prev_%s' % key] = val

        if window > 0 and index < len(tokens) - 1:
            next_feats = self.__class__(tokens, index + 1, window=window - 1)        
            for key, val in next_feats.items():
                if not key.startswith('prev_') and not key == 'word':
                    self['next_%s' % key] = val        

        if 'prev_pos' in self:
            self['prev_pos_pair'] = '%s/%s' % \
                (self.get('prev_pos'), self.get('pos'))

        if history is not None:
            if len(history) > 0 and index > 0:
                self['prev_tag'] = history[index - 1]
            else:
                self['prev_tag'] = 'O'


def unittest(verbose=False): 
    import doctest
    failed, passed = doctest.testfile('test/ne.doctest', verbose)
    if not verbose:
        print '%d passed and %d failed.' % (failed, passed)
        if failed == 0:
            print 'Test passed.'
        else:
            print '***Test Failed*** %d failures.' % failed
    return failed, passed

_NE_CHUNK_TYPES = ('PERSON', 'LOCATION', 'ORGANIZATION', 'MONEY')
_TRAINERS = ['NaiveBayesChunkTagger.train', 'MaxentChunkTagger.train',
             'CRFChunkTagger.train']
_CORPORA = ['nltk_contrib.coref.muc6']
if __name__ == '__main__':
    import optparse

    try:
        import cPickle as pickle
    except:
        import pickle    

    import nltk_contrib
    from nltk_contrib.coref.ne import *
    from nltk_contrib.coref.chunk import NaiveBayesChunkTagger, \
        MaxentChunkTagger, CRFChunkTagger

    try:
        parser = optparse.OptionParser()
        parser.add_option('-d', '--demo', action='store_true', dest='demo',
            default=False, help='run demo')
        parser.add_option('-t', '--trainer', metavar='TRAINER', 
            dest='trainer',
            type='choice', choices=_TRAINERS, default=_TRAINERS[0], 
            help='train model using TRAINER, e.g. %s' % ', '.join(_TRAINERS))
        parser.add_option('-n', '--num-sents', metavar='TRAIN,TEST',  
            dest='numsents', type=str, default=None,
            help='number of TRAIN and TEST sentences to train model with')
        parser.add_option('-c', '--corpus', metavar='CORPUS', dest='corpus',
            type=str, default=_CORPORA[0],
            help='train model using CORPUS, e.g. %s' % ', '.join(_CORPORA))
        parser.add_option('-p', '--pos', action='store_true',
            default=False, dest='pos', help='create POS tags for CORPUS')
        parser.add_option('-m', '--model', metavar='MODEL',
            dest='model', type='str', default=None,
            help='save model file to MODEL')
        parser.add_option('-e', '--extract-features', metavar='TRAIN,TEST',
            dest='extract', type=str, default=None,
            help='extract features to TRAIN and TEST for use outside NLTK')
        parser.add_option('-u', '--unit-test', action='store_true', 
            default=False, dest='unittest', help='run unit tests')
        parser.add_option('-v', '--verbose', action='store_true', 
            default=False, dest='verbose', help='verbose')
        (options, args) = parser.parse_args()

        if options.numsents:
            m = re.match('^(?P<train>\d+)\s*(\,\s*(?P<test>\d+))?$', 
                options.numsents)
            if m:
                num_train = int(m.group('train'))
                num_test = int(m.group('test') or 0)
                options.numsents = (num_train, num_test)
            else:
                raise ValueError, "malformed argument for option -n"
        else:
            options.numsents = (None, None)
            
        if options.extract:        
            m = re.match('^(?P<train>[\w\.]+)\s*(\,\s*(?P<test>[\w\.]+))?$',
                options.extract)
            if m:
                file_train = m.group('train')
                file_test = m.group('test')
                options.extract = (file_train, file_test)
            else:
                raise ValueError, "malformed argument for option -e"            

    except ValueError, v:
        print 'error: %s' % v.message
        parser.print_help()            

    if options.unittest:
        failed, passed = unittest(options.verbose)
        sys.exit(int(bool(failed)))

    if options.demo:
        demo()
        sys.exit(0) 
        
    if options.extract:
        train_file, test_file = options.extract
                
        corpus = eval(options.corpus).iob_sents()
        
        num_train, num_test = options.numsents
        num_train = num_train or int(len(corpus) * 0.9)
        num_test = num_test or (len(corpus) - num_train)
        train = corpus[:num_train]
        test = corpus[num_train:num_train + num_test]               
        
        feature_detector = NERChunkTaggerFeatureDetector
        
        keys = set()
        
        fd, train_tmp = mkstemp('nltk-')
        stream = open(train_tmp, 'wb')
        for tokens in train:
            history = []
            for index in range(len(tokens)):
                tag = tokens[index][-1]                
                feats = feature_detector(tokens, index, history)    
                keys.update(feats.keys())                            
                stream.write('%s %s\n' % (tag, ' '.join(['%s=%s' % (k, re.escape(str(v)))
                    for k, v in feats.items()])))
                history.append(tag)                    
            history = []
        stream.close()
        
        fd, test_tmp = mkstemp('nltk-')
        stream = open(test_tmp, 'wb')
        for tokens in test:
            history = []
            for index in range(len(tokens)):
                tag = tokens[index][-1]
                feats = feature_detector(tokens, index, history)
                keys.update(feats.keys())
                stream.write('%s %s\n' % (tag, ' '.join(['%s=%s' % (k, re.escape(str(v)))
                    for k, v in feats.items()])))
                history.append(tag)
            history = []                    
        stream.close()
        
        keys = list(keys)
        keys.sort()
        
        stream = open(train_file, 'wb')
        stream.write('iob_tag,%s\n' % ','.join(keys))        
        for line in open(train_tmp, 'rb'):
            data = line.split()
            tag = data.pop(0)            
            feats = dict([tuple(f.split('=', 1)) for f in data])
            stream.write('"%s",%s\n' % (tag, ','.join(['"%s"' % feats.get(k, '') for k in keys])))
        stream.close() 
        os.remove(train_tmp)        
        
        stream = open(test_file, 'wb')
        stream.write('iob_tag,%s\n' % ','.join(keys))
        for line in open(test_tmp, 'rb'):
            data = line.split()
            tag = data.pop(0)
            feats = dict([tuple(f.split('=', 1)) for f in data])
            stream.write('"%s",%s\n' % (tag, ','.join(['"%s"' % feats.get(k, '') for k in keys])))
        stream.close()      
        os.remove(test_tmp)     
        
        sys.exit(0)                

    if options.trainer:
        if options.pos:
            reader = MXPostTaggerCorpusReader(eval(options.corpus))
            iob_sents = reader.iob_sents()
            tagged_sents = reader.tagged_sents()
            corpus = LazyMap(lambda (iob_sent, tagged_sent): 
                    [(iw, tt, iob) for ((iw, iob), (tw, tt))
                     in zip(iob_sent, tagged_sent)], 
                 LazyZip(iob_sents, tagged_sents))
        else:
            iob_sents = eval(options.corpus).iob_sents()
            corpus = LazyMap(lambda iob_sent:
                [(w, None, i) for w, i in iob_sent], iob_sents)

        num_train, num_test = options.numsents
        num_train = num_train or int(len(corpus) * 0.9)
        num_test = num_test or (len(corpus) - num_train)
        train = corpus[:num_train]
        test = corpus[num_train:num_train + num_test]

        trainer = eval(options.trainer)        
        if options.verbose:
            print 'Training %s with %d sentences' % \
                (options.trainer, num_train)
        ner = trainer(train, 
            feature_detector=NERChunkTaggerFeatureDetector,
            chunk_types=_NE_CHUNK_TYPES,
            verbose=options.verbose)

        if options.model:
            options.model = os.path.abspath(options.model)
            try:
                if ner.__class__ == CRFChunkTagger:
                    pass
                else:
                    if options.model.endswith('.gz'):
                        _open = BufferedGzipFile
                    else:
                        _open = open                    
                    stream = _open(options.model, 'w')
                    pickle.dump(ner, stream)
                    stream.close()                    
                    ner = pickle.load(_open(options.model, 'r'))
                if options.verbose:
                    print 'Model saved as %s' % options.model                    
            except Exception, e:
                print "error: %s" % e

        if test:
            if options.verbose:
                print 'Testing %s on %d sentences' % \
                    (options.trainer, num_test)
            ner.test(test, verbose=options.verbose)

########NEW FILE########
__FILENAME__ = resolve
# Natural Language Toolkit (NLTK) Coreference Resolver
#
# Copyright (C) 2001-2011 NLTK Project 
# Author: Joseph Frazee <jfrazee@mail.utexas.edu>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

import gzip
import time
import optparse

try:
    import cPickle as pickle
except:
    import pickle
    
try:
    from cStringIO import StringIO
except:
    from StringIO import StringIO

from nltk.util import LazyMap, LazyZip, LazyConcatenation, LazyEnumerate

from nltk.corpus import BracketParseCorpusReader, ConllChunkCorpusReader
from nltk.corpus.util import LazyCorpusLoader

from nltk.tag.hmm import HiddenMarkovModelTagger

from nltk_contrib.coref.api import *

from nltk_contrib.coref import MUC6CorpusReader

from nltk_contrib.coref.features import *

from nltk_contrib.coref.chunk import HiddenMarkovModelChunkTagger, \
     ClosedCategoryChunkTransform
    
from nltk_contrib.coref.ne import BaselineNamedEntityChunkTagger, \
     NamedEntityChunkTransform, NamedEntityFeatureDetector, \
     NamedEntityClassifier, NamedEntityFeatureDetector2
     
from nltk_contrib.coref.util import LidstoneProbDistFactory, \
    TreebankTaggerCorpusReader, TreebankChunkTaggerCorpusReader, \
    zipzip, load_treebank

TREEBANK_CLOSED_CATS = set(['CC', 'DT', 'MD', 'POS', 'PP$', 'RP', 'TO', 'WDT',
                            'WP$', 'EX', 'IN', 'PDT', 'PRP', 'WP', 'WRB'])

class BaselineCorefResolver(CorefResolverI):
    def __init__(self):
        self._chunk_tagger = BaselineNamedEntityChunkTagger()

    def resolve_mention(self, mentions, index, history):
        mention, mention_id, sent_index, chunk_index = mentions[index]
        
        # If the mention is a pronoun and there is an antecedent available,
        # link it to the most recent mention.
        if history and isinstance(mention, str) and is_pronoun(mention):
            prev_mention, prev_mention_id = history[-1][:2]
            return (mention, prev_mention_id, sent_index, chunk_index)
        
        for previous_mention, previous_mention_id, previous_sent_index, \
            previous_chunk_index in history:
            
            M = set([word.lower() for word in mention if len(word.lower()) > 2])
            P = set([word.lower() for word in previous_mention if len(word.lower()) > 2])
            
            # If the current mention and a previous mention have some
            # non-trivial overlap of words, then link the current mention to
            # that previous mention.
            if M.intersection(P):
                return (mention, previous_mention_id, sent_index, chunk_index)
        
        # Otherwise, the current mention does not co-refer with any previous
        # mentions.
        return (mention, mention_id, sent_index, chunk_index)
            
    def mentions(self, discourse):
        mentions = []
        
        chunked_discourse = LazyMap(self._chunk_tagger.chunk, discourse)
        
        mention_id = 0       
        for sent_index, chunked_sent in LazyEnumerate(chunked_discourse):
            for chunk_index, chunk in LazyEnumerate(chunked_sent):
                if isinstance(chunk, list) or is_pronoun(chunk):
                    mentions.append((chunk, mention_id, sent_index, chunk_index))
                    mention_id += 1

        return mentions

    def resolve_mentions(self, mentions, **kwargs):
        history = []
        for index, (mention, i, j, k) in LazyEnumerate(mentions):
            mentions[index] = self.resolve_mention(mentions, index, history)
            history.append(mentions[index])
        return mentions
    
    def resolve(self, discourse, **kwargs):
        resolved_discourse = []
                
        mentions = {}
        for mention, mention_id, sent_index, chunk_index \
        in self.resolve_mentions(self.mentions(discourse)):
            mentions[(sent_index, chunk_index)] = (mention, mention_id)
        
        chunked_discourse = LazyMap(self._chunk_tagger.chunk, discourse)        
        for sent_index, sent in LazyEnumerate(chunked_discourse):
            resolved_discourse.append([])
            for chunk_index, chunk in LazyEnumerate(sent):
                mention, mention_id = \
                    mentions.get((sent_index, chunk_index), (None, None))
                resolved_discourse[sent_index].append((chunk, mention_id))                    

        return resolved_discourse

def baseline_coref_resolver_demo():
    from nltk.corpus.util import LazyCorpusLoader
    from nltk.corpus import BracketParseCorpusReader
    from nltk_contrib.coref.resolve import BaselineCorefResolver
    
    resolver = BaselineCorefResolver()
    treebank = load_treebank('0[12]')
    
    sents = LazyMap(lambda sent: \
            [word for word in sent if not word.startswith('*')],
        treebank.sents()[:10])
    mentions = resolver.mentions(sents)
    resolved_mentions = resolver.resolve_mentions(mentions)
    resolved_discourse = resolver.resolve(sents)
        
    print 'Baseline coref resolver demo...'
    print 'Mentions:'
    for mention in mentions:
        print mention
    print
    print 'Resolved mentions:'
    for mention in resolved_mentions:
        print mention
    print
    print 'Resolved discourse:'
    for sent in resolved_discourse:
        print sent
        print
    print
    
def demo():
    print 'Demo...'
    baseline_coref_resolver_demo()
    # muc6_test = LazyCorpusLoader(
    #         'muc6', MUC6CorpusReader, 
    #         r'.*\-(01[8-9][0-9])\..*\.sgm')
    # 
    # test_iob_sents = muc6_test.iob_sents()
    # gold_iob_tags = LazyMap(lambda sent: \
    #         [iob_tag for (word, iob_tag) in sent],
    #     test_iob_sents)
    # 
    # muc6_test = ChunkTaggerCorpusReader(muc6_test, load(MUC6_NE_CHUNKER))
    # predicted_chunked_sents = muc6_test.chunked_sents()
    # predicted_iob_tags = LazyMap(lambda sent: \
    #         [iob_tag for (word, tag, iob_tag) in sent],
    #     predicted_chunked_sents)
    # 
    # for token in LazyZip(LazyConcatenation(predicted_iob_tags),
    #                      LazyConcatenation(gold_iob_tags),
    #                      LazyConcatenation(test_iob_sents)):
    #     print token
    # print
    # 
    # evaluate(gold_iob_tags, predicted_iob_tags)
    # 
    # return
    # 
    # conll2000_test = LazyCorpusLoader(
    #      'conll2000', ConllChunkCorpusReader, ['test.txt'], ('NP','VP','PP')) 
    # tagger_reader = TreebankTaggerCorpusReader(conll2000_test)
    # chunker_reader = TreebankChunkerCorpusReader(conll2000_test)
    # for sent in tagger_reader.tagged_sents()[:1]:
    #     print sent
    #     print
    # for sent in chunker_reader.parsed_sents()[:5]:
    #     print sent
    #     print

if __name__ == '__main__':
    print time.ctime(time.time())
        
    parser = optparse.OptionParser()
    parser.add_option('-d', '--demo', action='store_true', dest='demo',
                      default=True, help='run demo')
    parser.add_option('-t', '--train-tagger', action='store_true',
                      default=False, dest='train_tagger', 
                      help='train Treebank POS tagger')
    parser.add_option('-c', '--train-chunker', action='store_true',
                      default=False, dest='train_chunker', 
                      help='train Treebank chunker')
    parser.add_option('-n', '--train-ner', metavar='NER_COMPONENT',
                      dest='train_ner', type='choice',
                      choices=('chunker', 'classifier', 'classifier2', 'recognizer'),
                      help='train NER components (chunker, classifier, classifier2, recognizer)')
    parser.add_option('-f', '--model-file', metavar='FILE',
                      dest='model_file', help='save model to FILE')
    parser.add_option('-e', '--num-test-sents', metavar='NUM_TEST',
                      dest='num_test_sents', type=int, 
                      help='number of test sentences')
    parser.add_option('-r', '--num-train-sents', metavar='NUM_TRAIN',
                      dest='num_train_sents', type=int, 
                      help='number of training sentences')
    parser.add_option('-p', '--psyco', action='store_true',
                      default=False, dest='psyco',
                      help='use Psyco JIT, if available')
    parser.add_option('-v', '--verbose', action='store_true',
                      default=False, dest='verbose',
                      help='verbose')
    (options, args) = parser.parse_args()

    if options.psyco:
        try:
            import psyco
            psyco.profile()
        except:
            pass

    if options.train_tagger:
        treebank_train = load_treebank('0[2-9]|1[0-9]|2[01]')
        treebank_train_sequence = treebank_train.tagged_sents()
        treebank_test = load_treebank('24')
        treebank_test_sequence = treebank_test.tagged_sents()
        treebank_estimator = LidstoneProbDistFactory
        model = train_model(HiddenMarkovModelTagger, 
                            treebank_train_sequence, 
                            treebank_test_sequence,
                            options.model_file, 
                            options.num_train_sents, 
                            options.num_test_sents,
                            estimator=treebank_estimator,
                            verbose=options.verbose)

    elif options.train_chunker:
        conll2k_train = LazyCorpusLoader(
            'conll2000', ConllChunkCorpusReader, 
            ['train.txt'], ('NP','VP','PP'))
        conll2k_train_sequence = conll2k_train.iob_sents()
        conll2k_test = LazyCorpusLoader(
            'conll2000', ConllChunkCorpusReader,
            ['test.txt'], ('NP','VP','PP'))
        conll2k_test_sequence = conll2k_test.iob_sents()
        conll2k_estimator = LidstoneProbDistFactory
        conll2k_transform = ClosedCategoryChunkTransform(TREEBANK_CLOSED_CATS)
        model = train_model(HiddenMarkovModelChunkTagger, 
                            conll2k_train_sequence, 
                            conll2k_test_sequence,
                            options.model_file, 
                            options.num_train_sents, 
                            options.num_test_sents,
                            estimator=conll2k_estimator,
                            transform=conll2k_transform,
                            verbose=options.verbose)
        
    elif options.train_ner == 'chunker':
        muc6_train = TreebankTaggerCorpusReader(
            LazyCorpusLoader(
                'muc6', MUC6CorpusReader, 
                r'.*\-(0[01][0-7][0-9])\..*\.sgm'))
        muc6_train_sequence = LazyMap(lambda token: \
                [(word, tag, iob_tag) 
                 for ((word, tag), (word, iob_tag)) in token],
            zipzip(muc6_train.tagged_sents()[:options.num_train_sents], 
                   muc6_train.iob_sents()[:options.num_train_sents]))
        muc6_test = TreebankTaggerCorpusReader(
            LazyCorpusLoader(
                'muc6', MUC6CorpusReader, 
                r'.*\-(01[8-9][0-9])\..*\.sgm'))
        muc6_test_sequence = LazyMap(lambda token: \
                [(word, tag, iob_tag) 
                 for ((word, tag), (word, iob_tag)) in token],
            zipzip(muc6_test.tagged_sents()[:options.num_test_sents], 
                   muc6_test.iob_sents()[:options.num_test_sents]))
        muc6_estimator = LidstoneProbDistFactory
        muc6_transform = NamedEntityChunkTransform(TREEBANK_CLOSED_CATS)
        model = train_model(HiddenMarkovModelChunkTagger, 
                            muc6_train_sequence, 
                            muc6_test_sequence,
                            options.model_file, 
                            options.num_train_sents, 
                            options.num_test_sents,
                            estimator=muc6_estimator,
                            transform=muc6_transform,
                            verbose=options.verbose)

    elif options.train_ner == 'classifier':
        def join(chunk):
            if isinstance(chunk, tuple):
                word, iob_tag, ne_tag = chunk
            else:
                word = ' '.join([word for (word, iob_tag, ne_tag) in chunk])
                ne_tag = chunk[0][-1]
            return [(word, ne_tag)]
        muc6_train = LazyCorpusLoader(
                        'muc6', MUC6CorpusReader, 
                        r'.*\-(0[01][0-7][0-9])\..*\.sgm')
        muc6_train_sequence = LazyMap(join, muc6_train.ne_chunks())
        muc6_test = LazyCorpusLoader(
                        'muc6', MUC6CorpusReader, 
                        r'.*\-(01[8-9][0-9])\..*\.sgm')
        muc6_test_sequence = LazyMap(join, muc6_test.ne_chunks())
        model = train_model(NamedEntityClassifier, 
                            muc6_train_sequence, 
                            muc6_test_sequence,
                            options.model_file, 
                            options.num_train_sents, 
                            options.num_test_sents,
                            feature_detector=NamedEntityFeatureDetector,
                            out_tag=None,                 
                            verbose=options.verbose)
                            
        from nltk.tag.util import *
        for sent in muc6_test_sequence[:options.num_test_sents]:
            words = untag(sent)
            gold_tags = tags(sent)
            pred_tags = model.tag(words)
            for x, y, z in zip(pred_tags, gold_tags, words):
                if x == y:
                    print '  ', (x, y, z)
                else:
                    print '* ', (x, y, z)

    elif options.train_ner == 'classifier2':
        muc6_train = LazyCorpusLoader(
                        'muc6', MUC6CorpusReader, 
                        r'.*\-(0[01][0-7][0-9])\..*\.sgm')
        muc6_train_sequence = muc6_train.iob_sents()
        muc6_test = LazyCorpusLoader(
                        'muc6', MUC6CorpusReader, 
                        r'.*\-(01[8-9][0-9])\..*\.sgm')
        muc6_test_sequence = muc6_test.iob_sents()
        model = train_model(NamedEntityClassifier, 
                            muc6_train_sequence, 
                            muc6_test_sequence,
                            options.model_file, 
                            options.num_train_sents, 
                            options.num_test_sents,
                            feature_detector=NamedEntityFeatureDetector2,
                            out_tag='O',                 
                            verbose=options.verbose)
                            
        from nltk.tag.util import *
        for sent in muc6_test_sequence[:options.num_test_sents]:
            words = untag(sent)
            gold_tags = tags(sent)
            pred_tags = model.tag(words)
            for x, y, z in zip(pred_tags, gold_tags, words):
                if x == y:
                    print '  ', (x, y, z)
                else:
                    print '* ', (x, y, z)

    elif options.demo:
        demo()

    print time.ctime(time.time())

########NEW FILE########
__FILENAME__ = tag
import os
import re
import subprocess

try:
    from cStringIO import StringIO
except:
    from StringIO import StringIO

from nltk.util import LazyMap, LazyConcatenation
from nltk.internals import find_binary, java
from nltk.tag import TaggerI

from nltk_contrib.coref import CorpusReaderDecorator

class TaggerCorpusReader(CorpusReaderDecorator):
    """
    A C{CorpusReaderDecorator} that adds tagger functionality to an arbitrary
    C{CorpusReader}.
    """
    
    def __init__(self, reader, **kwargs):
        """
        @return: a corpus reader
        @rtype: C{TaggerCorpusReader}
        @param reader: the corpus reader to decorate
        @type reader: C{CorpusReader}
        @kwparam tagger: a tagger object to defer tagging to
        @type tagger: C{TaggerI}
        """
        self._tagger = kwargs.get('tagger') 
        CorpusReaderDecorator.__init__(self, reader, **kwargs)
    
    def tagged_sents(self):
        return LazyMap(self._tagger.tag, self.sents())
    
    def tagged_words(self):
        return LazyConcatenation(LazyMap(self._tagger.tag, self.sents()))

    def tagger(self):
        return self._tagger
        
        
class MXPostTaggerCorpusReader(TaggerCorpusReader):
    def __init__(self, reader, **kwargs):
        kwargs['tagger'] = MXPostTagger()
        TaggerCorpusReader.__init__(self, reader, **kwargs)
        
    def tagged_sents(self):
        sents = self.sents()
        batch_indices = range(len(sents) / 1024 + 1)
        return LazyConcatenation(LazyMap(lambda i: 
                self._tagger.batch_tag(sents[i * 1024: i * 1024 + 1024]),
            batch_indices))


class MXPostTagger(TaggerI):
    def tag(self, tokens):
        return self.batch_tag([tokens])[0]
    
    def batch_tag(self, sents):
        return mxpost_tag(sents)
        

_mxpost_home = None
_mxpost_classpath = None
def config_mxpost(mxpost_home=None):
    global _mxpost_classpath, _mxpost_home
    classpath = os.environ.get('CLASSPATH', '').split(':')
    mxpost_jar = filter(lambda c: c.endswith('mxpost.jar'), classpath)
    if mxpost_jar:
        _mxpost_home = os.path.dirname(mxpost_jar[0])
        _mxpost_classpath = mxpost_jar[0]
    elif os.environ.get('MXPOST'):
        _mxpost_home = os.environ.get('MXPOST')
        _mxpost_classpath = '%s/mxpost.jar' % os.environ.get('MXPOST')
    elif os.environ.get('MXPOST_HOME'):
        _mxpost_home = os.environ.get('MXPOST_HOME')
        _mxpost_classpath = '%s/mxpost.jar' % os.environ.get('MXPOST_HOME')
    elif os.path.exists('/usr/local/mxpost/mxpost.jar'):
        _mxpost_home = '/usr/local/mxpost'
        _mxpost_classpath = '/usr/local/mxpost/mxpost.jar'
    else:
        _mxpost_home = None
        _mxpost_classpath = None
        raise Exception, "can't find mxpost.jar"

def call_mxpost(classpath=None, stdin=None, stdout=None, stderr=None,
                blocking=False):
    if not classpath:
        config_mxpost()
    
    if not classpath:
        classpath = _mxpost_classpath
    elif 'mxpost.jar' not in classpath:
        classpath += ':%s' % _mxpost_classpath
    
    cmd = ['tagger.TestTagger', '%s/%s' % (_mxpost_home, 'wsj-02-21.mxpost')]
    return java(cmd, classpath, stdin, stdout, stderr, blocking)

_MXPOST_OUTPUT_RE = \
    re.compile(r'^\s*(?P<word>\S+)\_(?P<tag>\S+)\s*$')
def mxpost_parse_output(mxpost_output):
    result = []
    mxpost_output = mxpost_output.strip()
    for sent in filter(None, mxpost_output.split('\n')):
        tokens = filter(None, re.split(r'\s+', sent))
        if tokens:
            result.append([])
        for token in tokens:
            m = _MXPOST_OUTPUT_RE.match(token)
            if not m:
                raise Exception, "invalid mxpost tag pattern: %s, %s" % (token, tokens)
            word = m.group('word')
            tag = m.group('tag')
            result[-1].append((word, tag))
    return result

def mxpost_tag(sents, **kwargs):
    p = call_mxpost(stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    stdout, stderr = \
        p.communicate('\n'.join([' '.join(sent) for sent in sents]))
    rc = p.returncode
    if rc != 0:
        raise Exception, 'exited with non-zero status %s' % rc
    if kwargs.get('verbose'):
        print 'warning: %s' % stderr
    return mxpost_parse_output(stdout)
########NEW FILE########
__FILENAME__ = train
# Natural Language Toolkit (NLTK) Coreference Training Utilities
#
# Copyright (C) 2001-2011 NLTK Project 
# Author: Joseph Frazee <jfrazee@mail.utexas.edu>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

import gzip
import time

from nltk.util import LazyMap
from nltk.classify import MaxentClassifier
from nltk.metrics.scores import accuracy
from nltk.probability import LidstoneProbDist

try:
    from nltk.data import BufferedGzipFile
except:
    from nltk_contrib.coref.data import BufferedGzipFile

try:
    import cPickle as pickle
except:
    import pickle
    
try:
    from cStringIO import StringIO
except:
    from StringIO import StringIO
    
    
class LidstoneProbDistFactory(LidstoneProbDist):
    def __init__(self, fd, *args, **kwargs):
        LidstoneProbDist.__init__(self, fd, 0.01, args[-1])
        samples = fd.samples()
        self._probs = dict(zip([0]*len(samples), samples))
        self._logprobs = dict(zip([0]*len(samples), samples))        
        for sample in samples:
            self._logprobs[sample] = LidstoneProbDist.logprob(self, sample)
            self._probs[sample] = LidstoneProbDist.prob(self, sample)

    def logprob(self, sample):
        if sample not in self._logprobs:
            self._logprobs[sample] = LidstoneProbDist.logprob(self, sample)
        return self._logprobs.get(sample)

    def prob(self, sample):
        if sample not in self._probs:
            self._probs[sample] = LidstoneProbDist.prob(self, sample)
        return self._probs.get(sample)
    

class MegamMaxentClassifier(MaxentClassifier):
    @classmethod
    def train(cls, training_sequence, **kwargs):
        feature_detector = kwargs.get('feature_detector')
        gaussian_prior_sigma = kwargs.get('gaussian_prior_sigma', 10)
        count_cutoff = kwargs.get('count_cutoff', 1)
        stopping_condition = kwargs.get('stopping_condition', 1e-7)
        def __featurize(tagged_token):
            tag = tagged_token[-1]
            feats = feature_detector(tagged_token)
            return (feats, tag)
        labeled_featuresets = LazyMap(__featurize, training_sequence)
        classifier = MaxentClassifier.train(labeled_featuresets,
                                algorithm='megam',
                                gaussian_prior_sigma=gaussian_prior_sigma,
                                count_cutoff=count_cutoff,
                                min_lldelta=stopping_condition)
        return cls(classifier._encoding, classifier.weights())

    def test(self, test_sequence, **kwargs):
        feature_detector = kwargs.get('feature_detector')        
        def __tags(token):
            return token[-1]
        def __untag(token):
            return token[0]
        def __featurize(tagged_token):
            tag = tagged_token[-1]
            feats = feature_detector(tagged_token)
            return (feats, tag)
        count = sum([len(sent) for sent in test_sequence])
        correct_tags = LazyMap(__tags, test_sequence)        
        untagged_sequence = LazyMap(__untag, LazyMap(__featurize, test_sequence))
        predicted_tags = LazyMap(self.classify, untagged_sequence)
        acc = accuracy(correct_tags, predicted_tags)
        print 'accuracy over %d tokens: %.2f' % (count, acc)
        
        
class MaxentClassifierFactory(object):
    def __init__(self, **kwargs):
        self._maxent_classifier_class = \
            kwargs.get('maxent_classifier_class', MegamMaxentClassifier)    
        self._feature_detector = kwargs.get('feature_detector')
        self._gaussian_prior_sigma = kwargs.get('gaussian_prior_sigma', 10)
        self._count_cutoff = kwargs.get('count_cutoff', 1)
        self._stopping_condition = kwargs.get('stopping_condition', 1e-7)        
        
    def train(self, training_sequence, **kwargs):
        kwargs = {
            'feature_detector':self._feature_detector,
            'gaussian_prior_sigma':self._gaussian_prior_sigma,
            'count_cutoff':self._count_cutoff,
            'min_lldelta':self._stopping_condition,
        }
        return self._maxent_classifier_class.train(training_sequence, **kwargs)


def train_model(train_class, labeled_sequence, test_sequence, pickle_file,
                num_train_sents, num_test_sents, **kwargs):
    """
    Train a C{TrainableI} object.
    
    @param train_class: the C{TrainableI} type to be trained.
    @type train_class: C{type}
    @labeled_sequence: a sequence of labeled training instances.
    @type labeled_sequence: C{list} of C{list}
    @param pickle_file: the path to save the pickled model to.
    @type pickle_file: C{str}
    @param num_train_sents: the number of sentences to train on. 
    @type num_train_sents: C{int}
    @param num_test_sents: the number of sentences to test on.
    @type num_test_sents: C{int}
    @kwparam verbose: boolean flag indicating whether training should be
        verbose or include printed output.
    @type verbose: C{bool}
    """
    print 'Training ', train_class
    print 'Loading training data (supervised)...'

    labeled_sequence = labeled_sequence[:num_train_sents]
    sent_count = len(labeled_sequence)
    word_count = sum([len(sent) for sent in labeled_sequence])

    print '%s sentences' % (sent_count)
    print '%s words' % (word_count)

    print 'Training...'

    start = time.time()
    model = train_class.train(labeled_sequence, **kwargs)
    end = time.time()

    print 'Training time: %.3fs' % (end - start)
    print 'Training time per sentence: %.3fs' % (float(end - start) / sent_count)    
    print 'Training time per word: %.3fs' % (float(end - start) / word_count)

    print 'Loading test data...'

    test_sequence = test_sequence[:num_test_sents]
    sent_count = len(test_sequence)
    word_count = sum([len(sent) for sent in test_sequence])

    print '%s sentences' % (sent_count)
    print '%s words' % (word_count)

    try:
        print 'Saving model...'        
        if isinstance(pickle_file, str):
            if pickle_file.endswith('.gz'):
                 _open = BufferedGzipFile
            else:
                 _open = open
            stream = _open(pickle_file, 'wb')
            pickle.dump(model, stream)
            stream.close()
            model = pickle.load(_open(pickle_file, 'rb'))
            print 'Model saved as %s' % pickle_file
        else:
            stream = StringIO()
            pickle.dump(model, stream)
            stream = StringIO(stream.getvalue())
            model = pickle.load(stream)
    except Exception, e:
        print 'Error saving model, %s' % str(e)

    print 'Testing...'

    start = time.time()
    model.test(test_sequence, **kwargs)
    end = time.time()    

    print 'Test time: %.3fs' % (end - start)
    print 'Test time per sentence: %.3fs' % (float(end - start) / sent_count)    
    print 'Test time per word: %.3fs' % (float(end - start) / word_count)

    return model

########NEW FILE########
__FILENAME__ = util
# Natural Language Toolkit (NLTK) Coreference Utilities
#
# Copyright (C) 2001-2011 NLTK Project 
# Author: Joseph Frazee <jfrazee@mail.utexas.edu>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

import time

try:
    import cPickle as pickle
except:
    import pickle
    
try:
    from cStringIO import StringIO
except:
    from StringIO import StringIO

from nltk.data import load, find
from nltk.corpus import CorpusReader, BracketParseCorpusReader
from nltk.util import LazyMap, LazyConcatenation, LazyZip

#from nltk.tag.sequential import ClassifierBasedTagger

#from nltk.classify import ClassifierI
#from nltk.classify.maxent import MaxentClassifier, BinaryMaxentFeatureEncoding
#from nltk.classify.naivebayes import NaiveBayesClassifier

from nltk.probability import LidstoneProbDist

from nltk_contrib.coref import *
from nltk_contrib.coref.ne import *
from nltk_contrib.coref.chunk import *
from nltk_contrib.coref.features import *

TREEBANK_TAGGER = 'nltk:taggers/treebank.tagger.pickle.gz'

TREEBANK_CHUNKER = 'nltk:chunkers/treebank.chunker.pickle.gz'

MUC6_CHUNK_TAGGER = 'nltk:chunkers/muc6.chunk.tagger.pickle.gz'
        
class LidstoneProbDistFactory(LidstoneProbDist):
    def __init__(self, fd, bins, *factory_args):
        LidstoneProbDist.__init__(self, fd, 0.1, bins)


class TaggerCorpusReader(CorpusReaderDecorator):
    """A decorator.
    """

    def __init__(self, reader, **kwargs):
        self._tagger = kwargs.get('tagger') 
        CorpusReaderDecorator.__init__(self, reader, **kwargs)
    
    def tagged_sents(self):
        return LazyMap(self._tagger.tag, self.sents())
    
    def tagged_words(self):
        return LazyConcatenation(LazyMap(self._tagger.tag, self.sents()))

    def tagger(self):
        return self._tagger


class ChunkTaggerCorpusReader(CorpusReaderDecorator):
    """A decorator.
    """
    
    def __init__(self, reader, **kwargs):
        reader = TaggerCorpusReader(reader, tagger=kwargs.get('tagger'))
        self._chunker = kwargs.get('chunker')
        CorpusReaderDecorator.__init__(self, reader, **kwargs)

    def chunked_sents(self):
        return LazyMap(self._chunker.chunk, self.tagged_sents())

    def chunker(self):
        return self._chunker

    def tagger(self):
        return self._tagger

        
class TreebankTaggerCorpusReader(TaggerCorpusReader):
    """A decorator.
    """

    def __init__(self, reader):
        tagger = load(TREEBANK_TAGGER)
        TaggerCorpusReader.__init__(self, reader, tagger=tagger)
            
       
class TreebankChunkTaggerCorpusReader(ChunkTaggerCorpusReader):
    """A decorator.
    """
            
    def __init__(self, reader):
        chunker = load(TREEBANK_CHUNKER)
        tagger = load(TREEBANK_TAGGER)
        ChunkTaggerCorpusReader.__init__(self, reader, 
                                               chunker=chunker, tagger=tagger)
    
    def parsed_sents(self):
        return LazyMap(self._chunker.parse, self.tagged_sents())


class MUC6NamedEntityChunkTaggerCorpusReader(ChunkTaggerCorpusReader):
    def __init__(self, reader):
        chunker = load(MUC6_CHUNK_TAGGER)
        tagger = load(TREEBANK_TAGGER)
        ChunkTaggerCorpusReader.__init__(self, reader, 
                                               chunker=chunker, tagger=tagger)

	
def zipzip(*lists):
    return LazyMap(lambda lst: zip(*lst), LazyZip(*lists))

def load_treebank(sections):
    treebank_path = os.environ.get('NLTK_TREEBANK', 'treebank/combined')
    treebank = LazyCorpusLoader(
        treebank_path,
        BracketParseCorpusReader, 
        r'(%s\/)?wsj_%s.*\.mrg' % (sections, sections))
    return treebank

def treebank_tagger_demo():
    from nltk.corpus.util import LazyCorpusLoader    
    from nltk.corpus.reader import PlaintextCorpusReader
    from nltk_contrib.coref.util import TreebankTaggerCorpusReader
    
    state_union = LazyCorpusLoader(
        'state_union', PlaintextCorpusReader, r'(?!\.svn).*\.txt')
    state_union = TreebankTaggerCorpusReader(state_union)
    
    print 'Treebank tagger demo...'
    print 'Tagged sentences:'
    for sent in state_union.tagged_sents()[500:505]:
        print sent
        print
    print
    print 'Tagged words:'
    for word in state_union.tagged_words()[500:505]:
        print word
    print

def treebank_chunk_tagger_demo():
    from nltk.corpus.util import LazyCorpusLoader    
    from nltk.corpus.reader import PlaintextCorpusReader
    from nltk_contrib.coref.util import TreebankChunkTaggerCorpusReader
    
    state_union = LazyCorpusLoader(
        'state_union', PlaintextCorpusReader, r'(?!\.svn).*\.txt')
    state_union = TreebankChunkTaggerCorpusReader(state_union)

    print 'Treebank chunker demo...'
    print 'Chunked sentences:'
    for sent in state_union.chunked_sents()[500:505]:
        print sent
        print
    print
    print 'Parsed sentences:'
    for tree in state_union.parsed_sents()[500:505]:
        print tree
        print
    print
    
def muc6_chunk_tagger_demo():
    from nltk.corpus.util import LazyCorpusLoader
    from nltk.corpus import BracketParseCorpusReader
    from nltk_contrib.coref.util import MUC6NamedEntityChunkTaggerCorpusReader
     
    treebank = MUC6NamedEntityChunkTaggerCorpusReader(load_treebank('0[12]'))
    
    print 'MUC6 named entity chunker demo...'
    print 'Chunked sentences:'
    for sent in treebank.chunked_sents()[:10]:
        print sent
        print      
    print

def baseline_chunk_tagger_demo():
    from nltk.corpus.util import LazyCorpusLoader
    from nltk.corpus import BracketParseCorpusReader
    
    chunker = BaselineNamedEntityChunkTagger()
    treebank = load_treebank('0[12]')
    
    print 'Baseline named entity chunker demo...'
    print 'Chunked sentences:'
    for sent in treebank.sents()[:10]:
        print chunker.chunk(sent)
        print
    print 'IOB-tagged sentences:'
    for sent in treebank.sents()[:10]:
        print chunker.tag(sent)
        print
    print

def demo():
    from nltk_contrib.coref.util import treebank_tagger_demo, \
         treebank_chunk_tagger_demo, muc6_chunk_tagger_demo
    treebank_tagger_demo()
    treebank_chunk_tagger_demo()
    muc6_chunk_tagger_demo()
    
if __name__ == '__main__':
    try:
        import psyco
        psyco.profile()
    except:
        pass
    demo()

########NEW FILE########
__FILENAME__ = deptree
# Natural Language Toolkit: Dependency Trees
#
# Author: Ewan Klein <ewan@inf.ed.ac.uk>
#
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

"""
Tools for reading and writing dependency trees.
The input is assumed to be in U{Malt-TAB<http://w3.msi.vxu.se/~nivre/research/MaltXML.html>} format.

Currently only reads the first tree in a file.
"""

from nltk.parse import Tree
from pprint import pformat
import re


class DepGraph(object):
    """
    A container for the nodes and labelled edges of a dependency structure.
    """
    def __init__(self):
        """
        We place a dummy 'top' node in the first position 
        in the nodelist, since the root node is often assigned '0'
        as its head. This also means that the indexing of the nodelist
        corresponds directly to the Malt-TAB format, which starts at 1.
        """
        top = {'word':None, 'deps':[], 'rel': 'TOP'}
        self.nodelist = [top]
        self.root = None
        self.stream = None

    def __str__(self):
#        return '\n'.join([str(n) for n in self.nodelist])
        return '\n'.join([', '.join(['%s: %15s'%item for item in n.iteritems()]) for n in self.nodelist])
                        
    def load(self, file):
        """
        @param file: a file in Malt-TAB format
        """
        input = open(file).read()
        return self.read(input)
    
    def _normalize(self, line):
        """
        Deal with lines in which spaces are used rather than tabs.
        """
        import re
        SPC = re.compile(' +')
        return re.sub(SPC, '\t', line)

    def read(self, input):
        """
        @param input: a string in Malt-TAB format
        """
        lines = input.split('\n')
        count = 1
        temp = []
    
        for line in lines:
            line = self._normalize(line)
            node = {}
            try:
                (word, tag, head, rel) = line.split('\t')
                head = int(head)
                #not required, but useful for inspection
                node['address'] = count        
                node['word'] = word
                node['tag'] = tag
                node['head'] = head
                node['rel']= rel
                node['deps'] = []
                self.nodelist.append(node)
                
                for (dep, hd) in temp:
                    #check whether I'm the head for a node with address dep
                    if hd == count:
                        #add dep to my list of dependents
                        node['deps'].append(dep)
                try:
                    #try to add my address to my head's dependents list
                    self.nodelist[head]['deps'].append(count)
                except IndexError:
                    #my head hasn't been seen yet, so store the info
                    temp.append((count, head))
                            
                count += 1
                
            except ValueError:
                break
                
        root_address = self.nodelist[0]['deps'][0]
        self.root = self.nodelist[root_address]
        return self

    def _word(self, node, filter=True):
        w = node['word']
        if filter:
            if w != ',': return w
        return w

    def _deptree(self, i):
        """
        Recursive function for turning dependency graphs into
        NLTK trees.
        @type i: C{int}
        @param i: index of a node in C{nodelist}
        @return: either a word (if the indexed node 
        is a leaf) or a L{Tree}.
        """

        node = self.nodelist[i]
        word = node['word']
        deps = node['deps']

        if len(deps) == 0:
            return word
        else:
            return Tree(word, [self._deptree(j) for j in deps])

            
    def deptree(self):
        """
        Starting with the C{root} node, build a dependency tree using the NLTK 
        L{Tree} constructor. Dependency labels are omitted.
        """
        node = self.root
        word = node['word']
        deps = node['deps']

        return Tree(word, [self._deptree(i) for i in deps])
    
    def _hd(self, i):
        try:
            return self.nodelist[i]['head']
        except IndexError:
            return None
        
    def _rel(self, i):
        try:
            return self.nodelist[i]['rel']
        except IndexError:
            return None
    
    def nx_graph(self):
        """
        Convert the data in a C{nodelist} into a networkx 
        labeled directed graph.
        @rtype: C{XDigraph}
        """
        nx_nodelist = range(1, len(self.nodelist))
        nx_edgelist = [(n, self._hd(n), self._rel(n)) 
                        for n in nx_nodelist if self._hd(n)]
        self.nx_labels = {}
        for n in nx_nodelist:
            self.nx_labels[n] = self.nodelist[n]['word']
        
        g = NX.XDiGraph()
        g.add_nodes_from(nx_nodelist)
        g.add_edges_from(nx_edgelist)
        
        return g
        


def demo(nx=False):
    """
    A demonstration of the result of reading a dependency
    version of the first sentence of the Penn Treebank.
    """
    dg = DepGraph().read("""Pierre  NNP     2       NMOD
Vinken  NNP     8       SUB
,       ,       2       P
61      CD      5       NMOD
years   NNS     6       AMOD
old     JJ      2       NMOD
,       ,       2       P
will    MD      0       ROOT
join    VB      8       VC
the     DT      11      NMOD
board   NN      9       OBJ
as      IN      9       VMOD
a       DT      15      NMOD
nonexecutive    JJ      15      NMOD
director        NN      12      PMOD
Nov.    NNP     9       VMOD
29      CD      16      NMOD
.       .       9       VMOD
""")
    tree = dg.deptree()
    print tree.pprint()
    if nx:
        #currently doesn't work
        try:
            import networkx as NX
            import pylab as P
        except ImportError:
            raise
            g = dg.nx_graph()
        g.info()
        pos = NX.spring_layout(g, dim=1)
        NX.draw_networkx_nodes(g, pos, node_size=50)
        #NX.draw_networkx_edges(g, pos, edge_color='k', width=8)
        NX.draw_networkx_labels(g, pos, dg.nx_labels)
        P.xticks([])
        P.yticks([])
        P.savefig('deptree.png')
        P.show()

if __name__ == '__main__':
    demo()

    

########NEW FILE########
__FILENAME__ = ptbconv
# Natural Language Toolkit: ptbconv.py
#
# Author: Dan Garrette <dhgarrette@gmail.com>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT


"""
This module serves as an interface with the ptbconv tool found at 
http://www.jaist.ac.jp/~h-yamada/.

For this interface to work correctly, the environment variable "PTBCONV" should
be set to the location of the ptbconv binary.
"""


import subprocess
import math
from nltk.internals import find_binary
import os

OUTPUT_FORMAT = '%s\t%s\t_\t%s\t_\t_\t%s\t%s\t_\t_\n'

def convert(num, format='D', tofile=False, verbose=False):
    """
    @param num: the number of the treebank file
    @param format: D = dependency list
                   V = dependency tree
    @param tofile: write the output to a file?
    """
    stdout = _run_ptbconv(num, format, verbose)
        
    if tofile and format == 'D':
        _write_file(num, stdout, verbose)
    else:
        return stdout


def _write_file(num, dep_list_str, verbose=False):
    output_filename = reduce(os.path.join, 
                             [_treebank_path(), 'dep', 
                              'wsj_%04d.dep.untagged' % num])
    output_file = open(output_filename, 'w')
    
    i = 1
    for line in dep_list_str.split('\n'):
        if not line:
            output_file.write('\n')
            i = 1
        else:
            word, pos, head = line.split()
            
            if head == '-1': head = 0
            rel = _get_relation(word, pos, head)
            output_file.write(OUTPUT_FORMAT % (i, word, pos, head, rel))
            i += 1
    
def _get_relation(word, pos, head):
    if head == '0':
        return 'ROOT'
    if pos == 'JJ':
        return 'MOD'
    if pos == 'DT':
        return 'SPEC'
    if pos == '.' or pos == ',':
        return 'PUNCT'
    
    return '<rel>'

def _run_ptbconv(num, format='D', verbose=False):
    bin = find_binary('ptbconv', 
                      env_vars=['PTBCONV'],
                      url='http://www.jaist.ac.jp/~h-yamada/',
                      verbose=False)
    
    input_filename = reduce(os.path.join, 
                        [_treebank_path(), 'combined', 'wsj_%04d.mrg' % num])
    
    cmd = [bin, '-'+format]
    p = subprocess.Popen(cmd, 
                         stdout=subprocess.PIPE,
                         stderr=subprocess.PIPE,
                         stdin=open(input_filename))
    (stdout, stderr) = p.communicate()
    
    if verbose:
        print stderr.strip()
        
    return stdout


def _treebank_path():
    return reduce(os.path.join, 
                  [os.environ['NLTK_DATA'], 'corpora', 'treebank'])

def convert_all():
    for i in xrange(199):
        print '%s:' % (i+1),
        convert(i+1, 'D', True, True)

if __name__ == '__main__':
    print convert(1, 'D')
    
########NEW FILE########
__FILENAME__ = util
"""
Utilities for converting chunked treebank into format that can be 
input to Nivre's MaltParser.
"""
from nltk import tokenize
from itertools import islice
import os
from deptree import DepGraph
from nltk.stem.wordnet import WordNetLemmatizer 

def tag2tab(s, sep='/'):
    loc = s.rfind(sep)
    if loc >= 0:
            word = s[:loc]
            tag = s[loc+1:]
            tag = tag.replace('(', '-LRB-').replace(')', '-RRB-')
            return "%s\t%s\n" % (word, tag)
    else:
            return (s, None)

def tabtagged(files = 'chunked', basedir= None):
    """
    @param files: One or more treebank files to be processed
    @type files: L{string} or L{tuple(string)}
    @return: iterator over lines in Malt-TAB input format
    """       
    if type(files) is str: files = (files,)

    if not basedir: basedir = os.environ['NLTK_DATA']

    for file in files:
        path = os.path.join(get_basedir(), "treebank", file)
        f = open(path).read()

        for sent in tokenize.blankline(f):
            l = []
            for t in tokenize.whitespace(sent):
                if (t != '[' and t != ']'):
                    l.append(tag2tab(t))
            #add a blank line as sentence separator
            l.append('\n')
            yield l

def conll_to_depgraph(input_str, stem=False, verbose=False):
    if stem: 
        stemmer = WordNetLemmatizer()

    tokenizer = tokenize.TabTokenizer()
    depgraph_input = ''
    for line in _normalize(input_str).split('\n'):
        tokens = tokenizer.tokenize(line.strip())
        if len(tokens) > 1:
            word = tokens[1]
            if stem:
                word_stem = stemmer.lemmatize(word)
                if word_stem:
                    word = word_stem
            depgraph_input += '%s\t%s\t%s\t%s\n' % (word, tokens[3], tokens[6], tokens[7])

    assert depgraph_input, 'depgraph_input is empty'

    if verbose:
        print 'Begin DepGraph creation'
        print 'depgraph_input=\n%s' % depgraph_input
    
    return DepGraph().read(depgraph_input)

def _normalize(line):
    """
    Deal with lines in which spaces are used rather than tabs.
    """
    import re
    SPC = re.compile(' +')
    return re.sub(SPC, '\t', line)

def demo():
    from nltk.corpora import treebank
    #f = open('ptb_input.tab', 'w')
    #s = ''
    for sent in islice(tabtagged(), 3):
        for line in sent:
            print line,
            #s += ''.join(sent)
    #print >>f, s
    #f.close()
    


if __name__ == '__main__':
    demo()


########NEW FILE########
__FILENAME__ = featuredemo
from nltk.parse import GrammarFile
from nltk.parse.featurechart import *

"""
An interactive interface to the feature-based parser. Run "featuredemo.py -h" for
command-line options.

This interface will read a grammar from a *.cfg file, in the format of
test.cfg. It will prompt for a filename for the grammar (unless one is given on
the command line) and for a sentence to parse, then display the edges being
generated and any resulting parse trees.
"""

def text_parse(grammar, sent, trace=2, drawtrees=False, latex=False):
    parser = grammar.earley_parser(trace=trace)
    print parser._grammar
    tokens = sent.split()
    trees = parser.get_parse_list(tokens)
    if drawtrees:
        from treeview import TreeView
        TreeView(trees)
    else:
        for tree in trees:
            if latex: print tree.latex_qtree()
            else: print tree

def main():
    import sys
    from optparse import OptionParser, OptionGroup
    usage = """%%prog [options] [grammar_file]

by Rob Speer
Distributed under the GPL. See LICENSE.TXT for information.""" % globals()

    opts = OptionParser(usage=usage)
    opts.add_option("-b", "--batch",
    metavar="FILE", dest="batchfile", default=None,
    help="Batch test: parse all the lines in a file")

    opts.add_option("-v", "--verbose",
    action="count", dest="verbosity", default=0,
    help="show more information during parse")
    opts.add_option("-q", "--quiet",
    action="count", dest="quietness", default=0,
    help="show only the generated parses (default in batch mode)")
    opts.add_option("-l", "--latex",
    action="store_true", dest="latex",
    help="output parses as LaTeX trees (using qtree.sty)")
    opts.add_option("-d", "--drawtrees",
    action="store_true", dest="drawtrees",
    help="show parse trees in a GUI window")

    (options, args) = opts.parse_args()
    trace = 0
    batch = False
    
    if options.batchfile is not None:
        trace = 0
        batch = True
        if options.drawtrees:
            sys.stderr.write("Cannot use --drawtrees and --batch simultaneously.")
            sys.exit(1)
    if options.quietness > 0: trace = 0
    trace += options.verbosity

    if len(args): filename = args[0]
    else: filename = None

    if filename is None:
        sys.stderr.write("Load rules from file: ")
        filename = sys.stdin.readline()[:-1]
        if filename == '': return
    
    grammar = GrammarFile(filename)

    if not batch:
        sys.stderr.write("Sentence: ")
        sentence = sys.stdin.readline()[:-1]
        if sentence == '': return
        text_parse(grammar, sentence, trace, options.drawtrees, options.latex)
    else:
        for line in open(options.batchfile):
            sentence = line.strip()
            if sentence == '': continue
            if sentence[0] == '#': continue
            print "Sentence: %s" % sentence
            text_parse(grammar, sentence, trace, False, options.latex)

if __name__ == '__main__':
    main()


########NEW FILE########
__FILENAME__ = draw_graph
# Natural Language Toolkit: Graph Visualization
#
# Copyright (C) 2001 NLTK Project
# Author: Edward Loper <edloper@gradient.cis.upenn.edu>
#
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT
#
# $Id: graph.py,v 1.2 2004/03/18 21:02:36 edloper Exp $

"""
Graphically display a graph.  This module defines two new canvas
widgets: L{GraphEdgeWidget}, and L{GraphWidget}.  Together, these two
widgets can be used to display directed graphs.

C{GraphEdgeWidget} is an arrow, optionally annotated with a 'label',
which can be any canvas widget.  In addition to a source location and
a destination location, it has a 'curve' attribute, which can be used
to define how curved it is (positive values curve one way, and
negative values the other).  This is useful, e.g., if you want to have
two separate graph edges with the same source and the same
destination.  It is also useful for drawing arrows that have the same
source and destination (i.e., loops).

The C{GraphWidget} widget is used to display a single directed graph.
It is a container widget, containing zero or more I{node widgets},
which are connected by zero or more I{edge widgets}.  Any canvas
widget can be used as a node widget.  E.g., a StackWidget containing
an OvalWidget and a LabelWidget could be used to draw a circle with a
label below it.  Edge widgets must be C{GraphEdgeWidgets}.  The
C{GraphWidget} is responsible for adjusting the start and end
positions of edge widgets whenever node widgets move.  Thus, you can
make a node widget draggable, and when the user drags it, the edges
will update automatically.  The C{GraphWidget} also defines a method
C{arrange}, which will automatically choose a layout for the nodes,
attempting to minimize crossing edges.
"""

import math
from nltk.draw import *

class GraphEdgeWidget(CanvasWidget):
    """
    A canvas widget used to display graph edges.

    @todo: Add an 'arrow' attribute, which can be used to control the
           direction and/or shape of the arrow.
    """
    def __init__(self, canvas, x1, y1, x2, y2, label=None, **attribs):
        self._curve = 0
        coords = self._line_coords((x1, y1), (x2, y2))
        self._line = canvas.create_line(arrow='last', smooth=1, *coords)
        canvas.lower(self._line)
        self._label = label
        if label is not None:
            self._add_child_widget(label)

        CanvasWidget.__init__(self, canvas, **attribs)

    def __setitem__(self, attr, value):
        if attr == 'curve':
            self._curve = value
            coords = self._line_coords(self.start(), self.end())
            self.canvas().coords(self._line, *coords)
        elif attr == 'color':
            self.canvas().itemconfig(self._line, fill=value)
        elif attr == 'width':
            self.canvas().itemconfig(self._line, width=value)
        else:
            CanvasWidget.__setitem__(self, attr, value)
        
    def __getitem__(self, attr):
        if attr == 'curve':
            return self._curve
        elif attr == 'color':
            return self.canvas().itemcget(self._line, fill)
        elif attr == 'width':
            return self.canvas().itemcget(self._line, width)
        else:
            return CanvasWidget.__getitem__(self, attr)

    def _tags(self): return [self._line]

    def __repr__(self):
        return '[GraphEdge: %r %r->%r]' % (self._label, self.start(),
                                           self.end())

    def start(self):
        return self.canvas().coords(self._line)[:2]
    
    def end(self):
        return self.canvas().coords(self._line)[-2:]

    def set_start(self, x, y):
        coords = self._line_coords((x, y), self.end())
        self.canvas().coords(self._line, *coords)
        self.update(self._label)

    def set_end(self, x, y):
        coords = self._line_coords(self.start(), (x, y))
        self.canvas().coords(self._line, *coords)
        self.update(self._label)

    def _update(self, child):
        # The label moved?
        (x1, y1, x2, y2) = child.bbox()
        (x, y) = self._label_coords()
        child.move(x-(x1+x2)/2, y-(y1+y2)/2)

    def _manage(self):
        if self._label is not None:
            self._update(self._label)

    def _label_coords(self):
        line_coords = self.canvas().coords(self._line)
        (x1, y1) = line_coords[:2]
        (x2, y2) = line_coords[-2:]
        if (x1, y1) == (x2, y2):
            # Self-loops.  Emperically, this formula seems about
            # right, but it wasn't derived mathmatically.
            radius = 0
            return (x1, y1 + 0.81*(150*self._curve+radius) + 10)
        elif self._curve >= 0:
            # Normal edges.
            r = max(math.sqrt((x1-x2)**2 + (y1-y2)**2), 1)
            labelx = (x1+x2)*0.5 + (y2-y1)*(self._curve*.6)
            labely = (y1+y2)*0.5 - (x2-x1)*(self._curve*.6)
            return (int(labelx), int(labely))
        else:
            # Normal edges.
            r = max(math.sqrt((x1-x2)**2 + (y1-y2)**2), 1)
            labelx = (x1+x2)*0.5 + (y2-y1)*(self._curve/2 + 8/r)
            labely = (y1+y2)*0.5 - (x2-x1)*(self._curve/2 + 8/r)
            return (int(labelx), int(labely))
    
    def _line_coords(self, (startx, starty), (endx, endy)):
        (x1, y1) = int(startx), int(starty)
        (x2, y2) = int(endx), int(endy)
        radius1 = 0
        radius2 = 0

        if abs(x1-x2)+abs(y1-y2) < 5:
            # Self-loops
            x3 = x1 - 70*self._curve - radius1
            y3 = y1 + 70*self._curve + radius1
            x4 = x1
            y4 = y1 + 140*self._curve + radius1
            x5 = x1 + 70*self._curve + radius1
            y5 = y1 + 70*self._curve + radius1
            return (int(x1), int(y1), int(x3), int(y3), int(x4),
                    int(y4), int(x5), int(y5), int(x1), int(y1))
        else:
            # Normal edges.
            x3 = (x1+x2)*0.5 + (y2-y1)*self._curve
            y3 = (y1+y2)*0.5 - (x2-x1)*self._curve
            # Adjust endpoints so they end at the node parimeter.
            r = max(math.sqrt((x1-x2)**2 + (y1-y2)**2), 0.001)
            (dx, dy) = (x2-x1, y2-y1)
            x1 += dx/r * radius1
            y1 += dy/r * radius1
            x2 -= dx/r * radius2
            y2 -= dy/r * radius2
            return (int(x1), int(y1), int(x3), int(y3), int(x2), int(y2))
        
class GraphWidget(CanvasWidget):
    """
    A canvas widget used to display directed graphs.  This container
    widget contains zero or more 'node widgets', which are connected by
    zero or more C{GraphEdgeWidget}s.  The C{GraphWidget} is responsible
    for updating the edge widgets when nodes move; and for initially
    arranging the nodes.
    """
    def __init__(self, canvas, nodes, edges, **attrs):
        """
        @param edges: A list of tuples (n1, n2, e), where n1 is a
            CanvasWidget in C{nodes}; n2 is a CanvasWidget in
            C{nodes}; and e is a GraphEdgeWidget.
        """
        if len(nodes) == 0:
            # dummy node, since we need to have a bbox.
            nodes = [SpaceWidget(canvas,0,0)]
            
        self._nodes = set(nodes)

        # Management parameters.  I should add attributes for these. 
        self._arrange = 'dfs'
        self._xspace = attrs.pop('xspace', 50)
        
        self._yspace = attrs.pop('yspace', 50)
        self._orientation = attrs.pop('orientation', 'horizontal')

        # Attributes for edges.

        # Out & in edges for a given node
        self._outedges = {}
        self._inedges = {}

        # Start & end nodes for a given edge.
        self._startnode = {}
        self._endnode = {}

        # Keep track of edge widgets.
        self._edgewidgets = {}

        self._initialized = False
        for node in self._nodes:
            self.add_node(node)
        for (start, end, edgewidget) in edges:
            self.add_edge(start, end, edgewidget)
        self._initialized = True
            
        CanvasWidget.__init__(self, canvas, **attrs)

    def add_node(self, node):
        """
        Add a new node to the graph.
        """
        self._add_child_widget(node)
        self._nodes.add(node)

    def add_edge(self, start, end, edgewidget):
        """
        Add a new edge to the graph.
        @param start: The start node
        @type start: C{CanvasWidget}
        @param end: The end node
        @type end: C{CanvasWidget}
        @param edgewidget: The edge
        @type edgewidget: C{GraphEdgeWidget}
        """
        num_edges = (len(self._edgewidgets.get( (start, end), [])) +
                     len(self._edgewidgets.get( (end, start), [])))
        if start is end:
            num_edges = num_edges/2+1
            curve = 0.3 * ((num_edges+1)/2) * (num_edges%2*2-1)
        else:
            curve = 0.4 * ((num_edges+1)/2) * (num_edges%2*2-1)
        edgewidget['curve'] = curve

        # Add the edge to the outedge & inedge dictionaries
        self._outedges.setdefault(start, []).append(edgewidget)
        self._inedges.setdefault(end, []).append(edgewidget)

        self._startnode[edgewidget] = start
        self._endnode[edgewidget] = end
            
        self._edgewidgets.setdefault((start, end),[]).append(edgewidget)
        self._add_child_widget(edgewidget)
        if self._initialized: self._update_edge(edgewidget)

    def remove_edge(self, edge):
        """
        Remove an edge from the graph (but don't destroy it).
        @type edge: L{GraphEdgeWidget}
        """
        print 'remove', edge
        # Get the edge's start & end nodes.
        start, end = self._startnode[edge], self._endnode[edge]

        # Remove the edge from the node->edge maps
        self._outedges[start].remove(edge)
        self._inedges[end].remove(edge)
        
        # Remove the edge from the edge->node maps.
        del self._startnode[edge]
        del self._endnode[edge]
        
        # Remove the edge from the list of edge widgets that connect 2
        # nodes.  (Recompute curves?)
        self._edgewidgets[start, end].remove(edge)

        # Remove the edge from our list of child widgets.
        self._remove_child_widget(edge)

    def remove_node(self, node):
        """
        Remove a node from the graph (but don't destroy it).
        @type node: L{CanvasWidget}
        @return: A list of widgets that were removed from the
            graph.  Note that this will include any edges that
            connected to C{node}.
        """
        # Remove all edges that connect to this node.
        removed_edges = []
        for edge in self._outedges.get(node, [])[:]:
            self.remove_edge(edge)
            removed_edges.append(edge)
        for edge in self._inedges.get(node, [])[:]:
            self.remove_edge(edge)
            removed_edges.append(edge)

        # Remove the node from the node->edges map
        try: del self._outedges[node]
        except KeyError: pass
        try: del self._inedges[node]
        except KeyError: pass

        # Remove the node from our list of nodes
        self._nodes.remove(node)

        # Remove the node from our list of child widgets.
        self._remove_child_widget(node)

        # Return the list of removed widgets
        return removed_edges + [node]

    def destroy_edge(self, edge):
        """
        Remove an edge from the graph, and destroy the edge.
        """
        self.remove_edge(edge)
        edge.destroy()

    def destroy_node(self, node):
        """
        Remove a node from the graph, and destroy the node.
        """
        print 'removing', node
        for widget in self.remove_node(node):
            print 'destroying', widget
            widget.destroy()

    def _tags(self): return []

    def _update(self, child):
        """
        Make sure all edges/nodes are connected correctly.
        """
        if isinstance(child, GraphEdgeWidget):
            # Moved an edge.
            pass
        else:
            # Moved a node.
            for outedge in self._outedges.get(child, []):
                self._update_edge(outedge)
            for inedge in self._inedges.get(child, []):
                self._update_edge(inedge)

    def _update_edge(self, edge):
        curve = edge['curve']
        # Set the start.
        src_x, src_y = self._node_center(self._endnode[edge])
        x, y = self._node_port(self._startnode[edge], src_x, src_y, curve)
        edge.set_start(x, y)
        # Set the end.
        src_x, src_y = x, y#self._node_center(self._startnode[edge])
        x, y = self._node_port(self._endnode[edge], src_x, src_y, curve)
        edge.set_end(x, y)

    def _node_port(self, node, src_x, src_y, curve):
        x1, y1, x2, y2 = node.bbox()
        x, y = (x1+x2)/2, (y1+y2)/2
        w, h = abs(x2-x1), abs(y2-y1)
        dx, dy = x-src_x, y-src_y

        if dx > abs(dy)/5: return x-w/2, y
        elif dx < -abs(dy)/5: return x+w/2, y
        #if dx > abs(dy): return x-w/2, y
        #elif dx < -abs(dy): return x+w/2, y
        elif dy > 0: return x, y-h/2
        elif dy < 0: return x, y+h/2
        elif curve > 0:
            return x, y+h/2
        else:
            return x, y-h/2

    def _node_center(self, node):
        (x1, y1, x2, y2) = node.bbox()
        return (x1+x2)/2, (y1+y2)/2

    def _manage(self):
        self.arrange()

    ##////////////////////////
    ##  Graph Layout
    ##////////////////////////
    def arrange(self, arrange_algorithm=None, toplevel=None):
        """
        Set the node positions.  This routine should attempt to
        minimize the number of crossing edges, in order to make the
        graph easier to read.
        """
        if arrange_algorithm is not None:
            self._arrange = arrange_algorithm
            
        self._arrange_into_levels(toplevel)
        self._arrange_levels()

        (old_left, old_top) = self.bbox()[:2]
        for node in self._nodes:
            (x1, y1) = node.bbox()[:2]
            node.move(-x1, -y1)

        # Now we want to minimize crossing edges.. how, again? :)
        for i in range(len(self._levels)):
            for j in range(len(self._levels[i])):
                if self._levels[i][j] is not None:
                    node = self._levels[i][j]
                    if self._orientation == 'horizontal':
                        node.move(i*self._xspace, j*self._yspace)
                    else:
                        node.move(j*self._xspace, i*self._yspace)

                    # If there is an edge from a node at the same
                    # position within its level, but whose level is at
                    # least 2 levels prior, then it's likely that that
                    # edge goes through an intervening node; so if its
                    # curve is zero, then increment it.
                    for edge in self._inedges.get(node,[]):
                        from_node = self._startnode[edge]
                        from_levelnum = self._nodelevel[from_node]
                        from_level = self._levels[from_levelnum]
                        if (abs(i-from_levelnum)>1 and
                            len(from_level) > j and
                            from_node == from_level[j] and
                            edge['curve'] == 0):
                            edge['curve'] = -0.25

        (left, top) = self.bbox()[:2]
        self.move(int(old_left-left), int(old_top-top))

    def _arrange_levels(self):
        """
        Re-arrange each level to (locally) minimize the number of
        crossing edges.
        """
        # For now, put nodes with more incoming edges further down.
        for levelnum in range(len(self._levels)):
            self._arrange_level(levelnum)

    def _arrange_level(self, levelnum):
        """
        Arrange a given level..  This algorithm is simple and pretty
        heuristic..
        """
        if levelnum == 0: return

        # For each position where we might want to put a node, create
        # a scores dictionary, mapping candidate nodes to scores.  We
        # will then use these scores to distribute nodes to level positions.
        scores = [{} for i in range(max(len(self._levels[levelnum]),
                                       len(self._levels[levelnum-1])))]
        for node in self._levels[levelnum]:
            # All else being equal, put nodes with more incoming
            # connections towards the end (=bottom) of the level.
            for pos in range(len(scores)):
                scores[pos][node] = 1.0/len(self._inedges.get(node, []))
                
            # Try to put a node at level position x if nodes
            # in previous levels at position x point to it.
            for edge in self._inedges.get(node, []):
                from_node = self._startnode[edge]
                from_levelnum = self._nodelevel[from_node]
                if from_levelnum < levelnum:
                    from_pos = self._levels[from_levelnum].index(from_node)
                    score = (scores[from_pos].get(node, 0) + 1.0 /
                             (levelnum - from_levelnum))
                    scores[from_pos][node] = score

        # Get the list of nodes that we need to fill in, and empty the
        # level.
        nodes = self._levels[levelnum]
        self._levels[levelnum] = [None] * len(scores)
        level = self._levels[levelnum]

        # Fill in nodes, picking the best first..
        while len(nodes) > 0:
            best = (None, None, -1) # node, position, score.
            for pos in range(len(scores)):
                for (node, score) in scores[pos].items():
                    if (score > best[2] and level[pos] is None and
                        node in nodes):
                        best = (node, pos, score)
                    elif (score == best[2] and pos<best[1] and
                        level[pos] is None and node in nodes):
                        # Put higher scores at lower level positions
                        best = (node, pos, score)
            nodes.remove(best[0])
            level[best[1]] = best[0]

    def _arrange_into_levels(self, toplevel):
        """
        Assign a level to each node.
        """
        # Mapping from node to level.
        self._nodelevel = {}
        self._levels = []

        # Find any nodes that have no incoming edges; put all of these
        # in level 0.
        if toplevel is None:
            toplevel = []
            for node in self._nodes:
                if len(self._inedges.get(node, [])) == 0:
                    toplevel.append(node)
                    self._nodelevel[node] = 0
        else:
            for node in toplevel:
                self._nodelevel[node] = 0

        # Expand all of their children.
        self._levels = [toplevel]
        self._add_descendants(toplevel, 1)

        # If we didn't get all the nodes, we'll have to start picking
        # nodes that do have incoming transitions.  Pick the ones that
        # have the most reachable nodes.  (n.b., this implementation
        # isn't terribly efficient, but we dont' expect to be
        # displaying huge graphs, so it should be ok)
        while len(self._nodelevel) < len(self._nodes):
            expand_node = None
            max_reachable = -1

            for node in self._nodes:
                reachable = self._reachable(node)
                if reachable >= max_reachable:
                    max_reachable = reachable
                    expand_node = node
            
            # Expand the new node's children.
            self._levels[0].append(expand_node)
            self._nodelevel[expand_node] = 0
            self._add_descendants(toplevel, 1)
    
    def _reachable(self, node, reached=None):
        """
        How many *unexpanded* nodes can be reached from the given node?
        """
        if self._nodelevel.has_key(node): return 0
        if reached is None: reached = {}
        if not reached.has_key(node):
            reached[node] = 1
            for edge in self._outedges.get(node, []):
                self._reachable(self._endnode[edge], reached)
        return len(reached)

    def _add_descendants(self, parent_level, levelnum):
        """
        Add all the descendants of the nodes in the list parent_level
        to the structures self._level and self._nodelevel.
        """
        if self._arrange == 'bfs':
            self._add_descendants_bfs(parent_level, levelnum)
        elif self._arrange == 'dfs':
            self._add_descendants_dfs(parent_level, levelnum)
        else:
            # Default to dfs
            self._add_descendants_dfs(parent_level, levelnum)

    def _add_descendants_dfs(self, parent_level, levelnum):
        if levelnum >= len(self._levels): self._levels.append([])
        for parent_node in parent_level:
            # Add the parent node
            if not self._nodelevel.has_key(parent_node):
                self._levels[levelnum-1].append(parent_node)
                self._nodelevel[parent_node] = levelnum-1

            # Recurse to its children
            child_nodes = [self._endnode[edge]
                           for edge in self._outedges.get(parent_node, [])
                           if not self._nodelevel.has_key(self._endnode[edge])]
            if len(child_nodes) > 0:
                self._add_descendants_dfs(child_nodes, levelnum+1)

    def _add_descendants_bfs(self, parent_level, levelnum):
        frontier_nodes = []
        if levelnum >= len(self._levels): self._levels.append([])
        for parent_node in parent_level:
            child_nodes = [self._endnode[edge]
                           for edge in self._outedges.get(parent_node, [])]
            for node in child_nodes:
                if not self._nodelevel.has_key(node):
                    self._levels[levelnum].append(node)
                    self._nodelevel[node] = levelnum
                    frontier_nodes.append(node)
        if len(frontier_nodes) > 0:
            self._add_descendants_bfs(frontier_nodes, levelnum+1)

    def _add_descendants_bfs2(self, parent_level, levelnum):
        frontier_nodes = []
        if levelnum >= len(self._levels): self._levels.append([])
        for parent_node in parent_level:
            child_nodes = [self._endnode[edge]
                           for edge in self._outedges.get(parent_node, [])]
            child_nodes += [self._startnode[edge]
                           for edge in self._inedges.get(parent_node, [])]
            for node in child_nodes:
                if not self._nodelevel.has_key(node):
                    self._levels[levelnum].append(node)
                    self._nodelevel[node] = levelnum
                    frontier_nodes.append(node)
        if len(frontier_nodes) > 0:
            self._add_descendants_bfs2(frontier_nodes, levelnum+1)




########NEW FILE########
__FILENAME__ = fst
# Natural Language Toolkit: Finite State Transducers
#
# Copyright (C) 2001-2011 NLTK Project
# Author: Edward Loper <edloper@gradient.cis.upenn.edu>
#         Steven Bird <sb@csse.unimelb.edu.au>
#
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT


"""
Finite state transducers.

A finite state trasducer, or FST, is a directed graph that is used to
encode a mapping from a set of I{input strings} to a set of I{output
strings}.  An X{input string} is a sequence of immutable values (such
as integers, characters, or strings) called X{input symbols}.
Similarly, an C{output string} is a sequence of immutable values
called X{output symbols}.  Collectively, input strings and output
strings are called X{symbol strings}, or simply X{strings} for short.
Note that this notion of I{string} is different from the python string
type -- symbol strings are always encoded as tuples of input or output
symbols, even if those symbols are characters.  Also, note that empty
sequences are valid symbol strings.

The nodes of an FST are called X{states}, and the edges are called
X{transition arcs} or simply X{arcs}.  States may be marked as
X{final}, and each final state is annotated with an output string,
called the X{finalizing string}.  Each arc is annotated with an input
string and an output string.  An arc with an empty input string is
called an I{epsilon-input arc}; and an arc with an empty output
string is called an I{epsilon-output arc}.

The set of mappings encoded by the FST are defined by the set of paths
through the graph, starting at a special state known as the X{initial
state}, and ending at a final state.  In particular, the FST maps an
input string X to an output string Y iff there exists a path from the
initial state to a final state such that:

  - The input string X is formed by concatenating the input strings
    of the arcs along the path (in order).
  - The output string Y is formed by concatenating the output strings
    of the arcs along the path (in order), plus the final state's
    output string.

The following list defines some terms that apply to finite state
transducers.

  - The X{transduction} defined by a FST is the mapping from input
    strings to output strings.
    
  - An FST X{encodes a deterministic transduction} if each input
    string maps to at most one output string.  An FST X{encodes a
    nondeterministic transduction} if any input string maps to more
    than one output string.

  - An FST is X{deterministic} if it every state contains at most one
    outgoing arc that is consistent with any input string; otherwise,
    the FST is X{nondeterministic}.  If an FST is deterministic, then
    it necessarily encodes a deterministic transduction; however, it
    is possible to define an FST that is nondeterministic but that
    encodes a deterministic transduction.

  - An FST is X{sequential} if each arc is labeled with exactly one
    input symbol, no two outgoing arcs from any state have the same
    input symbol, and all finalizing strings are empty.  (Sequential
    implies deterministic).

  - An FST is I{subsequential} if each arc is labeled with exactly
    one input symbol, and no two outgoing arcs from any state have
    the same input symbol.  (Finalizing strings may be non-empty.)

An FSA can be represented as an FST that generates no output symbols.

The current FST class does not provide support for:

  - Weighted arcs.  (However, weights can be used as, or included
    in, the output symbols.  The total weight of a path can then
    be found after transduction by combining the weights.  But
    there's no support for e.g., finding the path with the minimum
    weight.
    
  - Multiple initial states.
  
  - Initializing strings (an output string associated with the initial
    state, which is always generated when the FST begins).

Possible future changes:

  - Define several classes, in a class hierarchy?  E.g., FSA is a base
    class, FST inherits from it.  And maybe a further subclass to add
    finalizing sequences.  I would need to be more careful to only
    access the private variables when necessary, and to usually go
    through the accessor functions.
"""

import re, os, random, tempfile
from subprocess import Popen, PIPE
from nltk.draw import *
from nltk_contrib.fst.draw_graph import *

######################################################################
# CONTENTS
######################################################################
# 1. Finite State Transducer
#    - State information
#    - Transition Arc Information
#    - FST Information
#    - State Modification
#    - Transition Arc Modification
#    - Transformations
#    - Misc
#    - Transduction
# 2. AT&T fsmtools support
# 3. Graphical Display
#    - FSTDisplay
#    - FSTDemo
######################################################################

######################################################################
#{ Finite State Transducer
######################################################################

class FST(object):
    """
    A finite state transducer.  Each state is uniquely identified by a
    label, which is typically a string name or an integer id.  A
    state's label is used to access and modify the state.  Similarly,
    each arc is uniquely identified by a label, which is used to
    access and modify the arc.

    The set of arcs pointing away from a state are that state's
    I{outgoing} arcs.  The set of arcs pointing to a state are that
    state's I{incoming} arcs.  The state at which an arc originates is
    that arc's I{source} state (or C{src}), and the state at which it
    terminates is its I{destination} state (or C{dst}).

    It is possible to define an C{FST} object with no initial state.
    This is represented by assigning a value of C{None} to the
    C{initial_state} variable.  C{FST}s with no initial state are
    considered to encode an empty mapping.  I.e., transducing any
    string with such an C{FST} will result in failure.
    """
    def __init__(self, label):
        """
        Create a new finite state transducer, containing no states.
        """
        self.label = label
        """A label identifying this FST.  This is used for display &
        debugging purposes only."""

        #{ State Information
        self._initial_state = None
        """The label of the initial state, or C{None} if this FST
        does not have an initial state."""
        
        self._incoming = {}
        """A dictionary mapping state labels to lists of incoming
        transition arc labels."""

        self._outgoing = {}
        """A dictionary mapping state labels to lists of outgoing
        transition arc labels."""

        self._is_final = {}
        """A dictionary mapping state labels to boolean values,
        indicating whether the state is final."""

        self._finalizing_string = {}
        """A dictionary mapping state labels of final states to output
        strings.  This string should be added to the output
        if the FST terminates at this state."""

        self._state_descr = {}
        """A dictionary mapping state labels to (optional) state
        descriptions."""
        #}

        #{ Transition Arc Information
        self._src = {}
        """A dictionary mapping each transition arc label to the label of
        its source state."""

        self._dst = {}
        """A dictionary mapping each transition arc label to the label of
        its destination state."""

        self._in_string = {}
        """A dictionary mapping each transition arc label to its input
        string, a (possibly empty) tuple of input symbols."""

        self._out_string = {}
        """A dictionary mapping each transition arc label to its output
        string, a (possibly empty) tuple of input symbols."""

        self._arc_descr = {}
        """A dictionary mapping transition arc labels to (optional)
        arc descriptions."""
        #}
        
    #////////////////////////////////////////////////////////////
    #{ State Information
    #////////////////////////////////////////////////////////////

    def states(self):
        """Return an iterator that will generate the state label of
        each state in this FST."""
        return iter(self._incoming)

    def has_state(self, label):
        """Return true if this FST contains a state with the given
        label."""
        return label in self._incoming

    def _get_initial_state(self):
        return self._initial_state
    def _set_initial_state(self, label):
        if label is not None and label not in self._incoming:
            raise ValueError('Unknown state label %r' % label)
        self._initial_state = label
    initial_state = property(_get_initial_state, _set_initial_state,
                             doc="The label of the initial state (R/W).")

    def incoming(self, state):
        """Return an iterator that will generate the incoming
        transition arcs for the given state.  The effects of modifying
        the FST's state while iterating are undefined, so if you plan
        to modify the state, you should copy the incoming transition
        arcs into a list first."""
        return iter(self._incoming[state])

    def outgoing(self, state):
        """Return an iterator that will generate the outgoing
        transition arcs for the given state.  The effects of modifying
        the FST's state while iterating are undefined, so if you plan
        to modify the state, you should copy the outgoing transition
        arcs into a list first."""
        return iter(self._outgoing[state])

    def is_final(self, state):
        """Return true if the state with the given state label is
        final."""
        return self._is_final[state]

    def finalizing_string(self, state):
        """Return the output string associated with the given final
        state.  If the FST terminates at this state, then this string
        will be emitted."""
        #if not self._is_final[state]:
        #    raise ValueError('%s is not a final state' % state)
        return self._finalizing_string.get(state, ())

    def state_descr(self, state):
        """Return the description for the given state, if it has one;
        or None, otherwise."""
        return self._state_descr.get(state)

    #////////////////////////////////////////////////////////////
    #{ Transition Arc Information
    #////////////////////////////////////////////////////////////

    def arcs(self):
        """Return an iterator that will generate the arc label of
        each transition arc in this FST."""
        return iter(self._src)

    def src(self, arc):
        """Return the state label of this transition arc's source
        state."""
        return self._src[arc]

    def dst(self, arc):
        """Return the state label of this transition arc's destination
        state."""
        return self._dst[arc]

    def in_string(self, arc):
        """Return the given transition arc's input string, a (possibly
        empty) tuple of input symbols."""
        return self._in_string[arc]
    
    def out_string(self, arc):
        """Return the given transition arc's output string, a
        (possibly empty) tuple of output symbols."""
        return self._out_string[arc]
    
    def arc_descr(self, arc):
        """Return the description for the given transition arc, if it
        has one; or None, otherwise."""
        return self._arc_descr.get(arc)

    def arc_info(self, arc):
        """Return a tuple (src, dst, in_string, out_string) for the
        given arc, where:
          - C{src} is the label of the arc's source state.
          - C{dst} is the label of the arc's destination state.
          - C{in_string} is the arc's input string.
          - C{out_string} is the arc's output string.
        """
        return (self._src[arc], self._dst[arc],
                self._in_string[arc], self._out_string[arc])

    #////////////////////////////////////////////////////////////
    #{ FST Information
    #////////////////////////////////////////////////////////////

    def is_sequential(self):
        """
        Return true if this FST is sequential.
        """
        for state in self.states():
            if self.finalizing_string(state): return False
        return self.is_subsequential()

    def is_subsequential(self):
        """
        Return true if this FST is subsequential.
        """
        for state in self.states():
            out_syms = set()
            for arc in self.outgoing(state):
                out_string = self.out_string(arc)
                if len(out_string) != 1: return False
                if out_string[0] in out_syms: return False
                out_syms.add(out_string)
        return True

    #////////////////////////////////////////////////////////////
    #{ State Modification
    #////////////////////////////////////////////////////////////

    def add_state(self, label=None, is_final=False,
                  finalizing_string=(), descr=None):
        """
        Create a new state, and return its label.  The new state will
        have no incoming or outgoing arcs.  If C{label} is specified,
        then it will be used as the state's label; otherwise, a new
        unique label value will be chosen.  The new state will be
        final iff C{is_final} is true.  C{descr} is an optional
        description string for the new state.
        
        Arguments should be specified using keywords!
        """
        label = self._pick_label(label, 'state', self._incoming)
        
        # Add the state.
        self._incoming[label] = []
        self._outgoing[label] = []
        self._is_final[label] = is_final
        self._state_descr[label] = descr
        self._finalizing_string[label] = tuple(finalizing_string)
        
        # Return the new state's label.
        return label

    def del_state(self, label):
        """
        Delete the state with the given label.  This will
        automatically delete any incoming or outgoing arcs attached to
        the state.
        """
        if label not in self._incoming:
            raise ValueError('Unknown state label %r' % label)

        # Delete the incoming/outgoing arcs.
        for arc in self._incoming[label]:
            del (self._src[arc], self._dst[arc], self._in_string[arc],
                 self._out_string[arc], self._arc_descr[arc])
        for arc in self._outgoing[label]:
            del (self._src[arc], self._dst[arc], self._in_string[arc],
                 self._out_string[arc], self._arc_descr[arc])

        # Delete the state itself.
        del (self._incoming[label], self._otugoing[label],
             self._is_final[label], self._state_descr[label],
             self._finalizing_string[label])

        # Check if we just deleted the initial state.
        if label == self._initial_state:
            self._initial_state = None

    def set_final(self, state, is_final=True):
        """
        If C{is_final} is true, then make the state with the given
        label final; if C{is_final} is false, then make the state with
        the given label non-final.
        """
        if state not in self._incoming:
            raise ValueError('Unknown state label %r' % state)
        self._is_final[state] = is_final

    def set_finalizing_string(self, state, finalizing_string):
        """
        Set the given state's finalizing string.
        """
        if not self._is_final[state]:
            raise ValueError('%s is not a final state' % state)
        if state not in self._incoming:
            raise ValueError('Unknown state label %r' % state)
        self._finalizing_string[state] = tuple(finalizing_string)

    def set_descr(self, state, descr):
        """
        Set the given state's description string.
        """
        if state not in self._incoming:
            raise ValueError('Unknown state label %r' % state)
        self._state_descr[state] = descr

    def dup_state(self, orig_state, label=None):
        """
        Duplicate an existing state.  I.e., create a new state M{s}
        such that:
          - M{s} is final iff C{orig_state} is final.
          - If C{orig_state} is final, then M{s.finalizing_string}
            is copied from C{orig_state}
          - For each outgoing arc from C{orig_state}, M{s} has an
            outgoing arc with the same input string, output
            string, and destination state.

        Note that if C{orig_state} contained self-loop arcs, then the
        corresponding arcs in M{s} will point to C{orig_state} (i.e.,
        they will I{not} be self-loop arcs).

        The state description is I{not} copied.
            
        @param label: The label for the new state.  If not specified,
            a unique integer will be used.
        """
        if orig_state not in self._incoming: 
            raise ValueError('Unknown state label %r' % src)
        
        # Create a new state.
        new_state = self.add_state(label=label)

        # Copy finalization info.
        if self.is_final(orig_state):
            self.set_final(new_state)
            self.set_finalizing_string(new_state,
                                       self.finalizing_string(orig_state))

        # Copy the outgoing arcs.
        for arc in self._outgoing[orig_state]:
            self.add_arc(src=new_state, dst=self._dst[arc],
                         in_string=self._in_string[arc],
                         out_string=self._out_string[arc])

        return new_state

    #////////////////////////////////////////////////////////////
    #{ Transition Arc Modification
    #////////////////////////////////////////////////////////////

    def add_arc(self, src, dst, in_string, out_string,
                label=None, descr=None):
        """
        Create a new transition arc, and return its label.

        Arguments should be specified using keywords!
        
        @param src: The label of the source state.
        @param dst: The label of the destination state.
        @param in_string: The input string, a (possibly empty) tuple of
            input symbols.  Input symbols should be hashable
            immutable objects.
        @param out_string: The output string, a (possibly empty) tuple
            of output symbols.  Output symbols should be hashable
            immutable objects.
        """
        label = self._pick_label(label, 'arc', self._src)

        # Check that src/dst are valid labels.
        if src not in self._incoming:
            raise ValueError('Unknown state label %r' % src)
        if dst not in self._incoming:
            raise ValueError('Unknown state label %r' % dst)

        # Add the arc.
        self._src[label] = src
        self._dst[label] = dst
        self._in_string[label] = tuple(in_string)
        self._out_string[label] = tuple(out_string)
        self._arc_descr[label] = descr

        # Link the arc to its src/dst states.
        self._incoming[dst].append(label)
        self._outgoing[src].append(label)

        # Return the new arc's label.
        return label
            
    def del_arc(self, label):
        """
        Delete the transition arc with the given label.
        """
        if label not in self._src:
            raise ValueError('Unknown arc label %r' % src)

        # Disconnect the arc from its src/dst states.
        self._incoming[self._dst[label]].remove(label)
        self._outgoing[self._src[label]].remove(label)

        # Delete the arc itself.
        del (self._src[label], self._dst[label], self._in_string[label],
             self._out_string[label], self._arc_descr[label])

    #////////////////////////////////////////////////////////////
    #{ Transformations
    #////////////////////////////////////////////////////////////

    def inverted(self):
        """Swap all in_string/out_string pairs."""
        fst = self.copy()
        fst._in_string, fst._out_string = fst._out_string, fst._in_string
        return fst

    def reversed(self):
        """Reverse the direction of all transition arcs."""
        fst = self.copy()
        fst._incoming, fst._outgoing = fst._outgoing, fst._incoming
        fst._src, fst._dst = fst._dst, fst._src
        return fst

    def trimmed(self):
        fst = self.copy()
        
        if fst.initial_state is None:
            raise ValueError("No initial state!")

        # Determine whether there is a path from the initial node to
        # each node.
        queue = [fst.initial_state]
        path_from_init = set(queue)
        while queue:
            state = queue.pop()
            dsts = [fst.dst(arc) for arc in fst.outgoing(state)]
            queue += [s for s in dsts if s not in path_from_init]
            path_from_init.update(dsts)

        # Determine whether there is a path from each node to a final
        # node.
        queue = [s for s in fst.states() if fst.is_final(s)]
        path_to_final = set(queue)
        while queue:
            state = queue.pop()
            srcs = [fst.src(arc) for arc in fst.incoming(state)]
            queue += [s for s in srcs if s not in path_to_final]
            path_to_final.update(srcs)

        # Delete anything that's not on a path from the initial state
        # to a final state.
        for state in list(fst.states()):
            if not (state in path_from_init and state in path_to_final):
                fst.del_state(state)

        return fst
    
    def relabeled(self, label=None, relabel_states=True, relabel_arcs=True):
        """
        Return a new FST that is identical to this FST, except that
        all state and arc labels have been replaced with new labels.
        These new labels are consecutive integers, starting with zero.

        @param relabel_states: If false, then don't relabel the states.
        @param relabel_arcs: If false, then don't relabel the arcs.
        """
        if label is None: label = '%s (relabeled)' % self.label
        fst = FST(label)

        # This will ensure that the state relabelling is canonical, *if*
        # the FST is subsequential.
        state_ids = self._relabel_state_ids(self.initial_state, {})
        if len(state_ids) < len(self._outgoing):
            for state in self.states():
                if state not in state_ids:
                    state_ids[state] = len(state_ids)

        # This will ensure that the arc relabelling is canonical, *if*
        # the state labelling is canonical.
        arcs = sorted(self.arcs(), key=self.arc_info)
        arc_ids = dict([(a,i) for (i,a) in enumerate(arcs)])

        for state in self.states():
            if relabel_states: label = state_ids[state]
            else: label = state
            fst.add_state(label, is_final=self.is_final(state),
                          finalizing_string=self.finalizing_string(state),
                          descr=self.state_descr(state))

        for arc in self.arcs():
            if relabel_arcs: label = arc_ids[arc]
            else: label = arc
            src, dst, in_string, out_string = self.arc_info(arc)
            if relabel_states:
                src = state_ids[src]
                dst = state_ids[dst]
            fst.add_arc(src=src, dst=dst, in_string=in_string,
                        out_string=out_string,
                        label=label, descr=self.arc_descr(arc))

        if relabel_states:
            fst.initial_state = state_ids[self.initial_state]
        else:
            fst.initial_state = self.initial_state
            
        return fst

    def _relabel_state_ids(self, state, ids):
        """
        A helper function for L{relabel()}, which decides which new
        label should be assigned to each state.
        """
        if state in ids: return
        ids[state] = len(ids)
        for arc in sorted(self.outgoing(state),
                          key = lambda a:self.in_string(a)):
            self._relabel_state_ids(self.dst(arc), ids)
        return ids

    def determinized(self, label=None):
        """
        Return a new FST which defines the same mapping as this FST,
        but is determinized.

        The algorithm used is based on [...].

        @require: All arcs in this FST must have exactly one input
            symbol.
        @require: The mapping defined by this FST must be
            deterministic.
        @raise ValueError: If the determinization algorithm was unable
            to determinize this FST.  Typically, this happens because
            a precondition is not met.
        """
        # Check preconditions..
        for arc in self.arcs():
            if len(self.in_string(arc)) != 1:
                raise ValueError("All arcs must have exactly one "
                                 "input symbol.")
        
        # State labels have the form:
        #   frozenset((s1,w1),(s2,w2),...(sn,wn))
        # Where si is a state and wi is a string of output symbols.
        if label is None: label = '%s (determinized)' % self.label
        new_fst = FST(label)

        initial_state = frozenset( [(self.initial_state,())] )
        new_fst.add_state(initial_state)
        new_fst.initial_state = initial_state
                          
        queue = [initial_state]
        while queue:
            new_fst_state = queue.pop()

            # For each final state from the original FSM that's
            # contained in the new FST's state, compute the finalizing
            # string.  If there is at least one finalizing string,
            # then the new state is a final state.  However, if the
            # finalizing strings are not all identical, then the
            # transduction defined by this FST is nondeterministic, so
            # fail.
            finalizing_strings = [w+self.finalizing_string(s)
                                  for (s,w) in new_fst_state
                                  if self.is_final(s)]
            if len(set(finalizing_strings)) > 0:
                if not self._all_equal(finalizing_strings):
                    # multiple conflicting finalizing strings -> bad!
                    raise ValueError("Determinization failed")
                new_fst.set_final(new_fst_state)
                new_fst.set_finalizing_string(new_fst_state,
                                              finalizing_strings[0])

            # sym -> dst -> [residual]
            # nb: we checked above that len(in_string)==1 for all arcs.
            arc_table = {}
            for (s,w) in new_fst_state:
                for arc in self.outgoing(s):
                    sym = self.in_string(arc)[0]
                    dst = self.dst(arc)
                    residual = w + self.out_string(arc)
                    arc_table.setdefault(sym,{}).setdefault(dst,set())
                    arc_table[sym][dst].add(residual)

            # For each symbol in the arc table, we need to create a
            # single edge in the new FST.  This edge's input string
            # will be the input symbol; its output string will be the
            # shortest common prefix of strings that can be generated
            # by the original FST in response to the symbol; and its
            # destination state will encode the set of states that the
            # original FST can go to when it sees this symbol, paired
            # with the residual output strings that would have been
            # generated by the original FST, but have not yet been
            # generated by the new FST.
            for sym in arc_table:
                for dst in arc_table[sym]:
                    if len(arc_table[sym][dst]) > 1:
                        # two arcs w/ the same src, dst, and insym,
                        # but different residuals -> bad!
                        raise ValueError("Determinization failed")

                # Construct a list of (destination, residual) pairs.
                dst_residual_pairs = [(dst, arc_table[sym][dst].pop())
                                     for dst in arc_table[sym]]

                # Find the longest common prefix of all the residuals.
                # Note that it's ok if some of the residuals disagree,
                # but *only* if the states associated with those
                # residuals can never both reach a final state with a
                # single input string.
                residuals = [res for (dst, res) in dst_residual_pairs]
                prefix = self._common_prefix(residuals)

                # Construct the new arc's destination state.  The new
                # arc's output string will be `prefix`, so the new
                # destination state should be the set of all pairs
                # (dst, residual-prefix).
                new_arc_dst = frozenset([(dst, res[len(prefix):]) 
                                         for (dst,res) in dst_residual_pairs])

                # If the new arc's destination state isn't part of
                # the FST yet, then add it; and add it to the queue.
                if not new_fst.has_state(new_arc_dst):
                    new_fst.add_state(new_arc_dst)
                    queue.append(new_arc_dst)

                # Create the new arc.
                new_fst.add_arc(src=new_fst_state, dst=new_arc_dst,
                                in_string=(sym,), out_string=prefix)
        return new_fst

    def _all_equal(self, lst):
        """Return true if all elements in the list are equal"""
        for item in lst[1:]:
            if item != lst[0]: return False
        return True

    def _common_prefix(self, sequences):
        """Return the longest sequence that is a prefix of all of the
        given sequences."""
        prefix = sequences[0]
        for seq in sequences[1:]:
            # If the sequence is longer then the prefix, then truncate
            # the prefix to the length of the sequence.
            prefix = prefix[:len(seq)]
            # If the prefix doesn't match item i of the sequence, then
            # truncate the prefix to include everything up to (but not
            # including) element i.
            for i in range(len(prefix)):
                if seq[i] != prefix[i]:
                    prefix = prefix[:i]
                    break
        return prefix

    #////////////////////////////////////////////////////////////
    #{ Misc
    #////////////////////////////////////////////////////////////

    def copy(self, label=None):
        # Choose a label & create the FST.
        if label is None: label = '%s-copy' % self.label
        fst = FST(label)

        # Copy all state:
        fst._initial_state = self._initial_state
        fst._incoming = self._incoming.copy()
        fst._outgoing = self._outgoing.copy()
        fst._is_final = self._is_final.copy()
        fst._finalizing_string = self._finalizing_string.copy()
        fst._state_descr = self._state_descr.copy()
        fst._src = self._src.copy()
        fst._dst = self._dst.copy()
        fst._in_string = self._in_string.copy()
        fst._out_string = self._out_string.copy()
        fst._arc_descr = self._arc_descr.copy()
        return fst

    def __str__(self):
        lines = ['FST %s' % self.label]
        for state in sorted(self.states()):
            # State information.
            if state == self.initial_state:
                line = '-> %s' % state
                lines.append('  %-40s # Initial state' % line)
            if self.is_final(state):
                line = '%s ->' % state
                if self.finalizing_string(state):
                    line += ' [%s]' % ' '.join(self.finalizing_string(state))
                lines.append('  %-40s # Final state' % line)
            # List states that would otherwise not be listed.
            if (state != self.initial_state and not self.is_final(state)
                and not self.outgoing(state) and not self.incoming(state)):
                lines.append('  %-40s # State' % state)
        # Outgoing edge information.
        for arc in sorted(self.arcs()):
            src, dst, in_string, out_string = self.arc_info(arc)
            line = ('%s -> %s [%s:%s]' %
                    (src, dst, ' '.join(in_string), ' '.join(out_string)))
            lines.append('  %-40s # Arc' % line)
        return '\n'.join(lines)
            
    @staticmethod
    def load(filename):
        label = os.path.split(filename)[-1]
        return FST.parse(label, open(filename).read())

    @staticmethod
    def parse(label, s):
        fst = FST(label)
        prev_src = None
        lines = s.split('\n')[::-1]
        while lines:
            line = lines.pop().split('#')[0].strip() # strip comments
            if not line: continue

            # Initial state
            m = re.match(r'->\s*(\S+)$', line)
            if m:
                label = m.group(1)
                if not fst.has_state(label): fst.add_state(label)
                fst.initial_state = label
                continue

            # Final state
            m = re.match('(\S+)\s*->\s*(?:\[([^\]]*)\])?$', line)
            if m:
                label, finalizing_string = m.groups()
                if not fst.has_state(label): fst.add_state(label)
                fst.set_final(label)
                if finalizing_string is not None:
                    finalizing_string = finalizing_string.split()
                    fst.set_finalizing_string(label, finalizing_string)
                continue

            # State
            m = re.match('(\S+)$', line)
            if m:
                label = m.group(1)
                if not fst.has_state(label): fst.add_state(label)
                continue

            # State description
            m = re.match(r'descr\s+(\S+?):\s*(.*)$', line)
            if m:
                label, descr = m.groups()
                # Allow for multi-line descriptions:
                while lines and re.match(r'\s+\S', lines[-1]):
                    descr = descr.rstrip()+' '+lines.pop().lstrip()
                if not fst.has_state(label): fst.add_state(label)
                fst.set_descr(label, descr)
                continue

            # Transition arc
            m = re.match(r'(\S+)?\s*->\s*(\S+)\s*'
                         r'\[(.*?):(.*?)\]$', line)
            if m:
                src, dst, in_string, out_string = m.groups()
                if src is None: src = prev_src
                if src is None: raise ValueError("bad line: %r" % line)
                prev_src = src
                if not fst.has_state(src): fst.add_state(src)
                if not fst.has_state(dst): fst.add_state(dst)
                in_string = tuple(in_string.split())
                out_string = tuple(out_string.split())
                fst.add_arc(src, dst, in_string, out_string)
                continue
    
            raise ValueError("bad line: %r" % line)
    
        return fst

    def dotgraph(self):
        """
        Return an AT&T graphviz dot graph.
        """
        # [xx] mark initial node??
        lines = ['digraph %r {' % self.label,
                 'node [shape=ellipse]']
        state_id = dict([(s,i) for (i,s) in enumerate(self.states())])
        if self.initial_state is not None:
            lines.append('init [shape="plaintext" label=""]')
            lines.append('init -> %s' % state_id[self.initial_state])
        for state in self.states():
            if self.is_final(state):
                final_str = self.finalizing_string(state)
                if len(final_str)>0:
                    lines.append('%s [label="%s\\n%s", shape=doublecircle]' %
                                 (state_id[state], state, ' '.join(final_str)))
                else:
                    lines.append('%s [label="%s", shape=doublecircle]' %
                                 (state_id[state], state))
            else:
                lines.append('%s [label="%s"]' % (state_id[state], state))
        for arc in self.arcs():
            src, dst, in_str, out_str = self.arc_info(arc)
            lines.append('%s -> %s [label="%s:%s"]' %
                         (state_id[src], state_id[dst],
                          ' '.join(in_str), ' '.join(out_str)))
        lines.append('}')
        return '\n'.join(lines)
    
    #////////////////////////////////////////////////////////////
    #{ Transduction
    #////////////////////////////////////////////////////////////

    def transduce_subsequential(self, input, step=True):
        return self.step_transduce_subsequential(input, step=False).next()[1]
    
    def step_transduce_subsequential(self, input, step=True):
        """
        This is implemented as a generator, to make it easier to
        support stepping.
        """
        if not self.is_subsequential():
            raise ValueError('FST is not subsequential!')

        # Create a transition table that indicates what action we
        # should take at any state for a given input symbol.  In
        # paritcular, this table maps from (src, in) tuples to
        # (dst, out, arc) tuples.  (arc is only needed in case
        # we want to do stepping.)
        transitions = {}
        for arc in self.arcs():
            src, dst, in_string, out_string = self.arc_info(arc)
            assert len(in_string) == 1
            assert (src, in_string[0]) not in transitions
            transitions[src, in_string[0]] = (dst, out_string, arc)

        output = []
        state = self.initial_state
        try:
            for in_pos, in_sym in enumerate(input):
                (state, out_string, arc) = transitions[state, in_sym]
                if step: yield 'step', (arc, in_pos, output)
                output += out_string
            yield 'succeed', output
        except KeyError:
            yield 'fail', None

    def transduce(self, input):
        return self.step_transduce(input, step=False).next()[1]

    def step_transduce(self, input, step=True):
        """
        This is implemented as a generator, to make it easier to
        support stepping.
        """
        input = tuple(input)
        output = []
        in_pos = 0

        # 'frontier' is a stack used to keep track of which parts of
        # the search space we have yet to examine.  Each element has
        # the form (arc, in_pos, out_pos), and indicates that we
        # should try rolling the input position back to in_pos, the
        # output position back to out_pos, and applying arc.  Note
        # that the order that we check elements in is important, since
        # rolling the output position back involves discarding
        # generated output.
        frontier = []

        # Start in the initial state, and search for a valid
        # transduction path to a final state.
        state = self.initial_state
        while in_pos < len(input) or not self.is_final(state):
            # Get a list of arcs we can possibly take.
            arcs = self.outgoing(state)
    
            # Add the arcs to our backtracking stack.  (The if condition
            # could be eliminated if I used eliminate_multi_input_arcs;
            # but I'd like to retain the ability to trace what's going on
            # in the FST, as its specified.)
            for arc in arcs:
                in_string = self.in_string(arc)
                if input[in_pos:in_pos+len(in_string)] == in_string:
                    frontier.append( (arc, in_pos, len(output)) )
    
            # Get the top element of the frontiering stack.
            if len(frontier) == 0:
                yield 'fail', None

            # perform the operation from the top of the frontier.
            arc, in_pos, out_pos = frontier.pop()
            if step:
                yield 'step', (arc, in_pos, output[:out_pos])
            
            # update our state, input position, & output.
            state = self.dst(arc)
            assert out_pos <= len(output)
            in_pos = in_pos + len(self.in_string(arc))
            output = output[:out_pos]
            output.extend(self.out_string(arc))

        # If it's a subsequential transducer, add the final output for
        # the terminal state.
        output += self.finalizing_string(state)
    
        yield 'succeed', output
        

    #////////////////////////////////////////////////////////////
    #{ Helper Functions
    #////////////////////////////////////////////////////////////

    def _pick_label(self, label, typ, used_labels):
        """
        Helper function for L{add_state} and C{add_arc} that chooses a
        label for a new state or arc.
        """
        if label is not None and label in used_labels:
            raise ValueError("%s with label %r already exists" %
                             (typ, label))
        # If no label was specified, pick one.
        if label is not None:
            return label
        else:
            label = 1
            while '%s%d' % (typ[0], label) in used_labels: label += 1
            return '%s%d' % (typ[0], label)

######################################################################
#{ AT&T fsmtools Support
######################################################################

class FSMTools:
    """
    A class used to interface with the AT&T fsmtools package.  In
    particular, L{FSMTools.transduce} can be used to transduce an
    input string using any subsequential transducer where each input
    and output arc is labelled with at most one symbol.
    """
    EPSILON = object()
    """A special symbol object used to represent epsilon strings in
    the symbol<->id mapping (L{FSMTools._symbol_ids})."""

    def __init__(self, fsmtools_path=''):
        self.fsmtools_path = fsmtools_path
        """The path of the directory containing the fsmtools binaries."""

        self._symbol_ids = self.IDMapping(self.EPSILON)
        """A mapping from symbols to unique integer IDs.  We manage
        our own mapping, rather than using 'symbol files', since
        symbol files can't handle non-string symbols, symbols
        containing whitespace, unicode symbols, etc."""
        
        self._state_ids = self.IDMapping()
        """A mapping from state labels to unique integer IDs."""

    #////////////////////////////////////////////////////////////
    #{ Transduction
    #////////////////////////////////////////////////////////////

    def transduce(self, fst, input_string):
        try:
            # Create a temporary working directory for intermediate files.
            tempdir = tempfile.mkdtemp()
            def tmp(s): return os.path.join(tempdir, s+'.fsm')

            # Comile the FST & input file into binary fmstool format.
            self.compile_fst(fst, tmp('fst'))
            self.compile_string(input_string, tmp('in'))

            # Transduce the input using the FST.  We do this in two
            # steps: first, use fsmcompose to eliminate any paths that
            # are not consistent with the input string; and then use
            # fsmbestpath to choose a path through the FST.  If the
            # FST is nondeterministic, then the path chosen is
            # arbitrary.  Finally, print the result, so we can process
            # it and extract the output sequence.
            p1 = Popen([self._bin('fsmcompose'), tmp('in'), tmp('fst')],
                       stdout=PIPE)
            p2 = Popen([self._bin('fsmbestpath')],
                       stdin=p1.stdout, stdout=PIPE)
            p3 = Popen([self._bin('fsmprint')],
                       stdin=p2.stdout, stdout=PIPE)
            out_string_fsm = p3.communicate()[0]
        finally:
            for f in os.listdir(tempdir):
                os.unlink(os.path.join(tempdir, f))
            os.rmdir(tempdir)

        # If the empty string was returned, then the input was not
        # accepted by the FST; return None.
        if len(out_string_fsm) == 0:
            return None

        # Otherwise, the input was accepted, so extract the
        # corresponding output string.
        out_string = []
        final_state_id = 0
        for line in out_string_fsm.split('\n'):
            words = line.split()
            if len(words) == 5:
                out_string.append(self._symbol_ids.getval(words[3]))
                final_state_id += int(words[4])
            elif len(words) == 4:
                out_string.append(self._symbol_ids.getval(words[3]))
            elif len(words) == 2:
                final_state_id += int(words[1])
            elif len(words) != 0:
                raise ValueError("Bad output line: %r" % line)

        # Add on the finalizing string for the final state.
        final_state = self._state_ids.getval(final_state_id)
        out_string += fst.finalizing_string(final_state)
        return out_string

    #////////////////////////////////////////////////////////////
    #{ FSM Compilation
    #////////////////////////////////////////////////////////////

    def compile_fst(self, fst, outfile):
        """
        Compile the given FST to an fsmtools .fsm file, and write it
        to the given filename.
        """
        if fst.initial_state is None:
            raise ValueError("FST has no initial state!")
        if not (fst.is_final(fst.initial_state) or
                len(fst.outgoing(fst.initial_state)) > 0):
            raise ValueError("Initial state is nonfinal & "
                             "has no outgoing arcs")

        # Put the initial state first, since that's how fsmtools
        # decides which state is the initial state.
        states = [fst.initial_state] + [s for s in fst.states() if
                                        s != fst.initial_state]

        # Write the outgoing edge for each state, & mark final states.
        lines = []
        for state in states:
            for arc in fst.outgoing(state):
                src, dst, in_string, out_string = fst.arc_info(arc)
                lines.append('%d %d %d %d\n' %
                         (self._state_ids.getid(src),
                          self._state_ids.getid(dst),
                          self._string_id(in_string),
                          self._string_id(out_string)))
            if fst.is_final(state):
                lines.append('%d %d\n' % (self._state_ids.getid(state),
                                        self._state_ids.getid(state)))
                
        # Run fsmcompile to compile it.
        p = Popen([self._bin('fsmcompile'), '-F', outfile], stdin=PIPE)
        p.communicate(''.join(lines))

    def compile_string(self, sym_string, outfile):
        """
        Compile the given symbol string into an fsmtools .fsm file,
        and write it to the given filename.  This FSM will generate
        the given symbol string, and no other strings.
        """
        # Create the input for fsmcompile.
        lines = []
        for (i, sym) in enumerate(sym_string):
            lines.append('%d %d %d\n' % (i, i+1, self._symbol_ids.getid(sym)))
        lines.append('%d\n' % len(sym_string))
    
        # Run fsmcompile to compile it.
        p = Popen([self._bin('fsmcompile'), '-F', outfile], stdin=PIPE)
        p.communicate(''.join(lines))

    #////////////////////////////////////////////////////////////
    #{ Helpers
    #////////////////////////////////////////////////////////////

    def _bin(self, command):
        return os.path.join(self.fsmtools_path, command)

    def _string_id(self, sym_string):
        if len(sym_string) == 0:
            return self._symbol_ids.getid(self.EPSILON)
        elif len(sym_string) == 1:
            return self._symbol_ids.getid(sym_string[0])
        else:
            raise ValueError('fsmtools does not support multi-symbol '
                             'input or output strings on arcs.??')

    class IDMapping:
        def __init__(self, *values):
            self._id_to_val = list(values)
            self._val_to_id = dict([(v,i) for (i,v) in enumerate(values)])
            
        def getid(self, val):
            if val not in self._val_to_id:
                self._id_to_val.append(val)
                self._val_to_id[val] = len(self._id_to_val)-1
            return self._val_to_id[val]

        def getval(self, identifier):
            return self._id_to_val[int(identifier)]

######################################################################
#{ Graphical Display
######################################################################

class FSTWidget(GraphWidget):
    def __init__(self, canvas, fst, **attribs):
        GraphWidget.__init__(self, canvas, (), (), **attribs)
                             
        # Create a widget for each state.
        self.state_widgets = state_widgets = {}
        for state in fst.states():
            label = TextWidget(canvas, state)
            if fst.is_final(state) and fst.finalizing_string(state):
                fstr = fst.finalizing_string(state)
                label = StackWidget(canvas, label,
                              SequenceWidget(canvas,
                                       SymbolWidget(canvas, 'rightarrow'),
                                       TextWidget(canvas, fstr)))
            w = OvalWidget(canvas, label,
                           double=fst.is_final(state), margin=2,
                           fill='white')
            if state == fst.initial_state:
                w = SequenceWidget(canvas,
                                   SymbolWidget(canvas, 'rightarrow'), w)
            w['draggable'] = True
            state_widgets[state] = w
            self.add_node(w)
        
        # Create a widget for each arc.
        self.arc_widgets = arc_widgets = {}
        for arc in fst.arcs():
            label = TextWidget(canvas, ' '.join(fst.in_string(arc)) + ' : ' +
                               ' '.join(fst.out_string(arc)))
            w = GraphEdgeWidget(canvas, 0,0,0,0, label, color='cyan4')
            arc_widgets[arc] = w
            self.add_edge(state_widgets[fst.src(arc)],
                          state_widgets[fst.dst(arc)], w)

        # Arrange the graph.
        if fst.initial_state is not None:
            toplevel = [self.state_widgets[fst.initial_state]]
        else:
            toplevel = None
        self.arrange(toplevel=toplevel)

    def mark_state(self, state, color='green2'):
        oval = self.state_widgets[state]
        if isinstance(oval, SequenceWidget):
            oval = oval.child_widgets()[1]
        oval['fill'] = color
                      
    def unmark_state(self, state):
        oval = self.state_widgets[state]
        if isinstance(oval, SequenceWidget):
            oval = oval.child_widgets()[1]
        oval['fill'] = 'white'

    def mark_arc(self, arc):
        edge = self.arc_widgets[arc]
        edge['width'] = 2
        edge['color'] = 'blue'
                      
    def unmark_arc(self, arc):
        edge = self.arc_widgets[arc]
        edge['width'] = 1
        edge['color'] = 'cyan4'

class FSTDisplay:
    def __init__(self, *fsts):
        self.cf = CanvasFrame(width=580, height=600, background='#f0f0f0')
        self.cf._parent.geometry('+650+50') # hack..

        self.fst_widgets = {}
        for fst in fsts:
            self.add_fst(fst)

    def add_fst(self, fst):
        w = FSTWidget(self.cf.canvas(), fst, draggable=True,
                      xspace=130, yspace=100)
        self.fst_widgets[fst] = w
        self.cf.add_widget(w, x=20)

class FSTDemo:
    def __init__(self, fst):
        self.top = Tk()
        f1 = Frame(self.top)
        f2 = Frame(self.top)
        f3 = Frame(self.top)
        f4 = Frame(self.top)

        # The canvas for the FST itself.
        self.cf = CanvasFrame(f1, width=800, height=400,
                              background='#f0f0f0', 
                              relief="sunken", border="2")
        self.cf.pack(expand=True, fill='both')

        # The description of the current state.
        self.state_label = Label(f4, font=('bold', -16))
        self.state_label.pack(side='top', anchor='sw')
        self.state_descr = Text(f4, height=3, wrap='word', border="2",
                               relief="sunken", font='helvetica',
                               width=10)
        self.state_descr.pack(side='bottom', expand=True, fill='x')

        # The input string.
        font = ('courier', -16, 'bold')
        Label(f2,text=' Input:', font='courier').pack(side='left')
        self.in_text = in_text = Text(f2, height=1, wrap='none',
                                      font=font, background='#c0ffc0',
                                      #padx=0, pady=0,
                                      width=10,
                                      highlightcolor='green',
                                      highlightbackground='green',
                                      highlightthickness=1)
        in_text.tag_config('highlight', foreground='white',
                           background='green4')
        in_text.insert('end', 'r = reset; <space> = step')
        in_text.tag_add('read', '1.0', '1.4')
        in_text.pack(side='left', expand=True, fill="x")

        # The input string.
        Label(f3,text='Output:', font='courier').pack(side='left')
        self.out_text = out_text = Text(f3, height=1, wrap='none',
                                        font=font, background='#ffc0c0',
                                        #padx=0, pady=0,
                                        width=10,
                                        highlightcolor='red',
                                        highlightbackground='red',
                                        highlightthickness=1)
        out_text.tag_config('highlight', foreground='white',
                           background='red4')
        out_text.pack(side='left', expand=True, fill="x")
        
        f1.pack(expand=True, fill='both', side='top', padx=5, pady=5)
        f4.pack(expand=False, fill='x', side='bottom', padx=5, pady=5)
        f3.pack(expand=False, fill='x', side='bottom', padx=5, pady=5)
        f2.pack(expand=False, fill='x', side='bottom', padx=5, pady=5)

        self.top.title('FST')
        self.top.geometry('+650+50')
        self.top.bind('<Control-p>', lambda e: self.cf.print_to_file())
        self.top.bind('<Control-x>', self.destroy)
        self.top.bind('<Control-q>', self.destroy)
        self.top.bind('<space>', self.step)
        self.top.bind('r', lambda e: self.transduce(self.stepper_input))

        self.stepper = None

        self.graph = None
        self.set_fst(fst)

    def transduce(self, input):
        if self.fst.is_subsequential():
            self.stepper = self.fst.step_transduce_subsequential(input)
        else:
            self.stepper = self.fst.step_transduce(input)
        self.stepper_input = input
        # the following really duplicates code in step(), and should be
        # factored out.
        self.in_text.delete('1.0', 'end')
        self.out_text.delete('1.0', 'end')
        self.in_text.insert('1.0', ' '.join(self.stepper_input))
        for state in self.fst.states():
            self.graph.unmark_state(state)
        self.graph.mark_state(self.fst.initial_state)
        self.state_label['text'] = 'State = %s' % self.fst.initial_state
        self.state_descr.delete('1.0', 'end')
        state_descr = fst.state_descr(self.fst.initial_state)
        self.state_descr.insert('end', state_descr or '')
        
    def step(self, *e):
        if self.stepper is None: return

        # Perform one step.
        try: result, val = self.stepper.next()
        except StopIteration: return

        if result == 'fail':
            self.stepper = None
            self.out_text.insert('end', ' (Failed!)')
        elif result == 'succeed':
            self.stepper = None
            self.out_text.delete('1.0', 'end')
            self.out_text.insert('end', ' '.join(val))
            self.out_text.tag_add('highlight', '1.0', 'end-1c')
            self.out_text.insert('end', ' (Finished!)')
        elif result == 'backtrack':
            self.out_text.insert('end', ' (Backtrack)')
            for state, widget in self.graph.state_widgets.items():
                if state == val: self.graph.mark_state(state, '#f0b0b0')
                else: self.graph.unmark_state(state)
        else:
            (arc, in_pos, output) = val
            
            # Update in text display
            in_pos += len(fst.in_string(arc))
            output = list(output)+list(fst.out_string(arc))
            self.in_text.delete('1.0', 'end')
            self.in_text.insert('end', ' '.join(self.stepper_input[:in_pos]))
            self.in_text.tag_add('highlight', '1.0', 'end-1c')
            if in_pos > 0:
                self.in_text.insert('end', ' ')
            l,r= self.in_text.xview()
            if (r-l) < 1:
                self.in_text.xview_moveto(1.0-(r-l)/2)
            self.in_text.insert('end', ' '.join(self.stepper_input[in_pos:]))

            # Update out text display
            self.out_text.delete('1.0', 'end')
            self.out_text.insert('end', ' '.join(output))
            self.out_text.tag_add('highlight', '1.0', 'end-1c')

            # Update the state descr display
            self.state_label['text'] = 'State = %s' % fst.dst(arc)
            self.state_descr.delete('1.0', 'end')
            state_descr = fst.state_descr(fst.dst(arc))
            self.state_descr.insert('end', state_descr or '')

            # Highlight the new dst state.
            for state, widget in self.graph.state_widgets.items():
                if state == fst.dst(arc):
                    self.graph.mark_state(state, '#00ff00')
                elif state == fst.src(arc):
                    self.graph.mark_state(state, '#b0f0b0')
                else: self.graph.unmark_state(state)
        
            # Highlight the new arc.
            for a, widget in self.graph.arc_widgets.items():
                if a == arc: self.graph.mark_arc(a)
                else: self.graph.unmark_arc(a)

        # Make end of output visible..
        l,r= self.out_text.xview()
        if (r-l) < 1:
            self.out_text.xview_moveto(1.0-(r-l)/2)
            self.out_text.insert('end', ' '*100)

    def set_fst(self, fst):
        self.fst = fst
        c = self.cf.canvas()
        if self.graph is not None:
            self.cf.remove_widget(self.graph)
        self.graph = FSTWidget(c, self.fst, xspace=130, yspace=100)
        self.cf.add_widget(self.graph, 20, 20)

    def destroy(self, *e):
        if self.top is None: return
        self.top.destroy()
        self.top = None

    def mainloop(self, *args, **kwargs):
        self.top.mainloop(*args, **kwargs)

######################################################################
#{ Test Code
######################################################################

if __name__ == '__main__':
    # This is a very contrived example.  :)
    # Something phonetic might be better.
    fst = FST.parse("test", """
        -> start
        start -> vp [john:john]
        start -> vp [mary:mary]

        # Delay production of the determiner until we know the gender.
        start -> subj_noun [the:]
        subj_noun -> vp [dog:le chien]
        subj_noun -> vp [cow:la vache]

        vp -> obj [eats:mange]
        obj -> obj_noun [the:]
        obj -> obj_noun [:]
        obj_noun -> end [grass:de l'herbe]
        obj_noun -> end [bread:du pain]
        end ->
        """)

    print "john eats the bread ->"
    print '  '+ ' '.join(fst.transduce("john eats the bread".split()))
    rev = fst.inverted()
    print "la vache mange de l'herbe ->"
    print '  '+' '.join(rev.transduce("la vache mange de l'herbe".split()))

    demo = FSTDemo(fst)
    demo.transduce("the cow eats the bread".split())
    demo.mainloop()

########NEW FILE########
__FILENAME__ = fst2
# Natural Language Toolkit: Finite State Transducers
#
# Copyright (C) 2001-2011 NLTK Project
# Author: Edward Loper <edloper@gradient.cis.upenn.edu>
#         Steven Bird <sb@csse.unimelb.edu.au>
#         Jonathan Epstein <gieguy98@gmail.com> -- additions
#
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT


"""
Finite state transducers.

A finite state trasducer, or FST, is a directed graph that is used to
encode a mapping from a set of I{input strings} to a set of I{output
strings}.  An X{input string} is a sequence of immutable values (such
as integers, characters, or strings) called X{input symbols}.
Similarly, an C{output string} is a sequence of immutable values
called X{output symbols}.  Collectively, input strings and output
strings are called X{symbol strings}, or simply X{strings} for short.
Note that this notion of I{string} is different from the python string
type -- symbol strings are always encoded as tuples of input or output
symbols, even if those symbols are characters.  Also, note that empty
sequences are valid symbol strings.

The nodes of an FST are called X{states}, and the edges are called
X{transition arcs} or simply X{arcs}.  States may be marked as
X{final}, and each final state is annotated with an output string,
called the X{finalizing string}.  Each arc is annotated with an input
string and an output string.  An arc with an empty input string is
called an I{epsilon-input arc}; and an arc with an empty output
string is called an I{epsilon-output arc}.

The set of mappings encoded by the FST are defined by the set of paths
through the graph, starting at a special state known as the X{initial
state}, and ending at a final state.  In particular, the FST maps an
input string X to an output string Y iff there exists a path from the
initial state to a final state such that:

  - The input string X is formed by concatenating the input strings
    of the arcs along the path (in order).
  - The output string Y is formed by concatenating the output strings
    of the arcs along the path (in order), plus the final state's
    output string.

The following list defines some terms that apply to finite state
transducers.

  - The X{transduction} defined by a FST is the mapping from input
    strings to output strings.
    
  - An FST X{encodes a deterministic transduction} if each input
    string maps to at most one output string.  An FST X{encodes a
    nondeterministic transduction} if any input string maps to more
    than one output string.

  - An FST is X{deterministic} if it every state contains at most one
    outgoing arc that is consistent with any input string; otherwise,
    the FST is X{nondeterministic}.  If an FST is deterministic, then
    it necessarily encodes a deterministic transduction; however, it
    is possible to define an FST that is nondeterministic but that
    encodes a deterministic transduction.

  - An FST is X{sequential} if each arc is labeled with exactly one
    input symbol, no two outgoing arcs from any state have the same
    input symbol, and all finalizing strings are empty.  (Sequential
    implies deterministic).

  - An FST is I{subsequential} if each arc is labeled with exactly
    one input symbol, and no two outgoing arcs from any state have
    the same input symbol.  (Finalizing strings may be non-empty.)

An FSA can be represented as an FST that generates no output symbols.

The current FST class does not provide support for:

  - Weighted arcs.  (However, weights can be used as, or included
    in, the output symbols.  The total weight of a path can then
    be found after transduction by combining the weights.  But
    there's no support for e.g., finding the path with the minimum
    weight.
    
  - Multiple initial states.
  
  - Initializing strings (an output string associated with the initial
    state, which is always generated when the FST begins).

Possible future changes:

  - Define several classes, in a class hierarchy?  E.g., FSA is a base
    class, FST inherits from it.  And maybe a further subclass to add
    finalizing sequences.  I would need to be more careful to only
    access the private variables when necessary, and to usually go
    through the accessor functions.
"""

import re, os, random, tempfile
from subprocess import Popen, PIPE
from nltk.draw import *
from nltk_contrib.fst.draw_graph import *

######################################################################
# CONTENTS
######################################################################
# 1. Finite State Transducer
#    - State information
#    - Transition Arc Information
#    - FST Information
#    - State Modification
#    - Transition Arc Modification
#    - Transformations
#    - Misc
#    - Transduction
# 2. AT&T fsmtools support
# 3. Graphical Display
#    - FSTDisplay
#    - FSTDemo
######################################################################

######################################################################
#{ Finite State Transducer
######################################################################

class FST(object):
    """
    A finite state transducer.  Each state is uniquely identified by a
    label, which is typically a string name or an integer id.  A
    state's label is used to access and modify the state.  Similarly,
    each arc is uniquely identified by a label, which is used to
    access and modify the arc.

    The set of arcs pointing away from a state are that state's
    I{outgoing} arcs.  The set of arcs pointing to a state are that
    state's I{incoming} arcs.  The state at which an arc originates is
    that arc's I{source} state (or C{src}), and the state at which it
    terminates is its I{destination} state (or C{dst}).

    It is possible to define an C{FST} object with no initial state.
    This is represented by assigning a value of C{None} to the
    C{initial_state} variable.  C{FST}s with no initial state are
    considered to encode an empty mapping.  I.e., transducing any
    string with such an C{FST} will result in failure.
    """
    def __init__(self, label):
        """
        Create a new finite state transducer, containing no states.
        """
        self.label = label
        """A label identifying this FST.  This is used for display &
        debugging purposes only."""

        #{ State Information
        self._initial_state = None
        """The label of the initial state, or C{None} if this FST
        does not have an initial state."""
        
        self._incoming = {}
        """A dictionary mapping state labels to lists of incoming
        transition arc labels."""

        self._outgoing = {}
        """A dictionary mapping state labels to lists of outgoing
        transition arc labels."""

        self._is_final = {}
        """A dictionary mapping state labels to boolean values,
        indicating whether the state is final."""

        self._finalizing_string = {}
        """A dictionary mapping state labels of final states to output
        strings.  This string should be added to the output
        if the FST terminates at this state."""

        self._state_descr = {}
        """A dictionary mapping state labels to (optional) state
        descriptions."""
        #}

        #{ Transition Arc Information
        self._src = {}
        """A dictionary mapping each transition arc label to the label of
        its source state."""

        self._dst = {}
        """A dictionary mapping each transition arc label to the label of
        its destination state."""

        self._in_string = {}
        """A dictionary mapping each transition arc label to its input
        string, a (possibly empty) tuple of input symbols."""

        self._out_string = {}
        """A dictionary mapping each transition arc label to its output
        string, a (possibly empty) tuple of input symbols."""

        self._arc_descr = {}
        """A dictionary mapping transition arc labels to (optional)
        arc descriptions."""
        #}
        
    #////////////////////////////////////////////////////////////
    #{ State Information
    #////////////////////////////////////////////////////////////

    def states(self):
        """Return an iterator that will generate the state label of
        each state in this FST."""
        return iter(self._incoming)

    def has_state(self, label):
        """Return true if this FST contains a state with the given
        label."""
        return label in self._incoming

    def _get_initial_state(self):
        return self._initial_state
    def _set_initial_state(self, label):
        if label is not None and label not in self._incoming:
            raise ValueError('Unknown state label %r' % label)
        self._initial_state = label
    initial_state = property(_get_initial_state, _set_initial_state,
                             doc="The label of the initial state (R/W).")

    def incoming(self, state):
        """Return an iterator that will generate the incoming
        transition arcs for the given state.  The effects of modifying
        the FST's state while iterating are undefined, so if you plan
        to modify the state, you should copy the incoming transition
        arcs into a list first."""
        return iter(self._incoming[state])

    def outgoing(self, state):
        """Return an iterator that will generate the outgoing
        transition arcs for the given state.  The effects of modifying
        the FST's state while iterating are undefined, so if you plan
        to modify the state, you should copy the outgoing transition
        arcs into a list first."""
        return iter(self._outgoing[state])

    def is_final(self, state):
        """Return true if the state with the given state label is
        final."""
        return self._is_final[state]

    def finalizing_string(self, state):
        """Return the output string associated with the given final
        state.  If the FST terminates at this state, then this string
        will be emitted."""
        #if not self._is_final[state]:
        #    raise ValueError('%s is not a final state' % state)
        return self._finalizing_string.get(state, ())

    def state_descr(self, state):
        """Return the description for the given state, if it has one;
        or None, otherwise."""
        return self._state_descr.get(state)

    #////////////////////////////////////////////////////////////
    #{ Transition Arc Information
    #////////////////////////////////////////////////////////////

    def arcs(self):
        """Return an iterator that will generate the arc label of
        each transition arc in this FST."""
        return iter(self._src)

    def src(self, arc):
        """Return the state label of this transition arc's source
        state."""
        return self._src[arc]

    def dst(self, arc):
        """Return the state label of this transition arc's destination
        state."""
        return self._dst[arc]

    def in_string(self, arc):
        """Return the given transition arc's input string, a (possibly
        empty) tuple of input symbols."""
        return self._in_string[arc]
    
    def out_string(self, arc):
        """Return the given transition arc's output string, a
        (possibly empty) tuple of output symbols."""
        return self._out_string[arc]
    
    def arc_descr(self, arc):
        """Return the description for the given transition arc, if it
        has one; or None, otherwise."""
        return self._arc_descr.get(arc)

    def arc_info(self, arc):
        """Return a tuple (src, dst, in_string, out_string) for the
        given arc, where:
          - C{src} is the label of the arc's source state.
          - C{dst} is the label of the arc's destination state.
          - C{in_string} is the arc's input string.
          - C{out_string} is the arc's output string.
        """
        return (self._src[arc], self._dst[arc],
                self._in_string[arc], self._out_string[arc])

    #////////////////////////////////////////////////////////////
    #{ FST Information
    #////////////////////////////////////////////////////////////

    def is_sequential(self):
        """
        Return true if this FST is sequential.
        """
        for state in self.states():
            if self.finalizing_string(state): return False
        return self.is_subsequential()

    def is_subsequential(self):
        """
        Return true if this FST is subsequential.
        """
        for state in self.states():
            out_syms = set()
            for arc in self.outgoing(state):
                out_string = self.out_string(arc)
                if len(out_string) != 1: return False
                if out_string[0] in out_syms: return False
                out_syms.add(out_string)
        return True

    #////////////////////////////////////////////////////////////
    #{ State Modification
    #////////////////////////////////////////////////////////////

    def add_state(self, label=None, is_final=False,
                  finalizing_string=(), descr=None):
        """
        Create a new state, and return its label.  The new state will
        have no incoming or outgoing arcs.  If C{label} is specified,
        then it will be used as the state's label; otherwise, a new
        unique label value will be chosen.  The new state will be
        final iff C{is_final} is true.  C{descr} is an optional
        description string for the new state.
        
        Arguments should be specified using keywords!
        """
        label = self._pick_label(label, 'state', self._incoming)
        
        # Add the state.
        self._incoming[label] = []
        self._outgoing[label] = []
        self._is_final[label] = is_final
        self._state_descr[label] = descr
        self._finalizing_string[label] = tuple(finalizing_string)
        
        # Return the new state's label.
        return label

    def del_state(self, label):
        """
        Delete the state with the given label.  This will
        automatically delete any incoming or outgoing arcs attached to
        the state.
        """
        if label not in self._incoming:
            raise ValueError('Unknown state label %r' % label)

        # Delete the incoming/outgoing arcs.
        for arc in self._incoming[label]:
            del (self._src[arc], self._dst[arc], self._in_string[arc],
                 self._out_string[arc], self._arc_descr[arc])
        for arc in self._outgoing[label]:
            del (self._src[arc], self._dst[arc], self._in_string[arc],
                 self._out_string[arc], self._arc_descr[arc])

        # Delete the state itself.
        del (self._incoming[label], self._otugoing[label],
             self._is_final[label], self._state_descr[label],
             self._finalizing_string[label])

        # Check if we just deleted the initial state.
        if label == self._initial_state:
            self._initial_state = None

    def set_final(self, state, is_final=True):
        """
        If C{is_final} is true, then make the state with the given
        label final; if C{is_final} is false, then make the state with
        the given label non-final.
        """
        if state not in self._incoming:
            raise ValueError('Unknown state label %r' % state)
        self._is_final[state] = is_final

    def set_finalizing_string(self, state, finalizing_string):
        """
        Set the given state's finalizing string.
        """
        if not self._is_final[state]:
            raise ValueError('%s is not a final state' % state)
        if state not in self._incoming:
            raise ValueError('Unknown state label %r' % state)
        self._finalizing_string[state] = tuple(finalizing_string)

    def set_descr(self, state, descr):
        """
        Set the given state's description string.
        """
        if state not in self._incoming:
            raise ValueError('Unknown state label %r' % state)
        self._state_descr[state] = descr

    def dup_state(self, orig_state, label=None):
        """
        Duplicate an existing state.  I.e., create a new state M{s}
        such that:
          - M{s} is final iff C{orig_state} is final.
          - If C{orig_state} is final, then M{s.finalizing_string}
            is copied from C{orig_state}
          - For each outgoing arc from C{orig_state}, M{s} has an
            outgoing arc with the same input string, output
            string, and destination state.

        Note that if C{orig_state} contained self-loop arcs, then the
        corresponding arcs in M{s} will point to C{orig_state} (i.e.,
        they will I{not} be self-loop arcs).

        The state description is I{not} copied.
            
        @param label: The label for the new state.  If not specified,
            a unique integer will be used.
        """
        if orig_state not in self._incoming: 
            raise ValueError('Unknown state label %r' % src)
        
        # Create a new state.
        new_state = self.add_state(label=label)

        # Copy finalization info.
        if self.is_final(orig_state):
            self.set_final(new_state)
            self.set_finalizing_string(new_state,
                                       self.finalizing_string(orig_state))

        # Copy the outgoing arcs.
        for arc in self._outgoing[orig_state]:
            self.add_arc(src=new_state, dst=self._dst[arc],
                         in_string=self._in_string[arc],
                         out_string=self._out_string[arc])

        return new_state

    #////////////////////////////////////////////////////////////
    #{ Transition Arc Modification
    #////////////////////////////////////////////////////////////

    def add_arc(self, src, dst, in_string, out_string,
                label=None, descr=None):
        """
        Create a new transition arc, and return its label.

        Arguments should be specified using keywords!
        
        @param src: The label of the source state.
        @param dst: The label of the destination state.
        @param in_string: The input string, a (possibly empty) tuple of
            input symbols.  Input symbols should be hashable
            immutable objects.
        @param out_string: The output string, a (possibly empty) tuple
            of output symbols.  Output symbols should be hashable
            immutable objects.
        """
        label = self._pick_label(label, 'arc', self._src)

        # Check that src/dst are valid labels.
        if src not in self._incoming:
            raise ValueError('Unknown state label %r' % src)
        if dst not in self._incoming:
            raise ValueError('Unknown state label %r' % dst)

        # Add the arc.
        self._src[label] = src
        self._dst[label] = dst
        self._in_string[label] = tuple(in_string)
        self._out_string[label] = tuple(out_string)
        self._arc_descr[label] = descr

        # Link the arc to its src/dst states.
        self._incoming[dst].append(label)
        self._outgoing[src].append(label)

        # Return the new arc's label.
        return label
            
    def del_arc(self, label):
        """
        Delete the transition arc with the given label.
        """
        if label not in self._src:
            raise ValueError('Unknown arc label %r' % src)

        # Disconnect the arc from its src/dst states.
        self._incoming[self._dst[label]].remove(label)
        self._outgoing[self._src[label]].remove(label)

        # Delete the arc itself.
        del (self._src[label], self._dst[label], self._in_string[label],
             self._out_string[label], self._arc_descr[label])

    #////////////////////////////////////////////////////////////
    #{ Transformations
    #////////////////////////////////////////////////////////////

    def inverted(self):
        """Swap all in_string/out_string pairs."""
        fst = self.copy()
        fst._in_string, fst._out_string = fst._out_string, fst._in_string
        return fst

    def reversed(self):
        """Reverse the direction of all transition arcs."""
        fst = self.copy()
        fst._incoming, fst._outgoing = fst._outgoing, fst._incoming
        fst._src, fst._dst = fst._dst, fst._src
        return fst

    def trimmed(self):
        fst = self.copy()
        
        if fst.initial_state is None:
            raise ValueError("No initial state!")

        # Determine whether there is a path from the initial node to
        # each node.
        queue = [fst.initial_state]
        path_from_init = set(queue)
        while queue:
            state = queue.pop()
            dsts = [fst.dst(arc) for arc in fst.outgoing(state)]
            queue += [s for s in dsts if s not in path_from_init]
            path_from_init.update(dsts)

        # Determine whether there is a path from each node to a final
        # node.
        queue = [s for s in fst.states() if fst.is_final(s)]
        path_to_final = set(queue)
        while queue:
            state = queue.pop()
            srcs = [fst.src(arc) for arc in fst.incoming(state)]
            queue += [s for s in srcs if s not in path_to_final]
            path_to_final.update(srcs)

        # Delete anything that's not on a path from the initial state
        # to a final state.
        for state in list(fst.states()):
            if not (state in path_from_init and state in path_to_final):
                fst.del_state(state)

        return fst
    
    def relabeled(self, label=None, relabel_states=True, relabel_arcs=True):
        """
        Return a new FST that is identical to this FST, except that
        all state and arc labels have been replaced with new labels.
        These new labels are consecutive integers, starting with zero.

        @param relabel_states: If false, then don't relabel the states.
        @param relabel_arcs: If false, then don't relabel the arcs.
        """
        if label is None: label = '%s (relabeled)' % self.label
        fst = FST(label)

        # This will ensure that the state relabelling is canonical, *if*
        # the FST is subsequential.
        state_ids = self._relabel_state_ids(self.initial_state, {})
        if len(state_ids) < len(self._outgoing):
            for state in self.states():
                if state not in state_ids:
                    state_ids[state] = len(state_ids)

        # This will ensure that the arc relabelling is canonical, *if*
        # the state labelling is canonical.
        arcs = sorted(self.arcs(), key=self.arc_info)
        arc_ids = dict([(a,i) for (i,a) in enumerate(arcs)])

        for state in self.states():
            if relabel_states: label = state_ids[state]
            else: label = state
            fst.add_state(label, is_final=self.is_final(state),
                          finalizing_string=self.finalizing_string(state),
                          descr=self.state_descr(state))

        for arc in self.arcs():
            if relabel_arcs: label = arc_ids[arc]
            else: label = arc
            src, dst, in_string, out_string = self.arc_info(arc)
            if relabel_states:
                src = state_ids[src]
                dst = state_ids[dst]
            fst.add_arc(src=src, dst=dst, in_string=in_string,
                        out_string=out_string,
                        label=label, descr=self.arc_descr(arc))

        if relabel_states:
            fst.initial_state = state_ids[self.initial_state]
        else:
            fst.initial_state = self.initial_state
            
        return fst

    def _relabel_state_ids(self, state, ids):
        """
        A helper function for L{relabel()}, which decides which new
        label should be assigned to each state.
        """
        if state in ids: return
        ids[state] = len(ids)
        for arc in sorted(self.outgoing(state),
                          key = lambda a:self.in_string(a)):
            self._relabel_state_ids(self.dst(arc), ids)
        return ids

    def determinized(self, label=None):
        """
        Return a new FST which defines the same mapping as this FST,
        but is determinized.

        The algorithm used is based on [...].

        @require: All arcs in this FST must have exactly one input
            symbol.
        @require: The mapping defined by this FST must be
            deterministic.
        @raise ValueError: If the determinization algorithm was unable
            to determinize this FST.  Typically, this happens because
            a precondition is not met.
        """
        # Check preconditions..
        for arc in self.arcs():
            if len(self.in_string(arc)) != 1:
                raise ValueError("All arcs must have exactly one "
                                 "input symbol.")
        
        # State labels have the form:
        #   frozenset((s1,w1),(s2,w2),...(sn,wn))
        # Where si is a state and wi is a string of output symbols.
        if label is None: label = '%s (determinized)' % self.label
        new_fst = FST(label)

        initial_state = frozenset( [(self.initial_state,())] )
        new_fst.add_state(initial_state)
        new_fst.initial_state = initial_state
                          
        queue = [initial_state]
        while queue:
            new_fst_state = queue.pop()

            # For each final state from the original FSM that's
            # contained in the new FST's state, compute the finalizing
            # string.  If there is at least one finalizing string,
            # then the new state is a final state.  However, if the
            # finalizing strings are not all identical, then the
            # transduction defined by this FST is nondeterministic, so
            # fail.
            finalizing_strings = [w+self.finalizing_string(s)
                                  for (s,w) in new_fst_state
                                  if self.is_final(s)]
            if len(set(finalizing_strings)) > 0:
                if not self._all_equal(finalizing_strings):
                    # multiple conflicting finalizing strings -> bad!
                    raise ValueError("Determinization failed")
                new_fst.set_final(new_fst_state)
                new_fst.set_finalizing_string(new_fst_state,
                                              finalizing_strings[0])

            # sym -> dst -> [residual]
            # nb: we checked above that len(in_string)==1 for all arcs.
            arc_table = {}
            for (s,w) in new_fst_state:
                for arc in self.outgoing(s):
                    sym = self.in_string(arc)[0]
                    dst = self.dst(arc)
                    residual = w + self.out_string(arc)
                    arc_table.setdefault(sym,{}).setdefault(dst,set())
                    arc_table[sym][dst].add(residual)

            # For each symbol in the arc table, we need to create a
            # single edge in the new FST.  This edge's input string
            # will be the input symbol; its output string will be the
            # shortest common prefix of strings that can be generated
            # by the original FST in response to the symbol; and its
            # destination state will encode the set of states that the
            # original FST can go to when it sees this symbol, paired
            # with the residual output strings that would have been
            # generated by the original FST, but have not yet been
            # generated by the new FST.
            for sym in arc_table:
                for dst in arc_table[sym]:
                    if len(arc_table[sym][dst]) > 1:
                        # two arcs w/ the same src, dst, and insym,
                        # but different residuals -> bad!
                        raise ValueError("Determinization failed")

                # Construct a list of (destination, residual) pairs.
                dst_residual_pairs = [(dst, arc_table[sym][dst].pop())
                                     for dst in arc_table[sym]]

                # Find the longest common prefix of all the residuals.
                # Note that it's ok if some of the residuals disagree,
                # but *only* if the states associated with those
                # residuals can never both reach a final state with a
                # single input string.
                residuals = [res for (dst, res) in dst_residual_pairs]
                prefix = self._common_prefix(residuals)

                # Construct the new arc's destination state.  The new
                # arc's output string will be `prefix`, so the new
                # destination state should be the set of all pairs
                # (dst, residual-prefix).
                new_arc_dst = frozenset([(dst, res[len(prefix):]) 
                                         for (dst,res) in dst_residual_pairs])

                # If the new arc's destination state isn't part of
                # the FST yet, then add it; and add it to the queue.
                if not new_fst.has_state(new_arc_dst):
                    new_fst.add_state(new_arc_dst)
                    queue.append(new_arc_dst)

                # Create the new arc.
                new_fst.add_arc(src=new_fst_state, dst=new_arc_dst,
                                in_string=(sym,), out_string=prefix)
        return new_fst

    def _all_equal(self, lst):
        """Return true if all elements in the list are equal"""
        for item in lst[1:]:
            if item != lst[0]: return False
        return True

    def _common_prefix(self, sequences):
        """Return the longest sequence that is a prefix of all of the
        given sequences."""
        prefix = sequences[0]
        for seq in sequences[1:]:
            # If the sequence is longer then the prefix, then truncate
            # the prefix to the length of the sequence.
            prefix = prefix[:len(seq)]
            # If the prefix doesn't match item i of the sequence, then
            # truncate the prefix to include everything up to (but not
            # including) element i.
            for i in range(len(prefix)):
                if seq[i] != prefix[i]:
                    prefix = prefix[:i]
                    break
        return prefix

    #////////////////////////////////////////////////////////////
    #{ Misc
    #////////////////////////////////////////////////////////////

    def copy(self, label=None):
        # Choose a label & create the FST.
        if label is None: label = '%s-copy' % self.label
        fst = FST(label)

        # Copy all state:
        fst._initial_state = self._initial_state
        fst._incoming = self._incoming.copy()
        fst._outgoing = self._outgoing.copy()
        fst._is_final = self._is_final.copy()
        fst._finalizing_string = self._finalizing_string.copy()
        fst._state_descr = self._state_descr.copy()
        fst._src = self._src.copy()
        fst._dst = self._dst.copy()
        fst._in_string = self._in_string.copy()
        fst._out_string = self._out_string.copy()
        fst._arc_descr = self._arc_descr.copy()
        return fst

    def __str__(self):
        lines = ['FST %s' % self.label]
        for state in sorted(self.states()):
            # State information.
            if state == self.initial_state:
                line = '-> %s' % state
                lines.append('  %-40s # Initial state' % line)
            if self.is_final(state):
                line = '%s ->' % state
                if self.finalizing_string(state):
                    line += ' [%s]' % ' '.join(self.finalizing_string(state))
                lines.append('  %-40s # Final state' % line)
            # List states that would otherwise not be listed.
            if (state != self.initial_state and not self.is_final(state)
                and not self.outgoing(state) and not self.incoming(state)):
                lines.append('  %-40s # State' % state)
        # Outgoing edge information.
        for arc in sorted(self.arcs()):
            src, dst, in_string, out_string = self.arc_info(arc)
            line = ('%s -> %s [%s:%s]' %
                    (src, dst, ' '.join(in_string), ' '.join(out_string)))
            lines.append('  %-40s # Arc' % line)
        return '\n'.join(lines)
            
    @staticmethod
    def load(filename):
        label = os.path.split(filename)[-1]
        return FST.parse(label, open(filename).read())

    @staticmethod
    def parse(label, s):
        fst = FST(label)
        prev_src = None
        lines = s.split('\n')[::-1]
        while lines:
            line = lines.pop().split('#')[0].strip() # strip comments
            if not line: continue

            # Initial state
            m = re.match(r'->\s*(\S+)$', line)
            if m:
                label = m.group(1)
                if not fst.has_state(label): fst.add_state(label)
                fst.initial_state = label
                continue

            # Final state
            m = re.match('(\S+)\s*->\s*(?:\[([^\]]*)\])?$', line)
            if m:
                label, finalizing_string = m.groups()
                if not fst.has_state(label): fst.add_state(label)
                fst.set_final(label)
                if finalizing_string is not None:
                    finalizing_string = finalizing_string.split()
                    fst.set_finalizing_string(label, finalizing_string)
                continue

            # State
            m = re.match('(\S+)$', line)
            if m:
                label = m.group(1)
                if not fst.has_state(label): fst.add_state(label)
                continue

            # State description
            m = re.match(r'descr\s+(\S+?):\s*(.*)$', line)
            if m:
                label, descr = m.groups()
                # Allow for multi-line descriptions:
                while lines and re.match(r'\s+\S', lines[-1]):
                    descr = descr.rstrip()+' '+lines.pop().lstrip()
                if not fst.has_state(label): fst.add_state(label)
                fst.set_descr(label, descr)
                continue

            # Transition arc
            m = re.match(r'(\S+)?\s*->\s*(\S+)\s*'
                         r'\[(.*?):(.*?)\]$', line)
            if m:
                src, dst, in_string, out_string = m.groups()
                if src is None: src = prev_src
                if src is None: raise ValueError("bad line: %r" % line)
                prev_src = src
                if not fst.has_state(src): fst.add_state(src)
                if not fst.has_state(dst): fst.add_state(dst)
                in_string = tuple(in_string.split())
                out_string = tuple(out_string.split())
                fst.add_arc(src, dst, in_string, out_string)
                continue
    
            raise ValueError("bad line: %r" % line)
    
        return fst

    @staticmethod
    def mergeRuns(sortedArcs,minRun):
        run = []
        result = []
        rightIncreasing = False
    
        for temp_in_str,temp_out_str in sortedArcs:
            in_str = ' '.join(temp_in_str) # reformat strings
            out_str = ' '.join(temp_out_str)
            contRun = False # Are we able to continue an existing run?  Usually not.
            if len(in_str) == 1:
                if len(run) > 0:
                    (last_in,last_out) = run[-1]
                    if len(last_in) == 1 and ord(in_str) == ord(last_in)+1:
                        if len(out_str) == 1 and len(last_out) == 1 and ord(out_str) == ord(last_out)+1:
                            if len(run) == 1:
                                rightIncreasing = True
                                contRun = True
                            else:
                                if rightIncreasing:
                                    contRun = True
                        if out_str == last_out and not rightIncreasing:
                            contRun = True
            if len(run) > 0 and not contRun:
                if len(run) >= minRun:
                    (firstleft,firstright) = run[0]
                    (lastleft,lastright) = run[-1]
                    left = firstleft + "-" + lastleft
                    right = firstright
                    if rightIncreasing:
                        right = firstright + "-" + lastright
                    result += [(left,right)]
                    run = []
                    rightIncreasing = False
                else:
                    result.extend(run)
                    run = []
    
            if len(run) == 0 and len(in_str) == 1:
                contRun = True
            if contRun:
                run += [(in_str,out_str)]
            else:
                result += [(in_str,out_str)]
    
        if len(run) > 0:
            if len(run) >= minRun:
                (firstleft,firstright) = run[0]
                (lastleft,lastright) = run[-1]
                left = firstleft + "-" + lastleft
                right = firstright
                if rightIncreasing:
                    right = firstright + "-" + lastright
                result += [(left,right)]
                run = []
                rightIncreasing = False
            else:
                result.extend(run)
                run = []
    
        return result

    def dotgraph(self, mergeEdges=True, multiEdgesToNodesColoringThreshold=2.5,
            maxEdgeLineLen=15, minRun=3):
        """
        Return an AT&T graphviz dot graph.
        """
        # [xx] mark initial node??
        lines = ['digraph %r {' % self.label,
                 'node [shape=ellipse]']
        state_id = dict([(s,i) for (i,s) in enumerate(self.states())])
        if self.initial_state is not None:
            lines.append('init [shape="plaintext" label=""]')
            lines.append('init -> %s' % state_id[self.initial_state])
        stateCount = 0
        for state in self.states():
            stateCount += 1
            if self.is_final(state):
                final_str = self.finalizing_string(state)
                if len(final_str)>0:
                    lines.append('%s [label="%s\\n%s", shape=doublecircle]' %
                                 (state_id[state], state, ' '.join(final_str)))
                else:
                    lines.append('%s [label="%s", shape=doublecircle]' %
                                 (state_id[state], state))
            else:
                lines.append('%s [label="%s"]' % (state_id[state], state))
        if mergeEdges:
            uniqueArcs = dict()
            colors = ('black','red','blue','green','purple','orange')
            for arc in self.arcs():
                src, dst, in_str, out_str = self.arc_info(arc)
                if (src,dst) in uniqueArcs:
                    uniqueArcs[(src,dst)] += [(in_str,out_str)]
                else:
                    uniqueArcs[(src,dst)] = [(in_str,out_str)]
            ratio = float(len(uniqueArcs.keys())) / float(stateCount)
            for src,dst in uniqueArcs.keys():
                uniqueArcs[(src,dst)].sort()
                sortedArcs = FST.mergeRuns(uniqueArcs[(src,dst)],minRun)
                label = ""
                sinceLastNewLine = 0
                for in_str,out_str in sortedArcs:
                    locallabel = ""
                    if label != "":
                        label += ","
                        sinceLastNewLine += 1
                    if sinceLastNewLine > maxEdgeLineLen:
                        sinceLastNewLine = 0
                        label += r'\n'
                    if in_str == out_str and len(in_str) > 0:
#                        locallabel += ' '.join(in_str)
                        locallabel += in_str
                    else:
#                        locallabel += ' '.join(in_str) + ":" + ' '.join(out_str)
                        locallabel += in_str + ":" + out_str
                    label += locallabel
                    sinceLastNewLine += len(locallabel)
                if ratio > multiEdgesToNodesColoringThreshold:
                    color = colors[random.randrange(0,len(colors))]
                else: # no reason to light up like a Christmas tree
                    color = colors[0]
                lines.append('%s -> %s [label="%s" color=%s fontcolor=%s]' %
                             (state_id[src], state_id[dst], label, color, color))
        else:
            for arc in self.arcs():
                src, dst, in_str, out_str = self.arc_info(arc)
                lines.append('%s -> %s [label="%s:%s"]' %
                             (state_id[src], state_id[dst],
                              ' '.join(in_str), ' '.join(out_str)))
        lines.append('}')
        return '\n'.join(lines)
    
    #////////////////////////////////////////////////////////////
    #{ Transduction
    #////////////////////////////////////////////////////////////

    def transduce_subsequential(self, input, step=True):
        return self.step_transduce_subsequential(input, step=False).next()[1]
    
    def step_transduce_subsequential(self, input, step=True):
        """
        This is implemented as a generator, to make it easier to
        support stepping.
        """
        if not self.is_subsequential():
            raise ValueError('FST is not subsequential!')

        # Create a transition table that indicates what action we
        # should take at any state for a given input symbol.  In
        # paritcular, this table maps from (src, in) tuples to
        # (dst, out, arc) tuples.  (arc is only needed in case
        # we want to do stepping.)
        transitions = {}
        for arc in self.arcs():
            src, dst, in_string, out_string = self.arc_info(arc)
            assert len(in_string) == 1
            assert (src, in_string[0]) not in transitions
            transitions[src, in_string[0]] = (dst, out_string, arc)

        output = []
        state = self.initial_state
        try:
            for in_pos, in_sym in enumerate(input):
                (state, out_string, arc) = transitions[state, in_sym]
                if step: yield 'step', (arc, in_pos, output)
                output += out_string
            yield 'succeed', output
        except KeyError:
            yield 'fail', None

    def transduce(self, input):
        return self.step_transduce(input, step=False).next()[1]

    def step_transduce(self, input, step=True):
        """
        This is implemented as a generator, to make it easier to
        support stepping.
        """
        input = tuple(input)
        output = []
        in_pos = 0

        # 'frontier' is a stack used to keep track of which parts of
        # the search space we have yet to examine.  Each element has
        # the form (arc, in_pos, out_pos), and indicates that we
        # should try rolling the input position back to in_pos, the
        # output position back to out_pos, and applying arc.  Note
        # that the order that we check elements in is important, since
        # rolling the output position back involves discarding
        # generated output.
        frontier = []

        # Start in the initial state, and search for a valid
        # transduction path to a final state.
        state = self.initial_state
        while in_pos < len(input) or not self.is_final(state):
            # Get a list of arcs we can possibly take.
            arcs = self.outgoing(state)
    
            # Add the arcs to our backtracking stack.  (The if condition
            # could be eliminated if I used eliminate_multi_input_arcs;
            # but I'd like to retain the ability to trace what's going on
            # in the FST, as its specified.)
            for arc in arcs:
                in_string = self.in_string(arc)
                if input[in_pos:in_pos+len(in_string)] == in_string:
                    frontier.append( (arc, in_pos, len(output)) )
    
            # Get the top element of the frontiering stack.
            if len(frontier) == 0:
                yield 'fail', None

            # perform the operation from the top of the frontier.
            arc, in_pos, out_pos = frontier.pop()
            if step:
                yield 'step', (arc, in_pos, output[:out_pos])
            
            # update our state, input position, & output.
            state = self.dst(arc)
            assert out_pos <= len(output)
            in_pos = in_pos + len(self.in_string(arc))
            output = output[:out_pos]
            output.extend(self.out_string(arc))

        # If it's a subsequential transducer, add the final output for
        # the terminal state.
        output += self.finalizing_string(state)
    
        yield 'succeed', output
        

    #////////////////////////////////////////////////////////////
    #{ Helper Functions
    #////////////////////////////////////////////////////////////

    def _pick_label(self, label, typ, used_labels):
        """
        Helper function for L{add_state} and C{add_arc} that chooses a
        label for a new state or arc.
        """
        if label is not None and label in used_labels:
            raise ValueError("%s with label %r already exists" %
                             (typ, label))
        # If no label was specified, pick one.
        if label is not None:
            return label
        else:
            label = 1
            while '%s%d' % (typ[0], label) in used_labels: label += 1
            return '%s%d' % (typ[0], label)

######################################################################
#{ AT&T fsmtools Support
######################################################################

class FSMTools:
    """
    A class used to interface with the AT&T fsmtools package.  In
    particular, L{FSMTools.transduce} can be used to transduce an
    input string using any subsequential transducer where each input
    and output arc is labelled with at most one symbol.
    """
    EPSILON = object()
    """A special symbol object used to represent epsilon strings in
    the symbol<->id mapping (L{FSMTools._symbol_ids})."""

    def __init__(self, fsmtools_path=''):
        self.fsmtools_path = fsmtools_path
        """The path of the directory containing the fsmtools binaries."""

        self._symbol_ids = self.IDMapping(self.EPSILON)
        """A mapping from symbols to unique integer IDs.  We manage
        our own mapping, rather than using 'symbol files', since
        symbol files can't handle non-string symbols, symbols
        containing whitespace, unicode symbols, etc."""
        
        self._state_ids = self.IDMapping()
        """A mapping from state labels to unique integer IDs."""

    #////////////////////////////////////////////////////////////
    #{ Transduction
    #////////////////////////////////////////////////////////////

    def transduce(self, fst, input_string):
        try:
            # Create a temporary working directory for intermediate files.
            tempdir = tempfile.mkdtemp()
            def tmp(s): return os.path.join(tempdir, s+'.fsm')

            # Comile the FST & input file into binary fmstool format.
            self.compile_fst(fst, tmp('fst'))
            self.compile_string(input_string, tmp('in'))

            # Transduce the input using the FST.  We do this in two
            # steps: first, use fsmcompose to eliminate any paths that
            # are not consistent with the input string; and then use
            # fsmbestpath to choose a path through the FST.  If the
            # FST is nondeterministic, then the path chosen is
            # arbitrary.  Finally, print the result, so we can process
            # it and extract the output sequence.
            p1 = Popen([self._bin('fsmcompose'), tmp('in'), tmp('fst')],
                       stdout=PIPE)
            p2 = Popen([self._bin('fsmbestpath')],
                       stdin=p1.stdout, stdout=PIPE)
            p3 = Popen([self._bin('fsmprint')],
                       stdin=p2.stdout, stdout=PIPE)
            out_string_fsm = p3.communicate()[0]
        finally:
            for f in os.listdir(tempdir):
                os.unlink(os.path.join(tempdir, f))
            os.rmdir(tempdir)

        # If the empty string was returned, then the input was not
        # accepted by the FST; return None.
        if len(out_string_fsm) == 0:
            return None

        # Otherwise, the input was accepted, so extract the
        # corresponding output string.
        out_string = []
        final_state_id = 0
        for line in out_string_fsm.split('\n'):
            words = line.split()
            if len(words) == 5:
                out_string.append(self._symbol_ids.getval(words[3]))
                final_state_id += int(words[4])
            elif len(words) == 4:
                out_string.append(self._symbol_ids.getval(words[3]))
            elif len(words) == 2:
                final_state_id += int(words[1])
            elif len(words) != 0:
                raise ValueError("Bad output line: %r" % line)

        # Add on the finalizing string for the final state.
        final_state = self._state_ids.getval(final_state_id)
        out_string += fst.finalizing_string(final_state)
        return out_string

    #////////////////////////////////////////////////////////////
    #{ FSM Compilation
    #////////////////////////////////////////////////////////////

    def compile_fst(self, fst, outfile):
        """
        Compile the given FST to an fsmtools .fsm file, and write it
        to the given filename.
        """
        if fst.initial_state is None:
            raise ValueError("FST has no initial state!")
        if not (fst.is_final(fst.initial_state) or
                len(fst.outgoing(fst.initial_state)) > 0):
            raise ValueError("Initial state is nonfinal & "
                             "has no outgoing arcs")

        # Put the initial state first, since that's how fsmtools
        # decides which state is the initial state.
        states = [fst.initial_state] + [s for s in fst.states() if
                                        s != fst.initial_state]

        # Write the outgoing edge for each state, & mark final states.
        lines = []
        for state in states:
            for arc in fst.outgoing(state):
                src, dst, in_string, out_string = fst.arc_info(arc)
                lines.append('%d %d %d %d\n' %
                         (self._state_ids.getid(src),
                          self._state_ids.getid(dst),
                          self._string_id(in_string),
                          self._string_id(out_string)))
            if fst.is_final(state):
                lines.append('%d %d\n' % (self._state_ids.getid(state),
                                        self._state_ids.getid(state)))
                
        # Run fsmcompile to compile it.
        p = Popen([self._bin('fsmcompile'), '-F', outfile], stdin=PIPE)
        p.communicate(''.join(lines))

    def compile_string(self, sym_string, outfile):
        """
        Compile the given symbol string into an fsmtools .fsm file,
        and write it to the given filename.  This FSM will generate
        the given symbol string, and no other strings.
        """
        # Create the input for fsmcompile.
        lines = []
        for (i, sym) in enumerate(sym_string):
            lines.append('%d %d %d\n' % (i, i+1, self._symbol_ids.getid(sym)))
        lines.append('%d\n' % len(sym_string))
    
        # Run fsmcompile to compile it.
        p = Popen([self._bin('fsmcompile'), '-F', outfile], stdin=PIPE)
        p.communicate(''.join(lines))

    #////////////////////////////////////////////////////////////
    #{ Helpers
    #////////////////////////////////////////////////////////////

    def _bin(self, command):
        return os.path.join(self.fsmtools_path, command)

    def _string_id(self, sym_string):
        if len(sym_string) == 0:
            return self._symbol_ids.getid(self.EPSILON)
        elif len(sym_string) == 1:
            return self._symbol_ids.getid(sym_string[0])
        else:
            raise ValueError('fsmtools does not support multi-symbol '
                             'input or output strings on arcs.??')

    class IDMapping:
        def __init__(self, *values):
            self._id_to_val = list(values)
            self._val_to_id = dict([(v,i) for (i,v) in enumerate(values)])
            
        def getid(self, val):
            if val not in self._val_to_id:
                self._id_to_val.append(val)
                self._val_to_id[val] = len(self._id_to_val)-1
            return self._val_to_id[val]

        def getval(self, identifier):
            return self._id_to_val[int(identifier)]

######################################################################
#{ Graphical Display
######################################################################

class FSTWidget(GraphWidget):
    def __init__(self, canvas, fst, **attribs):
        GraphWidget.__init__(self, canvas, (), (), **attribs)
                             
        # Create a widget for each state.
        self.state_widgets = state_widgets = {}
        for state in fst.states():
            label = TextWidget(canvas, state)
            if fst.is_final(state) and fst.finalizing_string(state):
                fstr = fst.finalizing_string(state)
                label = StackWidget(canvas, label,
                              SequenceWidget(canvas,
                                       SymbolWidget(canvas, 'rightarrow'),
                                       TextWidget(canvas, fstr)))
            w = OvalWidget(canvas, label,
                           double=fst.is_final(state), margin=2,
                           fill='white')
            if state == fst.initial_state:
                w = SequenceWidget(canvas,
                                   SymbolWidget(canvas, 'rightarrow'), w)
            w['draggable'] = True
            state_widgets[state] = w
            self.add_node(w)
        
        # Create a widget for each arc.
        self.arc_widgets = arc_widgets = {}
        for arc in fst.arcs():
            label = TextWidget(canvas, ' '.join(fst.in_string(arc)) + ' : ' +
                               ' '.join(fst.out_string(arc)))
            w = GraphEdgeWidget(canvas, 0,0,0,0, label, color='cyan4')
            arc_widgets[arc] = w
            self.add_edge(state_widgets[fst.src(arc)],
                          state_widgets[fst.dst(arc)], w)

        # Arrange the graph.
        if fst.initial_state is not None:
            toplevel = [self.state_widgets[fst.initial_state]]
        else:
            toplevel = None
        self.arrange(toplevel=toplevel)

    def mark_state(self, state, color='green2'):
        oval = self.state_widgets[state]
        if isinstance(oval, SequenceWidget):
            oval = oval.child_widgets()[1]
        oval['fill'] = color
                      
    def unmark_state(self, state):
        oval = self.state_widgets[state]
        if isinstance(oval, SequenceWidget):
            oval = oval.child_widgets()[1]
        oval['fill'] = 'white'

    def mark_arc(self, arc):
        edge = self.arc_widgets[arc]
        edge['width'] = 2
        edge['color'] = 'blue'
                      
    def unmark_arc(self, arc):
        edge = self.arc_widgets[arc]
        edge['width'] = 1
        edge['color'] = 'cyan4'

class FSTDisplay:
    def __init__(self, *fsts):
        self.cf = CanvasFrame(width=580, height=600, background='#f0f0f0')
        self.cf._parent.geometry('+650+50') # hack..

        self.fst_widgets = {}
        for fst in fsts:
            self.add_fst(fst)

    def add_fst(self, fst):
        w = FSTWidget(self.cf.canvas(), fst, draggable=True,
                      xspace=130, yspace=100)
        self.fst_widgets[fst] = w
        self.cf.add_widget(w, x=20)

class FSTDemo:
    def __init__(self, fst):
        self.top = Tk()
        f1 = Frame(self.top)
        f2 = Frame(self.top)
        f3 = Frame(self.top)
        f4 = Frame(self.top)

        # The canvas for the FST itself.
        self.cf = CanvasFrame(f1, width=800, height=400,
                              background='#f0f0f0', 
                              relief="sunken", border="2")
        self.cf.pack(expand=True, fill='both')

        # The description of the current state.
        self.state_label = Label(f4, font=('bold', -16))
        self.state_label.pack(side='top', anchor='sw')
        self.state_descr = Text(f4, height=3, wrap='word', border="2",
                               relief="sunken", font='helvetica',
                               width=10)
        self.state_descr.pack(side='bottom', expand=True, fill='x')

        # The input string.
        font = ('courier', -16, 'bold')
        Label(f2,text=' Input:', font='courier').pack(side='left')
        self.in_text = in_text = Text(f2, height=1, wrap='none',
                                      font=font, background='#c0ffc0',
                                      #padx=0, pady=0,
                                      width=10,
                                      highlightcolor='green',
                                      highlightbackground='green',
                                      highlightthickness=1)
        in_text.tag_config('highlight', foreground='white',
                           background='green4')
        in_text.insert('end', 'r = reset; <space> = step')
        in_text.tag_add('read', '1.0', '1.4')
        in_text.pack(side='left', expand=True, fill="x")

        # The input string.
        Label(f3,text='Output:', font='courier').pack(side='left')
        self.out_text = out_text = Text(f3, height=1, wrap='none',
                                        font=font, background='#ffc0c0',
                                        #padx=0, pady=0,
                                        width=10,
                                        highlightcolor='red',
                                        highlightbackground='red',
                                        highlightthickness=1)
        out_text.tag_config('highlight', foreground='white',
                           background='red4')
        out_text.pack(side='left', expand=True, fill="x")
        
        f1.pack(expand=True, fill='both', side='top', padx=5, pady=5)
        f4.pack(expand=False, fill='x', side='bottom', padx=5, pady=5)
        f3.pack(expand=False, fill='x', side='bottom', padx=5, pady=5)
        f2.pack(expand=False, fill='x', side='bottom', padx=5, pady=5)

        self.top.title('FST')
        self.top.geometry('+650+50')
        self.top.bind('<Control-p>', lambda e: self.cf.print_to_file())
        self.top.bind('<Control-x>', self.destroy)
        self.top.bind('<Control-q>', self.destroy)
        self.top.bind('<space>', self.step)
        self.top.bind('r', lambda e: self.transduce(self.stepper_input))

        self.stepper = None

        self.graph = None
        self.set_fst(fst)

    def transduce(self, input):
        if self.fst.is_subsequential():
            self.stepper = self.fst.step_transduce_subsequential(input)
        else:
            self.stepper = self.fst.step_transduce(input)
        self.stepper_input = input
        # the following really duplicates code in step(), and should be
        # factored out.
        self.in_text.delete('1.0', 'end')
        self.out_text.delete('1.0', 'end')
        self.in_text.insert('1.0', ' '.join(self.stepper_input))
        for state in self.fst.states():
            self.graph.unmark_state(state)
        self.graph.mark_state(self.fst.initial_state)
        self.state_label['text'] = 'State = %s' % self.fst.initial_state
        self.state_descr.delete('1.0', 'end')
        state_descr = fst.state_descr(self.fst.initial_state)
        self.state_descr.insert('end', state_descr or '')
        
    def step(self, *e):
        if self.stepper is None: return

        # Perform one step.
        try: result, val = self.stepper.next()
        except StopIteration: return

        if result == 'fail':
            self.stepper = None
            self.out_text.insert('end', ' (Failed!)')
        elif result == 'succeed':
            self.stepper = None
            self.out_text.delete('1.0', 'end')
            self.out_text.insert('end', ' '.join(val))
            self.out_text.tag_add('highlight', '1.0', 'end-1c')
            self.out_text.insert('end', ' (Finished!)')
        elif result == 'backtrack':
            self.out_text.insert('end', ' (Backtrack)')
            for state, widget in self.graph.state_widgets.items():
                if state == val: self.graph.mark_state(state, '#f0b0b0')
                else: self.graph.unmark_state(state)
        else:
            (arc, in_pos, output) = val
            
            # Update in text display
            in_pos += len(fst.in_string(arc))
            output = list(output)+list(fst.out_string(arc))
            self.in_text.delete('1.0', 'end')
            self.in_text.insert('end', ' '.join(self.stepper_input[:in_pos]))
            self.in_text.tag_add('highlight', '1.0', 'end-1c')
            if in_pos > 0:
                self.in_text.insert('end', ' ')
            l,r= self.in_text.xview()
            if (r-l) < 1:
                self.in_text.xview_moveto(1.0-(r-l)/2)
            self.in_text.insert('end', ' '.join(self.stepper_input[in_pos:]))

            # Update out text display
            self.out_text.delete('1.0', 'end')
            self.out_text.insert('end', ' '.join(output))
            self.out_text.tag_add('highlight', '1.0', 'end-1c')

            # Update the state descr display
            self.state_label['text'] = 'State = %s' % fst.dst(arc)
            self.state_descr.delete('1.0', 'end')
            state_descr = fst.state_descr(fst.dst(arc))
            self.state_descr.insert('end', state_descr or '')

            # Highlight the new dst state.
            for state, widget in self.graph.state_widgets.items():
                if state == fst.dst(arc):
                    self.graph.mark_state(state, '#00ff00')
                elif state == fst.src(arc):
                    self.graph.mark_state(state, '#b0f0b0')
                else: self.graph.unmark_state(state)
        
            # Highlight the new arc.
            for a, widget in self.graph.arc_widgets.items():
                if a == arc: self.graph.mark_arc(a)
                else: self.graph.unmark_arc(a)

        # Make end of output visible..
        l,r= self.out_text.xview()
        if (r-l) < 1:
            self.out_text.xview_moveto(1.0-(r-l)/2)
            self.out_text.insert('end', ' '*100)

    def set_fst(self, fst):
        self.fst = fst
        c = self.cf.canvas()
        if self.graph is not None:
            self.cf.remove_widget(self.graph)
        self.graph = FSTWidget(c, self.fst, xspace=130, yspace=100)
        self.cf.add_widget(self.graph, 20, 20)

    def destroy(self, *e):
        if self.top is None: return
        self.top.destroy()
        self.top = None

    def mainloop(self, *args, **kwargs):
        self.top.mainloop(*args, **kwargs)

######################################################################
#{ Test Code
######################################################################

if __name__ == '__main__':
    # This is a very contrived example.  :)
    # Something phonetic might be better.
    fst = FST.parse("test", """
        -> start
        start -> vp [john:john]
        start -> vp [mary:mary]

        # Delay production of the determiner until we know the gender.
        start -> subj_noun [the:]
        subj_noun -> vp [dog:le chien]
        subj_noun -> vp [cow:la vache]

        vp -> obj [eats:mange]
        obj -> obj_noun [the:]
        obj -> obj_noun [:]
        obj_noun -> end [grass:de l'herbe]
        obj_noun -> end [bread:du pain]
        end ->
        """)

    print "john eats the bread ->"
    print '  '+ ' '.join(fst.transduce("john eats the bread".split()))
    rev = fst.inverted()
    print "la vache mange de l'herbe ->"
    print '  '+' '.join(rev.transduce("la vache mange de l'herbe".split()))

    demo = FSTDemo(fst)
    demo.transduce("the cow eats the bread".split())
    demo.mainloop()

########NEW FILE########
__FILENAME__ = fstypes
"""
C{fstypes.py} module contains the implementation of feature
value types as defined in the FUF manual (v5.2)
"""
from sexp import *
from nltk.featstruct import CustomFeatureValue, UnificationFailure

class FeatureTypeTable(object):
    """
    The C{FeatureTypeTable} maintains relationships between 
    a given feature value and its specializations.
    """

    def __init__(self):
        """
        Instantiate and return the object
        """
        self.table = {}

    def define_type(self, name, children):
        """
        Add a relationship between C{name} and it is specializations
        C{children}.
    
        @param name: The most general value
        @type name: string
        @param children: The specialization of a name
        @type children: single string or list of strings
        """

        if name not in self.table.keys():
            self.table[name] = []
        if isinstance(children, basestring):
            children = [children]
        for child in children:
            self.table[name].append(child)

    def subsume(self, name, specialization):
        """
        Check if C{name} is subsumes the C{specialization}.

        @param name: Feature value
        @type name: string
        @param specialization: Possible specialization of C{name}
        @type specialization: string
        """

        # quick check if the specialization is the immediate one
        spec = specialization
        if name == spec: return True
        if not self.table.has_key(name): return False
        if spec in self.table[name]:
            return True
        return any(self.subsume(item, spec) for item in self.table[name])

    def __repr__(self):
        output = ""
        for key, value in self.table.items():
            output += "%s <--- %s\n" % (key, value)
        return output

class TypedFeatureValue(CustomFeatureValue):
    """
    Feature value that has been defined as a FUF type
    """
    def __init__(self, value, type_table):
        """
        Initialize and return the object.
        
        @param value: The feature value
        @type value: string
        @param type_table: The table of feature value types defined in the
        current grammar.
        @type type_table: FeatureTypeTable
        """
        assert value
        assert type_table
        self.value = value
        self.table = type_table
    
    def __repr__(self):
        """
        Return the string representation of the typed
        feature value.

        @returns: t'featurevalue'
        """
        return "t'%s'" % (self.value)

    def unify(self, other):
        """
        Unify the typed feature value with the other 
        feature value. 
        
        The unification succeeds if both, this and the other
        feature are of C{TypefFeatureValue} type and:
            - this feature is a specialization of the other feature
            - the other feature is a specialization of this feature
            - both features are the same value

        The unification fails if:
            - the feature values are of different types
            - there is no relationship between the two features

        @param other: Feature value
        @type other: C{TypedFeatureValue}
        @return: The most specific type based on the relationship
        defined in the type table of this feature value (C{self.table})

        """
        # feature values must be of the same type
        if not isinstance(other, TypedFeatureValue):
            return UnificationFailure

        # other.value is the specialization of self.value
        if (self.table.subsume(self.value, other.value)):
            return TypedFeatureValue(other.value, self.table)

        # this value is the specialization of other value
        if (self.table.subsume(other.value, self.value)):
            return TypedFeatureValue(self.value, self.table)

        # everything failed
        return UnificationFailure
        
    def __cmp__(self, other):
        if not isinstance(other, TypedFeatureValue):
            return -1
        return cmp(self.value, other.value)

def assign_types(table, fs):
    """
    Convert those feature values that are in the type table to
    C{TypedFeatureValue} objects. This function modifies
    the C{fs} feature structure.

    @param table: The type table
    @type table: C{FeatureTypeTable}
    @param fs: Feature structure
    @type fs: C{nltk.featstruct.FeatStruct}
    """
    def assign_types_helper(fs, type_table, flat_type_table):
        # go through the feature structure and convert the typed values
        for fkey, fval in fs.items():
            if isinstance(fval, nltk.FeatStruct):
                assign_types_helper(fval, type_table, flat_type_table)
            elif isinstance(fval, basestring) and (fval in flat_type_table):
                newval = TypedFeatureValue(fval, table)
                fs[fkey] = newval

    # flattten the table 
    flat_type_table = list()
    for tkey, tvalue in table.table.items():
        flat_type_table.append(tkey)
        for tval in tvalue:
            flat_type_table.append(tval)
   
    assign_types_helper(fs, table, flat_type_table)
    return fs

if __name__ == "__main__":
    typedefs = open('tests/types.fuf').readlines()
    type_table = FeatureTypeTable()
    for typedef in typedefs:
        sexp = SexpListParser().parse(typedef)
        type_table.define_type(sexp[1], sexp[2])

    print type_table
    print type_table.subsume('np', 'common')
    print type_table.subsume('mood', 'imperative')




########NEW FILE########
__FILENAME__ = fuf
import os
import nltk

from fufconvert import *
from link import *
from linearizer import *
from util import output_html, flatten


class GrammarPathResolver(object):
    """
    C{GrammarPathResolver} analyses a given grammar and generates 
    the list of possible paths through the grammar by unpacking
    all the possible alternations.
    """
    def __init__(self, grammar, table):
        """
        Instantiate and return the object
        
        @param grammar: The FUF grammar
        @type grammar: C{nltk.featsturct.FeatStruct}
        @param table: Feature type table
        @type table: C{FeatureTypeTable}
        """
        self.grammar = grammar
        self.table = table


    @staticmethod
    def filter_for_alt(grammar):
        """
        Helper function. Separates the I{alt} from other key, value pairs

        @param grammar: The feature structure to filter
        @type grammar: C{nltk.featstruct.FeatStruct}

        @return: Tuple, where the first item is the feature structure
        that does not contain any alternations on the top level and 
        a list of keys of alternations.
        """

        alts = list()
        fs = nltk.FeatStruct()
        for gkey, gvalue in grammar.items():
            if gkey != "alt" and not gkey.startswith("alt_"):
                #if isinstance(gvalue, basestring):
                fs[gkey] = gvalue
                #else:
                    #fs[gkey] = gvalue.copy()
            else:
                alts.append(gkey)
        return fs, alts 

    @staticmethod
    def alt_to_list(fs, altname):
        """
        Helper function. Converts an altenation structure into a list of values

        @param fs: The alternation feature structure
        @type fs: C{nltk.featstruct.FeatStruct}
        @param altname: The key for the alternation structure
        @type altname: string

        @return: list
        """
        altkeys = fs[altname].keys()
        altkeys = sorted([int(key) for key in altkeys if key != "_index_"], cmp)
        altkeys = [str(key) for key in altkeys]

        alternations = list()
        for key in altkeys:
            alternations.append(fs[altname][key])
        return alternations


    @staticmethod
    def _is_subsumed_val(table, fs, fkey, other):
        """
        Check if the value specified by key I{fkey} in 
        feature structure I{fs} subsumes the value
        at key I{fkey} in the feature structure C{other}

        @param table: Feature type table
        @type table: C{fstypes.FeatureTypeTable}
        @param fs: Feature structure
        @type fs: C{nltk.featstruct.FeatStruct}
        @param other: Feature structure
        @type other: C{nltk.featstruct.FeatStruct}
        @return: True, if fs[key] subsumes other[fkey], False otherwise.
        """
        return table and table.subsume(fs[fkey], other[fkey])

    @staticmethod
    def _copy_vals(table, fs, pack):
        """
        Helper method. Copy the keys and values of I{fs}
        into the pack structure. 
        If pack is a list of feature structures then each of them is 
        mofified. Otherwise, pack is a feature structure and it modified.

        @param table: Feature type table
        @type table: C{fstypes.FeatureTypeTable}
        @param fs: Feature structure
        @type fs: C{nltk.featstruct.FeatSturct}
        @param pack: feature structure or list of feature structures
        @type pack: C{nltk.featstruct.FeatStruct} or list of above
        """
        if isinstance(pack, list):
            for subpack in pack:
                for fkey, fvalue in fs.items():
                    if (fkey in subpack) and \
                       GrammarPathResolver._is_subsumed_val(table, fs, fkey, subpack):
                        pass
                    else:
                        if isinstance(subpack, list): 
                            GrammarPathResolver._copy_vals(table, fs, subpack)
                            return
                        assert not isinstance(subpack, list)
                        assert isinstance(subpack, nltk.FeatStruct)
                        subpack[fkey] = fvalue
        else:
            assert isinstance(pack, nltk.FeatStruct)
            for fkey, fvalue in fs.items():
                if (fkey in pack) and \
                   GrammarPathResolver._is_subsumed_val(table, fs, fkey, pack):
                    pass
                else:
                    pack[fkey] = fvalue

    def resolve(self, fstruct):
        """
        Traverse the grammar and find all the possible paths
        throught the alternations.

        @param fstruct: grammar
        @type fstruct: C{nltk.featstruct.FeatStruct}
        @return: A list of feature structure each of which is a possible
        path through the alternations.
        """

        if isinstance(fstruct, basestring):
            return fstruct
        fs, alts = GrammarPathResolver.filter_for_alt(fstruct)

        if len(alts) > 0:
            result = list()
            for altname in alts:
                toplevel_pack =  GrammarPathResolver.alt_to_list(fstruct, altname)
                subpack = list()
                for item in toplevel_pack:
                    if isinstance(item, nltk.FeatStruct) and len(item.keys()) == 0:
                        # empty feature - result of having opts
                        pass
                    elif isinstance(item, nltk.FeatStruct):
                        temp = self.resolve(item)
                        GrammarPathResolver._copy_vals(self.table, fs, temp)
                        subpack.append(temp)
                    else:
                        subpack.append(item)
                for item in subpack:
                    result.append(item)
            return result
        else:
            total_packs = list()
            for fkey, fvalue in fstruct.items():
                if isinstance(fvalue, nltk.FeatStruct):
                    subpack = list()
                    fs, alts = GrammarPathResolver.filter_for_alt(fvalue)
                    if len(alts) > 0:
                        for item in self.resolve(fvalue):
                            newfs = nltk.FeatStruct()
                            newfs[fkey] = item
                            for key, value in fvalue.items():
                                if not ('alt' in value):
                                    newfs[key] = value
                            subpack.append(newfs)
                        for i in subpack: total_packs.append(i)
            if len(total_packs) > 0:
                return total_packs
            else:
                return fstruct

class Unifier(object):
    """
    Class for unification of feature structures defined by FUF. 
    Example usage:

        >>> itext, gtext = open('tests/uni.fuf').readlines()
        # set up the input structure
        >>> fsinput = fuf_to_featstruct(itext)
        >>> print fsinput
        [ cat  = 's'                      ]
        [                                 ]
        [ goal = [ n = [ lex = 'mary' ] ] ]
        [                                 ]
        [ prot = [ n = [ lex = 'john' ] ] ]
        [                                 ]
        [ verb = [ v = [ lex = 'link' ] ] ]
        # set up the grammar structure
        >>> fsgrammar = fuf_to_featstruct(gtext)
        >>> print fsgrammar
        [           [     [ cat     = 's'                        ]            ] ]
        [           [     [                                      ]            ] ]
        [           [     [ goal    = [ cat = 'np' ]             ]            ] ]
        [           [     [                                      ]            ] ]
        [           [ 1 = [ pattern = (prot, verb, goal)         ]            ] ]
        [           [     [                                      ]            ] ]
        [           [     [ prot    = [ cat = 'np' ]             ]            ] ]
        [           [     [                                      ]            ] ]
        [           [     [ verb    = [ cat    = 'vp'          ] ]            ] ]
        [           [     [           [ number = {prot number} ] ]            ] ]
        [           [                                                         ] ]
        [           [     [       [ 1 = [ pattern = (n)   ]               ] ] ] ]
        [           [     [       [     [ proper  = 'yes' ]               ] ] ] ]
        [           [     [       [                                       ] ] ] ]
        [           [     [ alt = [     [ det     = [ cat = 'article' ] ] ] ] ] ]
        [           [     [       [     [           [ lex = 'the'     ] ] ] ] ] ]
        [           [     [       [ 2 = [                               ] ] ] ] ]
        [ alt_top = [ 2 = [       [     [ pattern = (det, n)            ] ] ] ] ]
        [           [     [       [     [ proper  = 'no'                ] ] ] ] ]
        [           [     [                                                 ] ] ]
        [           [     [ cat = 'np'                                      ] ] ]
        [           [     [                                                 ] ] ]
        [           [     [ n   = [ cat    = 'noun'     ]                   ] ] ]
        [           [     [       [ number = {^^number} ]                   ] ] ]
        [           [                                                         ] ]
        [           [     [ cat     = 'vp'             ]                      ] ]
        [           [ 3 = [ pattern = (v)              ]                      ] ]
        [           [     [                            ]                      ] ]
        [           [     [ v       = [ cat = 'verb' ] ]                      ] ]
        [           [                                                         ] ]
        [           [ 4 = [ cat = 'noun' ]                                    ] ]
        [           [                                                         ] ]
        [           [ 5 = [ cat = 'verb' ]                                    ] ]
        [           [                                                         ] ]
        [           [ 6 = [ cat = 'article' ]                                 ] ]
        # unify the input and the grammar
        >>> fuf = Unifier(fsinput, fsgrammar)
        >>> result = fuf.unify()
        # show the result
        >>> print result
        [ cat     = 's'                               ]
        [                                             ]
        [           [ cat     = 'np'                ] ]
        [           [                               ] ]
        [           [           [ cat    = 'noun' ] ] ]
        [           [ n       = [ lex    = 'mary' ] ] ]
        [ goal    = [           [ number = ?x2    ] ] ]
        [           [                               ] ]
        [           [ number  = ?x2                 ] ]
        [           [ pattern = (n)                 ] ]
        [           [ proper  = 'yes'               ] ]
        [                                             ]
        [ pattern = (prot, verb, goal)                ]
        [                                             ]
        [           [ cat     = 'np'                ] ]
        [           [                               ] ]
        [           [           [ cat    = 'noun' ] ] ]
        [           [ n       = [ lex    = 'john' ] ] ]
        [ prot    = [           [ number = ?x1    ] ] ]
        [           [                               ] ]
        [           [ number  = ?x1                 ] ]
        [           [ pattern = (n)                 ] ]
        [           [ proper  = 'yes'               ] ]
        [                                             ]
        [           [ cat     = 'vp'             ]    ]
        [           [ number  = ?x1              ]    ]
        [ verb    = [ pattern = (v)              ]    ]
        [           [                            ]    ]
        [           [ v       = [ cat = 'verb' ] ]    ]
        [           [           [ lex = 'link' ] ]    ]
    
    .
    """

    def __init__(self, fsinput, fsgrammar, table=None):
        """
        Initialize and return the object.

        @param fsinput: The input feature structure
        @type fsinput: C{nltk.featstruct.FeatStruct}
        @param fsgrammar: The generation grammar
        @type fsgrammar: C{nltk.featstruct.FeatStruct}
        @param table: The feature value type table
        @type table: C{fstypes.FeatureTypeTable}
        """
        import copy
        self.fsinput = fsinput
        self.fsgrammar = fsgrammar
        self.table = table
        self.lr = LinkResolver()
        self.gpr = GrammarPathResolver(copy.deepcopy(fsgrammar), table)

        self.grammar_paths = flatten(self.gpr.resolve(copy.deepcopy(fsgrammar)))

        # the type table has been passed in
        # assign types to the feature values
        if table:
            for i, path in enumerate(self.grammar_paths):
                path = assign_types(table, path)
                self.grammar_paths[i] = path


        
    @staticmethod
    def _isconstituent(fstruct, subfs_key, subfs_val):
        """
        Features containing cat attributes are constituents.
        If feature (cset (c1 .. cn)) is foudn i the FS then the cset is just (c1 ..
        cn)
        if no feature cset is found, the set is the unifion o the following fs:
            - if a pair contains feature (cat xx), it is constituent
            - if sub-fd is mentioned in the (pattern ..) it is a constituent
        """
        if not isinstance(subfs_val, nltk.FeatStruct):
            return False

        if 'cat' in subfs_val:
            return True

        if ('pattern' in fstruct):
            for fkey in subfs_val.keys():
                if fkey in fstruct['pattern']:
                    return True
        return False


    @staticmethod
    def _unify(fs, grs, resolver=None, trace=False):
        unifs = None
        for gr in grs:
            unifs = fs.unify(gr)
            if unifs:
                resolver.resolve(unifs)
                for fname, fval in unifs.items():
                    if Unifier._isconstituent(unifs, fname, fval):
                        newval = Unifier._unify(fval, grs, resolver)
                        if newval:
                            unifs[fname] = newval
                return unifs
        return unifs
    

    def unify(self):
        """
        Unify the input feature structure with the grammar feature structure
        
        @return: If unification is succesful the result is the unified
        structure. Otherwise return value is None.
        
        """
        
        self.lr.resolve(self.fsinput)
        # make a copy of the original input
        return Unifier._unify(self.fsinput, self.grammar_paths, self.lr)


if __name__ == "__main__":
    # tests for unification

    # inputs and grammars from fuf distribution
    grammar_files = [gfile for gfile in os.listdir('tests/') if gfile.startswith('gr')]
    input_files = [ifile for ifile in os.listdir('tests/') if ifile.startswith('ir')]

    grammar_files = ['gr2.fuf']
    input_files = ['ir2.fuf']
    for ifile, gfile in zip(input_files, grammar_files):
        if ifile == 'ir3.fuf' and gfile == 'gr3.fuf':
            print 'gr3.fuf doesn\'t work because of the (* focus) s-expression in the feature structure'
            continue
        # input files contain more than one definition of input
        output = None
        result = None
        print "\nINPUT FILE: %s, GRAMMAR FILE: %s" % (ifile, gfile)
        gfs = fuf_to_featstruct(open('tests/%s' % gfile).read())
        for i, iline in enumerate(open('tests/%s' % ifile).readlines()):
            try:
                ifs = fuf_to_featstruct(iline)
            except Exception, e:
                print 'Failed to convert %s to nltk.FeatStruct' % iline
                exit()
            fuf = Unifier(ifs, gfs)
            result = fuf.unify()
            if result:
                output = " ".join(linearize(result))
                print output_html([ifs, gfs, result, output])
                print i, "result:", output
            else:
                print i, 'result: failed'

########NEW FILE########
__FILENAME__ = fufconvert
import re
import os
import nltk
from sexp import *
from link import *
from specialfs import *
from fstypes import *

def fuf_to_featstruct(fuf):
    """
    Convert a FUF string into a FeatStruct. Note the string
    must be a s-expression.

    @param fuf: The fuf string
    @type fuf: string
    @return: C{nltk.featstruct.FeatStruct}
    """
    slp = SexpListParser()
    sexp = slp.parse(fuf)
    return _convert_fuf_featstruct(sexp)

def _convert_fuf_featstruct(sexp):
    assert sexp.lparen == '('
    fs = nltk.FeatStruct()
    for child in sexp:
        if isinstance(child, basestring):
            feat, val = _convert_fuf_feature(sexp)
            fs[feat] = val
            break
        else:
            feat, val = _convert_fuf_feature(child)
            fs[feat] = val
    return fs

def _convert_triple_eq(sexp):
    temp = SexpList('(', ')')
    temp.append('lex')
    temp.append(sexp.pop())
    sexp[1] = temp
    return sexp

def _convert_fuf_feature(sexp):
    assert sexp.lparen == '(', sexp
    feat, name, index, val = ('', '', '', '')

    # Special handling for the alt feature
    if sexp[0] == 'alt':
        feat, name, index, val = parse_alt(sexp)
    elif sexp[0] == 'opt':
        feat, name, index, val = parse_opt(sexp)
    elif len(sexp) == 3 and sexp[1] == '===':
        feat, val = _convert_triple_eq(sexp)

    elif len(sexp) == 3 and sexp[1] == '~':
        del sexp[1]
        result = _list_convert(sexp[1])
        sexp[1] = result
        print sexp[1]
        feat, val = sexp
    else:
        assert len(sexp) == 2, sexp[1]
        assert isinstance(sexp[0], basestring), sexp
        feat, val = sexp

    # Special handling for pattern feature
    if feat in ('pattern', 'cset'):
        assert isinstance(val, SexpList) and val.lparen == '('
        return feat, nltk.FeatureValueTuple(val)

    # Special handling of the alt feature
    if feat == 'alt':
        assert isinstance(val, SexpList) and val.lparen == '('
        choices = list()
        for c in val:
            if isinstance(c, basestring):
                choices.append(c)
            else:
                choices.append(_convert_fuf_featstruct(c))
            val = nltk.FeatStruct(dict([('%d' % (i+1), choice)
                                        for i, choice in enumerate(choices)]))
        # Process the alt with a name
        if len(name) > 0:
            return "%s_%s" % (feat, name), val

        # there is an index defined on this alt
        if isinstance(index, SexpList):
            ifs = _convert_fuf_featstruct(index)
            val["_index_"] = ifs[":index"]
        return feat, val

    if isinstance(val, SexpList):
        # If value is a feature structure, then recurse.
        if val.lparen == '(':
            return feat, _convert_fuf_featstruct(val)
        # If value is a pointer, then do something.
        if val.lparen == '{':
            # We'll resolve this later, using _resolve_fs_links():
            return feat, ReentranceLink(val)
        else:
            assert False, 'unexpected sexp type'
            
    # Otherwise, return the value as a string.
    return feat, val

 
def fuf_file_to_featstruct(fuf_filename):
    """
    Convert fuf file to C{nltk.FeatStruct} and processed the type definitions
    Returns the type table and the converted feature structure

    @param fuf_filename: The name of the file that contains the grammar
    @type fuf_filename: string
    @return: The type table (C{fstypes.FeatureTypeTable}) and the grammar
    as a C{nltk.featstruct.FeatStruct}.
    """

    # Convert the fuf code into expression lists
    sfp = SexpFileParser(fuf_filename)
    lsexp = sfp.parse()
    assert lsexp

    type_table = FeatureTypeTable()
    fs = nltk.FeatStruct()

    # process the type defs and the grammar
    for sexp in lsexp:
        if isinstance(sexp[0], basestring) and sexp[0] == 'define-feature-type':
            assert len(sexp) == 3
            name, children = sexp[1], sexp[2]
            type_table.define_type(name, children)
        else:
            # assuming that it is a feature structure
            fs = _convert_fuf_featstruct(sexp)
            # there should be nothing following the feature definition
            break
    return type_table, fs
            
        
def _list_convert(lst):
    result = SexpList('(', ')')
    res_copy = result
    for item in lst:
        result.append('car')
        result.append(SexpList('(', ')'))
        result = result[-1]
        if isinstance(item, SexpList):
            if '===' in item:
                item = _convert_triple_eq(item)
            result.append(_list_convert(item))
        else:
            result.append(item)
        result.append(SexpList('(', ')'))
        result = result[-1]
        result.append('cdr')
        result.append(SexpList('(', ')'))
        result = result[-1]
    return res_copy

######################################################################
# Test code:
######################################################################


if __name__ == '__main__':
    # the tests below are for conversion of fuf syntax to nltk.FeatStruct

    #test the alt feature

    print 'START LIST TEST'
    #listlines = open('tests/list.fuf').readlines()
    #for line in listlines:
        #print 'INPUTS:', line
        #print '<pre>'
        #print fuf_to_featstruct(line)
        #print '</pre>'
        #print


    
    # test the relative link feature
    #print "START LINK TEST"
    #linklines = open('tests/link.fuf').readlines()
    #for line in linklines:
        #print "INPUT:", line
        #print '<pre>'
        #print fuf_to_featstruct(line)
        #print '</pre>'
        #print

    # test the opt feature
    #print "START OPT TEST"
    #optlines = open('tests/opt.fuf').readlines()
    #for line in optlines:
        #print "INPUT:", line
        #print fuf_to_featstruct(line)
        #print


    # test the example grammars
    grammar_files = [gfile for gfile in os.listdir('tests/') if gfile.startswith('gr')]
    print grammar_files
    for gfile in grammar_files:
        print "FILE: %s" % gfile
        text = open('tests/%s' % gfile).read()
        print text
        print fuf_to_featstruct(text)
        print
        1/0

    
    type_table, grammar = fuf_file_to_featstruct('tests/typed_gr4.fuf')
    print type_table
    print grammar

    gr5 = fuf_to_featstruct(open('tests/gr5.fuf').read())
    print gr5

########NEW FILE########
__FILENAME__ = lexicon

IRREG_PLURALS = {
    "calf": "calves",
    "child": "children",
    "clothes" "clothes"
    "data": "data",
    "deer": "deer",
    "die": "dice",
    "elf": "elves" ,
    "glasses": "glasses",
    "goods": "goods",
    "goose": "geese",
    "half": "halves" ,
    "knife": "knives",
    "leaf": "leaves",
    "life": "lives",
    "loaf": "loaves",
    "man": "men",
    "many": "many" ,
    "mouse": "mice",
    "oats": "oats",
    "ox": "oxen",
    "pants": "pants",
    "person": "people",
    "pliers": "pliers",
    "scissors": "scissors",
    "self": "selves",
    "sheaf": "sheaves",
    "sheep": "sheep",
    "some": "some",
    "thanks": "thanks",
    "that": "that",
    "them": "them",
    "there": "there",
    "these": "these",
    "they": "they",
    "thief": "thieves",
    "those": "those",
    "tongs": "tongs",
    "trousers": "trousers",
    "trout": "trout" ,
    "which": "which",
    "wife": "wives",
    "woman": "women"}

IRREG_VERBS = {
    # "root", "present third person singular", "past", "present-participle", "past-participle"
    'become': {'present-third-person-singlular' : "becomes",
               'past' : "became",
               'present-participle' : "becoming",
               'past-participle' : "become"},
    "buy" : {'present-third-person-singular' : "buys",
             'past': "bought",
             'present-participle' : "buying",
             'past-participle' : "bought"},

    "come" : {'present-third-person-singular' : "comes",
            'past' : "came",
            'present-participle' : "coming",
            'past-participle' : "come"},
    "do" : {'present-third-person-singular' : "does",
          'past' : "did",
          'present-participle' : "doing",
          'past-participle' : "done"},
    "eat" : {'present-third-person-singular' : "eats",
           'past' : "ate",
           'present-participle' : "eating",
           'past-participle' : "eaten"},
    "fall" : {'present-third-person-singular' : "falls",
            'past' : "fell",
            'present-participle' : "falling",
            'past-participle' : "fallen"},
    "get" : {'present-third-person-singular' : "gets",
           'past' : "got",
           'present-participle' : "getting",
           'past-participle' : "gotten"},
    "give" : {'present-third-person-singular' : "gives",
            'past' : "gave",
            'present-participle' : "giving",
            'past-participle' : "given"},
    "go" : {'present-third-person-singular' : "goes",
          'past' : "went",
          'present-participle' : "going",
          'past-participle' : "gone"},
    "grow" : {'present-third-person-singular' : "grows",
            'past' : "grew",
            'present-participle' : "growing",
            'past-participle' : "grown"},
    "harbor" : {'present-third-person-singular' : "harbors",
              'past' : "harbored",
              'present-participle' : "harboring",
              'past-participle' : "harbored"},
    "have" : {'present-third-person-singular' : "has",
            'past' : "had",
            'present-participle' : "having",
            'past-participle' : "had"},
    "know" : {'present-third-person-singular' : "knows",
            'past' : "knew",
            'present-participle' : "knowing",
            'past-participle' : "known"},
    "leave" : {'present-third-person-singular' : "leaves",
             'past' : "left",
             'present-participle' : "leaving",
             'past-participle' : "left"},
    "make" : {'present-third-person-singular' : "makes",
            'past' : "made",
            'present-participle' : "making",
            'past-participle' : "made"},
    "see" : {'present-third-person-singular' : "sees",
           'past' : "saw",
           'present-participle' : "seeing",
           'past-participle' : "seen"},
    "take" : {'present-third-person-singular' : "takes",
            'past' : "took",
            'present-participle' : "taking",
            'past-participle' : "taken"},
    "throw" : {'present-third-person-singular' : "throws",
             'past' : "threw",
             'present-participle' : "throwing",
             'past-participle' : "thrown"},
    "undertake" : {'present-third-person-singular' : "undertakes",
                 'past' : "undertook",
                 'present-participle' : "undertaking",
                 'past-participle' : "undertaken"},
    "write" : {'present-third-person-singular' : "writes",
             'past' : "wrote",
             'present-participle' : "writing",
             'past-participle' : "written"}}


PRONOUNS_PERSON = {'third':'he', 
                   'first':"I", 
                   'second':'you'}

PRONOUNS_GENDER = {
    # key(masculine) feminine neuter
    'masculine':'he',
    'feminine':'she',
    'neuter':'it'}

PROUNOUNS_NUMBER = {
    "I": "we",
    'he': 'they',
    'she': 'they',
    'it': 'they'}

PRONOUNS_CASE = {
    #key(subjective) objective, possessive, reflexive
    "I": {'objective':"me", 
          'possessive': "mine", 
          'reflexive': "myself"},
    "you": {'objective':"you",
            'possessive': "yours",
            'reflexive': "yourself"},
    "he": {'objective':"him",
           'possessive': "his",
           'reflexive': "himself"},
    "she": {'objective':"her",
            'possessive': "hers"
            , 'reflexive': "herself"},
    "it": {'objective':"it", 
           'possessive': "its"
           , 'reflexive': "itself"},
    "we": {'objective':"us" , 
           'possessive': "ours", 
           'reflexive': "ourselves"},
    "they": {'objective':"them",
             'possessive': "theirs",
             'reflexive': "themselves" } } 

IRREG_SUPER_COMPARATIVES = {
    "good" : {'comparative': "better", 'seperlative':"best"},
    "bad" : {'comparative': "worse", 'seperlative':"worst"},
    "far" : {'comparative': "further", 'seperlative':"furthest"},
    "many" : {'comparative': "more", 'seperlative':"most"},
    "little" : {'comparative': "less", 'seperlative':"least"} }

########NEW FILE########
__FILENAME__ = linearizer
"""
The linearizer for unified fuf feature structures

"""

import nltk
from link import *
from util import output_html

def linearize(fstruct):
    """
    Perfom the linearilization of the input feature structure.

    @param fstruct: feature structure
    @type fstruct: C{nltk.featstruct.FeatStruct}
    @return: string
    """
    def lin_helper(fs, pattern, output):
        for item in pattern:
            if item == 'dots':
                output.append('')
            elif not (item in fs):
                # the key in the pattern is not in the feature
                pass
            else:
                if isinstance(fs[item], ReentranceLink):
                    LinkResolver().resolve(fs)
                if fs[item].has_key('pattern'):
                    lin_helper(fs[item], fs[item]['pattern'], output)
                elif fs[item].has_key('lex'):
                    output.append(fs[item]['lex'])

    assert isinstance(fstruct, nltk.FeatStruct)
    output = []
    assert fstruct
    lin_helper(fstruct, fstruct['pattern'], output)
    return output

if __name__ == '__main__':
    from fufconvert import *
    from fuf import *
   
    gfs = fuf_to_featstruct(open('tests/gr0.fuf').read())
 
    itext = open('tests/ir0.fuf').readlines()[3]
    ifs = fuf_to_featstruct(itext)
    result = unify_with_grammar(ifs, gfs) 
    print result
    print linearize(result)



########NEW FILE########
__FILENAME__ = link
"""Module for resolving relative and absolute paths in FUF"""
import nltk

class LinkResolver(object):
    """
    Object for resolving links
    Note: links should not be resolved if 'alt' is present 
    in the feature structure.
    """

    def __init__(self):
        """
        Initialize and return the object.
        """
        self.unique_var_counter = 0

    
    def _unique_var(self):
        """
        Ensures that unique variable names are used when resolving links
        """

        self.unique_var_counter += 1
        return nltk.Variable("?x%s" % self.unique_var_counter)
    
    def _get_value(self, fs, path):
        """
        Find and return the value within the feature structure
        given a path.

        @param fs: Feature structre
        @type fs: C{nltk.featstruct.FeatStruct}
        @param path: list of keys to follow
        @type path: list
        @return: the feature value at the end of the path
        """
        target = None
        
        # in case we find another link keep a copy
        ancestors = [fs]

        # to to the end
        last_step = path[-1]
        path = path[:-1]

        for step in path:
            if step in fs and not isinstance(fs[step], ReentranceLink):
                fs = fs[step]
                ancestors.append(fs)
            elif step not in fs:
                fs[step] = nltk.FeatStruct()
                fs = fs[step]
                ancestors.append(fs)
            elif isinstance(fs[step], ReentranceLink):
                parent = ancestors[-1*fs[step].up]
                new_path = fs[step].down
                fs[step] = self._get_value(parent, new_path)
                fs = fs[step]
                
        if isinstance(fs, nltk.sem.Variable):
            return fs

        if last_step in fs:
            assert (not isinstance(fs[last_step], ReentranceLink))
            return fs[last_step]

        # All the way through the path but the value doesn't exist
        # create a variable
        fs[last_step] = self._unique_var()
        return fs[last_step]

    def resolve(self, fstruct):
        """
        Resolve the relative and absolute links in the feature structure
        
        @param fstruct: Feature structure that may contain relative and absolute
        links
        @type fstruct: C{nltk.featstruct.FeatStruct}
        """

        def resolve_helper(fs, ancestors):
            # start looking for links
            for feat, val in fs.items():
                # add to path and recurse
                if isinstance(val, nltk.FeatStruct):
                    ancestors.append(val)
                    resolve_helper(val, ancestors)
                    ancestors.pop()
                # found the link
                elif isinstance(val, ReentranceLink):
                    if val.up > 0 and len(val.down) > 0:
                        # relative link with a path
                        if (val.up >= len(ancestors)):
                            parent = ancestors[-1*val.up]
                            path = val.down
                            fs[feat] = self._get_value(parent, path)
                        else:
                            # we will try to resolve this link later
                            pass
                    elif val.up == 0 and len(val.down) > 0:
                        # get the value for the absolute link
                        parent = ancestors[0]
                        path = val.down
                        fs[feat] = self._get_value(parent, path)
                    else:
                        raise ValueError("Malformed Link: %s" % val)

        resolve_helper(fstruct, [fstruct])


class ReentranceLink(object):
    """
    Used to store fuf's reentrance links; these are resolved
    after the parsing and alt structure generation.

    First go up C{self.up} levels then follow the 
    featrue path C{self.down}
    """

    def __init__(self, path):
        """
        Initialize and return the object

        @param path: the path to the value of the link
        @type path: list
        """
        self.up = 0
        self.down = []

        for feat in path:
            if feat == "^":
                self.up +=1
                assert self.down == []
            else:
                self.down.append(feat)
        self.down = tuple(self.down)

    def __repr__(self):
        """
        Return a string representation of this link
        """
        return "{%s%s}" % ("^"* self.up, ' '.join( self.down))

if __name__ == '__main__':
    # testing the link resolution using gr0.fuf grammar and ir0.fuf inputs
    import os
    from fufconvert import *
    from fuf import *

    gfs = fuf_to_featstruct(open('tests/gr0.fuf').read())
    itext = open('tests/ir0.fuf').readlines()[2]
    
    ifs = fuf_to_featstruct(itext)
    result = unify_with_grammar(ifs, gfs)

    print output_html([ifs, gfs, result])

########NEW FILE########
__FILENAME__ = morphology
"""
Two functions are not yet implemented
    - morph_fraction: numeric fraction to text
    - morph_numeric: integer number to text
"""

import lexicon

def _is_vowel(char):
    return char in ['o', 'e', 'i', 'a', 'y']


def pluralize(word):
    """
    Handles word sring ending with 'ch', 'sh', 'o',
    's', 'x', 'y', 'z'

    >>> print pluralize('copy')
    copies
    >>> print pluralize('cat')
    cats
    >>> print pluralize('language')
    languages
    """

    assert word
    assert isinstance(word, basestring)
    assert len(word) > 0

    second_last = word[-2]
    last = word[-1]
    if last in ['s', 'z','x']:
        return word + "es"
    elif last == 'h':
        if second_last in ['s', 'c']:
            return word + "es"
        else:
            return word + 's'
    elif last == 'o':
        if not _is_vowel(second_last):
            return word + "es"
        else: 
            return word + 's'
    elif last == 'y':
        if not _is_vowel(second_last):
            return word[:-1] + "ies"
        else:
            return word + 's'
    else:
        return word + 's'

            
def morph_fraction(lex, num, den, digit):
    """
    Return the string representation of a fraction
    """
    raise NotImplementedError

def morph_numeric(lex, ord_or_card, value, digit):
    """
    Convert a number into text form
    """
    raise NotImplementedError

def form_ing(word):
    """
    Adding 'ing to the word dropping the final 'e if any,
    handlies duplication of final consonat and special cases.
    """

    # last char of the word
    last = word[-1]

    if last == 'e':
        return word[:-1] + 'ing'
    elif last == 'r':
        if word[-2] == 'a': 
            return word + "ring"
    elif last in ['b', 'd', 'g', 'm', 'n', 'p', 't']:
        if _is_vowel(word[-2]) and not (_is_vowel(word[-3])):
            return word + word[-1] + "ing"

    return word + "ing" 
        
def form_past(word):
    """
    Form past tense of the word by adding 'ed to it.
    Handles duplication of final consonant and special values
    """

    last = word[-1]
    assert word
    assert isinstance(word, basestring)

    if last == 'e':
        return word + 'd'
    elif last == 'y':
        if _is_vowel(word[-2]):
            return word + "ed"
        else: 
            return word[:-1] + "ied"
    elif last == 'r':
        if word[-2] == 'a':
            return word + word[-1] + 'ed'
        else:
            return word + 'ed'
    elif last in ['b', 'd', 'g', 'm', 'n', 't', 'p']:
        if _is_vowel(word[-2]) and not _is_vowel(word[-3]):
            return word + word[-1] + 'ed'
    return word + 'ed'

def _is_first_person(person):
    return person in ['I', 'i', 'We', 'we'] or person == 'first'

def _is_second_person(person):
    return person in ['You', 'you'] or person == 'second'

def _is_third_person(person):
    return person in ['He', 'She', 'he', 'she', 'they', "They"] or person == 'third'

def _is_singular(number):
    return number == 'one' or number == 'sing' or number == 'singular'

def _is_dual(number):
    return number == 'two' or number == 'dual'

def _is_plural(number):
    return not (_is_singular(number) or _is_dual(number)) or number == 'plural'

def form_present_verb(word, number, person):
    """
    Forms the suffix for the present tense of the verb WORD
    """
    assert word
    assert isinstance(word, basestring)
    if _is_first_person(person) or _is_second_person(person):
        return word
    elif _is_third_person(person):
        if _is_singular(number):
            return pluralize(word)
        if _is_dual(number) or _is_plural(number): 
            return word
    return None
    
def morph_be(number, person, tense):
    if tense == 'present':
        if _is_singular(number):
            if _is_first_person(person):
                return 'am'
            elif _is_second_person(person):
                return 'are'
            elif _is_third_person(person):
                return 'is'
            else:
                return 'are'
    elif tense == 'past':
        if _is_singular(number):
            if _is_first_person(person):
                return 'was'
            elif _is_second_person(person):
                return 'were'
            elif _is_third_person(person):
                return 'was'
            else:
                return 'were'
    elif tense == 'present-participle':
        return 'being'
    elif tense == 'past-participle':
        return 'been'


def morph_verb(word, ending, number, person, tense):
    """
    Adds the proper suffix to the verb root taking into 
    account ending, number, person, and tense.
    """

    if ending == 'root':
        return word
    if ending == 'infinitive': 
        return "to %s" % word
    if word == 'be':
        # what 'internal' does no one knows... 
        if ending == "internal":
            return morph_be(number, person, ending)
        else:
            return morph_be(number, person, tense)
    if ending == 'present-participle':
        # if the verb is irregular 
        # it should be in the lexicon fetch it and return it
        # otherwise it is not irregular, so add 'ing' to it.
        if word in lexicon.IRREG_VERBS:
            return lexicon.IRREG_VERBS[word]['present-participle']
        else:
            return form_ing(word)
    if ending == 'past-participle':
        if word in lexicon.IRREG_VERBS:
            return lexicon.IRREG_VERBS[word]['past-participle']
        else:
            return form_past(word)
    if tense == 'present' and person == 'third' and number == 'singular' and word in lexicon.IRREG_VERBS:
        return lexicon.IRREG_VERBS[word]['present-third-person-singular']
    if tense == 'present':
        return form_present_verb(word, number, person)
    if tense == 'past':
        return form_past(word)
    return None

def form_adj(word, ending):
    """
    changes in spelling:
    1. final base consonants are doubled when preceding vowel is stressed and
    spelled with a single letter: big-bigger-biggest; sad-sadder-saddest.
    2. bases ending in a consonant+y, final y changed to i
    angry-angrier-angriest.
    3. base ends in a mute -e, dropped before inflectional suffix:
        pure-purer-purest
        free-freer-freest
            """
    last = word[-1]
    if last == 'e':
        return word[:-1] + ending
    if last in ['b', 'd', 'g', 'm', 'n', 'p', 't'] and _is_vowel(word[-2]) and not _is_vowel(word[-3]):
        return word + last + ending

    if last == 'y' and not _is_vowel(word[-2]):
        return word[:-1] + 'i' + ending
    return word + ending


def morph_adj(word, superlative, comparative, inflection):
    ending = None
    if superlative == 'no':
        if not comparative == 'no':
            ending = None
        else:
            ending = 'comparative'
    else:
        ending = 'superlative'
    
    if inflection == 'no' or not ending: 
        return word
    else:
        if word in lexicon.IRREG_SUPER_COMPARATIVES:
            return lexicon.IRREG_SUPER_COMPARATIVES[word][ending]
        elif ending == 'superlative':
            return form_adj(word, 'est')
        else:
            return form_adj(word, 'er')

def morph_pronoun(lex, pronoun_type, case, gender, number, distance, animate,
                  person, restrictive):
    """
    Returns the correct pronoun given the features
    """
    if lex and isinstance(lex, basestring) and not (lex in ['none', 'nil']):
        return lex
    if pronoun_type == 'personal':
        # start with the 'he' then augmen by person, then, by number, 
        # gender and finally by case
        # this is a port of the hack in the morphology.scm code
        if (not animate) or (gender == 'feminine'):
            anime = 'yes'
        if animate == 'no':
            gender = 'neuter'
        
        lex = 'he'
        if person and not (person == 'third'):
            lex = lexicon.PRONOUNS_PERSON[person]
        if number and not (number == 'masculine'):
            lex = lexicon.PROUNOUNS_NUMBER[number]
        if gender and not (gender == 'masculine'):
            lex = lexicon.PRONOUNS_GENDER[gender]
        if case and not (case == 'subjective'):
            lex = lexicon.PRONOUNS_CASE[case]
    else:
        return lex

    if pronoun-type and pronoun-type == 'demonstrative':
        if not (number in ['first', 'second']):
            if distance == 'far':
                return 'those'
            else:
                return 'these'
        elif distance == 'far':
            return 'that'
    else: 
        return 'this'

    if pronoun-type == 'relative':
        if not animate:
            animate = 'no'
        if animate == 'no':
            gender = 'neuter'
        if restrictive == 'yes':
            return 'that'
        elif case == 'possessive':
            return 'whose'
        elif case == 'objective':
            if gender == 'neuter':
                return 'which'
            else:
                return 'whom'
        elif gender == 'neuter':
            return 'which'
        elif animate == 'no':
            return 'which'
        else:
            return 'who'
    
    if pronoun-type == 'question':
        # this is the conversion of another hack in FUF morphology
        # start at line 76 in morphology.scm
        if not animate:
            animate == 'no'
        if animate == 'no':
            gender == 'neuter'
        if restrictive == 'yes':
            return 'which'
        elif case == 'possessive':
            return 'whose'
        elif case == 'objective':
            if gender == 'neuter':
                return 'what'
            else:
                return 'whom'
        elif gender == 'neuter':
            return 'what'
        elif animate == 'no':
            return 'what'
        else:
            return 'who'
    
    if pronoun-type == 'quantified':
        return lex


def morph_number(word, number):
    """
    Adds the plural suffix to work if number is 
    plural. Note: the default is singular
    """

    if (not number) or (number == ''): 
        return word
    elif not word:
        return word
    elif number not in ['first', 'second'] or number == 'plural':
        if word in lexicon.IRREG_PLURALS:
            return lexicon.IRREG_PLURALS[word]
        else:
            pluralize(word)
    else:
        return word

# markers, not sure what they are used for
# the original code is not very well documented.

PLURAL_MARKER = "****"
INDEFINITE_ARTICLE = 'a**'
A_AN_MARKER = '***'

def mark_noun_as_plural(word):
    return PLURAL_MARKER +  word

def unmark_noun_as_plural(word):
    return word[PLURAL_MARKER:]

def is_noun_marked_as_plural(word):
    return  (word[:len(PLURAL_MARKER)] == PLURAL_MARKER)

def mark_noun_as_an(word):
    return A_AN_MARKER + word

def unmark_noun_as_an(word):
    return word[A_AN_MAKER:]

def is_noun_maked_as_an(word):
    return (word[:len(A_AN_MARKER)] == A_AN_MARKER)

def is_final_punctuation(letter):
    return (letter in ['.', ';', '!', ':', '?'])

def morph_noun(word, number, a_an, feature):
    """If feature is possessive, then return the apostrephised form of the noun
    appropriate to the number.
    If a_an is 'an mark noun with mark-noun-as-an to agree with determiner.
    If noun is plural and ends with 's', mark noun as plural to let it agree
    with possessive mark that may follow it (single apostrophe or 's).
    Return word with number suffix."""
    word = morph_number(word, number)
    if not (number in ['first', 'second']) and word[-1] == 's':
        return mark_noun_as_plural(word)
    else:
        return word

    if a_an == 'an':
        return mark_noun_as_an(word)
    else:
        return word

########NEW FILE########
__FILENAME__ = sexp
"""
Module for tokenizing and parsing s-expressions
"""
import nltk
import re
import os


from statemachine import PushDownMachine

class SexpList(list):
    """
    A list, extracted from an s-expressoin string. The open and 
    closin gparentheses used by the list are strong in L{lparen} and 
    L{rparen}, respectively. Assuming this C{SexpList} was generated by 
    parsing a string, its istem will all be either strings or nested 
    C{SexpList}s.
    """
    
    def __init__(self, lparen, rparen, values=()):
        """
        Initialize and return the object.

        @param lparen: The style of left parenthesis to use for this list
        @type lparen: string
        @param rparen: The style of right parenthesis to use for this list
        @type rparen: string
        @param values: The values the the list should contain
        @type values: Tuple
        """

        # left (open) parenthesis
        self.lparen = lparen
        # right (close) parenthesis
        self.rparen = rparen

    def pp(self):
        s = self.lparen
        for i, val in enumerate(self):
            if isinstance(val, SexpList):
                s += val.pp()
            elif isinstance(val, basestring):
                s += val
            else:
                s += repr(val)
            if i < len(self)-1:
                s += ' '
        return s + self.rparen

    def __repr__(self):
        """
        Returns the string representation of this list

        @return: The string representation of the list
        """
        return '<SexpList: %s>' % self.pp()


class SexpListParser(object):
    """
    Parse the text and return a a C{SexpList}
    """

    def __init__(self):
        """
        Create and return the object
        """

        self.machine = PushDownMachine()
        self.tokenizer = None

        # set up the parenthesis
        self.parens = {'(':')', '[':']', '{':'}'}
        self.lparens = self.parens.keys()
        self.rparens = self.parens.values()
        self._build_machine()
        self.machine.stack = [[]]
    
    def _build_machine(self):
        """
        Build the state machine with the defined states
        """
        self.machine.addstate(self._lparen)
        self.machine.addstate(self._rparen)
        self.machine.addstate(self._word)
        self.machine.addstate(self._end, end_state=True)
        self.machine.setstart(self._lparen)

    def _tokenizer(self, to_tokenize):
        """
        Return a tokenizer
        """
        lparen_res = ''.join([re.escape(lparen) for lparen in self.parens.keys()])
        rparen_res = ''.join([re.escape(rparen) for rparen in self.parens.values()])

        tok_re = re.compile('[%s]|[%s]|[^%s%s\s]+' %
                            (lparen_res, rparen_res, lparen_res, rparen_res))
    
        return tok_re.finditer(to_tokenize)

    def parse(self, to_parse):
        """
        Parse the text and return the C{SexpList}

        @param to_parse: The string to parse
        @type to_parse: string
        @return: list of lists 
        """

        self.tokenizer = self._tokenizer(SexpListParser.remove_comments(to_parse))
        start = self.tokenizer.next().group()
        if (start not in self.lparens):
            raise ValueError("Expression must start with an open paren")
        stack_w_list = self.machine.run(tokens=start)
        return stack_w_list[0][0]
    

    def _transition(self):
        """
        Move to the next state based on the following token
        """
        try:
            tok = self.tokenizer.next().group()
            # collect strings
            if tok.startswith('"') and tok.endswith('"'):
                tok = tok[1:-1]
            elif tok.startswith('"') and not tok.endswith('"'):
                while True:
                    tok = "%s %s" % (tok, self.tokenizer.next().group())
                    if tok.endswith('"'):
                        tok = tok.replace('"', '')
                        break
        except Exception:
            return self._end, ""
        
        if tok in self.lparens:
            return self._lparen, tok
        elif tok in self.rparens:
            return self._rparen, tok
        return self._word, tok

    def _lparen(self, current):
        """
        Actions to take given the left parenthesis
        """
        self.machine.push(SexpList(current, self.parens[current]))
        return self._transition()
        

    def _rparen(self, current):
        """
        Actions to take given the right parenthesis
        """
        if len(self.machine.stack) == 1:
            raise ValueError("Unexpected close paren")
        if current != self.machine.stack[-1].rparen:
            raise ValueError("Mismatched paren")
        # close the and make it an item of the previous list
        closed_sexp_list = self.machine.stack.pop()
        # this makes sure that we dont add any of the tracing stuff 
        if not any(i in closed_sexp_list for i in ("control-demo", "control", ":demo", "trace")):
            self.machine.stack[-1].append(closed_sexp_list)
        return self._transition()

    def _word(self, current):
        """
        Actions to take given that the current token is a word
        """
        # add the word to the last list
        if len(self.machine.stack) == 0:
            raise ValueError("Expected open paren")
        if not ('%' in current):
            self.machine.stack[-1].append(current)
        return self._transition()

    def _end(self, token):
        """
        Actions to take when the string is finished processing
        """
        if len(self.machine.stack) > 1:
            raise ValueError("Expected close paren")
        assert len(self.machine.stack) == 1
        if len(self.machine.stack[0]) == 0:
            raise ValueError("Excepted open paren")
        if len(self.machine.stack[0]) > 1:
            raise ValueError("Expected a single sexp list")

        return self.machine.stack[0][0]

    @staticmethod
    def _error(s, match):
        pos = match.start()
        for lineno, line in enumerate(s.splitlines(True)):
            if pos < len(line): break
            pos -= len(line)
        return 'line %s, char %s' % (lineno+1, pos)

    @staticmethod
    def remove_comments(text):
        """
        Remove the comments from a given expression
        """
        remover = re.compile("\;.*")
        result = []
        for line in text.splitlines(True):
            temp = remover.sub('', line)
            if not temp.isspace():
                result.append(temp)
        return "".join(result)
    
class SexpFileParser(object):
    """
    Parse a file that contains multiple s-expressions
    """

    def __init__(self, filename):
        """
        Construct and return the object
        """
        self.sfile = filename

        # The SexpListParser parses one expression at a time
        # We wrap the whole file into single expression
        self.source = "(\n%s\n)" % open(self.sfile).read()
        
    def parse(self):
        """
        Parse the file and retun a list of s-expressions

        @return: list of lists
        """

        slp = SexpListParser()
        return slp.parse(self.source)


if __name__ == "__main__":
    # testing SexpListParser
    lines = open('tests/sexp.txt').readlines()
    for test in lines:
        try:
            print '%s' % test
            l = SexpListParser().parse(test)
            print '==>', SexpListParser().parse(test)
            print
        except Exception, e:
            print 'Exception:', e
    
    # testing the SexpFileParser
    sfp = SexpFileParser('tests/typed_gr4.fuf')
    print sfp.parse()



    
    

########NEW FILE########
__FILENAME__ = specialfs
"""
Handling for special feature names during parsing
"""

from sexp import *

def parse_alt(sexpl):
    """
    Parse the I{alt} feature definition. 

    @param sexpl: An s-expression list that represents an I{alt} struture
    @type sexpl: C{sexp.SexpList} 
    @return: A tuple composed of ('alt', 'optional alt name', 'optional alt
    index', 'alt values')
    """

    feat, name, index, val = ('', '', '', '')

    # named alt
    if isinstance(sexpl[1], basestring):
        # alt with index
        if len(sexpl) == 4:
            feat, name, index, val = sexpl
        # alt without index
        if len(sexpl) == 3:
            feat, name, val = sexpl
    # alt without name
    elif isinstance(sexpl[1], SexpList):
        if len(sexpl) == 3:
            feat, index, val = sexpl
        if len(sexpl) == 2:
            feat, val = sexpl

    if all(i == "" for i in (feat, name, index, val)):
        return ValueError("Maformed alt: %s" % sexpl)
    return (feat, name, index, val)

def parse_opt(sexpl):
    """
    Parse the I{opt} structure

    @param sexpl: An s-expression list that represents an I{opt} structure
    @type sexpl: C{sexp.SexpList}
    @return: A tuple composed of ('opt', 'optional opt name', 'opt index', 'opt
    value')
    """
    feat, name, index, val = ('','','','')
    sexpl[0] = "alt"
    feat, name, index, val = parse_alt(sexpl)
    val.append(SexpList("(", ")"))
    return feat, name, index, val

########NEW FILE########
__FILENAME__ = statemachine
"""
Basic state machine class
"""
class StateMachine(object):
    """
    A basic but flexible state machine
    """
    def __init__(self):
        """
        Initialize and return the object
        """
        self.nodes = list()
        self.start_state = None
        self.end_states = list()

    def addstate(self, node, end_state=False):
        """
        Add a function that represents a state within the machine
        """
        self.nodes.append(node)
        if end_state:
            self.end_states.append(node)

    def setstart(self, node): 
        """
        Mark a given state as a starting state
        """
        self.start_state = node

    def run(self, tokens=None):
        """
        Invoke the machine

        @param tokens: list of tokens
        @type tokens: list
        """
        if self.start_state is None:
            raise RuntimeError("No start state defined")
        if not self.end_states:
            raise RuntimeError("No end states defined")
        node = self.start_state
        while True:
            tup = node(tokens)
            (new_node, tokens) = tup
            #(new_node, tokens) = node(tokens)
            if new_node in self.end_states:
                new_node(tokens)
                break 
            elif new_node not in self.nodes:
                raise RuntimeErrror, "Invalid target %s", new_state
            else:
                node = new_node

class PushDownMachine(StateMachine):
    """
    State machine that uses and additional stack.
    """

    def __init__(self):
        """
        Initialize and return the object
        """
        StateMachine.__init__(self)
        self.stack = []

    def push(self, token):
        """
        Push a value onto the stack

        @param token: A token to be pushed onto the stack
        @type token: object
        """

        self.stack.append(token)

    def pop(self):
        """
        Remove an object from the top of the stack.

        @return: The token on the top of the stack
        """
        self.stack.pop()

    def run(self, tokens=None):
        """
        Process all the tokens with the machine

        @param tokens: The list of tokens
        @type tokens: list

        @return: The stack after the machine had finished processing the tokens
        """
        super(PushDownMachine, self).run(tokens)
        return self.stack

########NEW FILE########
__FILENAME__ = util
"""
This module contains a couple of functions for better viewing of the 
FUF convesion/unification results
"""

def output_html(lst, header=[], style=""):
    """
    Output the I{lst} as an html table with I{header} items
    and the applied I{style}

    @param lst: The list of items to be shown
    @type lst: list
    @param header: The table header
    @type header: list
    @param style: The style to apply to the table
    @type style: string
    """
    s = " <table border=1 style='%s'> <tr> " % style
    for item in header:
        s += "<td><b>%s</b></td>" % item
    s += "</tr>"
    

    for item in lst:
        s += "<td><pre>%s</pre></td>" % item
    s += "</tr></table>"
    return s

def draw(fstruct, filename=None):
    """
    Draw graph representation of the feature structure using graphviz syntax

    @param fstruct: A feature structure
    @type fstruct: C{nltk.featstruct.FeatStruct}
    @param filename: The filename to output the graphviz code to
    @type filename: string
    """
    def draw_helper(output, fstruct, pcount, ccount):
        output += 'fs%d [label=" " style="filled" fillcolor="white"];\n' % (pcount)
        for fs, val in fstruct.items():
            if isinstance(val, nltk.FeatStruct):
                output +=  'fs%d -> fs%d [label="%s"];\n' % (pcount, ccount, fs)
                output, ccount = draw_helper(output, val, ccount,
                                                     ccount+1)
            else:
                output +=  'fs%d -> fs%d [label="%s"]; fs%d [label="%s" \
                style=filled fillcolor=grey];\n' % (pcount, ccount, fs,
                                                            ccount, val)
            ccount +=1 
        return output, ccount

    output, ccount = draw_helper("", fstruct, 0, 1)
    return "digraph fs {\n nodesep=1.0;\n" + output + "\n}";

def flatten(lst):
    """
    Flatten a list that contains nested lists
    
    @param lst: The source list
    @type lst: list
    @return: flat list
    """
    def flatten_helper(current, flat):
        # flatten a list of nested lists
        for item in current:
            if isinstance(item, list):
                flat = flatten_helper(item, flat)
            else:
                flat.append(item)
        return flat 

    assert isinstance(lst, list)
    return flatten_helper(lst, [])

########NEW FILE########
__FILENAME__ = EM_mapper
from nltk import FreqDist, ConditionalFreqDist, ConditionalProbDist, \
    DictionaryProbDist, DictionaryConditionalProbDist, LidstoneProbDist, \
    MutableProbDist, MLEProbDist, UniformProbDist, HiddenMarkovModelTagger

from nltk.tag.hmm import _log_add
from hadooplib.mapper import MapperBase
from hadooplib.util import *

from numpy import *

# _NINF = float('-inf')  # won't work on Windows
_NINF = float('-1e300')

_TEXT = 0  # index of text in a tuple
_TAG = 1   # index of tag in a tuple

class EM_Mapper(MapperBase):
    """
    compute the local hmm parameters from one the input sequences
    """

    def pd(self, values, samples):
        """
        helper methods to get DictionaryProbDist from 
        a list of values
        """
        d = {}
        for value, item in zip(values, samples):
            d[item] = value
        return DictionaryProbDist(d)
    
    def cpd(self, array, conditions, samples):
        """
        helper methods to get DictionaryConditionalProbDist from 
        a two dimension array
        """
        d = {}
        for values, condition in zip(array, conditions):
            d[condition] = self.pd(values, samples)
        return DictionaryConditionalProbDist(d)

    def read_params(self):
        """
        read parameter file, initialize the hmm model
        """
        params = open("hmm_parameter", 'r')
        d = {}
        A = {}
        B = {}
        for line in params:
            words = line.strip().split()
            # ignore blank lines and comment lines
            if len(words) == 0 or line.strip()[0] == "#":
                continue
            if words[0] == "Pi":
                d[words[1]] = float(words[2])
            elif words[0] == "A":
                A[(words[1], words[2])] = float(words[3]) 
            elif words[0] == "B":
                B[(words[1], words[2])] = float(words[3]) 
        params.close()

        # get initial state probability p (state)
        Pi = DictionaryProbDist(d)

        A_keys = A.keys()
        B_keys = B.keys()
        states = set()
        symbols = set()
        for e in A_keys:
            states.add(e[0])
            states.add(e[1])
        for e in B_keys:
            states.add(e[0])
            symbols.add(e[1])

        states = list(states)
        states.sort()
        symbols = list(symbols)
        symbols.sort()

        # get transition probability p(state | state)
        prob_matrix = []
        for condition in states:
            li = []
            for state in states:
                li.append(A.get((condition, state), 0))
            prob_matrix.append(li)
        A = self.cpd(array(prob_matrix, float64), states, states)

        # get emit probability p(symbol | state)
        prob_matrix = []
        for state in states:
            li = []
            for symbol in symbols:
                li.append(B.get((state, symbol), 0))
            prob_matrix.append(li)
        B = self.cpd(array(prob_matrix, float64), states, symbols)
        
        return symbols, states, A, B, Pi

    def map(self, key, value):
        """
        establish the hmm model and estimate the local
        hmm parameters from the input sequences

        @param key: None
        @param value: input sequence
        """

        symbols, states, A, B, pi = self.read_params()
        N = len(states)
        M = len(symbols)
        symbol_dict = dict((symbols[i], i) for i in range(M))

        model = HiddenMarkovModelTagger(symbols=symbols, states=states, \
                transitions=A, outputs=B, priors=pi)

        logprob = 0
        sequence = list(value)
        if not sequence:
            return

        # compute forward and backward probabilities
        alpha = model._forward_probability(sequence)
        beta = model._backward_probability(sequence)

        # find the log probability of the sequence
        T = len(sequence)
        lpk = _log_add(*alpha[T-1, :])
        logprob += lpk

        # now update A and B (transition and output probabilities)
        # using the alpha and beta values. Please refer to Rabiner's
        # paper for details, it's too hard to explain in comments
        local_A_numer = ones((N, N), float64) * _NINF
        local_B_numer = ones((N, M), float64) * _NINF
        local_A_denom = ones(N, float64) * _NINF
        local_B_denom = ones(N, float64) * _NINF

        # for each position, accumulate sums for A and B
        for t in range(T):
            x = sequence[t][_TEXT] #not found? FIXME
            if t < T - 1:
                xnext = sequence[t+1][_TEXT] #not found? FIXME
            xi = symbol_dict[x]
            for i in range(N):
                si = states[i]
                if t < T - 1:
                    for j in range(N):
                        sj = states[j]
                        local_A_numer[i, j] =  \
                            _log_add(local_A_numer[i, j],
                                    alpha[t, i] + 
                                    model._transitions[si].logprob(sj) + 
                                    model._outputs[sj].logprob(xnext) +
                                    beta[t+1, j])
                    local_A_denom[i] = _log_add(local_A_denom[i],
                                alpha[t, i] + beta[t, i])
                else:
                    local_B_denom[i] = _log_add(local_A_denom[i],
                            alpha[t, i] + beta[t, i])

                local_B_numer[i, xi] = _log_add(local_B_numer[i, xi],
                        alpha[t, i] + beta[t, i])

        for i in range(N):
            self.outputcollector.collect("parameters", \
                    tuple2str(("Pi", states[i], pi.prob(states[i]))))

        self.collect_matrix('A', local_A_numer, lpk, N, N)
        self.collect_matrix('B', local_B_numer, lpk, N, M)
        self.collect_matrix('A_denom', [local_A_denom], lpk, 1, N)
        self.collect_matrix('B_denom', [local_B_denom], lpk, 1, N)

        self.outputcollector.collect("parameters", "states " + \
                tuple2str(tuple(states)))
        self.outputcollector.collect("parameters", "symbols " + \
                tuple2str(tuple(symbols)))


    def collect_matrix(self, name, matrix, lpk, row, col):
        """
        a utility function to collect the content in matrix
        """
        for i in range(row):
            for j in range(col):
                self.outputcollector.collect("parameters", \
                        tuple2str((name, i, j, matrix[i][j], lpk, row, col)))
                        



if __name__ == "__main__":
    EM_Mapper().call_map()

########NEW FILE########
__FILENAME__ = EM_reducer
from hadooplib.reducer import ReducerBase
from hadooplib.util import *
from nltk.tag.hmm import _log_add, _NINF
from numpy import *
import sys


class EM_Reducer(ReducerBase):
    """
    combine local hmm parameters to estimate a global parameter
    """

    def reduce(self, key, values):
        """
        combine local hmm parameters to estimate a global parameter

        @param key: 'parameters' const string, not used in program
        @param values: various parameter quantity
        """
        A_numer = B_numer = A_denom = B_denom = None
        N = M = 0
        logprob = 0

        states  = []
        symbols = []
        pi = {}
        pi_printed = False

        for value in values:
            # identifier identify different parameter type
            identifier = value.split()[0]
            if identifier == "states":
                if not states:
                    states = value.split()[1:]
            elif identifier == "symbols":
                if not symbols:
                    symbols = value.split()[1:]
            elif identifier == "Pi":
                state, prob = value.split()[1:]
                pi[state] = float(prob)
            else:
                # extract quantities from value
                name, i, j, value, lpk, row, col = str2tuple(value)
                row = int (row)
                col = int (col)
                i = int(i)
                j = int(j)
                value = float(value)
                lpk = float(lpk)
                logprob += lpk

                # add these sums to the global A and B values
                if name == "A":
                    if A_numer is None:
                        A_numer = ones((row, col), float64) * _NINF
                        N = row
                    A_numer[i, j] = _log_add(A_numer[i, j],
                            value - lpk)
                elif name == "B":
                    if B_numer is None:
                        B_numer = ones((row, col), float64) * _NINF
                        M = col
                    B_numer[i, j] = _log_add(B_numer[i, j],
                            value - lpk)
                elif name == "A_denom":
                    if A_denom is None:
                        A_denom = ones(col, float64) * _NINF
                    A_denom[j] = _log_add(A_denom[j], value - lpk)
                elif name == "B_denom":
                    if B_denom is None:
                        B_denom = ones(col, float64) * _NINF
                    B_denom[j] = _log_add(B_denom[j], value - lpk)

        # output the global hmm parameter
        for e in pi:
            self.outputcollector.collect("Pi", tuple2str((e, pi[e])))

        for i in range(N):
            for j in range(N):
                self.outputcollector.collect("A", tuple2str((states[i], \
                        states[j], 2 ** (A_numer[i, j] - A_denom[i]))))


        for i in range(N):
            for j in range(M):
                self.outputcollector.collect("B", tuple2str((states[i], \
                        symbols[j], 2 ** (B_numer[i, j] - B_denom[i]))))

        self.outputcollector.collect("loglikelihood", logprob)


if __name__ == "__main__":
    EM_Reducer().call_reduce()

########NEW FILE########
__FILENAME__ = runStreaming
"""
this script shows how to iteratively run the MapReduce task
"""

from subprocess import Popen
import sys

# convergence threshold
diff = 0.0001
oldlog = 0
newlog = 1
iter = 100
i = 0

# while not converged or not reach maximum iteration number
while (abs(newlog - oldlog) > diff and i <= iter):
    print "oldlog", oldlog
    print "newlog", newlog

    i += 1
    oldlog = newlog


    # iteratively execute the MapReduce task
    userdir = '/home/mxf/nltknew/nltk_contrib/hadoop/EM/'
    p = Popen([userdir + 'runStreaming.sh' ], shell=True, stdout=sys.stdout)
    p.wait()
    print "returncode", p.returncode

    # open the parameter output from finished iteration
    # and get the new loglikelihood
    f = open("hmm_parameter", 'r')
    for line in f:
        li = line.strip().split()
        if li[0] == "loglikelihood":
            newlog = float(li[1])
    f.close()

print "oldlog", oldlog
print "newlog", newlog

########NEW FILE########
__FILENAME__ = inputformat
from sys import stdin

class TextLineInput:
    """ 
    treat the input as lines of text
    
    emit None as key and text line as value
    """

    @staticmethod
    def read_line(file=stdin):
        """
        read and parse input file, for each line, yield a (None, line) pair

        @return: yield a (None, line) pair
        @param file: input file, default to stdin
        """
        for line in file:
        # split the line into words, trailing space truncated
            yield None, line.strip()

class KeyValueInput:
    """ 
    treat the input as lines of (key, value) pair splited by separator
    
    emit text before separator as key and the rest as value
    """

    @staticmethod
    def read_line(file=stdin, separator='\t'):
        """
        read and parse input file, for each line, 
        separate the line and yield a (key, value) pair

        @return: yield a (key, value) pair
        @param file: input file, default to stdin
        @param separator: specify the character to 
        separate a line of input, default to '\t'
        """
        for line in file:
            yield line.rstrip().split(separator, 1)

########NEW FILE########
__FILENAME__ = mapper
from inputformat import TextLineInput
from outputcollector import LineOutput


class MapperBase:
    """ 
    Base class for every map tasks
    
    Your map class should extend this base class 
    and override the map function
    """

    def __init__(self):
        """
        set the default input formatter and output collector
        """
        self.inputformat = TextLineInput
        self.outputcollector = LineOutput

    def set_inputformat(self, format):
        """
        set the input formatter for map task

        @param format: the input formatter to parse the input
        """

        self.inputformat = format
    
    def set_outputcollector(self, collector):
        """
        set the output collector for map task 

        @param collector: the ouput collector to collect output
        """

        self.outputcollector = collector

    def map(self, key, value):
        """
        do map operation on each (key, value) pair

        It is the only function needs to and 
        have to be implemented in your own class

        It is a callback function that is called in C{call_map()} function
        you should not call this function directly

        the actual content of key and value is determined by the inputformat

        @param key: key part in (key, value) pair
        @type key: C{string}
        @param value: value part in (key, value) pair
        @type value: C{string}
        """

        raise NotImplementedError('map() is not implemented in this class')

    def call_map(self):
        """ 
        driver function for map task, you should call this method 
        instead of the map method in main function 
        """

        data = self.inputformat.read_line()
        for key, value in data:
            self.map(key, value)

########NEW FILE########
__FILENAME__ = outputcollector
class LineOutput:
    """ 
    default output class, output key and value 
    as (key, value) pair separated by separator
    """

    @staticmethod
    def collect(key, value, separator = '\t'):
        """
        collect the key and value, output them to
        a line separated by a separator character

        @param key: key part in (key, value) pair
        @type key: C{string}
        @param value: value part in (key, value) pair
        @type value: C{string}
        @param separator: character to separate the key and value
        @type separator: C{string}
        """
        
        keystr = str(key)
        valuestr = str(value)
        print '%s%s%s' % (keystr, separator, valuestr)

########NEW FILE########
__FILENAME__ = reducer
from itertools import groupby
from operator import itemgetter

from inputformat import KeyValueInput
from outputcollector import LineOutput

class ReducerBase:
    """
    Base class for every reduce tasks

    Your reduce class should extend this base class 
    and override the reduce function 
    """

    def __init__(self):
        """
        set the default input formatter and output collector
        """
        self.inputformat = KeyValueInput
        self.outputcollector = LineOutput

    def set_inputformat(self, format):
        """
        set the input formatter to parse the input

        @param format: the input formatter to parse the input
        """

        self.inputformat = format

    def set_outputcollector(self, collector):
        """ 
        set the output collector for reduce task

        @param collector: the ouput collector to collect output
        """

        self.outputcollector = collector

    def group_data(self, data):
        """ 
        collect data that have the same key into a group
        assume the data is sorted 
        """

        for key, group in  groupby(data, itemgetter(0)):
            values = map(itemgetter(1), group)
            yield key, values

    def reduce(self, key, values):
        """
        do reduce operation for each (key, [value,...]) pair

        It is the only function needs to and 
        have to be implemented in your own class

        It is a callback function that is called in C{call_reduce()} function
        you should not call this function directly

        the actual content of key and values is determined by the inputformat

        @param key: key
        @type key: C{string}
        @param values: a list of values correspond to the key
        @type values: C{list}
        """

        raise NotImplementedError('reduce() is not implemented in this class')

    def call_reduce(self):
        """
        driver function for reduce task, you should call this method 
        instead of the reduce method in main function 
        """

        data = self.inputformat.read_line()
        for key, values in self.group_data(data):
            self.reduce(key, values)

########NEW FILE########
__FILENAME__ = util
"""
utility to convert data representation between tuple and string

provide convenient methods for parsing the input string to tuple and
formatting the tuple to string output
"""

def tuple2str(t, separator = ' '):
    """
    convert tuple into string expression

    @param t: tuple to be converted
    @type t: C{tuple}
    @param separator: character to separate each element in the tuple
    @type separator: C{string}

    >>> tuple2str((1, 2))
    '1 2'
    >>> tuple2str((1, "word"), '#')
    '1#word'
    >>> tuple2str(1)
    Traceback (most recent call last):
    ...
    ValueError: The first parameter must be a tuple
    """
    if isinstance(t, tuple):
        s = ""
        for e in t[:-1]:
            s += str(e) + separator
        s += str(t[-1])

        return s
    else:
        raise ValueError, "The first parameter must be a tuple"

def str2tuple(s, separator = ' '):
    """
    convert the string representation to a tuple

    @param s: string to be converted
    @type s: C{string}
    @param separator: character to separate each element in the tuple
    @type separator: C{string}

    >>> str2tuple("20 3")
    ('20', '3')
    >>> str2tuple("hello#world", '#')
    ('hello', 'world')
    >>> str2tuple(1)
    Traceback (most recent call last):
    ...
    ValueError: the first parameter must be a string
    """
    if isinstance(s, str):
        t = s.strip().split(separator)
        return tuple(t)
    else:
        raise ValueError, "the first parameter must be a string"


if __name__ == "__main__":
    import doctest 
    doctest.testmod()

########NEW FILE########
__FILENAME__ = name_mapper1
from hadooplib.mapper import MapperBase

class NameMapper(MapperBase):
    """
    map a name to its first character

    e.g. Adam -> (Adam, A)
    """

    def map(self, key, value):
        """
        map a name to its first character
        
        @param key: None
        @param value: name
        """
        self.outputcollector.collect(value.strip(), value[0])

if __name__ == "__main__":
    NameMapper().call_map()

########NEW FILE########
__FILENAME__ = name_mapper2
from hadooplib.inputformat import KeyValueInput
from hadooplib.mapper import MapperBase
from hadooplib.util import tuple2str

class Name2Names(MapperBase):
    """
    map a name to the name before and after it

    e.g. (A, Ada Adam Adams) -> (Adam, Ada Adams)
    """

    def __init__(self):
        MapperBase.__init__(self)
        self.set_inputformat(KeyValueInput)

    def map(self, key, value):
        """
        map a name to the name before and after it

        @param key: the first character of a name
        @param value: all the names start with the key
        """
        namelist = value.strip().split()
        namelist.insert(0, '')
        namelist.append('')
        n = len(namelist)
        for i in range(1, n - 1):
            self.outputcollector.collect(namelist[i], \
                    tuple2str((namelist[i-1] ,namelist[i+1])))
    
if __name__ == "__main__":
    Name2Names().call_map()

########NEW FILE########
__FILENAME__ = similiar_name_reducer
from hadooplib.reducer import ReducerBase

class Name2SimiliarName(ReducerBase):
    """
    find the most simliar name for the given name, 
    from the name before and after it

    e.g. (Adam, Ada Adams) -> (Adam, Ada)
    """

    def reduce(self, key, values):
        """
        find the most simliar name for the given name 

        @param key: the given name
        @param values: the name before and after the given name
        """
        li  = values[0].strip().split()

        # solve the boundary case: the name is at the head or at the tail
        if len(li) == 1:
            li.append("")
        before, after = li[0], li[1]

        sim1 = self.similiarity(key, before)
        sim2 = self.similiarity(key, after)

        if sim1 >= sim2:
            self.outputcollector.collect(key, before)
        else:
            self.outputcollector.collect(key, after)

    def similiarity(self, name1, name2):
        """
        compute the similarity between name1 and name2

        e.g. similiarity("Ada", "Adam") = 3
        """

        n = min(len(name1), len(name2))
        for i in range(n):
            if (name1[i] != name2[i]):
                return i + 1
        return n 

if __name__ == "__main__":
    Name2SimiliarName().call_reduce()

########NEW FILE########
__FILENAME__ = swap_mapper
from hadooplib.mapper import MapperBase
from hadooplib.inputformat import KeyValueInput

class SwapMapper(MapperBase):
    """
    swap (key, value) pair to (value, key) pair,
    i.e. swap the role of key and value

    e.g. word 1 -> 1 word
    """

    def __init__(self):
        MapperBase.__init__(self)
        # use KeyValueInput instead of the default TextLineInput
        self.set_inputformat(KeyValueInput)


    def map(self, key, value):
        """
        swap the key and value
        """
        self.outputcollector.collect(value, key)

if __name__ == "__main__":
    SwapMapper().call_map()

########NEW FILE########
__FILENAME__ = value_aggregater
from hadooplib.reducer import ReducerBase
from hadooplib.util import tuple2str

class ValueAggregater(ReducerBase):
    """
    aggregate the values having the same key.

    e.g. (animal, cat)
         (animal, dog)
         (animal, mouse)
         
         -> 
         
         (animal, cat dog mouse)
    """

    def reduce(self, key, values):
        """
        aggregate the values having the same key.
        """
        values_str = tuple2str(tuple(values))
        self.outputcollector.collect(key, values_str)

if __name__ == "__main__":
    ValueAggregater().call_reduce()

########NEW FILE########
__FILENAME__ = idf_map
from hadooplib.mapper import MapperBase
from hadooplib.inputformat import KeyValueInput

class IDFMapper(MapperBase):
    """
    output (word All, 1) for every (word filename, tf) pair

    (word filename, tf) -> (word All, 1)
    """

    def __init__(self):
        MapperBase.__init__(self)
        # use KeyValueInput instead of the default TextLineInput
        self.set_inputformat(KeyValueInput)

    def map(self, key, value):
        """
        output (word All, 1) for every (word filename, tf) pair
        """
        word, filename = key.split()
        # use 'All' to mark the unique occurence
        # of every word in a document
        self.outputcollector.collect(word + " All", 1)

if __name__ == "__main__":
    IDFMapper().call_map()

########NEW FILE########
__FILENAME__ = idf_reduce
from hadooplib.reducer import ReducerBase

class IDFReducer(ReducerBase):

    def reduce(self, key, values):
        sum  = 0
        try:
            for value in values:
                sum += int(value) 
            self.outputcollector.collect(key, sum)
        except ValueError:
            #count was not a number, so silently discard this item
            pass

if __name__ == "__main__":
    IDFReducer().call_reduce()

########NEW FILE########
__FILENAME__ = sort
"""
sort program in windows sometimes behaves strangely
so we write a small sorting program to be used in testing
"""

import sys

li = []
for line in sys.stdin:
    li.append(line)

li.sort()
for e in li:
    print e,

########NEW FILE########
__FILENAME__ = tfidf_map1
from hadooplib.mapper import MapperBase
from hadooplib.inputformat import KeyValueInput

class TFIDFMapper1(MapperBase):
    """
    keep only the word in the key field
    remove filename from key and put it into value

    (word filename, number) -> (word, filename number)
    e.g. (dog 1.txt, 1) -> (dog, 1.txt 1)
    """
    def __init__(self):
        MapperBase.__init__(self)
        self.set_inputformat(KeyValueInput)

    def map(self, key, value):
        """
        extract filename from key and put it into value

        @param key: word and filename
        @param value: term frequency
        """
        word, filename = key.split()
        self.outputcollector.collect(word, filename + "," + value)

if __name__ == "__main__":
    TFIDFMapper1().call_map()

########NEW FILE########
__FILENAME__ = tfidf_map2
from hadooplib.mapper import MapperBase
from hadooplib.inputformat import KeyValueInput

class TFIDFMapper2(MapperBase):
    """
    sort TF*IDF value by filename

    (word, [filename TF*IDF...]) -> (filename TF*IDF, word)
    """

    def __init__(self):
        MapperBase.__init__(self)
        self.set_inputformat(KeyValueInput)
    
    def map(self, key, value):
        """
        do some content swapping and extraction on key and value

        @param key: word
        @param value: filename TF*IDF
        """
        elements  = value.split()
        for e in elements:
            e = e.replace(',', ' ')
            self.outputcollector.collect(e, key)
    
if __name__ == "__main__":
    TFIDFMapper2().call_map()

########NEW FILE########
__FILENAME__ = tfidf_reduce1

from hadooplib.reducer import ReducerBase
from math import log

class TFIDFReducer1(ReducerBase):
    """
    computing the TF*IDF value for every word

    (word, [filename occurences...]) -> (word, [filename TF*IDF...])
    """

    def reduce(self, key, values):
        """
        computing the TF*IDF value for every word

        @param key: word
        @param values: filename occurences
        """
        idf = 1
        # first compute the IDF value
        for value in values:
            if value[:3] == "All":
                idf = 1.0/(1 + log(int(value.split(',')[1].strip())))
                values.remove(value)
                break

        # then compute TF*IDF value for each word
        value_str = ""
        for value in values:
            file, tf = value.split(',')
            tf = int(tf.strip())
            value_str += " " + file + "," + str(tf*idf) + " "
        self.outputcollector.collect(key, value_str)

if __name__ == "__main__":
    TFIDFReducer1().call_reduce()

########NEW FILE########
__FILENAME__ = tf_map
from hadooplib.mapper import MapperBase

class TFMapper(MapperBase):
    """
    get the filename (one filename per line), 
    open the file and count the term frequency.
    """

    def map(self, key, value):
        """
        output (word filename, 1) for every word in files

        @param key: None
        @param value: filename
        """
        filename = value.strip()
        if len(filename) == 0:
            return
        file = open(filename, 'r')
        for line in file:
            words = line.strip().split()
            for word in words:
                self.outputcollector.collect(word + " " + filename, 1)

if __name__ == "__main__":
    TFMapper().call_map()

########NEW FILE########
__FILENAME__ = tf_reduce
from hadooplib.reducer import ReducerBase

class TFReducer(ReducerBase):
    """
    sum the occurences of every word
    """

    def reduce(self, key, values):
        """
        @param key: word
        @param values: list of partial sum
        """
        sum  = 0
        try:
            for value in values:
                sum += int(value) 
            self.outputcollector.collect(key, sum)
        except ValueError:
            #value was not a number, so silently discard this item
            pass

if __name__ == "__main__":
    TFReducer().call_reduce()

########NEW FILE########
__FILENAME__ = wordcount_mapper

from hadooplib.mapper import MapperBase

class WordCountMapper(MapperBase):
    """
    count the occurences of each word
    """

    def map(self, key, value):
        """
        for each word in input, output a (word, 1) pair

        @param key: None, no use
        @param value: line from input
        """
        words = value.split()
        for word in words:
            self.outputcollector.collect(word, 1)

if __name__ == "__main__":
    WordCountMapper().call_map()

########NEW FILE########
__FILENAME__ = wordcount_reducer

from hadooplib.reducer import ReducerBase

class WordCountReducer(ReducerBase):
    """
    count the occurences of each word
    """

    def reduce(self, key, values):
        """
        for each word, accmulate all the partial sum

        @param key: word
        @param values: list of partical sum
        """
        sum  = 0
        try:
            for value in values:
                sum += int(value) 
            self.outputcollector.collect(key, sum)
        except ValueError:
            #count was not a number, so silently discard this item
            pass

if __name__ == "__main__":
    WordCountReducer().call_reduce()

########NEW FILE########
__FILENAME__ = lambek
#
# lambek.py
#
# Edward Loper
# Created [12/10/00 03:41 AM]
# $Id$
#

"""Lambek Calculus Theorem Prover

"""

# For while I'm coding..
#import term;reload(term)
#import typedterm; reload(typedterm)

_VERBOSE = 0
_VAR_NAMES = 1
_SHOW_VARMAP = not _VAR_NAMES

from term import *
from typedterm import *
from lexicon import *
import sys, re

class Sequent:
    """A sequent maps an ordered sequence of TypedTerm's to an ordered 
    sequence of TypedTerm's."""
    # left and right are lists of TypedTerms.
    def __init__(self, left, right):

        # Check types, because we're paranoid.
        if type(left) not in [types.ListType, types.TupleType] or \
           type(right) not in [types.ListType, types.TupleType]:
            raise TypeError('Expected lists of TypedTerms')
        for elt in left+right:
            if not isinstance(elt, TypedTerm):
                raise TypeError('Expected lists of TypedTerms')
        
        self.left = left
        self.right = right

    def __repr__(self):
        left_str = `self.left`[1:-1]
        right_str = `self.right`[1:-1]
        return left_str + ' => ' + right_str

    def to_latex(self, pp_varmap=None):
        if pp_varmap == None: pp_varmap = {}
        for te in self.left+self.right:
            extend_pp_varmap(pp_varmap, te.term)
        str = ''
        for i in range(len(self.left)):
            str += self.left[i].to_latex(pp_varmap) + ', '
        str = str[:-2] + ' \Rightarrow '
        for i in range(len(self.right)):
            str += self.right[i].to_latex(pp_varmap) + ', '
        return str[:-2]
        
    def pp(self, pp_varmap=None):
        if pp_varmap == None: pp_varmap = {}
        for te in self.left+self.right:
            extend_pp_varmap(pp_varmap, te.term)
        str = ''
        for i in range(len(self.left)):
            str += self.left[i].pp(pp_varmap) + ', '
        str = str[:-2] + ' => '
        for i in range(len(self.right)):
            str += self.right[i].pp(pp_varmap) + ', '
        return str[:-2]

    def simplify(self, varmap):
        left = [te.simplify(varmap) for te in self.left]
        right = [te.simplify(varmap) for te in self.right]
        return Sequent(left, right)
    
class Proof:
    "Represents 1 step of a proof tree.."
    # rule: which rule was used (str)
    # assumptions: what is assumed (list of Proofs)
    # conclusion: what is concluded (Sequent)
    def __init__(self, rule, assumptions, conclusion, varmap):
        self.rule = rule
        self.assumptions = assumptions
        self.conclusion = conclusion
        self.varmap = varmap

    def __repr__(self):
        return self.rule+' '+`self.assumptions`+' -> '\
               +`self.conclusion`

    def simplify(self, varmap=None):
        if varmap == None:
            varmap = self.varmap
        assum = [a.simplify(varmap) for a in self.assumptions] 
        concl = self.conclusion.simplify(varmap)
        return Proof(self.rule, assum, concl, varmap)

    def to_latex_array(self, depth = 1, pp_varmap=None):
        if pp_varmap == None: pp_varmap={}

        # Draw asumptions
        str = '\\begin{array}{c}\n'
        for assumption in self.assumptions:
            str += '      '*depth + \
                   assumption.to_latex(depth+1, pp_varmap) + \
                   ' \\quad \n'
        str = str[:-1] + '\\\\'+'\\hline'+'\n'

        # Add conclusion
        str += '      '*depth + '{' + \
               self.conclusion.to_latex(pp_varmap) + '}'

        # Close array
        str += '\\\\\n'+'      '*depth+'\\end{array}'
        
        # The rule type
        str += ' \\textrm{' + \
               re.sub(r'\\', r'$\\backslash$', self.rule) + '}' 

        if depth == 1:
            return '$$\n'+str+'\n$$'
        else:
            return '{'+str+'}'
    
    def to_latex(self, depth = 1, pp_varmap=None):
        if pp_varmap == None: pp_varmap={}

        # Draw asumptions
        str = '\\frac{\\textrm{$ \n'
        for assumption in self.assumptions:
            str += '      '*depth + \
                   assumption.to_latex(depth+1, pp_varmap) + \
                   ' \\quad \n'
        str = str[:-1] + '$}}\n'

        # Add conclusion
        str += '      '*depth + '{' + \
               self.conclusion.to_latex(pp_varmap) + '}'

        # The rule type
        rule = re.sub(r'\\', r'$\\backslash$', self.rule)
        rule = re.sub(r'\*', r'$\\cdot$', rule)
        str += ' \\textrm{' + rule + '}'

        if depth == 1:
            return '$$\n'+str+'\n$$'
        else:
            return '{'+str+'}'
    
    # Returns (str, right)
    def pp(self, left=0, toplevel=1, pp_varmap=None):
        if pp_varmap == None: pp_varmap={}
        right = left
        str = ''
        
        if _VAR_NAMES:
            concl = self.conclusion.pp(pp_varmap)
        else:
            concl = `self.conclusion`

        # Draw assumptions
        for assumption in self.assumptions:
            (s, right) = assumption.pp(right, 0, pp_varmap)
            str = _align_str(str, s)
            right += 5

        # Draw line.
        right = max(right-5, left+len(concl))
        str += ' '*left + '-'*(right-left) + ' ' + self.rule + '\n' 

        # Draw conclusion
        start = left+(right-left-len(concl))/2
        str += ' '*start + concl + '\n'

        if toplevel:
            if _SHOW_VARMAP:
                return str+'\nVarmap: '+ `self.varmap`+'\n'
            else:
                return str
        else:
            return (str, right)

def _align_str(s1, s2):
    lines1 = s1.split('\n')
    lines2 = s2.split('\n')

    if lines1[-1] == '': lines1 = lines1[:-1]
    if lines2[-1] == '': lines2 = lines2[:-1]

    str = ''

    while len(lines1) > len(lines2):
        str += lines1[0] + '\n'
        lines1 = lines1[1:]
        
    while len(lines2) > len(lines1):
        str += lines2[0] + '\n'
        lines2 = lines2[1:]

    for n in range(len(lines1)):
        x = 0
        for x in range(min(len(lines1[n]), len(lines2[n]))):
            if lines1[n][x] == ' ':
                str += lines2[n][x]
            elif lines2[n][x] == ' ':
                str += lines1[n][x]
            else:
                raise ValueError('Overlapping strings')
        str += lines1[n][x+1:]
        str += lines2[n][x+1:]
        str += '\n'
    return str
        
######################################
# PROOF LOGIC
######################################

# Prove a sequent.  Variables can have their values filled in.
# If short_circuit is 1, return once we find any proof.  If
# short_circuit is 0, return all proofs.
def prove(sequent, short_circuit=0):
    proofs = _prove(sequent, VarMap(), short_circuit, 0)
    return [proof.simplify() for proof in proofs]

def _prove(sequent, varmap, short_circuit, depth):
    if _VERBOSE:
        print ('  '*depth)+'Trying to prove', sequent

    proofs = []

    if proofs == [] or not short_circuit:
        proofs = proofs + introduce(sequent, varmap, short_circuit, depth+1)
    if proofs == [] or not short_circuit:
        proofs = proofs + rslash_l(sequent, varmap, short_circuit, depth+1)
    if proofs == [] or not short_circuit:
        proofs = proofs + lslash_l(sequent, varmap, short_circuit, depth+1)
    if proofs == [] or not short_circuit:
        proofs = proofs + rslash_r(sequent, varmap, short_circuit, depth+1)
    if proofs == [] or not short_circuit:
        proofs = proofs + lslash_r(sequent, varmap, short_circuit, depth+1)
    if proofs == [] or not short_circuit:
        proofs = proofs + dot_l(sequent, varmap, short_circuit, depth+1)
    if proofs == [] or not short_circuit:
        proofs = proofs + dot_r(sequent, varmap, short_circuit, depth+1)

    if _VERBOSE:
        print '  '*depth+'Found '+`len(proofs)`+' proof(s)'

    return proofs

def introduce(sequent, varmap, short_circuit, depth):
    if len(sequent.left) != 1 or \
       len(sequent.right) != 1 or \
       sequent.left[0].type != sequent.right[0].type:
        return []

    newseq = Sequent(sequent.left, sequent.right)
    r_term = sequent.right[0].term
    l_term = sequent.left[0].term

    # Try to unify
    te = sequent.left[0].unify(sequent.right[0], varmap)
    if te == None: newseq = sequent
    else: newseq = Sequent([te], [te])

    return [Proof('I', (), newseq, varmap)]

def rslash_l(sequent, varmap_in, short_circuit, depth):
    proofs = []

    for i in range(len(sequent.left)-1):
        if isinstance(sequent.left[i].type, RSlash) and \
           len(sequent.right) == 1:
            # Set up some variables...
            beta = Var()
            alpha = sequent.left[i].term
            A = sequent.left[i].type.result
            B = sequent.left[i].type.arg
            Gamma1 = sequent.left[:i]
            gamma = sequent.right[0].term
            C = sequent.right[0].type

            # Try all combinations of Delta, Gamma2..
            for j in range(i+1, len(sequent.left)):
                Delta = sequent.left[i+1:j+1]
                Gamma2 = sequent.left[j+1:]

                # Try proving the left assumption.
                l_seq = Sequent(Delta, [TypedTerm(beta, B)])
                l_proofs = _prove(l_seq, varmap_in, short_circuit, depth)

                # For each proof, try proving the right half.  Make
                # sure to keep beta bound to the same thing..
                for l_proof in l_proofs:
                    beta = l_proof.conclusion.right[0].term
                    r_seq = Sequent(Gamma1+\
                                    [TypedTerm(Appl(alpha, beta), A)]+\
                                    Gamma2, [TypedTerm(gamma, C)])
                    r_proofs = _prove(r_seq, varmap_in, short_circuit, depth)
                    for r_proof in r_proofs:
                        varmap = r_proof.varmap + l_proof.varmap
                        right = r_proof.conclusion.right[0]
                        right = right.unify(TypedTerm(gamma, C), varmap)
                        proofs.append(Proof('/L', [l_proof, r_proof],\
                                            Sequent(sequent.left,[right]),\
                                            varmap))
                        if short_circuit: return proofs

    return proofs

def lslash_l(sequent, varmap_in, short_circuit, depth):
    proofs = []

    for i in range(1, len(sequent.left)):
        if isinstance(sequent.left[i].type, LSlash) and \
           len(sequent.right) == 1:
            # Set up some variables...
            beta = Var()
            alpha = sequent.left[i].term
            A = sequent.left[i].type.result
            B = sequent.left[i].type.arg
            gamma = sequent.right[0].term
            C = sequent.right[0].type
            Gamma2 = sequent.left[i+1:]

            # Try all combinations of Delta, Gamma2..
            for j in range(i):
                Delta = sequent.left[j:i]
                Gamma1 = sequent.left[:j]

                # Try proving the left assumption.
                l_seq = Sequent(Delta, [TypedTerm(beta, B)])
                l_proofs = _prove(l_seq, varmap_in, short_circuit, depth)

                # For each proof, try proving the right half.  Make
                # sure to keep beta bound to the same thing..
                for l_proof in l_proofs:
                    beta = l_proof.conclusion.right[0].term
                    r_seq = Sequent(Gamma1+\
                                    [TypedTerm(Appl(alpha, beta), A)]+\
                                    Gamma2, [TypedTerm(gamma, C)])
                    r_proofs = _prove(r_seq, varmap_in, short_circuit, depth)
                    for r_proof in r_proofs:
                        varmap = r_proof.varmap + l_proof.varmap
                        right = r_proof.conclusion.right[0]
                        right = right.unify(TypedTerm(gamma, C), varmap)
                        
                        proofs.append(Proof('\\L', [l_proof, r_proof],\
                                            Sequent(sequent.left,[right]),
                                            varmap))
                        if short_circuit: return proofs
    return proofs

def rslash_r(sequent, varmap, short_circuit, depth):
    proofs = []

    # Make sure the right side is properly formatted..
    if len(sequent.right) != 1 or \
       not isinstance(sequent.right[0].type, RSlash):
        return proofs

    # Set up variables..
    x = Var()
    varmap.add(x, None)
    alpha = Appl(sequent.right[0].term, x)
    B = sequent.right[0].type.result
    A = sequent.right[0].type.arg
    Gamma = sequent.left

    seq = Sequent(Gamma + [TypedTerm(x, A)], \
                  [TypedTerm(alpha, B)])

    s_proofs = _prove(seq, varmap, short_circuit, depth)
    for proof in s_proofs:
        varmap = proof.varmap.copy()
        right1 = TypedTerm(Abstr(x, proof.conclusion.right[0].term),\
                           RSlash(B, A))
        right2 = TypedTerm(Abstr(x, alpha), sequent.right[0].type)
        right = right1.unify(right2, varmap)
        if right == None: continue
        varmap.add(x, None)
        concl = Sequent(Gamma, [right])
        proofs.append(Proof('/R', [proof], concl, varmap))

    return proofs

def lslash_r(sequent, varmap, short_circuit, depth):
    proofs = []

    # Make sure the right side is properly formatted..
    if len(sequent.right) != 1 or \
       not isinstance(sequent.right[0].type, LSlash):
        return proofs

    # Set up variables..
    x = Var()
    varmap.add(x, None)
    alpha = Appl(sequent.right[0].term, x)
    B = sequent.right[0].type.result
    A = sequent.right[0].type.arg
    Gamma = sequent.left

    seq = Sequent([TypedTerm(x, A)] + Gamma, \
                  [TypedTerm(alpha, B)])

    s_proofs = _prove(seq, varmap, short_circuit, depth)
    for proof in s_proofs:
        right1 = TypedTerm(Abstr(x, proof.conclusion.right[0].term),\
                           LSlash(A, B))
        right2 = TypedTerm(Abstr(x, alpha), sequent.right[0].type)
        right = right1.unify(right2, varmap)
        if right == None: continue
        varmap = varmap + proof.varmap
        varmap.add(x, None)
        concl = Sequent(Gamma, [right])
        proofs.append(Proof('\\R', [proof], concl, varmap))

    return proofs

def dot_l(sequent, varmap, short_circuit, depth):
    proofs = []

    for i in range(0, len(sequent.left)):
        if isinstance(sequent.left[i].type, Dot) and \
           len(sequent.right) == 1:
            Gamma1 = sequent.left[:i]
            Gamma2 = sequent.left[i+1:]
            A = sequent.left[i].type.left
            B = sequent.left[i].type.right
            alpha = sequent.left[i].term

            # Deal with alpha if we can
            if isinstance(alpha, Tuple):
                alpha1 = alpha.left
                alpha2 = alpha.right
            elif isinstance(alpha, Var):
                alpha_var = alpha
                alpha1 = Var()
                alpha2 = Var()
                alpha = Tuple(alpha1, alpha2)
                varmap.add(alpha_var, alpha)
            else:
                # We can't deal.. :(  Move on...
                continue

            left = Gamma1 + [TypedTerm(alpha1, A)] + \
                   [TypedTerm(alpha2, B)] + Gamma2
            right = sequent.right
            s_proofs = _prove(Sequent(left, right), varmap, \
                              short_circuit, depth)
            for proof in s_proofs:
                varmap = proof.varmap.copy()
                sequent.right[0].unify(proof.conclusion.right[0], varmap) 
                proofs.append(Proof('*L', [proof], sequent, varmap))
                if short_circuit: return proofs
                
    return proofs

def dot_r(sequent, varmap_in, short_circuit, depth):
    proofs = []

    for i in range(1, len(sequent.left)):
        if isinstance(sequent.right[0].type, Dot) and \
           len(sequent.right) == 1:
            Gamma1 = sequent.left[:i]
            Gamma2 = sequent.left[i:]
            A = sequent.right[0].type.left
            B = sequent.right[0].type.right
            alphabeta = sequent.right[0].term

            # Deal with alpha if we can
            if isinstance(alphabeta, Tuple):
                alpha = alphabeta.left
                beta = alphabeta.right
            elif isinstance(alphabeta, Var):
                alphabeta_var = alphabeta
                alpha = Var()
                beta = Var()
                alphabeta = Tuple(alpha, beta)
                varmap_in.add(alphabeta_var, alphabeta)
            else:
                # We can't deal.. :(  Move on...
                continue

            left = Sequent(Gamma1, [TypedTerm(alpha, A)])
            right = Sequent(Gamma2, [TypedTerm(beta, B)])

            for r_proof in _prove(right, varmap_in, short_circuit, depth):
                for l_proof in _prove(left, varmap_in, short_circuit, depth):
                    varmap = r_proof.varmap + l_proof.varmap
                    right = TypedTerm(Tuple(l_proof.conclusion.right[0].term,\
                                            r_proof.conclusion.right[0].term),
                                      sequent.right[0].type)
                    right = right.unify(sequent.right[0], varmap)
                    concl = Sequent(Gamma1+Gamma2, [right])
                    proofs.append(Proof('*R', [l_proof, r_proof],\
                                        concl, varmap))
                    if short_circuit: return proofs
    return proofs
######################################
# TESTING
######################################

def find_proof(left, right, short_circuit=1):
    sq = Sequent(left, right)
    proofs = prove(sq, short_circuit)
    if proofs:
        print '#'*60
        print "## Proof(s) for", sq.pp()
        for proof in proofs:
            print
            print proof.to_latex()
    else:
        print '#'*60
        print "## Can't prove", sq.pp()

def test_lambek():
    lex = Lexicon()
    lex.load(open('lexicon.txt', 'r'))

    find_proof(lex.parse('[np/n] [n]'), lex.parse('[np]'))
    find_proof(lex.parse('[np] [np\s]'), lex.parse('[s]'))
    find_proof(lex.parse('[n] [np\s]'), lex.parse('[(np/n)\s]'))
    find_proof(lex.parse('dog sleeps'), lex.parse('[(np/n)\s]'))
    find_proof(lex.parse('the kid runs'), lex.parse('[s]'))
    find_proof(lex.parse('john believes tom likes'), lex.parse('[s/np]'))
    find_proof(lex.parse('john likes mary'), lex.parse('[s]'))
    find_proof(lex.parse('likes'), lex.parse('[np\s/np]'), 0)
    find_proof(lex.parse('[a/b] [b]'), lex.parse('foo'))
    find_proof(lex.parse('[(np/n)*n]'), lex.parse('[np]'))
    find_proof(lex.parse('[(np\\s)/np]'), lex.parse('[np\\(s/np)]'))
    find_proof(lex.parse('gives2 tom mary'), lex.parse('[np\\s]'))
    find_proof(lex.parse('gives'), lex.parse('[np\\s/(np*np)]'))
    find_proof(lex.parse('the city tom likes'), lex.parse('[np*(s/np)]'))


HELP="""% Lambek Calculus Theorem Proover
%
% Type a sequent you would like prooved.  Examples are:
%   [np/n] [n] => [np]
%   [np] [np\s] => [s]
%   [n] [np\s] => [(np/n)\s]
%   dog sleeps => [(np/n)\s]
%   the kid runs => [s]
%   john believes tom likes => [s/np]
%   john likes mary => [s]
%   likes => [np\s/np]
%   [a/b] [b] => foo
%   [(np/n)*n] => [np]
%   [(np\\s)/np] => [np\\(s/np)]
%   gives2 tom mary => [np\\s]
%   gives => [np\\s/(np*np)]
%   the city tom likes => [np*(s/np)]
%
% Other commands:
%   help         -- show this information
%   latexmode    -- toggle latexmode (outputs in LaTeX)
%   shortcircuit -- toggle shortcircuit mode (return just one proof)
%   lexicon      -- display the lexicon contents
%   quit         -- quit
"""
    
def mainloop(input, out, lex, latexmode, shortcircuit):
    while 1:
        out.write('%>> ')
        str = input.readline()
        if str == '': return
        str = str.strip()
        if (str=='') or (str[0]=='#') or (str[0]=='%'): continue
        if str.find('=>') == -1:
            if str.lower().startswith('latex'):
                if str.lower().endswith('off'): latexmode = 0
                elif str.lower().endswith('on'): latexmode = 1
                else: latexmode = not latexmode
                if latexmode: print >>out, '% latexmode on'
                else: print >>out, 'latexmode off'
            elif str.lower().startswith('short'):
                if str.lower().endswith('off'): shortcircuit = 0
                elif str.lower().endswith('on'): shortcircuit = 1
                else: shortcircuit = not shortcircuit
                if shortcircuit: print >>out, '%shortcircuit on'
                else: print >>out, '% shortcircuit off'
            elif str.lower().startswith('lex'):
                words = lex.words()
                print >>out, '% Lexicon: '
                for word in words:
                    print >>out, '%  ' + word + ':', \
                          ' '*(14-len(word)) + lex[word].pp() 
            elif str.lower().startswith('q'): return
            elif str.lower().startswith('x'): return
            else:
                print >>out, HELP
        else:
            try:
                (left, right) = str.split('=>')
                seq = Sequent(lex.parse(left), lex.parse(right))
                proofs = prove(seq, shortcircuit)
                print >>out
                print >>out, '%'*60
                if proofs:
                    print >>out, "%% Proof(s) for", seq.pp()
                    for proof in proofs:
                        print >>out
                        if latexmode: print >>out, proof.to_latex()
                        else: print >>out, proof.pp()
                else:
                    print >>out, "%% Can't prove", seq.pp()
            except KeyError, e:
                print 'Mal-formatted sequent'
                print 'Key error (unknown lexicon entry?)'
                print e
            except ValueError, e:
                print 'Mal-formatted sequent'
                print e

# Usage: argv[0] lexiconfile
def main(argv):
    if (len(argv) != 2) and (len(argv) != 4):
        print 'Usage:', argv[0], '<lexicon_file>'
        print 'Usage:', argv[0], '<lexicon_file> <input_file> <output file>'
        return
    lex = Lexicon()
    try: lex.load(open(argv[1], 'r'))
    except:
        print "Error loading lexicon file"
        return

    if len(argv) == 2:
        mainloop(sys.stdin, sys.stdout, lex, 0, 1)
    else:
        out = open(argv[3], 'w')
        print >>out, '\documentclass{article}'
        print >>out, '\usepackage{fullpage}'
        print >>out, '\\begin{document}'
        print >>out
        mainloop(open(argv[2], 'r'), out, lex, 1, 1)
        print >>out
        print >>out, '\\end{document}'

if __name__ == '__main__':
    main(sys.argv)

    


########NEW FILE########
__FILENAME__ = lexicon
#
# lexicon.py
#
# Edward Loper
# Created [12/10/00 11:51 PM]
# $Id$
#

"""Lexicon!

Files have lines of the form:
<word> : <lambda-term> : <type>

"""

from term import *
from typedterm import *

# Map from word to TypedTerm
class Lexicon:
    def __init__(self):
        self._map = {}

    def load(self, file):
        for line in file.readlines():
            line = (line.split('#')[0]).strip()
            if len(line) == 0:  continue
            try:
                (word, term, type) = line.split(':')
                te = TypedTerm(parse_term(term), parse_type(type)) 
            except ValueError:
                print 'Bad line:', line
                continue
            
            word = word.strip().lower()
            if self._map.has_key(word):
                print 'Duplicate definitions for', word
            self._map[word] = te

    def words(self):
        return self._map.keys()
            
    def __getitem__(self, word):
        word = word.strip().lower()
        if word[0] == '[' and word[-1] == ']':
            try: return TypedTerm(Var(), parse_type(word[1:-1]))
            except ValueError: return None
        return self._map[word]

    def parse(self, str):
        """parse(self, str)
        Return a list of TypedTerms, for each word.  You can use
        forms like [np] and [s/np\n] to specify anonymous typed
        things.."""
        return [self[w] for w in str.split() if w != '']

    def __setitem__(self, word, te):
        word = word.strip().lower()
        if type(word) != type('') or \
           not isinstance(te, TypedTerm):
            raise ValueError('Expected string and TypedTerm')
        self._map[word] = te

########NEW FILE########
__FILENAME__ = term
#
# term.py
#
# Edward Loper
# Created [12/10/00 01:58 PM]
# $Id$
#

"""Lambda calculus stuff"""

import types, re
from copy import deepcopy

class Term:
    #FREEVAR_NAME = ['e', 'd', 'c', 'b', 'a']
    FREEVAR_NAME = ['$\\epsilon$', '$\\delta$', '$\\gamma$', \
                    '$\\beta$', '$\\alpha$']
    BOUNDVAR_NAME = ['z', 'y', 'x']
    def __init__(self):
        raise TypeError("Term is an abstract class")
    
class Var(Term):
    _max_id = 0
    def __init__(self):
        Var._max_id += 1
        self.id = Var._max_id
    def __repr__(self):
        return '?' + `self.id`
    def pp(self, pp_varmap=None):
        if pp_varmap == None: pp_varmap = make_pp_varmap(self)
        return pp_varmap[self]
    def to_latex(self, pp_varmap=None):
        if pp_varmap == None: pp_varmap = make_pp_varmap(self)
        return '\\textit{'+pp_varmap[self]+'}'
    def __hash__(self):
        return self.id
    def __cmp__(self, other):
        if isinstance(other, Var) and other.id == self.id: return 0
        else: return -1

class Const(Term):
    def __init__(self, name):
        if type(name) != types.StringType:
            raise TypeError("Expected a string name")
        self.name = name
    def __repr__(self):
        return self.name
    def pp(self, pp_varmap=None):
        return self.name
    def to_latex(self, pp_varmap=None):
        return '\\textbf{'+self.name+'}'
    def __cmp__(self, other):
        if isinstance(other, Const) and other.name == self.name:
            return 0
        else: return -1

class Appl(Term):
    def __init__(self, func, arg):
        self.func = func
        self.arg = arg
        if not isinstance(self.func, Term) or \
           not isinstance(self.arg, Term):
            raise TypeError('Expected Term argument', func, arg)
    def __repr__(self):
        if isinstance(self.func, Appl) or \
           isinstance(self.func, Abstr):
            return '('+`self.func` + ')(' + `self.arg` + ')'
        else:
            return `self.func` + '(' + `self.arg` + ')'
    def pp(self, pp_varmap=None):
        if pp_varmap == None: pp_varmap = make_pp_varmap(self)
        if isinstance(self.func, Appl) or \
           isinstance(self.func, Abstr):
            return '(' + self.func.pp(pp_varmap) + ')(' + \
                   self.arg.pp(pp_varmap) + ')'
        else:
            return self.func.pp(pp_varmap) + '(' + \
                   self.arg.pp(pp_varmap) + ')'
    def to_latex(self, pp_varmap=None):
        if pp_varmap == None: pp_varmap = make_pp_varmap(self)
        if isinstance(self.func, Appl) or \
           isinstance(self.func, Abstr):
            return '\\left(' + self.func.to_latex(pp_varmap) + \
                   '\\right)\\left(' + \
                   self.arg.to_latex(pp_varmap) + '\\right)'
        else:
            return self.func.to_latex(pp_varmap) + '(' + \
                   self.arg.to_latex(pp_varmap) + ')'
    def __cmp__(self, other):
        if isinstance(other, Appl) and other.func == self.func and \
           other.arg == self.arg: return 0
        else: return -1

class Abstr(Term):
    def __init__(self, var, body):
        self.var = var
        self.body = body
        if not isinstance(self.var, Var) or \
           not isinstance(self.body, Term):
            raise TypeError('Expected Var and Term arguments')
    def __repr__(self):
        if isinstance(self.body, Abstr) or \
           isinstance(self.body, Appl):
            return '(\\' + `self.var` + '.' + `self.body`+')'
        else:
            return '\\' + `self.var` + '.' + `self.body`
    def pp(self, pp_varmap=None):
        if pp_varmap == None: pp_varmap = make_pp_varmap(self)
        if isinstance(self.body, Abstr) or \
           isinstance(self.body, Appl):
            return '(' + '\\' + self.var.pp(pp_varmap) + '.' + \
                   self.body.pp(pp_varmap) + ')'
        else:
            return '\\' + self.var.pp(pp_varmap) + '.' + \
                   self.body.pp(pp_varmap)
    def to_latex(self, pp_varmap):
        if pp_varmap == None: pp_varmap = make_pp_varmap(self)
        if isinstance(self.body, Abstr) or \
           isinstance(self.body, Appl):
            return '\\left(' + '\\lambda ' + self.var.to_latex(pp_varmap) + \
                   '.' + self.body.to_latex(pp_varmap) + '\\right)'
        else:
            return '\\lambda' + self.var.to_latex(pp_varmap) + \
                   '.' + self.body.to_latex(pp_varmap)
    def __cmp__(self, other):
        if isinstance(other, Abstr) and \
           self.body == replace(other.var, self.var, other.body):
            return 0
        else: return -1

class Tuple(Term):
    def __init__(self, left, right):
        self.left = left
        self.right = right
        if not isinstance(self.left, Term) or \
           not isinstance(self.right, Term):
            raise TypeError('Expected Term arguments')
    def __repr__(self):
        return '<'+`self.left`+', '+`self.right`+'>'
    def pp(self, pp_varmap=None):
        if pp_varmap == None: pp_varmap = make_pp_varmap(self)
        return '<'+self.left.pp(pp_varmap)+', '+\
               self.right.pp(pp_varmap)+'>'
    def to_latex(self, pp_varmap=None):
        if pp_varmap == None: pp_varmap = make_pp_varmap(self)
        return '\\left\\langle'+self.left.to_latex(pp_varmap)+', '+\
               self.right.to_latex(pp_varmap)+'\\right\\rangle'
    def __cmp__(self, other):
        if isinstance(other, Tuple) and other.left == self.left and \
           other.right == self.right: return 0
        else: return -1

def make_pp_varmap(term):
    return extend_pp_varmap({}, term)
        
def extend_pp_varmap(pp_varmap, term):
    # Get free and bound vars
    free = freevars(term)
    bound = boundvars(term)

    # Get the remaining names.
    freenames = [n for n in Term.FREEVAR_NAME \
                 if n not in pp_varmap.values()]
    boundnames = Term.BOUNDVAR_NAME[:]

    for fv in free:
        if not pp_varmap.has_key(fv):
            if freenames == []:
                pp_varmap[fv] = `fv`
            else:
                pp_varmap[fv] = freenames.pop()

    for bv in bound:
        if not pp_varmap.has_key(bv):
            if boundnames == []:
                pp_varmap[bv] = `bv`
            else:
                pp_varmap[bv] = boundnames.pop()

    return pp_varmap

class VarMap:
    def __init__(self):
        self._map = {}
    def add(self, var, term):
        if self._map.has_key(var):
            if term != None and term != self._map[var]:
                # Unclear what I should do here -- for now, just pray
                # for the best. :)
                None
        else:
            self._map[var] = term
    def __repr__(self):
        return `self._map`
    def _get(self, var, orig, getNone=1):
        val = self._map[var]
        if not getNone and val == None: return var
        if not isinstance(val, Var): return val
        if val == orig:
            #print 'WARNING: CIRCULAR LOOP'
            # Break the loop at an arbitrary point.
            del(self._map[val])
            return val
        elif self._map.has_key(val):
            return(self._get(val, orig, getNone))
        else:
            return val
    def __getitem__(self, var):
        if self._map.has_key(var):
            return self._get(var, var, 1)
        else:
            return var
    def simplify(self, var):
        if self._map.has_key(var):
            return self._get(var, var, 0)
        else:
            return var
    def copy(self):
        result = VarMap()
        result._map = self._map.copy()
        return result
    def __add__(self, other):
        result = self.copy()
        for var in other._map.keys():
            result.add(var, other[var])
        return result
    def copy_from(self, other):
        self._map = other._map.copy()
    def force(self, var, term):
        self._map[var] = term

# Use a varmap to simplify an term.
def simplify(term, varmap):
    if isinstance(term, Var):
        e = varmap.simplify(term)
        if e == term or e == None:
            return term
        else:
            return simplify(e, varmap)
    if isinstance(term, Appl):
        return Appl(simplify(term.func, varmap), \
                    simplify(term.arg, varmap))
    if isinstance(term, Tuple):
        return Tuple(simplify(term.left, varmap), \
                     simplify(term.right, varmap))
    if isinstance(term, Abstr):
        return Abstr(term.var, simplify(term.body, varmap))
    if isinstance(term, Const):
        return term

_VERBOSE = 0
    
def unify(term1, term2, varmap=None, depth=0):
    if _VERBOSE: print '  '*depth+'>> unify', term1, term2, varmap
    term1 = reduce(term1)
    term2 = reduce(term2)
    if varmap == None: varmap = VarMap()

    old_varmap = varmap.copy()
    result = unify_oneway(term1, term2, varmap, depth+1)
    if result:
        if _VERBOSE:
            print '  '*depth+'<<unify', term1, term2, varmap, '=>', result
        return result
    varmap.copy_from(old_varmap)

    result = unify_oneway(term2, term1, varmap, depth+1)
    if result:
        if _VERBOSE:
            print '  '*depth+'<<unify', term1, term2, varmap, '=>', result
        return result
    #raise(ValueError("can't unify", term1, term2, varmap))
    if _VERBOSE:
        print '  '*depth+'unify', term1, term2, varmap, '=>', None
    
    return None
    
#### THIS FUNCTION IS CURRENTLY PARTIALLY BROKEN
# Possible pairings:
#   var <-> abstr
#   var <-> appl
#   var <-> var
#   var <-> const
#   abstr <-> abstr
#   abstr <-> apl
#   apl <-> apl
#   const <-> const
#   tuple <-> tuple
#
def unify_oneway(term1, term2, varmap, depth):
    term1 = reduce(term1)
    term2 = reduce(term2)

    # Identical
    if term1 == term2: return term1

    # If term1 is a var in varmap, get its value...
    if isinstance(term1, Var):
        if varmap[term1] != None:
            term1 = varmap[term1]
        
    # Variable
    if isinstance(term1, Var):
        if varmap[term1] == None:
            # It's a bound var
            if term1 == term2: return term1
            else: return None
        elif term1 in freevars(term2):
            if  term1 == term2: return term1
            else: return None
        else:
            # Eliminate it.
            varmap.add(term1, term2)
            return term2

    # Tuple
    if isinstance(term1, Tuple):
        if isinstance(term2, Tuple):
            left = unify(term1.left, term2.left, varmap, depth)
            right = unify(term1.right, term2.right, varmap, depth)
            if left != None and right != None:
                return Tuple(left, right)
        
    # Abstraction
    if isinstance(term1, Abstr):
        if isinstance(term2, Abstr):
            x = Var()
            body1 = replace(term1.var, x, term1.body)
            body2 = replace(term2.var, x, term2.body)
            
            varmap.force(term1.var, x)
            varmap.force(term2.var, x)
            varmap.add(x, None)
            abstr = Abstr(x, unify(body1, body2, varmap, depth))
            return abstr
        if isinstance(term2, Appl):
            ## ***** There is a way to do this, but I haven't figured
            ## ***** it out yet.
            return None

    if isinstance(term1, Appl):
        if isinstance(term2, Appl):
            # Try unifying func and arg..
            old_varmap = varmap.copy()
            func = unify(term1.func, term2.func, varmap, depth)
            arg = unify(term1.arg, term2.arg, varmap, depth)
            if func != None and arg != None:
                return Appl(func, arg)
            varmap.copy_from(old_varmap)

            # If the functor of term1 is a variable, try instantiating 
            # it as a lambda term of some sort.
            if isinstance(term1.func, Var) and \
               varmap[term1.func] != None and \
               isinstance(term1.arg, Var):
                x = Var()
                body = replace(term1.arg, x, term2)
                # I need some sort of check here!!
                abstr = Abstr(x, body)
                varmap.add(x, None)
                varmap.add(term1.func, abstr)
                return term2
            
    if isinstance(term1, Const):
        if term1 == term2: return term1
        else: return None

    return None
                
def replace(oldval, newval, term):
    "Replace all occurances of oldval with newval in term"
    if term == oldval:
        return newval
    elif isinstance(term, Appl):
        return Appl(replace(oldval, newval, term.func),\
                    replace(oldval, newval, term.arg))
    elif isinstance(term, Abstr):
        if (oldval == term.var):
            return term
        else:
            return Abstr(term.val, replace(oldval, newval, term.body))
    elif isinstance(term, Tuple):
        return Tuple(replace(oldval, newval, term.left),
                     replace(oldval, newval, term.right))
    else:
        return term

def union(lst1, lst2):
    lst = lst1[:]
    for elt in lst2:
        if elt not in lst:
            lst.append(elt)
    return lst
    
def freevars(term):
    if isinstance(term, Var):
        return [term]
    elif isinstance(term, Appl):
        return union(freevars(term.func), freevars(term.arg))
    elif isinstance(term, Abstr):
        return [var for var in freevars(term.body) if var != term.var]
    elif isinstance(term, Tuple):
        return union(freevars(term.left), freevars(term.right))
    else:
        return []

def vars(term):
    if isinstance(term, Var):
        return [term]
    elif isinstance(term, Appl):
        return union(vars(term.func), vars(term.arg))
    elif isinstance(term, Abstr):
        return union(vars(term.body), [term.var])
    elif isinstance(term, Tuple):
        return union(vars(term.left), vars(term.right))
    else:
        return []

def boundvars(term):
    free = freevars(term)
    return [var for var in vars(term) if var not in free]
    
def reduce(term):
    if isinstance(term, Var) or isinstance(term, Const):
        return term

    if isinstance(term, Tuple):
        return Tuple(reduce(term.left), reduce(term.right))

    if isinstance(term, Appl):
        # Reduce the function and argument
        func = reduce(term.func)
        arg = reduce(term.arg)

        if isinstance(func, Abstr):
            return reduce(replace(func.var, arg, func.body))
        else:
            return Appl(func, arg)

    if isinstance(term, Abstr):
        # Reduce the body
        var = term.var
        body = reduce(term.body)

        if isinstance(body, Appl) and \
           body.arg == var and \
           var not in freevars(body.func):
            return body.func
        else:
            return Abstr(var, body)

# Strip outermost parens from a string..  inefficient, but simple :)
def strip_parens(str):
    if len(str) < 2 or str[0] != '(' or str[-1] != ')': return str

    depth = 0
    for c in str[:-1]:
        if c == '(': depth += 1
        if c == ')': depth -= 1
        if depth == 0: return str

    return strip_parens(str[1:-1])

def extract_tuple(str):
    if str[0] != '<' or str[-1] != '>': return None
    comma = None
    depth = 1
    for i in range(1, len(str)-1):
        if str[i] in '(<': depth += 1
        if str[i] in ')>': depth -= 1
        if depth == 1 and str[i] == ',':
            if comma == None: comma = i
            else: raise ValueError('bad tuple')
        if depth == 0: return None
    if comma == None: raise ValueError('bad tuple', str)
    return (str[1:comma], str[comma+1:-1])


# Maps str -> lambda term.
# Vars should start with '?'
def parse_term(str, varmap=None):
    if varmap == None: varmap = {}

    str = strip_parens(str.strip())

    # Abstractions with numbered vars
    abstr = re.match(r'\\\?([^\.]+)\.(.*)', str)
    if abstr:
        (varname, body) = abstr.groups()
        var = Var()
        varmap[varname]=var
        return Abstr(var, parse_term(body, varmap))

    # Tuple
    tuple = extract_tuple(str)
    if tuple:
        return Tuple(parse_term(tuple[0], varmap), \
                     parse_term(tuple[1], varmap))
    
    # Application
    if '(' in str:
        depth = 0
        for i in range(len(str)):
            if str[i] in '(<':
                if depth == 0 and i > 0: break
                else: depth += 1
            if str[i] in ')>':
                depth -= 1
        func = parse_term(str[:i], varmap)
        arg = parse_term(str[i:], varmap)
        return Appl(func, arg)
    
    # Variable
    var = re.match(r'\?(.*)', str)
    if var:
        varname = var.groups()[0]
        if varmap.has_key(varname):
            return varmap[varname]
        else:
            var = Var()
            varmap[varname] = var
            return var

    # Constant
    return Const(str)
        
def test():
    x = Var()
    y = Var()
    z = Var()
    c = Const('c')
    
    f1 = Appl(Abstr(x, Appl(x, c)), z)
    f2 = Appl(Abstr(x, Appl(c, x)), z)
    f3 = Abstr(x, Appl(c, x))
    f4 = Abstr(y, Appl(c, y))
    
    print f1, '=>', reduce(f1)
    print f2, '=>', reduce(f2)
    print f3, '=>', reduce(f3)

    print f1.pp()
    print f2.pp()
    print f3.pp()

    print
    print unify(x, y)
    print unify(x, c)
    print unify(x, f1)
    print unify(f3, f4)
    print unify(Abstr(x,Appl(x,x)), Abstr(y,Appl(y,y)))

    print parse_term('<(\?var.<const,const2>(?var))(?other_var),?x>').pp()

    reduce(parse_term('<a,b>'))
    
if __name__ == '__main__':
    test()


########NEW FILE########
__FILENAME__ = typedterm
#
# type.py
#
# Edward Loper
# Created [12/10/00 01:58 PM]
# $Id$
#

"""CG-style types"""

import types
from term import *

#####################################
# TYPEDTERM
#####################################

class TypedTerm:
    def __init__(self, term, type):
        # Check types, because we're paranoid.
        if not isinstance(term, Term) or \
           not isinstance(type, Type):
            raise TypeError('Expected Term, Type arguments', term, type)
        
        self.term = term
        self.type = type

    def __repr__(self):
        return `self.term`+': '+`self.type`

    def pp(self, pp_varmap=None):
        return self.term.pp(pp_varmap)+': '+`self.type`

    def to_latex(self, pp_varmap=None):
        term = self.term.to_latex(pp_varmap)
        type = `self.type`
        type = re.sub(r'\\', r'$\\backslash$', type)
        type = re.sub(r'\*', r'$\\cdot$', type)
        return term+': \\textrm{'+type+'}'

    def unify(self, other, varmap):
        if not isinstance(other, TypedTerm):
            raise TypeError('Expected TypedTerm')
        if self.type != other.type:
            raise ValueError("Can't unify -- types don't match", \
                             self.type, other.type)
        term = unify(self.term, other.term, varmap)
        if term == None:
            return None
        else:
            return TypedTerm(term, self.type)

    def simplify(self, varmap):
        return TypedTerm(reduce(simplify(self.term, varmap)), self.type) 

#####################################
# TYPES
#####################################
    
class Type:
    def __init__(self):
        raise TypeError("Type is an abstract class")

class LSlash(Type):
    # arg is the type of the expected argument
    # result is the resultant type.
    def __init__(self, arg, result):
        self.arg = arg
        self.result = result
        if not isinstance(arg, Type) or not isinstance(result, Type):
            raise TypeError('Expected Type arguments')
    def __repr__(self):
        if isinstance(self.result, RSlash) or \
           isinstance(self.result, LSlash):
            right = '('+`self.result`+')'
        else: right = `self.result`
        if isinstance(self.arg, RSlash):
            left = '('+`self.arg`+')'
        else: left = `self.arg`
        return left + '\\' + right
    def __cmp__(self, other):
        if isinstance(other, LSlash) and  self.arg == other.arg and \
           self.result == other.result:
            return 0
        else:
            return -1

class RSlash(Type):
    # arg is the type of the expected argument
    # result is the resultant type.
    def __init__(self, result, arg):
        self.arg = arg
        self.result = result
        if not isinstance(arg, Type) or not isinstance(result, Type):
            raise TypeError('Expected Type arguments')
    def __repr__(self):
        if isinstance(self.result, RSlash):
            left = '('+`self.result`+')'
        else: left = `self.result`
        return left + '/' + `self.arg`
    
        #return '('+`self.result`+'/'+`self.arg`+')'
        if isinstance(self.arg, LSlash):
            return `self.result`+'/('+`self.arg`+')'
        else:
            return `self.result`+'/'+`self.arg`
    def __cmp__(self, other):
        if isinstance(other, RSlash) and  self.arg == other.arg and \
           self.result == other.result:
            return 0
        else:
            return -1
            
class BaseType(Type):
    def __init__(self, name):
        if type(name) != types.StringType:
            raise TypeError("Expected a string name")
        self.name = name
    def __repr__(self):
        return self.name
    def __cmp__(self, other):
        if isinstance(other,BaseType) and self.name == other.name:
            return 0
        else:
            return -1
        
class Dot(Type):
    def __init__(self, left, right):
        self.right = right
        self.left = left
        if not isinstance(right, Type) or not isinstance(left, Type):
            raise TypeError('Expected Type arguments')
    def __repr__(self):
        return '('+`self.left`+'*'+`self.right`+')'
    def __cmp__(self, other):
        if isinstance(other, Dot) and  self.left == other.left and \
           self.right == other.right:
            return 0
        else:
            return -1

# Strip outermost parens from a string..  inefficient, but simple :)
def strip_parens(str):
    if len(str) < 2 or str[0] != '(' or str[-1] != ')': return str

    depth = 0
    for c in str[:-1]:
        if c == '(': depth += 1
        if c == ')': depth -= 1
        if depth == 0: return str

    return strip_parens(str[1:-1])
        
def parse_type(str):
    """parse(str)
    Parse a type string.  Use the order-of-operations specified in
    Carpenter.  An example input string is 'A/B\(C\(D*E))'"""  
    str = strip_parens(str.strip())

    # Find all the top-level operators
    ops = [('BEGIN', -1)]
    depth = 0
    for i in range(0, len(str)):
        if str[i] == '(': depth += 1
        if str[i] == ')': depth -= 1
        if depth == 0 and str[i] in '/\\*':
            ops.append((str[i], i))
    ops.append(('END', len(str)+1))

    # Base type?
    if len(ops) == 2:
        return BaseType(str)

    # Get the segments, and parse them
    segments = []
    for i in range(len(ops)-1):
        seg_type = parse_type(str[ops[i][1]+1:ops[i+1][1]])
        segments.append(seg_type)

    # Dot binds most strongly, left-to-right
    i = 1
    while i < len(ops)-1:
        if ops[i][0] == '*':
            segments[i-1] = Dot(segments[i-1], segments[i])
            segments.remove(segments[i])
            ops.remove(ops[i])
        else: i += 1
        
    # Then left slashes, right-to-left.
    i = len(ops)-2
    while i > 0:
        if ops[i][0] == '\\':
            segments[i-1] = LSlash(segments[i-1], segments[i])
            segments.remove(segments[i])
            ops.remove(ops[i])
        else: i -= 1

    # Then right slashes, left-to-right.
    i = 1
    while i < len(ops)-1:
        if ops[i][0] == '/':
            segments[i-1] = RSlash(segments[i-1], segments[i])
            segments.remove(segments[i])
            ops.remove(ops[i])
        else: i += 1

    if len(segments) != 1:
        print 'Ouch!!', segments, ops

    return segments[0]
        
def test():
    A = BaseType('A')
    B = BaseType('B')
    n = BaseType('n')
    np = BaseType('np')
    s = BaseType('s')
    det = RSlash(np, n)
    vp = LSlash(np, s)
    v2 = RSlash(vp, np)
    AB = Dot(A, B)
    print v2
    print AB
    print LSlash(AB, v2)
    print Dot(v2, AB)

    print parse_type('A / B')
    print parse_type('A \\ B')
    print parse_type('A / B / C')
    print parse_type('A * B')
    print parse_type('A \\ B \\ C')
    print parse_type('A \\ (B / C)')
    print parse_type('(A / B) \\ C')
    print parse_type('(A / B) \\ C')

########NEW FILE########
__FILENAME__ = error
# 9xx : file io error
ERR_TDF_IMPORT = 900
ERR_TDF_EXPORT = 901
ERR_TRANS_IMPORT = 910
ERR_TRANS_EXPORT = 911
ERR_TYP_IMPORT = 920
ERR_TXT_EXPORT = 931

ERRORS = {
    900:"tdf file import error",
    901:"tdf file export error",
    910:"transcriber file import error",
    911:"transcriber file export error",
    920:"typ file import error",
    931:"cts-style txt file export error",
    }

class Error:
    def __init__(self, errno, msg=None):
        if errno in ERRORS:
            self.errno = errno
        else:
            self.errno = None
        self.msg = msg

    def errstr(self):
        if self.errno in ERRORS:
            return ERRORS[self.errno]
        else:
            return "Unknown error"

    def __str__(self):
        if self.msg:
            return self.msg
        elif self.errno in ERRORS:
            return ERRORS[self.errno]
        else:
            return "Unknown error"
        

########NEW FILE########
__FILENAME__ = myaccel
from qt import Qt

__all__ = ["AccelKeyHandler"]

class AccelKeyHandler:
    """ Per-widget accel key handler mixin.

    While QAccel permits only application level key-bindings,
    AcceKeyHandler allows to define key-bindings on widget level.
    This is useful when the action of the key binding should
    affect only the currently focused widget.

    To use it, add AccelKeyHandler to the bases of the class that you
    are deriving from another Qt widget.  Name the actions to be
    associated with key bindings.  Then, implement methods that
    take the actions that you just have named.  The name of these
    methods are the action names prefixed with "accel_".  Finally,
    call the setKeyBindings method with a dictionary of key sequences
    and action names.  The following code snippet illustrate the usage::

        class MyWidget(QWidget,AccelKeyHandler):
            def __init__(self, parent=None):
                QWidget.__init__(self, parent)
                self.setKeyBindings({"Ctrl+A":"myAction"})
                ...
            
            def accel_myAction(self):
                ...

    The values of the key binding dictionary passed to setKeyBindings
    can also be callables.  For example, the following code has the
    same effect as the one above:

        class MyWidget(QWidget,AccelKeyHandler):
            def __init__(self, parent=None):
                QWidget.__init__(self, parent)
                self.setKeyBindings({"Ctrl+A":self.accel_myAction})
                ...
            
            def accel_myAction(self):
                ...
                
    """

    KeyAliases = {
        'Ctrl':'Control',
        'Del':'Delete',
        'Ins':'Insert'
        }

    def setKeyBindings(self, keyBindings):
        """ Set key-bindings for the widget.

        Key-bindings are specified by a dictionary of key sequence
        string and the action name.

        Key sequence is a list of sub-sequences delimited by comma (,).
        A sub sequence consists of a key name optionally prefixed by
        modifier key names. Some examples are::

          "K"
          "Ctrl+K"
          "Ctrl+Alt+K"
          "Alt+X,Ctrl+K"

        @type keyBindings: dict
        @param keyBindings: Key-bindings specified by a dictionary of
        key sequences and action names.

        """
        bindings = {}
        for keyseq,binding in keyBindings.items():
            seq = []
            for subkeyseq in keyseq.split(','):
                a = []
                for key in subkeyseq.split('+'):
                    key = key.strip()
                    key = key[0].upper() + key[1:].lower()
                    if key in self.KeyAliases:
                        key = self.KeyAliases[key]
                    a.append(key)
                state = Qt.NoButton
                for key in a[:-1]:
                    state |= eval("Qt.%sButton" % key)
                seq.append((state,eval("Qt.Key_%s"%a[-1])))

            b = bindings
            for e in seq[:-1]:
                if e in b:
                    b = b[e]
                else:
                    b[e] = {}
                    b = b[e]
            b[seq[-1]] = self.translateToBindingName(binding)
            
        self.AKH_keyBindings = bindings
        self.AKH_keyBindingsWaiting = {}

    def keyPressEvent(self, e):
        """ This implements the host class' keyPressEvent method.

        If the key event is a part of a key sequence that is set by
        setKeyBindings call, the event will be processed and accepted.
        Otherwise, the keyPressEvent method of the host class' base
        class will be called in turn.
        
        If the host class explicitly re-implements the keyPressEvent,
        this method will be overridden by it.

        @param e: Keyboard Event.
        @type e: QKeyEvent
        """
        if self.processKeyPressEvent(e):
            e.accept()
        elif hasattr(self, "_keyHandlerBase"):
            self._keyHandlerBase.keyPressEvent(self,e)
        else:
            c = list(self.__class__.__bases__)
            while AccelKeyHandler not in c:
                d = []
                for x in c:
                    d += list(x.__bases__)
                c = d
            self._keyHandlerBase = c[0]
            self._keyHandlerBase.keyPressEvent(self,e)
            #self.__class__.__base__.keyPressEvent(self,e)

    def processKeyPressEvent(self, e):
        """ Process the current keyboard event.

        If the key event is a part of a key sequence that is set by
        setKeyBindings call, the event will be processed and True
        will be returned. Otherwise, False will be returned.
        
        @param e: Keyboard Event.
        @type e: QKeyEvent
        @return: True if the event if processed, False if the event is
        rejected.
        """
        
        event = (e.state(),e.key())
        if event in self.AKH_keyBindingsWaiting:
            found = self.AKH_keyBindingsWaiting[event]
        elif event in self.AKH_keyBindings:
            found = self.AKH_keyBindings[event]
        else:
            key = event[1]
            if key!=Qt.Key_Control and key!=Qt.Key_Alt and key!=Qt.Key_Shift:
                self.AKH_keyBindingsWaiting = {}
            return False
        if type(found) == dict:
            self.AKH_keyBindingsWaiting = found
        else:
            found()
        return True

    def translateToBindingName(self, action):
        """ Translates the action name to the bound method name.

        @param action: action name (str) or a callable
        @type actionName: str/callable
        @return: method bound to the action if action is a string,
        or action itself if it is a callable
        @rtype: callable
        """
        if type(action) == str:
            return eval("self.accel_%s" % action)
        else:
            return action


########NEW FILE########
__FILENAME__ = table
from tableio import TableIo
import bisect

__all__ = ['TableModel']

class TableModel(TableIo):
    
    class TableRow:
        def __init__(self, tab, iv=[]):
            """
            @param tab: TableModel
            @param iv: initial value
            """
            self.tab = tab
            self.data = list(iv)
            
        def __len__(self):
            return len(self.data)
        def __getitem__(self,i):
            if type(i) == str:
                i = self.tab.getColumnIndex(i)
            return self.data[i]
        def __setitem__(self,i,v):
            if type(i) == str:
                i = self.getColumnIndex(i)
            self.data[i] = self.transformValue(i, v)
        def __str__(self):
            return str(self.data)
        def getColumnIndex(self, i):
            return self.tab.getColumnIndex(i)
        def getColumnName(self,i):
            return self.tab.getColumnName(i)
        def getColumnType(self,i):
            return self.tab.getColumnType(i)
        def getColumnHeader(self,i):
            return self.tab.getColumnHeader(i)
        def transformValue(self, i, v):
            return self.tab.transformValue(i,v)
        def toList(self):
            return self.data[:]
    
    def __init__(self, header):
        self.header = header
        self.str2col = {}
        self.colnum = 0
        for i,(h,t) in enumerate(header):
            self.str2col[h] = i
        self.table = []
        self.metadata = {}

    def setColumnHeader(self, colIdx, colName, colType):
        """ Sets the header for one column.

        @param colIdx: Column index for which the given header
        information will be set.
        @type colIdx: int
        @param colName: Column name.
        @type colName: string
        @param colType: Data type of the values in the column.
        @type colType: type
        """
        del self.str2col[self.header[col][0]]
        self.header[col] = header
        self.str2col[header[0]] = col

    def insertRow(self,i=None,row=None):
        """ Inserts a row at a given index.  If the row size is less than
        that of header, None is padded.  If the row size if larger than
        that of header, the row will be truncated.

        @param i: Row index. New row is inserted before the i-th row,
        thus the new row become the i-th row after insertion.  If i is
        None or larger than the last index, the new row will be
        appended at the end of the table.
        @type i: int
        @param row: The new row to be inserted.  The size of the row
        must match the width of the table, and the value of each
        column must match  Otherwise, nothing happens
        and False will be returned.  If row is None, a new row filled
        with Nones is inserted.
        @type row: list
        """
        if i is None: i = len(self.table)
        if row is None:
            row = [None for x in self.header]
        else:
            n = len(row)
            m = len(self.header)
            if n < m:
                row += [None] * (m-n)
            elif n > m:
                del row[m:]
        self.table.insert(i,self.TableRow(self,row))
        return True
    
    def insertColumn(self,i=None,col=None):
        if i is None: i = len(self.header)
        if col is None:
            s = "col%d" % self.colnum
            self.colnum += 1
            self.header.insert(i,(s,str))
            self.str2col[s] = i
            for row in self.table:
                row.data.insert(i,None)
        elif len(col)-1 != len(self.table):
            return False
        else:
            self.header.insert(i,col[0])
            self.str2col[col[0][0]] = i
            for r,row in enumerate(self.table):
                row.data.insert(i,col[r+1])
        return True
    
    def takeRow(self,i):
        row = self.table[i]
        self.table.remove(row)
        return row
    
    def takeColumn(self,i):
        col = [self.header[i]]
        del self.str2col[self.header[i][0]]
        del self.header[i]
        for row in self.table:
            col.append(row[i])
            del row.data[i]
        return col

    def sort(self, *args):
        L = list(range(len(self.header)))
        for i in args:
            if type(i) == str:
                i = self.getColumnIndex(i)
            L.remove(i)
        L = list(args) + L

        def sf(a,b):
            for c in L:
                if a[c] < b[c]:
                    return -1
                elif a[c] > b[c]:
                    return 1
            return 0

        self.table.sort(sf)
        
    def __getitem__(self, i):
        return self.table[i]

    def __len__(self):
        return len(self.table)

    def transformValue(self, col, val):
        if val is None: return None
        try:
            return self.header[col][1](val)
        except TypeError:
            return None

    def getColumnIndex(self, col):
        return self.str2col[col]

    def toList(self):
        return [row.toList() for row in self.table]

    def getColumnName(self, col):
        return self.header[col][0]

    def getColumnType(self, col):
        return self.header[col][1]

    def getColumnHeader(self, col):
        return self.header[col]

    def bisect_left(self, col, val):
        """
        Assume that the column col is sorted.
        """
        if type(col) != int:
            col = self.str2col[col]
        return bisect.bisect_left(map(lambda x:x[col],self.table),val)

    def bisect_right(self, col, val):
        """
        Assume that the column col is sorted.
        """
        if type(col) != int:
            col = self.str2col[col]
        return bisect.bisect_right(map(lambda x:x[col],self.table),val)

    def setMetadata(self, nam, val):
        if type(val) != str:
            val = str(val)
        self.metadata[nam] = val

    def getMetadata(self, nam, evl=False):
        if evl:
            return eval(self.metadata[nam])
        else:
            return self.metadata[nam]

if __name__ == "__main__":
    tab = TableModel([('a',str),('b',int),('c',int)])
    tab.insertRow(0)
    tab.insertRow(1)
    tab[0][0] = "apple"
    tab[0][1] = 2
    tab[0][2] = 3
    tab[1][0] = "orange"
    tab[1][1] = 5
    tab[1][2] = 3
    tab.printTable()

    print

    tab.insertColumn(1,["extra",10,9])
    tab.printTable()

    print

    c = tab.takeColumn(1)
    tab.insertColumn(3,c)
    tab.printTable()

    print

    r = tab.takeRow(0)
    tab.insertRow(1,r)
    tab.printTable()

    print
    
    tab.sort(2,3)
    tab.printTable()


########NEW FILE########
__FILENAME__ = tableedit_qtable
from qttable import QTable
from qt import SIGNAL,PYSIGNAL

class TableEdit(QTable):
    def __init__(self, parent=None):
        QTable.__init__(self, parent)
        self.data = None

    def setData(self, data):
        self.removeColumns(range(self.numCols()))
        self.removeRows(range(self.numRows()))

        self.setNumCols(len(data.header))
        for j,(h,t) in enumerate(data.header):
            self.horizontalHeader().setLabel(j,h)
        self.setNumRows(len(data))
        for i,row in enumerate(data):
            for j,h in enumerate(row):
                if h is not None:
                    if type(h)==str or type(h)==unicode:
                        self.setText(i,j,h)
                    else:
                        self.setText(i,j,str(h))
        for j,_ in enumerate(data.header):
            self.adjustColumn(j)
        if data != self.data:
            for sig in ('setHeader','cellChanged','insertRow','insertColumn','takeRow','takeColumn','sort'):
                self.connect(data.emitter,PYSIGNAL(sig),eval("self._%s"%sig))
            self.connect(self,SIGNAL("valueChanged(int,int)"),self.__cellChanged)
            self.data = data
            
    # incoming; model-to-gui
    def _setHeader(self, col, header):
        self.horizontalHeader().setLabel(col,header[0])
    def _cellChanged(self, i, j, val):
        if val is None:
            val = ''
        self.disconnect(self,SIGNAL("valueChanged(int,int)"),self.__cellChanged)
        self.setText(i,j,unicode(val))
        self.connect(self,SIGNAL("valueChanged(int,int)"),self.__cellChanged)

    def _insertRow(self, i, row):
        self.insertRows(i)
        if row is not None:
            for j,c in enumerate(row):
                if c is not None:
                    self.setText(i,j,str(c))
    def _insertColumn(self, j, col):
        self.insertColumns(j)
        if col is not None:
            self.horizontalHeader().setLabel(j,col[0][0])
            for i,c in enumerate(col[1:]):
                if c is not None:
                    self.setText(i,j,str(c))
    def _takeRow(self, i, r):
        self.removeRow(i)
    def _takeColumn(self, j):
        self.removeColumn(j)
    def _sort(self):
        self.setData(self.data)

    # outgoing; gui-to-model
    def __cellChanged(self, row, col):
        self.disconnect(self,SIGNAL("valueChanged(int,int)"),self.__cellChanged)
        self.disconnect(self.data.emitter,PYSIGNAL("cellChanged"),self._cellChanged)
        self.data[row][col] = self.item(row,col).text()
        self.setText(row,col,self.data[row][col])
        self.connect(self,SIGNAL("valueChanged(int,int)"),self.__cellChanged)
        self.connect(self.data.emitter,PYSIGNAL("cellChanged"),self._cellChanged)
        
if __name__ == '__main__':
    import qt
    from table_qt import TableModel

    class Demo(qt.QVBox):
        def __init__(self):
            qt.QVBox.__init__(self)
            self.b1 = qt.QPushButton('reset / load table',self)
            self.connect(self.b1,qt.SIGNAL("clicked()"),self.action)
            self.tab = TableEdit(self)
            self.stage = 0
            self.b2 = qt.QPushButton('print table at console',self)
            self.connect(self.b2,qt.SIGNAL("clicked()"),self.printTable)

        def printTable(self):
            self.data.printTable()
            
        def action(self):
            if self.stage == 0:
                t = [[('start',float),('end',float),('ch',str),('transcript',str)],
                     [1.23,2.34,'A','hello'],
                     [2.45,2.67,'B','hi'],
                     [2.88,3.09,'A','how are you']]
                self.data = TableModel.importList(t)
                self.tab.setData(self.data)
                self.stage = 1
                self.b1.setText('add row')
            elif self.stage == 1:
                self.data.insertRow(len(self.data))
                self.stage = 2
                self.b1.setText('take row 4')
            elif self.stage == 2:
                self.tmprow = self.data.takeRow(3)
                self.stage = 3
                self.b1.setText('insert row at the top')
            elif self.stage == 3:
                self.data.insertRow(0,self.tmprow)
                self.stage = 4
                self.b1.setText('sort by start')
            elif self.stage == 4:
                self.data.sort()
                self.stage = 5
                self.b1.setText('add column at the begining')
            elif self.stage == 5:
                self.data.insertColumn(0)
                self.data.setHeader(0,('review',str))
                self.stage = 6
                self.b1.setText('take review column')
            elif self.stage == 6:
                self.tmpcol = self.data.takeColumn(0)
                self.stage = 7
                self.b1.setText('insert the column before ch column')
            elif self.stage == 7:
                self.data.insertColumn(2,self.tmpcol)
                self.stage = 8
                self.b1.setText('change start time of row 1 to 9.99')
            elif self.stage == 8:
                self.data[0][0] = 9.99
                self.stage = 0
                self.b1.setText('reset / load table')
                

    app = qt.QApplication([])
    w = Demo()
    app.setMainWidget(w)
    w.show()
    app.exec_loop()

########NEW FILE########
__FILENAME__ = tableio

#
# ChangeLogs:
# $Log: tableio.py,v $
# Revision 1.9  2006/06/27 19:03:28  haejoong
# tdf reader now handles different formats of newline characters
#
# Revision 1.8  2006/04/12 14:55:24  haejoong
# fixes for error handling
#
# Revision 1.7  2006/01/26 15:46:46  haejoong
# *** empty log message ***
#
# Revision 1.6  2006/01/23 16:32:29  haejoong
# improved exception handling
#
# Revision 1.5  2006/01/19 17:53:17  haejoong
# added some error handling for importTdf
#
# Revision 1.4  2005/12/15 19:08:40  haejoong
# added missing "
#
# Revision 1.3  2005/12/15 19:05:55  haejoong
# added error handling for TableIo.importTdf
#
#

import codecs
import re
from error import *

__all__ = ['TableIo']

version = "$Revision: 1.9 $"

class TableIo:
    def printTable(self):
        size = [len(str(x)) for x,t in self.header]
        for row in self.table:
            for i,c in enumerate(row):
                if type(c)==str or type(c)==unicode:
                    n = len(c)
                else:
                    n = len(str(c))
                if n > size[i]:
                    size[i] = n

        def printRow(row,bar=True):
            s = ""
            for i,c in enumerate(row):
                if type(c) == int or type(c) == float:
                    s += "%%%ds|" % size[i] % str(c)
                else:
                    s += "%%-%ds|" % size[i] % c
            print s[:-1] 

        printRow([s for s,t in self.header])
        for row in self.table:
            printRow(row)
            
    def importList(cls, L):
        data = cls(L[0])
        for i,row in enumerate(L[1:]):
            data.insertRow(i,row)
        data.resetUndoStack()
        return data

    importList = classmethod(importList)

    def exportTdf(self, filename):
        try:
            _,_,_,writer = codecs.lookup('utf-8')
            f = writer(file(filename,'w'))
            f.write("\t".join([a[0]+';'+a[1].__name__
                               for a in self.header]) + "\n")
            for item in self.metadata.items():
                f.write(";;MM %s\t%s\n" % item)
            for row in self.table:
                for c in row[:-1]:
                    if c is None:
                        f.write("\t")
                    else:
                        t = type(c)
                        if t==str or t==unicode:
                            f.write(c+"\t")
                        else:
                            f.write(str(c)+"\t")
                if row[-1] is None:
                    f.write("\n")
                else:
                    t = type(row[-1])
                    if t==str or t==unicode:
                        f.write(row[-1]+"\n")
                    else:
                        f.write(str(row[-1])+"\n")
        except IOError, e:
            raise Error(ERR_TDF_EXPORT, str(e))

    def importTdf(cls, filename):
        _,_,reader,_ = codecs.lookup('utf-8')
        try:
            f = reader(file(filename))
        except IOError, e:
            raise Error(ERR_TDF_IMPORT, e)
        head = []
        for h in f.readline().rstrip("\r\n").split("\t"):
            try:
                a,b = h.split(';')
            except ValueError:
                raise Error(ERR_TDF_IMPORT, "invalid header")
            head.append((a,eval(b)))
        tab = cls(head)
        l = f.readline().rstrip('\r\n')
        lno = 2
        while l:
            if l[:2] != ';;': break
            if l[2:4] == 'MM':
                nam,val = re.split("\t+",l[4:].strip())
                tab.metadata[nam] = val
            l = f.readline().rstrip('\r\n')
            lno += 1
        while l:
            if l[:2] != ';;':
                row = []
                try:
                    for i,cell in enumerate(l.rstrip("\n").split("\t")):
                        row.append(head[i][1](cell))
                except ValueError, e:
                    raise Error(ERR_TDF_IMPORT,
                                "[%d:%d] %s" % (lno,i,str(e)))
                except IndexError, e:
                    msg = "record has too many fields"
                    raise Error(ERR_TDF_IMPORT,
                                "[%d:%d] %s" % (lno,i,msg))
                tab.insertRow(None,row)
            l = f.readline().rstrip('\r\n')
            lno += 1
        tab.resetUndoStack()
        return tab

    importTdf = classmethod(importTdf)

########NEW FILE########
__FILENAME__ = tableproxy
from qt import QObject, PYSIGNAL

__all__ = ['getProxy']

class UndoStack:
    def __init__(self):
        self.reset()

    def reset(self):
        self._stack = []
        self._top = -1
        self._limit = -1
        
    def prev(self):
        if self._top >= 0:
            r = self._stack[self._top]
            self._top -= 1
            return r
        else:
            return None

    def next(self):
        if self._limit > self._top:
            self._top += 1
            return self._stack[self._top]
        else:
            return None
        
    def push(self, *args):
        self._top += 1
        if len(self._stack) == self._top:
            self._stack.append(args)
        else:
            self._stack[self._top] = args
        self._limit = self._top
        if self._limit >= 2000:
            del self._stack[0]
            self._limit -= 1
            self._top -= 1

    def status(self):
        return self._top+1, self._limit-self._top
    
class _TableRowProxy(object):
    """
    self.num is a row index in the table
    """
    def __setitem__(self,i,v):
        if type(i) == str:
            i = self.getColumnIndex(i)
        w = self[i]
        super(self.__class__,self).__setitem__(i,v)
        self.tab.emitter.emit(PYSIGNAL("cellChanged"),(self.num,i,self.data[i],w))
        if not self.tab.undoStackReadOnly:
            self.tab.undoStack.push("cellChanged",(self.num,i),(v,w))

    def select(self):
        self.tab.select(self.num)
        
class _TableModelProxy(object):
    def __init__(self, *args):
        self.super = super(self.__class__,self)
        self.super.__init__(*args)
        self.emitter = QObject()
        self.selection = None
        self.undoStack = UndoStack()
        self.undoStackReadOnly = False
        
    def setHeader(self, col, header):
        self.super.setHeader(col, header)
        self.emitter.emit(PYSIGNAL("setHeader"),(col,header))
        
    def insertRow(self,i=None,row=None):
        if i is None: i=len(self.table)
        if self.super.insertRow(i,row):
            for k in range(i,len(self.table)):
                self.table[k].num = k
            self.emitter.emit(PYSIGNAL("insertRow"),(i,self.table[i]))
            if not self.undoStackReadOnly:
                self.undoStack.push("insertRow",i,row)
            return True
        else:
            return False
        
    def insertColumn(self,i=None,col=None):
        if self.super.insertColumn(i,col):
            self.emitter.emit(PYSIGNAL("insertColumn"),(i,col))
            if type(i)==int and i <= self.selection:
                self.selection += 1
            return True
        else:
            return False
    
    def takeRow(self,i):
        r = self.super.takeRow(i)
        for k in range(i,len(self.table)):
            self.table[k].num = k
        if i == self.selection:
            self.selection = None
        self.emitter.emit(PYSIGNAL("takeRow"),(i,r))
        if not self.undoStackReadOnly:
            self.undoStack.push("takeRow",i,r)
        return r
    
    def takeColumn(self,i):
        c = self.super.takeColumn(i)
        self.emitter.emit(PYSIGNAL("takeColumn"),(i,))
        if i == self.selection:
            self.selection = None
        elif i < self.selection:
            self.selection -= 1
        return c
    
    def sort(self, *args):
        self.super.sort(*args)
        for i,row in enumerate(self.table):
            row.num = i
        self.emitter.emit(PYSIGNAL("sort"),())

    def select(self, sel):
        self.selection = sel
        self.emitter.emit(PYSIGNAL("select"),(sel,))

    def getSelection(self):
        return self.selection

    def resetUndoStack(self):
        self.undoStack.reset()
        
    def undo(self, n=1):
        for m in range(n):
            try:
                op, arg1, arg2 = self.undoStack.prev()
                #print "undo", op, arg1, arg2
                #print len(self.undoStack._stack)
            except TypeError:
                break
            self.undoStackReadOnly = True
            if op == 'insertRow':
                self.takeRow(arg1)
            elif op == 'takeRow':
                self.insertRow(arg1,arg2)
            elif op == 'cellChanged':
                i, j = arg1
                v, w = arg2
                self[i][j] = w
            self.undoStackReadOnly = False
    
    def redo(self, n=1):
        for m in range(n):
            try:
                op, arg1, arg2 = self.undoStack.next()
                #print "redo", op, arg1, arg2
                #print len(self.undoStack._stack)
            except TypeError:
                break
            self.undoStackReadOnly = True
            if op == 'insertRow':
                self.insertRow(arg1,arg2)
            elif op == 'takeRow':
                self.takeRow(arg1)
            elif op == 'cellChanged':
                i, j = arg1
                v, w = arg2
                self[i][j] = v
            self.undoStackReadOnly = False

    def undoStackStatus(self):
        return self.undoStack.status()
    
def getProxy(cls):
    name = "Proxy_TableRow"
    bases = (cls.TableRow,) + _TableRowProxy.__bases__
    dic = dict(_TableRowProxy.__dict__)
    rowModelClass = type(name, bases, dic)
    
    name = "Proxy_" + cls.__name__
    bases = (cls,) + _TableModelProxy.__bases__
    dic = dict(_TableModelProxy.__dict__)
    dic['TableRow'] = rowModelClass
    
    return type(name, bases, dic)


########NEW FILE########
__FILENAME__ = table_qt
import tableproxy
from table import TableModel

__all__ = ['TableModel']

TableModel = tableproxy.getProxy(TableModel)
        
if __name__ == "__main__":
    tab = TableModel([('a',str),('b',str),('c',str)])
    tab.insertRow(0)
    tab.insertRow(1)
    tab[0][0] = "apple"
    tab[0][1] = 2
    tab[0][2] = 3
    tab[1][0] = "orange"
    tab[1][1] = 5
    tab[1][2] = 3
    tab.printTable()

    print

    tab.insertColumn(1,[("extra",int),10,9])
    tab.printTable()

    print

    c = tab.takeColumn(1)
    tab.insertColumn(3,c)
    tab.printTable()

    print

    r = tab.takeRow(0)
    tab.insertRow(1,r)
    tab.printTable()

    print
    
    tab.sort(1,2)
    tab.printTable()


########NEW FILE########
__FILENAME__ = tree
from treeio import TreeIo

__all__ = ['TreeModel']

class TreeModel(TreeIo):
    def __init__(self):
        self.root = self
        self.parent = None
        self.leftSibling = None
        self.rightSibling = None
        self.children = []
        self.data = {}

    def dfs(self, func, *args):
        L = [self]
        while L:
            n = L[0]
            func(n, *args)
            L = n.children + L[1:]
                
    def prune(self):
        if self.root == self:
            return False
        if self.parent is not None:
            self.parent.children.remove(self)
        if self.leftSibling is not None:
            self.leftSibling.rightSibling = self.rightSibling
        if self.rightSibling is not None:
            self.rightSibling.leftSibling = self.leftSibling
        self.parent = None
        self.leftSibling = None
        self.rightSibling = None

        def f(t):t.root=self
        self.dfs(f)

        return True
    
    def splice(self):
        if self.parent is not None:
            i = self.parent.children.index(self)
            # error should be raised if i < 0
            self.parent.children.remove(self)
            if self.children:
                for c in self.children:
                    c.parent = self.parent
                    self.parent.children.insert(i,c)
                    i += 1
                if self.leftSibling:
                    self.leftSibling.rightSibling = self.children[0]
                    self.children[0].leftSibling = self.leftSibling
                if self.rightSibling:
                    self.rightSibling.leftSibling = self.children[-1]
                    self.children[-1].rightSibling = self.rightSibling
            else:
                if self.leftSibling is not None:
                    self.leftSibling.rightSibling = self.rightSibling
                if self.rightSibling is not None:
                    self.rightSibling.leftSibling = self.leftSibling
            self.parent = None
            self.leftSibling = None
            self.rightSibling = None
            self.children = []

            self.root = self

            return True
        else:
            return False
            
    def insertLeft(self,node):
        if self.root == self or self.root == node.root:
            return False
        
        if self.leftSibling:
            self.leftSibling.rightSibling = node
            node.leftSibling = self.leftSibling
        node.rightSibling = self
        node.parent = self.parent
        i = self.parent.children.index(self)
        self.parent.children.insert(i,node)

        def f(t):t.root=self.root
        node.dfs(f)

        return True
    
    def insertRight(self,node):
        if self.root == self or self.root == node.root:
            return False
        
        if self.rightSibling:
            self.rightSibling.leftSibling = node
            node.rightSibling = self.rightSibling
        node.leftSibling = self
        node.parent = self.parent
        i = self.parent.children.index(self)
        self.parent.children.insert(i+1,node)

        def f(t):t.root=self.root
        node.dfs(f)

        return True
    
    def attach(self,node):
        if self.root == node.root:
            return False
        
        node.parent = self
        if self.children:
            node.leftSibling = self.children[-1]
            self.children[-1].rightSibling = node
        node.rightSibling = None
        self.children.append(node)

        def f(t):t.root=self.root
        node.dfs(f)

        return True

    def isParentOf(self, n):
        return n.parent == self

    def isAncestorOf(self, n):
        p = n.parent
        while p:
            if p == self: return True
            p = p.parent
        return False

    def follows(self, n):
        L = [n]
        p = n.parent
        while p:
            L.append(p)
            p = p.parent
        p = self
        pp = None
        while p not in L:
            pp, p = p, p.parent
        if pp is None or p==n: return False
        i = L.index(p)
        ni = p.children.index(L[i-1])
        mi = p.children.index(pp)
        return ni < mi
    
if __name__ == "__main__":
    from nltk.tree import bracket_parse
    s = "(S (NP (N I)) (VP (VP (V saw) (NP (DT the) (N man))) (PP (P with) (NP (DT a) (N telescope)))))"
    t = bracket_parse(s)
    root = TreeModel.importNltkLiteTree(t)
    print root.treebankString("label")

########NEW FILE########
__FILENAME__ = treeedit_qlistview
from qt import QListView, QListViewItem, PYSIGNAL
from myaccel import AccelKeyHandler

__all__ = ['TreeEdit']
           
class TreeEditItem(QListViewItem):
    def __init__(self, *args):
        QListViewItem.__init__(self, *args)
        self.setOpen(True)
        for i in range(self.listView().columns()):
            self.setRenameEnabled(i,True)
            
    def attach(self, node):
        return self.treenode.attach(node.treenode)
    def insertLeft(self, node):
        return self.treenode.insertLeft(node.treenode)
    def insertRight(self, node):
        return self.treenode.insertRight(node.treenode)
    def prune(self):
        return self.treenode.prune()
    def splice(self):
        return self.treenode.splice()

    # mode -> gui
    def _attach(self,dst,src):
        c = dst.gui.firstChild()
        if c is None:
            dst.gui.insertItem(src.gui)
        else:
            while c.nextSibling():
                c = c.nextSibling()
            dst.gui.insertItem(src.gui)
            src.gui.moveItem(c)
        
    def _insertLeft(self,dst,src):
        p = dst.gui.parent()
        c = p.firstChild()
        p.insertItem(src.gui)
        if c != dst.gui:
            src.gui.moveItem(dst.gui)
            dst.gui.moveItem(src.gui)
        
    def _insertRight(self,dst,src):
        dst.gui.parent().insertItem(src.gui)
        src.gui.moveItem(dst.gui)
    
    def _prune(self,n):
        self.parent().takeItem(self)
        
    def _splice(self,n):
        p = self.parent()
        c = self.firstChild()
        M = []  # list of children
        while c:
            M.append(c)
            c = c.nextSibling()
        last = self
        for c in M:
            self.takeItem(c)
            p.insertItem(c)
            c.moveItem(last)
            last = c
        p.takeItem(self)

    def okRename(self, col):
        QListViewItem.okRename(self,col)
        f = self.listView().col2str[col]
        self.treenode.data[f] = self.text(col).ascii()
        

class TreeEdit(QListView,AccelKeyHandler):
    def __init__(self,parent=None):
        QListView.__init__(self,parent)
        self.data = None
        self.col2str = None
        self.setRootIsDecorated(True)
        self.setSorting(-1)
        self.clipBoard = None
        self.accelFilter = None
        self.keyBindingDescriptor = {
            "Ctrl+N":"new",
            "Ctrl+A":"attach",
            "Ctrl+I,Ctrl+L":"insertLeft",
            "Ctrl+I,Ctrl+R":"insertRight",
            "Ctrl+P":"prune",
            "Ctrl+S":"splice"
            }
        self.setKeyBindings(self.keyBindingDescriptor)
        
    def accel_new(self):
        if self.data is None: return
        n = self.data.__class__()
        x = [self] + [None] * len(self.col2str)
        item = apply(TreeEditItem,x)
        for sig in ("attach","insertLeft","insertRight","prune","splice"):
            n.connect(n,PYSIGNAL(sig),eval("item._%s"%sig))
        self.takeItem(item)
        item.treenode = n
        n.gui = item
        self.clipBoard = n
    def accel_attach(self):
        item = self.currentItem()
        if item and self.clipBoard is not None and \
           item.treenode.attach(self.clipBoard):
            self.clipBoard = None
    def accel_insertLeft(self):
        item = self.currentItem()
        if item and self.clipBoard is not None and \
           item.treenode.insertLeft(self.clipBoard):
            self.clipBoard = None
    def accel_insertRight(self):
        item = self.currentItem()
        if item and self.clipBoard is not None and \
           item.treenode.insertRight(self.clipBoard):
            self.clipBoard = None
    def accel_prune(self):
        item = self.currentItem()
        if item and item.treenode.prune():
            self.clipBoard = item.treenode
    def accel_splice(self):
        item = self.currentItem()
        if item and item.treenode.splice():
            self.clipBoard = item.treenode
        
    def setData(self, data, fields=[]):
        if data != data.root:
            return
        if self.data is not None:
            self.clear()
            for i in range(self.columns()):
                self.removeColumn(0)
        
        self.data = data
        self.col2str = {}
        for i,(f,v) in enumerate(fields):
            self.addColumn(v)
            self.col2str[i] = f

        L = [data]
        T = [self]
        while L:
            n = L.pop()
            if n is None:
                T.pop()
                continue
            c = n.children
            x = [T[-1],n.data[fields[0][0]]]
            for f,v in fields[1:]:
                x.append(str(n.data[f]))
            e = apply(TreeEditItem, x)
            for sig in ("attach","insertLeft","insertRight","prune","splice"):
                n.connect(n,PYSIGNAL(sig),eval("e._%s"%sig))
            e.treenode = n
            n.gui = e
            if c:
                T.append(e)
                L += [None] + c


    
if __name__ == "__main__":
    from tree_qt import TreeModel
    import qt

    class Demo(qt.QVBox):
        def __init__(self):
            qt.QVBox.__init__(self)
            self.button = qt.QPushButton('Reset',self)
            self.connect(self.button,qt.SIGNAL("clicked()"),self.load)
            self.button2 = qt.QPushButton(self)
            self.connect(self.button2,qt.SIGNAL("clicked()"),self.change)
            hbox = qt.QGrid(2, self)
            lbl1 = qt.QLabel("Edit Panel",hbox)
            lbl2 = qt.QLabel("Clipboard",hbox)
            lbl1.setMargin(2)
            lbl1.setAlignment(lbl1.AlignCenter)
            lbl2.setAlignment(lbl1.AlignCenter)
            self.treeview = TreeEdit(hbox)
            self.clipboard = TreeEdit(hbox)
            self.root = None
            self.load()
            self.resize(400,450)

        def load(self):
            from nltk import bracket_parse
            s = "(S (NP (N I)) (VP1 (VP2 (V saw) (NP (ART the) (N man))) (PP (P with) (NP (ART a) (N telescope)))))"
            t = bracket_parse(s)
            self.root = TreeModel.importNltkLiteTree(t)
            self.treeview.setData(self.root,'label')
            self.vp1 = self.root.children[1]
            self.pp = self.vp1.children[1]
            self.vp2 = self.vp1.children[0]
            self.stage = 0
            self.button2.setText('splice VP1')

        def change(self):
            if self.stage == 0:
                self.vp1.splice()
                self.clipboard.setData(self.vp1,'label')
                self.button2.setText('prune PP')
                self.stage = 1
            elif self.stage == 1:
                self.pp.prune()
                self.clipboard.setData(self.pp,'label')
                self.button2.setText('attach PP to VP2')
                self.stage = 2
            elif self.stage == 2:
                self.vp2.attach(self.pp)
                self.clipboard.setData(self.vp2,'label')
                self.button2.setText('')
                self.stage = 3

    app = qt.QApplication([])
    w = Demo()
    app.setMainWidget(w)
    w.show()
    app.exec_loop()
    

########NEW FILE########
__FILENAME__ = treeio
import re

__all__ = ['TreeIo']

class TreeIo:
    def treebankString(self, p):
        L = [self]
        s = ''
        while L:
            n = L[0]
            if n is None:
                s += ')'
                L = L[1:]
                continue
            c = n.children
            if c:
                s += ' (' + unicode(n.data[p])
                L = c + [None] + L[1:]
            else:
                s += ' ' + unicode(n.data[p])
                L = L[1:]
        return s[1:]

    def importNltkLiteTree(cls, t):
        from nltk.tree import Tree as NltkTree

        L = [t]
        T = [cls()]
        while L:
            n = L[0]
            if n is None:
                L = L[1:]
                T.pop()
                continue
            
            x = cls()
            if type(n) == NltkTree:
                x.data['label'] = n.node
                T[-1].attach(x)
                T.append(x)
                L = n[:] + [None] + L[1:]
            else:
                x.data['label'] = n
                T[-1].attach(x)
                L = L[1:]

        root = T[0].children[0]
        root.prune()
        return root

    importNltkLiteTree = classmethod(importNltkLiteTree)

    def importTreebank(cls, lines):
        p = re.compile(r"\s*([^()]+)\s*")
        L = [cls()]
        for i,l in enumerate(lines):
            l = l.lstrip()
            while l:
                if l[0] == '(':
                    l = l[1:].lstrip()
                    args = []
                    m = p.match(l)
                    if m:
                        args = m.group(1).split()
                        l = l[m.span()[1]:]
                    n = len(args)
                    node = cls()
                    L[-1].attach(node)
                    if n > 0:
                        node.data['label'] = args[0]
                        if n > 1:
                            node1 = cls()
                            node1.data['label'] = args[1]
                            node.attach(node1)
                            if n > 2:
                                node.data['extra'] = args[2:]
                    else:
                        node.data['label'] = ''
                    L.append(node)
                elif l[0] == ')':
                    l = l[1:].lstrip()
                    L.pop()
                else:
                    node = cls()
                    node.data['label'] = 'garbage'
                    node.data['garbage'] = l
                    L[-1].attach(node)
                    break
                if len(L) == 1 and L[0].children:
                    t = L[0].children[-1]
                    t.prune()
                    yield t
        #res = L[0].children[:]
        #for c in res: c.prune()
        #return res

    importTreebank = classmethod(importTreebank)

    def exportLPathTable(self, TableModel, sid=0, tid=0):
        L = [self]     # dfs queue, None delimits nodes of different levels
        C = []         # stack storing information about latest parent node
        TAB = []
        left = 0
        depth = 0
        nodeid = 0
        while L:
            n = L[0]
            if n is None:
                # done with children; update parent's "right" index
                r = C.pop()
                r['right'] = left
                L = L[1:]
                depth -= 1
                continue
            else:
                r = {}

            # Make sure all the node's application-specific attributes are recorded.
            r['attributes'] = []
            if n.data != None:
                for attr, value in n.data.iteritems():
                    if attr == 'label':
                        r['name'] = value
                    else:
                        r['attributes'].append(('@' + attr, value))

            r['left'] = left
            if n.children:
                nodeid += 1
                r['depth'] = depth
                if depth == 0:
                    r['pid'] = nodeid
                else:
                    r['pid'] = C[-1]['id']
                C.append(r)
                L = n.children + [None] + L[1:]
                depth += 1
                TAB.append(r)
            else:
                if depth == 0:
                    # this is one-node tree
                    nodeid += 1
                    r['depth'] = depth
                    r['pid'] = nodeid
                    TAB.append(r)
                else:
                    # Attributes from lexical nodes get pushed up one level.
                    # @label is a special case.
                    C[-1]['attributes'].append(('@label',r['name']))
                    for attr in r['attributes']:
                        C[-1]['attributes'].append(attr)
                left += 1
                L = L[1:]
            r['sid'] = sid
            r['tid'] = tid
            r['id'] = nodeid

        TAB2 = TableModel([("sid",int),("tid",int),("id",int),("pid",int),
                           ("left",int),("right",int),("depth",int),
                           ("type",str),("name",str),("value",str)])

        fields = ('sid','tid','id','pid','left','right','depth')
        for r in TAB:
            TAB2.insertRow()
            row = TAB2[-1]
            for f in fields:
                row[f] = r[f]
            row['type'] = 'syn'
            row['name'] = r['name']
            row['value'] = None
            for k,v in r['attributes']:
                TAB2.insertRow(None,row)
                row2 = TAB2[-1]
                row2['type'] = 'att'
                row2['name'] = k
                row2['value'] = v
        return TAB2

    def importLPathTable(cls, table):
        # copy "table" to "tab" to avoid changing "table"
        # "tab" is sorted later
        tab = table.__class__(table.header)
        for row in table:
            tab.insertRow(None,row)
            
        tab.sort('sid','tid','left','depth','value')
        T = [cls()]
        P = [tab[0]['pid']]
        for row in tab:
            if row['type'] == 'att':
                p = row['id']
                while P[-1] != p:
                    T.pop()
                    P.pop()
                f = row['name']
                v = row['value']
                if f == '@label':
                    node = cls()
                    node.data['label'] = v
                    T[-1].attach(node)
                else:
                    if f in T[-1].data:
                        T[-1].data[f].append(v)
                    else:
                        T[-1].data[f] = [v]
            else:
                p = row['pid']
                while P[-1] != p:
                    T.pop()
                    P.pop()
                node = cls()
                node.data['label'] = row['name']
                node.data['type'] = row['type']
                node.data['id'] = row['id']
                T[-1].attach(node)
                T.append(node)
                P.append(row['id'])
        t = T[0].children[0]
        t.prune()
        return t

    importLPathTable = classmethod(importLPathTable)


########NEW FILE########
__FILENAME__ = tree_qt
from qt import QObject, PYSIGNAL
from tree import TreeModel as PureTree

__all__ = ['TreeModel']

class TreeModel(PureTree, QObject):
    def __init__(self):
        PureTree.__init__(self)
        QObject.__init__(self)

    def attach(self,node):
        if PureTree.attach(self,node):
            self.emit(PYSIGNAL("attach"),(self,node,))
            return True
        else:
            return False
        
    def insertLeft(self,node):
        if PureTree.insertLeft(self,node):
            self.emit(PYSIGNAL("insertLeft"),(self,node,))
            return True
        else:
            return False
       
    def insertRight(self,node):
        if PureTree.insertRight(self,node):
            self.emit(PYSIGNAL("insertRight"),(self,node,))
            return True
        else:
            return False
        
    def prune(self):
        if PureTree.prune(self):
            self.emit(PYSIGNAL("prune"),(self,))
            return True
        else:
            return False
        
    def splice(self):
        if PureTree.splice(self):
            self.emit(PYSIGNAL("splice"),(self,))
            return True
        else:
            return False

########NEW FILE########
__FILENAME__ = axis
from qt import *
from qtcanvas import *
import math

def dxdy(w, x1, y1, x2, y2):
    if x1 > x2:
        x1, x2 = x2, x1
        y1, y2 = y2, y1
    a = x2-x1
    b = y2-y1
    c = math.sqrt(a*a + b*b)

    if c == 0:
        dx = dy = 0
    else:
        dx = b * w / c / 2.0
        dy = a * w / c / 2.0

    return dx,dy, x1,y1, x2,y2


class Axis(QCanvasPolygonalItem):
    # there should be no gap between two consecutive types
    HeadContinue = 0
    HeadBranch = 1
    HeadNegation = 2
    
    def __init__(self, canvas):
        QCanvasPolygonalItem.__init__(self, canvas)
        self.pen = QPen(Qt.red)
        self.setPen(self.pen)

        self.points = (0,0,0,0)
        self.area = QPointArray(4)
        self.area.setPoint(0,0,0)
        self.area.setPoint(1,1,0)
        self.area.setPoint(2,1,1)
        self.area.setPoint(3,0,1)

        self.headType = self.HeadContinue
        self.root = None
        self.target = None
        
   
    def areaPoints(self):
        return self.area
    
    def setPoints(self, x1,y1,x2,y2):
        """
        The line has an orientation; (x1,y1) is the start point and
        (x2,y2) is the end point.
        """
        self.points = (x1,y1,x2,y2)

        dx,dy, x1,y1, x2,y2 = dxdy(self.pen.width()+5.0, x1,y1,x2,y2)

        self.area.setPoint(0, x1 + dx, y1 - dy)
        self.area.setPoint(1, x1 - dx, y1 + dy)
        self.area.setPoint(2, x2 - dx, y2 + dy)
        self.area.setPoint(3, x2 + dx, y2 - dy)

    def lineWidth(self):
        return self.pen.width()

    def _drawBranchHead(self, painter):
        h = 5.0
        x1, y1, x2, y2 = self.points
        if x1 == x2:
            x = x1 - h
            if y2 > y1:
                y = y1
            else:
                y = y1 - 2.0 * h
        elif y1 == y2:
            if x2 > x1:
                x = x1
            else:
                x = x1 - 2.0 * h
            y = y1 - h
        else:
            xx = float(x2 - x1)
            yy = float(y2 - y1)
            r = abs(yy / xx)
            h1 = h / math.sqrt(2.0)
            x = x1 + h * xx / abs(xx) / math.sqrt(1 + r*r) - h1
            y = y1 + h * yy / abs(yy) / math.sqrt(1 + r*r) * r - h1
        p = QPen(self.pen)
        p.setWidth(1)
        p.setStyle(Qt.SolidLine)
        painter.setPen(p)
        painter.setBrush(QBrush(p.color()))
        painter.drawEllipse(x,y, 2*h, 2*h)

    def _drawNegationHead(self, painter):
        h = 5.0
        x1, y1, x2, y2 = self.points
        if x1 == x2:
            new_x1 = x1 - h
            new_x2 = x1 + h
            if y1 > y2:
                new_y1 = new_y2 = y1 - h
            else:
                new_y1 = new_y2 = y1 + h
        elif y1 == y2:
            if x1 > x2:
                new_x1 = new_x2 = x1 - h
            else:
                new_x1 = new_x2 = x1 + h
            new_y1 = y1 - h
            new_y2 = y1 + h
        else:
            xx = x2 - x1
            yy = y2 - y1
            sigx = xx / abs(xx)
            sigy = yy / abs(yy)
            r = abs(yy / xx)
            c = h / r
            a = math.sqrt(c*c + h*h) / (1+r*r)
            dx = sigx * a * r * r
            dy = sigy * a * r
            x11 = x1 + sigx * abs(dy)
            y11 = y1 + sigy * abs(dx)
            new_x1 = x11 + dx
            new_y1 = y11 - dy
            new_x2 = x11 - dx
            new_y2 = y11 + dy
        pen = QPen(self.pen)
        pen.setWidth(2)
        pen.setStyle(Qt.SolidLine)
        painter.setPen(pen)
        painter.drawLine(new_x1,new_y1,new_x2,new_y2)
    
    def drawLineHead(self, painter):
        if self.headType == self.HeadBranch:
            self._drawBranchHead(painter)
        elif self.headType == self.HeadNegation:
            self._drawNegationHead(painter)
            
    def drawShape(self, painter):
        apply(painter.drawLine, self.points)
        self.drawLineHead(painter)
        
    def toggleHeadType(self):
        self.headType = (self.headType + 1) % 3
        self.update()
        self.canvas().update()

    def setHeadType(self, typ):
        if typ not in (self.HeadContinue,
                       self.HeadBranch,
                       self.HeadNegation):
            return
        self.headType = typ
        self.update()
        self.canvas().update()

class AxisFollowing(Axis):
    def __init__(self, canvas):
        Axis.__init__(self, canvas)
        self.pen.setWidth(3)
        self.pen.setStyle(Qt.DashLine)
        self.setPen(self.pen)

class AxisImmediateFollowing(Axis):
    def __init__(self, canvas):
        Axis.__init__(self, canvas)
        self.pen.setWidth(3)
        self.setPen(self.pen)

class AxisSibling(Axis):
    def __init__(self, canvas):
        Axis.__init__(self, canvas)
        self.pen.setWidth(2)
        self.pen.setStyle(Qt.DashLine)
        self.setPen(self.pen)
        self.distance = 2.0

    def drawShape(self, painter):
        x1,y1,x2,y2 = self.points
        dx, dy, x1,y1, x2,y2  = dxdy(self.distance+self.pen.width(), x1,y1, x2,y2)
        painter.drawLine(x1+dx,y1-dy,x2+dx,y2-dy)
        painter.drawLine(x1-dx,y1+dy,x2-dx,y2+dy)
        self.drawLineHead(painter)

    def lineWidth(self):
        return self.pen.width() * 2.0 + self.distance


class AxisImmediateSibling(Axis):
    def __init__(self, canvas):
        Axis.__init__(self, canvas)
        self.pen.setWidth(2)
        self.setPen(self.pen)
        self.distance = 2.0

    def drawShape(self, painter):
        x1,y1,x2,y2 = self.points
        dx, dy, x1,y1, x2,y2  = dxdy(self.distance+self.pen.width(), x1,y1, x2,y2)
        painter.drawLine(x1+dx,y1-dy,x2+dx,y2-dy)
        painter.drawLine(x1-dx,y1+dy,x2-dx,y2+dy)
        self.drawLineHead(painter)

    def lineWidth(self):
        return self.pen.width() * 2.0 + self.distance


class AxisAncestor(Axis):
    def __init__(self, canvas):
        Axis.__init__(self, canvas)
        self.pen.setWidth(3)
        self.pen.setStyle(Qt.DashLine)
        self.pen.setColor(Qt.blue)
        self.setPen(self.pen)


class AxisParent(Axis):
    def __init__(self, canvas):
        Axis.__init__(self, canvas)
        self.pen.setWidth(3)
        self.pen.setColor(Qt.blue)
        self.setPen(self.pen)


pen = QPen(Qt.red)
pen.setWidth(3)
pen.setStyle(Qt.DashLine)
penFollowing = pen

pen = QPen(Qt.red)
pen.setWidth(3)
penImmFollowing = pen

class AxisButton(QPushButton):
    def __init__(self, pen, parent):
        QPushButton.__init__(self, parent)
        self.pen = pen

    def drawLine(self, y):
        p = QPainter(self)
        p.setPen(self.pen)
        x1 = 10
        x2 = self.width() - 10
        p.drawLine(x1, y, x2, y)

    def paintEvent(self, e):
        QPushButton.paintEvent(self, e)
        self.drawLine(self.height() / 2.0)
        
class AxisButtonFollowing(AxisButton):
    def __init__(self, parent, pen=penFollowing):
        AxisButton.__init__(self, pen, parent)
        
class AxisButtonImmFollowing(AxisButtonFollowing):
    def __init__(self, parent, pen=penImmFollowing):
        AxisButtonFollowing.__init__(self, parent, pen)
        
class AxisButtonSibling(AxisButton):
    def __init__(self, parent, pen=penFollowing):
        AxisButton.__init__(self, pen, parent)

    def paintEvent(self, e):
        QPushButton.paintEvent(self, e)
        y = self.height() / 2.0
        dy = 1 + self.pen.width() / 2.0
        y1 = y - dy
        y2 = y + dy
        self.drawLine(y1)
        self.drawLine(y2)

class AxisButtonImmSibling(AxisButtonSibling):
    def __init__(self, parent):
        AxisButtonSibling.__init__(self, parent, penImmFollowing)

class AxisButtonParent(AxisButtonImmFollowing):
    def __init__(self, parent):
        AxisButtonImmFollowing.__init__(self, parent)
        self.pen = QPen(self.pen)
        self.pen.setColor(Qt.blue)
        
class AxisButtonAncestor(AxisButtonFollowing):
    def __init__(self, parent):
        AxisButtonFollowing.__init__(self, parent)
        self.pen = QPen(self.pen)
        self.pen.setColor(Qt.blue)
        
iconAxisFollowing = [
    '22 1 1 1',
    'r c #FF0000',
    'rrrrrrrrrrrrrrrrrrrrrr',
    ]

textfileopen = [
    '16 13 5 1',
    '. c #040404',
    '# c #333333',
    'a c None',
    'b c #ffffff',
    'c c #ffffff',
    'aaaaaaaaa...aaaa',
    'aaaaaaaa.aaa.a.a',
    'aaaaaaaaaaaaa..a',
    'a...aaaaaaaa...a',
    '.bcb.......aaaaa',
    '.cbcbcbcbc.aaaaa',
    '.bcbcbcbcb.aaaaa',
    '.cbcb...........',
    '.bcb.#########.a',
    '.cb.#########.aa',
    '.b.#########.aaa',
    '..#########.aaaa',
    '...........aaaaa'
    ]

########NEW FILE########
__FILENAME__ = db
import re
import traceback
import sys
import time
from qt import *
from threading import Thread, Lock
import lpath
import at_lite as at
#from pyPgSQL import PgSQL
try:
    from sqlite3 import dbapi2 as sqlite
except ImportError:
    from pysqlite2 import dbapi2 as sqlite
from lpathtree_qt import *


__all__ = ["LPathDB", "LPathDbI", "LPathPgSqlDB", "LPathOracleDB", "LPathMySQLDB"]

class Prefetcher(QThread):
    def __init__(self, conn, indexSql, tableName, maxQueue=25, callback=None):
        """
        @param conn:
        @param indexSql: a query that returns a list of (sid,tid)
        @param tableName: the LPath table to query
        @param maxQueue: the maximum size of the queue
        @param callback: a callback function called for each tree loaded with
        the number of trees in the queue as an argument.
        """
        QThread.__init__(self)

        self.conn = conn
        self.indexSql = indexSql
        self.sqlTemplate = "select * from %s where sid=%%d and tid=%%d" % tableName
        self.maxQueue = maxQueue
        if callback is not None:
            self.callback = callback
        else:
            self.callback = lambda x:1
        self.queue = []
        self.keepRunning = True
        self.lock = QMutex()

    def run(self):
        indexCursor = self.conn.cursor()
        queryCursor = self.conn.cursor()
        queryCursor.arraysize = 250
        indexCursor.execute(self.indexSql.encode('utf-8'))
        
        row = indexCursor.fetchone()
        while row and self.keepRunning:
            if len(self.queue) >= self.maxQueue:
                time.sleep(1)
                continue
            sql = self.sqlTemplate % tuple(row)
            queryCursor.execute(sql.encode('utf-8'))
            tab = queryCursor.fetchall()
            self.lock.lock()
            ### critical region begins ###
            self.queue.append(tab)
            ### critical region ends #####
            self.lock.unlock()
            self.callback(len(self.queue))
            row = indexCursor.fetchone()

        indexCursor.close()
        queryCursor.close()
        
    def getNumTrees(self):
        return len(self.queue)
    
    def getNextTree(self):
        self.lock.lock()
        ### critical region begins ###
        if self.queue:
            row = self.queue[0]
            del self.queue[0]
        else:
            row = None
        ### critical region ends #####
        self.lock.unlock()
        return row

    def stop(self):
        self.callback = lambda x:1
        self.keepRunning = False
        

class LPathDbI:
    LPATH_TABLE_HEADER = [
        ('sid',int),('tid',int),('id',int),('pid',int),
        ('left',int),('right',int),('depth',int),
        ('type',unicode),('name',unicode),('value',unicode)
        ]

    EVENT_MORE_TREE = QEvent.User
    
    def submitQuery(self, query):
        """
        Run a given SQL query.
        
        @type  query: string
        @param query: SQL query to run.
        @return: True is query is submitted successfully, False otherwise.
        """
        return True
    
    def fetchNextTree(self):
        """
        @rtype: list
        @return: List of sentence ID, tree ID, SQL query cuased current result,
        TreeModel object corresponding to the tree, and LPathLocalDB object
        for the current tree.
        """
        pass

    def switchLPathTable(self, tableName):
        """
        @param tableName: Name of the new table to switch to.
        """
        pass

    def listTables(self):
        """
        @return: List of table names.
        """
        return ['T']
    
class LPathDB(LPathDbI):
    def __init__(self, conn, conn2=None):
        """
        @param conn: DB API 2 database connection object.
        @param conn2: DB connection object used for prefetcher. Note that the
        second connection is needed when the DB module cannot provide thread
        safety.
        """
        self.conn = conn
        self.conn2 = conn2
        self.tableName = None
        self.currentSql = None
        self.currentSql2 = None
        self.emitter = QObject()
        self.prefetcher = None
        self.eventReceivers = {}
        
    def _prefetchCallback(self, numTrees):
        for obj in self.eventReceivers[QEvent.User]:
            QApplication.postEvent(obj, QCustomEvent(self.EVENT_MORE_TREE,numTrees))
        #self.emitter.emit(PYSIGNAL("gotMoreTree"), (numTrees,))

    def connectToEvent(self, event, receiver):
        if event in self.eventReceivers:
            L = self.eventReceivers[event]
            if receiver not in L:
                L.append(receiver)
        else:
            self.eventReceivers[event] = [receiver]

    def disconnectFromEvent(self, event, receiver):
        try:
            self.eventReceivers[event].remove(receiver)
        except KeyError:
            pass
        except ValueError:
            pass
                
    def submitQuery(self, query):
        self.currentSql, self.currentSql2 = lpath.translate2(query, self.tableName)
        if self.currentSql is None:
            return False
        
        sql = re.sub(r"\s(%s[0-9]+).\*\s(.*)$" % self.tableName,
                     r" distinct \1.sid, \1.tid \2 order by \1.sid, \1.tid",
                     self.currentSql, 1)
        if self.prefetcher.running():
            self.prefetcher.stop()
            self.prefetcher.wait()
        if self.conn2 is not None:
            conn = self.conn2
        else:
            conn = self.conn
        self.prefetcher = Prefetcher(conn, sql, self.tableName,
                                     callback=self._prefetchCallback)
        self.prefetcher.start()
        return True

    def fetchNextTree(self):
        if self.prefetcher:
            rawtab = self.prefetcher.getNextTree()
            if rawtab:
                tab = at.TableModel(self.LPATH_TABLE_HEADER)
                for row in rawtab:
                    v = row[-1]
                    if v is not None:
                        row = list(row)
                        row[-1] = v.decode('utf-8')
                    tab.insertRow(row=row)
                tree = LPathTreeModel.importLPathTable(tab)
                sid, tid = rawtab[0][0:2]
                return sid, tid, self.currentSql, tree, \
                       LPathLocalDB(tab,self.tableName), self.currentSql2
        return None

    def switchLPathTable(self, tableName):
        if self.prefetcher and self.prefetcher.running():
            self.prefetcher.stop()
            self.prefetcher.wait()
        self.tableName = tableName
        self.currentSql = "select distinct sid, tid from %s order by sid, tid" % self.tableName
        self.currentSql2 = None
        if self.conn2 is not None:
            conn = self.conn2
        else:
            conn = self.conn
        self.prefetcher = Prefetcher(conn, self.currentSql, self.tableName,
                                     callback=self._prefetchCallback)
        self.prefetcher.start()
        
    def getNumTreesInMem(self):
        return self.prefetcher.getNumTrees()

class LPathPgSqlDB(LPathDB):
    def __init__(self, conn, conn2, user):
        LPathDB.__init__(self, conn, conn2)
        self.user = user

    def listTables(self):
        cur = self.conn.cursor()
        cur.execute("select tablename from pg_tables where tableowner=%s",
                    (self.user,))
        L = [x[0] for x in cur.fetchall()]
        cur.close()
        return L
    
class LPathOracleDB(LPathDB):
    def __init__(self, conn, conn2, user):
        """
        @param conn: DB API 2 connection object for an Oracle database.
        @param conn2: Another db connection object user for prefetcher.
        """
        LPathDB.__init__(self, conn, conn2)
        self.user = user

    def listTables(self):
        cur = self.conn.cursor()
        cur.execute("select table_name from all_tables where owner=:1",
                    (self.user.upper(),))
        L = [x[0] for x in cur.fetchall()]
        cur.close()
        return L
        
class LPathMySQLDB(LPathDB):
    def __init__(self, conn, conn2=None):
        """
        @param conn: DB API 2 database connection object.
        @param conn2: DB connection object used for prefetcher.
        """
        LPathDB.__init__(self, conn, conn2)

    def listTables(self):
        cur = self.conn.cursor()
        cur.execute("show tables")
        L = [x[0] for x in cur.fetchall()]
        cur.close()
        return L
        
class LPathLocalDB(LPathDbI):
    def __init__(self, tab, tableName='T'):
        self.tableName = tableName
        self.conn = sqlite.connect(':memory:')
        self.cursor = self.conn.cursor()
        self.cursor.execute("create table %s (sid,tid,id,pid,l,r,d,type,name,value)" % self.tableName)
        sql = "insert into %s values (?,?,?,?,?,?,?,?,?,?)" % self.tableName
        for r in tab:
            self.cursor.execute(sql, r)

    def _fetchTreeAsTable(self, sid, tid):
        sql = "select * from %s where sid=%d and tid=%d" % (self.tableName,sid,tid)
        c = self.conn.cursor()
        c.execute(sql)
        tab = at.TableModel(self.LPATH_TABLE_HEADER)
        for row in c.fetchall():
            tab.insertRow(row=row)
        return tab

    def submitQuery(self, query):
        if not self.conn:
            #raise Exception("no database connection")
            return False

        self.currentSql, self.currentSql2 = lpath.translate2(query, self.tableName)
        sql = re.sub(r"\s(%s[0-9]+).\*\s(.*)$" % self.tableName,
                     r" distinct \1.sid, \1.tid \2 order by \1.sid, \1.tid",
                     self.currentSql, 1)
        self.cursor.execute(sql)
        return True

    def fetchNextTree(self):    
        if self.cursor:
            res = self.cursor.fetchone()
            if res:
                sid, tid = res
                tab = self._fetchTreeAsTable(sid, tid)
                tree = LPathTreeModel.importLPathTable(tab)
                return sid, tid, self.currentSql, tree, \
                       LPathLocalDB(tab,self.tableName), self.currentSql2
            else:
                return None


########NEW FILE########
__FILENAME__ = dbdialog
from qt import *
from db import *
import os
try:
    from pyPgSQL import PgSQL
    CAP_PGSQL = True
except ImportError:
    CAP_PGSQL = False
try:
    os.environ['NLS_LANG'] = '.UTF8'
    import cx_Oracle
    CAP_ORACLE = True
except ImportError:
    CAP_ORACLE = False
try:
    import MySQLdb
    CAP_MYSQL = True
except ImportError:
    CAP_MYSQL = False

class ConnectionError(Exception):
    def __init__(self, msg):
        Exception.__init__(self, msg)

class DummyPanel(QWidget):        
    def __init__(self, parent=None, name=None):
        QWidget.__init__(self, parent, name)
    def connect(self):
        raise ConnectionError("no database server type selected")

class ConnectionPanelI:
    def connect(self): pass
    
class PgsqlPanel(QWidget, ConnectionPanelI):
    def __init__(self, parent=None, name=None):
        QWidget.__init__(self, parent, name)
        
        layout = QGridLayout(self)
        layout.addWidget(QLabel("Host: ",self),0,0)
        layout.addWidget(QLabel("Port: ",self),1,0)
        layout.addWidget(QLabel("Database: ",self),2,0)
        layout.addWidget(QLabel("User: ",self),3,0)
        layout.addWidget(QLabel("Password: ",self),4,0)
        self.entHost = QLineEdit(self)        
        self.entPort = QLineEdit("5432", self)        
        self.entDatabase = QLineEdit("qldb", self)        
        self.entUser = QLineEdit("qldb", self)        
        self.entPassword = QLineEdit(self)
        self.entPassword.setEchoMode(QLineEdit.Password)
        layout.addWidget(self.entHost,0,1)
        layout.addWidget(self.entPort,1,1)
        layout.addWidget(self.entDatabase,2,1)
        layout.addWidget(self.entUser,3,1)
        layout.addWidget(self.entPassword,4,1)

    def connect(self):
        conninfo = {
            "host":self.entHost.text(),
            "port":self.entPort.text(),
            "database":self.entDatabase.text(),
            "user":self.entUser.text(),
            "password":self.entPassword.text()
            }
        if conninfo['password'] is None:
            del conninfo['password']
        try:
            conn = PgSQL.connect(**conninfo)
            conn2 = PgSQL.connect(**conninfo)
            return LPathPgSqlDB(conn, conn2, conninfo["user"].ascii())
        except PgSQL.libpq.DatabaseError, e:
            try:
                enc = os.environ['LANG'].split('.')[-1]
                msg = e.message.decode(enc)
            except:
                msg = e.message
            raise ConnectionError(msg)

class OraclePanel(QWidget, ConnectionPanelI):
    def __init__(self, parent=None, name=None):
        QWidget.__init__(self, parent, name)
        
        user = os.path.basename(os.path.expanduser("~"))
        try:
            orahome = os.environ["ORACLE_HOME"]
        except KeyError:
            orahome = ''
            
        layout = QGridLayout(self)
        #layout.addWidget(QLabel("ORACLE_HOME: ", self),2,0)
        layout.addWidget(QLabel("User: ",self),0,0)
        layout.addWidget(QLabel("Password: ",self),1,0)
        #self.entOraHome = QLineEdit(orahome, self)
        self.entUser = QLineEdit(user, self)        
        self.entPassword = QLineEdit(self)
        self.entPassword.setEchoMode(QLineEdit.Password)
        #layout.addWidget(self.entOraHome,2,1)
        layout.addWidget(self.entUser,0,1)
        layout.addWidget(self.entPassword,1,1)

    def connect(self):
        #orahome = self.entOraHome.text().ascii()
        user = self.entUser.text().ascii()
        if '@' in user:
            user,service = user.split('@')
            service = '@' + service
        else:
            service = ''
        pw = self.entPassword.text().ascii()
        #if orahome:
        #    os.putenv("ORACLE_HOME", str(orahome))
        try:
            conn = cx_Oracle.connect(user+'/'+pw+service)
            conn2 = cx_Oracle.connect(user+'/'+pw+service)
        except cx_Oracle.DatabaseError, e:
            try:
                enc = os.environ['LANG'].split('.')[-1]
                msg = e.__str__().decode(enc)
            except:
                msg = e.__str__()
            raise ConnectionError(msg)
        return LPathOracleDB(conn, conn2, user)

class MySQLPanel(QWidget, ConnectionPanelI):
    def __init__(self, parent=None, name=None):
        QWidget.__init__(self, parent, name)
        
        user = os.path.basename(os.path.expanduser("~"))

        layout = QGridLayout(self)
        layout.addWidget(QLabel("Host: ",self),0,0)
        layout.addWidget(QLabel("Port: ",self),1,0)
        layout.addWidget(QLabel("Database: ",self),2,0)
        layout.addWidget(QLabel("User: ",self),3,0)
        layout.addWidget(QLabel("Password: ",self),4,0)
        self.entHost = QLineEdit(self)        
        self.entPort = QLineEdit("5432", self)        
        self.entDatabase = QLineEdit("qldb", self)        
        self.entUser = QLineEdit(user, self)        
        self.entPassword = QLineEdit(self)
        self.entPassword.setEchoMode(QLineEdit.Password)
        layout.addWidget(self.entHost,0,1)
        layout.addWidget(self.entPort,1,1)
        layout.addWidget(self.entDatabase,2,1)
        layout.addWidget(self.entUser,3,1)
        layout.addWidget(self.entPassword,4,1)

    def connect(self):
        conninfo = {
            "host":str(self.entHost.text()),
            "port":int(self.entPort.text()),
            "db":str(self.entDatabase.text()),
            "user":str(self.entUser.text()),
            "passwd":str(self.entPassword.text())
            }
        if conninfo['passwd'] is None:
            del conninfo['passwd']
        try:
            conn = MySQLdb.connect(**conninfo)
            return LPathMySQLDB(conn)
        except MySQLdb.DatabaseError, e:
            try:
                enc = os.environ['LANG'].split('.')[-1]
                msg = e.message.decode(enc)
            except:
                msg = e.message
            raise ConnectionError(msg)

class DatabaseConnectionDialog(QDialog):
    def __init__(self, parent=None, name=None,
                 modal=False, wflag=0):
        QDialog.__init__(self, parent, name, modal, wflag)
        self.setCaption("DB Connection")

        self.wstack = QWidgetStack(self)
        self.conpans = {"--":self.conpanNone,
                       "PostgreSQL":self.conpanPostgreSQL,
                       "Oracle":self.conpanOracle,
                       "MySQL":self.conpanMySQL}
        self.conpan = None
        self.conpanNone()

        layout = QVBoxLayout(self)
        hbox = QHBox(self)
        QLabel('Server type', hbox)
        combo = QComboBox(False, hbox)
        combo.insertItem("--")
        if CAP_PGSQL: combo.insertItem("PostgreSQL")
        if CAP_ORACLE: combo.insertItem("Oracle")
        if CAP_MYSQL: combo.insertItem("MySQL")

        layout.addWidget(hbox)
        layout.addWidget(self.wstack)
        layout.setStretchFactor(self.wstack, 1)
        
        buttons = QHBox(self)
        self.btnConnect = QPushButton("Co&nnect", buttons)
        self.btnCancel = QPushButton("&Cancel", buttons)
        layout.addWidget(buttons)

        layout.setMargin(5)
        layout.setSpacing(5)
        buttons.setMargin(10)
        buttons.setSpacing(10)

        self.connect(self.btnConnect, SIGNAL("clicked()"), self.connectToDb)
        self.connect(self.btnCancel, SIGNAL("clicked()"), self.reject)
        self.connect(combo, SIGNAL("activated(const QString&)"), self.changePanel)
        
        self.db = None

    def changePanel(self, s):
        self.conpans[str(s)]()
    def conpanNone(self):
        self.conpan = DummyPanel(self)
        self.wstack.removeWidget(self.wstack.visibleWidget())
        self.wstack.addWidget(self.conpan)
        self.wstack.raiseWidget(self.conpan)
    def conpanPostgreSQL(self):
        self.conpan = PgsqlPanel(self)
        self.wstack.removeWidget(self.wstack.visibleWidget())
        self.wstack.addWidget(self.conpan)
        self.wstack.raiseWidget(self.conpan)
    def conpanOracle(self):
        self.conpan = OraclePanel(self)
        self.wstack.removeWidget(self.wstack.visibleWidget())
        self.wstack.addWidget(self.conpan)
        self.wstack.raiseWidget(self.conpan)
    def conpanMySQL(self):
        self.conpan = MySQLPanel(self)
        self.wstack.removeWidget(self.wstack.visibleWidget())
        self.wstack.addWidget(self.conpan)
        self.wstack.raiseWidget(self.conpan)

    def connectToDb(self, *args):
        try:
            self.db = self.wstack.visibleWidget().connect()
            self.accept()
        except ConnectionError, e:
            QMessageBox.critical(self, "Connection Error",
                                 "Unable to connect to database:\n" + e.__str__())

    def getLPathDb(self):
        return self.db
        

class TableSelectionDialog(QDialog):
    def __init__(self, tableNames, parent=None, name=None,
                 modal=False, wflag=0):
        QDialog.__init__(self, parent, name, modal, wflag)
        self.setCaption("DB Connection")
        layout = QVBoxLayout(self)
        layout.addWidget(QLabel("Select a table below:",self))
        listbox = QListBox(self)
        layout.addWidget(listbox)
        hbox = QHBox(self)
        okButton = QPushButton("OK", hbox)
        cancelButton = QPushButton("Cancel", hbox)
        hbox.setMargin(10)
        hbox.setSpacing(10)
        layout.addWidget(hbox)

        layout.setMargin(5)
        layout.setSpacing(5)

        L = tableNames[:]
        L.sort()
        for tab in L:
            listbox.insertItem(tab)

        self.listbox = listbox

        self.connect(okButton, SIGNAL("clicked()"), self._okClicked)
        self.connect(cancelButton, SIGNAL("clicked()"), self._cancelClicked)
        self.tab = None

    def _okClicked(self):
        sel = self.listbox.selectedItem()
        if sel is not None:
            self.tab = unicode(sel.text())
            self.accept()
        else:
            QMessageBox.critical(self, "Error", "You didn't select a table.")
                                 
    def _cancelClicked(self):
        self.reject()

    def getSelectedTable(self):
        return self.tab
    
if __name__ == "__main__":
    app = QApplication([])
    d = TableSelectionDialog(['a','b','c','d','f','g','e','h','i'])
    d.exec_loop()
    

########NEW FILE########
__FILENAME__ = lpath
import time

T0 = time.time()

import sys
import re
from nltk import parse_cfg
from nltk import Tree
from nltk import parse

__all__ = ["translate", "translate2", "get_profile", "get_grammar", "get_base_grammar", "tokenize"]


GR = ""
# The following is the LPath+ grammar.
# Some rules of original grammar, which appear in LPath papers
# until 2006, are commented out using a single pound sign (#).
grammar_text = """
#P -> AP | AP '{' P '}'
P -> S | P S | P LCB P RCB
#AP ->  | S AP
#S -> A T | A T '[' R ']'
S -> S2 | LRB S2 RRB STAR | LRB S2 RRB PLUS | LRB S2 RRB OPT
#
S2 -> S1 | S1 LSB R RSB
#
S1 -> A T
A -> "/" | "//" | "." | "\\" | "\\\\" | "<=" | "=>" | "<==" | "==>" | "<-" | "->" | "<--" | "-->"
#T -> Qname | "_" | "@" Qname C Qname
T -> Qname | "_"
ATT -> AT Qname C Qname
#R -> R "or" R | R "and" R | "not" R | "(" R ")" | P | P C Qname
R -> R OR R | R AND R | LRB R RRB | P | LCB R RCB | ATT | NOT P | NOT ATT | NOT LRB R RRB
C -> "=" | "<=" | ">=" | "<>" | "like"
LCB -> '{'
RCB -> '}'
LRB -> '('
RRB -> ')'
LSB -> '['
RSB -> ']'
PLUS -> '+'
STAR -> '*'
OPT -> '?'
AT -> '@'
OR -> 'or'
AND -> 'and'
NOT -> 'not'
"""


def tokenize(q):
    p = re.compile(r"\s*({|}|\[|]|//|/|\.|\\\\|\\|<==|==>|<=|=>|<--|-->|<-|->|@|or|and|not|\(|\)|>=|=|<>|like|_|\*|\+|\?)\s*")

    tokens = []
    q = q.strip()
    N = len(q)
    d = 0           # scan position
    while d < N:

        # scan and add double-quoted string
        if q[d] == '"':
            for j in range(d+1,N):
                if q[j]=='"' and m!='\\':
                    break
            tokens.append(('s',q[d+1:j]))
            d = j+1
        elif q[d] == '@':
            # If we have an attribute, scan ahead until we reach the end of the attribute name.
            for j in range(d + 1, N):
                if not q[j].isalnum():
                    break
            tokens.append(('r', '@'))
            tokens.append(('s', q[d + 1: j]))
            d = j

        # find next reserved word while scanning free string before it
        d0 = d
        while d < N:
            m = p.match(q, d)
            if m:
                if m.group(1) == '_':
                    if d == d0:
                        break
                else:
                    break
            d += 1
        qname = q[d0:d]

        # if there is a free string, split it and add them to the tokens list
        if qname:
            for s in qname.split():
                tokens.append(('s',s))

        # add the reserved word to the tokens list
        if m:
            tokens.append(('r',m.group(1)))
            d = m.span()[1]
    return tokens


class SerialNumber:
    def __init__(self):
        self.n = 0
    def inc(self):
        self.n += 1
    def __int__(self):
        return self.n
    def __sub__(self, n):
        return self.n - n

    
class AND(list):
    def __init__(self, *args):
        list.__init__(self)
        self.joiner = "and"
        for e in args:
            if isinstance(e, flatten):
                self += e
            else:
                self.append(e)

    def __str__(self):
        L = []
        for x in self:
            if isinstance(x, str):
                L.append(x)
            elif isinstance(x, AND) or isinstance(x, OR) or isinstance(x, NOT):
                L.append(str(x))
            elif isinstance(x, flatten):
                for e in x:
                    L.append("%s%s%s" % tuple(e))
            elif isinstance(x, list):
                L.append("%s%s%s" % tuple(x))
            elif isinstance(x, Trans):
                L.append("exists (%s)" % x.getSql())
            else:
                L.append(str(x))
            L.append(self.joiner)
        return "(" + " ".join(L[:-1]) + ")"

    def __unicode__(self):
        L = []
        for x in self:
            if isinstance(x, str):
                L.append(unicode(x))
            elif isinstance(x, unicode):
                L.append(x)
            elif isinstance(x, AND) or isinstance(x, OR) or isinstance(x, NOT):
                L.append(unicode(x))
            elif isinstance(x, flatten):
                for e in x:
                    L.append("%s%s%s" % tuple(e))
            elif isinstance(x, list):
                L.append("%s%s%s" % tuple(x))
            elif isinstance(x, Trans):
                L.append("exists (%s)" % x.getSql())
            else:
                L.append(unicode(x))
            L.append(self.joiner)
        return "(" + " ".join(L[:-1]) + ")"
    
    def __add__(self, lst):
        self += lst
        return self

        
class OR(AND):
    def __init__(self, *args):
        AND.__init__(self, *args)
        self.joiner = "or"


class GRP(AND):
    pass

    
class NOT:
    def __init__(self, lst):
        self.lst = lst

    def __str__(self):
        return "not " + str(self.lst)

    def __unicode__(self):
        return "not " + unicode(self.lst)

    
class flatten(list):
    pass


class Step:
    FIELDMAP = {
        'sid':'sid',
        'tid':'tid',
        'id':'id',
        'pid':'pid',
        'left':'l',
        'right':'r',
        'depth':'d',
        'type':'type',
        'name':'name',
        'value':'value',
        }
    def __init__(self):
        self.conditional = None
        self.WHERE = []

    def getConstraints(self):
        C = []
        for c1,op,c2 in self.WHERE:
            C.append(["%s.%s" % (self.tab,c1), op, c2])
        return C

    def __getattr__(self, k):
        if k in self.FIELDMAP:
            return self.tab + "." + self.FIELDMAP[k]
        else:
            if hasattr(self, k):
                eval('self.' + k)
            else:
                raise(AttributeError("Step instance has no attribute '%s'" % k))

    
class Trans:
    TR = {
        '_':'under',
        }
    
    def __init__(self, t, sn, pstep=None, tname='T', scope=None):
        assert(type(t) == Tree)
        assert(t.node == 'P' or t.node == 'ATT' or t.node == 'R')

        self.sn = sn
        self.pstep = pstep
        self.tname = tname
        if self.pstep:
            self.prefix = self.pstep.tab
            self.step = pstep
        else:
            self.prefix = ""
        self.WHERE = AND()
        self.WHEREs = []    # context stack
        #self.WHERE2 = []    # restrictions
        #self.WHERE3 = []    # inter-step constraints
        #self.WHERE3 = []
        #self.WHERE4 = []    # scopic constraints
        #self.WHERE5 = []    # alignment constraints
        #self.WHERE6 = []    # conditional axis
        self.steps = []
        self.scope = scope
        
        self._expand(t)

        if self.pstep:
            self._interpreteAxis(self.pstep, self.steps[0].axis, self.steps[0])

    def _getNewTableName(self):
        s = "%s%d" % (self.tname,self.sn,)
        self.sn.inc()
        return s
    
    def _beginGrp(self, cls=GRP):
        self.WHEREs.append(self.WHERE)
        self.WHERE = cls()

    def _finishGrp(self, cls=None):
        if cls is not None:
            self.WHEREs[-1].append(cls(self.WHERE))
        else:
            self.WHEREs[-1].append(self.WHERE)
        self.WHERE = self.WHEREs.pop()
        
    def getSql(self):
        if self.pstep:
            sql = "select 1 "
        else:
            sql = "select %s.* " % self.steps[-1].tab
        sql += "from %s " % ",".join([self.tname+" "+s.tab for s in self.steps])

        for s in self.steps:
            if not s.conditional:
                self.WHERE += s.getConstraints()
            
        for i,s in enumerate(self.steps[:-1]):
            s2 = self.steps[i+1]
            self._interpreteAxis(s, s2.axis, s2)

        w = unicode(self.WHERE).strip()
        if w: sql += "where %s" % w

        return sql
            
    def _expand(self, t):
        name = "_" + t.node
        for c in t:
            name += "_"
            if isinstance(c,str) or isinstance(c,unicode):
                name += self.TR[c]
            else:
                name += c.node
        return eval("self.%s" % name)(t)
            
    def _interpreteScope(self, scope, step):
        self.WHERE += [
            [scope.left, "<=", step.left],
            [scope.right, ">=", step.right]
            ]

    def _alignLeft(self, step1, step2):
        self.WHERE += [
            [step1.left, "=", step2.left],
            ]

    def _alignRight(self, step1, step2):
        self.WHERE += [
            [step1.right, "=", step2.right],
            ]
        
    def _interpreteAxis(self, step1, axis, step2):
        if step2.conditional is not None:
            if axis == '/': 
                zWHERE = AND(
                    ["z.sid", "=", step2.sid],
                    ["z.tid", "=", step2.tid],
                    ["z.left", ">=", step1.left],
                    ["z.left", "<=", step2.left],
                    ["z.right", "<=", step1.right],
                    ["z.right", ">=", step2.right],
                    ["z.depth", ">", step1.depth],
                    ["z.depth", "<=", step2.depth],
                    )

                if hasattr(step2, 'conditionalRestriction'):
                    s = step2.conditionalRestriction.getSql()
                    s = re.sub(" "+step2.tab+"\\.", " z.", s)
                    s = "exists (%s)" % s
                    zWHERE.append(NOT(GRP(flatten(step2.getConstraints()),s)))
                else:
                    zWHERE.append(NOT(GRP(flatten(step2.getConstraints()))))

                self.WHERE += [
                    [step1.sid, "=", step2.sid],
                    [step1.tid, "=", step2.tid],
                    ]
                    
                if step2.conditional == '?':
                    self.WHERE += [
                        GRP(OR(
                        [step1.id, "=", step2.id],
                        AND([step1.id, "=", step2.pid]) +
                        step2.getConstraints()
                        ))
                        ]
                elif step2.conditional == '+':
                    self.WHERE += [
                        [step1.left, "<=", step2.left],
                        [step1.right, ">=", step2.right],
                        [step1.depth, "<", step2.depth],
                        "not exists (select 1 from %s z where %s)" % (self.tname,unicode(zWHERE))
                        ]
                elif step2.conditional == '*':
                    self.WHERE += [
                        GRP(OR(
                        [step1.id, "=", step2.id],
                        AND([step1.left, "<=", step2.left],
                            [step1.right, ">=", step2.right],
                            [step1.depth, "<", step2.depth],
                            "not exists (select 1 from %s z where %s)" % (self.tname,unicode(zWHERE)))
                        ))
                        ]
                    
            elif axis == '\\': 
                zWHERE = AND(
                    ["z.sid", "=", step2.sid],
                    ["z.tid", "=", step2.tid],
                    ["z.left", "<=", step1.left],
                    ["z.left", ">=", step2.left],
                    ["z.right", ">=", step1.right],
                    ["z.right", "<=", step2.right],
                    ["z.depth", "<", step1.depth],
                    ["z.depth", ">=", step2.depth],
                    )

                if hasattr(step2, 'conditionalRestriction'):
                    s = step2.conditionalRestriction.getSql()
                    s = re.sub(" "+step2.tab+"\\.", " z.", s)
                    s = "exists (%s)" % s
                    zWHERE.append(NOT(GRP(flatten(step2.getConstraints()),s)))
                else:
                    zWHERE.append(NOT(GRP(flatten(step2.getConstraints()))))

                self.WHERE += [
                    [step1.sid, "=", step2.sid],
                    [step1.tid, "=", step2.tid],
                    ]
                    
                if step2.conditional == '?':
                    self.WHERE += [
                        GRP(OR(
                        [step1.id, "=", step2.id],
                        AND([step1.pid, "=", step2.id]) +
                        step2.getConstraints()
                        ))
                        ]
                elif step2.conditional == '+':
                    self.WHERE += [
                        [step1.left, ">=", step2.left],
                        [step1.right, "<=", step2.right],
                        [step1.depth, ">", step2.depth],
                        "not exists (select 1 from %s z where %s)" % (self.tname,unicode(zWHERE))
                        ]
                elif step2.conditional == '*':
                    self.WHERE += [
                        GRP(OR(
                        [step1.id, "=", step2.id],
                        AND([step1.left, ">=", step2.left],
                            [step1.right, "<=", step2.right],
                            [step1.depth, ">", step2.depth],
                            "not exists (select 1 from %s z where %s)" % (self.tname,unicode(zWHERE)))
                        ))
                        ]
                    
            elif axis == '->':
                cWHERE = AND(
                    ["c.sid", "=", "z.sid"],
                    ["c.tid", "=", "z.tid"],
                    ["c.pid", "=", "z.id"]
                    )
                wWHERE = AND(
                    ["w.sid", "=", "z.sid"],
                    ["w.tid", "=", "z.tid"],
                    ["w.left", "<", "z.right"],
                    ["w.right", ">", "z.left"],
                    ["w.left", ">=", step1.right],
                    ["w.right", "<=", step2.left],
                    flatten(step2.getConstraints())
                    )

                if hasattr(step2, 'conditionalRestriction'):
                    s = step2.conditionalRestriction.getSql()
                    s = re.sub(" "+step2.tab+"\\.", " w.", s)
                    s = "exists (%s)" % s
                    zWHERE.append(s)

                zWHERE = AND(
                    ["z.sid", "=", step2.sid],
                    ["z.tid", "=", step2.tid],
                    ["z.left", ">=", step1.right],
                    ["z.right", "<=", step2.left],
                    NOT(GRP(flatten(step2.getConstraints()))),
                    "not exists (select 1 from %s c where %s)" % (self.tname,unicode(cWHERE)),
                    "not exists (select 1 from %s w where %s)" % (self.tname,unicode(wWHERE))
                    )

                self.WHERE += [
                    [step1.sid, "=", step2.sid],
                    [step1.tid, "=", step2.tid],
                    ]
                
                if step2.conditional == '?':
                    self.WHERE += [
                        GRP(OR(
                        [step1.id, "=", step2.id],
                        AND([step1.right, "=", step2.left],
                            flatten(step2.getConstraints())
                        )))
                        ]
                elif step2.conditional == '+':
                    self.WHERE += [
                        [step1.right, "<=", step2.left],
                        flatten(step2.getConstraints()),
                        "not exists (select 1 from %s z where %s)" % (self.tname,unicode(zWHERE))
                        ]
                elif step2.conditional == '*':
                    self.WHERE += [
                        GRP(OR(
                        [step1.id, "=", step2.id],
                        GRP(AND(
                        [step1.right, "<=", step2.left],
                        flatten(step2.getConstraints()),
                        "not exists (select 1 from %s z where %s)" % (self.tname,unicode(zWHERE))
                        ))))
                        ]
                        
            elif axis == '<-':
                cWHERE = AND(
                    ["c.sid", "=", "z.sid"],
                    ["c.tid", "=", "z.tid"],
                    ["c.pid", "=", "z.id"]
                    )
                wWHERE = AND(
                    ["w.sid", "=", "z.sid"],
                    ["w.tid", "=", "z.tid"],
                    ["w.left", "<", "z.right"],
                    ["w.right", ">", "z.left"],
                    ["w.left", ">=", step2.right],
                    ["w.right", "<=", step1.left],
                    flatten(step2.getConstraints())
                    )

                if hasattr(step2, 'conditionalRestriction'):
                    s = step2.conditionalRestriction.getSql()
                    s = re.sub(" "+step2.tab+"\\.", " w.", s)
                    s = "exists (%s)" % s
                    zWHERE.append(s)

                zWHERE = AND(
                    ["z.sid", "=", step2.sid],
                    ["z.tid", "=", step2.tid],
                    ["z.left", ">=", step2.right],
                    ["z.right", "<=", step1.left],
                    NOT(GRP(flatten(step2.getConstraints()))),
                    "not exists (select 1 from %s c where %s)" % (self.tname,unicode(cWHERE)),
                    "not exists (select 1 from %s w where %s)" % (self.tname,unicode(wWHERE))
                    )

                self.WHERE += [
                    [step1.sid, "=", step2.sid],
                    [step1.tid, "=", step2.tid],
                    ]
                
                if step2.conditional == '?':
                    self.WHERE += [
                        GRP(OR(
                        [step1.id, "=", step2.id],
                        AND([step1.left, "=", step2.right],
                            flatten(step2.getConstraints())
                        )))
                        ]
                elif step2.conditional == '+':
                    self.WHERE += [
                        [step1.left, ">=", step2.right],
                        flatten(step2.getConstraints()),
                        "not exists (select 1 from %s z where %s)" % (self.tname,unicode(zWHERE))
                        ]
                elif step2.conditional == '*':
                    self.WHERE += [
                        GRP(OR(
                        [step1.id, "=", step2.id],
                        GRP(AND(
                        [step1.left, ">=", step2.right],
                        flatten(step2.getConstraints()),
                        "not exists (select 1 from %s z where %s)" % (self.tname,unicode(zWHERE))
                        ))))
                        ]
                        
            elif axis == '=>':
                cWHERE = AND(
                    ["c.sid", "=", "z.sid"],
                    ["c.tid", "=", "z.tid"],
                    ["c.pid", "=", "z.id"]
                    )
                wWHERE = AND(
                    ["w.sid", "=", "z.sid"],
                    ["w.tid", "=", "z.tid"],
                    ["w.left", "<", "z.right"],
                    ["w.right", ">", "z.left"],
                    ["w.left", ">=", step1.right],
                    ["w.right", "<=", step2.left],
                    flatten(step2.getConstraints())
                    )

                if hasattr(step2, 'conditionalRestriction'):
                    s = step2.conditionalRestriction.getSql()
                    s = re.sub(" "+step2.tab+"\\.", " w.", s)
                    s = "exists (%s)" % s
                    zWHERE.append(s)

                zWHERE = AND(
                    ["z.sid", "=", step2.sid],
                    ["z.tid", "=", step2.tid],
                    ["z.left", ">=", step1.right],
                    ["z.right", "<=", step2.left],
                    NOT(GRP(flatten(step2.getConstraints()))),
                    "not exists (select 1 from %s c where %s)" % (self.tname,unicode(cWHERE)),
                    "not exists (select 1 from %s w where %s)" % (self.tname,unicode(wWHERE))
                    )

                self.WHERE += [
                    [step1.sid, "=", step2.sid],
                    [step1.tid, "=", step2.tid],
                    ]
                
                if step2.conditional == '?':
                    self.WHERE += [
                        GRP(OR(
                        [step1.id, "=", step2.id],
                        AND([step1.right, "=", step2.left],
                            [step1.pid, "=", step2.pid],
                            flatten(step2.getConstraints())
                        )))
                        ]
                elif step2.conditional == '+':
                    self.WHERE += [
                        [step1.right, "<=", step2.left],
                        [step1.pid, "=", step2.pid],
                        flatten(step2.getConstraints()),
                        "not exists (select 1 from %s z where %s)" % (self.tname,unicode(zWHERE))
                        ]
                elif step2.conditional == '*':
                    self.WHERE += [
                        GRP(OR(
                        [step1.id, "=", step2.id],
                        GRP(AND(
                        [step1.right, "<=", step2.left],
                        [step1.pid, "=", step2.pid],
                        flatten(step2.getConstraints()),
                        "not exists (select 1 from %s z where %s)" % (self.tname,unicode(zWHERE))
                        ))))
                        ]
                        
            elif axis == '<=':
                cWHERE = AND(
                    ["c.sid", "=", "z.sid"],
                    ["c.tid", "=", "z.tid"],
                    ["c.pid", "=", "z.id"]
                    )
                wWHERE = AND(
                    ["w.sid", "=", "z.sid"],
                    ["w.tid", "=", "z.tid"],
                    ["w.left", "<", "z.right"],
                    ["w.right", ">", "z.left"],
                    ["w.left", ">=", step2.right],
                    ["w.right", "<=", step1.left],
                    flatten(step2.getConstraints())
                    )

                if hasattr(step2, 'conditionalRestriction'):
                    s = step2.conditionalRestriction.getSql()
                    s = re.sub(" "+step2.tab+"\\.", " w.", s)
                    s = "exists (%s)" % s
                    zWHERE.append(s)

                zWHERE = AND(
                    ["z.sid", "=", step2.sid],
                    ["z.tid", "=", step2.tid],
                    ["z.left", ">=", step2.right],
                    ["z.right", "<=", step1.left],
                    NOT(GRP(flatten(step2.getConstraints()))),
                    "not exists (select 1 from %s c where %s)" % (self.tname,unicode(cWHERE)),
                    "not exists (select 1 from %s w where %s)" % (self.tname,unicode(wWHERE))
                    )

                self.WHERE += [
                    [step1.sid, "=", step2.sid],
                    [step1.tid, "=", step2.tid],
                    ]
                
                if step2.conditional == '?':
                    self.WHERE += [
                        GRP(OR(
                        [step1.id, "=", step2.id],
                        AND([step1.left, "=", step2.right],
                            [step1.pid, "=", step2.pid],
                            flatten(step2.getConstraints())
                        )))
                        ]
                elif step2.conditional == '+':
                    self.WHERE += [
                        [step1.left, ">=", step2.right],
                        [step1.pid, "=", step2.pid],
                        flatten(step2.getConstraints()),
                        "not exists (select 1 from %s z where %s)" % (self.tname,unicode(zWHERE))
                        ]
                elif step2.conditional == '*':
                    self.WHERE += [
                        GRP(OR(
                        [step1.id, "=", step2.id],
                        GRP(AND(
                        [step1.left, ">=", step2.right],
                        [step1.pid, "=", step2.pid],
                        flatten(step2.getConstraints()),
                        "not exists (select 1 from %s z where %s)" % (self.tname,unicode(zWHERE))
                        ))))
                        ]
                        
        # normal (non-conditional) axis
        elif step2.conditional is None:
            if axis == '/':
                self.WHERE += [
                    [step1.sid, "=", step2.sid],
                    [step1.tid, "=", step2.tid],
                    [step1.id, "=", step2.pid],
                    ]
            elif axis == '//':
                self.WHERE += [
                    [step1.sid, "=", step2.sid],
                    [step1.tid, "=", step2.tid],
                    [step1.left, "<=", step2.left],
                    [step1.right, ">=", step2.right],
                    [step1.depth, "<", step2.depth],
                    ]
            elif axis == '\\':
                self.WHERE += [
                    [step1.sid, "=", step2.sid],
                    [step1.tid, "=", step2.tid],
                    [step1.pid, "=", step2.id]
                    ]
            elif axis == '\\\\':
                self.WHERE += [
                    [step1.sid, "=", step2.sid],
                    [step1.tid, "=", step2.tid],
                    [step1.depth, ">", step2.depth],
                    [step1.left, ">=", step2.left],
                    [step1.right, "<=", step2.right]
                    ]
            elif axis == '->':
                self.WHERE += [
                    [step1.sid, "=", step2.sid],
                    [step1.tid, "=", step2.tid],
                    [step1.right, "=", step2.left],
                    ]
            elif axis == '-->':
                self.WHERE += [
                    [step1.sid, "=", step2.sid],
                    [step1.tid, "=", step2.tid],
                    [step1.right, "<=", step2.left],
                    ]
            elif axis == '<-':
                self.WHERE += [
                    [step1.sid, "=", step2.sid],
                    [step1.tid, "=", step2.tid],
                    [step1.left, "=", step2.right],
                    ]
            elif axis == '<--':
                self.WHERE += [
                    [step1.sid, "=", step2.sid],
                    [step1.tid, "=", step2.tid],
                    [step1.left, ">=", step2.right],
                    ]
            elif axis == '=>':
                self.WHERE += [
                    [step1.sid, "=", step2.sid],
                    [step1.tid, "=", step2.tid],
                    [step1.right, "=", step2.left],
                    [step1.pid, "=", step2.pid]
                    ]
            elif axis == '==>':
                self.WHERE += [
                    [step1.sid, "=", step2.sid],
                    [step1.tid, "=", step2.tid],
                    [step1.right, "<=", step2.left],
                    [step1.pid, "=", step2.pid]
                    ]
            elif axis == '<=':
                self.WHERE += [
                    [step1.sid, "=", step2.sid],
                    [step1.tid, "=", step2.tid],
                    [step1.left, "=", step2.right],
                    [step1.pid, "=", step2.pid]
                    ]
            elif axis == '<==':
                self.WHERE += [
                    [step1.sid, "=", step2.sid],
                    [step1.tid, "=", step2.tid],
                    [step1.left, ">=", step2.right],
                    [step1.pid, "=", step2.pid]
                    ]
            elif axis == '.' or axis == '@':
                self.WHERE += [
                    [step1.sid, "=", step2.sid],
                    [step1.tid, "=", step2.tid],
                    [step1.id, "=", step2.id]
                    ]
                
    def _P_S(self, tree):
        self._expand(tree[0])
        
    def _P_P_S(self, tree):
        p = tree[0]
        s = tree[1]
        self._expand(p)
        self._expand(s)
        
    def _P_P_LCB_P_RCB(self, tree):
        p1 = tree[0]
        p2 = tree[2]
        self._expand(p1)
        oldscope = self.scope
        self.scope = self.step
        self._expand(p2)
        self.scope = oldscope
        
    def _S_S2(self, tree):
        self._expand(tree[0])
        
    def _S_LRB_S2_RRB_STAR(self, tree):
        s2 = tree[1]
        
        self.step.conditional = '*'
        self._expand(s2)
    
    def _S_LRB_S2_RRB_PLUS(self, tree):
        s2 = tree[1]
        
        self.step.conditional = '+'
        self._expand(s2)
        
    def _S_LRB_S2_RRB_OPT(self, tree):
        s2 = tree[1]
        
        self.step.conditional = '?'
        self._expand(s2)
        
    def _S2_S1(self, tree):
        self._expand(tree[0])
        
    def _S2_S1_LSB_R_RSB(self, tree):
        s1 = tree[0]
        r = tree[2]
        
        self._expand(s1)
        self._expand(r)
        
    def _S1_A_T(self, tree):
        a = tree[0]
        t = tree[1]
        
        self.step = Step()
        if self.steps:
            self.step.sn = self.steps[-1].sn + 1
        else:
            self.step.sn = 0
        self.step.axis = a[0]
        #self.step.tab = self.prefix + self.tname + str(self.step.sn)
        self.step.tab = self._getNewTableName()
        self.steps.append(self.step)

        self._expand(t)

        if self.scope:
            self._interpreteScope(self.scope, self.step)

    def _T_Qname(self, t):
        tag = t[0][0]
        if tag[0] == '^':
            tag = tag[1:]
            if len(self.steps) > 1:
                if self.scope:
                    self._alignLeft(self.scope, self.step)
        if tag[-1] == '$':
            tag = tag[:-1]
            if len(self.steps) > 1:
                if self.scope:
                    self._alignRight(self.scope, self.step)
        self.step.WHERE = [
                ['type','=',"'syn'"],
                ['name','=',"'%s'" % tag],
                ]

    def _T_under(self, t):
        self.step.WHERE = [
            ['type','=',"'syn'"]
            ]
    
    def _ATT_AT_Qname_C_Qname(self, t):
        self.step = Step()
        if self.steps:
            self.step.sn = self.steps[-1].sn + 1
        else:
            self.step.sn = 0
        self.step.axis = '@'
        #self.step.tab = self.prefix + self.tname + str(self.step.sn)
        self.step.tab = self._getNewTableName()
        self.steps.append(self.step)

        self.step.WHERE = [
            ['type','=',"'att'"],
            ['name','=',"'@%s'" % t[1][0]],
            ['value', " %s " % t[2][0],"'%s'" % t[3][0]],
            ]

    def _R_R_OR_R(self, t):
        self._beginGrp(OR)
        self._expand(t[0])
        self._expand(t[2])
        self._finishGrp()

    def _R_R_AND_R(self, t):
        self._beginGrp(AND)
        self._expand(t[0])
        self._expand(t[2])
        self._finishGrp()

    def _R_LRB_R_RRB(self, t):
        self._beginGrp(GRP)
        self._expand(t[1])
        self._finishGrp()

    def _R_P(self, t):
        tr = Trans(t[0], self.sn, self.step, self.tname, self.scope)
        self.WHERE.append(tr)

    def _R_LCB_R_RCB(self, t):
        oldscope = self.scope
        self.scope = self.step
        self._beginGrp()
        self._expand(t[1])
        self._finishGrp()
        self.scope = oldscope
        
    def _R_ATT(self, t):
        tr = Trans(t[0], self.sn, self.step, self.tname, self.scope)
        self.WHERE.append(tr)

    def _R_NOT_P(self, t):
        tr = Trans(t[1], self.sn, self.step, self.tname, self.scope)
        self.WHERE.append(NOT(GRP(tr)))

    def _R_NOT_ATT(self, t):
        tr = Trans(t[1], self.sn, self.step, self.tname, self.scope)
        self.WHERE.append(NOT(GRP(tr)))
        
    def _R_NOT_LRB_R_RRB(self, t):
        self._beginGrp(GRP)
        self._expand(t[2])
        self._finishGrp(NOT)

class TransFlat(Trans):
    def getSql(self):
        if not hasattr(self,"steps2"):
            self.steps2 = []
        if self.pstep:
            sql = "select 1 "
        else:
            s = ",".join([x.tab+".*" for x in self.steps+self.steps2])                
            sql = "select %s " % s
        sql += "from %s " % ",".join([self.tname+" "+s.tab for s in self.steps+self.steps2])

        for s in self.steps:
            if not s.conditional:
                self.WHERE += s.getConstraints()
            
        for i,s in enumerate(self.steps[:-1]):
            s2 = self.steps[i+1]
            self._interpreteAxis(s, s2.axis, s2)

        w = unicode(self.WHERE).strip()
        if w: sql += "where %s" % w
        return sql
    
    def _R_P(self, t):
        if not hasattr(self,'steps2'): self.steps2 = []
        tr = TransFlat(t[0], self.sn, self.step, self.tname, self.scope)
        self.steps2 += tr.steps
        for s in tr.steps:
            if not s.conditional:
                tr.WHERE += s.getConstraints()
        for i,s in enumerate(tr.steps[:-1]):
            s2 = tr.steps[i+1]
            tr._interpreteAxis(s, s2.axis, s2)
        self.WHERE.append(unicode(tr.WHERE).strip())
    
def translate2(q,tname='T'):
    global T2, T3, T4, T5, T6, GR
    
    T2 = time.time()

    # tokenization
    l = tokenize(q)
    tokens = [a[1] for a in l]
    assert(tokens[0] == '//')
    T3 = time.time()

    # build grammar
    GR = grammar_text
    for typ, t in l:
        if typ == 's':
            GR += "Qname -> '" + t + "'\n"
    grammar = parse_cfg(GR)
    parser = parse.ChartParser(grammar, parse.TD_STRATEGY)
    T4 = time.time()

    # chart-parse the query
    trees = parser.nbest_parse(tokens)
    if not trees:
        T5 = T6 = time.time()
        return None, None
    tree = trees[0]
    T5 = time.time()

    # translate the parse tree
    r = Trans(tree,SerialNumber(),tname=tname).getSql()
    T6 = time.time()

    try:
        r1 = TransFlat(tree,SerialNumber(),tname=tname).getSql()
    except:
        r1 = None
   
    r1 = TransFlat(tree,SerialNumber(),tname=tname).getSql()
    return r, r1


def translate(q,tname='T'):
    return translate2(q,tname)[0]


def print_profile():
    print
    print "     python startup: %6.3fs" % (T1-T0)
    print " query tokenization: %6.3fs" % (T3-T2)
    print "    grammar parsing: %6.3fs" % (T4-T3)
    print "      chart parsing: %6.3fs" % (T5-T4)
    print "        translation: %6.3fs" % (T6-T5)
    print

def get_profile():
    # tok/grammar/parsing/trans times
    return (T3-T2,T4-T3,T5-T4,T6-T5)

def get_grammar():
    """
    Returns the CFG grammar that has recently been used.
    """
    return GR

def get_base_grammar():
    """
    Returns the base LPath+ CFG grammar.
    """
    return grammar_text

T1 = time.time()
T2 = T3 = T4 = T5 = T6 = 0.0


if __name__ == "__main__":
    
    import sys
    #l = tokenize('//A//B')
    q = '//A(/B[{//C->D$}])+'
    #l = tokenize('//A[{//B-->C}]')
    #l = tokenize('//A[//B or //C]')
    #l = tokenize('//S[//@lex="saw"]')
    #l = tokenize('//VP[//NP$]')
    #l = tokenize('//VP[{//^V->NP->PP$}]')
    #l = tokenize('//A//B//C')

    print translate2(sys.argv[1])[1]
    print_profile()
    #print get_grammar()

########NEW FILE########
__FILENAME__ = tb2tbl
import sys
import os
import codecs
import socket
import re

from optparse import OptionParser
import platform
from nltk_contrib.lpath.at_lite.table import TableModel
from nltk_contrib.lpath.at_lite.tree import TreeModel

def tb2tbl(tree,a,b):
    #conn.begin()
    #cursor.execute("begin")
    for r in tree.exportLPathTable(TableModel,a,b):
        print r
        cursor.execute(SQL1, tuple(r))
    #cursor.execute("commit")
    conn.commit()

def connectdb(opts):
    try:
        if opts.servertype == 'postgresql':
            try:
                conn = PgSQL.connect(
                    host=opts.host, port=opts.port, database=opts.db,
                    user=opts.user, password=opts.passwd)
            except PgSQL.libpq.DatabaseError, e:
                print e
                sys.exit(1)
            return conn
        elif opts.servertype == 'oracle':
            if '@' in opts.user:
                user,suffix = opts.user.split('@')
            else:
                user = opts.user
                suffix = ''
            dsn = "%s/%s@%s" % (user,opts.passwd,suffix)
            conn = cx_Oracle.connect(dsn)
            return conn
        elif opts.servertype == 'mysql':
            import MySQLdb
            try:
                conn = MySQLdb.connect(host=opts.host, port=opts.port, db=opts.db,
                                       user=opts.user, passwd=opts.passwd)
            except DatabaseError, e:
                print e
                sys.exit(1)
            return conn
    except ImportError, e:
        print e
        sys.exit(1)

def limit(servertype, sql, num):
    if servertype in ('postgresql', 'mysql'):
        sql += " limit %d" % num
    elif servertype == 'oracle':
        if 'where' in sql.lower():
            if 'and' in sql.lower():
                sql += " and rownum<=%d" % num
            else:
                sql += " rownum<=%d" % num
        else:
            sql += " where rownum<=%d" % num
    else:
        sql += " limit %d" % num
    return sql
 
if platform.system() == 'Windows':
    default_user = os.getenv('USERNAME')
    def getpass():
        import msvcrt
        s = ''
        c = msvcrt.getch()
        while ord(c) != 13:
            if ord(c) == 8:
                if len(s) > 0:
                    s = s[:-1]
            else:
                s += c
            c = msvcrt.getch()
        return s
else:
    # os.getlogin() seems to have some bug
    default_user = os.path.basename(os.path.expanduser("~"))
    def getpass():
        import termios
        tty = file('/dev/tty')
        attr = termios.tcgetattr(tty.fileno())
        attr[3] &= ~termios.ECHO
        termios.tcsetattr(tty.fileno(), termios.TCSANOW, attr)
        p = tty.readline().strip('\n')
        attr[3] |= termios.ECHO
        termios.tcsetattr(tty.fileno(), termios.TCSANOW, attr)
        tty.close()
        return p

usage = "%prog [options] treebank"
desc = """Load treebank data into a LPath table.
It takes one argument "treebank", which should be a path to the directory
containing treebank files. Every file under this directory tree is assumed
to be an input.  If '-' is given instead of a directory, treebank data should
be streamed into standard input.
"""
  
optpar = OptionParser(usage=usage,description=desc)
optpar.add_option("-c", "--create-table", dest="create", default=False,
                  action="store_true",
                  help="create table if not exist")
optpar.add_option("-d", "--database", dest="db", default="qldb",
                  help="use NAME as the target LPath database",
                  metavar="DB")
optpar.add_option("-f", "--file-filter", dest="filter",
                  help="use REGEX to filter treebank file names. "
                  "Ignored if treebank data is given from standard input",
                  metavar="REGEX")
optpar.add_option("-H", "--host", dest="host", default=socket.gethostname(),
                  help="LPath database is hosted by HOST",
                  metavar="HOST")
optpar.add_option("-n", "--num-trees", dest="numtree", default=0,
                  help="load only NUM trees (0=all)",
                  metavar="NUM", type="int")
optpar.add_option("-P", "--port", dest="port", default=5432,
                  help="database server is listening on port PORT",
                  metavar="PORT", type="int")
optpar.add_option("-p", "--password", dest="passwd",
                  help="use PASSWD to connect to the database",
                  metavar="PASSWD")
optpar.add_option("-t", "--table", dest="table", default="t",
                  help="use TABLE as the target LPath table",
                  metavar="TABLE")
optpar.add_option("-u", "--user", dest="user", default=default_user,
                  help="use USER to connect to the database",
                  metavar="USER")
optpar.add_option("-x", "--purge", dest="purge", default=False,
                  action="store_true",
                  help="empty the database before loading")
optpar.add_option("-y", "--server-type", dest="servertype",
                  help="server type (=postgresql|oracle)",
                  metavar="STR")

if __name__ == '__main__':
    opts, args = optpar.parse_args()

    # check arguments
    if len(args) == 0:
        optpar.error("required argument is missing")
    elif len(args) > 1:
        optpar.error("too many arguments")
    tbdir = args[0]
            
    # check options
    if not opts.user:
        optpar.error("user name is missing")
        
    if opts.passwd is None:
        print "Password:",
        opts.passwd = getpass()
    else:
        passwd = opts.passwd

    if opts.filter is None:
        filter = re.compile(".*")
    else:
        try:
            filter = re.compile(opts.filter)
        except:
            optpar.error("invalid regex for -f (--filter) option")

    if opts.servertype is None:
        optpar.error("you must specify the server type; use -y option")
    elif opts.servertype == 'postgresql':
        from pyPgSQL import PgSQL
        DatabaseError = PgSQL.libpq.DatabaseError
    elif opts.servertype == 'oracle':
        os.environ['NLS_LANG'] = '.UTF8'
        import cx_Oracle
        from cx_Oracle import DatabaseError
    elif opts.servertype == 'mysql':
        import MySQLdb
        DatabaseError = MySQLdb.DatabaseError
    else:
        optpar.error("server type should be one of the followins: postgresql, oracle, mysql")
        
    # try to connect to database
    conn = connectdb(opts)
    cursor = conn.cursor()

    print os.path.join('',os.path.dirname(sys.argv[0]))

    # check if table exists
    try:
        sql = limit(opts.servertype, "select * from "+opts.table, 1)
        cursor.execute(sql)
    except DatabaseError, e:
        if opts.create:
            p = os.path.join(os.path.dirname(sys.argv[0]),'lpath-schema.sql')
            for line in file(p).read().replace("TABLE",opts.table).split(';'):
                if line.strip():
                    cursor.execute(line)
        else:
            print "table %s doesn't exist" % `opts.table`
            sys.exit(1)

    # set correct table name in the insertion SQL
    if opts.servertype in ('postgresql', 'mysql'):
        SQL1 = "insert into TABLE values(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)"
    elif opts.servertype == 'oracle':
        SQL1 = "insert into TABLE values(:c1,:c2,:c3,:c4,:c5,:c6,:c7,:c8,:c9,:c10)"
    SQL1 = SQL1.replace('TABLE', opts.table)

    # empty the table if necessary
    if opts.purge:
        cursor.execute("delete from "+opts.table)

    # obtain the next sid
    cursor.execute("select max(sid) from "+opts.table)
    r = cursor.fetchone()
    if r[0] is None:
        sid = 1
    else:
        sid = r[0] + 1
            
    def do(tree):
        global sid
        t = tree.children[0]
        t.prune()
        tb2tbl(t, sid, 1)
        sid += 1

    count = opts.numtree
    reader = codecs.getreader('utf-8')
    if tbdir == '-':
        for tree in TreeModel.importTreebank(reader(sys.stdin)):
            print tree
            do(tree)
            count -= 1
            if count == 0: break
    else:
        for root, dirs, files in os.walk(tbdir):
            for f in files:
                print f,
                if filter.match(f):
                    p = os.path.join(root,f)
                    for tree in TreeModel.importTreebank(reader(file(p))):
                        do(tree)
                        count -= 1
                        if count == 0: sys.exit(0)  # done
                    print sid
                else:
                    print 'skipped'

########NEW FILE########
__FILENAME__ = lpathtree
import at_lite as at
from at_lite.tree import TreeModel as PureTree

__all__ = ['LPathTreeModel']

# IMPORTANT: We assume that the underlying tree is frozen and not modified
# in any case.

class LPathTreeModel(PureTree):
    AxisParent = "AxisParent"
    AxisAncestor = "AxisAncestor"
    AxisSibling = "AxisSibling"
    AxisImmediateSibling = "AxisImmediateSibling"
    AxisFollowing = "AxisFollowing"
    AxisImmediateFollowing = "AxisImmediateFollowing"
    AlignNone = 0
    AlignLeft = 1
    AlignRight = 2
    AlignBoth = 3
    
    def __init__(self):
        PureTree.__init__(self)
        self.lpRoot = self
        self.lpScope = None
        self.lpParent = None
        self.lpChildren = [None]
        self._lpAxisType = None
        self._lpNot = False
        self._lpAlignment = self.AlignNone
        # self.lpChildren[0]  - the main trunk child
        # self.lpChildren[1:] - branches (rectrictions)

    def getNot(self):
        """
        @rtype: bool
        @return: True if it is a negation branch, False otherwise.
        """
        return self._lpNot

    def setNot(self, v):
        """
        @type  v: bool
        @param v: True for negation branch, False for normal branch.
        @rtype: bool
        @return: True if successful, False otherwise.
        """
        if self._lpAxisType is None or self.lpParent.lpChildren[0]==self:
            return False
        self._lpNot = v
        self.lpDfs(lambda t:t.resetScope())
        return True

    def getAxisType(self):
        return self._lpAxisType

    def setAxisType(self, v):
        if self._lpAxisType is None:
            return False
        self._lpAxisType = v
        #self.lpDfs(lambda t:t.resetScope())
        return True
    
    def lpDfs(self, func, *args):
        L = [self]
        while L:
            n = L[0]
            func(n, *args)
            if n.lpChildren[0] is None:
                L = n.lpChildren[1:] + L[1:]
            else:
                L = n.lpChildren + L[1:]

    def lpBfs(self, func, *args):
        """
        Branch First Search.
        """
        L = [self]
        while L:
            n = L[0]
            func(n, *args)
            if n.lpChildren[0] is None:
                L = n.lpChildren[1:] + L[1:]
            else:
                L = n.lpChildren[1:] + [n.lpChildren[0]] + L[1:]
            
        
    def lpIsolated(self):
        return len(self.lpChildren)==1 and self.lpChildren[0] is None and self.lpParent is None
    
    def lpHasChild(self):
        return self.Children[0] is not None

    def lpOnMainTrunk(self):
        if self.lpParent is None:
            return True
        else:
            return self.lpParent.lpChildren[0] == self

    def _computeAxisType(self, n1, n2):
        if n1.parent == n2 or n2.parent == n1:
            return self.AxisParent

        if n1.rightSibling == n2 or n2.rightSibling == n1:
            return self.AxisImmediateSibling

        if n1.parent == n2.parent:
            return self.AxisSibling

        a1L = [n1]
        p = n1.parent
        while p:
            if p == n2:
                return self.AxisAncestor
            a1L.append(p)
            p = p.parent

        a2L = [n2]
        p = n2.parent
        while p:
            if p == n1:
                return self.AxisAncestor
            a2L.append(p)
            p = p.parent

        for i,x in enumerate(a1L):
            try:
                j = a2L.index(x)
                break
            except ValueError:
                pass

        a1L = a1L[:i]
        a2L = a2L[:j]
        i = x.children.index(a1L.pop())
        j = x.children.index(a2L.pop())

        if abs(i-j) > 1:
            return self.AxisFollowing
        
        if i < j:
            L1 = a1L
            L2 = a2L
        else:
            L1 = a2L
            L2 = a1L

        for x in L1:
            if x.rightSibling:
                return self.AxisFollowing
        for x in L2:
            if x.leftSibling:
                return self.AxisFollowing

        return self.AxisImmediateFollowing


    def resetScope(self):
        if not self.lpOnMainTrunk():
            if self.lpParent.isAncestorOf(self):
                self.lpScope = self.lpParent
            else:
                self.lpScope = self.lpParent.lpScope
            return
        
        p = self.lpParent
        while p:

            if p.isAncestorOf(self):
                self.lpScope = p
                break
            elif not p.lpOnMainTrunk():
                self.lpScope = p.lpScope
                break

            p1 = p.lpParent
            while p1 and p1.lpOnMainTrunk() and p1 != p.lpScope:
                p1 = p1.lpParent
            p = p1

        else:
            self.lpScope = None


    def lpBranchRoot(self):
        if not self.lpOnMainTrunk():
            return self
        p = self.lpParent
        while p and p.lpOnMainTrunk():
            p = p.lpParent
        return p


    def lpAncestorOf(self, node):
        p = node.lpParent
        while p and p != self:
            p = p.lpParent
        return p is not None

    def setScope(self, node):
        if node is None or node.lpAncestorOf(self):
            self.lpScope = node
            for c in self.lpChildren:
                if c is not None:
                    c.lpDfs(lambda t:t.resetScope())
            
    def shiftScope(self):
        if self._lpAxisType == self.AxisParent or \
           self._lpAxisType == self.AxisAncestor or \
           self._lpAxisType is None:
            return

        br = self.lpBranchRoot()
        if br:
            if br._lpAxisType == self.AxisParent or \
               br._lpAxisType == self.AxisAncestor:
                limit = br.lpParent
            else:
                limit = br.lpParent.lpScope
        else:
            limit = None

        if self.lpScope:
            self.lpScope = self.lpScope.lpScope
            if self.lpScope is None:
                if limit is not None:
                    self.lpScope = self.lpParent.lpScope
            else:
                if limit is not None and self.lpScope.lpAncestorOf(limit):
                    self.lpScope = self.lpParent.lpScope
        else:
            self.lpScope = self.lpParent.lpScope
            
        for c in self.lpChildren:
            if c is not None:
                c.lpDfs(lambda t:t.resetScope())
            
    def canShiftScope(self):
        if self._lpAxisType == self.AxisParent or \
           self._lpAxisType == self.AxisAncestor or \
           self._lpAxisType is None:
            return False

        br = self.lpBranchRoot()
        if br:
            if br._lpAxisType == self.AxisParent or \
               br._lpAxisType == self.AxisAncestor:
                limit = br.lpParent
            else:
                limit = br.lpParent.lpScope
        else:
            limit = None

        if self.lpScope:
            newScope = self.lpScope.lpScope
            if self.lpScope is None:
                if limit is not None:
                    newScope = self.lpParent.lpScope
            else:
                if limit is not None and self.lpScope.lpAncestorOf(limit):
                    newScope = self.lpParent.lpScope
        else:
            newScope = self.lpParent.lpScope
            
        return self.lpScope != newScope
    
            
    def lpSetChild(self, node):
        # terminal node can't have an LPath child
        # also, terminal node can't be put on the backbone
        if len(self.children) == 0 or len(node.children) == 0:
            return False
        
        # if the node comes from siblings, just change order of children
        if node in self.lpChildren and node != self.lpChildren[0]:
            oldroot = self.lpChildren[0]
            self.lpChildren.remove(node)
            self.lpChildren[0] = node
            if oldroot is not None:
                self.lpChildren.append(oldroot)
            node._lpNot = False
            node.lpDfs(lambda t:t.resetScope())
            return True
        
        if self.lpChildren[0] or node.lpRoot!=node or node.lpRoot==self.lpRoot:
            return False
            
        node.lpParent = self
        self.lpChildren[0] = node
        
        def f(t):t.lpRoot=self.lpRoot
        node.lpDfs(f)

        node._lpAxisType = self._computeAxisType(self, node)
        node.lpDfs(lambda t:t.resetScope())

        return True
    
    def lpAttachBranch(self, node):
        # terminal node can't have an LPath child
        if len(self.children) == 0 or \
            (len(node.children)==0 and node.parent!=self):
            return False
        
        if node == self.lpChildren[0]:
            self.lpChildren[0] = None
            self.lpChildren.append(node)
            node.lpDfs(lambda t:t.resetScope())
            return True

        # node is non-root node or in the same tree as self
        if node.lpRoot!=node or self.lpRoot==node.lpRoot:
            return False
        
        node.lpParent = self
        self.lpChildren.append(node)

        def f(t):t.lpRoot=self.lpRoot
        node.lpDfs(f)

        node._lpAxisType = self._computeAxisType(self, node)
        node.lpDfs(lambda t:t.resetScope())

        return True
    
    def lpPrune(self):
        if self.lpRoot == self:
            return False
        if self.lpParent is not None:
            if self.lpParent.lpChildren[0] == self:
                self.lpParent.lpChildren[0] = None
            else:
                self.lpParent.lpChildren.remove(self)
        self.lpParent = None
        self._lpAxisType = None
        self._lpNot = False

        def f(t):t.lpRoot=self
        self.lpDfs(f)
        self.lpDfs(lambda t:t.resetScope())

        return True

    def lpRoots(self):
        def f(t,L):
            if t.lpRoot==t and not t.lpIsolated():
                L.append(t)
        L = []
        self.root.dfs(f,L)
        return L

    def lpScopeDepth(self):
        d = 0
        n = self
        while n.lpScope:
            n = n.lpScope
            d += 1
        return d
    
#    def _hasHorizontalAxis(self, n):
#        axis = n.getAxisType()
#        return axis is not None and axis not in (self.AxisAncestor,self.AxisParent)
#    
#    def _upperHorizontalSpanningTree(self, filter=lambda x:True):
#        L = []
#        c = self
#        while c.lpParent and self._hasHorizontalAxis(c):
#            if filter(c.lpParent): L.append(c.lpParent)
#            c = c.lpParent
#        return L
#    
#    def _lowerHorizontalSpanningTree(self, filter=lambda x:True):
#        L = []
#        for c in self.lpChildren:
#            if c and self._hasHorizontalAxis(c):
#                if filter(c): L.append(c)
#                L += c._lowerHorizontalSpanningTree(filter)
#        return L
#    
    def lpScopeSiblings(self, filter=lambda x:True):
        L = []
        if self.lpScope is not None:
            def f(node):
                if node.lpScope == self.lpScope and filter(node):
                    L.append(node)
            self.root.dfs(f)
        return L
        
    def lpLeftAlignable(self):
        return len(self.children) > 0 and \
                self.lpScope is not None and \
                len(self.lpScopeSiblings(self.follows)) == 0
                 
            
    def lpRightAlignable(self):
        filter = lambda x:x.follows(self)
        return len(self.children) > 0 and \
                self.lpScope is not None and \
                len(self.lpScopeSiblings(filter)) == 0
        
    def lpAlignLeft(self):
        if self.lpLeftAlignable():
            if self._lpAlignment == self.AlignBoth or self._lpAlignment == self.AlignRight:
                self._lpAlignment = self.AlignBoth
            else:
                self._lpAlignment = self.AlignLeft
            return True
        else:
            return False
        
    def lpAlignRight(self):
        if self.lpRightAlignable():
            if self._lpAlignment == self.AlignBoth or self._lpAlignment == self.AlignLeft:
                self._lpAlignment = self.AlignBoth
            else:
                self._lpAlignment = self.AlignRight
            return True
        else:
            return False
    
    def lpClearAlignment(self):
        self._lpAlignment = self.AlignNone
        
    def lpResetAlignment(self):
        def f(node):
            a = node.lpAlignment()
            if ((a == self.AlignRight and not node.lpRightAlignable()) or \
                (a == self.AlignLeft and not node.lpLeftAlignable())):
                node.lpClearAlignment()
        self.lpRoot.lpDfs(f)
        
    def lpAlignment(self):
        return self._lpAlignment


########NEW FILE########
__FILENAME__ = lpathtree_qt
from lpathtree import LPathTreeModel as PureLPathTree
from axis import *
from qt import QObject

__all__ = ['LPathTreeModel']

class LPathTreeModel(PureLPathTree,object):
    def __init__(self):
        PureLPathTree.__init__(self)
        self.gui = None
        self.axis = None
        self.line = None
        self.hidden = False

    def hide(self):
        self.gui.hide()
        self.line.hide()
        if self.axis:
            self.axis.hide()
        self.hidden = True
    
    def show(self):
        self.gui.show()
        self.line.show()
        if self.axis:
            self.axis.show()
        self.hidden = False
        
    def _findCollapseRoot(self, node):
        while node:
            if node.collapsed and node.hidden==False:
                return node
            else:
                node = node.parent
                
    def redrawAxis(self):
        if self.axis:
            root = root1 = self.axis.root
            target = target1 = self.axis.target
            if root.hidden:
                root1 = self._findCollapseRoot(root)
            if target.hidden:
                target1 = self._findCollapseRoot(target)
            coords = list(root1.gui.connectingLine(target1.gui))
            if root.hidden:
                coords[1] += root.gui.height()
            if target.hidden:
                coords[3] += target.gui.height()
            if root==target1 and (len(root1.lpChildren) > 1 or root1.lpChildren[0]):
                coords[1] += root1.gui.height() / 2.0
            if target==root1 and target1.lpParent:
                coords[3] += target1.gui.height() / 2.0
            self.axis.setCanvas(None)
            cls = eval(self.getAxisType())
            self.axis = cls(self.gui.canvas())
            self.axis.target = target
            self.axis.root = root
            apply(self.axis.setPoints, coords)
            if self.getNot():
                self.axis.setHeadType(Axis.HeadNegation)
            elif not self.lpOnMainTrunk():
                self.axis.setHeadType(Axis.HeadBranch)
            self.axis.show()
            self.axis.canvas().update()

    def _newAxis(self, node):
        if node.axis is None:
            cls = eval(node.getAxisType())
            node.axis = cls(self.gui.canvas())
            node.axis.root = self
            node.axis.target = node
            #coords = node.gui.connectingLine(self.gui)
            coords = self.gui.connectingLine(node.gui)
            apply(node.axis.setPoints, coords)
            if node.getNot():
                node.axis.setHeadType(Axis.HeadNegation)
            elif not node.lpOnMainTrunk():
                node.axis.setHeadType(Axis.HeadBranch)
            node.axis.show()
            node.axis.canvas().update()

    def setNot(self, v):
        if PureLPathTree.setNot(self,v):
            self.redrawAxis()

    def setAxisType(self, v):
        if PureLPathTree.setAxisType(self,v):
            self.redrawAxis()
            return True
        else:
            return False

    def resetScope(self):
        PureLPathTree.resetScope(self)
        self.gui.canvas().update()

    def setScope(self, node):
        PureLPathTree.setScope(self, node)
        self.gui.canvas().update()
        self.gui.canvas().signal()
        
    def shiftScope(self):
        PureLPathTree.shiftScope(self)
        self.gui.canvas().update()
        self.gui.canvas().signal()
        
    def lpSetChild(self, node):
        if PureLPathTree.lpSetChild(self, node):
            self.lpResetAlignment()
            if node.axis is None:
                self._newAxis(node)
            else:
                node.redrawAxis()
            return True
        else:
            return False

    def lpAttachBranch(self, node):
        if PureLPathTree.lpAttachBranch(self, node):
            self.lpResetAlignment()
            if node.axis is None:
                self._newAxis(node)
            else:
                node.redrawAxis()
            return True
        else:
            return False

    def lpPrune(self):
        p = self.lpParent
        if PureLPathTree.lpPrune(self):
            self.axis.setCanvas(None)
            self.axis = None
            self.gui.updateNumber()
            self.lpResetAlignment()
            if p:
                p.gui.updateNumber()
                p.lpResetAlignment()
            self.gui.canvas().update()
            return True
        else:
            return False
        
    def lpAlignLeft(self):
        if PureLPathTree.lpAlignLeft(self):
            self.gui.update()
            self.gui.canvas().update()
            self.gui.canvas().signal()
            self.gui.canvas().redraw()
            return True
        else:
            return False
    
    def lpAlignRight(self):
        if PureLPathTree.lpAlignRight(self):
            self.gui.update()
            self.gui.canvas().update()
            self.gui.canvas().signal()
            self.gui.canvas().redraw()
            return True
        else:
            return False
    
    def lpClearAlignment(self):
        PureLPathTree.lpClearAlignment(self)
        self.gui.update()
        self.gui.canvas().update()
        self.gui.canvas().signal()
        self.gui.canvas().redraw()

    def _getFilterExpression(self):
        if 'lpathFilter' in self.data:
            return self.data['lpathFilter']
        else:
            return None
    def _setFilterExpression(self, v):
        self.data['lpathFilter'] = v
        self.gui.updateTrace()
        self.gui.canvas().signal()
    def _delFilterExpression(self):
        if 'lpathFilter' in self.data:
            del self.data['lpathFilter']
            self.gui.updateTrace()
            self.gui.canvas().signal()
    def _getCollapsed(self):
        if 'collapsed' in self.data:
            return self.data['collapsed']
        else:
            return False
    def _setCollapsed(self, v):
        self.data['collapsed'] = v
        self.gui.canvas().redraw()
        self.gui.canvas().update()
    def _getLabel(self):
        return self.data['label']
    def _setLabel(self, v):
        self.data['label'] = v
        #self.gui.update()
        self.gui.canvas().update()
        self.gui.canvas().signal()
        self.gui.canvas().redraw()
    def _getFuncAtts(self):    
        return self.data['@func']
    def _setFuncAtts(self, v):
        self.data['@func'] = v
        self.gui.update()
        self.gui.canvas().update()
        self.gui.canvas().signal()
    def _delFuncAtts(self):
        if '@func' in self.data:
            del self.data['@func']
            self.gui.update()
            self.gui.canvas().update()
            self.gui.canvas().signal()
            
    label = property(_getLabel,_setLabel)
    filterExpression = property(_getFilterExpression,_setFilterExpression,_delFilterExpression)
    funcatts = property(_getFuncAtts,_setFuncAtts,_delFuncAtts)
    collapsed = property(_getCollapsed,_setCollapsed)
    

########NEW FILE########
__FILENAME__ = nodefeaturedialog
from qt import *
from at_lite import TableModel, TableEdit
import lpath

class NodeFeatureDialog(QDialog):
    def __init__(self, node, parent):
        QDialog.__init__(self, parent)
        self.setCaption('Node Attribute Dialog')
        self.resize(320,240)
        
        tab = TableModel([("Name",unicode),("Value",unicode)])
        tab.insertRow(None, ['label',node.data['label']])
        if '@func' in node.data:
            for v in node.data['@func']:
                tab.insertRow(None, ['@func',v])
        if 'lpathFilter' in node.data:
            tab.insertRow(None, ['filter', node.data['lpathFilter']])
        
        if node.children:
            tv = QVBox(self)
            QLabel("n      - add a new filter expression", tv)
            QLabel("Insert - add a new 'function' attribute ", tv)
            QLabel("Delete - delete an attribute", tv)
            font = QFont('typewriter')
            font.setFixedPitch(True)
            tv.setFont(font)
            tv.setMargin(10)
        
        hbox = QHBox(self)
        bOk = QPushButton("&OK", hbox)
        bCancel = QPushButton("&Cancel", hbox)
        hbox.setMargin(5)
        hbox.setSpacing(10)

        te = TableEdit(self)
        te.setData(tab)
        te.setColumnReadOnly(0,True)
        te.setColumnStretchable(1,True)
        
        self.connect(bOk, SIGNAL("clicked()"), self.slotOk)
        self.connect(bCancel, SIGNAL("clicked()"), self.slotCancel)
        self.connect(tab.emitter, PYSIGNAL("cellChanged"), self.validateFilter)
        
        layout = QVBoxLayout(self)
        layout.addWidget(te)
        if node.children: layout.addWidget(tv)
        layout.addWidget(hbox)
        layout.setSpacing(5)
        layout.setMargin(5)
        
        self.tab = tab
        self.te = te
        self.node = node
        self.filterOK = True
        
    def slotOk(self):
        if self.filterOK == False:
            QMessageBox.critical(self, "Error", "Invalid filter expression.")
            return
        f = []
        for name in ('label','@func','lpathFilter'):
            try:
                del self.node.data[name]
            except KeyError:
                pass
        for row in self.tab:
            if row['Name'] == 'label':
                self.node.label = row['Value']
            elif row['Name'] == '@func':
                f.append(row['Value'])
            elif row['Name'] == 'filter':
                if row['Value']:
                    self.node.filterExpression = row['Value']
        if f: self.node.funcatts = f
        #self.node.gui.update()
        self.accept()
        
    def slotCancel(self):
        self.reject()
        
    def validateFilter(self, row, col, val, w):
        if self.tab[row][0] == 'filter':
            if not val or lpath.translate("//A[%s]"%val.strip()) is None:
                QMessageBox.critical(self, "Error", "Invalid filter expression.")
                self.filterOK = False
            else:
                self.filterOK = True
        
    def keyPressEvent(self, e):
        if e.key() == Qt.Key_Delete:
            L = []
            for i in range(self.te.numRows()):
                if self.te.isRowSelected(i):
                    if self.tab[i]['Name'] != 'label':
                        L.append(self.tab[i])
            for row in L:
                self.tab.takeRow(row.num)
        elif e.key() == Qt.Key_Insert and self.node.children:
            i = self.te.currentRow() + 1
            self.tab.insertRow(i, ["@func",""])
            self.te.setCurrentCell(i,1)
        elif e.key() == Qt.Key_N and self.node.children:
            i = self.te.currentRow() + 1
            # make sure that there's only one filter row
            for row in self.tab:
                if row['Name'] == 'filter':
                    return
            self.tab.insertRow(i, ["filter",""])
            self.te.setCurrentCell(i,1)
            

########NEW FILE########
__FILENAME__ = overlay
import re
from translator import translate_sub

__all__ = ["find_overlays", "Overlay"];

class Overlay:
    def __init__(self, matches):
        self._L = matches
        self._h = dict(matches)
        
    def findMatchingNode(self, node):
        for oldNode, newNode in self._L:
            if oldNode == node:
                return newNode
            elif newNode == node:
                return oldNode
        return None
    
    def markNegation(self, n, m):
        """
        @type  n: TreeModel
        @param n: A node in the old tree.
        @type  m: TreeModel
        @param m: A node in the new tree.
        """
        for c in n.lpChildren[1:]:
            if c.getNot():
                s = re.sub(r"[^[]*\[(.*)\].*",r"\1",translate_sub(c,None))
                if 'lpathFilter' in m.data:
                    v = m.data['lpathFilter']
                    v += " and not " + s
                    v = v.strip()
                else:
                    v = "not " + s.strip()
                m.data['lpathFilter'] = v
                m.gui.updateTrace()
                
    def display(self):
        A = [a[0] for a in self._L]
        for i,(a0,b0) in enumerate(self._L):
            a1 = a0.lpChildren[0]
            if a1 in A:
                j = A.index(a1)
                b1 = self._L[j][1]
                b0.lpSetChild(b1)
                b1.setAxisType(a1.getAxisType())
                alignment = a1.lpAlignment()
                if alignment == a1.AlignLeft:
                    b1.lpAlignLeft()
                elif alignment == a1.AlignRight:
                    b1.lpAlignRight()
                elif alignment == a1.AlignBoth:
                    b1.lpAlignLeft()
                    b1.lpAlignRight()
                if a1.lpScope is None:
                    b1.setScope(None)
                else:
                    b1.setScope(self._h[a1.lpScope])
            for a1 in a0.lpChildren[1:]:
                if 'lexical' in a1.data and a1.data['lexical']==True:
                    # terminal node == lexical node
                    b1 = b0.children[0]
                else:
                    try:
                        j = A.index(a1)
                    except ValueError:
                        continue
                    b1 = self._L[j][1]
                b0.lpAttachBranch(b1)
                b1.setAxisType(a1.getAxisType())
                alignment = a1.lpAlignment()
                if alignment == a1.AlignLeft:
                    b1.lpAlignLeft()
                elif alignment == a1.AlignRight:
                    b1.lpAlignRight()
                elif alignment == a1.AlignBoth:
                    b1.lpAlignLeft()
                    b1.lpAlignRight()
                if a1.lpScope is None:
                    b1.setScope(None)
                else:
                    b1.setScope(self._h[a1.lpScope])
            self.markNegation(a0,b0)
        self._L[0][1].gui.canvas().update()
    
    def clear(self):
        def g(t,L): L.append(t)
        L = []
        self._L[0][1].lpDfs(g,L)
        for n in L:
            n.lpPrune()
            n.gui.clear()
            if 'lpathFilter' in n.data:
                del n.data['lpathFilter']
        if L: n.gui.canvas().update()


def find_overlays(sql, localdb, oldTree, newTree):
    # get table names from sql: tnames
    if sql is None: return []
    m = re.match(r"^\s*select (.*?) from .*", sql)
    if m is None: return []
    a = m.group(1)
    tnames = [s.strip().split('.')[0] for s in a.split(",")]
    
    # get result table
    TAB = []
    localdb.cursor.execute(sql)
    for r in localdb.cursor.fetchall():
        h = {}
        for i,nam in enumerate(tnames):
            h[nam] = r[i*10:(i+1)*10]
        TAB.append(h)

    # create mapping from node id to tree node
    def f(t, h):
        if 'id' in t.data:
            h[t.data['id']] = t
        else:
            # attribute nodes
            h[-t.parent.data['id']] = t
    h = {}
    newTree.dfs(f, h)

    #
    def g(t, L):
        # skip lexical nodes
        if 'lexical' in t.data and t.data['lexical']==True:
            return
        # skip if it belongs to a negative branch
        p = t.lpBranchRoot()
        while p:
            if p.getNot(): return
            p = p.lpParent
        # add (old,new) pair
        L.append((t,L[0]))
        del L[0]
    
    M = []
    for match in TAB:
        m = match.items()
        m.sort()
        L = []
        for sym,tup in m:
            L.append(h[tup[2]])
        oldTree.lpBfs(g,L)

        # The sql allows a node to be selected multiple time in a single
        # query result. This caused a problem of totally wrong overlay
        # display. Here we just filter out those query results.
        seen = {}
        for x in L:
            if x[1] not in seen:
                seen[x[1]] = 1
        if len(seen) != len(L): continue

        M.append(Overlay(L))
    return M


########NEW FILE########
__FILENAME__ = parselpath
from lpath import tokenize
from lpathtree import LPathTreeModel
from translator import translate

SCOPE = ['{','}']
BRANCH = ['[',']']
VAXIS = ['//','/','.','\\\\','\\']
HAXIS = ['<==','==>','<=','=>','<--','-->','<-','->']
AXIS = VAXIS + HAXIS
ATTRIBUTE = ['@']
CONNECTIVES = ['or','and']
GROUP = ['(',')']
OPERATION = ['>=','=','<>','like']
ANNONYMOUS = ['_']

AXISMAP = {
    '//':LPathTreeModel.AxisAncestor,
    '/':LPathTreeModel.AxisParent,
    '\\\\':LPathTreeModel.AxisAncestor,
    '\\':LPathTreeModel.AxisParent,
    '==>':LPathTreeModel.AxisSibling,
    '<==':LPathTreeModel.AxisSibling,
    '=>':LPathTreeModel.AxisImmediateSibling,
    '<=':LPathTreeModel.AxisImmediateSibling,
    '<--':LPathTreeModel.AxisFollowing,
    '-->':LPathTreeModel.AxisFollowing,
    '<-':LPathTreeModel.AxisImmediateFollowing,
    '->':LPathTreeModel.AxisImmediateFollowing,
    }

def parse_lpath(q):
    tokens = tokenize(q)
    root = p = LPathTreeModel()
    p.data['label'] = 'root'
    i = 0
    ret = [p]
    branch = False
    scope = [None]
    while i < len(tokens):
        a, b = tokens[i]
        if a == 'r':
            if b == '[':
                ret.append(None)
                branch = True
            elif b == ']':
                ret.pop()
                p = ret[-1]
                branch = False
            elif b in AXIS:
                node = LPathTreeModel()
                node.data['label'] = tokens[i+1][1]
                dummy = LPathTreeModel()
                dummy.data['label'] = '.'
                node.attach(dummy)
                
                if b in HAXIS:
                    if b == '<=':
                        p.insertLeft(node)
                    elif b == '=>':
                        p.insertRight(node)
                    elif b[0] == '<':
                        if p.leftSibling:
                            p.leftSibling.insertLeft(node)
                        else:
                            p.insertLeft(node)
                    else:
                        if p.rightSibling:
                            p.rightSibling.insertRight(node)
                        else:
                            p.insertRight(node)
                else:
                    if b[0] == '/':
                        p.attach(node)
                    elif b == '\\':
                        if p.root != p:
                            pp = p.parent
                            p.prune()
                            node.attach(p)
                            pp.attach(node)
                    else:
                        node.attach(p.root)

                if branch:
                    assert(p.lpAttachBranch(node))
                else:
                    assert(p.lpSetChild(node))
                    
                node.setAxisType(AXISMAP[b])   # reset axis type
                if tokens[i-1][1] == 'not': node.setNot(True)
                if node.data['label'][0] == '^': node.lpAlignLeft()
                if node.data['label'][-1] == '$': node.lpAlignRight()

                node.setScope(scope[-1])
                
                p = node
                ret[-1] = p
                i += 1
                branch = False
            elif b in CONNECTIVES:
                p = ret[-2]
                branch = True
            elif b == '@':
                if tokens[i+1][1] == 'label':
                    # lexical node
                    node = LPathTreeModel()
                    node.data['lexical'] = True
                    if tokens[i+2][1] == '=':
                        node.data['label'] = tokens[i+3][1]
                    p.attach(node)
                    p.lpAttachBranch(node)
                    node.setScope(scope[-1])
                if tokens[i+2][1] in OPERATION:
                    i += 3
                else:
                    i += 1
            elif b == '{':
                scope.append(p)
            elif b == '}':
                scope.pop()
        i += 1

    root = root.lpChildren[0]
    #root.lpPrune()
    return root
                
if __name__ == "__main__":
    q = '//VP//NP[==>JJ and ==>NN]'
    t = parse_lpath(q)

    def f(t, n):
        if t is not None:
            print (" "*n) + t.data['label']
            for c in t.children:
                f(c, n+4)
        
    def g(t, n):
        if t is not None:
            print (" "*n) + t.data['label']
            for c in t.lpChildren:
                g(c, n+4)
        else:
            print " "*n + "None"

    g(t,0)
    print translate(t)

########NEW FILE########
__FILENAME__ = qba
import sys
import os
from qt import *
from qtcanvas import *
from treecanvas import *
from treecanvasview import *
from lpathtree_qt import *
from axis import *
from db import *
from dbdialog import *
from sqlviewdialog import *
from overlay import *
from translator import translate
from parselpath import parse_lpath
from lpath import tokenize

class QBA(QMainWindow):
    def __init__(self, tree=None):
        QMainWindow.__init__(self)

        self.setCaption("LPath QBA")
        self.statusBar()   # create a status bar
        self.db = None
        self.queryTree = None  # tree on which LPath query was built
        self.overlayIdx = None
        self.overlays = None
        self.treecanvas = None
        
        menuBar = self.menuBar()
        menu_File = QPopupMenu(menuBar)
        menu_File.insertItem("Save image", self.menu_File_SaveImage)
        menu_File.insertSeparator()
        menu_File.insertItem("E&xit", qApp, SLOT("closeAllWindows()"))
        menu_View = QPopupMenu(menuBar)
        menu_View.insertItem("&SQL Translation", self.menu_View_SqlTranslation)
        menu_Tools = QPopupMenu(menuBar)
        menu_Tools.insertItem("Connect to Database", self.menu_Tools_SetupDatabase)
        menu_Tools.insertItem("Select LPath table", self.menu_Tools_SelectLPathTable)
        menuBar.insertItem("&File", menu_File)
        menuBar.insertItem("&View", menu_View)
        menuBar.insertItem("&Tools", menu_Tools)

        self.cw = QWidget(self)
        self.setCentralWidget(self.cw)
        self.layout = QGridLayout(self.cw)
        self.treeview = TreeCanvasView(self.cw)
        self.layout.addWidget(self.treeview,2,1)
        self.layout.setRowStretch(2,1)

        if tree:
            self.setTree(tree)
            
        hbox = QHBox(self.cw)
        hbox.setSpacing(2)
        hbox.setMargin(3)
        QLabel("LPath\nQuery:", hbox)
        self.entQuery = QTextView(hbox)
        self.entQuery.setFixedHeight(40)
        self.btnQuery = QPushButton("Submit Query", hbox)
        self.btnQuery.setFixedHeight(40)
        self.layout.addWidget(hbox, 1, 1)
        self.layout.setRowStretch(1,0)

        self.toolPanel = QDockWindow(self)
        self.bgrpTools = QButtonGroup(1, Qt.Horizontal, self.toolPanel)
        self.toolPanel.setWidget(self.bgrpTools)
        self.addDockWindow(self.toolPanel, Qt.DockLeft)

        self.bgrpTools.setExclusive(True)
        self.btnClear = QPushButton("Clear", self.bgrpTools)
        self.btnNextTree = QPushButton("Next tree", self.bgrpTools)
        self.btnNextMatch = QPushButton("Next match", self.bgrpTools)
        self.btnNextTree.setEnabled(False)

        self.connect(self.btnClear, SIGNAL("clicked()"), self.clearDisplay)
        self.connect(self.btnQuery, SIGNAL("clicked()"), self.query)
        self.connect(self.btnNextTree, SIGNAL("clicked()"), self.fetchNextTree)
        self.connect(self.btnNextMatch, SIGNAL("clicked()"), self.displayNextOverlay)
        self.connect(self.treeview, PYSIGNAL("changed"), self._setLPath)
        self.connect(self.treeview, PYSIGNAL("highlightLPath"), self._setLPathColor)

        self._saveImageDir = None  # remember the last "save image" directory
        self._queryJustSubmitted = False
        
    def clearDisplay(self):
        self.treeview.clear()
        self.entQuery.clear()
        
    def query(self):
        if self.db is not None:            
            if not self.treeview.canvas(): return
            t = self.treeview.canvas().getTreeModel()
            lpath = translate(t)
            if lpath is None: return

            self.disconnect(self.db.emitter, PYSIGNAL("gotMoreTree"), self.gotMoreTree)

            self._queryJustSubmitted = True
            self.statusBar().message("Submitted the query. Please wait...")
            self.btnNextTree.setEnabled(False)
            if self.db.submitQuery(lpath) == True:
                self.queryTree = parse_lpath(lpath)
            else:
                self.statusBar().message("Query failed.")

    def _setLPath(self):
        t = self.treeview.canvas().getTreeModel()
        lpath = translate(t,space=' ')
        if lpath is None:
            self.entQuery.setText('')
        else:
            self.entQuery.setText(lpath)
    
    def _setLPathColor(self, s):
        self.entQuery.setText(s)
        
    def fetchNextTree(self):
        if self.db:
            res = self.db.fetchNextTree()
            if not res: return
            
            sid, tid, sql, t, ldb, sql2 = res
            self.setCaption("LPath QBA: Tree %s" % sid)
            self.setTree(t)
            self.overlays = find_overlays(sql2, ldb, self.queryTree, t)
            self.overlayIdx = len(self.overlays)-1
            self.displayNextOverlay()

            n = self.db.getNumTreesInMem()
            self.btnNextTree.setText("Next tree (%d)" % n)
            if n == 0:
                self.btnNextTree.setEnabled(False)
                
    def setLineShapeFollowing(self):
        self.treeview.overrideLineShape(AxisFollowing)
    def setLineShapeImmFollowing(self):
        self.treeview.overrideLineShape(AxisImmediateFollowing)
    def setLineShapeSibling(self):
        self.treeview.overrideLineShape(AxisSibling)
    def setLineShapeImmSibling(self):
        self.treeview.overrideLineShape(AxisImmediateSibling)
    def setLineShapeParent(self):
        self.treeview.overrideLineShape(AxisParent)
    def setLineShapeAncestor(self):
        self.treeview.overrideLineShape(AxisAncestor)
        
    def setTree(self, treemodel):
        self.treecanvas = TreeCanvas(treemodel)
        self.treeview.setCanvas(self.treecanvas)
        self.connect(self.treecanvas,PYSIGNAL('treeUpdated'),self._treeUpdated)
        #self.treecanvas.setData(treemodel)

    def _treeUpdated(self, *args):
        self._setLPath()
        
    # menu callbacks
    def menu_File_SaveImage(self):
        if self.treecanvas is None:
            QMessageBox.warning(self, "No image available",
                                "There is not tree image to save.")
            return
        pixmap = self.treecanvas.getAsPixmap()
        d = QFileDialog(self, None, True)
        d.setCaption("Save As")
        d.setMode(QFileDialog.AnyFile)
        if self._saveImageDir:
            d.setDir(self._saveImageDir)
        d.setFilters("PNG (*.png);;"
                     "JPEG (*.jpg);;"
                     "BMP (*.bmp);;"
                     "XPM (*.xpm)")
        if d.exec_loop() == QDialog.Rejected: return
        filenam = d.selectedFile()
        filenam = unicode(filenam)
        self._saveImageDir = os.path.dirname(filenam)
        if os.path.exists(filenam):
            res = QMessageBox.question(
                     self,
                     "Error",
                     "File already exists.\n\n%s\n\n"
                     "It will be overwritten." % filenam,
                     QMessageBox.Ok,
                     QMessageBox.Cancel
                     )
            if res == QMessageBox.Cancel: return
        filter = d.selectedFilter()
        fmt = str(filter).split()[0]
        res = pixmap.save(filenam, fmt)
        if res == False:
            QMessageBox.warning(
                    self,
                    "Error",
                    "Failed to save image as:\n\n%s" % filenam)

    def menu_View_SqlTranslation(self):
        t = self.treeview.canvas().getTreeModel()
        q = translate(t)
        if q.strip():
            d = SqlViewDialog(lpql=q, parent=self)
            d.exec_loop()
        
    def menu_Tools_SetupDatabase(self):
        d = DatabaseConnectionDialog(parent=self)
        if d.exec_loop():
            self.statusBar().message("Connecting to the database...")
            self.db = d.getLPathDb()
            self.statusBar().message("Connecting to the database... ok", 150)
            self.db.connectToEvent(self.db.EVENT_MORE_TREE, self)
            tables = self.db.listTables()
            if len(tables) > 1:
                self.menu_Tools_SelectLPathTable()
            elif len(tables) == 1:
                msg = "Accessing table %s. Please wait..." % tables[0]
                self.statusBar().message(msg)
                self.db.switchLPathTable(tables[0])
                self._queryJustSubmitted = True
        else:
            self.db = None

    def menu_Tools_SelectLPathTable(self):
        # check if self.tables is not empty
        if self.db:
            d = TableSelectionDialog(self.db.listTables())
            if d.exec_loop():
                table = d.getSelectedTable()
                msg = "Accessing table %s. Please wait..." % table
                self.statusBar().message(msg)
                self.db.switchLPathTable(table)
                self._queryJustSubmitted = True
        else:
            QMessageBox.warning(self, "No DB connection",
                                "Connect to a database first.")

    def customEvent(self, e):
        if self.db and e.type()==self.db.EVENT_MORE_TREE:
            self.gotMoreTree(e.data())
        
    def gotMoreTree(self, numTrees):
        self.statusBar().clear()
        self.btnNextTree.setText("Next tree (%d)" % numTrees)
        if numTrees == 0:
            self.btnNextTree.setEnabled(False)
        else:
            self.btnNextTree.setEnabled(True)
            if numTrees == 1 and self._queryJustSubmitted:
                self.statusBar().message("Received the first tree.", 300)
                self.fetchNextTree()
        self._queryJustSubmitted = False
            
    def displayNextOverlay(self):
        if self.overlays:
            self.overlays[self.overlayIdx].clear()
            self.overlayIdx = (self.overlayIdx + 1) % len(self.overlays)
            self.overlays[self.overlayIdx].display()
            self.btnNextMatch.setText("Next match (%d/%d)" % \
                                      (self.overlayIdx+1, len(self.overlays)))
            self._setLPath()

def main():
    app = QApplication(sys.argv)
    w = QBA()
    app.setMainWidget(w)
    if len(sys.argv) == 2:
        generator = LPathTreeModel.importTreebank(file(sys.argv[1]))
        w.setTree(generator.next())
    w.show()
    w.setCaption('LPath QBA')   # this is only necessary on windows
    app.exec_loop()

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = sqlviewdialog
from qt import *
import lpath

class SqlViewDialog(QDialog):
    def __init__(self, lpql=None, parent=None, name=None,
                 modal=False, wflag=0):
        QDialog.__init__(self, parent, name, modal, wflag)

        layout = QGridLayout(self)
        text = QTextEdit(self)
        if lpql is not None:
            sql,_ = lpath.translate2(lpql)
            stat =  " query tokenization: %6.3fs\n"
            stat += "    grammar parsing: %6.3fs\n"
            stat += "      chart parsing: %6.3fs\n"
            stat += "        translation: %6.3fs\n"
            stat = stat % lpath.get_profile()
            text.setText(str(sql))
            font = QFont("Courier")
            font.setBold(True)
            text.setCurrentFont(font)
            text.append("\n\n" + stat)
        layout.addWidget(text,0,0)

        buttons = QHBox(self)
        self.btnDismiss = QPushButton("&Dismiss", buttons)
        layout.addMultiCellWidget(buttons, 1,1,0,0)

        layout.setMargin(5)
        layout.setSpacing(5)
        buttons.setMargin(10)
        buttons.setSpacing(10)
        self.resize(600,300)

        self.connect(self.btnDismiss, SIGNAL("clicked()"), self.accept)

########NEW FILE########
__FILENAME__ = translator
from StringIO import StringIO
import at_lite as at

__all__ = ["translate", "translate_sub"]

def translate_sub(t, selected, space):
    f = StringIO()

    scope = [None]
    while t:
        while scope[-1] and scope[-1] != t.lpScope:
            f.write('}'+space)
            scope.pop()
            
        if len(t.children) == 0:
            if t == selected: f.write('<font color="red">')
            f.write('@label%s=%s"%s"' % (space,space,t.data['label']))
        else:
            ax = translate_axis(t)
            if selected is not None: ax = ax.replace('<','&lt;')
            f.write(ax)
            if t == selected: f.write('<font color="red">')
            if t.lpAlignment() == t.AlignLeft or t.lpAlignment() == t.AlignBoth:
                f.write('^')
            f.write(t.data['label'])
            if t.lpAlignment() == t.AlignRight or t.lpAlignment() == t.AlignBoth:
                f.write('$')

        g = StringIO()
        if '@func' in t.data:
            for v in t.data['@func']:
                g.write('@func="%s" and ' % v)
        if 'lpathFilter' in t.data:
            v = t.data['lpathFilter']
            if selected is not None: v=v.replace('<','&lt;')
            g.write(v+' and ')
        for c in t.lpChildren[1:]:
            h = StringIO()
            if c.getNot():
                h.write('not ')
            h.write(translate_sub(c,selected,space))
            if c.lpScope == t and len(c.children)>0:
                # terminal(=lexical) node doesn't need '{' and '}'
                g.write(space+'{')
                g.write(h.getvalue())
                g.write('}'+space)
            else:
                g.write(h.getvalue())
            g.write(' and ')
            
        if g.getvalue():
            f.write(space+'[')
            f.write(g.getvalue())
            f.write(']'+space)

        if t == selected: f.write('</font>')

        if t.lpChildren[0] and t.lpChildren[0].lpScope == t:
            scope.append(t)
            f.write('{')
            
        t = t.lpChildren[0]

    f.write('}'*(len(scope)-1))

    return f.getvalue().replace(' and ]', ']')

def translate(t, selected=None, space=''):
    L = t.lpRoots()
    if L:
        if t not in L:
            return translate_sub(L[0], selected, space)
        else:
            return translate_sub(t, selected, space)
    
def translate_axis(t):
    if t.lpParent is None:
        return '//'

    x = t.getAxisType()
    n1 = t.lpParent
    n2 = t
    if x == t.AxisFollowing:
        if n1.follows(n2):
            return "<--"
        else:
            return "-->"
    elif x == t.AxisImmediateFollowing:
        if n1.follows(n2):
            return "<-"
        else:
            return "->"
    elif x == t.AxisSibling:
        if n1.follows(n2):
            return "<=="
        else:
            return "==>"
    elif x == t.AxisImmediateSibling:
        if n1.follows(n2):
            return "<="
        else:
            return "=>"
    elif x == t.AxisAncestor:
        if n1.isAncestorOf(n2):
            return "//"
        else:
            return "\\\\"
    elif x == t.AxisParent:
        if n1.isAncestorOf(n2):
            return "/"
        else:
            return "\\"


########NEW FILE########
__FILENAME__ = treecanvas
from qtcanvas import *
from qt import *
from treecanvasnode import *

__all__ = ["TreeCanvas"]

class TreeCanvas(QCanvas):
    """
    TreeCanvas is a canvas on which a tree and LPath query are drawn.
    The tree to be rendered is set when TreeCanvas is instantiated.
    Once the tree is set, it is impossible to replace the currently
    tree with another tree.
    """
    
    def __init__(self, treeModel):
        """
        @type  treeModel: TreeModel
        @param treeModel: tree model that will be rendered on this canvas.
        """
        QCanvas.__init__(self)

        self._depth = {}
        self._width = {}

        self._data = treeModel
        self._w = None     # width and height of the box
        self._h = None     # enclosing the tree
        self._layout()

    def getTreeModel(self):
        """
        @rtype: TreeModel
        @return: tree model currently rendered on the canvas.
        """
        return self._data
    
    def item2node(self, item):
        """
        Deprecated.
        """
        return item.node

    def getAsPixmap(self, left=0,top=0,width=None,height=None):
        """
        Take a snapshot of the canvas. The region to be captured can be
        specified as a rectangle which is described by the top left point
        and witdth and height.
        
        Note: Currently the 4 arguments of the method are just place-holders.
        These arguments are ignored and the image of the whole canvas is
        returned.
        
        @type left: int
        @param left: x coordinate of the top left point.
        @type top: int
        @param top: y coordinate of the top left point.
        @type width: int
        @param width: width of the rectangle.
        @type height: int
        @param height: height of the rectangle.
        @rtype: QPixmap
        @return: a snapshot of the canvas
        """
        if width is None or width > self._w:
            width = self._w
        if height is None or height > self._h:
            height = self._h
            
        pixmap = QPixmap(width, height)
        painter = QPainter(pixmap)
        clip = QRect(left, top, width, height)
        wm = painter.worldMatrix()
        wm.translate(-left, -top)
        painter.setWorldMatrix(wm)
        self.drawArea(clip, painter)
        return pixmap
        
    def resize(self, w, h):
        if w < self._w:
            w = self._w
        if h < self._h:
            h = self._h
        QCanvas.resize(self, w, h)
    
    def collapse(self, node):    
        if node.collapsed == True:
            hide = lambda x:x.hide()
            for c in node.children:
                c.dfs(hide)
        else:
            for c in node.children:
                self.collapse(c)
        
    def redraw(self):
        self._depth = {}
        self._width = {}
        self._computeWidthDepthRecursive(self._data)
        levelHeight = 30
        h = self._depth[self._data] * levelHeight
        self._computeXYRecursive(self._data, 0, 0, h)
        Q = []
        self._data.dfs(lambda x:Q.append(x))
        self._data.gui.show()
        for node in Q[1:]:
            item = node.gui
            item2 = node.parent.gui
            coords = item.connectingLine(item2)
            apply(node.line.setPoints, coords)
            node.show()
            
        self.collapse(self._data)
        
        self._w = self._width[self._data]
        self._h = h + levelHeight
        w = max(self.width(), self._w)
        h = max(self.height(), self._h)
        QCanvas.resize(self, w, h)
        
        for root in self._data.lpRoots():
            root.lpDfs(lambda x:x.redrawAxis())
        self.update()
        
    def _layout(self):
        self._depth = {}
        self._width = {}
        
        self._data.dfs(TreeCanvasNode, self)
        self._computeWidthDepthRecursive(self._data)

        levelHeight = 30
        h = self._depth[self._data] * levelHeight
        self._computeXYRecursive(self._data, 0, 0, h)

        Q = []
        self._data.dfs(lambda x:Q.append(x))
        self._data.gui.show()
        for node in Q[1:]:
            item = node.gui
            item2 = node.parent.gui
            coords = item.connectingLine(item2)
            pen = QPen(QColor(0xc0,0xc0,0xc0))
            pen.setStyle(Qt.DotLine)
            line = QCanvasLine(self)
            line.setPen(pen)
            node.line = line
            apply(line.setPoints, coords)
            node.show()
            
        self._w = self._width[self._data]
        self._h = h + levelHeight
        QCanvas.resize(self, self._w, self._h)

    def _pad(self, node, w):
        if node.children and node.collapsed == False:
            d = w / len(node.children)
            for c in node.children:
                self._width[c] += d
                self._pad(c, d)

    def _computeWidthDepthRecursive(self, node):
        sumwidth = 0.0
        maxdepth = 0
        if node.collapsed == False:
            for c in node.children:
                w,d = self._computeWidthDepthRecursive(c)
                sumwidth += w
                if d > maxdepth: maxdepth = d
        width = node.gui.boundingRect().width() + 10.0
        if sumwidth < width:
            self._pad(node, width-sumwidth)
        else:
            width = sumwidth
        #if not node.children: width += 10
        maxdepth += 1
        self._depth[node] = maxdepth
        self._width[node] = width
        return width, maxdepth

    def _computeXYRecursive(self, node, x, y, height):
        item = node.gui

        if node.children and node.collapsed == False:
            fromLeft = 0
            fromTop = height / (self._depth[node]-1)
            y1 = y + fromTop
            h1 = height - fromTop
            for c in node.children:
                self._computeXYRecursive(c, x+fromLeft, y1, h1)
                fromLeft += self._width[c]

            c1 = node.children[0].gui
            c2 = node.children[-1].gui
            x1 = c1.boundingRect().left()
            x2 = c2.boundingRect().left() + c2.width()
            if abs(item.boundingRect().left()-item.x()) < 1.0:
                item.setX( (x1 + x2 - item.width()) / 2.0 )
            else:
                item.setX( (x1 + x2 + item.width()) / 2.0 )
        else:
            if abs(item.boundingRect().left()-item.x()) < 1.0:
                item.setX(x + (self._width[node] - item.width()) / 2.0)
            else:
                item.setX(x + (self._width[node] + item.width()) / 2.0)
        item.setY(y)

    def signal(self, *args):
        self.emit(PYSIGNAL('treeUpdated'),args)

########NEW FILE########
__FILENAME__ = treecanvasnode
from qt import *
from qtcanvas import *
from lpathtree_qt import *

class Point:
    def __init__(self, *args):
        if len(args) == 2 and \
           (isinstance(args[0],int) or isinstance(args[0],float)) and \
           (isinstance(args[1],int) or isinstance(args[0],float)):
            self.x = float(args[0])
            self.y = float(args[1])
        elif len(args) == 1 and \
             isinstance(args[0],QPoint):
            self.x = float(args[0].x())
            self.y = float(args[0].y())
        else:
            raise TypeError("invalid argument type")

    def __add__(self, p):
        if not isinstance(p,Point):
            raise TypeError("invalid argument type")
        return Point(self.x+p.x, self.y+p.y)

    def __sub__(self, p):
        if not isinstance(p,Point):
            raise TypeError("invalid argument type")
        return Point(self.x-p.x, self.y-p.y)

    def __mul__(self, n):
        if not isinstance(n,int) and \
           not isinstance(n,float):
            raise TypeError("invalid argument type")
        n = float(n)
        return Point(self.x*n,self.y*n)

    def __div__(self, n):
        if not isinstance(n,int) and \
           not isinstance(n,float):
            raise TypeError("invalid argument type")
        n = float(n)
        return Point(self.x/n,self.y/n)

    
class TreeCanvasNode(QCanvasText):
    def __init__(self, node=None, canvas=None):
        assert(isinstance(node,LPathTreeModel))
        if 'label' in node.data and node.data['label']:
            QCanvasText.__init__(self, node.data['label'], canvas)
        else:
            QCanvasText.__init__(self, '', canvas)
        node.gui = self

        self.numberWidget = QCanvasText(canvas)
        self.numberWidget.setColor(Qt.lightGray)
        self.numberHidden = True
        self.node = node
        self.triangle = QCanvasPolygon(canvas)
        self.triangle.setBrush(QBrush(Qt.gray))

    def hide(self):
        self.numberWidget.hide()
        self.triangle.hide()
        QCanvasText.hide(self)
        
    def draw(self, painter):
        self.updateNumber()
        alignment = self.node.lpAlignment()
        if alignment == self.node.AlignLeft:
            self.setText('^'+self.node.data['label'])
        elif alignment == self.node.AlignRight:
            self.setText(self.node.data['label']+'$')
        elif alignment == self.node.AlignBoth:
            self.setText("^%s$" % self.node.data['label'])
        elif self.node.data['label']:
            self.setText(self.node.data['label'])
        else:
            self.setText('')

        if self.node.collapsed:
            dw = self.width() / 2.0
            x1 = self.x() + dw
            y1 = self.y() + self.height()
            pa = QPointArray(3)
            pa.setPoint(0, x1,y1)
            pa.setPoint(1, x1-dw,y1+self.height())
            pa.setPoint(2, x1+dw,y1+self.height())
            self.triangle.setPoints(pa)
            self.triangle.show()
        else:
            self.triangle.hide()
            
        QCanvasText.draw(self, painter)

    def clear(self):
        f = self.font()
        f.setUnderline(False)
        self.setFont(f)
        
    def width(self):
        return self.boundingRect().width()

    def height(self):
        return self.boundingRect().height()

    def intersection(self, item):
        p = Point(item.boundingRect().center())
        box = self.boundingRect()
        c = Point(box.center())
        v = p - c
        if self == item:
            return c
        elif v.x != 0:
            v = v / abs(v.x)
        elif v.y > 0:
            return Point(c.x,box.bottom())
        else:
            return Point(c.x,box.top())
        v1 = Point(box.bottomRight() - box.topLeft())
        if v1.x > 0.0:
            v1 = v1 / v1.x

        if abs(v.y) < v1.y:
            dx = box.width() / 2.0
            x = c.x + dx * v.x
            y = c.y + dx * v.y
        else:
            if v.y != 0:
                v = v / abs(v.y)
                dy = box.height() / 2.0
                x = c.x + dy * v.x
                y = c.y + dy * v.y
            elif v.x > 0:
                x = box.right()
                y = c.y
            else:
                x = box.left()
                y = c.y
        return Point(x, y)

    def connectingLine(self, item):
        p1 = self.intersection(item)
        p2 = item.intersection(self)
        return p1.x,p1.y,p2.x,p2.y

    def updateNumber(self):
        if self.node.lpIsolated():
            self.numberHidden = True
            self.numberWidget.hide()
        else:
            number = self.node.lpScopeDepth()
            c = self.canvas()
            w = self.numberWidget
            c.setChanged(w.boundingRect())
            w.setText("%d" % number)
            r = self.boundingRect()
            wr = w.boundingRect()
            wy = r.top() - wr.height()
            wx = r.left() + (r.width() - wr.width()) / 2.0
            w.move(wx,wy)
            c.setChanged(w.boundingRect())
            self.numberHidden = False
            w.show()

    def getNumber(self):
        self.node.lpScopeDepth()

    def updateTrace(self):
        f = self.font()
        f.setUnderline(self.node.filterExpression is not None)
        self.setFont(f)
        self.canvas().update()
        
if __name__ == "__main__":
    from qt import *
    app = QApplication([])
    c = QCanvas(100,100)
    c.setBackgroundColor(Qt.blue)
    w = QCanvasView(c)
    n = TreeCanvasNode("test",c)
    n.setColor(Qt.red)
    n.show()
    
    app.setMainWidget(w)
    w.show()
    app.exec_loop()

########NEW FILE########
__FILENAME__ = treecanvasview
from qt import *
from qtcanvas import *
from treecanvasnode import *
from nodefeaturedialog import *
from translator import translate
from axis import *
import lpath
import math

class FilterExpressionPopup(QLabel):
    def __init__(self, parent):
        self.super = QLineEdit
        self.super.__init__(self, parent)
        self.super.hide(self)
        self.timer = None
        self.locked = False
        
    def popup(self, p, node):
        self.node = node
        r = QRect(p,QSize(100,20))
        self.setGeometry(r)
        if node.filterExpression:
            self.setText(node.filterExpression)
        else:
            self.setText('')
        self.show()
        self.locked = False

    def show(self):
        self.stopTimer()
        self.super.show(self)
        
    def startTimer(self):
        self.stopTimer()
        self.timer = self.super.startTimer(self,300)
        
    def stopTimer(self):
        if self.timer is not None:
            self.killTimer(self.timer)
            self.timer = None
            
    def hide(self):
        if not self.locked:
            self.stopTimer()
            self.startTimer()
    
    def getText(self):
        return self._text
    
    def timerEvent(self, e):
        if e.timerId() == self.timer:
            self.super.hide(self)
            self.stopTimer()
        else:
            self.super.timerEvent(self, e)
            
    def mousePressEvent(self, e):
        self.super.hide(self)
        s,ans = QInputDialog.getText('Edit Filter Expression','Enter new filter expression',
                                     QLineEdit.Normal,self.text(),self)
        if ans:
            s = unicode(s).strip()
            if s:
                self.node.filterExpression = s
            else:
                del self.node.filterExpression
        
class TreeCanvasView(QCanvasView):

    AXIS_TOGGLE_MAP = {
        AxisParent:AxisAncestor,
        AxisAncestor:AxisParent,
        AxisImmediateSibling:AxisSibling,
        AxisSibling:AxisImmediateFollowing,
        AxisImmediateFollowing:AxisFollowing,
        AxisFollowing:AxisImmediateSibling,
        }
    
    def __init__(self, *args):
        QCanvasView.__init__(self, *args)
        self.setMouseTracking(True)
        self.viewport().setMouseTracking(True)
        self.filterExpPopup = FilterExpressionPopup(self)
        self.rightClickPopup = QPopupMenu(self)
        self.rightClickPopup.insertItem('Shift &Scope',self.processRightClickPopup,0,1)
        self.rightClickPopup.insertItem('&Toggle Left Alignment',self.processRightClickPopup,0,2)
        self.rightClickPopup.insertItem('Toggle &Right Alignment',self.processRightClickPopup,0,7)
        self.rightClickPopup.insertItem('Edit &Label',self.processRightClickPopup,0,5)
        self.rightClickPopup.insertItem('Edit &Filter Expression',self.processRightClickPopup,0,3)
        self.rightClickPopup.insertItem('Delete Filter E&xpression',self.processRightClickPopup,0,4)
        self.rightClickPopup.insertItem('Node Attribute &Dialog',self.processRightClickPopup,0,6)
        self.rightClickPopup.insertItem('&Reset',self.processRightClickPopup,0,8)
        self.rightClickPopupNode = None  # temporary storage of right-clicked node
        self._initialize()

    def _initialize(self):
        self.highlightedNode = None
        self.selectedNode = None
        self.clickCount = 0
        
    def setCanvas(self, c):
        self._initialize()
        c.resize(self.width(), self.height())
        QCanvasView.setCanvas(self, c)
        
    def processRightClickPopup(self, *args):
        item = self.rightClickPopupNode
        menuid, = args
        if menuid == 1:
            item.node.shiftScope()
        elif menuid == 2:
            a = item.node.lpAlignment()
            if a == item.node.AlignNone or a == item.node.AlignRight:
                item.node.lpAlignLeft()
            else:
                item.node.lpClearAlignment()
                if a == item.node.AlignBoth:
                    item.node.lpAlignRight()
        elif menuid == 7:
            a = item.node.lpAlignment()
            if a == item.node.AlignNone or a == item.node.AlignLeft:
                item.node.lpAlignRight()
            else:
                item.node.lpClearAlignment()
                if a == item.node.AlignBoth:
                    item.node.lpAlignLeft()
                
        elif menuid == 3:
            s = item.node.filterExpression
            if s is None: s = ''
            while True:
                s,ans = QInputDialog.getText('New Filter Expression','Enter filter expression',
                                             QLineEdit.Normal,s,self)
                if ans:
                    s = unicode(s).strip()
                    if s:
                        if lpath.translate("//A[%s]"%s) is None:
                            QMessageBox.critical(self,"Error","Invalid filter expression.")
                            continue
                        else:
                            item.node.filterExpression = s
                break
                        
        elif menuid == 4 and item.node.filterExpression is not None:
            del item.node.filterExpression
        elif menuid == 5:
            s,ans = QInputDialog.getText('Edit Label','Enter new label',
                                         QLineEdit.Normal,item.node.label,self)
            if ans:
                s = unicode(s).strip()
                if s:
                    if 'originalLabel' not in item.node.data:
                        item.node.data['originalLabel'] = item.node.label
                    item.node.label = s
        elif menuid == 6:
            d = NodeFeatureDialog(item.node, self)
            d.exec_loop()
        elif menuid == 8:
            if 'originalLabel' in item.node.data:
                item.node.label = item.node.data['originalLabel']
                del item.node.data['originalLabel']
            item.node.lpClearAlignment()
            del item.node.filterExpression

    def enableDisableRightClickPopupMenuItems(self):
        item = self.rightClickPopupNode
        # menuid == 1:
        self.rightClickPopup.setItemEnabled(1, item.node.canShiftScope())

        # menuid == 2:
        self.rightClickPopup.setItemEnabled(2, item.node.lpLeftAlignable())

        # menuid == 7:
        self.rightClickPopup.setItemEnabled(7, item.node.lpRightAlignable())
        
        # menuid == 3:
        self.rightClickPopup.setItemEnabled(3, len(item.node.children) != 0)
        
        # menuid == 4:
        self.rightClickPopup.setItemEnabled(4, item.node.filterExpression is not None)

        # menuid == 5:
        self.rightClickPopup.setItemEnabled(5, True)
        
        # menuid == 6:
        
        # menuid == 8
        self.rightClickPopup.setItemEnabled(8, 'originalLabel' in item.node.data or \
                                            item.node.lpAlignment()!=item.node.AlignNone or \
                                            item.node.filterExpression is not None)
        
    def mousePressEvent(self, e):
        b = e.button()
        c = self.canvas()
        if b not in (Qt.LeftButton,Qt.RightButton,Qt.MidButton) or c is None:
            return
        axes = []
        nodes = []
        p = self.viewportToContents(e.pos())
        for item in c.collisions(p):
            if isinstance(item,TreeCanvasNode):
                nodes.append(item)
            elif isinstance(item,Axis):
                axes.append(item)
        if b == Qt.LeftButton:
            if nodes:
                item = nodes[0]
                node = item.node
                if self.selectedNode:
                    if item != self.selectedNode:
                        if not self.selectedNode.lpSetChild(node):
                            self.selectedNode.lpAttachBranch(node)
                        if len(node.children) > 0:
                            self.selectNode(node)
                else:
                    self.selectNode(node)
                self.emitLPath()
            elif axes:
                # toggle through different axis
                item = axes[0]
                self.overrideLineShape(item, self.AXIS_TOGGLE_MAP[item.__class__])
                self.emitLPath()
                    
        elif b == Qt.RightButton:
            if nodes:
                item = nodes[0]
                self.rightClickPopupNode = item
                p = self.mapToGlobal(self.contentsToViewport(item.boundingRect().bottomLeft()))
                # don't want to edit features on terminal node
                #enableVal = len(item.node.children) != 0
                #for i in (1,2,3,4,6):
                #    self.rightClickPopup.setItemEnabled(i,enableVal)
                self.enableDisableRightClickPopupMenuItems()
                self.rightClickPopup.popup(p)
            elif axes:
                item = axes[0]
                if item.target.lpOnMainTrunk():
                    # -> branch
                    item.root.lpAttachBranch(item.target)
                elif item.target.getNot():
                    if item.root.lpChildren[0] or len(item.target.children)==0:
                        # -> branch
                        item.target.setNot(False)
                    else:
                        # -> main trunk
                        item.root.lpSetChild(item.target)
                else:
                    # -> negation
                    item.target.setNot(True)
                self.emitLPath()
            else:
                self.unselectNode()
        elif b == Qt.MidButton:
            if nodes:
                item = nodes[0]
                item.node.collapsed = not item.node.collapsed
            elif axes:
                item = axes[0]
                item.target.lpPrune()
                self.emitLPath()
                    
    def resetNode(self, node):
        if 'originalLabel' in node.data:
            node.label = node.data['originalLabel']
            del node.data['originalLabel']
        node.lpClearAlignment()
        del node.filterExpression
    
    def mousePressEventHandler_destMode(self, e):
        if e.button() == Qt.LeftButton:
            self.mousePressEventHandler_removeLine(e)
        elif e.button() == Qt.RightButton:
            self.unselectNode()
        
    def highlight(self, node):
        #if node != self.selectedNode:
        node.gui.setColor(Qt.red)
        self.canvas().update()
        self.highlightedNode = node

    def unhighlight(self):
        if self.highlightedNode and self.highlightedNode != self.selectedNode:
            self.highlightedNode.gui.setColor(Qt.black)
            self.canvas().update()
        self.highlightedNode = None

    def selectNode(self, node):
        self.unselectNode()
        item = node.gui
        item.setColor(Qt.red)
        self.canvas().update()
        self.selectedNode = node

    def unselectNode(self):
        if self.selectedNode:
            self.selectedNode.gui.setColor(Qt.black)
            self.canvas().update()
            self.selectedNode = None

    def mouseMoveEvent(self, e):
        c = self.canvas()
        if not c: return

        p = self.viewportToContents(e.pos())
        for item in c.collisions(p):
            if isinstance(item, TreeCanvasNode):
                if item.node != self.highlightedNode:
                    if item.node.filterExpression:
                        x = self.contentsToViewport(item.boundingRect().topRight())
                        self.filterExpPopup.popup(x, item.node)
                    self.unhighlight()
                    self.highlight(item.node)
                    
                    s = translate(item.node.lpRoot, item.node, ' ')
                    if s is not None:
                        self.emit(PYSIGNAL('highlightLPath'),(s,))
                break
        else:
            if self.highlightedNode is not None:
                self.filterExpPopup.hide()
                self.unhighlight()

    def _computeAxisType(self, n1, n2):
        c = self.canvas()

        if n1.parent == n2 or n2.parent == n1:
            return AxisParent

        if n1.rightSibling == n2 or n2.rightSibling == n1:
            return AxisImmediateSibling

        if n1.parent == n2.parent:
            return AxisSibling

        a1L = [n1]
        p = n1.parent
        while p:
            if p == n2:
                return AxisAncestor
            a1L.append(p)
            p = p.parent

        a2L = [n2]
        p = n2.parent
        while p:
            if p == n1:
                return AxisAncestor
            a2L.append(p)
            p = p.parent

        for i,x in enumerate(a1L):
            try:
                j = a2L.index(x)
                break
            except ValueError:
                pass

        a1L = a1L[:i]
        a2L = a2L[:j]
        i = x.children.index(a1L.pop())
        j = x.children.index(a2L.pop())

        if abs(i-j) > 1:
            return AxisFollowing
        
        if i < j:
            L1 = a1L
            L2 = a2L
        else:
            L1 = a2L
            L2 = a1L

        for x in L1:
            if x.rightSibling:
                return AxisFollowing
        for x in L2:
            if x.leftSibling:
                return AxisFollowing

        return AxisImmediateFollowing
            

    def _scopicRoot(self, n):
        num = n.getNumber()
        L = [(n1,l) for n1,n2,l in self.edges if n2==n]
        while L and L[0][0].getNumber() >= num:
            n = L[0][0]
            L = [(n1,l) for n1,n2,l in self.edges if n2==n]
        else:
            if L:
                return L[0][0]
        return None
            
    def _computeNestLevel(self, n1, n2):
        m1 = self.canvas().item2node(n1)
        m2 = self.canvas().item2node(n2)
        if m1.isAncestorOf(m2):
            return n1.getNumber() + 1
        else:
            r = self._scopicRoot(n1)
            if r:
                rnode = self.canvas().item2node(r)
                if rnode.isAncestorOf(m2):
                    return r.getNumber() + 1
                else:
                    return self._computeNestLevel(r, n2)
            else:
                return 0

    def overrideLineShape(self, item, cls):
        clsName = str(cls).split("'")[1].split('.')[-1]
        item.target.setAxisType(clsName)
        self.emitLPath()

    def clear(self):
        c = self.canvas()
        if c is None: return
        tree = c.getTreeModel()
        def g(t,L): L.append(t)
        L = []
        for r in tree.lpRoots(): r.lpDfs(g,L)
        for n in L:
            n.lpPrune()
            #n.lpClearAlignment()
            self.resetNode(n)
            n.gui.clear()
        self.unselectNode()
        c.update()


    def emitLPath(self):
        self.emit(PYSIGNAL("changed"),())
                        
    def resizeEvent(self, e):
        QCanvasView.resizeEvent(self, e)
        if self.canvas():
            self.canvas().resize(e.size().width(),e.size().height())
            

########NEW FILE########
__FILENAME__ = annotationgraph
# Natural Language Toolkit: Annotation Graphs
#
# Copyright (C) 2001-2011 NLTK Project
# Author: Steven Bird <sb@csse.unimelb.edu.au>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

from nltk import Tree, Index

class AnnotationGraph(object):
    
    def __init__(self, t):
        self._edges = []
        self._len = len(t.leaves())
        self._nodes = range(self._len)
        self._convert(t, 0)
        self._index = Index((start, (end, label)) for (start, end, label) in self._edges)
    
    def _convert(self, t, start):
        next = start
        if isinstance(t, Tree):
            label = t.node
            for child in t:
                next = self._convert(child, next)
        else:
            label = t
            next += 1
        
        edge = (start, next, label)
        self._edges.append(edge)
        return next

    def edges(self):
        return self._edges
    
    # partial proper analyses
    def ppa(self, maxlen=None, left=0):
        if maxlen is None:
            maxlen = self._len
        if maxlen == 0:
            yield []
        else:
            for i in range(left, self._len):  # each starting node
                for (right, label) in self._index[i]:  # each edge starting here
                    for continuation in self.ppa(maxlen-1, right):
                        if continuation == []:
                            yield [label]
                        else:
                            result = [label]
                            result.extend(continuation)
                            yield result
    
    # proper analysis segments
    def pas(self, Vr):
        for i in range(self._len):
            for p in self.pas2(Vr, i):
                yield p
        
    def pas2(self, Vr, left=0):
        if left == self._len:
            yield []
        for (right, label) in self._index[left]:
            for continuation in self.pas2(Vr, right):
                if continuation == []:
                    yield [label]
                else:
                    result = [label]
                    result.extend(continuation)
                    yield result


def demo():
    s = '(S (NP (DT the) (NN cat)) (VP (VBD ate) (NP (DT a) (NN cookie))))'
    s = '(VP (VBD ate) (NP (DT a) (NN cookie)))'
    t = Tree(s)
    ag = AnnotationGraph(t)
    for p in ag.pas2([]):
        print p

if __name__ == '__main__':
    demo()

    
########NEW FILE########
__FILENAME__ = didyoumean
# Spelling corrector by Maxime Biais http://www.biais.org/blog/
# http://snippets.dzone.com/posts/show/3395

from nltk import PorterStemmer
from nltk.corpus import brown
 
import sys
from collections import defaultdict
import operator
 
def sortby(nlist ,n, reverse=0):
    nlist.sort(key=operator.itemgetter(n), reverse=reverse)
 
class mydict(dict):
    def __missing__(self, key):
        return 0
 
class DidYouMean:
    def __init__(self):
        self.stemmer = PorterStemmer()
 
    def specialhash(self, s):
        s = s.lower()
        s = s.replace("z", "s")
        s = s.replace("h", "")
        for i in [chr(ord("a") + i) for i in range(26)]:
            s = s.replace(i+i, i)
        s = self.stemmer.stem(s)
        return s
 
    def test(self, token):
        hashed = self.specialhash(token)
        if hashed in self.learned:
            words = self.learned[hashed].items()
            sortby(words, 1, reverse=1)
            if token in [i[0] for i in words]:
                return 'This word seems OK'
            else:
                if len(words) == 1:
                    return 'Did you mean "%s" ?' % words[0][0]
                else:
                    return 'Did you mean "%s" ? (or %s)' \
                           % (words[0][0], ", ".join(['"'+i[0]+'"' \
                                                      for i in words[1:]]))
        return "I can't found similar word in my learned db"
 
    def learn(self, listofsentences=[], n=2000):
        self.learned = defaultdict(mydict)
        if listofsentences == []:
            listofsentences = brown.sents()
        for i, sent in enumerate(listofsentences):
            if i >= n: # Limit to the first nth sentences of the corpus
                break
            for word in sent:
                self.learned[self.specialhash(word)][word.lower()] += 1
 
def demo():
    d = DidYouMean()
    d.learn()
    # choice of words to be relevant related to the brown corpus
    for i in "birdd, oklaoma, emphasise, bird, carot".split(", "):
        print i, "-", d.test(i)
 
if __name__ == "__main__":
    demo()

########NEW FILE########
__FILENAME__ = fsa
# Natural Language Toolkit: Finite State Automata
#
# Copyright (C) 2001-2006 NLTK Project
# Authors: Steven Bird <sb@ldc.upenn.edu>
#          Rob Speer <rspeer@mit.edu>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

"""
A module for finite state automata. 
Operations are based on Aho, Sethi & Ullman (1986) Chapter 3.
"""

from nltk import tokenize, Tree
from nltk.grammar import WeightedProduction, Nonterminal, WeightedGrammar
from nltk.parse import InsideChartParser
import yaml

epsilon = None

# some helper functions

# TODO - check that parse was complete, and report error otherwise

class FSA(yaml.YAMLObject):
    """
    A class for finite state automata. In general, it represents
    nondetermnistic finite state automata, with DFAs being a special case.
    """
    yaml_tag = '!FSA'
    def __init__(self, sigma='', transitions=None, start=0, finals=None):
        """Set up the FSA.

        @param sigma: the alphabet of the FSA
        @type sigma: sequence
        @param transitions: A dictionary representing the states and
        transitions in the FSA. The keys are state identifiers (any hashable
        object), and the values are dictionaries that map input symbols to the
        sets of states they lead to.
        @type transitions: dict
        @param start: The identifier of the start state
        @type start: hashable object
        @param finals: The identifiers of the accept states
        @type finals: sequence
        """
        self._transitions = transitions or {0: {}}
        self._start = start
        self._reverse = {}
        self._build_reverse_transitions()
        if finals: self._finals = set(finals)
        else: self._finals = set([0])
        self._sigma = set(sigma)
        assert isinstance(self._transitions, dict)
        self._next_state_num = 0

    def _build_reverse_transitions(self):
        for state in self._transitions:
            self._reverse.setdefault(state, {})
        for (state, symbol, target) in self.generate_transitions():
            self._add_transition(self._reverse, target, symbol, state)

    def generate_transitions(self):
        """
        A generator that yields each transition arrow in the FSA in the form
        (source, label, target).
        """
        for (state, map) in self._transitions.items():
            for (symbol, targets) in map.items():
                for target in targets:
                    yield (state, symbol, target)

    def labels(self, s1, s2):
        """
        A generator for all possible labels taking state s1 to state s2.
        """
        map = self._transitions.get(s1, {})
        for (symbol, targets) in map.items():
            if s2 in targets: yield symbol
    
    def sigma(self):
        "The alphabet of the FSA."
        return self._sigma
    alphabet = sigma

    def check_in_sigma(self, label):
        "Check whether a given object is in the alphabet."
        if label and label not in self._sigma:
            raise ValueError('Label "%s" not in alphabet: %s' % (label, str(self._sigma)))
    
    def __len__(self):
        "The number of states in the FSA."
        return len(self._transitions)
    
    def new_state(self):
        """
        Add a new state to the FSA.
        @returns: the ID of the new state (a sequentially-assigned number).
        @rtype: int
        """
        while self._next_state_num in self._transitions:
            self._next_state_num += 1
        self._transitions[self._next_state_num] = {}
        self._reverse[self._next_state_num] = {}
        return self._next_state_num

    def add_state(self, name):
        self._transitions[name] = {}
        self._reverse[name] = {}
        return name

    def start(self):
        """
        @returns: the ID of the FSA's start state.
        """
        return self._start

    def finals(self):
        """
        @returns: the IDs of all accept states.
        @rtype: set
        """
        # was a tuple before
        return self._finals

    def states(self):
        """
        @returns: a list of all states in the FSA.
        @rtype: list
        """
        return self._transitions.keys()
    
    def add_final(self, state):
        """
        Make a state into an accept state.
        """
        self._finals.add(state)

    def delete_final(self, state):
        """
        Make an accept state no longer be an accept state.
        """
        self._finals = self._finals.difference(set([state]))
#        del self._finals[state]

    def set_final(self, states):
        """
        Set the list of accept states.
        """
        self._finals = set(states)

    def set_start(self, start):
        """
        Set the start state of the FSA.
        """
        self._start = start
    
    def in_finals(self, list):
        """
        Check whether a sequence contains any final states.
        """
        return [state for state in list
                if state in self.finals()] != []

    def insert_safe(self, s1, label, s2):
        if s1 not in self.states():
            self.add_state(s1)
        if s2 not in self.states():
            self.add_state(s2)
        self.insert(s1, label, s2)

    def insert(self, s1, label, s2):
        """
        Add a new transition to the FSA.

        @param s1: the source of the transition
        @param label: the element of the alphabet that labels the transition
        @param s2: the destination of the transition
        """
        if s1 not in self.states():
            raise ValueError, "State %s does not exist" % s1
        if s2 not in self.states():
            raise ValueError, "State %s does not exist" % s1
        self._add_transition(self._transitions, s1, label, s2)
        self._add_transition(self._reverse, s2, label, s1)

    def _add_transition(self, map, s1, label, s2):
        mapping = map[s1]
        targets = mapping.setdefault(label, set())
        targets.add(s2)

    def _del_transition(self, map, s1, label, s2):
        mapping = map[s1]
        targets = mapping.setdefault(label, set())
        targets.remove(s2)
        if len(targets) == 0: del mapping[label]

    def delete(self, s1, label, s2):
        """
        Removes a transition from the FSA.

        @param s1: the source of the transition
        @param label: the element of the alphabet that labels the transition
        @param s2: the destination of the transition
        """
        if s1 not in self.states():
            raise ValueError, "State %s does not exist" % s1
        if s2 not in self.states():
            raise ValueError, "State %s does not exist" % s1
        self._del_transition(self._transitions, s1, label, s2)
        self._del_transition(self._reverse, s2, label, s1)

    def delete_state(self, state):
        "Removes a state and all its transitions from the FSA."
        if state not in self.states():
            raise ValueError, "State %s does not exist" % state
        for (s1, label, s2) in self.incident_transitions(state):
            self.delete(s1, label, s2)
        del self._transitions[state]
        del self._reverse[state]

    def incident_transitions(self, state):
        """
        @returns: a set of transitions into or out of a state.
        @rtype: set
        """
        result = set()
        forward = self._transitions[state]
        backward = self._reverse[state]
        for label, targets in forward.items():
            for target in targets:
                result.add((state, label, target))
        for label, targets in backward.items():
            for target in targets:
                result.add((target, label, state))
        return result

    def relabel_state(self, old, new):
        """
        Assigns a state a new identifier.
        """
        if old not in self.states():
            raise ValueError, "State %s does not exist" % old
        if new in self.states():
            raise ValueError, "State %s already exists" % new
        changes = []
        for (s1, symbol, s2) in self.generate_transitions():
            if s1 == old and s2 == old:
                changes.append((s1, symbol, s2, new, symbol, new))
            elif s1 == old:
                changes.append((s1, symbol, s2, new, symbol, s2))
            elif s2 == old:
                changes.append((s1, symbol, s2, s1, symbol, new))
        for (leftstate, symbol, rightstate, newleft, newsym, newright)\
        in changes:
            self.remove(leftstate, symbol, rightstate)
            self.insert(newleft, newsym, newright)
        del self._transitions[old]
        del self._reverse[old]

    def next(self, state, symbol):
        "The set of states reached from a certain state via a given symbol."
        return self.e_closure(self._transitions[state].get(symbol, set()))
    nextStates = next
    
    def move(self, states, symbol):
        "The set of states reached from a set of states via a given symbol."
        result = set()
        for state in states:
            result = result.union(self.next(state, symbol))
        return self.e_closure(result)

    def is_deterministic(self):
        """
        Return whether this is a DFA
        (every symbol leads from a state to at most one target state).
        """
        for map in self._transitions.values():
            for targets in map.values():
                if len(targets) > 1: return False
        return True
    
    def nextState(self, state, symbol):
        """
        The single state reached from a state via a given symbol.
        If there is more than one such state, raises a ValueError.
        If there is no such state, returns None.
        """
        next = self.next(state, symbol)
        if len(next) > 1:
            raise ValueError, "This FSA is nondeterministic -- use nextStates instead."
        elif len(next) == 1: return list(next)[0]
        else: return None

    def forward_traverse(self, state):
        "All states reachable by following transitions from a given state."
        result = set()
        for (symbol, targets) in self._transitions[state].items():
            result = result.union(targets)
        return result

    def reverse_traverse(self, state):
        """All states from which a given state is reachable by following
        transitions."""
        result = set()
        for (symbol, targets) in self._reverse[state].items():
            result = result.union(targets)
        return result
    
    def _forward_accessible(self, s1, visited):
        for s2 in self.forward_traverse(s1):
            if not s2 in visited:
                visited.add(s2)
                self._forward_accessible(s2, visited)
        return visited
                
    def _reverse_accessible(self, s1, visited):
        for s2 in self.reverse_traverse(s1):
            if not s2 in visited:
                visited.add(s2)
                self._reverse_accessible(s2, visited)
        return visited
        
    # delete inaccessible nodes and unused transitions
    def prune(self):
        """
        Modifies an FSA to remove inaccessible states and unused transitions.
        """
        acc = self.accessible()
        for state in self.states():
            if state not in acc:
               self.delete_state(state)
            else:
                self._clean_map(self._transitions[state])
                self._clean_map(self._reverse[state])

    def _clean_map(self, map):
        for (key, value) in map.items():
            if len(value) == 0:
                del map[key]

    # mark accessible nodes
    def accessible(self):
        acc = set()
        for final in self.finals():
            reverse_acc = set([final])
            self._reverse_accessible(final, reverse_acc)
            acc = acc.union(reverse_acc)

        forward_acc = set([self.start()])
        self._forward_accessible(self.start(), forward_acc)

        acc = acc.intersection(forward_acc)
        return acc
    
    def e_closure(self, states):
        """
        Given a set of states, return the set of states reachable from
        those states by following epsilon transitions.

        @param states: the initial set of states
        @type states: sequence
        @returns: a superset of the given states, reachable by epsilon
        transitions
        @rtype: set
        """
        stack = list(states)
        closure = list(states)
        while stack:
            s1 = stack.pop()
            for s2 in self.next(s1, epsilon):
                if s2 not in closure:
                    closure.append(s2)
                    stack.append(s2)
        return set(closure)
    
    # return the corresponding DFA using subset construction (ASU p118)
    # NB representation of (a*) still isn't minimal; should have 1 state not 2
    def dfa(self):
        "Return a DFA that is equivalent to this FSA."
        dfa = FSA(self.sigma())
        dfa_initial = dfa.start()
        nfa_initial = tuple(self.e_closure((self.start(),)))
        map = {}
        map[dfa_initial] = nfa_initial
        map[nfa_initial] = dfa_initial
        if nfa_initial in self.finals():
            dfa.add_final(dfa_initial)
        unmarked = [dfa_initial]
        marked = []
        while unmarked:
            dfa_state = unmarked.pop()
            marked.append(dfa_state)
            # is a final state accessible via epsilon transitions?
            if self.in_finals(self.e_closure(map[dfa_state])):
                dfa.add_final(dfa_state)
            for label in self.sigma():
                nfa_next = tuple(self.e_closure(self.move(map[dfa_state],
                label)))
                if map.has_key(nfa_next):
                    dfa_next = map[nfa_next]
                else:
                    dfa_next = dfa.new_state()
                    map[dfa_next] = nfa_next
                    map[nfa_next] = dfa_next
                    if self.in_finals(nfa_next):
                        dfa.add_final(dfa_next)
                    unmarked.append(dfa_next)
                dfa.insert(dfa_state, label, dfa_next)
        return dfa
    
    def generate(self, maxlen, state=0, prefix=""):
        "Generate all accepting sequences of length at most maxlen."
        if maxlen > 0:
            if state in self._finals:
                print prefix
            for (s1, labels, s2) in self.outgoing_transitions(state):
                for label in labels():
                    self.generate(maxlen-1, s2, prefix+label)

    def pp(self):
        """
        Print a representation of this FSA (in human-readable YAML format).
        """
        print yaml.dump(self)
    
    @classmethod
    def from_yaml(cls, loader, node):
        map = loader.construct_mapping(node)
        result = cls(map.get('sigma', []), {}, map.get('finals', []))
        for (s1, map1) in map['transitions'].items():
            for (symbol, targets) in map1.items():
                for s2 in targets:
                    result.insert(s1, symbol, s2)
        return result
    
    @classmethod
    def to_yaml(cls, dumper, data):
        sigma = data.sigma()
        transitions = {}
        for (s1, symbol, s2) in data.generate_transitions():
            map1 = transitions.setdefault(s1, {})
            map2 = map1.setdefault(symbol, [])
            map2.append(s2)
        try: sigma = "".join(sigma)
        except: sigma = list(sigma)
        node = dumper.represent_mapping(cls.yaml_tag, dict(
            sigma = sigma,
            finals = list(data.finals()),
            start = data._start,
            transitions = transitions))
        return node

    def __str__(self):
        return yaml.dump(self)

### FUNCTIONS TO BUILD FSA FROM REGEXP

# the grammar of regular expressions
# (probabilities ensure that unary operators
# have stronger associativity than juxtaposition)

def regexp_grammar(terminals):
    (S, Expr, Star, Plus, Qmk, Paren) = [Nonterminal(s) for s in 'SE*+?(']
    rules = [WeightedProduction(Expr, [Star], prob=0.2),
             WeightedProduction(Expr, [Plus], prob=0.2),
             WeightedProduction(Expr, [Qmk], prob=0.2),
             WeightedProduction(Expr, [Paren], prob=0.2),
             WeightedProduction(S, [Expr], prob=0.5),
             WeightedProduction(S, [S, Expr], prob=0.5),
             WeightedProduction(Star, [Expr, '*'], prob=1),
             WeightedProduction(Plus, [Expr, '+'], prob=1),
             WeightedProduction(Qmk, [Expr, '?'], prob=1),
             WeightedProduction(Paren, ['(', S, ')'], prob=1)]

    prob_term = 0.2/len(terminals) # divide remaining pr. mass
    for terminal in terminals:
        rules.append(WeightedProduction(Expr, [terminal], prob=prob_term))

    return WeightedGrammar(S, rules)

_parser = InsideChartParser(regexp_grammar('abcde'))

# create NFA from regexp (Thompson's construction)
# assumes unique start and final states

def re2nfa(fsa, regexp):
    tokens = tokenize.regexp_tokenize(regexp, pattern=r'.')
    tree = _parser.parse(tokens)
    if tree is None: raise ValueError('Bad Regexp')
    state = re2nfa_build(fsa, fsa.start(), tree)
    fsa.set_final([state])
#        fsa.minimize()

def re2nfa_build(fsa, node, tree):
    # Terminals.
    if not isinstance(tree, Tree):
        return re2nfa_char(fsa, node, tree)
    elif len(tree) == 1:
        return re2nfa_build(fsa, node, tree[0])
    elif tree.node == '(':
        return re2nfa_build(fsa, node, tree[1])
    elif tree.node == '*': return re2nfa_star(fsa, node, tree[0])
    elif tree.node == '+': return re2nfa_plus(fsa, node, tree[0])
    elif tree.node == '?': return re2nfa_qmk(fsa, node, tree[0])
    else:
        node = re2nfa_build(fsa, node, tree[0])
        return re2nfa_build(fsa, node, tree[1])

def re2nfa_char(fsa, node, char):
    new = fsa.new_state()
    fsa.insert(node, char, new)
    return new

def re2nfa_qmk(fsa, node, tree):
    node1 = fsa.new_state()
    node2 = re2nfa_build(fsa, node1, tree)
    node3 = fsa.new_state()
    fsa.insert(node, epsilon, node1)
    fsa.insert(node, epsilon, node3)
    fsa.insert(node2, epsilon, node3)
    return node3

def re2nfa_plus(fsa, node, tree):
    node1 = re2nfa_build(fsa, node, tree[0])
    fsa.insert(node1, epsilon, node)
    return node1

def re2nfa_star(fsa, node, tree):
    node1 = fsa.new_state()
    node2 = re2nfa_build(fsa, node1, tree)
    node3 = fsa.new_state()
    fsa.insert(node, epsilon, node1)
    fsa.insert(node, epsilon, node3)
    fsa.insert(node2, epsilon, node1)
    fsa.insert(node2, epsilon, node3)
    return node3

#################################################################
# Demonstration
#################################################################

def demo():
    """
    A demonstration showing how FSAs can be created and used.
    """
    # Define an alphabet.
    alphabet = "abcd"

    # Create a new FSA.
    fsa = FSA(alphabet)
    
    # Use a regular expression to initialize the FSA.
    re = 'abcd'
    print 'Regular Expression:', re
    re2nfa(fsa, re)
    print "NFA:"
    fsa.pp()

    # Convert the (nondeterministic) FSA to a deterministic FSA.
    dfa = fsa.dfa()
    print "DFA:"
    dfa.pp()

    # Prune the DFA
    dfa.prune()
    print "PRUNED DFA:"
    dfa.pp()

    # Use the FSA to generate all strings of length less than 3
    # (broken)
    #fsa.generate(3)

if __name__ == '__main__': demo()

########NEW FILE########
__FILENAME__ = huffman
# Simple Huffman encoding/decoding, Steven Bird
# http://en.wikipedia.org/wiki/Huffman_coding

import nltk
from operator import itemgetter

def huffman_tree(text):
    coding = nltk.FreqDist(text).items()
    coding.sort(key=itemgetter(1))
    while len(coding) > 1:
        a, b = coding[:2]
        pair = (a[0], b[0]), a[1] + b[1]
        coding[:2] = [pair]
        coding.sort(key=itemgetter(1))
    return coding[0][0]

def tree2codes(tree, prefix=""):
    if type(tree) is not tuple:
        yield (tree, prefix)
    else:
        for code in tree2codes(tree[0], prefix + "0"):
            yield code
        for code in tree2codes(tree[1], prefix + "1"):
            yield code

def encode(tree, text):
    mapping = dict(tree2codes(tree))
    return "".join(mapping[c] for c in text)

def decode(tree, text):
    cur_tree = tree
    for bit in text:
        if bit == "0":
            cur_tree = cur_tree[0]
        else:
            cur_tree = cur_tree[1]
        if type(cur_tree) is not tuple:
            yield cur_tree
            cur_tree = tree

text1 = nltk.corpus.udhr.raw('English-Latin1')
text2 = nltk.corpus.udhr.raw('German_Deutsch-Latin1')
text3 = nltk.corpus.udhr.raw('French_Francais-Latin1')
#text2 = nltk.corpus.udhr.raw('Swedish_Svenska-Latin1')
#text3 = nltk.corpus.udhr.raw('Samoan-Latin1')

# text = text1
# code_tree = huffman_tree(text)
# print text == "".join(decode(code_tree, encode(code_tree, text)))

alphabet = "".join(set(text1 + text2 + text3))
text1 += alphabet
text2 += alphabet
text3 += alphabet

# train1, test1 = alphabet + text1[:5000], text1[5000:]
# train2, test2 = alphabet + text2[:5000], text2[5000:]
# train3, test3 = alphabet + text3[:5000], text3[5000:]

train1 = test1 = text1
train2 = test2 = text2
train3 = test3 = text3

def trial(train, texts):
    code_tree = huffman_tree(train)
    for text in texts:
        text_len = len(text)
        comp_len = len(encode(code_tree, text)) / 8.0
        compression = (text_len - comp_len) / text_len
        print compression,
    print

trial(train1, [test1, test2, test3])
trial(train2, [test1, test2, test3])
trial(train3, [test1, test2, test3])



########NEW FILE########
__FILENAME__ = kimmo
# Natural Language Toolkit: Kimmo Morphological Analyzer
#
# Copyright (C) 2001-2007 MIT
# Author: Carl de Marcken <carl@demarcken.org>
#         Beracah Yankama <beracah@mit.edu>
#         Robert Berwick <berwick@ai.mit.edu>
#
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

"""
Kimmo Morphological Analyzer.  Supports proper recognizer completion,
generator ordering, kimmo control class, loader for own file format,
also .rul compatible with old pckimmo.
"""

# TODO: remove Unix dependencies

import Tkinter
import os, re, sys, types, string, glob, time, md5

from nltk_contrib.fsa import *
from nltk.corpus import find_corpus_file
from nltk import tokenize

############################# KIMMO GUI ##################################
"""
A gui for input of generative & recognition models
need 3 input boxes, one for text input, lexicon box, rules box
one output box?

need alternations rules and lexicon
plus 1 input test & recognition box.

we want to "step" through alternations
we want to "show" the rules that fire.
and we want batch mode, big file, or big input test with output.
"""
###########################################################################
from ScrolledText import ScrolledText

class KimmoGUI:
    def __init__(self, grammar, text, title='Kimmo Interface v1.78'):
        self.root = None
        try:
            self.dbgTracing = None
            self.highlightIds = []
            self.tagId = 0

            self.lexmd5 = None
            self.rulemd5 = None
            self.lexicalGraphWindow = None

            self.rulfilename = ''
            self.lexfilename = ''
            self.altfilename = ''
            self.kimmoResultFile = ''

            self.helpFilename = 'kimmo.help'

            self._root = Tkinter.Tk()
            self._root.title(title)

            ctlbuttons = Tkinter.Frame(self._root)
            ctlbuttons.pack(side='top', fill='x')
            level1 = Tkinter.Frame(self._root)
            level1.pack(side='top', fill='none')
            Tkinter.Frame(self._root).pack(side='top', fill='none')
            level2 = Tkinter.Frame(self._root)
            level2.pack(side='top', fill='x')
            buttons = Tkinter.Frame(self._root)
            buttons.pack(side='top', fill='none')
            batchFrame = Tkinter.Frame(self._root)
            batchFrame.pack(side='top', fill='x')

            self.batchpath = Tkinter.StringVar()
            Tkinter.Label(batchFrame, text="Batch File:").pack(side='left')
            Tkinter.Entry(batchFrame, background='white', foreground='black',
                          width=30, textvariable=self.batchpath).pack(side='left')
            Tkinter.Button(batchFrame, text='Go!',
                           background='#a0c0c0', foreground='black',
                           command=self.batch).pack(side='left')

            self.debugWin = Tkinter.StringVar() # change to a window and field eventually.
            Tkinter.Entry(batchFrame, background='grey', foreground='red',
                          width=30, textvariable=self.debugWin).pack(side='right')

            self.wordIn = Tkinter.StringVar()
            Tkinter.Label(level2, text="Generate or Recognize:").pack(side='left')
            Tkinter.Entry(level2, background='white', foreground='black',
                          width=30, textvariable=self.wordIn).pack(side='left')

            lexiconFrame = Tkinter.Frame(level1)
            Tkinter.Label(lexiconFrame, text="Lexicon & Alternations").pack(side='top',
                                                              fill='x')
            self.lexicon = ScrolledText(lexiconFrame, background='white',
                                              foreground='black', width=50, height=36, wrap='none')

            # setup the scrollbar
            scroll = Tkinter.Scrollbar(lexiconFrame, orient='horizontal',command=self.lexicon.xview)

            scroll.pack(side='bottom', fill='x')
            self.lexicon.configure(xscrollcommand = scroll.set)

            self.lexicon.pack(side='top')


            midFrame = Tkinter.Frame(level1)
            rulesFrame = Tkinter.Frame(midFrame)
            rulesFrame.pack(side='top', fill='x')
            Tkinter.Label(rulesFrame, text="Rules/Subsets").pack(side='top',
                                                              fill='x')
            self.rules = ScrolledText(rulesFrame, background='white',
                                              foreground='black', width=60, height=19, wrap='none')
            # setup the scrollbar
            scroll = Tkinter.Scrollbar(rulesFrame, orient='horizontal',command=self.rules.xview)
            scroll.pack(side='bottom', fill='x')
            self.rules.configure(xscrollcommand = scroll.set)

            self.rules.pack(side='top')

            midbetweenFrame = Tkinter.Frame(midFrame)
            midbetweenFrame.pack(side='top', fill='x')

            Tkinter.Button(midbetweenFrame, text='clear',
                           background='#f0f0f0', foreground='black',
                           command= lambda start=1.0, end=Tkinter.END : self.results.delete(start,end)
                           ).pack(side='right')

            Tkinter.Label(midbetweenFrame,
                          text="Results           ").pack(side='right')

            self.results = ScrolledText(midFrame, background='white',
                                              foreground='black', width=60, height=13, wrap='none')

            # setup the scrollbar
            scroll = Tkinter.Scrollbar(midFrame, orient='horizontal',command=self.results.xview)
            scroll.pack(side='bottom', fill='x')
            self.results.configure(xscrollcommand = scroll.set)

            self.results.pack(side='bottom')



            """
            alternationFrame = Tkinter.Frame(level1)
            Tkinter.Label(alternationFrame, text="Alternations").pack(side='top',
                                                              fill='x')
            self.alternation = ScrolledText(alternationFrame, background='white',
                                              foreground='black', width=1, wrap='none')
            self.alternation.pack(side='top')
            """

            Tkinter.Button(ctlbuttons, text='Quit',
                           background='#a0c0c0', foreground='black',
                           command=self.destroy).pack(side='left')

            self.loadMenuButton = Tkinter.Menubutton(ctlbuttons, text='Load', background='#a0c0c0', foreground='black', relief='raised')
            self.loadMenuButton.pack(side='left')
            self.loadMenu=Tkinter.Menu(self.loadMenuButton,tearoff=0)

            self.loadMenu.add_command(label='Load Lexicon', underline=0,command = lambda filetype='.lex', targetWindow = self.lexicon, tf = 'l' : self.loadTypetoTarget(filetype, targetWindow, tf))
            self.loadMenu.add_command(label='Load Rules', underline=0,command = lambda filetype='.rul', targetWindow = self.rules, tf = 'r' : self.loadTypetoTarget(filetype, targetWindow, tf))
            # self.loadMenu.add_command(label='Load Lexicon', underline=0,command = lambda filetype='.lex', targetWindow = self.lexicon : loadTypetoTarget(self, filetype, targetWindow))
            self.loadMenuButton["menu"]=self.loadMenu

#

            self.saveMenuButton = Tkinter.Menubutton(ctlbuttons, text='Save',background='#a0c0c0', foreground='black', relief='raised')
            self.saveMenuButton.pack(side='left')
            self.saveMenu=Tkinter.Menu(self.saveMenuButton,tearoff=0)
            self.saveMenu.add_command(label='Save Lexicon', underline=0,command = lambda filename=self.lexfilename, sourceWindow = self.lexicon : self.writeToFilefromWindow(filename, sourceWindow,'w',0,'l'))
            self.saveMenu.add_command(label='Save Rules', underline=0,command = lambda filename=self.rulfilename, sourceWindow = self.rules : self.writeToFilefromWindow(filename, sourceWindow,'w',0,'r'))
            self.saveMenu.add_command(label='Save Results', underline=0,command = lambda filename='.results', sourceWindow = self.results : self.writeToFilefromWindow(filename, sourceWindow,'w',0))
            self.saveMenu.add_command(label='Save All', underline=0,command = self.saveAll)
            self.saveMenuButton["menu"]=self.saveMenu


            Tkinter.Label(ctlbuttons, text="       Preset:").pack(side='left')

            self.configValue = Tkinter.StringVar()
            self.configsMenuButton = Tkinter.Menubutton(ctlbuttons, text='Configs', background='#a0c0c0', foreground='black', relief='raised')
            self.configsMenuButton.pack(side='left')
            self.configsMenu=Tkinter.Menu(self.configsMenuButton,tearoff=0)
            # read the directory for cfgs, add them to the menu
            # add path expander, to expand ~ & given home dirs.


            # !!! this does not handle student student directories, if not the current dir!
            currentconfigfiles = glob.glob('*.cfg')
            for x in currentconfigfiles:
                newname = x # [0:len(x)-4]      # remove the '.cfg'
                self.configsMenu.add_command(label=newname, underline=0,command = lambda newname=x : self.configLoader(newname)) # Callback(self.configLoader,newname))
                # we want this to call load on the specific config file

            if len(currentconfigfiles) == 0:
                # configsMenu.add_command(label='<none>',underline=0)
                self.configsMenuButton.configure(text='<none>')
                
            self.configsMenuButton["menu"]=self.configsMenu



            # toggle the different modes of this window
#            Tkinter.Button(ctlbuttons, text='->',
#                           background='#ffd564', foreground='red',
#                           command=self.generate).pack(side='right')
#
#            Tkinter.Checkbutton(ctlbuttons, text='Stepping',
#                           background='#b0f0d0', foreground='#008b45',
#                           command=self.generate).pack(side='right')

            self.tracingbtn = Tkinter.Button(ctlbuttons, text='Tracing',
                           background='#fff0f0', foreground='black',
                           command=lambda : self.create_destroyDebugTracing()).pack(side='right')


            self.graphMenuButton = Tkinter.Menubutton(ctlbuttons, text='Graph', background='#d0d0e8', foreground='black', relief='raised')
            self.graphMenuButton.pack(side='right')
            self.graphMenu=Tkinter.Menu(self.graphMenuButton,tearoff=0)

            self.graphMenu.add_command(label='Graph Lexicon', underline=0,command = lambda which = 'l' : self.graph(which))
            self.graphMenu.add_command(label='Graph FSA Rules', underline=0,command = lambda which = 'r' : self.graph(which))
            # self.loadMenu.add_command(label='Load Lexicon', underline=0,command = lambda filetype='.lex', targetWindow = self.lexicon : loadTypetoTarget(self, filetype, targetWindow))
            self.graphMenuButton["menu"]=self.graphMenu

            self.helpbtn = Tkinter.Button(ctlbuttons, text='Help',
                           background='#f0fff0', foreground='black',
                           command=self.kimmoHelp).pack(side='right')


            lexiconFrame.pack(side='left')
            midFrame.pack(side='left')
            # alternationFrame.pack(side='left')

            Tkinter.Button(level2, text='Generate',
                           background='#a0c0c0', foreground='black',
                           command=self.generate).pack(side='left')
            Tkinter.Button(level2, text='Recognize',
                           background='#a0c0c0', foreground='black',
                           command=self.recognize).pack(side='left')


            # setup the vars for kimmo
            # eventually make this a kimmo object
            """
            self.klexicons = []
            self.kalternations = []
            self.ksubsets = []
            self.kdefaults = []
            self.krules = []
            """

            self.kimmoinstance = None

            self.kimmoResultFile = ''
            self.traceWindow = ''

            self.debug = False

            self.configLoader('kimmo.cfg')
            # self.batchpath.set("kimmo.batch_test")

            # capture all print messages
            self.phOut = PrintHook()
            self.phOut.Start(self.capturePrint)


            # Enter mainloop.
            Tkinter.mainloop()
        except:
            print 'Error creating Tree View'
            self.destroy()
            raise

    def init_menubar(self):
        menubar = Tkinter.Menu(self._root)

        filemenu = Tkinter.Menu(menubar, tearoff=0)
        filemenu.add_command(label='Save Rules', underline=0,
                             command=self.save, accelerator='Ctrl-s')
        self._root.bind('<Control-s>', self.save)
        filemenu.add_command(label='Load Rules', underline=0,
                             command=self.load, accelerator='Ctrl-o')
        self._root.bind('<Control-o>', self.load)
        filemenu.add_command(label='Clear Rules', underline=0,
                             command=self.clear, accelerator='Ctrl-r')
        self._root.bind('<Control-r>', self.clear)
        filemenu.add_command(label='Exit', underline=1,
                             command=self.destroy, accelerator='Ctrl-q')
        self._root.bind('<Control-q>', self.destroy)
        menubar.add_cascade(label='File', underline=0,
                            menu=filemenu)
        self._root.config(menu=menubar)

    def guiError(self, *args):
        self.debugWin.set(args[0].strip())
        

    def create_destroyDebugTracing(self, *args):
        # test creating tracing/debug window
        
        if (self.dbgTracing):
                self.dbgTracing.destroy()
                self.dbgTracing = None
                self.debug = False

        else:
                try:
                    # have in its own special di decial class
                    self.dbgTracing = Tkinter.Toplevel()
                    self.dbgTracing.title("Tracing/Debug")
                    dbgTraceFrame2 = Tkinter.Frame(self.dbgTracing)
                    dbgTraceFrame2.pack(side='top', fill='x')
                    dbgTraceFrame = Tkinter.Frame(self.dbgTracing)
                    dbgTraceFrame.pack(side='top', fill='x',expand='yes')
                    self.traceWindow = ScrolledText(dbgTraceFrame, background='#f4f4f4',
                                                      foreground='#aa0000', width=45, height=24, wrap='none')
        
                    Tkinter.Button(dbgTraceFrame2, text='clear',
                                   background='#a0c0c0', foreground='black',
                                   command= lambda start=1.0, end=Tkinter.END : self.traceWindow.delete(start,end)
                                   ).pack(side='right')
                    Tkinter.Button(dbgTraceFrame2, text='Save',
                                   background='#a0c0c0', foreground='black',
                                   command= lambda file=self.kimmoResultFile,windowName=self.traceWindow,mode='w',auto=0 : self.writeToFilefromWindow(file,windowName,mode,auto)
                                   ).pack(side='left')
        
        
                    scroll = Tkinter.Scrollbar(dbgTraceFrame, orient='horizontal',command=self.traceWindow.xview)
                    scroll.pack(side='bottom', fill='x')
        
                    self.traceWindow.configure(xscrollcommand = scroll.set)
                    self.traceWindow.pack(side='bottom')
        
        
                    self.debug = True
        
                    # this will automatically clean itself up.
                    self.dbgTracing.protocol("WM_DELETE_WINDOW", self.create_destroyDebugTracing)
        
                except:
                    print 'Error creating Tree View'
                    self.dbgTracing.destroy()
                    self.dbgTracing = None
                    self.debug = False
                    raise


    def writeToFilefromWindow(self, filename, windowName, mode, auto, wt=None):
        # filename from var
        
        # if not file: file='.txt'
        # if append, add on, if overwrite, then ya

        if not (auto and windowName and filename):
                
                from tkFileDialog import asksaveasfilename
                ftypes = [('Text file', '.txt'),('Rule file', '.rul'),('Lexicon file', '.lex'),('Alternations file', '.alt'),
                          ('All files', '*')]
                filename = asksaveasfilename(filetypes=ftypes,
                                             defaultextension='', initialfile=filename)

        if not filename:
                self.guiError('Need File Name')
                return
        f = open(filename, 'w')
        f.write(windowName.get(1.0,Tkinter.END))
        f.close()

        if filename:
                if wt == 'l': self.lexfilename = filename
                elif wt == 'r': self.rulfilename = filename


    # create a window update class
    # and a window resize class

    # default save; all file names are known, so it saves to them.
    def saveAll(self, *args):
        
        # automatic write
        self.writeToFilefromWindow(self.lexfilename,self.lexicon,'w',1)
        self.writeToFilefromWindow(self.rulfilename,self.rules,'w',1)
        # self.writeToFilefromWindow(self.altfilename,self.alternation,'w',1)
        self.writeToFilefromWindow(self.resfilename,self.results,'w',1)
        
    """
    def save(self, *args):
        "Save a rule/lexicon set to a text file"
        from tkFileDialog import asksaveasfilename
        ftypes = [('Text file', '.txt'),
                  ('All files', '*')]
        filename = asksaveasfilename(filetypes=ftypes,
                                     defaultextension='.txt')
        if not filename: return
        f = open(filename, 'w')
        f.write('---- Rules -----\n%s\n' % '\n'.join(self.getRules(False)))
        f.write('---- Lexicon -----\n%s\n' % '\n'.join(self.getLexicon(False)))
        f.close()
    """

    def configLoader(self,*args):
        print args[0]
        filename = args[0]

        # if arg is a valid file, load by line.
        # handle the different types of files
        if filename:
                f = read_kimmo_file(filename, self)
                lines = f.readlines()
                f.close()
        
                # clear all panes
                self.clear()
                
                # now set the menu
                self.configsMenuButton.configure(text=filename)
        
                # reset gui name variables
                # so that nothing gets overwritten.
                # these file name variables will be changed if
                # either the cfg changes it, or the person loads a different file
        
                self.rulfilename = ''   
                self.lexfilename = ''
                self.altfilename = ''
                self.kimmoResultFile = ''
                self.batchpath.set('')
        
                for line in lines:
                    line = line.strip()
                    cfgargs = line.split(":")
                    for x in range(len(cfgargs)): cfgargs[x] = cfgargs[x].strip()
        
                    if len(line) == 0: continue
                    elif (line[0] == '#') or (line[0] == ';'): continue # comment
                    elif cfgargs[0] == 'lexicon':
                        self.lexfilename = self.loadIntoWindow(os.path.expanduser(cfgargs[1]),self.lexicon)
                    elif cfgargs[0] == 'rules':
                        self.rulfilename = self.loadIntoWindow(os.path.expanduser(cfgargs[1]),self.rules)
                    #elif cfgargs[0] == 'alternations':
                    #   self.loadIntoWindow(cfgargs[1],self.alternation)
                    #   self.altfilename = cfgargs[1]
                    elif cfgargs[0] == 'results':
                        self.kimmoResultFile = os.path.expanduser(cfgargs[1])
                        self.resfilename = os.path.expanduser(cfgargs[1])
                    elif cfgargs[0] == 'batch': self.batchpath.set(os.path.expanduser(cfgargs[1]))
                    # !
                    else: self.guiError('unknown line :' + line)
                    # print line
        
        else: self.guiError('Empty Filename')
        
        

    def loadIntoWindow(self, filename, windowField):
        "Load rule/lexicon set from a text file directly into the window pane specified"
        # filename = args[0]
        # windowField = args[1]

        if filename:
            filename = os.path.expanduser(filename)
            f = read_kimmo_file(filename, self)
            lines = f.readlines()
            f.close()

            text = []
            for line in lines:
                line = line.strip()
                text.append(line)

            # empty the window now that the file was valid
            windowField.delete(1.0, Tkinter.END)
    
            windowField.insert(1.0, '\n'.join(text))
    
            return filename
        return ''       

        # opens a load dialog for files of a specified type to be loaded into a specified window
    def loadTypetoTarget(self, fileType, targetWindow, ftype = None):
        
        if not (fileType and targetWindow): return
        
        from tkFileDialog import askopenfilename
        ftypes = [(fileType, fileType)]

        filename = askopenfilename(filetypes=ftypes, defaultextension=fileType)

        self.loadIntoWindow(filename, targetWindow)

        # set the config menu to blank
        self.configsMenuButton.configure(text='<none>')

        # !!! remember to reset all the filenames as well!
        if filename:
                if ftype == 'l': self.lexfilename = filename
                elif ftype == 'r': self.rulfilename = filename

    def load(self, *args):
        # graphical interface to file loading.
        
        "Load rule/lexicon set from a text file"
        from tkFileDialog import askopenfilename
        ftypes = [('Text file', '.txt'),
                  ('All files', '*')]
        # filename = askopenfilename(filetypes=ftypes, defaultextension='.txt')
        filename = 'kimmo.lex'

        if filename:
            f = read_kimmo_file(filename, self)
            lines = f.readlines()
            f.close()
    
            rules = []
            lexicon = []
            alternations = []
    
            state = 'rules'
            for line in lines:
                line = line.strip()
                lexicon.append(line)
    
            self.clear()
            self.lexicon.insert(1.0, '\n'.join(lexicon))


        # now load up the alternations

        filename = 'kimmo.alt'

        if filename:
            f = read_kimmo_file(filename, self)
            lines = f.readlines()
            f.close()
    
            for line in lines:
                line = line.strip()
                alternations.append(line)

            self.alternation.insert(1.0, '\n'.join(alternations))

        filename = 'kimmo.rul'

        if filename:
            f = read_kimmo_file(filename, self)
            lines = f.readlines()
            f.close()
    
            for line in lines:
                line = line.strip()
                rules.append(line)

            self.rules.insert(1.0, '\n'.join(rules))

    def clear(self, *args):
        "Clears the grammar and lexical and sentence inputs"
        self.lexicon.delete(1.0, Tkinter.END)
        self.rules.delete(1.0, Tkinter.END)
        # self.alternation.delete(1.0, Tkinter.END)
        self.results.delete(1.0, Tkinter.END)

    def destroy(self, *args):
        if self._root is None: return
        self.phOut.Stop()
        self._root.destroy()
        self._root = None

# for single stepping through a trace.
# need to make the kimmo class capable of being interrupted & resumed.
    def step(self, *args):
        print 'a'

    def singlestep(self, *args):
        print 'a'

    def batch(self, *args):
        filename = self.batchpath.get()
        if filename:
            f = read_kimmo_file(filename, self)
            lines = f.readlines()
            f.close()
                                    
            self.initKimmo()
            
            # space the results out a little
            self.results.insert(1.0, '\n')
            
            results_string = ''
            for line in lines:
                # a 'g word' 'r word' format
                singleword = line.strip()       # should be a single word, no spaces, etc.
                spcr = re.compile(r"\s+")
                linevals = []
                linevals = spcr.split(singleword)
                                                
                
                batch_result = []
                batch_result_str = ''
                if not singleword: continue     # ignore blank lines
                elif (singleword[0] == '#') or (singleword[0] == ';'):  # commented;
                        results_string += (singleword + '\n')
                        # self.results.insert(Tkinter.END, singleword + '\n')                   # send directly to results pane

                elif (linevals[0] == 'g') and (len(linevals) == 2):
                        batch_result = self.kimmoinstance.generate(linevals[1])
                elif (linevals[0] == 'r') and (len(linevals) == 2):
                        batch_result = self.kimmoinstance.recognize(linevals[1])
                        
                elif '+' in singleword:
                        batch_result = self.kimmoinstance.generate(singleword)
                else:
                        batch_result = self.kimmoinstance.recognize(singleword)
                
                # if a valid results
                if len(batch_result) > 0:
                        for x in batch_result: batch_result_str = batch_result_str + x
                        batch_result_str = batch_result_str + '\n'
                        results_string += (batch_result_str)
                        # self.results.insert(Tkinter.END, batch_result_str)
                
            # place a separator between results
            self.results.insert(1.0, '----- '+ time.strftime("%a, %d %b %Y %I:%M %p", time.gmtime()) +' -----\n')   
            self.results.insert(2.0, results_string)        
            self.results.see(1.0)
    
            if self.traceWindow:
                    self.highlightMatches('    BLOCKED',self.traceWindow,'#ffe0e0') 
                    self.highlightMatches('      AT END OF WORD',self.traceWindow,'#e0ffe0')        
                    

        # if the path is set, load the file
                # init the engine
                # choose between recognize & generate



        # generation test
    def generate(self, *args):
        if self._root is None: return

        if len(self.wordIn.get()) > 0:
                self.initKimmo()
        
                tmpword = self.wordIn.get()

                tmpword.strip()
        
                # generate_result = _generate_test(self.ks, tmpword)
                generate_result = self.kimmoinstance.generate(tmpword)
                generate_result_str = ''
                # convert list to string
                for x in generate_result: generate_result_str = generate_result_str + x
                generate_result_str = generate_result_str + '\n'
                self.results.insert(1.0, generate_result_str)
        
                if self.dbgTracing:
                        self.highlightMatches('    BLOCKED',self.traceWindow,'#ffe0e0') 
                        self.highlightMatches('      AT END OF WORD',self.traceWindow,'#e0ffe0')        
                        self.highlightMatches('SUCCESS!',self.traceWindow,'#e0ffe0')

        
    def recognize(self, *args):
        self.lexicon.tag_delete("highlight")
        if self._root is None: return

        if len(self.wordIn.get()) > 0:
                self.initKimmo()
        
                tmpword = self.wordIn.get()
                # pad with terminators
                tmpword.strip()
        
                # recognize_result = _recognize_test(self.ks, tmpword, self.km)
                recognize_result = self.kimmoinstance.recognize(tmpword)
                recognize_result_str = ''
                # convert list to string
                for x in recognize_result: recognize_result_str = recognize_result_str + x
                recognize_result_str = recognize_result_str + '\n'
                self.results.insert(1.0, recognize_result_str)
        
                if self.dbgTracing:
                        self.highlightMatches('    BLOCKED',self.traceWindow,'#ffe0e0') 
                        self.highlightMatches('      AT END OF WORD',self.traceWindow,'#e0ffe0')        



        # accept gui graph command
        # create kimmoinstance
        # and then process / display one of the graphs.
    def graph(self, which):
        
        self.initKimmo()
        graphtitle = ''
        
        
        # we want to save in the local dir.
        # lex/rulefilenames are fully qualified.
        
        # so we test the local dir & strip the path off of the filename.
        

        # check & set path, if necessary, need read and write access to path
        path = ''
        pathstatus = os.stat('./')      # 0600 is r/w, binary evaluation
        if not ((pathstatus[0] & 0600) == 0600):
                path = '/tmp/' + str(os.environ.get("USER")) + '/' # need terminating /
                if not os.path.exists(path):
                        os.mkdir(path,0777)
        
        pathre = re.compile(r"^.*\/")
        
        if which == 'l':
                graphfname = path + pathre.sub("", self.lexfilename)
                dotstring = dotformat(self.kimmoinstance.lexicalNodes)
                leximagefile = dot2image(graphfname, dotstring)
                graphtitle = 'Lexicon Graph'

        elif which == 'r':
                graphfname = path + pathre.sub("", self.rulfilename)
                
                tmpOptions = []
                for x in self.kimmoinstance.fsasNodes:
                        # print x['name']
                        tmpOptions.append(x['name'])
                        
                ld = ListDialog(self._root,tmpOptions,"Select FSA")
                
                if not ld.result: return
                
                # now create the dotstring & image from the (single) selection
                dotstring = dotformat(self.kimmoinstance.fsasNodes[string.atoi(ld.result[0])]['nodes'])
                graphtitle = 'FSA ' + self.kimmoinstance.fsasNodes[string.atoi(ld.result[0])]['name']
                
                # make file read:
                # something.rul.1.gif  (where 1 is the rule index number)
                graphfname += ('.' + str(ld.result[0]))
                
                # check if that file already exists, if so, append an iteration number onto it.
                
                leximagefile = dot2image(graphfname, dotstring)
                

        # if this is an imagefile, then create a new window for it.
        if leximagefile:
                if self.lexicalGraphWindow: self.lexicalGraphWindow.destroy()
                self.lexicalGraphWindow = tkImageView(leximagefile, graphtitle)



        # validates the lexicon against the alternations to make certain there
        # are no misreferences/mispellings of refs.
    def validate(self,*args):
        self.tagId = 1
        
        for x in self.lexicon.tag_names(): self.lexicon.tag_delete(x)
        
        # for x in self.highlightIds: x[0].tag_delete(x[1])
        
        for l in self.kimmoinstance.validateLexicon:
                if not l in self.kimmoinstance.validateAlternations:
                        if l:
                                self.guiError('Unused Alternation')
                                self.highlightMatches(l,self.lexicon,'#ffffc0')
        
        for a in self.kimmoinstance.validateAlternations:
                if not a in self.kimmoinstance.validateLexicon:
                        if a:
                                self.guiError('Unknown Alternation Name')
                                self.highlightMatches(a,self.lexicon,'#ffffc0')
        

    # highlight matching words in given window
    def highlightMatches(self, word, window,color):
        # assumes unbroken with whitespace words.
        if not word: return
        
        matchIdx = '1.0'
        matchRight = '1.0'
        while matchIdx != '':
                matchIdx = window.search(word,matchRight,count=1,stopindex=Tkinter.END)
                if matchIdx == '': break
                
                strptr = matchIdx.split(".")
                matchRight = strptr[0] + '.' + str((int(strptr[1],10) + len(word)))

                window.tag_add(self.tagId, matchIdx, matchRight )
                window.tag_configure(self.tagId,background=color, foreground='black')
                self.highlightIds.append([window,self.tagId])
                self.tagId = self.tagId + 1
        
        

# INIT KIMMO
    def initKimmo(self, *args):
                """
                Initialize the Kimmo engine from the lexicon.  This will get called no matter generate
                or recognize.  (i.e. loading all rules, lexicon, and alternations
                """
                # only initialize Kimmo if the contents of the *rules* have changed
                tmprmd5 = md5.new(self.rules.get(1.0, Tkinter.END))
                tmplmd5 = md5.new(self.lexicon.get(1.0, Tkinter.END))
                if (not self.kimmoinstance) or (self.rulemd5 != tmprmd5) or (self.lexmd5 != tmplmd5):
                        self.guiError("Creating new Kimmo instance")
                        self.kimmoinstance = KimmoControl(self.lexicon.get(1.0, Tkinter.END),self.rules.get(1.0, Tkinter.END),'','',self.debug)
                        self.guiError("")
                        self.rulemd5 = tmprmd5
                        self.lexmd5 = tmplmd5

                if not self.kimmoinstance.ok:
                        self.guiError("Creation of Kimmo Instance Failed")
                        return
                if not self.kimmoinstance.m.initial_state() :
                        self.guiError("Morphology Setup Failed")
                elif self.kimmoinstance.errors:
                        self.guiError(self.kimmoinstance.errors)
                        self.kimmoinstance.errors = ''
                # self.validate()

    def refresh(self, *args):
        if self._root is None: return
        print self.wordIn.get()


# CAPTURE PYTHON-KIMMO OUTPUT
# redirect to debug window, if operational
    def capturePrint(self,*args):
        # self.debugWin.set(string.join(args," "))
        
        # if there is a trace/debug window
        if self.dbgTracing:
                self.traceWindow.insert(Tkinter.END, string.join(args," "))
                self.traceWindow.see(Tkinter.END)
                
        
        # otherwise, just drop the output.
        
        # no no, if tracing is on, but no window, turn tracing off and cleanup window
        
        # !!! if tracing is on, but window is not defined, create it.
                # this will cause a post-recover from an improper close of the debug window
                
        # if tracing is not on, ignore it.
        
        # return 1,1,'Out Hooked:'+text
        return 0,0,''
        
        

    def kimmoHelp(self,*args):

        # helpText = """
        # """
                
        # load help into helpfile

        # helpText = Tkinter.StringVar()
        helpText = ''
        try: f = open(self.helpFilename, 'r')
        except IOError, e:
                self.guiError("HelpFile not loaded")
                return
                
        self.guiError("")       # no errors to report here
                                # this is not the best idea, what if there are many errors
                                # from different functions?

        helpText = str(f.read())
        f.close()       
        
        # clean any crl stuff
        helpText = re.sub("\r","",helpText)

        
        helpWindow = Tkinter.Toplevel()
        helpWindow.title("PyKimmo Documentation & Help")
        
        # help = Tkinter.Label(helpWindow,textvariable=helpText, justify='left' ) #
        help = ScrolledText(helpWindow, background='#f0f0f0',
                            foreground='black', width=70, height=40,wrap='none',
                            font='Times 12 bold') #

        help.pack(side='top')
        help.insert(1.0, helpText)
        # setup the scrollbar
        scroll = Tkinter.Scrollbar(helpWindow, orient='horizontal',command=help.xview)
        scroll.pack(side='bottom', fill='x')
        help.configure(xscrollcommand = scroll.set)

        # now highlight up the file
        matchIdx = Tkinter.END
        matchRight = Tkinter.END
        matchLen = Tkinter.IntVar()
        tagId = 1
        while 1:
                matchIdx = help.search(r"::[^\n]*::",matchIdx, stopindex=1.0, backwards=True, regexp=True, count=matchLen  )
                if not matchIdx: break
                
                matchIdxFields = matchIdx.split(".")
                matchLenStr = matchIdxFields[0] + "." + str(string.atoi(matchIdxFields[1],10) + matchLen.get())

                print (matchIdx, matchLenStr)
                help.tag_add(tagId, matchIdx, matchLenStr )
                help.tag_configure(tagId, background='aquamarine', foreground='blue', underline=True)
                tagId += 1
                

        

################################ PRINT HOOK ######################
# this class gets all output directed to stdout(e.g by print statements)
# and stderr and redirects it to a user defined function
class PrintHook:
    #out = 1 means stdout will be hooked
    #out = 0 means stderr will be hooked
    def __init__(self,out=1):
        self.func = None ##self.func is userdefined function
        self.origOut = None
        self.out = out
    #user defined hook must return three variables
    #proceed,lineNoMode,newText
    def TestHook(self,text):
        f = open('hook_log.txt','a')
        f.write(text)
        f.close()
        return 0,0,text
    def Start(self,func=None):
        if self.out:
            sys.stdout = self
            self.origOut = sys.__stdout__
        else:
            sys.stderr= self
            self.origOut = sys.__stderr__
        if func:
            self.func = func
        else:
            self.func = self.TestHook
    #Stop will stop routing of print statements thru this class
    def Stop(self):
        self.origOut.flush()
        if self.out:
            sys.stdout = sys.__stdout__
        else:
            sys.stderr = sys.__stderr__
            self.func = None
    #override write of stdout
    def write(self,text):
        proceed = 1
        lineNo = 0
        addText = ''
        if self.func != None:
            proceed,lineNo,newText = self.func(text)
            if proceed:
                if text.split() == []:
                        self.origOut.write(text)
                else:
                #if goint to stdout then only add line no file etc
                #for stderr it is already there
                    if self.out:
                        if lineNo:
                            try:
                                raise "Dummy"
                            except:
                                newText =  'line('+str(sys.exc_info()[2].tb_frame.f_back.f_lineno)+'):'+newText
                                codeObject = sys.exc_info()[2].tb_frame.f_back.f_code
                                fileName = codeObject.co_filename
                                funcName = codeObject.co_name
                        self.origOut.write('file '+fileName+','+'func '+funcName+':')
                        self.origOut.write(newText)
    #pass all other methods to __stdout__ so that we don't have to override them
    def __getattr__(self, name):
        return self.origOut.__getattr__(name)

class tkImageView:
        def __init__(self, imagefileName, title):
                self._root = Tkinter.Toplevel()
                self._root.title(title + ' (' + imagefileName + ')')
                self.image = Tkinter.PhotoImage("LGraph",file=imagefileName)

                Tkinter.Label(self._root, image=self.image).pack(side='top',fill='x')
                # self._root.mainloop()
                
        def destroy(self, *args):
                if self._root:
                        self._root.destroy()
                self._root = None
                self.image = None
                
        
######################### Dialog Boxes ##############################
class ListDialog(Tkinter.Toplevel):

    def __init__(self, parent, listOptions, title = None):

        Tkinter.Toplevel.__init__(self, parent)
        self.transient(parent)

        if title:
            self.title(title)

        self.parent = parent

        self.result = None

        body = Tkinter.Frame(self)

        self.initial_focus = self.body(body)
        body.pack(padx=5, pady=5)

        box = Tkinter.Frame(self)
        Tkinter.Label(box,text="Select an FSA to graph").pack(side='top',fill='x')
        box.pack()



        self.listbox(listOptions)

        self.buttonbox()

        self.grab_set()

        if not self.initial_focus:
            self.initial_focus = self

        self.protocol("WM_DELETE_WINDOW", self.cancel)

        self.geometry("+%d+%d" % (parent.winfo_rootx()+50,
                                  parent.winfo_rooty()+50))

        self.initial_focus.focus_set()

        self.wait_window(self)

    #
    # construction hooks

    def body(self, master):
        # create dialog body.  return widget that should have
        # initial focus.  this method should be overridden

        pass


    def listbox(self, listOptions):
        box = Tkinter.Frame(self)
        self.lb = Tkinter.Listbox(box,height=len(listOptions),width=30,background='#f0f0ff', selectbackground='#c0e0ff'
                ,selectmode='single')
        self.lb.pack()
        
        for x in listOptions:
                self.lb.insert(Tkinter.END,x)
        
        box.pack()

    def buttonbox(self):
        # add standard button box. override if you don't want the
        # standard buttons

        box = Tkinter.Frame(self)

        w = Tkinter.Button(box, text="OK", width=10, command=self.ok, default="active")
        w.pack(side="left", padx=5, pady=5)
        w = Tkinter.Button(box, text="Cancel", width=10, command=self.cancel)
        w.pack(side="left", padx=5, pady=5)

        self.bind("&lt;Return&gt;", self.ok)
        self.bind("&lt;Escape&gt;", self.cancel)

        box.pack()

    #
    # standard button semantics

    def ok(self, event=None):

        if not self.validate():
            self.initial_focus.focus_set() # put focus back
            return

        self.withdraw()
        self.update_idletasks()

        self.apply()

        # we want to return self.lb.curselection()
        self.result = self.lb.curselection()

        self.cancel()


    def cancel(self, event=None):

        # put focus back to the parent window
        self.parent.focus_set()
        self.destroy()

    #
    # command hooks

    def validate(self):

        return 1 # override

    def apply(self):

        pass # override





################################ Dot Grapher ######################
# given a state table with names, draw graphs in dot format.

"""
     +  CNsib  +    s    #    y    o   @
     e  CNsib  @    s    #    i    o   @
 1:  0    2    1    2    1    2    7   1
 2:  3    2    5    2    1    2    7   1
 3.  0    0    0    4    0    0    0   0
 4.  0    0    1    0    1    0    0   0
 5:  0    1    1    6    1    1    1   1
 6:  0    1    0    1    0    1    1   1
 7:  3    2    1    2    1    2    7   1
"""

# so first we will create the states.
# then we will write the edges & name them.
# name 0 as fail

# call the dot drawer on the file & display the graph.

def dotformat(nodeEdgeAry):
        # choose graphsize based upon number of nodes
        graphWH = '4,4'
        if len(nodeEdgeAry) > 3: graphWH = '5,5'
        if len(nodeEdgeAry) > 5: graphWH = '6,6'
        if len(nodeEdgeAry) > 7: graphWH = '7,7'
        if len(nodeEdgeAry) > 10: graphWH = '7.5,7.5'
        
        # print len(nodeEdgeAry)
        # print graphWH
        
        dotstring = ''
        dotstring += "  size=\""+ graphWH +"\"\n"
        # dotstring += "        page=\"7,7\"\n"
        dotstring += "  ratio=fill\n"
        # dotstring += "        rankdir=LR\n"
        # dotstring += "        center=1\n"
        for x in nodeEdgeAry:
                if x['node'] == 'Begin': features = ' [' + 'shape=box,color=lightblue,style=filled] '
                elif x['node'] == 'End': features = ' [' + 'color="Light Coral",style=filled] '
                elif x['features'] : features = ' [' + x['features'] + '] '
                elif not x['features'] : features = ''
                
                dotstring += (' "' + x['node'] + '" ' + features + ";\n")
                for e in range(len(x['edges'])):
                        dotstring += (' "' + x['node'] + '" -> "' + x['edges'][e] + '" ')
                        if e < len(x['edgenames']) : dotstring += ('[label="\l'+ x['edgenames'][e] + '"]' )
                        dotstring += ";\n"
                        
        dotstring = "digraph autograph {\n" + dotstring + "\n}\n"
        return dotstring
                
def _classeq(instance1, instance2):
    """
    @return: true iff the given objects are instances of the same
        class.
    @rtype: C{bool}
    """
    return (type(instance1) == types.InstanceType and
            type(instance2) == types.InstanceType and
            instance1.__class__ == instance2.__class__)

# given a dot string, write to a tmp file and invoke the grapher
# return a filename to open.
# imagetype is hardcoded for now
def dot2image(filename, dotstring):
        dotfilename = filename + '.dot'
        # imgfilename = filename + '.gif'
        psfilename = filename + '.ps'
        imgfilename = filename + '.ppm'
        pngfilename = filename + '.png'

        # whack the file if already there... (for now)
        f = open(dotfilename, 'w')
        f.write(dotstring)
        f.close()

        os.system('dot -Tps -o ' + psfilename +' ' + dotfilename)       
        # os.system('dot -Tgif -o ' + imgfilename +' ' + dotfilename)

        #print filename + "\n"
        #print imgfilename + "\n"

        # cheap hack now that graphviz is not working right...
        os.system('rm -f ' + imgfilename)
        os.system('pstopnm -stdout -portrait -ppm ' + psfilename + ' > ' + imgfilename)

        if os.path.isfile(imgfilename) : return imgfilename
        
        return ''





################################ KIMMO SET ######################
                
# ----------- KIMMOCONTROL ---------------
# Master instance for creating a kimmo object
# from files or strings or rules & lexical entries
# -------------------------------------
class KimmoControl:
    def __init__(self, lexicon_string, rule_string, lexicon_file, rule_file, debug):

        self.validateLexicon = []
        self.validateAlternations = []
        
        self.lexicalNodes = []  # transition states and edges for graphing lexicon
        self.ruleNodes = []     # transition states & edges for graphing of rules
        
        # a better way is just to use a destructor and check if the object exists.
        self.ok = 0
        self.errors = ''
        
        # load lexicon file
        if lexicon_file:
                f = read_kimmo_file(lexicon_file)
                lexicon_string = string.join(f.readlines(),"")
                f.close()

        # load rule file
        if rule_file:
                f = read_kimmo_file(rule_file)
                rule_string = string.join(f.readlines(),"")
                f.close()       
        
        try:
                self.processRules(rule_string)
                self.processLexicon(lexicon_string)
                self.m = KimmoMorphology(self.kalternations, self.klexicons)
                self.m.set_boundary(self.boundary_char)
                self.s = KimmoRuleSet(self.ksubsets, self.kdefaults, self.krules)
                self.s.debug = debug
                self.ok = 1
        except RuntimeError, e:
                self.errors = ('Caught:' + str(e) + ' ' + self.errors)
                print 'Caught:', e
                print "Setup of the kimmoinstance failed.  Most likely cause"
                print "is infinite recursion due to self-referential lexicon"
                print "For instance:"
                print "Begin: Begin Noun End"
                print "Begin is pointing to itself.  Simple example, but check"
                print "to insure no directed loops"
                self.ok = 0
        
        

    def generate(self, word):
        if self.boundary_char: word += self.boundary_char
        genlist = _generate_test(self.s, word)
        
        genliststr = genlist.__repr__()
        if self.boundary_char: genliststr = genliststr.replace(self.boundary_char,'')

        return eval(genliststr)
        
    def recognize(self, word):
        return _recognize_test(self.s, word, self.m)


        # run a batch and print to console.  This is different than the
        # batch for the gui;
        # the kimmo object should already be created when the batch is run.
        # the output is also not formatted nicely
    def batch(self, filename):
        if filename:
                f = read_kimmo_file(filename)
                lines = f.readlines()
                f.close()
                
                # space the results out a little
                results_string = ''
                for line in lines:
                                                        # a 'g word' 'r word' format
                        singleword = line.strip()       # should be a single word, no spaces, etc.
                        spcr = re.compile(r"\s+")
                        linevals = []
                        linevals = spcr.split(singleword)
                        
                        batch_result = []
                        batch_result_str = ''
                        if not singleword: continue     # ignore blank lines
                        elif (singleword[0] == '#') or (singleword[0] == ';'):  # commented;
                                results_string += (singleword + '\n')

                        elif (linevals[0] == 'g') and (len(linevals) == 2):
                                batch_result = self.generate(linevals[1])
                        elif (linevals[0] == 'r') and (len(linevals) == 2):
                                batch_result = self.recognize(linevals[1])
                                
                        elif '+' in singleword:
                                batch_result = self.generate(singleword)
                        else:
                                batch_result = self.recognize(singleword)
                        
                        # if a valid results
                        if len(batch_result) > 0:
                                for x in batch_result: batch_result_str = batch_result_str + x
                                batch_result_str = batch_result_str + '\n'
                                results_string += (batch_result_str)
                        
                # place a separator between results
                print '----- '+ time.strftime("%a, %d %b %Y %I:%M %p", time.gmtime()) +' -----\n'
                print results_string




        # move this out into a kimmo files & frontend class.
        # make this also process alternations, if contained.
    def processLexicon(self, text):
        """
        Takes the currently typed in lexicon and turns them from text into
        the kimmo lexicon array.
        """
        # text = self.lexicon.get(1.0, Tkinter.END)
        testlex = []
        self.klexicons = []     # lexicons needs to be an object of the gui scope
        lexigroup = ''
        kimmoWords = []
        alternationText = ''

        tmpnode = {}            # a node and its edges
        tmpnode['node'] = ''
        tmpnode['features'] = ''
        tmpnode['edges'] = []
        tmpnode['edgenames'] = []
        self.lexicalNodes = []  # list of nodes & their edges for the lexicon

        for item in text.split("\n"):
            # ''   None         Genitive
            cleanLine = item.strip()


            if len(cleanLine) == 0 : continue           # blank line
            elif cleanLine[0] == '#' : continue         # a comment
            elif cleanLine[0] == ';' : continue         # a comment

            # elsif there is a : then start up this lexicon entry.
            # if there is already a value in lexigroup, then append to lexicons
            # assume that : is the last char.
            # LEXICON N_ROOT1
            elif cleanLine[len(cleanLine)-1] == ':' :
                if (len(lexigroup) > 0):
                        if len(kimmoWords):
                                # print lexigroup
                                # print kimmoWord
                                self.klexicons.append( KimmoLexicon(lexigroup, kimmoWords) )
                                self.lexicalNodes.append(tmpnode)
                        kimmoWords = []
                lexigroup = cleanLine[0:len(cleanLine)-1]       # remove trailing ':'  , new group
                
                # create the state transitions for the lexicon.
                tmpnode = {}
                tmpnode['node'] = lexigroup
                tmpnode['features'] = ''
                tmpnode['edges'] = []
                tmpnode['edgenames'] = []
                
                self.validateLexicon.append(lexigroup)
                # print lexigroup
                
            # assume that a : contained in the line that is not a last char means it is an alternation.
            elif ':' in cleanLine:
                alternationText += ( cleanLine + "\n")

            elif lexigroup:
                p = re.compile(r"\s+")
                moreitems = []
                # moreitems = item.split(" ")                   # make sure to add tabs and other whitespace..
                moreitems = p.split(item)
                
                # this is splitting on the wrong char
                
                # *recollect*.  doesn't work on multiple spaces.
                # this code only works for the last field
                rangestart = -1
                for x in range(len(moreitems)):
                        # print moreitems[x]
                        if (moreitems[x][0] == '"') and (rangestart < 0): rangestart = x
                        elif (moreitems[x][len(moreitems[x])-1] == '"') and (rangestart > -1):
                                rangeend = x
                                moreitems[rangestart] = string.join(moreitems[rangestart:rangeend+1], " ")

                i = 0
                for furtheritem in moreitems:
                        furtheritem = furtheritem.strip()
                        moreitems[i] = furtheritem
                        
                        if not len(moreitems[i]): continue
                        if i > 2 : continue
                        else: testlex.append(moreitems[i])
                        i += 1

                for x in range(len(moreitems)):
                        if x > 2: continue
                        elif (moreitems[x] == '\'\'') or (moreitems[x] == '""'):
                                moreitems[x] = ''
                        elif (moreitems[x][0] == '"') and (moreitems[x][len(moreitems[x])-1] == '"'):
                                moreitems[x] = moreitems[x][1:len(moreitems[x])-1]
                        elif (moreitems[x][0] == '\'') and (moreitems[x][len(moreitems[x])-1] == '\''):
                                
                                tmpitem = moreitems[x]
                                moreitems[x] = tmpitem[1:(len(tmpitem)-1)]
                                
                        elif moreitems[x] == 'None' : moreitems[x] = None
                
                # EXPECTED FORMAT IS:
                # WORD ALTERNATION DESCRIPTION
                if len(moreitems) > 2 :
                        kimmoWords.append( KimmoWord(moreitems[0], moreitems[2], moreitems[1]) )
                        self.validateLexicon.append(moreitems[1])
                        # print moreitems
                elif len(moreitems) > 1 :
                        kimmoWords.append( KimmoWord(moreitems[0], '', moreitems[1]) )
                        self.validateLexicon.append(moreitems[1])
                        
                if (len(moreitems) > 1) and not (moreitems[1] in tmpnode['edges']):
                        tmpnode['edges'].append(moreitems[1])

            else :
                # an undefined line.
                self.errors += "Unknown Line in Lexicon (" + cleanLine + ")"

        # if the end of file and there is a group defined, add this last group
        if (len(lexigroup) > 0) and (len(kimmoWords)):
                self.klexicons.append( KimmoLexicon(lexigroup, kimmoWords) )
                self.lexicalNodes.append(tmpnode)

        # process the alternations
        # print alternationText
        self.processAlternations(alternationText)


        # return an array of state and edge objects.
        return self.lexicalNodes



    # process ALTERNATIONS
    # self.kalternations = [
        #           KimmoAlternation('Begin',          [ 'N_ROOT', 'ADJ_PREFIX', 'V_PREFIX', 'End' ]),

    def processAlternations(self, text):
        """
        Takes the currently typed in alternations and turns them from text into
        the kimmo alternation array.
        """
        # text = self.alternation.get(1.0, Tkinter.END)
        testalt = []
        self.kalternations = [] # lexicons needs to be an object of the gui scope
        altgroup = ''
        kimmoAlts = []

        for line in text.split("\n"):
            # ''   None         Genitive
            cleanLine = line.strip()

            if len(cleanLine) == 0 : continue           # blank line
            elif cleanLine[0] == '#' : continue         # a comment
            elif cleanLine[0] == ';' : continue         # a comment
            else:
                # lets do this one differently.
                # lets break it first, then keep on looping until we find the next group (signified by a : )
                p = re.compile(r"\s+")
                items = []
                items = p.split(cleanLine)

                for item in items:
                        item_tmp = item.strip()
                        
                        
                        if len(item_tmp) == 0 : continue
                        # ALTERNATION V_root    
                        elif ':' in item_tmp :
                                # all all prior alternations to prior altgroup (if defined)
                                if len(altgroup) > 0:
                                        if len(kimmoAlts) > 0:
                                                self.kalternations.append(
                                                        KimmoAlternation(altgroup, kimmoAlts) )
                                                        
                                                self.validateAlternations.append(altgroup)
                                                for x in kimmoAlts: self.validateAlternations.append(x)
                                                self.lexicalNodes.append(tmpnode)
                                        
                                        
                                # set new altgroup
                                altgroup = cleanLine[0:len(item_tmp)-1]
                                kimmoAlts = []
                                
                                tmpnode = {}
                                tmpnode['node'] = altgroup
                                tmpnode['features'] = 'color=\"aquamarine2\", style=filled'
                                tmpnode['edges'] = []
                                tmpnode['edgenames'] = []

                        
                        else :
                                # remove '' surrounding alternations
                                if (item_tmp[0] == '\'') and (item_tmp[len(item_tmp)-1] == '\''):
                                        item_tmp = item_tmp[1:(len(item_tmp)-1)]
                                # convert None
                                elif item_tmp == 'None' : item_tmp = None
                                        
                                # print 'a \'' + item_tmp + '\''
                                kimmoAlts.append(item_tmp)
                                
                                # add alternation edges ; order independent.
                                tmpnode['edges'].append(item_tmp)

        if len(altgroup) > 0:
                if len(kimmoAlts) > 0:
                        self.kalternations.append(
                        KimmoAlternation(altgroup, kimmoAlts) )
                        self.validateAlternations.append(altgroup)
                        for x in kimmoAlts: self.validateAlternations.append(x)
                        self.lexicalNodes.append(tmpnode)

        # print self.validateAlternations



    # RULES
    # Rule format
    # KimmoFSARule('08:elision: e:0 <= VCC*___+:0 V',
        #                        '             Cpal C    e:0 e:@ +:0 Vbk V   @', # english.rul needed pairs re-ordered
        #                        [ (1, True, [ 1,   1,   1,  2,  1,  2,  2,  1 ]),
        #                          (2, True, [ 3,   6,   1,  2,  1,  2,  2,  1 ]),    # V...
        #                          (3, True, [ 3,   6,   1,  4,  1,  2,  2,  1 ]),    # V Cpal...
        #                          (4, True, [ 1,   1,   1,  2,  5,  2,  2,  1 ]),    # V Cpal e...
        #                          (5, True, [ 1,   1,   1,  0,  1,  2,  0,  1 ]),    # V Cpal e +:0... [english.rul needed fixing]
        #                          (6, True, [ 1,   1,   1,  7,  1,  2,  2,  1 ]),    # V C...
        #                          (7, True, [ 1,   1,   1,  2,  8,  2,  2,  1 ]),    # V C e...
        #                          (8, True, [ 1,   1,   1,  0,  1,  0,  0,  1 ]) ]), # V C e +:0... [english.rul needed fixing]
    def processRules(self, text):
        """
        Takes the currently typed in rules and processes them into the python kimmo
        format.  expects rules to be in c version of .rul file format.  needs to
        be file compatible.
        """
        # text = self.rules.get(1.0, Tkinter.END)
        testrule = []
        self.krules = []        
        self.ksubsets = []      
        self.kdefaults = []
        self.boundary_char = ''
        setgroup = ''
        rulegroup = ''
        rulerowcnt = 0
        rulecolcnt = 0
        kimmoRule = []



        ruleFrom = []
        ruleTo = []
        ruleTran = []

        anyset = ['','','','']


        tmpnode = {}            # a node and its edges
        tmpnode['node'] = ''
        tmpnode['features'] = ''
        tmpnode['edges'] = []           # list of the transitions
        tmpnode['edgenames'] = []       # matched array naming each transition

        tmpfsanodes = {}
        tmpfsanodes['nodes'] = []
        tmpfsanodes['name'] = ''
        self.fsasNodes = []     # list of nodes & their edges for the lexicon


        for line in text.split("\n"):
            # ''   None         Genitive
            cleanLine = line.strip()



            if len(cleanLine) == 0 : continue           # blank line
            # this char can be a comment if it is not the boundary char.
            # yes, yes, it should be defined such that it is not in the alphabet at all
            # also boundary would need to be defined before ...
            elif (cleanLine[0] == '#') and (anyset[3] != '#'): continue         # a comment
            elif (cleanLine[0] == ';') and (anyset[3] != ';') : continue                # a comment
            else:
                # lets do this one differently.
                # lets break it first, then keep on looping until we find the next group (signified by a : )
                p = re.compile(r"\s+")
                items = []
                items = p.split(cleanLine)
                
                # now handle subset keywords
                # KimmoSubset('C', 'b c d f g h j k l m n p q r s t v w x y z'),
                
                if items[0] == 'SUBSET':
                        if items[1] == 'ALL': items[1] = '@'
                        self.ksubsets.append(
                                KimmoSubset(items[1], string.join(items[2:len(items)]," ") ))
                        # print items[1] + ' ' + string.join(items[2:len(items)]," ")
                        
                # load up the fsa regexp based on alphabet      
                # also set up the @ subset if alphabet is defined (old rule file style)
                elif items[0] == 'ALPHABET': anyset[1] = string.join(items[1:len(items)]," ")
                
                elif items[0] == 'ANY': anyset[0] = items[1]
                
                elif items[0] == 'NULL': anyset[2] = items[1]
                
                # using the boundary char, set the final boundary & also add to the any set.
                elif items[0] == 'BOUNDARY':
                        anyset[3] = items[1]
                        self.boundary_char = items[1]
                
                elif items[0] == 'DEFAULT':
                        self.kdefaults = [ KimmoDefaults(string.join(items[1:len(items)]," ")) ]
                        
                elif items[0] == 'ARROWRULE':
                        # ARROWRULE 03:epenthesis1 0:e ==> [Csib (c h) (s h) y:i] +:0 _ s [+:0 #]
                        # KimmoArrowRule('03:epenthesis1',  '0:e ==> [Csib (c h) (s h) y:i] +:0 _ s [+:0 #]'),
                        # print items[1] + ' ' + string.join(items[2:len(items)]," ")
                        self.krules.append(
                                KimmoArrowRule(items[1],  string.join(items[2:len(items)]," "))
                                # KimmoArrowRule('05:y:i-spelling', 'y:i <=> @:C +:0? _ +:0 ~I')
                                )

                elif items[0] == 'RULE':        # this is actually FSArules
                                                                        # make compatible with rul files
                        
                        if rulegroup: self.guiError('error, fsa rule not finished')
                        
                        rulecolcnt = string.atoi(items[len(items)-1])
                        rulerowcnt = string.atoi(items[len(items)-2])
                        rulegroup = string.join(items[1:len(items)-2])
                        
                        # create the structure (for graphing) for storing the transitions
                        # of the fsas
                        tmpfsanodes = {}
                        tmpfsanodes['nodes'] = []
                        tmpfsanodes['name'] = rulegroup
                        
                        # add the fail node by default
                        tmpnode = {}            # a node and its edges
                        tmpnode['node'] = '0'
                        tmpnode['features'] = 'color="indianred1", style=filled, shape=box'
                        tmpnode['edges'] = []
                        tmpnode['edgenames'] = []
                        
                        tmpfsanodes['nodes'].append(tmpnode)
                        
                        
                        
                elif rulegroup:

                        # assume TRUE rules for now
                        # non-char test; already stripped of whitespace
                        ct = re.compile('[^0-9:\.]')    # go with [A-Za-z]
                        # if text, then add to first lines of fsa
                                # get row1 and row2 of text & translate into x:y col format.
                                
                        # if a number and until number is equal to row count, add
                                # i.e. not text
                        if ((':' in items[0]) or ('.' in items[0])) and (not ct.match(items[0])):
                                # make sure to check for TRUE vs FALSE rows...
                                # sprint items[0][0:len(items[0])-1] + ' -- ' + string.join(items[1:len(items)], " ")
                                
                                if (items[0][len(items[0])-1] == ':') : finalstate = True
                                elif (items[0][len(items[0])-1] == '.') : finalstate = False
                                else :
                                        self.guiError("FSA table failure -- 'final state defn'")
                                        continue
                                
                                items[0] = items[0][0:len(items[0])-1]  # remove the ':'
                                
                                # convert to integers (instead of strings)
                                for x in range(rulecolcnt + 1): items[x] = string.atoi(items[x]) # including the first row number - i.e. '4:'
                                
                                # add this row.
                                kimmoRule.append((items[0], finalstate, items[1:len(items)]))
                                
                                # now make this row into graph transitions
                                tmpnode = {}            # a node and its edges
                                tmpnode['node'] = str(items[0])
                                tmpnode['features'] = 'shape=box, fillcolor="lavender blush", style=filled'
                                if finalstate and (items[0] == 1):
                                        tmpnode['features'] = 'shape=circle, color="paleturquoise2", style=filled'
                                elif (items[0] == 1):
                                        tmpnode['features'] = 'color="paleturquoise2", style=filled, shape=box'
                                elif (finalstate):
                                        tmpnode['features'] = 'shape=circle,fillcolor="honeydew2", style=filled'
                                tmpnode['edges'] = []
                                tmpnode['edgenames'] = []
                                # add as strings
                                # add unique, but group edgenames together
                                
                                tmpitems = items[1:len(items)]
                                for i in range(len(tmpitems)):
                                        if str(tmpitems[i]) in tmpnode['edges']:
                                                # find the index j of the matching target
                                                for j in range(len(tmpnode['edges'])):
                                                        if str(tmpnode['edges'][j]) == str(tmpitems[i]):
                                                                
                                                                m = re.match(r"(^|\\n)([^\\]*)$", tmpnode['edgenames'][j])
                                                                # instead use a regular expression...
                                                                # this should really be done in dotstring
                                                                                                                                        
                                                                if not m:
                                                                        tmpnode['edgenames'][j] += (',' + ruleTran[i])
                                                                elif (len(m.group(2)) >= 15):
                                                                        tmpnode['edgenames'][j] += ('\\n ' + ruleTran[i])
                                                                else:
                                                                        tmpnode['edgenames'][j] += (',' + ruleTran[i])
                                        else:
                                                tmpnode['edges'].append(str(tmpitems[i]))
                                                tmpnode['edgenames'].append(ruleTran[i])
                                                
                                        
                                """
                                for x in items[1:len(items)]:
                                        # go through and check, already added?
                                        # for i in range(len(tmpnode['edges'])):
                                        #       if tmpnode['edges'][i] == x:
                                        #               tmpnode['edgenames'][i] += "," +
                                        
                                        tmpnode['edges'].append(str(x))
                                for x in ruleTran: tmpnode['edgenames'].append(x)
                                """
                                tmpfsanodes['nodes'].append(tmpnode)
                                
                                
                                # if number is equal to row count, then add total and reset rule group
                                if ( items[0] == rulerowcnt):
                                        self.krules.append(
                                                KimmoFSARule(str(rulerowcnt)+':'+rulegroup, string.join(ruleTran," "), kimmoRule))
                                        
                                        # add to the master graph list
                                        self.fsasNodes.append(tmpfsanodes)
                                        
                                        
                                        rulegroup = ''
                                        rulerowcnt = 0
                                        rulecolcnt = 0
                                        ruleTran = []   # reset the translation array
                                        kimmoRule = []  # resent the kimmo rules as well
                        
                        # the char class/translations
                        elif len(items) == rulecolcnt:
                                # old style has 2 rows, class from, class to
                                if len(ruleFrom) == 0: ruleFrom = items
                                elif len(ruleTo) == 0: ruleTo = items
                                
                                # if ruleTo is ruleFrom: continue
                                
                                if (len(ruleTo) != rulecolcnt) or (len(ruleFrom) != rulecolcnt): continue
                                else:
                                        for x in range(rulecolcnt):
                                                if ruleTo[x] == ruleFrom[x]: ruleTran.append(ruleTo[x])
                                                else:
                                                        ruleTran.append(ruleFrom[x] + ':' + ruleTo[x])
                                        
                                        ruleTo = []
                                        ruleFrom = []

        # take care of the anyset, if it was defined (make into a subset)
        if (anyset[0] and anyset[1]):
                self.ksubsets.append(KimmoSubset(anyset[0], string.join(anyset[1:len(anyset)]," ") ))
                
        # print self.fsasNodes
                
                
# ----------- KIMMOPAIR ---------------
#
# -------------------------------------
class KimmoPair:
    """
    Input/Output character pair
    """
    def __init__(self, input_subset, output_subset):
        self._input = input_subset
        self._output = output_subset


    def input(self): return self._input
    def output(self): return self._output


    def __repr__(self):
        sI = self.input()
        sO = self.output()
        s = sI + ':' + sO
        return s


    def __eq__(self, other):
        return (_classeq(self, other) and
                self._input == other._input and
                self._output == other._output)


    def __hash__(self):
        return hash( (self._input, self._output,) )


    def matches(self, input, output, subsets, negatedOutputMatch=False):
        if not(self._matches(self.input(), input, subsets)): return False
        m = self._matches(self.output(), output, subsets)
        if negatedOutputMatch: return not(m)
        return m


    def _matches(self, me, terminal, subsets):
        if (me == terminal): return True
        if (me[0] == '~'):
            m = me[1:]
            if (m in subsets):
                return not(terminal in subsets[m])
            else:
                return False
        if (me in subsets):
            return terminal in subsets[me]
        else:
            return False

_kimmo_terminal_regexp    = '[a-zA-Z0-9\+\'\-\#\@\$\%\!\^\`\}\{]+' # \}\{\<\>\,\.\~ # (^|\s)?\*(\s|$) !!! * is already covered in the re tokenizer
_kimmo_terminal_regexp_fsa    = '[^:\s]+' # for FSA, only invalid chars are whitespace and :
                                          # '[a-zA-Z0-9\+\'\-\#\@\$\%\!\^\`\}\{\<\>\,\.\~\*]+'
_kimmo_terminal_regexp_ext= '~?' + _kimmo_terminal_regexp

_kimmo_defaults           = _kimmo_terminal_regexp + '|\:'
_kimmo_defaults_fsa       = _kimmo_terminal_regexp_fsa + '|\:'
_kimmo_rule               = _kimmo_terminal_regexp_ext + '|[\:\(\)\[\]\?\&\*\_]|<=>|==>|<==|/<='

_arrows = ['==>', '<=>', '<==', '/<=']


_special_tokens = ['(', ')', '[', ']', '*', '&', '_', ':']
_special_tokens.extend(_arrows)
_non_list_initial_special_tokens = [')', ']', '*', '&', '_', ':']
_non_list_initial_special_tokens.extend(_arrows)


def parse_pair_sequence(description,token_type):
    """Read the description, which should be in form [X|X:Y]+, and return a list of pairs"""

    if token_type == 'FSA':
        desc = list(tokenize.regexp(description, _kimmo_defaults_fsa))
    else:
        desc = list(tokenize.regexp(description, _kimmo_defaults))

    prev = None
    colon = False
    result = []
    for token in desc:
        if token == ':':
            if colon: raise ValueError('two colons in a row')
            if prev == None: raise ValueError('colon must follow identifier')
            colon = True
        elif colon:
            result.append(KimmoPair(prev, token))
            prev = None
            colon = False
        else:
            if prev:
                result.append(KimmoPair(prev, prev))
            prev = token
            colon = False
    if colon: raise ValueError('colon with no following identifier')
    if prev: result.append(KimmoPair(prev, prev))
    return result



class KimmoSubset:
    def __init__(self, name, description):
        self._name = name
        self._description = description
        self._subset = list(set(tokenize.regexp(description, _kimmo_terminal_regexp_fsa)))
    def name(self): return self._name
    def description(self): return self._description
    def subset(self): return self._subset
    def __repr__(self):
        return '<KimmoSubset %s: %s>' % (self.name(), self.description(),)

class KimmoDefaults:
    def __init__(self, description):
        self._description = description
        self._defaults = set()
        for p in parse_pair_sequence(description, ''):
            self.defaults().add(p)
    def defaults(self): return self._defaults
    def __repr__(self):
        return '<KimmoDefaults %s>' % (self._description,)

class KimmoRule:
    def pairs(self): raise RuntimeError('unimplemented: KimmoRule.pairs()')
    def right_advance(self, current_states, input, output, subsets):
        raise RuntimeError('unimplemented: KimmoRule.right_advance()')


class KimmoArrowRule:
    """
    Two level rule
    """

    def leftFSA(self): return self._left_fsa
    def rightFSA(self): return self._right_fsa
    def pairs(self): return self._pairs
    def arrow(self): return self._arrow
    def lhpair(self): return self._lhpair

    def __init__(self, name, description):
        self._name = name
        self._description = description
        self._negated = False
        self._pairs = set()
        desc = list(tokenize.regexp(description, _kimmo_rule))
        self._parse(desc)

    def __repr__(self):
        return '<KimmoArrowRule %s: %s>' % (self._name, self._description)

    def advance(self, fsa, current_states, input, output, subsets):
        """Returns a tuple of (next_states, contains_halt_state)"""
        result = []
        contains_halt_state = False
        for current_state in current_states:
            for next_state in fsa.forward_traverse(current_state):
                ok = False
                for pair in fsa._labels[(current_state, next_state)]:
                    if pair.matches(input, output, subsets):
                        ok = True
                        break
                if (ok):
                    if (next_state in fsa.finals()): contains_halt_state = True
                    if not(next_state in result): result.append(next_state)
        return (result, contains_halt_state)


    def right_advance(self, current_states, input, output, subsets):
        return self.advance(self.rightFSA(), current_states, input, output, subsets)

    def matches(self, input, output, subsets):
        """Does this rule's LHS match this input/output pair?


        If it doesn't, return None.  If it does, return True if the rule must pass, False if the rule must fail."""


        if (self.arrow() == '==>'):
            if self.lhpair().matches(input, output, subsets):
                return True
            else:
                return None
        elif (self.arrow() == '<=='):
            if self.lhpair().matches(input, output, subsets, negatedOutputMatch=True):
                return False
            else:
                return None
        elif (self.arrow() == '/<='):
            if self.lhpair().matches(input, output, subsets, negatedOutputMatch=False):
                return False
            else:
                return None
        elif (self.arrow() == '<=>'):
            if self.lhpair().matches(input, output, subsets, negatedOutputMatch=False):
                return True
            elif self.lhpair().matches(input, output, subsets, negatedOutputMatch=True):
                return False
            else:
                return None
        else:
            raise RuntimeError('unknown arrow: '+self.arrow())

    def _parse(self, tokens):

        (end_pair, tree)  = self._parse_pair(tokens, 0)
        lhpair = self._pair_from_tree(tree)
        self._lhpair = lhpair
        self._pairs.add(lhpair)

        end_arrow         = self._parse_arrow(tokens, end_pair)
        (end_left, lfsa)  = self._parse_context(tokens, end_arrow, True)
        end_slot          = self._parse_slot(tokens, end_left)
        (end_right, rfsa) = self._parse_context(tokens, end_slot, False)
        if not(end_right == len(tokens)):
            raise ValueError('unidentified tokens')

        self._left_fsa  = lfsa
        self._right_fsa = rfsa

    def _next_token(self, tokens, i, raise_error=False):
        if i >= len(tokens):
            if raise_error:
                raise ValueError('ran off end of input')
            else:
                return None
        return tokens[i]

    def _pair_from_tree(self, tree):
        if (tree.node != 'Pair'): raise RuntimeException('expected Pair, got ' + str(tree))
        if len(tree) == 1:
            return KimmoPair(tree[0], tree[0])
        else:
            return KimmoPair(tree[0], tree[2])

    def _parse_pair(self, tokens, i):
        # print 'parsing pair at ' + str(i)
        t1 = self._next_token(tokens, i, True)
        if t1 in _special_tokens: raise ValueError('expected identifier, not ' + t1)
        t2 = t1
        j = i + 1
        if self._next_token(tokens, j) == ':':
            t2 = self._next_token(tokens, j+1, True)
            if t2 in _special_tokens: raise ValueError('expected identifier, not ' + t2)
            j = j + 2
            tree = Tree('Pair', tokens[i:j])
        else:
            tree = Tree('Pair', [tokens[i]])
        #print str(self._pair_from_tree(tree)) + ' from ' + str(i) + ' to ' + str(j)
        return (j, tree)


    def _parse_arrow(self, tokens, i):
        self._arrow = self._next_token(tokens, i, True)
        if not(self.arrow() in _arrows):
            raise ValueError('expected arrow, not ' + self.arrow())
        #print 'arrow from ' + str(i) + ' to ' + str(i+1)
        return i + 1


    def _parse_slot(self, tokens, i):
        slot = self._next_token(tokens, i, True)
        if slot != '_':
            raise ValueError('expected _, not ' + slot)
        # print 'slot from ' + str(i) + ' to ' + str(i+1)
        return i + 1


    def _parse_context(self, tokens, i, reverse):
        (j, tree) = self._parse_list(tokens, i)
        if j == i: return (i, None)

        sigma = set()
        self._collect_alphabet(tree, sigma)
        fsa = FSA(sigma)
        final_state = self._build_fsa(fsa, fsa.new_state(), tree, reverse)
        fsa.set_final([final_state])
        #fsa.pp()
        dfa = fsa.dfa()
        #dfa.pp()
        dfa.prune()
        #dfa.pp()
        return (j, dfa)


    def _collect_alphabet(self, tree, sigma):
        if tree.node == 'Pair':
            pair = self._pair_from_tree(tree)
            sigma.add(pair)
            self._pairs.add(pair)
        else:
            for d in tree: self._collect_alphabet(d, sigma)


    def _parse_list(self, tokens, i, type='Cons'):
        # print 'parsing list at ' + str(i)
        t = self._next_token(tokens, i)
        if t == None or t in _non_list_initial_special_tokens:
            # print '  failing immediately '
            return (i, None)
        (j, s) = self._parse_singleton(tokens, i)
        (k, r) = self._parse_list(tokens, j, type)
        # print (k,r)
        if r == None:
            # print '  returning (%d, %s)' % (j, s)
            return (j, s)
        tree = Tree(type, [s, r])
        # print '  returning (%d, %s)' % (k, tree)
        return (k, tree)


    def _parse_singleton(self, tokens, i):
        # print 'parsing singleton at ' + str(i)
        t = self._next_token(tokens, i, True)
        j = i
        result = None
        if t == '(':
            (j, result) = self._parse_list(tokens, i + 1, 'Cons')
            if result == None: raise ValueError('missing contents of (...)')
            t = self._next_token(tokens, j, True)
            if t != ')': raise ValueError('missing final parenthesis, instead found ' + t)
            j = j + 1
        elif t == '[':
            (j, result) = self._parse_list(tokens, i + 1, 'Or')
            if result == None: raise ValueError('missing contents of [...]')
            t = self._next_token(tokens, j, True)
            if t != ']': raise ValueError('missing final bracket, instead found ' + t)
            j = j + 1
        elif t in _special_tokens:
            raise ValueError('expected identifier, found ' + t)
        else:
            (j, tree) = self._parse_pair(tokens, i)
            result = tree
        t = self._next_token(tokens, j)
        if t in ['*', '&', '?']:
            j = j + 1
            result = Tree(t, [result])
        return (j, result)


    def _build_fsa(self, fsa, entry_node, tree, reverse):
        if tree.node == 'Pair':
            return self._build_terminal(fsa, entry_node, self._pair_from_tree(tree))
        elif tree.node == 'Cons':
            return self._build_seq(fsa, entry_node, tree[0], tree[1], reverse)
        elif tree.node == 'Or':
            return self._build_or(fsa, entry_node, tree[0], tree[1], reverse)
        elif tree.node == '*':
            return self._build_star(fsa, entry_node, tree[0], reverse)
        elif tree.node == '&':
            return self._build_plus(fsa, entry_node, tree[0], reverse)
        elif tree.node == '?':
            return self._build_qmk(fsa, entry_node, tree[0], reverse)
        else:
            raise RuntimeError('unknown tree node'+tree.node)


    def _build_terminal(self, fsa, entry_node, terminal):
        new_exit_node = fsa.new_state()
        fsa.insert(entry_node, terminal, new_exit_node)
        #print '_build_terminal(%d,%s) -> %d' % (entry_node, terminal, new_exit_node)
        return new_exit_node


    def _build_plus(self, fsa, node, tree, reverse):
        node1 = self._build_fsa(fsa, node, tree[0], reverse)
        fsa.insert(node1, epsilon, node)
        return node1


    def _build_qmk(self, fsa, node, tree, reverse):
        node1 = fsa.new_state()
        node2 = self._build_fsa(fsa, node1, tree, reverse)
        node3 = fsa.new_state()
        fsa.insert(node, epsilon, node1)
        fsa.insert(node, epsilon, node3)
        fsa.insert(node2, epsilon, node3)
        return node3


    def _build_star(self, fsa, node, tree, reverse):
        node1 = fsa.new_state()
        node2 = self._build_fsa(fsa, node1, tree, reverse)
        node3 = fsa.new_state()
        fsa.insert(node, epsilon, node1)
        fsa.insert(node, epsilon, node3)
        fsa.insert(node2, epsilon, node1)
        fsa.insert(node2, epsilon, node3)
        return node3


    def _build_seq(self, fsa, node, tree0, tree1, reverse):
        (d0, d1) = (tree0, tree1)
        if reverse: (d0, d1) = (d1, d0)
        node1 = self._build_fsa(fsa, node, d0, reverse)
        node2 = self._build_fsa(fsa, node1, d1, reverse)
        # print '_build_seq(%d,%s,%s) -> %d,%d' % (node, tree0, tree1, node1, node2)
        return node2

    def _build_or(self, fsa, node, tree0, tree1, reverse):
        node0 = fsa.new_state()
        node1 = fsa.new_state()
        node2 = self._build_fsa(fsa, node0, tree0, reverse)
        node3 = self._build_fsa(fsa, node1, tree1, reverse)
        node4 = fsa.new_state()
        fsa.insert(node, epsilon, node0)
        fsa.insert(node, epsilon, node1)
        fsa.insert(node2, epsilon, node4)
        fsa.insert(node3, epsilon, node4)
        return node4


class KimmoFSARule:
    def __init__(self, name, pair_description, state_descriptions):
        self._name        = name
        self._pairs       = parse_pair_sequence(pair_description, 'FSA')
        self.transitions = {}
        self.is_final   = {}
        self._state_descriptions = state_descriptions
        # validate transitions
        for (index, is_final, next_state_array) in state_descriptions:
            if not(is_final == True or is_final == False):
                raise ValueError('each state description must take the form (index, True/False, [next_state_indices...]')

            if len(next_state_array) != len(self.pairs()):
                raise ValueError('transition array of wrong size '+ str(len(next_state_array)) + ' ' + str(len(self.pairs())))
            self.transitions[index] = next_state_array
            self.is_final[index] = is_final

    def name(self): return self._name
    def pairs(self): return self._pairs
    def start(self): return self._state_descriptions[0][0]
    def is_state(self, index): return self.transitions.has_key(index)


    def contains_final(self, indices):
        for i in indices:
            if self.is_final[i]: return True
        return False


    def sorted_pairs(self, subsets):
        # pairs are ordered with the transition table, we want to order by the subset size.
        # returns a list of pairs AND their indices for use.
        # (index, pair) ; index represents the index of the position in the transitions table
        
        sorted_with_index = []
        for idx, pair in enumerate(self.pairs()):       # enumerate lists all & assigns an index
                                                                                                # important to note that pairs() are in order
                                                                                                # corresponding with transition table
                size1 = 1
                size2 = 1
                if pair.input() in subsets: size1 = len(subsets[pair.input()])
                if pair.output() in subsets: size2 = len(subsets[pair.output()])
                # setsize = size1 # + size2
                sorted_with_index.append([idx,pair,size1,size2])
        
        sorted_with_index.sort(lambda x,y: self.mycompare(x[2],y[2],x[3],y[3]) ) # lambda x, y: x[2] - y[2])
        return sorted_with_index


    # two field compare.
    def mycompare(self, x1, y1, x2=0, y2=0):
        if x1 == y1: return x2-y2
        else: return x1-y1

    def right_advance(self, current_states, input, output, subsets):

        next_states = []
        contains_halt_state = False
        for index in current_states:


            # flush the any states
            any_next_state = ''
            next_state_isset = 0
            any_next_states_ary = []

            for i, pair, size1, size2 in self.sorted_pairs(subsets): # enumerate(self.pairs()):
                
                # print pair.__repr__()

                if pair.matches(input, output, subsets):
                        
                    # print input, output
                    # we want to temporarily store an any state (if one matches)
                    # only 1 any_next_state allowed
                    # '@'
                    # consequence of this is that moving to the back prevents discovery
                    # of of all possible enumerations in forced -> 0 state cases.  ie. 0:i -> 0
                    # recognition causes a problem, here's why.  this routine encounters @ before +:i
                    # it ignores it and goes on to 0:i.  0:i returns under yield, maintaining iterator
                    # state.  advance is called again, iterator state is resumed, but @ was already
                    # passed, and memory of that state is lost.
                    # it would be best if enumerate would just sort, but it cannot as it would lose ordering
                    # also invert under recognize is not properly recursing, as it never even sees the possible
                    # +:i option.
                        # OLD CODE; PROBLEM SOLVED (ordering of subsets)
                    if 0: # ('@' in pair.__repr__()):
                        # print 'any state match'
                        # {col num, next state num (0 if fail), is final state}
                        # if transition row is valid
                        if self.transitions.has_key(self.transitions[index][i]): ft = self.is_final[self.transitions[index][i]]
                        else : ft = ''
                        any_next_states_ary.append([ i, self.transitions[index][i], ft, pair.__repr__() ] )
                        if not any_next_state:
                                    any_next_state = self.transitions[index][i]
                

                    else:
                            # if not an any state, add like usual
                            # if not already in next_states, add
                            # !!! but won't this break without evaluating @ when called several
                            # times?  (i.e. our state is already in next_state
                            next_state_isset = 1
                            next_state = self.transitions[index][i]
                            if self.transitions.has_key(next_state):
                                if not(next_state in next_states):
                                    next_states.append(next_state)
        
                                    if self.is_final[next_state]: contains_halt_state = True
                            break

        return (next_states, contains_halt_state)


    def __repr__(self):
        return '<KimmoFSARule %s>' % (self.name(), )


class KimmoWord:
    def __init__(self, letters, gloss, next_alternation=None):
        self._letters = letters
        self._gloss   = gloss
        self._next_alternation = next_alternation


    def __repr__(self):
        return '<KimmoWord %s: %s>' % (self.letters(), self.gloss())


    def letters(self): return self._letters
    def gloss(self): return self._gloss
    def next_alternation(self): return self._next_alternation


class KimmoLexicon:
    def __init__(self, name, words):
        self._name = name
        self._words = words
        self._trie = self.build_trie(words)


    def __repr__(self):
        return '<KimmoLexicon ' + self.name() + '>'


    def name(self): return self._name
    def words(self): return self._words
    def trie(self): return self._trie  # tree is ([KimmoWord], [ (char, sub-trie), ... ])


    def build_trie(self, words, word_position=0):
        if len(words) == 0: return ([], [])
        first_chars = {}
        for w in words:
            if len(w.letters()) <= word_position: continue
            fc = w.letters()[word_position]
            if first_chars.has_key(fc):
                first_chars[fc].append(w)
            else:
                first_chars[fc] = [ w ]
        sub_tries = []
        for c, sub_words in first_chars.items():
            sub_tries.append( (c, self.build_trie(sub_words, word_position+1)) )
        return ( [w for w in words if len(w.letters()) == word_position], sub_tries )


class KimmoAlternation:
    def __init__(self, name, lexicon_names):
        self._name = name
        self._lexicon_names = lexicon_names

    def __repr__(self):
        return '<KimmoAlternation ' + self.name() + ': ' + str(self.lexicon_names()) + '>'


    def name(self): return self._name
    def lexicon_names(self): return self._lexicon_names


class KimmoMorphology:
    def __init__(self, alternations, lexicons, start='Begin'):
        self.alternations = {}
        self.lexicons = {}
        self._start = start
        for a in alternations: self.alternations[a.name()] = a
        for l in lexicons: self.lexicons[l.name()] = l

    def set_boundary(self, boundary_char):
        self.boundary = boundary_char

    def initial_state(self):
        return self._collect(self._start)


    def possible_next_characters(self, state):
        chars = set()
        self._possible_next_characters(state, chars)
        return chars

        # from the lexicon, return the next possible character from all words that match the current state
        # for instance, if lexicon has iti, ili, and iyi, and current state is first [i], then
        # this function will return a set of (t,l,y)
    def _possible_next_characters(self, state, chars):
        for s in state:
            if isinstance(s, KimmoLexicon):
                (words, sub_tries) = s.trie()
            else:
                (words, sub_tries) = s
            for w in words:
                self._possible_next_characters(self._collect(w.next_alternation()), chars)
            for c, sub_trie in sub_tries:
                chars.add(c)

    def _collect(self, name):
        # print 'current alternation: ' + name
        if name == None:
            return []
        elif self.alternations.has_key(name):
            result = []
            for ln in self.alternations[name].lexicon_names():
                result.extend(self._collect(ln))
            return result
        elif self.lexicons.has_key(name):
            return [ self.lexicons[name] ]
        else:
            # raise ValueError('no lexicon or alternation named ' + name)
            return []

    def advance(self, state, char):
        result = []
        # print 'advance'

        for s in state:
            if isinstance(s, KimmoLexicon):
                # print s.name()
                (words, sub_tries) = s.trie()
            else:
                (words, sub_tries) = s
            for w in words:
                for v in self._advance_through_word(w, char):
                    yield v
            for c, sub_trie in sub_tries:
                if c == char: result.append(sub_trie)
        if len(result) > 0:
            yield (result, [])
        # else:
                # print 'No Matches in state '


    def _advance_through_word(self, word, char):
        for s in self.advance(self._collect(word.next_alternation()), char):
            state, words = s
            if word.gloss():
                yield (state, [word] + words)
            else:
                yield s

class KimmoRuleSet:
    def __init__(self, subsets, defaults, rules, null='0'):
        self.debug = False
        self._rules = rules
        self._pair_alphabet = set()
        self._subsets = {}
        self._null = null
        for s in subsets:
            self._subsets[s.name()] = s.subset()

        for kd in defaults:
            for pair in kd.defaults():
                # defaults shouldn't contain subsets
                if self.is_subset(pair.input()) or self.is_subset(pair.output()):
                    raise ValueError('default ' + str(pair) + ' contains subset')
                self._pair_alphabet.add( ( pair.input() , pair.output() ) )
        for r in self.rules():
            for kp in r.pairs():
                if (not (self.is_subset(kp.input()) or self.is_subset(kp.output()))):
                    self._pair_alphabet.add( ( kp.input(), kp.output() ) )

    def rules(self): return self._rules
    def subsets(self): return self._subsets
    def is_subset(self, key):
        return key[0] == '~' or key in self.subsets()

    def null(self): return self._null;


    def _evaluate_rule_left_context(self, rule, input, output):
        fsa = rule.leftFSA()
        if fsa == None: return True
        states = [ fsa.start() ]
        i = len(input) - 1
        while i >= 0:
            next_states = []
            (result, contains_halt_state) = rule.advance(fsa, states, input[i], output[i], self.subsets())
            if contains_halt_state: return True
            for s in result:
                if not(s in next_states): next_states.append(s)
            if (len(next_states) == 0): return False
            states = next_states
            i = i - 1
        return False

    def _debug_print_input_and_output(self, position, rule_states, morphological_state,
                                      input, output, this_input, this_output, invert):
        if (self.debug):
            #indent str
            padstring = ''
            for x in range(position): padstring = padstring + ' '

            print '%s%d  %s:%s \n' % (padstring, position, this_input, this_output),
            print '%s%d: Input:  ' % (padstring, position,),
            for i in input:
                print ' ' + i + ' ',
            if this_input:
                print '[' + this_input + ']...',
            print


            print '%s%d> Output: ' % (padstring, position,),
            for o in output:
                print ' ' + o + ' ',
            if this_output:
                print '<' + this_output + '>...',
            print


            # for (start, rule, fsa_states, required_truth_value) in rule_states:
            #    print '    {%d %s %s %s}' % (start, rule, fsa_states, required_truth_value)


            if False: # morphological_state:
                print '    possible input chars = %s' % invert.possible_next_characters(morphological_state)
                # print morphological_state


        # generate works by passing in the word at each position of the word
        # _generate is responsible for testing all the valid chars in the transition alphabet to see if
        # they are appropriate surface-underlying transitions.
        # it fails entirely if no valid transitions are found
        # if one is found, that is the one that is used.
        # essentially this is a possible word tree being expanded and failed on branches.
        # should return a list of matching words.
    def _generate(self, input_tokens, position, rule_states, morphological_state, input, output, result_str, result_words,
                  invert=False):
        # state is [ ( start, rule, states, required_truth_value ) ]
        # print 'morphological_state'
        # print morphological_state

        # if (self.debug) :
                # print '_generate'
                # print input_tokens, position, input, output, result_str, result_words
                # when at the last token or past it.
                
        if ((position >= len(input_tokens)) ): # and (not morphological_state)

            if (self.debug) : print '      AT END OF WORD'
                # FOR RECOGNIZER
                # this will yield some words twice, not all
                # also, recognizer is failing to put on the added information like "+genetive"
                
                # we are at the end, so check to see if a boundary char is in the possible set
                # and if so, add it and the remaining morphos
            if morphological_state:
                                                        
                    # print 'morpho'
                    possible_next_input_chars = invert.possible_next_characters(morphological_state)
                    # print 'possible_next_input_chars'
                    # print possible_next_input_chars
                    # change to boundary char, instead of hardcode
                    if ('0' in possible_next_input_chars) or ('#' in possible_next_input_chars):
                        if '0' in possible_next_input_chars: boundary = '0'
                        elif '#' in possible_next_input_chars: boundary = '#'
                        
                        # are at the end of the word, so we need to check and return those results
                        # that contain the boundary char.
                        
                        # should only be one potential boundary word '0'
                        # not correct, there can be more than one boundary word.
                        for next_morphological_state, new_words in invert.advance(morphological_state, boundary):
                                # yield result_str, result_words + new_words
                                # print new_words
                                # print next_morphological_state
                                # for o in self._generate(input_tokens, position + 1, [] , next_morphological_state,
                                #                    new_input, new_output, new_result_str,
                                #                    result_words + new_words,
                                #                    invert):
                                #       yield o
                                yield result_str, result_words + new_words

                    # yield result_str, result_words

            else:
                    # GENERATION CASE
                    # print 'no-morpho'
                    self._debug_print_input_and_output(position, rule_states, morphological_state, input, output, None, None, invert)
                    for (start, rule, fsa_states, required_truth_value) in rule_states:
                        if isinstance(rule, KimmoArrowRule):
                            truth_value = False # since it hasn't reached a halt state
                        elif isinstance(rule, KimmoFSARule):
                            truth_value = rule.contains_final(fsa_states)
        
                        if (required_truth_value != truth_value):
                            if (self.debug):
                                print '    BLOCKED by rule {%d %s %s}' % (start, rule, required_truth_value)
                                print fsa_states
                            break
                        else:
                            if 0: # (self.debug):
                                print '    passed rule {%d %s %s}' % (start, rule, required_truth_value)
        
                    else:
                        if (self.debug):
                            print '   SUCCESS!'
                        yield result_str, result_words
        else:
            if morphological_state: # recognizer; get the next possible surface chars that can result in
                                                        # the next char
                possible_next_input_chars = invert.possible_next_characters(morphological_state)
                # print 'possible_next_input_chars'
                # print possible_next_input_chars

            # foreach pair in our alphabet (includes per subset)
            # print self._pair_alphabet
            for pair_input, pair_output in self._pair_alphabet:

                if (pair_input != self.null() and morphological_state):
                    # if this pair does not apply, i.e. it is not in the possible
                    # chars from the lexicon
                    if not(pair_input in possible_next_input_chars):
                        continue

                if invert:
                        # check if the output of a transition is in the input string (input_tokens)
                    compare_token = pair_output
                else:
                    compare_token = pair_input

                if not(compare_token == self.null() or compare_token == input_tokens[position]): continue


                self._debug_print_input_and_output(position, rule_states, morphological_state,
                                                   input, output, pair_input, pair_output, invert)


                fail = None
                next_rule_states = []

                # first, evaluate currently activated rules
                # s is the current rule & its state
                rule_state_debug  = '   '
                for s in rule_states:

                    # advance one through each rule
                    (start, rule, fsa_state_set, required_truth_value) = s

                    current_state_str = '['
                    for x in fsa_state_set: current_state_str += str(x)
                    rule_state_debug += current_state_str

                    (next_fsa_state_set, contains_halt_state) = rule.right_advance(fsa_state_set, pair_input, pair_output,
                                                                                   self.subsets())

                    current_state_str = ''
                    for x in next_fsa_state_set: current_state_str += str(x)
                    if not current_state_str: current_state_str = '0 (FAIL)'
                    rule_state_debug += ('->' + current_state_str + '] ')

                    if (contains_halt_state == True and isinstance(rule, KimmoArrowRule)):
                        if (required_truth_value == False):
                            fail = s
                            break
                        else:
                            if (0): # (self.debug):
                                print '    passed rule {%d %s %s}' % (start, rule, required_truth_value)
                    elif (len(next_fsa_state_set) == 0):
                        # if it isn't true, then it will have to fail, bcs we are at
                        # the end of the state set.
                        # truth is evaluated by following the states until the end.
                        if (required_truth_value == True):
                            fail = s
                            break
                        else:
                            if (0): # (self.debug):
                                print '    passed rule {%d %s %s}' % (start, rule, required_truth_value)
                    else:
                        next_rule_states.append( (start, rule, next_fsa_state_set, required_truth_value) )

                if (self.debug) : print rule_state_debug

                if (fail):
                    if (self.debug):
                        print '    BLOCKED by rule %s' % (fail,)
                    continue


                # activate new KimmoArrowRules
                for rule in self.rules():
                    if not(isinstance(rule, KimmoArrowRule)): continue

                    required_truth_value = rule.matches(pair_input, pair_output, self.subsets())
                    if required_truth_value == None: continue
                    left_value = self._evaluate_rule_left_context(rule, input, output)
                    if (left_value == False):
                        if (required_truth_value == True):
                            fail = rule
                        continue


                    if (rule.rightFSA()):
                        if (self.debug):
                            print '    adding rule {%d %s %s}' % (position, rule, required_truth_value)
                        next_rule_states.append( (position, rule, [ rule.rightFSA().start() ], required_truth_value) )
                    else:
                        if (required_truth_value == False):
                            fail = rule
                            continue
                        else:
                            if (0): # (self.debug):
                                print '    passed rule ' + str(rule)

                # if did not fail, call recursively on next chars
                if (fail == None):
                    new_position      = position
                    new_input         = input  + [pair_input]
                    new_output        = output + [pair_output]
                    new_result_str    = result_str

                    if (pair_input  != self.null()):
                        if invert:
                            new_result_str = result_str + pair_input
                        else:
                            new_position = position + 1
                    if (pair_output != self.null()):
                        if invert:
                            new_position = position + 1
                        else:
                            new_result_str = result_str + pair_output


                    # morph state & generation steps through a char at a time.
                    # as it is, it only yields its morph if there is a valid next morphology
                    if morphological_state and pair_input != self.null():
                        for next_morphological_state, new_words in invert.advance(morphological_state, pair_input):
                            # print 'ENTERING LEXICON '
                            for o in self._generate(input_tokens, new_position, next_rule_states, next_morphological_state,
                                                    new_input, new_output, new_result_str,
                                                    result_words + new_words,
                                                    invert):
                                yield o
                    else:
                        for o in self._generate(input_tokens, new_position, next_rule_states, morphological_state,
                                                new_input, new_output, new_result_str, result_words, invert):
                            yield o
                else:
                    if (self.debug):
                        print '    BLOCKED by rule ' + str(fail)

    def _initial_rule_states(self):
        return [ (0, rule, [ rule.start() ], True) for rule in self.rules() if isinstance(rule, KimmoFSARule)]

    def generate(self, input_tokens):
        """Generator: yields output strings"""
        for o, w in self._generate(input_tokens, 0, self._initial_rule_states(), None, [], [], '', None):
            yield o


    def recognize(self, input_tokens, morphology=None):
        """Recognizer: yields (input_string, input_words)"""
        morphology_state = None
        output_words = None
        invert = True
        if morphology:
            morphology_state = morphology.initial_state()
            output_words = []
            invert = morphology


        if not morphology_state:
            print "Bad Morphological State, failing recognition"
            return              
        if (self.debug) : print 'recognize: ' + input_tokens
#        print output_words
        for o in self._generate(input_tokens, 0, self._initial_rule_states(), morphology_state, [], [], '',
                                output_words, invert):
            yield o     # yielding a list of possible words.


def _generate_test(s, input):
    resultlist = '%s -> ' % (input,),
    padlevel = len(input) + 4
    padstring = ''
    # for x in range(padlevel): padstring = padstring + ' '

    tmplist = '%s' % ('***NONE***'),
    for o in s.generate(input):
        tmplist = '%s%s\n' % (padstring,o,),
        resultlist = resultlist + tmplist
        padstring = ''
        for x in range(padlevel): padstring = padstring + ' '
        tmplist = '%s' % (''),
    resultlist = resultlist + tmplist

    return resultlist


def _recognize_test(s, input, morphology=None):
    resultlist =  '%s <- ' % (input,),
    padlevel = len(input) + 4
    padstring = ''
    # for x in range(padlevel): padstring = padstring + ' '

    tmplist = '%s' % ('***NONE***'),
    for o, w in s.recognize(input, morphology):
        if w:
            # print
            tmplist = '\n  %s   %s \n' % (o, w),
            resultlist = resultlist + tmplist
        else:
            tmplist = '%s%s \n' % (padstring,o,),
            resultlist = resultlist + tmplist

        padstring = ''
        for x in range(padlevel): padstring = padstring + ' '
        tmplist = '%s' % (''),
    # print
    # q = re.compile('(\{|\})')
    # q.sub("", resultstring[0])
    resultlist = resultlist + tmplist

    return resultlist

def read_kimmo_file(filename, gui=None):
    path = os.path.expanduser(filename)
    try:
        f = open(path, 'r')
    except IOError, e:
        path = find_corpus_file("kimmo", filename)
        try:
            f = open(path, 'r')
        except IOError, e:
            if gui:
                gui.guiError(str(e))
            else:
                print str(e)
            print "FAILURE"
            return ""
    print "Loaded:", path
    return f

# MAIN
# if __name__ == '__main__': KimmoGUI(None, None)
# if __name__ == '__main__': tkImageView("")
if __name__ == '__main__':
        filename_lex = ''
        filename_rul = ''
        filename_batch_test = ''
        recognize_string = ''
        generate_string = ''
        console_debug = 0
        
        for x in sys.argv:
                # if -r/g is defined (recognize or generate word)
                # or batch file is defined
                # run in commandline mode.
                
                if ".lex" in x: filename_lex = x
                elif ".rul" in x: filename_rul = x
                elif ".batch" in x: filename_batch_test = x
                elif x[0:3] == "-r:": recognize_string = x[3:len(x)]
                elif x[0:3] == "-g:": generate_string = x[3:len(x)]
                elif x == "debug": console_debug = 1
                

        print 'Tips:'
        print 'kimmo.cfg is loaded by default, so if you name your project that, '
        print "it will be loaded at startup\n"
        
        print 'For commandline operation:'
        print '         (for instance if you want to use a different editor)'
        print "To Recognize:"
        print " % python kimmo.py english.lex english.rul -r:cats"
        print "To Generate:"
        print " % python kimmo.py english.lex english.rul -g:cat+s"
        print "To Batch Test:"
        print " % python kimmo.py english.lex english.rul english.batch_test"
        print "With Debug and Tracing:"
        print " % python kimmo.py english.lex english.rul -r:cats debug\n"

        
        # print filename_lex    
        # print filename_rul
        # print filename_batch_test
        # print recognize_string
        # print generate_string
        
        
        if (recognize_string or generate_string or filename_batch_test) and filename_rul:
                kimmoinstance = KimmoControl("","",filename_lex,filename_rul,console_debug)
                
                # creation failed, stop
                if not kimmoinstance.ok :
                        print kimmoinstance.errors
                        sys.exit()

                
                if recognize_string:
                        recognize_results = kimmoinstance.recognize(recognize_string)
                        print recognize_results
                
                if generate_string:
                        generate_results = kimmoinstance.generate(generate_string)
                        print generate_results  # remember to format
                        
                if filename_batch_test:         # run a batch
                        kimmoinstance.batch(filename_batch_test)
        
        else:
                KimmoGUI(None, None)
        
        # constructor takes arguments:
        # KimmoControl(lexicon_string, rule_string, lexicon_filename, rule_filename, debug)
        # the constructor requires both lexicon and rules for recognition.
        # you can provide either the file contents as a string, or as a filename.
        # if only used to generate, only a rule file/string is necessary.
        
        # kimmoinstance = KimmoControl("","",'','./englex/english.rul',0)
        # kimmoinstance = KimmoControl("","",'kimmo.lex','kimmo.rul',0)
        # generate_results = kimmoinstance.generate("cat+s")
        # print generate_results
        
        # recognize_results = kimmoinstance.recognize("cats")
        # print recognize_results


########NEW FILE########
__FILENAME__ = langid
"""
Sam Huston 2007

This is a simulation of the article:
"Evaluation of a language identification system for mono- and multilingual text documents"
by Artemenko, O; Mandl, T; Shramko, M; Womser-Hacker, C.
presented at: Applied Computing 2006, 21st Annual ACM Symposium on Applied Computing; 23-27 April 2006

This implementation is intended for monolingual documents only,
however it is performed over a much larger range of languages.
Additionally three supervised methods of classification are explored:
Cosine distance, NaiveBayes, and Spearman-rho

"""

from nltk_contrib import classify
from nltk import detect
from nltk.corpus import udhr
import string

def run(classifier, training_data, gold_data):
    classifier.train(training_data)
    correct = 0
    for lang in gold_data:
        cls = classifier.get_class(gold_data[lang])
        if cls == lang:
            correct += 1
    print correct, "in", len(gold_data), "correct"

# features: character bigrams
fd = detect.feature({"char-bigrams" : lambda t: [string.join(t)[n:n+2] for n in range(len(t)-1)]})

training_data = udhr.langs(['English-Latin1', 'French_Francais-Latin1', 'Indonesian-Latin1', 'Zapoteco-Latin1'])
gold_data = {}
for lang in training_data:
    gold_data[lang] = training_data[lang][:50]
    training_data[lang] = training_data[lang][100:200]

print "Cosine classifier: ",
run(classify.Cosine(fd), training_data, gold_data)

print "Naivebayes classifier: ",
run(classify.NaiveBayes(fd), training_data, gold_data)

print "Spearman classifier: ",
run(classify.Spearman(fd), training_data, gold_data)

########NEW FILE########
__FILENAME__ = lex
"""
Ewan Klein, March 2007

Experimental module to provide support for implementing English morphology by
feature unification.

Main challenge is to find way of encoding morphosyntactic rules. Current idea is to let a concatenated form such as 'walk + s' be encoded as a dictionary C{'stem': 'walk', 'affix': 's'}. This allows the morpho-phonological representation to undergo unification in the normal way.
"""

from nltk.featstruct import *
import re

class Phon(dict):
        """
        A Phon object is just a stem and an affix.
        """
        def __init__(self, stem=None, affix=None):
                dict.__init__(self)
                self['stem'] = stem
                self['affix'] = affix
                
        def __repr__(self):
                return "%s + %s" % (self['stem'] , self['affix'] )

"""
>>> print Phon('a', 'b')
         a + b
"""

def phon_representer(dumper, data):
        """
        Output 'phon' values in 'stem + affix' notation.
        """
        return dumper.represent_scalar(u'!phon', u'%s + %s' % \
                                       (data['stem'], data['affix']))

yaml.add_representer(Phon, phon_representer)

"""
>>> print yaml.dump({'phon': Phon('a', 'b')})
        {phon: !phon 'a + b'}
"""

def normalize(s):
        """
        Turn input into non-Unicode strings without spaces.
        Return a Variable if input is of the form '?name'.
        """
        s = str(s.strip())
        patt = re.compile(r'^\?\w+$')
        if patt.match(s):
                name = s[1:]
                return Variable(name)
        return s

def phon_constructor(loader, node):
        """
        Recognize 'stem + affix' as Phon objects in YAML.
        """     
        value = loader.construct_scalar(node)
        stem, affix = [normalize(s) for s in value.split('+')]
        return Phon(stem, affix)

yaml.add_constructor(u'!phon', phon_constructor)

#following causes YAML to barf for some reason:
#pattern = re.compile(r'^(\?)?\w+\s*\+\s*(\?)?\w+$')
#yaml.add_implicit_resolver(u'phon', pattern)

"""
We have to specify the input using the '!phon' constructor.

>>> print yaml.load('''
...        form: !phon 'walk + s'
...        ''')
{'form': 'walk + s'}

Unifying a stem and a phonological output:

>>> f1 = yaml.load('''
...      form: !phon ?x + s
...      stem: ?x
...      ''')

>>> f2 = yaml.load('''
...      stem: walk
...      ''')

>>> f3 = unify(f1, f2)
>>> print f3
{'form': walk + s, 'stem': 'walk'}

In the next example, we follow B&B in using 'sym' as the name of the semantic constant in the lexical entry. We might want to have a semantic constructor like Phon so that we could write things like '\\x. (?sem x)'. Or perhaps not.

>>> lex_walk = yaml.load('''
...      sym: 'walk'
...      stem: 'walk'
...      ''')

>>> thirdsg = yaml.load('''
...      sym: ?x
...      sem: ?x
...      stem: ?y
...      phon: !phon ?x + s
...      ''')


>>> walks = unify(lex_walk, thirdsg)
>>> print walks
{'sem': 'walk', 'phon': walk + s, 'sym': 'walk', 'stem': 'walk'}
"""

def test():
    "Run unit tests on unification."
    import doctest
    doctest.testmod()

if __name__ == "__main__":
    test()

########NEW FILE########
__FILENAME__ = marshal
# Marshaling code, contributed by Tiago Tresoldi
# This saves/loads models to/from plain text files.
# Unlike Python's shelve and pickle utilities,
# this is useful for inspecting or tweaking the models.
# We may incorporate this as a marshal method in each model.

# TODO: describe each tagger marshal format in the epydocs?

from itertools import islice
import re

from nltk import tag
from nltk.corpus import brown

# marshal-classes

class MarshalDefault (tag.Default):
    _classname = "DefaultTagger"

    def marshal (self, filename):
        """
        Marshals (saves to a plain text file) the tagger model.
       
        @param filename: Name of the file to which save the model (will
                         be overwritten if it already exists).
        @type filename: C{string}
        """
        handler = file(filename, "w")
        handler.write(self._tag)
        handler.close()

    def unmarshal (self, filename):
        """
        Unmarshals (loads from a plain text file) the tagger model. For
        safety, this operation is intended to be performed only on
        newly created taggers (i.e., without any previous model).

        @param filename: Name of the file from which the model will
                         be read.
        @type filename: C{string}
        """
        handler = file(filename, "r")
        self._tag = handler.read()
        handler.close()

class MarshalUnigram (tag.Unigram):
    _classname = "UnigramTagger"

    def marshal (self, filename):
        """
        Marshals (saves to a plain text file) the tagger model.

        @param filename: Name of the file to which save the model (will
                         be overwritten if it already exists).
        @type filename: C{string}
        """
        handler = file(filename, "w")
        
        for text, tag in self._model.iteritems():
            handler.write("%s:%s\n" % (text, tag))

        handler.close()
        
    def unmarshal (self, filename):
        """
        Unmarshals (loads from a plain text file) the tagger model. For
        safety, this operation is intended to be performed only on
        newly created taggers (i.e., without any previous model).
       
        @param filename: Name of the file from which the model will
                         be read.
        @type filename: C{string}
        """
        handler = file(filename, "r")
        
        pattern = re.compile(r'^(.+):(.+?)$', re.UNICODE)
        for line in handler.readlines():
            m = re.match(pattern, line)
            text, tag = m.groups()
            self._model[text] = tag
        
        handler.close()

class MarshalAffix (tag.Affix):
    _classname = "AffixTagger"

    def marshal (self, filename):
        """
        Marshals (saves to a plain text file) the tagger model.
        
        @param filename: Name of the file to which save the model (will
                         be overwritten if it already exists).
        @type filename: C{string}
        """
        handler = file(filename, "w")
        
        handler.write("length %i\n" % self._length)
        handler.write("minlength %i\n" % self._minlength)
        
        for text, tag in self._model.iteritems():
            handler.write("%s:%s\n" % (text, tag))

        handler.close()

    def unmarshal (self, filename):
        """
        Unmarshals (loads from a plain text file) the tagger model. For
        safety, this operation is intended to be performed only on
        newly created taggers (i.e., without any previous model).
        
        @param filename: Name of the file from which the model will
                         be read.
        @type filename: C{string}
        """
        handler = file(filename, "r")
        
        lines = handler.readlines()
        # will fail if "length " and "minlength " are not present
        self._length = int(lines[0].split("length ")[1])
        self._minlength = int(lines[1].split("minlength ")[1])
        
        pattern = re.compile(r'^(.+):(.+?)$', re.UNICODE)
        for line in lines[2:]:
            m = re.match(pattern, line)
            text, tag = m.groups()
            self._model[text] = tag
        
        handler.close()

class MarshalNgram (tag.Ngram):
    _classname = "NgramTagger"

    def marshal (self, filename):
        """
        Marshals (saves to a plain text file) the tagger model.
        
        @param filename: Name of the file to which save the model (will
                         be overwritten if it already exists).
        @type filename: C{string}
        """
        handler = file(filename, "w")
        
        handler.write("n %i\n" % self._n)

        for entry in self._model:
            context, text, tag = entry[0], entry[1], self._model[entry]
            
            try:
                entry_str = "[%s]:%s:%s\n" % (":".join(context), text, tag)
                handler.write(entry_str)
            except TypeError:
                # None found in 'context', pass silently
                pass
        
        handler.close()

    def unmarshal (self, filename):
        """
        Unmarshals (loads from a plain text file) the tagger model. For
        safety, this operation is intended to be performed only on
        newly created taggers (i.e., without any previous model).
        
        @param filename: Name of the file from which the model will
                         be read.
        @type filename: C{string}
        """
        handler = file(filename, "r")
        
        lines = handler.readlines()
        # will fail if "n " is not present
        self._n = int(lines[0].split("n ")[1])
        
        
        pattern = re.compile(r'^\[(.+)\]:(.+):(.+?)$', re.UNICODE)
        
        # As the separator-char ":" can be used as a tag or as a text,
        # 'context_pattern' is built based on the context's size (self._n),
        # for example:
        #   self._n = 2 -> r'^(.+?)$', like 'tag1'
        #   self._n = 3 -> r'^(.+?):(.+?)$', like 'tag1:tag2'
        #   self._n = 4 -> r'^(.+?):(.+?):(.+?)$', like 'tag1:tag2:tag3'
        context_pattern_str = r'^(.+?)%s$' % ( r':(.+?)' * (self._n-2) )
        context_pattern = re.compile(context_pattern_str, re.UNICODE)
        
        for line in lines[1:]:
            m = re.match(pattern, line)
            context, text, tag = m.groups()
            
            c_m = re.match(context_pattern, context)
            key = (c_m.groups(), text)
            self._model[key] = tag
        
        handler.close()

def demo ():
    # load train corpus
    train_sents = brown.tagged('a')[:500]

    # create taggers
    tagger = MarshalNgram(3)

    #tagger.train(train_sents)
    #tagger.marshal("ngram.test")

    tagger.unmarshal("ngram.test")
    print tagger._model

########NEW FILE########
__FILENAME__ = marshalbrill
# Natural Language Toolkit: Brill Tagger
#
# Copyright (C) 2001-2005 NLTK Project
# Authors: Christopher Maloof <cjmaloof@gradient.cis.upenn.edu>
#          Edward Loper <edloper@gradient.cis.upenn.edu>
#          Steven Bird <sb@ldc.upenn.edu>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

"""
Brill's transformational rule-based tagger.
"""

from nltk.tag import TagI

import bisect        # for binary search through a subset of indices
import os            # for finding WSJ files
import random        # for shuffling WSJ files
import sys           # for getting command-line arguments
import re            # for performing regular expression matching

######################################################################
## The Brill Tagger
######################################################################

class Brill(TagI):
    """
    Brill's transformational rule-based tagger.  Brill taggers use an
    X{initial tagger} (such as L{tag.Default}) to assign an intial
    tag sequence to a text; and then apply an ordered list of
    transformational rules to correct the tags of individual tokens.
    These transformation rules are specified by the L{BrillRuleI}
    interface.

    Brill taggers can be created directly, from an initial tagger and
    a list of transformational rules; but more often, Brill taggers
    are created by learning rules from a training corpus, using either
    L{BrillTrainer} or L{FastBrillTrainer}.
    """
    
    # TODO: move into __init__() when all marshalling classes will be moved into
    # standard tree
    _classname = "BrillTagger"

    def __init__(self, initial_tagger, rules):
        """
        @param initial_tagger: The initial tagger
        @type initial_tagger: L{TagI}
        @param rules: An ordered list of transformation rules that
            should be used to correct the initial tagging.
        @type rules: C{list} of L{BrillRuleI}
        """
        self._initial_tagger = initial_tagger
        self._rules = rules

    def rules(self):
        return self._rules[:]

    def tag (self, tokens):
        # Inherit documentation from TagI
        
        # Run the initial tagger.
        tagged_tokens = list(self._initial_tagger.tag(tokens))

        # Create a dictionary that maps each tag to a list of the
        # indices of tokens that have that tag.
        tag_to_positions = {}
        for i, (token, tag) in enumerate(tagged_tokens):
            if tag not in tag_to_positions:
                tag_to_positions[tag] = set([i])
            else:
                tag_to_positions[tag].add(i)

        # Apply each rule, in order.  Only try to apply rules at
        # positions that have the desired original tag.
        for rule in self._rules:
            # Find the positions where it might apply
            positions = tag_to_positions.get(rule.original_tag(), [])
            # Apply the rule at those positions.
            changed = rule.apply_at(tagged_tokens, positions)
            # Update tag_to_positions with the positions of tags that
            # were modified.
            for i in changed:
                tag_to_positions[rule.original_tag()].remove(i)
                if rule.replacement_tag() not in tag_to_positions:
                    tag_to_positions[rule.replacement_tag()] = set([i])
                else:
                    tag_to_positions[rule.replacement_tag()].add(i)
        for t in tagged_tokens:
            yield t

    # marshal() and unmarshal() methods by Tiago Tresoldi <tresoldi@users.sf.net>            
    def marshal (self, filename):
        """
        Marshals (saves to a plain text file) the tagger model.
        
        @param filename: Name of the file to which save the model (will
                         be overwritten if it already exists).
        @type filename: C{string}
        """
        
        handler = file(filename, "w")

        for rule in self.rules():
            handler.write("%s\n" % rule)
            
        handler.close()
        
    def unmarshal (self, filename):
        """
        Unmarshals (loads from a plain text file) the tagger model. This
        operation will override any previously stored rules.

        @param filename: Name of the file from which the model will
                         be read.
        @type filename: C{string}
        """
        rule_a = re.compile(r"^(.+) -> (.+) if the (.+) of words i([+-]\d+)...i([+-]\d+) is '(.+)'$", re.UNICODE)
        rule_b = re.compile(r"^(.+) -> (.+) if the (.+) of the (.+) word is '(.+)'$", re.UNICODE)

        # erase any previous rules
        self._rules = []

        # load from file
        handler = file(filename, "r")
        lines = handler.readlines()
        handler.close()

        # remove '\n's, even though $ would catch them
        lines = [line[:-1] for line in lines]
        # remove empty lines
        lines = [line for line in lines if len(line)>0]

        # parse rules
        for rule in lines:
            match = re.match(rule_b, rule)
            if match:
               groups = list( match.groups() )
               if groups[3] == "preceding":
                  groups.pop(3)
                  groups.insert(3, "-1")
                  groups.insert(4, "-1")
               else:
                  groups.pop(3)
                  groups.insert(3, "1")
                  groups.insert(4, "1")
            else:
               match = re.match(rule_a, rule)
               groups = list( match.groups() )

            conditions = (int(groups[3]), int(groups[4]), groups[5])
            if groups[2] == "tag":
               r = ProximateTagsRule(groups[0], groups[1], conditions)
            else:
               r = ProximateWordsRule(groups[0], groups[1], conditions)
           
            self._rules.append(r)


######################################################################
## Brill Rules
######################################################################

class BrillRuleI(object):
    """
    An interface for tag transformations on a tagged corpus, as
    performed by brill taggers.  Each transformation finds all tokens
    in the corpus that are tagged with a specific X{original tag} and
    satisfy a specific X{condition}, and replaces their tags with a
    X{replacement tag}.  For any given transformation, the original
    tag, replacement tag, and condition are fixed.  Conditions may
    depend on the token under consideration, as well as any other
    tokens in the corpus.

    Brill rules must be comparable and hashable.
    """    
    def apply_to(self, tokens):
        """
        Apply this rule everywhere it applies in the corpus.  I.e.,
        for each token in the corpus that is tagged with this rule's
        original tag, and that satisfies this rule's condition, set
        its tag to be this rule's replacement tag.

        @param tokens: The tagged corpus
        @type tokens: C{list} of C{tuple}
        @return: The indices of tokens whose tags were changed by this
            rule.
        @rtype: C{list} of C{int}
        """
        return self.apply_at(tokens, range(len(tokens)))

    def apply_at(self, tokens, positions):
        """
        Apply this rule at every position in C{positions} where it
        applies to the corpus.  I.e., for each position M{p} in
        C{positions}, if C{tokens[M{p}]} is tagged with this rule's
        original tag, and satisfies this rule's condition, then set
        its tag to be this rule's replacement tag.

        @param tokens: The tagged corpus
        @type tokens: list of Token
        @type positions: C{list} of C{int}
        @param positions: The positions where the transformation is to
            be tried.
        @return: The indices of tokens whose tags were changed by this
            rule.
        @rtype: C{int}
        """
        assert False, "BrillRuleI is an abstract interface"

    def applies(self, tokens, index):
        """
        @return: True if the rule would change the tag of 
            C{tokens[index]}, False otherwise
        @rtype: Boolean

        @param tokens: A tagged corpus
        @type tokens: list of Token
        @param index: The index to check
        @type index: int
        """
        assert False, "BrillRuleI is an abstract interface"
        
    def original_tag(self):
        """
        @return: The tag which this C{BrillRuleI} may cause to be
        replaced.
        @rtype: any
        """
        assert False, "BrillRuleI is an abstract interface"

    def replacement_tag(self):
        """
        @return: the tag with which this C{BrillRuleI} may replace
        another tag.
        @rtype: any
        """
        assert False, "BrillRuleI is an abstract interface"

    # Rules must be comparable and hashable for the algorithm to work
    def __eq__(self):
        assert False, "Brill rules must be comparable"
    def __hash__(self):
        assert False, "Brill rules must be hashable"

class ProximateTokensRule(BrillRuleI):
    """
    An abstract base class for brill rules whose condition checks for
    the presence of tokens with given properties at given ranges of
    positions, relative to the token.

    Each subclass of proximate tokens brill rule defines a method
    M{extract_property}, which extracts a specific property from the
    the token, such as its text or tag.  Each instance is
    parameterized by a set of tuples, specifying ranges of positions
    and property values to check for in those ranges:
    
      - (M{start}, M{end}, M{value})

    The brill rule is then applicable to the M{n}th token iff:
    
      - The M{n}th token is tagged with the rule's original tag; and
      - For each (M{start}, M{end}, M{value}) triple:
        - The property value of at least one token between
          M{n+start} and M{n+end} (inclusive) is M{value}.

    For example, a proximate token brill template with M{start=end=-1}
    generates rules that check just the property of the preceding
    token.  Note that multiple properties may be included in a single
    rule; the rule applies if they all hold.
    """

    def __init__(self, original_tag, replacement_tag, *conditions):
        """

        Construct a new brill rule that changes a token's tag from
        C{original_tag} to C{replacement_tag} if all of the properties
        specified in C{conditions} hold.

        @type conditions: C{tuple} of C{(int, int, *)}
        @param conditions: A list of 3-tuples C{(start, end, value)},
            each of which specifies that the property of at least one
            token between M{n}+C{start} and M{n}+C{end} (inclusive) is
            C{value}.
        @raise ValueError: If C{start}>C{end} for any condition.
        """
        assert self.__class__ != ProximateTokensRule, \
               "ProximateTokensRule is an abstract base class"

        self._original = original_tag
        self._replacement = replacement_tag
        self._conditions = conditions
        for (s,e,v) in conditions:
            if s>e:
                raise ValueError('Condition %s has an invalid range' %
                                 ((s,e,v),))

    def extract_property(token): # [staticmethod]
        """
        Returns some property characterizing this token, such as its
        base lexical item or its tag.

        Each implentation of this method should correspond to an
        implementation of the method with the same name in a subclass
        of L{ProximateTokensTemplate}.

        @param token: The token
        @type token: Token
        @return: The property
        @rtype: any
        """
        assert False, "ProximateTokensRule is an abstract interface"
    extract_property = staticmethod(extract_property)

    def apply_at(self, tokens, positions):
        # Inherit docs from BrillRuleI

        # Find all locations where the rule is applicable
        change = []
        for i in positions:
            if self.applies(tokens, i):
                change.append(i)

        # Make the changes.  Note: this must be done in a separate
        # step from finding applicable locations, since we don't want
        # the rule to interact with itself.
        for i in change:
            (token, tag) = tokens[i]
            tokens[i] = (token, self._replacement)
        
        return change

    def applies(self, tokens, index):
        # Inherit docs from BrillRuleI
        
        # Does the given token have this rule's "original tag"?
        if tokens[index][1] != self._original:
            return False
        
        # Check to make sure that every condition holds.
        for (start, end, val) in self._conditions:
            # Find the (absolute) start and end indices.
            s = max(0, index+start)
            e = min(index+end+1, len(tokens))
            
            # Look for *any* token that satisfies the condition.
            for i in range(s, e):
                if self.extract_property(tokens[i]) == val:
                    break
            else:
                # No token satisfied the condition; return false.
                return False

        # Every condition checked out, so the rule is applicable.
        return True

    def original_tag(self):
        # Inherit docs from BrillRuleI
        return self._original

    def replacement_tag(self):
        # Inherit docs from BrillRuleI
        return self._replacement

    def __eq__(self, other):
        return (other != None and 
                other.__class__ == self.__class__ and 
                self._original == other._original and 
                self._replacement == other._replacement and 
                self._conditions == other._conditions)

    def __hash__(self):
        # Needs to include extract_property in order to distinguish subclasses
        # A nicer way would be welcome.
        return hash( (self._original, self._replacement, self._conditions,
                      self.extract_property.func_code) )

    def __repr__(self):
        conditions = ' and '.join(['%s in %d...%d' % (v,s,e)
                                   for (s,e,v) in self._conditions])
        return '<%s: %s->%s if %s>' % (self.__class__.__name__,
                                       self._original, self._replacement,
                                       conditions)

    def __str__(self):
        replacement = '%s -> %s' % (self._original,
                                              self._replacement)
        if len(self._conditions) == 0:
            conditions = ''
        else:
            conditions = ' if '+ ', and '.join([self._condition_to_str(c)
                                               for c in self._conditions])
        return replacement+conditions
    
    def _condition_to_str(self, condition):
        """
        Return a string representation of the given condition.
        This helper method is used by L{__str__}.
        """
        (start, end, value) = condition
        return ('the %s of %s is %r' %
                (self.PROPERTY_NAME, self._range_to_str(start, end), value))

    def _range_to_str(self, start, end):
        """
        Return a string representation for the given range.  This
        helper method is used by L{__str__}.
        """
        if start == end == 0:
            return 'this word'
        if start == end == -1:
            return 'the preceding word'
        elif start == end == 1:
            return 'the following word'
        elif start == end and start < 0:
            return 'word i-%d' % -start
        elif start == end and start > 0:
            return 'word i+%d' % start
        else:
            if start >= 0: start = '+%d' % start
            if end >= 0: end = '+%d' % end
            return 'words i%s...i%s' % (start, end)

class ProximateTagsRule(ProximateTokensRule):
    """
    A rule which examines the tags of nearby tokens.
    @see: superclass L{ProximateTokensRule} for details.
    @see: L{ProximateTagsTemplate}, which generates these rules.
    """
    PROPERTY_NAME = 'tag' # for printing.
    def extract_property(token): # [staticmethod]
        """@return: The given token's tag."""
        return token[1]
    extract_property = staticmethod(extract_property)

class ProximateWordsRule(ProximateTokensRule):
    """
    A rule which examines the base types of nearby tokens.
    @see: L{ProximateTokensRule} for details.
    @see: L{ProximateWordsTemplate}, which generates these rules.
    """
    PROPERTY_NAME = 'text' # for printing.
    def extract_property(token): # [staticmethod]
        """@return: The given token's text."""
        return token[0]
    extract_property = staticmethod(extract_property)

######################################################################
## Brill Templates
######################################################################

class BrillTemplateI(object):
    """
    An interface for generating lists of transformational rules that
    apply at given corpus positions.  C{BrillTemplateI} is used by
    C{Brill} training algorithms to generate candidate rules.
    """
    def __init__(self):
        raise AssertionError, "BrillTemplateI is an abstract interface"

    def applicable_rules(self, tokens, i, correctTag):
        """
        Return a list of the transformational rules that would correct
        the C{i}th subtoken's tag in the given token.  In particular,
        return a list of zero or more rules that would change
        C{tagged_tokens[i][1]} to C{correctTag}, if applied
        to C{token}.

        If the C{i}th subtoken already has the correct tag (i.e., if
        C{tagged_tokens[i][1]} == C{correctTag}), then
        C{applicable_rules} should return the empty list.
        
        @param tokens: The tagged tokens being tagged.
        @type tokens: C{list} of C{tuple}
        @param i: The index of the token whose tag should be corrected.
        @type i: C{int}
        @param correctTag: The correct tag for the C{i}th token.
        @type correctTag: (any)
        @rtype: C{list} of L{BrillRuleI}
        """
        raise AssertionError, "BrillTemplateI is an abstract interface"
    
    def get_neighborhood(self, token, index):
        """
        Returns the set of indices C{i} such that
        C{applicable_rules(token, index, ...)} depends on the value of
        the C{i}th subtoken of C{token}.

        This method is used by the \"fast\" Brill tagger trainer.

        @param token: The tokens being tagged.
        @type token: C{list} of C{tuple}
        @param index: The index whose neighborhood should be returned.
        @type index: C{int}
        @rtype: C{Set}
        """
        raise AssertionError, "BrillTemplateI is an abstract interface"
    
class ProximateTokensTemplate(BrillTemplateI):
    """
    An brill templates that generates a list of
    L{ProximateTokensRule}s that apply at a given corpus
    position.  In particular, each C{ProximateTokensTemplate} is
    parameterized by a proximate token brill rule class and a list of
    boundaries, and generates all rules that:
    
      - use the given brill rule class
      - use the given list of boundaries as the C{start} and C{end}
        points for their conditions
      - are applicable to the given token.
    """
    def __init__(self, rule_class, *boundaries):
        """
        Construct a template for generating proximate token brill
        rules.

        @type rule_class: C{class}
        @param rule_class: The proximate token brill rule class that
        should be used to generate new rules.  This class must be a
        subclass of L{ProximateTokensRule}.
        @type boundaries: C{tuple} of C{(int, int)}
        @param boundaries: A list of tuples C{(start, end)}, each of
            which specifies a range for which a condition should be
            created by each rule.
        @raise ValueError: If C{start}>C{end} for any boundary.
        """
        self._rule_class = rule_class
        self._boundaries = boundaries
        for (s,e) in boundaries:
            if s>e:
                raise ValueError('Boundary %s has an invalid range' %
                                 ((s,e),))

    def applicable_rules(self, tokens, index, correct_tag):
        if tokens[index][1] == correct_tag:
            return []

        # For each of this template's boundaries, Find the conditions
        # that are applicable for the given token.
        applicable_conditions = \
             [self._applicable_conditions(tokens, index, start, end)
              for (start, end) in self._boundaries]
            
        # Find all combinations of these applicable conditions.  E.g.,
        # if applicable_conditions=[[A,B], [C,D]], then this will
        # generate [[A,C], [A,D], [B,C], [B,D]].
        condition_combos = [[]]
        for conditions in applicable_conditions:
            condition_combos = [old_conditions+[new_condition]
                                for old_conditions in condition_combos
                                for new_condition in conditions]

        # Translate the condition sets into rules.
        return [self._rule_class(tokens[index][1], correct_tag, *conds)
                for conds in condition_combos]

    def _applicable_conditions(self, tokens, index, start, end):
        """
        @return: A set of all conditions for proximate token rules
        that are applicable to C{tokens[index]}, given boundaries of
        C{(start, end)}.  I.e., return a list of all tuples C{(start,
        end, M{value})}, such the property value of at least one token
        between M{index+start} and M{index+end} (inclusive) is
        M{value}.
        """
        conditions = set()
        s = max(0, index+start)
        e = min(index+end+1, len(tokens))
        for i in range(s, e):
            value = self._rule_class.extract_property(tokens[i])
            conditions.add( (start, end, value) )
        return conditions

    def get_neighborhood(self, tokens, index):
        # inherit docs from BrillTemplateI
        neighborhood = set([index])
        for (start, end) in self._boundaries:
            s = max(0, index+start)
            e = min(index+end+1, len(tokens))
            for i in range(s, e):
                neighborhood.add(i)

        return neighborhood

class SymmetricProximateTokensTemplate(BrillTemplateI):
    """
    Simulates two L{ProximateTokensTemplate}s which are symmetric
    across the location of the token.  For rules of the form \"If the
    M{n}th token is tagged C{A}, and any tag preceding B{or} following
    the M{n}th token by a distance between M{x} and M{y} is C{B}, and
    ... , then change the tag of the nth token from C{A} to C{C}.\"

    One C{ProximateTokensTemplate} is formed by passing in the
    same arguments given to this class's constructor: tuples
    representing intervals in which a tag may be found.  The other
    C{ProximateTokensTemplate} is constructed with the negative
    of all the arguments in reversed order.  For example, a
    C{SymmetricProximateTokensTemplate} using the pair (-2,-1) and the
    constructor C{ProximateTagsTemplate} generates the same rules as a
    C{ProximateTagsTemplate} using (-2,-1) plus a second
    C{ProximateTagsTemplate} using (1,2).

    This is useful because we typically don't want templates to
    specify only \"following\" or only \"preceding\"; we'd like our
    rules to be able to look in either direction.
    """
    def __init__(self, rule_class, *boundaries):
        """
        Construct a template for generating proximate token brill
        rules.
        
        @type rule_class: C{class}
        @param rule_class: The proximate token brill rule class that
        should be used to generate new rules.  This class must be a
        subclass of L{ProximateTokensRule}.
        @type boundaries: C{tuple} of C{(int, int)}
        @param boundaries: A list of tuples C{(start, end)}, each of
            which specifies a range for which a condition should be
            created by each rule.
        @raise ValueError: If C{start}>C{end} for any boundary.
        """
        self._ptt1 = ProximateTokensTemplate(rule_class, *boundaries)
        reversed = [(-e,-s) for (s,e) in boundaries]
        self._ptt2 = ProximateTokensTemplate(rule_class, *reversed)

    # Generates lists of a subtype of ProximateTokensRule.
    def applicable_rules(self, tokens, index, correctTag):
        """
        See L{BrillTemplateI} for full specifications.

        @rtype: list of ProximateTokensRule
        """
        return (self._ptt1.applicable_rules(tokens, index, correctTag) +
                self._ptt2.applicable_rules(tokens, index, correctTag))

    def get_neighborhood(self, tokens, index):
        # inherit docs from BrillTemplateI
        n1 = self._ptt1.get_neighborhood(tokens, index)
        n2 = self._ptt2.get_neighborhood(tokens, index)
        return n1.union(n2)

######################################################################
## Brill Tagger Trainer
######################################################################

class BrillTrainer(object):
    """
    A trainer for brill taggers.
    """
    def __init__(self, initial_tagger, templates, trace=0):
        self._initial_tagger = initial_tagger
        self._templates = templates
        self._trace = trace

    #////////////////////////////////////////////////////////////
    # Training
    #////////////////////////////////////////////////////////////

    def train(self, train_tokens, max_rules=200, min_score=2):
        """
        Trains the Brill tagger on the corpus C{train_token},
        producing at most C{max_rules} transformations, each of which
        reduces the net number of errors in the corpus by at least
        C{min_score}.
        
        @type train_tokens: C{list} of L{tuple}
        @param train_tokens: The corpus of tagged tokens
        @type max_rules: C{int}
        @param max_rules: The maximum number of transformations to be created
        @type min_score: C{int}
        @param min_score: The minimum acceptable net error reduction
            that each transformation must produce in the corpus.
        """
        if self._trace > 0: print ("Training Brill tagger on %d tokens..." %
                                   len(train_tokens))

        # Create a new copy of the training token, and run the initial
        # tagger on this.  We will progressively update this test
        # token to look more like the training token.

        test_tokens = list(self._initial_tagger.tag(t[0] for t in train_tokens))
        
        if self._trace > 2: self._trace_header()
            
        # Look for useful rules.
        rules = []
        try:
            while len(rules) < max_rules:
                old_tags = [t[1] for t in test_tokens]
                (rule, score, fixscore) = self._best_rule(test_tokens,
                                                          train_tokens)
                if rule is None or score < min_score:
                    if self._trace > 1:
                        print 'Insufficient improvement; stopping'
                    break
                else:
                    # Add the rule to our list of rules.
                    rules.append(rule)
                    # Use the rules to update the test token.
                    k = rule.apply_to(test_tokens)
                    # Display trace output.
                    if self._trace > 1:
                        self._trace_rule(rule, score, fixscore, len(k))
        # The user can also cancel training manually:
        except KeyboardInterrupt: pass

        # Create and return a tagger from the rules we found.
        return Brill(self._initial_tagger, rules)

    #////////////////////////////////////////////////////////////
    # Finding the best rule
    #////////////////////////////////////////////////////////////

    # Finds the rule that makes the biggest net improvement in the corpus.
    # Returns a (rule, score) pair.
    def _best_rule(self, test_tokens, train_tokens):

        # Create a dictionary mapping from each tag to a list of the
        # indices that have that tag in both test_tokens and
        # train_tokens (i.e., where it is correctly tagged).
        correct_indices = {}
        for i in range(len(test_tokens)):
            if test_tokens[i][1] == train_tokens[i][1]:
                tag = test_tokens[i][1]
                correct_indices.setdefault(tag, []).append(i)

        # Find all the rules that correct at least one token's tag,
        # and the number of tags that each rule corrects (in
        # descending order of number of tags corrected).
        rules = self._find_rules(test_tokens, train_tokens)

        # Keep track of the current best rule, and its score.
        best_rule, best_score, best_fixscore = None, 0, 0

        # Consider each rule, in descending order of fixscore (the
        # number of tags that the rule corrects, not including the
        # number that it breaks).
        for (rule, fixscore) in rules:
            # The actual score must be <= fixscore; so if best_score
            # is bigger than fixscore, then we already have the best
            # rule.
            if best_score >= fixscore:
                return best_rule, best_score, best_fixscore

            # Calculate the actual score, by decrementing fixscore
            # once for each tag that the rule changes to an incorrect
            # value.
            score = fixscore
            if correct_indices.has_key(rule.original_tag()):
                for i in correct_indices[rule.original_tag()]:
                    if rule.applies(test_tokens, i):
                        score -= 1
                        # If the score goes below best_score, then we know
                        # that this isn't the best rule; so move on:
                        if score <= best_score: break

            #print '%5d %5d %s' % (fixscore, score, rule)

            # If the actual score is better than the best score, then
            # update best_score and best_rule.
            if score > best_score:
                best_rule, best_score, best_fixscore = rule, score, fixscore

        # Return the best rule, and its score.
        return best_rule, best_score, best_fixscore

    def _find_rules(self, test_tokens, train_tokens):
        """
        Find all rules that correct at least one token's tag in
        C{test_tokens}.

        @return: A list of tuples C{(rule, fixscore)}, where C{rule}
            is a brill rule and C{fixscore} is the number of tokens
            whose tag the rule corrects.  Note that C{fixscore} does
            I{not} include the number of tokens whose tags are changed
            to incorrect values.        
        """

        # Create a list of all indices that are incorrectly tagged.
        error_indices = [i for i in range(len(test_tokens))
                         if (test_tokens[i][1] !=
                             train_tokens[i][1])]

        # Create a dictionary mapping from rules to their positive-only
        # scores.
        rule_score_dict = {}
        for i in range(len(test_tokens)):
            rules = self._find_rules_at(test_tokens, train_tokens, i)
            for rule in rules:
                rule_score_dict[rule] = rule_score_dict.get(rule,0) + 1

        # Convert the dictionary into a list of (rule, score) tuples,
        # sorted in descending order of score.
        rule_score_items = rule_score_dict.items()
        temp = [(-score, rule) for (rule, score) in rule_score_items]
        temp.sort()
        return [(rule, -negscore) for (negscore, rule) in temp]

    def _find_rules_at(self, test_tokens, train_tokens, i):
        """
        @rtype: C{Set}
        @return: the set of all rules (based on the templates) that
        correct token C{i}'s tag in C{test_tokens}.
        """
        
        applicable_rules = set()
        if test_tokens[i][1] != train_tokens[i][1]:
            correct_tag = train_tokens[i][1]
            for template in self._templates:
                new_rules = template.applicable_rules(test_tokens, i,
                                                      correct_tag)
                applicable_rules.update(new_rules)
                
        return applicable_rules

    #////////////////////////////////////////////////////////////
    # Tracing
    #////////////////////////////////////////////////////////////

    def _trace_header(self):
        print """
           B      |     
   S   F   r   O  |        Score = Fixed - Broken
   c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct
   o   x   k   h  |  u     Broken = num tags changed correct -> incorrect
   r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect
   e   d   n   r  |  e
------------------+-------------------------------------------------------
        """.rstrip()

    def _trace_rule(self, rule, score, fixscore, numchanges):
        if self._trace > 2:
            print ('%4d%4d%4d%4d ' % (score, fixscore, fixscore-score,
                                      numchanges-fixscore*2+score)), '|',
        print rule

######################################################################
## Fast Brill Tagger Trainer
######################################################################

class FastBrillTrainer(object):
    """
    A faster trainer for brill taggers.
    """
    def __init__(self, initial_tagger, templates, trace=0):
        self._initial_tagger = initial_tagger
        self._templates = templates
        self._trace = trace

    #////////////////////////////////////////////////////////////
    # Training
    #////////////////////////////////////////////////////////////

    def train(self, train_tokens, max_rules=200, min_score=2):

        # If TESTING is true, extra computation is done to determine whether
        # each "best" rule actually reduces net error by the score it received.
        TESTING = False
        
        # Basic idea: Keep track of the rules that apply at each position.
        # And keep track of the positions to which each rule applies.

        # The set of somewhere-useful rules that apply at each position
        rulesByPosition = []
        for i in range(len(train_tokens)):
            rulesByPosition.append(set())

        # Mapping somewhere-useful rules to the positions where they apply.
        # Then maps each position to the score change the rule generates there.
        # (always -1, 0, or 1)
        positionsByRule = {}

        # Map scores to sets of rules known to achieve *at most* that score.
        rulesByScore = {0:{}}
        # Conversely, map somewhere-useful rules to their minimal scores.
        ruleScores = {}

        tagIndices = {}   # Lists of indices, mapped to by their tags

        # Maps rules to the first index in the corpus where it may not be known
        # whether the rule applies.  (Rules can't be chosen for inclusion
        # unless this value = len(corpus).  But most rules are bad, and
        # we won't need to check the whole corpus to know that.)
        # Some indices past this may actually have been checked; it just isn't
        # guaranteed.
        firstUnknownIndex = {}

        # Make entries in the rule-mapping dictionaries.
        # Should be called before _updateRuleApplies.
        def _initRule (rule):
            positionsByRule[rule] = {}
            rulesByScore[0][rule] = None
            ruleScores[rule] = 0
            firstUnknownIndex[rule] = 0

        # Takes a somewhere-useful rule which applies at index i;
        # Updates all rule data to reflect that the rule so applies.
        def _updateRuleApplies (rule, i):

            # If the rule is already known to apply here, ignore.
            # (This only happens if the position's tag hasn't changed.)
            if positionsByRule[rule].has_key(i):
                return

            if rule.replacement_tag() == train_tokens[i][1]:
                positionsByRule[rule][i] = 1
            elif rule.original_tag() == train_tokens[i][1]:
                positionsByRule[rule][i] = -1
            else: # was wrong, remains wrong
                positionsByRule[rule][i] = 0

            # Update rules in the other dictionaries
            del rulesByScore[ruleScores[rule]][rule]
            ruleScores[rule] += positionsByRule[rule][i]
            if not rulesByScore.has_key(ruleScores[rule]):
                rulesByScore[ruleScores[rule]] = {}
            rulesByScore[ruleScores[rule]][rule] = None
            rulesByPosition[i].add(rule)

        # Takes a rule which no longer applies at index i;
        # Updates all rule data to reflect that the rule doesn't apply.
        def _updateRuleNotApplies (rule, i):
            del rulesByScore[ruleScores[rule]][rule]
            ruleScores[rule] -= positionsByRule[rule][i]
            if not rulesByScore.has_key(ruleScores[rule]):
                rulesByScore[ruleScores[rule]] = {}
            rulesByScore[ruleScores[rule]][rule] = None

            del positionsByRule[rule][i]
            rulesByPosition[i].remove(rule)
            # Optional addition: if the rule now applies nowhere, delete
            # all its dictionary entries.

        tagged_tokens = list(self._initial_tagger.tag(t[0] for t in train_tokens))

        # First sort the corpus by tag, and also note where the errors are.
        errorIndices = []  # only used in initialization
        for i in range(len(tagged_tokens)):
            tag = tagged_tokens[i][1]
            if tag != train_tokens[i][1]:
                errorIndices.append(i)
            if not tagIndices.has_key(tag):
                tagIndices[tag] = []
            tagIndices[tag].append(i)

        print "Finding useful rules..."
        # Collect all rules that fix any errors, with their positive scores.
        for i in errorIndices:
            for template in self._templates:
                # Find the templated rules that could fix the error.
                for rule in template.applicable_rules(tagged_tokens, i,
                                                    train_tokens[i][1]):
                    if not positionsByRule.has_key(rule):
                        _initRule(rule)
                    _updateRuleApplies(rule, i)

        print "Done initializing %i useful rules." %len(positionsByRule)

        if TESTING:
            after = -1 # bug-check only

        # Each iteration through the loop tries a new maxScore.
        maxScore = max(rulesByScore.keys())
        rules = []
        while len(rules) < max_rules and maxScore >= min_score:

            # Find the next best rule.  This is done by repeatedly taking a rule with
            # the highest score and stepping through the corpus to see where it
            # applies.  When it makes an error (decreasing its score) it's bumped
            # down, and we try a new rule with the highest score.
            # When we find a rule which has the highest score AND which has been
            # tested against the entire corpus, we can conclude that it's the next
            # best rule.

            bestRule = None
            bestRules = rulesByScore[maxScore].keys()

            for rule in bestRules:
                # Find the first relevant index at or following the first
                # unknown index.  (Only check indices with the right tag.)
                ti = bisect.bisect_left(tagIndices[rule.original_tag()],
                                        firstUnknownIndex[rule])
                for nextIndex in tagIndices[rule.original_tag()][ti:]:
                    if rule.applies(tagged_tokens, nextIndex):
                        _updateRuleApplies(rule, nextIndex)
                        if ruleScores[rule] < maxScore:
                            firstUnknownIndex[rule] = nextIndex+1
                            break  # the _update demoted the rule

                # If we checked all remaining indices and found no more errors:
                if ruleScores[rule] == maxScore:
                    firstUnknownIndex[rule] = len(tagged_tokens) # i.e., we checked them all
                    print "%i) %s (score: %i)" %(len(rules)+1, rule, maxScore)
                    bestRule = rule
                    break
                
            if bestRule == None: # all rules dropped below maxScore
                del rulesByScore[maxScore]
                maxScore = max(rulesByScore.keys())
                continue  # with next-best rules

            # bug-check only
            if TESTING:
                before = len(_errorPositions(tagged_tokens, train_tokens))
                print "There are %i errors before applying this rule." %before
                assert after == -1 or before == after, \
                        "after=%i but before=%i" %(after,before)
                        
            print "Applying best rule at %i locations..." \
                    %len(positionsByRule[bestRule].keys())
            
            # If we reach this point, we've found a new best rule.
            # Apply the rule at the relevant sites.
            # (apply_at is a little inefficient here, since we know the rule applies
            #  and don't actually need to test it again.)
            rules.append(bestRule)
            bestRule.apply_at(tagged_tokens, positionsByRule[bestRule].keys())

            # Update the tag index accordingly.
            for i in positionsByRule[bestRule].keys(): # where it applied
                # Update positions of tags
                # First, find and delete the index for i from the old tag.
                oldIndex = bisect.bisect_left(tagIndices[bestRule.original_tag()], i)
                del tagIndices[bestRule.original_tag()][oldIndex]

                # Then, insert i into the index list of the new tag.
                if not tagIndices.has_key(bestRule.replacement_tag()):
                    tagIndices[bestRule.replacement_tag()] = []
                newIndex = bisect.bisect_left(tagIndices[bestRule.replacement_tag()], i)
                tagIndices[bestRule.replacement_tag()].insert(newIndex, i)

            # This part is tricky.
            # We need to know which sites might now require new rules -- that
            # is, which sites are close enough to the changed site so that
            # a template might now generate different rules for it.
            # Only the templates can know this.
            #
            # If a template now generates a different set of rules, we have
            # to update our indices to reflect that.
            print "Updating neighborhoods of changed sites.\n" 

            # First, collect all the indices that might get new rules.
            neighbors = set()
            for i in positionsByRule[bestRule].keys(): # sites changed
                for template in self._templates:
                    neighbors.update(template.get_neighborhood(tagged_tokens, i))

            # Then collect the new set of rules for each such index.
            c = d = e = 0
            for i in neighbors:
                siteRules = set()
                for template in self._templates:
                    # Get a set of the rules that the template now generates
                    siteRules.update(set(template.applicable_rules(
                                        tagged_tokens, i, train_tokens[i][1])))

                # Update rules no longer generated here by any template
                for obsolete in rulesByPosition[i] - siteRules:
                    c += 1
                    _updateRuleNotApplies(obsolete, i)

                # Update rules only now generated by this template
                for newRule in siteRules - rulesByPosition[i]:
                    d += 1
                    if not positionsByRule.has_key(newRule):
                        e += 1
                        _initRule(newRule) # make a new rule w/score=0
                    _updateRuleApplies(newRule, i) # increment score, etc.

            if TESTING:
                after = before - maxScore
            print "%i obsolete rule applications, %i new ones, " %(c,d)+ \
                    "using %i previously-unseen rules." %e        

            maxScore = max(rulesByScore.keys()) # may have gone up

        
        if self._trace > 0: print ("Training Brill tagger on %d tokens..." %
                                   len(train_tokens))
        
        # Maintain a list of the rules that apply at each position.
        rules_by_position = [{} for tok in train_tokens]

        # Create and return a tagger from the rules we found.
        return Brill(self._initial_tagger, rules)

######################################################################
## Testing
######################################################################

def _errorPositions (train_tokens, tokens):
    return [i for i in range(len(tokens)) 
            if tokens[i][1] !=
            train_tokens[i][1] ]

# returns a list of errors in string format
def errorList (train_tokens, tokens, radius=2):
    """
    Returns a list of human-readable strings indicating the errors in the
    given tagging of the corpus.

    @param train_tokens: The correct tagging of the corpus
    @type train_tokens: C{list} of C{tuple}
    @param tokens: The tagged corpus
    @type tokens: C{list} of C{tuple}
    @param radius: How many tokens on either side of a wrongly-tagged token
        to include in the error string.  For example, if C{radius}=2, each error
        string will show the incorrect token plus two tokens on either side.
    @type radius: int
    """
    errors = []
    indices = _errorPositions(train_tokens, tokens)
    tokenLen = len(tokens)
    for i in indices:
        ei = tokens[i][1].rjust(3) + " -> " \
             + train_tokens[i][1].rjust(3) + ":  "
        for j in range( max(i-radius, 0), min(i+radius+1, tokenLen) ):
            if tokens[j][0] == tokens[j][1]:
                s = tokens[j][0] # don't print punctuation tags
            else:
                s = tokens[j][0] + "/" + tokens[j][1]
                
            if j == i:
                ei += "**"+s+"** "
            else:
                ei += s + " "
        errors.append(ei)

    return errors

#####################################################################################
# Demonstration
#####################################################################################

def demo(num_sents=100, max_rules=200, min_score=2, error_output = "errors.out",
         rule_output="rules.out", randomize=False, train=.8, trace=3):
    """
    Brill Tagger Demonstration

    @param num_sents: how many sentences of training and testing data to use
    @type num_sents: L{int}
    @param max_rules: maximum number of rule instances to create
    @type max_rules: L{int}
    @param min_score: the minimum score for a rule in order for it to be considered
    @type min_score: L{int}
    @param error_output: the file where errors will be saved
    @type error_output: L{string}
    @param rule_output: the file where rules will be saved
    @type rule_output: L{string}
    @param randomize: whether the training data should be a random subset of the corpus
    @type randomize: L{boolean}
    @param train: the fraction of the the corpus to be used for training (1=all)
    @type train: L{float}
    @param trace: the level of diagnostic tracing output to produce (0-3)
    @type trace: L{int}
    """

    from nltk.corpus import treebank
    from nltk import tag
    from nltk.tag import brill

    NN_CD_tagger = tag.Regexp([(r'^-?[0-9]+(.[0-9]+)?$', 'CD'), (r'.*', 'NN')])

    # train is the proportion of data used in training; the rest is reserved
    # for testing.

    print "Loading tagged data..."
    sents = []
    for item in treebank.items:
        sents.extend(treebank.tagged(item))
    if randomize:
        random.seed(len(sents))
        random.shuffle(sents)

    tagged_data = [t for s in sents[:num_sents] for t in s]
    cutoff = int(len(tagged_data)*train)

    training_data = tagged_data[:cutoff]
    gold_data = tagged_data[cutoff:]

    testing_data = [t[0] for t in gold_data]

    # Unigram tagger

    print "Training unigram tagger:",
    u = tag.Unigram(backoff=NN_CD_tagger)

    # NB training and testing are required to use a list-of-lists structure,
    # so we wrap the flattened corpus data with the extra list structure.
    u.train([training_data])
    print("[accuracy: %f]" % tag.accuracy(u, [gold_data]))

    # Brill tagger

    templates = [
        brill.SymmetricProximateTokensTemplate(brill.ProximateTagsRule, (1,1)),
        brill.SymmetricProximateTokensTemplate(brill.ProximateTagsRule, (2,2)),
        brill.SymmetricProximateTokensTemplate(brill.ProximateTagsRule, (1,2)),
        brill.SymmetricProximateTokensTemplate(brill.ProximateTagsRule, (1,3)),
        brill.SymmetricProximateTokensTemplate(brill.ProximateWordsRule, (1,1)),
        brill.SymmetricProximateTokensTemplate(brill.ProximateWordsRule, (2,2)),
        brill.SymmetricProximateTokensTemplate(brill.ProximateWordsRule, (1,2)),
        brill.SymmetricProximateTokensTemplate(brill.ProximateWordsRule, (1,3)),
        brill.ProximateTokensTemplate(brill.ProximateTagsRule, (-1, -1), (1,1)),
        brill.ProximateTokensTemplate(brill.ProximateWordsRule, (-1, -1), (1,1)),
        ]

    #trainer = brill.FastBrillTrainer(u, templates, trace)
    trainer = brill.BrillTrainer(u, templates, trace)
    b = trainer.train(training_data, max_rules, min_score)

    print
    print("Brill accuracy: %f" % tag.accuracy(b, [gold_data]))

    print("\nRules: ")
    printRules = file(rule_output, 'w')
    for rule in b.rules():
        print(str(rule))
        printRules.write(str(rule)+"\n\n")

    testing_data = list(b.tag(testing_data))
    el = errorList(gold_data, testing_data)
    errorFile = file(error_output, 'w')

    for e in el:
        errorFile.write(e+"\n\n")
    errorFile.close()
    print "Done; rules and errors saved to %s and %s." % (rule_output, error_output)

if __name__ == '__main__':
    demo()

########NEW FILE########
__FILENAME__ = paradigm
# Natural Language Toolkit: Paradigm Visualisation
#
# Copyright (C) 2005 University of Melbourne
# Author: Will Hardy
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

# Front end to a Python implementation of David
# Penton's paradigm visualisation model.
# Author: 
#
# Run: To run, first load a paradigm using
#      >>> a = paradigm('paradigm.xml')
#      And run the system to produce output
#      >>> a.show('table(one, two, three)')
#
#      Other methods:
#      demo()                   # a quick demonstration
#      a.setFormat('html')      # output is formatted as HTML
#      a.setFormat('text')      # output is formatted as HTML
#      a.setOutput('filename')  # output is sent to filename
#      a.setOutput('term')      # output is sent to terminal

from xml.dom.ext.reader import Sax2
from paradigmquery import ParadigmQuery
import re, os

class Paradigm(object):
    """
    Paradigm visualisation class

    *Usage*
    
    Simple usage of the system would be:
      >>> from paradigm import Paradigm
      >>> p = Paradigm('german.xml')
      >>> p.show('table(case, gender/number, content)')
    
    Here, a table is generated in HTML format and sent to the file ``output.html``.
    The table can be viewed in a browser, and is updated for every new query. 
    
    A more advanced usage of the system is show below.
    The user simply creates a paradigm p, changes the output format and location, 
    and calls a dedicated prompt to enter the query:
      >>> from paradigm import Paradigm
      >>> p = Paradigm('german.xml')
      >>> p.setFormat('html')
      >>> p.setOutput('test.html')
      >>> p.setCSS('simple.css')
      >>> p.prompt()
      > table(case, gender/number, content)
    
    Please note, however, that plain text tables have not yet been implemented.
    """

    def __init__(self, p_filename):
        """
        Load the given paradigm
        p_filename is a string representing the filename of a paradigm xml file
        """
        # Store input paradigm filename
        self.loadParadigm(p_filename)
        # set default values (text output, to terminal)
        self.format = "html"
        self.output = "output.html"
        self.css = "simple.css"

    def prompt(self):
        """
        Changes to a dedicated prompt
        Type 'exit' or 'quit' to exit
        """
        s = ""
        while s != "exit":
            s = "exit"
            try: s = raw_input(">")
            except EOFError:
                print s
            if s == "exit":
                return
            if s == "quit":
                return
            if s:
                while s[-1] in "!.": s = s[:-1]
                self.show(s)

    def show(self, p_string):
        """
        Process and display the given query
        """

        try:  
          # parse the query
          parse = ParadigmQuery(p_string)
        except:
          print "Could not parse query."
          return

        try:  
          # Fetch the parsed tree and make presentation
          result = Sentence(self, parse.getTree())
          # Check that a presentation actually exists
          if result == None:
            raise Error
        except:
            print "Sorry, no result can be returned"
            return

        try:  
        # Print HTML output if format is set, otherwise plain text
          if self.format == "html":
            output = '<html>\n'
            # Include CSS if we need to
            if self.css <> None:
                output += '<link rel="stylesheet" href="' 
                output += self.css
                output += '" type="text/css" media="screen" />\n'
            output += '<body>'
            output += "<table cellspacing=\"0\" cellpadding=\"0\">"
            output += result.getHTML()
            output += "</table>\n"
            output += '</body></html>\n'
          else:
            output = result.getText()
        except:
            output = None
            print "--no output--"
            return

        # Print to terminal if output is set, otherwise to file
        if self.output == "term":
            print output
        else:
            print "Output written to file:", self.output
            f = open(self.output, 'w')
            f.write(output)

        # Return happily
        return

    def setFormat(self, p_string=None):
        """
        Set the output format: "html" or "text"
        """
        # Default value
        if p_string == None:
            p_string = "text"
        # set to html if requested, otherwise text
        if p_string == "html":
            self.format = "html"
        elif p_string == "text":
            self.format = "text"
        else:
            print "Unknown format:", p_string
            print "Valid formats are: text, html"
            print "Setting format = text"
            self.format = "text"

    def setCSS(self, p_string=None):
        """
        Set the file location for a Cascading Stylesheet: None or filename
        This allows for simple formatting
        """
        if p_string <> None:
            print "Using CSS file:", p_string
        self.output = p_string

    def setOutput(self, p_string=None):
        """
        Set the output location: "term" or filename
        """
        # Default
        if p_string == None:
            p_string = "term"
        # set to term if requested, otherwise filename
        if p_string == "term":
            print "Directing output to terminal"
        else:
            print "Directing output to file:", p_string
        self.output = p_string


    def loadParadigm(self, p_filename ):
        """
        Load the given paradigm (XML file)
        Attributes are stored in self.attributes
        Data are stored in self.data
    
        They can be accessed as follows:
        self.attributes['gender']   # list of genders
        self.data[6]['gender']      # gender for the sixth data object
        self.data[6]['content']     # content for the sixth data object
        """

        from nltk.corpus import find_corpus_file
        basedir = get_basedir()

        # Look for the file
        try_filename = find_corpus_file("paradigms", p_filename)
        try:
            f = open(try_filename)
            p_filename = try_filename
        except IOError:
            print "Cannot find file"
            return None
        f.close()

        # These variables will be set by this method
        self.attributes = {}  # A new dictionary
        self.data = []        # A new list

        # XML admin: create Reader object, parse document
        reader = Sax2.Reader()
        doc = reader.fromStream(p_filename)

        # Cycle through the given attributes and add them to self.attributes
        # for <name> in <attributes>
        attributes = doc.getElementsByTagName('attributes')[0]
        for name in attributes.getElementsByTagName('name'):

            # Setup a list of attribute values
            tmp_list = []

            # for each value under name, store in list
            for value in name.getElementsByTagName('value'):
                tmp_list.append(value.getAttribute('value'))

            # Store list of values in dictionary
            self.attributes[name.getAttribute('name')] = tmp_list


        # Cycle through data objects and add them to self.data
        # for <form> in <paradigm>
        forms = doc.getElementsByTagName('paradigm')[0]
        for form in forms.getElementsByTagName('form'):
            # Initialise a temporary dictionary
            tmp_dict = {}
            for value in form.getElementsByTagName('attribute'):
                tmp_dict[value.getAttribute('name')] = value.getAttribute('value')
            # Add the new dictionary to the data list
            self.data.append(tmp_dict)

        # Talk to the user
        print "Paradigm information successfully loaded from file:", p_filename
        # State the number and print out a list of attributes
        print " "*4 + str(len(self.attributes)) + " attributes imported:",
        for att in self.attributes:
            print att,
        print
        # State the number of paradigm objects imported
        print " "*4 + str(len(self.data)) + " paradigm objects imported."

        return

class Sentence(object):
    """
    Manages any operation
    Passes request onto other handlers if necessary
    """

    def __init__(self, p_paradigm, p_tree):
        """
        p_paradigm is the given paradigm (attributes and data)
        p_tree is the query tree
        """
        # store parameters
        self.paradigm = p_paradigm
        self.tree = p_tree
        # discover the type
        self.type = self.getType(self.tree)
        # Handle each possible type
        if self.type == 'O':
            self.item = Sentence(self.paradigm, self.tree[0])
        if self.type == 'D':
            self.item = Domain(self.paradigm, self.tree)
        if self.type == 'H':
            self.item = Hierarchy(self.paradigm, self.tree)
        if self.type == 'T':
            self.item = Table(self.paradigm, self.tree)

    def getList(self):
        """
        Returns values in the form of a list
        """
        if self.tree == None:
            return None
        return self.item.getList()

    def getHTML(self):
        """
        Returns values in html (table) form
        """
        return self.item.getHTML()

    def getHorizontalHTML(self,p_parentSpan=1):
        """
        Returns values in html (table) form
        """
        return self.item.getHorizontalHTML(p_parentSpan)

    def getText(self):
        """
        Returns values in plain text form
        """
        return self.item.getText()

    def getConditions(self):
        """
        Return a list of conditions for each combination (cell)
        """
        return self.item.getConditions()

    def getMaxWidth(self):
        """
        Returns the width in number of characters
        """
        return self.item.getMaxWidth()

    def getSpan(self):
        """
        Returns the span (requred for "rowspan" and "colspan" HTML attributes)
        """
        return self.item.getSpan()

    def getDepth(self):
        """
        Get the depth
        """
        return self.item.getDepth()

    def getType(self, p_tree=None):
        """
        Determine the type of the current node of the tree
        This need not be overridden
        """
        if p_tree == None:
            p_tree = self.tree
        # This is in the second character of the string representation
        return str(p_tree)[1:2]

class Domain(Sentence):
    """
    Manages a domain operation
    
    Provides: Domain(paradigm,tree)
    """
    def __init__(self, p_paradigm, p_tree):
        """
        p_paradigm is the given paradigm (attributes and data)
        p_tree is the query tree
        """
        self.paradigm = p_paradigm
        # Validate that this is a domain
        assert self.getType(p_tree) == 'D'
        # Store the attribute
        self.attribute = p_tree[0]
        self.error = None
        # Check that the requested attribute is available
        try:
            self.paradigm.attributes[self.attribute]
        except KeyError:
            self.error = "I couldn't find this attribute: " + self.attribute
            print self.error

    def __getitem__(self, p_index):
        return self.paradigm.attributes[self.attribute][p_index]

    def getList(self):
        """
        Return the domain in list form
        """
        return self.paradigm.attributes[self.attribute]

    def getHTML(self):
        """
        Return html for this domain
        """
        ret_string = ""
        for item in self.getList():
            ret_string += "<tr><td>" + item + "</td></tr>"
        return ret_string

    def getHorizontalHTML(self,p_parentSpan=1):
        """
        Return a horizontal html table
        """
        ret_string = ""
        for item in self.getList():
            ret_string += "<td>" + item + "</td>"
        return "<tr>" + ret_string*p_parentSpan + "</tr>"


    def getText(self):
        """
        Return text for this domain
        """
        ret_string = ""
        for item in self.getList():
            ret_string += item + "\n"
        return ret_string

    def getConditions(self):
        """
        Return a list of conditions for each combination (cell)
        """
        ret_conds = []
        for item in self.getList():
            new = {self.attribute: item}
            #new[self.attribute] = item
            ret_conds.append(new)
        return ret_conds

    def getMaxWidth(self):
        """
        Get max width (chars) for display purposes
        """
        max_width = 0
        for item in self.getList():
            if max_width < len(item):
                max_width = len(item)
        return max_width

    def getSpan(self):
        """
        Get the span of this domain (number of elements)
        """
        return len(self.getList())

    def getDepth(self):
        """
        Get the depth of this domain (always one!)
        """
        return 1 

class Hierarchy(Sentence):
    """
    Manages a hierarchy operation
    
    Provides: Hierarchy(paradigm,tree)
    """
    def __init__(self, p_paradigm, p_tree):
        """
        p_paradigm is the given paradigm (attributes and data)
        p_tree is the tree representation of this part of the query (Tree)
        """
        self.paradigm = p_paradigm
        self.error = None

        self.tree = p_tree
        # Validate that this is a Hierarchy
        assert self.getType(p_tree) == 'H'
        # Validate that the root is a Domain
        assert self.getType(p_tree[0]) == 'D'
        # Set the root and the leaf 
        self.root = Domain(self.paradigm, p_tree[0])
        self.leaf = Sentence(self.paradigm, p_tree[1])


    def getList(self):
        """
        Return the hierarchy in list form
        """
        # Get child lists
        rootList = self.root.getList()
        leafList = self.leaf.getList()
        
        # Combine lists into an array
        ret_val = []
        for item_root in rootList:
            for item_leaf in leafList:
                ret_val.append([item_root,item_leaf])

        return ret_val

    def getHTML(self):
        """
        Return a html table for this hierarchy
        """
        ret_string = ""
        for index in range(len(self.root.getList())):
            leafCells = self.leaf.getHTML()[4:]
            ret_string += "<tr><td rowspan=\"" + str(self.leaf.getSpan()) + "\">" + self.root[index] \
                             + "</td>" + leafCells
        return ret_string

    def getHorizontalHTML(self,p_parentSpan=1):
        """
        Return a horizontal html table
        """
        ret_string = ""
        # Add a new cell for each root item
        for index in range(len(self.root.getList())):
            ret_string += "<td colspan=\"" + str(self.leaf.getSpan()) + "\">" \
                             + self.root[index] + "</td>" 
        # Recusively get the horizontalHTML from the leaf children
        leafCells = self.leaf.getHorizontalHTML(p_parentSpan*len(self.root.getList()))
        # Return the new row and the leaf cells
        return "<tr>" + ret_string*p_parentSpan + "</tr>" + leafCells 

    def getText(self):
        """
        Return text for this hierarchy
        """
        ret_string = ""
        # Lengths for rendering display
        max_width_root = self.root.getMaxWidth()
        max_width_leaf = self.leaf.getMaxWidth()
        # add root string and call getText() for leaf node
        # (newlines in the leaf node need to have whitespace added)
        for index in range(len(self.root.getList())):
            ret_string += self.root[index].ljust(max_width_root) + " " \
              + self.leaf.getText().ljust(max_width_leaf).replace('\n',"\n" \
              + " "*(max_width_root+1)) + "\n"
        # Remove any blank lines and return the string
        re_blank = re.compile('\n[ ]+\n')
        return re_blank.sub('\n',ret_string)

    def getConditions(self):
        """
        Return a list of conditions for each combination (cell)
        """
        ret_conds = []
        # For each root item
        for item_r in self.root.getList():
            # for each leaf condition
            for cond_l in self.leaf.getConditions():
                # Add the root node's condition
                cond_l[self.root.attribute] = item_r
                # Append this to the return list of conditions 
                ret_conds.append(cond_l)
        # Return our list
        return ret_conds

    def getMaxWidth(self):
        """
        Return the maximum width (in chars) this hierarchy will take up
        """
        return self.root.getMaxWidth() + self.leaf.getMaxWidth() + 1

    def getDepth(self):
        """
        Get the depth of this hierarchy
        """
        return 1 + self.leaf.getDepth() 
        
    def getSpan(self):
        """
        Get the span (for HTML tables) of this hierarchy
        """
        return self.root.getSpan() * self.leaf.getSpan() 

class Table(Sentence):
    """
    Manages a table operation
    
    Provides: Table(paradigm,tree)
    """
    def __init__(self, p_paradigm, p_tree):
        """
        p_paradigm is the given paradigm (attributes and data)
        p_tree is the tree representation of this part of the query (Tree)
        """
        self.paradigm = p_paradigm
        self.error = None

        self.tree = p_tree
        # Validate that this is a Table
        assert self.getType(p_tree) == 'T'
        # Set the table arguments
        self.horizontal = Sentence(self.paradigm, p_tree[0])
        self.vertical = Sentence(self.paradigm, p_tree[1])
        self.cells = Sentence(self.paradigm, p_tree[2])


    def getList(self):
        """
        Return the table (cells) in list form
        """
        ret_val = []
        return ret_val

    def getHTML(self):
        """
        Return a html table for this table operation
        """
        # Start with the dead cell
        dead_cell = "<tr><td colspan=\"" + str(self.vertical.getDepth()) \
                        + "\" rowspan=\"" + str(self.horizontal.getDepth()) \
                        + "\"></td>"
        # Insert horizintal header
        horizontal_header = self.horizontal.getHorizontalHTML()[4:].replace('td','th')
        #horizontal_header = self.horizontal.getHorizontalHTML().replace('td','th')
        # Get the vertical header
        vertical_header = self.vertical.getHTML().replace('td','th')
        str_cells = ""
        # Reset conditions
        conditions = {}
        # get a list of conditions for the row
        conditions_v = self.vertical.getConditions()
        # for each row
        for cond_v in conditions_v:
            str_cells += "<tr>"
            # get a list of conditions for the row
            conditions_h = self.horizontal.getConditions()
            # For each column
            for cond_h in conditions_h:
                # Get the data for this cell, given the hori and vert conditions
                cell_data = self.getData(self.cells.tree, dictJoin(cond_v,cond_h))
                # Add the cell
                str_cells += "<td>" + cell_data + "</td>"
            # End the row
            str_cells += "</tr>"
        
        # VERTICAL HEADER INCLUSION
        # Split rows into a list
        vertical_header_rows = vertical_header.split('</tr>')
        cell_rows = str_cells.replace('<tr>','').split('</tr>')
        # Join two lists
        zipped = zip(vertical_header_rows, cell_rows)
        str_zipped = ""
        for (header,cells) in zipped:
            if header <> '':
                str_zipped += header + cells + "</tr>\n"

        # Return all the elements
        return dead_cell + horizontal_header + str_zipped

    def getHorizontalHTML(self,p_parentSpan=1):
        """
        Return a horizontal html table (?)
        """
        print "?: getHorizontalHTML() called on a table."
        return None

    def getText(self):
        """
        Return text for this table (?)
        """
        print "?: getText() for a table? HAHAHAHAHA"
        print "call setFormat('html') if you want to run queries like that"
        return 
    
    def getConditions(self):
        """
        Return conditions for this table (?)
        """
        print "?: getConditions() called on a table. I don't think so."
        return None

    def getMaxWidth(self):
        """
        Return the maximum width this table could take up. 
        ... I hope you're not trying to nest tables ...
        """
        return self.cells.getMaxWidth() + self.vertical.getMaxWidth() + 1

    def getSpan(self):
        """
        Return span for this table (?)
        """
        print "WTF: getSpan() called on a table."
        return None

    def getData(self, p_return, p_attDict):
        """
        Retrieve data that matches the given list of attributes
        Returns (an HTML) string of values that match.
    
        p_return is a tree pointing to the key of the value to include in the return
        p_attDict is a dictionary of conditions.
        """
        output = []
        return_key = p_return.leaves()[0]

        # For each data object in the paradigm
        for datum in self.paradigm.data:
            inc = True
            # For each given attribute requirement
            for att in p_attDict.keys():
                # If the data object fails the requirement do not include
                if datum[att] != p_attDict[att]:
                    inc = False
                    break
            # If it passed all the tests, include it
            if inc == True:
                output.append(datum[return_key])

        # Return what we found (make sure this is a string)
        if len(output) == 1:
            return output[0]
        else:
            # Hardcoded HTML goodness
            # (Obviously this will have to change for text output)
            ret_str = "<table>"
            for item in output:
                ret_str += "<tr><td>" + item + "</td></tr>"
            ret_str += "</table>"
            return ret_str


def dictJoin(dict1,dict2):
    """
    A handy function to join two dictionaries
    If there is any key overlap, dict1 wins!
    (just make sure this doesn't happen)
    """
    for key in dict1.keys():
        dict2[key] = dict1[key]
    return dict2

def demo():

    # Print the query
    print """
================================================================================
Load: Paradigm(file)
================================================================================
"""
    print
    print ">>> a = Paradigm('german.xml')"
    print 
    a = Paradigm('german.xml')
    print 
    print ">>> a.setOutput('term')"
    print 
    a.setOutput('term')
    print 
    print ">>> a.setFormat('text')"
    print 
    a.setFormat('text')

    # Print a domain
    print """
================================================================================
Domain: case
================================================================================
"""
    print 
    print ">>> a.show('case')"
    print 
    a.show('case')

    # Print a hierarchy
    print """
================================================================================
Hierarchy: case/gender
================================================================================
"""
    print 
    print ">>> a.show('case/gender')"
    print 
    a.show('case/gender')

    # Print a table
    print """
================================================================================
Table: table(case/number,gender,content)
================================================================================
"""
    print 
    print ">>> a.setOutput('demo.html')"
    print 
    a.setOutput('demo.html')
    print 
    print ">>> a.setFormat('html')"
    print 
    a.setFormat('html')
    print 
    print ">>> a.show('table(case/number,gender,content)')"
    print 
    a.show('table(case/number,gender,content)')

    # Some space
    print 

if __name__ == '__main__':
    demo()    

########NEW FILE########
__FILENAME__ = paradigmquery
# Natural Language Toolkit: Paradigm Visualisation
#
# Copyright (C) 2005 University of Melbourne
# Author: Will Hardy
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

# Parses a paradigm query and produces an XML representation of
# that query. This is part of a Python implementation of David
# Penton's paradigm visualisation model.

#This is the query XML version of "table(person, number, content)"
#
#<?xml version="1.0"?>
#<document>
#  <parse-tree>
#    <operator opcode="table" instruction="1">
#      <operand type="domain"
#        arg="horizontal">person</operand>
#      <operand type="domain"
#        arg="vertical">number</operand>
#      <operand type="domain"
#        arg="cell">content</operand>
#    </operator>
#  </parse-tree>
#</document>

from nltk import tokenize, parse
from nltk.grammar import ContextFreeGrammar, nonterminals, Production
from re import *

class ParadigmQuery(object):
    """
    Class to read and parse a paradigm visualisation query
    """

    def __init__(self, p_string=None):
        """
        Construct a query.
        Setup various attributes and parse given string
        """
        self.nltktree = None
        self.string = p_string
        self.parseList = None
        self.nltkTree = None
        self.parseTree = None
        self.xml = None

        # If p_string was given, parse it
        if p_string <> None:
            self.parse(p_string)

    def parse(self, p_string):
        """
        Parses a string and stores the resulting hierarchy of "domains"
        "hierarchies" and "tables"

        For the sake of NLP I've parsed the string using the nltk 
        context free grammar library.

        A query is a "sentence" and can either be a domain, hierarchy or a table.
        A domain is simply a word.
        A hierarchy is expressed as "domain/domain"
        A table is exressed as "table(sentence, sentence, sentence)"

        Internally the query is represented as a nltk.parse.tree

        Process:
          1. string is tokenized
          2. develop a context free grammar
          3. parse
          4. convert to a tree representation
        """
        self.nltktree = None

        # Store the query string
        self.string = p_string

        # Tokenize the query string, allowing only strings, parentheses,
        # forward slashes and commas.
        re_all = r'table[(]|\,|[)]|[/]|\w+'
        data_tokens = tokenize.regexp_tokenize(self.string, re_all)

        # Develop a context free grammar
        # S = sentence, T = table, H = hierarchy, D = domain
        O, T, H, D = nonterminals('O, T, H, D')

        # Specify the grammar
        productions = (
            # A sentence can be either a table, hierarchy or domain
            Production(O, [D]), Production(O, [H]), Production(O, [T]),
            
            # A table must be the following sequence:
            # "table(", sentence, comma, sentence, comma, sentence, ")" 
            Production(T, ['table(', O, ',', O, ',', O, ')']),

            # A hierarchy must be the following sequence:
            # domain, forward slash, domain
            Production(H, [D, '/', D]),
            # domain, forward slash, another operator
            Production(H, [D, '/', O])
        )

        # Add domains to the cfg productions
        # A domain is a token that is entirely word chars
        re_domain = compile(r'^\w+$') 
        # Try every token and add if it matches the above regular expression
        for tok in data_tokens:
            if re_domain.match(tok):
                prod = Production(D,[tok]),
                productions = productions + prod

        # Make a grammar out of our productions
        grammar = ContextFreeGrammar(O, productions)
        rd_parser = parse.RecursiveDescentParser(grammar)
       
        # Tokens need to be redefined. 
        # It disappears after first use, and I don't know why.
        tokens = tokenize.regexp_tokenize(self.string, re_all)
        toklist = list(tokens)

        # Store the parsing. 
        # Only the first one, as the grammar should be completely nonambiguous.
        try:
            self.parseList = rd_parser.get_parse_list(toklist)[0]
        except IndexError: 
            print "Could not parse query."
            return

        # Set the nltk.parse.tree tree for this query to the global sentence
        string = str(self.parseList)
        string2 = string.replace(":","").replace("')'","").replace("table(","").replace("','","").replace("'","").replace("/","")
        self.nltktree = parse.tree.bracket_parse(string2)
        
        # Store the resulting nltk.parse.tree tree
        self.parseTree = QuerySentence(self.nltktree)
        self.xml = self.parseTree.toXML()


    def getTree(self):
        """
        Returns the results from the CFG parsing
        """
        if self.string == None:
            print "No string has been parsed. Please use parse(string)."
            return None
        return self.nltktree

    def getXML(self):
        if self.string == None:
            print "No string has been parsed. Please use parse(string)."
            return None
        return '<?xml version="1.0"?>\n<document><parse-tree>' + self.xml \
                  + "</parse-tree></document>"



# Additional Classes for handling The various types of recursive operations

class QuerySentence(object):
    """
    Handles the XML export of sentences
    """
    def __init__(self, tree):
        self.tree = tree
        type = str(tree[0])[1:2]
        # Move on, nothing to see here
        if type == "O":
            self.child = QuerySentence(tree[0])
            self.content = self.child.content
        # Get the child and replicate the data
        elif type == "D":
            self.child = QueryDomain(tree[0])
            self.content = self.child.content
        elif type == "H":
            self.child = QueryHierarchy(tree[0])
            self.root = self.child.root
            self.leaf = self.child.leaf
        elif type == "T":
            self.child = QueryTable(tree[0])
            self.horizontal = self.child.horizontal
            self.vertical = self.child.vertical
        # Otherwise, must simply be a domain...
        else:
            self.child = QueryDomain(tree[0])
            self.content = self.child.content
        self.type = self.child.type


    def __str__(self):
        return str(self.tree[0])

    def toXML(self):
        """
        Export this class to an xml string
        """
        return self.child.toXML()


class QueryDomain(object):
    """
    Handles the XML export of the domain operation
    """
    def __init__(self, tree):
        self.type = 'domain'
        self.content = tree[0]

    def __str__(self):
        return tree[0]

    def toXML(self):
        """
        Export this class to an xml string
        """
        return self.content


class QueryHierarchy(object):
    """
    Handles the XML export of the hierarchy operation
    """
    def __init__(self, tree):
        self.type = 'hierarchy'
        # First argument must be a Domain
        self.root = QueryDomain(tree[0]) 
        # Second argument can conceivably be anything
        self.leaf = QuerySentence(tree[1]) 

    def __str__(self):
        return tree[0]

    def toXML(self):
        """
        Export this class to an xml string
        """
        return '<operator opcode="hierarchy">' \
               + '<operand type="' + self.root.type + '" arg="root">' \
               + self.root.toXML() + "</operand>" \
               + '<operand type="' + self.leaf.type + '" arg="leaf">' \
               + self.leaf.toXML() + "</operand>" \
               + '</operator>'


class QueryTable(object):
    """
    Handles the XML export of the hierarchy operation
    """
    def __init__(self, tree):
        """
        Simply stores attributes, passing off handling of attributes to the
        QuerySentence class
        """
        self.type = 'table'
        self.horizontal = QuerySentence(tree[0])
        self.vertical = QuerySentence(tree[1])
        self.content = QuerySentence(tree[2])

    def __str__(self):
        return tree[0]

    def toXML(self):
        """
        Export this class to an xml string
        """
        return '<operator opcode="table">' \
               + '<operand type="' + self.horizontal.type + '" arg="horizontal">' \
               + self.horizontal.toXML() + "</operand>" \
               + '<operand type="' + self.vertical.type + '" arg="vertical">' \
               + self.vertical.toXML() + "</operand>" \
               + '<operand type="' + self.content.type + '" arg="cell">' \
               + self.content.toXML() + "</operand>" \
               + '</operator>'


def demo():
    """
    A demonstration of the use of this class
    """
    query = r'table(one/two/three, four, five)'

    # Print the query
    print """
================================================================================
Query: ParadigmQuery(query)
================================================================================
"""
    a = ParadigmQuery(query)
    print query

    # Print the Tree representation
    print """
================================================================================
Tree: getTree()
  O is an operator
  T is a table
  H is a hierarchy
  D is a domain
================================================================================
"""
    print a.getTree()

    # Print the XML representation
    print """
================================================================================
XML: getXML()
================================================================================
"""
    print a.getXML()

    # Some space
    print 


if __name__ == '__main__':
    demo()    

########NEW FILE########
__FILENAME__ = draw
import Tkinter as tk
from morphology import KimmoMorphology
from fsa import FSA

class KimmoGUI(object):
    def __init__(self, ruleset, startTk=False):
        import Tkinter as tk
        if startTk: self._root = tk.Tk()
        else: self._root = tk.Toplevel()
        
        self.ruleset = ruleset
        self.rules = ruleset.rules()
        self.lexicon = ruleset.morphology()

        frame = tk.Frame(self._root)
        tk.Label(frame, text='Rules').pack(side=tk.TOP)
        scrollbar = tk.Scrollbar(frame, orient=tk.VERTICAL)
        self.listbox = tk.Listbox(frame, yscrollcommand=scrollbar.set,
        exportselection=0)
        scrollbar.config(command=self.listbox.yview)
        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        self.listbox.pack(side=tk.LEFT, fill=tk.BOTH, expand=1)
        
        frame2 = tk.Frame(self._root)
        tk.Label(frame2, text='Steps').pack(side=tk.TOP)
        scrollbar2 = tk.Scrollbar(frame2, orient=tk.VERTICAL)
        self.steplist = tk.Listbox(frame2, yscrollcommand=scrollbar2.set,
        font='Sans 10', width='40', exportselection=0)
        scrollbar2.config(command=self.steplist.yview)
        scrollbar2.pack(side=tk.RIGHT, fill=tk.Y)
        self.steplist.pack(side=tk.LEFT, fill=tk.BOTH, expand=1)

        wordbar = tk.Frame(self._root)
        self.genbox = tk.Entry(wordbar, width=30, font="Courier 14")
        genbutton = tk.Button(wordbar, text="Generate ->",
        command=self.generate)
        recbutton = tk.Button(wordbar, text="<- Recognize",
        command=self.recognize)
        self.recbox = tk.Entry(wordbar, width=30, font="Courier 14")
        self.resultlabel = tk.Label(wordbar, justify=tk.CENTER, text='')
        
        self.genbox.pack(side=tk.LEFT)
        genbutton.pack(side=tk.LEFT)
        self.resultlabel.pack(side=tk.LEFT, fill=tk.X, expand=1)
        self.recbox.pack(side=tk.RIGHT)
        recbutton.pack(side=tk.RIGHT)
        wordbar.pack(side=tk.TOP, fill=tk.X, expand=1)
        frame.pack(side=tk.LEFT, fill=tk.Y, expand=1)
        frame2.pack(side=tk.LEFT, fill=tk.Y, expand=1)
        
        if self.lexicon: self.listbox.insert(tk.END, 'Lexicon')
        else: self.listbox.insert(tk.END, '(no lexicon)')
        for rule in self.rules:
            self.listbox.insert(tk.END, rule.name())
        
        self.rframe = tk.Frame(self._root)
        self.rframe.pack(side=tk.LEFT, fill=tk.BOTH, expand=1)
        
        self.graph_widget = None
        self.listbox.bind("<ButtonRelease-1>", self.graph_selected)
        self.steplist.bind("<ButtonRelease-1>", self.step_selected)

        self._root.bind('<Up>', self.select_up)
        self._root.bind('<Down>', self.select_down)
        
        self.widget_store = {}
        self.steps = []
        for i in range(len(self.rules), -1, -1): # yes, including the last one.
            self.draw_rule(i)
        
        if startTk:
            tk.mainloop()

    def step_selected(self, event):
        values = self.steplist.curselection()
        if len(values) == 0: return
        index = int(values[0])
        self.highlight_states(self.steps[index][1], self.steps[index][2])
        #self.draw_rule(index)
        
    def graph_selected(self, event):
        values = self.listbox.curselection()
        if len(values) == 0: return
        index = int(values[0])
        self.draw_rule(index)

    def select_up(self, event):
        values = self.steplist.curselection()
        if len(values) == 0: values = [0]
        index = int(values[0])
        if index == 0: return
        self.steplist.selection_clear(0, tk.END)
        self.steplist.selection_set(index-1)
        self.step_selected(event)

    def select_down(self, event):
        values = self.steplist.curselection()
        if len(values) == 0: values = [0]
        index = int(values[0])
        if index == len(self.steps) - 1: return
        self.steplist.selection_clear(0, tk.END)
        self.steplist.selection_set(index+1)
        self.step_selected(event)

    def draw_rule(self, index):
        if index == 0:
            rule = self.lexicon
        else: rule = self.rules[index-1]
        if rule is None: return
        if self.graph_widget is not None:
            self.graph_widget.pack_forget()
        if index-1 in self.widget_store:
            self.graph, self.graph_widget = self.widget_store[index-1]
            self.graph_widget.pack(side=tk.LEFT, fill=tk.BOTH, expand=1)
        else:
            self.graph_widget = self.wrap_pygraph(rule)
            self.graph_widget.pack(side=tk.LEFT, fill=tk.BOTH, expand=1)
            self.widget_store[index-1] = (self.graph, self.graph_widget)
    
    def wrap_pygraph(self, rule):
        if isinstance(rule, KimmoMorphology):
            title = 'Lexicon'
            labels = False
        else:
            title = rule.name()
            labels = True
        frame = tk.Frame(self.rframe)
        self.graph = rule.fsa().show_pygraph(title=title, labels=labels, root=frame)
        return frame
    
    def highlight_states(self, states, morph):
        select = self.listbox.curselection() or 0
        self.listbox.delete(0, tk.END)
        for (index, stored) in self.widget_store.items():
            graph, widget = stored
            if index == -1: state = morph
            else: state = states[index]
            graph.deHighlightNodes()
            graph.HighlightNode(state, None)
        for index in range(-1, len(self.rules)):
            if index == -1:
                if self.lexicon:
                    state = morph
                    name = 'Lexicon'
                    text = '%s [%s]' % (name, state)
                else: text = '(no lexicon)'
            else:
                state = states[index]
                name = self.rules[index].name()
                text = '%s [%s]' % (name, state)
            self.listbox.insert(tk.END, text)
        self.listbox.selection_clear(0, tk.END)
        self.listbox.selection_set(select)
    
    def step(self, pairs, curr, rules, prev_states, states, morphology_state,
    word):
        lexical = ''.join(p.input() for p in pairs)
        surface = ''.join(p.output() for p in pairs)
        text = '%s<%s> | %s<%s>'%(lexical, curr.input(), surface, curr.output())
        blocked = []
        for rule, state in zip(rules, states):
            if str(state).lower() in ['0', 'reject']:
                blocked.append(rule.name())
        if blocked:
             text +=' [%s failed]' % (', '.join(blocked))
        self.steplist.insert(tk.END, text)
        self.steps.append((text, states, morphology_state, word))

    def generate(self):
        if not self.genbox.get(): return
        gentext = self.genbox.get().split()[0]
        result = " ".join(self.ruleset.generate(gentext, self))
        self.recbox.delete(0, tk.END)
        self.recbox.insert(0, result)

    def recognize(self):
        if not self.recbox.get(): return
        rectext = self.recbox.get().split()[0]
        result = ", ".join('%s (%s)' % (word, feat) for (word, feat) in
          self.ruleset.recognize(rectext, self))
        self.genbox.delete(0, tk.END)
        self.genbox.insert(0, result)

    def succeed(self, pairs):
        lexical = ''.join(p.input() for p in pairs)
        surface = ''.join(p.output() for p in pairs)
        self.steplist.insert(tk.END, 'SUCCESS: %s / %s' % (lexical, surface))
        self.num_results += 1
        if self.num_results == 1:
            self.resultlabel.configure(text='1 result')
        else:
            self.resultlabel.configure(text='%d results' % self.num_results)
        self.steps.append((lexical, [None]*len(self.rules), None, ''))
        
    def reset(self):
        self.steplist.delete(0, tk.END)
        self.num_results = 0
        self.steps = []
        self.resultlabel.configure(text='')

# vim:et:ts=4:sts=4:sw=4:

########NEW FILE########
__FILENAME__ = featurelite
"""
This module provides utilities for treating Python dictionaries as X{feature
structures}. Specifically, it contains the C{unify} function, which can be used
to merge the properties of two dictionaries, and the C{Variable} class, which
holds an unknown value to be used in unification.

A X{feature structure} is a mapping from feature names to feature values,
where:

  - Each X{feature name} is a case sensitive string.
  - Each X{feature value} can be a base value (such as a string), a
    variable, or a nested feature structure.

However, feature structures are not a specialized class; they are represented
by dictionaries, or more generally by anything that responds to the C{has_key}
method. The YAML representation can be used to create and display feature
structures intuitively:

>>> f1 = yaml.load('''
... A:
...   B: b
...   D: d
... ''')
>>> f2 = yaml.load('''
... A:
...   C: c
...   D: d
... ''')
>>> print show(unify(f1, f2))
A:
  B: b
  C: c
  D: d

Feature structures are typically used to represent partial information
about objects.  A feature name that is not mapped to a value stands
for a feature whose value is unknown (I{not} a feature without a
value).  Two feature structures that represent (potentially
overlapping) information about the same object can be combined by
X{unification}.  When two inconsistant feature structures are unified,
the unification fails and raises an error.

Features can be specified using X{feature paths}, or tuples of feature names
that specify paths through the nested feature structures to a value.

Feature structures may contain reentrant feature values.  A
X{reentrant feature value} is a single feature value that can be
accessed via multiple feature paths.  Unification preserves the
reentrance relations imposed by both of the unified feature
structures.  After unification, any extensions to a reentrant feature
value will be visible using any of its feature paths.

Feature structure variables are encoded using the L{Variable} class. The scope
of a variable is determined by the X{bindings} used when the structure
including that variable is unified. Bindings can be reused between unifications
to ensure that variables with the same name get the same value.

"""

from copy import copy, deepcopy
import re
import yaml
#import unittest
import sys

class UnificationFailure(Exception):
    """
    An exception that is raised when two values cannot be unified.
    """
    pass

def isMapping(obj):
    """
    Determine whether to treat a given object as a feature structure. The
    test is whether it responds to C{has_key}. This can be overridden if the
    object includes an attribute or method called C{_no_feature}.

    @param obj: The object to be tested
    @type obj: C{object}
    @return: True iff the object can be treated as a feature structure
    @rtype: C{bool}
    """
    return ('has_key' in dir(obj)) and ('_no_feature' not in dir(obj)) 

class _FORWARD(object):
    """
    _FORWARD is a singleton value, used in unification as a flag that a value
    has been forwarded to another object.

    This class itself is used as the singleton value. It cannot be
    instantiated.
    """
    def __init__(self):
        raise TypeError, "The _FORWARD class is not meant to be instantiated"

class Variable(object):
    """
    A Variable is an object that can be used in unification to hold an
    initially unknown value. Two equivalent Variables, for example, can be used
    to require that two features have the same value.

    When a Variable is assigned a value, it will eventually be replaced by
    that value. However, in order to make that value show up everywhere the
    variable appears, the Variable temporarily stores its assigned value and
    becomes a I{bound variable}. Bound variables do not appear in the results
    of unification.

    Variables are distinguished by their name, and by the dictionary of
    I{bindings} that is being used to determine their values. Two variables can
    have the same name but be associated with two different binding
    dictionaries: those variables are not equal.
    """
    _next_numbered_id = 1
    
    def __init__(self, name=None, value=None):
        """
        Construct a new feature structure variable.
        
        The value should be left at its default of None; it is only used
        internally to copy bound variables.

        @type name: C{string}
        @param name: An identifier for this variable. Two C{Variable} objects
          with the same name will be given the same value in a given dictionary
          of bindings.
        """
        self._uid = Variable._next_numbered_id
        Variable._next_numbered_id += 1
        if name is None: name = self._uid
        self._name = str(name)
        self._value = value
    
    def name(self):
        """
        @return: This variable's name.
        @rtype: C{string}
        """
        return self._name
    
    def value(self):
        """
        If this varable is bound, find its value. If it is unbound or aliased
        to an unbound variable, returns None.
        
        @return: The value of this variable, if any.
        @rtype: C{object}
        """
        if isinstance(self._value, Variable): return self._value.value()
        else: return self._value
    def copy(self):
        """
        @return: A copy of this variable.
        @rtype: C{Variable}
        """
        return Variable(self.name(), self.value())
    
    def forwarded_self(self):
        """
        Variables are aliased to other variables by one variable _forwarding_
        to the other. The first variable simply has the second as its value,
        but it acts like the second variable's _value_ is its value.

        forwarded_self returns the final Variable object that actually stores
        the value.

        @return: The C{Variable} responsible for storing this variable's value.
        @rtype: C{Variable}
        """
        if isinstance(self._value, Variable):
            return self._value.forwarded_self()
        else: return self
    
    def bindValue(self, value, ourbindings, otherbindings):
        """
        Bind this variable to a value. C{ourbindings} are the bindings that
        accompany the feature structure this variable came from;
        C{otherbindings} are the bindings from the structure it's being unified
        with.

        @type value: C{object}
        @param value: The value to be assigned.
        @type ourbindings: C{dict}
        @param ourbindings: The bindings associated with this variable.
        @type otherbindings: C{dict}
        @param otherbindings: The bindings associated with the value being
          assigned. (May be identical to C{ourbindings}.)
        """
        if isinstance(self._value, Variable):
            # Forward the job of binding to the variable we're aliased to.
            return self._value.bindValue(value, ourbindings, otherbindings)
        if self._value is None:
            # This variable is unbound, so bind it.
            self._value = value
        else:
            # This variable is already bound; try to unify the existing value
            # with the new one.
            self._value = unify(self._value, value, ourbindings, otherbindings)

    def forwardTo(self, other, ourbindings, otherbindings):
        """
        A unification wants this variable to be aliased to another variable.
        Forward this variable to the other one, and return the other.

        @type other: C{Variable}
        @param other: The variable to replace this one.
        @type ourbindings: C{dict}
        @param ourbindings: The bindings associated with this variable.
        @type otherbindings: C{dict}
        @param otherbindings: The bindings associated with the other variable.
        (May be identical to C{ourbindings}.)
        @return: C{other}
        @rtype: C{Variable}
        """
        other.bindValue(self.value(), ourbindings, otherbindings)
        self._value = other
        return other
        
    def __hash__(self): return hash(self._uid)
    def __cmp__(self, other):
        """
        Variables are equal if they are the same object or forward to the
        same object. Variables with the same name may still be unequal.
        """
        if not isinstance(other, Variable): return -1
        if isinstance(self._value, Variable): return cmp(self._value, other)
        else: return cmp(self._uid, other._uid)
    def __repr__(self):
        if self._value is None: return '?%s' % self._name
        else: return '?%s: %r' % (self._name, self._value)

def show(data):
    """
    Works like yaml.dump(), but with output suited for doctests. Flow style
    is always off, and there is no blank line at the end.
    """
    return yaml.dump(data, default_flow_style=False).strip()

def variable_representer(dumper, var):
    "Output variables in YAML as ?name."
    return dumper.represent_scalar(u'!var', u'?%s' % var.name())
yaml.add_representer(Variable, variable_representer)

def variable_constructor(loader, node):
    "Recognize variables written as ?name in YAML."
    value = loader.construct_scalar(node)
    name = value[1:]
    return Variable(name)
yaml.add_constructor(u'!var', variable_constructor)
yaml.add_implicit_resolver(u'!var', re.compile(r'^\?\w+$'))

def _copy_and_bind(feature, bindings, memo=None):
    """
    Make a deep copy of a feature structure, preserving reentrance using the
    C{memo} dictionary. Meanwhile, variables are replaced by their bound
    values, if these values are already known, and variables with unknown
    values are given placeholder bindings.
    """
    if memo is None: memo = {}
    if id(feature) in memo: return memo[id(feature)]
    if isinstance(feature, Variable) and bindings is not None:
        if not bindings.has_key(feature.name()):
            bindings[feature.name()] = feature.copy()
        result = _copy_and_bind(bindings[feature.name()], None, memo)
    else:
        if isMapping(feature):
            # Construct a new object of the same class
            result = feature.__class__()
            for (key, value) in feature.items():
                result[key] = _copy_and_bind(value, bindings, memo)
        else: result = feature
    memo[id(feature)] = result
    memo[id(result)] = result
    return result

def unify(feature1, feature2, bindings1=None, bindings2=None, fail=None):
    """
    In general, the 'unify' procedure takes two values, and either returns a
    value that provides the information provided by both values, or fails if
    that is impossible.
    
    These values can have any type, but fall into a few general cases:
      - Values that respond to C{has_key} represent feature structures. The
        C{unify} procedure will recurse into them and unify their inner values.
      - L{Variable}s represent an unknown value, and are handled specially.
        The values assigned to variables are tracked using X{bindings}.
      - C{None} represents the absence of information.
      - Any other value is considered a X{base value}. Base values are
        compared to each other with the == operation.

    The value 'None' represents the absence of any information. It specifies no
    properties and acts as the identity in unification.
    >>> unify(3, None)
    3

    >>> unify(None, 'fish')
    'fish'

    A base value unifies with itself, but not much else.
    >>> unify(True, True)
    True

    >>> unify([1], [1])
    [1]

    >>> unify('a', 'b')
    Traceback (most recent call last):
        ...
    UnificationFailure

    When two mappings (representing feature structures, and usually implemented
    as dictionaries) are unified, any chain of keys that accesses a value in
    either mapping will access an equivalent or more specific value in the
    unified mapping. If this is not possible, UnificationFailure is raised.

    >>> f1 = dict(A=dict(B='b'))
    >>> f2 = dict(A=dict(C='c'))
    >>> unify(f1, f2) == dict(A=dict(B='b', C='c'))
    True
    
    The empty dictionary specifies no features. It unifies with any mapping.
    >>> unify({}, dict(foo='bar'))
    {'foo': 'bar'}

    >>> unify({}, True)
    Traceback (most recent call last):
        ...
    UnificationFailure
    
    Representing dictionaries in YAML form is useful for making feature
    structures readable:
    
    >>> f1 = yaml.load("number: singular")
    >>> f2 = yaml.load("person: 3")
    >>> print show(unify(f1, f2))
    number: singular
    person: 3

    >>> f1 = yaml.load('''
    ... A:
    ...   B: b
    ...   D: d
    ... ''')
    >>> f2 = yaml.load('''
    ... A:
    ...   C: c
    ...   D: d
    ... ''')
    >>> print show(unify(f1, f2))
    A:
      B: b
      C: c
      D: d
    
    Variables are names for unknown values. Variables are assigned values
    that will make unification succeed. The values of variables can be reused
    in later unifications if you provide a dictionary of _bindings_ from
    variables to their values.
    >>> bindings = {}
    >>> print unify(Variable('x'), 5, bindings)
    5
    
    >>> print bindings
    {'x': 5}
    
    >>> print unify({'a': Variable('x')}, {}, bindings)
    {'a': 5}
    
    The same variable name can be reused in different binding dictionaries
    without collision. In some cases, you may want to provide two separate
    binding dictionaries to C{unify} -- one for each feature structure, so
    their variables do not collide.

    In the following examples, two different feature structures use the
    variable ?x to require that two values are equal. The values assigned to
    ?x are consistent within each structure, but would be inconsistent if every
    ?x had to have the same value.

    >>> f1 = yaml.load('''
    ... a: 1
    ... b: 1
    ... c: ?x
    ... d: ?x
    ... ''')
    >>> f2 = yaml.load('''
    ... a: ?x
    ... b: ?x
    ... c: 2
    ... d: 2
    ... ''')
    >>> bindings1 = {}
    >>> bindings2 = {}
    >>> print show(unify(f1, f2, bindings1, bindings2))
    a: 1
    b: 1
    c: 2
    d: 2
    
    >>> print bindings1
    {'x': 2}
    
    >>> print bindings2
    {'x': 1}

    Feature structures can involve _reentrant_ values, where multiple feature
    paths lead to the same value. This is represented by the features having
    the same Python object as a value. (This kind of identity can be tested
    using the C{is} operator.)
    
    Unification preserves the properties of reentrance. So if a reentrant value
    is changed by unification, it is changed everywhere it occurs, and it is
    still reentrant. Reentrant features can even form cycles; these
    cycles can now be printed through the current YAML library.

    >>> f1 = yaml.load('''
    ... A: &1                # &1 defines a reference in YAML...
    ...   B: b
    ... E:
    ...   F: *1              # and *1 uses the previously defined reference.
    ... ''')
    >>> f1['E']['F']['B']
    'b'
    >>> f1['A'] is f1['E']['F']
    True
    >>> f2 = yaml.load('''
    ... A:
    ...   C: c
    ... E:
    ...   F:
    ...     D: d
    ... ''')
    >>> f3 = unify(f1, f2)
    >>> print show(f3)
    A: &id001
      B: b
      C: c
      D: d
    E:
      F: *id001
    >>> f3['A'] is f3['E']['F']    # Showing that the reentrance still holds.
    True
    
    This unification creates a cycle:
    >>> f1 = yaml.load('''
    ... F: &1 {}
    ... G: *1
    ... ''')
    >>> f2 = yaml.load('''
    ... F:
    ...   H: &2 {}
    ... G: *2
    ... ''')
    >>> f3 = unify(f1, f2)
    >>> print f3
    {'G': {'H': {...}}, 'F': {'H': {...}}}
    >>> print f3['F'] is f3['G']
    True
    >>> print f3['F'] is f3['G']['H']
    True
    >>> print f3['F'] is f3['G']['H']['H']
    True

    A cycle can also be created using variables instead of reentrance.
    Here we supply a single set of bindings, so that it is used on both sides
    of the unification, making ?x mean the same thing in both feature
    structures.
    
    >>> f1 = yaml.load('''
    ... F:
    ...   H: ?x
    ... ''')
    >>> f2 = yaml.load('''
    ... F: ?x
    ... ''')
    >>> f3 = unify(f1, f2, {})
    >>> print f3
    {'F': {'H': {...}}}
    >>> print f3['F'] is f3['F']['H']
    True
    >>> print f3['F'] is f3['F']['H']['H']
    True

    Two sets of bindings can be provided because the variable names on each
    side of the unification may be unrelated. An example involves unifying the
    following two structures, which each require that two values are
    equivalent, and happen to both use ?x to express that requirement.

    >>> f1 = yaml.load('''
    ... a: 1
    ... b: 1
    ... c: ?x
    ... d: ?x
    ... ''')
    >>> f2 = yaml.load('''
    ... a: ?x
    ... b: ?x
    ... c: 2
    ... d: 2
    ... ''')
    >>> bindings1 = {}
    >>> bindings2 = {}
    >>> # We could avoid defining two empty dictionaries by simply using the
    >>> # defaults, with unify(f1, f2) -- but we want to be able to examine
    >>> # the bindings afterward.
    >>> print show(unify(f1, f2, bindings1, bindings2))
    a: 1
    b: 1
    c: 2
    d: 2
    >>> print bindings1
    {'x': 2}
    >>> print bindings2
    {'x': 1}

    If a variable is unified with another variable, the two variables are
    _aliased_ to each other; they share the same value, similarly to reentrant
    feature structures. This is represented in a set of bindings as one
    variable having the other as its value.
    >>> f1 = yaml.load('''
    ... a: ?x
    ... b: ?x
    ... ''')
    >>> f2 = yaml.load('''
    ... b: ?y
    ... c: ?y
    ... ''')
    >>> bindings = {}
    >>> print show(unify(f1, f2, bindings))
    a: &id001 ?y
    b: *id001
    c: *id001
    >>> print bindings
    {'x': ?y}

    Reusing the same variable bindings ensures that appropriate bindings are
    made after the fact:
    >>> bindings = {}
    >>> f1 = {'a': Variable('x')}
    >>> f2 = unify(f1, {'a': {}}, bindings)
    >>> f3 = unify(f2, {'b': Variable('x')}, bindings)
    >>> print show(f3)
    a: &id001 {}
    b: *id001
    >>> print bindings
    {'x': {}}

    @param feature1: The first object to be unified.
    @type feature1: C{object} (probably a mapping)
    @param feature2: The second object to be unified.
    @type feature2: C{object} (probably a mapping)
    @param bindings1: The variable bindings associated with the first object.
    @type bindings1: C{dict} or None
    @param bindings2: The variable bindings associated with the second object,
      if these are distinct from C{bindings1}.
    @type bindings2: C{dict} or None
    @return: The result of unifying the two objects.
    @rtype: C{object} (probably a mapping)
    """

    if fail is None:
        def failerror(f1, f2):
            raise UnificationFailure
        fail = failerror
        
    if bindings1 is None and bindings2 is None:
        bindings1 = {}
        bindings2 = {}
    else:
        if bindings1 is None: bindings1 = {}
        if bindings2 is None: bindings2 = bindings1
    
    # Make copies of the two structures (since the unification algorithm is
    # destructive). Use the same memo, to preserve reentrance links between
    # them.
    copymemo = {}
    copy1 = _copy_and_bind(feature1, bindings1, copymemo)
    copy2 = _copy_and_bind(feature2, bindings2, copymemo)
    # Preserve links between bound variables and the two feature structures.
    for b in (bindings1, bindings2):
        for (vname, value) in b.items():
            value_id = id(value)
            if value_id in copymemo:
                b[vname] = copymemo[value_id]

    # Go on to doing the unification.
    unified = _destructively_unify(copy1, copy2, bindings1, bindings2, {}, fail)

    _apply_forwards_to_bindings(bindings1)
    _apply_forwards_to_bindings(bindings2)
    _apply_forwards(unified, {})
    unified = _lookup_values(unified, {}, remove=False)
    _lookup_values(bindings1, {}, remove=True)
    _lookup_values(bindings2, {}, remove=True)

    return unified

def _destructively_unify(feature1, feature2, bindings1, bindings2, memo, fail):
    """
    Attempt to unify C{self} and C{other} by modifying them
    in-place.  If the unification succeeds, then C{self} will
    contain the unified value, and the value of C{other} is
    undefined.  If the unification fails, then a
    UnificationFailure is raised, and the values of C{self}
    and C{other} are undefined.
    """
    if memo.has_key((id(feature1), id(feature2))):
        return memo[id(feature1), id(feature2)]
    unified = _do_unify(feature1, feature2, bindings1, bindings2, memo, fail)
    memo[id(feature1), id(feature2)] = unified
    return unified

def _do_unify(feature1, feature2, bindings1, bindings2, memo, fail):
    """
    Do the actual work of _destructively_unify when the result isn't memoized.
    """

    # Trivial cases.
    if feature1 is None: return feature2
    if feature2 is None: return feature1
    if feature1 is feature2: return feature1
    
    # Deal with variables by binding them to the other value.
    if isinstance(feature1, Variable):
        if isinstance(feature2, Variable):
            # If both objects are variables, forward one to the other. This
            # has the effect of unifying the variables.
            return feature1.forwardTo(feature2, bindings1, bindings2)
        else:
            feature1.bindValue(feature2, bindings1, bindings2)
            return feature1
    if isinstance(feature2, Variable):
        feature2.bindValue(feature1, bindings2, bindings1)
        return feature2
    
    # If it's not a mapping or variable, it's a base object, so we just
    # compare for equality.
    if not isMapping(feature1):
        if feature1 == feature2: return feature1
        else: 
            return fail(feature1, feature2)
    if not isMapping(feature2):
        return fail(feature1, feature2)
    
    # At this point, we know they're both mappings.
    # Do the destructive part of unification.

    while feature2.has_key(_FORWARD): feature2 = feature2[_FORWARD]
    feature2[_FORWARD] = feature1
    for (fname, val2) in feature2.items():
        if fname == _FORWARD: continue
        val1 = feature1.get(fname)
        feature1[fname] = _destructively_unify(val1, val2, bindings1,
        bindings2, memo, fail)
    return feature1

def _apply_forwards(feature, visited):
    """
    Replace any feature structure that has a forward pointer with
    the target of its forward pointer (to preserve reentrance).
    """
    if not isMapping(feature): return
    if visited.has_key(id(feature)): return
    visited[id(feature)] = True

    for fname, fval in feature.items():
        if isMapping(fval):
            while fval.has_key(_FORWARD):
                fval = fval[_FORWARD]
                feature[fname] = fval
            _apply_forwards(fval, visited)

def _lookup_values(mapping, visited, remove=False):
    """
    The unification procedure creates _bound variables_, which are Variable
    objects that have been assigned a value. Bound variables are not useful
    in the end result, however, so they should be replaced by their values.

    This procedure takes a mapping, which may be a feature structure or a
    binding dictionary, and replaces bound variables with their values.
    
    If the dictionary is a binding dictionary, then 'remove' should be set to
    True. This ensures that unbound, unaliased variables are removed from the
    dictionary. If the variable name 'x' is mapped to the unbound variable ?x,
    then, it should be removed. This is not done with features, because a
    feature named 'x' can of course have a variable ?x as its value.
    """
    if isinstance(mapping, Variable):
        # Because it's possible to unify bare variables, we need to gracefully
        # accept a variable in place of a dictionary, and return a result that
        # is consistent with that variable being inside a dictionary.
        #
        # We can't remove a variable from itself, so we ignore 'remove'.
        var = mapping
        if var.value() is not None:
            return var.value()
        else:
            return var.forwarded_self()
    if not isMapping(mapping): return mapping
    if visited.has_key(id(mapping)): return mapping
    visited[id(mapping)] = True

    for fname, fval in mapping.items():
        if isMapping(fval):
            _lookup_values(fval, visited)
        elif isinstance(fval, Variable):
            if fval.value() is not None:
                mapping[fname] = fval.value()
                if isMapping(mapping[fname]):
                    _lookup_values(mapping[fname], visited)
            else:
                newval = fval.forwarded_self()
                if remove and newval.name() == fname:
                    del mapping[fname]
                else:
                    mapping[fname] = newval
    return mapping

def _apply_forwards_to_bindings(bindings):
    """
    Replace any feature structures that have been forwarded by their new
    identities.
    """
    for (key, value) in bindings.items():
        if isMapping(value) and value.has_key(_FORWARD):
            while value.has_key(_FORWARD):
                value = value[_FORWARD]
            bindings[key] = value

def test():
    "Run unit tests on unification."
    import doctest
    doctest.testmod()

if __name__ == "__main__":
    test()


########NEW FILE########
__FILENAME__ = fsa
# Natural Language Toolkit: Finite State Automata
#
# Copyright (C) 2001-2006 NLTK Project
# Authors: Steven Bird <sb@ldc.upenn.edu>
#          Rob Speer <rspeer@mit.edu>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

"""
A module for finite state automata. 
Operations are based on Aho, Sethi & Ullman (1986) Chapter 3.
"""

from nltk import tokenize, cfg, Tree
from nltk.parse import pchart
import yaml

epsilon = None

# some helper functions

# TODO - check that parse was complete, and report error otherwise

class FSA(yaml.YAMLObject):
    """
    A class for finite state automata. In general, it represents
    nondetermnistic finite state automata, with DFAs being a special case.
    """
    yaml_tag = '!FSA'
    def __init__(self, sigma='', transitions=None, start=0, finals=None):
        """Set up the FSA.

        @param sigma: the alphabet of the FSA
        @type sigma: sequence
        @param transitions: A dictionary representing the states and
        transitions in the FSA. The keys are state identifiers (any hashable
        object), and the values are dictionaries that map input symbols to the
        sets of states they lead to.
        @type transitions: dict
        @param start: The identifier of the start state
        @type start: hashable object
        @param finals: The identifiers of the accept states
        @type finals: sequence
        """
        self._transitions = transitions or {}
        self._start = start
        self._reverse = {}
        self._build_reverse_transitions()
        if finals: self._finals = set(finals)
        else: self._finals = set([0])
        self._sigma = set(sigma)
        assert isinstance(self._transitions, dict)
        self._next_state_num = 0

    def _build_reverse_transitions(self):
        for state in self._transitions:
            self._reverse.setdefault(state, {})
        for (state, symbol, target) in self.generate_transitions():
            self._add_transition(self._reverse, target, symbol, state)

    def generate_transitions(self):
        """
        A generator that yields each transition arrow in the FSA in the form
        (source, label, target).
        """
        for (state, map) in self._transitions.items():
            for (symbol, targets) in map.items():
                for target in targets:
                    yield (state, symbol, target)

    def labels(self, s1, s2):
        """
        A generator for all possible labels taking state s1 to state s2.
        """
        map = self._transitions.get(s1, {})
        for (symbol, targets) in map.items():
            if s2 in targets: yield symbol
    
    def sigma(self):
        "The alphabet of the FSA."
        return self._sigma
    alphabet = sigma

    def check_in_sigma(self, label):
        "Check whether a given object is in the alphabet."
        if label and label not in self._sigma:
            raise ValueError('Label "%s" not in alphabet: %s' % (label, str(self._sigma)))
    
    def __len__(self):
        "The number of states in the FSA."
        return len(self._transitions)
    
    def new_state(self):
        """
        Add a new state to the FSA.
        @returns: the ID of the new state (a sequentially-assigned number).
        @rtype: int
        """
        while self._next_state_num in self._transitions:
            self._next_state_num += 1
        self._transitions[self._next_state_num] = {}
        self._reverse[self._next_state_num] = {}
        return self._next_state_num

    def add_state(self, name):
        self._transitions[name] = {}
        self._reverse[name] = {}
        return name

    def start(self):
        """
        @returns: the ID of the FSA's start state.
        """
        return self._start

    def finals(self):
        """
        @returns: the IDs of all accept states.
        @rtype: set
        """
        # was a tuple before
        return self._finals
    
    def nonfinals(self):
        """
        @returns: the IDs of all accept states.
        @rtype: set
        """
        # was a tuple before
        return set(self.states()).difference(self._finals)

    def states(self):
        """
        @returns: a list of all states in the FSA.
        @rtype: list
        """
        return self._transitions.keys()
    
    def add_final(self, state):
        """
        Make a state into an accept state.
        """
        self._finals.add(state)

    def delete_final(self, state):
        """
        Make an accept state no longer be an accept state.
        """
        self._finals = self._finals.difference(set([state]))
#        del self._finals[state]

    def set_final(self, states):
        """
        Set the list of accept states.
        """
        self._finals = set(states)

    def set_start(self, start):
        """
        Set the start state of the FSA.
        """
        self._start = start
    
    def in_finals(self, list):
        """
        Check whether a sequence contains any final states.
        """
        return [state for state in list
                if state in self.finals()] != []

    def insert_safe(self, s1, label, s2):
        if s1 not in self.states():
            self.add_state(s1)
        if s2 not in self.states():
            self.add_state(s2)
        self.insert(s1, label, s2)

    def insert(self, s1, label, s2):
        """
        Add a new transition to the FSA.

        @param s1: the source of the transition
        @param label: the element of the alphabet that labels the transition
        @param s2: the destination of the transition
        """
        if s1 not in self.states():
            raise ValueError, "State %s does not exist in %s" % (s1,
            self.states())
        if s2 not in self.states():
            raise ValueError, "State %s does not exist in %s" % (s2,
            self.states())
        self._add_transition(self._transitions, s1, label, s2)
        self._add_transition(self._reverse, s2, label, s1)

    def _add_transition(self, map, s1, label, s2):
        mapping = map[s1]
        targets = mapping.setdefault(label, [])
        targets.append(s2)

    def _del_transition(self, map, s1, label, s2):
        mapping = map[s1]
        targets = mapping.setdefault(label, [])
        targets.remove(s2)
        if len(targets) == 0: del mapping[label]

    def delete(self, s1, label, s2):
        """
        Removes a transition from the FSA.

        @param s1: the source of the transition
        @param label: the element of the alphabet that labels the transition
        @param s2: the destination of the transition
        """
        if s1 not in self.states():
            raise ValueError, "State %s does not exist" % s1
        if s2 not in self.states():
            raise ValueError, "State %s does not exist" % s1
        self._del_transition(self._transitions, s1, label, s2)
        self._del_transition(self._reverse, s2, label, s1)

    def delete_state(self, state):
        "Removes a state and all its transitions from the FSA."
        if state not in self.states():
            raise ValueError, "State %s does not exist" % state
        for (s1, label, s2) in self.incident_transitions(state):
            self.delete(s1, label, s2)
        del self._transitions[state]
        del self._reverse[state]

    def incident_transitions(self, state):
        """
        @returns: a set of transitions into or out of a state.
        @rtype: set
        """
        result = set()
        forward = self._transitions[state]
        backward = self._reverse[state]
        for label, targets in forward.items():
            for target in targets:
                result.add((state, label, target))
        for label, targets in backward.items():
            for target in targets:
                result.add((target, label, state))
        return result

    def relabel_state(self, old, new):
        """
        Assigns a state a new identifier.
        """
        if old not in self.states():
            raise ValueError, "State %s does not exist" % old
        if new in self.states():
            raise ValueError, "State %s already exists" % new
        changes = []
        for (s1, symbol, s2) in self.generate_transitions():
            if s1 == old and s2 == old:
                changes.append((s1, symbol, s2, new, symbol, new))
            elif s1 == old:
                changes.append((s1, symbol, s2, new, symbol, s2))
            elif s2 == old:
                changes.append((s1, symbol, s2, s1, symbol, new))
        for (leftstate, symbol, rightstate, newleft, newsym, newright)\
        in changes:
            print leftstate, symbol, rightstate, newleft, newsym, newright
            self.delete(leftstate, symbol, rightstate)
            self.insert_safe(newleft, newsym, newright)
        del self._transitions[old]
        del self._reverse[old]

    def next(self, state, symbol):
        "The set of states reached from a certain state via a given symbol."
        return self.e_closure(self._transitions[state].get(symbol, set()))
    nextStates = next
    
    def move(self, states, symbol):
        "The set of states reached from a set of states via a given symbol."
        result = set()
        for state in states:
            result = result.union(self.next(state, symbol))
        return self.e_closure(result)

    def is_deterministic(self):
        """
        Return whether this is a DFA
        (every symbol leads from a state to at most one target state).
        """
        for map in self._transitions.values():
            for targets in map.values():
                if len(targets) > 1: return False
        return True
    
    def nextState(self, state, symbol):
        """
        The single state reached from a state via a given symbol.
        If there is more than one such state, raises a ValueError.
        If there is no such state, returns None.
        """
        next = self.next(state, symbol)
        if len(next) > 1:
            raise ValueError, "This FSA is nondeterministic -- use nextStates instead."
        elif len(next) == 1: return list(next)[0]
        else: return None

    def forward_traverse(self, state):
        "All states reachable by following transitions from a given state."
        result = set()
        for (symbol, targets) in self._transitions[state].items():
            result = result.union(targets)
        return result

    def reverse_traverse(self, state):
        """All states from which a given state is reachable by following
        transitions."""
        result = set()
        for (symbol, targets) in self._reverse[state].items():
            result = result.union(targets)
        return result
    
    def _forward_accessible(self, s1, visited):
        for s2 in self.forward_traverse(s1):
            if not s2 in visited:
                visited.add(s2)
                self._forward_accessible(s2, visited)
        return visited
                
    def _reverse_accessible(self, s1, visited):
        for s2 in self.reverse_traverse(s1):
            if not s2 in visited:
                visited.add(s2)
                self._reverse_accessible(s2, visited)
        return visited
        
    # delete inaccessible nodes and unused transitions
    def prune(self):
        """
        Modifies an FSA to remove inaccessible states and unused transitions.
        """
        acc = self.accessible()
        for state in self.states():
            if state not in acc:
               self.delete_state(state)
            else:
                self._clean_map(self._transitions[state])
                self._clean_map(self._reverse[state])

    def _clean_map(self, map):
        for (key, value) in map.items():
            if len(value) == 0:
                del map[key]

    # mark accessible nodes
    def accessible(self):
        acc = set()
        for final in self.finals():
            reverse_acc = set([final])
            self._reverse_accessible(final, reverse_acc)
            acc = acc.union(reverse_acc)

        forward_acc = set([self.start()])
        self._forward_accessible(self.start(), forward_acc)

        acc = acc.intersection(forward_acc)
        return acc
    
    def e_closure(self, states):
        """
        Given a set of states, return the set of states reachable from
        those states by following epsilon transitions.

        @param states: the initial set of states
        @type states: sequence
        @returns: a superset of the given states, reachable by epsilon
        transitions
        @rtype: set
        """
        stack = list(states)
        closure = list(states)
        while stack:
            s1 = stack.pop()
            for s2 in self.next(s1, epsilon):
                if s2 not in closure:
                    closure.append(s2)
                    stack.append(s2)
        return set(closure)
    
    # return the corresponding DFA using subset construction (ASU p118)
    # NB representation of (a*) still isn't minimal; should have 1 state not 2
    def dfa(self):
        "Return a DFA that is equivalent to this FSA."
        dfa = FSA(self.sigma())
        dfa_initial = dfa.start()
        nfa_initial = tuple(self.e_closure((self.start(),)))
        map = {}
        map[dfa_initial] = nfa_initial
        map[nfa_initial] = dfa_initial
        if nfa_initial in self.finals():
            dfa.add_final(dfa_initial)
        unmarked = [dfa_initial]
        marked = []
        while unmarked:
            dfa_state = unmarked.pop()
            marked.append(dfa_state)
            # is a final state accessible via epsilon transitions?
            if self.in_finals(self.e_closure(map[dfa_state])):
                dfa.add_final(dfa_state)
            for label in self.sigma():
                nfa_next = tuple(self.e_closure(self.move(map[dfa_state],
                label)))
                if map.has_key(nfa_next):
                    dfa_next = map[nfa_next]
                else:
                    dfa_next = dfa.new_state()
                    map[dfa_next] = nfa_next
                    map[nfa_next] = dfa_next
                    if self.in_finals(nfa_next):
                        dfa.add_final(dfa_next)
                    unmarked.append(dfa_next)
                dfa.insert(dfa_state, label, dfa_next)
        return dfa
    
    def generate(self, maxlen, state=0, prefix=""):
        "Generate all accepting sequences of length at most maxlen."
        if maxlen > 0:
            if state in self._finals:
                print prefix
            for (s1, labels, s2) in self.outgoing_transitions(state):
                for label in labels():
                    self.generate(maxlen-1, s2, prefix+label)

    def pp(self):
        """
        Print a representation of this FSA (in human-readable YAML format).
        """
        print yaml.dump(self)
    
    @classmethod
    def from_yaml(cls, loader, node):
        map = loader.construct_mapping(node)
        result = cls(map.get('sigma', []), {}, map.get('finals', []))
        for (s1, map1) in map['transitions'].items():
            for (symbol, targets) in map1.items():
                for s2 in targets:
                    result.insert(s1, symbol, s2)
        return result
    
    @classmethod
    def to_yaml(cls, dumper, data):
        sigma = data.sigma()
        transitions = {}
        for (s1, symbol, s2) in data.generate_transitions():
            map1 = transitions.setdefault(s1, {})
            map2 = map1.setdefault(symbol, [])
            map2.append(s2)
        try: sigma = "".join(sigma)
        except: sigma = list(sigma)
        node = dumper.represent_mapping(cls.yaml_tag, dict(
            sigma = sigma,
            finals = list(data.finals()),
            start = data._start,
            transitions = transitions))
        return node

    def show_pygraph(self, title='FSA', outfile=None, labels=True, root=None):
        from pygraph import pygraph, tkgraphview
        graph = pygraph.Grapher('directed')

        for state in self.states():
            color = '#eee'
            if state in self.finals():
                shape = 'oval'
            else:
                shape = 'rect'
            if state == self.start():
                color = '#afa'
            term = ''
            if state == self.start(): term = 'start'
            elif state == 'End': term = 'end'
            if state in [0, '0', 'reject', 'Reject']: color='#e99'
            
            graph.addNode(state, state, color, shape, term)

        #for source, trans in self._transitions.items():
        for source, label, target in self.generate_transitions():
            if not labels: label = ''
            graph.addEdge(source, target, label, color='black', dup=False)
        
        if outfile is None: outfile = title
        
        return tkgraphview.tkGraphView(graph, title, outfile, root=root,
        startTk=(not root))
        
    def __str__(self):
        return yaml.dump(self)

### FUNCTIONS TO BUILD FSA FROM REGEXP

# the grammar of regular expressions
# (probabilities ensure that unary operators
# have stronger associativity than juxtaposition)

def grammar(terminals):
    (S, Expr, Star, Plus, Qmk, Paren) = [cfg.Nonterminal(s) for s in 'SE*+?(']
    rules = [cfg.WeightedProduction(Expr, [Star], prob=0.2),
             cfg.WeightedProduction(Expr, [Plus], prob=0.2),
             cfg.WeightedProduction(Expr, [Qmk], prob=0.2),
             cfg.WeightedProduction(Expr, [Paren], prob=0.2),
             cfg.WeightedProduction(S, [Expr], prob=0.5),
             cfg.WeightedProduction(S, [S, Expr], prob=0.5),
             cfg.WeightedProduction(Star, [Expr, '*'], prob=1),
             cfg.WeightedProduction(Plus, [Expr, '+'], prob=1),
             cfg.WeightedProduction(Qmk, [Expr, '?'], prob=1),
             cfg.WeightedProduction(Paren, ['(', S, ')'], prob=1)]

    prob_term = 0.2/len(terminals) # divide remaining pr. mass
    for terminal in terminals:
        rules.append(cfg.WeightedProduction(Expr, [terminal], prob=prob_term))

    return cfg.WeightedGrammar(S, rules)

_parser = pchart.InsideParse(grammar('abcde'))

# create NFA from regexp (Thompson's construction)
# assumes unique start and final states

def re2nfa(fsa, re):
    tokens = tokenize.regexp(re, pattern=r'.')
    tree = _parser.parse(tokens)
    if tree is None: raise ValueError('Bad Regexp')
    state = re2nfa_build(fsa, fsa.start(), tree)
    fsa.set_final([state])
#        fsa.minimize()

def re2nfa_build(fsa, node, tree):
    # Terminals.
    if not isinstance(tree, Tree):
        return re2nfa_char(fsa, node, tree)
    elif len(tree) == 1:
        return re2nfa_build(fsa, node, tree[0])
    elif tree.node == '(':
        return re2nfa_build(fsa, node, tree[1])
    elif tree.node == '*': return re2nfa_star(fsa, node, tree[0])
    elif tree.node == '+': return re2nfa_plus(fsa, node, tree[0])
    elif tree.node == '?': return re2nfa_qmk(fsa, node, tree[0])
    else:
        node = re2nfa_build(fsa, node, tree[0])
        return re2nfa_build(fsa, node, tree[1])

def re2nfa_char(fsa, node, char):
    new = fsa.new_state()
    fsa.insert(node, char, new)
    return new

def re2nfa_qmk(fsa, node, tree):
    node1 = fsa.new_state()
    node2 = re2nfa_build(fsa, node1, tree)
    node3 = fsa.new_state()
    fsa.insert(node, epsilon, node1)
    fsa.insert(node, epsilon, node3)
    fsa.insert(node2, epsilon, node3)
    return node3

def re2nfa_plus(fsa, node, tree):
    node1 = re2nfa_build(fsa, node, tree[0])
    fsa.insert(node1, epsilon, node)
    return node1

def re2nfa_star(fsa, node, tree):
    node1 = fsa.new_state()
    node2 = re2nfa_build(fsa, node1, tree)
    node3 = fsa.new_state()
    fsa.insert(node, epsilon, node1)
    fsa.insert(node, epsilon, node3)
    fsa.insert(node2, epsilon, node1)
    fsa.insert(node2, epsilon, node3)
    return node3

#################################################################
# Demonstration
#################################################################

def demo():
    """
    A demonstration showing how FSAs can be created and used.
    """
    # Define an alphabet.
    alphabet = "abcd"

    # Create a new FSA.
    fsa = FSA(alphabet)
    
    # Use a regular expression to initialize the FSA.
    re = 'abcd'
    print 'Regular Expression:', re
    re2nfa(fsa, re)
    print "NFA:"
    fsa.pp()

    # Convert the (nondeterministic) FSA to a deterministic FSA.
    dfa = fsa.dfa()
    print "DFA:"
    dfa.pp()

    # Prune the DFA
    dfa.prune()
    print "PRUNED DFA:"
    dfa.pp()

    # Use the FSA to generate all strings of length less than 3
    # (broken)
    #fsa.generate(3)

if __name__ == '__main__': demo()

########NEW FILE########
__FILENAME__ = kimmo
# pykimmo 3.0.0 -- a two-level morphology tool for nltk 1.7
# by Rob Speer (rspeer@mit.edu)
# based on code from Carl de Marcken, Beracah Yankama, and Rob Speer

from rules import KimmoArrowRule, KimmoFSARule
from pairs import KimmoPair, sort_subsets
from morphology import *
from fsa import FSA
import yaml

def _pairify(state):
    newstate = {}
    for label, targets in state.items():
        newstate[KimmoPair.make(label)] = targets
    return newstate


class KimmoRuleSet(yaml.YAMLObject):
    """
    An object that represents the morphological rules for a language.
    
    The KimmoRuleSet stores a list of rules which must all succeed when they
    process a given string. These rules can be used for generating a surface
    form from a lexical form, or recognizing a lexical form from a surface
    form.
    """
    yaml_tag = '!KimmoRuleSet'
    def __init__(self, subsets, defaults, rules, morphology=None, null='0', boundary='#'):
        """
        Creates a KimmoRuleSet. You may not want to do this directly, but use
        KimmoRuleSet.load to load one from a YAML file.

        A KimmoRuleSet takes these parameters:
        subsets: a dictionary mapping strings to lists of strings. The strings
          in the map become subsets representing all of the strings in the
          list.
        defaults: a list of KimmoPairs that can appear without being
          specifically mentioned by a rule.
        rules: a list of KimmoFSARules or KimmoArrowRules that define the
          two-level morphology rules.
        morphology: a KimmoMorphology object that defines a lexicon of word
          roots and affixes.
        null: the symbol representing the empty string in rules.
        boundary: the symbol that will always appear at the end of lexical and
          surface forms.
        """
        self.debug = False
        self._rules = list(rules)
        self._pair_alphabet = set()
        self._subsets = {}
        self._null = null
        self._boundary = boundary
        self._subsets = subsets
        self._morphology = morphology
        
        for pair in defaults:
            # defaults shouldn't contain subsets
            if self.is_subset(pair.input()) or self.is_subset(pair.output()):
                raise ValueError('default ' + str(pair) + ' contains subset')
            self._pair_alphabet.add( pair )
        
        for r in self.rules():
            for kp in r.pairs():
                if (not (self.is_subset(kp.input()) or self.is_subset(kp.output()))):
                    self._pair_alphabet.add( kp )

    def rules(self):
        "The list of rules in this ruleset."
        return self._rules
    def subsets(self):
        "The dictionary defining subsets of characters of the language."
        return self._subsets
    def is_subset(self, key):
        "Is this string a subset representing other strings?"
        return key[0] == '~' or key in self.subsets()
    def null(self):
        "The null symbol for this ruleset."
        return self._null
    def morphology(self):
        """The morphological lexicon (as a KimmoMorphology). Could be None, if
        the ruleset is only used for generation.
        """
        return self._morphology
    
    def _pairtext(self, char):
        if char == self._null: return ''
        else: return char
    
    def _generate(self, pairs, state_list, morphology_state=None, word='',
    lexical=None, surface=None, features='', log=None):
        if morphology_state:
            morph = self._morphology
            morphed = False
            for state, feat in morph.next_states(morphology_state, word):
                if feat is not None:
                    newfeat = combine_features(features, feat)
                else: newfeat = features
                for result in self._generate(pairs, state_list,
                state, '', lexical, surface, newfeat, log):
                    yield result
                    morphed = True
            if morphed: return
            lexical_chars = list(morph.valid_lexical(morphology_state,
            word, self._pair_alphabet)) + list(self._null)
        else: lexical_chars = None
        
        if lexical == '' or surface == '':
            if morphology_state is None or morphology_state.lower() == 'end':
                # check that all rules are in accepting states
                for r in range(len(self._rules)):
                    rule = self._rules[r]
                    state = state_list[r]
                    if state not in rule.fsa().finals():
                        return
                if log:
                    log.succeed(pairs)
                yield pairs, features
                return
            
        next_pairs = [p for p in self._pair_alphabet if
          (lexical is None or startswith(lexical, self._pairtext(p.input()))) and
          (surface is None or startswith(surface, self._pairtext(p.output())))]
        for pair in next_pairs:
            if pair.input() == self._null and pair.output() == self._null:
                continue
            if lexical_chars is not None and pair.input() not in lexical_chars:
                continue
            new_states = state_list[:]
            for r in range(len(self._rules)):
                rule = self._rules[r]
                state = state_list[r]
                next_state = self._advance_rule(rule, state, pair)
                new_states[r] = next_state
            
            newword = word + self._pairtext(pair.input())

            if log:
                log.step(pairs, pair, self._rules, state_list, new_states,
                morphology_state, newword)
            fail = False
            for new_state in new_states:
                if new_state is None or str(new_state) == '0'\
                or str(new_state) == 'reject':
                    fail = True
                    break
            if fail: continue
            newlex, newsurf = lexical, surface
            if lexical: newlex = lexical[len(self._pairtext(pair.input())):]
            if surface: newsurf = surface[len(self._pairtext(pair.output())):]
            for result in self._generate(pairs+[pair], new_states,
            morphology_state, newword, newlex, newsurf, features, log):
                yield result
        
    def generate(self, lexical, log=None):
        """
        Given a lexical form, return all possible surface forms that fit
        these rules.

        Optionally, a 'log' object such as TextTrace(1) can be provided; this
        object will display to the user all the steps of the Kimmo algorithm.
        """
        if log: log.reset()
        if not lexical.endswith(self._boundary):
            lexical += self._boundary
        got = self._generate([], [rule.fsa().start() for rule in
        self._rules], lexical=lexical, log=log)
        results = []
        for (pairs, features) in got:
            results.append(''.join(self._pairtext(pair.output()).strip(self._boundary) for pair in pairs))
        return results
    
    def recognize(self, surface, log=None):
        """
        Given a surface form, return all possible lexical forms that fit
        these rules. Because the components of a lexical form can include
        features such as the grammatical part of speech, each surface form
        is returned as a 2-tuple of (surface text, features).

        Optionally, a 'log' object such as TextTrace(1) can be provided; this
        object will display to the user all the steps of the Kimmo algorithm.
        """
        if log: log.reset()
        if not surface.endswith(self._boundary):
            surface += self._boundary
        got = self._generate([], [rule.fsa().start() for rule in
        self._rules], morphology_state='Begin', surface=surface, log=log)
        results = []
        for (pairs, features) in got:
            results.append((''.join(self._pairtext(pair.input()).strip(self._boundary) for pair in pairs), features))
        return results

    def _advance_rule(self, rule, state, pair):
        trans = rule.fsa()._transitions[state]
        expected_pairs = sort_subsets(trans.keys(), self._subsets)
        for comppair in expected_pairs:
            if comppair.includes(pair, self._subsets):
                return rule.fsa().nextState(state, comppair)
        return None
    
    def _test_case(self, input, outputs, arrow, method):
        outputs.sort()
        if arrow == '<=':
            print '%s %s %s' % (', '.join(outputs), arrow, input)
        else:
            print '%s %s %s' % (input, arrow, ', '.join(outputs))
        value = method(input)
        if len(value) and isinstance(value[0], tuple):
            results = [v[0] for v in value]
        else: results = value
        results.sort()
        if outputs != results:
            print '  Failed: got %s' % (', '.join(results) or 'no results')
            return False
        else: return True
    
    def batch_test(self, filename):
        """
        Test a rule set by reading lines from a file.

        Each line contains one or more lexical forms on the left, and one or
        more surface forms on the right (separated by commas if there are more
        than one). In between, there is an arrow (=>, <=, or <=>), indicating
        whether recognition, generation, or both should be tested. Comments
        can be marked with ;.

        Each form should produce the exact list of forms on the other side of
        the arrow; if one is missing, or an extra one is produced, the test
        will fail.

        Examples of test lines:
          cat+s => cats             ; test generation only
          conoc+o <=> conozco       ; test generation and recognition
           <= conoco                ; this string should fail to be recognized
        """
        f = open(filename)
        try:
            for line in f:
                line = line[:line.find(';')].strip()
                if not line: continue
                arrow = None
                for arrow_to_try in ['<=>', '=>', '<=']:
                    if line.find(arrow_to_try) >= 0:
                        lexicals, surfaces = line.split(arrow_to_try)
                        arrow = arrow_to_try
                        break
                if arrow is None:
                    raise ValueError, "Can't find arrow in line: %s" % line
                lexicals = lexicals.strip().split(', ')
                surfaces = surfaces.strip().split(', ')
                if lexicals == ['']: lexicals = []
                if surfaces == ['']: surfaces = []
                if arrow == '=>' or arrow == '<=>':
                    outputs = surfaces
                    for input in lexicals:
                        self._test_case(input, outputs, '=>', self.generate)
                if arrow == '<=' or arrow == '<=>':
                    outputs = lexicals
                    for input in surfaces:
                        self._test_case(input, outputs, '<=', self.recognize)
        finally:
            f.close()
    
    @classmethod
    def from_yaml(cls, loader, node):
        """
        Loads a KimmoRuleSet from a parsed YAML node.
        """
        map = loader.construct_mapping(node)
        return cls.from_yaml_dict(map)
    
    @classmethod
    def load(cls, filename):
        """
        Loads a KimmoRuleSet from a YAML file.
        
        The YAML file should contain a dictionary, with the following keys:
          lexicon: the filename of the lexicon to load.
          subsets: a dictionary mapping subset characters to space-separated
            lists of symbols. One of these should usually be '@', mapping
            to the entire alphabet.
          defaults: a space-separated list of KimmoPairs that should be allowed
            without a rule explicitly mentioning them.
          null: the symbol that will be used to represent 'null' (usually '0').
          boundary: the symbol that represents the end of the word
            (usually '#').
          rules: a dictionary mapping rule names to YAML representations of
            those rules.
          
        A rule can take these forms:
        * a dictionary of states, where each state is a dictionary mapping
          input pairs to following states. The start state is named 'start',
          the state named 'reject' instantly rejects, and state names can be
          prefixed with the word 'rejecting' so that they reject if the machine
          ends in that state.

          i-y-spelling: 
            start:
              'i:y': step1
              '@': start
            rejecting step1:
              'e:0': step2
              '@': reject
            rejecting step2:
              '+:0': step3
              '@': reject
            rejecting step3:
              'i:i': start
              '@': reject

          
        * a block of text with a DFA table in it, of the form used by
          PC-KIMMO. The text should begin with a | so that YAML keeps your
          line breaks, and the next line should be 'FSA'. State 0 instantly
          rejects, and states with a period instead of a colon reject if the
          machine ends in that state.
          Examples:

          i-y-spelling: |        # this is the same rule as above
            FSA
                i  e  +  i      @
                y  0  0  i  @
            1:  2  1  1  1      1
            2.  0  3  0  0      0
            3.  0  0  4  0      0
            4.  0  0  0  1      0

          epenthesis: |
            FSA
               c h s Csib y + # 0 @
               c h s Csib i 0 # e @
            1: 2 1 4 3    3 1 1 0 1
            2: 2 3 3 3    3 1 1 0 1
            3: 2 1 3 3    3 5 1 0 1
            4: 2 3 3 3    3 5 1 0 1
            5: 2 1 2 2    2 1 1 6 1
            6. 0 0 7 0    0 0 0 0 0
            7. 0 0 0 0    0 1 1 0 0
          
        """
        f = open(filename)
        result = cls._from_yaml_dict(yaml.load(f))
        f.close()
        return result
    
    @classmethod
    def _from_yaml_dict(cls, map):
        lexicon = map.get('lexicon')
        if lexicon:
            lexicon = KimmoMorphology.load(lexicon)
        subsets = map['subsets']
        for key, value in subsets.items():
            if isinstance(value, basestring):
                subsets[key] = value.split()
        defaults = map['defaults']
        if isinstance(defaults, basestring):
            defaults = defaults.split()
        defaults = [KimmoPair.make(text) for text in defaults]
        ruledic = map['rules']
        rules = []
        for (name, rule) in ruledic.items():
            if isinstance(rule, dict):
                rules.append(KimmoFSARule.from_dfa_dict(name, rule, subsets))
            elif isinstance(rule, basestring):
                if rule.strip().startswith('FSA'):
                    rules.append(KimmoFSARule.parse_table(name, rule, subsets))
                else: rules.append(KimmoArrowRule(name, rule, subsets))
            else:
                raise ValueError, "Can't recognize the data structure in '%s' as a rule: %s" % (name, rule)
        return cls(subsets, defaults, rules, lexicon)
    
    def gui(self, startTk=True):
        import draw
        return draw.KimmoGUI(self, startTk)
    draw_graphs = gui

class TextTrace(object):
    """
    Supply a TextTrace object as the 'log' argument to KimmoRuleSet.generate or
    KimmoRuleSet.recognize, and it will output the steps it goes through
    on a text terminal.
    """
    def __init__(self, verbosity=1):
        """
        Creates a TextTrace. The 'verbosity' argument ranges from 1 to 3, and
        specifies how much text will be output to the screen.
        """
        self.verbosity = verbosity
    def reset(self): pass
    def step(self, pairs, curr, rules, prev_states, states,
    morphology_state, word):
        lexical = ''.join(p.input() for p in pairs)
        surface = ''.join(p.output() for p in pairs)
        indent = ' '*len(lexical)
        if self.verbosity > 2:
            print '%s%s<%s>' % (indent, lexical, curr.input())
            print '%s%s<%s>' % (indent, surface, curr.output())
            for rule, state1, state2 in zip(rules, prev_states, states):
                print '%s%s: %s => %s' % (indent, rule.name(), state1, state2)
            if morphology_state:
                print '%sMorphology: %r => %s' % (indent, word, morphology_state)
            print
        elif self.verbosity > 1:
            print '%s%s<%s>' % (indent, lexical, curr.input())
            print '%s%s<%s>' % (indent, surface, curr.output())
            z = zip(prev_states, states)
            if morphology_state:
                z.append((word, morphology_state))
            print indent + (" ".join('%s>%s' % (old, new) for old, new in z))
            blocked = []
            for rule, state in zip(rules, states):
                if str(state).lower() in ['0', 'reject']:
                    blocked.append(rule.name())
            if blocked:
                print '%s[blocked by %s]' % (indent, ", ".join(blocked))
            print
        else:
            print '%s%s<%s> | %s<%s>' % (indent, lexical, curr.input(),
              surface, curr.output()),
            if morphology_state:
                print '\t%r => %s' % (word, morphology_state),
            blocked = []
            for rule, state in zip(rules, states):
                if str(state).lower() in ['0', 'reject']:
                    blocked.append(rule.name())
            if blocked:
                print ' [blocked by %s]' % (", ".join(blocked)),
            print

    def succeed(self, pairs):
        lexical = ''.join(p.input() for p in pairs)
        surface = ''.join(p.output() for p in pairs)
        indent = ' '*len(lexical)

        print '%s%s' % (indent, lexical)
        print '%s%s' % (indent, surface)
        print '%sSUCCESS: %s <=> %s' % (indent, lexical, surface)
        print
        print

def load(filename):
    """
    Loads a ruleset from a file in YAML format.
    
    See KimmoRuleSet.load for a more detailed description.
    """
    return KimmoRuleSet.load(filename)

def guidemo():
    "An example of loading rules into the GUI."
    rules = load('turkish.yaml')
    rules.gui()

def main():
    """If a YAML file is specified on the command line, load it as a
    KimmoRuleSet in the GUI."""
    import sys
    if len(sys.argv) > 1:
        filename = sys.argv[1]
        k = load(filename)
        k.gui()

if __name__ == '__main__': main()

########NEW FILE########
__FILENAME__ = kimmotest
from kimmo import *
k = KimmoRuleSet.load('english.yaml')
print list(k.generate('`slip+ed', TextTrace(3)))
print list(k.recognize('slipped', TextTrace(1)))

########NEW FILE########
__FILENAME__ = morphology
from fsa import FSA
import yaml
from featurelite import unify

def startswith(stra, strb):
    return stra[:len(strb)] == strb

def endswith(stra, strb):
    return strb == '' or stra[-len(strb):] == strb

class YAMLwrapper(object):
    def __init__(self, yamlstr):
        self.yamlstr = yamlstr
        self._cache = None
    def value(self):
        if self._cache is not None: return self._cache
        self._cache = yaml.load(self.yamlstr)
        return self._cache

def combine_features(a, b):
    """
    Return an object that combines the feature labels a and b.
    
    For now, this only does string concatenation; it can be extended
    to unify 'featurelite' style dictionaries.
    """
    def override_features(a, b):
        return b

    if isinstance(a, YAMLwrapper): a = a.value()
    if isinstance(b, YAMLwrapper): b = b.value()
    if isinstance(a, str) and isinstance(b, str):
        return a+b
    else:
        d = {}
        vars = {}

        return unify(a, b, vars, fail=override_features)
    return '%s%s' % (a, b)

class KimmoMorphology(object):
    def __init__(self, fsa):
        self._fsa = fsa
    def fsa(self): return self._fsa
    def valid_lexical(self, state, word, alphabet):
        trans = self.fsa()._transitions[state]
        for label in trans.keys():
            if label is not None and startswith(label[0], word) and len(label[0]) > len(word):
                next = label[0][len(word):]
                for pair in alphabet:
                    if startswith(next, pair.input()): yield pair.input()
    def next_states(self, state, word):
        choices = self.fsa()._transitions[state]
        for (key, value) in choices.items():
            if key is None:
                if word == '':
                    for next in value: yield (next, None)
            else:
                if key[0] == word:
                    for next in value:
                        yield (next, key[1])
                    
    @staticmethod
    def load(filename):
        #import codecs
        #f = codecs.open(filename, encoding='utf-8')
        f = open(filename)
        result = KimmoMorphology.from_text(f.read())
        f.close()
        return result
    @staticmethod
    def from_text(text):
        fsa = FSA([], {}, 'Begin', ['End'])
        state = 'Begin'
        for line in text.split('\n'):
            line = line.strip()
            if not line or startswith(line, ';'): continue
            if line[-1] == ':':
                state = line[:-1]
            else:
                if endswith(line.split()[0], ':'):
                    parts = line.split()
                    name = parts[0][:-1]
                    next_states = parts[1:]
                    for next in next_states:
                        fsa.insert_safe(name, None, next)
                elif len(line.split()) > 2:
                    # this is a lexicon entry
                    word, next, features = line.split(None, 2)
                    if startswith(word, '"') or\
                    startswith(word, "'") and endswith(word, "'"):
                        word = eval(word)
                    if features:
                        if features == 'None': features = None
                        elif features[0] in '\'"{':
                            features = YAMLwrapper(features)
                    fsa.insert_safe(state, (word, features), next)
                elif len(line.split()) == 2:
                    word, next = line.split()
                    features = ''
                    if word == "''":
                        word = ''
                    fsa.insert_safe(state, (word, features), next)
                else:
                    print "Ignoring line in morphology: %r" % line
        return KimmoMorphology(fsa)

def demo():
    print KimmoMorphology.load('english.lex')

if __name__ == '__main__':
    demo()

########NEW FILE########
__FILENAME__ = pairs
def sort_subsets(pairs, subsets):
    def subset_size(pair):
        if pair.input() in subsets: size1 = len(subsets[pair.input()])
        else: size1 = 1
        if pair.output() in subsets: size2 = len(subsets[pair.output()])
        else: size2 = 1
        return (min(size1, size2), max(size1, size2))
        
    sort_order = [(subset_size(pair), pair) for pair in pairs]
    sort_order.sort()
    return [item[1] for item in sort_order]

class KimmoPair(object):
    """
    Input/Output character pair
    """
    def __init__(self, input_subset, output_subset):
        self._input = input_subset
        self._output = output_subset


    def input(self): return self._input
    def output(self): return self._output


    def __repr__(self):
        sI = self.input()
        sO = self.output()
        s = sI + ':' + sO
        return s

    def __cmp__(self, other):
        if type(self) != type(other): return -1
        return cmp( (self._input, self._output), (other.input(),
        other.output()))

    def __hash__(self):
        return hash( (self._input, self._output) )

    def includes(self, pair, subsets):
        return (self._matches(self.input(), pair.input(), subsets) and
                self._matches(self.output(), pair.output(), subsets))
    
    def matches(self, input, output, subsets, negatedOutputMatch=False):
        if not(self._matches(self.input(), input, subsets)): return False
        m = self._matches(self.output(), output, subsets)
        if negatedOutputMatch: return not(m)
        return m


    def _matches(self, me, terminal, subsets):
        if (me == terminal): return True
        if (me[0] == '~'):
            m = me[1:]
            if (m in subsets):
                return not(terminal in subsets[m])
            else:
                return False
        if (me in subsets):
            return terminal in subsets[me]
        else:
            return False
    
    @staticmethod
    def make(text):
        parts = text.split(':')
        if len(parts) == 1: return KimmoPair(text, text)
        elif len(parts) == 2: return KimmoPair(parts[0], parts[1])
        else: raise ValueError, "Bad format for pair: %s" % text


########NEW FILE########
__FILENAME__ = rules
from nltk.parse import Tree
from fsa import FSA
from nltk import tokenize
from pairs import KimmoPair, sort_subsets
from copy import deepcopy
import re, yaml

_kimmo_terminal_regexp    = '[a-zA-Z0-9\+\'\-\#\@\$\%\!\^\`\}\{]+' # \}\{\<\>\,\.\~ # (^|\s)?\*(\s|$) !!! * is already covered in the re tokenizer
_kimmo_terminal_regexp_fsa    = '[^:\s]+' # for FSA, only invalid chars are whitespace and :
                                          # '[a-zA-Z0-9\+\'\-\#\@\$\%\!\^\`\}\{\<\>\,\.\~\*]+'
_kimmo_terminal_regexp_ext= '~?' + _kimmo_terminal_regexp

_kimmo_defaults           = _kimmo_terminal_regexp + '|\:'
_kimmo_defaults_fsa       = _kimmo_terminal_regexp_fsa + '|\:'
_kimmo_rule               = _kimmo_terminal_regexp_ext + '|[\:\(\)\[\]\?\&\*\_]|<=>|==>|<==|/<='
_arrows = ['==>', '<=>', '<==', '/<=']


_special_tokens = ['(', ')', '[', ']', '*', '&', '_', ':']
_special_tokens.extend(_arrows)
_non_list_initial_special_tokens = [')', ']', '*', '&', '_', ':']
_non_list_initial_special_tokens.extend(_arrows)
epsilon = None

class KimmoFSARule(object):
    """
    A rule for two-level morphology, expressed as a deterministic finite
    automaton.
    """
    def __init__(self, name, fsa, subsets):
        self._name = name
        self._fsa = fsa
        self._pairs = set()
        self._subsets = subsets
        for (start, pair, finish) in self._fsa.generate_transitions():
            self._pairs.add(pair)

    def fsa(self): return self._fsa
    def pairs(self): return self._pairs
    def name(self): return self._name

    def show_pygraph(self, root=None):
        return self.fsa().show_pygraph(self.name(), root=root)
    
    def complete_fsa(self, fsa, fail_state=None):
        fsa = deepcopy(fsa)
        if fail_state is None:
            fail_state = fsa.add_state('Fail')
            fsa.insert('Fail', KimmoPair.make('@'), 'Fail')
        sorted_pairs = sort_subsets(self._pairs, self._subsets)
        for state in fsa.states():
            trans = fsa._transitions[state]
            for pair in self._pairs:
                if pair not in trans:
                    for sp in sorted_pairs:
                        if sp in trans and sp.includes(pair, self._subsets):
                            trans[pair] = trans[sp]
                            break
                    trans[pair] = [fail_state]
                if trans[pair] == []: trans[pair] = [fail_state]
        fsa._build_reverse_transitions()
        return fsa

    @staticmethod
    def parse_table(name, table, subsets):
        lines = table.split('\n')
        if len(lines) < 4:
            raise ValueError,\
            "Rule %s has too few lines to be an FSA table." % name
        pairs1 = lines[1].strip().split()
        pairs2 = lines[2].strip().split()
        if len(pairs1) != len(pairs2):
            raise ValueError,\
            "Rule %s has pair definitions that don't line up." % name
        pairs = [KimmoPair(p1, p2) for p1, p2 in zip(pairs1, pairs2)]
        finals = []
        fsa = FSA()
        for line in lines[3:]:
            line = line.strip()
            if not line: continue
            groups = re.match(r'(\w+)(\.|:)\s*(.*)', line)
            if groups is None:
                raise ValueError,\
                "Can't parse this line of the state table for rule %s:\n%s"\
                % (name, line)
            state, char, morestates = groups.groups()
            if fsa.start() == 0: fsa.set_start(state)
            if char == ':': finals.append(state)
            fsa.add_state(state)
            morestates = morestates.split()
            if len(morestates) != len(pairs):
                raise ValueError,\
                "Rule %s has a row of the wrong length:\n%s\ngot %d items, should be %d"\
                % (name, line, len(morestates), len(pairs))
            for pair, nextstate in zip(pairs, morestates):
                fsa.insert_safe(state, pair, nextstate)
        fsa.set_final(finals)
        return KimmoFSARule(name, fsa, subsets)
    
    @staticmethod 
    def from_dfa_dict(name, states, subsets):
        fsa = FSA()
        pairs = set([KimmoPair.make('@')])
        for (statename, trans) in states.items():
            for label in trans:
                if label != 'others':
                    pairs.add(KimmoPair.make(label))
        for (statename, trans) in states.items():
            parts = statename.split()
            source = parts[-1]
            if not parts[0].startswith('rej'):
                fsa.add_final(source)
            
            if fsa.start() == 0 and source in ['begin', 'Begin', '1', 1]:
                fsa.set_start(source)
            if source in ['start', 'Start']:
                fsa.set_start(source)
                
            used_pairs = set()
            for label in trans:
                if label != 'others':
                    used_pairs.add(KimmoPair.make(label))
            for label, target in trans.items():
                if label.lower() == 'others':
                    fsa.insert_safe(source, KimmoPair.make('@'), target)
                    for pair in pairs.difference(used_pairs):
                        fsa.insert_safe(source, pair, target)
                else:
                    fsa.insert_safe(source, KimmoPair.make(label), target)
        return KimmoFSARule(name, fsa, subsets)

class KimmoArrowRule(KimmoFSARule):
    def arrow(self): return self._arrow
    def lhpair(self): return self._lhpair

    def __init__(self, name, description, subsets):
        self._name = name
        self._description = description
        self._negated = False
        self._pairs = set()
        self._subsets = subsets
        desc = list(tokenize.regexp(description, _kimmo_rule))
        self._parse(desc)

    def __repr__(self):
        return '<KimmoArrowRule %s: %s>' % (self._name, self._description)

    def _parse(self, tokens):
        (end_pair, tree)  = self._parse_pair(tokens, 0)
        lhpair = self._pair_from_tree(tree)
        self._lhpair = lhpair
        self._pairs.add(lhpair)

        end_arrow         = self._parse_arrow(tokens, end_pair)
        (end_left, lfsa)  = self._parse_context(tokens, end_arrow, True)
        end_slot          = self._parse_slot(tokens, end_left)
        (end_right, rfsa) = self._parse_context(tokens, end_slot, False)
        if not(end_right == len(tokens)):
            raise ValueError('unidentified tokens')

        self._left_fsa  = lfsa
        self._right_fsa = rfsa
        self._merge_fsas()

    def _next_token(self, tokens, i, raise_error=False):
        if i >= len(tokens):
            if raise_error:
                raise ValueError('ran off end of input')
            else:
                return None
        return tokens[i]

    def _pair_from_tree(self, tree):
        if (tree.node != 'Pair'): raise RuntimeException('expected Pair, got ' + str(tree))
        if len(tree) == 1:
            return KimmoPair(tree[0], tree[0])
        else:
            return KimmoPair(tree[0], tree[2])

    def _parse_pair(self, tokens, i):
        # print 'parsing pair at ' + str(i)
        t1 = self._next_token(tokens, i, True)
        if t1 in _special_tokens: raise ValueError('expected identifier, not ' + t1)
        t2 = t1
        j = i + 1
        if self._next_token(tokens, j) == ':':
            t2 = self._next_token(tokens, j+1, True)
            if t2 in _special_tokens: raise ValueError('expected identifier, not ' + t2)
            j = j + 2
            tree = Tree('Pair', tokens[i:j])
        else:
            tree = Tree('Pair', [tokens[i]])
        #print str(self._pair_from_tree(tree)) + ' from ' + str(i) + ' to ' + str(j)
        return (j, tree)


    def _parse_arrow(self, tokens, i):
        self._arrow = self._next_token(tokens, i, True)
        if not(self.arrow() in _arrows):
            raise ValueError('expected arrow, not ' + self.arrow())
        #print 'arrow from ' + str(i) + ' to ' + str(i+1)
        return i + 1


    def _parse_slot(self, tokens, i):
        slot = self._next_token(tokens, i, True)
        if slot != '_':
            raise ValueError('expected _, not ' + slot)
        # print 'slot from ' + str(i) + ' to ' + str(i+1)
        return i + 1


    def _parse_context(self, tokens, i, reverse):
        (j, tree) = self._parse_list(tokens, i)
        if j == i: return (i, None)

        sigma = set()
        self._collect_alphabet(tree, sigma)
        fsa = FSA(sigma)
        final_state = self._build_fsa(fsa, fsa.start(), tree, reverse)
        fsa.set_final([final_state])
        #fsa.pp()
        dfa = fsa.dfa()
        #dfa.pp()
        dfa.prune()
        #dfa.pp()
        return (j, dfa)


    def _collect_alphabet(self, tree, sigma):
        if tree.node == 'Pair':
            pair = self._pair_from_tree(tree)
            sigma.add(pair)
            self._pairs.add(pair)
        else:
            for d in tree: self._collect_alphabet(d, sigma)


    def _parse_list(self, tokens, i, type='Cons'):
        # print 'parsing list at ' + str(i)
        t = self._next_token(tokens, i)
        if t == None or t in _non_list_initial_special_tokens:
            # print '  failing immediately '
            return (i, None)
        (j, s) = self._parse_singleton(tokens, i)
        (k, r) = self._parse_list(tokens, j, type)
        # print (k,r)
        if r == None:
            # print '  returning (%d, %s)' % (j, s)
            return (j, s)
        tree = Tree(type, [s, r])
        # print '  returning (%d, %s)' % (k, tree)
        return (k, tree)


    def _parse_singleton(self, tokens, i):
        # print 'parsing singleton at ' + str(i)
        t = self._next_token(tokens, i, True)
        j = i
        result = None
        if t == '(':
            (j, result) = self._parse_list(tokens, i + 1, 'Cons')
            if result == None: raise ValueError('missing contents of (...)')
            t = self._next_token(tokens, j, True)
            if t != ')': raise ValueError('missing final parenthesis, instead found ' + t)
            j = j + 1
        elif t == '[':
            (j, result) = self._parse_list(tokens, i + 1, 'Or')
            if result == None: raise ValueError('missing contents of [...]')
            t = self._next_token(tokens, j, True)
            if t != ']': raise ValueError('missing final bracket, instead found ' + t)
            j = j + 1
        elif t in _special_tokens:
            raise ValueError('expected identifier, found ' + t)
        else:
            (j, tree) = self._parse_pair(tokens, i)
            result = tree
        t = self._next_token(tokens, j)
        if t in ['*', '&', '?']:
            j = j + 1
            result = Tree(t, [result])
        return (j, result)


    def _build_fsa(self, fsa, entry_node, tree, reverse):
        if tree.node == 'Pair':
            return self._build_terminal(fsa, entry_node, self._pair_from_tree(tree))
        elif tree.node == 'Cons':
            return self._build_seq(fsa, entry_node, tree[0], tree[1], reverse)
        elif tree.node == 'Or':
            return self._build_or(fsa, entry_node, tree[0], tree[1], reverse)
        elif tree.node == '*':
            return self._build_star(fsa, entry_node, tree[0], reverse)
        elif tree.node == '&':
            return self._build_plus(fsa, entry_node, tree[0], reverse)
        elif tree.node == '?':
            return self._build_qmk(fsa, entry_node, tree[0], reverse)
        else:
            raise RuntimeError('unknown tree node'+tree.node)


    def _build_terminal(self, fsa, entry_node, terminal):
        new_exit_node = fsa.new_state()
        fsa.insert(entry_node, terminal, new_exit_node)
        #print '_build_terminal(%d,%s) -> %d' % (entry_node, terminal, new_exit_node)
        return new_exit_node


    def _build_plus(self, fsa, node, tree, reverse):
        node1 = self._build_fsa(fsa, node, tree[0], reverse)
        fsa.insert(node1, epsilon, node)
        return node1


    def _build_qmk(self, fsa, node, tree, reverse):
        node1 = fsa.new_state()
        node2 = self._build_fsa(fsa, node1, tree, reverse)
        node3 = fsa.new_state()
        fsa.insert(node, epsilon, node1)
        fsa.insert(node, epsilon, node3)
        fsa.insert(node2, epsilon, node3)
        return node3


    def _build_star(self, fsa, node, tree, reverse):
        node1 = fsa.new_state()
        node2 = self._build_fsa(fsa, node1, tree, reverse)
        node3 = fsa.new_state()
        fsa.insert(node, epsilon, node1)
        fsa.insert(node, epsilon, node3)
        fsa.insert(node2, epsilon, node1)
        fsa.insert(node2, epsilon, node3)
        return node3


    def _build_seq(self, fsa, node, tree0, tree1, reverse):
        (d0, d1) = (tree0, tree1)
        if reverse: (d0, d1) = (d1, d0)
        node1 = self._build_fsa(fsa, node, d0, reverse)
        node2 = self._build_fsa(fsa, node1, d1, reverse)
        # print '_build_seq(%d,%s,%s) -> %d,%d' % (node, tree0, tree1, node1, node2)
        return node2

    def _build_or(self, fsa, node, tree0, tree1, reverse):
        node0 = fsa.new_state()
        node1 = fsa.new_state()
        node2 = self._build_fsa(fsa, node0, tree0, reverse)
        node3 = self._build_fsa(fsa, node1, tree1, reverse)
        node4 = fsa.new_state()
        fsa.insert(node, epsilon, node0)
        fsa.insert(node, epsilon, node1)
        fsa.insert(node2, epsilon, node4)
        fsa.insert(node3, epsilon, node4)
        return node4
        
    def left_arrow(self):
        working = deepcopy(self._left_fsa)
        right = self._right_fsa.dfa()
        lstates = left.states()
        for state in lstates:
            working.relabel_state(state, 'L%s' % state)
        working.add_state('Trash')
        for state in working.finals():
            workng.insert_safe(source, KimmoPair)
        
def demo():
    rule = KimmoArrowRule("elision-e", "e:0 <== CN u _ +:@ VO", {'@':
    'aeiouhklmnpw', 'VO': 'aeiou', 'CN': 'hklmnpw'})
    print rule
    print rule._left_fsa
    print rule._right_fsa
    print
    print rule._fsa

if __name__ == '__main__':
    demo()

# vim:et:ts=4:sts=4:sw=4:

########NEW FILE########
__FILENAME__ = category
# Natural Language Toolkit: Categories
#
# Copyright (C) 2001-2007 NLTK Project
# Author: Contributed by Rob Speer (NLTK version)
#         Steven Bird <sb@csse.unimelb.edu.au> (NLTK-Lite Port)
#         Ewan Klein <ewan@inf.ed.ac.uk> (Hooks for semantics)
#         Peter Wang <wangp@csse.unimelb.edu.au> (Overhaul)
# URL: <http://nltk.sourceforge.net>
# For license information, see LICENSE.TXT
#
# $Id: category.py 4162 2007-03-01 00:46:05Z stevenbird $

from nltk.semantics import logic
from cfg import *
from kimmo import kimmo

from featurelite import *
from copy import deepcopy
import yaml
# import nltk.yamltags

def makevar(varname):
    """
    Given a variable representation such as C{?x}, construct a corresponding
    Variable object.
    """
    return Variable(varname[1:])

class Category(Nonterminal, FeatureI):
    """
    A C{Category} is a wrapper for feature dictionaries, intended for use in
    parsing. It can act as a C{Nonterminal}.

    A C{Category} acts like a dictionary, except in the following ways:

        - Categories can be "frozen" so that they can act as hash keys;
          before they are frozen, they are mutable.

        - In addition to being hashable, Categories have consistent str()
          representations.

        - Categories have one feature marked as the 'head', which prints
          differently than other features if it has a value. For example,
          in the C{repr()} representation of a Category, the head goes to the
          left, on the outside of the brackets. Subclasses of C{Category}
          may change the feature name that is designated as the head, which is
          _head by default.

    Categories can contain any kind of object as their values, and can be
    recursive and even re-entrant. Categories are not necessarily "categories
    all the way down"; they can contain plain dictionaries as their values, and
    converting inner dictionaries to categories would probably lead to messier
    code for no gain.

    Because Categories can contain any kind of object, they do not try to
    keep control over what their inner objects do. If you freeze a Category
    but mutate its inner objects, undefined behavior will occur.
    """
    
    headname = 'head'
    
    def __init__(self, features=None, **morefeatures):
        if features is None: features = {}
        self._features = unify(features, morefeatures)
        self._hash = None
        self._frozen = False
        self._memostr = None

    def __cmp__(self, other):
        return cmp(repr(self), repr(other))
    
    def __div__(self, other):
        """
        @return: A new Category based on this one, with its C{/} feature set to 
        C{other}.
        """
        return unify(self, {'/': other})

    def __eq__(self, other):
        """
        Compare Categories for equality. This relies on Python's built-in
        __eq__ for dictionaries, which is fairly thorough in checking for
        recursion and reentrance.

        @return: True if C{self} and C{other} assign the same value to
        every feature.  In particular, return true if
        C{self[M{p}]==other[M{p}]} for every feature path M{p} such
        that C{self[M{p}]} or C{other[M{p}]} is a base value (i.e.,
        not a nested Category).
        @rtype: C{bool}
        """
        if not other.__class__ == self.__class__: return False
        return self._features == other._features

    def __ne__(self, other):
        return not (self == other)

    def __hash__(self):
        if self._hash is not None: return self._hash
        return hash(str(self))
    
    def freeze(self):
        """
        Freezing a Category memoizes its hash value, to make comparisons on it
        faster. After freezing, the Category and all its values are immutable.

        @return: self
        """
        self._memostr = str(self)
        self._hash = hash(self)
        self._frozen = True
        return self

    def frozen(self):
        """
        Returns whether this Category is frozen (immutable).
        
        @rtype: C{bool}
        """
        return self._frozen
    
    def get(self, key):
        return self._features.get(key)

    def __getitem__(self, key):
        return self._features.get(key)
    
    def __setitem__(self, key, value):
        if self._frozen: raise "Cannot modify a frozen Category"
        self._features[key] = value

    def items(self):
        return self._features.items()

    def keys(self):
        return self._features.keys()

    def values(self):
        return self._features.values()

    def has_key(self, key):
        return self._features.has_key(key)
    
    def symbol(self):
        """
        @return: The node value corresponding to this C{Category}. 
        @rtype: C{Category}
        """
        return self

    def head(self):
        """
        @return: The head of this category (the value shown outside the
        brackets in its string representation). If there is no head, returns
        None.
        @rtype: C{str} or C{None}
        """
        return self.get(self.__class__.headname)
    
    def copy(self):
        """
        @return: A deep copy of C{self}.
        """
        # Create a reentrant deep copy by round-tripping it through YAML.
        return deepcopy(self)
    
    def feature_names(self):
        """
        @return: a list of all features that have values.
        """
        return self._features.keys()
    
    has_feature = has_key

    #################################################################
    ## Variables
    #################################################################
    
    def remove_unbound_vars(self):
        selfcopy = self.copy()
        Category._remove_unbound_vars(self)
        return selfcopy

    @staticmethod
    def _remove_unbound_vars(obj):
        for (key, value) in obj.items():
            if isinstance(value, Variable):
                del obj[key]
            elif isinstance(value, (Category, dict)):
                Category._remove_unbound_vars(value)

    #################################################################
    ## String Representations
    #################################################################

    def __repr__(self):
        """
        @return: A string representation of this feature structure.
        """
        return str(self)
    
    def __str__(self):
        """
        @return: A string representation of this feature structure.
        """
        if self._memostr is not None: return self._memostr
        return self.__class__._str(self, {}, {})
    
    @classmethod
    def _str(cls, obj, reentrances, reentrance_ids):
        segments = []

        keys = obj.keys()
        keys.sort()
        for fname in keys:
            if fname == cls.headname: continue
            fval = obj[fname]
            if isinstance(fval, bool):
                if fval: segments.append('+%s' % fname)
                else: segments.append('-%s' % fname)
            elif not isinstance(fval, dict):
                segments.append('%s=%r' % (fname, fval))
            else:
                fval_repr = cls._str(fval, reentrances, reentrance_ids)
                segments.append('%s=%s' % (fname, fval_repr))

        head = obj.get(cls.headname)
        if head is None: head = ''
        if head and not len(segments): return str(head)
        return '%s[%s]' % (head, ', '.join(segments))
    
    yaml_tag = '!parse.Category'
    
    @classmethod
    def to_yaml(cls, dumper, data):
        node = dumper.represent_mapping(cls.yaml_tag, data._features)
        return node

    @classmethod
    def from_yaml(cls, loader, node):
        features = loader.construct_mapping(node, deep=True)
        return cls(features)

    #################################################################
    ## Parsing
    #################################################################

    # Regular expressions for parsing.
    _PARSE_RE = {'name': re.compile(r'\s*([^\s\(\)"\'=,\[\]/\?]+)\s*'),
                 'ident': re.compile(r'\s*\((\d+)\)\s*'),
                 'reentrance': re.compile(r'\s*->\s*'),
                 'assign': re.compile(r'\s*=?\s*'),
                 'bracket': re.compile(r'\s*]\s*'),
                 'comma': re.compile(r'\s*,\s*'),
                 'none': re.compile(r'None(?=\s|\]|,)'),
                 'int': re.compile(r'-?\d+(?=\s|\]|,)'),
                 'var': re.compile(r'\?[a-zA-Z_][a-zA-Z0-9_]*'+'|'+
                                   r'\?<[a-zA-Z_][a-zA-Z0-9_]*'+
                                   r'(=[a-zA-Z_][a-zA-Z0-9_]*)*>'),
                 'symbol': re.compile(r'\w+'),
                 'stringmarker': re.compile("['\"\\\\]"),
    
                 'categorystart':re.compile(r'\s*([^\s\(\)"\'\-=,\[\]/\?]*)\s*\['),
                 'bool': re.compile(r'\s*([-\+])'),
                 'arrow': re.compile(r'\s*->\s*'),
                 'disjunct': re.compile(r'\s*\|\s*'),
                 'whitespace': re.compile(r'\s*'),
                 'semantics': re.compile(r'<([^>]+)>'), 
                 'application': re.compile(r'<(app)\((\?[a-z][a-z]*)\s*,\s*(\?[a-z][a-z]*)\)>'),
                 'slash': re.compile(r'\s*/\s*'),
                }
    
    @classmethod
    def parse(cls, s):
        parsed, position = cls._parse(s, 0)
        if position != len(s):
            raise ValueError('end of string', position)
        return cls(parsed)

    @classmethod
    def inner_parse(cls, s, position, reentrances={}):
        if reentrances is None: reentrances = {}
        parsed, position = cls._parse(s, position)
        return cls(parsed), position
    
    @classmethod
    def _parse(cls, s, position=0, reentrances=None):
        """
        Helper function that parses a Category.
        @param s: The string to parse.
        @param position: The position in the string to start parsing.
        @param reentrances: A dictionary from reentrance ids to values.
        @return: A tuple (val, pos) of the feature structure created
            by parsing and the position where the parsed feature
            structure ends.
        """
        # A set of useful regular expressions (precompiled)
        _PARSE_RE = cls._PARSE_RE

        features = {}
        
        # Find the head, if there is one.
        match = _PARSE_RE['name'].match(s, position)
        if match is not None:
            features[cls.headname] = match.group(1)
            position = match.end()
        else:
            match = _PARSE_RE['var'].match(s, position)
            if match is not None:
                features[cls.headname] = makevar(match.group(0))
                position = match.end()

        
        # If the name is followed by an open bracket, start looking for
        # features.
        if position < len(s) and s[position] == '[':
            position += 1
    
            # Build a list of the features defined by the structure.
            # Each feature has one of the three following forms:
            #     name = value
            #     +name
            #     -name
            while True:
                if not position < len(s):
                    raise ValueError('close bracket', position)
            
                # Use these variables to hold info about the feature:
                name = target = val = None
                
                # Check for a close bracket at the beginning
                match = _PARSE_RE['bracket'].match(s, position)
                if match is not None:
                    position = match.end()
                    break   
                    
                # Is this a shorthand boolean value?
                match = _PARSE_RE['bool'].match(s, position)
                if match is not None:
                    if match.group(1) == '+': val = True
                    else: val = False
                    position = match.end()
                
                # Find the next feature's name.
                match = _PARSE_RE['name'].match(s, position)
                if match is None: raise ValueError('feature name', position)
                name = match.group(1)
                position = match.end()
                
                # If it's not a shorthand boolean, it must be an assignment.
                if val is None:
                    match = _PARSE_RE['assign'].match(s, position)
                    if match is None: raise ValueError('equals sign', position)
                    position = match.end()
    
                    val, position = cls._parseval(s, position, reentrances)
                features[name] = val
                        
                # Check for a close bracket
                match = _PARSE_RE['bracket'].match(s, position)
                if match is not None:
                    position = match.end()
                    break   
                    
                # Otherwise, there should be a comma
                match = _PARSE_RE['comma'].match(s, position)
                if match is None: raise ValueError('comma', position)
                position = match.end()
            
        return features, position
    
    @classmethod
    def _parseval(cls, s, position, reentrances):
        """
        Helper function that parses a feature value.  Currently
        supports: None, bools, integers, variables, strings, nested feature
        structures.
        @param s: The string to parse.
        @param position: The position in the string to start parsing.
        @param reentrances: A dictionary from reentrance ids to values.
        @return: A tuple (val, pos) of the value created by parsing
            and the position where the parsed value ends.
        """
        
        # A set of useful regular expressions (precompiled)
        _PARSE_RE = cls._PARSE_RE
        
        # End of string (error)
        if position == len(s): raise ValueError('value', position)

        # Semantic value of the form <app(?x, ?y) >'; return an ApplicationExpression
        match = _PARSE_RE['application'].match(s, position)
        if match is not None:
            fun = ParserSubstitute(match.group(2)).next()
            arg = ParserSubstitute(match.group(3)).next()
            return ApplicationExpressionSubst(fun, arg), match.end()       

        # other semantic value enclosed by '< >'; return value given by the lambda expr parser
        match = _PARSE_RE['semantics'].match(s, position)
        if match is not None:
            return ParserSubstitute(match.group(1)).next(), match.end()
        
        # String value
        if s[position] in "'\"":
            start = position
            quotemark = s[position:position+1]
            position += 1
            while 1: 
                match = cls._PARSE_RE['stringmarker'].search(s, position)
                if not match: raise ValueError('close quote', position)
                position = match.end()
                if match.group() == '\\': position += 1
                elif match.group() == quotemark:
                    return s[start+1:position-1], position

        # Nested category
        if _PARSE_RE['categorystart'].match(s, position) is not None:
            return cls._parse(s, position, reentrances)

        # Variable
        match = _PARSE_RE['var'].match(s, position)
        if match is not None:
            return makevar(match.group()), match.end()

        # None
        match = _PARSE_RE['none'].match(s, position)
        if match is not None:
            return None, match.end()

        # Integer value
        match = _PARSE_RE['int'].match(s, position)
        if match is not None:
            return int(match.group()), match.end()

        # Alphanumeric symbol (must be checked after integer)
        match = _PARSE_RE['symbol'].match(s, position)
        if match is not None:
            return match.group(), match.end()

        # We don't know how to parse this value.
        raise ValueError('value', position)
    
    @classmethod
    def parse_rules(cls, s):
        """
        Parse a L{CFG} line involving C{Categories}. A line has this form:
        
        C{lhs -> rhs | rhs | ...}

        where C{lhs} is a Category, and each C{rhs} is a sequence of
        Categories.
        
        @returns: a list of C{Productions}, one for each C{rhs}.
        """
        _PARSE_RE = cls._PARSE_RE
        position = 0
        try:
            lhs, position = cls.inner_parse(s, position)
            lhs = cls(lhs)
        except ValueError, e:
            estr = ('Error parsing field structure\n\n\t' +
                    s + '\n\t' + ' '*e.args[1] + '^ ' +
                    'Expected %s\n' % e.args[0])
            raise ValueError, estr
        lhs.freeze()

        match = _PARSE_RE['arrow'].match(s, position)
        if match is None:
            raise ValueError('expected arrow', s, s[position:])
        else: position = match.end()
        rules = []
        while position < len(s):
            rhs = []
            while position < len(s) and _PARSE_RE['disjunct'].match(s, position) is None:
                try:
                    val, position = cls.inner_parse(s, position, {})
                    if isinstance(val, dict): val = cls(val)
                except ValueError, e:
                    estr = ('Error parsing field structure\n\n\t' +
                        s + '\n\t' + ' '*e.args[1] + '^ ' +
                        'Expected %s\n' % e.args[0])
                    raise ValueError, estr
                if isinstance(val, Category): val.freeze()
                rhs.append(val)
                position = _PARSE_RE['whitespace'].match(s, position).end()
            rules.append(Production(lhs, rhs))
            
            if position < len(s):
                match = _PARSE_RE['disjunct'].match(s, position)
                position = match.end()
        
        # Special case: if there's nothing after the arrow, it is one rule with
        # an empty RHS, instead of no rules.
        if len(rules) == 0: rules = [Production(lhs, ())]
        return rules

class GrammarCategory(Category):
    """
    A class of C{Category} for use in parsing.

    The name of the head feature in a C{GrammarCategory} is C{pos} (for "part
    of speech").
    
    In addition, GrammarCategories are displayed and parse differently, to be
    consistent with NLP teaching materials: the value of the C{/} feature can
    be written with a slash after the right bracket, so that the string
    representation looks like: C{head[...]/value}.

    Every GrammarCategory has a / feature implicitly present; if it is not
    explicitly written, it has the value False. This is so that "slashed"
    features cannot unify with "unslashed" ones.

    An example of a C{GrammarCategory} is C{VP[+fin]/NP}, for a verb phrase
    that is finite and has an omitted noun phrase inside it.
    """
    
    headname = 'pos'
    yaml_tag = '!parse.GrammarCategory'
    
    @classmethod
    def _str(cls, obj, reentrances, reentrance_ids):
        segments = []

        keys = obj.keys()
        keys.sort()
        for fname in keys:
            if fname == cls.headname: continue
            if isinstance(obj, GrammarCategory) and fname == '/': continue
            fval = obj[fname]
            if isinstance(fval, bool):
                if fval: segments.append('+%s' % fname)
                else: segments.append('-%s' % fname)
            elif not isinstance(fval, dict):
                segments.append('%s=%r' % (fname, fval))
            else:
                fval_repr = cls._str(fval, reentrances, reentrance_ids)
                segments.append('%s=%s' % (fname, fval_repr))

        if segments: features = '[%s]' % ', '.join(segments)
        else: features = ''
        head = obj.get(cls.headname)
        if head is None: head = ''
        slash = None
        if isinstance(obj, GrammarCategory): slash = obj.get('/')
        if not slash: slash = ''
        else:
            if isinstance(slash, dict):
                slash = '/%s' % cls._str(slash, reentrances, reentrance_ids)
            else:
                slash = '/%r' % slash

        
        return '%s%s%s' % (head, features, slash)
    
    @staticmethod
    def parse(s, position=0):
        return GrammarCategory.inner_parse(s, position)[0]
    
    @classmethod
    def inner_parse(cls, s, position, reentrances=None):
        if reentrances is None: reentrances = {}
        if s[position] in "'\"":
            start = position
            quotemark = s[position:position+1]
            position += 1
            while 1: 
                match = cls._PARSE_RE['stringmarker'].search(s, position)
                if not match: raise ValueError('close quote', position)
                position = match.end()
                if match.group() == '\\': position += 1
                elif match.group() == quotemark:
                    return s[start+1:position-1], position

        body, position = GrammarCategory._parse(s, position, reentrances)
        slash_match = Category._PARSE_RE['slash'].match(s, position)
        if slash_match is not None:
            position = slash_match.end()
            slash, position = GrammarCategory._parseval(s, position, reentrances)
            if isinstance(slash, basestring): slash = {'pos': slash}
            body['/'] = unify(body.get('/'), slash)
        elif not body.has_key('/'):
            body['/'] = False
        return cls(body), position
    
class SubstituteBindingsI:
    """
    An interface for classes that can perform substitutions for feature
    variables.
    """
    def substitute_bindings(self, bindings):
        """
        @return: The object that is obtained by replacing
        each variable bound by C{bindings} with its values.
        @rtype: (any)
        """
        raise NotImplementedError

class ParserSubstitute(logic.Parser):
    """
    A lambda calculus expression parser, extended to create application
    expressions which support the SubstituteBindingsI interface.
    """
    def make_ApplicationExpression(self, first, second):
        return ApplicationExpressionSubst(first, second)

class ApplicationExpressionSubst(logic.ApplicationExpression, SubstituteBindingsI):
    """
    A lambda application expression, extended to implement the
    SubstituteBindingsI interface.
    """
    def substitute_bindings(self, bindings):
        newval = self
        for semvar in self.variables():
            varstr = str(semvar)
            # discard Variables which are not FeatureVariables
            if varstr.startswith('?'): 
                var = makevar(varstr)
                if bindings.is_bound(var):
                    newval = newval.replace(semvar, bindings.lookup(var))
        return newval

############################################################################
# Read a grammar from a file
############################################################################

class GrammarFile(object):
    def __init__(self):
        self.grammatical_productions = []
        self.lexical_productions = []
        self.start = GrammarCategory(pos='Start')
        self.kimmo = None
        
    def grammar(self):
        return Grammar(self.start, self.grammatical_productions +\
        self.lexical_productions)
        
    def earley_grammar(self):
        return Grammar(self.start, self.grammatical_productions)
    
    def earley_lexicon(self):
        lexicon = {}
        for prod in self.lexical_productions:
            lexicon.setdefault(prod.rhs()[0].upper(), []).append(prod.lhs())
        def lookup(word):
            return lexicon.get(word.upper(), [])
        return lookup

    def kimmo_lexicon(self):
        def lookup(word):
            kimmo_results = self.kimmo.recognize(word.lower())
            return [GrammarCategory(k[1]) for k in kimmo_results]
        return lookup

    def earley_parser(self, trace=1):
        from featurechart import FeatureEarleyChartParse
        if self.kimmo is None: lexicon = self.earley_lexicon()
        else: lexicon = self.kimmo_lexicon()
        
        return FeatureEarleyChartParse(self.earley_grammar(),
                           lexicon, trace=trace)

    def apply_lines(self, lines):
        for line in lines:
            line = line.strip()
            if not len(line): continue
            if line[0] == '#': continue
            if line[0] == '%':
                parts = line[1:].split()
                directive = parts[0]
                args = " ".join(parts[1:])
                if directive == 'start':
                    self.start = GrammarCategory.parse(args).freeze()
                elif directive == 'include':
                    filename = args.strip('"')
                    self.apply_file(filename)
                elif directive == 'tagger_file':
                    import yaml, nltk.yamltags
                    filename = args.strip('"')
                    tagger = yaml.load(filename)
                    self.tagproc = chart_tagger(tagger)
                elif directive == 'kimmo':
                    filename = args.strip('"')
                    kimmorules = kimmo.load(filename)
                    self.kimmo = kimmorules
            else:
                rules = GrammarCategory.parse_rules(line)
                for rule in rules:
                    if len(rule.rhs()) == 1 and isinstance(rule.rhs()[0], str):
                        self.lexical_productions.append(rule)
                    else:
                        self.grammatical_productions.append(rule)

    def apply_file(self, filename):
        f = open(filename)
        lines = f.readlines()
        self.apply_lines(lines)
        f.close()
    
    @staticmethod
    def read_file(filename):
        result = GrammarFile()
        result.apply_file(filename)
        return result

yaml.add_representer(Category, Category.to_yaml)
yaml.add_representer(GrammarCategory, GrammarCategory.to_yaml)

def demo():
    print "Category(pos='n', agr=dict(number='pl', gender='f')):"
    print
    print Category(pos='n', agr=dict(number='pl', gender='f'))
    print repr(Category(pos='n', agr=dict(number='pl', gender='f')))
    print
    print "GrammarCategory.parse('NP/NP'):"
    print
    print GrammarCategory.parse('NP/NP')
    print repr(GrammarCategory.parse('NP/NP'))
    print
    print "GrammarCategory.parse('?x/?x'):"
    print
    print GrammarCategory.parse('?x/?x')
    print repr(GrammarCategory.parse('?x/?x'))
    print
    print "GrammarCategory.parse('VP[+fin, agr=?x, tense=past]/NP[+pl, agr=?x]'):"
    print
    print GrammarCategory.parse('VP[+fin, agr=?x, tense=past]/NP[+pl, agr=?x]')
    print repr(GrammarCategory.parse('VP[+fin, agr=?x, tense=past]/NP[+pl, agr=?x]'))
    print
    g = GrammarFile.read_file("speer.cfg")
    print g.grammar()
    
if __name__ == '__main__':
    demo()
    

########NEW FILE########
__FILENAME__ = cfg
# Natural Language Toolkit: Context Free Grammars
#
# Copyright (C) 2001-2007 NLTK Project
# Author: Steven Bird <sb@csse.unimelb.edu.au>
#         Edward Loper <edloper@ldc.upenn.edu>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT
#

"""
Basic data classes for representing context free grammars.  A
X{grammar} specifies which trees can represent the structure of a
given text.  Each of these trees is called a X{parse tree} for the
text (or simply a X{parse}).  In a X{context free} grammar, the set of
parse trees for any piece of a text can depend only on that piece, and
not on the rest of the text (i.e., the piece's context).  Context free
grammars are often used to find possible syntactic structures for
sentences.  In this context, the leaves of a parse tree are word
tokens; and the node values are phrasal categories, such as C{NP}
and C{VP}.

The L{Grammar} class is used to encode context free grammars.  Each C{Grammar}
consists of a start symbol and a set of productions.  The X{start
symbol} specifies the root node value for parse trees.  For example,
the start symbol for syntactic parsing is usually C{S}.  Start
symbols are encoded using the C{Nonterminal} class, which is discussed
below.

A Grammar's X{productions} specify what parent-child relationships a parse
tree can contain.  Each production specifies that a particular
node can be the parent of a particular set of children.  For example,
the production C{<S> -> <NP> <VP>} specifies that an C{S} node can
be the parent of an C{NP} node and a C{VP} node.

Grammar productions are implemented by the C{Production} class.
Each C{Production} consists of a left hand side and a right hand
side.  The X{left hand side} is a C{Nonterminal} that specifies the
node type for a potential parent; and the X{right hand side} is a list
that specifies allowable children for that parent.  This lists
consists of C{Nonterminals} and text types: each C{Nonterminal}
indicates that the corresponding child may be a C{TreeToken} with the
specified node type; and each text type indicates that the
corresponding child may be a C{Token} with the with that type.

The C{Nonterminal} class is used to distinguish node values from leaf
values.  This prevents the grammar from accidentally using a leaf
value (such as the English word "A") as the node of a subtree.  Within
a C{Grammar}, all node values are wrapped in the C{Nonterminal} class.
Note, however, that the trees that are specified by the grammar do
B{not} include these C{Nonterminal} wrappers.

Grammars can also be given a more procedural interpretation.  According to
this interpretation, a Grammar specifies any tree structure M{tree} that
can be produced by the following procedure:

    - Set M{tree} to the start symbol
    - Repeat until M{tree} contains no more nonterminal leaves:
      - Choose a production M{prod} with whose left hand side
        M{lhs} is a nonterminal leaf of M{tree}.
      - Replace the nonterminal leaf with a subtree, whose node
        value is the value wrapped by the nonterminal M{lhs}, and
        whose children are the right hand side of M{prod}.

The operation of replacing the left hand side (M{lhs}) of a production
with the right hand side (M{rhs}) in a tree (M{tree}) is known as
X{expanding} M{lhs} to M{rhs} in M{tree}.
"""

import re


#################################################################
# Nonterminal
#################################################################

class Nonterminal(object):
    """
    A non-terminal symbol for a context free grammar.  C{Nonterminal}
    is a wrapper class for node values; it is used by
    C{Production}s to distinguish node values from leaf values.
    The node value that is wrapped by a C{Nonterminal} is known as its
    X{symbol}.  Symbols are typically strings representing phrasal
    categories (such as C{"NP"} or C{"VP"}).  However, more complex
    symbol types are sometimes used (e.g., for lexicalized grammars).
    Since symbols are node values, they must be immutable and
    hashable.  Two C{Nonterminal}s are considered equal if their
    symbols are equal.

    @see: L{Grammar}
    @see: L{Production}
    @type _symbol: (any)
    @ivar _symbol: The node value corresponding to this
        C{Nonterminal}.  This value must be immutable and hashable. 
    """
    def __init__(self, symbol):
        """
        Construct a new non-terminal from the given symbol.

        @type symbol: (any)
        @param symbol: The node value corresponding to this
            C{Nonterminal}.  This value must be immutable and
            hashable. 
        """
        self._symbol = symbol
        self._hash = hash(symbol)

    def symbol(self):
        """
        @return: The node value corresponding to this C{Nonterminal}. 
        @rtype: (any)
        """
        return self._symbol

    def __eq__(self, other):
        """
        @return: True if this non-terminal is equal to C{other}.  In
            particular, return true iff C{other} is a C{Nonterminal}
            and this non-terminal's symbol is equal to C{other}'s
            symbol.
        @rtype: C{boolean}
        """
        try:
            return ((self._symbol == other._symbol) \
                    and isinstance(other, self.__class__))
        except AttributeError:
            return False

    def __ne__(self, other):
        """
        @return: True if this non-terminal is not equal to C{other}.  In
            particular, return true iff C{other} is not a C{Nonterminal}
            or this non-terminal's symbol is not equal to C{other}'s
            symbol.
        @rtype: C{boolean}
        """
        return not (self==other)

    def __cmp__(self, other):
        if self == other: return 0
        else: return -1

    def __hash__(self):
        return self._hash

    def __repr__(self):
        """
        @return: A string representation for this C{Nonterminal}.
            The string representation for a C{Nonterminal} whose
            symbol is C{M{s}} is C{<M{s}>}.
        @rtype: C{string}
        """
        # [XX] not a good repr!  Token uses this now!!
        return '<%s>' % (self._symbol,)

    def __str__(self):
        """
        @return: A string representation for this C{Nonterminal}.
            The string representation for a C{Nonterminal} whose
            symbol is C{M{s}} is C{M{s}}.
        @rtype: C{string}
        """
        return '%s' % (self._symbol,)

    def __div__(self, rhs):
        """
        @return: A new nonterminal whose symbol is C{M{A}/M{B}}, where
            C{M{A}} is the symbol for this nonterminal, and C{M{B}}
            is the symbol for rhs.
        @rtype: L{Nonterminal}
        @param rhs: The nonterminal used to form the right hand side
            of the new nonterminal.
        @type rhs: L{Nonterminal}
        """
        return Nonterminal('%s/%s' % (self._symbol, rhs._symbol))

def nonterminals(symbols):
    """
    Given a string containing a list of symbol names, return a list of
    C{Nonterminals} constructed from those symbols.  

    @param symbols: The symbol name string.  This string can be
        delimited by either spaces or commas.
    @type symbols: C{string}
    @return: A list of C{Nonterminals} constructed from the symbol
        names given in C{symbols}.  The C{Nonterminals} are sorted
        in the same order as the symbols names.
    @rtype: C{list} of L{Nonterminal}
    """
    if ',' in symbols: symbol_list = symbols.split(',')
    else: symbol_list = symbols.split()
    return [Nonterminal(s.strip()) for s in symbol_list]

#################################################################
# Production and Grammar
#################################################################

class Production(object):
    """
    A context-free grammar production.  Each production
    expands a single C{Nonterminal} (the X{left-hand side}) to a
    sequence of terminals and C{Nonterminals} (the X{right-hand
    side}).  X{terminals} can be any immutable hashable object that is
    not a C{Nonterminal}.  Typically, terminals are strings
    representing word types, such as C{"dog"} or C{"under"}.

    Abstractly, a Grammar production indicates that the right-hand side is
    a possible X{instantiation} of the left-hand side.  Grammar
    productions are X{context-free}, in the sense that this
    instantiation should not depend on the context of the left-hand
    side or of the right-hand side.

    @see: L{Grammar}
    @see: L{Nonterminal}
    @type _lhs: L{Nonterminal}
    @ivar _lhs: The left-hand side of the production.
    @type _rhs: C{tuple} of (C{Nonterminal} and (terminal))
    @ivar _rhs: The right-hand side of the production.
    """

    def __init__(self, lhs, rhs):
        """
        Construct a new C{Production}.

        @param lhs: The left-hand side of the new C{Production}.
        @type lhs: L{Nonterminal}
        @param rhs: The right-hand side of the new C{Production}.
        @type rhs: sequence of (C{Nonterminal} and (terminal))
        """
        if isinstance(rhs, (str, unicode)):
            raise TypeError, 'production right hand side should be a list, not a string'
        self._lhs = lhs
        self._rhs = tuple(rhs)
        self._hash = hash((self._lhs, self._rhs))

    def lhs(self):
        """
        @return: the left-hand side of this C{Production}.
        @rtype: L{Nonterminal}
        """
        return self._lhs

    def rhs(self):
        """
        @return: the right-hand side of this C{Production}.
        @rtype: sequence of (C{Nonterminal} and (terminal))
        """
        return self._rhs

    def __str__(self):
        """
        @return: A verbose string representation of the
            C{Production}.
        @rtype: C{string}
        """
        str = '%s ->' % (self._lhs.symbol(),)
        for elt in self._rhs:
            if isinstance(elt, Nonterminal):
                str += ' %s' % (elt.symbol(),)
            else:
                str += ' %r' % (elt,)
        return str

    def __repr__(self):
        """
        @return: A concise string representation of the
            C{Production}. 
        @rtype: C{string}
        """
        return '%s' % self

    def __eq__(self, other):
        """
        @return: true if this C{Production} is equal to C{other}.
        @rtype: C{boolean}
        """
        return (isinstance(other, self.__class__) and
                self._lhs == other._lhs and
                self._rhs == other._rhs)
                 
    def __ne__(self, other):
        return not (self == other)

    def __cmp__(self, other):
        if not isinstance(other, self.__class__): return -1
        return cmp((self._lhs, self._rhs), (other._lhs, other._rhs))

    def __hash__(self):
        """
        @return: A hash value for the C{Production}.
        @rtype: C{int}
        """
        return self._hash


class Grammar(object):
    """
    A context-free grammar.  A Grammar consists of a start state and a set
    of productions.  The set of terminals and nonterminals is
    implicitly specified by the productions.

    If you need efficient key-based access to productions, you
    can use a subclass to implement it.
    """
    def __init__(self, start, productions):
        """
        Create a new context-free grammar, from the given start state
        and set of C{Production}s.
        
        @param start: The start symbol
        @type start: L{Nonterminal}
        @param productions: The list of productions that defines the grammar
        @type productions: C{list} of L{Production}
        """
        self._start = start
        self._productions = productions
        self._lhs_index = {}
        self._rhs_index = {}
        for prod in self._productions:
            if prod._lhs not in self._lhs_index:
                self._lhs_index[prod._lhs] = []
            if prod._rhs and prod._rhs[0] not in self._rhs_index:
                self._rhs_index[prod._rhs[0]] = []
            self._lhs_index[prod._lhs].append(prod)
            if prod._rhs:
                self._rhs_index[prod._rhs[0]].append(prod)
        
    def start(self):
        return self._start

    # tricky to balance readability and efficiency here!
    # can't use set operations as they don't preserve ordering
    def productions(self, lhs=None, rhs=None):
        # no constraints so return everything
        if not lhs and not rhs:
            return self._productions

        # only lhs specified so look up its index
        elif lhs and not rhs:
            if lhs in self._lhs_index:
                return self._lhs_index[lhs]
            else:
                return []

        # only rhs specified so look up its index
        elif rhs and not lhs:
            if rhs in self._rhs_index:
                return self._rhs_index[rhs]
            else:
                return []

        # intersect
        else:
            if lhs in self._lhs_index:
                return [prod for prod in self._lhs_index[lhs]
                        if prod in self._rhs_index[rhs]]
            else:
                return []

    def __repr__(self):
        return '<Grammar with %d productions>' % len(self._productions)

    def __str__(self):
        str = 'Grammar with %d productions' % len(self._productions)
        str += ' (start state = %s)' % self._start
        for production in self._productions:
            str += '\n    %s' % production
        return str

_PARSE_RE = re.compile(r'''^(\w+)\s*           # lhs
                          (?:-+>|=+>)\s*       # arrow
                          (?:(                 # rhs:
                               "[^"]+"         # doubled-quoted terminal
                               |'[^']+'        # single-quoted terminal
                               |\w+|           # non-terminal
                               \|              # disjunction
                             )
                             \s*)              # trailing space
                             *$''',
                       re.VERBOSE)
_SPLIT_RE = re.compile(r'''(\w+|-+>|=+>|"[^"]+"|'[^']+'|\|)''')

def parse_production(s):
    """
    Returns a list of productions
    """
    # Use _PARSE_RE to check that it's valid.
    if not _PARSE_RE.match(s):
        raise ValueError, 'Bad production string'
    # Use _SPLIT_RE to process it.
    pieces = _SPLIT_RE.split(s)
    pieces = [p for i,p in enumerate(pieces) if i%2==1]
    lhside = Nonterminal(pieces[0])
    rhsides = [[]]
    for piece in pieces[2:]:
        if piece == '|':
            rhsides.append([])                     # Vertical bar
        elif piece[0] in ('"', "'"):
            rhsides[-1].append(piece[1:-1])        # Terminal
        else:
            rhsides[-1].append(Nonterminal(piece)) # Nonterminal
    return [Production(lhside, rhside) for rhside in rhsides]

def parse_grammar(s):
    productions = []
    for linenum, line in enumerate(s.split('\n')):
        line = line.strip()
        if line.startswith('#') or line=='': continue
        try: productions += parse_production(line)
        except ValueError:
            raise ValueError, 'Unable to parse line %s' % linenum
    if len(productions) == 0:
        raise ValueError, 'No productions found!'
    start = productions[0].lhs()
    return Grammar(start, productions)

#################################################################
# Demonstration
#################################################################

def demo():
    """
    A demonstration showing how C{Grammar}s can be created and used.
    """

    from nltk import cfg

    # Create some nonterminals
    S, NP, VP, PP = cfg.nonterminals('S, NP, VP, PP')
    N, V, P, Det = cfg.nonterminals('N, V, P, Det')
    VP_slash_NP = VP/NP

    print 'Some nonterminals:', [S, NP, VP, PP, N, V, P, Det, VP/NP]
    print '    S.symbol() =>', `S.symbol()`
    print

    print cfg.Production(S, [NP])

    # Create some Grammar Productions
    grammar = cfg.parse_grammar("""
    S -> NP VP
    PP -> P NP
    NP -> Det N
    NP -> NP PP
    VP -> V NP
    VP -> VP PP
    Det -> 'a'
    Det -> 'the'
    N -> 'dog'
    N -> 'cat'
    V -> 'chased'
    V -> 'sat'
    P -> 'on'
    P -> 'in'
    """)

    print 'A Grammar:', `grammar`
    print '    grammar.start()       =>', `grammar.start()`
    print '    grammar.productions() =>',
    # Use string.replace(...) is to line-wrap the output.
    print `grammar.productions()`.replace(',', ',\n'+' '*25)
    print

if __name__ == '__main__': demo()

########NEW FILE########
__FILENAME__ = chart
# Natural Language Toolkit: A Chart Parser
#
# Copyright (C) 2001-2007 NLTK Project
# Author: Edward Loper <edloper@gradient.cis.upenn.edu>
#         Steven Bird <sb@csse.unimelb.edu.au>
#         Jean Mark Gawron <gawron@mail.sdsu.edu>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT
#
# $Id: chart.py 4157 2007-02-28 09:56:25Z stevenbird $

from __init__ import *
from tree import Tree
from nltk import cfg

"""
Data classes and parser implementations for \"chart parsers\", which
use dynamic programming to efficiently parse a text.  A X{chart
parser} derives parse trees for a text by iteratively adding \"edges\"
to a \"chart.\"  Each X{edge} represents a hypothesis about the tree
structure for a subsequence of the text.  The X{chart} is a
\"blackboard\" for composing and combining these hypotheses.

When a chart parser begins parsing a text, it creates a new (empty)
chart, spanning the text.  It then incrementally adds new edges to the
chart.  A set of X{chart rules} specifies the conditions under which
new edges should be added to the chart.  Once the chart reaches a
stage where none of the chart rules adds any new edges, parsing is
complete.

Charts are encoded with the L{Chart} class, and edges are encoded with
the L{TreeEdge} and L{LeafEdge} classes.  The chart parser module
defines three chart parsers:

  - C{ChartParse} is a simple and flexible chart parser.  Given a
    set of chart rules, it will apply those rules to the chart until
    no more edges are added.

  - C{SteppingChartParse} is a subclass of C{ChartParse} that can
    be used to step through the parsing process.

  - C{EarleyChartParse} is an implementation of the Earley chart parsing
    algorithm.  It makes a single left-to-right pass through the
    chart, and applies one of three rules (predictor, scanner, and
    completer) to each edge it encounters.
"""

import re

########################################################################
##  Edges
########################################################################

class EdgeI(object):
    """
    A hypothesis about the structure of part of a sentence.
    Each edge records the fact that a structure is (partially)
    consistent with the sentence.  An edge contains:

        - A X{span}, indicating what part of the sentence is
          consistent with the hypothesized structure.
          
        - A X{left-hand side}, specifying what kind of structure is
          hypothesized.

        - A X{right-hand side}, specifying the contents of the
          hypothesized structure.

        - A X{dot position}, indicating how much of the hypothesized
          structure is consistent with the sentence.

    Every edge is either X{complete} or X{incomplete}:

      - An edge is X{complete} if its structure is fully consistent
        with the sentence.

      - An edge is X{incomplete} if its structure is partially
        consistent with the sentence.  For every incomplete edge, the
        span specifies a possible prefix for the edge's structure.
    
    There are two kinds of edge:

        - C{TreeEdges<TreeEdge>} record which trees have been found to
          be (partially) consistent with the text.
          
        - C{LeafEdges<leafEdge>} record the tokens occur in the text.

    The C{EdgeI} interface provides a common interface to both types
    of edge, allowing chart parsers to treat them in a uniform manner.
    """
    def __init__(self):
        if self.__class__ == EdgeI: 
            raise TypeError('Edge is an abstract interface')
        
    #////////////////////////////////////////////////////////////
    # Span
    #////////////////////////////////////////////////////////////

    def span(self):
        """
        @return: A tuple C{(s,e)}, where C{subtokens[s:e]} is the
            portion of the sentence that is consistent with this
            edge's structure.
        @rtype: C{(int, int)}
        """
        raise AssertionError('EdgeI is an abstract interface')

    def start(self):
        """
        @return: The start index of this edge's span.
        @rtype: C{int}
        """
        raise AssertionError('EdgeI is an abstract interface')

    def end(self):
        """
        @return: The end index of this edge's span.
        @rtype: C{int}
        """
        raise AssertionError('EdgeI is an abstract interface')

    def length(self):
        """
        @return: The length of this edge's span.
        @rtype: C{int}
        """
        raise AssertionError('EdgeI is an abstract interface')

    #////////////////////////////////////////////////////////////
    # Left Hand Side
    #////////////////////////////////////////////////////////////

    def lhs(self):
        """
        @return: This edge's left-hand side, which specifies what kind
            of structure is hypothesized by this edge.
        @see: L{TreeEdge} and L{LeafEdge} for a description of
            the left-hand side values for each edge type.
        """
        raise AssertionError('EdgeI is an abstract interface')

    #////////////////////////////////////////////////////////////
    # Right Hand Side
    #////////////////////////////////////////////////////////////

    def rhs(self):
        """
        @return: This edge's right-hand side, which specifies
            the content of the structure hypothesized by this
            edge.
        @see: L{TreeEdge} and L{LeafEdge} for a description of
            the right-hand side values for each edge type.
        """
        raise AssertionError('EdgeI is an abstract interface')

    def dot(self):
        """
        @return: This edge's dot position, which indicates how much of
            the hypothesized structure is consistent with the
            sentence.  In particular, C{self.rhs[:dot]} is consistent
            with C{subtoks[self.start():self.end()]}.
        @rtype: C{int}
        """
        raise AssertionError('EdgeI is an abstract interface')

    def next(self):
        """
        @return: The element of this edge's right-hand side that
            immediately follows its dot.
        @rtype: C{Nonterminal} or X{terminal} or C{None}
        """
        raise AssertionError('EdgeI is an abstract interface')

    def is_complete(self):
        """
        @return: True if this edge's structure is fully consistent
            with the text.
        @rtype: C{boolean}
        """
        raise AssertionError('EdgeI is an abstract interface')

    def is_incomplete(self):
        """
        @return: True if this edge's structure is partially consistent
            with the text.
        @rtype: C{boolean}
        """
        raise AssertionError('EdgeI is an abstract interface')

    #////////////////////////////////////////////////////////////
    # Comparisons
    #////////////////////////////////////////////////////////////
    def __cmp__(self, other):
        raise AssertionError('EdgeI is an abstract interface')

    def __hash__(self, other):
        raise AssertionError('EdgeI is an abstract interface')

class TreeEdge(EdgeI):
    """
    An edge that records the fact that a tree is (partially)
    consistent with the sentence.  A tree edge consists of:

        - A X{span}, indicating what part of the sentence is
          consistent with the hypothesized tree.
          
        - A X{left-hand side}, specifying the hypothesized tree's node
          value.

        - A X{right-hand side}, specifying the hypothesized tree's
          children.  Each element of the right-hand side is either a
          terminal, specifying a token with that terminal as its leaf
          value; or a nonterminal, specifying a subtree with that
          nonterminal's symbol as its node value.

        - A X{dot position}, indicating which children are consistent
          with part of the sentence.  In particular, if C{dot} is the
          dot position, C{rhs} is the right-hand size, C{(start,end)}
          is the span, and C{sentence} is the list of subtokens in the
          sentence, then C{subtokens[start:end]} can be spanned by the
          children specified by C{rhs[:dot]}.

    For more information about edges, see the L{EdgeI} interface.
    """
    def __init__(self, span, lhs, rhs, dot=0):
        """
        Construct a new C{TreeEdge}.
        
        @type span: C{(int, int)}
        @param span: A tuple C{(s,e)}, where C{subtokens[s:e]} is the
            portion of the sentence that is consistent with the new
            edge's structure.
        @type lhs: L{Nonterminal}
        @param lhs: The new edge's left-hand side, specifying the
            hypothesized tree's node value.
        @type rhs: C{list} of (L{Nonterminal} and C{string})
        @param rhs: The new edge's right-hand side, specifying the
            hypothesized tree's children.
        @type dot: C{int}
        @param dot: The position of the new edge's dot.  This position
            specifies what prefix of the production's right hand side
            is consistent with the text.  In particular, if
            C{sentence} is the list of subtokens in the sentence, then
            C{subtokens[span[0]:span[1]]} can be spanned by the
            children specified by C{rhs[:dot]}.
        """
        self._lhs = lhs
        self._rhs = tuple(rhs)
        self._span = span
        self._dot = dot

    # [staticmethod]
    def from_production(production, index):
        """
        @return: A new C{TreeEdge} formed from the given production.
            The new edge's left-hand side and right-hand side will
            be taken from C{production}; its span will be C{(index,
            index)}; and its dot position will be C{0}.
        @rtype: L{TreeEdge}
        """
        return TreeEdge(span=(index, index), lhs=production.lhs(),
                        rhs=production.rhs(), dot=0)
    from_production = staticmethod(from_production)

    # Accessors
    def lhs(self): return self._lhs
    def span(self): return self._span
    def start(self): return self._span[0]
    def end(self): return self._span[1]
    def length(self): return self._span[1] - self._span[0]
    def rhs(self): return self._rhs
    def dot(self): return self._dot
    def is_complete(self): return self._dot == len(self._rhs)
    def is_incomplete(self): return self._dot != len(self._rhs)
    def next(self):
        if self._dot >= len(self._rhs): return None
        else: return self._rhs[self._dot]

    # Comparisons & hashing
    def __cmp__(self, other):
        if self.__class__ != other.__class__: return -1
        return cmp((self._span, self.lhs(), self.rhs(), self._dot),
                   (other._span, other.lhs(), other.rhs(), other._dot))
    def __hash__(self):
        return hash((self.lhs(), self.rhs(), self._span, self._dot))

    # String representation
    def __str__(self):
        str = '[%s:%s] ' % (self._span[0], self._span[1])
        str += '%-2s ->' % (self._lhs.symbol(),)
            
        for i in range(len(self._rhs)):
            if i == self._dot: str += ' *'
            if isinstance(self._rhs[i], cfg.Nonterminal):
                str += ' %s' % (self._rhs[i].symbol(),)
            else:
                str += ' %r' % (self._rhs[i],)
        if len(self._rhs) == self._dot: str += ' *'
        return str
        
    def __repr__(self):
        return '[Edge: %s]' % self

class LeafEdge(EdgeI):
    """
    An edge that records the fact that a leaf value is consistent with
    a word in the sentence.  A leaf edge consists of:

      - An X{index}, indicating the position of the word.
      - A X{leaf}, specifying the word's content.

    A leaf edge's left-hand side is its leaf value, and its right hand
    side is C{()}.  Its span is C{[index, index+1]}, and its dot
    position is C{0}.
    """
    def __init__(self, leaf, index):
        """
        Construct a new C{LeafEdge}.

        @param leaf: The new edge's leaf value, specifying the word
            that is recorded by this edge.
        @param index: The new edge's index, specifying the position of
            the word that is recorded by this edge.
        """
        self._leaf = leaf
        self._index = index

    # Accessors
    def lhs(self): return self._leaf
    def span(self): return (self._index, self._index+1)
    def start(self): return self._index
    def end(self): return self._index+1
    def length(self): return 1
    def rhs(self): return ()
    def dot(self): return 0
    def is_complete(self): return True
    def is_incomplete(self): return False
    def next(self): return None

    # Comparisons & hashing
    def __cmp__(self, other):
        if not isinstance(other, LeafEdge): return -1
        return cmp((self._index, self._leaf), (other._index, other._leaf))
    def __hash__(self):
        return hash((self._index, self._leaf))

    # String representations
    def __str__(self): return '[%s:%s] %r' % (self._index, self._index+1, self._leaf)
    def __repr__(self):
        return '[Edge: %s]' % (self)

########################################################################
##  Chart
########################################################################

class Chart(object):
    """
    A blackboard for hypotheses about the syntactic constituents of a
    sentence.  A chart contains a set of edges, and each edge encodes
    a single hypothesis about the structure of some portion of the
    sentence.

    The L{select} method can be used to select a specific collection
    of edges.  For example C{chart.select(is_complete=True, start=0)}
    yields all complete edges whose start indices are 0.  To ensure
    the efficiency of these selection operations, C{Chart} dynamically
    creates and maintains an index for each set of attributes that
    have been selected on.

    In order to reconstruct the trees that are represented by an edge,
    the chart associates each edge with a set of child pointer lists.
    A X{child pointer list} is a list of the edges that license an
    edge's right-hand side.

    @ivar _tokens: The sentence that the chart covers.
    @ivar _num_leaves: The number of tokens.
    @ivar _edges: A list of the edges in the chart
    @ivar _edge_to_cpls: A dictionary mapping each edge to a set
        of child pointer lists that are associated with that edge.
    @ivar _indexes: A dictionary mapping tuples of edge attributes
        to indices, where each index maps the corresponding edge
        attribute values to lists of edges.
    """
    def __init__(self, tokens):
        """
        Construct a new empty chart.

        @type tokens: L{list}
        @param tokens: The sentence that this chart will be used to parse.
        """
        # Record the sentence token and the sentence length.
        self._tokens = list(tokens)
        self._num_leaves = len(self._tokens)

        # A list of edges contained in this chart.
        self._edges = []
        
        # The set of child pointer lists associated with each edge.
        self._edge_to_cpls = {}

        # Indexes mapping attribute values to lists of edges (used by
        # select()).
        self._indexes = {}

    #////////////////////////////////////////////////////////////
    # Sentence Access
    #////////////////////////////////////////////////////////////

    def num_leaves(self):
        """
        @return: The number of words in this chart's sentence.
        @rtype: C{int}
        """
        return self._num_leaves

    def leaf(self, index):
        """
        @return: The leaf value of the word at the given index.
        @rtype: C{string}
        """
        return self._tokens[index]

    def leaves(self):
        """
        @return: A list of the leaf values of each word in the
            chart's sentence.
        @rtype: C{list} of C{string}
        """
        return self._tokens

    #////////////////////////////////////////////////////////////
    # Edge access
    #////////////////////////////////////////////////////////////

    def edges(self):
        """
        @return: A list of all edges in this chart.  New edges
            that are added to the chart after the call to edges()
            will I{not} be contained in this list.
        @rtype: C{list} of L{EdgeI}
        @see: L{iteredges}, L{select}
        """
        return self._edges[:]

    def iteredges(self):
        """
        @return: An iterator over the edges in this chart.  Any
            new edges that are added to the chart before the iterator
            is exahusted will also be generated.
        @rtype: C{iter} of L{EdgeI}
        @see: L{edges}, L{select}
        """
        return iter(self._edges)

    # Iterating over the chart yields its edges.
    __iter__ = iteredges

    def num_edges(self):
        """
        @return: The number of edges contained in this chart.
        @rtype: C{int}
        """
        return len(self._edge_to_cpls)

    def select(self, **restrictions):
        """
        @return: An iterator over the edges in this chart.  Any
            new edges that are added to the chart before the iterator
            is exahusted will also be generated.  C{restrictions}
            can be used to restrict the set of edges that will be
            generated.
        @rtype: C{iter} of L{EdgeI}

        @kwarg span: Only generate edges C{e} where C{e.span()==span}
        @kwarg start: Only generate edges C{e} where C{e.start()==start}
        @kwarg end: Only generate edges C{e} where C{e.end()==end}
        @kwarg length: Only generate edges C{e} where C{e.length()==length}
        @kwarg lhs: Only generate edges C{e} where C{e.lhs()==lhs}
        @kwarg rhs: Only generate edges C{e} where C{e.rhs()==rhs}
        @kwarg next: Only generate edges C{e} where C{e.next()==next}
        @kwarg dot: Only generate edges C{e} where C{e.dot()==dot}
        @kwarg is_complete: Only generate edges C{e} where
            C{e.is_complete()==is_complete}
        @kwarg is_incomplete: Only generate edges C{e} where
            C{e.is_incomplete()==is_incomplete}
        """
        # If there are no restrictions, then return all edges.
        if restrictions=={}: return iter(self._edges)
            
        # Find the index corresponding to the given restrictions.
        restr_keys = restrictions.keys()
        restr_keys.sort()
        restr_keys = tuple(restr_keys)

        # If it doesn't exist, then create it.
        if not self._indexes.has_key(restr_keys):
            self._add_index(restr_keys)
                
        vals = [restrictions[k] for k in restr_keys]
        return iter(self._indexes[restr_keys].get(tuple(vals), []))

    def _add_index(self, restr_keys):
        """
        A helper function for L{select}, which creates a new index for
        a given set of attributes (aka restriction keys).
        """
        # Make sure it's a valid index.
        for k in restr_keys:
            if not hasattr(EdgeI, k):
                raise ValueError, 'Bad restriction: %s' % k

        # Create the index.
        self._indexes[restr_keys] = {}

        # Add all existing edges to the index.
        for edge in self._edges:
            vals = [getattr(edge, k)() for k in restr_keys]
            index = self._indexes[restr_keys]
            index.setdefault(tuple(vals),[]).append(edge)

    #////////////////////////////////////////////////////////////
    # Edge Insertion
    #////////////////////////////////////////////////////////////

    def insert(self, edge, child_pointer_list):
        """
        Add a new edge to the chart.

        @type edge: L{Edge}
        @param edge: The new edge
        @type child_pointer_list: C{tuple} of L{Edge}
        @param child_pointer_list: A list of the edges that were used to
            form this edge.  This list is used to reconstruct the trees
            (or partial trees) that are associated with C{edge}.
        @rtype: C{bool}
        @return: True if this operation modified the chart.  In
            particular, return true iff the chart did not already
            contain C{edge}, or if it did not already associate
            C{child_pointer_list} with C{edge}.
        """
        # Is it a new edge?
        if not self._edge_to_cpls.has_key(edge):
            # Add it to the list of edges.
            self._edges.append(edge)

            # Register with indexes
            for (restr_keys, index) in self._indexes.items():
                vals = [getattr(edge, k)() for k in restr_keys]
                index = self._indexes[restr_keys]
                index.setdefault(tuple(vals),[]).append(edge)

        # Get the set of child pointer lists for this edge.
        cpls = self._edge_to_cpls.setdefault(edge,{})
        child_pointer_list = tuple(child_pointer_list)

        if cpls.has_key(child_pointer_list):
            # We've already got this CPL; return false.
            return False
        else:
            # It's a new CPL; register it, and return true.
            cpls[child_pointer_list] = True
            return True

    #////////////////////////////////////////////////////////////
    # Tree extraction & child pointer lists
    #////////////////////////////////////////////////////////////

    def parses(self, root, tree_class=Tree):
        """
        @return: A list of the complete tree structures that span
        the entire chart, and whose root node is C{root}.
        """
        trees = []
        for edge in self.select(span=(0,self._num_leaves), lhs=root):
            trees += self.trees(edge, tree_class=tree_class, complete=True)
        return trees

    def trees(self, edge, tree_class=Tree, complete=False):
        """
        @return: A list of the tree structures that are associated
        with C{edge}.

        If C{edge} is incomplete, then the unexpanded children will be
        encoded as childless subtrees, whose node value is the
        corresponding terminal or nonterminal.
            
        @rtype: C{list} of L{Tree}
        @note: If two trees share a common subtree, then the same
            C{Tree} may be used to encode that subtree in
            both trees.  If you need to eliminate this subtree
            sharing, then create a deep copy of each tree.
        """
        return self._trees(edge, complete, memo={}, tree_class=tree_class)

    def _trees(self, edge, complete, memo, tree_class):
        """
        A helper function for L{trees}.
        @param memo: A dictionary used to record the trees that we've
            generated for each edge, so that when we see an edge more
            than once, we can reuse the same trees.
        """
        # If we've seen this edge before, then reuse our old answer.
        if memo.has_key(edge): return memo[edge]

        trees = []

        # when we're reading trees off the chart, don't use incomplete edges
        if complete and edge.is_incomplete():
            return trees

        # Until we're done computing the trees for edge, set
        # memo[edge] to be empty.  This has the effect of filtering
        # out any cyclic trees (i.e., trees that contain themselves as
        # descendants), because if we reach this edge via a cycle,
        # then it will appear that the edge doesn't generate any
        # trees.
        memo[edge] = []
        
        # Leaf edges.
        if isinstance(edge, LeafEdge):
            leaf = self._tokens[edge.start()]
            memo[edge] = leaf
            return [leaf]
        
        # Each child pointer list can be used to form trees.
        for cpl in self.child_pointer_lists(edge):
            # Get the set of child choices for each child pointer.
            # child_choices[i] is the set of choices for the tree's
            # ith child.
            child_choices = [self._trees(cp, complete, memo, tree_class)
                             for cp in cpl]

            # Kludge to ensure child_choices is a doubly-nested list
            if len(child_choices) > 0 and type(child_choices[0]) == type(""):
                child_choices = [child_choices]

            # For each combination of children, add a tree.
            for children in self._choose_children(child_choices):
                lhs = edge.lhs().symbol()
                trees.append(tree_class(lhs, children))

        # If the edge is incomplete, then extend it with "partial trees":
        if edge.is_incomplete():
            unexpanded = [tree_class(elt,[])
                          for elt in edge.rhs()[edge.dot():]]
            for tree in trees:
                tree.extend(unexpanded)

        # Update the memoization dictionary.
        memo[edge] = trees

        # Return the list of trees.
        return trees

    def _choose_children(self, child_choices):
        """
        A helper function for L{_trees} that finds the possible sets
        of subtrees for a new tree.
        
        @param child_choices: A list that specifies the options for
        each child.  In particular, C{child_choices[i]} is a list of
        tokens and subtrees that can be used as the C{i}th child.
        """
        children_lists = [[]]
        for child_choice in child_choices:
            children_lists = [child_list+[child]
                              for child in child_choice
                              for child_list in children_lists]
        return children_lists
    
    def child_pointer_lists(self, edge):
        """
        @rtype: C{list} of C{list} of C{Edge}
        @return: The set of child pointer lists for the given edge.
            Each child pointer list is a list of edges that have
            been used to form this edge.
        """
        # Make a copy, in case they modify it.
        return self._edge_to_cpls.get(edge, {}).keys()

    #////////////////////////////////////////////////////////////
    # Display
    #////////////////////////////////////////////////////////////
    def pp_edge(self, edge, width=None):
        """
        @return: A pretty-printed string representation of a given edge
            in this chart.
        @rtype: C{string}
        @param width: The number of characters allotted to each
            index in the sentence.
        """
        if width is None: width = 50/(self.num_leaves()+1)
        (start, end) = (edge.start(), edge.end())

        str = '|' + ('.'+' '*(width-1))*start

        # Zero-width edges are "#" if complete, ">" if incomplete
        if start == end:
            if edge.is_complete(): str += '#'
            else: str += '>'

        # Spanning complete edges are "[===]"; Other edges are
        # "[---]" if complete, "[--->" if incomplete
        elif edge.is_complete() and edge.span() == (0,self._num_leaves):
            str += '['+('='*width)*(end-start-1) + '='*(width-1)+']'
        elif edge.is_complete():
            str += '['+('-'*width)*(end-start-1) + '-'*(width-1)+']'
        else:
            str += '['+('-'*width)*(end-start-1) + '-'*(width-1)+'>'
        
        str += (' '*(width-1)+'.')*(self._num_leaves-end)
        return str + '| %s ' % edge

    def pp_leaves(self, width=None):
        """
        @return: A pretty-printed string representation of this
            chart's leaves.  This string can be used as a header
            for calls to L{pp_edge}.
        """
        if width is None: width = 50/(self.num_leaves()+1)
        
        if self._tokens is not None and width>1:
            header = '|.'
            for tok in self._tokens:
                header += tok[:width-1].center(width-1)+'.'
            header += '|'
        else:
            header = ''

        return header

    def pp(self, width=None):
        """
        @return: A pretty-printed string representation of this chart.
        @rtype: C{string}
        @param width: The number of characters allotted to each
            index in the sentence.
        """
        if width is None: width = 50/(self.num_leaves()+1)
        # sort edges: primary key=length, secondary key=start index.
        # (and filter out the token edges)
        edges = [(e.length(), e.start(), e) for e in self]
        edges.sort()
        edges = [e for (_,_,e) in edges]
        
        return (self.pp_leaves(width) + '\n' +
                '\n'.join(self.pp_edge(edge, width) for edge in edges))
                
    #////////////////////////////////////////////////////////////
    # Display: Dot (AT&T Graphviz)
    #////////////////////////////////////////////////////////////

    def dot_digraph(self):
        # Header
        s = 'digraph nltk_chart {\n'
        #s += '  size="5,5";\n'
        s += '  rankdir=LR;\n'
        s += '  node [height=0.1,width=0.1];\n'
        s += '  node [style=filled, color="lightgray"];\n'

        # Set up the nodes
        for y in range(self.num_edges(), -1, -1):
            if y == 0:
                s += '  node [style=filled, color="black"];\n'
            for x in range(self.num_leaves()+1):
                if y == 0 or (x <= self._edges[y-1].start() or
                              x >= self._edges[y-1].end()):
                    s += '  %04d.%04d [label=""];\n' % (x,y)

        # Add a spacer
        s += '  x [style=invis]; x->0000.0000 [style=invis];\n'

        # Declare ranks.
        for x in range(self.num_leaves()+1):
            s += '  {rank=same;'
            for y in range(self.num_edges()+1):
                if y == 0 or (x <= self._edges[y-1].start() or
                              x >= self._edges[y-1].end()):
                    s += ' %04d.%04d' % (x,y)
            s += '}\n'

        # Add the leaves
        s += '  edge [style=invis, weight=100];\n'
        s += '  node [shape=plaintext]\n'
        s += '  0000.0000'
        for x in range(self.num_leaves()):
            s += '->%s->%04d.0000' % (self.leaf(x), x+1)
        s += ';\n\n'

        # Add the edges
        s += '  edge [style=solid, weight=1];\n'
        for y, edge in enumerate(self):
            for x in range(edge.start()):
                s += ('  %04d.%04d -> %04d.%04d [style="invis"];\n' %
                      (x, y+1, x+1, y+1))
            s += ('  %04d.%04d -> %04d.%04d [label="%s"];\n' %
                  (edge.start(), y+1, edge.end(), y+1, edge))
            for x in range(edge.end(), self.num_leaves()):
                s += ('  %04d.%04d -> %04d.%04d [style="invis"];\n' %
                      (x, y+1, x+1, y+1))
        s += '}\n'
        return s

########################################################################
##  Chart Rules
########################################################################

class ChartRuleI(object):
    """
    A rule that specifies what new edges are licensed by any given set
    of existing edges.  Each chart rule expects a fixed number of
    edges, as indicated by the class variable L{NUM_EDGES}.  In
    particular:
    
      - A chart rule with C{NUM_EDGES=0} specifies what new edges are
        licensed, regardless of existing edges.

      - A chart rule with C{NUM_EDGES=1} specifies what new edges are
        licensed by a single existing edge.

      - A chart rule with C{NUM_EDGES=2} specifies what new edges are
        licensed by a pair of existing edges.
      
    @type NUM_EDGES: C{int}
    @cvar NUM_EDGES: The number of existing edges that this rule uses
        to license new edges.  Typically, this number ranges from zero
        to two.
    """
    def apply(self, chart, grammar, *edges):
        """
        Add the edges licensed by this rule and the given edges to the
        chart.

        @type edges: C{list} of L{EdgeI}
        @param edges: A set of existing edges.  The number of edges
            that should be passed to C{apply} is specified by the
            L{NUM_EDGES} class variable.
        @rtype: C{list} of L{EdgeI}
        @return: A list of the edges that were added.
        """
        raise AssertionError, 'ChartRuleI is an abstract interface'

    def apply_iter(self, chart, grammar, *edges):
        """
        @return: A generator that will add edges licensed by this rule
            and the given edges to the chart, one at a time.  Each
            time the generator is resumed, it will either add a new
            edge and yield that edge; or return.
        @rtype: C{iter} of L{EdgeI}
        
        @type edges: C{list} of L{EdgeI}
        @param edges: A set of existing edges.  The number of edges
            that should be passed to C{apply} is specified by the
            L{NUM_EDGES} class variable.
        """
        raise AssertionError, 'ChartRuleI is an abstract interface'

    def apply_everywhere(self, chart, grammar):
        """
        Add all the edges licensed by this rule and the edges in the
        chart to the chart.
        
        @rtype: C{list} of L{EdgeI}
        @return: A list of the edges that were added.
        """
        raise AssertionError, 'ChartRuleI is an abstract interface'

    def apply_everywhere_iter(self, chart, grammar):
        """
        @return: A generator that will add all edges licensed by
            this rule, given the edges that are currently in the
            chart, one at a time.  Each time the generator is resumed,
            it will either add a new edge and yield that edge; or
            return.
        @rtype: C{iter} of L{EdgeI}
        """
        raise AssertionError, 'ChartRuleI is an abstract interface'
        
class AbstractChartRule(object):
    """
    An abstract base class for chart rules.  C{AbstractChartRule}
    provides:
      - A default implementation for C{apply}, based on C{apply_iter}.
      - A default implementation for C{apply_everywhere_iter},
        based on C{apply_iter}.
      - A default implementation for C{apply_everywhere}, based on
        C{apply_everywhere_iter}.  Currently, this implementation
        assumes that C{NUM_EDGES}<=3.
      - A default implementation for C{__str__}, which returns a
        name basd on the rule's class name.
    """

    # Subclasses must define apply_iter.
    def apply_iter(self, chart, grammar, *edges):
        raise AssertionError, 'AbstractChartRule is an abstract class'

    # Default: loop through the given number of edges, and call
    # self.apply() for each set of edges.
    def apply_everywhere_iter(self, chart, grammar):
        if self.NUM_EDGES == 0:
            for new_edge in self.apply_iter(chart, grammar):
                yield new_edge

        elif self.NUM_EDGES == 1:
            for e1 in chart:
                for new_edge in self.apply_iter(chart, grammar, e1):
                    yield new_edge

        elif self.NUM_EDGES == 2:
            for e1 in chart:
                for e2 in chart:
                    for new_edge in self.apply_iter(chart, grammar, e1, e2):
                        yield new_edge

        elif self.NUM_EDGES == 3:
            for e1 in chart:
                for e2 in chart:
                    for e3 in chart:
                        for new_edge in self.apply_iter(chart,grammar,e1,e2,e3):
                            yield new_edge

        else:
            raise AssertionError, 'NUM_EDGES>3 is not currently supported'

    # Default: delegate to apply_iter.
    def apply(self, chart, grammar, *edges):
        return list(self.apply_iter(chart, grammar, *edges))

    # Default: delegate to apply_everywhere_iter.
    def apply_everywhere(self, chart, grammar):
        return list(self.apply_everywhere_iter(chart, grammar))

    # Default: return a name based on the class name.
    def __str__(self):
        # Add spaces between InitialCapsWords.
        return re.sub('([a-z])([A-Z])', r'\1 \2', self.__class__.__name__)

#////////////////////////////////////////////////////////////
# Fundamental Rule
#////////////////////////////////////////////////////////////
class FundamentalRule(AbstractChartRule):
    """
    A rule that joins two adjacent edges to form a single combined
    edge.  In particular, this rule specifies that any pair of edges:
    
        - [AS{->}S{alpha}*BS{beta}][i:j]
        - [BS{->}S{gamma}*][j:k]
    licenses the edge:
        - [AS{->}S{alpha}B*S{beta}][i:j]
    """
    NUM_EDGES = 2
    def apply_iter(self, chart, grammar, left_edge, right_edge):
        # Make sure the rule is applicable.
        if not (left_edge.end() == right_edge.start() and
                left_edge.next() == right_edge.lhs() and
                left_edge.is_incomplete() and right_edge.is_complete()):
            return

        # Construct the new edge.
        new_edge = TreeEdge(span=(left_edge.start(), right_edge.end()),
                            lhs=left_edge.lhs(), rhs=left_edge.rhs(),
                            dot=left_edge.dot()+1)

        # Add it to the chart, with appropriate child pointers.
        changed_chart = False
        for cpl1 in chart.child_pointer_lists(left_edge):
            if chart.insert(new_edge, cpl1+(right_edge,)):
                changed_chart = True

        # If we changed the chart, then generate the edge.
        if changed_chart: yield new_edge

class SingleEdgeFundamentalRule(AbstractChartRule):
    """
    A rule that joins a given edge with adjacent edges in the chart,
    to form combined edges.  In particular, this rule specifies that
    either of the edges:
        - [AS{->}S{alpha}*BS{beta}][i:j]
        - [BS{->}S{gamma}*][j:k]
    licenses the edge:
        - [AS{->}S{alpha}B*S{beta}][i:j]
    if the other edge is already in the chart.
    @note: This is basically L{FundamentalRule}, with one edge is left
        unspecified.
    """
    NUM_EDGES = 1

    _fundamental_rule = FundamentalRule()
    
    def apply_iter(self, chart, grammar, edge1):
        fr = self._fundamental_rule
        if edge1.is_incomplete():
            # edge1 = left_edge; edge2 = right_edge
            for edge2 in chart.select(start=edge1.end(), is_complete=True,
                                     lhs=edge1.next()):
                for new_edge in fr.apply_iter(chart, grammar, edge1, edge2):
                    yield new_edge
        else:
            # edge2 = left_edge; edge1 = right_edge
            for edge2 in chart.select(end=edge1.start(), is_complete=False,
                                     next=edge1.lhs()):
                for new_edge in fr.apply_iter(chart, grammar, edge2, edge1):
                    yield new_edge

    def __str__(self): return 'Fundamental Rule'
    
class BottomUpInitRule(AbstractChartRule):
    """
    A rule licensing any edges corresponding to terminals in the
    text.  In particular, this rule licenses the leaf edge:
        - [wS{->}*][i:i+1]
    for C{w} is a word in the text, where C{i} is C{w}'s index.
    """
    NUM_EDGES = 0
    def apply_iter(self, chart, grammar):
        for index in range(chart.num_leaves()):
            new_edge = LeafEdge(chart.leaf(index), index)
            if chart.insert(new_edge, ()):
                yield new_edge

#////////////////////////////////////////////////////////////
# Top-Down Parsing
#////////////////////////////////////////////////////////////
    

class TopDownInitRule(AbstractChartRule):
    """
    A rule licensing edges corresponding to the grammar productions for
    the grammar's start symbol.  In particular, this rule specifies that:
        - [SS{->}*S{alpha}][0:i]
    is licensed for each grammar production C{SS{->}S{alpha}}, where
    C{S} is the grammar's start symbol.
    """
    NUM_EDGES = 0
    def apply_iter(self, chart, grammar):
        for prod in grammar.productions(lhs=grammar.start()):
            new_edge = TreeEdge.from_production(prod, 0)
            if chart.insert(new_edge, ()):
                yield new_edge

class TopDownExpandRule(AbstractChartRule):
    """
    A rule licensing edges corresponding to the grammar productions
    for the nonterminal following an incomplete edge's dot.  In
    particular, this rule specifies that:
        - [AS{->}S{alpha}*BS{beta}][i:j]
    licenses the edge:
        - [BS{->}*S{gamma}][j:j]
    for each grammar production C{BS{->}S{gamma}}.
    """
    NUM_EDGES = 1
    def apply_iter(self, chart, grammar, edge):
        if edge.is_complete(): return
        for prod in grammar.productions(lhs=edge.next()):
            new_edge = TreeEdge.from_production(prod, edge.end())
            if chart.insert(new_edge, ()):
                yield new_edge

class TopDownMatchRule(AbstractChartRule):
    """
    A rule licensing an edge corresponding to a terminal following an
    incomplete edge's dot.  In particular, this rule specifies that:
        - [AS{->}S{alpha}*w{beta}][i:j]
    licenses the leaf edge:
        - [wS{->}*][j:j+1]
    if the C{j}th word in the text is C{w}.
    """
    NUM_EDGES = 1
    def apply_iter(self, chart, grammar, edge):
        if edge.is_complete() or edge.end() >= chart.num_leaves(): return
        index = edge.end()
        leaf = chart.leaf(index)
        if edge.next() == leaf:
            new_edge = LeafEdge(leaf, index)
            if chart.insert(new_edge, ()):
                yield new_edge

# Add a cache, to prevent recalculating.
class CachedTopDownInitRule(TopDownInitRule):
    """
    A cached version of L{TopDownInitRule}.  After the first time this
    rule is applied, it will not generate any more edges.

    If C{chart} or C{grammar} are changed, then the cache is flushed.
    """
    def __init__(self):
        AbstractChartRule.__init__(self)
        self._done = (None, None)

    def apply_iter(self, chart, grammar):
        # If we've already applied this rule, and the chart & grammar
        # have not changed, then just return (no new edges to add).
        if self._done[0] is chart and self._done[1] is grammar: return

        # Add all the edges indicated by the top down init rule.
        for e in TopDownInitRule.apply_iter(self, chart, grammar):
            yield e

        # Record the fact that we've applied this rule.
        self._done = (chart, grammar)

    def __str__(self): return 'Top Down Init Rule'
    
class CachedTopDownExpandRule(TopDownExpandRule):
    """
    A cached version of L{TopDownExpandRule}.  After the first time
    this rule is applied to an edge with a given C{end} and C{next},
    it will not generate any more edges for edges with that C{end} and
    C{next}.
    
    If C{chart} or C{grammar} are changed, then the cache is flushed.
    """
    def __init__(self):
        AbstractChartRule.__init__(self)
        self._done = {}
        
    def apply_iter(self, chart, grammar, edge):
        # If we've already applied this rule to an edge with the same
        # next & end, and the chart & grammar have not changed, then
        # just return (no new edges to add).
        done = self._done.get((edge.next(), edge.end()), (None,None))
        if done[0] is chart and done[1] is grammar: return

        # Add all the edges indicated by the top down expand rule.
        for e in TopDownExpandRule.apply_iter(self, chart, grammar, edge):
            yield e
            
        # Record the fact that we've applied this rule.
        self._done[edge.next(), edge.end()] = (chart, grammar)
    
    def __str__(self): return 'Top Down Expand Rule'

#////////////////////////////////////////////////////////////
# Bottom-Up Parsing
#////////////////////////////////////////////////////////////

class BottomUpInitRule(AbstractChartRule):
    """
    A rule licensing any edges corresponding to terminals in the
    text.  In particular, this rule licenses the leaf edge:
        - [wS{->}*][i:i+1]
    for C{w} is a word in the text, where C{i} is C{w}'s index.
    """
    NUM_EDGES = 0
    def apply_iter(self, chart, grammar):
        for index in range(chart.num_leaves()):
            new_edge = LeafEdge(chart.leaf(index), index)
            if chart.insert(new_edge, ()):
                yield new_edge

class BottomUpPredictRule(AbstractChartRule):
    """
    A rule licensing any edge corresponding to a production whose
    right-hand side begins with a complete edge's left-hand side.  In
    particular, this rule specifies that:
        - [AS{->}S{alpha}*]
    licenses the edge:
        - [BS{->}*AS{beta}]
    for each grammar production C{BS{->}AS{beta}}
    """
    NUM_EDGES = 1
    def apply_iter(self, chart, grammar, edge):
        if edge.is_incomplete(): return
        for prod in grammar.productions(rhs=edge.lhs()):
            new_edge = TreeEdge.from_production(prod, edge.start())
            if chart.insert(new_edge, ()):
                yield new_edge

#////////////////////////////////////////////////////////////
# Earley Parsing
#////////////////////////////////////////////////////////////

class CompleterRule(AbstractChartRule):
    """
    A rule that joins a given complete edge with adjacent incomplete
    edges in the chart, to form combined edges.  In particular, this
    rule specifies that:
        - [BS{->}S{gamma}*][j:k]
    licenses the edge:
        - [AS{->}S{alpha}B*S{beta}][i:j]
    given that the chart contains:
        - [AS{->}S{alpha}*BS{beta}][i:j]
    @note: This is basically L{FundamentalRule}, with the left edge
        left unspecified.
    """
    NUM_EDGES = 1
    
    _fundamental_rule = FundamentalRule()
    
    def apply_iter(self, chart, grammar, right_edge):
        if right_edge.is_incomplete(): return
        fr = self._fundamental_rule
        for left_edge in chart.select(end=right_edge.start(),
                                     is_complete=False,
                                     next=right_edge.lhs()):
            for e in fr.apply_iter(chart, grammar, left_edge, right_edge):
                yield e

    def __str__(self): return 'Completer Rule'
    
class ScannerRule(AbstractChartRule):
    """
    A rule licensing a leaf edge corresponding to a part-of-speech
    terminal following an incomplete edge's dot.  In particular, this
    rule specifies that:
        - [AS{->}S{alpha}*PS{beta}][i:j]
    licenses the edges:
        - [PS{->}w*][j:j+1]
        - [wS{->}*][j:j+1]
    if the C{j}th word in the text is C{w}; and C{P} is a valid part
    of speech for C{w}.
    """
    NUM_EDGES = 1
    def __init__(self, word_to_pos_lexicon):
        self._word_to_pos = word_to_pos_lexicon

    def apply_iter(self, chart, gramar, edge):
        if edge.is_complete() or edge.end()>=chart.num_leaves(): return
        index = edge.end()
        leaf = chart.leaf(index)
        if edge.next() in self._word_to_pos.get(leaf, []):
            new_leaf_edge = LeafEdge(leaf, index)
            if chart.insert(new_leaf_edge, ()):
                yield new_leaf_edge
            new_pos_edge = TreeEdge((index,index+1), edge.next(),
                                    [leaf], 1)
            if chart.insert(new_pos_edge, (new_leaf_edge,)):
                yield new_pos_edge

# This is just another name for TopDownExpandRule:
class PredictorRule(TopDownExpandRule): pass

########################################################################
##  Simple Earley Chart Parser
########################################################################

class EarleyChartParse(AbstractParse):
    """
    A chart parser implementing the Earley parsing algorithm:

        - For each index I{end} in [0, 1, ..., N]:
          - For each I{edge} s.t. I{edge}.end = I{end}:
            - If I{edge} is incomplete, and I{edge}.next is not a part
              of speech:
                - Apply PredictorRule to I{edge}
            - If I{edge} is incomplete, and I{edge}.next is a part of
              speech:
                - Apply ScannerRule to I{edge}
            - If I{edge} is complete:
                - Apply CompleterRule to I{edge}
        - Return any complete parses in the chart

    C{EarleyChartParse} uses a X{lexicon} to decide whether a leaf
    has a given part of speech.  This lexicon is encoded as a
    dictionary that maps each word to a list of parts of speech that
    word can have.
    """
    def __init__(self, grammar, lexicon, trace=0):
        """
        Create a new Earley chart parser, that uses C{grammar} to
        parse texts.
        
        @type grammar: C{cfg.Grammar}
        @param grammar: The grammar used to parse texts.
        @type lexicon: C{dict} from C{string} to (C{list} of C{string})
        @param lexicon: A lexicon of words that records the parts of
            speech that each word can have.  Each key is a word, and
            the corresponding value is a list of parts of speech.
        @type trace: C{int}
        @param trace: The level of tracing that should be used when
            parsing a text.  C{0} will generate no tracing output;
            and higher numbers will produce more verbose tracing
            output.
        """
        self._grammar = grammar
        self._lexicon = lexicon
        self._trace = trace
        AbstractParse.__init__(self)

    def get_parse_list(self, tokens, tree_class=Tree):
        chart = Chart(list(tokens))
        grammar = self._grammar

        # Width, for printing trace edges.
        w = 50/(chart.num_leaves()+1)
        if self._trace > 0: print ' ', chart.pp_leaves(w)

        # Initialize the chart with a special "starter" edge.
        root = cfg.Nonterminal('[INIT]')
        edge = TreeEdge((0,0), root, (grammar.start(),))
        chart.insert(edge, ())

        # Create the 3 rules:
        predictor = PredictorRule()
        completer = CompleterRule()
        scanner = ScannerRule(self._lexicon)

        for end in range(chart.num_leaves()+1):
            if self._trace > 1: print 'Processing queue %d' % end
            for edge in chart.select(end=end):
                if edge.is_incomplete():
                    for e in predictor.apply(chart, grammar, edge):
                        if self._trace > 0:
                            print 'Predictor', chart.pp_edge(e,w)
                if edge.is_incomplete():
                    for e in scanner.apply(chart, grammar, edge):
                        if self._trace > 0:
                            print 'Scanner  ', chart.pp_edge(e,w)
                if edge.is_complete():
                    for e in completer.apply(chart, grammar, edge):
                        if self._trace > 0:
                            print 'Completer', chart.pp_edge(e,w)

        # Output a list of complete parses.
        return chart.parses(grammar.start(), tree_class=tree_class)
            
########################################################################
##  Generic Chart Parser
########################################################################

TD_STRATEGY = [CachedTopDownInitRule(), CachedTopDownExpandRule(), 
               TopDownMatchRule(), SingleEdgeFundamentalRule()]
BU_STRATEGY = [BottomUpInitRule(), BottomUpPredictRule(),
               SingleEdgeFundamentalRule()]

class ChartParse(AbstractParse):
    """
    A generic chart parser.  A X{strategy}, or list of
    L{ChartRules<ChartRuleI>}, is used to decide what edges to add to
    the chart.  In particular, C{ChartParse} uses the following
    algorithm to parse texts:

        - Until no new edges are added:
          - For each I{rule} in I{strategy}:
            - Apply I{rule} to any applicable edges in the chart.
        - Return any complete parses in the chart
    """
    def __init__(self, grammar, strategy, trace=0):
        """
        Create a new chart parser, that uses C{grammar} to parse
        texts.

        @type grammar: L{cfg.Grammar}
        @param grammar: The grammar used to parse texts.
        @type strategy: C{list} of L{ChartRuleI}
        @param strategy: A list of rules that should be used to decide
            what edges to add to the chart.
        @type trace: C{int}
        @param trace: The level of tracing that should be used when
            parsing a text.  C{0} will generate no tracing output;
            and higher numbers will produce more verbose tracing
            output.
        """
        self._grammar = grammar
        self._strategy = strategy
        self._trace = trace
        AbstractParse.__init__(self)

    def get_parse_list(self, tokens, tree_class=Tree):
        chart = Chart(list(tokens))
        grammar = self._grammar

        # Width, for printing trace edges.
        w = 50/(chart.num_leaves()+1)
        if self._trace > 0: print chart.pp_leaves(w)
        
        edges_added = 1
        while edges_added > 0:
            edges_added = 0
            for rule in self._strategy:
                edges_added_by_rule = 0
                for e in rule.apply_everywhere(chart, grammar):
                    if self._trace > 0 and edges_added_by_rule == 0:
                        print '%s:' % rule
                    edges_added_by_rule += 1
                    if self._trace > 1: print chart.pp_edge(e,w)
                if self._trace == 1 and edges_added_by_rule > 0:
                    print '  - Added %d edges' % edges_added_by_rule
                edges_added += edges_added_by_rule
        
        # Return a list of complete parses.
        return chart.parses(grammar.start(), tree_class=tree_class)

########################################################################
##  Stepping Chart Parser
########################################################################

class SteppingChartParse(ChartParse):
    """
    A C{ChartParse} that allows you to step through the parsing
    process, adding a single edge at a time.  It also allows you to
    change the parser's strategy or grammar midway through parsing a
    text.

    The C{initialize} method is used to start parsing a text.  C{step}
    adds a single edge to the chart.  C{set_strategy} changes the
    strategy used by the chart parser.  C{parses} returns the set of
    parses that has been found by the chart parser.

    @ivar _restart: Records whether the parser's strategy, grammar,
        or chart has been changed.  If so, then L{step} must restart
        the parsing algorithm.
    """
    def __init__(self, grammar, strategy=None, trace=0):
        self._chart = None
        self._current_chartrule = None
        self._restart = False
        ChartParse.__init__(self, grammar, strategy, trace)

    #////////////////////////////////////////////////////////////
    # Initialization
    #////////////////////////////////////////////////////////////

    def initialize(self, tokens):
        "Begin parsing the given tokens."
        self._chart = Chart(list(tokens))
        self._restart = True

    #////////////////////////////////////////////////////////////
    # Stepping
    #////////////////////////////////////////////////////////////

    def step(self):
        """
        @return: A generator that adds edges to the chart, one at a
        time.  Each time the generator is resumed, it adds a single
        edge and yields that edge.  If no more edges can be added,
        then it yields C{None}.

        If the parser's strategy, grammar, or chart is changed, then
        the generator will continue adding edges using the new
        strategy, grammar, or chart.

        Note that this generator never terminates, since the grammar
        or strategy might be changed to values that would add new
        edges.  Instead, it yields C{None} when no more edges can be
        added with the current strategy and grammar.
        """
        if self._chart is None:
            raise ValueError, 'Parser must be initialized first'
        while 1:
            self._restart = False
            w = 50/(self._chart.num_leaves()+1)
            
            for e in self._parse():
                if self._trace > 1: print self._current_chartrule
                if self._trace > 0: print self._chart.pp_edge(e,w)
                yield e
                if self._restart: break
            else:
                yield None # No more edges.

    def _parse(self):
        """
        A generator that implements the actual parsing algorithm.
        L{step} iterates through this generator, and restarts it
        whenever the parser's strategy, grammar, or chart is modified.
        """
        chart = self._chart
        grammar = self._grammar
        edges_added = 1
        while edges_added > 0:
            edges_added = 0
            for rule in self._strategy:
                self._current_chartrule = rule
                for e in rule.apply_everywhere_iter(chart, grammar):
                    edges_added += 1
                    yield e

    #////////////////////////////////////////////////////////////
    # Accessors
    #////////////////////////////////////////////////////////////

    def strategy(self):
        "@return: The strategy used by this parser."
        return self._strategy

    def grammar(self):
        "@return: The grammar used by this parser."
        return self._grammar

    def chart(self):
        "@return: The chart that is used by this parser."
        return self._chart

    def current_chartrule(self):
        "@return: The chart rule used to generate the most recent edge."
        return self._current_chartrule

    def parses(self, tree_class=Tree):
        "@return: The parse trees currently contained in the chart."
        return self._chart.parses(self._grammar.start(), tree_class)
    
    #////////////////////////////////////////////////////////////
    # Parser modification
    #////////////////////////////////////////////////////////////

    def set_strategy(self, strategy):
        """
        Change the startegy that the parser uses to decide which edges
        to add to the chart.
        @type strategy: C{list} of L{ChartRuleI}
        @param strategy: A list of rules that should be used to decide
            what edges to add to the chart.
        """
        if strategy == self._strategy: return
        self._strategy = strategy[:] # Make a copy.
        self._restart = True

    def set_grammar(self, grammar):
        "Change the grammar used by the parser."
        if grammar is self._grammar: return
        self._grammar = grammar
        self._restart = True

    def set_chart(self, chart):
        "Load a given chart into the chart parser."
        if chart is self._chart: return
        self._chart = chart
        self._restart = True

    #////////////////////////////////////////////////////////////
    # Standard parser methods
    #////////////////////////////////////////////////////////////

    def get_parse_list(self, token, tree_class=Tree):
        # Initialize ourselves.
        self.initialize(token)

        # Step until no more edges are generated.
        for e in self.step():
            if e is None: break
            
        # Return a list of complete parses.
        return self.parses(tree_class=tree_class)

########################################################################
##  Demo Code
########################################################################

def demo():
    """
    A demonstration of the chart parsers.
    """
    import sys, time
    
    # Define some nonterminals
    S, VP, NP, PP = cfg.nonterminals('S, VP, NP, PP')
    V, N, P, Name, Det = cfg.nonterminals('V, N, P, Name, Det')

    # Define some grammatical productions.
    grammatical_productions = [
        cfg.Production(S, [NP, VP]),  cfg.Production(PP, [P, NP]),
        cfg.Production(NP, [Det, N]), cfg.Production(NP, [NP, PP]),
        cfg.Production(VP, [VP, PP]), cfg.Production(VP, [V, NP]),
        cfg.Production(VP, [V]),]

    # Define some lexical productions.
    lexical_productions = [
        cfg.Production(NP, ['John']), cfg.Production(NP, ['I']), 
        cfg.Production(Det, ['the']), cfg.Production(Det, ['my']),
        cfg.Production(Det, ['a']),
        cfg.Production(N, ['dog']),   cfg.Production(N, ['cookie']),
        cfg.Production(V, ['ate']),  cfg.Production(V, ['saw']),
        cfg.Production(P, ['with']), cfg.Production(P, ['under']),
        ]

    # Convert the grammar productions to an earley-style lexicon.
    earley_lexicon = {}
    for prod in lexical_productions:
        earley_lexicon.setdefault(prod.rhs()[0], []).append(prod.lhs())

    # The grammar for ChartParse and SteppingChartParse:
    grammar = cfg.Grammar(S, grammatical_productions+lexical_productions)

    # The grammar for EarleyChartParse:
    earley_grammar = cfg.Grammar(S, grammatical_productions)

    # Tokenize a sample sentence.
    sent = 'I saw John with a dog with my cookie'
    print "Sentence:\n", sent
    from nltk import tokenize
    tokens = list(tokenize.whitespace(sent))

    print tokens

    # Ask the user which parser to test
    print '  1: Top-down chart parser'
    print '  2: Bottom-up chart parser'
    print '  3: Earley parser'
    print '  4: Stepping chart parser (alternating top-down & bottom-up)'
    print '  5: All parsers'
    print '\nWhich parser (1-5)? ',
    choice = sys.stdin.readline().strip()
    print
    if choice not in '12345':
        print 'Bad parser number'
        return

    # Keep track of how long each parser takes.
    times = {}

    # Run the top-down parser, if requested.
    if choice in ('1', '5'):
        cp = ChartParse(grammar, TD_STRATEGY, trace=2)
        t = time.time()
        parses = cp.get_parse_list(tokens)
        times['top down'] = time.time()-t
        assert len(parses)==5, 'Not all parses found'
        for tree in parses: print tree

    # Run the bottom-up parser, if requested.
    if choice in ('2', '5'):
        cp = ChartParse(grammar, BU_STRATEGY, trace=2)
        t = time.time()
        parses = cp.get_parse_list(tokens)
        times['bottom up'] = time.time()-t
        assert len(parses)==5, 'Not all parses found'
        for tree in parses: print tree

    # Run the earley, if requested.
    if choice in ('3', '5'):
        cp = EarleyChartParse(earley_grammar, earley_lexicon, trace=1)
        t = time.time()
        parses = cp.get_parse_list(tokens)
        times['Earley parser'] = time.time()-t
        assert len(parses)==5, 'Not all parses found'
        for tree in parses: print tree

    # Run the stepping parser, if requested.
    if choice in ('4', '5'):
        t = time.time()
        cp = SteppingChartParse(grammar, trace=1)
        cp.initialize(tokens)
        for i in range(5):
            print '*** SWITCH TO TOP DOWN'
            cp.set_strategy(TD_STRATEGY)
            for j, e in enumerate(cp.step()):
                if j>20 or e is None: break
            print '*** SWITCH TO BOTTOM UP'
            cp.set_strategy(BU_STRATEGY)
            for j, e in enumerate(cp.step()):
                if j>20 or e is None: break
        times['stepping'] = time.time()-t
        assert len(cp.parses())==5, 'Not all parses found'
        for parse in cp.parses(): print parse

    # Print the times of all parsers:
    maxlen = max(len(key) for key in times.keys())
    format = '%' + `maxlen` + 's parser: %6.3fsec'
    times_items = times.items()
    times_items.sort(lambda a,b:cmp(a[1], b[1]))
    for (parser, t) in times_items:
        print format % (parser, t)
            
if __name__ == '__main__': demo()

########NEW FILE########
__FILENAME__ = featurechart
# Natural Language Toolkit: Chart Parser for Feature-Based Grammars
#
# Copyright (C) 2001-2007 NLTK Project
# Author: Rob Speer <rspeer@mit.edu>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT
#
# $Id: featurechart.py 4107 2007-02-01 00:07:42Z stevenbird $

"""
Extension of chart parsing implementation to handle grammars with
feature structures as nodes.
"""

import yaml
from parse import *
#from chart import *
#from category import *
from nltk import cfg

from featurelite import *

def load_earley(filename, trace=1):
    """
    Load a grammar from a file, and build an Earley feature parser based on
    that grammar.

    You can optionally specify a tracing level, for how much output you
    want to see:

    0: No output.
    1: Show edges from scanner and completer rules (not predictor).
    2 (default): Show all edges as they are added to the chart.
    3: Show all edges, plus the results of successful unifications.
    4: Show all edges, plus the results of all attempted unifications.
    5: Show all edges, plus the results of all attempted unifications,
       including those with cached results.
    """

    grammar = GrammarFile.read_file(filename)
    return grammar.earley_parser(trace)

class FeatureTreeEdge(TreeEdge):
    """
    FIXME: out of date documentation
    
    A modification of L{TreeEdge} to handle nonterminals with features
    (known as L{Categories<Category>}.

    In addition to the span, left-hand side, right-hand side, and dot position
    (described at L{TreeEdge}), a C{FeatureTreeEdge} includes X{vars}, a
    set of L{FeatureBindings} saying which L{FeatureVariable}s are set to which
    values.

    These values are applied when examining the C{lhs} or C{rhs} of a
    C{FeatureTreeEdge}.
    
    For more information about edges, see the L{EdgeI} interface.
    """
    def __init__(self, span, lhs, rhs, dot=0, vars=None):
        """
        Construct a new C{FeatureTreeEdge}.
        
        @type span: C{(int, int)}
        @param span: A tuple C{(s,e)}, where C{subtokens[s:e]} is the
            portion of the sentence that is consistent with the new
            edge's structure.
        @type lhs: L{Category}
        @param lhs: The new edge's left-hand side, specifying the
            hypothesized tree's node value.
        @type rhs: C{list} of (L{Category} and C{string})
        @param rhs: The new edge's right-hand side, specifying the
            hypothesized tree's children.
        @type dot: C{int}
        @param dot: The position of the new edge's dot.  This position
            specifies what prefix of the production's right hand side
            is consistent with the text.  In particular, if
            C{sentence} is the list of subtokens in the sentence, then
            C{subtokens[span[0]:span[1]]} can be spanned by the
            children specified by C{rhs[:dot]}.
        @type vars: L{FeatureBindings}
        @param vars: The bindings specifying what values certain variables in
            this edge must have.
        """
        TreeEdge.__init__(self, span, lhs, rhs, dot)
        if vars is None: vars = {}
        self._vars = vars

    @staticmethod
    def from_production(production, index, bindings=None):
        """
        @return: A new C{FeatureTreeEdge} formed from the given production.
            The new edge's left-hand side and right-hand side will
            be taken from C{production}; its span will be C{(index,
            index)}; its dot position will be C{0}, and it may have specified
            variables set.
        @rtype: L{FeatureTreeEdge}
        """
        return FeatureTreeEdge(span=(index, index), lhs=production.lhs(),
                               rhs=production.rhs(), dot=0, vars=bindings)
    
    # Accessors
    def vars(self):
        """
        @return: the L{VariableBindings} mapping L{FeatureVariable}s to values.
        @rtype: L{VariableBindings}
        """
        return self._vars
        
    def lhs(self):
        """
        @return: the value of the left-hand side with variables set.
        @rtype: C{Category}
        """
        return apply(TreeEdge.lhs(self), self._vars)
    
    def orig_lhs(self):
        """
        @return: the value of the left-hand side with no variables set.
        @rtype: C{Category}
        """
        return TreeEdge.lhs(self)
    
    def rhs(self):
        """
        @return: the value of the right-hand side with variables set.
        @rtype: C{Category}
        """
        return tuple(apply(x, self._vars) for x in TreeEdge.rhs(self))
    
    def orig_rhs(self):
        """
        @return: the value of the right-hand side with no variables set.
        @rtype: C{Category}
        """
        return TreeEdge.rhs(self)

    # String representation
    def __str__(self):
        str = '%s ->' % self.lhs()

        for i in range(len(self._rhs)):
            if i == self._dot: str += ' *'
            str += ' %s' % (self.rhs()[i],)
        if len(self._rhs) == self._dot: str += ' *'
        return '%s %s' % (str, self._vars)

class FeatureFundamentalRule(FundamentalRule):
    def __init__(self, trace=0):
        FundamentalRule.__init__(self)
        self.trace = trace
        self.unify_memo = {}
    def apply_iter(self, chart, grammar, left_edge, right_edge):
        # Make sure the rule is applicable.
        if not (left_edge.end() == right_edge.start() and
                left_edge.is_incomplete() and right_edge.is_complete() and
                isinstance(left_edge, FeatureTreeEdge) and
                isinstance(right_edge, FeatureTreeEdge)
               ):
            return
        left_bindings = left_edge.vars().copy()
        right_bindings = right_edge.vars().copy()
        try:
            unified = unify(left_edge.next(), right_edge.lhs(), left_bindings,
            right_bindings, memo=self.unify_memo, trace=self.trace-2)
            if isinstance(unified, Category): unified.freeze()
        except UnificationFailure: return

        # Construct the new edge.
        new_edge = FeatureTreeEdge(span=(left_edge.start(), right_edge.end()),
                            lhs=left_edge.lhs(), rhs=left_edge.rhs(),
                            dot=left_edge.dot()+1, vars=left_bindings)
        
        # Add it to the chart, with appropraite child pointers.
        changed_chart = False
        for cpl1 in chart.child_pointer_lists(left_edge):
            if chart.insert(new_edge, cpl1+(right_edge,)):
                changed_chart = True

        # If we changed the chart, then generate the edge.
        if changed_chart: yield new_edge

class SingleEdgeFeatureFundamentalRule(SingleEdgeFundamentalRule):
    def __init__(self, trace=0):
        self.trace = trace
        self._fundamental_rule = FeatureFundamentalRule(trace)
    
    def apply_iter(self, chart, grammar, edge1):
        fr = self._fundamental_rule
        if edge1.is_incomplete():
            # edge1 =   left_edge; edge2 = right_edge
            for edge2 in chart.select(start=edge1.end(), is_complete=True):
                for new_edge in fr.apply_iter(chart, grammar, edge1, edge2):
                    yield new_edge
        else:
            # edge2 = left_edge; edge1 = right_edge
            for edge2 in chart.select(end=edge1.start(), is_complete=False):
                for new_edge in fr.apply_iter(chart, grammar, edge2, edge1):
                    yield new_edge

class FeatureTopDownExpandRule(TopDownExpandRule):
    """
    The @C{TopDownExpandRule} specialised for feature-based grammars.
    """
    def __init__(self, trace=0):
        TopDownExpandRule.__init__(self)
        self.unify_memo = {}
        self.trace = trace
    def apply_iter(self, chart, grammar, edge):
        if edge.is_complete(): return
        for prod in grammar.productions():
            bindings = edge.vars().copy()
            try:
                unified = unify(edge.next(), prod.lhs(), bindings, {},
                memo=self.unify_memo, trace=self.trace-2)
                if isinstance(unified, Category): unified.freeze()
            except UnificationFailure:
                continue
            new_edge = FeatureTreeEdge.from_production(prod, edge.end())
            if chart.insert(new_edge, ()):
                yield new_edge

class FeatureEarleyChartParse(EarleyChartParse):
    """
    A chart parser implementing the Earley parsing algorithm, allowing
    nonterminals that have features (known as L{Categories<Category>}).

        - For each index I{end} in [0, 1, ..., N]:
          - For each I{edge} s.t. I{edge}.end = I{end}:
            - If I{edge} is incomplete, and I{edge}.next is not a part
              of speech:
                - Apply PredictorRule to I{edge}
            - If I{edge} is incomplete, and I{edge}.next is a part of
              speech:
                - Apply ScannerRule to I{edge}
            - If I{edge} is complete:
                - Apply CompleterRule to I{edge}
        - Return any complete parses in the chart

    C{FeatureEarleyChartParse} uses a X{lexicon} to decide whether a leaf
    has a given part of speech.  This lexicon is encoded as a
    dictionary that maps each word to a list of parts of speech that
    word can have. Unlike in the L{EarleyChartParse}, this lexicon is
    case-insensitive.
    """
    def __init__(self, grammar, lexicon, trace=0):
        # Build a case-insensitive lexicon.
        #ci_lexicon = dict((k.upper(), v) for k, v in lexicon.iteritems())
        # Call the super constructor.
        EarleyChartParse.__init__(self, grammar, lexicon, trace)
        
    def get_parse_list(self, tokens):
        chart = Chart(tokens)
        grammar = self._grammar

        # Width, for printing trace edges.
        #w = 40/(chart.num_leaves()+1)
        w = 2
        if self._trace > 0: print ' '*9, chart.pp_leaves(w)

        # Initialize the chart with a special "starter" edge.
        root = GrammarCategory(pos='[INIT]')
        edge = FeatureTreeEdge((0,0), root, (grammar.start(),), 0,
                {})
        chart.insert(edge, ())

        # Create the 3 rules:
        predictor = FeatureTopDownExpandRule(self._trace)
        completer = SingleEdgeFeatureFundamentalRule(self._trace)
        #scanner = FeatureScannerRule(self._lexicon)

        for end in range(chart.num_leaves()+1):
            if self._trace > 1: print 'Processing queue %d' % end
            
            # Scanner rule substitute, i.e. this is being used in place
            # of a proper FeatureScannerRule at the moment.
            if end > 0 and end-1 < chart.num_leaves():
                leaf = chart.leaf(end-1)
                for pos in self._lexicon(leaf):
                    new_leaf_edge = LeafEdge(leaf, end-1)
                    chart.insert(new_leaf_edge, ())
                    new_pos_edge = FeatureTreeEdge((end-1, end), pos, [leaf], 1,
                        {})
                    chart.insert(new_pos_edge, (new_leaf_edge,))
                    if self._trace > 0:
                        print  'Scanner  ', chart.pp_edge(new_pos_edge,w)
            
            
            for edge in chart.select(end=end):
                if edge.is_incomplete():
                    for e in predictor.apply(chart, grammar, edge):
                        if self._trace > 1:
                            print 'Predictor', chart.pp_edge(e,w)
                #if edge.is_incomplete():
                #    for e in scanner.apply(chart, grammar, edge):
                #        if self._trace > 0:
                #            print 'Scanner  ', chart.pp_edge(e,w)
                if edge.is_complete():
                    for e in completer.apply(chart, grammar, edge):
                        if self._trace > 0:
                            print 'Completer', chart.pp_edge(e,w)

        # Output a list of complete parses.
        return chart.parses(root)

def demo():
    import sys, time

    S = GrammarCategory.parse('S')
    VP = GrammarCategory.parse('VP')
    NP = GrammarCategory.parse('NP')
    PP = GrammarCategory.parse('PP')
    V = GrammarCategory.parse('V')
    N = GrammarCategory.parse('N')
    P = GrammarCategory.parse('P')
    Name = GrammarCategory.parse('Name')
    Det = GrammarCategory.parse('Det')
    DetSg = GrammarCategory.parse('Det[-pl]')
    DetPl = GrammarCategory.parse('Det[+pl]')
    NSg = GrammarCategory.parse('N[-pl]')
    NPl = GrammarCategory.parse('N[+pl]')

    # Define some grammatical productions.
    grammatical_productions = [
        cfg.Production(S, (NP, VP)),  cfg.Production(PP, (P, NP)),
        cfg.Production(NP, (NP, PP)),
        cfg.Production(VP, (VP, PP)), cfg.Production(VP, (V, NP)),
        cfg.Production(VP, (V,)), cfg.Production(NP, (DetPl, NPl)),
        cfg.Production(NP, (DetSg, NSg))]

    # Define some lexical productions.
    lexical_productions = [
        cfg.Production(NP, ('John',)), cfg.Production(NP, ('I',)),
        cfg.Production(Det, ('the',)), cfg.Production(Det, ('my',)),
        cfg.Production(Det, ('a',)),
        cfg.Production(NSg, ('dog',)),   cfg.Production(NSg, ('cookie',)),
        cfg.Production(V, ('ate',)),  cfg.Production(V, ('saw',)),
        cfg.Production(P, ('with',)), cfg.Production(P, ('under',)),
        ]
    
    earley_grammar = cfg.Grammar(S, grammatical_productions)
    earley_lexicon = {}
    for prod in lexical_productions:
        earley_lexicon.setdefault(prod.rhs()[0].upper(), []).append(prod.lhs())
    def lexicon(word):
        return earley_lexicon.get(word.upper(), [])

    sent = 'I saw John with a dog with my cookie'
    print "Sentence:\n", sent
    from nltk import tokenize
    tokens = list(tokenize.whitespace(sent))
    t = time.time()
    cp = FeatureEarleyChartParse(earley_grammar, lexicon, trace=1)
    trees = cp.get_parse_list(tokens)
    print "Time: %s" % (time.time() - t)
    for tree in trees: print tree

def run_profile():
    import profile
    profile.run('for i in range(1): demo()', '/tmp/profile.out')
    import pstats
    p = pstats.Stats('/tmp/profile.out')
    p.strip_dirs().sort_stats('time', 'cum').print_stats(60)
    p.strip_dirs().sort_stats('cum', 'time').print_stats(60)

if __name__ == '__main__':
    demo()


########NEW FILE########
__FILENAME__ = featurelite
"""
This module provides utilities for treating Python dictionaries as X{feature
structures}. Specifically, it contains the C{unify} function, which can be used
to merge the properties of two dictionaries, and the C{Variable} class, which
holds an unknown value to be used in unification.

A X{feature structure} is a mapping from feature names to feature values,
where:

  - Each X{feature name} is a case sensitive string.
  - Each X{feature value} can be a base value (such as a string), a
    variable, or a nested feature structure.

However, feature structures are not a specialized class; they are represented
by dictionaries, or more generally by anything that responds to the C{has_key}
method. The YAML representation can be used to create and display feature
structures intuitively:

>>> f1 = yaml.load('''
... A:
...   B: b
...   D: d
... ''')
>>> f2 = yaml.load('''
... A:
...   C: c
...   D: d
... ''')
>>> print show(unify(f1, f2))
A:
  B: b
  C: c
  D: d

Feature structures are typically used to represent partial information
about objects.  A feature name that is not mapped to a value stands
for a feature whose value is unknown (I{not} a feature without a
value).  Two feature structures that represent (potentially
overlapping) information about the same object can be combined by
X{unification}.  When two inconsistant feature structures are unified,
the unification fails and raises an error.

Features can be specified using X{feature paths}, or tuples of feature names
that specify paths through the nested feature structures to a value.

Feature structures may contain reentrant feature values.  A
X{reentrant feature value} is a single feature value that can be
accessed via multiple feature paths.  Unification preserves the
reentrance relations imposed by both of the unified feature
structures.  After unification, any extensions to a reentrant feature
value will be visible using any of its feature paths.

Feature structure variables are encoded using the L{Variable} class. The scope
of a variable is determined by the X{bindings} used when the structure
including that variable is unified. Bindings can be reused between unifications
to ensure that variables with the same name get the same value.

"""

from copy import copy, deepcopy
import re
import yaml
#import unittest
import sys

class UnificationFailure(Exception):
    """
    An exception that is raised when two values cannot be unified.
    """
    pass

def isMapping(obj):
    """
    Determine whether to treat a given object as a feature structure. The
    test is whether it responds to C{has_key}. This can be overridden if the
    object includes an attribute or method called C{_no_feature}.

    @param obj: The object to be tested
    @type obj: C{object}
    @return: True iff the object can be treated as a feature structure
    @rtype: C{bool}
    """
    return isinstance(obj, dict) or isinstance(obj, FeatureI)

class FeatureI(object):
    def __init__(self):
        raise TypeError, "FeatureI is an abstract interface"

class _FORWARD(object):
    """
    _FORWARD is a singleton value, used in unification as a flag that a value
    has been forwarded to another object.

    This class itself is used as the singleton value. It cannot be
    instantiated.
    """
    def __init__(self):
        raise TypeError, "The _FORWARD class is not meant to be instantiated"

class Variable(object):
    """
    A Variable is an object that can be used in unification to hold an
    initially unknown value. Two equivalent Variables, for example, can be used
    to require that two features have the same value.

    When a Variable is assigned a value, it will eventually be replaced by
    that value. However, in order to make that value show up everywhere the
    variable appears, the Variable temporarily stores its assigned value and
    becomes a I{bound variable}. Bound variables do not appear in the results
    of unification.

    Variables are distinguished by their name, and by the dictionary of
    I{bindings} that is being used to determine their values. Two variables can
    have the same name but be associated with two different binding
    dictionaries: those variables are not equal.
    """
    _next_numbered_id = 1
    
    def __init__(self, name=None, value=None):
        """
        Construct a new feature structure variable.
        
        The value should be left at its default of None; it is only used
        internally to copy bound variables.

        @type name: C{string}
        @param name: An identifier for this variable. Two C{Variable} objects
          with the same name will be given the same value in a given dictionary
          of bindings.
        """
        self._uid = Variable._next_numbered_id
        Variable._next_numbered_id += 1
        if name is None: name = self._uid
        self._name = str(name)
        self._value = value
    
    def name(self):
        """
        @return: This variable's name.
        @rtype: C{string}
        """
        return self._name
    
    def value(self):
        """
        If this varable is bound, find its value. If it is unbound or aliased
        to an unbound variable, returns None.
        
        @return: The value of this variable, if any.
        @rtype: C{object}
        """
        if isinstance(self._value, Variable): return self._value.value()
        else: return self._value
    def copy(self):
        """
        @return: A copy of this variable.
        @rtype: C{Variable}
        """
        return Variable(self.name(), self.value())
    
    def forwarded_self(self):
        """
        Variables are aliased to other variables by one variable _forwarding_
        to the other. The first variable simply has the second as its value,
        but it acts like the second variable's _value_ is its value.

        forwarded_self returns the final Variable object that actually stores
        the value.

        @return: The C{Variable} responsible for storing this variable's value.
        @rtype: C{Variable}
        """
        if isinstance(self._value, Variable):
            return self._value.forwarded_self()
        else: return self
    
    def bindValue(self, value, ourbindings, otherbindings):
        """
        Bind this variable to a value. C{ourbindings} are the bindings that
        accompany the feature structure this variable came from;
        C{otherbindings} are the bindings from the structure it's being unified
        with.

        @type value: C{object}
        @param value: The value to be assigned.
        @type ourbindings: C{dict}
        @param ourbindings: The bindings associated with this variable.
        @type otherbindings: C{dict}
        @param otherbindings: The bindings associated with the value being
          assigned. (May be identical to C{ourbindings}.)
        """
        if isinstance(self._value, Variable):
            # Forward the job of binding to the variable we're aliased to.
            return self._value.bindValue(value, ourbindings, otherbindings)
        if self._value is None:
            # This variable is unbound, so bind it.
            self._value = value
        else:
            # This variable is already bound; try to unify the existing value
            # with the new one.
            self._value = unify(self._value, value, ourbindings, otherbindings)

    def forwardTo(self, other, ourbindings, otherbindings):
        """
        A unification wants this variable to be aliased to another variable.
        Forward this variable to the other one, and return the other.

        @type other: C{Variable}
        @param other: The variable to replace this one.
        @type ourbindings: C{dict}
        @param ourbindings: The bindings associated with this variable.
        @type otherbindings: C{dict}
        @param otherbindings: The bindings associated with the other variable.
        (May be identical to C{ourbindings}.)
        @return: C{other}
        @rtype: C{Variable}
        """
        other.bindValue(self.value(), ourbindings, otherbindings)
        self._value = other
        return other
        
    def __hash__(self): return hash(self._uid)
    def __cmp__(self, other):
        """
        Variables are equal if they are the same object or forward to the
        same object. Variables with the same name may still be unequal.
        """
        if not isinstance(other, Variable): return -1
        if isinstance(self._value, Variable): return cmp(self._value, other)
        else: return cmp((self._name, self._value), (other._name, other._value))
    def __repr__(self):
        if self._value is None: return '?%s' % self._name
        else: return '?%s: %r' % (self._name, self._value)

def show(data):
    """
    Works like yaml.dump(), but with output suited for doctests. Flow style
    is always off, and there is no blank line at the end.
    """
    return yaml.dump(data, default_flow_style=False).strip()

def variable_representer(dumper, var):
    "Output variables in YAML as ?name."
    return dumper.represent_scalar(u'!var', u'?%s' % var.name())
yaml.add_representer(Variable, variable_representer)

def variable_constructor(loader, node):
    "Recognize variables written as ?name in YAML."
    value = loader.construct_scalar(node)
    name = value[1:]
    return Variable(name)
yaml.add_constructor(u'!var', variable_constructor)
yaml.add_implicit_resolver(u'!var', re.compile(r'^\?\w+$'))

def _copy_and_bind(feature, bindings, memo=None):
    """
    Make a deep copy of a feature structure, preserving reentrance using the
    C{memo} dictionary. Meanwhile, variables are replaced by their bound
    values, if these values are already known, and variables with unknown
    values are given placeholder bindings.
    """
    if memo is None: memo = {}
    if id(feature) in memo: return memo[id(feature)]
    if isinstance(feature, Variable) and bindings is not None:
        if not bindings.has_key(feature.name()):
            bindings[feature.name()] = feature.copy()
        result = _copy_and_bind(bindings[feature.name()], None, memo)
    else:
        if isMapping(feature):
            # Construct a new object of the same class
            result = feature.__class__()
            for (key, value) in feature.items():
                result[key] = _copy_and_bind(value, bindings, memo)
        else: result = feature
    memo[id(feature)] = result
    memo[id(result)] = result
    return result

def apply(feature, bindings):
    """
    Replace variables in a feature structure with their bound values.
    """
    return _copy_and_bind(feature, bindings.copy())

def unify(feature1, feature2, bindings1=None, bindings2=None, memo=None, fail=None, trace=0):
    """
    In general, the 'unify' procedure takes two values, and either returns a
    value that provides the information provided by both values, or fails if
    that is impossible.
    
    These values can have any type, but fall into a few general cases:
      - Values that respond to C{has_key} represent feature structures. The
        C{unify} procedure will recurse into them and unify their inner values.
      - L{Variable}s represent an unknown value, and are handled specially.
        The values assigned to variables are tracked using X{bindings}.
      - C{None} represents the absence of information.
      - Any other value is considered a X{base value}. Base values are
        compared to each other with the == operation.

    The value 'None' represents the absence of any information. It specifies no
    properties and acts as the identity in unification.
    >>> unify(3, None)
    3

    >>> unify(None, 'fish')
    'fish'

    A base value unifies with itself, but not much else.
    >>> unify(True, True)
    True

    >>> unify([1], [1])
    [1]

    >>> unify('a', 'b')
    Traceback (most recent call last):
        ...
    UnificationFailure

    When two mappings (representing feature structures, and usually implemented
    as dictionaries) are unified, any chain of keys that accesses a value in
    either mapping will access an equivalent or more specific value in the
    unified mapping. If this is not possible, UnificationFailure is raised.

    >>> f1 = dict(A=dict(B='b'))
    >>> f2 = dict(A=dict(C='c'))
    >>> unify(f1, f2) == dict(A=dict(B='b', C='c'))
    True
    
    The empty dictionary specifies no features. It unifies with any mapping.
    >>> unify({}, dict(foo='bar'))
    {'foo': 'bar'}

    >>> unify({}, True)
    Traceback (most recent call last):
        ...
    UnificationFailure
    
    Representing dictionaries in YAML form is useful for making feature
    structures readable:
    
    >>> f1 = yaml.load("number: singular")
    >>> f2 = yaml.load("person: 3")
    >>> print show(unify(f1, f2))
    number: singular
    person: 3

    >>> f1 = yaml.load('''
    ... A:
    ...   B: b
    ...   D: d
    ... ''')
    >>> f2 = yaml.load('''
    ... A:
    ...   C: c
    ...   D: d
    ... ''')
    >>> print show(unify(f1, f2))
    A:
      B: b
      C: c
      D: d
    
    Variables are names for unknown values. Variables are assigned values
    that will make unification succeed. The values of variables can be reused
    in later unifications if you provide a dictionary of _bindings_ from
    variables to their values.
    >>> bindings = {}
    >>> print unify(Variable('x'), 5, bindings)
    5
    
    >>> print bindings
    {'x': 5}
    
    >>> print unify({'a': Variable('x')}, {}, bindings)
    {'a': 5}
    
    The same variable name can be reused in different binding dictionaries
    without collision. In some cases, you may want to provide two separate
    binding dictionaries to C{unify} -- one for each feature structure, so
    their variables do not collide.

    In the following examples, two different feature structures use the
    variable ?x to require that two values are equal. The values assigned to
    ?x are consistent within each structure, but would be inconsistent if every
    ?x had to have the same value.

    >>> f1 = yaml.load('''
    ... a: 1
    ... b: 1
    ... c: ?x
    ... d: ?x
    ... ''')
    >>> f2 = yaml.load('''
    ... a: ?x
    ... b: ?x
    ... c: 2
    ... d: 2
    ... ''')
    >>> bindings1 = {}
    >>> bindings2 = {}
    >>> print show(unify(f1, f2, bindings1, bindings2))
    a: 1
    b: 1
    c: 2
    d: 2
    
    >>> print bindings1
    {'x': 2}
    
    >>> print bindings2
    {'x': 1}

    Feature structures can involve _reentrant_ values, where multiple feature
    paths lead to the same value. This is represented by the features having
    the same Python object as a value. (This kind of identity can be tested
    using the C{is} operator.)
    
    Unification preserves the properties of reentrance. So if a reentrant value
    is changed by unification, it is changed everywhere it occurs, and it is
    still reentrant. Reentrant features can even form cycles; these
    cycles can now be printed through the current YAML library.

    >>> f1 = yaml.load('''
    ... A: &1                # &1 defines a reference in YAML...
    ...   B: b
    ... E:
    ...   F: *1              # and *1 uses the previously defined reference.
    ... ''')
    >>> f1['E']['F']['B']
    'b'
    >>> f1['A'] is f1['E']['F']
    True
    >>> f2 = yaml.load('''
    ... A:
    ...   C: c
    ... E:
    ...   F:
    ...     D: d
    ... ''')
    >>> f3 = unify(f1, f2)
    >>> print show(f3)
    A: &id001
      B: b
      C: c
      D: d
    E:
      F: *id001
    >>> f3['A'] is f3['E']['F']    # Showing that the reentrance still holds.
    True
    
    This unification creates a cycle:
    >>> f1 = yaml.load('''
    ... F: &1 {}
    ... G: *1
    ... ''')
    >>> f2 = yaml.load('''
    ... F:
    ...   H: &2 {}
    ... G: *2
    ... ''')
    >>> f3 = unify(f1, f2)
    >>> print f3
    {'G': {'H': {...}}, 'F': {'H': {...}}}
    >>> print f3['F'] is f3['G']
    True
    >>> print f3['F'] is f3['G']['H']
    True
    >>> print f3['F'] is f3['G']['H']['H']
    True

    A cycle can also be created using variables instead of reentrance.
    Here we supply a single set of bindings, so that it is used on both sides
    of the unification, making ?x mean the same thing in both feature
    structures.
    
    >>> f1 = yaml.load('''
    ... F:
    ...   H: ?x
    ... ''')
    >>> f2 = yaml.load('''
    ... F: ?x
    ... ''')
    >>> f3 = unify(f1, f2, {})
    >>> print f3
    {'F': {'H': {...}}}
    >>> print f3['F'] is f3['F']['H']
    True
    >>> print f3['F'] is f3['F']['H']['H']
    True

    Two sets of bindings can be provided because the variable names on each
    side of the unification may be unrelated. An example involves unifying the
    following two structures, which each require that two values are
    equivalent, and happen to both use ?x to express that requirement.

    >>> f1 = yaml.load('''
    ... a: 1
    ... b: 1
    ... c: ?x
    ... d: ?x
    ... ''')
    >>> f2 = yaml.load('''
    ... a: ?x
    ... b: ?x
    ... c: 2
    ... d: 2
    ... ''')
    >>> bindings1 = {}
    >>> bindings2 = {}
    >>> # We could avoid defining two empty dictionaries by simply using the
    >>> # defaults, with unify(f1, f2) -- but we want to be able to examine
    >>> # the bindings afterward.
    >>> print show(unify(f1, f2, bindings1, bindings2))
    a: 1
    b: 1
    c: 2
    d: 2
    >>> print bindings1
    {'x': 2}
    >>> print bindings2
    {'x': 1}

    If a variable is unified with another variable, the two variables are
    _aliased_ to each other; they share the same value, similarly to reentrant
    feature structures. This is represented in a set of bindings as one
    variable having the other as its value.
    >>> f1 = yaml.load('''
    ... a: ?x
    ... b: ?x
    ... ''')
    >>> f2 = yaml.load('''
    ... b: ?y
    ... c: ?y
    ... ''')
    >>> bindings = {}
    >>> print show(unify(f1, f2, bindings))
    a: &id001 ?y
    b: *id001
    c: *id001
    >>> print bindings
    {'x': ?y}

    Reusing the same variable bindings ensures that appropriate bindings are
    made after the fact:
    >>> bindings = {}
    >>> f1 = {'a': Variable('x')}
    >>> f2 = unify(f1, {'a': {}}, bindings)
    >>> f3 = unify(f2, {'b': Variable('x')}, bindings)
    >>> print show(f3)
    a: &id001 {}
    b: *id001
    >>> print bindings
    {'x': {}}

    @param feature1: The first object to be unified.
    @type feature1: C{object} (probably a mapping)
    @param feature2: The second object to be unified.
    @type feature2: C{object} (probably a mapping)
    @param bindings1: The variable bindings associated with the first object.
    @type bindings1: C{dict} or None
    @param bindings2: The variable bindings associated with the second object,
      if these are distinct from C{bindings1}.
    @type bindings2: C{dict} or None
    @return: The result of unifying the two objects.
    @rtype: C{object} (probably a mapping)
    """
    if fail is None:
        def failerror(f1, f2):
            raise UnificationFailure
        fail = failerror
        
    if bindings1 is None and bindings2 is None:
        bindings1 = {}
        bindings2 = {}
    else:
        if bindings1 is None: bindings1 = {}
        if bindings2 is None: bindings2 = bindings1

    if memo is None: memo = {}
    copymemo = {}
    if memo.has_key((id(feature1), id(feature2))):
        result = memo[id(feature1), id(feature2)]
        if result is UnificationFailure:
            if trace > 2:
                print '(cached) Unifying: %r + %r --> [fail]' % (feature1, feature2)
            raise result()
        if trace > 2:
            print '(cached) Unifying: %r + %r --> ' % (feature1, feature2),
            print repr(result)
        return result

    if trace > 1:
        print 'Unifying: %r + %r --> ' % (feature1, feature2),
    
    # Make copies of the two structures (since the unification algorithm is
    # destructive). Use the same memo, to preserve reentrance links between
    # them.
    copy1 = _copy_and_bind(feature1, bindings1, copymemo)
    copy2 = _copy_and_bind(feature2, bindings2, copymemo)
    # Preserve links between bound variables and the two feature structures.
    for b in (bindings1, bindings2):
        for (vname, value) in b.items():
            value_id = id(value)
            if value_id in copymemo:
                b[vname] = copymemo[value_id]

    # Go on to doing the unification.
    try:
        unified = _destructively_unify(copy1, copy2, bindings1, bindings2, memo,
        fail)
    except UnificationFailure:
        if trace > 1: print '[fail]'
        memo[id(feature1), id(feature2)] = UnificationFailure
        raise

    _apply_forwards_to_bindings(bindings1)
    _apply_forwards_to_bindings(bindings2)
    _apply_forwards(unified, {})
    unified = _lookup_values(unified, {}, remove=False)
    _lookup_values(bindings1, {}, remove=True)
    _lookup_values(bindings2, {}, remove=True)

    if trace > 1:
        print repr(unified)
    elif trace > 0:
        print 'Unifying: %r + %r --> %r' % (feature1, feature2, repr(unified))
    
    memo[id(feature1), id(feature2)] = unified
    return unified

def _destructively_unify(feature1, feature2, bindings1, bindings2, memo, fail,
depth=0):
    """
    Attempt to unify C{self} and C{other} by modifying them
    in-place.  If the unification succeeds, then C{self} will
    contain the unified value, and the value of C{other} is
    undefined.  If the unification fails, then a
    UnificationFailure is raised, and the values of C{self}
    and C{other} are undefined.
    """
    if depth > 50:
        print "Infinite recursion in this unification:"
        print show(dict(feature1=feature1, feature2=feature2,
        bindings1=bindings1, bindings2=bindings2, memo=memo))
        raise ValueError, "Infinite recursion in unification"
    if memo.has_key((id(feature1), id(feature2))):
        result = memo[id(feature1), id(feature2)]
        if result is UnificationFailure: raise result()
    unified = _do_unify(feature1, feature2, bindings1, bindings2, memo, fail,
    depth)
    memo[id(feature1), id(feature2)] = unified
    return unified

def _do_unify(feature1, feature2, bindings1, bindings2, memo, fail, depth=0):
    """
    Do the actual work of _destructively_unify when the result isn't memoized.
    """

    # Trivial cases.
    if feature1 is None: return feature2
    if feature2 is None: return feature1
    if feature1 is feature2: return feature1
    
    # Deal with variables by binding them to the other value.
    if isinstance(feature1, Variable):
        if isinstance(feature2, Variable):
            # If both objects are variables, forward one to the other. This
            # has the effect of unifying the variables.
            return feature1.forwardTo(feature2, bindings1, bindings2)
        else:
            feature1.bindValue(feature2, bindings1, bindings2)
            return feature1
    if isinstance(feature2, Variable):
        feature2.bindValue(feature1, bindings2, bindings1)
        return feature2
    
    # If it's not a mapping or variable, it's a base object, so we just
    # compare for equality.
    if not isMapping(feature1):
        if feature1 == feature2: return feature1
        else: 
            return fail(feature1, feature2)
    if not isMapping(feature2):
        return fail(feature1, feature2)
    
    # At this point, we know they're both mappings.
    # Do the destructive part of unification.

    while feature2.has_key(_FORWARD): feature2 = feature2[_FORWARD]
    if feature1 is not feature2: feature2[_FORWARD] = feature1
    for (fname, val2) in feature2.items():
        if fname == _FORWARD: continue
        val1 = feature1.get(fname)
        feature1[fname] = _destructively_unify(val1, val2, bindings1,
        bindings2, memo, fail, depth+1)
    return feature1

def _apply_forwards(feature, visited):
    """
    Replace any feature structure that has a forward pointer with
    the target of its forward pointer (to preserve reentrance).
    """
    if not isMapping(feature): return
    if visited.has_key(id(feature)): return
    visited[id(feature)] = True

    for fname, fval in feature.items():
        if isMapping(fval):
            while fval.has_key(_FORWARD):
                fval = fval[_FORWARD]
                feature[fname] = fval
            _apply_forwards(fval, visited)

def _lookup_values(mapping, visited, remove=False):
    """
    The unification procedure creates _bound variables_, which are Variable
    objects that have been assigned a value. Bound variables are not useful
    in the end result, however, so they should be replaced by their values.

    This procedure takes a mapping, which may be a feature structure or a
    binding dictionary, and replaces bound variables with their values.
    
    If the dictionary is a binding dictionary, then 'remove' should be set to
    True. This ensures that unbound, unaliased variables are removed from the
    dictionary. If the variable name 'x' is mapped to the unbound variable ?x,
    then, it should be removed. This is not done with features, because a
    feature named 'x' can of course have a variable ?x as its value.
    """
    if isinstance(mapping, Variable):
        # Because it's possible to unify bare variables, we need to gracefully
        # accept a variable in place of a dictionary, and return a result that
        # is consistent with that variable being inside a dictionary.
        #
        # We can't remove a variable from itself, so we ignore 'remove'.
        var = mapping
        if var.value() is not None:
            return var.value()
        else:
            return var.forwarded_self()
    if not isMapping(mapping): return mapping
    if visited.has_key(id(mapping)): return mapping
    visited[id(mapping)] = True

    for fname, fval in mapping.items():
        if isMapping(fval):
            _lookup_values(fval, visited)
        elif isinstance(fval, Variable):
            if fval.value() is not None:
                mapping[fname] = fval.value()
                if isMapping(mapping[fname]):
                    _lookup_values(mapping[fname], visited)
            else:
                newval = fval.forwarded_self()
                if remove and newval.name() == fname:
                    del mapping[fname]
                else:
                    mapping[fname] = newval
    return mapping

def _apply_forwards_to_bindings(bindings):
    """
    Replace any feature structures that have been forwarded by their new
    identities.
    """
    for (key, value) in bindings.items():
        if isMapping(value) and value.has_key(_FORWARD):
            while value.has_key(_FORWARD):
                value = value[_FORWARD]
            bindings[key] = value

def test():
    "Run unit tests on unification."
    import doctest
    doctest.testmod()

if __name__ == "__main__":
    test()


########NEW FILE########
__FILENAME__ = test
from featurechart import *
from treeview import *

def demo():
    cp = load_earley('gazdar6.cfg', trace=2)
    trees = cp.parse('the man who chased Fido returned')
    for tree in trees: print tree

#run_profile()
if __name__ == '__main__': demo()

########NEW FILE########
__FILENAME__ = tree
# placeholder
from nltk.parse.tree import *

########NEW FILE########
__FILENAME__ = treeview
import Tkinter
from nltk.draw import TreeWidget
from nltk.draw import CanvasFrame

"""A class that draws parse trees in a simple Tk window."""

class TreeView:
    def __init__(self, trees, root=None):
        if len(trees) == 0:
            print "No trees to display."
            return

        newroot = False
        if root is None:
            root = Tkinter.Tk()
            window = root
            newroot = True
        else:
            window = Tkinter.Toplevel(root)
        
        window.title("Parse Tree")
        window.geometry("600x400")
        self.cf = CanvasFrame(window)
        self.cf.pack(side='top', expand=1, fill='both')
        buttons = Tkinter.Frame(window)
        buttons.pack(side='bottom', fill='x')

        self.spin = Tkinter.Spinbox(buttons, from_=1, to=len(trees),
            command=self.showtree, width=3)
        if len(trees) > 1: self.spin.pack(side='left')
        self.label = Tkinter.Label(buttons, text="of %d" % len(trees))
        if len(trees) > 1: self.label.pack(side='left')
        self.done = Tkinter.Button(buttons, text="Done", command=window.destroy)
        self.done.pack(side='right')
        self.printps = Tkinter.Button(buttons, text="Print to Postscript", command=self.cf.print_to_file)
        self.printps.pack(side='right')
        
        self.trees = trees
        self.treeWidget = None
        self.showtree()
        if newroot: root.mainloop()
    
    def showtree(self):
        try: n = int(self.spin.get())
        except ValueError: n=1
        if self.treeWidget is not None: self.cf.destroy_widget(self.treeWidget)
        self.treeWidget = TreeWidget(self.cf.canvas(),
        self.trees[n-1], draggable=1, shapeable=1)
        self.cf.add_widget(self.treeWidget, 0, 0)
    

########NEW FILE########
__FILENAME__ = batchtest
from featurechart import *
from treeview import *

def demo():
    cp = load_earley('gazdar6.cfg', trace=2)
    cp.batch_test('all-gazdar-sentences.txt')

#run_profile()
if __name__ == '__main__': demo()

########NEW FILE########
__FILENAME__ = category
# Natural Language Toolkit: Categories
#
# Copyright (C) 2001-2007 NLTK Project
# Author: Contributed by Rob Speer (NLTK version)
#         Steven Bird <sb@csse.unimelb.edu.au> (NLTK-Lite Port)
#         Ewan Klein <ewan@inf.ed.ac.uk> (Hooks for semantics)
#         Peter Wang <wangp@csse.unimelb.edu.au> (Overhaul)
# URL: <http://nltk.sourceforge.net>
# For license information, see LICENSE.TXT
#
# $Id: category.py 4162 2007-03-01 00:46:05Z stevenbird $

import logic
from nltk.cfg import *
#from kimmo import kimmo

from featurelite import *
from copy import deepcopy
import yaml
# import nltk.yamltags

class Category(Nonterminal, FeatureI, SubstituteBindingsI):
    """
    A C{Category} is a wrapper for feature dictionaries, intended for use in
    parsing. It can act as a C{Nonterminal}.

    A C{Category} acts like a dictionary, except in the following ways:

        - Categories can be "frozen" so that they can act as hash keys;
          before they are frozen, they are mutable.

        - In addition to being hashable, Categories have consistent str()
          representations.

        - Categories have one feature marked as the 'head', which prints
          differently than other features if it has a value. For example,
          in the C{repr()} representation of a Category, the head goes to the
          left, on the outside of the brackets. Subclasses of C{Category}
          may change the feature name that is designated as the head, which is
          _head by default.

    Categories can contain any kind of object as their values, and can be
    recursive and even re-entrant. Categories are not necessarily "categories
    all the way down"; they can contain plain dictionaries as their values, and
    converting inner dictionaries to categories would probably lead to messier
    code for no gain.

    Because Categories can contain any kind of object, they do not try to
    keep control over what their inner objects do. If you freeze a Category
    but mutate its inner objects, undefined behavior will occur.
    """
    
    headname = 'head'
    
    def __init__(self, features=None, **morefeatures):
        if features is None: features = {}
        self._features = unify(features, morefeatures)
        self._hash = None
        self._frozen = False
        self._memostr = None

    def __cmp__(self, other):
        return cmp(repr(self), repr(other))
    
    def __div__(self, other):
        """
        @return: A new Category based on this one, with its C{/} feature set to 
        C{other}.
        """
        return unify(self, {'/': other})

    def __eq__(self, other):
        """
        Compare Categories for equality. This relies on Python's built-in
        __eq__ for dictionaries, which is fairly thorough in checking for
        recursion and reentrance.

        @return: True if C{self} and C{other} assign the same value to
        every feature.  In particular, return true if
        C{self[M{p}]==other[M{p}]} for every feature path M{p} such
        that C{self[M{p}]} or C{other[M{p}]} is a base value (i.e.,
        not a nested Category).
        @rtype: C{bool}
        """
        if not other.__class__ == self.__class__: return False
        return self._features == other._features

    def __ne__(self, other):
        return not (self == other)

    def __hash__(self):
        if self._hash is not None: return self._hash
        return hash(self.__class__._str(self, {}, {}, True))
    
    def freeze(self):
        """
        Freezing a Category memoizes its hash value, to make comparisons on it
        faster. After freezing, the Category and all its values are immutable.

        @return: self
        """
        self._memostr = str(self)
        self._hash = hash(self)
        self._frozen = True
        return self

    def frozen(self):
        """
        Returns whether this Category is frozen (immutable).
        
        @rtype: C{bool}
        """
        return self._frozen
    
    def get(self, key):
        return self._features.get(key)

    def __getitem__(self, key):
        return self._features.get(key)
    
    def __setitem__(self, key, value):
        if self._frozen: raise "Cannot modify a frozen Category"
        self._features[key] = value

    def items(self):
        return self._features.items()

    def keys(self):
        return self._features.keys()

    def values(self):
        return self._features.values()

    def has_key(self, key):
        return self._features.has_key(key)
    
    def symbol(self):
        """
        @return: The node value corresponding to this C{Category}. 
        @rtype: C{Category}
        """
        return self

    def head(self):
        """
        @return: The head of this category (the value shown outside the
        brackets in its string representation). If there is no head, returns
        None.
        @rtype: C{str} or C{None}
        """
        return self.get(self.__class__.headname)
    
    def copy(self):
        """
        @return: A deep copy of C{self}.
        """
        # Create a reentrant deep copy by round-tripping it through YAML.
        return deepcopy(self)
    
    def feature_names(self):
        """
        @return: a list of all features that have values.
        """
        return self._features.keys()
    
    has_feature = has_key

    #################################################################
    ## Variables
    #################################################################
    
    def remove_unbound_vars(self):
        selfcopy = self.copy()
        Category._remove_unbound_vars(self)
        return selfcopy

    def substitute_bindings(self, bindings):
        return self.__class__(substitute_bindings(self._features, bindings))
    
    @staticmethod
    def _remove_unbound_vars(obj):
        for (key, value) in obj.items():
            if isinstance(value, Variable):
                del obj[key]
            elif isinstance(value, (Category, dict)):
                Category._remove_unbound_vars(value)

    #################################################################
    ## String Representations
    #################################################################

    def __repr__(self):
        """
        @return: A string representation of this feature structure.
        """
        return str(self)
    
    def __str__(self):
        """
        @return: A string representation of this feature structure.
        """
        if self._memostr is not None: return self._memostr
        return self.__class__._str(self, {}, {})
    
    @classmethod
    def _str(cls, obj, reentrances, reentrance_ids, normalize=False):
        segments = []

        keys = obj.keys()
        keys.sort()
        for fname in keys:
            if fname == cls.headname: continue
            fval = obj[fname]
            if isinstance(fval, bool):
                if fval: segments.append('+%s' % fname)
                else: segments.append('-%s' % fname)
            elif normalize and isinstance(fval, logic.Expression):
                segments.append('%s=%s' % (fname, fval.normalize()))
            elif not isinstance(fval, dict):
                segments.append('%s=%s' % (fname, fval))
            else:
                fval_repr = cls._str(fval, reentrances, reentrance_ids)
                segments.append('%s=%s' % (fname, fval_repr))

        head = obj.get(cls.headname)
        if head is None: head = ''
        if head and not len(segments): return str(head)
        return '%s[%s]' % (head, ', '.join(segments))
    
    yaml_tag = '!parse.Category'
    
    @classmethod
    def to_yaml(cls, dumper, data):
        node = dumper.represent_mapping(cls.yaml_tag, data._features)
        return node

    @classmethod
    def from_yaml(cls, loader, node):
        features = loader.construct_mapping(node, deep=True)
        return cls(features)

    #################################################################
    ## Parsing
    #################################################################

    # Regular expressions for parsing.
    _PARSE_RE = {'name': re.compile(r'\s*([^\s\(\)"\'=,\[\]/\?]+)\s*'),
                 'ident': re.compile(r'\s*\((\d+)\)\s*'),
                 'reentrance': re.compile(r'\s*->\s*'),
                 'assign': re.compile(r'\s*=?\s*'),
                 'bracket': re.compile(r'\s*]\s*'),
                 'comma': re.compile(r'\s*,\s*'),
                 'none': re.compile(r'None(?=\s|\]|,)'),
                 'int': re.compile(r'-?\d+(?=\s|\]|,)'),
                 'var': re.compile(r'\?[a-zA-Z_][a-zA-Z0-9_]*'+'|'+
                                   r'\?<[a-zA-Z_][a-zA-Z0-9_]*'+
                                   r'(=[a-zA-Z_][a-zA-Z0-9_]*)*>'),
                 'symbol': re.compile(r'\w+'),
                 'stringmarker': re.compile("['\"\\\\]"),
    
                 'categorystart':re.compile(r'\s*([^\s\(\)"\'\-=,\[\]/\?]*)\s*\['),
                 'bool': re.compile(r'\s*([-\+])'),
                 'arrow': re.compile(r'\s*->\s*'),
                 'disjunct': re.compile(r'\s*\|\s*'),
                 'whitespace': re.compile(r'\s*'),
                 'semantics': re.compile(r'<([^>]+)>'), 
                 'application': re.compile(r'<(app)\((\?[a-z][a-z]*)\s*,\s*(\?[a-z][a-z]*)\)>'),
                 'slash': re.compile(r'\s*/\s*'),
                }
    
    @classmethod
    def parse(cls, s):
        parsed, position = cls._parse(s, 0)
        if position != len(s):
            raise ValueError('end of string', position)
        return cls(parsed)

    @classmethod
    def inner_parse(cls, s, position, reentrances={}):
        if reentrances is None: reentrances = {}
        parsed, position = cls._parse(s, position)
        return cls(parsed), position
    
    @classmethod
    def _parse(cls, s, position=0, reentrances=None):
        """
        Helper function that parses a Category.
        @param s: The string to parse.
        @param position: The position in the string to start parsing.
        @param reentrances: A dictionary from reentrance ids to values.
        @return: A tuple (val, pos) of the feature structure created
            by parsing and the position where the parsed feature
            structure ends.
        """
        # A set of useful regular expressions (precompiled)
        _PARSE_RE = cls._PARSE_RE

        features = {}
        
        # Find the head, if there is one.
        match = _PARSE_RE['name'].match(s, position)
        if match is not None:
            features[cls.headname] = match.group(1)
            position = match.end()
        else:
            match = _PARSE_RE['var'].match(s, position)
            if match is not None:
                features[cls.headname] = makevar(match.group(0))
                position = match.end()

        
        # If the name is followed by an open bracket, start looking for
        # features.
        if position < len(s) and s[position] == '[':
            position += 1
    
            # Build a list of the features defined by the structure.
            # Each feature has one of the three following forms:
            #     name = value
            #     +name
            #     -name
            while True:
                if not position < len(s):
                    raise ValueError('close bracket', position)
            
                # Use these variables to hold info about the feature:
                name = target = val = None
                
                # Check for a close bracket at the beginning
                match = _PARSE_RE['bracket'].match(s, position)
                if match is not None:
                    position = match.end()
                    break   
                    
                # Is this a shorthand boolean value?
                match = _PARSE_RE['bool'].match(s, position)
                if match is not None:
                    if match.group(1) == '+': val = True
                    else: val = False
                    position = match.end()
                
                # Find the next feature's name.
                match = _PARSE_RE['name'].match(s, position)
                if match is None: raise ValueError('feature name', position)
                name = match.group(1)
                position = match.end()
                
                # If it's not a shorthand boolean, it must be an assignment.
                if val is None:
                    match = _PARSE_RE['assign'].match(s, position)
                    if match is None: raise ValueError('equals sign', position)
                    position = match.end()
    
                    val, position = cls._parseval(s, position, reentrances)
                features[name] = val
                        
                # Check for a close bracket
                match = _PARSE_RE['bracket'].match(s, position)
                if match is not None:
                    position = match.end()
                    break   
                    
                # Otherwise, there should be a comma
                match = _PARSE_RE['comma'].match(s, position)
                if match is None: raise ValueError('comma', position)
                position = match.end()
            
        return features, position
    
    @classmethod
    def _parseval(cls, s, position, reentrances):
        """
        Helper function that parses a feature value.  Currently
        supports: None, bools, integers, variables, strings, nested feature
        structures.
        @param s: The string to parse.
        @param position: The position in the string to start parsing.
        @param reentrances: A dictionary from reentrance ids to values.
        @return: A tuple (val, pos) of the value created by parsing
            and the position where the parsed value ends.
        """
        
        # A set of useful regular expressions (precompiled)
        _PARSE_RE = cls._PARSE_RE
        
        # End of string (error)
        if position == len(s): raise ValueError('value', position)

        # Semantic value of the form <app(?x, ?y) >'; return an ApplicationExpression
        match = _PARSE_RE['application'].match(s, position)
        if match is not None:
            fun = ParserSubstitute(match.group(2)).next()
            arg = ParserSubstitute(match.group(3)).next()
            return logic.ApplicationExpressionSubst(fun, arg), match.end()       

        # other semantic value enclosed by '< >'; return value given by the lambda expr parser
        match = _PARSE_RE['semantics'].match(s, position)
        if match is not None:
            return ParserSubstitute(match.group(1)).next(), match.end()
        
        # String value
        if s[position] in "'\"":
            start = position
            quotemark = s[position:position+1]
            position += 1
            while 1: 
                match = cls._PARSE_RE['stringmarker'].search(s, position)
                if not match: raise ValueError('close quote', position)
                position = match.end()
                if match.group() == '\\': position += 1
                elif match.group() == quotemark:
                    return s[start+1:position-1], position

        # Nested category
        if _PARSE_RE['categorystart'].match(s, position) is not None:
            return cls._parse(s, position, reentrances)

        # Variable
        match = _PARSE_RE['var'].match(s, position)
        if match is not None:
            return makevar(match.group()), match.end()

        # None
        match = _PARSE_RE['none'].match(s, position)
        if match is not None:
            return None, match.end()

        # Integer value
        match = _PARSE_RE['int'].match(s, position)
        if match is not None:
            return int(match.group()), match.end()

        # Alphanumeric symbol (must be checked after integer)
        match = _PARSE_RE['symbol'].match(s, position)
        if match is not None:
            return match.group(), match.end()

        # We don't know how to parse this value.
        raise ValueError('value', position)
    
    @classmethod
    def parse_rules(cls, s):
        """
        Parse a L{CFG} line involving C{Categories}. A line has this form:
        
        C{lhs -> rhs | rhs | ...}

        where C{lhs} is a Category, and each C{rhs} is a sequence of
        Categories.
        
        @returns: a list of C{Productions}, one for each C{rhs}.
        """
        _PARSE_RE = cls._PARSE_RE
        position = 0
        try:
            lhs, position = cls.inner_parse(s, position)
            lhs = cls(lhs)
        except ValueError, e:
            estr = ('Error parsing field structure\n\n\t' +
                    s + '\n\t' + ' '*e.args[1] + '^ ' +
                    'Expected %s\n' % e.args[0])
            raise ValueError, estr
        lhs.freeze()

        match = _PARSE_RE['arrow'].match(s, position)
        if match is None:
            raise ValueError('expected arrow', s, s[position:])
        else: position = match.end()
        rules = []
        while position < len(s):
            rhs = []
            while position < len(s) and _PARSE_RE['disjunct'].match(s, position) is None:
                try:
                    val, position = cls.inner_parse(s, position, {})
                    if isinstance(val, dict): val = cls(val)
                except ValueError, e:
                    estr = ('Error parsing field structure\n\n\t' +
                        s + '\n\t' + ' '*e.args[1] + '^ ' +
                        'Expected %s\n' % e.args[0])
                    raise ValueError, estr
                if isinstance(val, Category): val.freeze()
                rhs.append(val)
                position = _PARSE_RE['whitespace'].match(s, position).end()
            rules.append(Production(lhs, rhs))
            
            if position < len(s):
                match = _PARSE_RE['disjunct'].match(s, position)
                position = match.end()
        
        # Special case: if there's nothing after the arrow, it is one rule with
        # an empty RHS, instead of no rules.
        if len(rules) == 0: rules = [Production(lhs, ())]
        return rules

class GrammarCategory(Category):
    """
    A class of C{Category} for use in parsing.

    The name of the head feature in a C{GrammarCategory} is C{pos} (for "part
    of speech").
    
    In addition, GrammarCategories are displayed and parse differently, to be
    consistent with NLP teaching materials: the value of the C{/} feature can
    be written with a slash after the right bracket, so that the string
    representation looks like: C{head[...]/value}.

    Every GrammarCategory has a / feature implicitly present; if it is not
    explicitly written, it has the value False. This is so that "slashed"
    features cannot unify with "unslashed" ones.

    An example of a C{GrammarCategory} is C{VP[+fin]/NP}, for a verb phrase
    that is finite and has an omitted noun phrase inside it.
    """
    
    headname = 'pos'
    yaml_tag = '!parse.GrammarCategory'
    
    @classmethod
    def _str(cls, obj, reentrances, reentrance_ids, normalize=False):
        segments = []

        keys = obj.keys()
        keys.sort()
        for fname in keys:
            if fname == cls.headname: continue
            if isinstance(obj, GrammarCategory) and fname == '/': continue
            fval = obj[fname]
            if isinstance(fval, bool):
                if fval: segments.append('+%s' % fname)
                else: segments.append('-%s' % fname)
            elif normalize and isinstance(fval, logic.Expression):
                segments.append('%s=%s' % (fname, fval.normalize()))
            elif not isinstance(fval, dict):
                segments.append('%s=%s' % (fname, fval))
            else:
                fval_repr = cls._str(fval, reentrances, reentrance_ids)
                segments.append('%s=%s' % (fname, fval_repr))

        if segments: features = '[%s]' % ', '.join(segments)
        else: features = ''
        head = obj.get(cls.headname)
        if head is None: head = ''
        slash = None
        if isinstance(obj, GrammarCategory): slash = obj.get('/')
        if not slash: slash = ''
        else:
            if isinstance(slash, dict):
                slash = '/%s' % cls._str(slash, reentrances, reentrance_ids)
            else:
                slash = '/%r' % slash

        
        return '%s%s%s' % (head, features, slash)
    
    @staticmethod
    def parse(s, position=0):
        return GrammarCategory.inner_parse(s, position)[0]
    
    @classmethod
    def inner_parse(cls, s, position, reentrances=None):
        if reentrances is None: reentrances = {}
        if s[position] in "'\"":
            start = position
            quotemark = s[position:position+1]
            position += 1
            while 1: 
                match = cls._PARSE_RE['stringmarker'].search(s, position)
                if not match: raise ValueError('close quote', position)
                position = match.end()
                if match.group() == '\\': position += 1
                elif match.group() == quotemark:
                    return s[start+1:position-1], position

        body, position = GrammarCategory._parse(s, position, reentrances)
        slash_match = Category._PARSE_RE['slash'].match(s, position)
        if slash_match is not None:
            position = slash_match.end()
            slash, position = GrammarCategory._parseval(s, position, reentrances)
            if isinstance(slash, basestring): slash = {'pos': slash}
            body['/'] = unify(body.get('/'), slash)
        elif not body.has_key('/'):
            body['/'] = False
        return cls(body), position
    

class ParserSubstitute(logic.Parser):
    """
    A lambda calculus expression parser, extended to create application
    expressions which support the SubstituteBindingsI interface.
    """
    def make_ApplicationExpression(self, first, second):
        return logic.ApplicationExpressionSubst(first, second)
    def make_LambdaExpression(self, first, second):
        return logic.LambdaExpressionSubst(first, second)
    def make_SomeExpression(self, first, second):
        return logic.SomeExpressionSubst(first, second)
    def make_AllExpression(self, first, second):
        return logic.AllExpressionSubst(first, second)
    


############################################################################
# Read a grammar from a file
############################################################################

class GrammarFile(object):
    def __init__(self):
        self.grammatical_productions = []
        self.lexical_productions = []
        self.start = GrammarCategory(pos='Start')
        self.kimmo = None
        
    def grammar(self):
        return Grammar(self.start, self.grammatical_productions +\
        self.lexical_productions)
        
    def earley_grammar(self):
        return Grammar(self.start, self.grammatical_productions)
    
    def earley_lexicon(self):
        lexicon = {}
        for prod in self.lexical_productions:
            lexicon.setdefault(prod.rhs()[0].upper(), []).append(prod.lhs())
        def lookup(word):
            return lexicon.get(word.upper(), [])
        return lookup

    def kimmo_lexicon(self):
        def lookup(word):
            kimmo_results = self.kimmo.recognize(word.lower())
            return [GrammarCategory(k[1]) for k in kimmo_results]
        return lookup

    def earley_parser(self, trace=1):
        from featurechart import FeatureEarleyChartParse
        if self.kimmo is None: lexicon = self.earley_lexicon()
        else: lexicon = self.kimmo_lexicon()
        
        return FeatureEarleyChartParse(self.earley_grammar(),
                           lexicon, trace=trace)

    def apply_lines(self, lines):
        for line in lines:
            line = line.strip()
            if not len(line): continue
            if line[0] == '#': continue
            if line[0] == '%':
                parts = line[1:].split()
                directive = parts[0]
                args = " ".join(parts[1:])
                if directive == 'start':
                    self.start = GrammarCategory.parse(args).freeze()
                elif directive == 'include':
                    filename = args.strip('"')
                    self.apply_file(filename)
                elif directive == 'tagger_file':
                    import yaml, nltk.yamltags
                    filename = args.strip('"')
                    tagger = yaml.load(filename)
                    self.tagproc = chart_tagger(tagger)
                elif directive == 'kimmo':
                    filename = args.strip('"')
                    kimmorules = kimmo.load(filename)
                    self.kimmo = kimmorules
            else:
                rules = GrammarCategory.parse_rules(line)
                for rule in rules:
                    if len(rule.rhs()) == 1 and isinstance(rule.rhs()[0], str):
                        self.lexical_productions.append(rule)
                    else:
                        self.grammatical_productions.append(rule)

    def apply_file(self, filename):
        f = open(filename)
        lines = f.readlines()
        self.apply_lines(lines)
        f.close()
    
    @staticmethod
    def read_file(filename):
        result = GrammarFile()
        result.apply_file(filename)
        return result

yaml.add_representer(Category, Category.to_yaml)
yaml.add_representer(GrammarCategory, GrammarCategory.to_yaml)

def demo():
    print "Category(pos='n', agr=dict(number='pl', gender='f')):"
    print
    print Category(pos='n', agr=dict(number='pl', gender='f'))
    print repr(Category(pos='n', agr=dict(number='pl', gender='f')))
    print
    print "GrammarCategory.parse('NP[sem=<bob>/NP'):"
    print
    print GrammarCategory.parse(r'NP[sem=<bob>]/NP')
    print repr(GrammarCategory.parse(r'NP[sem=<bob>]/NP'))
    print
    print "GrammarCategory.parse('?x/?x'):"
    print
    print GrammarCategory.parse('?x/?x')
    print repr(GrammarCategory.parse('?x/?x'))
    print
    print "GrammarCategory.parse('VP[+fin, agr=?x, tense=past]/NP[+pl, agr=?x]'):"
    print
    print GrammarCategory.parse('VP[+fin, agr=?x, tense=past]/NP[+pl, agr=?x]')
    print repr(GrammarCategory.parse('VP[+fin, agr=?x, tense=past]/NP[+pl, agr=?x]'))
    print
    g = GrammarFile.read_file("speer.cfg")
    print g.grammar()
    
if __name__ == '__main__':
    demo()
    

########NEW FILE########
__FILENAME__ = cfg
# Natural Language Toolkit: Context Free Grammars
#
# Copyright (C) 2001-2007 NLTK Project
# Author: Steven Bird <sb@csse.unimelb.edu.au>
#         Edward Loper <edloper@ldc.upenn.edu>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT
#

"""
Basic data classes for representing context free grammars.  A
X{grammar} specifies which trees can represent the structure of a
given text.  Each of these trees is called a X{parse tree} for the
text (or simply a X{parse}).  In a X{context free} grammar, the set of
parse trees for any piece of a text can depend only on that piece, and
not on the rest of the text (i.e., the piece's context).  Context free
grammars are often used to find possible syntactic structures for
sentences.  In this context, the leaves of a parse tree are word
tokens; and the node values are phrasal categories, such as C{NP}
and C{VP}.

The L{Grammar} class is used to encode context free grammars.  Each C{Grammar}
consists of a start symbol and a set of productions.  The X{start
symbol} specifies the root node value for parse trees.  For example,
the start symbol for syntactic parsing is usually C{S}.  Start
symbols are encoded using the C{Nonterminal} class, which is discussed
below.

A Grammar's X{productions} specify what parent-child relationships a parse
tree can contain.  Each production specifies that a particular
node can be the parent of a particular set of children.  For example,
the production C{<S> -> <NP> <VP>} specifies that an C{S} node can
be the parent of an C{NP} node and a C{VP} node.

Grammar productions are implemented by the C{Production} class.
Each C{Production} consists of a left hand side and a right hand
side.  The X{left hand side} is a C{Nonterminal} that specifies the
node type for a potential parent; and the X{right hand side} is a list
that specifies allowable children for that parent.  This lists
consists of C{Nonterminals} and text types: each C{Nonterminal}
indicates that the corresponding child may be a C{TreeToken} with the
specified node type; and each text type indicates that the
corresponding child may be a C{Token} with the with that type.

The C{Nonterminal} class is used to distinguish node values from leaf
values.  This prevents the grammar from accidentally using a leaf
value (such as the English word "A") as the node of a subtree.  Within
a C{Grammar}, all node values are wrapped in the C{Nonterminal} class.
Note, however, that the trees that are specified by the grammar do
B{not} include these C{Nonterminal} wrappers.

Grammars can also be given a more procedural interpretation.  According to
this interpretation, a Grammar specifies any tree structure M{tree} that
can be produced by the following procedure:

    - Set M{tree} to the start symbol
    - Repeat until M{tree} contains no more nonterminal leaves:
      - Choose a production M{prod} with whose left hand side
        M{lhs} is a nonterminal leaf of M{tree}.
      - Replace the nonterminal leaf with a subtree, whose node
        value is the value wrapped by the nonterminal M{lhs}, and
        whose children are the right hand side of M{prod}.

The operation of replacing the left hand side (M{lhs}) of a production
with the right hand side (M{rhs}) in a tree (M{tree}) is known as
X{expanding} M{lhs} to M{rhs} in M{tree}.
"""

import re


#################################################################
# Nonterminal
#################################################################

class Nonterminal(object):
    """
    A non-terminal symbol for a context free grammar.  C{Nonterminal}
    is a wrapper class for node values; it is used by
    C{Production}s to distinguish node values from leaf values.
    The node value that is wrapped by a C{Nonterminal} is known as its
    X{symbol}.  Symbols are typically strings representing phrasal
    categories (such as C{"NP"} or C{"VP"}).  However, more complex
    symbol types are sometimes used (e.g., for lexicalized grammars).
    Since symbols are node values, they must be immutable and
    hashable.  Two C{Nonterminal}s are considered equal if their
    symbols are equal.

    @see: L{Grammar}
    @see: L{Production}
    @type _symbol: (any)
    @ivar _symbol: The node value corresponding to this
        C{Nonterminal}.  This value must be immutable and hashable. 
    """
    def __init__(self, symbol):
        """
        Construct a new non-terminal from the given symbol.

        @type symbol: (any)
        @param symbol: The node value corresponding to this
            C{Nonterminal}.  This value must be immutable and
            hashable. 
        """
        self._symbol = symbol
        self._hash = hash(symbol)

    def symbol(self):
        """
        @return: The node value corresponding to this C{Nonterminal}. 
        @rtype: (any)
        """
        return self._symbol

    def __eq__(self, other):
        """
        @return: True if this non-terminal is equal to C{other}.  In
            particular, return true iff C{other} is a C{Nonterminal}
            and this non-terminal's symbol is equal to C{other}'s
            symbol.
        @rtype: C{boolean}
        """
        try:
            return ((self._symbol == other._symbol) \
                    and isinstance(other, self.__class__))
        except AttributeError:
            return False

    def __ne__(self, other):
        """
        @return: True if this non-terminal is not equal to C{other}.  In
            particular, return true iff C{other} is not a C{Nonterminal}
            or this non-terminal's symbol is not equal to C{other}'s
            symbol.
        @rtype: C{boolean}
        """
        return not (self==other)

    def __cmp__(self, other):
        if self == other: return 0
        else: return -1

    def __hash__(self):
        return self._hash

    def __repr__(self):
        """
        @return: A string representation for this C{Nonterminal}.
            The string representation for a C{Nonterminal} whose
            symbol is C{M{s}} is C{<M{s}>}.
        @rtype: C{string}
        """
        # [XX] not a good repr!  Token uses this now!!
        return '<%s>' % (self._symbol,)

    def __str__(self):
        """
        @return: A string representation for this C{Nonterminal}.
            The string representation for a C{Nonterminal} whose
            symbol is C{M{s}} is C{M{s}}.
        @rtype: C{string}
        """
        return '%s' % (self._symbol,)

    def __div__(self, rhs):
        """
        @return: A new nonterminal whose symbol is C{M{A}/M{B}}, where
            C{M{A}} is the symbol for this nonterminal, and C{M{B}}
            is the symbol for rhs.
        @rtype: L{Nonterminal}
        @param rhs: The nonterminal used to form the right hand side
            of the new nonterminal.
        @type rhs: L{Nonterminal}
        """
        return Nonterminal('%s/%s' % (self._symbol, rhs._symbol))

def nonterminals(symbols):
    """
    Given a string containing a list of symbol names, return a list of
    C{Nonterminals} constructed from those symbols.  

    @param symbols: The symbol name string.  This string can be
        delimited by either spaces or commas.
    @type symbols: C{string}
    @return: A list of C{Nonterminals} constructed from the symbol
        names given in C{symbols}.  The C{Nonterminals} are sorted
        in the same order as the symbols names.
    @rtype: C{list} of L{Nonterminal}
    """
    if ',' in symbols: symbol_list = symbols.split(',')
    else: symbol_list = symbols.split()
    return [Nonterminal(s.strip()) for s in symbol_list]

#################################################################
# Production and Grammar
#################################################################

class Production(object):
    """
    A context-free grammar production.  Each production
    expands a single C{Nonterminal} (the X{left-hand side}) to a
    sequence of terminals and C{Nonterminals} (the X{right-hand
    side}).  X{terminals} can be any immutable hashable object that is
    not a C{Nonterminal}.  Typically, terminals are strings
    representing word types, such as C{"dog"} or C{"under"}.

    Abstractly, a Grammar production indicates that the right-hand side is
    a possible X{instantiation} of the left-hand side.  Grammar
    productions are X{context-free}, in the sense that this
    instantiation should not depend on the context of the left-hand
    side or of the right-hand side.

    @see: L{Grammar}
    @see: L{Nonterminal}
    @type _lhs: L{Nonterminal}
    @ivar _lhs: The left-hand side of the production.
    @type _rhs: C{tuple} of (C{Nonterminal} and (terminal))
    @ivar _rhs: The right-hand side of the production.
    """

    def __init__(self, lhs, rhs):
        """
        Construct a new C{Production}.

        @param lhs: The left-hand side of the new C{Production}.
        @type lhs: L{Nonterminal}
        @param rhs: The right-hand side of the new C{Production}.
        @type rhs: sequence of (C{Nonterminal} and (terminal))
        """
        if isinstance(rhs, (str, unicode)):
            raise TypeError, 'production right hand side should be a list, not a string'
        self._lhs = lhs
        self._rhs = tuple(rhs)
        self._hash = hash((self._lhs, self._rhs))

    def lhs(self):
        """
        @return: the left-hand side of this C{Production}.
        @rtype: L{Nonterminal}
        """
        return self._lhs

    def rhs(self):
        """
        @return: the right-hand side of this C{Production}.
        @rtype: sequence of (C{Nonterminal} and (terminal))
        """
        return self._rhs

    def __str__(self):
        """
        @return: A verbose string representation of the
            C{Production}.
        @rtype: C{string}
        """
        str = '%s ->' % (self._lhs.symbol(),)
        for elt in self._rhs:
            if isinstance(elt, Nonterminal):
                str += ' %s' % (elt.symbol(),)
            else:
                str += ' %r' % (elt,)
        return str

    def __repr__(self):
        """
        @return: A concise string representation of the
            C{Production}. 
        @rtype: C{string}
        """
        return '%s' % self

    def __eq__(self, other):
        """
        @return: true if this C{Production} is equal to C{other}.
        @rtype: C{boolean}
        """
        return (isinstance(other, self.__class__) and
                self._lhs == other._lhs and
                self._rhs == other._rhs)
                 
    def __ne__(self, other):
        return not (self == other)

    def __cmp__(self, other):
        if not isinstance(other, self.__class__): return -1
        return cmp((self._lhs, self._rhs), (other._lhs, other._rhs))

    def __hash__(self):
        """
        @return: A hash value for the C{Production}.
        @rtype: C{int}
        """
        return self._hash


class Grammar(object):
    """
    A context-free grammar.  A Grammar consists of a start state and a set
    of productions.  The set of terminals and nonterminals is
    implicitly specified by the productions.

    If you need efficient key-based access to productions, you
    can use a subclass to implement it.
    """
    def __init__(self, start, productions):
        """
        Create a new context-free grammar, from the given start state
        and set of C{Production}s.
        
        @param start: The start symbol
        @type start: L{Nonterminal}
        @param productions: The list of productions that defines the grammar
        @type productions: C{list} of L{Production}
        """
        self._start = start
        self._productions = productions
        self._lhs_index = {}
        self._rhs_index = {}
        for prod in self._productions:
            if prod._lhs not in self._lhs_index:
                self._lhs_index[prod._lhs] = []
            if prod._rhs and prod._rhs[0] not in self._rhs_index:
                self._rhs_index[prod._rhs[0]] = []
            self._lhs_index[prod._lhs].append(prod)
            if prod._rhs:
                self._rhs_index[prod._rhs[0]].append(prod)
        
    def start(self):
        return self._start

    # tricky to balance readability and efficiency here!
    # can't use set operations as they don't preserve ordering
    def productions(self, lhs=None, rhs=None):
        # no constraints so return everything
        if not lhs and not rhs:
            return self._productions

        # only lhs specified so look up its index
        elif lhs and not rhs:
            if lhs in self._lhs_index:
                return self._lhs_index[lhs]
            else:
                return []

        # only rhs specified so look up its index
        elif rhs and not lhs:
            if rhs in self._rhs_index:
                return self._rhs_index[rhs]
            else:
                return []

        # intersect
        else:
            if lhs in self._lhs_index:
                return [prod for prod in self._lhs_index[lhs]
                        if prod in self._rhs_index[rhs]]
            else:
                return []

    def __repr__(self):
        return '<Grammar with %d productions>' % len(self._productions)

    def __str__(self):
        str = 'Grammar with %d productions' % len(self._productions)
        str += ' (start state = %s)' % self._start
        for production in self._productions:
            str += '\n    %s' % production
        return str

_PARSE_RE = re.compile(r'''^(\w+)\s*           # lhs
                          (?:-+>|=+>)\s*       # arrow
                          (?:(                 # rhs:
                               "[^"]+"         # doubled-quoted terminal
                               |'[^']+'        # single-quoted terminal
                               |\w+|           # non-terminal
                               \|              # disjunction
                             )
                             \s*)              # trailing space
                             *$''',
                       re.VERBOSE)
_SPLIT_RE = re.compile(r'''(\w+|-+>|=+>|"[^"]+"|'[^']+'|\|)''')

def parse_production(s):
    """
    Returns a list of productions
    """
    # Use _PARSE_RE to check that it's valid.
    if not _PARSE_RE.match(s):
        raise ValueError, 'Bad production string'
    # Use _SPLIT_RE to process it.
    pieces = _SPLIT_RE.split(s)
    pieces = [p for i,p in enumerate(pieces) if i%2==1]
    lhside = Nonterminal(pieces[0])
    rhsides = [[]]
    for piece in pieces[2:]:
        if piece == '|':
            rhsides.append([])                     # Vertical bar
        elif piece[0] in ('"', "'"):
            rhsides[-1].append(piece[1:-1])        # Terminal
        else:
            rhsides[-1].append(Nonterminal(piece)) # Nonterminal
    return [Production(lhside, rhside) for rhside in rhsides]

def parse_grammar(s):
    productions = []
    for linenum, line in enumerate(s.split('\n')):
        line = line.strip()
        if line.startswith('#') or line=='': continue
        try: productions += parse_production(line)
        except ValueError:
            raise ValueError, 'Unable to parse line %s' % linenum
    if len(productions) == 0:
        raise ValueError, 'No productions found!'
    start = productions[0].lhs()
    return Grammar(start, productions)

#################################################################
# Demonstration
#################################################################

def demo():
    """
    A demonstration showing how C{Grammar}s can be created and used.
    """

    from nltk import cfg

    # Create some nonterminals
    S, NP, VP, PP = cfg.nonterminals('S, NP, VP, PP')
    N, V, P, Det = cfg.nonterminals('N, V, P, Det')
    VP_slash_NP = VP/NP

    print 'Some nonterminals:', [S, NP, VP, PP, N, V, P, Det, VP/NP]
    print '    S.symbol() =>', `S.symbol()`
    print

    print cfg.Production(S, [NP])

    # Create some Grammar Productions
    grammar = cfg.parse_grammar("""
    S -> NP VP
    PP -> P NP
    NP -> Det N
    NP -> NP PP
    VP -> V NP
    VP -> VP PP
    Det -> 'a'
    Det -> 'the'
    N -> 'dog'
    N -> 'cat'
    V -> 'chased'
    V -> 'sat'
    P -> 'on'
    P -> 'in'
    """)

    print 'A Grammar:', `grammar`
    print '    grammar.start()       =>', `grammar.start()`
    print '    grammar.productions() =>',
    # Use string.replace(...) is to line-wrap the output.
    print `grammar.productions()`.replace(',', ',\n'+' '*25)
    print

if __name__ == '__main__': demo()

########NEW FILE########
__FILENAME__ = chart
# Natural Language Toolkit: A Chart Parser
#
# Copyright (C) 2001-2007 NLTK Project
# Author: Edward Loper <edloper@gradient.cis.upenn.edu>
#         Steven Bird <sb@csse.unimelb.edu.au>
#         Jean Mark Gawron <gawron@mail.sdsu.edu>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT
#
# $Id: chart.py 4157 2007-02-28 09:56:25Z stevenbird $

from __init__ import *
from nltk import cfg, Tree

"""
Data classes and parser implementations for \"chart parsers\", which
use dynamic programming to efficiently parse a text.  A X{chart
parser} derives parse trees for a text by iteratively adding \"edges\"
to a \"chart.\"  Each X{edge} represents a hypothesis about the tree
structure for a subsequence of the text.  The X{chart} is a
\"blackboard\" for composing and combining these hypotheses.

When a chart parser begins parsing a text, it creates a new (empty)
chart, spanning the text.  It then incrementally adds new edges to the
chart.  A set of X{chart rules} specifies the conditions under which
new edges should be added to the chart.  Once the chart reaches a
stage where none of the chart rules adds any new edges, parsing is
complete.

Charts are encoded with the L{Chart} class, and edges are encoded with
the L{TreeEdge} and L{LeafEdge} classes.  The chart parser module
defines three chart parsers:

  - C{ChartParse} is a simple and flexible chart parser.  Given a
    set of chart rules, it will apply those rules to the chart until
    no more edges are added.

  - C{SteppingChartParse} is a subclass of C{ChartParse} that can
    be used to step through the parsing process.

  - C{EarleyChartParse} is an implementation of the Earley chart parsing
    algorithm.  It makes a single left-to-right pass through the
    chart, and applies one of three rules (predictor, scanner, and
    completer) to each edge it encounters.
"""

import re

########################################################################
##  Edges
########################################################################

class EdgeI(object):
    """
    A hypothesis about the structure of part of a sentence.
    Each edge records the fact that a structure is (partially)
    consistent with the sentence.  An edge contains:

        - A X{span}, indicating what part of the sentence is
          consistent with the hypothesized structure.
          
        - A X{left-hand side}, specifying what kind of structure is
          hypothesized.

        - A X{right-hand side}, specifying the contents of the
          hypothesized structure.

        - A X{dot position}, indicating how much of the hypothesized
          structure is consistent with the sentence.

    Every edge is either X{complete} or X{incomplete}:

      - An edge is X{complete} if its structure is fully consistent
        with the sentence.

      - An edge is X{incomplete} if its structure is partially
        consistent with the sentence.  For every incomplete edge, the
        span specifies a possible prefix for the edge's structure.
    
    There are two kinds of edge:

        - C{TreeEdges<TreeEdge>} record which trees have been found to
          be (partially) consistent with the text.
          
        - C{LeafEdges<leafEdge>} record the tokens occur in the text.

    The C{EdgeI} interface provides a common interface to both types
    of edge, allowing chart parsers to treat them in a uniform manner.
    """
    def __init__(self):
        if self.__class__ == EdgeI: 
            raise TypeError('Edge is an abstract interface')
        
    #////////////////////////////////////////////////////////////
    # Span
    #////////////////////////////////////////////////////////////

    def span(self):
        """
        @return: A tuple C{(s,e)}, where C{subtokens[s:e]} is the
            portion of the sentence that is consistent with this
            edge's structure.
        @rtype: C{(int, int)}
        """
        raise AssertionError('EdgeI is an abstract interface')

    def start(self):
        """
        @return: The start index of this edge's span.
        @rtype: C{int}
        """
        raise AssertionError('EdgeI is an abstract interface')

    def end(self):
        """
        @return: The end index of this edge's span.
        @rtype: C{int}
        """
        raise AssertionError('EdgeI is an abstract interface')

    def length(self):
        """
        @return: The length of this edge's span.
        @rtype: C{int}
        """
        raise AssertionError('EdgeI is an abstract interface')

    #////////////////////////////////////////////////////////////
    # Left Hand Side
    #////////////////////////////////////////////////////////////

    def lhs(self):
        """
        @return: This edge's left-hand side, which specifies what kind
            of structure is hypothesized by this edge.
        @see: L{TreeEdge} and L{LeafEdge} for a description of
            the left-hand side values for each edge type.
        """
        raise AssertionError('EdgeI is an abstract interface')

    #////////////////////////////////////////////////////////////
    # Right Hand Side
    #////////////////////////////////////////////////////////////

    def rhs(self):
        """
        @return: This edge's right-hand side, which specifies
            the content of the structure hypothesized by this
            edge.
        @see: L{TreeEdge} and L{LeafEdge} for a description of
            the right-hand side values for each edge type.
        """
        raise AssertionError('EdgeI is an abstract interface')

    def dot(self):
        """
        @return: This edge's dot position, which indicates how much of
            the hypothesized structure is consistent with the
            sentence.  In particular, C{self.rhs[:dot]} is consistent
            with C{subtoks[self.start():self.end()]}.
        @rtype: C{int}
        """
        raise AssertionError('EdgeI is an abstract interface')

    def next(self):
        """
        @return: The element of this edge's right-hand side that
            immediately follows its dot.
        @rtype: C{Nonterminal} or X{terminal} or C{None}
        """
        raise AssertionError('EdgeI is an abstract interface')

    def is_complete(self):
        """
        @return: True if this edge's structure is fully consistent
            with the text.
        @rtype: C{boolean}
        """
        raise AssertionError('EdgeI is an abstract interface')

    def is_incomplete(self):
        """
        @return: True if this edge's structure is partially consistent
            with the text.
        @rtype: C{boolean}
        """
        raise AssertionError('EdgeI is an abstract interface')

    #////////////////////////////////////////////////////////////
    # Comparisons
    #////////////////////////////////////////////////////////////
    def __cmp__(self, other):
        raise AssertionError('EdgeI is an abstract interface')

    def __hash__(self, other):
        raise AssertionError('EdgeI is an abstract interface')

class TreeEdge(EdgeI):
    """
    An edge that records the fact that a tree is (partially)
    consistent with the sentence.  A tree edge consists of:

        - A X{span}, indicating what part of the sentence is
          consistent with the hypothesized tree.
          
        - A X{left-hand side}, specifying the hypothesized tree's node
          value.

        - A X{right-hand side}, specifying the hypothesized tree's
          children.  Each element of the right-hand side is either a
          terminal, specifying a token with that terminal as its leaf
          value; or a nonterminal, specifying a subtree with that
          nonterminal's symbol as its node value.

        - A X{dot position}, indicating which children are consistent
          with part of the sentence.  In particular, if C{dot} is the
          dot position, C{rhs} is the right-hand size, C{(start,end)}
          is the span, and C{sentence} is the list of subtokens in the
          sentence, then C{subtokens[start:end]} can be spanned by the
          children specified by C{rhs[:dot]}.

    For more information about edges, see the L{EdgeI} interface.
    """
    def __init__(self, span, lhs, rhs, dot=0):
        """
        Construct a new C{TreeEdge}.
        
        @type span: C{(int, int)}
        @param span: A tuple C{(s,e)}, where C{subtokens[s:e]} is the
            portion of the sentence that is consistent with the new
            edge's structure.
        @type lhs: L{Nonterminal}
        @param lhs: The new edge's left-hand side, specifying the
            hypothesized tree's node value.
        @type rhs: C{list} of (L{Nonterminal} and C{string})
        @param rhs: The new edge's right-hand side, specifying the
            hypothesized tree's children.
        @type dot: C{int}
        @param dot: The position of the new edge's dot.  This position
            specifies what prefix of the production's right hand side
            is consistent with the text.  In particular, if
            C{sentence} is the list of subtokens in the sentence, then
            C{subtokens[span[0]:span[1]]} can be spanned by the
            children specified by C{rhs[:dot]}.
        """
        self._lhs = lhs
        self._rhs = tuple(rhs)
        self._span = span
        self._dot = dot

    # [staticmethod]
    def from_production(production, index):
        """
        @return: A new C{TreeEdge} formed from the given production.
            The new edge's left-hand side and right-hand side will
            be taken from C{production}; its span will be C{(index,
            index)}; and its dot position will be C{0}.
        @rtype: L{TreeEdge}
        """
        return TreeEdge(span=(index, index), lhs=production.lhs(),
                        rhs=production.rhs(), dot=0)
    from_production = staticmethod(from_production)

    # Accessors
    def lhs(self): return self._lhs
    def span(self): return self._span
    def start(self): return self._span[0]
    def end(self): return self._span[1]
    def length(self): return self._span[1] - self._span[0]
    def rhs(self): return self._rhs
    def dot(self): return self._dot
    def is_complete(self): return self._dot == len(self._rhs)
    def is_incomplete(self): return self._dot != len(self._rhs)
    def next(self):
        if self._dot >= len(self._rhs): return None
        else: return self._rhs[self._dot]

    # Comparisons & hashing
    def __cmp__(self, other):
        if self.__class__ != other.__class__: return -1
        return cmp((self._span, self.lhs(), self.rhs(), self._dot),
                   (other._span, other.lhs(), other.rhs(), other._dot))
    def __hash__(self):
        return hash((self.lhs(), self.rhs(), self._span, self._dot))

    # String representation
    def __str__(self):
        str = '[%s:%s] ' % (self._span[0], self._span[1])
        str += '%-2s ->' % (self._lhs.symbol(),)
            
        for i in range(len(self._rhs)):
            if i == self._dot: str += ' *'
            if isinstance(self._rhs[i], cfg.Nonterminal):
                str += ' %s' % (self._rhs[i].symbol(),)
            else:
                str += ' %r' % (self._rhs[i],)
        if len(self._rhs) == self._dot: str += ' *'
        return str
        
    def __repr__(self):
        return '[Edge: %s]' % self

class LeafEdge(EdgeI):
    """
    An edge that records the fact that a leaf value is consistent with
    a word in the sentence.  A leaf edge consists of:

      - An X{index}, indicating the position of the word.
      - A X{leaf}, specifying the word's content.

    A leaf edge's left-hand side is its leaf value, and its right hand
    side is C{()}.  Its span is C{[index, index+1]}, and its dot
    position is C{0}.
    """
    def __init__(self, leaf, index):
        """
        Construct a new C{LeafEdge}.

        @param leaf: The new edge's leaf value, specifying the word
            that is recorded by this edge.
        @param index: The new edge's index, specifying the position of
            the word that is recorded by this edge.
        """
        self._leaf = leaf
        self._index = index

    # Accessors
    def lhs(self): return self._leaf
    def span(self): return (self._index, self._index+1)
    def start(self): return self._index
    def end(self): return self._index+1
    def length(self): return 1
    def rhs(self): return ()
    def dot(self): return 0
    def is_complete(self): return True
    def is_incomplete(self): return False
    def next(self): return None

    # Comparisons & hashing
    def __cmp__(self, other):
        if not isinstance(other, LeafEdge): return -1
        return cmp((self._index, self._leaf), (other._index, other._leaf))
    def __hash__(self):
        return hash((self._index, self._leaf))

    # String representations
    def __str__(self): return '[%s:%s] %r' % (self._index, self._index+1, self._leaf)
    def __repr__(self):
        return '[Edge: %s]' % (self)

########################################################################
##  Chart
########################################################################

class Chart(object):
    """
    A blackboard for hypotheses about the syntactic constituents of a
    sentence.  A chart contains a set of edges, and each edge encodes
    a single hypothesis about the structure of some portion of the
    sentence.

    The L{select} method can be used to select a specific collection
    of edges.  For example C{chart.select(is_complete=True, start=0)}
    yields all complete edges whose start indices are 0.  To ensure
    the efficiency of these selection operations, C{Chart} dynamically
    creates and maintains an index for each set of attributes that
    have been selected on.

    In order to reconstruct the trees that are represented by an edge,
    the chart associates each edge with a set of child pointer lists.
    A X{child pointer list} is a list of the edges that license an
    edge's right-hand side.

    @ivar _tokens: The sentence that the chart covers.
    @ivar _num_leaves: The number of tokens.
    @ivar _edges: A list of the edges in the chart
    @ivar _edge_to_cpls: A dictionary mapping each edge to a set
        of child pointer lists that are associated with that edge.
    @ivar _indexes: A dictionary mapping tuples of edge attributes
        to indices, where each index maps the corresponding edge
        attribute values to lists of edges.
    """
    def __init__(self, tokens):
        """
        Construct a new empty chart.

        @type tokens: L{list}
        @param tokens: The sentence that this chart will be used to parse.
        """
        # Record the sentence token and the sentence length.
        self._tokens = list(tokens)
        self._num_leaves = len(self._tokens)

        # A list of edges contained in this chart.
        self._edges = []
        
        # The set of child pointer lists associated with each edge.
        self._edge_to_cpls = {}

        # Indexes mapping attribute values to lists of edges (used by
        # select()).
        self._indexes = {}

    #////////////////////////////////////////////////////////////
    # Sentence Access
    #////////////////////////////////////////////////////////////

    def num_leaves(self):
        """
        @return: The number of words in this chart's sentence.
        @rtype: C{int}
        """
        return self._num_leaves

    def leaf(self, index):
        """
        @return: The leaf value of the word at the given index.
        @rtype: C{string}
        """
        return self._tokens[index]

    def leaves(self):
        """
        @return: A list of the leaf values of each word in the
            chart's sentence.
        @rtype: C{list} of C{string}
        """
        return self._tokens

    #////////////////////////////////////////////////////////////
    # Edge access
    #////////////////////////////////////////////////////////////

    def edges(self):
        """
        @return: A list of all edges in this chart.  New edges
            that are added to the chart after the call to edges()
            will I{not} be contained in this list.
        @rtype: C{list} of L{EdgeI}
        @see: L{iteredges}, L{select}
        """
        return self._edges[:]

    def iteredges(self):
        """
        @return: An iterator over the edges in this chart.  Any
            new edges that are added to the chart before the iterator
            is exahusted will also be generated.
        @rtype: C{iter} of L{EdgeI}
        @see: L{edges}, L{select}
        """
        return iter(self._edges)

    # Iterating over the chart yields its edges.
    __iter__ = iteredges

    def num_edges(self):
        """
        @return: The number of edges contained in this chart.
        @rtype: C{int}
        """
        return len(self._edge_to_cpls)

    def select(self, **restrictions):
        """
        @return: An iterator over the edges in this chart.  Any
            new edges that are added to the chart before the iterator
            is exahusted will also be generated.  C{restrictions}
            can be used to restrict the set of edges that will be
            generated.
        @rtype: C{iter} of L{EdgeI}

        @kwarg span: Only generate edges C{e} where C{e.span()==span}
        @kwarg start: Only generate edges C{e} where C{e.start()==start}
        @kwarg end: Only generate edges C{e} where C{e.end()==end}
        @kwarg length: Only generate edges C{e} where C{e.length()==length}
        @kwarg lhs: Only generate edges C{e} where C{e.lhs()==lhs}
        @kwarg rhs: Only generate edges C{e} where C{e.rhs()==rhs}
        @kwarg next: Only generate edges C{e} where C{e.next()==next}
        @kwarg dot: Only generate edges C{e} where C{e.dot()==dot}
        @kwarg is_complete: Only generate edges C{e} where
            C{e.is_complete()==is_complete}
        @kwarg is_incomplete: Only generate edges C{e} where
            C{e.is_incomplete()==is_incomplete}
        """
        # If there are no restrictions, then return all edges.
        if restrictions=={}: return iter(self._edges)
            
        # Find the index corresponding to the given restrictions.
        restr_keys = restrictions.keys()
        restr_keys.sort()
        restr_keys = tuple(restr_keys)

        # If it doesn't exist, then create it.
        if not self._indexes.has_key(restr_keys):
            self._add_index(restr_keys)
        vals = [restrictions[k] for k in restr_keys]
        return iter(self._indexes[restr_keys].get(tuple(vals), []))

    def _add_index(self, restr_keys):
        """
        A helper function for L{select}, which creates a new index for
        a given set of attributes (aka restriction keys).
        """
        # Make sure it's a valid index.
        for k in restr_keys:
            if not hasattr(EdgeI, k):
                raise ValueError, 'Bad restriction: %s' % k

        # Create the index.
        self._indexes[restr_keys] = {}

        # Add all existing edges to the index.
        for edge in self._edges:
            vals = [getattr(edge, k)() for k in restr_keys]
            index = self._indexes[restr_keys]
            index.setdefault(tuple(vals),[]).append(edge)

    #////////////////////////////////////////////////////////////
    # Edge Insertion
    #////////////////////////////////////////////////////////////

    def insert(self, edge, child_pointer_list):
        """
        Add a new edge to the chart.

        @type edge: L{Edge}
        @param edge: The new edge
        @type child_pointer_list: C{tuple} of L{Edge}
        @param child_pointer_list: A list of the edges that were used to
            form this edge.  This list is used to reconstruct the trees
            (or partial trees) that are associated with C{edge}.
        @rtype: C{bool}
        @return: True if this operation modified the chart.  In
            particular, return true iff the chart did not already
            contain C{edge}, or if it did not already associate
            C{child_pointer_list} with C{edge}.
        """
        # Is it a new edge?
        if not self._edge_to_cpls.has_key(edge):
            # Add it to the list of edges.
            self._edges.append(edge)

            # Register with indexes
            for (restr_keys, index) in self._indexes.items():
                vals = [getattr(edge, k)() for k in restr_keys]
                index = self._indexes[restr_keys]
                index.setdefault(tuple(vals),[]).append(edge)

        # Get the set of child pointer lists for this edge.
        cpls = self._edge_to_cpls.setdefault(edge,{})
        child_pointer_list = tuple(child_pointer_list)

        if cpls.has_key(child_pointer_list):
            # We've already got this CPL; return false.
            return False
        else:
            # It's a new CPL; register it, and return true.
            cpls[child_pointer_list] = True
            return True

    #////////////////////////////////////////////////////////////
    # Tree extraction & child pointer lists
    #////////////////////////////////////////////////////////////

    def parses(self, root, tree_class=Tree):
        """
        @return: A list of the complete tree structures that span
        the entire chart, and whose root node is C{root}.
        """
        trees = []
        #for edge in self._edges:
        #    if edge._start == 0 and edge._end == self._num_leaves
        #    and str(edge._lhs ==)
        for edge in self.select(span=(0,self._num_leaves), lhs=root):
            trees += self.trees(edge, tree_class=tree_class, complete=True)
        return trees

    def trees(self, edge, tree_class=Tree, complete=False):
        """
        @return: A list of the tree structures that are associated
        with C{edge}.

        If C{edge} is incomplete, then the unexpanded children will be
        encoded as childless subtrees, whose node value is the
        corresponding terminal or nonterminal.
            
        @rtype: C{list} of L{Tree}
        @note: If two trees share a common subtree, then the same
            C{Tree} may be used to encode that subtree in
            both trees.  If you need to eliminate this subtree
            sharing, then create a deep copy of each tree.
        """
        return self._trees(edge, complete, memo={}, tree_class=tree_class)

    def _trees(self, edge, complete, memo, tree_class):
        """
        A helper function for L{trees}.
        @param memo: A dictionary used to record the trees that we've
            generated for each edge, so that when we see an edge more
            than once, we can reuse the same trees.
        """
        # If we've seen this edge before, then reuse our old answer.
        if memo.has_key(edge): return memo[edge]

        trees = []

        # when we're reading trees off the chart, don't use incomplete edges
        if complete and edge.is_incomplete():
            return trees

        # Until we're done computing the trees for edge, set
        # memo[edge] to be empty.  This has the effect of filtering
        # out any cyclic trees (i.e., trees that contain themselves as
        # descendants), because if we reach this edge via a cycle,
        # then it will appear that the edge doesn't generate any
        # trees.
        memo[edge] = []
        
        # Leaf edges.
        if isinstance(edge, LeafEdge):
            leaf = self._tokens[edge.start()]
            memo[edge] = leaf
            return [leaf]
        
        # Each child pointer list can be used to form trees.
        for cpl in self.child_pointer_lists(edge):
            # Get the set of child choices for each child pointer.
            # child_choices[i] is the set of choices for the tree's
            # ith child.
            child_choices = [self._trees(cp, complete, memo, tree_class)
                             for cp in cpl]

            # Kludge to ensure child_choices is a doubly-nested list
            if len(child_choices) > 0 and type(child_choices[0]) == type(""):
                child_choices = [child_choices]

            # For each combination of children, add a tree.
            for children in self._choose_children(child_choices):
                lhs = edge.lhs().symbol()
                trees.append(tree_class(lhs, children))

        # If the edge is incomplete, then extend it with "partial trees":
        if edge.is_incomplete():
            unexpanded = [tree_class(elt,[])
                          for elt in edge.rhs()[edge.dot():]]
            for tree in trees:
                tree.extend(unexpanded)

        # Update the memoization dictionary.
        memo[edge] = trees

        # Return the list of trees.
        return trees

    def _choose_children(self, child_choices):
        """
        A helper function for L{_trees} that finds the possible sets
        of subtrees for a new tree.
        
        @param child_choices: A list that specifies the options for
        each child.  In particular, C{child_choices[i]} is a list of
        tokens and subtrees that can be used as the C{i}th child.
        """
        children_lists = [[]]
        for child_choice in child_choices:
            children_lists = [child_list+[child]
                              for child in child_choice
                              for child_list in children_lists]
        return children_lists
    
    def child_pointer_lists(self, edge):
        """
        @rtype: C{list} of C{list} of C{Edge}
        @return: The set of child pointer lists for the given edge.
            Each child pointer list is a list of edges that have
            been used to form this edge.
        """
        # Make a copy, in case they modify it.
        return self._edge_to_cpls.get(edge, {}).keys()

    #////////////////////////////////////////////////////////////
    # Display
    #////////////////////////////////////////////////////////////
    def pp_edge(self, edge, width=None):
        """
        @return: A pretty-printed string representation of a given edge
            in this chart.
        @rtype: C{string}
        @param width: The number of characters allotted to each
            index in the sentence.
        """
        if width is None: width = 50/(self.num_leaves()+1)
        (start, end) = (edge.start(), edge.end())

        str = '|' + ('.'+' '*(width-1))*start

        # Zero-width edges are "#" if complete, ">" if incomplete
        if start == end:
            if edge.is_complete(): str += '#'
            else: str += '>'

        # Spanning complete edges are "[===]"; Other edges are
        # "[---]" if complete, "[--->" if incomplete
        elif edge.is_complete() and edge.span() == (0,self._num_leaves):
            str += '['+('='*width)*(end-start-1) + '='*(width-1)+']'
        elif edge.is_complete():
            str += '['+('-'*width)*(end-start-1) + '-'*(width-1)+']'
        else:
            str += '['+('-'*width)*(end-start-1) + '-'*(width-1)+'>'
        
        str += (' '*(width-1)+'.')*(self._num_leaves-end)
        return str + '| %s ' % edge

    def pp_leaves(self, width=None):
        """
        @return: A pretty-printed string representation of this
            chart's leaves.  This string can be used as a header
            for calls to L{pp_edge}.
        """
        if width is None: width = 50/(self.num_leaves()+1)
        
        if self._tokens is not None and width>1:
            header = '|.'
            for tok in self._tokens:
                header += tok[:width-1].center(width-1)+'.'
            header += '|'
        else:
            header = ''

        return header

    def pp(self, width=None):
        """
        @return: A pretty-printed string representation of this chart.
        @rtype: C{string}
        @param width: The number of characters allotted to each
            index in the sentence.
        """
        if width is None: width = 50/(self.num_leaves()+1)
        # sort edges: primary key=length, secondary key=start index.
        # (and filter out the token edges)
        edges = [(e.length(), e.start(), e) for e in self]
        edges.sort()
        edges = [e for (_,_,e) in edges]
        
        return (self.pp_leaves(width) + '\n' +
                '\n'.join(self.pp_edge(edge, width) for edge in edges))
                
    #////////////////////////////////////////////////////////////
    # Display: Dot (AT&T Graphviz)
    #////////////////////////////////////////////////////////////

    def dot_digraph(self):
        # Header
        s = 'digraph nltk_chart {\n'
        #s += '  size="5,5";\n'
        s += '  rankdir=LR;\n'
        s += '  node [height=0.1,width=0.1];\n'
        s += '  node [style=filled, color="lightgray"];\n'

        # Set up the nodes
        for y in range(self.num_edges(), -1, -1):
            if y == 0:
                s += '  node [style=filled, color="black"];\n'
            for x in range(self.num_leaves()+1):
                if y == 0 or (x <= self._edges[y-1].start() or
                              x >= self._edges[y-1].end()):
                    s += '  %04d.%04d [label=""];\n' % (x,y)

        # Add a spacer
        s += '  x [style=invis]; x->0000.0000 [style=invis];\n'

        # Declare ranks.
        for x in range(self.num_leaves()+1):
            s += '  {rank=same;'
            for y in range(self.num_edges()+1):
                if y == 0 or (x <= self._edges[y-1].start() or
                              x >= self._edges[y-1].end()):
                    s += ' %04d.%04d' % (x,y)
            s += '}\n'

        # Add the leaves
        s += '  edge [style=invis, weight=100];\n'
        s += '  node [shape=plaintext]\n'
        s += '  0000.0000'
        for x in range(self.num_leaves()):
            s += '->%s->%04d.0000' % (self.leaf(x), x+1)
        s += ';\n\n'

        # Add the edges
        s += '  edge [style=solid, weight=1];\n'
        for y, edge in enumerate(self):
            for x in range(edge.start()):
                s += ('  %04d.%04d -> %04d.%04d [style="invis"];\n' %
                      (x, y+1, x+1, y+1))
            s += ('  %04d.%04d -> %04d.%04d [label="%s"];\n' %
                  (edge.start(), y+1, edge.end(), y+1, edge))
            for x in range(edge.end(), self.num_leaves()):
                s += ('  %04d.%04d -> %04d.%04d [style="invis"];\n' %
                      (x, y+1, x+1, y+1))
        s += '}\n'
        return s

########################################################################
##  Chart Rules
########################################################################

class ChartRuleI(object):
    """
    A rule that specifies what new edges are licensed by any given set
    of existing edges.  Each chart rule expects a fixed number of
    edges, as indicated by the class variable L{NUM_EDGES}.  In
    particular:
    
      - A chart rule with C{NUM_EDGES=0} specifies what new edges are
        licensed, regardless of existing edges.

      - A chart rule with C{NUM_EDGES=1} specifies what new edges are
        licensed by a single existing edge.

      - A chart rule with C{NUM_EDGES=2} specifies what new edges are
        licensed by a pair of existing edges.
      
    @type NUM_EDGES: C{int}
    @cvar NUM_EDGES: The number of existing edges that this rule uses
        to license new edges.  Typically, this number ranges from zero
        to two.
    """
    def apply(self, chart, grammar, *edges):
        """
        Add the edges licensed by this rule and the given edges to the
        chart.

        @type edges: C{list} of L{EdgeI}
        @param edges: A set of existing edges.  The number of edges
            that should be passed to C{apply} is specified by the
            L{NUM_EDGES} class variable.
        @rtype: C{list} of L{EdgeI}
        @return: A list of the edges that were added.
        """
        raise AssertionError, 'ChartRuleI is an abstract interface'

    def apply_iter(self, chart, grammar, *edges):
        """
        @return: A generator that will add edges licensed by this rule
            and the given edges to the chart, one at a time.  Each
            time the generator is resumed, it will either add a new
            edge and yield that edge; or return.
        @rtype: C{iter} of L{EdgeI}
        
        @type edges: C{list} of L{EdgeI}
        @param edges: A set of existing edges.  The number of edges
            that should be passed to C{apply} is specified by the
            L{NUM_EDGES} class variable.
        """
        raise AssertionError, 'ChartRuleI is an abstract interface'

    def apply_everywhere(self, chart, grammar):
        """
        Add all the edges licensed by this rule and the edges in the
        chart to the chart.
        
        @rtype: C{list} of L{EdgeI}
        @return: A list of the edges that were added.
        """
        raise AssertionError, 'ChartRuleI is an abstract interface'

    def apply_everywhere_iter(self, chart, grammar):
        """
        @return: A generator that will add all edges licensed by
            this rule, given the edges that are currently in the
            chart, one at a time.  Each time the generator is resumed,
            it will either add a new edge and yield that edge; or
            return.
        @rtype: C{iter} of L{EdgeI}
        """
        raise AssertionError, 'ChartRuleI is an abstract interface'
        
class AbstractChartRule(object):
    """
    An abstract base class for chart rules.  C{AbstractChartRule}
    provides:
      - A default implementation for C{apply}, based on C{apply_iter}.
      - A default implementation for C{apply_everywhere_iter},
        based on C{apply_iter}.
      - A default implementation for C{apply_everywhere}, based on
        C{apply_everywhere_iter}.  Currently, this implementation
        assumes that C{NUM_EDGES}<=3.
      - A default implementation for C{__str__}, which returns a
        name basd on the rule's class name.
    """

    # Subclasses must define apply_iter.
    def apply_iter(self, chart, grammar, *edges):
        raise AssertionError, 'AbstractChartRule is an abstract class'

    # Default: loop through the given number of edges, and call
    # self.apply() for each set of edges.
    def apply_everywhere_iter(self, chart, grammar):
        if self.NUM_EDGES == 0:
            for new_edge in self.apply_iter(chart, grammar):
                yield new_edge

        elif self.NUM_EDGES == 1:
            for e1 in chart:
                for new_edge in self.apply_iter(chart, grammar, e1):
                    yield new_edge

        elif self.NUM_EDGES == 2:
            for e1 in chart:
                for e2 in chart:
                    for new_edge in self.apply_iter(chart, grammar, e1, e2):
                        yield new_edge

        elif self.NUM_EDGES == 3:
            for e1 in chart:
                for e2 in chart:
                    for e3 in chart:
                        for new_edge in self.apply_iter(chart,grammar,e1,e2,e3):
                            yield new_edge

        else:
            raise AssertionError, 'NUM_EDGES>3 is not currently supported'

    # Default: delegate to apply_iter.
    def apply(self, chart, grammar, *edges):
        return list(self.apply_iter(chart, grammar, *edges))

    # Default: delegate to apply_everywhere_iter.
    def apply_everywhere(self, chart, grammar):
        return list(self.apply_everywhere_iter(chart, grammar))

    # Default: return a name based on the class name.
    def __str__(self):
        # Add spaces between InitialCapsWords.
        return re.sub('([a-z])([A-Z])', r'\1 \2', self.__class__.__name__)

#////////////////////////////////////////////////////////////
# Fundamental Rule
#////////////////////////////////////////////////////////////
class FundamentalRule(AbstractChartRule):
    """
    A rule that joins two adjacent edges to form a single combined
    edge.  In particular, this rule specifies that any pair of edges:
    
        - [AS{->}S{alpha}*BS{beta}][i:j]
        - [BS{->}S{gamma}*][j:k]
    licenses the edge:
        - [AS{->}S{alpha}B*S{beta}][i:j]
    """
    NUM_EDGES = 2
    def apply_iter(self, chart, grammar, left_edge, right_edge):
        # Make sure the rule is applicable.
        if not (left_edge.end() == right_edge.start() and
                left_edge.next() == right_edge.lhs() and
                left_edge.is_incomplete() and right_edge.is_complete()):
            return

        # Construct the new edge.
        new_edge = TreeEdge(span=(left_edge.start(), right_edge.end()),
                            lhs=left_edge.lhs(), rhs=left_edge.rhs(),
                            dot=left_edge.dot()+1)

        # Add it to the chart, with appropriate child pointers.
        changed_chart = False
        for cpl1 in chart.child_pointer_lists(left_edge):
            if chart.insert(new_edge, cpl1+(right_edge,)):
                changed_chart = True

        # If we changed the chart, then generate the edge.
        if changed_chart: yield new_edge

class SingleEdgeFundamentalRule(AbstractChartRule):
    """
    A rule that joins a given edge with adjacent edges in the chart,
    to form combined edges.  In particular, this rule specifies that
    either of the edges:
        - [AS{->}S{alpha}*BS{beta}][i:j]
        - [BS{->}S{gamma}*][j:k]
    licenses the edge:
        - [AS{->}S{alpha}B*S{beta}][i:j]
    if the other edge is already in the chart.
    @note: This is basically L{FundamentalRule}, with one edge is left
        unspecified.
    """
    NUM_EDGES = 1

    _fundamental_rule = FundamentalRule()
    
    def apply_iter(self, chart, grammar, edge1):
        fr = self._fundamental_rule
        if edge1.is_incomplete():
            # edge1 = left_edge; edge2 = right_edge
            for edge2 in chart.select(start=edge1.end(), is_complete=True,
                                     lhs=edge1.next()):
                for new_edge in fr.apply_iter(chart, grammar, edge1, edge2):
                    yield new_edge
        else:
            # edge2 = left_edge; edge1 = right_edge
            for edge2 in chart.select(end=edge1.start(), is_complete=False,
                                     next=edge1.lhs()):
                for new_edge in fr.apply_iter(chart, grammar, edge2, edge1):
                    yield new_edge

    def __str__(self): return 'Fundamental Rule'
    
class BottomUpInitRule(AbstractChartRule):
    """
    A rule licensing any edges corresponding to terminals in the
    text.  In particular, this rule licenses the leaf edge:
        - [wS{->}*][i:i+1]
    for C{w} is a word in the text, where C{i} is C{w}'s index.
    """
    NUM_EDGES = 0
    def apply_iter(self, chart, grammar):
        for index in range(chart.num_leaves()):
            new_edge = LeafEdge(chart.leaf(index), index)
            if chart.insert(new_edge, ()):
                yield new_edge

#////////////////////////////////////////////////////////////
# Top-Down Parsing
#////////////////////////////////////////////////////////////
    

class TopDownInitRule(AbstractChartRule):
    """
    A rule licensing edges corresponding to the grammar productions for
    the grammar's start symbol.  In particular, this rule specifies that:
        - [SS{->}*S{alpha}][0:i]
    is licensed for each grammar production C{SS{->}S{alpha}}, where
    C{S} is the grammar's start symbol.
    """
    NUM_EDGES = 0
    def apply_iter(self, chart, grammar):
        for prod in grammar.productions(lhs=grammar.start()):
            new_edge = TreeEdge.from_production(prod, 0)
            if chart.insert(new_edge, ()):
                yield new_edge

class TopDownExpandRule(AbstractChartRule):
    """
    A rule licensing edges corresponding to the grammar productions
    for the nonterminal following an incomplete edge's dot.  In
    particular, this rule specifies that:
        - [AS{->}S{alpha}*BS{beta}][i:j]
    licenses the edge:
        - [BS{->}*S{gamma}][j:j]
    for each grammar production C{BS{->}S{gamma}}.
    """
    NUM_EDGES = 1
    def apply_iter(self, chart, grammar, edge):
        if edge.is_complete(): return
        for prod in grammar.productions(lhs=edge.next()):
            new_edge = TreeEdge.from_production(prod, edge.end())
            if chart.insert(new_edge, ()):
                yield new_edge

class TopDownMatchRule(AbstractChartRule):
    """
    A rule licensing an edge corresponding to a terminal following an
    incomplete edge's dot.  In particular, this rule specifies that:
        - [AS{->}S{alpha}*w{beta}][i:j]
    licenses the leaf edge:
        - [wS{->}*][j:j+1]
    if the C{j}th word in the text is C{w}.
    """
    NUM_EDGES = 1
    def apply_iter(self, chart, grammar, edge):
        if edge.is_complete() or edge.end() >= chart.num_leaves(): return
        index = edge.end()
        leaf = chart.leaf(index)
        if edge.next() == leaf:
            new_edge = LeafEdge(leaf, index)
            if chart.insert(new_edge, ()):
                yield new_edge

# Add a cache, to prevent recalculating.
class CachedTopDownInitRule(TopDownInitRule):
    """
    A cached version of L{TopDownInitRule}.  After the first time this
    rule is applied, it will not generate any more edges.

    If C{chart} or C{grammar} are changed, then the cache is flushed.
    """
    def __init__(self):
        AbstractChartRule.__init__(self)
        self._done = (None, None)

    def apply_iter(self, chart, grammar):
        # If we've already applied this rule, and the chart & grammar
        # have not changed, then just return (no new edges to add).
        if self._done[0] is chart and self._done[1] is grammar: return

        # Add all the edges indicated by the top down init rule.
        for e in TopDownInitRule.apply_iter(self, chart, grammar):
            yield e

        # Record the fact that we've applied this rule.
        self._done = (chart, grammar)

    def __str__(self): return 'Top Down Init Rule'
    
class CachedTopDownExpandRule(TopDownExpandRule):
    """
    A cached version of L{TopDownExpandRule}.  After the first time
    this rule is applied to an edge with a given C{end} and C{next},
    it will not generate any more edges for edges with that C{end} and
    C{next}.
    
    If C{chart} or C{grammar} are changed, then the cache is flushed.
    """
    def __init__(self):
        AbstractChartRule.__init__(self)
        self._done = {}
        
    def apply_iter(self, chart, grammar, edge):
        # If we've already applied this rule to an edge with the same
        # next & end, and the chart & grammar have not changed, then
        # just return (no new edges to add).
        done = self._done.get((edge.next(), edge.end()), (None,None))
        if done[0] is chart and done[1] is grammar: return

        # Add all the edges indicated by the top down expand rule.
        for e in TopDownExpandRule.apply_iter(self, chart, grammar, edge):
            yield e
            
        # Record the fact that we've applied this rule.
        self._done[edge.next(), edge.end()] = (chart, grammar)
    
    def __str__(self): return 'Top Down Expand Rule'

#////////////////////////////////////////////////////////////
# Bottom-Up Parsing
#////////////////////////////////////////////////////////////

class BottomUpInitRule(AbstractChartRule):
    """
    A rule licensing any edges corresponding to terminals in the
    text.  In particular, this rule licenses the leaf edge:
        - [wS{->}*][i:i+1]
    for C{w} is a word in the text, where C{i} is C{w}'s index.
    """
    NUM_EDGES = 0
    def apply_iter(self, chart, grammar):
        for index in range(chart.num_leaves()):
            new_edge = LeafEdge(chart.leaf(index), index)
            if chart.insert(new_edge, ()):
                yield new_edge

class BottomUpPredictRule(AbstractChartRule):
    """
    A rule licensing any edge corresponding to a production whose
    right-hand side begins with a complete edge's left-hand side.  In
    particular, this rule specifies that:
        - [AS{->}S{alpha}*]
    licenses the edge:
        - [BS{->}*AS{beta}]
    for each grammar production C{BS{->}AS{beta}}
    """
    NUM_EDGES = 1
    def apply_iter(self, chart, grammar, edge):
        if edge.is_incomplete(): return
        for prod in grammar.productions(rhs=edge.lhs()):
            new_edge = TreeEdge.from_production(prod, edge.start())
            if chart.insert(new_edge, ()):
                yield new_edge

#////////////////////////////////////////////////////////////
# Earley Parsing
#////////////////////////////////////////////////////////////

class CompleterRule(AbstractChartRule):
    """
    A rule that joins a given complete edge with adjacent incomplete
    edges in the chart, to form combined edges.  In particular, this
    rule specifies that:
        - [BS{->}S{gamma}*][j:k]
    licenses the edge:
        - [AS{->}S{alpha}B*S{beta}][i:j]
    given that the chart contains:
        - [AS{->}S{alpha}*BS{beta}][i:j]
    @note: This is basically L{FundamentalRule}, with the left edge
        left unspecified.
    """
    NUM_EDGES = 1
    
    _fundamental_rule = FundamentalRule()
    
    def apply_iter(self, chart, grammar, right_edge):
        if right_edge.is_incomplete(): return
        fr = self._fundamental_rule
        for left_edge in chart.select(end=right_edge.start(),
                                     is_complete=False,
                                     next=right_edge.lhs()):
            for e in fr.apply_iter(chart, grammar, left_edge, right_edge):
                yield e

    def __str__(self): return 'Completer Rule'
    
class ScannerRule(AbstractChartRule):
    """
    A rule licensing a leaf edge corresponding to a part-of-speech
    terminal following an incomplete edge's dot.  In particular, this
    rule specifies that:
        - [AS{->}S{alpha}*PS{beta}][i:j]
    licenses the edges:
        - [PS{->}w*][j:j+1]
        - [wS{->}*][j:j+1]
    if the C{j}th word in the text is C{w}; and C{P} is a valid part
    of speech for C{w}.
    """
    NUM_EDGES = 1
    def __init__(self, word_to_pos_lexicon):
        self._word_to_pos = word_to_pos_lexicon

    def apply_iter(self, chart, gramar, edge):
        if edge.is_complete() or edge.end()>=chart.num_leaves(): return
        index = edge.end()
        leaf = chart.leaf(index)
        if edge.next() in self._word_to_pos.get(leaf, []):
            new_leaf_edge = LeafEdge(leaf, index)
            if chart.insert(new_leaf_edge, ()):
                yield new_leaf_edge
            new_pos_edge = TreeEdge((index,index+1), edge.next(),
                                    [leaf], 1)
            if chart.insert(new_pos_edge, (new_leaf_edge,)):
                yield new_pos_edge

# This is just another name for TopDownExpandRule:
class PredictorRule(TopDownExpandRule): pass

########################################################################
##  Simple Earley Chart Parser
########################################################################

class EarleyChartParse(AbstractParse):
    """
    A chart parser implementing the Earley parsing algorithm:

        - For each index I{end} in [0, 1, ..., N]:
          - For each I{edge} s.t. I{edge}.end = I{end}:
            - If I{edge} is incomplete, and I{edge}.next is not a part
              of speech:
                - Apply PredictorRule to I{edge}
            - If I{edge} is incomplete, and I{edge}.next is a part of
              speech:
                - Apply ScannerRule to I{edge}
            - If I{edge} is complete:
                - Apply CompleterRule to I{edge}
        - Return any complete parses in the chart

    C{EarleyChartParse} uses a X{lexicon} to decide whether a leaf
    has a given part of speech.  This lexicon is encoded as a
    dictionary that maps each word to a list of parts of speech that
    word can have.
    """
    def __init__(self, grammar, lexicon, trace=0):
        """
        Create a new Earley chart parser, that uses C{grammar} to
        parse texts.
        
        @type grammar: C{cfg.Grammar}
        @param grammar: The grammar used to parse texts.
        @type lexicon: C{dict} from C{string} to (C{list} of C{string})
        @param lexicon: A lexicon of words that records the parts of
            speech that each word can have.  Each key is a word, and
            the corresponding value is a list of parts of speech.
        @type trace: C{int}
        @param trace: The level of tracing that should be used when
            parsing a text.  C{0} will generate no tracing output;
            and higher numbers will produce more verbose tracing
            output.
        """
        self._grammar = grammar
        self._lexicon = lexicon
        self._trace = trace
        AbstractParse.__init__(self)

    def get_parse_list(self, tokens, tree_class=Tree):
        chart = Chart(list(tokens))
        grammar = self._grammar

        # Width, for printing trace edges.
        w = 50/(chart.num_leaves()+1)
        if self._trace > 0: print ' ', chart.pp_leaves(w)

        # Initialize the chart with a special "starter" edge.
        root = cfg.Nonterminal('[INIT]')
        edge = TreeEdge((0,0), root, (grammar.start(),))
        chart.insert(edge, ())

        # Create the 3 rules:
        predictor = PredictorRule()
        completer = CompleterRule()
        scanner = ScannerRule(self._lexicon)

        for end in range(chart.num_leaves()+1):
            if self._trace > 1: print 'Processing queue %d' % end
            for edge in chart.select(end=end):
                if edge.is_incomplete():
                    for e in predictor.apply(chart, grammar, edge):
                        if self._trace > 0:
                            print 'Predictor', chart.pp_edge(e,w)
                if edge.is_incomplete():
                    for e in scanner.apply(chart, grammar, edge):
                        if self._trace > 0:
                            print 'Scanner  ', chart.pp_edge(e,w)
                if edge.is_complete():
                    for e in completer.apply(chart, grammar, edge):
                        if self._trace > 0:
                            print 'Completer', chart.pp_edge(e,w)

        # Output a list of complete parses.
        return chart.parses(grammar.start(), tree_class=tree_class)
            
########################################################################
##  Generic Chart Parser
########################################################################

TD_STRATEGY = [CachedTopDownInitRule(), CachedTopDownExpandRule(), 
               TopDownMatchRule(), SingleEdgeFundamentalRule()]
BU_STRATEGY = [BottomUpInitRule(), BottomUpPredictRule(),
               SingleEdgeFundamentalRule()]

class ChartParse(AbstractParse):
    """
    A generic chart parser.  A X{strategy}, or list of
    L{ChartRules<ChartRuleI>}, is used to decide what edges to add to
    the chart.  In particular, C{ChartParse} uses the following
    algorithm to parse texts:

        - Until no new edges are added:
          - For each I{rule} in I{strategy}:
            - Apply I{rule} to any applicable edges in the chart.
        - Return any complete parses in the chart
    """
    def __init__(self, grammar, strategy, trace=0):
        """
        Create a new chart parser, that uses C{grammar} to parse
        texts.

        @type grammar: L{cfg.Grammar}
        @param grammar: The grammar used to parse texts.
        @type strategy: C{list} of L{ChartRuleI}
        @param strategy: A list of rules that should be used to decide
            what edges to add to the chart.
        @type trace: C{int}
        @param trace: The level of tracing that should be used when
            parsing a text.  C{0} will generate no tracing output;
            and higher numbers will produce more verbose tracing
            output.
        """
        self._grammar = grammar
        self._strategy = strategy
        self._trace = trace
        AbstractParse.__init__(self)

    def get_parse_list(self, tokens, tree_class=Tree):
        chart = Chart(list(tokens))
        grammar = self._grammar

        # Width, for printing trace edges.
        w = 50/(chart.num_leaves()+1)
        if self._trace > 0: print chart.pp_leaves(w)
        
        edges_added = 1
        while edges_added > 0:
            edges_added = 0
            for rule in self._strategy:
                edges_added_by_rule = 0
                for e in rule.apply_everywhere(chart, grammar):
                    if self._trace > 0 and edges_added_by_rule == 0:
                        print '%s:' % rule
                    edges_added_by_rule += 1
                    if self._trace > 1: print chart.pp_edge(e,w)
                if self._trace == 1 and edges_added_by_rule > 0:
                    print '  - Added %d edges' % edges_added_by_rule
                edges_added += edges_added_by_rule
        
        # Return a list of complete parses.
        return chart.parses(grammar.start(), tree_class=tree_class)

########################################################################
##  Stepping Chart Parser
########################################################################

class SteppingChartParse(ChartParse):
    """
    A C{ChartParse} that allows you to step through the parsing
    process, adding a single edge at a time.  It also allows you to
    change the parser's strategy or grammar midway through parsing a
    text.

    The C{initialize} method is used to start parsing a text.  C{step}
    adds a single edge to the chart.  C{set_strategy} changes the
    strategy used by the chart parser.  C{parses} returns the set of
    parses that has been found by the chart parser.

    @ivar _restart: Records whether the parser's strategy, grammar,
        or chart has been changed.  If so, then L{step} must restart
        the parsing algorithm.
    """
    def __init__(self, grammar, strategy=None, trace=0):
        self._chart = None
        self._current_chartrule = None
        self._restart = False
        ChartParse.__init__(self, grammar, strategy, trace)

    #////////////////////////////////////////////////////////////
    # Initialization
    #////////////////////////////////////////////////////////////

    def initialize(self, tokens):
        "Begin parsing the given tokens."
        self._chart = Chart(list(tokens))
        self._restart = True

    #////////////////////////////////////////////////////////////
    # Stepping
    #////////////////////////////////////////////////////////////

    def step(self):
        """
        @return: A generator that adds edges to the chart, one at a
        time.  Each time the generator is resumed, it adds a single
        edge and yields that edge.  If no more edges can be added,
        then it yields C{None}.

        If the parser's strategy, grammar, or chart is changed, then
        the generator will continue adding edges using the new
        strategy, grammar, or chart.

        Note that this generator never terminates, since the grammar
        or strategy might be changed to values that would add new
        edges.  Instead, it yields C{None} when no more edges can be
        added with the current strategy and grammar.
        """
        if self._chart is None:
            raise ValueError, 'Parser must be initialized first'
        while 1:
            self._restart = False
            w = 50/(self._chart.num_leaves()+1)
            
            for e in self._parse():
                if self._trace > 1: print self._current_chartrule
                if self._trace > 0: print self._chart.pp_edge(e,w)
                yield e
                if self._restart: break
            else:
                yield None # No more edges.

    def _parse(self):
        """
        A generator that implements the actual parsing algorithm.
        L{step} iterates through this generator, and restarts it
        whenever the parser's strategy, grammar, or chart is modified.
        """
        chart = self._chart
        grammar = self._grammar
        edges_added = 1
        while edges_added > 0:
            edges_added = 0
            for rule in self._strategy:
                self._current_chartrule = rule
                for e in rule.apply_everywhere_iter(chart, grammar):
                    edges_added += 1
                    yield e

    #////////////////////////////////////////////////////////////
    # Accessors
    #////////////////////////////////////////////////////////////

    def strategy(self):
        "@return: The strategy used by this parser."
        return self._strategy

    def grammar(self):
        "@return: The grammar used by this parser."
        return self._grammar

    def chart(self):
        "@return: The chart that is used by this parser."
        return self._chart

    def current_chartrule(self):
        "@return: The chart rule used to generate the most recent edge."
        return self._current_chartrule

    def parses(self, tree_class=Tree):
        "@return: The parse trees currently contained in the chart."
        return self._chart.parses(self._grammar.start(), tree_class)
    
    #////////////////////////////////////////////////////////////
    # Parser modification
    #////////////////////////////////////////////////////////////

    def set_strategy(self, strategy):
        """
        Change the startegy that the parser uses to decide which edges
        to add to the chart.
        @type strategy: C{list} of L{ChartRuleI}
        @param strategy: A list of rules that should be used to decide
            what edges to add to the chart.
        """
        if strategy == self._strategy: return
        self._strategy = strategy[:] # Make a copy.
        self._restart = True

    def set_grammar(self, grammar):
        "Change the grammar used by the parser."
        if grammar is self._grammar: return
        self._grammar = grammar
        self._restart = True

    def set_chart(self, chart):
        "Load a given chart into the chart parser."
        if chart is self._chart: return
        self._chart = chart
        self._restart = True

    #////////////////////////////////////////////////////////////
    # Standard parser methods
    #////////////////////////////////////////////////////////////

    def get_parse_list(self, token, tree_class=Tree):
        # Initialize ourselves.
        self.initialize(token)

        # Step until no more edges are generated.
        for e in self.step():
            if e is None: break
            
        # Return a list of complete parses.
        return self.parses(tree_class=tree_class)

########################################################################
##  Demo Code
########################################################################

def demo():
    """
    A demonstration of the chart parsers.
    """
    import sys, time
    
    # Define some nonterminals
    S, VP, NP, PP = cfg.nonterminals('S, VP, NP, PP')
    V, N, P, Name, Det = cfg.nonterminals('V, N, P, Name, Det')

    # Define some grammatical productions.
    grammatical_productions = [
        cfg.Production(S, [NP, VP]),  cfg.Production(PP, [P, NP]),
        cfg.Production(NP, [Det, N]), cfg.Production(NP, [NP, PP]),
        cfg.Production(VP, [VP, PP]), cfg.Production(VP, [V, NP]),
        cfg.Production(VP, [V]),]

    # Define some lexical productions.
    lexical_productions = [
        cfg.Production(NP, ['John']), cfg.Production(NP, ['I']), 
        cfg.Production(Det, ['the']), cfg.Production(Det, ['my']),
        cfg.Production(Det, ['a']),
        cfg.Production(N, ['dog']),   cfg.Production(N, ['cookie']),
        cfg.Production(V, ['ate']),  cfg.Production(V, ['saw']),
        cfg.Production(P, ['with']), cfg.Production(P, ['under']),
        ]

    # Convert the grammar productions to an earley-style lexicon.
    earley_lexicon = {}
    for prod in lexical_productions:
        earley_lexicon.setdefault(prod.rhs()[0], []).append(prod.lhs())

    # The grammar for ChartParse and SteppingChartParse:
    grammar = cfg.Grammar(S, grammatical_productions+lexical_productions)

    # The grammar for EarleyChartParse:
    earley_grammar = cfg.Grammar(S, grammatical_productions)

    # Tokenize a sample sentence.
    sent = 'I saw John with a dog with my cookie'
    print "Sentence:\n", sent
    from nltk import tokenize
    tokens = list(tokenize.whitespace(sent))

    print tokens

    # Ask the user which parser to test
    print '  1: Top-down chart parser'
    print '  2: Bottom-up chart parser'
    print '  3: Earley parser'
    print '  4: Stepping chart parser (alternating top-down & bottom-up)'
    print '  5: All parsers'
    print '\nWhich parser (1-5)? ',
    choice = sys.stdin.readline().strip()
    print
    if choice not in '12345':
        print 'Bad parser number'
        return

    # Keep track of how long each parser takes.
    times = {}

    # Run the top-down parser, if requested.
    if choice in ('1', '5'):
        cp = ChartParse(grammar, TD_STRATEGY, trace=2)
        t = time.time()
        parses = cp.get_parse_list(tokens)
        times['top down'] = time.time()-t
        assert len(parses)==5, 'Not all parses found'
        for tree in parses: print tree

    # Run the bottom-up parser, if requested.
    if choice in ('2', '5'):
        cp = ChartParse(grammar, BU_STRATEGY, trace=2)
        t = time.time()
        parses = cp.get_parse_list(tokens)
        times['bottom up'] = time.time()-t
        assert len(parses)==5, 'Not all parses found'
        for tree in parses: print tree

    # Run the earley, if requested.
    if choice in ('3', '5'):
        cp = EarleyChartParse(earley_grammar, earley_lexicon, trace=1)
        t = time.time()
        parses = cp.get_parse_list(tokens)
        times['Earley parser'] = time.time()-t
        assert len(parses)==5, 'Not all parses found'
        for tree in parses: print tree

    # Run the stepping parser, if requested.
    if choice in ('4', '5'):
        t = time.time()
        cp = SteppingChartParse(grammar, trace=1)
        cp.initialize(tokens)
        for i in range(5):
            print '*** SWITCH TO TOP DOWN'
            cp.set_strategy(TD_STRATEGY)
            for j, e in enumerate(cp.step()):
                if j>20 or e is None: break
            print '*** SWITCH TO BOTTOM UP'
            cp.set_strategy(BU_STRATEGY)
            for j, e in enumerate(cp.step()):
                if j>20 or e is None: break
        times['stepping'] = time.time()-t
        assert len(cp.parses())==5, 'Not all parses found'
        for parse in cp.parses(): print parse

    # Print the times of all parsers:
    maxlen = max(len(key) for key in times.keys())
    format = '%' + `maxlen` + 's parser: %6.3fsec'
    times_items = times.items()
    times_items.sort(lambda a,b:cmp(a[1], b[1]))
    for (parser, t) in times_items:
        print format % (parser, t)
            
if __name__ == '__main__': demo()

########NEW FILE########
__FILENAME__ = featurechart
# Natural Language Toolkit: Chart Parser for Feature-Based Grammars
#
# Copyright (C) 2001-2007 NLTK Project
# Author: Rob Speer <rspeer@mit.edu>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT
#
# $Id: featurechart.py 4107 2007-02-01 00:07:42Z stevenbird $

"""
Extension of chart parsing implementation to handle grammars with
feature structures as nodes.
"""

import yaml
from chart import *
from category import *
import cfg

from featurelite import *

def load_earley(filename, trace=1):
    """
    Load a grammar from a file, and build an Earley feature parser based on
    that grammar.

    You can optionally specify a tracing level, for how much output you
    want to see:

    0: No output.
    1: Show edges from scanner and completer rules (not predictor).
    2 (default): Show all edges as they are added to the chart.
    3: Show all edges, plus the results of successful unifications.
    4: Show all edges, plus the results of all attempted unifications.
    5: Show all edges, plus the results of all attempted unifications,
       including those with cached results.
    """

    grammar = GrammarFile.read_file(filename)
    return grammar.earley_parser(trace)

class FeatureTreeEdge(TreeEdge):
    """
    FIXME: out of date documentation
    
    A modification of L{TreeEdge} to handle nonterminals with features
    (known as L{Categories<Category>}.

    In addition to the span, left-hand side, right-hand side, and dot position
    (described at L{TreeEdge}), a C{FeatureTreeEdge} includes X{vars}, a
    set of L{FeatureBindings} saying which L{FeatureVariable}s are set to which
    values.

    These values are applied when examining the C{lhs} or C{rhs} of a
    C{FeatureTreeEdge}.
    
    For more information about edges, see the L{EdgeI} interface.
    """
    def __init__(self, span, lhs, rhs, dot=0, vars=None):
        """
        Construct a new C{FeatureTreeEdge}.
        
        @type span: C{(int, int)}
        @param span: A tuple C{(s,e)}, where C{subtokens[s:e]} is the
            portion of the sentence that is consistent with the new
            edge's structure.
        @type lhs: L{Category}
        @param lhs: The new edge's left-hand side, specifying the
            hypothesized tree's node value.
        @type rhs: C{list} of (L{Category} and C{string})
        @param rhs: The new edge's right-hand side, specifying the
            hypothesized tree's children.
        @type dot: C{int}
        @param dot: The position of the new edge's dot.  This position
            specifies what prefix of the production's right hand side
            is consistent with the text.  In particular, if
            C{sentence} is the list of subtokens in the sentence, then
            C{subtokens[span[0]:span[1]]} can be spanned by the
            children specified by C{rhs[:dot]}.
        @type vars: L{FeatureBindings}
        @param vars: The bindings specifying what values certain variables in
            this edge must have.
        """
        TreeEdge.__init__(self, span, lhs, rhs, dot)
        if vars is None: vars = {}
        self._vars = vars

    @staticmethod
    def from_production(production, index, bindings=None):
        """
        @return: A new C{FeatureTreeEdge} formed from the given production.
            The new edge's left-hand side and right-hand side will
            be taken from C{production}; its span will be C{(index,
            index)}; its dot position will be C{0}, and it may have specified
            variables set.
        @rtype: L{FeatureTreeEdge}
        """
        return FeatureTreeEdge(span=(index, index), lhs=production.lhs(),
                               rhs=production.rhs(), dot=0, vars=bindings)
    
    # Accessors
    def vars(self):
        """
        @return: the L{VariableBindings} mapping L{FeatureVariable}s to values.
        @rtype: L{VariableBindings}
        """
        return self._vars
        
    def lhs(self):
        """
        @return: the value of the left-hand side with variables set.
        @rtype: C{Category}
        """
        return substitute_bindings(TreeEdge.lhs(self), self._vars)
    
    def orig_lhs(self):
        """
        @return: the value of the left-hand side with no variables set.
        @rtype: C{Category}
        """
        return TreeEdge.lhs(self)
    
    def rhs(self):
        """
        @return: the value of the right-hand side with variables set.
        @rtype: C{Category}
        """
        return tuple(apply(x, self._vars) for x in TreeEdge.rhs(self))
    
    def orig_rhs(self):
        """
        @return: the value of the right-hand side with no variables set.
        @rtype: C{Category}
        """
        return TreeEdge.rhs(self)

    # String representation
    def __str__(self):
        str = '%s ->' % self.lhs()

        for i in range(len(self._rhs)):
            if i == self._dot: str += ' *'
            str += ' %s' % (self.rhs()[i],)
        if len(self._rhs) == self._dot: str += ' *'
        return '%s %s' % (str, self._vars)

class FeatureFundamentalRule(FundamentalRule):
    def __init__(self, trace=0):
        FundamentalRule.__init__(self)
        self.trace = trace
        self.unify_memo = {}
    def apply_iter(self, chart, grammar, left_edge, right_edge):
        # Make sure the rule is applicable.
        if not (left_edge.end() == right_edge.start() and
                left_edge.is_incomplete() and right_edge.is_complete() and
                isinstance(left_edge, FeatureTreeEdge) and
                isinstance(right_edge, FeatureTreeEdge)
               ):
            return
        left_bindings = left_edge.vars().copy()
        right_bindings = right_edge.vars().copy()
        try:
            unified = unify(left_edge.next(), right_edge.lhs(), left_bindings,
            right_bindings, memo=self.unify_memo, trace=self.trace-2)
            if isinstance(unified, Category): unified.freeze()
        except UnificationFailure: return

        # Construct the new edge.
        new_edge = FeatureTreeEdge(span=(left_edge.start(), right_edge.end()),
                            lhs=left_edge.lhs(), rhs=left_edge.rhs(),
                            dot=left_edge.dot()+1, vars=left_bindings)
        
        # Add it to the chart, with appropraite child pointers.
        changed_chart = False
        for cpl1 in chart.child_pointer_lists(left_edge):
            if chart.insert(new_edge, cpl1+(right_edge,)):
                changed_chart = True
                assert chart.child_pointer_lists(new_edge), new_edge

        # If we changed the chart, then generate the edge.
        if changed_chart:
            yield new_edge

class SingleEdgeFeatureFundamentalRule(SingleEdgeFundamentalRule):
    def __init__(self, trace=0):
        self.trace = trace
        self._fundamental_rule = FeatureFundamentalRule(trace)
    
    def apply_iter(self, chart, grammar, edge1):
        fr = self._fundamental_rule
        if edge1.is_incomplete():
            # edge1 =   left_edge; edge2 = right_edge
            for edge2 in chart.select(start=edge1.end(), is_complete=True):
                for new_edge in fr.apply_iter(chart, grammar, edge1, edge2):
                    yield new_edge
        else:
            # edge2 = left_edge; edge1 = right_edge
            for edge2 in chart.select(end=edge1.start(), is_complete=False):
                for new_edge in fr.apply_iter(chart, grammar, edge2, edge1):
                    yield new_edge
        

class FeatureTopDownExpandRule(TopDownExpandRule):
    """
    The @C{TopDownExpandRule} specialised for feature-based grammars.
    """
    def __init__(self, trace=0):
        TopDownExpandRule.__init__(self)
        self.unify_memo = {}
        self.trace = trace
    def apply_iter(self, chart, grammar, edge):
        if edge.is_complete(): return
        for prod in grammar.productions():
            bindings = edge.vars().copy()
            try:
                unified = unify(edge.next(), prod.lhs(), bindings, {},
                memo=self.unify_memo, trace=self.trace-2)
                if isinstance(unified, Category): unified.freeze()
            except UnificationFailure:
                continue
            new_edge = FeatureTreeEdge.from_production(prod, edge.end())
            if chart.insert(new_edge, ()):
                yield new_edge

class FeatureEarleyChartParse(EarleyChartParse):
    """
    A chart parser implementing the Earley parsing algorithm, allowing
    nonterminals that have features (known as L{Categories<Category>}).

        - For each index I{end} in [0, 1, ..., N]:
          - For each I{edge} s.t. I{edge}.end = I{end}:
            - If I{edge} is incomplete, and I{edge}.next is not a part
              of speech:
                - Apply PredictorRule to I{edge}
            - If I{edge} is incomplete, and I{edge}.next is a part of
              speech:
                - Apply ScannerRule to I{edge}
            - If I{edge} is complete:
                - Apply CompleterRule to I{edge}
        - Return any complete parses in the chart

    C{FeatureEarleyChartParse} uses a X{lexicon} to decide whether a leaf
    has a given part of speech.  This lexicon is encoded as a
    dictionary that maps each word to a list of parts of speech that
    word can have. Unlike in the L{EarleyChartParse}, this lexicon is
    case-insensitive.
    """
    def __init__(self, grammar, lexicon, trace=0):
        # Build a case-insensitive lexicon.
        #ci_lexicon = dict((k.upper(), v) for k, v in lexicon.iteritems())
        # Call the super constructor.
        EarleyChartParse.__init__(self, grammar, lexicon, trace)
        
    def get_parse_list(self, tokens):
        chart = Chart(tokens)
        grammar = self._grammar

        # Width, for printing trace edges.
        #w = 40/(chart.num_leaves()+1)
        w = 2
        if self._trace > 0: print ' '*9, chart.pp_leaves(w)

        # Initialize the chart with a special "starter" edge.
        root = GrammarCategory(pos='[INIT]')
        edge = FeatureTreeEdge((0,0), root, (grammar.start(),), 0,
                {})
        chart.insert(edge, ())

        # Create the 3 rules:
        predictor = FeatureTopDownExpandRule(self._trace)
        completer = SingleEdgeFeatureFundamentalRule(self._trace)
        #scanner = FeatureScannerRule(self._lexicon)

        for end in range(chart.num_leaves()+1):
            if self._trace > 1: print 'Processing queue %d' % end
            
            # Scanner rule substitute, i.e. this is being used in place
            # of a proper FeatureScannerRule at the moment.
            if end > 0 and end-1 < chart.num_leaves():
                leaf = chart.leaf(end-1)
                for pos in self._lexicon(leaf):
                    new_leaf_edge = LeafEdge(leaf, end-1)
                    chart.insert(new_leaf_edge, ())
                    new_pos_edge = FeatureTreeEdge((end-1, end), pos, [leaf], 1,
                        {})
                    chart.insert(new_pos_edge, (new_leaf_edge,))
                    if self._trace > 0:
                        print  'Scanner  ', chart.pp_edge(new_pos_edge,w)
            
            
            for edge in chart.select(end=end):
                if edge.is_incomplete():
                    for e in predictor.apply(chart, grammar, edge):
                        if self._trace > 1:
                            print 'Predictor', chart.pp_edge(e,w)
                #if edge.is_incomplete():
                #    for e in scanner.apply(chart, grammar, edge):
                #        if self._trace > 0:
                #            print 'Scanner  ', chart.pp_edge(e,w)
                if edge.is_complete():
                    for e in completer.apply(chart, grammar, edge):
                        if self._trace > 0:
                            print 'Completer', chart.pp_edge(e,w)

        # Output a list of complete parses.
        return chart.parses(root)

def demo():
    import sys, time

    S = GrammarCategory.parse('S')
    VP = GrammarCategory.parse('VP')
    NP = GrammarCategory.parse('NP')
    PP = GrammarCategory.parse('PP')
    V = GrammarCategory.parse('V')
    N = GrammarCategory.parse('N')
    P = GrammarCategory.parse('P')
    Name = GrammarCategory.parse('Name')
    Det = GrammarCategory.parse('Det')
    DetSg = GrammarCategory.parse('Det[-pl]')
    DetPl = GrammarCategory.parse('Det[+pl]')
    NSg = GrammarCategory.parse('N[-pl]')
    NPl = GrammarCategory.parse('N[+pl]')

    # Define some grammatical productions.
    grammatical_productions = [
        cfg.Production(S, (NP, VP)),  cfg.Production(PP, (P, NP)),
        cfg.Production(NP, (NP, PP)),
        cfg.Production(VP, (VP, PP)), cfg.Production(VP, (V, NP)),
        cfg.Production(VP, (V,)), cfg.Production(NP, (DetPl, NPl)),
        cfg.Production(NP, (DetSg, NSg))]

    # Define some lexical productions.
    lexical_productions = [
        cfg.Production(NP, ('John',)), cfg.Production(NP, ('I',)),
        cfg.Production(Det, ('the',)), cfg.Production(Det, ('my',)),
        cfg.Production(Det, ('a',)),
        cfg.Production(NSg, ('dog',)),   cfg.Production(NSg, ('cookie',)),
        cfg.Production(V, ('ate',)),  cfg.Production(V, ('saw',)),
        cfg.Production(P, ('with',)), cfg.Production(P, ('under',)),
        ]
    
    earley_grammar = cfg.Grammar(S, grammatical_productions)
    earley_lexicon = {}
    for prod in lexical_productions:
        earley_lexicon.setdefault(prod.rhs()[0].upper(), []).append(prod.lhs())
    def lexicon(word):
        return earley_lexicon.get(word.upper(), [])

    sent = 'I saw John with a dog with my cookie'
    print "Sentence:\n", sent
    from nltk import tokenize
    tokens = list(tokenize.whitespace(sent))
    t = time.time()
    cp = FeatureEarleyChartParse(earley_grammar, lexicon, trace=1)
    trees = cp.get_parse_list(tokens)
    print "Time: %s" % (time.time() - t)
    for tree in trees: print tree

def run_profile():
    import profile
    profile.run('for i in range(1): demo()', '/tmp/profile.out')
    import pstats
    p = pstats.Stats('/tmp/profile.out')
    p.strip_dirs().sort_stats('time', 'cum').print_stats(60)
    p.strip_dirs().sort_stats('cum', 'time').print_stats(60)

if __name__ == '__main__':
    demo()


########NEW FILE########
__FILENAME__ = featurelite
"""
This module provides utilities for treating Python dictionaries as X{feature
structures}. Specifically, it contains the C{unify} function, which can be used
to merge the properties of two dictionaries, and the C{Variable} class, which
holds an unknown value to be used in unification.

A X{feature structure} is a mapping from feature names to feature values,
where:

  - Each X{feature name} is a case sensitive string.
  - Each X{feature value} can be a base value (such as a string), a
    variable, or a nested feature structure.

However, feature structures are not a specialized class; they are represented
by dictionaries, or more generally by anything that responds to the C{has_key}
method. The YAML representation can be used to create and display feature
structures intuitively:

>>> f1 = yaml.load('''
... A:
...   B: b
...   D: d
... ''')
>>> f2 = yaml.load('''
... A:
...   C: c
...   D: d
... ''')
>>> print show(unify(f1, f2))
A:
  B: b
  C: c
  D: d

Feature structures are typically used to represent partial information
about objects.  A feature name that is not mapped to a value stands
for a feature whose value is unknown (I{not} a feature without a
value).  Two feature structures that represent (potentially
overlapping) information about the same object can be combined by
X{unification}.  When two inconsistant feature structures are unified,
the unification fails and raises an error.

Features can be specified using X{feature paths}, or tuples of feature names
that specify paths through the nested feature structures to a value.

Feature structures may contain reentrant feature values.  A
X{reentrant feature value} is a single feature value that can be
accessed via multiple feature paths.  Unification preserves the
reentrance relations imposed by both of the unified feature
structures.  After unification, any extensions to a reentrant feature
value will be visible using any of its feature paths.

Feature structure variables are encoded using the L{Variable} class. The scope
of a variable is determined by the X{bindings} used when the structure
including that variable is unified. Bindings can be reused between unifications
to ensure that variables with the same name get the same value.

"""

from copy import copy, deepcopy
import re
import yaml
#import unittest
import sys

class UnificationFailure(Exception):
    """
    An exception that is raised when two values cannot be unified.
    """
    pass

def makevar(varname):
    """
    Given a variable representation such as C{?x}, construct a corresponding
    Variable object.
    """
    return Variable(varname[1:])

def isMapping(obj):
    """
    Determine whether to treat a given object as a feature structure. The
    test is whether it responds to C{has_key}. This can be overridden if the
    object includes an attribute or method called C{_no_feature}.

    @param obj: The object to be tested
    @type obj: C{object}
    @return: True iff the object can be treated as a feature structure
    @rtype: C{bool}
    """
    return isinstance(obj, dict) or isinstance(obj, FeatureI)

class FeatureI(object):
    def __init__(self):
        raise TypeError, "FeatureI is an abstract interface"

class _FORWARD(object):
    """
    _FORWARD is a singleton value, used in unification as a flag that a value
    has been forwarded to another object.

    This class itself is used as the singleton value. It cannot be
    instantiated.
    """
    def __init__(self):
        raise TypeError, "The _FORWARD class is not meant to be instantiated"

class Variable(object):
    """
    A Variable is an object that can be used in unification to hold an
    initially unknown value. Two equivalent Variables, for example, can be used
    to require that two features have the same value.

    When a Variable is assigned a value, it will eventually be replaced by
    that value. However, in order to make that value show up everywhere the
    variable appears, the Variable temporarily stores its assigned value and
    becomes a I{bound variable}. Bound variables do not appear in the results
    of unification.

    Variables are distinguished by their name, and by the dictionary of
    I{bindings} that is being used to determine their values. Two variables can
    have the same name but be associated with two different binding
    dictionaries: those variables are not equal.
    """
    _next_numbered_id = 1
    
    def __init__(self, name=None, value=None):
        """
        Construct a new feature structure variable.
        
        The value should be left at its default of None; it is only used
        internally to copy bound variables.

        @type name: C{string}
        @param name: An identifier for this variable. Two C{Variable} objects
          with the same name will be given the same value in a given dictionary
          of bindings.
        """
        self._uid = Variable._next_numbered_id
        Variable._next_numbered_id += 1
        if name is None: name = self._uid
        self._name = str(name)
        self._value = value
    
    def name(self):
        """
        @return: This variable's name.
        @rtype: C{string}
        """
        return self._name
    
    def value(self):
        """
        If this varable is bound, find its value. If it is unbound or aliased
        to an unbound variable, returns None.
        
        @return: The value of this variable, if any.
        @rtype: C{object}
        """
        if isinstance(self._value, Variable): return self._value.value()
        else: return self._value
    def copy(self):
        """
        @return: A copy of this variable.
        @rtype: C{Variable}
        """
        return Variable(self.name(), self.value())
    
    def forwarded_self(self):
        """
        Variables are aliased to other variables by one variable _forwarding_
        to the other. The first variable simply has the second as its value,
        but it acts like the second variable's _value_ is its value.

        forwarded_self returns the final Variable object that actually stores
        the value.

        @return: The C{Variable} responsible for storing this variable's value.
        @rtype: C{Variable}
        """
        if isinstance(self._value, Variable):
            return self._value.forwarded_self()
        else: return self
    
    def bindValue(self, value, ourbindings, otherbindings):
        """
        Bind this variable to a value. C{ourbindings} are the bindings that
        accompany the feature structure this variable came from;
        C{otherbindings} are the bindings from the structure it's being unified
        with.

        @type value: C{object}
        @param value: The value to be assigned.
        @type ourbindings: C{dict}
        @param ourbindings: The bindings associated with this variable.
        @type otherbindings: C{dict}
        @param otherbindings: The bindings associated with the value being
          assigned. (May be identical to C{ourbindings}.)
        """
        if isinstance(self._value, Variable):
            # Forward the job of binding to the variable we're aliased to.
            return self._value.bindValue(value, ourbindings, otherbindings)
        if self._value is None:
            # This variable is unbound, so bind it.
            self._value = value
        else:
            # This variable is already bound; try to unify the existing value
            # with the new one.
            self._value = unify(self._value, value, ourbindings, otherbindings)

    def forwardTo(self, other, ourbindings, otherbindings):
        """
        A unification wants this variable to be aliased to another variable.
        Forward this variable to the other one, and return the other.

        @type other: C{Variable}
        @param other: The variable to replace this one.
        @type ourbindings: C{dict}
        @param ourbindings: The bindings associated with this variable.
        @type otherbindings: C{dict}
        @param otherbindings: The bindings associated with the other variable.
        (May be identical to C{ourbindings}.)
        @return: C{other}
        @rtype: C{Variable}
        """
        other.bindValue(self.value(), ourbindings, otherbindings)
        self._value = other
        return other
        
    def __hash__(self): return hash(self._uid)
    def __cmp__(self, other):
        """
        Variables are equal if they are the same object or forward to the
        same object. Variables with the same name may still be unequal.
        """
        if not isinstance(other, Variable): return -1
        if isinstance(self._value, Variable): return cmp(self._value, other)
        else: return cmp((self._name, self._value), (other._name, other._value))
    def __repr__(self):
        if self._value is None: return '?%s' % self._name
        else: return '?%s: %r' % (self._name, self._value)

class SubstituteBindingsI:
    """
    An interface for classes that can perform substitutions for feature
    variables.
    """
    def substitute_bindings(self, bindings):
        """
        @return: The object that is obtained by replacing
        each variable bound by C{bindings} with its values.
        @rtype: (any)
        """
        raise NotImplementedError

class SubstituteBindingsMixin(SubstituteBindingsI):
    def substitute_bindings(self, bindings):
        newval = self
        for semvar in self.variables():
            varstr = str(semvar)
            # discard Variables which don't look like FeatureVariables
            if varstr.startswith('?'):
                var = makevar(varstr)
                if bindings.has_key(var.name()):
                    newval = newval.replace(semvar, bindings[var.name()])
        return newval

def show(data):
    """
    Works like yaml.dump(), but with output suited for doctests. Flow style
    is always off, and there is no blank line at the end.
    """
    return yaml.dump(data, default_flow_style=False).strip()

def object_to_features(obj):
    if not hasattr(obj, '__dict__'): return obj
    if str(obj).startswith('?'):
        return Variable(str(obj)[1:])
    if isMapping(obj): return obj
    dict = {}
    dict['__class__'] = obj.__class__.__name__
    for (key, value) in obj.__dict__.items():
        dict[key] = object_to_features(value)
    return dict

def variable_representer(dumper, var):
    "Output variables in YAML as ?name."
    return dumper.represent_scalar(u'!var', u'?%s' % var.name())
yaml.add_representer(Variable, variable_representer)

def variable_constructor(loader, node):
    "Recognize variables written as ?name in YAML."
    value = loader.construct_scalar(node)
    name = value[1:]
    return Variable(name)
yaml.add_constructor(u'!var', variable_constructor)
yaml.add_implicit_resolver(u'!var', re.compile(r'^\?\w+$'))

def _copy_and_bind(feature, bindings, memo=None):
    """
    Make a deep copy of a feature structure, preserving reentrance using the
    C{memo} dictionary. Meanwhile, variables are replaced by their bound
    values, if these values are already known, and variables with unknown
    values are given placeholder bindings.
    """
    if memo is None: memo = {}
    if id(feature) in memo: return memo[id(feature)]
    if isinstance(feature, Variable) and bindings is not None:
        if not bindings.has_key(feature.name()):
            bindings[feature.name()] = feature.copy()
        result = _copy_and_bind(bindings[feature.name()], None, memo)
    else:
        if isMapping(feature):
            # Construct a new object of the same class
            result = feature.__class__()
            for (key, value) in feature.items():
                result[key] = _copy_and_bind(value, bindings, memo)
        elif isinstance(feature, SubstituteBindingsI):
            if bindings is not None:
                result = feature.substitute_bindings(bindings).simplify()
            else:
                result = feature.simplify()
        else: result = feature
    memo[id(feature)] = result
    memo[id(result)] = result
    return result

def substitute_bindings(feature, bindings):
    """
    Replace variables in a feature structure with their bound values.
    """
    return _copy_and_bind(feature, bindings.copy())

apply = substitute_bindings

def unify(feature1, feature2, bindings1=None, bindings2=None, memo=None, fail=None, trace=0):
    """
    In general, the 'unify' procedure takes two values, and either returns a
    value that provides the information provided by both values, or fails if
    that is impossible.
    
    These values can have any type, but fall into a few general cases:
      - Values that respond to C{has_key} represent feature structures. The
        C{unify} procedure will recurse into them and unify their inner values.
      - L{Variable}s represent an unknown value, and are handled specially.
        The values assigned to variables are tracked using X{bindings}.
      - C{None} represents the absence of information.
      - Any other value is considered a X{base value}. Base values are
        compared to each other with the == operation.

    The value 'None' represents the absence of any information. It specifies no
    properties and acts as the identity in unification.
    >>> unify(3, None)
    3

    >>> unify(None, 'fish')
    'fish'

    A base value unifies with itself, but not much else.
    >>> unify(True, True)
    True

    >>> unify([1], [1])
    [1]

    >>> unify('a', 'b')
    Traceback (most recent call last):
        ...
    UnificationFailure

    When two mappings (representing feature structures, and usually implemented
    as dictionaries) are unified, any chain of keys that accesses a value in
    either mapping will access an equivalent or more specific value in the
    unified mapping. If this is not possible, UnificationFailure is raised.

    >>> f1 = dict(A=dict(B='b'))
    >>> f2 = dict(A=dict(C='c'))
    >>> unify(f1, f2) == dict(A=dict(B='b', C='c'))
    True
    
    The empty dictionary specifies no features. It unifies with any mapping.
    >>> unify({}, dict(foo='bar'))
    {'foo': 'bar'}

    >>> unify({}, True)
    Traceback (most recent call last):
        ...
    UnificationFailure
    
    Representing dictionaries in YAML form is useful for making feature
    structures readable:
    
    >>> f1 = yaml.load("number: singular")
    >>> f2 = yaml.load("person: 3")
    >>> print show(unify(f1, f2))
    number: singular
    person: 3

    >>> f1 = yaml.load('''
    ... A:
    ...   B: b
    ...   D: d
    ... ''')
    >>> f2 = yaml.load('''
    ... A:
    ...   C: c
    ...   D: d
    ... ''')
    >>> print show(unify(f1, f2))
    A:
      B: b
      C: c
      D: d
    
    Variables are names for unknown values. Variables are assigned values
    that will make unification succeed. The values of variables can be reused
    in later unifications if you provide a dictionary of _bindings_ from
    variables to their values.
    >>> bindings = {}
    >>> print unify(Variable('x'), 5, bindings)
    5
    
    >>> print bindings
    {'x': 5}
    
    >>> print unify({'a': Variable('x')}, {}, bindings)
    {'a': 5}
    
    The same variable name can be reused in different binding dictionaries
    without collision. In some cases, you may want to provide two separate
    binding dictionaries to C{unify} -- one for each feature structure, so
    their variables do not collide.

    In the following examples, two different feature structures use the
    variable ?x to require that two values are equal. The values assigned to
    ?x are consistent within each structure, but would be inconsistent if every
    ?x had to have the same value.

    >>> f1 = yaml.load('''
    ... a: 1
    ... b: 1
    ... c: ?x
    ... d: ?x
    ... ''')
    >>> f2 = yaml.load('''
    ... a: ?x
    ... b: ?x
    ... c: 2
    ... d: 2
    ... ''')
    >>> bindings1 = {}
    >>> bindings2 = {}
    >>> print show(unify(f1, f2, bindings1, bindings2))
    a: 1
    b: 1
    c: 2
    d: 2
    
    >>> print bindings1
    {'x': 2}
    
    >>> print bindings2
    {'x': 1}

    Feature structures can involve _reentrant_ values, where multiple feature
    paths lead to the same value. This is represented by the features having
    the same Python object as a value. (This kind of identity can be tested
    using the C{is} operator.)
    
    Unification preserves the properties of reentrance. So if a reentrant value
    is changed by unification, it is changed everywhere it occurs, and it is
    still reentrant. Reentrant features can even form cycles; these
    cycles can now be printed through the current YAML library.

    >>> f1 = yaml.load('''
    ... A: &1                # &1 defines a reference in YAML...
    ...   B: b
    ... E:
    ...   F: *1              # and *1 uses the previously defined reference.
    ... ''')
    >>> f1['E']['F']['B']
    'b'
    >>> f1['A'] is f1['E']['F']
    True
    >>> f2 = yaml.load('''
    ... A:
    ...   C: c
    ... E:
    ...   F:
    ...     D: d
    ... ''')
    >>> f3 = unify(f1, f2)
    >>> print show(f3)
    A: &id001
      B: b
      C: c
      D: d
    E:
      F: *id001
    >>> f3['A'] is f3['E']['F']    # Showing that the reentrance still holds.
    True
    
    This unification creates a cycle:
    >>> f1 = yaml.load('''
    ... F: &1 {}
    ... G: *1
    ... ''')
    >>> f2 = yaml.load('''
    ... F:
    ...   H: &2 {}
    ... G: *2
    ... ''')
    >>> f3 = unify(f1, f2)
    >>> print f3
    {'G': {'H': {...}}, 'F': {'H': {...}}}
    >>> print f3['F'] is f3['G']
    True
    >>> print f3['F'] is f3['G']['H']
    True
    >>> print f3['F'] is f3['G']['H']['H']
    True

    A cycle can also be created using variables instead of reentrance.
    Here we supply a single set of bindings, so that it is used on both sides
    of the unification, making ?x mean the same thing in both feature
    structures.
    
    >>> f1 = yaml.load('''
    ... F:
    ...   H: ?x
    ... ''')
    >>> f2 = yaml.load('''
    ... F: ?x
    ... ''')
    >>> f3 = unify(f1, f2, {})
    >>> print f3
    {'F': {'H': {...}}}
    >>> print f3['F'] is f3['F']['H']
    True
    >>> print f3['F'] is f3['F']['H']['H']
    True

    Two sets of bindings can be provided because the variable names on each
    side of the unification may be unrelated. An example involves unifying the
    following two structures, which each require that two values are
    equivalent, and happen to both use ?x to express that requirement.

    >>> f1 = yaml.load('''
    ... a: 1
    ... b: 1
    ... c: ?x
    ... d: ?x
    ... ''')
    >>> f2 = yaml.load('''
    ... a: ?x
    ... b: ?x
    ... c: 2
    ... d: 2
    ... ''')
    >>> bindings1 = {}
    >>> bindings2 = {}
    >>> # We could avoid defining two empty dictionaries by simply using the
    >>> # defaults, with unify(f1, f2) -- but we want to be able to examine
    >>> # the bindings afterward.
    >>> print show(unify(f1, f2, bindings1, bindings2))
    a: 1
    b: 1
    c: 2
    d: 2
    >>> print bindings1
    {'x': 2}
    >>> print bindings2
    {'x': 1}

    If a variable is unified with another variable, the two variables are
    _aliased_ to each other; they share the same value, similarly to reentrant
    feature structures. This is represented in a set of bindings as one
    variable having the other as its value.
    >>> f1 = yaml.load('''
    ... a: ?x
    ... b: ?x
    ... ''')
    >>> f2 = yaml.load('''
    ... b: ?y
    ... c: ?y
    ... ''')
    >>> bindings = {}
    >>> print show(unify(f1, f2, bindings))
    a: &id001 ?y
    b: *id001
    c: *id001
    >>> print bindings
    {'x': ?y}

    Reusing the same variable bindings ensures that appropriate bindings are
    made after the fact:
    >>> bindings = {}
    >>> f1 = {'a': Variable('x')}
    >>> f2 = unify(f1, {'a': {}}, bindings)
    >>> f3 = unify(f2, {'b': Variable('x')}, bindings)
    >>> print show(f3)
    a: &id001 {}
    b: *id001
    >>> print bindings
    {'x': {}}

    @param feature1: The first object to be unified.
    @type feature1: C{object} (probably a mapping)
    @param feature2: The second object to be unified.
    @type feature2: C{object} (probably a mapping)
    @param bindings1: The variable bindings associated with the first object.
    @type bindings1: C{dict} or None
    @param bindings2: The variable bindings associated with the second object,
      if these are distinct from C{bindings1}.
    @type bindings2: C{dict} or None
    @return: The result of unifying the two objects.
    @rtype: C{object} (probably a mapping)
    """
    if fail is None:
        def failerror(f1, f2):
            raise UnificationFailure
        fail = failerror
        
    if bindings1 is None and bindings2 is None:
        bindings1 = {}
        bindings2 = {}
    else:
        if bindings1 is None: bindings1 = {}
        if bindings2 is None: bindings2 = bindings1

    if memo is None: memo = {}
    copymemo = {}
    if memo.has_key((id(feature1), id(feature2))):
        result = memo[id(feature1), id(feature2)]
        if result is UnificationFailure:
            if trace > 2:
                print '(cached) Unifying: %r + %r --> [fail]' % (feature1, feature2)
            raise result()
        if trace > 2:
            print '(cached) Unifying: %r + %r --> ' % (feature1, feature2),
            print repr(result)
        return result

    if trace > 1:
        print 'Unifying: %r + %r --> ' % (feature1, feature2),
    
    # Make copies of the two structures (since the unification algorithm is
    # destructive). Use the same memo, to preserve reentrance links between
    # them.
    copy1 = _copy_and_bind(feature1, bindings1, copymemo)
    copy2 = _copy_and_bind(feature2, bindings2, copymemo)
    # Preserve links between bound variables and the two feature structures.
    for b in (bindings1, bindings2):
        for (vname, value) in b.items():
            value_id = id(value)
            if value_id in copymemo:
                b[vname] = copymemo[value_id]

    # Go on to doing the unification.
    try:
        unified = _destructively_unify(copy1, copy2, bindings1, bindings2, memo,
        fail)
    except UnificationFailure:
        if trace > 1: print '[fail]'
        memo[id(feature1), id(feature2)] = UnificationFailure
        raise

    _apply_forwards_to_bindings(bindings1)
    _apply_forwards_to_bindings(bindings2)
    _apply_forwards(unified, {})
    unified = _lookup_values(unified, {}, remove=False)
    _lookup_values(bindings1, {}, remove=True)
    _lookup_values(bindings2, {}, remove=True)

    if trace > 1:
        print repr(unified)
    elif trace > 0:
        print 'Unifying: %r + %r --> %r' % (feature1, feature2, repr(unified))
    
    memo[id(feature1), id(feature2)] = unified
    return unified

def _destructively_unify(feature1, feature2, bindings1, bindings2, memo, fail,
depth=0):
    """
    Attempt to unify C{self} and C{other} by modifying them
    in-place.  If the unification succeeds, then C{self} will
    contain the unified value, and the value of C{other} is
    undefined.  If the unification fails, then a
    UnificationFailure is raised, and the values of C{self}
    and C{other} are undefined.
    """
    if depth > 50:
        print "Infinite recursion in this unification:"
        print show(dict(feature1=feature1, feature2=feature2,
        bindings1=bindings1, bindings2=bindings2, memo=memo))
        raise ValueError, "Infinite recursion in unification"
    if memo.has_key((id(feature1), id(feature2))):
        result = memo[id(feature1), id(feature2)]
        if result is UnificationFailure: raise result()
    unified = _do_unify(feature1, feature2, bindings1, bindings2, memo, fail,
    depth)
    memo[id(feature1), id(feature2)] = unified
    return unified

def _do_unify(feature1, feature2, bindings1, bindings2, memo, fail, depth=0):
    """
    Do the actual work of _destructively_unify when the result isn't memoized.
    """

    # Trivial cases.
    if feature1 is None: return feature2
    if feature2 is None: return feature1
    if feature1 is feature2: return feature1
    
    # Deal with variables by binding them to the other value.
    if isinstance(feature1, Variable):
        if isinstance(feature2, Variable):
            # If both objects are variables, forward one to the other. This
            # has the effect of unifying the variables.
            return feature1.forwardTo(feature2, bindings1, bindings2)
        else:
            feature1.bindValue(feature2, bindings1, bindings2)
            return feature1
    if isinstance(feature2, Variable):
        feature2.bindValue(feature1, bindings2, bindings1)
        return feature2
    
    # If it's not a mapping or variable, it's a base object, so we just
    # compare for equality.
    if not isMapping(feature1):
        if feature1 == feature2: return feature1
        else: 
            return fail(feature1, feature2)
    if not isMapping(feature2):
        return fail(feature1, feature2)
    
    # At this point, we know they're both mappings.
    # Do the destructive part of unification.

    while feature2.has_key(_FORWARD): feature2 = feature2[_FORWARD]
    if feature1 is not feature2: feature2[_FORWARD] = feature1
    for (fname, val2) in feature2.items():
        if fname == _FORWARD: continue
        val1 = feature1.get(fname)
        feature1[fname] = _destructively_unify(val1, val2, bindings1,
        bindings2, memo, fail, depth+1)
    return feature1

def _apply_forwards(feature, visited):
    """
    Replace any feature structure that has a forward pointer with
    the target of its forward pointer (to preserve reentrance).
    """
    if not isMapping(feature): return
    if visited.has_key(id(feature)): return
    visited[id(feature)] = True

    for fname, fval in feature.items():
        if isMapping(fval):
            while fval.has_key(_FORWARD):
                fval = fval[_FORWARD]
                feature[fname] = fval
            _apply_forwards(fval, visited)

def _lookup_values(mapping, visited, remove=False):
    """
    The unification procedure creates _bound variables_, which are Variable
    objects that have been assigned a value. Bound variables are not useful
    in the end result, however, so they should be replaced by their values.

    This procedure takes a mapping, which may be a feature structure or a
    binding dictionary, and replaces bound variables with their values.
    
    If the dictionary is a binding dictionary, then 'remove' should be set to
    True. This ensures that unbound, unaliased variables are removed from the
    dictionary. If the variable name 'x' is mapped to the unbound variable ?x,
    then, it should be removed. This is not done with features, because a
    feature named 'x' can of course have a variable ?x as its value.
    """
    if isinstance(mapping, Variable):
        # Because it's possible to unify bare variables, we need to gracefully
        # accept a variable in place of a dictionary, and return a result that
        # is consistent with that variable being inside a dictionary.
        #
        # We can't remove a variable from itself, so we ignore 'remove'.
        var = mapping
        if var.value() is not None:
            return var.value()
        else:
            return var.forwarded_self()
    if not isMapping(mapping): return mapping
    if visited.has_key(id(mapping)): return mapping
    visited[id(mapping)] = True

    for fname, fval in mapping.items():
        if isMapping(fval):
            _lookup_values(fval, visited)
        elif isinstance(fval, Variable):
            if fval.value() is not None:
                mapping[fname] = fval.value()
                if isMapping(mapping[fname]):
                    _lookup_values(mapping[fname], visited)
            else:
                newval = fval.forwarded_self()
                if remove and newval.name() == fname:
                    del mapping[fname]
                else:
                    mapping[fname] = newval
    return mapping

def _apply_forwards_to_bindings(bindings):
    """
    Replace any feature structures that have been forwarded by their new
    identities.
    """
    for (key, value) in bindings.items():
        if isMapping(value) and value.has_key(_FORWARD):
            while value.has_key(_FORWARD):
                value = value[_FORWARD]
            bindings[key] = value

def test():
    "Run unit tests on unification."
    import doctest
    doctest.testmod()

if __name__ == "__main__":
    test()


########NEW FILE########
__FILENAME__ = interact
from featurechart import *
from logic import Counter
import sys

def interact(grammar_filename, trace=2):
    cp = load_earley(grammar_filename, trace=trace-2)
    model = []
    counter = Counter()
    while True:
        sys.stdout.write('> ')
        line = sys.stdin.readline().strip()
        if not line: break

        # Read a line and parse it.
        trees = cp.parse(line)
        if len(trees) == 0:
            print "I don't understand."
            continue
        elif len(trees) > 1:
            print "That was ambiguous, but I'll guess at what you meant."
        
        # Extract semantic information from the parse tree.
        tree = trees[0]
        pos = tree[0][0].node['pos']
        sem = tree[0].node['sem']

        # We need variables to have unique names even if they didn't get
        # alpha-converted already. Replace all the variables that are unbound
        # via skolemization -- but not the ones that are completely free,
        # like "mary" -- with uniquely-named ones.
        free = sem.free()
        skolem = sem.skolemize()
        almostfree = skolem.free()
        vars = set(almostfree).difference(free)
        for var in vars:
            skolem = skolem.replace_unique(var, counter)
        
        if trace > 0:
            print tree
            print 'Semantic value:', skolem
        clauses = skolem.clauses()
        if trace > 1:
            print "Got these clauses:"
            for clause in clauses:
                print '\t', clause
        
        if pos == 'S':
            # Handle statements
            model += clauses
        elif pos == 'Q':
            # Handle questions
            bindings = {}
            success = True
            for clause in clauses:
                success = False
                for known in model:
                    newbindings = dict(bindings)
                    try:
                        unify(object_to_features(clause),
                        object_to_features(known), newbindings)
                        bindings = newbindings
                        success = True
                        break
                    except UnificationFailure:
                        continue
                if not success:
                    break
            if success:
                # answer 
                answer = bindings.get('wh', 'Yes.')
                print answer['variable']['name']
            else:
                # This is an open world without negation, so negative answers
                # aren't possible.
                print "I don't know."

def demo():
    interact('lab3-slash.cfg', trace=2)

if __name__ == '__main__':
    demo()


########NEW FILE########
__FILENAME__ = logic
# Natural Language Toolkit: Logic
from nltk.utilities import Counter
from featurelite import SubstituteBindingsMixin, FeatureI
from featurelite import Variable as FeatureVariable
_counter = Counter()

def unique_variable(counter=None):
    if counter is None: counter = _counter
    unique = counter.get()
    return VariableExpression(Variable('x'+str(unique)))

class Error(Exception): pass

class Variable(object):
    """A variable, either free or bound."""
    
    def __init__(self, name):
        """
        Create a new C{Variable}.

        @type name: C{string}
        @param name: The name of the variable.
        """
        self.name = name

    def __eq__(self, other):
        return self.equals(other)

    def __ne__(self, other):
        return not self.equals(other)

    def equals(self, other):
        """A comparison function."""
        if not isinstance(other, Variable): return False
        return self.name == other.name
        
    def __str__(self): return self.name

    def __repr__(self): return "Variable('%s')" % self.name

    def __hash__(self): return hash(repr(self))

class Constant:
    """A nonlogical constant."""
    
    def __init__(self, name):
        """
        Create a new C{Constant}.

        @type name: C{string}
        @param name: The name of the constant.
        """
        self.name = name

    def __eq__(self, other):
        return self.equals(other)

    def __ne__(self, other):
        return not self.equals(other)

    def equals(self, other):
        """A comparison function."""
        assert isinstance(other, Constant)
        return self.name == other.name
        
    def __str__(self): return self.name

    def __repr__(self): return "Constant('%s')" % self.name

    def __hash__(self): return hash(repr(self))

class Expression(object):
    """The abstract class of a lambda calculus expression."""
    def __init__(self):
        if self.__class__ is Expression:
            raise NotImplementedError

    def __eq__(self, other):
        return self.equals(other)

    def __ne__(self, other):
        return not self.equals(other)

    def equals(self, other):
        """Are the two expressions equal, modulo alpha conversion?"""
        return NotImplementedError

    def variables(self):
        """Set of all variables."""
        raise NotImplementedError

    def free(self):
        """Set of free variables."""
        raise NotImplementedError

    def subterms(self):
        """Set of all subterms (including self)."""
        raise NotImplementedError


    def replace(self, variable, expression, replace_bound=False):
        """Replace all instances of variable v with expression E in self,
        where v is free in self."""
        raise NotImplementedError
    
    def replace_unique(self, variable, counter=None, replace_bound=False):
        """
        Replace a variable v with a new, uniquely-named variable.
        """
        return self.replace(variable, unique_variable(counter),
        replace_bound)

    def simplify(self):
        """Evaluate the form by repeatedly applying applications."""
        raise NotImplementedError

    def skolemise(self):
        """
        Perform a simple Skolemisation operation.  Existential quantifiers are
        simply dropped and all variables they introduce are renamed so that
        they are unique.
        """
        return self._skolemise(set(), Counter())

    skolemize = skolemise

    def _skolemise(self, bound_vars, counter):
        raise NotImplementedError

    def clauses(self):
        return [self]

    def __str__(self):
        raise NotImplementedError

    def __repr__(self):
        raise NotImplementedError

    def __hash__(self):
        raise NotImplementedError, self.__class__
    
    def normalize(self):
        if hasattr(self, '_normalized'): return self._normalized
        result = self
        vars = self.variables()
        counter = 0
        for var in vars:
            counter += 1
            result = result.replace(var, Variable(str(counter)), replace_bound=True)
        self._normalized = result
        return result

class VariableExpression(Expression):
    """A variable expression which consists solely of a variable."""
    def __init__(self, variable):
        Expression.__init__(self)
        assert isinstance(variable, Variable)
        self.variable = variable

    def equals(self, other):
        """
        Allow equality between instances of C{VariableExpression} and
        C{IndVariableExpression}.
        """
        if isinstance(self, VariableExpression) and \
           isinstance(other, VariableExpression):
            return self.variable.equals(other.variable)
        else:
            return False

    def variables(self):
        return [self.variable]

    def free(self):
        return set([self.variable])

    def subterms(self):
        return set([self])

    def replace(self, variable, expression, replace_bound=False):
        if self.variable.equals(variable):
            if isinstance(expression, Variable):
                return VariableExpression(expression)
            else:
                return expression
        else:
            return self
        
    def simplify(self):
        return self

    def infixify(self):
        return self

    def name(self):
        return self.__str__()

    def _skolemise(self, bound_vars, counter):
        return self

    def __str__(self): return '%s' % self.variable

    def __repr__(self): return "VariableExpression('%s')" % self.variable

    def __hash__(self): return hash(repr(self))


def is_indvar(expr):
    """
    Check whether an expression has the form of an individual variable.
    
    An individual variable matches the following regex:
    C{'^[wxyz](\d*)'}.
    
    @rtype: Boolean
    @param expr: String
    """
    result = expr[0] in ['w', 'x', 'y', 'z']
    if len(expr) > 1:
        return result and expr[1:].isdigit()
    else:
        return result
    
class IndVariableExpression(VariableExpression):
    """
    An individual variable expression, as determined by C{is_indvar()}.
    """
    def __init__(self, variable):
        Expression.__init__(self)
        assert isinstance(variable, Variable), "Not a Variable: %s" % variable
        assert is_indvar(str(variable)), "Wrong format for an Individual Variable: %s" % variable
        self.variable = variable

    def __repr__(self): return "IndVariableExpression('%s')" % self.variable 
        

class ConstantExpression(Expression):
    """A constant expression, consisting solely of a constant."""
    def __init__(self, constant):
        Expression.__init__(self)
        assert isinstance(constant, Constant)
        self.constant = constant

    def equals(self, other):
        if self.__class__ == other.__class__:
            return self.constant.equals(other.constant)
        else:
            return False

    def variables(self):
        return []

    def free(self):
        return set()

    def subterms(self):
        return set([self])

    def replace(self, variable, expression, replace_bound=False):
        return self
        
    def simplify(self):
        return self

    def infixify(self):
        return self

    def name(self):
        return self.__str__()

    def _skolemise(self, bound_vars, counter):
        return self

    def __str__(self): return '%s' % self.constant

    def __repr__(self): return "ConstantExpression('%s')" % self.constant

    def __hash__(self): return hash(repr(self))


class Operator(ConstantExpression):
    """
    A boolean operator, such as 'not' or 'and', or the equality
    relation ('=').
    """
    def __init__(self, operator):
        Expression.__init__(self)
        assert operator in Parser.OPS
        self.constant = operator
        self.operator = operator

    def equals(self, other):
        if self.__class__ == other.__class__:
            return self.constant == other.constant
        else:
            return False

    def simplify(self):
        return self

    def __str__(self): return '%s' % self.operator

    def __repr__(self): return "Operator('%s')" % self.operator



class VariableBinderExpression(Expression):
    """A variable binding expression: e.g. \\x.M."""

    # for generating "unique" variable names during alpha conversion.
    _counter = Counter()

    def __init__(self, variable, term):
        Expression.__init__(self)
        assert isinstance(variable, Variable)
        assert isinstance(term, Expression)
        self.variable = variable
        self.term = term
        self.prefix = self.__class__.PREFIX.rstrip()
        self.binder = (self.prefix, self.variable.name)
        self.body = str(self.term)

    def equals(self, other):
        r"""
        Defines equality modulo alphabetic variance.

        If we are comparing \x.M  and \y.N, then
        check equality of M and N[x/y].
        """
        if self.__class__ == other.__class__:
            if self.variable == other.variable:
                return self.term == other.term
            else:
                # Comparing \x.M  and \y.N.
                # Relabel y in N with x and continue.
                relabeled = self._relabel(other)
                return self.term == relabeled
        else:
            return False

    def _relabel(self, other):
        """
        Relabel C{other}'s bound variables to be the same as C{self}'s
        variable.
        """
        var = VariableExpression(self.variable)
        return other.term.replace(other.variable, var)

    def variables(self):
        vars = [self.variable]
        for var in self.term.variables():
            if var not in vars: vars.append(var)
        return vars

    def free(self):
        return self.term.free().difference(set([self.variable]))

    def subterms(self):
        return self.term.subterms().union([self])

    def replace(self, variable, expression, replace_bound=False):
        if self.variable == variable:
            if not replace_bound: return self
            else: return self.__class__(expression,
                self.term.replace(variable, expression, True))
        if replace_bound or self.variable in expression.free():
            v = 'z' + str(self._counter.get())
            if not replace_bound: self = self.alpha_convert(Variable(v))
        return self.__class__(self.variable,
            self.term.replace(variable, expression, replace_bound))

    def alpha_convert(self, newvar):
        """
        Rename all occurrences of the variable introduced by this variable
        binder in the expression to @C{newvar}.
        """
        term = self.term.replace(self.variable, VariableExpression(newvar))
        return self.__class__(newvar, term)

    def simplify(self):
        return self.__class__(self.variable, self.term.simplify())

    def infixify(self):
        return self.__class__(self.variable, self.term.infixify())

    def __str__(self, continuation=0):
        # Print \x.\y.M as \x y.M.
        if continuation:
            prefix = ' '
        else:
            prefix = self.__class__.PREFIX
        if self.term.__class__ == self.__class__:
            return '%s%s%s' % (prefix, self.variable, self.term.__str__(1))
        else:
            return '%s%s.%s' % (prefix, self.variable, self.term)

    def __hash__(self):
        return hash(str(self.normalize()))
    
class LambdaExpression(VariableBinderExpression):
    """A lambda expression: \\x.M."""
    PREFIX = '\\'

    def _skolemise(self, bound_vars, counter):
        bv = bound_vars.copy()
        bv.add(self.variable)
        return self.__class__(self.variable, self.term._skolemise(bv, counter))

    def __repr__(self):
        return str(self)
        #return "LambdaExpression('%s', '%s')" % (self.variable, self.term)

class SomeExpression(VariableBinderExpression):
    """An existential quantification expression: some x.M."""
    PREFIX = 'some '

    def _skolemise(self, bound_vars, counter):
        if self.variable in bound_vars:
            var = Variable("_s" + str(counter.get()))
            term = self.term.replace(self.variable, VariableExpression(var))
        else:
            var = self.variable
            term = self.term
        bound_vars.add(var)
        return term._skolemise(bound_vars, counter)

    def __repr__(self):
        return str(self)
        #return "SomeExpression('%s', '%s')" % (self.variable, self.term)


class AllExpression(VariableBinderExpression):
    """A universal quantification expression: all x.M."""
    PREFIX = 'all '

    def _skolemise(self, bound_vars, counter):
        bv = bound_vars.copy()
        bv.add(self.variable)
        return self.__class__(self.variable, self.term._skolemise(bv, counter))

    def __repr__(self):
        return str(self)
        #return "AllExpression('%s', '%s')" % (self.variable, self.term)



class ApplicationExpression(Expression):
    """An application expression: (M N)."""
    def __init__(self, first, second):
        Expression.__init__(self)
        assert isinstance(first, Expression)
        assert isinstance(second, Expression)
        self.first = first
        self.second = second

    def equals(self, other):
        if self.__class__ == other.__class__:
            return self.first.equals(other.first) and \
                   self.second.equals(other.second)
        else:
            return False

    def variables(self):
        vars = self.first.variables()
        for var in self.second.variables():
            if var not in vars: vars.append(var)
        return vars

    def free(self):
        return self.first.free().union(self.second.free())

    def _functor(self):
        if isinstance(self.first, ApplicationExpression):
            return self.first._functor()
        else:
            return self.first

    fun = property(_functor,
                   doc="Every ApplicationExpression has a functor.")


    def _operator(self):
        functor = self._functor()
        if isinstance(functor, Operator):
            return str(functor)
        else: 
            raise AttributeError

    op = property(_operator,
                  doc="Only some ApplicationExpressions have operators." )

    def _arglist(self):
        """Uncurry the argument list."""
        arglist = [str(self.second)]
        if isinstance(self.first, ApplicationExpression):
            arglist.extend(self.first._arglist())
        return arglist

    def _args(self):
        arglist = self._arglist()
        arglist.reverse()
        return arglist

    args = property(_args,
                   doc="Every ApplicationExpression has args.")

    def subterms(self):
        first = self.first.subterms()

        second = self.second.subterms()
        return first.union(second).union(set([self]))

    def replace(self, variable, expression, replace_bound=False):
        return self.__class__(
            self.first.replace(variable, expression, replace_bound),
            self.second.replace(variable, expression, replace_bound))

    def simplify(self):
        first = self.first.simplify()
        second = self.second.simplify()
        if isinstance(first, LambdaExpression):
            variable = first.variable
            term = first.term
            return term.replace(variable, second).simplify()
        else:
            return self.__class__(first, second)

    def infixify(self):
        first = self.first.infixify()
        second = self.second.infixify()
        if isinstance(first, Operator) and not str(first) == 'not':
            return self.__class__(second, first)
        else:
            return self.__class__(first, second)    

    def _skolemise(self, bound_vars, counter):
        first = self.first._skolemise(bound_vars, counter)
        second = self.second._skolemise(bound_vars, counter)
        return self.__class__(first, second)

    def clauses(self):
        if isinstance(self.first, ApplicationExpression) and\
           isinstance(self.first.first, Operator) and\
           self.first.first.operator == 'and':
           return self.first.second.clauses() + self.second.clauses()
        else: return [self]
    def __str__(self):
        # Print ((M N) P) as (M N P).
        strFirst = str(self.first)
        if isinstance(self.first, ApplicationExpression):
            if not isinstance(self.second, Operator):
                strFirst = strFirst[1:-1]
        return '(%s %s)' % (strFirst, self.second)

    def __repr__(self):
        return str(self)
        #return "ApplicationExpression('%s', '%s')" % (self.first, self.second)

    def __hash__(self):
        return hash(str(self.normalize()))

class ApplicationExpressionSubst(ApplicationExpression, SubstituteBindingsMixin):
    pass

class LambdaExpressionSubst(LambdaExpression, SubstituteBindingsMixin):
    pass

class SomeExpressionSubst(SomeExpression, SubstituteBindingsMixin):
    pass

class AllExpressionSubst(AllExpression, SubstituteBindingsMixin):
    pass

class Parser:
    """A lambda calculus expression parser."""

    
    # Tokens.
    LAMBDA = '\\'
    SOME = 'some'
    ALL = 'all'
    DOT = '.'
    OPEN = '('
    CLOSE = ')'
    BOOL = ['and', 'or', 'not', 'implies', 'iff']
    EQ = '='
    OPS = BOOL
    OPS.append(EQ)
    
    def __init__(self, data=None, constants=None):
        if data is not None:
            self.buffer = data
            self.process()
        else:
            self.buffer = ''
        if constants is not None:
            self.constants = constants
        else:
            self.constants = []
        

    def feed(self, data):
        """Feed another batch of data to the parser."""
        self.buffer += data
        self.process()

    def parse(self, data):
        """
        Provides a method similar to other NLTK parsers.

        @type data: str
        @returns: a parsed Expression
        """
        self.feed(data)
        result = self.next()
        return result

    def process(self):
        """Process the waiting stream to make it trivial to parse."""
        self.buffer = self.buffer.replace('\t', ' ')
        self.buffer = self.buffer.replace('\n', ' ')
        self.buffer = self.buffer.replace('\\', ' \\ ')
        self.buffer = self.buffer.replace('.', ' . ')
        self.buffer = self.buffer.replace('(', ' ( ')
        self.buffer = self.buffer.replace(')', ' ) ')

    def token(self, destructive=1):
        """Get the next waiting token.  The destructive flag indicates
        whether the token will be removed from the buffer; setting it to
        0 gives lookahead capability."""
        if self.buffer == '':
            raise Error, "end of stream"
        tok = None
        buffer = self.buffer
        while not tok:
            seq = buffer.split(' ', 1)
            if len(seq) == 1:
                tok, buffer = seq[0], ''
            else:
                assert len(seq) == 2
                tok, buffer = seq
            if tok:
                if destructive:
                    self.buffer = buffer
                return tok
        assert 0 # control never gets here
        return None

    def isVariable(self, token):
        """Is this token a variable (that is, not one of the other types)?"""
        TOKENS = [Parser.LAMBDA, Parser.SOME, Parser.ALL,
               Parser.DOT, Parser.OPEN, Parser.CLOSE, Parser.EQ]
        TOKENS.extend(self.constants)
        TOKENS.extend(Parser.BOOL)
        return token not in TOKENS 

    def next(self):
        """Parse the next complete expression from the stream and return it."""
        tok = self.token()
        
        if tok in [Parser.LAMBDA, Parser.SOME, Parser.ALL]:
            # Expression is a lambda expression: \x.M
            # or a some expression: some x.M
            if tok == Parser.LAMBDA:
                factory = self.make_LambdaExpression
            elif tok == Parser.SOME:
                factory = self.make_SomeExpression
            elif tok == Parser.ALL:
                factory = self.make_AllExpression
            else:
                raise ValueError(tok)

            vars = [self.token()]
            while self.isVariable(self.token(0)):
                # Support expressions like: \x y.M == \x.\y.M
                # and: some x y.M == some x.some y.M
                vars.append(self.token())
            tok = self.token()

            if tok != Parser.DOT:
                raise Error, "parse error, unexpected token: %s" % tok
            term = self.next()
            accum = factory(Variable(vars.pop()), term)
            while vars:
                accum = factory(Variable(vars.pop()), accum)
            return accum
            
        elif tok == Parser.OPEN:
            # Expression is an application expression: (M N)
            first = self.next()
            second = self.next()
            exps = []
            while self.token(0) != Parser.CLOSE:
                # Support expressions like: (M N P) == ((M N) P)
                exps.append(self.next())
            tok = self.token() # swallow the close token
            assert tok == Parser.CLOSE
            if isinstance(second, Operator):
                accum = self.make_ApplicationExpression(second, first)
            else:
                accum = self.make_ApplicationExpression(first, second)
            while exps:
                exp, exps = exps[0], exps[1:]
                accum = self.make_ApplicationExpression(accum, exp)
            return accum

        elif tok in self.constants:
            # Expression is a simple constant expression: a
            return ConstantExpression(Constant(tok))

        elif tok in Parser.OPS:
            # Expression is a boolean operator or the equality symbol
            return Operator(tok)

        elif is_indvar(tok):
            # Expression is a boolean operator or the equality symbol
            return IndVariableExpression(Variable(tok))
        
        else:
            if self.isVariable(tok):
                # Expression is a simple variable expression: x
                return VariableExpression(Variable(tok))
            else:
                raise Error, "parse error, unexpected token: %s" % tok
    
    # This is intended to be overridden, so that you can derive a Parser class
    # that constructs expressions using your subclasses.  So far we only need
    # to overridde ApplicationExpression, but the same thing could be done for
    # other expression types.
    def make_ApplicationExpression(self, first, second):
        return ApplicationExpression(first, second)
    def make_LambdaExpression(self, first, second):
        return LambdaExpression(first, second)
    def make_SomeExpression(self, first, second):
        return SomeExpression(first, second)
    def make_AllExpression(self, first, second):
        return AllExpression(first, second)

def expressions():
    """Return a sequence of test expressions."""
    a = Variable('a')
    x = Variable('x')
    y = Variable('y')
    z = Variable('z')
    A = VariableExpression(a)
    X = IndVariableExpression(x)
    Y = IndVariableExpression(y)
    Z = IndVariableExpression(z)
    XA = ApplicationExpression(X, A)
    XY = ApplicationExpression(X, Y)
    XZ = ApplicationExpression(X, Z)
    YZ = ApplicationExpression(Y, Z)
    XYZ = ApplicationExpression(XY, Z)
    I = LambdaExpression(x, X)
    K = LambdaExpression(x, LambdaExpression(y, X))
    L = LambdaExpression(x, XY)
    S = LambdaExpression(x, LambdaExpression(y, LambdaExpression(z, \
            ApplicationExpression(XZ, YZ))))
    B = LambdaExpression(x, LambdaExpression(y, LambdaExpression(z, \
            ApplicationExpression(X, YZ))))
    C = LambdaExpression(x, LambdaExpression(y, LambdaExpression(z, \
            ApplicationExpression(XZ, Y))))
    O = LambdaExpression(x, LambdaExpression(y, XY))
    N = ApplicationExpression(LambdaExpression(x, XA), I)
    T = Parser('\\x y.(x y z)').next()
    return [X, XZ, XYZ, I, K, L, S, B, C, O, N, T]

def demo():
    p = Variable('p')
    q = Variable('q')
    P = VariableExpression(p)
    Q = VariableExpression(q)
    for l in expressions():
        print "Expression:", l
        print "Variables:", l.variables()
        print "Free:", l.free()
        print "Subterms:", l.subterms()
        print "Simplify:",l.simplify()
        la = ApplicationExpression(ApplicationExpression(l, P), Q)
        las = la.simplify()
        print "Apply and simplify: %s -> %s" % (la, las)
        ll = Parser(str(l)).next()
        print 'l is:', l
        print 'll is:', ll
        assert l.equals(ll)
        print "Serialize and reparse: %s -> %s" % (l, ll)
        print "Variables:", ll.variables()
        print "Normalize: %s" % ll.normalize()


if __name__ == '__main__':
    demo()


########NEW FILE########
__FILENAME__ = test
from featurechart import *
from treeview import *

def demo():
    cp = load_earley('lab3-slash.cfg', trace=0)
    trees = cp.parse('Mary sees a dog in Noosa')
    for tree in trees:
        print tree
        sem = tree[0].node['sem']
        print sem
        print sem.skolemise().clauses()
        return sem.skolemise().clauses()

#run_profile()
if __name__ == '__main__': demo()

########NEW FILE########
__FILENAME__ = testw
from featurechart import *
from treeview import *

def demo():
    cp = load_earley('lab3-slash.cfg', trace=1)
    trees = cp.parse('Mary walks')
    for tree in trees:
        print tree
        sem = tree[0].node['sem']
        print sem
        print sem.skolemise().clauses()
        return sem.skolemise().clauses()

#run_profile()
if __name__ == '__main__': demo()

########NEW FILE########
__FILENAME__ = tree
# placeholder
from nltk.parse.tree import *

########NEW FILE########
__FILENAME__ = treeview
import Tkinter
from nltk.draw import TreeWidget
from nltk.draw import CanvasFrame

"""A class that draws parse trees in a simple Tk window."""

class TreeView:
    def __init__(self, trees, root=None):
        if len(trees) == 0:
            print "No trees to display."
            return

        newroot = False
        if root is None:
            root = Tkinter.Tk()
            window = root
            newroot = True
        else:
            window = Tkinter.Toplevel(root)
        
        window.title("Parse Tree")
        window.geometry("600x400")
        self.cf = CanvasFrame(window)
        self.cf.pack(side='top', expand=1, fill='both')
        buttons = Tkinter.Frame(window)
        buttons.pack(side='bottom', fill='x')

        self.spin = Tkinter.Spinbox(buttons, from_=1, to=len(trees),
            command=self.showtree, width=3)
        if len(trees) > 1: self.spin.pack(side='left')
        self.label = Tkinter.Label(buttons, text="of %d" % len(trees))
        if len(trees) > 1: self.label.pack(side='left')
        self.done = Tkinter.Button(buttons, text="Done", command=window.destroy)
        self.done.pack(side='right')
        self.printps = Tkinter.Button(buttons, text="Print to Postscript", command=self.cf.print_to_file)
        self.printps.pack(side='right')
        
        self.trees = trees
        self.treeWidget = None
        self.showtree()
        if newroot: root.mainloop()
    
    def showtree(self):
        try: n = int(self.spin.get())
        except ValueError: n=1
        if self.treeWidget is not None: self.cf.destroy_widget(self.treeWidget)
        self.treeWidget = TreeWidget(self.cf.canvas(),
        self.trees[n-1], draggable=1, shapeable=1)
        self.cf.add_widget(self.treeWidget, 0, 0)
    

########NEW FILE########
__FILENAME__ = drawchart
# Natural Language Toolkit: Chart Parser Demo
#
# Copyright (C) 2001-2007 NLTK Project
# Author: Edward Loper <edloper@gradient.cis.upenn.edu>
#         Jean Mark Gawron <gawron@mail.sdsu.edu>
#         Steven Bird <sb@csse.unimelb.edu.au>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT
#
# $Id: chart.py 4107 2007-02-01 00:07:42Z stevenbird $

"""
A graphical tool for exploring chart parsing.

Chart parsing is a flexible parsing algorithm that uses a data
structure called a "chart" to record hypotheses about syntactic
constituents.  Each hypothesis is represented by a single "edge" on
the chart.  A set of "chart rules" determine when new edges can be
added to the chart.  This set of rules controls the overall behavior
of the parser (e.g., whether it parses top-down or bottom-up).

The chart parsing tool demonstrates the process of parsing a single
sentence, with a given grammar and lexicon.  Its display is divided
into three sections: the bottom section displays the chart; the middle
section displays the sentence; and the top section displays the
partial syntax tree corresponding to the selected edge.  Buttons along
the bottom of the window are used to control the execution of the
algorithm.

The chart parsing tool allows for flexible control of the parsing
algorithm.  At each step of the algorithm, you can select which rule
or strategy you wish to apply.  This allows you to experiment with
mixing different strategies (e.g., top-down and bottom-up).  You can
exercise fine-grained control over the algorithm by selecting which
edge you wish to apply a rule to.
"""

# At some point, we should rewrite this tool to use the new canvas
# widget system.

import pickle
from tkFileDialog import asksaveasfilename, askopenfilename
import Tkinter, tkFont, tkMessageBox
import math
import os.path

from nltk.parse.chart import *
from nltk import cfg
from nltk import tokenize
from nltk import Tree
from nltk.draw import ShowText, EntryDialog, in_idle
from nltk.draw import MutableOptionMenu
from nltk.draw import ColorizedList, SymbolWidget, CanvasFrame
from nltk.draw.cfg import CFGEditor
from nltk.draw.tree import tree_to_treesegment, TreeSegmentWidget

# Known bug: ChartView doesn't handle edges generated by epsilon
# productions (e.g., [Production: PP -> ]) very well.

#######################################################################
# Edge List
#######################################################################

class EdgeList(ColorizedList):
    ARROW = SymbolWidget.SYMBOLS['rightarrow']
    
    def _init_colortags(self, textwidget, options):
        textwidget.tag_config('terminal', foreground='#006000')
        textwidget.tag_config('arrow', font='symbol', underline='0')
        textwidget.tag_config('dot', foreground = '#000000')
        textwidget.tag_config('nonterminal', foreground='blue',
                              font=('helvetica', -12, 'bold'))

    def _item_repr(self, item):
        contents = []
        contents.append(('%s\t' % item.lhs(), 'nonterminal'))
        contents.append((self.ARROW, 'arrow'))
        for i, elt in enumerate(item.rhs()):
            if i == item.dot():
                contents.append((' *', 'dot'))
            if isinstance(elt, cfg.Nonterminal):
                contents.append((' %s' % elt.symbol(), 'nonterminal'))
            else:
                contents.append((' %r' % elt, 'terminal'))
        if item.is_complete():
            contents.append((' *', 'dot'))
        return contents
    
#######################################################################
# Chart Matrix View
#######################################################################

class ChartMatrixView(object):
    """
    A view of a chart that displays the contents of the corresponding matrix.
    """
    def __init__(self, parent, chart, toplevel=True, title='Chart Matrix',
                 show_numedges=False):
        self._chart = chart
        self._cells = []
        self._marks = []
        
        self._selected_cell = None

        if toplevel:
            self._root = Tkinter.Toplevel(parent)
            self._root.title(title)
            self._root.bind('<Control-q>', self.destroy)
            self._init_quit(self._root)
        else:
            self._root = Tkinter.Frame(parent)

        self._init_matrix(self._root)
        self._init_list(self._root)
        if show_numedges:
            self._init_numedges(self._root)
        else:
            self._numedges_label = None

        self._callbacks = {}
        
        self._num_edges = 0

        self.draw()

    def _init_quit(self, root):
        quit = Tkinter.Button(root, text='Quit', command=self.destroy)
        quit.pack(side='bottom', expand=0, fill='none')

    def _init_matrix(self, root):
        cframe = Tkinter.Frame(root, border=2, relief='sunken')
        cframe.pack(expand=0, fill='none', padx=1, pady=3, side='top')
        self._canvas = Tkinter.Canvas(cframe, width=200, height=200,
                                      background='white')
        self._canvas.pack(expand=0, fill='none')

    def _init_numedges(self, root):
        self._numedges_label = Tkinter.Label(root, text='0 edges')
        self._numedges_label.pack(expand=0, fill='none', side='top')

    def _init_list(self, root):
        self._list = EdgeList(root, [], width=20, height=5)
        self._list.pack(side='top', expand=1, fill='both', pady=3)
        def cb(edge, self=self): self._fire_callbacks('select', edge)
        self._list.add_callback('select', cb)
        self._list.focus()

    def destroy(self, *e):
        if self._root is None: return
        try: self._root.destroy()
        except: pass
        self._root = None

    def set_chart(self, chart):
        if chart is not self._chart:
            self._chart = chart
            self._num_edges = 0
            self.draw()

    def update(self):
        if self._root is None: return

        # Count the edges in each cell
        N = len(self._cells)
        cell_edges = [[0 for i in range(N)] for j in range(N)]
        for edge in self._chart:
            cell_edges[edge.start()][edge.end()] += 1

        # Color the cells correspondingly.
        for i in range(N):
            for j in range(i, N):
                if cell_edges[i][j] == 0:
                    color = 'gray20'
                else:
                    color = ('#00%02x%02x' %
                             (min(255, 50+128*cell_edges[i][j]/10),
                              max(0, 128-128*cell_edges[i][j]/10)))
                cell_tag = self._cells[i][j]
                self._canvas.itemconfig(cell_tag, fill=color)
                if (i,j) == self._selected_cell:
                    self._canvas.itemconfig(cell_tag, outline='#00ffff',
                                            width=3)
                    self._canvas.tag_raise(cell_tag)
                else:
                    self._canvas.itemconfig(cell_tag, outline='black',
                                            width=1)

        # Update the edge list.
        edges = list(self._chart.select(span=self._selected_cell))
        self._list.set(edges)

        # Update our edge count.
        self._num_edges = self._chart.num_edges()
        if self._numedges_label is not None:
            self._numedges_label['text'] = '%d edges' % self._num_edges

    def activate(self):
        self._canvas.itemconfig('inactivebox', state='hidden')
        self.update()

    def inactivate(self):
        self._canvas.itemconfig('inactivebox', state='normal')
        self.update()

    def add_callback(self, event, func):
        self._callbacks.setdefault(event,{})[func] = 1

    def remove_callback(self, event, func=None):
        if func is None: del self._callbacks[event]
        else:
            try: del self._callbacks[event][func]
            except: pass

    def _fire_callbacks(self, event, *args):
        if not self._callbacks.has_key(event): return
        for cb_func in self._callbacks[event].keys(): cb_func(*args)

    def select_cell(self, i, j):
        if self._root is None: return

        # If the cell is already selected (and the chart contents
        # haven't changed), then do nothing.
        if ((i,j) == self._selected_cell and
            self._chart.num_edges() == self._num_edges): return
        
        self._selected_cell = (i,j)
        self.update()

        # Fire the callback.
        self._fire_callbacks('select_cell', i, j)

    def deselect_cell(self):
        if self._root is None: return
        self._selected_cell = None
        self._list.set([])
        self.update()
        
    def _click_cell(self, i, j):
        if self._selected_cell == (i,j):
            self.deselect_cell()
        else:
            self.select_cell(i, j)

    def view_edge(self, edge):
        self.select_cell(*edge.span())
        self._list.view(edge)

    def mark_edge(self, edge):
        if self._root is None: return
        self.select_cell(*edge.span())
        self._list.mark(edge)

    def unmark_edge(self, edge=None):
        if self._root is None: return
        self._list.unmark(edge)

    def markonly_edge(self, edge):
        if self._root is None: return
        self.select_cell(*edge.span())
        self._list.markonly(edge)

    def draw(self):
        if self._root is None: return
        LEFT_MARGIN = BOT_MARGIN = 15
        TOP_MARGIN = 5
        c = self._canvas
        c.delete('all')
        N = self._chart.num_leaves()+1
        dx = (int(c['width'])-LEFT_MARGIN)/N
        dy = (int(c['height'])-TOP_MARGIN-BOT_MARGIN)/N

        c.delete('all')

        # Labels and dotted lines
        for i in range(N):
            c.create_text(LEFT_MARGIN-2, i*dy+dy/2+TOP_MARGIN,
                          text=`i`, anchor='e')
            c.create_text(i*dx+dx/2+LEFT_MARGIN, N*dy+TOP_MARGIN+1,
                          text=`i`, anchor='n')
            c.create_line(LEFT_MARGIN, dy*(i+1)+TOP_MARGIN, 
                          dx*N+LEFT_MARGIN, dy*(i+1)+TOP_MARGIN, dash='.')
            c.create_line(dx*i+LEFT_MARGIN, TOP_MARGIN,
                          dx*i+LEFT_MARGIN, dy*N+TOP_MARGIN, dash='.')

        # A box around the whole thing
        c.create_rectangle(LEFT_MARGIN, TOP_MARGIN,
                           LEFT_MARGIN+dx*N, dy*N+TOP_MARGIN,
                           width=2)

        # Cells
        self._cells = [[None for i in range(N)] for j in range(N)]
        for i in range(N):
            for j in range(i, N):
                t = c.create_rectangle(j*dx+LEFT_MARGIN, i*dy+TOP_MARGIN,
                                       (j+1)*dx+LEFT_MARGIN,
                                       (i+1)*dy+TOP_MARGIN,
                                       fill='gray20')
                self._cells[i][j] = t
                def cb(event, self=self, i=i, j=j): self._click_cell(i,j)
                c.tag_bind(t, '<Button-1>', cb)

        # Inactive box
        xmax, ymax = int(c['width']), int(c['height'])
        t = c.create_rectangle(-100, -100, xmax+100, ymax+100, 
                               fill='gray50', state='hidden',
                               tag='inactivebox')
        c.tag_lower(t)

        # Update the cells.
        self.update()

    def pack(self, *args, **kwargs):
        self._root.pack(*args, **kwargs)
        
#######################################################################
# Chart Results View
#######################################################################

class ChartResultsView(object):
    def __init__(self, parent, chart, grammar, toplevel=True):
        self._chart = chart
        self._grammar = grammar
        self._trees = []
        self._y = 10
        self._treewidgets = []
        self._selection = None
        self._selectbox = None

        if toplevel:
            self._root = Tkinter.Toplevel(parent)
            self._root.title('Chart Parsing Demo: Results')
            self._root.bind('<Control-q>', self.destroy)
        else:
            self._root = Tkinter.Frame(parent)

        # Buttons
        if toplevel:
            buttons = Tkinter.Frame(self._root)
            buttons.pack(side='bottom', expand=0, fill='x')
            Tkinter.Button(buttons, text='Quit',
                           command=self.destroy).pack(side='right')
            Tkinter.Button(buttons, text='Print All',
                           command=self.print_all).pack(side='left')
            Tkinter.Button(buttons, text='Print Selection',
                           command=self.print_selection).pack(side='left')

        # Canvas frame.
        self._cframe = CanvasFrame(self._root, closeenough=20)
        self._cframe.pack(side='top', expand=1, fill='both')

        # Initial update
        self.update()

    def update(self, edge=None):
        if self._root is None: return
        # If the edge isn't a parse edge, do nothing.
        if edge is not None:
            if edge.lhs() != self._grammar.start(): return
            if edge.span() != (0, self._chart.num_leaves()): return
        
        for parse in self._chart.parses(self._grammar.start()):
            if parse not in self._trees:
                self._add(parse)

    def _add(self, parse):
        # Add it to self._trees.
        self._trees.append(parse)

        # Create a widget for it.
        c = self._cframe.canvas()
        treewidget = tree_to_treesegment(c, parse)

        # Add it to the canvas frame.
        self._treewidgets.append(treewidget)
        self._cframe.add_widget(treewidget, 10, self._y)

        # Register callbacks.
        treewidget.bind_click(self._click)

        # Update y.
        self._y = treewidget.bbox()[3] + 10

    def _click(self, widget):
        c = self._cframe.canvas()
        if self._selection is not None:
            c.delete(self._selectbox)
        self._selection = widget
        (x1, y1, x2, y2) = widget.bbox()
        self._selectbox = c.create_rectangle(x1, y1, x2, y2,
                                             width=2, outline='#088')

    def _color(self, treewidget, color):
        treewidget.node()['color'] = color
        for child in treewidget.subtrees():
            if isinstance(child, TreeSegmentWidget):
                self._color(child, color)
            else:
                child['color'] = color

    def print_all(self, *e):
        if self._root is None: return
        self._cframe.print_to_file()

    def print_selection(self, *e):
        if self._root is None: return
        if self._selection is None:
            tkMessageBox.showerror('Print Error', 'No tree selected')
        else:
            c = self._cframe.canvas()
            for widget in self._treewidgets:
                if widget is not self._selection:
                    self._cframe.destroy_widget(widget)
            c.delete(self._selectbox)
            (x1,y1,x2,y2) = self._selection.bbox()
            self._selection.move(10-x1,10-y1)
            c['scrollregion'] = '0 0 %s %s' % (x2-x1+20, y2-y1+20)
            self._cframe.print_to_file()

            # Restore our state.
            self._treewidgets = [self._selection]
            self.clear()
            self.update()

    def clear(self):
        if self._root is None: return
        for treewidget in self._treewidgets:
            self._cframe.destroy_widget(treewidget)
        self._trees = []
        self._treewidgets = []
        if self._selection is not None:
            self._cframe.canvas().delete(self._selectbox)
        self._selection = None
        self._y = 10

    def set_chart(self, chart):
        self.clear()
        self._chart = chart
        self.update()

    def set_grammar(self, grammar):
        self.clear()
        self._grammar = grammar
        self.update()

    def destroy(self, *e):
        if self._root is None: return
        try: self._root.destroy()
        except: pass
        self._root = None

    def pack(self, *args, **kwargs):
        self._root.pack(*args, **kwargs)

#######################################################################
# Chart Comparer
#######################################################################

class ChartComparer(object):
    """

    @ivar _root: The root window

    @ivar _charts: A dictionary mapping names to charts.  When 
        charts are loaded, they are added to this dictionary.
    
    @ivar _left_chart: The left L{Chart}.
    @ivar _left_name: The name C{_left_chart} (derived from filename)
    @ivar _left_matrix: The L{ChartMatrixView} for C{_left_chart}
    @ivar _left_selector: The drop-down L{MutableOptionsMenu} used
          to select C{_left_chart}.
    
    @ivar _right_chart: The right L{Chart}.
    @ivar _right_name: The name C{_right_chart} (derived from filename)
    @ivar _right_matrix: The L{ChartMatrixView} for C{_right_chart}
    @ivar _right_selector: The drop-down L{MutableOptionsMenu} used
          to select C{_right_chart}.
    
    @ivar _out_chart: The out L{Chart}.
    @ivar _out_name: The name C{_out_chart} (derived from filename)
    @ivar _out_matrix: The L{ChartMatrixView} for C{_out_chart}
    @ivar _out_label: The label for C{_out_chart}.

    @ivar _op_label: A Label containing the most recent operation.
    """

    _OPSYMBOL = {'-': '-',
                 'and': SymbolWidget.SYMBOLS['intersection'],
                 'or': SymbolWidget.SYMBOLS['union']}
    
    def __init__(self, *chart_filenames):
        # This chart is displayed when we don't have a value (eg
        # before any chart is loaded).
        faketok = [''] * 8
        self._emptychart = Chart(faketok)

        # The left & right charts start out empty.
        self._left_name = 'None'
        self._right_name = 'None'
        self._left_chart = self._emptychart
        self._right_chart = self._emptychart
            
        # The charts that have been loaded.
        self._charts = {'None': self._emptychart}

        # The output chart.
        self._out_chart = self._emptychart

        # The most recent operation
        self._operator = None

        # Set up the root window.
        self._root = Tkinter.Tk()
        self._root.title('Chart Comparison')
        self._root.bind('<Control-q>', self.destroy)
        self._root.bind('<Control-x>', self.destroy)

        # Initialize all widgets, etc.
        self._init_menubar(self._root)
        self._init_chartviews(self._root)
        self._init_divider(self._root)
        self._init_buttons(self._root)
        self._init_bindings(self._root)

        # Load any specified charts.
        for filename in chart_filenames:
            self.load_chart(filename)

    def destroy(self, *e):
        if self._root is None: return
        try: self._root.destroy()
        except: pass
        self._root = None

    def mainloop(self, *args, **kwargs):
        return
        self._root.mainloop(*args, **kwargs)

    #////////////////////////////////////////////////////////////
    # Initialization
    #////////////////////////////////////////////////////////////

    def _init_menubar(self, root):
        menubar = Tkinter.Menu(root)

        # File menu
        filemenu = Tkinter.Menu(menubar, tearoff=0)
        filemenu.add_command(label='Load Chart', accelerator='Ctrl-o',
                             underline=0, command=self.load_chart_dialog)
        filemenu.add_command(label='Save Output', accelerator='Ctrl-s',
                             underline=0, command=self.save_chart_dialog)
        filemenu.add_separator()
        filemenu.add_command(label='Exit', underline=1,
                             command=self.destroy, accelerator='Ctrl-x')
        menubar.add_cascade(label='File', underline=0, menu=filemenu)

        # Compare menu
        opmenu = Tkinter.Menu(menubar, tearoff=0)
        opmenu.add_command(label='Intersection',
                           command=self._intersection,
                           accelerator='+')
        opmenu.add_command(label='Union',
                           command=self._union,
                           accelerator='*')
        opmenu.add_command(label='Difference',
                           command=self._difference,
                           accelerator='-')
        opmenu.add_separator()
        opmenu.add_command(label='Swap Charts',
                           command=self._swapcharts)
        menubar.add_cascade(label='Compare', underline=0, menu=opmenu)

        # Add the menu
        self._root.config(menu=menubar)

    def _init_divider(self, root):
        divider = Tkinter.Frame(root, border=2, relief='sunken')
        divider.pack(side='top', fill='x', ipady=2)
        
    def _init_chartviews(self, root):
        opfont=('symbol', -36) # Font for operator.
        eqfont=('helvetica', -36) # Font for equals sign.
        
        frame = Tkinter.Frame(root, background='#c0c0c0')
        frame.pack(side='top', expand=1, fill='both')

        # The left matrix.
        cv1_frame = Tkinter.Frame(frame, border=3, relief='groove')
        cv1_frame.pack(side='left', padx=8, pady=7, expand=1, fill='both')
        self._left_selector = MutableOptionMenu(
            cv1_frame, self._charts.keys(), command=self._select_left)
        self._left_selector.pack(side='top', pady=5, fill='x')
        self._left_matrix = ChartMatrixView(cv1_frame, self._emptychart,
                                            toplevel=False,
                                            show_numedges=True)
        self._left_matrix.pack(side='bottom', padx=5, pady=5,
                               expand=1, fill='both')
        self._left_matrix.add_callback('select', self.select_edge)
        self._left_matrix.add_callback('select_cell', self.select_cell)
        self._left_matrix.inactivate()

        # The operator.
        self._op_label = Tkinter.Label(frame, text=' ', width=3,
                                       background='#c0c0c0', font=opfont)
        self._op_label.pack(side='left', padx=5, pady=5)

        # The right matrix.
        cv2_frame = Tkinter.Frame(frame, border=3, relief='groove')
        cv2_frame.pack(side='left', padx=8, pady=7, expand=1, fill='both')
        self._right_selector = MutableOptionMenu(
            cv2_frame, self._charts.keys(), command=self._select_right)
        self._right_selector.pack(side='top', pady=5, fill='x')
        self._right_matrix = ChartMatrixView(cv2_frame, self._emptychart,
                                            toplevel=False,
                                            show_numedges=True)
        self._right_matrix.pack(side='bottom', padx=5, pady=5,
                               expand=1, fill='both')
        self._right_matrix.add_callback('select', self.select_edge)
        self._right_matrix.add_callback('select_cell', self.select_cell)
        self._right_matrix.inactivate()

        # The equals sign
        Tkinter.Label(frame, text='=', width=3, background='#c0c0c0',
                      font=eqfont).pack(side='left', padx=5, pady=5)
                                        
        # The output matrix.
        out_frame = Tkinter.Frame(frame, border=3, relief='groove')
        out_frame.pack(side='left', padx=8, pady=7, expand=1, fill='both')
        self._out_label = Tkinter.Label(out_frame, text='Output')
        self._out_label.pack(side='top', pady=9)
        self._out_matrix = ChartMatrixView(out_frame, self._emptychart,
                                            toplevel=False,
                                            show_numedges=True)
        self._out_matrix.pack(side='bottom', padx=5, pady=5,
                                 expand=1, fill='both')
        self._out_matrix.add_callback('select', self.select_edge)
        self._out_matrix.add_callback('select_cell', self.select_cell)
        self._out_matrix.inactivate()
                                      
    def _init_buttons(self, root):
        buttons = Tkinter.Frame(root)
        buttons.pack(side='bottom', pady=5, fill='x', expand=0)
        Tkinter.Button(buttons, text='Intersection',
                       command=self._intersection).pack(side='left')
        Tkinter.Button(buttons, text='Union',
                       command=self._union).pack(side='left')
        Tkinter.Button(buttons, text='Difference',
                       command=self._difference).pack(side='left')
        Tkinter.Frame(buttons, width=20).pack(side='left')
        Tkinter.Button(buttons, text='Swap Charts',
                       command=self._swapcharts).pack(side='left')

        Tkinter.Button(buttons, text='Detatch Output',
                       command=self._detatch_out).pack(side='right')
        
    def _init_bindings(self, root):
        #root.bind('<Control-s>', self.save_chart)
        root.bind('<Control-o>', self.load_chart_dialog)
        #root.bind('<Control-r>', self.reset)

    #////////////////////////////////////////////////////////////
    # Input Handling
    #////////////////////////////////////////////////////////////

    def _select_left(self, name):
        self._left_name = name
        self._left_chart = self._charts[name]
        self._left_matrix.set_chart(self._left_chart)
        if name == 'None': self._left_matrix.inactivate()
        self._apply_op()

    def _select_right(self, name):
        self._right_name = name
        self._right_chart = self._charts[name]
        self._right_matrix.set_chart(self._right_chart)
        if name == 'None': self._right_matrix.inactivate()
        self._apply_op()

    def _apply_op(self):
        if self._operator == '-': self._difference()
        elif self._operator == 'or': self._union()
        elif self._operator == 'and': self._intersection()
        

    #////////////////////////////////////////////////////////////
    # File
    #////////////////////////////////////////////////////////////
    CHART_FILE_TYPES = [('Pickle file', '.pickle'),
                        ('All files', '*')]

    def save_chart_dialog(self, *args):
        filename = asksaveasfilename(filetypes=self.CHART_FILE_TYPES,
                                     defaultextension='.pickle')
        if not filename: return
        try: pickle.dump((self._out_chart), open(filename, 'w'))
        except Exception, e:
            tkMessageBox.showerror('Error Saving Chart', 
                                   'Unable to open file: %r\n%s' %
                                   (filename, e))
    
    def load_chart_dialog(self, *args):
        filename = askopenfilename(filetypes=self.CHART_FILE_TYPES,
                                   defaultextension='.pickle')
        if not filename: return
        try: self.load_chart(filename)
        except Exception, e:
            tkMessageBox.showerror('Error Loading Chart',
                                   'Unable to open file: %r\n%s' %
                                   (filename, e))

    def load_chart(self, filename):
        chart = pickle.load(open(filename, 'r'))
        name = os.path.basename(filename)
        if name.endswith('.pickle'): name = name[:-7]
        if name.endswith('.chart'): name = name[:-6]
        self._charts[name] = chart
        self._left_selector.add(name)
        self._right_selector.add(name)

        # If either left_matrix or right_matrix is empty, then
        # display the new chart.
        if self._left_chart is self._emptychart:
            self._left_selector.set(name)
        elif self._right_chart is self._emptychart:
            self._right_selector.set(name)
        
    def _update_chartviews(self):
        self._left_matrix.update()
        self._right_matrix.update()
        self._out_matrix.update()
        
    #////////////////////////////////////////////////////////////
    # Selection
    #////////////////////////////////////////////////////////////

    def select_edge(self, edge):
        if edge in self._left_chart:
            self._left_matrix.markonly_edge(edge)
        else:
            self._left_matrix.unmark_edge()
        if edge in self._right_chart:
            self._right_matrix.markonly_edge(edge)
        else:
            self._right_matrix.unmark_edge()
        if edge in self._out_chart:
            self._out_matrix.markonly_edge(edge)
        else:
            self._out_matrix.unmark_edge()

    def select_cell(self, i, j):
        self._left_matrix.select_cell(i, j)
        self._right_matrix.select_cell(i, j)
        self._out_matrix.select_cell(i, j)

    #////////////////////////////////////////////////////////////
    # Operations
    #////////////////////////////////////////////////////////////

    def _difference(self):
        if not self._checkcompat(): return

        out_chart = Chart(self._left_chart.tokens())
        for edge in self._left_chart:
            if edge not in self._right_chart:
                out_chart.insert(edge, [])

        self._update('-', out_chart)

    def _intersection(self):
        if not self._checkcompat(): return

        out_chart = Chart(self._left_chart.tokens())
        for edge in self._left_chart:
            if edge in self._right_chart:
                out_chart.insert(edge, [])
                    
        self._update('and', out_chart)

    def _union(self):
        if not self._checkcompat(): return

        out_chart = Chart(self._left_chart.tokens())
        for edge in self._left_chart:
            out_chart.insert(edge, [])
        for edge in self._right_chart:
            out_chart.insert(edge, [])
    
        self._update('or', out_chart)

    def _swapcharts(self):
        left, right = self._left_name, self._right_name
        self._left_selector.set(right)
        self._right_selector.set(left)

    def _checkcompat(self):
        if (self._left_chart.tokens() != self._right_chart.tokens() or
            self._left_chart.property_names() !=
            self._right_chart.property_names() or
            self._left_chart == self._emptychart or
            self._right_chart == self._emptychart):
            # Clear & inactivate the output chart.
            self._out_chart = self._emptychart
            self._out_matrix.set_chart(self._out_chart)
            self._out_matrix.inactivate()
            self._out_label['text'] = 'Output'
            # Issue some other warning?
            return False
        else:
            return True

    def _update(self, operator, out_chart):
        self._operator = operator
        self._op_label['text'] = self._OPSYMBOL[operator]
        self._out_chart = out_chart
        self._out_matrix.set_chart(out_chart)
        self._out_label['text'] = '%s %s %s' % (self._left_name,
                                                self._operator,
                                                self._right_name)
            
    def _clear_out_chart(self):
        self._out_chart = self._emptychart
        self._out_matrix.set_chart(self._out_chart)
        self._op_label['text'] = ' '
        self._out_matrix.inactivate()

    def _detatch_out(self):
        ChartMatrixView(self._root, self._out_chart,
                        title=self._out_label['text'])

                
        

            



#######################################################################
# Chart View
#######################################################################

class ChartView(object):
    """
    A component for viewing charts.  This is used by C{ChartDemo} to
    allow students to interactively experiment with various chart
    parsing techniques.  It is also used by C{Chart.draw()}.

    @ivar _chart: The chart that we are giving a view of.  This chart
       may be modified; after it is modified, you should call
       C{update}.
    @ivar _sentence: The list of tokens that the chart spans.

    @ivar _root: The root window.
    @ivar _chart_canvas: The canvas we're using to display the chart
        itself. 
    @ivar _tree_canvas: The canvas we're using to display the tree
        that each edge spans.  May be None, if we're not displaying
        trees. 
    @ivar _sentence_canvas: The canvas we're using to display the sentence
        text.  May be None, if we're not displaying the sentence text.
    @ivar _edgetags: A dictionary mapping from edges to the tags of
        the canvas elements (lines, etc) used to display that edge.
        The values of this dictionary have the form 
        C{(linetag, rhstag1, dottag, rhstag2, lhstag)}.
    @ivar _treetags: A list of all the tags that make up the tree;
        used to erase the tree (without erasing the loclines).
    @ivar _chart_height: The height of the chart canvas.
    @ivar _sentence_height: The height of the sentence canvas.
    @ivar _tree_height: The height of the tree

    @ivar _text_height: The height of a text string (in the normal
        font). 

    @ivar _edgelevels: A list of edges at each level of the chart (the
        top level is the 0th element).  This list is used to remember
        where edges should be drawn; and to make sure that no edges
        are overlapping on the chart view.

    @ivar _unitsize: Pixel size of one unit (from the location).  This
       is determined by the span of the chart's location, and the
       width of the chart display canvas.

    @ivar _fontsize: The current font size

    @ivar _marks: A dictionary from edges to marks.  Marks are
        strings, specifying colors (e.g. 'green').
    """
    
    _LEAF_SPACING = 10
    _MARGIN = 10
    _TREE_LEVEL_SIZE = 12
    _CHART_LEVEL_SIZE = 40
    
    def __init__(self, chart, root=None, **kw):
        """
        Construct a new C{Chart} display.
        """
        # Process keyword args.
        draw_tree = kw.get('draw_tree', 0)
        draw_sentence = kw.get('draw_sentence', 1)
        self._fontsize = kw.get('fontsize', -12)

        # The chart!
        self._chart = chart

        # Callback functions
        self._callbacks = {}

        # Keep track of drawn edges
        self._edgelevels = []
        self._edgetags = {}

        # Keep track of which edges are marked.
        self._marks = {}

        # These are used to keep track of the set of tree tokens
        # currently displayed in the tree canvas.
        self._treetoks = []
        self._treetoks_edge = None
        self._treetoks_index = 0

        # Keep track of the tags used to draw the tree
        self._tree_tags = []

        # Put multiple edges on each level?
        self._compact = 0

        # If they didn't provide a main window, then set one up.
        if root is None:
            top = Tkinter.Tk()
            top.title('Chart View')
            def destroy1(e, top=top): top.destroy()
            def destroy2(top=top): top.destroy()
            top.bind('q', destroy1)
            b = Tkinter.Button(top, text='Done', command=destroy2)
            b.pack(side='bottom')
            self._root = top
        else:
            self._root = root

        # Create some fonts.
        self._init_fonts(root)
        self._init_function = None

        # Create the chart canvas.
        (self._chart_sb, self._chart_canvas) = self._sb_canvas(self._root)
        self._chart_canvas['height'] = 300
        self._chart_canvas['closeenough'] = 15

        # Create the sentence canvas.
        if draw_sentence:
            cframe = Tkinter.Frame(self._root, relief='sunk', border=2)
            cframe.pack(fill='both', side='bottom')
            self._sentence_canvas = Tkinter.Canvas(cframe, height=50)
            self._sentence_canvas['background'] = '#e0e0e0'
            self._sentence_canvas.pack(fill='both')
            #self._sentence_canvas['height'] = self._sentence_height
        else:
            self._sentence_canvas = None

        # Create the tree canvas.
        if draw_tree:
            (sb, canvas) = self._sb_canvas(self._root, 'n', 'x')
            (self._tree_sb, self._tree_canvas) = (sb, canvas)
            self._tree_canvas['height'] = 200
        else:
            self._tree_canvas = None

        # Do some analysis to figure out how big the window should be
        self._analyze()
        self.draw()
        self._resize()
        self._grow()

        # Set up the configure callback, which will be called whenever
        # the window is resized.
        self._chart_canvas.bind('<Configure>', self._configure)
        

    def _init_fonts(self, root):
        self._boldfont = tkFont.Font(family='helvetica', weight='bold',
                                    size=self._fontsize)
        self._font = tkFont.Font(family='helvetica',
                                    size=self._fontsize)
        # See: <http://www.astro.washington.edu/owen/ROTKFolklore.html>
        self._sysfont = tkFont.Font(font=Tkinter.Button()["font"])
        root.option_add("*Font", self._sysfont)

    def _sb_canvas(self, root, expand='y', 
                   fill='both', side='bottom'):
        """
        Helper for __init__: construct a canvas with a scrollbar.
        """
        cframe =Tkinter.Frame(root, relief='sunk', border=2)
        cframe.pack(fill=fill, expand=expand, side=side)
        canvas = Tkinter.Canvas(cframe, background='#e0e0e0')
        
        # Give the canvas a scrollbar.
        sb = Tkinter.Scrollbar(cframe, orient='vertical')
        sb.pack(side='right', fill='y')
        canvas.pack(side='left', fill=fill, expand='yes')

        # Connect the scrollbars to the canvas.
        sb['command']= canvas.yview
        canvas['yscrollcommand'] = sb.set

        return (sb, canvas)

    def scroll_up(self, *e):
        self._chart_canvas.yview('scroll', -1, 'units')

    def scroll_down(self, *e):
        self._chart_canvas.yview('scroll', 1, 'units')

    def page_up(self, *e):
        self._chart_canvas.yview('scroll', -1, 'pages')

    def page_down(self, *e):
        self._chart_canvas.yview('scroll', 1, 'pages')

    def _grow(self):
        """
        Grow the window, if necessary
        """
        # Grow, if need-be
        N = self._chart.num_leaves()
        width = max(int(self._chart_canvas['width']),
                    N * self._unitsize + ChartView._MARGIN * 2 )

        # It won't resize without the second (height) line, but I
        # don't understand why not.
        self._chart_canvas.configure(width=width)
        self._chart_canvas.configure(height=self._chart_canvas['height'])
        
        self._unitsize = (width - 2*ChartView._MARGIN) / N

        # Reset the height for the sentence window.
        if self._sentence_canvas is not None:
            self._sentence_canvas['height'] = self._sentence_height
        
    def set_font_size(self, size):
        self._font.configure(size=-abs(size))
        self._boldfont.configure(size=-abs(size))
        self._sysfont.configure(size=-abs(size))
        self._analyze()
        self._grow()
        self.draw()
            
    def get_font_size(self):
        return abs(self._fontsize)
            
    def _configure(self, e):
        """
        The configure callback.  This is called whenever the window is
        resized.  It is also called when the window is first mapped.
        It figures out the unit size, and redraws the contents of each
        canvas.
        """
        N = self._chart.num_leaves()
        self._unitsize = (e.width - 2*ChartView._MARGIN) / N
        self.draw()

    def update(self, chart=None):
        """
        Draw any edges that have not been drawn.  This is typically
        called when a after modifies the canvas that a CanvasView is
        displaying.  C{update} will cause any edges that have been
        added to the chart to be drawn.

        If update is given a C{chart} argument, then it will replace
        the current chart with the given chart.
        """
        if chart is not None:
            self._chart = chart
            self._edgelevels = []
            self._marks = {}
            self._analyze()
            self._grow()
            self.draw()
            self.erase_tree()
            self._resize()
        else:
            for edge in self._chart:
                if not self._edgetags.has_key(edge):
                    self._add_edge(edge)
            self._resize()


    def _edge_conflict(self, edge, lvl):
        """
        Return 1 if the given edge overlaps with any edge on the given
        level.  This is used by _add_edge to figure out what level a
        new edge should be added to.
        """
        (s1, e1) = edge.span()
        for otheredge in self._edgelevels[lvl]:
            (s2, e2) = otheredge.span()
            if (s1 <= s2 < e1) or (s2 <= s1 < e2) or (s1==s2==e1==e2):
                return 1
        return 0

    def _analyze_edge(self, edge):
        """
        Given a new edge, recalculate:

            - _text_height
            - _unitsize (if the edge text is too big for the current
              _unitsize, then increase _unitsize)
        """
        c = self._chart_canvas

        if isinstance(edge, TreeEdge):
            lhs = edge.lhs()
            rhselts = []
            for elt in edge.rhs():
                if isinstance(elt, cfg.Nonterminal):
                    rhselts.append(str(elt.symbol()))
                else:
                    rhselts.append(repr(elt))
            rhs = ' '.join(rhselts)
        else:
            lhs = edge.lhs()
            rhs = ''
            
        for s in (lhs, rhs):
            tag = c.create_text(0,0, text=s,
                                font=self._boldfont,
                                anchor='nw', justify='left')
            bbox = c.bbox(tag)
            c.delete(tag)
            width = bbox[2] #+ ChartView._LEAF_SPACING
            edgelen = max(edge.length(), 1)
            self._unitsize = max(self._unitsize, width/edgelen)
            self._text_height = max(self._text_height, bbox[3] - bbox[1])
    
    def _add_edge(self, edge, minlvl=0):
        """
        Add a single edge to the ChartView:

            - Call analyze_edge to recalculate display parameters
            - Find an available level
            - Call _draw_edge
        """
        if self._edgetags.has_key(edge): return
        self._analyze_edge(edge)
        self._grow()

        if not self._compact:
            self._edgelevels.append([edge])
            lvl = len(self._edgelevels)-1
            self._draw_edge(edge, lvl)
            self._resize()
            return

        # Figure out what level to draw the edge on.
        lvl = 0
        while 1:
            # If this level doesn't exist yet, create it.
            while lvl >= len(self._edgelevels):
                self._edgelevels.append([])
                self._resize()

            # Check if we can fit the edge in this level.
            if lvl>=minlvl and not self._edge_conflict(edge, lvl):
                # Go ahead and draw it.
                self._edgelevels[lvl].append(edge)
                break

            # Try the next level.
            lvl += 1

        self._draw_edge(edge, lvl)

    def view_edge(self, edge):
        level = None
        for i in range(len(self._edgelevels)):
            if edge in self._edgelevels[i]:
                level = i
                break
        if level == None: return
        # Try to view the new edge..
        y = (level+1) * self._chart_level_size
        dy = self._text_height + 10
        self._chart_canvas.yview('moveto', 1.0)
        if self._chart_height != 0:
            self._chart_canvas.yview('moveto',
                                     float(y-dy)/self._chart_height)

    def _draw_edge(self, edge, lvl):
        """
        Draw a single edge on the ChartView.
        """
        c = self._chart_canvas
        
        # Draw the arrow.
        x1 = (edge.start() * self._unitsize + ChartView._MARGIN)
        x2 = (edge.end() * self._unitsize + ChartView._MARGIN)
        if x2 == x1: x2 += max(4, self._unitsize/5)
        y = (lvl+1) * self._chart_level_size
        linetag = c.create_line(x1, y, x2, y, arrow='last', width=3)

        # Draw a label for the edge.
        if isinstance(edge, TreeEdge):
            rhs = []
            for elt in edge.rhs():
                if isinstance(elt, cfg.Nonterminal):
                    rhs.append(str(elt.symbol()))
                else:
                    rhs.append(repr(elt))
            pos = edge.dot()
        else:
            rhs = []
            pos = 0
            
        rhs1 = ' '.join(rhs[:pos])
        rhs2 = ' '.join(rhs[pos:])
        rhstag1 = c.create_text(x1+3, y, text=rhs1,
                                font=self._font,
                                anchor='nw')
        dotx = c.bbox(rhstag1)[2] + 6
        doty = (c.bbox(rhstag1)[1]+c.bbox(rhstag1)[3])/2
        dottag = c.create_oval(dotx-2, doty-2, dotx+2, doty+2)
        rhstag2 = c.create_text(dotx+6, y, text=rhs2,
                                font=self._font,
                                anchor='nw')
        lhstag =  c.create_text((x1+x2)/2, y, text=str(edge.lhs()),
                                anchor='s',
                                font=self._boldfont)

        # Keep track of the edge's tags.
        self._edgetags[edge] = (linetag, rhstag1,
                                dottag, rhstag2, lhstag)

        # Register a callback for clicking on the edge.
        def cb(event, self=self, edge=edge):
            self._fire_callbacks('select', edge)
        c.tag_bind(rhstag1, '<Button-1>', cb)
        c.tag_bind(rhstag2, '<Button-1>', cb)
        c.tag_bind(linetag, '<Button-1>', cb)
        c.tag_bind(dottag, '<Button-1>', cb)
        c.tag_bind(lhstag, '<Button-1>', cb)

        self._color_edge(edge)
        
    def _color_edge(self, edge, linecolor=None, textcolor=None):
        """
        Color in an edge with the given colors.
        If no colors are specified, use intelligent defaults
        (dependant on selection, etc.)
        """
        if not self._edgetags.has_key(edge): return
        c = self._chart_canvas

        if linecolor is not None and textcolor is not None:
            if self._marks.has_key(edge):
                linecolor = self._marks[edge]
            tags = self._edgetags[edge]
            c.itemconfig(tags[0], fill=linecolor)
            c.itemconfig(tags[1], fill=textcolor)
            c.itemconfig(tags[2], fill=textcolor,
                         outline=textcolor)
            c.itemconfig(tags[3], fill=textcolor)
            c.itemconfig(tags[4], fill=textcolor)
            return
        else:
            N = self._chart.num_leaves()
            if self._marks.has_key(edge):
                self._color_edge(self._marks[edge])
            if (edge.is_complete() and edge.span() == (0, N)):
                self._color_edge(edge, '#084', '#042')
            elif isinstance(edge, LeafEdge):
                self._color_edge(edge, '#48c', '#246')
            else:
                self._color_edge(edge, '#00f', '#008')

    def mark_edge(self, edge, mark='#0df'):
        """
        Mark an edge
        """
        self._marks[edge] = mark
        self._color_edge(edge)

    def unmark_edge(self, edge=None):
        """
        Unmark an edge (or all edges)
        """
        if edge == None:
            old_marked_edges = self._marks.keys()
            self._marks = {}
            for edge in old_marked_edges:
                self._color_edge(edge)
        else:
            del self._marks[edge]
            self._color_edge(edge)

    def markonly_edge(self, edge, mark='#0df'):
        self.unmark_edge()
        self.mark_edge(edge, mark)

    def _analyze(self):
        """
        Analyze the sentence string, to figure out how big a unit needs 
        to be, How big the tree should be, etc.
        """
        # Figure out the text height and the unit size.
        unitsize = 70 # min unitsize
        text_height = 0
        c = self._chart_canvas

        # Check against all tokens
        for leaf in self._chart.leaves():
            tag = c.create_text(0,0, text=repr(leaf),
                                font=self._font,
                                anchor='nw', justify='left')
            bbox = c.bbox(tag)
            c.delete(tag)
            width = bbox[2] + ChartView._LEAF_SPACING
            unitsize = max(width, unitsize)
            text_height = max(text_height, bbox[3] - bbox[1])

        self._unitsize = unitsize
        self._text_height = text_height
        self._sentence_height = (self._text_height +
                               2*ChartView._MARGIN)

        # Check against edges.
        for edge in self._chart.edges():
            self._analyze_edge(edge)

        # Size of chart levels
        self._chart_level_size = self._text_height * 2.5
        
        # Default tree size..
        self._tree_height = (3 * (ChartView._TREE_LEVEL_SIZE +
                                  self._text_height))

        # Resize the scrollregions.
        self._resize()

    def _resize(self):
        """
        Update the scroll-regions for each canvas.  This ensures that
        everything is within a scroll-region, so the user can use the
        scrollbars to view the entire display.  This does I{not}
        resize the window.
        """
        c = self._chart_canvas

        # Reset the chart scroll region
        width = ( self._chart.num_leaves() * self._unitsize +
                  ChartView._MARGIN * 2 )

        levels = len(self._edgelevels)
        self._chart_height = (levels+2)*self._chart_level_size
        c['scrollregion']=(0,0,width,self._chart_height)

        # Reset the tree scroll region
        if self._tree_canvas:
            self._tree_canvas['scrollregion'] = (0, 0, width,
                                                 self._tree_height)
            
    def _draw_loclines(self):
        """
        Draw location lines.  These are vertical gridlines used to
        show where each location unit is.
        """
        BOTTOM = 50000
        c1 = self._tree_canvas
        c2 = self._sentence_canvas
        c3 = self._chart_canvas
        margin = ChartView._MARGIN
        self._loclines = []
        for i in range(0, self._chart.num_leaves()+1):
            x = i*self._unitsize + margin

            if c1:
                t1=c1.create_line(x, 0, x, BOTTOM)
                c1.tag_lower(t1)
            if c2:
                t2=c2.create_line(x, 0, x, self._sentence_height)
                c2.tag_lower(t2)
            t3=c3.create_line(x, 0, x, BOTTOM)
            c3.tag_lower(t3)
            t4=c3.create_text(x+2, 0, text=`i`, anchor='nw',
                              font=self._font)
            c3.tag_lower(t4)
            #if i % 4 == 0:
            #    if c1: c1.itemconfig(t1, width=2, fill='gray60')
            #    if c2: c2.itemconfig(t2, width=2, fill='gray60')
            #    c3.itemconfig(t3, width=2, fill='gray60')
            if i % 2 == 0:
                if c1: c1.itemconfig(t1, fill='gray60')
                if c2: c2.itemconfig(t2, fill='gray60')
                c3.itemconfig(t3, fill='gray60')
            else:
                if c1: c1.itemconfig(t1, fill='gray80')
                if c2: c2.itemconfig(t2, fill='gray80')
                c3.itemconfig(t3, fill='gray80')

    def _draw_sentence(self):
        """Draw the sentence string."""
        if self._chart.num_leaves() == 0: return
        c = self._sentence_canvas
        margin = ChartView._MARGIN
        y = ChartView._MARGIN
        
        for i, leaf in enumerate(self._chart.leaves()):
            x1 = i * self._unitsize + margin
            x2 = x1 + self._unitsize
            x = (x1+x2)/2
            tag = c.create_text(x, y, text=repr(leaf),
                                font=self._font,
                                anchor='n', justify='left')
            bbox = c.bbox(tag)
            rt=c.create_rectangle(x1+2, bbox[1]-(ChartView._LEAF_SPACING/2),
                                  x2-2, bbox[3]+(ChartView._LEAF_SPACING/2),
                                  fill='#f0f0f0', outline='#f0f0f0')
            c.tag_lower(rt)

    def erase_tree(self):
        for tag in self._tree_tags: self._tree_canvas.delete(tag)
        self._treetoks = []
        self._treetoks_edge = None
        self._treetoks_index = 0

    def draw_tree(self, edge=None):
        if edge is None and self._treetoks_edge is None: return
        if edge is None: edge = self._treetoks_edge
        
        # If it's a new edge, then get a new list of treetoks.
        if self._treetoks_edge != edge:
            self._treetoks = [t for t in self._chart.trees(edge)
                              if isinstance(t, Tree)]
            self._treetoks_edge = edge
            self._treetoks_index = 0

        # Make sure there's something to draw.
        if len(self._treetoks) == 0: return

        # Erase the old tree.
        for tag in self._tree_tags: self._tree_canvas.delete(tag)

        # Draw the new tree.
        tree = self._treetoks[self._treetoks_index]
        self._draw_treetok(tree, edge.start())

        # Show how many trees are available for the edge.
        self._draw_treecycle()

        # Update the scroll region.
        w = self._chart.num_leaves()*self._unitsize+2*ChartView._MARGIN
        h = tree.height() * (ChartView._TREE_LEVEL_SIZE+self._text_height)
        self._tree_canvas['scrollregion'] = (0, 0, w, h)

    def cycle_tree(self):
        self._treetoks_index = (self._treetoks_index+1)%len(self._treetoks)
        self.draw_tree(self._treetoks_edge)

    def _draw_treecycle(self):
        if len(self._treetoks) <= 1: return

        # Draw the label.
        label = '%d Trees' % len(self._treetoks)
        c = self._tree_canvas
        margin = ChartView._MARGIN
        right = self._chart.num_leaves()*self._unitsize+margin-2
        tag = c.create_text(right, 2, anchor='ne', text=label,
                            font=self._boldfont)
        self._tree_tags.append(tag)
        _, _, _, y = c.bbox(tag)

        # Draw the triangles.
        for i in range(len(self._treetoks)):
            x = right - 20*(len(self._treetoks)-i-1)
            if i == self._treetoks_index: fill = '#084'
            else: fill = '#fff'
            tag = c.create_polygon(x, y+10, x-5, y, x-10, y+10,
                             fill=fill, outline='black')
            self._tree_tags.append(tag)

            # Set up a callback: show the tree if they click on its
            # triangle.
            def cb(event, self=self, i=i):
                self._treetoks_index = i
                self.draw_tree()
            c.tag_bind(tag, '<Button-1>', cb)

    def _draw_treetok(self, treetok, index, depth=0):
        """
        @param index: The index of the first leaf in the tree.
        @return: The index of the first leaf after the tree.
        """
        c = self._tree_canvas
        margin = ChartView._MARGIN

        # Draw the children
        child_xs = []
        for child in treetok:
            if isinstance(child, Tree):
                child_x, index = self._draw_treetok(child, index, depth+1)
                child_xs.append(child_x)
            else:
                child_xs.append((2*index+1)*self._unitsize/2 + margin)
                index += 1

        # If we have children, then get the node's x by averaging their
        # node x's.  Otherwise, make room for ourselves.
        if child_xs:
            nodex = sum(child_xs)/len(child_xs)
        else:
            # [XX] breaks for null productions.
            nodex = (2*index+1)*self._unitsize/2 + margin
            index += 1
        
        # Draw the node
        nodey = depth * (ChartView._TREE_LEVEL_SIZE + self._text_height)
        tag = c.create_text(nodex, nodey, anchor='n', justify='center',
                            text=str(treetok.node), fill='#042',
                            font=self._boldfont)
        self._tree_tags.append(tag)

        # Draw lines to the children.
        childy = nodey + ChartView._TREE_LEVEL_SIZE + self._text_height
        for childx, child in zip(child_xs, treetok):
            if isinstance(child, Tree) and child:
                # A "real" tree token:
                tag = c.create_line(nodex, nodey + self._text_height,
                                    childx, childy, width=2, fill='#084')
                self._tree_tags.append(tag)
            if isinstance(child, Tree) and not child:
                # An unexpanded tree token:
                tag = c.create_line(nodex, nodey + self._text_height,
                                    childx, childy, width=2,
                                    fill='#048', dash='2 3')
                self._tree_tags.append(tag)
            if not isinstance(child, Tree):
                # A leaf:
                tag = c.create_line(nodex, nodey + self._text_height,
                                    childx, 10000, width=2, fill='#084')
                self._tree_tags.append(tag)

        return nodex, index

    def draw(self):
        """
        Draw everything (from scratch).
        """
        if self._tree_canvas:
            self._tree_canvas.delete('all')
            self.draw_tree()

        if self._sentence_canvas:
            self._sentence_canvas.delete('all')
            self._draw_sentence()

        self._chart_canvas.delete('all')
        self._edgetags = {}
        
        # Redraw any edges we erased.
        for lvl in range(len(self._edgelevels)):
            for edge in self._edgelevels[lvl]:
                self._draw_edge(edge, lvl)

        for edge in self._chart:
            self._add_edge(edge)

        self._draw_loclines()

    def add_callback(self, event, func):
        self._callbacks.setdefault(event,{})[func] = 1

    def remove_callback(self, event, func=None):
        if func is None: del self._callbacks[event]
        else:
            try: del self._callbacks[event][func]
            except: pass

    def _fire_callbacks(self, event, *args):
        if not self._callbacks.has_key(event): return
        for cb_func in self._callbacks[event].keys(): cb_func(*args)

#######################################################################
# Pseudo Earley Rule
#######################################################################
# This isn't *true* Early, since it doesn't use the separate lexicon
# dictionary.  (I.e., it uses TopDownMatchRule instead of ScannerRule)
# But it's close enough for demonstration purposes.

class PseudoEarleyRule(AbstractChartRule):
    NUM_EDGES = 1
    _completer = CompleterRule()
    _scanner = TopDownMatchRule()
    _predictor = PredictorRule()
    def __init__(self):
        self._most_recent_rule = None
    def apply_iter(self, chart, grammar, edge):
        for e in self._predictor.apply_iter(chart, grammar, edge):
            self._most_recent_rule = self._predictor
            yield e
        for e in self._scanner.apply_iter(chart, grammar, edge):
            self._most_recent_rule = self._scanner
            yield e
        for e in self._completer.apply_iter(chart, grammar, edge):
            self._most_recent_rule = self._completer
            yield e
    def __str__(self):
        if self._most_recent_rule is self._completer:
            return 'Completer Rule (aka Fundamental Rule)'
        elif self._most_recent_rule is self._scanner:
            return 'Scanner Rule (aka Top Down Match Rule)'
        elif self._most_recent_rule is self._predictor:
            return 'Predictor Rule (aka Top Down Expand Rule)'
        else:
            return 'Pseudo Earley Rule'

class PseudoEarleyInitRule(TopDownInitRule):
    def __str__(self):
        return 'Predictor Rule (aka Top Down Expand Rule)'

#######################################################################
# Edge Rules
#######################################################################
# These version of the chart rules only apply to a specific edge.
# This lets the user select an edge, and then apply a rule.

class EdgeRule(object):
    """
    To create an edge rule, make an empty base class that uses
    EdgeRule as the first base class, and the basic rule as the
    second base class.  (Order matters!)  
    """
    def __init__(self, edge):
        super = self.__class__.__bases__[1]
        self._edge = edge
        self.NUM_EDGES = super.NUM_EDGES-1
    def apply_iter(self, chart, grammar, *edges):
        super = self.__class__.__bases__[1]
        edges += (self._edge,)
        for e in super.apply_iter(self, chart, grammar, *edges): yield e
    def __str__(self):
        super = self.__class__.__bases__[1]
        return super.__str__(self)

class TopDownExpandEdgeRule(EdgeRule, TopDownExpandRule): pass
class TopDownMatchEdgeRule(EdgeRule, TopDownMatchRule): pass
class BottomUpEdgeRule(EdgeRule, BottomUpPredictRule): pass
class BottomUpInitEdgeRule(EdgeRule, BottomUpInitRule): pass
class FundamentalEdgeRule(EdgeRule, SingleEdgeFundamentalRule): pass
class PseudoEarleyEdgeRule(EdgeRule, PseudoEarleyRule): pass

#######################################################################
# Chart Demo
#######################################################################

class ChartDemo(object):
    def __init__(self, grammar, tokens, title='Chart Parsing Demo',
    initfunc=None):
        # Initialize the parser
        self._init_parser(grammar, tokens)
        
        self._root = None
        try:
            # Create the root window.
            self._root = Tkinter.Tk()
            self._root.title(title)
            self._root.bind('<Control-q>', self.destroy)

            # Set up some frames.
            frame3 = Tkinter.Frame(self._root)
            frame2 = Tkinter.Frame(self._root)
            frame1 = Tkinter.Frame(self._root)
            frame3.pack(side='bottom', fill='none')
            frame2.pack(side='bottom', fill='x')
            frame1.pack(side='bottom', fill='both', expand=1)

            self._init_fonts(self._root)
            self._init_animation()
            self._init_chartview(frame1)
            self._init_rulelabel(frame2)
            self._init_buttons(frame3)
            self._init_menubar()

            self._init_function = initfunc
            self._matrix = None
            self._results = None
            
            # Set up keyboard bindings.
            self._init_bindings()
            self.reset()

        except:
            print 'Error creating Tree View'
            self.destroy()
            raise

    def destroy(self, *args):
        if self._root is None: return
        self._root.destroy()
        self._root = None

    def mainloop(self, *args, **kwargs):
        """
        Enter the Tkinter mainloop.  This function must be called if
        this demo is created from a non-interactive program (e.g.
        from a secript); otherwise, the demo will close as soon as
        the script completes.
        """
        if in_idle(): return
        self._root.mainloop(*args, **kwargs)

    #////////////////////////////////////////////////////////////
    # Initialization Helpers
    #////////////////////////////////////////////////////////////

    def _init_parser(self, grammar, tokens):
        self._grammar = grammar
        self._tokens = tokens
        self._cp = SteppingChartParse(self._grammar)
        self._cp.initialize(self._tokens)
        self._chart = self._cp.chart()

        # The step iterator -- use this to generate new edges
        self._cpstep = self._cp.step()

        # The currently selected edge
        self._selection = None

    def _init_fonts(self, root):
        # See: <http://www.astro.washington.edu/owen/ROTKFolklore.html>
        self._sysfont = tkFont.Font(font=Tkinter.Button()["font"])
        root.option_add("*Font", self._sysfont)
        
        # TWhat's our font size (default=same as sysfont)
        self._size = Tkinter.IntVar(root)
        self._size.set(self._sysfont.cget('size'))

        self._boldfont = tkFont.Font(family='helvetica', weight='bold',
                                    size=self._size.get())
        self._font = tkFont.Font(family='helvetica',
                                    size=self._size.get())

    def _init_animation(self):
        # Are we stepping? (default=yes)
        self._step = Tkinter.IntVar(self._root)
        self._step.set(1)

        # What's our animation speed (default=fast)
        self._animate = Tkinter.IntVar(self._root)
        self._animate.set(3) # Default speed = fast

        # Are we currently animating?
        self._animating = 0  
        
    def _init_chartview(self, parent):
        self._cv = ChartView(self._chart, parent,
                             draw_tree=1, draw_sentence=1)
        self._cv.add_callback('select', self._click_cv_edge)

    def _init_rulelabel(self, parent):
        ruletxt = 'Last edge generated by:'

        self._rulelabel1 = Tkinter.Label(parent,text=ruletxt,
                                         font=self._boldfont)
        self._rulelabel2 = Tkinter.Label(parent, width=40,
                                         relief='groove', anchor='w',
                                         font=self._boldfont)
        self._rulelabel1.pack(side='left')
        self._rulelabel2.pack(side='left')
        step = Tkinter.Checkbutton(parent, variable=self._step,
                                   text='Step')
        step.pack(side='right')
        
    def _init_buttons(self, parent):
        frame1 = Tkinter.Frame(parent)
        frame2 = Tkinter.Frame(parent)
        frame1.pack(side='bottom', fill='x')
        frame2.pack(side='top', fill='none')

        Tkinter.Button(frame1, text='Reset\nParser',
                       background='#90c0d0', foreground='black',
                       command=self.reset).pack(side='right')
        #Tkinter.Button(frame1, text='Pause',
        #               background='#90c0d0', foreground='black',
        #               command=self.pause).pack(side='left')
        
        Tkinter.Button(frame1, text='Top Down\nStrategy',
                       background='#90c0d0', foreground='black',
                       command=self.top_down_strategy).pack(side='left')
        Tkinter.Button(frame1, text='Bottom Up\nStrategy',
                       background='#90c0d0', foreground='black',
                       command=self.bottom_up_strategy).pack(side='left')
        Tkinter.Button(frame1, text='Earley\nAlgorithm',
                       background='#90c0d0', foreground='black',
                       command=self.earley_algorithm).pack(side='left')

        Tkinter.Button(frame2, text='Top Down Init\nRule',
                       background='#90f090', foreground='black',
                       command=self.top_down_init).pack(side='left')
        Tkinter.Button(frame2, text='Top Down Expand\nRule',
                       background='#90f090', foreground='black',
                       command=self.top_down_expand).pack(side='left')
        Tkinter.Button(frame2, text='Top Down Match\nRule',
                       background='#90f090', foreground='black',
                       command=self.top_down_match).pack(side='left')
        Tkinter.Frame(frame2, width=20).pack(side='left')
        
        Tkinter.Button(frame2, text='Bottom Up Init\nRule',
                       background='#90f090', foreground='black',
                       command=self.bottom_up_init).pack(side='left')
        Tkinter.Button(frame2, text='Bottom Up Predict\nRule',
                       background='#90f090', foreground='black',
                       command=self.bottom_up).pack(side='left')
        Tkinter.Frame(frame2, width=20).pack(side='left')
        
        Tkinter.Button(frame2, text='Fundamental\nRule',
                       background='#90f090', foreground='black',
                       command=self.fundamental).pack(side='left')

    def _init_bindings(self):
        self._root.bind('<Up>', self._cv.scroll_up)
        self._root.bind('<Down>', self._cv.scroll_down)
        self._root.bind('<Prior>', self._cv.page_up)
        self._root.bind('<Next>', self._cv.page_down)
        self._root.bind('<Control-q>', self.destroy)
        self._root.bind('<Control-x>', self.destroy)
        self._root.bind('<F1>', self.help)
        
        self._root.bind('<Control-s>', self.save_chart)
        self._root.bind('<Control-o>', self.load_chart)
        self._root.bind('<Control-r>', self.reset)
        
        self._root.bind('t', self.top_down_strategy)
        self._root.bind('b', self.bottom_up_strategy)
        self._root.bind('e', self.earley_algorithm)
        self._root.bind('<space>', self._stop_animation)
        
        self._root.bind('<Control-g>', self.edit_grammar)
        self._root.bind('<Control-t>', self.edit_sentence)

        # Animation speed control
        self._root.bind('-', lambda e,a=self._animate:a.set(1))
        self._root.bind('=', lambda e,a=self._animate:a.set(2))
        self._root.bind('+', lambda e,a=self._animate:a.set(3))

        # Step control
        self._root.bind('s', lambda e,s=self._step:s.set(not s.get()))

    def _init_menubar(self):
        menubar = Tkinter.Menu(self._root)
        
        filemenu = Tkinter.Menu(menubar, tearoff=0)
        filemenu.add_command(label='Save Chart', underline=0,
                             command=self.save_chart, accelerator='Ctrl-s')
        filemenu.add_command(label='Load Chart', underline=0,
                             command=self.load_chart, accelerator='Ctrl-o')
        filemenu.add_command(label='Reset Chart', underline=0,
                             command=self.reset, accelerator='Ctrl-r')
        filemenu.add_separator()
        filemenu.add_command(label='Save Grammar', 
                             command=self.save_grammar)
        filemenu.add_command(label='Load Grammar', 
                             command=self.load_grammar)
        filemenu.add_separator()
        filemenu.add_command(label='Exit', underline=1,
                             command=self.destroy, accelerator='Ctrl-x')
        menubar.add_cascade(label='File', underline=0, menu=filemenu)
        
        editmenu = Tkinter.Menu(menubar, tearoff=0)
        editmenu.add_command(label='Edit Grammar', underline=5,
                             command=self.edit_grammar,
                             accelerator='Ctrl-g')
        editmenu.add_command(label='Edit Text', underline=5,
                             command=self.edit_sentence,
                             accelerator='Ctrl-t')
        menubar.add_cascade(label='Edit', underline=0, menu=editmenu)
        
        viewmenu = Tkinter.Menu(menubar, tearoff=0)
        viewmenu.add_command(label='Chart Matrix', underline=6,
                             command=self.view_matrix)
        viewmenu.add_command(label='Results', underline=0,
                             command=self.view_results)
        menubar.add_cascade(label='View', underline=0, menu=viewmenu)
        
        rulemenu = Tkinter.Menu(menubar, tearoff=0)
        rulemenu.add_command(label='Top Down Strategy', underline=0,
                             command=self.top_down_strategy,
                             accelerator='t')
        rulemenu.add_command(label='Bottom Up Strategy', underline=0,
                             command=self.bottom_up_strategy,
                             accelerator='b')
        rulemenu.add_command(label='Earley Algorithm', underline=0,
                             command=self.bottom_up_strategy,
                             accelerator='e')
        rulemenu.add_separator()
        rulemenu.add_command(label='Bottom Up Init Rule',
                             command=self.bottom_up_init)
        rulemenu.add_command(label='Bottom Up Rule',
                             command=self.bottom_up)
        rulemenu.add_command(label='Top Down Init Rule',
                             command=self.top_down_init)
        rulemenu.add_command(label='Top Down Expand Rule',
                             command=self.top_down_expand)
        rulemenu.add_command(label='Top Down Match Rule',
                             command=self.top_down_match)
        rulemenu.add_command(label='Fundamental Rule', 
                             command=self.fundamental)
        menubar.add_cascade(label='Apply', underline=0, menu=rulemenu)

        animatemenu = Tkinter.Menu(menubar, tearoff=0)
        animatemenu.add_checkbutton(label="Step", underline=0,
                                    variable=self._step,
                                    accelerator='s')
        animatemenu.add_separator()
        animatemenu.add_radiobutton(label="No Animation", underline=0,
                                    variable=self._animate, value=0)
        animatemenu.add_radiobutton(label="Slow Animation", underline=0,
                                    variable=self._animate, value=1,
                                    accelerator='-')
        animatemenu.add_radiobutton(label="Normal Animation", underline=0,
                                    variable=self._animate, value=2,
                                    accelerator='=')
        animatemenu.add_radiobutton(label="Fast Animation", underline=0,
                                    variable=self._animate, value=3,
                                    accelerator='+')
        menubar.add_cascade(label="Animate", underline=1, menu=animatemenu)
        
        zoommenu = Tkinter.Menu(menubar, tearoff=0)
        zoommenu.add_radiobutton(label='Tiny', variable=self._size,
                                 underline=0, value=10, command=self.resize)
        zoommenu.add_radiobutton(label='Small', variable=self._size,
                                 underline=0, value=12, command=self.resize)
        zoommenu.add_radiobutton(label='Medium', variable=self._size,
                                 underline=0, value=14, command=self.resize)
        zoommenu.add_radiobutton(label='Large', variable=self._size,
                                 underline=0, value=18, command=self.resize)
        zoommenu.add_radiobutton(label='Huge', variable=self._size,
                                 underline=0, value=24, command=self.resize)
        menubar.add_cascade(label='Zoom', underline=0, menu=zoommenu)

        helpmenu = Tkinter.Menu(menubar, tearoff=0)
        helpmenu.add_command(label='About', underline=0,
                             command=self.about)
        helpmenu.add_command(label='Instructions', underline=0,
                             command=self.help, accelerator='F1')
        menubar.add_cascade(label='Help', underline=0, menu=helpmenu)
        
        self._root.config(menu=menubar)

    #////////////////////////////////////////////////////////////
    # Selection Handling
    #////////////////////////////////////////////////////////////

    def _click_cv_edge(self, edge):
        if edge != self._selection:
            # Clicking on a new edge selects it.
            self._select_edge(edge)
        else:
            # Repeated clicks on one edge cycle its trees.
            self._cv.cycle_tree()
            # [XX] this can get confused if animation is running
            # faster than the callbacks...

    def _select_matrix_edge(self, edge):
        self._select_edge(edge)
        self._cv.view_edge(edge)

    def _select_edge(self, edge):
        self._selection = edge
        # Update the chart view.
        self._cv.markonly_edge(edge, '#f00')
        self._cv.draw_tree(edge)
        # Update the matrix view.
        if self._matrix: self._matrix.markonly_edge(edge)
        if self._matrix: self._matrix.view_edge(edge)
        
    def _deselect_edge(self):
        self._selection = None
        # Update the chart view.
        self._cv.unmark_edge()
        self._cv.erase_tree()
        # Update the matrix view
        if self._matrix: self._matrix.unmark_edge()

    def _show_new_edge(self, edge):
        self._display_rule(self._cp.current_chartrule())
        # Update the chart view.
        self._cv.update()
        self._cv.draw_tree(edge)
        self._cv.markonly_edge(edge, '#0df')
        self._cv.view_edge(edge)
        # Update the matrix view.
        if self._matrix: self._matrix.update()
        if self._matrix: self._matrix.markonly_edge(edge)
        if self._matrix: self._matrix.view_edge(edge)
        # Update the results view.
        if self._results: self._results.update(edge)

    #////////////////////////////////////////////////////////////
    # Help/usage
    #////////////////////////////////////////////////////////////

    def help(self, *e):
        self._animating = 0
        # The default font's not very legible; try using 'fixed' instead. 
        try:
            ShowText(self._root, 'Help: Chart Parser Demo',
                     (__doc__).strip(), width=75, font='fixed')
        except:
            ShowText(self._root, 'Help: Chart Parser Demo',
                     (__doc__).strip(), width=75)

    def about(self, *e):
        ABOUT = ("NLTK Chart Parser Demo\n"+
                 "Written by Edward Loper")
        tkMessageBox.showinfo('About: Chart Parser Demo', ABOUT)

    #////////////////////////////////////////////////////////////
    # File Menu
    #////////////////////////////////////////////////////////////

    CHART_FILE_TYPES = [('Pickle file', '.pickle'),
                        ('All files', '*')]
    GRAMMAR_FILE_TYPES = [('Plaintext grammar file', '.cfg'),
                          ('Pickle file', '.pickle'),
                          ('All files', '*')]

    def load_chart(self, *args):
        "Load a chart from a pickle file"
        filename = askopenfilename(filetypes=self.CHART_FILE_TYPES,
                                   defaultextension='.pickle')
        if not filename: return
        try:
            chart = pickle.load(open(filename, 'r'))
            self._chart = chart
            self._cv.update(chart)
            if self._matrix: self._matrix.set_chart(chart)
            if self._matrix: self._matrix.deselect_cell()
            if self._results: self._results.set_chart(chart)
            self._cp.set_chart(chart)
        except Exception, e:
            raise
            tkMessageBox.showerror('Error Loading Chart',
                                   'Unable to open file: %r' % filename)

    def save_chart(self, *args):
        "Save a chart to a pickle file"
        filename = asksaveasfilename(filetypes=self.CHART_FILE_TYPES,
                                     defaultextension='.pickle')
        if not filename: return
        try:
            pickle.dump(self._chart, open(filename, 'w'))
        except Exception, e:
            raise
            tkMessageBox.showerror('Error Saving Chart', 
                                   'Unable to open file: %r' % filename)

    def load_grammar(self, *args):
        "Load a grammar from a pickle file"
        filename = askopenfilename(filetypes=self.GRAMMAR_FILE_TYPES,
                                   defaultextension='.cfg')
        if not filename: return
        try:
            if filename.endswith('.pickle'):
                grammar = pickle.load(open(filename, 'r'))
            else:
                grammar = cfg.parse_grammar(open(filename, 'r').read())
            self.set_grammar(grammar)
        except Exception, e:
            tkMessageBox.showerror('Error Loading Grammar', 
                                   'Unable to open file: %r' % filename)

    def save_grammar(self, *args):
        filename = asksaveasfilename(filetypes=self.GRAMMAR_FILE_TYPES,
                                     defaultextension='.cfg')
        if not filename: return
        try:
            if filename.endswith('.pickle'):
                pickle.dump((self._chart, self._tokens), open(filename, 'w'))
            else:
                file = open(filename, 'w')
                prods = self._grammar.productions()
                start = [p for p in prods if p.lhs() == self._grammar.start()]
                rest = [p for p in prods if p.lhs() != self._grammar.start()]
                for prod in start: file.write('%s\n' % prod)
                for prod in rest: file.write('%s\n' % prod)
                file.close()
        except Exception, e:
            tkMessageBox.showerror('Error Saving Grammar',
                                   'Unable to open file: %r' % filename)
            
    def reset(self, *args):
        self._animating = 0
        self._cp = SteppingChartParse(self._grammar)
        self._cp.initialize(self._tokens)
        self._chart = self._cp.chart()
        self._cv.update(self._chart)
        if self._matrix: self._matrix.set_chart(self._chart)
        if self._matrix: self._matrix.deselect_cell()
        if self._results: self._results.set_chart(self._chart)
        self._cpstep = self._cp.step()
        if self._init_function is not None:
            self._init_function(self._chart, self._tokens)

    #////////////////////////////////////////////////////////////
    # Edit
    #////////////////////////////////////////////////////////////

    def edit_grammar(self, *e):
        CFGEditor(self._root, self._grammar, self.set_grammar)

    def set_grammar(self, grammar):
        self._grammar = grammar
        self._cp.set_grammar(grammar)
        if self._results: self._results.set_grammar(grammar)

    def edit_sentence(self, *e):
        sentence = ' '.join(self._tokens)
        title = 'Edit Text'
        instr = 'Enter a new sentence to parse.'
        EntryDialog(self._root, sentence, instr, self.set_sentence, title)

    def set_sentence(self, sentence):
        self._tokens = list(tokenize.whitespace(sentence))
        self.reset()

    #////////////////////////////////////////////////////////////
    # View Menu
    #////////////////////////////////////////////////////////////

    def view_matrix(self, *e):
        if self._matrix is not None: self._matrix.destroy()
        self._matrix = ChartMatrixView(self._root, self._chart)
        self._matrix.add_callback('select', self._select_matrix_edge)

    def view_results(self, *e):
        if self._results is not None: self._results.destroy()
        self._results = ChartResultsView(self._root, self._chart,
                                         self._grammar)

    #////////////////////////////////////////////////////////////
    # Zoom Menu
    #////////////////////////////////////////////////////////////

    def resize(self):
        self._animating = 0
        self.set_font_size(self._size.get())

    def set_font_size(self, size):
        self._cv.set_font_size(size)
        self._font.configure(size=-abs(size))
        self._boldfont.configure(size=-abs(size))
        self._sysfont.configure(size=-abs(size))
            
    def get_font_size(self):
        return abs(self._size.get())
            
    #////////////////////////////////////////////////////////////
    # Parsing
    #////////////////////////////////////////////////////////////

    def apply_strategy(self, strategy, edge_strategy=None):
        # If we're animating, then stop.
        if self._animating:
            self._animating = 0
            return

        # Clear the rule display & mark.
        self._display_rule(None)
        #self._cv.unmark_edge()

        if self._step.get():
            selection = self._selection
            if (selection is not None) and (edge_strategy is not None):
                # Apply the given strategy to the selected edge.
                self._cp.set_strategy([edge_strategy(selection)])
                newedge = self._apply_strategy()

                # If it failed, then clear the selection.
                if newedge is None:
                    self._cv.unmark_edge()
                    self._selection = None
            else:
                self._cp.set_strategy(strategy)
                self._apply_strategy()

        else:
            self._cp.set_strategy(strategy)
            if self._animate.get():
                self._animating = 1
                self._animate_strategy()
            else:
                for edge in self._cpstep:
                    if edge is None: break
                self._cv.update()
                if self._matrix: self._matrix.update()
                if self._results: self._results.update()

    def _stop_animation(self, *e):
        self._animating = 0
                                      
    def _animate_strategy(self, speed=1):
        if self._animating == 0: return
        if self._apply_strategy() is not None:
            if self._animate.get() == 0 or self._step.get() == 1:
                return
            if self._animate.get() == 1:
                self._root.after(3000, self._animate_strategy)
            elif self._animate.get() == 2:
                self._root.after(1000, self._animate_strategy)
            else:
                self._root.after(20, self._animate_strategy)
        
    def _apply_strategy(self):
        new_edge = self._cpstep.next()

        if new_edge is not None:
            self._show_new_edge(new_edge)
        return new_edge

    def _display_rule(self, rule):
        if rule == None:
            self._rulelabel2['text'] = ''
        else:
            name = str(rule)
            self._rulelabel2['text'] = name
            size = self._cv.get_font_size()

    #////////////////////////////////////////////////////////////
    # Parsing Strategies
    #////////////////////////////////////////////////////////////

    # Basic rules:
    _TD_INIT     = [TopDownInitRule()]
    _TD_EXPAND   = [TopDownExpandRule()]
    _TD_MATCH    = [TopDownMatchRule()]
    _BU_INIT     = [BottomUpInitRule()]
    _BU_RULE     = [BottomUpPredictRule()]
    _FUNDAMENTAL = [SingleEdgeFundamentalRule()]
    _EARLEY      = [PseudoEarleyRule()]
    _EARLEY_INIT = [PseudoEarleyInitRule()]

    # Complete strategies:
    _TD_STRATEGY = _TD_INIT + _TD_EXPAND + _TD_MATCH + _FUNDAMENTAL
    _BU_STRATEGY = _BU_INIT + _BU_RULE + _FUNDAMENTAL
    _EARLEY      = _EARLEY_INIT + _EARLEY

    # Button callback functions:
    def top_down_init(self, *e):
        self.apply_strategy(self._TD_INIT, None)
    def top_down_expand(self, *e):
        self.apply_strategy(self._TD_EXPAND, TopDownExpandEdgeRule)
    def top_down_match(self, *e):
        self.apply_strategy(self._TD_MATCH, TopDownMatchEdgeRule)
    def bottom_up_init(self, *e):
        self.apply_strategy(self._BU_INIT, BottomUpInitEdgeRule)
    def bottom_up(self, *e):
        self.apply_strategy(self._BU_RULE, BottomUpEdgeRule)
    def fundamental(self, *e):
        self.apply_strategy(self._FUNDAMENTAL, FundamentalEdgeRule)
    def bottom_up_strategy(self, *e):
        self.apply_strategy(self._BU_STRATEGY, BottomUpEdgeRule)
    def top_down_strategy(self, *e):
        self.apply_strategy(self._TD_STRATEGY, TopDownExpandEdgeRule)
    def earley_algorithm(self, *e):
        self.apply_strategy(self._EARLEY, PseudoEarleyEdgeRule)
        
def demo():
    grammar = cfg.parse_grammar("""
    # Grammatical productions.
        S -> NP VP
        VP -> VP PP | V NP | V
        NP -> Det N | NP PP
        PP -> P NP
    # Lexical productions.
        NP -> 'John' | 'I'
        Det -> 'the' | 'my' | 'a'
        N -> 'dog' | 'cookie' | 'table' | 'cake' | 'fork'
        V -> 'ate' | 'saw'
        P -> 'on' | 'under' | 'with'
    """)
    
    sent = 'John ate the cake on the table with a fork'
    sent = 'John ate the cake on the table'
    tokens = list(tokenize.whitespace(sent))

    print 'grammar= ('
    for rule in grammar.productions():
        print '    ', repr(rule)+','
    print ')'
    print 'tokens = %r' % tokens
    print 'Calling "ChartDemo(grammar, tokens)"...'
    ChartDemo(grammar, tokens).mainloop()
        
if __name__ == '__main__':
    demo()

    # Chart comparer:
    #charts = ['/tmp/earley.pickle',
    #          '/tmp/topdown.pickle',
    #          '/tmp/bottomup.pickle']
    #ChartComparer(*charts).mainloop()
    
    #import profile
    #profile.run('demo2()', '/tmp/profile.out')
    #import pstats
    #p = pstats.Stats('/tmp/profile.out')
    #p.strip_dirs().sort_stats('time', 'cum').print_stats(60)
    #p.strip_dirs().sort_stats('cum', 'time').print_stats(60)


########NEW FILE########
__FILENAME__ = tagparse
from nltk.parse import chart
from nltk import cfg
from drawchart import ChartDemo
from nltk.tokenize.regexp import wordpunct
#from nltk_contrib.mit.six863.kimmo import *
import re, pickle

def chart_tagger(tagger):
    def insert_tags(thechart, tokens):
        """
        Initialize a chart parser based on the results of a tagger.
        """
        tagged_tokens = list(tagger.tag(tokens))
        for i in range(len(tagged_tokens)):
            word, tag = tagged_tokens[i]
            leafedge = chart.LeafEdge(word, i)
            thechart.insert(chart.TreeEdge((i, i+1),
              cfg.Nonterminal(tag), [word], dot=1), [leafedge])
    return insert_tags

def chart_kimmo(kimmorules):
    def insert_tags(thechart, tokens):
        for i in range(len(tokens)):
            word = tokens[i]
            results = kimmorules.recognize(word.lower())
            for surface, feat in results:
                match = re.match(r"PREFIX\('.*?'\)(.*?)\(.*", feat)
                if match: pos = match.groups()[0]
                else: pos = feat.split('(')[0]
                print surface, pos
                leafedge = chart.LeafEdge(word, i)
                thechart.insert(chart.TreeEdge((i, i+1),
                  cfg.Nonterminal(pos), [word], dot=1), [leafedge])
    return insert_tags

def tagged_chart_parse(sentence, grammar, tagger):
    tokens = list(wordpunct(sentence))
    demo = ChartDemo(grammar, tokens, initfunc=chart_tagger(tagger))
    demo.mainloop()

def kimmo_chart_parse(sentence, grammar, kimmo):
    tokens = list(wordpunct(sentence))
    demo = ChartDemo(grammar, tokens, initfunc=chart_kimmo(kimmo))
    demo.mainloop()

def read(filename):
    f = open(filename)
    return f.read()

def main():
    sentence = 'The quick brown fox jumped over the lazy dog'
    grammar = cfg.parse_grammar(read('demo.cfg'))
    # load from pickle so it's faster
    tagger = pickle.load(open('demo_tagger.pickle'))
    tagged_chart_parse(sentence, grammar, tagger)
    #kimmo = load('english.yaml')
    #kimmo_chart_parse(sentence, grammar, kimmo)
    
if __name__ == '__main__': main()


########NEW FILE########
__FILENAME__ = train
from nltk import tag
from nltk.corpus import brown
import yaml

t0 = tag.Default('nn')
t1 = tag.Unigram(backoff=t0)
t1.train(brown.tagged('f'))    # section a: press-reportage

f = open('demo_tagger.yaml', 'w')
yaml.dump(t1, f)


########NEW FILE########
__FILENAME__ = rdf
### Natural Language Toolkit: Generating RDF Triples from NL Relations
#
# Author: Ewan Klein <ewan@inf.ed.ac.uk>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

"""
This code shows how relational information extracted from text 
can be converted into an RDF Graph.
"""

from nltk.sem.relextract import extract_rels, class_abbrev
from string import join
import re
from rdflib import Namespace, ConjunctiveGraph

BASE = "http://www.nltk.org/terms/"
RDFNS = Namespace('http://www.w3.org/1999/02/22-rdf-syntax-ns#')
RDFSNS = Namespace('http://www.w3.org/2000/01/rdf-schema#')

def make_rdf(ns, reldict, relsym=None, verbose=False):
    """
    Convert a reldict into an RDF triple.
    """
    subject = sym2uri(ns, reldict['subjclass'], reldict['subjsym'])
    predicate = sym2uri(ns, 'pred', relsym)
    object = sym2uri(ns, reldict['objclass'], reldict['objsym'])
    triple = (subject, predicate, object)
    if verbose:
        print triple
    return triple

def make_rdfs(ns, reldict):
    """
    Convert a reldict into a lst of RDFS type statements.
    """
    triples = []
    predicate = RDFNS.type
    i1 = sym2uri(ns, reldict['subjclass'], reldict['subjsym'])
    t1 = sym2uri(ns, 'class', reldict['subjclass'].title())
    i2 = sym2uri(ns, reldict['objclass'], reldict['objsym'])
    t2 = sym2uri(ns, 'class', reldict['objclass'].title())
    triples += [(i1, predicate, t1), (i2, predicate, t2)]
    return triples 
    
def sym2uri(base, rdfclass, sym):
    """
    Build a URI out of a base, a class term, and a symbol.
    """
    from urllib import quote
    from rdflib import Namespace
    rdfclass = class_abbrev(rdfclass)
    rdfclass = rdfclass.lower()
    ns = Namespace(base)
    sym = quote(sym)
    local = '%s#%s' % (rdfclass, sym)
    return ns[local]


def rels2rdf(ns, verbose=False):
    """
    Convert the reldicts derived from the IEER corpus in an RDF Graph.
    """
    graph = ConjunctiveGraph()
    graph.bind('nltk',BASE)
    graph.bind('org', "http://nltk.org/terms/org#")
    graph.bind('loc', "http://nltk.org/terms/loc#")
    graph.bind('pred', "http://nltk.org/terms/pred#")
    graph.bind('class', "http://nltk.org/terms/class#")
    in_uri = sym2uri(ns, 'pred', 'in')
    loc_uri = sym2uri(ns, 'class', 'Location')
    org_uri = sym2uri(ns, 'class', 'Organization')
    graph.add((in_uri, RDFNS.type, RDFSNS.Property))
    graph.add((loc_uri, RDFNS.type, RDFSNS.Class))
    graph.add((org_uri, RDFNS.type, RDFSNS.Class))
    graph.add((in_uri, RDFSNS.domain, org_uri))
    graph.add((in_uri, RDFSNS.range, loc_uri))
    from nltk.corpus import ieer
    IN = re.compile(r'.*\bin\b(?!\b.+ing\b)')
    for item in ieer.fileids():
        for doc in ieer.parsed_docs(item):
            for reldict in extract_rels('ORG', 'LOC', doc, corpus='ieer', pattern=IN):
                graph.add(make_rdf(ns, reldict, relsym='in'))
                for triple in make_rdfs(ns, reldict):
                    graph.add(triple)
    return graph

def demo():
    import rdfvizualize
    graph = rels2rdf(BASE)
    #print graph.serialize(format='turtle')
    viz = rdfvizualize.Visualizer(graph)
    g = viz.graph2dot(filter_edges=True)
    g.write_png('rdf.png', prog='dot') 
    
if __name__ == '__main__':
    demo()





########NEW FILE########
__FILENAME__ = rdfquery
# Natural Language Toolkit: Convert English sentences in SPARQL Queries
#
# Copyright (C) 2001-2011 NLTK Project
# Author: Ewan Klein <ewan@inf.ed.ac.uk>,
# URL: <http://nltk.sourceforge.net>
# For license information, see LICENSE.TXT
#

import nltk
from nltk import parse, LogicParser, sem
from string import join


class SPARQLTranslator(object):
    """
    SPARQLTranslator to convert logical expressions into a SPARQL query
    """
    def __init__(self, ns=None):
        self.clauses = []
        self._query = []
        self.query = ''

        
    def flatten_conjunct(self, expr):
        """
        Turn a nested AndExpression into a list of atomic clauses.
        """
        if isinstance(expr, sem.ApplicationExpression):
            self.clauses.append(expr)
        elif isinstance(expr, sem.AndExpression):
            self.flatten_conjunct(expr.first)
            self.flatten_conjunct(expr.second)
 
    def rdf_atom(self, clause):
        """
        Turn an atomic clause into an RDF triple.
        """
        if isinstance(clause, sem.ApplicationExpression):
            function, arguments = clause.uncurry()
            if len(arguments) == 1:
                return "?%s rdf:type %s.\n" % (str(arguments[0].variable.name), str(function.variable))
            elif len(arguments) == 2:
                return "?%s %s ?%s.\n" % (str(arguments[0].variable.name), str(function.variable), str(arguments[1].variable.name))
            else:
                raise ValueError("cannot parse clause %s" % clause)
        
    def translate(self, semrep):
        """
        Given a semantic representation as parsed by sem.LogicParser(),
        return a SPARQL query.
        """
        function = ''
        # This needs to be more clever about deciding whether a given semrep
        # can in fact be converted into a valid SPARQL query
        selectword = False
        
        # Assume the main input is a function application
        if isinstance(semrep, sem.ApplicationExpression):
            function, arguments = semrep.uncurry()
 
        if str(function.variable) in ['list', 'name']:
            self._query.append('SELECT DISTINCT')
            body = arguments[1]
        
        # Everything is a lot easier if we can get hold of a lambda expression
        # which represents the body of the query
        if isinstance(body, sem.LambdaExpression):
            var = body.variable.name
            self._query.append('?'+var)
            self._query.append('WHERE\n{\n')
            if isinstance(body.term.term, sem.AndExpression):
                
                self.flatten_conjunct(body.term.term)
                triples = [self.rdf_atom(c) for c in self.clauses]
                self._query.extend(triples)
                
            
        self.query = join(self._query) + '}'
        

def demo():
    cp = parse.load_parser('file:rdf.fcfg', trace=0)
    tokens = 'list the actors in the_shining'.split()
    trees = cp.nbest_parse(tokens)
    tree = trees[0]
    semrep = sem.root_semrep(tree)
    trans = SPARQLTranslator()
    trans.translate(semrep)
    print trans.query
    
    
if __name__ == '__main__':
    demo()


########NEW FILE########
__FILENAME__ = rdfvizualize
#!/opt/local/bin/python
# Natural Language Toolkit: Generating RDF Triples from NL Relations
#
# Author: Ewan Klein <ewan@inf.ed.ac.uk>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

"""
Visualize RDF Graphs
"""
from rdflib import ConjunctiveGraph, Literal, BNode
import pydot
from Cheetah.Template import Template


class Visualizer(object):
    def __init__(self, graph, **vizoptions):
        self.graph = graph
        self.options = vizoptions
    
    def graph_options(self, uri, count):
        # set the defaults
        self.options['shape'] = 'oval'
        self.options['color'] = 'blue'
        self.options['label'] = '"%s"' % self.graph.namespace_manager.normalizeUri(uri)    
        self.options['fontname'] = 'Arial'
        self.options['fontsize'] = '10'
        self.options['fontcolor'] = 'black'
        
        if isinstance(uri, Literal):
            self.options['shape'] = 'box'
            self.options['color'] = 'green'
                        
        if isinstance(uri, BNode):
            self.options['label'] = '"_:bn%03d"' % count
            
        
    def graph2dot(self, filter_edges=False):
        """ 
        Convert an RDF graph into a pydot (U{http://code.google.com/p/pydot/}) C{Dot} object.
        This can be serialized as a string in dot (Graphviz graph drawing) format, or converted
        into an image file.
        
        Usage examples:
        
            >>> g = graph2dot(store, filter_edges=True)
            >>> print g.to_string()
            >>> g.write('myrdf.dot')
            >>> g.write_jpeg('myrdf.jpg', prog='dot')
        
        See U{http://www.research.att.com/sw/tools/graphviz/} for information on the dot language.
        
        @param filter_edges: if True, don't generate any edges which match the URIs in the list FILTER.
        @rtype: C{pydot.Dot}
        """

        dot = pydot.Dot(rankdir='LR', bgcolor='white', arrowsize='0.7')
        dot.set_suppress_disconnected(True)
        
        FILTER = ["rdfs:%s" % p for p in ['label', 'comment', 'description', 'isDefinedBy']] 
     
        # dictionary to associate URIs with node identifiers
        nodes = {}
        count = 1
        
        # add subjects and objects as nodes in the Dot instance
        for s, o in self.graph.subject_objects():
            for uri in s, o:
                if uri not in nodes.keys():
                    # generate a new node identifier
                    node_id = "n%03d" % count
                    nodes[uri] = node_id
                    
                    if isinstance(uri, Literal):
                        shape = 'box'
                        color = 'green'                
                    else:
                        shape = 'oval'
                        color = 'blue'
                        
                    # make nicer labels for blank nodes
                    if isinstance(uri, BNode):
                        label = "_:bn%03d" % count
                    else:
                        label = '"%s"' % self.graph.namespace_manager.normalizeUri(uri)
                    
                    
                    #n = pydot.Node(node_id, shape=shape, fontcolor='white', fontname ='Helvetica', color=color, label=label)
                    self.graph_options(uri, count)
                    n = pydot.Node(node_id, **self.options)
                    dot.add_node(n)
                    count += 1           
                    
        # add edges between subject and object nodes
        for s, p, o in self.graph.triples((None,None,None)):
            p = '"%s"' % self.graph.namespace_manager.normalizeUri(p)
            if filter_edges and p in FILTER: continue
            e = pydot.Edge(nodes[s], nodes[o], label=p, color='blue', fontcolor='black', fontname='Helvetica', fontsize='10')
            dot.add_edge(e)
        return dot


#def print_rdf_demo():
    #(postsdict, tagsdict, bundlesdict) = fetch_dlcs(USER, PASSWORD, BUNDLETAG)
    #posts = postsdict['posts']
    #rdf = Template(file='dlcs-tmpl01.txt')
    #rdf.posts = posts
    #print str(rdf)

#def write_rdf_demo():
    #(postsdict, tagsdict, bundlesdict) = fetch_dlcs(USER, PASSWORD, BUNDLETAG)
    #posts = postsdict['posts']
    #rdf = Template(file='dlcs-tmpl01.txt')
    #rdf.posts = posts
    #f = open(FILE, mode='w')
    #f.write(str(rdf))
    #print "Wrote some rdf to '%s'" % FILE
    #f.close()

def serialize_demo():
    try:
        store = ConjunctiveGraph()    
        store.parse(FILE, format='xml')
        print store.serialize(format='xml')
    except OSError:
        print "Cannot read file '%s'" % FILE

def make_dot_demo(infile):    
    try:
        store = ConjunctiveGraph()    
        store.parse(infile, format='xml')
        basename = infile.split('.')[0]
        v = Visualizer(store)
        g = v.graph2dot(filter_edges=True)
        g.write('%s.dot' % basename) 
        print "Wrote '%s.dot'" % basename
        g.write_png('%s.png' % basename, prog='dot') 
        print "Wrote '%s.png'" % basename
        g.write_svg('%s.svg' % basename, prog='dot') 
        print "Wrote '%s.svg'" % basename
    except OSError:
        print "Cannot read file '%s'" % FILE
        
        
def main():
    import sys
    from optparse import OptionParser 
    description = \
    """
Turn an RDF file into a viewable graph using Graphviz.
    """
    opts = OptionParser(description=description)    
    (options, args) = opts.parse_args()
    if len(args) != 1:
        parser.error("incorrect number of arguments")
    infile = args[0]
    #print
    #print "Fill up a template and print out the resulting rdf in n3 format"
    #print '*' * 30
    #print_rdf_demo()
    
    #print
    #print "Write some rdf to a file"
    #print '*' * 30
    #write_rdf_demo()
    
    #print
    #print "Serialize some rdf in XML format"
    #print '*' * 30
    #serialize_demo()
    
    print
    print "Visualise an rdf graph with Graphviz"
    print '*' * 30
    make_dot_demo(infile)
    
if __name__ == '__main__':    
    main()
    

    

    


########NEW FILE########
__FILENAME__ = crawler
# -*- coding: utf-8 -*- 
# Sets the encoding to utf-8 to avoid problems with æøå

import random
import os,re
from urlextracter import *
from sgmllib import *

class Crawler:
    current = ""
    urls = []
    crawled = []
    
    def crawl(self,url):
        self.current = url
        print "Crawling " + url
        try:
            ue = URLextracter(url)
        except SGMLParseError:
            print "This URL contains error that can't be handled by this app.\nSorry!"
            print "=" * 30
            print "Trying new random URL"
            self.crawl(self.urls[random.randint(1,len(self.urls))])
            return
        
        urlparts = url.replace("http://", "").replace("/",".").split(".")
        filename = ""
        for part in urlparts:
            if not str(part).__contains__("www"):
                filename += part + "."
        filename += "txt"
        
        print "Stored as: " + filename
        urls = ""
        try:
            # Set the path of where to store your data
            f = open("/path/to/saved/data/lang/%s" % filename, 'wb')
            content = str(ue.output())         
            content = re.sub(r"[^a-zA-ZæøåÆØÅ]", " ", content)
            content = content.strip()

            if len(content) > 2:    # Minimum 3 words
                try:
                    textToWrite = unicode("".join(content))
                except UnicodeDecodeError:
                    textToWrite = str("".join(content))
                f.write(textToWrite)
                f.close()
            else:
                # Set this path to same as storage path
                os.remove("/path/to/saved/data/lang/%s" % filename)
            urls = ue.linklist
            print "" + url + " mined!"
        except IOError:
            print "Mined, but failed to store as file.\nSkipping this, going on to next!"
            urls = self.urls
        ok_urls = []
        for i in urls:
            url = i
            if str(url).__contains__("javascript"):
                urls.remove(url)
            elif str(url).__contains__("http://"):
                ok_urls.append(url)
            elif str(url).__contains__("https://"):
                ok_urls.append(url)
            else:
                urls.remove(url)
        if len(ok_urls) < 2:
            ok_urls = self.crawled
            unique = True            # Fake true
            print str(len(ok_urls))
        else:
            unique = False
        
        next = random.randint(1,len(ok_urls)-1)
        print next
        new_url = ok_urls[next]
        while not unique:
            next = random.randint(1,len(ok_urls)-1)
            new_url = ok_urls[next]
            if not new_url in self.crawled:
                unique = True
            elif len(ok_urls) < 2:                    # with this few left we fake it
                ok_urls = self.crawled
                next = random.randint(1,len(ok_urls)-1)
                new_url = ok_urls[next]
                unique = True
            else:
                print "Already crawled " + new_url
                ok_urls.remove(new_url)
                if len(ok_urls) < 2:
                    ok_urls = self.crawled
                    next = random.randint(1,len(ok_urls)-1)
                    new_url = ok_urls[next]
                    unique = True
                
        self.urls = ok_urls
        self.crawled.append(self.current)
        self.crawl(new_url)



if __name__=="__main__":
    url = "http://www.vg.no"    # start url (should be a large known site)
    c = Crawler()
    c.crawl(url)

########NEW FILE########
__FILENAME__ = languageclassifier
# -*- coding: utf-8 -*- 
# Sets the encoding to utf-8 to avoid problems with æøå

import glob
import re
import random
import math
import pickle
import os
import logging

from nltk.corpus import stopwords

from urlextracter import URLextracter
from sgmllib import *

class NaiveBayes(object):
       
    p_word_given_lang = {}
    
    training_files = []
    test_files = []
    candidate_languages = []

    def __init__(self):
        self.nor_stopwords = {}
        for word in stopwords.words('norwegian'):
            self.nor_stopwords[word] = True
            
        self.eng_stopwords = {}
        for word in stopwords.words('english'):
            self.eng_stopwords[word] = True
            
        self.load(os.path.join("files","lang_data.pickle"))

     
    def load(self,picklepath):
        """
        load pickled training results
        picklepath is the local path to your picklefile
        """
        try:
            p = open(picklepath, 'rb')
            data = pickle.load(p)
            self.p_word_given_lang = data["p_word_given_lang"]
            self.candidate_languages = data["canidate_languages"]
            self.p_lang = data["p_lang"]
            self.vocabulary = data["vocabulary"]
        except IOError:
            self.p_word_given_lang = {}
            logging.warning("No pickled language classifier available.")

        
    def train(self, path):
        """
        Train the classifier with data placed
        in a folder named as the related language.
        Example: /path/to/files/eng/file01.txt
        """
        # Setup
        data_files = glob.glob(path + "/*/*")
        random.shuffle(data_files)
        
        self.training_files = data_files[0:300]
        self.test_files = data_files[300:]
        
        self.files = {}
        self.p_lang = {}
        
        # Calculate P(H)
        for file in self.training_files:
            values = file.split('/')
            lang = values[-2]
        
            if not self.p_lang.has_key(lang):
                self.p_lang[lang] = 0.0
            
            self.p_lang[lang] += 1.0
            
            if not self.files.has_key(lang):
                self.files[lang] = []
            
            f = open(file, 'r')
            self.files[lang] += f.read().replace("\n", " ").replace(".", "").split(" ")
            f.close()
            
        # Calculate probabilities
        for lang in self.p_lang.keys():
            self.p_lang[lang] /= len(self.training_files)
            
        self.vocabulary = self.__createVocabulary(self.files)
        
        # Calculate P(O | H) 
        p_word_given_lang = self.p_word_given_lang
        for lang in self.files.keys():
            p_word_given_lang[lang] = {}
            
            for word in self.vocabulary[lang].keys():
                p_word_given_lang[lang][word] = 1.0
            
            for word in self.files[lang]:
                if self.vocabulary[lang].has_key(word):
                    p_word_given_lang[lang][word] += 1.0
                    
            for word in self.vocabulary[lang].keys():
                p_word_given_lang[lang][word] /= len(self.files[lang]) + len(self.vocabulary[lang])
                
        print "Training finished...(training-set of size %d)" % len(self.training_files)
        self.p_word_given_lang = p_word_given_lang
        self.candidate_languages = self.files.keys()
        
        # Save result as a file
        output = open(os.path.join("files","lang_data.pickle"),'wb')
        data = {}
        data["p_word_given_lang"] = p_word_given_lang
        data["canidate_languages"] = self.files.keys()
        data["p_lang"] = self.p_lang
        data["vocabulary"] = self.vocabulary
        pickler = pickle.dump(data, output, -1)
        output.close()   
    
    def __createVocabulary(self, files):
        """
        Filter out the words we're not interessted in
        and return a dictionary with all remaining words
        sorted by language.
        Example: vocabulary[eng] = {'lazy','fox',...}
        """
        # Count number of occurance of each word
        word_count = {}
        for lang in files.keys():
            for word in files[lang]:
                if not word_count.has_key(word):
                    word_count[word] = 0
                word_count[word] += 1
        
        vocabulary = {}
        vocabulary['eng'] = {}
        vocabulary['no'] = {}
        for word in word_count.keys():
            if word_count[word] > 2:
                if word != '':
                    if not word in self.nor_stopwords:
                        vocabulary['no'][word] = True
                    if not word in self.eng_stopwords:
                        vocabulary['eng'][word] = True
        return vocabulary
    
    
    def testAccuracy(self,test_files = ""):
        """
        Test the accuracy of the classifier.
        Provide test files as list or path.
        The path must be on the same form as when training.
        """

        if test_files == "":
            print "No test files given"
            return
        elif os.path.isdir(str(test_files)):
            self.test_files = glob.glob(test_files + "/*/*")
            random.shuffle(self.test_files)
        else:
            self.test_files = test_files
  
        errors = 0.0
        total = 0.0
        
        # Use if test_files is provided as path
        #test_files = glob.glob(path + "/*/*")
        #random.shuffle(test_files)
        
        
        for file in self.test_files:
            values = file.split(os.sep)
            true_lang = values[-2]

            f = open(file, "r")    
            file_to_be_classified = f.read().replace("\n", " ").replace(".", "").split(" ")
            f.close()
            
            # Finds group with max P(O | H) * P(H)
            max_lang = 0
            max_p = 1
            for candidate_lang in self.candidate_languages:
                # Calculates P(O | H) * P(H) for candidate group
                p = math.log(self.p_lang[candidate_lang])
                for word in file_to_be_classified:
                    if self.vocabulary[candidate_lang].has_key(word):
                        p += math.log(self.p_word_given_lang[candidate_lang][word])
        
                if p > max_p or max_p == 1:
                    max_p = p
                    max_lang = candidate_lang
        
            total += 1.0
            if true_lang != max_lang:
                errors += 1.0
        print "Classifying finished...(test-set of size %d)" % len(self.test_files)
        print "Errors %d" % errors
        print "Total %d" % total
        print "Accuracy: %.3f" % (1.0 - errors/total)
    
    def classifyText(self, text):
        """
        If no vocabularies have been provided language should be 'unknown'
        >>> classifier = NaiveBayes()
        >>> classifier.classifyText("is this some english text")
        'unknown'
        """
        max_lang = 0
        max_p = 1
        unknown_words = []
        known_words = []
        for candidate_lang in self.candidate_languages:
            # Calculates P(O | H) * P(H) for candidate group
            p = math.log(self.p_lang[candidate_lang])
            words = text.split(' ')
            unknown_words = []
            known_words = []
            for word in words:
                if self.vocabulary[candidate_lang].has_key(word):
                    p += math.log(self.p_word_given_lang[candidate_lang][word])
                    if word not in known_words:
                        known_words.append(word)
                else:
                    if word not in unknown_words:
                        unknown_words.append(word)
            if p > max_p or max_p == 1:
                max_p = p
                max_lang = candidate_lang


        # TODO: this is wrong, it's checking the last one, not the max
        percent = (float(len(known_words)) / (float(len(unknown_words)) + 0.1))
        # return unknown if the ratio of known words is less or equal to 0.25
        if percent <= 0.25:        
            max_lang = "unknown"
        
        return max_lang
    
    def classifyURL(self, url):
        ue = URLextracter(url)
        print 'Classifying %s' % url
        content = ue.output() 
        content = re.sub(r"[^a-zA-ZæøåÆØÅ]", " ", content)
        content = content.strip()
        return self.classifyText(content)
    
    def handle_decl(self,data):
        pass

    def report_unbalanced(self,tag):
        pass
    
    def demo(self):
        print "Demo of language classifier"
        print "=" * 40
        nb = NaiveBayes()
        nb.load(os.path.join("files","lang_data.pickle"))
        
        print "Classifying plain text(10 first sentences from \"nltk.corpus.abc.sents\")"
        print "=" * 40
        text = ""
        import nltk.corpus
        sents = nltk.corpus.abc.sents()
        for words in sents[0:10]:
            text+= " ".join(words) + "\n"
        print text
        print "=" * 40
        print "Languages is: %s" % nb.classifyText(text)
        
        print "\n"
        print "Classifying 10 URLs"
        print "=" * 40
        
        lang = nb.classifyURL("http://harvardscience.harvard.edu/")
        print "-->language: %s \n" % lang
        lang = nb.classifyURL("http://vg.no")
        print "-->language: %s \n" % lang
        lang = nb.classifyURL("http://bbc.co.uk")
        print "-->language: %s \n" % lang
        lang = nb.classifyURL("http://startsiden.no")
        print "-->language: %s \n" % lang
        lang = nb.classifyURL("http://news.com")
        print "-->language: %s \n" % lang
        lang = nb.classifyURL("http://www.munimadrid.es")
        print "-->language: %s \n" % lang
        lang = nb.classifyURL("http://www.welt.de/")
        print "-->language: %s \n" % lang
        lang = nb.classifyURL("http://www.news.pl/")
        print "-->language: %s \n" % lang
        lang = nb.classifyURL("http://www.ekstrabladet.dk/")
        print "-->language: %s \n" % lang
        lang = nb.classifyURL("http://www.gazzetta.it/")
        print "-->language: %s \n" % lang      
    demo = classmethod(demo)
    
def demo():
    NaiveBayes.demo()
    
if __name__=="__main__":
    NaiveBayes.demo()    
    
    

########NEW FILE########
__FILENAME__ = readabilitytests
from textanalyzer import *
import math

class ReadabilityTool:

    analyzedVars = {}
    text = ""
    lang = ""
    
    tests_given_lang = {}

    def __init__(self, text = ''):
        self.tests_given_lang['all'] = {}
        self.tests_given_lang['all']["ARI"] = self.ARI
        self.tests_given_lang['all']['Flesch Reading Ease'] = self.FleschReadingEase
        self.tests_given_lang['all']["Flesch-Kincaid Grade Level"] = self.FleschKincaidGradeLevel
        self.tests_given_lang['all']["Gunning Fog Index"] = self.GunningFogIndex
        self.tests_given_lang['all']["SMOG Index"] = self.SMOGIndex
        self.tests_given_lang['all']['Coleman Liau Index'] = self.ColemanLiauIndex
        self.tests_given_lang['all']['LIX'] = self.LIX
        self.tests_given_lang['all']['RIX'] = self.RIX
        
        self.tests_given_lang['eng'] = {}
        self.tests_given_lang['eng']["ARI"] = self.ARI
        self.tests_given_lang['eng']['Flesch Reading Ease'] = self.FleschReadingEase
        self.tests_given_lang['eng']["Flesch-Kincaid Grade Level"] = self.FleschKincaidGradeLevel
        self.tests_given_lang['eng']["Gunning Fog Index"] = self.GunningFogIndex
        self.tests_given_lang['eng']["SMOG Index"] = self.SMOGIndex
        self.tests_given_lang['eng']['Coleman Liau Index'] = self.ColemanLiauIndex
        self.tests_given_lang['eng']['LIX'] = self.LIX
        self.tests_given_lang['eng']['RIX'] = self.RIX
        
        self.tests_given_lang['no'] = {}
        self.tests_given_lang['no']["ARI"] = self.ARI
        self.tests_given_lang['no']['Coleman Liau Index'] = self.ColemanLiauIndex
        self.tests_given_lang['no']['LIX'] = self.LIX
        self.tests_given_lang['no']['RIX'] = self.RIX
        
        if text != '':
            self.__analyzeText(text)
                
    def __analyzeText(self, text=''):
        if text != '':
            if text != self.text:
                self.text = text
                lang = NaiveBayes().classifyText(text)
                self.lang = lang
                t = textanalyzer(lang)
                words = t.getWords(text)
                charCount = t.getCharacterCount(words)
                wordCount = len(words)
                sentenceCount = len(t.getSentences(text))
                syllableCount = t.countSyllables(words)
                complexwordsCount = t.countComplexWords(text)
                averageWordsPerSentence = wordCount/sentenceCount
                
                analyzedVars = {}
                
                analyzedVars['words'] = words
                analyzedVars['charCount'] = float(charCount)
                analyzedVars['wordCount'] = float(wordCount)
                analyzedVars['sentenceCount'] = float(sentenceCount)
                analyzedVars['syllableCount'] = float(syllableCount)
                analyzedVars['complexwordCount'] = float(complexwordsCount)
                analyzedVars['averageWordsPerSentence'] = float(averageWordsPerSentence)
                self.analyzedVars = analyzedVars
        
     
    
    def ARI(self, text = ''):
        """
        >>> tool = ReadabilityTool()
        >>> tool.ARI("the cat jumped over the fox. the car jumped over the bus. the tree jumped over the hedge.")
        -0.375
        """
        self.__analyzeText(text)
        analyzedVars = self.analyzedVars
        score = 4.71 * (analyzedVars['charCount'] / analyzedVars['wordCount']) + 0.5 * (analyzedVars['wordCount'] / analyzedVars['sentenceCount']) - 21.43
        return score
    
    def FleschReadingEase(self, text = ''):
        """
        >>> tool = ReadabilityTool()
        >>> tool.FleschReadingEase("the cat jumped over the fox. the car jumped over the bus. the tree jumped over the hedge.")
        116.145
        """
        self.__analyzeText(text)
        score = 0.0
        analyzedVars = self.analyzedVars        
        score = 206.835 - (1.015 * (analyzedVars['averageWordsPerSentence'])) - (84.6 * (analyzedVars['syllableCount']/ analyzedVars['wordCount']))
        return round(score, 4)
    
    def FleschKincaidGradeLevel(self, text = ''):
        """
        >>> tool = ReadabilityTool()
        >>> tool.FleschKincaidGradeLevel("the cat jumped over the fox. the car jumped over the bus. the tree jumped over the hedge.")
        -1.45
        """
        self.__analyzeText(text)
        analyzedVars = self.analyzedVars
        score = 0.39 * (analyzedVars['averageWordsPerSentence']) + 11.8 * (analyzedVars['syllableCount']/ analyzedVars['wordCount']) - 15.59
        return round(score, 4)
    
    def GunningFogIndex(self, text = ''):
        """
        >>> tool = ReadabilityTool()
        >>> tool.GunningFogIndex("the cat jumped over the fox. the car jumped over the bus. the tree jumped over the hedge.")
        2.4
        """
        self.__analyzeText(text)
        analyzedVars = self.analyzedVars
        score = 0.4 * ((analyzedVars['averageWordsPerSentence']) + (100 * (analyzedVars['complexwordCount']/analyzedVars['wordCount'])))
        return round(score, 4)
    
    def SMOGIndex(self, text = ''):
        """
        It should work if no training data is provided. It uses the default language of eng.
        >>> tool = ReadabilityTool()
        >>> tool.SMOGIndex("the cat jumped over the fox. the car jumped over the bus. the tree jumped over the hedge.")
        3.0
        """
        self.__analyzeText(text)
        analyzedVars = self.analyzedVars
        score = (math.sqrt(analyzedVars['complexwordCount']*(30/analyzedVars['sentenceCount'])) + 3)
        return score
    
    def ColemanLiauIndex(self, text = ''):
        """
        >>> tool = ReadabilityTool()
        >>> tool.ColemanLiauIndex("the cat jumped over the fox. the car jumped over the bus. the tree jumped over the hedge.")
        1.7783
        """
        self.__analyzeText(text)
        analyzedVars = self.analyzedVars
        score = (5.89*(analyzedVars['charCount']/analyzedVars['wordCount']))-(30*(analyzedVars['sentenceCount']/analyzedVars['wordCount']))-15.8
        return round(score, 4)
    
    def LIX(self, text = ''):
        """
        >>> tool = ReadabilityTool()
        >>> tool.LIX("the cat jumped over the fox. the car jumped over the bus. the tree jumped over the hedge.")
        6.0
        """
        self.__analyzeText(text)
        analyzedVars = self.analyzedVars
        longwords = 0.0
        for word in analyzedVars['words']:
            if len(word) >= 7:
                longwords += 1.0
        score = analyzedVars['wordCount'] / analyzedVars['sentenceCount'] + float(100 * longwords) / analyzedVars['wordCount']
        return score
    
    def RIX(self, text = ''):
        """
        >>> tool = ReadabilityTool()
        >>> tool.RIX("the cat jumped over the fox. the car jumped over the bus. the tree jumped over the hedge.")
        0.0
        """
        self.__analyzeText(text)
        analyzedVars = self.analyzedVars
        score = 0.0
        longwords = 0.0
        for word in analyzedVars['words']:
            if len(word) >= 7:
                longwords += 1.0
        score = longwords / analyzedVars['sentenceCount']
        return score
        
    
    def getReportAll(self, text = ''):
        self.__analyzeText(text)
#        ari = 0.0
#        fleschEase = 0.0
#        fleschGrade = 0.0
#        gunningFog = 0.0
#        smog = 0.0
#        coleman = 0.0
#        
#        ari = self.ARI()
#        fleschEase = self.FleschReadingEase()
#        fleschGrade = self.FleschKincaidGradeLevel()
#        gunningFog = self.GunningFogIndex()
#        smog = self.SMOGIndex()
#        coleman = self.ColemanLiauIndex()
#        lix = self.LIX()
#        rix = self.RIX()
#        
#        print '*' * 70
#        print ' ARI: %.1f' % ari
#        print ' Flesch Reading Ease: %.1f' % fleschEase
#        print ' FleschKincaid Grade Level: %.1f' % fleschGrade
#        print ' Gunning Fog: %.1f' % gunningFog
#        print ' SMOG Index: %.1f' % smog
#        print ' Coleman-Liau Index: %.1f' % coleman
#        print ' LIX : %.1f' % lix
#        print ' RIX : %.1f' % rix
#        print '*' * 70
        
        print "=" * 100
        print "Recommended tests for lang: %s" % self.lang 
        print "=" * 100
        for testname in self.tests_given_lang[self.lang].keys():
            print testname + " : %.2f" % self.tests_given_lang[self.lang][testname](text)
        print "=" * 100
        print "Other tests: (Warning! Use with care)"
        print "=" * 100 
        for testname in self.tests_given_lang["all"].keys():
            if not self.tests_given_lang[self.lang].has_key(testname):
                print testname + " : %.2f" % self.tests_given_lang["all"][testname](text) 
            
 
    def demo(self):
        self = ReadabilityTool()
        text = """
                It is for us the living, rather,
                to be dedicated here to the unfinished
                work which they who fought here have
                thus far so nobly advanced. It is
                rather for us to be here dedicated
                to the great task remaining before us,
                that from these honored dead we take 
                increased devotion to that cause for which they
                gave the last full measure of devotion, that we
                here highly resolve that these dead shall not have
                died in vain, that this nation, under God, shall have a
                new birth of freedom, and that government of the people, by
                the people, for the people, shall not perish from this earth.  
               """
       
        self.__analyzeText(text)
        self.getReportAll(text)
    demo = classmethod(demo) 
 
 
def demo():
    ReadabilityTool.demo()
    
if __name__ == "__main__":
    ReadabilityTool.demo()
 
 
 
 
 
 
 
 
 
    
    
    
    

########NEW FILE########
__FILENAME__ = syllables_en
import string, re, os

###
### Fallback syllable counter
###
### This is based on the algorithm in Greg Fast's perl module
### Lingua::EN::Syllable.
###

specialSyllables_en = """tottered 2
chummed 1
peeped 1
moustaches 2
shamefully 3
messieurs 2
satiated 4
sailmaker 4
sheered 1
disinterred 3
propitiatory 6
bepatched 2
particularized 5
caressed 2
trespassed 2
sepulchre 3
flapped 1
hemispheres 3
pencilled 2
motioned 2
poleman 2
slandered 2
sombre 2
etc 4
sidespring 2
mimes 1
effaces 2
mr 2
mrs 2
ms 1
dr 2
st 1
sr 2
jr 2
truckle 2
foamed 1
fringed 2
clattered 2
capered 2
mangroves 2
suavely 2
reclined 2
brutes 1
effaced 2
quivered 2
h'm 1
veriest 3
sententiously 4
deafened 2
manoeuvred 3
unstained 2
gaped 1
stammered 2
shivered 2
discoloured 3
gravesend 2
60 2
lb 1
unexpressed 3
greyish 2
unostentatious 5
"""

fallback_cache = {}

fallback_subsyl = ["cial", "tia", "cius", "cious", "gui", "ion", "iou",
                   "sia$", ".ely$"]

fallback_addsyl = ["ia", "riet", "dien", "iu", "io", "ii",
                   "[aeiouy]bl$", "mbl$",
                   "[aeiou]{3}",
                   "^mc", "ism$",
                   "(.)(?!\\1)([aeiouy])\\2l$",
                   "[^l]llien",
                   "^coad.", "^coag.", "^coal.", "^coax.",
                   "(.)(?!\\1)[gq]ua(.)(?!\\2)[aeiou]",
                   "dnt$"]


# Compile our regular expressions
for i in range(len(fallback_subsyl)):
    fallback_subsyl[i] = re.compile(fallback_subsyl[i])
for i in range(len(fallback_addsyl)):
    fallback_addsyl[i] = re.compile(fallback_addsyl[i])

def _normalize_word(word):
    return word.strip().lower()

# Read our syllable override file and stash that info in the cache
for line in specialSyllables_en.splitlines():
    line = line.strip()
    if line:
        toks = line.split()
        assert len(toks) == 2
        fallback_cache[_normalize_word(toks[0])] = int(toks[1])

def count(word):
    word = _normalize_word(word)

    if not word:
        return 0

    # Check for a cached syllable count
    count = fallback_cache.get(word, -1)
    if count > 0:
        return count

    # Remove final silent 'e'
    if word[-1] == "e":
        word = word[:-1]

    # Count vowel groups
    count = 0
    prev_was_vowel = 0
    for c in word:
        is_vowel = c in ("a", "e", "i", "o", "u", "y")
        if is_vowel and not prev_was_vowel:
            count += 1
        prev_was_vowel = is_vowel

    # Add & subtract syllables
    for r in fallback_addsyl:
        if r.search(word):
            count += 1
    for r in fallback_subsyl:
        if r.search(word):
            count -= 1

    # Cache the syllable count
    fallback_cache[word] = count

    return count

###
### Phoneme-driven syllable counting
###

def count_decomp(decomp):
    count = 0
    for unit in decomp:
        if gnoetics.phoneme.is_xstressed(unit):
            count += 1
    return count

########NEW FILE########
__FILENAME__ = syllables_no
# -*- coding: utf-8 -*- 
# Sets the encoding to utf-8 to avoid problems with æøå

import string
import re
import os

specialSyllables_no = """distribuert 4
lÊreinstitusjoner 7
spesielt 3
offisielle 5
arbeidssprÂk 3
utarbeidet 4
verdenserklÊringen 6
h¯yeste 3
tvinges 2
utvei 2
arbeide 3
samarbeid 3
verdenserklÊring 5
overh¯yhet 4
frie 2
noen 2
eiendom 3
uavhengig 4
straffeanklage 5
garantier 4
familie 4
anseelse 4
reelt 2
inngÂelse 4
familien 4
ideer 3
reelle 3
uunnvÊrlige 5
arbeid 2
arbeidsforhold 4
arbeidsl¯shet 4
arbeider 3
arbeidstiden 4
ferier 3
families 4
arbeidsuf¯rhet 5
spesiell 3
h¯yere 2
religi¯se 5
materielle 5
noe 2
"""

syllablesInFile = {}

#The last 7, starting at "ai", are the Norwegian diphthongs.
subSyllableIf = ["ai","au","oy","oi","ui","øy"]
for item in subSyllableIf:
    item = item.decode("utf-8")
#Tok bort "en$" og "et$"; må forskes mer på.


#Syllables who are not counted as one, but should be.
#Between two vowels that do not form a diphthong.
addSyllableIf = ["oa", "io", "eo", "ia", "ee", "ie"]
for item in addSyllableIf:
    item = item.decode("utf-8")

# Compile the regular expressions in aubSyllableIf
for i in range(len(subSyllableIf)):
    subSyllableIf[i] = re.compile(subSyllableIf[i])
for i in range(len(addSyllableIf)):
    addSyllableIf[i] = re.compile(addSyllableIf[i])

def _stripWord(word):
    return word.strip().lower()

# Read our syllable override file and add to the syllablesInFile list
for line in specialSyllables_no.splitlines():
    line = line.strip()
    if line:
        toks = line.split()
        assert len(toks) == 2
        syllablesInFile[_stripWord(unicode(toks[0],"latin-1").encode("utf-8"))] = int(toks[1])

def count(word):
    word = unicode(word,"utf-8").encode("utf-8")
    word = _stripWord(word)

    if not word:
        return 0

    # Check for a cached syllable count
    count = syllablesInFile.get(word, -1)
    
    if count > 0:
        return count

    # Count vowel groups
    count = 0
    prev_was_vowel = 0
    vowels = [u"a", u"e", u"i", u"o", u"u", u"y", u"æ", u"ø", u"å"]
    #for vow in vowels:
        #vow = vow.decode("utf-8")
    for c in word.decode("utf-8"):
        is_vowel = c in vowels
        if is_vowel and not prev_was_vowel:
            count += 1
        prev_was_vowel = is_vowel

    # Add & subtract syllables
    for r in addSyllableIf:
        if r.search(word):
            count += 1
    for r in subSyllableIf:
        if r.search(word):
            count -= 1

    # Cache the syllable count
    syllablesInFile[word] = count
    
#    Add syllable to file
#    if count > 0:
#        file = open(syllable_path, "a")
#        file.write( unicode(word,"utf-8").encode("latin-1") + " " + str(count) + "\n")
#        file.close()

    return count

########NEW FILE########
__FILENAME__ = textanalyzer
# -*- coding: utf-8 -*- 
# Sets the encoding to utf-8 to avoid problems with æøå

import nltk.data
from nltk.tokenize import *
import syllables_en
import syllables_no
from languageclassifier import *
import logging

class textanalyzer(object):

    tokenizer = RegexpTokenizer('(?u)\W+|\$[\d\.]+|\S+')
    special_chars = ['.', ',', '!', '?']
    lang = "eng"
    
    def __init__(self, lang):
        self.lang = lang
    
    def setLang(self,lang):
        self.lang = lang

    def analyzeText(self, text=''):
        words = self.getWords(text)
        charCount = self.getCharacterCount(words)
        wordCount = len(words)
        sentenceCount = len(self.getSentences(text))
        syllablesCount = self.countSyllables(words)
        complexwordsCount = self.countComplexWords(text)
        averageWordsPerSentence = wordCount/sentenceCount
        print ' Language: ' + self.lang
        print ' Number of characters: ' + str(charCount)
        print ' Number of words: ' + str(wordCount)
        print ' Number of sentences: ' + str(sentenceCount)
        print ' Number of syllables: ' + str(syllablesCount)
        print ' Number of complex words: ' + str(complexwordsCount)
        print ' Average words per sentence: ' + str(averageWordsPerSentence)
    #analyzeText = classmethod(analyzeText)  
        

    def getCharacterCount(self, words):
        characters = 0
        for word in words:
            word = self._setEncoding(word)
            characters += len(word.decode("utf-8"))
        return characters
    #getCharacterCount = classmethod(getCharacterCount)    
        
    def getWords(self, text=''):
        text = self._setEncoding(text)
        words = []
        words = self.tokenizer.tokenize(text)
        filtered_words = []
        for word in words:
            if word in self.special_chars or word == " ":
                pass
            else:
                new_word = word.replace(",","").replace(".","")
                new_word = new_word.replace("!","").replace("?","")
                filtered_words.append(new_word)
        return filtered_words
    #getWords = classmethod(getWords)
    
    def getSentences(self, text=''):
        tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
        sentences = tokenizer.tokenize(text)
        return sentences
    #getSentences = classmethod(getSentences)
    
    def countSyllables(self, words = None):
        if words is None:
          words = []
#        if self.lang == "":
#            self.lang = NaiveBayes().classifyText(" " .join(words))
            
        if self.lang == "unknown":
            logging.warning("Unknown language, using English")
            self.lang = "eng"
        
        syllableCount = 0
        syllableCounter = {}
        syllableCounter['eng'] = syllables_en.count
        syllableCounter['no'] = syllables_no.count
        for word in words:
            syllableCount += syllableCounter[self.lang](word)
            
        return syllableCount
    #countSyllables = classmethod(countSyllables)
    
    
    #This method must be enhanced. At the moment it only
    #considers the number of syllables in a word.
    #This often results in that too many complex words are detected.
    def countComplexWords(self, text=''):
        words = self.getWords(text)
        sentences = self.getSentences(text)
        complexWords = 0
        found = False
        #Just for manual checking and debugging.
        cWords = []
        curWord = []
        
        for word in words:          
            curWord.append(word)
            if self.countSyllables(curWord)>= 3:
                
                #Checking proper nouns. If a word starts with a capital letter
                #and is NOT at the beginning of a sentence we don't add it
                #as a complex word.
                if not(word[0].isupper()):
                    complexWords += 1
                    #cWords.append(word)
                else:
                    for sentence in sentences:
                        if str(sentence).startswith(word):
                            found = True
                            break
                    
                    if found: 
                        complexWords+=1
                        found = False
                    
            curWord.remove(word)
        #print cWords
        return complexWords
    #countComplexWords = classmethod(countComplexWords)
    
    def _setEncoding(self,text):
        try:
            text = unicode(text, "utf8").encode("utf8")
        except UnicodeError:
            try:
                text = unicode(text, "iso8859_1").encode("utf8")
            except UnicodeError:
                text = unicode(text, "ascii", "replace").encode("utf8")
        return text
    #_setEncoding = classmethod(_setEncoding)
        
        
    def demo(self):
#        text = "It is for us the living, rather,\
#                \nto be dedicated here to the unfinished\
#                \nwork which they who fought here have\
#                \nthus far so nobly advanced. It is\
#                \nrather for us to be here dedicated\
#                \nto the great task remaining before us,\
#                \nthat from these honored dead we take\
#                \nincreased devotion to that cause for which they\
#                \ngave the last full measure of devotion, that we\
#                \nhere highly resolve that these dead shall not have\
#                \ndied in vain, that this nation, under God, shall have a\
#                \nnew birth of freedom, and that government of the people, by\
#                \nthe people, for the people, shall not perish from this earth."
        text = "Den 10. desember 1948 vedtok og kunngjorde De Forente Nasjoners tredje Generalforsamling Verdenserklæringen om Menneskerettighetene. Erklæringen ble vedtatt med 48 lands ja-stemmer. Ingen land stemte mot. 8 land avsto. Umiddelbart etter denne historiske begivenhet henstilte Generalforsamlingen til alle medlemsstater å bekjentgjøre Erklæringens tekst og sørge for at den blir distribuert, framvist, lest og forklart spesielt i skoler og andre læreinstitusjoner, uten hensyn til de forskjellige lands eller områders politiske status. Erklæringens offisielle tekst foreligger på FNs seks arbeidsspråk: arabisk, engelsk, fransk, kinesisk, russisk og spansk. En lang rekke av FNs medlemsstater har fulgt Generalforsamlingens oppfordring og oversatt Erklæringen til de nasjonale språk. Denne oversettelsen til norsk er utarbeidet i Utenriksdepartementet. På henvendelse til FNs nordiske informasjonskontor i København kan en få gratis eksemplarer av Erklæringen på FNs offisielle språk, de øvrige nordiske språk og et begrenset antall andre språk. VERDENSERKLÆRINGEN OM MENNESKERETTIGHETENE INNLEDNING Da anerkjennelsen av menneskeverd og like og umistelige rettigheter for alle medlemmer av menneskeslekten er grunnlaget for frihet, rettferdighet og fred i verden, da tilsidesettelse av og forakt for menneskerettighetene har ført til barbariske handlinger som har rystet menneskehetens samvittighet, og da framveksten av en verden hvor menneskene har tale- og trosfrihet og frihet fra frykt og nød, er blitt kunngjort som folkenes høyeste mål, da det er nødvendig at menneskerettighetene blir beskyttet av loven for at menneskene ikke skal tvinges til som siste utvei å gjøre opprør mot tyranni og undertrykkelse, da det er viktig å fremme utviklingen av vennskapelige forhold mellom nasjonene, da De Forente Nasjoners folk i Pakten på ny har bekreftet sin tro på grunnleggende menneskerettigheter, på menneskeverd og på like rett for menn og kvinner og har besluttet å arbeide for sosialt framskritt og bedre levevilkår under større Frihet, da medlemsstatene har forpliktet seg til i samarbeid med De Forente Nasjoner å sikre at menneskerettighetene og de grunnleggende friheter blir alminnelig respektert og overholdt, da en allmenn forståelse av disse rettigheter og friheter er av den største betydning for å virkeliggjøre denne forpliktelse, kunngjør GENERALFORSAMLINGEN nå denne VERDENSERKLÆRING OM MENNESKERETTIGHETENE som et felles mål for alle folk og alle nasjoner, for at hvert individ og hver samfunnsmyndighet, med denne erklæring stadig i tankene, skal søke gjennom undervisning og oppdragelse å fremme respekt for disse rettigheter og friheter, og ved nasjonale og internasjonale tiltak å sikre at de blir allment og effektivt anerkjent og overholdt både blant folkene i medlemsstatene selv og blant folkene i de områder som står under deres overhøyhet. Artikkel 1. Alle mennesker er født frie og med samme menneskeverd og menneskerettigheter. De er utstyrt med fornuft og samvittighet og bør handle mot hverandre i brorskapets ånd. Artikkel 2. Enhver har krav på alle de rettigheter og friheter som er nevnt i denne erklæring, uten forskjell av noen art, f. eks. på grunn av rase, farge, kjønn, språk, religion, politisk eller annen oppfatning, nasjonal eller sosial opprinnelse eiendom, fødsel eller annet forhold. Det skal heller ikke gjøres noen forskjell på grunn av den politiske, rettslige eller internasjonale stilling som innehas av det land eller det område en person hører til, enten landet er uavhengig, står under tilsyn, er ikke-selvstyrende, eller på annen måte har begrenset suverenitet. Artikkel 3. Enhver har rett til liv, frihet og personlig sikkerhet. Artikkel 4. Ingen må holdes i slaveri eller trelldom. Slaveri og slavehandel i alle former er forbudt. Artikkel 5. Ingen må utsettes for tortur eller grusom, umenneskelig eller nedverdigende behandling eller straff. Artikkel 6. Ethvert menneske har krav på overalt å bli anerkjent som rettssubjekt. Artikkel 7. Alle er like for loven og har uten diskriminering rett til samme beskyttelse av loven. Alle har krav på samme beskyttelse mot diskriminering i strid med denne erklæring og mot enhver oppfordring til slik diskriminering. Artikkel 8. Enhver har rett til effektiv hjelp av de kompetente nasjonale domstoler mot handlinger som krenker de grunnleggende rettigheter han er gitt i forfatning eller lov. Artikkel 9. Ingen må utsettes for vilkårlig arrest, fengsling eller landsforvisning. Artikkel 10. Enhver har krav på under full likestilling å få sin sak rettferdig og offentlig behandlet av en uavhengig og upartisk domstol når hans rettigheter og plikter skal fastsettes,og når en straffeanklage mot ham skal avgjøres. Artikkel 11. 1. Enhver som er anklaget for en straffbar handling har rett til å bli ansett som uskyldig til det er bevist ved offentlig domstolsbehandling, hvor han har hatt alle de garantier som er nødvendig for hans forsvar, at han er skyldig etter loven. 2. Ingen må dømmes for en handling eller unnlatelse som i henhold til nasjonal lov eller folkeretten ikke var straffbar på den tid da den ble begått. Heller ikke skal det kunne idømmes strengere straff enn den som det var hjemmel for på den tid da den straffbare handling ble begått. Artikkel 12. Ingen må utsettes for vilkårlig innblanding i privatliv, familie, hjem og korrespondanse, eller for angrep på ære og anseelse. Enhver har rett til lovens beskyttelse mot slik innblanding eller slike angrep. Artikkel 13. 1. Enhver har rett til å bevege seg fritt og til fritt å velge oppholdssted innenfor en stats grenser. 2. Enhver har rett til å forlate et hvilket som helst land innbefattet sitt eget og til å vende tilbake til sitt land. Artikkel 14. 1. Enhver har rett til i andre land å søke og ta imot asyl mot forfølgelse. 2. Denne rett kan ikke påberopes ved rettsforfølgelse som har reelt grunnlag i upolitiske forbrytelser eller handlinger som strider mot De Forente Nasjoners formål og prinsipper. Artikkel 15. 1. Enhver har rett til et statsborgerskap. Ingen skal vilkårlig berøves sitt statsborgerskap eller nektes retten til å forandre det. Artikkel 16. 1. Voksne menn og kvinner har rett til å gifte seg og stifte familie uten noen begrensning som skyldes rase, nasjonalitet eller religion. De har krav på like rettigheter ved inngåelse av ekteskapet, under ekteskapet og ved dets oppløsning. 2. Ekteskap må bare inngås etter fritt og fullt samtykke av de vordende ektefeller. 3. Familien er den naturlige og grunnleggende enhet i samfunnet og har krav på samfunnets og statens beskyttelse. Artikkel 17. 1. Enhver har rett til å eie eiendom alene eller sammen med andre. 2. Ingen må vilkårlig fratas sin eiendom. Artikkel 18. Enhver har rett til tanke-, samvittighets- og religionsfrihet. Denne rett omfatter frihet til å skifte religion eller tro, og frihet til enten alene eller sammen med andre, og offentlig eller privat, å gi uttrykk for sin religion eller tro gjennom undervisning, utøvelse, tilbedelse og ritualer. Artikkel 19. Enhver har rett til menings- og ytringsfrihet. Denne rett omfatter frihet til å hevde meninger uten innblanding og til å søke, motta og meddele opplysninger og ideer gjennom ethvert meddelelsesmiddel og uten hensyn til landegrenser. Artikkel 20. 1. Enhver har rett til fritt å delta i fredelige møter og organisasjoner. 2. Ingen må tvinges til å tilhøre en organisasjon. Artikkel 21. 1. Enhver har rett til å ta del i sitt lands styre, direkte eller gjennom fritt valgte representanter. 2. Enhver har rett til lik adgang til offentlig tjeneste i sitt land. 3. Folkets vilje skal være grunnlaget for offentlig myndighet. Denne vilje skal komme til uttrykk gjennom periodiske og reelle valg med allmenn og lik stemmerett og med hemmelig avstemning eller likeverdig fri stemmemåte. Artikkel 22. Enhver har som medlem av samfunnet rett til sosial trygghet og har krav på at de økonomiske, sosiale og kulturelle goder som er uunnværlige for hans verdighet og den frie utvikling av hans personlighet, blir skaffet til veie gjennom nasjonale tiltak og internasjonalt samarbeid i samsvar med hver enkelt stats organisasjon og ressurser. Artikkel 23. 1. Enhver har rett til arbeid, til fritt valg av yrke, til rettferdige og gode arbeidsforhold og til beskyttelse mot arbeidsløshet. 2. Enhver har uten diskriminering rett til lik betaling for likt arbeid. 3. Enhver som arbeider har rett til en rettferdig og god betaling som sikrer hans familie og ham selv en menneskeverdig tilværelse, og som om nødvendig blir utfylt ved annen sosial beskyttelse. 4. Enhver har rett til å danne og gå inn i fagforeninger for å beskytte sine interesser. Artikkel 24. Enhver har rett til hvile og fritid, herunder rimelig begrensning av arbeidstiden og regelmessige ferier med lønn. Artikkel 25. 1. Enhver har rett til en levestandard som er tilstrekkelig for hans og hans families helse og velvære, og som omfatter mat, klær, bolig og helseomsorg og nødvendige sosiale ytelser, og rett til trygghet i tilfelle av arbeidsløshet, sykdom, arbeidsuførhet, enkestand, alderdom eller annen mangel på eksistensmuligheter som skyldes forhold han ikke er herre over. 2. Mødre og barn har rett til spesiell omsorg og hjelp. Alle barn skal ha samme sosiale beskyttelse enten de er født i eller utenfor ekteskap. Artikkel 26. 1. Enhver har rett til undervisning. Undervisningen skal være gratis, i det minste på de elementære og grunnleggende trinn. Elementærundervisning skal være obligatorisk. Alle skal ha adgang til yrkesopplæring, og det skal være lik adgang for alle til høyere undervisning på grunnlag av kvalifikasjoner. 2. Undervisningen skal ta sikte på å utvikle den menneskelige personlighet og styrke respekten for menneskerettighetene og de grunnleggende friheter. Den skal fremme forståelse, toleranse og vennskap mellom alle nasjoner og rasegrupper eller religiøse grupper og skal støtte De Forente Nasjoners arbeid for å opprettholde fred. 3. Foreldre har fortrinnsrett til å bestemme hva slags undervisning deres barn skal få. Artikkel 27. 1. Enhver har rett til fritt å delta i samfunnets kulturelle liv, til å nyte kunst og til å få del i den vitenskapelige framgang og dens goder. 2. Enhver har rett til beskyttelse av de åndelige og materielle interesser som er et resultat av ethvert vitenskapelig, litterært eller kunstnerisk verk som han har skapt. Artikkel 28. Enhver har krav på en sosial og internasjonal orden som fullt ut kan virkeliggjøre de rettigheter og friheter som er nevnt i denne erklæring. Artikkel 29. 1. Enhver har plikter overfor samfunnet som alene gjør den frie og fulle utvikling av hans personlighet mulig. 2. Under utøvelsen av sine rettigheter og friheter skal enhver bare være undergitt slike begrensninger som er fastsatt i lov utelukkende med det formål å sikre den nødvendige anerkjennelse av og respekt for andres rettigheter og friheter, og de krav som moralen, den offentlige orden og den alminnelige velferd i et demokratisk samfunn med rette stiller. 3. Disse rettigheter og friheter må ikke i noe tilfelle utøves i strid med De Forente Nasjoners formål og prinsipper. Artikkel 30. Intet i denne erklæring skal tolkes slik at det gir noen stat, gruppe eller person rett til å ta del i noen virksomhet eller foreta noen handling som tar sikte på å ødelegge noen av de rettigheter og friheter som er nevnt i Erklæringen." 

        print "The text : \n" + ("=" * 40)
        print text
        print ("=" * 40) + "\nHas the following statistics\n" + ("=" * 40)
        nb = NaiveBayes()
        ta = textanalyzer(nb.classifyText(text))
        ta.analyzeText(text)
        pass
    demo = classmethod(demo)
    
def demo():
    textanalyzer.demo()
    
if __name__ == "__main__":
    textanalyzer.demo()

########NEW FILE########
__FILENAME__ = urlextracter
# -*- coding: utf-8 -*- 
# Sets the encoding to utf-8 to avoid problems with æøå
import urllib
import htmlentitydefs
from sgmllib import *
import re

class URLextracter(SGMLParser):
    
    style = False
    script = False
    hyperlink = False
    
    linklist = []
    
    def __init__(self, url='http://python.org'):
        self.reset()
        try:
            self.sock = urllib.urlopen(url)
            self.feed(self.sock.read())
            self.sock.close()
            self.close()
        except IOError:
            print "Could not connect, or the markup has (too) bad structure"
            raise SGMLParseError
        
    def start_style(self,attr):
        self.style = True
    
    def end_style(self):
        self.style = False
        
    def start_script(self,attr):
        self.script = True
    
    def do_script(self,attr):
        pass
    
    def end_script(self):
        self.script = False
        
    def start_a(self,attr):
        self.hyperlink = True
        href = [v for k, v in attr if k=='href']
        if href:
            self.linklist.extend(href)
    
    def end_a(self):
        self.hyperlink = False
   
    def reset(self):
        self.pieces = []
        self.linklist = []
        SGMLParser.reset(self)
        
    def handle_data(self, text):
        #tags_to_skip = [self.style, self.script, self.hyperlink]
        tags_to_skip = [self.style, self.script]
        if True in tags_to_skip:
            pass
        else: 
            self.pieces.append(text)
    
    def handle_declaration(self,decl):
        pass
    
    def report_unbalanced(self,tag):
        pass
    
    def unknown_starttag(self, tag, attrs):
        pass

    def unknown_endtag(self, tag):         
        pass
    
    def unknown_charref(self,tag):
        pass

        
    def output(self):
        # Clean up text and return it
        content = "" . join(self.pieces)
        content = self._setEncoding(content)
        content = re.sub(r"[^0-9a-zA-ZæøåÆØÅ.?!\n\r]", " ", content)
        content = content.strip()
        #lines = content.replace("\t", " ").replace("\r","\n").splitlines()
        lines = content.splitlines()
        content = ""
        for line in lines:
            temp = line.strip()
            if temp == "" or temp == " ":
                lines.remove(line)
            else:    
                if len(temp.split(" ")) > 2:
                    content += line + "\n"
        return content
        
        
    def _setEncoding(self,text):
        try:
            text = unicode(text, "utf8").encode("utf8")
        except UnicodeError:
            try:
                text = unicode(text, "iso8859_1").encode("utf8")
            except UnicodeError:
                text = unicode(text, "ascii", "replace").encode("utf8")
        return text
    
    def demo(self):
        print 'This class takes an URL, and extracts the text it contains.'
        print 'It also removes special characters and numbers,'
        print 'and sentences must consist of at least'
        print '3 words to not be ignored.'
        
        print '\nFetching text from www.python.org'
        u = URLextracter()
        print "=" * 40
        print u.output()
    demo = classmethod(demo)

def demo():
    URLextracter.demo()

if __name__ == "__main__":
    demo()

########NEW FILE########
__FILENAME__ = referring
# Natural Language Toolkit: Generating referring expressions
#
# Author: Margaret Mitchell <itallow@u.washington.edu>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

import sys
import re

class IncrementalAlgorithm:
    """ 
    An implementation of the Incremental Algorithm, introduced in:
    Dale, Robert and Reiter, Ehud. (1995). 
    "Computational Interpretations of the Gricean Maxims 
     in the Generation of Referring Expressions".
    Cognitive Science, 18, 233-263.
    """

    def __init__(self, KB, r, contrast_set, preferred_attrs):
        self.L = []
        self.KB = KB
        self.r = r
        self.C = contrast_set
        self.P = preferred_attrs
        self.RE = self.make_referring_expression()
        
    def make_referring_expression(self):
        """
        This returns a list of attribute-value pairs which
        specify a referring expression for the intended referent.
        The attributes are tried in the order specified in self.P,
        the preferred attributes list, and a value for 'type' is always
        included, even if 'type' has no discriminatory power.        
        """

        for attr in self.P:
            if attr in self.r:
                value = self.find_best_value(attr, \
                                       self.KB.basic_level_value(self.r, attr))
            else:
                continue
            rules_out_list = self.rules_out((attr, value))
            if rules_out_list != []:
                self.L += [(attr, value)]
                for distractor in rules_out_list:
                    self.C.remove(distractor)
            if self.C == []:
                head_noun = False
                # Catch-all to make sure that the head noun, 'type', 
                # is included.
                for attr_value in self.L:
                    if "type" == attr_value[0]:
                        head_noun = True
                if head_noun == False:
                    self.L = [("type", \
                           self.KB.basic_level_value(self.r, "type"))] + self.L
                else:
                    return self.L

    def find_best_value(self, attr, init_val):
        """
        Takes an attribute and an initial value; it returns a value 
        for that attribute that is subsumed by the initial value, 
        accurately describes the intended referent, rules out as many 
        distractors as possible, and, subject to these constraints, 
        is as close as possible in the taxonomy to the initial value.
        """

        if self.KB.user_knows(self.r, (attr, init_val)):
            value = init_val
        else:
            value = None

        more_specific_value = self.KB.more_specific_value(self.r, attr, value)
        if more_specific_value is not None:
            new_value = self.find_best_value(attr, more_specific_value)
            if new_value is not None and \
            self.rules_out((attr, new_value)) > self.rules_out((attr, value)):
                value = new_value
        return value

    def rules_out(self, attr_value_pair):
        """
        Takes an attribute-value pair and returns the elements of the
        set of remaining distractors that are ruled out by this
        attribute-value pair.
        """        
        
        attr = attr_value_pair[0]
        value = attr_value_pair[1]
        if value is None:
            return []
        rules_out_list = []
        for distractor in self.C:
            if self.KB.user_knows(distractor, (attr, value)) is False:
                rules_out_list += [distractor]
        return rules_out_list


class BackgroundKnowledge:
    """
    Interface functions required by the Incremental Algorithm.
    Although these functions are included for use here, 
    their nature varies depending on the program using them,
    and should in practice be created independently of this module.
    """

    def __init__(self, specificity_hash, user_knowledge=None):
        self.specificity_hash = specificity_hash
        self.user_knowledge = user_knowledge

        if user_knowledge is None:
            self.user_knowledge = specificity_hash

    def more_specific_value(self, entity, attr, value):
        """ 
        Returns a new value for 'attr' where that value is 
        more specific than 'value'.  If no more specific 
        value of 'attr' is known, returns None. 
        """
        
        if value in self.specificity_hash:
            spec_tuples = self.specificity_hash[value]
            val_list = []
            found_vals = self._recurse_values(spec_tuples, val_list)
            for found_val in found_vals:
                if found_val == entity[attr] or len(found_vals) == 1:
                    return found_val
        else:
            for basic_value in self.specificity_hash:
                spec_tuples = self.specificity_hash[basic_value]
                spec_val = self._find_more_specific_val(spec_tuples, value)
                if spec_val is not None:
                    return spec_val
        return None

    def _find_more_specific_val(self, spec_tuples, value):
        """
        Recursively searches for a more specific value for a given value.
        """

        x = 0
        while x < len(spec_tuples):
            spec_val = spec_tuples[x]
            if spec_val == value:
                new_val = spec_tuples[x + 1]
                if new_val == str(new_val):
                    return new_val
                elif spec_val == tuple(spec_val):
                    new_val = self._find_more_specific_val(spec_val, value)
                    if new_val is not None:
                        return new_val
            x += 1  
        return None

    def basic_level_value(self, entity, attr):
        """ 
        Returns the basic-level value of an attribute of an object. 
        """

        value = entity[attr]
        for basic_value in self.specificity_hash:
            spec_tuples = self.specificity_hash[basic_value]
            val_list = []
            found_vals = self._recurse_values(spec_tuples, val_list)
            for found_val in found_vals:
                if found_val == value:
                    value = basic_value
                    break
        return value

    def _recurse_values(self, spec_tuples, val_list):
        """
        Function used by basic_level_value to return a non-nested
        list of values under a basic level.  This is helpful when
        it doesn't matter how specific the value is, what matters
        is what its basic level value is.
        """

        for sub_val in spec_tuples:
            if sub_val == str(sub_val):
                val_list += [sub_val]
            if sub_val == tuple(sub_val):
                return self._recurse_values(sub_val, val_list)
        return val_list

    # For now, the user knowledge is the same as the specificity list.
    # This could be expanded to use wordnet as the wordnet modules expand.
    def user_knows(self, entity, attr_value_pair):
        """ 
        Returns true if the user knows or can easily determine
        that the attribute-value pair applies to the object;
        false if the user knows or can easily determine that the
        attribute-value pair does not apply to the object;
        and None otherwise.  
        """

        attr = attr_value_pair[0]
        value = attr_value_pair[1]
        if value == tuple(value):
            for spec_value in value:
                return self.user_knows(entity, (attr, spec_value))
        elif attr in entity:
            if value in entity[attr]:
                return True
            else:
                for gen_value in self.user_knowledge:
                    if gen_value == value:
                        spec_values = self.user_knowledge[gen_value]
                        for spec_value in spec_values:
                            if self.user_knows(entity, (attr, spec_value)):
                                return True
            return False
        return None
        
def demo():
    specificity_hash = {"dog" : (("chihuahua", "long-haired chihuahua"), "poodle"),
                        "cat" : ("siamese-cat", "tabby")}
    KB = BackgroundKnowledge(specificity_hash)
    Object1 = {"type":"chihuahua", "size":"small", "colour":"black"}
    Object2 = {"type":"chihuahua", "size":"large", "colour":"white"}
    Object3 = {"type":"siamese-cat", "size":"small", "colour":"black"}

    print "Given an entity defined as: "
    r = Object1
    print r
    preferred_attrs = ["type", "colour", "size"]
    print "In a set defined as: "
    contrast_set = [Object2, Object3]
    print contrast_set
    RE = IncrementalAlgorithm(KB, r, contrast_set, preferred_attrs).RE
    print "The referring expression created to uniquely identify",
    print "the referent is: "
    print RE
    RE_string = ""
    for attr, val in RE:
        RE_string = val + " " + RE_string
    RE_string = "The " + RE_string    
    print "This can be surface-realized as:"
    print RE_string

if __name__ == "__main__":
    demo()

########NEW FILE########
__FILENAME__ = constraint
#!/usr/bin/python
#
# Copyright 2005 Gustavo Niemeyer <niemeyer@conectiva.com>
#    Originally licensed under the GNU General Public License
# 
#    Relicensed with permission under the Apache License, Version 2.0 (the
#    "License"); you may not use this file except in compliance with the
#    License.  You may obtain a copy of the License at
# 
#        http://www.apache.org/licenses/LICENSE-2.0
# 
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS,
#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#    See the License for the specific language governing permissions and
#    limitations under the License.
#
"""
@var Unassigned: Helper object instance representing unassigned values

@sort: Problem, Variable, Domain
@group Solvers: Solver,
                BacktrackingSolver,
                RecursiveBacktrackingSolver,
                MinConflictsSolver
@group Constraints: Constraint,
                    FunctionConstraint,
                    AllDifferentConstraint,
                    AllEqualConstraint,
                    MaxSumConstraint,
                    ExactSumConstraint,
                    MinSumConstraint,
                    InSetConstraint,
                    NotInSetConstraint,
                    SomeInSetConstraint,
                    SomeNotInSetConstraint
"""
import random
import copy

__all__ = ["Problem", "Variable", "Domain", "Unassigned",
           "Solver", "BacktrackingSolver", "RecursiveBacktrackingSolver",
           "MinConflictsSolver", "Constraint", "FunctionConstraint",
           "AllDifferentConstraint", "AllEqualConstraint", "MaxSumConstraint",
           "ExactSumConstraint", "MinSumConstraint", "InSetConstraint",
           "NotInSetConstraint", "SomeInSetConstraint",
           "SomeNotInSetConstraint"]

class Problem(object):
    """
    Class used to define a problem and retrieve solutions
    """

    def __init__(self, solver=None):
        """
        @param solver: Problem solver used to find solutions
                       (default is L{BacktrackingSolver})
        @type solver:  instance of a L{Solver} subclass
        """
        self._solver = solver or BacktrackingSolver()
        self._constraints = []
        self._variables = {}

    def reset(self):
        """
        Reset the current problem definition

        Example:

        >>> problem = Problem()
        >>> problem.addVariable("a", [1, 2])
        >>> problem.reset()
        >>> problem.getSolution()
        >>>
        """
        del self._constraints[:]
        self._variables.clear()

    def setSolver(self, solver):
        """
        Change the problem solver currently in use

        Example:

        >>> solver = BacktrackingSolver()
        >>> problem = Problem(solver)
        >>> problem.getSolver() is solver
        True

        @param solver: New problem solver
        @type  solver: instance of a C{Solver} subclass
        """
        self._solver = solver

    def getSolver(self):
        """
        Obtain the problem solver currently in use

        Example:

        >>> solver = BacktrackingSolver()
        >>> problem = Problem(solver)
        >>> problem.getSolver() is solver
        True

        @return: Solver currently in use
        @rtype: instance of a L{Solver} subclass
        """
        return self._solver

    def addVariable(self, variable, domain):
        """
        Add a variable to the problem

        Example:

        >>> problem = Problem()
        >>> problem.addVariable("a", [1, 2])
        >>> problem.getSolution() in ({'a': 1}, {'a': 2})
        True

        @param variable: Object representing a problem variable
        @type  variable: hashable object
        @param domain: Set of items defining the possible values that
                       the given variable may assume
        @type  domain: list, tuple, or instance of C{Domain}
        """
        if variable in self._variables:
            raise ValueError, "Tried to insert duplicated variable %s" % \
                              repr(variable)
        if type(domain) in (list, tuple):
            domain = Domain(domain)
        elif isinstance(domain, Domain):
            domain = copy.copy(domain)
        else:
            raise TypeError, "Domains must be instances of subclasses of "\
                             "the Domain class"
        if not domain:
            raise ValueError, "Domain is empty"
        self._variables[variable] = domain

    def addVariables(self, variables, domain):
        """
        Add one or more variables to the problem

        Example:

        >>> problem = Problem()
        >>> problem.addVariables(["a", "b"], [1, 2, 3])
        >>> solutions = problem.getSolutions()
        >>> len(solutions)
        9
        >>> {'a': 3, 'b': 1} in solutions
        True

        @param variables: Any object containing a sequence of objects
                          represeting problem variables
        @type  variables: sequence of hashable objects
        @param domain: Set of items defining the possible values that
                       the given variables may assume
        @type  domain: list, tuple, or instance of C{Domain}
        """
        for variable in variables:
            self.addVariable(variable, domain)

    def addConstraint(self, constraint, variables=None):
        """
        Add a constraint to the problem

        Example:

        >>> problem = Problem()
        >>> problem.addVariables(["a", "b"], [1, 2, 3])
        >>> problem.addConstraint(lambda a, b: b == a+1, ["a", "b"])
        >>> solutions = problem.getSolutions()
        >>> 

        @param constraint: Constraint to be included in the problem
        @type  constraint: instance a L{Constraint} subclass or a
                           function to be wrapped by L{FunctionConstraint}
        @param variables: Variables affected by the constraint (default to
                          all variables). Depending on the constraint type
                          the order may be important.
        @type  variables: set or sequence of variables
        """
        if not isinstance(constraint, Constraint):
            if callable(constraint):
                constraint = FunctionConstraint(constraint)
            else:
                raise ValueError, "Constraints must be instances of "\
                                  "subclasses of the Constraint class"
        self._constraints.append((constraint, variables))

    def getSolution(self):
        """
        Find and return a solution to the problem

        Example:

        >>> problem = Problem()
        >>> problem.getSolution() is None
        True
        >>> problem.addVariables(["a"], [42])
        >>> problem.getSolution()
        {'a': 42}

        @return: Solution for the problem
        @rtype: dictionary mapping variables to values
        """
        domains, constraints, vconstraints = self._getArgs()
        if not domains:
            return None
        return self._solver.getSolution(domains, constraints, vconstraints)

    def getSolutions(self):
        """
        Find and return all solutions to the problem

        Example:

        >>> problem = Problem()
        >>> problem.getSolutions() == []
        True
        >>> problem.addVariables(["a"], [42])
        >>> problem.getSolutions()
        [{'a': 42}]

        @return: All solutions for the problem
        @rtype: list of dictionaries mapping variables to values
        """
        domains, constraints, vconstraints = self._getArgs()
        if not domains:
            return []
        return self._solver.getSolutions(domains, constraints, vconstraints)

    def getSolutionIter(self):
        """
        Return an iterator to the solutions of the problem

        Example:

        >>> problem = Problem()
        >>> list(problem.getSolutionIter()) == []
        True
        >>> problem.addVariables(["a"], [42])
        >>> iter = problem.getSolutionIter()
        >>> iter.next()
        {'a': 42}
        >>> iter.next()
        Traceback (most recent call last):
          File "<stdin>", line 1, in ?
        StopIteration
        """
        domains, constraints, vconstraints = self._getArgs()
        if not domains:
            return iter(())
        return self._solver.getSolutionIter(domains, constraints,
                                            vconstraints)

    def _getArgs(self):
        domains = self._variables.copy()
        allvariables = domains.keys()
        constraints = []
        for constraint, variables in self._constraints:
            if not variables:
                variables = allvariables
            constraints.append((constraint, variables))
        vconstraints = {}
        for variable in domains:
            vconstraints[variable] = []
        for constraint, variables in constraints:
            for variable in variables:
                vconstraints[variable].append((constraint, variables))
        for constraint, variables in constraints[:]:
            constraint.preProcess(variables, domains,
                                  constraints, vconstraints)
        for domain in domains.values():
            domain.resetState()
            if not domain:
                return None, None, None
        #doArc8(getArcs(domains, constraints), domains, {})
        return domains, constraints, vconstraints

# ----------------------------------------------------------------------
# Solvers
# ----------------------------------------------------------------------

def getArcs(domains, constraints):
    """
    Return a dictionary mapping pairs (arcs) of constrained variables

    @attention: Currently unused.
    """
    arcs = {}
    for x in constraints:
        constraint, variables = x
        if len(variables) == 2:
            variable1, variable2 = variables
            arcs.setdefault(variable1, {})\
                .setdefault(variable2, [])\
                .append(x)
            arcs.setdefault(variable2, {})\
                .setdefault(variable1, [])\
                .append(x)
    return arcs

def doArc8(arcs, domains, assignments):
    """
    Perform the ARC-8 arc checking algorithm and prune domains

    @attention: Currently unused.
    """
    check = dict.fromkeys(domains, True)
    while check:
        variable, _ = check.popitem()
        if variable not in arcs or variable in assignments:
            continue
        domain = domains[variable]
        arcsvariable = arcs[variable]
        for othervariable in arcsvariable:
            arcconstraints = arcsvariable[othervariable]
            if othervariable in assignments:
                otherdomain = [assignments[othervariable]]
            else:
                otherdomain = domains[othervariable]
            if domain:
                changed = False
                for value in domain[:]:
                    assignments[variable] = value
                    if otherdomain:
                        for othervalue in otherdomain:
                            assignments[othervariable] = othervalue
                            for constraint, variables in arcconstraints:
                                if not constraint(variables, domains,
                                                  assignments, True):
                                    break
                            else:
                                # All constraints passed. Value is safe.
                                break
                        else:
                            # All othervalues failed. Kill value.
                            domain.hideValue(value)
                            changed = True
                        del assignments[othervariable]
                del assignments[variable]
                #if changed:
                #    check.update(dict.fromkeys(arcsvariable))
            if not domain:
                return False
    return True

class Solver(object):
    """
    Abstract base class for solvers

    @sort: getSolution, getSolutions, getSolutionIter
    """

    def getSolution(self, domains, constraints, vconstraints):
        """
        Return one solution for the given problem

        @param domains: Dictionary mapping variables to their domains
        @type  domains: dict
        @param constraints: List of pairs of (constraint, variables)
        @type  constraints: list
        @param vconstraints: Dictionary mapping variables to a list of
                             constraints affecting the given variables.
        @type  vconstraints: dict
        """
        raise NotImplementedError, \
              "%s is an abstract class" % self.__class__.__name__

    def getSolutions(self, domains, constraints, vconstraints):
        """
        Return all solutions for the given problem

        @param domains: Dictionary mapping variables to domains
        @type  domains: dict
        @param constraints: List of pairs of (constraint, variables)
        @type  constraints: list
        @param vconstraints: Dictionary mapping variables to a list of
                             constraints affecting the given variables.
        @type  vconstraints: dict
        """
        raise NotImplementedError, \
              "%s provides only a single solution" % self.__class__.__name__

    def getSolutionIter(self, domains, constraints, vconstraints):
        """
        Return an iterator for the solutions of the given problem

        @param domains: Dictionary mapping variables to domains
        @type  domains: dict
        @param constraints: List of pairs of (constraint, variables)
        @type  constraints: list
        @param vconstraints: Dictionary mapping variables to a list of
                             constraints affecting the given variables.
        @type  vconstraints: dict
        """
        raise NotImplementedError, \
              "%s doesn't provide iteration" % self.__class__.__name__

class BacktrackingSolver(Solver):
    """
    Problem solver with backtracking capabilities

    Examples:

    >>> result = [[('a', 1), ('b', 2)],
    ...           [('a', 1), ('b', 3)],
    ...           [('a', 2), ('b', 3)]]

    >>> problem = Problem(BacktrackingSolver())
    >>> problem.addVariables(["a", "b"], [1, 2, 3])
    >>> problem.addConstraint(lambda a, b: b > a, ["a", "b"])

    >>> solution = problem.getSolution()
    >>> sorted(solution.items()) in result
    True

    >>> for solution in problem.getSolutionIter():
    ...     sorted(solution.items()) in result
    True
    True
    True

    >>> for solution in problem.getSolutions():
    ...     sorted(solution.items()) in result
    True
    True
    True
    """#"""

    def __init__(self, forwardcheck=True):
        """
        @param forwardcheck: If false forward checking will not be requested
                             to constraints while looking for solutions
                             (default is true)
        @type  forwardcheck: bool
        """
        self._forwardcheck = forwardcheck

    def getSolutionIter(self, domains, constraints, vconstraints):
        forwardcheck = self._forwardcheck
        assignments = {}

        queue = []

        while True:

            # Mix the Degree and Minimum Remaing Values (MRV) heuristics
            lst = [(-len(vconstraints[variable]),
                    len(domains[variable]), variable) for variable in domains]
            lst.sort()
            for item in lst:
                if item[-1] not in assignments:
                    # Found unassigned variable
                    variable = item[-1]
                    values = domains[variable][:]
                    if forwardcheck:
                        pushdomains = [domains[x] for x in domains
                                                   if x not in assignments and
                                                      x != variable]
                    else:
                        pushdomains = None
                    break
            else:
                # No unassigned variables. We've got a solution. Go back
                # to last variable, if there's one.
                yield assignments.copy()
                if not queue:
                    return
                variable, values, pushdomains = queue.pop()
                if pushdomains:
                    for domain in pushdomains:
                        domain.popState()

            while True:
                # We have a variable. Do we have any values left?
                if not values:
                    # No. Go back to last variable, if there's one.
                    del assignments[variable]
                    while queue:
                        variable, values, pushdomains = queue.pop()
                        if pushdomains:
                            for domain in pushdomains:
                                domain.popState()
                        if values:
                            break
                        del assignments[variable]
                    else:
                        return

                # Got a value. Check it.
                assignments[variable] = values.pop()

                if pushdomains:
                    for domain in pushdomains:
                        domain.pushState()

                for constraint, variables in vconstraints[variable]:
                    if not constraint(variables, domains, assignments,
                                      pushdomains):
                        # Value is not good.
                        break
                else:
                    break

                if pushdomains:
                    for domain in pushdomains:
                        domain.popState()

            # Push state before looking for next variable.
            queue.append((variable, values, pushdomains))

        raise RuntimeError, "Can't happen"

    def getSolution(self, domains, constraints, vconstraints):
        iter = self.getSolutionIter(domains, constraints, vconstraints)
        try:
            return iter.next()
        except StopIteration:
            return None

    def getSolutions(self, domains, constraints, vconstraints):
        return list(self.getSolutionIter(domains, constraints, vconstraints))


class RecursiveBacktrackingSolver(Solver):
    """
    Recursive problem solver with backtracking capabilities

    Examples:

    >>> result = [[('a', 1), ('b', 2)],
    ...           [('a', 1), ('b', 3)],
    ...           [('a', 2), ('b', 3)]]

    >>> problem = Problem(RecursiveBacktrackingSolver())
    >>> problem.addVariables(["a", "b"], [1, 2, 3])
    >>> problem.addConstraint(lambda a, b: b > a, ["a", "b"])

    >>> solution = problem.getSolution()
    >>> sorted(solution.items()) in result
    True

    >>> for solution in problem.getSolutions():
    ...     sorted(solution.items()) in result
    True
    True
    True

    >>> problem.getSolutionIter()
    Traceback (most recent call last):
       ...
    NotImplementedError: RecursiveBacktrackingSolver doesn't provide iteration
    """#"""

    def __init__(self, forwardcheck=True):
        """
        @param forwardcheck: If false forward checking will not be requested
                             to constraints while looking for solutions
                             (default is true)
        @type  forwardcheck: bool
        """
        self._forwardcheck = forwardcheck

    def recursiveBacktracking(self, solutions, domains, vconstraints,
                              assignments, single):

        # Mix the Degree and Minimum Remaing Values (MRV) heuristics
        lst = [(-len(vconstraints[variable]),
                len(domains[variable]), variable) for variable in domains]
        lst.sort()
        for item in lst:
            if item[-1] not in assignments:
                # Found an unassigned variable. Let's go.
                break
        else:
            # No unassigned variables. We've got a solution.
            solutions.append(assignments.copy())
            return solutions

        variable = item[-1]
        assignments[variable] = None

        forwardcheck = self._forwardcheck
        if forwardcheck:
            pushdomains = [domains[x] for x in domains if x not in assignments]
        else:
            pushdomains = None

        for value in domains[variable]:
            assignments[variable] = value
            if pushdomains:
                for domain in pushdomains:
                    domain.pushState()
            for constraint, variables in vconstraints[variable]:
                if not constraint(variables, domains, assignments,
                                  pushdomains):
                    # Value is not good.
                    break
            else:
                # Value is good. Recurse and get next variable.
                self.recursiveBacktracking(solutions, domains, vconstraints,
                                           assignments, single)
                if solutions and single:
                    return solutions
            if pushdomains:
                for domain in pushdomains:
                    domain.popState()
        del assignments[variable]
        return solutions

    def getSolution(self, domains, constraints, vconstraints):
        solutions = self.recursiveBacktracking([], domains, vconstraints,
                                               {}, True)
        return solutions and solutions[0] or None

    def getSolutions(self, domains, constraints, vconstraints):
        return self.recursiveBacktracking([], domains, vconstraints,
                                          {}, False)


class MinConflictsSolver(Solver):
    """
    Problem solver based on the minimum conflicts theory

    Examples:

    >>> result = [[('a', 1), ('b', 2)],
    ...           [('a', 1), ('b', 3)],
    ...           [('a', 2), ('b', 3)]]

    >>> problem = Problem(MinConflictsSolver())
    >>> problem.addVariables(["a", "b"], [1, 2, 3])
    >>> problem.addConstraint(lambda a, b: b > a, ["a", "b"])

    >>> solution = problem.getSolution()
    >>> sorted(solution.items()) in result
    True

    >>> problem.getSolutions()
    Traceback (most recent call last):
       ...
    NotImplementedError: MinConflictsSolver provides only a single solution

    >>> problem.getSolutionIter()
    Traceback (most recent call last):
       ...
    NotImplementedError: MinConflictsSolver doesn't provide iteration
    """#"""

    def __init__(self, steps=1000):
        """
        @param steps: Maximum number of steps to perform before giving up
                      when looking for a solution (default is 1000)
        @type  steps: int
        """
        self._steps = steps

    def getSolution(self, domains, constraints, vconstraints):
        assignments = {}
        # Initial assignment
        for variable in domains:
            assignments[variable] = random.choice(domains[variable])
        for _ in xrange(self._steps):
            conflicted = False
            lst = domains.keys()
            random.shuffle(lst)
            for variable in lst:
                # Check if variable is not in conflict
                for constraint, variables in vconstraints[variable]:
                    if not constraint(variables, domains, assignments):
                        break
                else:
                    continue
                # Variable has conflicts. Find values with less conflicts.
                mincount = len(vconstraints[variable])
                minvalues = []
                for value in domains[variable]:
                    assignments[variable] = value
                    count = 0
                    for constraint, variables in vconstraints[variable]:
                        if not constraint(variables, domains, assignments):
                            count += 1
                    if count == mincount:
                        minvalues.append(value)
                    elif count < mincount:
                        mincount = count
                        del minvalues[:]
                        minvalues.append(value)
                # Pick a random one from these values.
                assignments[variable] = random.choice(minvalues)
                conflicted = True
            if not conflicted:
                return assignments
        return None

# ----------------------------------------------------------------------
# Variables
# ----------------------------------------------------------------------

class Variable(object):
    """
    Helper class for variable definition

    Using this class is optional, since any hashable object,
    including plain strings and integers, may be used as variables.
    """

    def __init__(self, name):
        """
        @param name: Generic variable name for problem-specific purposes
        @type  name: string
        """
        self.name = name

    def __repr__(self):
        return self.name

Unassigned = Variable("Unassigned")

# ----------------------------------------------------------------------
# Domains
# ----------------------------------------------------------------------

class Domain(list):
    """
    Class used to control possible values for variables

    When list or tuples are used as domains, they are automatically
    converted to an instance of that class.
    """

    def __init__(self, set):
        """
        @param set: Set of values that the given variables may assume
        @type  set: set of objects comparable by equality
        """
        list.__init__(self, set)
        self._hidden = []
        self._states = []

    def resetState(self):
        """
        Reset to the original domain state, including all possible values
        """
        self.extend(self._hidden)
        del self._hidden[:]
        del self._states[:]

    def pushState(self):
        """
        Save current domain state
        
        Variables hidden after that call are restored when that state
        is popped from the stack.
        """
        self._states.append(len(self))

    def popState(self):
        """
        Restore domain state from the top of the stack

        Variables hidden since the last popped state are then available
        again.
        """
        diff = self._states.pop()-len(self)
        if diff:
            self.extend(self._hidden[-diff:])
            del self._hidden[-diff:]

    def hideValue(self, value):
        """
        Hide the given value from the domain

        After that call the given value won't be seen as a possible value
        on that domain anymore. The hidden value will be restored when the
        previous saved state is popped.

        @param value: Object currently available in the domain
        """
        list.remove(self, value)
        self._hidden.append(value)

# ----------------------------------------------------------------------
# Constraints
# ----------------------------------------------------------------------

class Constraint(object):
    """
    Abstract base class for constraints
    """ 

    def __call__(self, variables, domains, assignments, forwardcheck=False):
        """
        Perform the constraint checking

        If the forwardcheck parameter is not false, besides telling if
        the constraint is currently broken or not, the constraint
        implementation may choose to hide values from the domains of
        unassigned variables to prevent them from being used, and thus
        prune the search space.

        @param variables: Variables affected by that constraint, in the
                          same order provided by the user
        @type  variables: sequence
        @param domains: Dictionary mapping variables to their domains
        @type  domains: dict
        @param assignments: Dictionary mapping assigned variables to their
                            current assumed value
        @type  assignments: dict
        @param forwardcheck: Boolean value stating whether forward checking
                             should be performed or not
        @return: Boolean value stating if this constraint is currently
                 broken or not
        @rtype: bool
        """#"""
        return True

    def preProcess(self, variables, domains, constraints, vconstraints):
        """
        Preprocess variable domains

        This method is called before starting to look for solutions,
        and is used to prune domains with specific constraint logic
        when possible. For instance, any constraints with a single
        variable may be applied on all possible values and removed,
        since they may act on individual values even without further
        knowledge about other assignments.

        @param variables: Variables affected by that constraint, in the
                          same order provided by the user
        @type  variables: sequence
        @param domains: Dictionary mapping variables to their domains
        @type  domains: dict
        @param constraints: List of pairs of (constraint, variables)
        @type  constraints: list
        @param vconstraints: Dictionary mapping variables to a list of
                             constraints affecting the given variables.
        @type  vconstraints: dict
        """#"""
        if len(variables) == 1:
            variable = variables[0]
            domain = domains[variable]
            for value in domain[:]:
                if not self(variables, domains, {variable: value}):
                    domain.remove(value)
            constraints.remove((self, variables))
            vconstraints[variable].remove((self, variables))

    def forwardCheck(self, variables, domains, assignments,
                     _unassigned=Unassigned):
        """
        Helper method for generic forward checking

        Currently, this method acts only when there's a single
        unassigned variable.

        @param variables: Variables affected by that constraint, in the
                          same order provided by the user
        @type  variables: sequence
        @param domains: Dictionary mapping variables to their domains
        @type  domains: dict
        @param assignments: Dictionary mapping assigned variables to their
                            current assumed value
        @type  assignments: dict
        @return: Boolean value stating if this constraint is currently
                 broken or not
        @rtype: bool
        """#"""
        unassignedvariable = _unassigned
        for variable in variables:
            if variable not in assignments:
                if unassignedvariable is _unassigned:
                    unassignedvariable = variable
                else:
                    break
        else:
            if unassignedvariable is not _unassigned:
                # Remove from the unassigned variable domain's all
                # values which break our variable's constraints.
                domain = domains[unassignedvariable]
                if domain:
                    for value in domain[:]:
                        assignments[unassignedvariable] = value
                        if not self(variables, domains, assignments):
                            domain.hideValue(value)
                    del assignments[unassignedvariable]
                if not domain:
                    return False
        return True

class FunctionConstraint(Constraint):
    """
    Constraint which wraps a function defining the constraint logic

    Examples:

    >>> problem = Problem()
    >>> problem.addVariables(["a", "b"], [1, 2])
    >>> def func(a, b):
    ...     return b > a
    >>> problem.addConstraint(func, ["a", "b"])
    >>> problem.getSolution()
    {'a': 1, 'b': 2}

    >>> problem = Problem()
    >>> problem.addVariables(["a", "b"], [1, 2])
    >>> def func(a, b):
    ...     return b > a
    >>> problem.addConstraint(FunctionConstraint(func), ["a", "b"])
    >>> problem.getSolution()
    {'a': 1, 'b': 2}
    """#"""
 
    def __init__(self, func, assigned=True):
        """
        @param func: Function wrapped and queried for constraint logic
        @type  func: callable object
        @param assigned: Whether the function may receive unassigned
                         variables or not
        @type  assigned: bool
        """
        self._func = func
        self._assigned = assigned

    def __call__(self, variables, domains, assignments, forwardcheck=False,
                 _unassigned=Unassigned):
        parms = [assignments.get(x, _unassigned) for x in variables]
        missing = parms.count(_unassigned)
        if missing:
            return ((self._assigned or self._func(*parms)) and
                    (not forwardcheck or missing != 1 or
                     self.forwardCheck(variables, domains, assignments)))
        return self._func(*parms)

class AllDifferentConstraint(Constraint):
    """
    Constraint enforcing that values of all given variables are different

    Example:

    >>> problem = Problem()
    >>> problem.addVariables(["a", "b"], [1, 2])
    >>> problem.addConstraint(AllDifferentConstraint())
    >>> sorted(sorted(x.items()) for x in problem.getSolutions())
    [[('a', 1), ('b', 2)], [('a', 2), ('b', 1)]]
    """#"""

    def __call__(self, variables, domains, assignments, forwardcheck=False,
                 _unassigned=Unassigned):
        seen = {}
        for variable in variables:
            value = assignments.get(variable, _unassigned)
            if value is not _unassigned:
                if value in seen:
                    return False
                seen[value] = True
        if forwardcheck:
            for variable in variables:
                if variable not in assignments:
                    domain = domains[variable]
                    for value in seen:
                        if value in domain:
                            domain.hideValue(value)
                            if not domain:
                                return False
        return True

class AllEqualConstraint(Constraint):
    """
    Constraint enforcing that values of all given variables are equal

    Example:

    >>> problem = Problem()
    >>> problem.addVariables(["a", "b"], [1, 2])
    >>> problem.addConstraint(AllEqualConstraint())
    >>> sorted(sorted(x.items()) for x in problem.getSolutions())
    [[('a', 1), ('b', 1)], [('a', 2), ('b', 2)]]
    """#"""

    def __call__(self, variables, domains, assignments, forwardcheck=False,
                 _unassigned=Unassigned):
        singlevalue = _unassigned
        for value in assignments.values():
            if singlevalue is _unassigned:
                singlevalue = value
            elif value != singlevalue:
                return False
        if forwardcheck and singlevalue is not _unassigned:
            for variable in variables:
                if variable not in assignments:
                    domain = domains[variable]
                    if singlevalue not in domain:
                        return False
                    for value in domain[:]:
                        if value != singlevalue:
                            domain.hideValue(value)
        return True

class MaxSumConstraint(Constraint):
    """
    Constraint enforcing that values of given variables sum up to
    a given amount

    Example:

    >>> problem = Problem()
    >>> problem.addVariables(["a", "b"], [1, 2])
    >>> problem.addConstraint(MaxSumConstraint(3))
    >>> sorted(sorted(x.items()) for x in problem.getSolutions())
    [[('a', 1), ('b', 1)], [('a', 1), ('b', 2)], [('a', 2), ('b', 1)]]
    """#"""

    def __init__(self, maxsum, multipliers=None):
        """
        @param maxsum: Value to be considered as the maximum sum
        @type  maxsum: number
        @param multipliers: If given, variable values will be multiplied by
                            the given factors before being summed to be checked
        @type  multipliers: sequence of numbers
        """
        self._maxsum = maxsum
        self._multipliers = multipliers

    def preProcess(self, variables, domains, constraints, vconstraints):
        Constraint.preProcess(self, variables, domains,
                              constraints, vconstraints)
        multipliers = self._multipliers
        maxsum = self._maxsum
        if multipliers:
            for variable, multiplier in zip(variables, multipliers):
                domain = domains[variable]
                for value in domain[:]:
                    if value*multiplier > maxsum:
                        domain.remove(value)
        else:
            for variable in variables:
                domain = domains[variable]
                for value in domain[:]:
                    if value > maxsum:
                        domain.remove(value)

    def __call__(self, variables, domains, assignments, forwardcheck=False):
        multipliers = self._multipliers
        maxsum = self._maxsum
        sum = 0
        if multipliers:
            for variable, multiplier in zip(variables, multipliers):
                if variable in assignments:
                    sum += assignments[variable]*multiplier
            if type(sum) is float:
                sum = round(sum, 10)
            if sum > maxsum:
                return False
            if forwardcheck:
                for variable, multiplier in zip(variables, multipliers):
                    if variable not in assignments:
                        domain = domains[variable]
                        for value in domain[:]:
                            if sum+value*multiplier > maxsum:
                                domain.hideValue(value)
                        if not domain:
                            return False
        else:
            for variable in variables:
                if variable in assignments:
                    sum += assignments[variable]
            if type(sum) is float:
                sum = round(sum, 10)
            if sum > maxsum:
                return False
            if forwardcheck:
                for variable in variables:
                    if variable not in assignments:
                        domain = domains[variable]
                        for value in domain[:]:
                            if sum+value > maxsum:
                                domain.hideValue(value)
                        if not domain:
                            return False
        return True

class ExactSumConstraint(Constraint):
    """
    Constraint enforcing that values of given variables sum exactly
    to a given amount

    Example:

    >>> problem = Problem()
    >>> problem.addVariables(["a", "b"], [1, 2])
    >>> problem.addConstraint(ExactSumConstraint(3))
    >>> sorted(sorted(x.items()) for x in problem.getSolutions())
    [[('a', 1), ('b', 2)], [('a', 2), ('b', 1)]]
    """#"""

    def __init__(self, exactsum, multipliers=None):
        """
        @param exactsum: Value to be considered as the exact sum
        @type  exactsum: number
        @param multipliers: If given, variable values will be multiplied by
                            the given factors before being summed to be checked
        @type  multipliers: sequence of numbers
        """
        self._exactsum = exactsum
        self._multipliers = multipliers

    def preProcess(self, variables, domains, constraints, vconstraints):
        Constraint.preProcess(self, variables, domains,
                              constraints, vconstraints)
        multipliers = self._multipliers
        exactsum = self._exactsum
        if multipliers:
            for variable, multiplier in zip(variables, multipliers):
                domain = domains[variable]
                for value in domain[:]:
                    if value*multiplier > exactsum:
                        domain.remove(value)
        else:
            for variable in variables:
                domain = domains[variable]
                for value in domain[:]:
                    if value > exactsum:
                        domain.remove(value)

    def __call__(self, variables, domains, assignments, forwardcheck=False):
        multipliers = self._multipliers
        exactsum = self._exactsum
        sum = 0
        missing = False
        if multipliers:
            for variable, multiplier in zip(variables, multipliers):
                if variable in assignments:
                    sum += assignments[variable]*multiplier
                else:
                    missing = True
            if type(sum) is float:
                sum = round(sum, 10)
            if sum > exactsum:
                return False
            if forwardcheck and missing:
                for variable, multiplier in zip(variables, multipliers):
                    if variable not in assignments:
                        domain = domains[variable]
                        for value in domain[:]:
                            if sum+value*multiplier > exactsum:
                                domain.hideValue(value)
                        if not domain:
                            return False
        else:
            for variable in variables:
                if variable in assignments:
                    sum += assignments[variable]
                else:
                    missing = True
            if type(sum) is float:
                sum = round(sum, 10)
            if sum > exactsum:
                return False
            if forwardcheck and missing:
                for variable in variables:
                    if variable not in assignments:
                        domain = domains[variable]
                        for value in domain[:]:
                            if sum+value > exactsum:
                                domain.hideValue(value)
                        if not domain:
                            return False
        if missing:
            return sum <= exactsum
        else:
            return sum == exactsum

class MinSumConstraint(Constraint):
    """
    Constraint enforcing that values of given variables sum at least
    to a given amount

    Example:

    >>> problem = Problem()
    >>> problem.addVariables(["a", "b"], [1, 2])
    >>> problem.addConstraint(MinSumConstraint(3))
    >>> sorted(sorted(x.items()) for x in problem.getSolutions())
    [[('a', 1), ('b', 2)], [('a', 2), ('b', 1)], [('a', 2), ('b', 2)]]
    """#"""

    def __init__(self, minsum, multipliers=None):
        """
        @param minsum: Value to be considered as the minimum sum
        @type  minsum: number
        @param multipliers: If given, variable values will be multiplied by
                            the given factors before being summed to be checked
        @type  multipliers: sequence of numbers
        """
        self._minsum = minsum
        self._multipliers = multipliers

    def __call__(self, variables, domains, assignments, forwardcheck=False):
        for variable in variables:
            if variable not in assignments:
                return True
        else:
            multipliers = self._multipliers
            minsum = self._minsum
            sum = 0
            if multipliers:
                for variable, multiplier in zip(variables, multipliers):
                    sum += assignments[variable]*multiplier
            else:
                for variable in variables:
                    sum += assignments[variable]
            if type(sum) is float:
                sum = round(sum, 10)
            return sum >= minsum

class InSetConstraint(Constraint):
    """
    Constraint enforcing that values of given variables are present in
    the given set

    Example:

    >>> problem = Problem()
    >>> problem.addVariables(["a", "b"], [1, 2])
    >>> problem.addConstraint(InSetConstraint([1]))
    >>> sorted(sorted(x.items()) for x in problem.getSolutions())
    [[('a', 1), ('b', 1)]]
    """#"""

    def __init__(self, set):
        """
        @param set: Set of allowed values
        @type  set: set
        """
        self._set = set

    def __call__(self, variables, domains, assignments, forwardcheck=False):
        # preProcess() will remove it.
        raise RuntimeError, "Can't happen"

    def preProcess(self, variables, domains, constraints, vconstraints):
        set = self._set
        for variable in variables:
            domain = domains[variable]
            for value in domain[:]:
                if value not in set:
                    domain.remove(value)
            vconstraints[variable].remove((self, variables))
        constraints.remove((self, variables))

class NotInSetConstraint(Constraint):
    """
    Constraint enforcing that values of given variables are not present in
    the given set

    Example:

    >>> problem = Problem()
    >>> problem.addVariables(["a", "b"], [1, 2])
    >>> problem.addConstraint(NotInSetConstraint([1]))
    >>> sorted(sorted(x.items()) for x in problem.getSolutions())
    [[('a', 2), ('b', 2)]]
    """#"""

    def __init__(self, set):
        """
        @param set: Set of disallowed values
        @type  set: set
        """
        self._set = set

    def __call__(self, variables, domains, assignments, forwardcheck=False):
        # preProcess() will remove it.
        raise RuntimeError, "Can't happen"

    def preProcess(self, variables, domains, constraints, vconstraints):
        set = self._set
        for variable in variables:
            domain = domains[variable]
            for value in domain[:]:
                if value in set:
                    domain.remove(value)
            vconstraints[variable].remove((self, variables))
        constraints.remove((self, variables))

class SomeInSetConstraint(Constraint):
    """
    Constraint enforcing that at least some of the values of given
    variables must be present in a given set

    Example:

    >>> problem = Problem()
    >>> problem.addVariables(["a", "b"], [1, 2])
    >>> problem.addConstraint(SomeInSetConstraint([1]))
    >>> sorted(sorted(x.items()) for x in problem.getSolutions())
    [[('a', 1), ('b', 1)], [('a', 1), ('b', 2)], [('a', 2), ('b', 1)]]
    """#"""

    def __init__(self, set, n=1, exact=False):
        """
        @param set: Set of values to be checked
        @type  set: set
        @param n: Minimum number of assigned values that should be present
                  in set (default is 1)
        @type  n: int
        @param exact: Whether the number of assigned values which are
                      present in set must be exactly C{n}
        @type  exact: bool
        """
        self._set = set
        self._n = n
        self._exact = exact

    def __call__(self, variables, domains, assignments, forwardcheck=False):
        set = self._set
        missing = 0
        found = 0
        for variable in variables:
            if variable in assignments:
                found += assignments[variable] in set
            else:
                missing += 1
        if missing:
            if self._exact:
                if not (found <= self._n <= missing+found):
                    return False
            else:
                if self._n > missing+found:
                    return False
            if forwardcheck and self._n-found == missing:
                # All unassigned variables must be assigned to
                # values in the set.
                for variable in variables:
                    if variable not in assignments:
                        domain = domains[variable]
                        for value in domain[:]:
                            if value not in set:
                                domain.hideValue(value)
                        if not domain:
                            return False
        else:
            if self._exact:
                if found != self._n:
                    return False
            else:
                if found < self._n:
                    return False
        return True

class SomeNotInSetConstraint(Constraint):
    """
    Constraint enforcing that at least some of the values of given
    variables must not be present in a given set

    Example:

    >>> problem = Problem()
    >>> problem.addVariables(["a", "b"], [1, 2])
    >>> problem.addConstraint(SomeNotInSetConstraint([1]))
    >>> sorted(sorted(x.items()) for x in problem.getSolutions())
    [[('a', 1), ('b', 2)], [('a', 2), ('b', 1)], [('a', 2), ('b', 2)]]
    """#"""

    def __init__(self, set, n=1, exact=False):
        """
        @param set: Set of values to be checked
        @type  set: set
        @param n: Minimum number of assigned values that should not be present
                  in set (default is 1)
        @type  n: int
        @param exact: Whether the number of assigned values which are
                      not present in set must be exactly C{n}
        @type  exact: bool
        """
        self._set = set
        self._n = n
        self._exact = exact

    def __call__(self, variables, domains, assignments, forwardcheck=False):
        set = self._set
        missing = 0
        found = 0
        for variable in variables:
            if variable in assignments:
                found += assignments[variable] not in set
            else:
                missing += 1
        if missing:
            if self._exact:
                if not (found <= self._n <= missing+found):
                    return False
            else:
                if self._n > missing+found:
                    return False
            if forwardcheck and self._n-found == missing:
                # All unassigned variables must be assigned to
                # values not in the set.
                for variable in variables:
                    if variable not in assignments:
                        domain = domains[variable]
                        for value in domain[:]:
                            if value in set:
                                domain.hideValue(value)
                        if not domain:
                            return False
        else:
            if self._exact:
                if found != self._n:
                    return False
            else:
                if found < self._n:
                    return False
        return True

if __name__ == "__main__":
    import doctest
    doctest.testmod()


########NEW FILE########
__FILENAME__ = drawers
from random import shuffle
from full_brevity import *
from relational import *
from incremental import *
from util import generate_phrase, generate_phrase_rel

if __name__ == '__main__':
  # This data is based on the drawer pictures from Vienthen and Dale (2006)
  # Drawers are numbered (oddly) from left to right on even rows and right to left on odd rows (top to bottom)
  facts = [
    # Row 1
    [Type, "drawer", "d1"], ["color", "blue", "d1"],
    ["row", "1", "d1"], ["col", "1", "d1"], ["corner", "true", "d1"],
    [Rel, "above", "d1", "d8"], [Rel, "left", "d1", "d2"],
    [Rel, "above", "d1", "d9"], [Rel, "left", "d1", "d3"],
    [Rel, "above", "d1", "d16"], [Rel, "left", "d1", "d4"],

    [Type, "drawer", "d2"], ["color", "orange", "d2"],
    ["row", "1", "d2"], ["col", "2", "d2"], ["corner", "false", "d2"],
    [Rel, "above", "d2", "d7"], [Rel, "left", "d2", "d3"], [Rel, "right", "d2", "d1"],
    [Rel, "above", "d2", "d10"], [Rel, "left", "d2", "d4"], [Rel, "above", "d2", "d15"],

    [Type, "drawer", "d3"], ["color", "pink", "d3"],
    ["row", "1", "d3"], ["col", "3", "d3"], ["corner", "false", "d3"],
    [Rel, "above", "d3", "d6"], [Rel, "left", "d3", "d4"], [Rel, "right", "d3", "d2"],
    [Rel, "above", "d3", "d11"], [Rel, "right", "d3", "d1"], [Rel, "above", "d3", "d14"],

    [Type, "drawer", "d4"], ["color", "yellow", "d4"],
    ["row", "1", "d4"], ["col", "4", "d4"], ["corner", "true", "d4"],
    [Rel, "above", "d4", "d5"], [Rel, "right", "d4", "d3"],
    [Rel, "above", "d4", "d12"], [Rel, "right", "d4", "d2"],
    [Rel, "above", "d4", "d13"], [Rel, "right", "d4", "d1"],

    # Row 2
    [Type, "drawer", "d5"], ["color", "pink", "d5"],
    ["row", "2", "d5"], ["col", "4", "d5"], ["corner", "false", "d5"],
    [Rel, "above", "d5", "d12"], [Rel, "below", "d5", "d4"],
    [Rel, "above", "d5", "d13"], [Rel, "right", "d5", "d6"], [Rel, "right", "d5", "d7"],  [Rel, "right", "d5", "d8"],

    [Type, "drawer", "d6"], ["color", "yellow", "d6"],
    ["row", "2", "d6"], ["col", "3", "d6"], ["corner", "false", "d6"],
    [Rel, "above", "d6", "d11"], [Rel, "left", "d6", "d5"], [Rel, "right", "d6", "d7"], [Rel, "below", "d6", "d3"],
    [Rel, "above", "d6", "d14"], [Rel, "right", "d6", "d8"],

    [Type, "drawer", "d7"], ["color", "blue", "d7"],
    ["row", "2", "d7"], ["col", "2", "d7"], ["corner", "false", "d7"],
    [Rel, "above", "d7", "d10"], [Rel, "left", "d7", "d6"], [Rel, "right", "d7", "d8"], [Rel, "below", "d7", "d2"],
    [Rel, "above", "d7", "d15"], [Rel, "left", "d7", "d5"], 

    [Type, "drawer", "d8"], ["color", "blue", "d8"],
    ["row", "2", "d8"], ["col", "1", "d8"], ["corner", "false", "d8"],
    [Rel, "above", "d8", "d9"], [Rel, "left", "d8", "d7"], [Rel, "below", "d8", "d1"],
    [Rel, "above", "d8", "d16"], [Rel, "left", "d8", "d6"], [Rel, "left", "d8", "d5"],

    # Row 3
    [Type, "drawer", "d9"], ["color", "orange", "d9"],
    ["row", "3", "d9"], ["col", "1", "d9"], ["corner", "false", "d9"],
    [Rel, "above", "d9", "d16"], [Rel, "left", "d9", "d10"], [Rel, "below", "d9", "d8"],
    [Rel, "left", "d9", "d11"], [Rel, "left", "d9", "d12"], [Rel, "below", "d9", "d1"],

    [Type, "drawer", "d10"], ["color", "blue", "d10"],
    ["row", "3", "d10"], ["col", "2", "d10"], ["corner", "false", "d10"],
    [Rel, "above", "d10", "d15"], [Rel, "left", "d10", "d11"], [Rel, "right", "d10", "d9"], [Rel, "below", "d10", "d7"],
    [Rel, "left", "d10", "d12"], [Rel, "below", "d10", "d2"],

    [Type, "drawer", "d11"], ["color", "yellow", "d11"],
    ["row", "3", "d11"], ["col", "3", "d11"], ["corner", "false", "d11"],
    [Rel, "above", "d11", "d14"], [Rel, "left", "d11", "d12"], [Rel, "right", "d11", "d10"], [Rel, "below", "d11", "d6"],
    [Rel, "right", "d11", "d9"], [Rel, "below", "d11", "d3"],

    [Type, "drawer", "d12"], ["color", "orange", "d12"],
    ["row", "3", "d12"], ["col", "4", "d12"], ["corner", "false", "d12"],
    [Rel, "above", "d12", "d13"], [Rel, "right", "d12", "d11"], [Rel, "below", "d12", "d5"],
    [Rel, "right", "d12", "d10"], [Rel, "right", "d12", "d9"], [Rel, "below", "d12", "d4"],

    # Row 4
    [Type, "drawer", "d13"], ["color", "pink", "d13"],
    ["row", "4", "d13"], ["col", "4", "d13"], ["corner", "true", "d13"],
    [Rel, "below", "d13", "d12"], [Rel, "right", "d13", "d14"],
    [Rel, "below", "d13", "d5"], [Rel, "right", "d13", "d15"],
    [Rel, "below", "d13", "d4"], [Rel, "right", "d13", "d16"],

    [Type, "drawer", "d14"], ["color", "orange", "d14"],
    ["row", "4", "d14"], ["col", "3", "d14"], ["corner", "false", "d14"],
    [Rel, "below", "d14", "d11"], [Rel, "left", "d14", "d13"], [Rel, "right", "d14", "d15"],
    [Rel, "below", "d14", "d6"], [Rel, "right", "d14", "d16"], [Rel, "below", "d14", "d3"], 

    [Type, "drawer", "d15"], ["color", "pink", "d15"],
    ["row", "4", "d15"], ["col", "2", "d15"], ["corner", "false", "d15"],
    [Rel, "below", "d15", "d10"], [Rel, "left", "d15", "d14"], [Rel, "right", "d15", "d16"],
    [Rel, "below", "d15", "d7"], [Rel, "below", "d15", "d2"], [Rel, "left", "d15", "d13"],

    [Type, "drawer", "d16"], ["color", "yellow", "d16"],
    ["row", "4", "d16"], ["col", "1", "d16"], ["corner", "true", "d16"],
    [Rel, "below", "d16", "d9"], [Rel, "left", "d16", "d15"],
    [Rel, "below", "d16", "d1"], [Rel, "left", "d16", "d14"],
    [Rel, "below", "d16", "d8"], [Rel, "left", "d16", "d13"]
  ]

  #These are the data collected from subjects in Viethen and Dale's work. (2006)
  human_facts = {
    1: [
      [["color", "blue", "d1"], ["row", "1", "d1"], ["col", "1", "d1"], ["corner", "true", "d1"]],
      [["row", "1", "d1"], ["col", "1", "d1"], ["corner", "true", "d1"]],
      [["row", "1", "d2"], ["col", "2", "d2"]]
    ],

    2: [
      [["color", "orange", "d2"], ["col", "2", "d2"]],
      [["color", "orange", "d2"], ["row", "1", "d2"]],
      [["color", "orange", "d2"], [Rel, "above", "d2", "d7"],  ["color", "blue", "d7"]],
      [["col", "2", "d2"], ["row", "1", "d2"]],
      [["color", "orange", "d2"], ["col", "2", "d2"]],
      [["row", "1", "d2"], ["col", "2", "d2"]],
    ],

    3: [
      [["color", "pink", "d3"], ["row", "1", "d3"]],
      [["color", "pink", "d3"], ["row", "1", "d3"]],
      [["color", "pink", "d3"], ["row", "1", "d3"], ["col", "3", "d3"]],
      [["row", "1", "d3"], ["col", "3", "d3"]],
      [["color", "pink", "d3"], ["row", "1", "d3"]],
      [["color", "pink", "d3"], ["row", "1", "d3"]],
      [["color", "pink", "d3"], ["row", "1", "d3"]],
      [["row", "1", "d3"], ["col", "3", "d3"]],
    ],

    4: [
      [["row", "1", "d4"], ["col", "4", "d4"], ["corner", "true", "d4"]],
      [["color", "yellow", "d4"], ["row", "1", "d4"], ["col", "4", "d4"], ["corner", "true", "d4"]],
      [["col", "4", "d4"], ["row", "1", "d4"]],
      [["row", "1", "d4"], ["col", "3", "d4"]],
      [["row", "1", "d4"], ["col", "3", "d4"]],
      [["row", "1", "d4"], ["col", "4", "d4"], ["corner", "true", "d4"]],
      [["row", "1", "d4"], ["col", "3", "d4"]],
      [["row", "1", "d4"], ["col", "3", "d4"]],
    ],

    5: [
      [["color", "yellow", "d4"], [Rel, "above", "d4", "d5"], ["color", "pink", "d5"]],
      [["color", "pink", "d5"], ["col", "4", "d5"], ["row", "2", "d5"]],
      [["color", "pink", "d5"], ["row", "2", "d5"], ["col", "4", "d5"]],
      [["color", "pink", "d5"], ["col", "4", "d5"], [Rel, "below", "d5", "d4"], ["color", "yellow", "d4"]],
      [["color", "pink", "d5"], ["row", "2", "d5"]],
    ],

    6: [
      [["col", "3", "d6"], ["row", "2", "d6"]],
      [["row", "2", "d6"], ["col", "3", "d6"]],
      [["row", "2", "d6"], ["col", "3", "d6"]],
      [["color", "yellow", "d6"], [Rel, "above", "d6", "d11"], ["color", "yellow", "d11"]],
      [["color", "yellow", "d6"], ["row", "2", "d6"]],
      [["color", "yellow", "d6"], ["row", "2", "d6"]],
      [[Rel, "right", "d6", "d7"], [Rel, "right" "d7", "d8"], ["color", "blue", "d7"], ["color", "blue", "d8"]],
      [["color", "yellow", "d6"], [Rel, "above", "d6", "d11"], ["color", "yellow", "d11"]],
      [["color", "yellow", "d6"], ["col", "3", "d6"], ["row", "2", "d6"]],
      [["col", "3", "d6"], ["row", "2", "d6"]],
      [["col", "3", "d6"], ["row", "2", "d6"]],
    ],

    7: [
      [["color", "blue", "d7"], ["col", "2", "d7"], ["row", "2", "d7"]],
      [["color", "blue", "d7"], ["col", "2", "d7"], ["row", "2", "d7"]],
      [["color", "blue", "d7"], [Rel, "below", "d7", "d2"], ["col", "2", "d7"], ["col", "2", "d2"]],
      [["col", "2", "d7"], ["row", "2", "d7"]],
      [["color", "blue", "d7"], [Rel, "below", "d7", "d2"], ["color", "orange", "d2"]],
      [["col", "2", "d7"], ["row", "2", "d7"]],
      [["row", "2", "d7"], ["col", "2", "d7"]],
      [["col", "2", "d7"], ["row", "2", "d7"]],
      [["row", "2", "d7"], ["col", "2", "d7"]],
      [["col", "2", "d7"], ["row", "2", "d7"]],
    ],

    8: [
      [["row", "2", "d8"], ["col", "1", "d8"]],
      [["col", "1", "d8"], ["row", "2", "d8"]],
      [["row", "2", "d8"], ["col", "1", "d8"]],
      [["color", "blue", "d8"], ["row", "2", "d8"], ["col", "1", "d8"]],
      [["row", "2", "d8"], ["col", "1", "d8"]],
      [["row", "2", "d8"], ["col", "1", "d8"]],
      [["row", "2", "d8"], ["col", "1", "d8"]],
    ],

    9: [
      [["color", "orange", "d9"], ["col", "1", "d9"], ["row", "3", "d9"]],
      [["color", "orange", "d9"], ["col", "1", "d9"], ["row", "3", "d9"]],
      [["color", "orange", "d9"], ["col", "1", "d9"], ["row", "3", "d9"]],
      [["row", "3", "d9"], ["col", "1", "d9"]],
      [["color", "orange", "d9"], ["col", "1", "d9"]],
      [["color", "orange", "d9"], ["col", "1", "d9"]],
      [["row", "3", "d9"], ["col", "1", "d9"]],
      [["color", "orange", "d9"], ["col", "1", "d9"]],
    ],

    10: [
      [["color", "blue", "d10"], ["row", "3", "d10"], ["col", "2", "d10"]],
      [["color", "blue", "d10"], ["row", "3", "d10"]],
      [["color", "blue", "d10"], [Rel, "above", "d10", "d15"], ["color", "pink", "d15"]],
      [["color", "blue", "d10"], [Rel, "above", "d10", "d15"], ["color", "pink", "d15"]],
      [["col", "2", "d10"], ["row", "3", "d10"]],
    ],

    11: [
      [["col", "3", "d11"], ["row", "3", "d11"]],
      [["row", "3", "d11"], ["col", "3", "d11"]],
      [["color", "yellow", "d11"], [Rel, "next", "d11", "d12"], ["color", "orange", "d12"]],
      [["color", "yellow", "d11"], [Rel, "next", "d11", "d12"], ["color", "orange", "d12"]],
      [["color", "yellow", "d11"], ["row", "3", "d11"], ["col", "3", "d11"]],
      [["row", "3", "d11"], ["col", "3", "d11"]],
      [["color", "yellow", "d11"], ["row", "3", "d11"]],
      [["color", "yellow", "d11"], ["row", "3", "d11"]],
    ],

    12: [
      [["color", "orange", "d12"], ["col", "4", "d12"]],
      [["row", "3", "d12"], ["col", "4", "d12"]],
      [["color", "orange", "d12"], [Rel, "below", "d12", "d5"], ["color", "pink", "d5"]],
      [["row", "3", "d12"], ["col", "4", "d12"]],
    ],

    13: [
      [["row", "4", "d13"], ["col", "4", "d13"]],
      [["color", "pink", "d13"], ["row", "4", "d13"], ["col", "4", "d13"]],
      [["row", "4", "d13"], ["col", "4", "d13"]],
      [["row", "4", "d13"], ["col", "4", "d13"]],
    ],

    14: [
      [["color", "orange", "d14"], ["row", "4", "d14"], ["col", "3", "d14"]],
      [["color", "orange", "d14"], ["row", "4", "d14"], ["col", "3", "d14"]],
      [["color", "orange", "d14"], ["row", "4", "d14"]],
      [["color", "orange", "d14"], ["row", "4", "d14"]],
      [["color", "orange", "d14"], ["row", "4", "d14"]],
      [["color", "orange", "d14"], [Rel, "below", "d14", "d11"], [Rel, "below", "d11", "d6"], ["color", "yellow", "d11"], ["color", "yellow", "d6"]],
      [["color", "orange", "d14"], ["row", "4", "d14"]],
      [["color", "orange", "d14"], [Rel, "below", "d14", "d11"], [Rel, "below", "d11", "d6"], ["color", "yellow", "d11"], ["color", "yellow", "d6"]],
      [["color", "orange", "d14"], ["row", "4", "d14"]],
    ],

    15: [
      [["col", "2", "d15"], ["row", "4", "d15"]],
      [["color", "pink", "d15"], ["row", "4", "d15"], ["col", "2", "d15"]],
      [["color", "pink", "d15"], ["row", "4", "d15"], ["col", "2", "d15"]],
      [["color", "pink", "d15"], ["col", "2", "d15"]],
      [["color", "pink", "d15"], ["row", "4", "d15"], ["col", "2", "d15"]],
      [["col", "2", "d15"], ["row", "4", "d15"]],
      [["color", "pink", "d15"], ["row", "4", "d15"], ["col", "2", "d15"]],
      [["color", "pink", "d15"], ["row", "4", "d15"], ["col", "2", "d15"]],
      [["color", "pink", "d15"], ["col", "2", "d15"]],
      [["color", "pink", "d15"], ["row", "4", "d15"], ["col", "2", "d15"]],
    ],

    16: [
      [["row", "4", "d16"], ["col", "1", "d16"]],
      [["row", "4", "d16"], ["col", "1", "d16"]],
      [["row", "4", "d16"], ["col", "1", "d16"]],
      [["row", "4", "d16"], ["col", "1", "d16"]],
      [["row", "4", "d16"], ["col", "1", "d16"]],
      [["row", "4", "d16"], ["col", "1", "d16"]],
      [["row", "4", "d16"], ["col", "1", "d16"]],
      [["row", "4", "d16"], ["col", "1", "d16"]],
      [["row", "4", "d16"], ["col", "1", "d16"]],
      [["row", "4", "d16"], ["col", "1", "d16"]],
      [["row", "4", "d16"], ["col", "1", "d16"]],
      [["row", "4", "d16"], ["col", "1", "d16"]],
    ],
  }

  shuffle(facts, lambda: 0.0)

  fb = FullBrevity(filter(lambda f: f[0] != Rel, facts))
  rel = Relational(facts)
  #The ordered priority for using attributes, important for incremental algorithm
  ranked_attrs = ["color", "row", "col", "corner"]
  #Taxonomy used in incremental algorithm to pick out a more common name when appropriate
  # For instance dog instead of Chihuahua when there is a referent dog amongst other animals (which are not dogs)
  taxonomy = Taxonomy({})
  incr = Incremental(facts, ranked_attrs, taxonomy)

  #defines how to turn these rules into English phrases
  handlers = {
    "col": lambda(desc): "column %s" % desc,
    "row": lambda(desc): "row %s" % desc,
    "corner": lambda(desc): "corner",
    "above": lambda(lr): "above" if lr else "below",
    "below": lambda(lr): "below" if lr else "above",
    "right": lambda(lr): "to the right of" if lr else "to the left of",
    "left": lambda(lr): "to the left of" if lr else "to the right of"
  }
  
  #Generate phrases with each algorithm and print to screen
  for i in range(1, 17):
    obj_id = "d%s" % i
    print "%#02d,\"Full Brevity\",\"%s\"" % (i, generate_phrase(fb.describe(obj_id), ranked_attrs, handlers))
    print "%#02d,\"Relational\",\"%s\"" % (i, generate_phrase_rel(rel.describe(obj_id), ranked_attrs, obj_id, handlers))
    print "%#02d,\"Incremental\",\"%s\"" % (i, generate_phrase(incr.describe(obj_id), ranked_attrs, handlers))


########NEW FILE########
__FILENAME__ = full_brevity
# Copyright 2010 Rebecca Ingram, Michael Hansen, Jason Yoder
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from util import validate_facts, Type, Rel, generate_phrase

class FullBrevity:
  """
  Implementation of the "full brevity" referring expression algorithm from Dale and Haddock 1991.
  """
  def __init__(self, facts):
    """
    Initializes class with a set of facts about objects in the world (see example method).
    facts is expected to be a list with each element being a list of the form:
      [fact category, fact name, object id]

    fact category may be Type or a user-defined name.
    fact name may be any string.
    object id must be a unique identifer for a given object.
    """
    self.facts = facts
    self.object_ids = validate_facts(self.facts)
    assert not any(map(lambda f: f == Rel, self.facts)), "Full Brevity does not support relationships"

  def describe(self, target_id):
    """
    Returns a list of facts that uniquely identify an object or None if no such
    description is possible.
    """
    assert target_id in self.object_ids, "No facts for %s" % target_id
    description = []

    # List of attribute/value dictionaries for every distractor object
    distractors = [dict([(f[0], f) for f in self.facts if f[2] == o_id]) for o_id in self.object_ids.difference([target_id])]

    # Attribute/value pairs for the target object
    properties = dict([(f[0], f) for f in self.facts if f[2] == target_id])

    while len(distractors) > 0:
      if len(properties) == 0:
        # No unique description is possible
        return None

      best_set = None
      best_prop = None

      # Find the property that best constrains the distractors set
      for prop_key in properties.keys():
        prop_val = properties[prop_key]
        dist_set = [dist for dist in distractors if dist[prop_key][1] == prop_val[1]]
        if (best_set is None) or (len(dist_set) < len(best_set)):
          best_set = dist_set
          best_prop = prop_val

      # Update description
      description.append(best_prop)
      properties.pop(best_prop[0])
      distractors = best_set
    
    return description

  @staticmethod
  def example():
    """Example of using the FullBrevity class."""
    facts = [[Type, "cube", "obj1"], ["color", "red", "obj1"], ["size", "big", "obj1"],
             [Type, "ball", "obj2"], ["color", "blue", "obj2"], ["size", "big", "obj2"],
             [Type, "ball", "obj3"], ["color", "red", "obj3"], ["size", "small", "obj3"]]

    fb = FullBrevity(facts)

    # Print English description for each object
    for obj_id in ["obj1", "obj2", "obj3"]:
      obj_type = [f for f in facts if f[0] == Type and f[2] == obj_id] # Include type for clarity
      print "%s: %s" % (obj_id, generate_phrase(fb.describe(obj_id) + obj_type, ["color", "size"]))


########NEW FILE########
__FILENAME__ = gre3d_facts
from full_brevity import *
from incremental import *
from relational import *

import util

def getFacts():
    """
    Always numbered from left to right
    Referent is alwasy r1
    Distractors are labeled based on the first letter of their type
    s1 = first sphere, c1= first cube, s2 = second sphere etc.
    This data was entered manually, but came from analyzing the
    the pictures from the GRE3D data set provided by Viethen and Dale 2008.
    """
    facts = {}
    facts[1] =  [[Type, "sphere", "r1"], [Type, "cube", "c1"], [Type, "cube", "c2"],
                 ["color", "green", "r1"], ["color", "blue", "c1"], ["color", "blue", "c2"], 
                 [Type, "floor", "f1"],
                 ["size", "small", "r1"], ["size", "large", "c1"],["size", "large", "c2"],
                 [Rel, "left_of", "c1", "c2"], [Rel, "right_of", "c2", "c1"],
                 [Rel, "left_of", "c1", "r1"], [Rel, "right_of", "r1", "c1"],
                 ["side", "right", "r1"], ["side", "left", "c1" ], ["side", "right", "c2" ],
                 [Rel, "on", "r1", "c2"], [Rel, "on", "c2", "f1"], [Rel, "on", "c1", "f1"],
                 [Rel, "under", "f1", "c2"], [Rel, "under", "f1", "c1"],
                 ["color", "None", "f1"], ["size", "None", "f1"], ["side", "None", "f1"]
                 ]
                

    facts[2] =  [[Type, "sphere", "r1"], [Type, "cube", "c1"], [Type, "cube", "c2"],
                 ["color", "red", "r1"], ["color", "yellow", "c1"], ["color", "red", "c2"], 
                 [Type, "floor", "f1"],
                 ["size", "small", "r1"], ["size", "large", "c1"],["size", "small", "c2"],
                 [Rel, "left_of", "c1", "c2"], [Rel, "right_of", "c2", "c1"],
                 [Rel, "left_of", "r1", "c2"], [Rel, "right_of", "c2", "r1"],
                 ["side", "left", "r1"], ["side", "left", "c1" ], ["side", "right", "c2" ],
                 [Rel, "in_front_of", "r1", "c1"], [Rel, "behind", "c1", "r1"],
                 [Rel, "on", "r1", "f1"], [Rel, "on", "c1", "f1"], [Rel, "on", "c2", "f1"],
                 [Rel, "under", "f1", "r1"], [Rel, "under", "f1", "c1"], [Rel, "under", "f1", "c2"],
                 ["color", "None", "f1"], ["size", "None", "f1"], ["side", "None", "f1"]
                ]

    facts[3] =  [[Type, "sphere", "r1"], [Type, "sphere", "s1"], [Type, "cube", "c1"],
                 ["color", "blue", "r1"], ["color", "blue", "s1"], ["color", "green", "c1"], 
                 [Type, "floor", "f1"],
                 ["size", "small", "r1"], ["size", "large", "s1"],["size", "large", "c1"],
                 [Rel, "left_of", "s1", "c1"], [Rel, "right_of", "c1", "s1"],
                 [Rel, "left_of", "s1", "r1"], [Rel, "right_of", "r1", "s1"],
                 ["side", "right", "r1"], ["side", "left", "s1" ], ["side", "right", "c1" ],
                 [Rel, "on", "r1", "c1"], [Rel, "under", "c1", "r1"], 
                 [Rel, "on", "c1", "f1"], [Rel, "on", "s1", "f1"], 
                 [Rel, "under", "f1", "c1"], [Rel, "under", "f1", "s1"],
                 ["color", "None", "f1"], ["size", "None", "f1"], ["side", "None", "f1"]
                ]

    facts[4] =  [[Type, "cube", "r1"], [Type, "cube", "c1"], [Type, "cube", "c2"],
                 ["color", "yellow", "r1"], ["color", "yellow", "c1"], ["color", "red", "c2"], 
                 [Type, "floor", "f1"],
                 ["size", "small", "r1"], ["size", "large", "c1"],["size", "small", "c2"],
                 [Rel, "left_of", "c1", "c2"], [Rel, "right_of", "c2", "c1"],
                 [Rel, "left_of", "r1", "c2"], [Rel, "right_of", "c2", "r1"],
                 ["side", "left", "r1"], ["side", "left", "c1" ], ["side", "right", "c2" ],
                 [Rel, "in_front_of", "r1", "c1"], [Rel, "behind", "c1", "r1"],
                 [Rel, "on", "r1", "f1"], [Rel, "on", "c1", "f1"], [Rel, "on", "c2", "f1"],
                 [Rel, "under", "f1", "r1"], [Rel, "under", "f1", "c1"], [Rel, "under", "f1", "c2"],
                 ["color", "None", "f1"], ["size", "None", "f1"], ["side", "None", "f1"]
                ]

    facts[5] =  [[Type, "cube", "r1"], [Type, "sphere", "s1"], [Type, "cube", "c1"],
                 ["color", "blue", "r1"], ["color", "blue", "s1"], ["color", "green", "c1"], 
                 [Type, "floor", "f1"],
                 ["size", "small", "r1"], ["size", "large", "s1"],["size", "small", "c1"],
                 [Rel, "left_of", "s1", "c1"], [Rel, "right_of", "c1", "s1"],
                 [Rel, "left_of", "s1", "r1"], [Rel, "right_of", "r1", "s1"],
                 ["side", "right", "r1"], ["side", "left", "s1" ], ["side", "right", "c1" ],
                 [Rel, "on", "r1", "c1"], [Rel, "under", "c1", "r1"],
                 [Rel, "on", "s1", "f1"], [Rel, "on", "c1", "f1"],
                 [Rel, "under", "f1", "s1"], [Rel, "under", "f1", "c1"],
                 ["color", "None", "f1"], ["size", "None", "f1"], ["side", "None", "f1"]
                ]

    facts[6] =  [[Type, "sphere", "r1"], [Type, "cube", "c1"], [Type, "cube", "c2"],
                 ["color", "green", "r1"], ["color", "blue", "c1"], ["color", "blue", "c2"], 
                 [Type, "floor", "f1"],
                 ["size", "small", "r1"], ["size", "large", "c1"],["size", "large", "c2"],
                 [Rel, "left_of", "c1", "c2"], [Rel, "right_of", "c2", "c1"],
                 [Rel, "left_of", "r1", "c2"], [Rel, "right_of", "r1", "c1"],
                 ["side", "left", "r1"], ["side", "left", "c1" ], ["side", "right", "c2" ],
                 [Rel, "in_front_of", "r1", "c1"], [Rel, "behind", "c1", "r1"],
                 [Rel, "on", "r1", "f1"],[Rel, "on", "c1", "f1"], [Rel, "on", "c2", "f1"],
                 [Rel, "under", "f1", "r1"],[Rel, "under", "f1", "c1"], [Rel, "under", "f1", "c2"],
                 ["color", "None", "f1"], ["size", "None", "f1"], ["side", "None", "f1"]
                ]

    facts[7] =  [[Type, "sphere", "r1"], [Type, "cube", "c1"], [Type, "cube", "c2"],
                 ["color", "yellow", "r1"], ["color", "yellow", "c1"], ["color", "red", "c2"], 
                 [Type, "floor", "f1"],
                 ["size", "small", "r1"], ["size", "small", "c1"],["size", "large", "c2"],
                 [Rel, "left_of", "c1", "c2"], [Rel, "right_of", "c2", "c1"],
                 [Rel, "left_of", "c1", "r1"], [Rel, "right_of", "r1", "c1"],
                 ["side", "right", "r1"], ["side", "left", "c1" ], ["side", "right", "c2" ],
                 [Rel, "on", "r1", "c2"], [Rel, "under", "c2", "r1"], 
                 [Rel, "on", "c1", "f1"], [Rel, "on", "c2", "f1"],
                 [Rel, "under", "f1", "c1"], [Rel, "under", "f1", "c2"],
                 ["color", "None", "f1"], ["size", "None", "f1"], ["side", "None", "f1"]
                ]

    facts[8] =  [[Type, "sphere", "r1"], [Type, "cube", "c1"], [Type, "sphere", "s1"],
                 ["color", "blue", "r1"], ["color", "green", "c1"], ["color", "blue", "s1"], 
                 [Type, "floor", "f1"],
                 ["size", "small", "r1"], ["size", "large", "c1"],["size", "large", "s1"],
                 [Rel, "left_of", "c1", "s1"], [Rel, "right_of", "s1", "c1"],
                 [Rel, "left_of", "r1", "s1"], [Rel, "right_of", "s1", "r1"],
                 ["side", "left", "r1"], ["side", "left", "c1" ], ["side", "right", "s1" ],
                 [Rel, "in_front_of", "r1", "c1"], [Rel, "behind", "c1", "r1"], 
                 [Rel, "on", "r1", "f1"], [Rel, "on", "c1", "f1"], [Rel, "on", "s1", "f1"],
                 [Rel, "under", "f1", "r1"], [Rel, "under", "f1", "c1"], [Rel, "under", "f1", "s1"],
                 ["color", "None", "f1"], ["size", "None", "f1"], ["side", "None", "f1"]
                ]


    facts[9] =  [[Type, "cube", "r1"], [Type, "cube", "c1"], [Type, "cube", "c2"],
                 ["color", "red", "r1"], ["color", "yellow", "c1"], ["color", "red", "c2"], 
                 [Type, "floor", "f1"],
                 ["size", "small", "r1"], ["size", "small", "c1"],["size", "large", "c2"],
                 [Rel, "left_of", "c1", "c2"], [Rel, "right_of", "c2", "c1"],
                 [Rel, "left_of", "c1", "r1"], [Rel, "right_of", "r1", "c1"],
                 ["side", "right", "r1"], ["side", "left", "c1" ], ["side", "right", "c2" ],
                 [Rel, "on", "r1", "c2"], [Rel, "under", "c2", "r1"], 
                 [Rel, "on", "c1", "f1"], [Rel, "on", "c2", "f1"],
                 [Rel, "under", "f1", "c1"], [Rel, "under", "f1", "c2"],
                 ["color", "None", "f1"], ["size", "None", "f1"], ["side", "None", "f1"]
                ]

    facts[10] =  [[Type, "cube", "r1"], [Type, "cube", "c1"], [Type, "sphere", "s1"],
                 ["color", "blue", "r1"], ["color", "green", "c1"], ["color", "blue", "s1"], 
                 [Type, "floor", "f1"],
                 ["size", "small", "r1"], ["size", "small", "c1"],["size", "large", "s1"],
                 [Rel, "left_of", "c1", "s1"], [Rel, "right_of", "s1", "c1"],
                 [Rel, "left_of", "r1", "s1"], [Rel, "right_of", "s1", "r1"],
                 ["side", "left", "r1"], ["side", "left", "c1" ], ["side", "right", "s1" ],
                 [Rel, "in_front_of", "r1", "c1"], [Rel, "behind", "c1", "r1"],
                 [Rel, "on", "r1", "f1"], [Rel, "on", "c1", "f1"], [Rel, "on", "s1", "f1"],
                 [Rel, "under", "f1", "r1"], [Rel, "under", "f1", "c1"], [Rel, "under", "f1", "s1"],
                 ["color", "None", "f1"], ["size", "None", "f1"], ["side", "None", "f1"]
                ]

    facts[11] =  [[Type, "sphere", "r1"], [Type, "cube", "c1"], [Type, "cube", "c2"],
                 ["color", "yellow", "r1"], ["color", "red", "c1"], ["color", "red", "c2"], 
                 [Type, "floor", "f1"],
                 ["size", "small", "r1"], ["size", "large", "c1"],["size", "large", "c2"],
                 [Rel, "left_of", "c1", "c2"], [Rel, "right_of", "c2", "c1"],
                 [Rel, "left_of", "r1", "c2"], [Rel, "right_of", "c2", "r1"],
                 ["side", "left", "r1"], ["side", "left", "c1" ], ["side", "right", "c2" ],
                 [Rel, "on", "r1", "c1"], [Rel, "on", "c2", "f1"], [Rel, "on", "c1", "f1"],
                 [Rel, "under", "c1", "r1"], [Rel, "under", "f1", "c2"], [Rel, "under", "f1", "c1"],
                 ["color", "None", "f1"], ["size", "None", "f1"], ["side", "None", "f1"]
                 ]
                

    facts[12] =  [[Type, "sphere", "r1"], [Type, "cube", "c1"], [Type, "cube", "c2"],
                 ["color", "blue", "r1"], ["color", "blue", "c1"], ["color", "green", "c2"], 
                 [Type, "floor", "f1"],
                 ["size", "small", "r1"], ["size", "small", "c1"],["size", "large", "c2"],
                 [Rel, "left_of", "c1", "c2"], [Rel, "right_of", "c2", "c1"],
                 [Rel, "left_of", "c1", "r1"], [Rel, "right_of", "r1", "c1"],
                 ["side", "right", "r1"], ["side", "left", "c1" ], ["side", "right", "c2" ],
                 [Rel, "in_front_of", "r1", "c2"], [Rel, "behind", "c2", "r1"],
                 [Rel, "on", "r1", "f1"], [Rel, "on", "c1", "f1"], [Rel, "on", "c2", "f1"],
                 [Rel, "under", "f1", "r1"], [Rel, "under", "f1", "c1"], [Rel, "under", "f1", "c2"],
                 ["color", "None", "f1"], ["size", "None", "f1"], ["side", "None", "f1"]
                ]

    facts[13] =  [[Type, "sphere", "r1"], [Type, "cube", "c1"], [Type, "sphere", "s1"],
                 ["color", "red", "r1"], ["color", "yellow", "c1"], ["color", "red", "s1"],
                 [Type, "floor", "f1"],
                 ["size", "small", "r1"], ["size", "large", "c1"],["size", "large", "s1"],
                 [Rel, "left_of", "c1", "s1"], [Rel, "right_of", "s1", "c1"],
                 [Rel, "left_of", "r1", "s1"], [Rel, "right_of", "s1", "r1"],
                 ["side", "left", "r1"], ["side", "left", "c1" ], ["side", "right", "s1" ],
                 [Rel, "on", "r1", "c1"], [Rel, "under", "c1", "r1"], 
                 [Rel, "on", "c1", "f1"], [Rel, "on", "s1", "f1"], 
                 [Rel, "under", "f1", "c1"], [Rel, "under", "f1", "s1"],
                 ["color", "None", "f1"], ["size", "None", "f1"], ["side", "None", "f1"]
                ]

    facts[14] =  [[Type, "cube", "r1"], [Type, "cube", "c1"], [Type, "cube", "c2"],
                 ["color", "green", "r1"], ["color", "blue", "c1"], ["color", "green", "c2"], 
                 [Type, "floor", "f1"],
                 ["size", "small", "r1"], ["size", "large", "c2"],["size", "small", "c1"],
                 [Rel, "left_of", "c1", "c2"], [Rel, "right_of", "c2", "c1"],
                 [Rel, "left_of", "c1", "r1"], [Rel, "right_of", "r1", "c2"],
                 ["side", "left", "r1"], ["side", "left", "c1" ], ["side", "right", "c2" ],
                 [Rel, "in_front_of", "r1", "c2"], [Rel, "behind", "c2", "r1"],
                 [Rel, "on", "r1", "f1"], [Rel, "on", "c1", "f1"], [Rel, "on", "c2", "f1"],
                 [Rel, "under", "f1", "r1"], [Rel, "under", "f1", "c1"], [Rel, "under", "f1", "c2"],
                 ["color", "None", "f1"], ["size", "None", "f1"], ["side", "None", "f1"]
                ]

    facts[15] =  [[Type, "cube", "r1"], [Type, "cube", "c1"], [Type, "sphere", "s1"],
                 ["color", "yellow", "r1"], ["color", "red", "c1"], ["color", "yellow", "s1"],
                 [Type, "floor", "f1"],
                 ["size", "small", "r1"], ["size", "small", "c1"], ["size", "large", "s1"],
                 [Rel, "right_of", "s1", "c1"], [Rel, "left_of", "c1", "s1"],
                 [Rel, "right_of", "s1", "r1"], [Rel, "left_of", "r1", "s1"],
                 ["side", "left", "r1"], ["side", "right", "s1" ], ["side", "left", "c1" ],
                 [Rel, "on", "r1", "c1"], [Rel, "under", "c1", "r1"],
                 [Rel, "on", "s1", "f1"], [Rel, "on", "c1", "f1"],
                 [Rel, "under", "f1", "s1"], [Rel, "under", "f1", "c1"],
                 ["color", "None", "f1"], ["size", "None", "f1"], ["side", "None", "f1"]
                ]

    facts[16] =  [[Type, "sphere", "r1"], [Type, "cube", "c1"], [Type, "cube", "c2"],
                 ["color", "red", "r1"], ["color", "yellow", "c1"], ["color", "yellow", "c2"], 
                 [Type, "floor", "f1"],
                 ["size", "small", "r1"], ["size", "large", "c1"],["size", "large", "c2"],
                 [Rel, "left_of", "c1", "c2"], [Rel, "right_of", "c2", "c1"],
                 [Rel, "right_of", "r1", "c1"], [Rel, "left_of", "c1", "r1"],
                 ["side", "right", "r1"], ["side", "left", "c1" ], ["side", "right", "c2" ],
                 [Rel, "in_front_of", "r1", "c2"], [Rel, "behind", "c2", "r1"],
                 [Rel, "on", "r1", "f1"],[Rel, "on", "c1", "f1"], [Rel, "on", "c2", "f1"],
                 [Rel, "under", "f1", "r1"],[Rel, "under", "f1", "c1"], [Rel, "under", "f1", "c2"],
                 ["color", "None", "f1"], ["size", "None", "f1"], ["side", "None", "f1"]
                ]

    facts[17] =  [[Type, "sphere", "r1"], [Type, "cube", "c1"], [Type, "cube", "c2"],
                 ["color", "blue", "r1"], ["color", "green", "c1"], ["color", "blue", "c2"], 
                 [Type, "floor", "f1"],
                 ["size", "small", "r1"], ["size", "large", "c1"],["size", "small", "c2"],
                 [Rel, "left_of", "c1", "c2"], [Rel, "right_of", "c2", "c1"],
                 [Rel, "left_of", "r1", "c2"], [Rel, "right_of", "c2", "r1"],
                 ["side", "left", "r1"], ["side", "left", "c1" ], ["side", "right", "c2" ],
                 [Rel, "on", "r1", "c1"], [Rel, "under", "c1", "r1"], 
                 [Rel, "on", "c1", "f1"], [Rel, "on", "c2", "f1"],
                 [Rel, "under", "f1", "c1"], [Rel, "under", "f1", "c2"],
                 ["color", "None", "f1"], ["size", "None", "f1"], ["side", "None", "f1"]
                ]

    facts[18] =  [[Type, "sphere", "r1"], [Type, "sphere", "s1"], [Type, "cube", "c1"],
                 ["color", "red", "r1"], ["color", "red", "s1"], ["color", "yellow", "c1"],
                 [Type, "floor", "f1"],
                 ["size", "small", "r1"], ["size", "large", "s1"],["size", "large", "c1"],
                 [Rel, "right_of", "c1", "s1"], [Rel, "left_of", "s1", "c1"],
                 [Rel, "right_of", "r1", "s1"], [Rel, "left_of", "s1", "r1"],
                 ["side", "right", "r1"], ["side", "left", "c1" ], ["side", "right", "s1" ],
                 [Rel, "in_front_of", "r1", "c1"], [Rel, "behind", "c1", "r1"], 
                 [Rel, "on", "r1", "f1"], [Rel, "on", "c1", "f1"], [Rel, "on", "s1", "f1"],
                 [Rel, "under", "f1", "r1"], [Rel, "under", "f1", "c1"], [Rel, "under", "f1", "s1"],
                 ["color", "None", "f1"], ["size", "None", "f1"], ["side", "None", "f1"]
                ]


    facts[19] =  [[Type, "cube", "r1"], [Type, "cube", "c1"], [Type, "cube", "c2"],
                 ["color", "green", "r1"], ["color", "green", "c1"], ["color", "blue", "c2"], 
                 [Type, "floor", "f1"],
                 ["size", "small", "r1"], ["size", "large", "c1"],["size", "small", "c2"],
                 [Rel, "left_of", "c1", "c2"], [Rel, "right_of", "c2", "c1"],
                 [Rel, "left_of", "r1", "c2"], [Rel, "right_of", "c2", "r1"],
                 ["side", "left", "r1"], ["side", "left", "c1" ], ["side", "right", "c2" ],
                 [Rel, "on", "r1", "c1"], [Rel, "under", "c1", "r1"], 
                 [Rel, "on", "c1", "f1"], [Rel, "on", "c2", "f1"],
                 [Rel, "under", "f1", "c1"], [Rel, "under", "f1", "c2"],
                 ["color", "None", "f1"], ["size", "None", "f1"], ["side", "None", "f1"]
                ]

    facts[20] =  [[Type, "cube", "r1"], [Type, "sphere", "s1"], [Type, "cube", "c1"],
                 ["color", "red", "r1"], ["color", "red", "s1"], ["color", "yellow", "c1"],
                 [Type, "floor", "f1"],
                 ["size", "small", "r1"], ["size", "small", "c1"],["size", "large", "s1"],
                 [Rel, "left_of", "s1", "c1"], [Rel, "right_of", "c1", "s1"],
                 [Rel, "left_of", "s1", "r1"], [Rel, "right_of", "r1", "s1"],
                 ["side", "right", "r1"], ["side", "left", "s1" ], ["side", "right", "c1" ],
                 [Rel, "in_front_of", "r1", "c1"], [Rel, "behind", "c1", "r1"],
                 [Rel, "on", "r1", "f1"], [Rel, "on", "c1", "f1"], [Rel, "on", "s1", "f1"],
                 [Rel, "under", "f1", "r1"], [Rel, "under", "f1", "c1"], [Rel, "under", "f1", "s1"],
                 ["color", "None", "f1"], ["size", "None", "f1"], ["side", "None", "f1"]
                ]

    return facts

if __name__ == '__main__':
  facts = getFacts()

  ranked_attrs = ["color", "size", Type]
  taxonomy = Taxonomy({})

  handlers = {
    "in_front_of": lambda(lr): "in front of",
    "left_of": lambda(lr): "to the left of",
    "right_of": lambda(lr): "to the right of"
  }

  #Print out the referring expressions generated by each algorithm for each scene
  for i in range(1, 21):
    fb = FullBrevity(facts[i])
    desc_fb = fb.describe("r1")

    incr = Incremental(facts[i], ranked_attrs, taxonomy)
    desc_incr = incr.describe("r1")

    rel = Relational(facts[i])
    desc_rel = rel.describe("r1")

    print "%#02d,\"Full Brevity\",\"%s\"" % (i, util.generate_phrase(desc_fb, ranked_attrs))
    print "%#02d,\"Incremental\",\"%s\"" % (i, util.generate_phrase(desc_incr, ranked_attrs))
    print "%#02d,\"Relational\",\"%s\"" % (i, util.generate_phrase_rel(desc_rel, ranked_attrs, "r1", handlers))


########NEW FILE########
__FILENAME__ = incremental
import string

from copy import copy, deepcopy
from util import validate_facts, Type, Rel, generate_phrase

class Incremental:
    """
    Implementation of the "incremental" referring expression algorithm from 
    Reiter and Dale (1992). 
    """
    def __init__(self, facts, ranked_attrs, taxonomy):
        """
        Initializes class with a set of facts about objects in the world (see 
            example method), a set of ranked attributes, and a taxonomy.
    
        facts is expected to be a list where each element is a list of the form:
            [fact category, fact name, object id, ...]

            fact category may be Type, or a user-defined name (all facts that 
                use Rel will be ignored for this algorithm).
            fact name may be any string.
            object id must be a unique identifier for a given object.
            ... = more object ids if fact category is Rel.

        ranked_attrs is expected to be a list of attributes in descending order
            of preference. Attributes may be either Type or user-defined names 
            used in the fact set above.
        
        taxonomy is expected to be a Taxonomy (or an object which implements the 
            methods listed in the Taxonomy documentation), which contains a 
            taxonomy for the fact names.
        """
        self.facts = deepcopy(facts)
        self.object_ids = validate_facts(self.facts)
        self.object_ids_list = list(self.object_ids)

        # TODO: may want to validate the attribute list and/or the taxonomy
        self.ranked_attrs = ranked_attrs
        self.tree = taxonomy
    
    def describe(self, target_id):
        """
        Returns a list of facts that distinguish the object from other objects 
        in the fact set.
        """
        description = []
        type_included = False
        assert target_id in self.object_ids, "No facts known for %s" % target_id

        distractor_ids = self.object_ids - set([target_id]) # set difference

        # Iterate through the preferred attributes in ranked order
        for cur_attr in self.ranked_attrs:
            cur_fact = [f for f in self.facts if f[0] == cur_attr and \
                    f[2] == target_id]

            # If the target has no value for this attribute in the fact set.
            if len(cur_fact) == 0:
                continue

            basic_val = self.tree.basic_level_value(target_id, cur_attr, \
                    cur_fact[0][1])
            best_val = self.find_best_value(target_id, cur_attr, basic_val, distractor_ids)

            # Did we find a good value?
            if(best_val != None):
                # If so, how many distractors are ruled out by it?
                dist_ruled_out = self.rules_out(cur_attr, best_val, distractor_ids)

                # Did we rule out at least one distractor?
                if len(dist_ruled_out) > 0:
                    description.append([cur_attr, best_val, target_id]) # non-destructive
                    distractor_ids = distractor_ids - set(dist_ruled_out) # set difference
                    if(cur_attr == Type):
                        type_included = True

            # Have all distractors been eliminated? If so, make sure type is
            # included in the description and return.
            if len(distractor_ids) == 0:
                # Make sure that the type is included in the description
                if not type_included:
                    type_included = True
                    type_fact = [f for f in self.facts if f[0] == Type and \
                            f[2] == target_id]
                    assert len(type_fact) > 0, \
                            "Object %s missing type attribute." % target_id
                    basic_val = self.tree.basic_level_value(target_id, Type,
                            type_fact[0][1])
                    description.append([Type, basic_val, target_id])
                return description

        # If there are still distractors, we didn't find a way to distinguish
        # the referent using the preferred attributes and the given fact set.
        if len(distractor_ids) > 0:
            return None
        return description

    def find_best_value(self, target_id, attr, initial_value, distractors):
        """
        Check the taxonomy, distractors, and fact set to determine the best 
        value for the attribute to describe the target_id.

        From the paper: return a value for the attribute that is subsumed by 
        the initial value, accurately describes the referent, rules out as 
        many distractors as possible, and, subject to these constraints, is
        as close as possible in the taxonomy to the initial value (where the
        initial "initial_value" is the basic level description).
        """

        # If there's no taxonomy, we can't generalize at all, so just return 
        # the initial value.
        if self.tree == None:
            return initial_value

        if self.tree.user_knows(target_id, attr, initial_value, self.facts) == True:
            value = initial_value
        else:
            value = None

        # For each child in the taxonomy, see if it rules out more distractors 
        # than the current best value.
        for child in self.tree.taxonomy_children(initial_value, attr):
            new_value = self.find_best_value(target_id, attr, child, distractors)
            len1 = len(self.rules_out(attr, new_value, distractors))
            len2 = len(self.rules_out(attr, value, distractors))
            
            attr_val = self.get_attribute_value(attr, target_id)
            if self.tree.subsumes(child, attr_val) and \
                    (not new_value == None) and (value == None or len1 > len2):
                value = new_value
        return value
    
    def rules_out(self, attr, value, distractors):
        """
        A distractor can be ruled out if the user knows that it does not have 
        the specified value for the attribute. (When user_knows returns false, 
        it does not meant that the user does not know; it means that the user 
        knows the description is false).
        """
        return [dist for dist in distractors \
                     if self.tree.user_knows(dist, attr, value, self.facts) == False]

    # TODO: We can probably make this more efficient if we sort and do a 
    # fancier search.
    def get_attribute_value(self, attrib, target_id):
        """
        Look up the value of an attribute for a target in the fact set.
        """
        for fact in self.facts:
            if (fact[0] == attrib) and (fact[1] == target_id):
                return fact[2]
        return None

    # Generate an English phrase from a description.
    def generate_phrase(self, description, attr_prefs=None):
        if attr_prefs == None:
            attr_prefs = self.ranked_attrs

        return util.generate_phrase(description, attr_prefs)

    @staticmethod
    def example():
        """Example of using the Incremental class."""
        facts = [[Type, "cube", "obj1"], ["color", "brick", "obj1"], ["size", "big", "obj1"],
                 [Type, "ball", "obj2"], ["color", "navy", "obj2"], ["size", "big", "obj2"],
                 [Type, "ball", "obj3"], ["color", "scarlet", "obj3"], ["size", "small", "obj3"]]

        ranked_attrs = ["color", "size", Type]
        tax = {
            "blue" : {"parent" : None, "children" : ["navy", "cerulean"]},
            "red"  : {"parent" : None, "children" : ["scarlet", "brick"]},
            "navy" : {"parent" : "blue", "children" : []},
            "cerulean" : {"parent" : "blue", "children" : []},
            "scarlet" : {"parent" : "red", "children" : []},
            "brick" : {"parent" : "red", "children" : []}
        }

        taxonomy = Taxonomy(tax)
        incr = Incremental(facts, ranked_attrs, taxonomy)

        # Print English description for each object
        for obj_id in ["obj1", "obj2", "obj3"]:
            obj_type = [f for f in facts if f[0] == Type and f[2] == obj_id] # Include type for clarity
            print "%s: %s" % (obj_id, generate_phrase(incr.describe(obj_id) + obj_type, ["color", "size"]))


class Taxonomy:
    """
    Class for storing a taxonomy. May be overridden, or different classes 
    may be used in place. All that is required is that these functions are 
    defined: basic_level_value, user_knows, taxonomy_children, subsumes.

    In this implementation, all words are stored in a dictionary, and each 
    entry is itself a dictionary with optional entries for a basic level 
    value, a parent, and a list of children. Refer to the example() method 
    in the Incremental class for a demonstration.
    """
    def __init__(self, tree={}):
        self.lookup = tree

    def basic_level_value(self, obj, attr, value, facts=None, user=None):
        """
        Look up the value in the lookup table. If it has a basic value entry 
        (called basic), then return that. Otherwise, return its parent. If it 
        has no parent, just return the original value. The set of known facts
        and a user are included as optional arguments but aren't used in this
        implementation.
        """
        
        # Make sure value isn't None before we start trying to look it up
        if value == None:
            return value

        if value in self.lookup:
            value_info = self.lookup[value]
            if "basic" in value_info:
                return value_info["basic"]
            else:
                parent = self.get_parent(value)
                if parent == None:
                    return value
                else:
                    return parent
        else:
            return value
    
    def taxonomy_children(self, initial_value, attrib):
        """
        Look up a value in the taxonomy, and return a list of all of its
        children. Return the empty list either if value is not in the taxonomy
        or if it has no children.
        """
        value_info = self.lookup.get(initial_value, None)
        if value_info == None:
            return []
        else:
            return value_info.get("children", [])

    def user_knows(self, object, attr, value, facts, user=None):
        """
        Takes an object, an attribute, a value, a fact set, and a user
        
        Returns true if the user knows that the object has the value for that
            attribute (currently, this happens when either the object has that
            attribute value or if value subsumes the object's value for that
            attribute in the taxonomy)
        Returns false if the user knows that the object does not have the value 
            for that attribute.
        Returns None if the user doesn't know anything about the value of that
            attribute for the object.
        
        In this implementation, we assume that the user knows everything that 
            is in the fact set
        """
        specific_val = None
        val_found = False
        i = 0
        while not val_found and i < len(facts):
            if facts[i][0] == attr and facts[i][2] == object:
                val_found = True
                specific_val = facts[i][1]
            i += 1

        # if no value for object/attr is found, the user knows nothing
        if not val_found:
            return None

        # if value subsumes the specific value of the object, return true
        if self.subsumes(value, specific_val):
            return True
        else:
            return False

    def subsumes(self, parent, child):
        """
        Does the parent value subsume the child value: Check if child is a
        descendant of parent in the tree.
        """
        if child == None:
            return False

        if parent == child:
            return True

        # See if child is in the taxonomy and either recur on its parent (if it
        # has one) or return False.
        if child in self.lookup:
            # Get the child's parent, and see if it is subsumed by the parent
            # value that was passed in. 
            return self.subsumes(parent, self.get_parent(child))

        # If child is not in the taxonomy
        else:
            return False

    def get_parent(self, child):
        """
        Return the parent of child in the taxonomy (if it has one). If the
        child is not in the taxonomy or has no parent, return None.
        """
        if (child == None) or (not child in self.lookup):
            return None

        child_node = self.lookup[child]
        if "parent" in child_node:
            return child_node["parent"]
        else:
            return None


########NEW FILE########
__FILENAME__ = relational
# Copyright 2010 Rebecca Ingram, Michael Hansen, Jason Yoder
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import constraint
from copy import copy, deepcopy
from util import validate_facts, Type, Rel, generate_phrase_rel

class _RelationalVar:
  """Internal class used to represent relational variables"""
  fresh_var_id = 0

  def __init__(self, var_id):
    self.var_id = var_id

  def __eq__(self, other):
    return self.var_id == other.var_id

  def __repr__(self):
    return "<_RelationalVar %s>" % self.var_id

  def __str__(self):
    return str(self.var_id)

  def __hash__(self):
    return hash(self.var_id)

  @staticmethod
  def get_fresh_var():
    _RelationalVar.fresh_var_id += 1
    return _RelationalVar(_RelationalVar.fresh_var_id - 1)


class Relational:
  """
  Implementation of the "relational" referring expression algorithm from Dale and Haddock 1991.
  """
  def __init__(self, facts):
    """
    Initializes class with a set of facts about objects in the world (see example method).
    facts is expected to be a list with each element being a list of the form:
      [fact category, fact name, object id, ...]

    fact category may be Type, Rel, or a user-defined name.
    fact name may be any string.
    object id must be a unique identifer for a given object.
    ... = more object ids if fact category is Rel
    """
    self.facts = deepcopy(facts)
    self.object_ids = validate_facts(self.facts)
    self.object_ids_list = list(self.object_ids)

  def __make_constraint(self, fact, var_arg_map):
    """Returns a function constraint that maps parameter values onto fact's variables"""
    return lambda *args: (fact[:2] + [args[var_arg_map[fact_part.var_id]] if isinstance(fact_part, _RelationalVar) else fact_part for fact_part in fact[2:]]) in self.facts

  def __get_facts_for(self, obj_id):
    """Returns all unused facts about obj_id"""
    return [fact for fact in self.facts if (not fact in self.used_facts) and any([fact_id == obj_id for fact_id in fact[2:]])]

  def __fact_replace(self, fact, to_replace, replace_with):
    """Replaces all occurrences of to_replace in fact with replace_with"""
    return fact[:2] + map(lambda fact_id: replace_with if (not isinstance(fact_id, _RelationalVar) and fact_id == to_replace) else fact_id, fact[2:])

  def __get_context_set(self, constraints, obj_var):
    """Returns a set of objects that fit the given constraints for obj_var"""
    network_var = str(obj_var)
    variables = set([obj_var] + [var for cst in constraints for var in cst[0][2:]])
    network = constraint.Problem()

    for var in variables:
      network.addVariable(str(var), self.object_ids_list)

    for cst in constraints:
      network.addConstraint(self.__make_constraint(cst[0], cst[1]), cst[2])

    solutions = network.getSolutions()
    return set([soln[network_var] for soln in solutions])

  def describe(self, target_id):
    """
    Returns a list of facts that uniquely identify an object or None if no such
    description is possible.
    """
    self.used_facts = []
    description = []
    var_obj_map = {}

    assert target_id in self.object_ids, "No facts for %s" % target_id

    # Initial state, goal is to describe target_id
    obj_id = target_id
    obj_var = _RelationalVar.get_fresh_var()
    var_obj_map[obj_var] = obj_id
    network_var = str(obj_var)
    goal_stack = [(obj_id, obj_var)]
    obj_facts_dict = {obj_id: self.__get_facts_for(obj_id)}
    constraints = []

    while len(goal_stack) > 0:
      obj_id, obj_var = goal_stack[len(goal_stack) - 1]
      network_var = str(obj_var)
      var_set = self.__get_context_set(constraints, obj_var)

      if len(var_set) == 0:
        # No unique description is possible
        return None

      if len(var_set) == 1:
        goal_stack.pop()
        continue

      obj_facts = obj_facts_dict[obj_id]

      assert len(obj_facts) > 0, "Ran out of facts for %s" % obj_id

      # Find the "best" fact -- the one that most constrains the object set
      best_fact = None
      best_soln_size = len(var_set)

      for fact in obj_facts:
        test_fact = self.__fact_replace(fact, obj_id, obj_var)
        test_soln_set = self.__get_context_set(constraints + [(test_fact, {obj_var.var_id: 0}, [network_var])], obj_var)

        if (len(test_soln_set) < best_soln_size):
          best_fact = fact
          best_soln_size = len(test_soln_set)

      # If no best fact is found (i.e. the set was not constrained any further), then
      # we should abandon this sub-goal. If we're on top of the stack, though, FAIL.
      if best_fact is None:
        goal_stack.pop()
        assert len(goal_stack) > 0, "No unique description is possible for %s" % obj_id
        continue

      obj_facts.remove(best_fact)
      self.used_facts.append(best_fact) # Avoid infinite loop

      # Replace constant portions of the best fact with variables
      best_fact = self.__fact_replace(best_fact, obj_id, obj_var)
      best_net_vars = [network_var]
      best_var_map = {obj_var.var_id: 0}

      for other_id in [fact_part for fact_part in best_fact[2:] if not isinstance(fact_part, _RelationalVar)]:
        other_var = _RelationalVar.get_fresh_var()
        var_obj_map[other_var] = other_id
        other_net_var = str(other_var)
        goal_stack.append((other_id, other_var)) # Add new goal to the stack
        obj_facts_dict[other_id] = self.__get_facts_for(other_id)

        # Prepare for constraint
        best_fact = self.__fact_replace(best_fact, other_id, other_var)
        best_net_vars.append(other_net_var)
        best_var_map[other_var.var_id] = len(best_net_vars) - 1

      # Update text description and network constraints
      description.append(best_fact[:2] + [var_obj_map[v] for v in best_fact[2:]])
      constraints.append((best_fact, best_var_map, best_net_vars))

    return description

  @staticmethod
  def example():
    """Example of using the Relational class."""
    facts = [[Type, "cup", "c1"], [Type, "cup", "c2"], [Type, "cup", "c3"],
             [Type, "bowl", "b1"], [Type, "bowl", "b2"],
             [Type, "table", "t1"], [Type, "table", "t2"], ["color", "red", "t2"],
             [Type, "floor", "f1"],
             [Rel, "in", "c1", "b1"], [Rel, "in", "c2", "b2"],
             [Rel, "on", "c3", "f1"], [Rel, "on", "b1", "f1"], [Rel, "on", "b2", "t1"],
             [Rel, "on", "t1", "f1"], [Rel, "on", "t2", "f1"]]

    rel = Relational(facts)
    obj_types = [f for f in facts if f[0] == Type] # Include types in the description for clarity
    handlers = {
      "on" : lambda(lr): "on" if lr else "on which lies",
      "in" : lambda(lr): "in" if lr else "in which lies"
    }

    # Generate an English description for each object
    for obj_id in ["c1", "c2", "c3", "b1", "b2", "t1", "t2", "f1"]:
      print "%s: %s" % (obj_id, generate_phrase_rel(rel.describe(obj_id) + obj_types, ["color"], obj_id, handlers))


########NEW FILE########
__FILENAME__ = util
# Copyright 2010 Rebecca Ingram, Michael Hansen, Jason Yoder
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

class _Type:
  """Internal class used to mark Type facts"""
  def __repr__(self):
    return "Type"

  def __eq__(self, other):
    return isinstance(other, self.__class__)

  def __ne__(self, other):
    return not self.__eq__(other)

  def __hash__(self):
    return hash(0)

class _Rel:
  """Internal class used to mark Rel facts"""
  def __repr__(self):
    return "Rel"

  def __eq__(self, other):
    return isinstance(other, self.__class__)

  def __ne__(self, other):
    return not self.__eq__(other)

  def __hash__(self):
    return hash(1)

Type = _Type() # Singleton Type object
Rel = _Rel() # Singleton Rel object

def validate_facts(facts):
  """
  Validates that a list of facts conform to expectations.
  Specifically:
    * Every object must have a Type fact
    * Type and predicate facts have fixed lengths
    * Rel facts have a minimum length

  Returns the set of object ids.
  """
  object_ids = set()
  objects_with_types = set()

  for f in facts:
    category = f[0] # First element is fact category

    if category == Type:
      assert len(f) == 3, "Type facts must be of the form [Type, type name, object id]"
      object_ids.add(f[2])
      objects_with_types.add(f[2])
    elif category == Rel:
      assert len(f) > 3, "Relational facts must be of the form [Rel, rel name, object id, object id, ...]"
      object_ids = object_ids.union(f[2:])
    else:
      assert len(f) == 3, "Predicate facts must be of the form [category, predicate name, object id]"
      object_ids.add(f[2])

  # All objects must have a type
  assert len(set.difference(object_ids, objects_with_types)) == 0, "All objects must have a Type fact"

  return object_ids

# Generate an English phrase from a description.
def generate_phrase(description, attr_prefs, handlers = None):
    if description == None:
        return "No disambiguating referring expression found."

    attrs = [f[0] for f in description]
    desc_dict = dict([(f[0], f[1]) for f in description])

    # Commenting this assertion because we don't want it for full brevity.
    # Instead, check if Type is included; if not, use "one" 
    #assert(attrs.count(Type) > 0)
    if attrs.count(Type) == 0:
        attrs.append(Type)
        desc_dict[Type] = "one"

    # Type always goes last
    attrs = [a for a in attrs if a != Type]
    attr_queue = [desc_dict[Type]]

    # Put the highest priority attributes next to the noun
    for attr in attr_prefs:
        if (attrs.count(attr) > 0):
            if (handlers != None) and (handlers.has_key(attr)):
                attr_queue.insert(0, handlers[attr](desc_dict[attr]))
            else:
              attr_queue.insert(0, desc_dict[attr])

            attrs.remove(attr)

    # Add the remaining attributes in the order we got them (farthest from 
    # the noun)
    for attr in attrs:
        attr_queue.insert(0, desc_dict[attr])

    return "the %s" % str.join(" ", attr_queue)

def generate_phrase_rel(description, attr_prefs, target_id, handlers = None, top_level = True):
  if description == None:
    return "No disambiguating referring expression found."
  #get the relational attributes of our referent
  target_rels = [f for f in description if f[0] == Rel and (f[2] == target_id or f[3] == target_id)]
  target_desc = generate_phrase([f for f in description if f[0] != Rel and f[2] == target_id], attr_prefs, handlers)

  #relational attributes are added after whatever other attributes are used in generating English
  if len(target_rels) == 0:
    return target_desc

  clauses = []
  
  if not top_level:
    target_rels = [target_rels[0]]
  
  #generate English for relational attributes
  for cur_rel in target_rels:
    #Define attributes that are not in the current description of the referent
    #and are not non-relational attributes that have the referent as their first object
    other_attrs = [f for f in description if f not in target_rels and not (f[0] != Rel and f[1] == target_id)]
    rel_desc = cur_rel[1]
   
    # We have to handle which position the referent is in the relational attribute
    # There is a difference between generating the phrases:
    # "the box on the table" and "the table on which the box sits"
    if cur_rel[2] == target_id:
      if (handlers != None) and (handlers.has_key(rel_desc)):
        rel_desc = handlers[rel_desc](True)

      other_desc = generate_phrase_rel(other_attrs, attr_prefs, cur_rel[3], handlers, False)
      clauses.append("%s %s %s" % (target_desc, rel_desc, other_desc))
    else:
      if (handlers != None) and (handlers.has_key(rel_desc)):
        rel_desc = handlers[rel_desc](False)

      other_desc = generate_phrase_rel(other_attrs, attr_prefs, cur_rel[2], handlers, False)
      clauses.append("%s %s %s" % (target_desc, rel_desc,  other_desc))
  #This simple construction of phrases seems to makes sense most of the time
  return str.join(", and is ", clauses)


########NEW FILE########
__FILENAME__ = logicentail
# Natural Language Toolkit: Recognizing Textual Entailment (RTE)
#                           using deep semantic entailment 
#
# Author: Dan Garrette <dhgarrette@gmail.com>
#
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

from nltk.corpus import rte
from nltk import wordnet
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk.sem.logic import LogicParser 
from nltk.sem.glue import DrtGlue
from nltk import inference

class RTEInferenceTagger(object):
    """
    Predict whether a hypothesis can be inferred from a text, 
    based on the degree of word overlap.
    """
    def __init__(self, threshold=33, stop=True):
        self.threshold = threshold
        self.stop = stop
        self.stopwords = set(['a', 'the', 'it', 'they', 'of', 'in', 'is', 'are', 'were', 'and'])
        self.stemmer = WordNetLemmatizer()
    
    def tag(self, rtepair, verbose=False):
        """
        Tag a RTEPair as to whether the hypothesis can be inferred from the text.
        """
        return self.tag_sentences(rtepair.text, rtepair.hyp )

    def tag_sentences(self, text, hyp, verbose=False):
        """
        Tag a RTEPair as to whether the hypothesis can be inferred from the text.
        """
        glueclass = DrtGlue()
        text_drs_list = glueclass.parse_to_meaning(text)
        if text_drs_list:
            text_ex = text_drs_list[0].simplify().toFol()
        else:
            if verbose: print 'ERROR: No readings were generated for the Text'
        
        hyp_drs_list = glueclass.parse_to_meaning(hyp)
        if hyp_drs_list:
            hyp_ex = hyp_drs_list[0].simplify().toFol()
        else:
            if verbose: print 'ERROR: No readings were generated for the Hypothesis'

        #1. proof T -> H
        #2. proof (BK & T) -> H
        #3. proof :(BK & T)
        #4. proof :(BK & T & H)
        #5. satisfy BK & T
        #6. satisfy BK & T & H
            
        result = inference.Prover9().prove(hyp_ex, [text_ex])
        if verbose: print 'prove: T -> H: %s' % result
        
        if not result:
            bk = self._generate_BK(text, hyp, verbose)
            bk_exs = [bk_pair[0] for bk_pair in bk]
            
            if verbose: 
                print 'Generated Background Knowledge:'
                for bk_ex in bk_exs:
                    print bk_ex
                
            result = inference.Prover9().prove(hyp_ex, [text_ex]+bk_exs)
            if verbose: print 'prove: (T & BK) -> H: %s' % result
            
            if not result:
                consistent = self.check_consistency(bk_exs+[text_ex])                
                if verbose: print 'consistency check: (BK & T): %s' % consistent

                if consistent:
                    consistent = self.check_consistency(bk_exs+[text_ex, hyp_ex])                
                    if verbose: print 'consistency check: (BK & T & H): %s' % consistent
                    
        return result
    
    def check_consistency(self, assumptions, verbose=False):
        return inference.ParallelProverBuilderCommand(assumptions=assumptions).build_model()
        
    def _tag(self, text, hyp, verbose=False):
        self._generate_BK(text, hyp, verbose)
    
    def _generate_BK(self, text, hyp, verbose=False):
        text = word_tokenize(text)
        hyp = word_tokenize(hyp)
        
        if self.stemmer:
            textbow = set(self._stem(word) for word in text)
            hypbow = set(self._stem(word) for word in hyp)
        else:
            textbow = set(word.lower() for word in text)
            hypbow = set(word.lower() for word in hyp)
        
        if verbose:
            print 'textbow: %s' % textbow
            print 'hypbow: %s' % hypbow
        
        if self.stop:
            textbow = textbow - self.stopwords
            hypbow = hypbow - self.stopwords

        bk = []
        fullbow = textbow|hypbow
        for word_text in fullbow:
            pos = None
            if word_text in wordnet.N:
                bk.extend(self._generate_BK_word(word_text, wordnet.N, fullbow))
            if word_text in wordnet.V:
                bk.extend(self._generate_BK_word(word_text, wordnet.V, fullbow))
            if word_text in wordnet.ADJ:
                bk.extend(self._generate_BK_word(word_text, wordnet.ADJ, fullbow))
            if word_text in wordnet.ADV:
                bk.extend(self._generate_BK_word(word_text, wordnet.ADV, fullbow))
                
        return bk
        
    def _generate_BK_word(self, word_text, pos, fullbow):
        bk = []
        synonyms = set()
        hypernyms = set()
                
        for synset in pos[word_text]:
            for synonym_text in synset:
                if synonym_text != word_text and synonym_text.lower() in fullbow \
                                             and word_text.lower() in fullbow:
                    synonyms.add(synonym_text)
            for hypernymset in synset[wordnet.HYPERNYM]:
                for hypernym_text in hypernymset:
                    if hypernym_text != word_text and hypernym_text.lower() in fullbow \
                                                  and word_text.lower() in fullbow:
                        hypernyms.add(hypernym_text)
                    
        ######################################
        # synonym: all x.((synonym x) -> (word x))
        # hypernym: all x.((word x) -> (hypernym x))
        # synset-sister: all x.((word x) -> (not (sister x)))
        ######################################            
        
        for synonym_text in synonyms:
            bk.append(self._create_axiom_reverse(word_text, synset, synonym_text, pos, 'implies'))

        for hypernym_text in hypernyms - synonyms:
            bk.append(self._create_axiom(word_text, synset, hypernym_text, pos, 'implies'))

        # Create synset-sisters
        for i in range(len(pos[word_text])):
            synset1 = pos[word_text][i]
            j = i+1
            while j < len(pos[word_text]):
                synset2 = pos[word_text][j]
                for word1 in synset1:
                    if word1 != word_text and word1.lower() in fullbow:
                        for word2 in synset2:
                            if word2 != word_text and word2 != word1 and word2.lower() in fullbow:
                                bk.append(self._create_axiom_synset_sisters(word1, synset1, word2, synset2, pos))
                j = j+1
        
        return bk
        
    def _common_BK():
        # From Recognising Textual Entailment by Bos&Markert
        return [LogicParser().parse('all x y z.((in(x,y) & in(y,z)) -> in(x,z))'),
                LogicParser().parse('all e x y.((event(e) & subj(e,x) & in(e,y)) -> in(x,y))'),
                LogicParser().parse('all e x y.((event(e) & obj(e,x) & in(e,y)) -> in(x,y))'),
                LogicParser().parse('all e x y.((event(e) & theme(e,x) & in(e,y)) -> in(x,y))'),
                LogicParser().parse('all x y.(in(x,y) -> some e.(locate(e) & obj(e,x) & in(e,y)))'),
                LogicParser().parse('all x y.(of(x,y) -> some e.(have(e) & subj(e,y) & obj(e,x)))'),
                LogicParser().parse('all e y.((event(e) & subj(e,x)) -> by(e,x))')]
    
    def _create_axiom(self, word_text, word_synset, nym_text, pos, operator):
        nym_text = nym_text.split('(')[0];
        
        nym_word = pos[nym_text]
        dist = 1#min([word_synset.shortest_path_distance(nym_synset) for nym_synset in nym_word])

        word_text = word_text.replace('.', '')
        nym_text = nym_text.replace('.', '')

        exp_text = 'all x.(%s(x) %s %s(x))' % (word_text, operator, nym_text)
        return (LogicParser().parse(exp_text), dist)

    def _create_axiom_reverse(self, word_text, word_synset, nym_text, pos, operator):
        nym_text = nym_text.split('(')[0];

        nym_word = pos[nym_text]
        dist = 1#min([word_synset.shortest_path_distance(nym_synset) for nym_synset in nym_word])

        word_text = word_text.replace('.', '')
        nym_text = nym_text.replace('.', '')

        exp_text = 'all x.(%s(x) %s %s(x))' % (nym_text, operator, word_text)
        return (LogicParser().parse(exp_text), dist)

    def _create_axiom_synset_sisters(self, text1, word1_synset, text2, word2_synset, pos):
        """
        Return an expression of the form 'all x.(word(x) -> (not sister(x)))'.
        The reverse is not needed because it is equal to 'all x.((not word(x)) or (not sister(x)))'
        """
        
        text2 = text2.split('(')[0];

        dist = 1#word1_synset.shortest_path_distance(word2_synset)

        text1 = text1.replace('.', '')
        text2 = text2.replace('.', '')

        exp_text = 'all x.(%s(x) -> (not %s(x)))' % (text1, text2)
        return (LogicParser().parse(exp_text), dist)
    
    def _stem(self, word):
        stem = self.stemmer.stem(word)
        if stem:
            return stem
        else:
            return word
    

def demo_inference_tagger(verbose=False):
    tagger = RTEInferenceTagger()
    
    text = 'John see a car'
    print 'Text: ', text
    hyp = 'John watch an auto'
    print 'Hyp:  ', hyp

#    text_ex = LogicParser().parse('exists e x y.(david(x) & own(e)  & subj(e,x) & obj(e,y) & car(y))')
#    hyp_ex  = LogicParser().parse('exists e x y.(david(x) & have(e) & subj(e,x) & obj(e,y) & auto(y))')

    glueclass = DrtGlue(verbose=verbose)
    text_drs_list = glueclass.parse_to_meaning(text)
    if text_drs_list:
        text_ex = text_drs_list[0].simplify().toFol()
    else:
        print 'ERROR: No readings were be generated for the Text'
    
    hyp_drs_list = glueclass.parse_to_meaning(hyp)
    if hyp_drs_list:
        hyp_ex = hyp_drs_list[0].simplify().toFol()
    else:
        print 'ERROR: No readings were be generated for the Hypothesis'

    print 'Text: ', text_ex
    print 'Hyp:  ', hyp_ex
    print ''

    #1. proof T -> H
    #2. proof (BK & T) -> H
    #3. proof :(BK & T)
    #4. proof :(BK & T & H)
    #5. satisfy BK & T
    #6. satisfy BK & T & H
        
    result = inference.Prover9().prove(hyp_ex, [text_ex])
    print 'prove: T -> H: %s' % result
    if result:
        print 'Logical entailment\n'
    else:
        print 'No logical entailment\n'

    bk = tagger._generate_BK(text, hyp, verbose)
    bk_exs = [bk_pair[0] for bk_pair in bk]
    
    print 'Generated Background Knowledge:'
    for bk_ex in bk_exs:
        print bk_ex
    print ''
        
    result = inference.Prover9().prove(hyp_ex, [text_ex]+bk_exs)
    print 'prove: (T & BK) -> H: %s' % result
    if result:
        print 'Logical entailment\n'
    else:
        print 'No logical entailment\n'

    # Check if the background knowledge axioms are inconsistent
    result = inference.Prover9().prove(assumptions=bk_exs+[text_ex]).prove()
    print 'prove: (BK & T): %s' % result
    if result:
        print 'Inconsistency -> Entailment unknown\n'
    else:
        print 'No inconsistency\n'

    result = inference.Prover9().prove(assumptions=bk_exs+[text_ex, hyp_ex])
    print 'prove: (BK & T & H): %s' % result
    if result:
        print 'Inconsistency -> Entailment unknown\n'
    else:
        print 'No inconsistency\n'

    result = inference.Mace().build_model(assumptions=bk_exs+[text_ex])
    print 'satisfy: (BK & T): %s' % result
    if result:
        print 'No inconsistency\n'
    else:
        print 'Inconsistency -> Entailment unknown\n'

    result = inference.Mace().build_model(assumptions=bk_exs+[text_ex, hyp_ex]).build_model()
    print 'satisfy: (BK & T & H): %s' % result
    if result:
        print 'No inconsistency\n'
    else:
        print 'Inconsistency -> Entailment unknown\n'
    
def test_check_consistency():
    a = LogicParser().parse('man(j)')
    b = LogicParser().parse('-man(j)')
    print '%s, %s: %s' % (a, b, RTEInferenceTagger().check_consistency([a,b], True))
    print '%s, %s: %s' % (a, a, RTEInferenceTagger().check_consistency([a,a], True))

def tag(text, hyp):
    print 'Text: ', text
    print 'Hyp:  ', hyp
    print 'Entailment =', RTEInferenceTagger().tag_sentences(text, hyp, True)
    print ''

if __name__ == '__main__':
#    test_check_consistency()
#    print '\n'
    
    demo_inference_tagger(False)
    
    tag('John sees a car', 'John watches an auto')
    tag('John sees a car', 'John watches an airplane')

########NEW FILE########
__FILENAME__ = alignpairsFST
## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

"""Uses the FST library to construct FSTs of two phone-strings
and align them using the provided cost-matrix.
"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
hollingk@cslu.ogi.edu (Kristy Hollingshead)
"""

import sys
import os
import getopt
import tempfile

UNK_ = '<unk>'
EPSILON_ = '<eps>'
SHORT_EPS_ = '-'
INF_ = 1e10

def MyOpen(*args):
  f = open(*args)
  return f

def MyTemporaryFile(sfx):
  fd, name = tempfile.mkstemp(suffix=sfx)
  os.close(fd)
  return name

def ReadSymbols(symbolfile):
  symbols = []
  symbolFP = MyOpen(symbolfile, 'r')
  for line in symbolFP:
    line = line.strip()
    for s in line.split():
      if s not in symbols: symbols.append(s)
  symbolFP.close()
  return symbols

def ReadCostMatrix(matfile, symbols):
  rows = []
  cols = []
  matrix = {}
  if matfile is None: matfp = sys.stdin
  else: matfp = MyOpen(matfile, 'r')
  if len(symbols) == 0: genSymbols = True
  else: genSymbols = False
  # read column labels off first line
  line = matfp.readline()
  line = line.strip()
  cols = line.split()
  if genSymbols:
    for c in cols:
      symbols.append(c)
  for line in matfp:
    line = line.strip()
    row_label, costs = line.split(None,1)
    if genSymbols: symbols.append(row_label)
    if row_label not in symbols:
      print "Error: label (%s) not in defined symbols list" % row_label
      sys.exit(1)
    rows.append(row_label)
    costs = costs.split()
    if len(costs) != len(cols):
      print 'Error: wrong number of costs on line %s' % line
      sys.exit(1)
    for c in range(len(costs)):
      if costs[c] in ('inf', 'Inf', 'INF'): costs[c] = INF_
      matrix[(row_label, cols[c])] = float(costs[c])
  symbols = set(symbols)
  if matfile is not None: matfp.close()
  return matrix, rows, cols, symbols

def PrintFSTSymbols(symbols, symfile=None):
  symbols = set(symbols) # no duplicates
  if symfile is None:
    symfile = MyTemporaryFile('.sym.txt.fst')
  fp = MyOpen(symfile, 'w')
  fp.write('%s\t0\n' % (EPSILON_))
  fp.write('%s\t1\n' % (UNK_))
  id = 2
  for s in symbols:
    if s == EPSILON_ or s == UNK_: continue
    fp.write(s+'\t'+str(id)+'\n')
    id += 1
  fp.close()
  return symfile

def PrintChainFST(inputstr, symbols=[], txtfst=None):
  # inputstr is space-delimited string of characters
  if txtfst is None:
    txtfst = MyTemporaryFile('.chain.txt.fst')
  fstfp = MyOpen(txtfst, 'w')
  if len(symbols) == 0: genSymbols = True
  else: genSymbols = False
  # simple chain fst
  start = 0
  next = start+1
  curr = start
  for p in inputstr.split():
    if genSymbols: symbols.append(p)
    if p not in symbols: p = UNK_
    fstfp.write('%d\t%d\t%s\t%s\n' % (curr, next, p, p))
    curr = next
    next += 1
  fstfp.write('%d\n' % curr) # final state
  fstfp.close()
  symbols = set(symbols)
  return (txtfst, symbols)

def PrintDaisyFST(matrix, rows, cols, symbols=[], txtfst=None):
  if len(symbols) == 0: genSymbols = True
  else: genSymbols = False
  if txtfst is None:
    txtfst = MyTemporaryFile('.daisy.txt.fst')
  fstfp = MyOpen(txtfst, 'w')
  for x in rows:
    if genSymbols: symbols.append(x)
    if x not in symbols: x = UNK_
    for y in cols:
      if genSymbols: symbols.append(y)
      if y not in symbols: y = UNK_
      fstfp.write("0\t0\t"+x+'\t'+y+'\t'+\
                  str(matrix[(x,y)])+'\n')
  fstfp.write("0\n")
  fstfp.close()
  symbols = set(symbols)
  return (txtfst, symbols)

def PrintExtDaisyFST(matrix, rows, cols, symbols=[], txtfst=None):
  global SONORANTS_
  SONORANTS_ = ReadSymbols("sonorants.arpabet.txt")
  if len(symbols) == 0: genSymbols = True
  else: genSymbols = False
  if txtfst is None:
    txtfst = MyTemporaryFile('.extdaisy.txt.fst')
  fstfp = MyOpen(txtfst, 'w')
  # simple daisy fst + init cost
  for x in rows:
    if genSymbols: symbols.append(x)
    if x not in symbols: x = UNK_
    for y in cols:
      if genSymbols: symbols.append(y)
      if y not in symbols: y = UNK_
      # init cost (double cost)
      fstfp.write('0\t1\t'+x+'\t'+y+'\t'+ \
                  str(2*matrix[(x,y)])+'\n')
      # simple daisy fst
      fstfp.write('1\t1\t'+x+'\t'+y+'\t'+ \
                  str(matrix[(x,y)])+'\n')
      # exception for word-final sonorants
      if x in SONORANTS_ or y in SONORANTS_:
        fstfp.write('1\t2\t'+x+'\t'+y+'\t0\n') # for free
      else:
        fstfp.write('1\t2\t'+x+'\t'+y+'\t'+ \
                    str(matrix[(x,y)])+'\n')
    fstfp.write('1\n')
    fstfp.write('2\n')
  fstfp.close()
  symbols = set(symbols)
  return (txtfst, symbols)

def CompileFST(txtfstfile, symbols):
  # by default, will delete txtfstfile and symfile
  symfile = PrintFSTSymbols(symbols)
  binfstfile = MyTemporaryFile('.bin.fst')
  fstcompile = 'fstcompile --isymbols=%s --osymbols=%s \
                --keep_isymbols --keep_osymbols %s > %s' % \
                (symfile, symfile, txtfstfile, binfstfile)
  ret = os.system(fstcompile)
  if ret != 0:
    sys.stderr.write('Error in fstcompile\'ing\n')
    sys.exit(2)
  ret = os.system('rm -f %s %s' % (symfile, txtfstfile))
  if ret != 0:
    sys.stderr.write('Error in rm\'ing txt.fst\n')
    sys.exit(2)
  return binfstfile

def AlignFSTs(fst1, fst2, costfst, symbols):
  symfile = PrintFSTSymbols(symbols)
  alnfile = MyTemporaryFile('.aln.fst')
  fstcompose = 'fstcompose %s %s | fstinvert | fstarcsort | \
    fstcompose %s - >  %s' % (fst2, costfst, fst1, alnfile)
  ret = os.system(fstcompose)
  if ret != 0:
    sys.stderr.write('Error in fstcompose\'ing\n')
    sys.exit(2)
  fstbest = 'fstshortestpath %s | fsttopsort | fstprint' % (alnfile)
  fin, fout = os.popen2(fstbest)
  fin.close()
  alnmat = fout.read()
  fout.close()
  #sys.stderr.write('alnmatrix: '+alnmat+'\n')
  ret = os.system('rm -f %s %s %s %s' % (fst1, fst2, symfile, alnfile))
  if ret != 0:
    sys.stderr.write('Error in rm\'ing alignment files\n')
    sys.exit(2)
  aln1 = []
  aln2 = []
  cost = 0.0
  for line in alnmat.split('\n'):
    line = line.strip()
    cols = line.split('\t')
    if len(cols) == 1: break
    aln1.append(cols[2])
    aln2.append(cols[3])
    try: cost += float(cols[4])
    except IndexError:
      # sys.stderr.write('zero cost\n')
      cost += 0.0
    except ValueError:
      sys.stderr.write('Error: non-float cost\n')
  return ' '.join(aln1), ' '.join(aln2), cost

def main(matrixfile, symfile=None, infile=None):
  if symfile is not None:
    syms = ReadSymbols(symfile)
  else: syms = []
  inputmatrix, row_heads, col_heads, syms = \
    ReadCostMatrix(matrixfile, syms)
  txtmatrix, syms = \
    PrintDaisyFST(inputmatrix, row_heads, col_heads, syms)
    # or:
    # PrintExtDaisyFST(inputmatrix, row_heads, col_heads, syms)
  binmatrix = CompileFST(txtmatrix, syms)
  if infile is not None: infp = MyOpen(infile, 'r')
  else: infp = sys.stdin
  for line in infp:
    line = line.strip()
    ph1, ph2 = line.split('\t')
    txtph1, syms = PrintChainFST(ph1, syms)
    binph1 = CompileFST(txtph1, syms)
    txtph2, syms = PrintChainFST(ph2, syms)
    binph2 = CompileFST(txtph2, syms)
    aln1, aln2, cost = AlignFSTs(binph1, binph2, binmatrix, syms)
    #aln1 = aln1.replace(EPSILON_, SHORT_EPS_)
    #aln2 = aln2.replace(EPSILON_, SHORT_EPS_)
    print '%s\t%s\t%.6f' % (aln1, aln2, cost)
  ret = os.system('rm -f %s' % (binmatrix))
  if ret != 0:
    sys.stderr.write('Error in rm\'ing matrix\n')
    sys.exit(2)
  if infile is not None: infp.close()

def usage(called):
  print '%s -m <matrix-file> [-s <symbols-file>]' % (called),
  print '[-i <phone-pairs-file>]'

if __name__ == '__main__':
  try:
    opts, args = \
      getopt.getopt(sys.argv[1:], '?m:s:i:', \
      ['help', 'matrix', 'symbols', 'input'])
  except getopt.GetoptError:
    # print help information and exit:
    usage(argv[0])
    sys.exit(2)
  matfile = None
  symfile = None
  infile = None
  for op, a in opts:
    if op in ('-?', '--help'):
      usage(sys.argv[0])
      sys.exit()
    if op in ('-m', '--matrix'):
      matfile = a
    if op in ('-s', '--symbols'):
      symfile = a
    if op in ('-i', '--input'):
      infile = a
  if matfile is None:
    usage(sys.argv[0])
    print "Error: must provide a cost-matrix file."
    sys.exit(2)
  main(matfile, symfile, infile)

########NEW FILE########
__FILENAME__ = auxiliary_comp
# -*- coding: utf-8 -*-

## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

"""Additional string-based comparators:

1) pinyin comparator: match a sequence of Chinese characters against
   its pinyin transcription.

"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
"""

import sys
import tokens
import token_comp
import Utils.script
from __init__ import BASE_


PINYIN_TABLE_FILE_ = '%s/Utils/pinyin.txt' % BASE_
PINYIN_TABLE_ = {}
PINYIN_TABLE_LOADED_ = False
PY_WG_MAP_FILE_ = '%s/Utils/py2wg.txt' % BASE_
PY_WG_MAP_ = {}
PY_WG_MAP_LOADED_ = False
MATCH_ = 0
NO_MATCH_ = 10000


def LoadPinyinTable():
  global PINYIN_TABLE_LOADED_
  p = open(PINYIN_TABLE_FILE_)
  for line in p.readlines():
    line = line.strip().split()
    try: PINYIN_TABLE_[line[0]].append(line[1])
    except KeyError: PINYIN_TABLE_[line[0]] = [line[1]]
  p.close()
  PINYIN_TABLE_LOADED_ = True


def LoadPYWGMap():
  global PY_WG_MAP_LOADED_
  p = open(PY_WG_MAP_FILE_)
  for line in p.readlines():
    line = line.strip().split()
    PY_WG_MAP_[line[0]] = line[1]
  p.close()
  PY_WG_MAP_LOADED_ = True


def Catenations(pys, result, string=''):
  if not pys:
    result.append(string)
  else:
    first = pys[0]
    for f in first:
      Catenations(pys[1:], result, string=string + f)


def LookupString(chars, convert=False):
  pys = []
  for u in unicode(chars, 'utf8'):
    try:
      py = PINYIN_TABLE_[u.encode('utf8')]
      npy = []
      if convert and PY_WG_MAP_LOADED_:
        for p in py:
          p = PY_WG_MAP_[p]
          p = p.replace("'", '')
          npy.append(p)
        py = npy
      pys.append(py)
    except KeyError:
      return []
  result = []
  Catenations(pys, result=result, string='')
  return result

class PinyinComparator(token_comp.TokenComparator):
  """Compare Chinese character sequence to Pinyin transcription such
  as one would find in standard newspapers.
  """
  def __init__(self, token1, token2):
    self.token1_ = token1
    self.token2_ = token2
    self.InitData()

  def InitData(self):
    if not PINYIN_TABLE_LOADED_: LoadPinyinTable()

  def ComputeDistance(self):
    """Assumes pinyin with no tone marks
    """
    latin = self.token1_.String()
    hanzi = self.token2_.String()
    result = token_comp.ComparisonResult(self.token1_, self.token2_)
    result.SetInfo('%s <-> %s' % (latin, hanzi))
    self.comparison_result_ = result
    script1 = Utils.script.StringToScript(latin)
    script2 = Utils.script.StringToScript(hanzi)
    if script1 == 'Latin' and script2 == 'CJK':
      pass
    elif script2 == 'Latin' and script1 == 'CJK': 
      latin, hanzi = hanzi, latin
    else:
      result.SetCost(NO_MATCH_)
      return
    latin = ''.join(latin.split()).lower()
    latin = latin.replace("'", '') ## ' sometimes used to mark boundary
    latin = latin.replace("-", '') ## - sometimes used to mark boundary
    pinyins = LookupString(hanzi)
    if latin in pinyins: result.SetCost(MATCH_)
    else: result.SetCost(NO_MATCH_)


class WadeGilesComparator(token_comp.TokenComparator):
  """Compare Chinese character sequence to Wade-Giles transcription such
  as one would find in older texts.
  """
  def __init__(self, token1, token2):
    self.token1_ = token1
    self.token2_ = token2
    self.InitData()

  def InitData(self):
    if not PINYIN_TABLE_LOADED_: LoadPinyinTable()
    if not PY_WG_MAP_LOADED_: LoadPYWGMap()

  def ComputeDistance(self):
    """Assumes pinyin with no tone marks
    """
    latin = self.token1_.String()
    hanzi = self.token2_.String()
    result = token_comp.ComparisonResult(self.token1_, self.token2_)
    result.SetInfo('%s <-> %s' % (latin, hanzi))
    self.comparison_result_ = result
    script1 = Utils.script.StringToScript(latin)
    script2 = Utils.script.StringToScript(hanzi)
    if script1 == 'Latin' and script2 == 'CJK':
      pass
    elif script2 == 'Latin' and script1 == 'CJK': 
      latin, hanzi = hanzi, latin
    else:
      result.SetCost(NO_MATCH_)
      return
    latin = ''.join(latin.split()).lower()
    latin = latin.replace("'", '') ## ' sometimes used to mark boundary
    latin = latin.replace("-", '') ## - sometimes used to mark boundary
    pinyins = LookupString(hanzi, convert=True)
    if latin in pinyins: result.SetCost(MATCH_)
    else: result.SetCost(NO_MATCH_)

########NEW FILE########
__FILENAME__ = chinese_extractor
# -*- coding: utf-8 -*-

## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

"""Chinese foreign-word-specific token extractor class.
"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
"""

import extractor
import tokens
import Utils.script
from __init__ import BASE_

MIN_LEN_ = 3

CDOT_ = '・'

FOREIGN_CHARS_ = {
    "丁" : 1,
    "乃" : 1,
    "三" : 1,
    "凡" : 1,
    "土" : 1,
    "士" : 1,
    "夕" : 1,
    "大" : 1,
    "山" : 1,
    "不" : 1,
    "丹" : 1,
    "什" : 1,
    "内" : 1,
    "厄" : 1,
    "及" : 1,
    "夫" : 1,
    "太" : 1,
    "巴" : 1,
    "日" : 1,
    "比" : 1,
    "牙" : 1,
    "他" : 1,
    "加" : 1,
    "北" : 1,
    "卡" : 1,
    "可" : 1,
    "史" : 1,
    "尼" : 1,
    "布" : 1,
    "打" : 1,
    "旦" : 1,
    "本" : 1,
    "瓜" : 1,
    "瓦" : 1,
    "甘" : 1,
    "生" : 1,
    "白" : 1,
    "立" : 1,
    "伊" : 1,
    "列" : 1,
    "印" : 1,
    "吉" : 1,
    "地" : 1,
    "圭" : 1,
    "多" : 1,
    "安" : 1,
    "托" : 1,
    "朵" : 1,
    "汗" : 1,
    "百" : 1,
    "米" : 1,
    "耳" : 1,
    "艾" : 1,
    "衣" : 1,
    "西" : 1,
    "佛" : 1,
    "伯" : 1,
    "克" : 1,
    "利" : 1,
    "宋" : 1,
    "希" : 1,
    "杜" : 1,
    "杉" : 1,
    "沙" : 1,
    "罕" : 1,
    "贝" : 1,
    "辛" : 1,
    "那" : 1,
    "里" : 1,
    "亚" : 1,
    "亚" : 1,
    "佳" : 1,
    "来" : 1,
    "典" : 1,
    "坡" : 1,
    "坦" : 1,
    "奈" : 1,
    "姆" : 1,
    "孟" : 1,
    "定" : 1,
    "尚" : 1,
    "居" : 1,
    "冈" : 1,
    "帕" : 1,
    "底" : 1,
    "拉" : 1,
    "易" : 1,
    "昂" : 1,
    "明" : 1,
    "果" : 1,
    "林" : 1,
    "松" : 1,
    "河" : 1,
    "波" : 1,
    "法" : 1,
    "治" : 1,
    "肯" : 1,
    "芭" : 1,
    "芬" : 1,
    "金" : 1,
    "门" : 1,
    "陀" : 1,
    "阿" : 1,
    "阿" : 1,
    "保" : 1,
    "俄" : 1,
    "南" : 1,
    "哇" : 1,
    "哈" : 1,
    "奎" : 1,
    "威" : 1,
    "度" : 1,
    "律" : 1,
    "拜" : 1,
    "柯" : 1,
    "查" : 1,
    "柏" : 1,
    "洛" : 1,
    "玻" : 1,
    "科" : 1,
    "突" : 1,
    "约" : 1,
    "美" : 1,
    "耶" : 1,
    "茅" : 1,
    "英" : 1,
    "迦" : 1,
    "迪" : 1,
    "伦" : 1,
    "哥" : 1,
    "埃" : 1,
    "夏" : 1,
    "库" : 1,
    "恩" : 1,
    "拿" : 1,
    "朗" : 1,
    "根" : 1,
    "格" : 1,
    "泰" : 1,
    "浦" : 1,
    "海" : 1,
    "乌" : 1,
    "特" : 1,
    "班" : 1,
    "索" : 1,
    "纽" : 1,
    "纳" : 1,
    "兹" : 1,
    "茨" : 1,
    "马" : 1,
    "勒" : 1,
    "曼" : 1,
    "基" : 1,
    "培" : 1,
    "婆" : 1,
    "密" : 1,
    "仑" : 1,
    "康" : 1,
    "得" : 1,
    "理" : 1,
    "毕" : 1,
    "莎" : 1,
    "莫" : 1,
    "莉" : 1,
    "荷" : 1,
    "都" : 1,
    "雪" : 1,
    "麦" : 1,
    "麻" : 1,
    "凯" : 1,
    "博" : 1,
    "喀" : 1,
    "喜" : 1,
    "乔" : 1,
    "堪" : 1,
    "堡" : 1,
    "几" : 1,
    "敦" : 1,
    "斐" : 1,
    "斯" : 1,
    "普" : 1,
    "森" : 1,
    "汤" : 1,
    "犹" : 1,
    "琴" : 1,
    "答" : 1,
    "丝" : 1,
    "舒" : 1,
    "华" : 1,
    "莱" : 1,
    "菲" : 1,
    "贺" : 1,
    "买" : 1,
    "开" : 1,
    "隆" : 1,
    "雅" : 1,
    "黑" : 1,
    "塞" : 1,
    "塞" : 1,
    "塔" : 1,
    "奥" : 1,
    "爱" : 1,
    "新" : 1,
    "瑟" : 1,
    "瑞" : 1,
    "圣" : 1,
    "蒂" : 1,
    "叶" : 1,
    "路" : 1,
    "达" : 1,
    "雷" : 1,
    "顿" : 1,
    "嫩" : 1,
    "汉" : 1,
    "满" : 1,
    "尔" : 1,
    "玛" : 1,
    "福" : 1,
    "维" : 1,
    "蒙" : 1,
    "盖" : 1,
    "宾" : 1,
    "赫" : 1,
    "魁" : 1,
    "德" : 1,
    "慕" : 1,
    "摩" : 1,
    "撒" : 1,
    "撒" : 1,
    "潘" : 1,
    "热" : 1,
    "鲁" : 1,
    "黎" : 1,
    "墨" : 1,
    "卢" : 1,
    "翰" : 1,
    "诺" : 1,
    "赖" : 1,
    "锡" : 1,
    "弥" : 1,
    "济" : 1,
    "赛" : 1,
    "迈" : 1,
    "萨" : 1,
    "琼" : 1,
    "罗" : 1,
    "腊" : 1,
    "颠" : 1,
    "苏" : 1,
    "兰" : 1,
    "铁" : 1,
    "露" : 1,
    "璐" : 1,
    "朱" : 1,
    "娅" : 1,
    "默" : 1,
#    "・" : 1,
    "戴" : 1,
    "卓" : 1,
    "白" : 1,
    "艾" : 1,
    "爱" : 1,
    "阿" : 1,
    "埃" : 1,
    "底" : 1,
    "旦" : 1,
    "丹" : 1,
#    "・" : 1,
    "卡" : 1,
    "孔" : 1,
    "喀" : 1,
    "堪" : 1,
    "康" : 1,
    "凯" : 1,
    "法" : 1,
    "范" : 1,
    "甫" : 1,
    "傅" : 1,
    "福" : 1,
    "弗" : 1,
    "定" : 1,
    "恩" : 1,
    "冬" : 1,
    "鼎" : 1,
    "丁" : 1,
    "贝" : 1,
    "鲍" : 1,
    "堡" : 1,
    "保" : 1,
    "北" : 1,
    "都" : 1,
    "及" : 1,
    "几" : 1,
    "娇" : 1,
    "蕉" : 1,
    "本" : 1,
    "吉" : 1,
    "悸" : 1,
    "济" : 1,
    "九" : 1,
    "金" : 1,
    "叫" : 1,
    "卷" : 1,
    "佳" : 1,
    "窖" : 1,
    "加" : 1,
    "简" : 1,
    "剪" : 1,
    "贾" : 1,
    "居" : 1,
    "爵" : 1,
    "晋" : 1,
    "杰" : 1,
    "捷" : 1,
    "狗" : 1,
    "苟" : 1,
    "贡" : 1,
    "古" : 1,
    "姑" : 1,
    "圭" : 1,
    "瓜" : 1,
    "果" : 1,
    "哈" : 1,
    "安" : 1,
    "坎" : 1,
    "凡" : 1,
    "波" : 1,
    "卜" : 1,
    "不" : 1,
    "博" : 1,
    "伯" : 1,
    "玻" : 1,
    "布" : 1,
    "勃" : 1,
    "猜" : 1,
    "查" : 1,
    "仓" : 1,
    "怀" : 1,
    "冻" : 1,
    "长" : 1,
    "臣" : 1,
    "楚" : 1,
    "开" : 1,
    "华" : 1,
    "岑" : 1,
    "岱" : 1,
    "噶" : 1,
    "哩" : 1,
    "恋" : 1,
    "练" : 1,
    "莲" : 1,
    "廉" : 1,
    "立" : 1,
    "班" : 1,
    "多" : 1,
    "来" : 1,
    "雷" : 1,
    "腊" : 1,
    "赖" : 1,
    "拉" : 1,
    "坤" : 1,
    "昆" : 1,
    "库" : 1,
    "奥" : 1,
    "德" : 1,
    "柯" : 1,
    "嘎" : 1,
    "伦" : 1,
    "卢" : 1,
    "娄" : 1,
    "隆" : 1,
    "路" : 1,
    "露" : 1,
    "芭" : 1,
    "杜" : 1,
    "努" : 1,
    "纽" : 1,
    "诺" : 1,
    "奴" : 1,
    "典" : 1,
    "夸" : 1,
    "朵" : 1,
    "茨" : 1,
    "默" : 1,
    "摩" : 1,
    "沫" : 1,
    "姆" : 1,
    "墨" : 1,
    "漠" : 1,
    "得" : 1,
    "明" : 1,
    "米" : 1,
    "梅" : 1,
    "毛" : 1,
    "茅" : 1,
    "芒" : 1,
    "兰" : 1,
    "慕" : 1,
    "木" : 1,
    "虏" : 1,
    "莱" : 1,
    "鲁" : 1,
    "琅" : 1,
    "吕" : 1,
    "洛" : 1,
    "帕" : 1,
    "培" : 1,
    "佩" : 1,
    "莫" : 1,
    "尼" : 1,
    "穆" : 1,
    "拿" : 1,
    "美" : 1,
    "沐" : 1,
    "缅" : 1,
    "门" : 1,
    "沛" : 1,
    "狼" : 1,
    "那" : 1,
    "列" : 1,
    "祖" : 1,
    "锋" : 1,
    "娣" : 1,
    "娅" : 1,
    "姗" : 1,
    "科" : 1,
    "菲" : 1,
    "皮" : 1,
    "黎" : 1,
    "朗" : 1,
    "麻" : 1,
    "律" : 1,
    "玛" : 1,
    "迦" : 1,
    "抨" : 1,
    "潘" : 1,
    "纳" : 1,
    "娜" : 1,
    "乃" : 1,
    "蒙" : 1,
    "汶" : 1,
    "撇" : 1,
    "媛" : 1,
    "坡" : 1,
    "婆" : 1,
    "婷" : 1,
    "奇" : 1,
    "齐" : 1,
    "浦" : 1,
    "普" : 1,
    "七" : 1,
    "廖" : 1,
    "理" : 1,
    "劳" : 1,
    "里" : 1,
    "马" : 1,
    "彭" : 1,
    "蓬" : 1,
    "耐" : 1,
    "念" : 1,
    "奈" : 1,
    "娘" : 1,
    "南" : 1,
    "孟" : 1,
    "嵊" : 1,
    "烈" : 1,
    "林" : 1,
    "琳" : 1,
    "柳" : 1,
    "莉" : 1,
    "丽" : 1,
    "勒" : 1,
    "麦" : 1,
    "买" : 1,
    "迈" : 1,
    "仑" : 1,
    "庞" : 1,
    "浜" : 1,
    "弥" : 1,
    "秘" : 1,
    "溥" : 1,
    "龙" : 1,
    "蕾" : 1,
    "利" : 1,
    "满" : 1,
    "蔓" : 1,
    "曼" : 1,
    "内" : 1,
    "涅" : 1,
    "嫩" : 1,
    "密" : 1,
    "恺" : 1,
    "妮" : 1,
    "罗" : 1,
    "哥" : 1,
    "绛" : 1,
    "登" : 1,
    "壳" : 1,
    "盖" : 1,
    "恰" : 1,
    "羌" : 1,
    "卿" : 1,
    "乔" : 1,
    "丘" : 1,
    "切" : 1,
    "珀" : 1,
    "琼" : 1,
    "缪" : 1,
    "茄" : 1,
    "钦" : 1,
    "沁" : 1,
    "琴" : 1,
    "答" : 1,
    "佛" : 1,
    "葛" : 1,
    "娥" : 1,
    "谢" : 1,
    "肖" : 1,
    "歇" : 1,
    "辛" : 1,
    "欣" : 1,
    "新" : 1,
    "信" : 1,
    "袖" : 1,
    "星" : 1,
    "休" : 1,
    "玄" : 1,
    "可" : 1,
    "莎" : 1,
    "森" : 1,
    "僧" : 1,
    "桑" : 1,
    "璐" : 1,
    "瑙" : 1,
    "兹" : 1,
    "度" : 1,
    "比" : 1,
    "搓" : 1,
    "戈" : 1,
    "臧" : 1,
    "甘" : 1,
    "士" : 1,
    "史" : 1,
    "诗" : 1,
    "施" : 1,
    "圣" : 1,
    "冯" : 1,
    "八" : 1,
    "邓" : 1,
    "克" : 1,
    "彼" : 1,
    "索" : 1,
    "丝" : 1,
    "朔" : 1,
    "帅" : 1,
    "杉" : 1,
    "山" : 1,
    "冉" : 1,
    "斯" : 1,
    "什" : 1,
    "槌" : 1,
    "沙" : 1,
    "瑟" : 1,
    "舍" : 1,
    "儒" : 1,
    "茹" : 1,
    "抒" : 1,
    "舒" : 1,
    "尚" : 1,
    "瑞" : 1,
    "枭" : 1,
    "热" : 1,
    "松" : 1,
    "颂" : 1,
    "宋" : 1,
    "珊" : 1,
    "若" : 1,
    "撒" : 1,
    "戎" : 1,
    "日" : 1,
    "萨" : 1,
    "苏" : 1,
    "滕" : 1,
    "生" : 1,
    "塞" : 1,
    "赛" : 1,
    "他" : 1,
    "三" : 1,
    "绳" : 1,
    "兽" : 1,
    "塔" : 1,
    "俄" : 1,
    "巴" : 1,
    "汀" : 1,
    "突" : 1,
    "桐" : 1,
    "廷" : 1,
    "黛" : 1,
    "嘴" : 1,
    "太" : 1,
    "泰" : 1,
    "邦" : 1,
    "孜" : 1,
    "滴" : 1,
    "西" : 1,
    "维" : 1,
    "韦" : 1,
    "达" : 1,
    "迪" : 1,
    "肯" : 1,
    "毕" : 1,
    "夕" : 1,
    "席" : 1,
    "希" : 1,
    "图" : 1,
    "坦" : 1,
    "喜" : 1,
    "斐" : 1,
    "土" : 1,
    "吐" : 1,
    "汤" : 1,
    "温" : 1,
    "铄" : 1,
    "毋" : 1,
    "箱" : 1,
    "夏" : 1,
    "唐" : 1,
    "托" : 1,
    "勿" : 1,
    "惕" : 1,
    "伍" : 1,
    "仙" : 1,
    "臀" : 1,
    "翁" : 1,
    "陀" : 1,
    "禅" : 1,
    "特" : 1,
    "魏" : 1,
    "沃" : 1,
    "瓦" : 1,
    "旺" : 1,
    "哇" : 1,
    "忘" : 1,
    "铁" : 1,
    "乌" : 1,
    "梯" : 1,
    "锡" : 1,
    "威" : 1,
    "娃" : 1,
    "格" : 1,
    "笛" : 1,
    "费" : 1,
    "炎" : 1,
    "选" : 1,
    "逊" : 1,
    "雪" : 1,
    "牙" : 1,
    "雅" : 1,
    "袂" : 1,
    "亚" : 1,
    "扬" : 1,
    "延" : 1,
    "演" : 1,
    "昂" : 1,
    "含" : 1,
    "海" : 1,
    "罕" : 1,
    "大" : 1,
    "庇" : 1,
    "莹" : 1,
    "印" : 1,
    "英" : 1,
    "打" : 1,
    "夫" : 1,
    "厄" : 1,
    "狄" : 1,
    "芬" : 1,
    "易" : 1,
    "叶" : 1,
    "耶" : 1,
    "尧" : 1,
    "佐" : 1,
    "霍" : 1,
    "冈" : 1,
    "宰" : 1,
    "裕" : 1,
    "钓" : 1,
    "宾" : 1,
    "芝" : 1,
    "斋" : 1,
    "乍" : 1,
    "柏" : 1,
    "地" : 1,
    "敦" : 1,
    "咏" : 1,
    "约" : 1,
    "汗" : 1,
    "翰" : 1,
    "詹" : 1,
    "亨" : 1,
    "豪" : 1,
    "伊" : 1,
    "衣" : 1,
    "义" : 1,
    "珍" : 1,
    "宙" : 1,
    "銮" : 1,
    "荷" : 1,
    "尤" : 1,
    "鲫" : 1,
    "侯" : 1,
    "何" : 1,
    "犹" : 1,
    "朱" : 1,
    "治" : 1,
    "蹴" : 1,
    "汉" : 1,
    "河" : 1,
    "赫" : 1,
    "贺" : 1,
    "艺" : 1,
    "泽" : 1,
    "趄" : 1,
    "哉" : 1,
    "拄" : 1,
    "胡" : 1,
    "黑" : 1,
    "扎" : 1,
    "赞" : 1,
    "颠" : 1,
    "叻" : 1,
    "耳" : 1,
    "讪" : 1,
    "讷" : 1,
    "诋" : 1,
    "根" : 1,
    "基" : 1,
    "踪" : 1,
    "百" : 1,
    "蒂" : 1,
    "顿" : 1,
    "蝶" : 1,
    "尔" : 1,
    "迭" : 1,
    "奎" : 1,
    "芙" : 1,
    "茜" : 1,
    "衮" : 1,
    "魁" : 1,
    "拜" : 1,
    "炳" : 1,
    "薇" : 1}

FAMILY_NAMES_ = {
  "丁" : 1,
  "万" : 1,
  "丘" : 1,
  "丛" : 1,
  "严" : 1,
  "乌" : 1,
  "乐" : 1,
  "乔" : 1,
  "于" : 1,
  "亓" : 1,
  "仇" : 1,
  "仝" : 1,
  "仲" : 1,
  "任" : 1,
  "伍" : 1,
  "伏" : 1,
  "但" : 1,
  "佐" : 1,
  "何" : 1,
  "佘" : 1,
  "余" : 1,
  "佟" : 1,
  "侯" : 1,
  "俞" : 1,
  "倪" : 1,
  "傅" : 1,
  "储" : 1,
  "儲" : 1,
  "党" : 1,
  "关" : 1,
  "冀" : 1,
  "冉" : 1,
  "冯" : 1,
  "冷" : 1,
  "冼" : 1,
  "凌" : 1,
  "刁" : 1,
  "刑" : 1,
  "刘" : 1,
  "劉" : 1,
  "劳" : 1,
  "勞" : 1,
  "勾" : 1,
  "包" : 1,
  "匡" : 1,
  "区" : 1,
  "區" : 1,
  "华" : 1,
  "卓" : 1,
  "单" : 1,
  "卜" : 1,
  "卞" : 1,
  "卢" : 1,
  "卫" : 1,
  "印" : 1,
  "厉" : 1,
  "厲" : 1,
  "叢" : 1,
  "古" : 1,
  "台" : 1,
  "史" : 1,
  "叶" : 1,
  "司" : 1,
  "吉" : 1,
  "后" : 1,
  "向" : 1,
  "吕" : 1,
  "吳" : 1,
  "吴" : 1,
  "呂" : 1,
  "周" : 1,
  "哈" : 1,
  "唐" : 1,
  "商" : 1,
  "喬" : 1,
  "單" : 1,
  "喻" : 1,
  "嚴" : 1,
  "塗" : 1,
  "夏" : 1,
  "奚" : 1,
  "姚" : 1,
  "姜" : 1,
  "姬" : 1,
  "娄" : 1,
  "婁" : 1,
  "孔" : 1,
  "孙" : 1,
  "孟" : 1,
  "季" : 1,
  "孫" : 1,
  "宁" : 1,
  "安" : 1,
  "宋" : 1,
  "宓" : 1,
  "宗" : 1,
  "官" : 1,
  "宣" : 1,
  "宫" : 1,
  "宮" : 1,
  "容" : 1,
  "寇" : 1,
  "寧" : 1,
  "封" : 1,
  "尉" : 1,
  "尚" : 1,
  "尤" : 1,
  "尹" : 1,
  "屈" : 1,
  "展" : 1,
  "屠" : 1,
  "岑" : 1,
  "岳" : 1,
  "崔" : 1,
  "嵇" : 1,
  "左" : 1,
  "巩" : 1,
  "巫" : 1,
  "巴" : 1,
  "帅" : 1,
  "师" : 1,
  "帥" : 1,
  "師" : 1,
  "席" : 1,
  "常" : 1,
  "干" : 1,
  "庄" : 1,
  "应" : 1,
  "庞" : 1,
  "康" : 1,
  "庾" : 1,
  "廉" : 1,
  "廖" : 1,
  "延" : 1,
  "张" : 1,
  "張" : 1,
  "归" : 1,
  "彭" : 1,
  "徐" : 1,
  "忻" : 1,
  "慕" : 1,
  "應" : 1,
  "成" : 1,
  "戚" : 1,
  "戴" : 1,
  "房" : 1,
  "扈" : 1,
  "扶" : 1,
  "承" : 1,
  "支" : 1,
  "敖" : 1,
  "文" : 1,
  "斐" : 1,
  "方" : 1,
  "施" : 1,
  "旷" : 1,
  "易" : 1,
  "晁" : 1,
  "晉" : 1,
  "晋" : 1,
  "晏" : 1,
  "普" : 1,
  "曠" : 1,
  "曲" : 1,
  "曹" : 1,
  "曾" : 1,
  "朱" : 1,
  "朴" : 1,
  "权" : 1,
  "李" : 1,
  "杜" : 1,
  "杨" : 1,
  "杭" : 1,
  "林" : 1,
  "柏" : 1,
  "查" : 1,
  "柯" : 1,
  "柳" : 1,
  "柴" : 1,
  "栾" : 1,
  "桂" : 1,
  "桑" : 1,
  "梁" : 1,
  "梅" : 1,
  "楊" : 1,
  "楚" : 1,
  "楼" : 1,
  "榮" : 1,
  "樂" : 1,
  "樊" : 1,
  "樓" : 1,
  "欉" : 1,
  "權" : 1,
  "欒" : 1,
  "欧" : 1,
  "歐" : 1,
  "步" : 1,
  "武" : 1,
  "歸" : 1,
  "段" : 1,
  "殷" : 1,
  "毕" : 1,
  "毛" : 1,
  "汉" : 1,
  "江" : 1,
  "池" : 1,
  "汤" : 1,
  "汪" : 1,
  "沈" : 1,
  "沐" : 1,
  "沙" : 1,
  "洪" : 1,
  "浦" : 1,
  "涂" : 1,
  "淩" : 1,
  "温" : 1,
  "游" : 1,
  "湛" : 1,
  "湯" : 1,
  "溫" : 1,
  "滕" : 1,
  "满" : 1,
  "滿" : 1,
  "漆" : 1,
  "漢" : 1,
  "潘" : 1,
  "濮" : 1,
  "烏" : 1,
  "焦" : 1,
  "熊" : 1,
  "燕" : 1,
  "牛" : 1,
  "牟" : 1,
  "狄" : 1,
  "王" : 1,
  "班" : 1,
  "璩" : 1,
  "甄" : 1,
  "甘" : 1,
  "甯" : 1,
  "田" : 1,
  "申" : 1,
  "畢" : 1,
  "白" : 1,
  "皮" : 1,
  "盖" : 1,
  "盛" : 1,
  "盧" : 1,
  "瞿" : 1,
  "石" : 1,
  "祁" : 1,
  "祝" : 1,
  "禚" : 1,
  "禹" : 1,
  "秦" : 1,
  "程" : 1,
  "穆" : 1,
  "窦" : 1,
  "竇" : 1,
  "章" : 1,
  "童" : 1,
  "竺" : 1,
  "符" : 1,
  "简" : 1,
  "管" : 1,
  "簡" : 1,
  "米" : 1,
  "粘" : 1,
  "糜" : 1,
  "紀" : 1,
  "綦" : 1,
  "練" : 1,
  "繆" : 1,
  "纪" : 1,
  "练" : 1,
  "缪" : 1,
  "罗" : 1,
  "羅" : 1,
  "羊" : 1,
  "翁" : 1,
  "翟" : 1,
  "耿" : 1,
  "聂" : 1,
  "聞" : 1,
  "聶" : 1,
  "胡" : 1,
  "胥" : 1,
  "臧" : 1,
  "臺" : 1,
  "舒" : 1,
  "艾" : 1,
  "芮" : 1,
  "花" : 1,
  "苏" : 1,
  "苗" : 1,
  "苟" : 1,
  "范" : 1,
  "茅" : 1,
  "茆" : 1,
  "荀" : 1,
  "荆" : 1,
  "荊" : 1,
  "荣" : 1,
  "莊" : 1,
  "莫" : 1,
  "華" : 1,
  "萧" : 1,
  "萨" : 1,
  "萬" : 1,
  "葉" : 1,
  "葛" : 1,
  "董" : 1,
  "蒋" : 1,
  "蒙" : 1,
  "蒯" : 1,
  "蒲" : 1,
  "蓋" : 1,
  "蓝" : 1,
  "蔡" : 1,
  "蔣" : 1,
  "蔺" : 1,
  "蕭" : 1,
  "薛" : 1,
  "薩" : 1,
  "藍" : 1,
  "藺" : 1,
  "蘇" : 1,
  "虞" : 1,
  "衛" : 1,
  "袁" : 1,
  "裘" : 1,
  "裴" : 1,
  "褚" : 1,
  "覃" : 1,
  "解" : 1,
  "言" : 1,
  "計" : 1,
  "許" : 1,
  "詹" : 1,
  "談" : 1,
  "謝" : 1,
  "譙" : 1,
  "譚" : 1,
  "计" : 1,
  "许" : 1,
  "谈" : 1,
  "谢" : 1,
  "谭" : 1,
  "谯" : 1,
  "谷" : 1,
  "貝" : 1,
  "費" : 1,
  "賀" : 1,
  "賈" : 1,
  "賴" : 1,
  "贝" : 1,
  "费" : 1,
  "贺" : 1,
  "贾" : 1,
  "赖" : 1,
  "赫" : 1,
  "赵" : 1,
  "趙" : 1,
  "路" : 1,
  "車" : 1,
  "车" : 1,
  "辛" : 1,
  "辜" : 1,
  "边" : 1,
  "连" : 1,
  "逄" : 1,
  "連" : 1,
  "逯" : 1,
  "邊" : 1,
  "邓" : 1,
  "邝" : 1,
  "邢" : 1,
  "邬" : 1,
  "邰" : 1,
  "邱" : 1,
  "邴" : 1,
  "邵" : 1,
  "邹" : 1,
  "郁" : 1,
  "郎" : 1,
  "郑" : 1,
  "郜" : 1,
  "郝" : 1,
  "郦" : 1,
  "郭" : 1,
  "鄒" : 1,
  "鄔" : 1,
  "鄞" : 1,
  "鄢" : 1,
  "鄧" : 1,
  "鄭" : 1,
  "鄺" : 1,
  "酆" : 1,
  "酈" : 1,
  "释" : 1,
  "釋" : 1,
  "金" : 1,
  "鈕" : 1,
  "錡" : 1,
  "錢" : 1,
  "鍾" : 1,
  "鐘" : 1,
  "钟" : 1,
  "钮" : 1,
  "钱" : 1,
  "锺" : 1,
  "閔" : 1,
  "閩" : 1,
  "閻" : 1,
  "闕" : 1,
  "關" : 1,
  "闞" : 1,
  "闵" : 1,
  "闻" : 1,
  "闽" : 1,
  "阎" : 1,
  "阙" : 1,
  "阚" : 1,
  "阮" : 1,
  "阴" : 1,
  "陆" : 1,
  "陈" : 1,
  "陰" : 1,
  "陳" : 1,
  "陶" : 1,
  "陸" : 1,
  "隆" : 1,
  "隋" : 1,
  "隗" : 1,
  "雷" : 1,
  "霍" : 1,
  "靳" : 1,
  "鞏" : 1,
  "鞠" : 1,
  "韋" : 1,
  "韓" : 1,
  "韦" : 1,
  "韩" : 1,
  "項" : 1,
  "顏" : 1,
  "顧" : 1,
  "项" : 1,
  "顾" : 1,
  "颜" : 1,
  "饒" : 1,
  "饶" : 1,
  "馬" : 1,
  "馮" : 1,
  "駱" : 1,
  "马" : 1,
  "骆" : 1,
  "高" : 1,
  "魏" : 1,
  "魯" : 1,
  "鮑" : 1,
  "鲁" : 1,
  "鲍" : 1,
  "麥" : 1,
  "麦" : 1,
  "麻" : 1,
  "黃" : 1,
  "黄" : 1,
  "黎" : 1,
  "黨" : 1,
  "齊" : 1,
  "齐" : 1,
  "龍" : 1,
  "龐" : 1,
  "龔" : 1,
  "龙" : 1,
  "龚" : 1,
  "上官" : 1,
  "令狐" : 1,
  "司徒" : 1,
  "司馬" : 1,
  "司马" : 1,
  "尉迟" : 1,
  "尉遲" : 1,
  "张简" : 1,
  "張簡" : 1,
  "欧阳" : 1,
  "歐陽" : 1,
  "淳于" : 1,
  "澹台" : 1,
  "澹臺" : 1,
  "皇甫" : 1,
  "端木" : 1,
  "范姜" : 1,
  "赫连" : 1,
  "赫連" : 1,
}


class EastAsianExtractor(extractor.Extractor):
  def FileExtract(self, filename):
    """Since spacing has no significance in East Asian Languages, names
    may cross word boundaries, so this must be defined
    differently. But empty lines should be kept, since they would
    indicate things like paragraph boundaries.
    """
    fp = open(filename, 'r')
    nlines = []
    for line in fp.readlines():
      line = line.strip()
      if line == '': line = ' ' ## Keep empty line
      nlines.append(line)
    text = ''.join(nlines)
    for line in text.split():
      self.LineSegment(line)
    return self.tokens_


class ChineseExtractor(EastAsianExtractor):
  """Chinese extractor for foreign transcriptions.
  """

  def LineSegment(self, line):
    try: utext = unicode(line.strip(), 'utf-8')
    except TypeError: utext = line.strip()
    word = []
    for u in utext:
      c = u.encode('utf-8')
      if c in FOREIGN_CHARS_:
        word.append(u)
      else:
        if word:
          if len(word) >= MIN_LEN_:
            self.tokens_.append(tokens.Token(''.join(word)))
          word = []


class ChinesePersonalNameExtractor(EastAsianExtractor):
  """Extractor for potential Chinese Personal names. Takes the
  conservative approach that insists there be one or two characters
  after the potential family name. This will of course massively
  overgenerate.

  In the comments below "F" is a single-character family name, "G" is
  a single-character given name, and "FF" and "GG" are two-character
  equivalents. 
  """

  def LineSegment(self, line):
    try: utext = unicode(line.strip(), 'utf-8')
    except TypeError: utext = line.strip()
    for i in range(len(utext)):
      for k in [4, 3, 2]:
        sub = utext[i:i+k]
        if len(sub) != k: continue
        if k > 2 and sub[:2].encode('utf-8') in FAMILY_NAMES_:
          if not (Utils.script.HasDigit(sub) or Utils.script.HasPunctuation(sub)):
            self.tokens_.append(tokens.Token(sub))
        elif k < 4 and sub[:1].encode('utf-8') in FAMILY_NAMES_:
          if not Utils.script.HasDigit(sub):
            self.tokens_.append(tokens.Token(sub))

########NEW FILE########
__FILENAME__ = def_pronouncers
# -*- coding: utf-8 -*-

## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

import pronouncer
import Utils.script

DEF_PRONOUNCERS_ = {
  "'gana_tuvalu" : [pronouncer.LatinPronouncer],
  'aa' : [pronouncer.LatinPronouncer],
  'aar' : [pronouncer.LatinPronouncer],
  'ab' : [pronouncer.UnitranPronouncer],
  'abk' : [pronouncer.UnitranPronouncer],
  'abkhazian' : [pronouncer.UnitranPronouncer],
  'ace' : [pronouncer.LatinPronouncer],
  'acehnese' : [pronouncer.LatinPronouncer],
  'ach' : [pronouncer.LatinPronouncer],
  'achinese' : [pronouncer.LatinPronouncer],
  'acoli' : [pronouncer.LatinPronouncer],
  'ada' : [pronouncer.LatinPronouncer],
  'adangbɛ' : [pronouncer.LatinPronouncer],
  'adangme' : [pronouncer.LatinPronouncer],
  'ady' : [pronouncer.UnitranPronouncer],
  'adygei' : [pronouncer.UnitranPronouncer],
  'adyghe' : [pronouncer.UnitranPronouncer],
  'ae' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'af' : [pronouncer.LatinPronouncer],
  'af_soomaali' : [pronouncer.LatinPronouncer],
  'afaan_oromoo' : [pronouncer.LatinPronouncer],
  'afar' : [pronouncer.LatinPronouncer],
  'afaraf' : [pronouncer.LatinPronouncer],
  'afh' : [pronouncer.LatinPronouncer],
  'afr' : [pronouncer.LatinPronouncer],
  'afrihili' : [pronouncer.LatinPronouncer],
  'afrikaans' : [pronouncer.LatinPronouncer],
  'ain' : [pronouncer.UnitranPronouncer],
  'ainu' : [pronouncer.UnitranPronouncer],
  'ainu_itak' : [pronouncer.UnitranPronouncer],
  'akk' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'akkadian' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'akkadû' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'alb' : [pronouncer.LatinPronouncer],
  'albanian' : [pronouncer.LatinPronouncer],
  'ale' : [pronouncer.LatinPronouncer],
  'alemannic' : [pronouncer.LatinPronouncer],
  'alemannisch' : [pronouncer.LatinPronouncer],
  'aleut' : [pronouncer.LatinPronouncer],
  'alt' : [pronouncer.UnitranPronouncer],
  'am' : [pronouncer.UnitranPronouncer],
  'amh' : [pronouncer.UnitranPronouncer],
  'amharic' : [pronouncer.UnitranPronouncer],
  'an' : [pronouncer.LatinPronouncer],
  'ancient_greek' : [pronouncer.UnitranPronouncer],
  'ang' : [pronouncer.LatinPronouncer],
  'angika' : [pronouncer.LatinPronouncer],
  'anishinaabemowin' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'anp' : [pronouncer.LatinPronouncer],
  'ar' : [pronouncer.UnitranPronouncer],
  'ara' : [pronouncer.UnitranPronouncer],
  'arabic' : [pronouncer.UnitranPronouncer],
  'aragonese' : [pronouncer.LatinPronouncer],
  'aragonés' : [pronouncer.LatinPronouncer],
  'aramaic' : [pronouncer.UnitranPronouncer],
  'arapaho' : [pronouncer.LatinPronouncer],
  'araucanian' : [pronouncer.LatinPronouncer],
  'arawak' : [pronouncer.LatinPronouncer],
  'arc' : [pronouncer.UnitranPronouncer],
  'arg' : [pronouncer.LatinPronouncer],
  'arm' : [pronouncer.UnitranPronouncer],
  'armenian' : [pronouncer.UnitranPronouncer],
  'armãneashce' : [pronouncer.LatinPronouncer],
  'armãneashti' : [pronouncer.LatinPronouncer],
  'arn' : [pronouncer.LatinPronouncer],
  'aromanian' : [pronouncer.LatinPronouncer],
  'arp' : [pronouncer.LatinPronouncer],
  'arumanian' : [pronouncer.LatinPronouncer],
  'arw' : [pronouncer.LatinPronouncer],
  'as' : [pronouncer.UnitranPronouncer],
  'asm' : [pronouncer.UnitranPronouncer],
  'assamese' : [pronouncer.UnitranPronouncer],
  'ast' : [pronouncer.LatinPronouncer],
  'asturian' : [pronouncer.LatinPronouncer],
  'asturianu' : [pronouncer.LatinPronouncer],
  'asturleonese' : [pronouncer.LatinPronouncer],
  'av' : [pronouncer.UnitranPronouncer],
  'ava' : [pronouncer.UnitranPronouncer],
  'avaric' : [pronouncer.UnitranPronouncer],
  "avañe'ẽ" : [pronouncer.LatinPronouncer],
  'ave' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'avesta' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'avestan' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'awa' : [pronouncer.UnitranPronouncer],
  'awadhi' : [pronouncer.UnitranPronouncer],
  'ay' : [pronouncer.LatinPronouncer],
  'aym' : [pronouncer.LatinPronouncer],
  'aymar_aru' : [pronouncer.LatinPronouncer],
  'aymara' : [pronouncer.LatinPronouncer],
  'az' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'aze' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'azerbaijani' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'azərbaycanca' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'ba' : [pronouncer.UnitranPronouncer],
  'bable' : [pronouncer.LatinPronouncer],
  'bahasa_indonesia' : [pronouncer.LatinPronouncer],
  'bahasa_melayu' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'bak' : [pronouncer.UnitranPronouncer],
  'bal' : [pronouncer.UnitranPronouncer],
  'balinese' : [pronouncer.LatinPronouncer],
  'baluchi' : [pronouncer.UnitranPronouncer],
  'bam' : [pronouncer.LatinPronouncer],
  'bamanankan' : [pronouncer.LatinPronouncer],
  'bambara' : [pronouncer.LatinPronouncer],
  'ban' : [pronouncer.LatinPronouncer],
  'baq' : [pronouncer.LatinPronouncer],
  'bas' : [pronouncer.LatinPronouncer],
  'basa' : [pronouncer.LatinPronouncer],
  'basa_acèh' : [pronouncer.LatinPronouncer],
  'basa_bali' : [pronouncer.LatinPronouncer],
  'basa_jawa' : [pronouncer.LatinPronouncer],
  'basa_sunda' : [pronouncer.LatinPronouncer],
  'bashkir' : [pronouncer.UnitranPronouncer],
  'baso_minangkabau' : [pronouncer.LatinPronouncer],
  'basque' : [pronouncer.LatinPronouncer],
  'be' : [pronouncer.UnitranPronouncer],
  'bedawiyet' : [pronouncer.UnitranPronouncer],
  'bej' : [pronouncer.UnitranPronouncer],
  'beja' : [pronouncer.UnitranPronouncer],
  'bel' : [pronouncer.UnitranPronouncer],
  'belarusian' : [pronouncer.UnitranPronouncer],
  'bem' : [pronouncer.LatinPronouncer],
  'bemba' : [pronouncer.LatinPronouncer],
  'ben' : [pronouncer.UnitranPronouncer],
  'bengali' : [pronouncer.UnitranPronouncer],
  'ber' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'berber' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'bg' : [pronouncer.UnitranPronouncer],
  'bh' : [pronouncer.UnitranPronouncer],
  'bho' : [pronouncer.UnitranPronouncer],
  'bhojpuri' : [pronouncer.UnitranPronouncer],
  'bhāṣa_kawi' : [pronouncer.LatinPronouncer],
  'bi' : [pronouncer.LatinPronouncer],
  'bih' : [pronouncer.UnitranPronouncer],
  'bihari' : [pronouncer.UnitranPronouncer],
  'bilin' : [pronouncer.LatinPronouncer],
  'bin' : [pronouncer.LatinPronouncer],
  'bini' : [pronouncer.LatinPronouncer],
  'bis' : [pronouncer.LatinPronouncer],
  'bislama' : [pronouncer.LatinPronouncer],
  'bla' : [pronouncer.UnitranPronouncer],
  'blin' : [pronouncer.LatinPronouncer],
  'bm' : [pronouncer.LatinPronouncer],
  'bn' : [pronouncer.UnitranPronouncer],
  'bo' : [pronouncer.UnitranPronouncer],
  'bod' : [pronouncer.UnitranPronouncer],
  'bos' : [pronouncer.LatinPronouncer],
  'bosanski_jezik' : [pronouncer.LatinPronouncer],
  'bosnian' : [pronouncer.LatinPronouncer],
  'br' : [pronouncer.LatinPronouncer],
  'bra' : [pronouncer.UnitranPronouncer],
  'braj' : [pronouncer.UnitranPronouncer],
  'bre' : [pronouncer.LatinPronouncer],
  'breton' : [pronouncer.LatinPronouncer],
  'brezhoneg' : [pronouncer.LatinPronouncer],
  'bs' : [pronouncer.LatinPronouncer],
  'bua' : [pronouncer.UnitranPronouncer],
  'bug' : [pronouncer.UnitranPronouncer],
  'buginese' : [pronouncer.UnitranPronouncer],
  'bul' : [pronouncer.UnitranPronouncer],
  'bulgarian' : [pronouncer.UnitranPronouncer],
  'bur' : [pronouncer.UnitranPronouncer],
  'buriat' : [pronouncer.UnitranPronouncer],
  'burmese' : [pronouncer.UnitranPronouncer],
  'byn' : [pronouncer.LatinPronouncer],
  'ca' : [pronouncer.LatinPronouncer],
  'cad' : [pronouncer.LatinPronouncer],
  'caddo' : [pronouncer.LatinPronouncer],
  'castellano' : [pronouncer.LatinPronouncer],
  'castilian' : [pronouncer.LatinPronouncer],
  'cat' : [pronouncer.LatinPronouncer],
  'catalan' : [pronouncer.LatinPronouncer],
  'català' : [pronouncer.LatinPronouncer],
  'ce' : [pronouncer.UnitranPronouncer],
  'ceb' : [pronouncer.LatinPronouncer],
  'cebuano' : [pronouncer.LatinPronouncer],
  'central_khmer' : [pronouncer.UnitranPronouncer],
  'ces' : [pronouncer.LatinPronouncer],
  'ch' : [pronouncer.LatinPronouncer],
  'cha' : [pronouncer.LatinPronouncer],
  'chagatai' : [pronouncer.UnitranPronouncer],
  'chahta_anumpa' : [pronouncer.LatinPronouncer],
  'chamorro' : [pronouncer.LatinPronouncer],
  'chamoru' : [pronouncer.LatinPronouncer],
  'chb' : [pronouncer.LatinPronouncer],
  'che' : [pronouncer.UnitranPronouncer],
  'chechen' : [pronouncer.UnitranPronouncer],
  'cherokee' : [pronouncer.UnitranPronouncer],
  'chewa' : [pronouncer.LatinPronouncer],
  'cheyenne' : [pronouncer.LatinPronouncer],
  'chg' : [pronouncer.UnitranPronouncer],
  'chi' : [pronouncer.UnitranPronouncer],
  'chibcha' : [pronouncer.LatinPronouncer],
  'chichewa' : [pronouncer.LatinPronouncer],
  'chicheŵa' : [pronouncer.LatinPronouncer],
  'chilunda' : [pronouncer.LatinPronouncer],
  'chinese' : [pronouncer.HanziPronouncer],
  'chinook_jargon' : [pronouncer.LatinPronouncer],
  'chinyanja' : [pronouncer.LatinPronouncer],
  'chishona' : [pronouncer.LatinPronouncer],
  'chitonga' : [pronouncer.LatinPronouncer],
  'chitumbuka' : [pronouncer.LatinPronouncer],
  'chiyao' : [pronouncer.LatinPronouncer],
  'chk' : [pronouncer.LatinPronouncer],
  'chm' : [pronouncer.UnitranPronouncer],
  'chn' : [pronouncer.LatinPronouncer],
  'cho' : [pronouncer.LatinPronouncer],
  'choctaw' : [pronouncer.LatinPronouncer],
  'chp' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'chr' : [pronouncer.UnitranPronouncer],
  'chu' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'chuang' : [pronouncer.LatinPronouncer],
  'church_slavic' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'church_slavonic' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'chuukese' : [pronouncer.LatinPronouncer],
  'chuvash' : [pronouncer.UnitranPronouncer],
  'chv' : [pronouncer.UnitranPronouncer],
  'chy' : [pronouncer.LatinPronouncer],
  'classical_nepal_bhasa' : [pronouncer.UnitranPronouncer],
  'classical_newari' : [pronouncer.UnitranPronouncer],
  'classical_syriac' : [pronouncer.LatinPronouncer],
  'co' : [pronouncer.LatinPronouncer],
  'cook_islands_maori' : [pronouncer.LatinPronouncer],
  'cop' : [pronouncer.UnitranPronouncer],
  'coptic' : [pronouncer.UnitranPronouncer],
  'cor' : [pronouncer.LatinPronouncer],
  'cornish' : [pronouncer.LatinPronouncer],
  'corsican' : [pronouncer.LatinPronouncer],
  'corsu' : [pronouncer.LatinPronouncer],
  'cos' : [pronouncer.LatinPronouncer],
  'cr' : [pronouncer.UnitranPronouncer],
  'cre' : [pronouncer.UnitranPronouncer],
  'cree' : [pronouncer.UnitranPronouncer],
  'creek' : [pronouncer.LatinPronouncer],
  'crh' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'crimean_tatar' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'crimean_turkish' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'croatian' : [pronouncer.LatinPronouncer],
  'cs' : [pronouncer.LatinPronouncer],
  'csb' : [pronouncer.LatinPronouncer],
  'cu' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'cv' : [pronouncer.UnitranPronouncer],
  'cy' : [pronouncer.LatinPronouncer],
  'cym' : [pronouncer.LatinPronouncer],
  'cymraeg' : [pronouncer.LatinPronouncer],
  'cze' : [pronouncer.LatinPronouncer],
  'czech' : [pronouncer.LatinPronouncer],
  'da' : [pronouncer.LatinPronouncer],
  'dak' : [pronouncer.LatinPronouncer],
  'dakota' : [pronouncer.LatinPronouncer],
  'dan' : [pronouncer.LatinPronouncer],
  'danish' : [pronouncer.LatinPronouncer],
  'dansk' : [pronouncer.LatinPronouncer],
  'dar' : [pronouncer.UnitranPronouncer],
  'dargwa' : [pronouncer.UnitranPronouncer],
  'de' : [pronouncer.LatinPronouncer],
  'del' : [pronouncer.LatinPronouncer],
  'delaware' : [pronouncer.LatinPronouncer],
  'den' : [pronouncer.LatinPronouncer],
  'dene_suline' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'deu' : [pronouncer.LatinPronouncer],
  'deutsch' : [pronouncer.LatinPronouncer],
  'dgr' : [pronouncer.LatinPronouncer],
  'dhivehi' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'dholuo' : [pronouncer.LatinPronouncer],
  'dimili' : [pronouncer.LatinPronouncer],
  'dimli' : [pronouncer.LatinPronouncer],
  'din' : [pronouncer.LatinPronouncer],
  'dinka' : [pronouncer.LatinPronouncer],
  'diné_bizaad' : [pronouncer.LatinPronouncer],
  'dinékʼehǰí' : [pronouncer.LatinPronouncer],
  'diutisc' : [pronouncer.LatinPronouncer],
  'diutisk' : [pronouncer.LatinPronouncer],
  'div' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'divehi' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'dogri' : [pronouncer.UnitranPronouncer],
  'dogrib' : [pronouncer.LatinPronouncer],
  'doi' : [pronouncer.UnitranPronouncer],
  'dolnoserbski' : [pronouncer.LatinPronouncer],
  'dsb' : [pronouncer.LatinPronouncer],
  'dua' : [pronouncer.LatinPronouncer],
  'duala' : [pronouncer.LatinPronouncer],
  'dum' : [pronouncer.LatinPronouncer],
  'dut' : [pronouncer.LatinPronouncer],
  'dutch' : [pronouncer.LatinPronouncer],
  'dv' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'dyu' : [pronouncer.LatinPronouncer],
  'dyula' : [pronouncer.LatinPronouncer],
  'dz' : [pronouncer.UnitranPronouncer],
  'dzo' : [pronouncer.UnitranPronouncer],
  'dzongkha' : [pronouncer.UnitranPronouncer],
  'dëne_sųłiné' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'edo' : [pronouncer.LatinPronouncer],
  'ee' : [pronouncer.LatinPronouncer],
  'eesti_keel' : [pronouncer.LatinPronouncer],
  'efi' : [pronouncer.LatinPronouncer],
  'efik' : [pronouncer.LatinPronouncer],
  'egy' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'egyptian' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'eka' : [pronouncer.LatinPronouncer],
  'ekajuk' : [pronouncer.LatinPronouncer],
  'ekakairũ_naoero' : [pronouncer.LatinPronouncer],
  'el' : [pronouncer.UnitranPronouncer],
  'elamite' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'ell' : [pronouncer.UnitranPronouncer],
  'elx' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'eme-ĝir' : [pronouncer.LatinPronouncer],
  'en' : [pronouncer.EnglishPronouncer, pronouncer.LatinPronouncer],
  'eng' : [pronouncer.EnglishPronouncer, pronouncer.LatinPronouncer],
  'englisc' : [pronouncer.LatinPronouncer],
  'english' : [pronouncer.EnglishPronouncer, pronouncer.LatinPronouncer],
  'enm' : [pronouncer.LatinPronouncer],
  'eo' : [pronouncer.LatinPronouncer],
  'epo' : [pronouncer.LatinPronouncer],
  'erzya' : [pronouncer.UnitranPronouncer],
  'es' : [pronouncer.LatinPronouncer],
  'español' : [pronouncer.LatinPronouncer],
  'esperanto' : [pronouncer.LatinPronouncer],
  'est' : [pronouncer.LatinPronouncer],
  'estonian' : [pronouncer.LatinPronouncer],
  'et' : [pronouncer.LatinPronouncer],
  'eu' : [pronouncer.LatinPronouncer],
  'eus' : [pronouncer.LatinPronouncer],
  'euskara' : [pronouncer.LatinPronouncer],
  'ewe' : [pronouncer.LatinPronouncer],
  'ewo' : [pronouncer.LatinPronouncer],
  'ewondo' : [pronouncer.LatinPronouncer],
  'fa' : [pronouncer.UnitranPronouncer],
  'faka-niue' : [pronouncer.LatinPronouncer],
  'faka-tonga' : [pronouncer.LatinPronouncer],
  'fan' : [pronouncer.LatinPronouncer],
  'fang' : [pronouncer.LatinPronouncer],
  'fanti' : [pronouncer.LatinPronouncer],
  'fao' : [pronouncer.LatinPronouncer],
  'faroese' : [pronouncer.LatinPronouncer],
  'fas' : [pronouncer.UnitranPronouncer],
  'fat' : [pronouncer.LatinPronouncer],
  'ff' : [pronouncer.LatinPronouncer],
  'fi' : [pronouncer.LatinPronouncer],
  'fij' : [pronouncer.LatinPronouncer],
  'fijian' : [pronouncer.LatinPronouncer],
  'fil' : [pronouncer.LatinPronouncer],
  'filipino' : [pronouncer.LatinPronouncer],
  'fin' : [pronouncer.LatinPronouncer],
  'finnish' : [pronouncer.LatinPronouncer],
  'fj' : [pronouncer.LatinPronouncer],
  'flemish' : [pronouncer.LatinPronouncer],
  'fo' : [pronouncer.LatinPronouncer],
  'fon' : [pronouncer.LatinPronouncer],
  'fr' : [pronouncer.LatinPronouncer],
  'fra' : [pronouncer.LatinPronouncer],
  'français' : [pronouncer.LatinPronouncer],
  'fre' : [pronouncer.LatinPronouncer],
  'french' : [pronouncer.LatinPronouncer],
  'friulian' : [pronouncer.LatinPronouncer],
  'frm' : [pronouncer.LatinPronouncer],
  'fro' : [pronouncer.LatinPronouncer],
  'frs' : [pronouncer.LatinPronouncer],
  'frysk' : [pronouncer.LatinPronouncer],
  'fräisk' : [pronouncer.LatinPronouncer],
  'ful' : [pronouncer.LatinPronouncer],
  'fulah' : [pronouncer.LatinPronouncer],
  'fulfulde' : [pronouncer.LatinPronouncer],
  'fur' : [pronouncer.LatinPronouncer],
  'furlan' : [pronouncer.LatinPronouncer],
  'føroyskt' : [pronouncer.LatinPronouncer],
  'fɔngbe' : [pronouncer.LatinPronouncer],
  'ga' : [pronouncer.LatinPronouncer],
  'gaa' : [pronouncer.LatinPronouncer],
  'gaeilge' : [pronouncer.LatinPronouncer],
  'gaelg' : [pronouncer.LatinPronouncer],
  'gaelic' : [pronouncer.LatinPronouncer],
  "gagana_fa'a_samoa" : [pronouncer.LatinPronouncer],
  'galego' : [pronouncer.LatinPronouncer],
  'galician' : [pronouncer.LatinPronouncer],
  'ganda' : [pronouncer.LatinPronouncer],
  'gaoidhealg' : [pronouncer.LatinPronouncer],
  'gay' : [pronouncer.LatinPronouncer],
  'gayo' : [pronouncer.LatinPronouncer],
  'gba' : [pronouncer.LatinPronouncer],
  'gbaya' : [pronouncer.LatinPronouncer],
  'gd' : [pronouncer.LatinPronouncer],
  "ge'ez" : [pronouncer.UnitranPronouncer],
  'geo' : [pronouncer.UnitranPronouncer],
  'georgian' : [pronouncer.UnitranPronouncer],
  'ger' : [pronouncer.LatinPronouncer],
  'german' : [pronouncer.LatinPronouncer],
  'gez' : [pronouncer.UnitranPronouncer],
  'gikuyu' : [pronouncer.LatinPronouncer],
  'gil' : [pronouncer.LatinPronouncer],
  'gilbertese' : [pronouncer.LatinPronouncer],
  'gl' : [pronouncer.LatinPronouncer],
  'gla' : [pronouncer.LatinPronouncer],
  'gle' : [pronouncer.LatinPronouncer],
  'glg' : [pronouncer.LatinPronouncer],
  'glv' : [pronouncer.LatinPronouncer],
  'gmh' : [pronouncer.LatinPronouncer],
  'gn' : [pronouncer.LatinPronouncer],
  'goh' : [pronouncer.LatinPronouncer],
  'gon' : [pronouncer.LatinPronouncer],
  'gondi' : [pronouncer.LatinPronouncer],
  'gor' : [pronouncer.LatinPronouncer],
  'gorontalo' : [pronouncer.LatinPronouncer],
  'got' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'gothic' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'goídelc' : [pronouncer.LatinPronouncer],
  'grb' : [pronouncer.LatinPronouncer],
  'grc' : [pronouncer.UnitranPronouncer],
  'gre' : [pronouncer.UnitranPronouncer],
  'grebo' : [pronouncer.LatinPronouncer],
  'greek' : [pronouncer.UnitranPronouncer],
  'greenlandic' : [pronouncer.LatinPronouncer],
  'grn' : [pronouncer.LatinPronouncer],
  'gsw' : [pronouncer.LatinPronouncer],
  'gu' : [pronouncer.UnitranPronouncer],
  'guarani' : [pronouncer.LatinPronouncer],
  'guj' : [pronouncer.UnitranPronouncer],
  'gujarati' : [pronouncer.UnitranPronouncer],
  'gv' : [pronouncer.LatinPronouncer],
  'gwi' : [pronouncer.LatinPronouncer],
  'gwichʼin' : [pronouncer.LatinPronouncer],
  'gàidhlig' : [pronouncer.LatinPronouncer],
  'gã' : [pronouncer.LatinPronouncer],
  'gĩkũyũ' : [pronouncer.LatinPronouncer],
  'gōndi' : [pronouncer.LatinPronouncer],
  'ha' : [pronouncer.LatinPronouncer],
  'hai' : [pronouncer.LatinPronouncer],
  'haida' : [pronouncer.LatinPronouncer],
  'haitian' : [pronouncer.LatinPronouncer],
  'haitian_creole' : [pronouncer.LatinPronouncer],
  'hasí:nay' : [pronouncer.LatinPronouncer],
  'hat' : [pronouncer.LatinPronouncer],
  'hau' : [pronouncer.LatinPronouncer],
  'hausa' : [pronouncer.LatinPronouncer],
  'hausancī' : [pronouncer.LatinPronouncer],
  'haw' : [pronouncer.LatinPronouncer],
  'hawaiian' : [pronouncer.LatinPronouncer],
  'he' : [pronouncer.UnitranPronouncer],
  'heb' : [pronouncer.UnitranPronouncer],
  'hebrew' : [pronouncer.UnitranPronouncer],
  'her' : [pronouncer.LatinPronouncer],
  'herero' : [pronouncer.LatinPronouncer],
  'hi' : [pronouncer.UnitranPronouncer],
  'hil' : [pronouncer.LatinPronouncer],
  'hiligaynon' : [pronouncer.LatinPronouncer],
  'him' : [pronouncer.LatinPronouncer],
  'himachali' : [pronouncer.LatinPronouncer],
  'hin' : [pronouncer.UnitranPronouncer],
  'hindi' : [pronouncer.UnitranPronouncer],
  "hinóno'eitíít" : [pronouncer.LatinPronouncer],
  'hiri_motu' : [pronouncer.LatinPronouncer],
  'hit' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'hittite' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'hmn' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'hmo' : [pronouncer.LatinPronouncer],
  'hmong' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'hmoob' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'ho' : [pronouncer.LatinPronouncer],
  'hornjoserbsce' : [pronouncer.LatinPronouncer],
  'hr' : [pronouncer.LatinPronouncer],
  'hrv' : [pronouncer.LatinPronouncer],
  'hrvatski_jezik' : [pronouncer.LatinPronouncer],
  'hsb' : [pronouncer.LatinPronouncer],
  'ht' : [pronouncer.LatinPronouncer],
  'hu' : [pronouncer.LatinPronouncer],
  'hun' : [pronouncer.LatinPronouncer],
  'hungarian' : [pronouncer.LatinPronouncer],
  'hup' : [pronouncer.LatinPronouncer],
  'hupa' : [pronouncer.LatinPronouncer],
  'hy' : [pronouncer.UnitranPronouncer],
  'hye' : [pronouncer.UnitranPronouncer],
  'hz' : [pronouncer.LatinPronouncer],
  'iba' : [pronouncer.LatinPronouncer],
  'iban' : [pronouncer.LatinPronouncer],
  'ibo' : [pronouncer.LatinPronouncer],
  'ice' : [pronouncer.LatinPronouncer],
  'icelandic' : [pronouncer.LatinPronouncer],
  'ichibemba' : [pronouncer.LatinPronouncer],
  'id' : [pronouncer.LatinPronouncer],
  'ido' : [pronouncer.LatinPronouncer],
  'ie' : [pronouncer.LatinPronouncer],
  'ig' : [pronouncer.LatinPronouncer],
  'igbo' : [pronouncer.LatinPronouncer],
  'ii' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'iii' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'ik' : [pronouncer.LatinPronouncer],
  'iku' : [pronouncer.UnitranPronouncer],
  'ile' : [pronouncer.LatinPronouncer],
  'ilo' : [pronouncer.LatinPronouncer],
  'iloko' : [pronouncer.LatinPronouncer],
  'ilonggo' : [pronouncer.LatinPronouncer],
  'inari_sami' : [pronouncer.LatinPronouncer],
  'ind' : [pronouncer.LatinPronouncer],
  'indonesian' : [pronouncer.LatinPronouncer],
  'ingush' : [pronouncer.UnitranPronouncer],
  'inh' : [pronouncer.UnitranPronouncer],
  'interlingue' : [pronouncer.LatinPronouncer],
  'inuktitut' : [pronouncer.UnitranPronouncer],
  'inupiaq' : [pronouncer.LatinPronouncer],
  'io' : [pronouncer.LatinPronouncer],
  'ipk' : [pronouncer.LatinPronouncer],
  'irish' : [pronouncer.LatinPronouncer],
  'is' : [pronouncer.LatinPronouncer],
  'isindebele' : [pronouncer.LatinPronouncer],
  'isixhosa' : [pronouncer.LatinPronouncer],
  'isizulu' : [pronouncer.LatinPronouncer],
  'isl' : [pronouncer.LatinPronouncer],
  'it' : [pronouncer.LatinPronouncer],
  'ita' : [pronouncer.LatinPronouncer],
  'italian' : [pronouncer.LatinPronouncer],
  'italiano' : [pronouncer.LatinPronouncer],
  'iu' : [pronouncer.UnitranPronouncer],
  'iñupiaq' : [pronouncer.LatinPronouncer],
  'iñupiatun' : [pronouncer.LatinPronouncer],
  'ja' : [pronouncer.UnitranPronouncer],
  'japanese' : [pronouncer.UnitranPronouncer, pronouncer.HanziPronouncer],
  'jav' : [pronouncer.LatinPronouncer],
  'javanese' : [pronouncer.LatinPronouncer],
  'jbo' : [pronouncer.LatinPronouncer],
  'jingpho' : [pronouncer.LatinPronouncer],
  'jpn' : [pronouncer.UnitranPronouncer, pronouncer.HanziPronouncer],
  'jpr' : [pronouncer.UnitranPronouncer],
  'jrb' : [pronouncer.UnitranPronouncer],
  'judeo-arabic' : [pronouncer.UnitranPronouncer],
  'judeo-persian' : [pronouncer.UnitranPronouncer],
  'julakan' : [pronouncer.LatinPronouncer],
  'jv' : [pronouncer.LatinPronouncer],
  'ka' : [pronouncer.UnitranPronouncer],
  'kaa' : [pronouncer.UnitranPronouncer],
  'kab' : [pronouncer.LatinPronouncer],
  'kabardian' : [pronouncer.UnitranPronouncer],
  'kabyle' : [pronouncer.LatinPronouncer],
  'kac' : [pronouncer.LatinPronouncer],
  'kachin' : [pronouncer.LatinPronouncer],
  'kajin_m̧ajeļ' : [pronouncer.LatinPronouncer],
  'kal' : [pronouncer.LatinPronouncer],
  'kalaallisut' : [pronouncer.LatinPronouncer],
  'kalaallit_oqaasii' : [pronouncer.LatinPronouncer],
  'kalmyk' : [pronouncer.UnitranPronouncer],
  'kam' : [pronouncer.LatinPronouncer],
  'kamba' : [pronouncer.LatinPronouncer],
  'kan' : [pronouncer.UnitranPronouncer],
  'kanien’keha' : [pronouncer.LatinPronouncer],
  'kannada' : [pronouncer.UnitranPronouncer],
  'kanuri' : [pronouncer.LatinPronouncer],
  'kapampangan' : [pronouncer.LatinPronouncer],
  'kara-kalpak' : [pronouncer.UnitranPronouncer],
  'karachay-balkar' : [pronouncer.UnitranPronouncer],
  'karelian' : [pronouncer.LatinPronouncer],
  'karjalan_kieli' : [pronouncer.LatinPronouncer],
  'kas' : [pronouncer.UnitranPronouncer],
  'kashmiri' : [pronouncer.UnitranPronouncer],
  'kashubian' : [pronouncer.LatinPronouncer],
  'kaszëbsczi_jãzëk' : [pronouncer.LatinPronouncer],
  'kat' : [pronouncer.UnitranPronouncer],
  'kau' : [pronouncer.LatinPronouncer],
  'kaw' : [pronouncer.LatinPronouncer],
  'kawi' : [pronouncer.LatinPronouncer],
  'kaz' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'kazakh' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'kbd' : [pronouncer.UnitranPronouncer],
  'kernewek' : [pronouncer.LatinPronouncer],
  'kg' : [pronouncer.LatinPronouncer],
  'kha' : [pronouncer.UnitranPronouncer],
  'khasa' : [pronouncer.LatinPronouncer],
  'khasi' : [pronouncer.UnitranPronouncer],
  'khm' : [pronouncer.UnitranPronouncer],
  'kho' : [pronouncer.LatinPronouncer],
  'khotanese' : [pronouncer.LatinPronouncer],
  'ki' : [pronouncer.LatinPronouncer],
  'kichwa' : [pronouncer.LatinPronouncer],
  'kik' : [pronouncer.LatinPronouncer],
  'kikongo' : [pronouncer.LatinPronouncer],
  'kikuyu' : [pronouncer.LatinPronouncer],
  'kimbundu' : [pronouncer.LatinPronouncer],
  'kin' : [pronouncer.LatinPronouncer],
  'kinyarwanda' : [pronouncer.LatinPronouncer],
  'kir' : [pronouncer.UnitranPronouncer],
  'kirdki' : [pronouncer.LatinPronouncer],
  'kirghiz' : [pronouncer.UnitranPronouncer],
  'kiribati' : [pronouncer.LatinPronouncer],
  'kirmanjki' : [pronouncer.LatinPronouncer],
  'kirundi' : [pronouncer.LatinPronouncer],
  'kiswahili' : [pronouncer.LatinPronouncer],
  'kj' : [pronouncer.LatinPronouncer],
  'kk' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'kl' : [pronouncer.LatinPronouncer],
  'klingon' : [pronouncer.LatinPronouncer],
  'km' : [pronouncer.UnitranPronouncer],
  'kmb' : [pronouncer.LatinPronouncer],
  'kn' : [pronouncer.UnitranPronouncer],
  'ko' : [pronouncer.UnitranPronouncer],
  'ko_e_vagahau_niuē' : [pronouncer.LatinPronouncer],
  'kok' : [pronouncer.UnitranPronouncer],
  'kom' : [pronouncer.UnitranPronouncer],
  'komi' : [pronouncer.UnitranPronouncer],
  'kon' : [pronouncer.LatinPronouncer],
  'kongo' : [pronouncer.LatinPronouncer],
  'konkani' : [pronouncer.UnitranPronouncer],
  'kor' : [pronouncer.UnitranPronouncer],
  'korean' : [pronouncer.UnitranPronouncer],
  'kos' : [pronouncer.LatinPronouncer],
  'kosrae' : [pronouncer.LatinPronouncer],
  'kosraean' : [pronouncer.LatinPronouncer],
  'kpe' : [pronouncer.LatinPronouncer],
  'kpele' : [pronouncer.LatinPronouncer],
  'kpelle' : [pronouncer.LatinPronouncer],
  'kr' : [pronouncer.LatinPronouncer],
  'krc' : [pronouncer.UnitranPronouncer],
  'kreyòl_ayisyen' : [pronouncer.LatinPronouncer],
  'krl' : [pronouncer.LatinPronouncer],
  'kru' : [pronouncer.LatinPronouncer],
  'ks' : [pronouncer.UnitranPronouncer],
  'ktunaxa' : [pronouncer.LatinPronouncer],
  'ku' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'kua' : [pronouncer.LatinPronouncer],
  'kuanyama' : [pronouncer.LatinPronouncer],
  'kum' : [pronouncer.UnitranPronouncer],
  'kumyk' : [pronouncer.UnitranPronouncer],
  'kur' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'kurdish' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'kurdî' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'kurukh' : [pronouncer.LatinPronouncer],
  'kut' : [pronouncer.LatinPronouncer],
  'kutenai' : [pronouncer.LatinPronouncer],
  'kv' : [pronouncer.UnitranPronouncer],
  'kw' : [pronouncer.LatinPronouncer],
  'kwanyama' : [pronouncer.LatinPronouncer],
  'ky' : [pronouncer.UnitranPronouncer],
  'kyrgyz' : [pronouncer.UnitranPronouncer],
  'la' : [pronouncer.LatinPronouncer],
  'lad' : [pronouncer.UnitranPronouncer],
  'ladino' : [pronouncer.UnitranPronouncer],
  'lah' : [pronouncer.UnitranPronouncer],
  'lahnda' : [pronouncer.UnitranPronouncer],
  'lakhota' : [pronouncer.LatinPronouncer],
  'lallans' : [pronouncer.LatinPronouncer],
  'lam' : [pronouncer.LatinPronouncer],
  'lamba' : [pronouncer.LatinPronouncer],
  'langue_française' : [pronouncer.LatinPronouncer],
  'lao' : [pronouncer.UnitranPronouncer],
  'lat' : [pronouncer.LatinPronouncer],
  'latine' : [pronouncer.LatinPronouncer],
  'latvian' : [pronouncer.LatinPronouncer],
  'latviešu_valoda' : [pronouncer.LatinPronouncer],
  'lav' : [pronouncer.LatinPronouncer],
  'lb' : [pronouncer.LatinPronouncer],
  'leonese' : [pronouncer.LatinPronouncer],
  'letzeburgesch' : [pronouncer.LatinPronouncer],
  'lez' : [pronouncer.UnitranPronouncer],
  'lezghian' : [pronouncer.UnitranPronouncer],
  'lg' : [pronouncer.LatinPronouncer],
  'lhéngua_mirandesa' : [pronouncer.LatinPronouncer],
  'li' : [pronouncer.LatinPronouncer],
  'lia-tetun' : [pronouncer.LatinPronouncer],
  'lietuvių_kalba' : [pronouncer.LatinPronouncer],
  'lim' : [pronouncer.LatinPronouncer],
  'limba_armãneascã' : [pronouncer.LatinPronouncer],
  'limburgan' : [pronouncer.LatinPronouncer],
  'limburger' : [pronouncer.LatinPronouncer],
  'limburgish' : [pronouncer.LatinPronouncer],
  'limburgs' : [pronouncer.LatinPronouncer],
  'lin' : [pronouncer.LatinPronouncer],
  'lineyte-samarnon' : [pronouncer.LatinPronouncer],
  'lingala' : [pronouncer.LatinPronouncer],
  'lingua_corsa' : [pronouncer.LatinPronouncer],
  'lingua_latina' : [pronouncer.LatinPronouncer],
  'lingít' : [pronouncer.LatinPronouncer],
  'lit' : [pronouncer.LatinPronouncer],
  'lithuanian' : [pronouncer.LatinPronouncer],
  'lišānum_akkadītum' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'ln' : [pronouncer.LatinPronouncer],
  'lo' : [pronouncer.UnitranPronouncer],
  'lojban' : [pronouncer.LatinPronouncer],
  'lol' : [pronouncer.LatinPronouncer],
  'low_german' : [pronouncer.LatinPronouncer],
  'low_saxon' : [pronouncer.LatinPronouncer],
  'lower_sorbian' : [pronouncer.LatinPronouncer],
  'loz' : [pronouncer.LatinPronouncer],
  'lozi' : [pronouncer.LatinPronouncer],
  'lt' : [pronouncer.LatinPronouncer],
  'ltz' : [pronouncer.LatinPronouncer],
  'lu' : [pronouncer.LatinPronouncer],
  'lua' : [pronouncer.LatinPronouncer],
  'lub' : [pronouncer.LatinPronouncer],
  'luba-katanga' : [pronouncer.LatinPronouncer],
  'luba-lulua' : [pronouncer.LatinPronouncer],
  'lug' : [pronouncer.LatinPronouncer],
  'luganda' : [pronouncer.LatinPronouncer],
  'lui' : [pronouncer.LatinPronouncer],
  'luiseño' : [pronouncer.LatinPronouncer],
  'lule_sami' : [pronouncer.LatinPronouncer],
  'lun' : [pronouncer.LatinPronouncer],
  'lunda' : [pronouncer.LatinPronouncer],
  'luo' : [pronouncer.LatinPronouncer],
  'lus' : [pronouncer.LatinPronouncer],
  'lushai' : [pronouncer.LatinPronouncer],
  'luxembourgish' : [pronouncer.LatinPronouncer],
  'lv' : [pronouncer.LatinPronouncer],
  'lwaà:' : [pronouncer.LatinPronouncer],
  'lënape' : [pronouncer.LatinPronouncer],
  'lëtzebuergesch' : [pronouncer.LatinPronouncer],
  'mac' : [pronouncer.UnitranPronouncer],
  'macedo-romanian' : [pronouncer.LatinPronouncer],
  'macedonian' : [pronouncer.UnitranPronouncer],
  'mad' : [pronouncer.LatinPronouncer],
  'madurese' : [pronouncer.LatinPronouncer],
  'mag' : [pronouncer.LatinPronouncer],
  'magahi' : [pronouncer.LatinPronouncer],
  'magyar' : [pronouncer.LatinPronouncer],
  'mah' : [pronouncer.LatinPronouncer],
  'mai' : [pronouncer.UnitranPronouncer],
  'maithili' : [pronouncer.UnitranPronouncer],
  'mak' : [pronouncer.LatinPronouncer],
  'makasar' : [pronouncer.LatinPronouncer],
  'mal' : [pronouncer.UnitranPronouncer],
  'malagasy' : [pronouncer.LatinPronouncer],
  'malagasy_fiteny' : [pronouncer.LatinPronouncer],
  'malay' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'malayalam' : [pronouncer.UnitranPronouncer],
  'maldivian' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'maltese' : [pronouncer.LatinPronouncer],
  'malti' : [pronouncer.LatinPronouncer],
  'man' : [pronouncer.LatinPronouncer],
  'manchu' : [pronouncer.UnitranPronouncer],
  'mandar' : [pronouncer.LatinPronouncer],
  'mandingo' : [pronouncer.LatinPronouncer],
  'manipuri' : [pronouncer.UnitranPronouncer],
  'manninagh' : [pronouncer.LatinPronouncer],
  'manx' : [pronouncer.LatinPronouncer],
  'mao' : [pronouncer.LatinPronouncer],
  'mapuche' : [pronouncer.LatinPronouncer],
  'mapuchedungun' : [pronouncer.LatinPronouncer],
  'mapudungun' : [pronouncer.LatinPronouncer],
  'mar' : [pronouncer.UnitranPronouncer],
  'marathi' : [pronouncer.UnitranPronouncer],
  'mari' : [pronouncer.UnitranPronouncer],
  'marip' : [pronouncer.LatinPronouncer],
  'marshallese' : [pronouncer.LatinPronouncer],
  'marwari' : [pronouncer.UnitranPronouncer],
  'mas' : [pronouncer.LatinPronouncer],
  'masai' : [pronouncer.LatinPronouncer],
  'maskoki' : [pronouncer.LatinPronouncer],
  'may' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'mdf' : [pronouncer.UnitranPronouncer],
  'mdr' : [pronouncer.LatinPronouncer],
  'men' : [pronouncer.LatinPronouncer],
  'mende' : [pronouncer.LatinPronouncer],
  'mg' : [pronouncer.LatinPronouncer],
  'mga' : [pronouncer.LatinPronouncer],
  'mh' : [pronouncer.LatinPronouncer],
  'mi' : [pronouncer.LatinPronouncer],
  "mi'gmaq" : [pronouncer.LatinPronouncer],
  "mi'kmaq" : [pronouncer.LatinPronouncer],
  'mic' : [pronouncer.LatinPronouncer],
  'micmac' : [pronouncer.LatinPronouncer],
  'middle_dutch' : [pronouncer.LatinPronouncer],
  'middle_english' : [pronouncer.LatinPronouncer],
  'middle_french' : [pronouncer.LatinPronouncer],
  'middle_high_german' : [pronouncer.LatinPronouncer],
  'middle_irish' : [pronouncer.LatinPronouncer],
  'min' : [pronouncer.LatinPronouncer],
  'minangkabau' : [pronouncer.LatinPronouncer],
  'mirandese' : [pronouncer.LatinPronouncer],
  'mk' : [pronouncer.UnitranPronouncer],
  'mkd' : [pronouncer.UnitranPronouncer],
  'ml' : [pronouncer.UnitranPronouncer],
  'mlg' : [pronouncer.LatinPronouncer],
  'mlt' : [pronouncer.LatinPronouncer],
  'mn' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'mnc' : [pronouncer.UnitranPronouncer],
  'mni' : [pronouncer.UnitranPronouncer],
  'mo' : [pronouncer.UnitranPronouncer],
  'modern_greek' : [pronouncer.UnitranPronouncer],
  'moh' : [pronouncer.LatinPronouncer],
  'mohawk' : [pronouncer.LatinPronouncer],
  'moksha' : [pronouncer.UnitranPronouncer],
  'mol' : [pronouncer.UnitranPronouncer],
  'moldavian' : [pronouncer.UnitranPronouncer],
  'moldovan' : [pronouncer.UnitranPronouncer],
  'mon' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'mongo' : [pronouncer.LatinPronouncer],
  'mongolian' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'mos' : [pronouncer.LatinPronouncer],
  'mossi' : [pronouncer.LatinPronouncer],
  'mr' : [pronouncer.UnitranPronouncer],
  'mri' : [pronouncer.LatinPronouncer],
  'ms' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'msa' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'mt' : [pronouncer.LatinPronouncer],
  'mus' : [pronouncer.LatinPronouncer],
  'muskogean' : [pronouncer.LatinPronouncer],
  'mvskokē_empunakv' : [pronouncer.LatinPronouncer],
  'mwl' : [pronouncer.LatinPronouncer],
  'mwr' : [pronouncer.UnitranPronouncer],
  'my' : [pronouncer.UnitranPronouncer],
  'mya' : [pronouncer.UnitranPronouncer],
  'myv' : [pronouncer.UnitranPronouncer],
  'míkmaq' : [pronouncer.LatinPronouncer],
  'mòoré' : [pronouncer.LatinPronouncer],
  'māori' : [pronouncer.LatinPronouncer],
  'mɛnde' : [pronouncer.LatinPronouncer],
  "n'ko" : [pronouncer.LatinPronouncer],
  'na' : [pronouncer.LatinPronouncer],
  'na:tinixwe_mixine:whe' : [pronouncer.LatinPronouncer],
  'nap' : [pronouncer.LatinPronouncer],
  'napulitano' : [pronouncer.LatinPronouncer],
  'nau' : [pronouncer.LatinPronouncer],
  'nauruan' : [pronouncer.LatinPronouncer],
  'nav' : [pronouncer.LatinPronouncer],
  'navaho' : [pronouncer.LatinPronouncer],
  'navajo' : [pronouncer.LatinPronouncer],
  'nb' : [pronouncer.LatinPronouncer],
  'nbl' : [pronouncer.LatinPronouncer],
  'nd' : [pronouncer.LatinPronouncer],
  'nde' : [pronouncer.LatinPronouncer],
  'ndo' : [pronouncer.LatinPronouncer],
  'ndonga' : [pronouncer.LatinPronouncer],
  'nds' : [pronouncer.LatinPronouncer],
  'ndébélé' : [pronouncer.LatinPronouncer],
  'ne' : [pronouncer.UnitranPronouncer],
  'neapolitan' : [pronouncer.LatinPronouncer],
  'nederdüütsch' : [pronouncer.LatinPronouncer],
  'nederlands' : [pronouncer.LatinPronouncer],
  'nep' : [pronouncer.UnitranPronouncer],
  'nepal_bhasa' : [pronouncer.UnitranPronouncer],
  'nepali' : [pronouncer.UnitranPronouncer],
  'new' : [pronouncer.UnitranPronouncer],
  'newari' : [pronouncer.UnitranPronouncer],
  'ng' : [pronouncer.LatinPronouncer],
  'nia' : [pronouncer.LatinPronouncer],
  'nias' : [pronouncer.LatinPronouncer],
  'niu' : [pronouncer.LatinPronouncer],
  'niuean' : [pronouncer.LatinPronouncer],
  'nl' : [pronouncer.LatinPronouncer],
  'nld' : [pronouncer.LatinPronouncer],
  'nn' : [pronouncer.LatinPronouncer],
  'nno' : [pronouncer.LatinPronouncer],
  'no' : [pronouncer.LatinPronouncer],
  'nob' : [pronouncer.LatinPronouncer],
  'nog' : [pronouncer.UnitranPronouncer],
  'nogai' : [pronouncer.UnitranPronouncer],
  'non' : [pronouncer.LatinPronouncer],
  'nor' : [pronouncer.LatinPronouncer],
  'norsk' : [pronouncer.LatinPronouncer],
  'norsk_bokmål' : [pronouncer.LatinPronouncer],
  'norsk_nynorsk' : [pronouncer.LatinPronouncer],
  'norskr' : [pronouncer.LatinPronouncer],
  'north_ndebele' : [pronouncer.LatinPronouncer],
  'northern_sami' : [pronouncer.LatinPronouncer],
  'northern_sotho' : [pronouncer.LatinPronouncer],
  'norwegian' : [pronouncer.LatinPronouncer],
  'norwegian_bokmål' : [pronouncer.LatinPronouncer],
  'norwegian_nynorsk' : [pronouncer.LatinPronouncer],
  'nqo' : [pronouncer.LatinPronouncer],
  'nr' : [pronouncer.LatinPronouncer],
  'nso' : [pronouncer.LatinPronouncer],
  'nuosu' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'nv' : [pronouncer.LatinPronouncer],
  'nwc' : [pronouncer.UnitranPronouncer],
  'ny' : [pronouncer.LatinPronouncer],
  'nya' : [pronouncer.LatinPronouncer],
  'nyamwezi_kinyamwezi' : [pronouncer.LatinPronouncer],
  'nyanja' : [pronouncer.LatinPronouncer],
  'nyankole' : [pronouncer.LatinPronouncer],
  'nym' : [pronouncer.LatinPronouncer],
  'nyn' : [pronouncer.LatinPronouncer],
  'nyo' : [pronouncer.LatinPronouncer],
  'nyoro' : [pronouncer.LatinPronouncer],
  'nzi' : [pronouncer.LatinPronouncer],
  'nzima' : [pronouncer.LatinPronouncer],
  "o'zbek" : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'oc' : [pronouncer.LatinPronouncer],
  'occidental' : [pronouncer.LatinPronouncer],
  'occitan' : [pronouncer.LatinPronouncer],
  'oci' : [pronouncer.LatinPronouncer],
  'oirat' : [pronouncer.UnitranPronouncer],
  'oj' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'oji' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'ojibwa' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'old_bulgarian' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'old_church_slavonic' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'old_english' : [pronouncer.LatinPronouncer],
  'old_french' : [pronouncer.LatinPronouncer],
  'old_high_german' : [pronouncer.LatinPronouncer],
  'old_irish' : [pronouncer.LatinPronouncer],
  'old_newari' : [pronouncer.UnitranPronouncer],
  'old_norse' : [pronouncer.LatinPronouncer],
  'old_persian' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'old_provençal' : [pronouncer.LatinPronouncer],
  'old_slavonic' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'om' : [pronouncer.LatinPronouncer],
  'or' : [pronouncer.UnitranPronouncer],
  'ori' : [pronouncer.UnitranPronouncer],
  'oriya' : [pronouncer.UnitranPronouncer],
  'orm' : [pronouncer.LatinPronouncer],
  'oromo' : [pronouncer.LatinPronouncer],
  'os' : [pronouncer.UnitranPronouncer],
  'osa' : [pronouncer.LatinPronouncer],
  'osage' : [pronouncer.LatinPronouncer],
  'oss' : [pronouncer.UnitranPronouncer],
  'ossetian' : [pronouncer.UnitranPronouncer],
  'ossetic' : [pronouncer.UnitranPronouncer],
  'ota' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'otjiherero' : [pronouncer.LatinPronouncer],
  'ottoman' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'owambo' : [pronouncer.LatinPronouncer],
  'pa' : [pronouncer.UnitranPronouncer],
  'pag' : [pronouncer.LatinPronouncer],
  'pahlavi' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'pal' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'palauan' : [pronouncer.LatinPronouncer],
  'pali' : [pronouncer.UnitranPronouncer],
  'pam' : [pronouncer.LatinPronouncer],
  'pampanga' : [pronouncer.LatinPronouncer],
  'pan' : [pronouncer.UnitranPronouncer],
  'pangasinan' : [pronouncer.LatinPronouncer],
  'panjabi' : [pronouncer.UnitranPronouncer],
  'pap' : [pronouncer.LatinPronouncer],
  'papiamento' : [pronouncer.LatinPronouncer],
  'papiamentu' : [pronouncer.LatinPronouncer],
  'pashto' : [pronouncer.UnitranPronouncer],
  'pau' : [pronouncer.LatinPronouncer],
  'pedi' : [pronouncer.LatinPronouncer],
  'peo' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'pepito_ote_henua' : [pronouncer.LatinPronouncer],
  'per' : [pronouncer.UnitranPronouncer],
  'persian' : [pronouncer.UnitranPronouncer],
  'phn' : [pronouncer.UnitranPronouncer],
  'phoenician' : [pronouncer.UnitranPronouncer],
  'pi' : [pronouncer.UnitranPronouncer],
  'pl' : [pronouncer.LatinPronouncer],
  'plattdüütsch' : [pronouncer.LatinPronouncer],
  'pli' : [pronouncer.UnitranPronouncer],
  'pohnpeian' : [pronouncer.LatinPronouncer],
  'pol' : [pronouncer.LatinPronouncer],
  'polish' : [pronouncer.LatinPronouncer],
  'polski' : [pronouncer.LatinPronouncer],
  'pon' : [pronouncer.LatinPronouncer],
  'por' : [pronouncer.LatinPronouncer],
  'portuguese' : [pronouncer.LatinPronouncer],
  'português' : [pronouncer.LatinPronouncer],
  'pro' : [pronouncer.LatinPronouncer],
  'pronouncer.latinpronouncer' : [pronouncer.LatinPronouncer],
  'provençal' : [pronouncer.LatinPronouncer],
  'ps' : [pronouncer.UnitranPronouncer],
  'pt' : [pronouncer.LatinPronouncer],
  'pulaar' : [pronouncer.LatinPronouncer],
  'pular' : [pronouncer.LatinPronouncer],
  'punjabi' : [pronouncer.UnitranPronouncer],
  'pus' : [pronouncer.UnitranPronouncer],
  'pushto' : [pronouncer.UnitranPronouncer],
  'qu' : [pronouncer.LatinPronouncer],
  'que' : [pronouncer.LatinPronouncer],
  'quechua' : [pronouncer.LatinPronouncer],
  'qırımtatar_tili' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'raeto-romance' : [pronouncer.LatinPronouncer],
  'raj' : [pronouncer.UnitranPronouncer],
  'rajasthani' : [pronouncer.UnitranPronouncer],
  'rap' : [pronouncer.LatinPronouncer],
  'rapanui' : [pronouncer.LatinPronouncer],
  'rar' : [pronouncer.LatinPronouncer],
  'rarotongan' : [pronouncer.LatinPronouncer],
  'rm' : [pronouncer.LatinPronouncer],
  'rn' : [pronouncer.LatinPronouncer],
  'ro' : [pronouncer.LatinPronouncer],
  'roh' : [pronouncer.LatinPronouncer],
  'rom' : [pronouncer.LatinPronouncer],
  'romani_šib' : [pronouncer.LatinPronouncer],
  'romanian' : [pronouncer.LatinPronouncer],
  'romansh' : [pronouncer.LatinPronouncer],
  'romany' : [pronouncer.LatinPronouncer],
  'romanó' : [pronouncer.LatinPronouncer],
  'română' : [pronouncer.LatinPronouncer],
  'ron' : [pronouncer.LatinPronouncer],
  'rromani_ćhib' : [pronouncer.LatinPronouncer],
  'ru' : [pronouncer.UnitranPronouncer],
  'rum' : [pronouncer.LatinPronouncer],
  'rumantsch_grischun' : [pronouncer.LatinPronouncer],
  'run' : [pronouncer.LatinPronouncer],
  'runa_simi' : [pronouncer.LatinPronouncer],
  'rundi' : [pronouncer.LatinPronouncer],
  'runyoro' : [pronouncer.LatinPronouncer],
  'rup' : [pronouncer.LatinPronouncer],
  'rus' : [pronouncer.UnitranPronouncer],
  'russian' : [pronouncer.UnitranPronouncer],
  'rw' : [pronouncer.LatinPronouncer],
  'sa' : [pronouncer.UnitranPronouncer],
  'sad' : [pronouncer.LatinPronouncer],
  'saemien_giele' : [pronouncer.LatinPronouncer],
  'sag' : [pronouncer.LatinPronouncer],
  'sah' : [pronouncer.UnitranPronouncer],
  'sam' : [pronouncer.UnitranPronouncer],
  'samaritan_aramaic' : [pronouncer.UnitranPronouncer],
  'samoan' : [pronouncer.LatinPronouncer],
  'san' : [pronouncer.UnitranPronouncer],
  'sandawe' : [pronouncer.LatinPronouncer],
  'sango' : [pronouncer.LatinPronouncer],
  'sanskrit' : [pronouncer.UnitranPronouncer],
  'santali' : [pronouncer.UnitranPronouncer],
  'sardinian' : [pronouncer.LatinPronouncer],
  'sardu' : [pronouncer.LatinPronouncer],
  'sas' : [pronouncer.LatinPronouncer],
  'sasak' : [pronouncer.LatinPronouncer],
  'sat' : [pronouncer.UnitranPronouncer],
  'saw_cuengh' : [pronouncer.LatinPronouncer],
  'saɯ_cueŋƅ' : [pronouncer.LatinPronouncer],
  'sc' : [pronouncer.LatinPronouncer],
  'schwyzerdütsch' : [pronouncer.LatinPronouncer],
  'scn' : [pronouncer.LatinPronouncer],
  'sco' : [pronouncer.LatinPronouncer],
  'scoats_leid' : [pronouncer.LatinPronouncer],
  'scots' : [pronouncer.LatinPronouncer],
  'scottish_gaelic' : [pronouncer.LatinPronouncer],
  'sd' : [pronouncer.UnitranPronouncer],
  'se' : [pronouncer.LatinPronouncer],
  'seeltersk' : [pronouncer.LatinPronouncer],
  'seeltersk_fräisk' : [pronouncer.LatinPronouncer],
  'sel' : [pronouncer.UnitranPronouncer],
  'selkup' : [pronouncer.UnitranPronouncer],
  'sepedi' : [pronouncer.LatinPronouncer],
  'sepêdi' : [pronouncer.LatinPronouncer],
  'serbian' : [pronouncer.UnitranPronouncer],
  'serbo-croatian' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'serer' : [pronouncer.LatinPronouncer],
  'sesotho' : [pronouncer.LatinPronouncer],
  'setswana' : [pronouncer.LatinPronouncer],
  'sg' : [pronouncer.LatinPronouncer],
  'sga' : [pronouncer.LatinPronouncer],
  'sh' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'shan' : [pronouncer.LatinPronouncer],
  'shiwi' : [pronouncer.LatinPronouncer],
  'shn' : [pronouncer.LatinPronouncer],
  'shona' : [pronouncer.LatinPronouncer],
  'shqip' : [pronouncer.LatinPronouncer],
  'si' : [pronouncer.UnitranPronouncer],
  'sichuan_yi' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'sicilian' : [pronouncer.LatinPronouncer],
  'sicilianu' : [pronouncer.LatinPronouncer],
  'sid' : [pronouncer.LatinPronouncer],
  'sidamo' : [pronouncer.LatinPronouncer],
  "sidámo_'afó" : [pronouncer.LatinPronouncer],
  'siksika' : [pronouncer.UnitranPronouncer],
  'siksiká' : [pronouncer.UnitranPronouncer],
  'silozi' : [pronouncer.LatinPronouncer],
  'sin' : [pronouncer.UnitranPronouncer],
  'sindhi' : [pronouncer.UnitranPronouncer],
  'sinhala' : [pronouncer.UnitranPronouncer],
  'sinhalese' : [pronouncer.UnitranPronouncer],
  'sinugboanon' : [pronouncer.LatinPronouncer],
  'siswati' : [pronouncer.LatinPronouncer],
  'sk' : [pronouncer.LatinPronouncer],
  'skolt_sami' : [pronouncer.LatinPronouncer],
  'sl' : [pronouncer.LatinPronouncer],
  'slave' : [pronouncer.LatinPronouncer],
  'slk' : [pronouncer.LatinPronouncer],
  'slo' : [pronouncer.LatinPronouncer],
  'slovak' : [pronouncer.LatinPronouncer],
  'slovenian' : [pronouncer.LatinPronouncer],
  'slovenčina' : [pronouncer.LatinPronouncer],
  'slovenščina' : [pronouncer.LatinPronouncer],
  'slv' : [pronouncer.LatinPronouncer],
  'sm' : [pronouncer.LatinPronouncer],
  'sma' : [pronouncer.LatinPronouncer],
  'sme' : [pronouncer.LatinPronouncer],
  'smj' : [pronouncer.LatinPronouncer],
  'smn' : [pronouncer.LatinPronouncer],
  'smo' : [pronouncer.LatinPronouncer],
  'sms' : [pronouncer.LatinPronouncer],
  'sn' : [pronouncer.LatinPronouncer],
  'sna' : [pronouncer.LatinPronouncer],
  'snd' : [pronouncer.UnitranPronouncer],
  'snk' : [pronouncer.LatinPronouncer],
  'so' : [pronouncer.LatinPronouncer],
  'sog' : [pronouncer.LatinPronouncer],
  'sogdian' : [pronouncer.LatinPronouncer],
  'som' : [pronouncer.LatinPronouncer],
  'somali' : [pronouncer.LatinPronouncer],
  'soninkanxaane' : [pronouncer.LatinPronouncer],
  'soninke' : [pronouncer.LatinPronouncer],
  'soomaaliga' : [pronouncer.LatinPronouncer],
  'sot' : [pronouncer.LatinPronouncer],
  'south_ndebele' : [pronouncer.LatinPronouncer],
  'southern_altai' : [pronouncer.UnitranPronouncer],
  'southern_sami' : [pronouncer.LatinPronouncer],
  'southern_sotho' : [pronouncer.LatinPronouncer],
  'spa' : [pronouncer.LatinPronouncer],
  'spanish' : [pronouncer.LatinPronouncer],
  'sq' : [pronouncer.LatinPronouncer],
  'sqi' : [pronouncer.LatinPronouncer],
  'sranan_tongo' : [pronouncer.LatinPronouncer],
  'srd' : [pronouncer.LatinPronouncer],
  'srn' : [pronouncer.LatinPronouncer],
  'srp' : [pronouncer.UnitranPronouncer],
  'srpskohrvatski' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'srr' : [pronouncer.LatinPronouncer],
  'ss' : [pronouncer.LatinPronouncer],
  'ssw' : [pronouncer.LatinPronouncer],
  'st' : [pronouncer.LatinPronouncer],
  'su' : [pronouncer.LatinPronouncer],
  'suk' : [pronouncer.LatinPronouncer],
  'sukuma' : [pronouncer.LatinPronouncer],
  'sumerian' : [pronouncer.LatinPronouncer],
  'sun' : [pronouncer.LatinPronouncer],
  'sundanese' : [pronouncer.LatinPronouncer],
  'suomen_kieli' : [pronouncer.LatinPronouncer],
  'suomi' : [pronouncer.LatinPronouncer],
  'sus' : [pronouncer.LatinPronouncer],
  'susu' : [pronouncer.LatinPronouncer],
  'sux' : [pronouncer.LatinPronouncer],
  'sv' : [pronouncer.LatinPronouncer],
  'svenska' : [pronouncer.LatinPronouncer],
  'sw' : [pronouncer.LatinPronouncer],
  'swa' : [pronouncer.LatinPronouncer],
  'swahili' : [pronouncer.LatinPronouncer],
  'swati' : [pronouncer.LatinPronouncer],
  'swe' : [pronouncer.LatinPronouncer],
  'swedish' : [pronouncer.LatinPronouncer],
  'swiss_german' : [pronouncer.LatinPronouncer],
  'syc' : [pronouncer.LatinPronouncer],
  'syr' : [pronouncer.UnitranPronouncer],
  'syriac' : [pronouncer.UnitranPronouncer],
  'sámegiella' : [pronouncer.LatinPronouncer],
  'sámi' : [pronouncer.LatinPronouncer],
  'säämegiella' : [pronouncer.LatinPronouncer],
  'sääʼmǩiõll' : [pronouncer.LatinPronouncer],
  'ta' : [pronouncer.UnitranPronouncer],
  'taetae_ni_kiribati' : [pronouncer.LatinPronouncer],
  'tagalog' : [pronouncer.LatinPronouncer],
  'tah' : [pronouncer.LatinPronouncer],
  'tahitian' : [pronouncer.LatinPronouncer],
  'tajik' : [pronouncer.UnitranPronouncer],
  'tam' : [pronouncer.UnitranPronouncer],
  'tamajeq' : [pronouncer.UnitranPronouncer],
  'tamashek' : [pronouncer.UnitranPronouncer],
  'tamil' : [pronouncer.UnitranPronouncer],
  'taqbaylit' : [pronouncer.LatinPronouncer],
  'tat' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'tatar' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'tatarça' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'te' : [pronouncer.UnitranPronouncer],
  'te_reo_māori' : [pronouncer.LatinPronouncer],
  'te_reo_māʼohi' : [pronouncer.LatinPronouncer],
  'te_reo_tahiti' : [pronouncer.LatinPronouncer],
  'tekoi_ra_belau' : [pronouncer.LatinPronouncer],
  'tel' : [pronouncer.UnitranPronouncer],
  'telugu' : [pronouncer.UnitranPronouncer],
  'tem' : [pronouncer.LatinPronouncer],
  'ter' : [pronouncer.LatinPronouncer],
  'tereno' : [pronouncer.LatinPronouncer],
  'tet' : [pronouncer.LatinPronouncer],
  'tetum' : [pronouncer.LatinPronouncer],
  'tetun' : [pronouncer.LatinPronouncer],
  'tg' : [pronouncer.UnitranPronouncer],
  'tgk' : [pronouncer.UnitranPronouncer],
  'tgl' : [pronouncer.LatinPronouncer],
  'th' : [pronouncer.UnitranPronouncer],
  'tha' : [pronouncer.UnitranPronouncer],
  'thai' : [pronouncer.UnitranPronouncer],
  'thuɔŋjäŋ' : [pronouncer.LatinPronouncer],
  'ti' : [pronouncer.UnitranPronouncer],
  'tib' : [pronouncer.UnitranPronouncer],
  'tibetan' : [pronouncer.UnitranPronouncer],
  'tig' : [pronouncer.LatinPronouncer],
  'tigre' : [pronouncer.LatinPronouncer],
  'tigrinya' : [pronouncer.UnitranPronouncer],
  'tigré' : [pronouncer.LatinPronouncer],
  'timne' : [pronouncer.LatinPronouncer],
  'tir' : [pronouncer.UnitranPronouncer],
  'tiv' : [pronouncer.LatinPronouncer],
  'tiếng_việt' : [pronouncer.LatinPronouncer],
  'tk' : [pronouncer.UnitranPronouncer],
  'tkl' : [pronouncer.LatinPronouncer],
  'tl' : [pronouncer.LatinPronouncer],
  'tlh' : [pronouncer.LatinPronouncer],
  'tlhingan-hol' : [pronouncer.LatinPronouncer],
  'tlhingan_hol' : [pronouncer.LatinPronouncer],
  'tli' : [pronouncer.LatinPronouncer],
  'tlingit' : [pronouncer.LatinPronouncer],
  'tmh' : [pronouncer.UnitranPronouncer],
  'tn' : [pronouncer.LatinPronouncer],
  'to' : [pronouncer.LatinPronouncer],
  'tog' : [pronouncer.LatinPronouncer],
  'tok_pisin' : [pronouncer.LatinPronouncer],
  'tokelau' : [pronouncer.LatinPronouncer],
  'ton' : [pronouncer.LatinPronouncer],
  'tonga' : [pronouncer.LatinPronouncer],
  'tongan' : [pronouncer.LatinPronouncer],
  'tpi' : [pronouncer.LatinPronouncer],
  'tr' : [pronouncer.LatinPronouncer],
  'ts' : [pronouncer.LatinPronouncer],
  'tshivenḓa' : [pronouncer.LatinPronouncer],
  'tsn' : [pronouncer.LatinPronouncer],
  'tso' : [pronouncer.LatinPronouncer],
  'tsonga' : [pronouncer.LatinPronouncer],
  'tswana' : [pronouncer.LatinPronouncer],
  'tsêhést' : [pronouncer.LatinPronouncer],
  'tt' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'tuk' : [pronouncer.UnitranPronouncer],
  'tum' : [pronouncer.LatinPronouncer],
  'tumbuka' : [pronouncer.LatinPronouncer],
  'tur' : [pronouncer.LatinPronouncer],
  'turkish' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'turkmen' : [pronouncer.UnitranPronouncer],
  'tuvalu' : [pronouncer.LatinPronouncer],
  'tuvinian' : [pronouncer.UnitranPronouncer],
  'tuḍḍungiyya' : [pronouncer.LatinPronouncer],
  'tvl' : [pronouncer.LatinPronouncer],
  'tw' : [pronouncer.LatinPronouncer],
  'twi' : [pronouncer.LatinPronouncer],
  'ty' : [pronouncer.LatinPronouncer],
  'tyv' : [pronouncer.UnitranPronouncer],
  'türkçe' : [pronouncer.LatinPronouncer],
  'tłįchǫ' : [pronouncer.LatinPronouncer],
  'udm' : [pronouncer.UnitranPronouncer],
  'udmurt' : [pronouncer.UnitranPronouncer],
  'ug' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'uga' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'ugaritic' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'uig' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'uighur' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'uk' : [pronouncer.UnitranPronouncer],
  'ukr' : [pronouncer.UnitranPronouncer],
  'ukrainian' : [pronouncer.UnitranPronouncer],
  'umb' : [pronouncer.LatinPronouncer],
  'umbundu' : [pronouncer.LatinPronouncer],
  'upper_sorbian' : [pronouncer.LatinPronouncer],
  'ur' : [pronouncer.UnitranPronouncer],
  'urd' : [pronouncer.UnitranPronouncer],
  'urdu' : [pronouncer.UnitranPronouncer],
  'uyghur' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'uyğurçe' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'uyƣurqə' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'uz' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'uzb' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'uzbek' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'vai' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'valencian' : [pronouncer.LatinPronouncer],
  'vaďďa_tšeeli' : [pronouncer.LatinPronouncer],
  've' : [pronouncer.LatinPronouncer],
  'ven' : [pronouncer.LatinPronouncer],
  'venda' : [pronouncer.LatinPronouncer],
  'vi' : [pronouncer.LatinPronouncer],
  'vie' : [pronouncer.LatinPronouncer],
  'vietnamese' : [pronouncer.LatinPronouncer],
  'vo' : [pronouncer.LatinPronouncer],
  'vol' : [pronouncer.LatinPronouncer],
  'volapük' : [pronouncer.LatinPronouncer],
  'vosa_vakaviti' : [pronouncer.LatinPronouncer],
  'vot' : [pronouncer.LatinPronouncer],
  'votic' : [pronouncer.LatinPronouncer],
  'wa' : [pronouncer.LatinPronouncer],
  'wal' : [pronouncer.LatinPronouncer],
  'walamo' : [pronouncer.LatinPronouncer],
  'walloon' : [pronouncer.LatinPronouncer],
  'walon' : [pronouncer.LatinPronouncer],
  'war' : [pronouncer.LatinPronouncer],
  'waray' : [pronouncer.LatinPronouncer],
  'was' : [pronouncer.LatinPronouncer],
  'washo' : [pronouncer.LatinPronouncer],
  'wel' : [pronouncer.LatinPronouncer],
  'welsh' : [pronouncer.LatinPronouncer],
  'winaray' : [pronouncer.LatinPronouncer],
  'wln' : [pronouncer.LatinPronouncer],
  'wo' : [pronouncer.LatinPronouncer],
  'wol' : [pronouncer.LatinPronouncer],
  'wolof' : [pronouncer.LatinPronouncer],
  'xal' : [pronouncer.UnitranPronouncer],
  'xh' : [pronouncer.LatinPronouncer],
  'xho' : [pronouncer.LatinPronouncer],
  'xhosa' : [pronouncer.LatinPronouncer],
  'xitsonga' : [pronouncer.LatinPronouncer],
  'x̲aat_kíl' : [pronouncer.LatinPronouncer],
  'yakut' : [pronouncer.UnitranPronouncer],
  'yao' : [pronouncer.LatinPronouncer],
  'yap' : [pronouncer.LatinPronouncer],
  'yapese' : [pronouncer.LatinPronouncer],
  'yi' : [pronouncer.UnitranPronouncer],
  'yid' : [pronouncer.UnitranPronouncer],
  'yiddish' : [pronouncer.UnitranPronouncer],
  'yo' : [pronouncer.LatinPronouncer],
  'yor' : [pronouncer.LatinPronouncer],
  'yoruba' : [pronouncer.LatinPronouncer],
  'yorùbá' : [pronouncer.LatinPronouncer],
  'yângâ_tî_sängö' : [pronouncer.LatinPronouncer],
  'za' : [pronouncer.LatinPronouncer],
  'zaza' : [pronouncer.LatinPronouncer],
  'zazaki' : [pronouncer.LatinPronouncer],
  'zen' : [pronouncer.LatinPronouncer],
  'zenaga' : [pronouncer.LatinPronouncer],
  'zh' : [pronouncer.UnitranPronouncer],
  'zha' : [pronouncer.LatinPronouncer],
  'zho' : [pronouncer.HanziPronouncer],
  'zhuang' : [pronouncer.LatinPronouncer],
  'zu' : [pronouncer.LatinPronouncer],
  'zul' : [pronouncer.LatinPronouncer],
  'zulu' : [pronouncer.LatinPronouncer],
  'zun' : [pronouncer.LatinPronouncer],
  'zuni' : [pronouncer.LatinPronouncer],
  'zza' : [pronouncer.LatinPronouncer],
  'íslenska' : [pronouncer.LatinPronouncer],
  'úmbúndú' : [pronouncer.LatinPronouncer],
  'česky' : [pronouncer.LatinPronouncer],
  'čeština' : [pronouncer.LatinPronouncer],
  'ɓasaá' : [pronouncer.LatinPronouncer],
  'ɔl_maa' : [pronouncer.LatinPronouncer],
  'ɛʋɛgbɛ' : [pronouncer.LatinPronouncer],
  'ɫ[ǂ…' : [pronouncer.UnitranPronouncer],
  'ελληνικά' : [pronouncer.UnitranPronouncer],
  'авар_мацӀ' : [pronouncer.UnitranPronouncer],
  'адыгабзэ' : [pronouncer.UnitranPronouncer],
  'адыгэбзэ' : [pronouncer.UnitranPronouncer],
  'алтай_тили' : [pronouncer.UnitranPronouncer],
  'аҧсуа' : [pronouncer.UnitranPronouncer],
  'башҡорт_теле' : [pronouncer.UnitranPronouncer],
  'беларуская_мова' : [pronouncer.UnitranPronouncer],
  'буряад_хэлэн' : [pronouncer.UnitranPronouncer],
  'български_език' : [pronouncer.UnitranPronouncer],
  'гӀалгӀай_мотт' : [pronouncer.UnitranPronouncer],
  'дарган_мез' : [pronouncer.UnitranPronouncer],
  'ирон_ӕвзаг' : [pronouncer.UnitranPronouncer],
  'коми_кыв' : [pronouncer.UnitranPronouncer],
  'кумык' : [pronouncer.UnitranPronouncer],
  'къарачай-малкъар_тил' : [pronouncer.UnitranPronouncer],
  'къырымтатар_тили' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'къэбэрдеибзэ' : [pronouncer.UnitranPronouncer],
  'кыргыз_тили' : [pronouncer.UnitranPronouncer],
  'лезги_чӀал' : [pronouncer.UnitranPronouncer],
  'лимба_молдовеняскэ' : [pronouncer.UnitranPronouncer],
  'магӀарул_мацӀ' : [pronouncer.UnitranPronouncer],
  'македонски_јазик' : [pronouncer.UnitranPronouncer],
  'марий_йылме' : [pronouncer.UnitranPronouncer],
  'мокшень_кяль' : [pronouncer.UnitranPronouncer],
  'монгол_хэл' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'ногай_тили' : [pronouncer.UnitranPronouncer],
  'нохчийн_мотт' : [pronouncer.UnitranPronouncer],
  'русский_язык' : [pronouncer.UnitranPronouncer],
  'саха_тыла' : [pronouncer.UnitranPronouncer],
  'српски_језик' : [pronouncer.UnitranPronouncer],
  'српскохрватски' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'татарча' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'тоҷикӣ' : [pronouncer.UnitranPronouncer],
  'тыва_дыл' : [pronouncer.UnitranPronouncer],
  'түркмен' : [pronouncer.UnitranPronouncer],
  'удмурт_кыл' : [pronouncer.UnitranPronouncer],
  'українська_мова' : [pronouncer.UnitranPronouncer],
  'хальмг_келн' : [pronouncer.UnitranPronouncer],
  'чӑваш_чӗлхи' : [pronouncer.UnitranPronouncer],
  'шӧльӄумыт_әты' : [pronouncer.UnitranPronouncer],
  'эрзянь_кель' : [pronouncer.UnitranPronouncer],
  'ўзбек' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'ѩзыкъ_словѣньскъ' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'қазақ_тілі' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'қарақалпақ_тили' : [pronouncer.UnitranPronouncer],
  'հայերեն_լեզու' : [pronouncer.UnitranPronouncer],
  'ארמית' : [pronouncer.UnitranPronouncer],
  "ג'ודיאו-איספאנייול" : [pronouncer.UnitranPronouncer],
  'ייִדיש' : [pronouncer.UnitranPronouncer],
  'עִבְרִית' : [pronouncer.UnitranPronouncer],
  'עברית' : [pronouncer.UnitranPronouncer],
  'أۇزبېك' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'ئۇيغۇرچ' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'اردو' : [pronouncer.UnitranPronouncer],
  'العربية' : [pronouncer.UnitranPronouncer],
  'بداوية' : [pronouncer.UnitranPronouncer],
  'بلوچی' : [pronouncer.UnitranPronouncer],
  'بهاس_ملايو' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'تاتارچا' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'تاجیکی' : [pronouncer.UnitranPronouncer],
  'جغتای' : [pronouncer.UnitranPronouncer],
  'سنڌي،_سندھی' : [pronouncer.UnitranPronouncer],
  'فارسی' : [pronouncer.UnitranPronouncer],
  'هَوُسَ' : [pronouncer.LatinPronouncer],
  'پنجابی' : [pronouncer.UnitranPronouncer],
  'پښتو' : [pronouncer.UnitranPronouncer],
  'کٲشُر' : [pronouncer.UnitranPronouncer],
  'ܐܪܡܝܐ' : [pronouncer.UnitranPronouncer],
  'ܣܘܪܝܝܐ' : [pronouncer.UnitranPronouncer],
  'ދިވެހިބަސ' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'अवधी' : [pronouncer.UnitranPronouncer],
  'कॉशुर' : [pronouncer.UnitranPronouncer],
  'कोंकणी' : [pronouncer.UnitranPronouncer],
  'डोगरी' : [pronouncer.UnitranPronouncer],
  'नेपाली' : [pronouncer.UnitranPronouncer],
  'पालि' : [pronouncer.UnitranPronouncer],
  'ब्रज_भाषा' : [pronouncer.UnitranPronouncer],
  'भोजपुरी' : [pronouncer.UnitranPronouncer],
  'मराठी' : [pronouncer.UnitranPronouncer],
  'मारवाड़ी' : [pronouncer.UnitranPronouncer],
  'मैथिली' : [pronouncer.UnitranPronouncer],
  'राजस्थानी' : [pronouncer.UnitranPronouncer],
  'संथाली' : [pronouncer.UnitranPronouncer],
  'संस्कृतम्' : [pronouncer.UnitranPronouncer],
  'सिन्धी' : [pronouncer.UnitranPronouncer],
  'हिन्दी' : [pronouncer.UnitranPronouncer],
  'অসমীয়া' : [pronouncer.UnitranPronouncer],
  'বাংলা' : [pronouncer.UnitranPronouncer],
  'মৈইতৈইলোন' : [pronouncer.UnitranPronouncer],
  'ਪੰਜਾਬੀ' : [pronouncer.UnitranPronouncer],
  'ਲਹਿੰਦੀ' : [pronouncer.UnitranPronouncer],
  'ગુજરાતી' : [pronouncer.UnitranPronouncer],
  'ଓଡ଼ିଆ' : [pronouncer.UnitranPronouncer],
  'தமிழ்' : [pronouncer.UnitranPronouncer],
  'తెలుగు' : [pronouncer.UnitranPronouncer],
  'ಕನ್ನಡ' : [pronouncer.UnitranPronouncer],
  'മലയാളം' : [pronouncer.UnitranPronouncer],
  'සිංහල' : [pronouncer.UnitranPronouncer],
  'ภาษาไทย' : [pronouncer.UnitranPronouncer],
  'ພາສາລາວ' : [pronouncer.UnitranPronouncer],
  'བོད་ཡིག' : [pronouncer.UnitranPronouncer],
  'རྫོང་ཁ' : [pronouncer.UnitranPronouncer],
  'ျမန္မာစာ' : [pronouncer.UnitranPronouncer],
  'ქართული_ენა' : [pronouncer.UnitranPronouncer],
  'ብሊና' : [pronouncer.LatinPronouncer],
  'ትግርኛ' : [pronouncer.UnitranPronouncer],
  'አማርኛ' : [pronouncer.UnitranPronouncer],
  'ግዕዝ' : [pronouncer.UnitranPronouncer],
  'ᏣᎳᎩ' : [pronouncer.UnitranPronouncer],
  'ᐃᓄᒃᑎᑐᑦ' : [pronouncer.UnitranPronouncer],
  'ᐊᓂᔑᓇᐯᒧᐏᐣ' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'ᑌᓀᓲᒢᕄᓀ' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  'ᓀᐦᐃᔭᐍᐏᐣ' : [pronouncer.UnitranPronouncer],
  'ᠮᠠᠨᠵᡠ_ᡤᡳᠰᡠᠨ ᠪᡝ' : [pronouncer.UnitranPronouncer],
  'ᨅᨔ_ᨕᨘᨁᨗ' : [pronouncer.UnitranPronouncer],
  'ἑλληνικά' : [pronouncer.UnitranPronouncer],
  ' ᓱᖽᐧᖿ' : [pronouncer.UnitranPronouncer],
  '‘ōlelo_hawai‘i' : [pronouncer.LatinPronouncer],
  'ⲙⲉⲧⲛ̀ⲣⲉⲙⲛ̀ⲭⲏⲙⲓ' : [pronouncer.UnitranPronouncer],
  'にほんご' : [pronouncer.UnitranPronouncer],
  'アイヌ_イタㇰ' : [pronouncer.UnitranPronouncer],
  'イタッㇰ' : [pronouncer.UnitranPronouncer],
  '中文' : [pronouncer.HanziPronouncer],
  '日本語' : [pronouncer.UnitranPronouncer, pronouncer.HanziPronouncer],
  '朝鮮語' : [pronouncer.UnitranPronouncer],
  '韓國語' : [pronouncer.UnitranPronouncer],
  'ꆇꉙ' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
  '조선말' : [pronouncer.UnitranPronouncer],
  '한국어' : [pronouncer.UnitranPronouncer],
  '𐌲𐌿𐍄𐌹𐍃𐌺' : [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer],
}

DEF_PRONOUNCER_LIST_ = [pronouncer.UnitranPronouncer, pronouncer.LatinPronouncer]


def DefPronouncers(lid):
  lid = Utils.script.Lower(lid)
  try: return DEF_PRONOUNCERS_[lid]
  except KeyError: return DEF_PRONOUNCER_LIST_

########NEW FILE########
__FILENAME__ = documents
# -*- coding: utf-8 -*-

## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

"""Handler for documents and document-lists.
"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
hollingk@cslu.ogi.edu (Kristy Hollingshead)
"""

import xml.sax.saxutils
import xmlhandler
import tokens
from __init__ import BASE_

XML_HEADER_ = '<?xml version="1.0" encoding="UTF-8"?>'
DOC_INDENT_   = ' ' * 2

class Doc:
  """Holder for languages. Represents one aligned "document" across
  languages/scripts, where "document" could be an actual n-tuple of
  translated documents, a single pair of terms known to be
  transcriptions of each other, or multiple documents in n languages
  that are roughly time aligned to each other.
  """
  def __init__(self):
    self.langs_ = []

  def XmlEncode(self):
    if len(self.langs_) == 0: return
    xml_string_ = '<doc>\n%s\n%s</doc>'
    langs = []
    for lang in self.langs_:
      langs.append(lang.XmlEncode())
    xml_result = xml_string_ % ('\n'.join(langs), DOC_INDENT_)
    return DOC_INDENT_ + xml_result

  def Langs(self):
    return self.langs_

  def AddLang(self, lang):
    self.langs_.append(lang)

  def AddTokens(self, toks, langid):
    success = False
    for lang in self.langs_:
      if lang.Id() == langid:
        for t in toks:
          lang.AddToken(t)
        lang.CompactTokens()
        success = True
    if not success:
      lang = tokens.Lang()
      lang.SetId(langid)
      lang.SetTokens(toks)
      lang.CompactTokens()
      self.AddLang(lang)

  def LookupLang(self, langid):
    for lang in self.langs_:
      if lang.Id() == langid:
        return lang
    return None

  def LookupToken(self, tokstr, langid=None):
    ## warning!: this will return the first instance of tokstr,
    ## regardless of pronunciations, morphs, or (if langid is
    ## not provided) language id.
    if langid is None: langset = self.langs_
    else: langset = [self.LookupLang(langid)]
    for lang in langset:
      if not lang: continue
      for t in lang.Tokens():
        if t.String() == tokstr:
          return t
    return None

class Doclist:
  """Holder for docs. It is assumed these are stored in some sensible
  sequence (e.g. in temporal order).
  """
  def __init__(self):
    self.docs_ = []
    self.tokenfreq_ = []

  def XmlDecode(self, filename):
    decoder = xmlhandler.XmlHandler()
    doclist = decoder.Decode(filename)
    self.docs_ = doclist.Docs()

  def XmlEncode(self, utf8=False):
    if len(self.docs_) == 0: return ''
    xml_string_ = '<doclist>\n%s\n</doclist>'
    xml_docs = []
    for doc in self.docs_:
      xml_docs.append(doc.XmlEncode())
    xml_result = '%s\n%s' % (XML_HEADER_, xml_string_ % '\n'.join(xml_docs))
    """
    TODO(rws): investigate why this is necessary. In
    dochandler_unittest when the XML is created internally from utf8
    text one can write it out without the explicit encode. However,
    when it is parsed back from the file and then written again, we
    get the error

    UnicodeEncodeError: 'ascii' codec can't encode characters in 
    position 255-257: ordinal not in range(128)
    """
    ## Result is already in utf8, as when generated internally
    if utf8: return xml_result
    ## Result is not in utf8, as when parsed from an XML file
    else: return xml_result.encode('utf-8')

  def XmlDump(self, file=None, utf8=False):
    if file is None:
      print '%s\n' % (self.XmlEncode(utf8))
      return
    p = open(file, 'w')
    p.write('%s\n' % self.XmlEncode(utf8))
    p.close()

  def Docs(self):
    return self.docs_

  def AddDoc(self, doc):
    self.docs_.append(doc)

  def LookupLang(self, langid):
    for doc in self.docs_:
      retlang = doc.LookupLang(langid)
      if retlang is not None: return retlang
    return None

  def LookupToken(self, tokstr, langid=None):
    ## warning!: this will return the first instance of tokstr,
    ## regardless of pronunciations, morphs, or (if langid is
    ## not provided) language id.
    for doc in self.docs_:
      rettok = doc.LookupToken(tokstr, langid)
      if rettok is not None: return rettok
    return None

########NEW FILE########
__FILENAME__ = extractor
# -*- coding: utf-8 -*-
## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

"""Token extractor class.
"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
hollingk@cslu.ogi.edu (Kristy Hollingshead)
"""

import Utils.script
import tokens
from __init__ import BASE_

MIN_LEN_ = 3
SENTENCE_END_ = ['.', ':', '?', '!']

class Extractor:
  """Input is raw UTF-8 text. Output is a sequence of "interesting"
  tokens.
  """
  def __init__(self):
    self.text_ = ''
    self.tokens_ = []

  def InitData(self, args = []):
    self.tokens_ = []
    return
  
  def LineSegment(self, text):
    self.text_ = text
    for word in text.split():
      self.tokens_.append(tokens.Token(word))
    return

  def FileExtract(self, filename):
    fp = open(filename, 'r')
    for line in fp:
      line = line.strip()
      self.LineSegment(line)
    fp.close()
    return self.tokens_

  def Tokens(self):
    return self.tokens_

class NameExtractor(Extractor):
  """Simple extractor based on capitalization (for scripts that
  support this) or any word (for scripts that do not). Also
  discards any words containing digits.
  """

  def LineSegment(self, line):
    ulinelist = []
    ## Go 'word' by word to make this more robust to unicode decode
    ## errors.
    for w in line.split():
      try: ulinelist.append(unicode(w, 'utf-8'))
      except UnicodeDecodeError: pass
    uline = ' '.join(ulinelist)
    clist = []
    for u in uline:
      if Utils.script.IsUnicodePunctuation(u):
        clist.append(' ')
        clist.append(u.encode('utf-8'))
        clist.append(' ')
      else:
        clist.append(u.encode('utf-8'))
    text = ''.join(clist)
    lastw = '.'
    for word in text.split():
      if len(word) < MIN_LEN_:
        continue # ignore word
      ## check for digits, discard word if found
      if Utils.script.HasDigit(word):
        continue
      ## for scripts that support capitalization...
      if Utils.script.SupportsCapitalization(word):
        if lastw in SENTENCE_END_:
          ## ignore sentence-initial capitalization
          pass
        ## check for initial-capitalization,
        ## ignore word if not capitalized
        elif Utils.script.IsCapitalized(word):
          self.tokens_.append(tokens.Token(word))
      else:
        self.tokens_.append(tokens.Token(word))
      lastw = word


########NEW FILE########
__FILENAME__ = extractor_unittest
## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

"""Unit test for extractor class.
"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
"""

import sys
import chinese_extractor
import japanese_extractor
from __init__ import BASE_

CHINESE_TESTFILE_ = '%s/testdata/chinese_isi_test.txt' % BASE_
JAPANESE_TESTFILE_ = '%s/testdata/japanese_test.txt' % BASE_
GOLDEN_FILE_ =  '%s/testdata/extractor_test.out' % BASE_ 
GOLDEN_ = []


def LoadGolden():
  p = open(GOLDEN_FILE_)
  lines = p.readlines()
  for line in lines:
    GOLDEN_.append(line.strip())
  p.close()


def main(output = False):
  if output: file = open(GOLDEN_FILE_, 'w')
  else: LoadGolden()
  extractor = chinese_extractor.ChineseExtractor()
  extractor.FileExtract(CHINESE_TESTFILE_)
  pname_extractor = chinese_extractor.ChinesePersonalNameExtractor()
  pname_extractor.FileExtract(CHINESE_TESTFILE_)
  katakana_extractor = japanese_extractor.KatakanaExtractor()
  katakana_extractor.FileExtract(JAPANESE_TESTFILE_)
  if output:
    for t in extractor.Tokens():
      file.write('%s\n' % t.String())
    for t in pname_extractor.Tokens():
      file.write('%s\n' % t.String())
    for t in katakana_extractor.Tokens():
      file.write('%s\n' % t.String())
    file.close()
  else:
    all_tokens = (extractor.Tokens() + 
                  pname_extractor.Tokens() +
                  katakana_extractor.Tokens())
    assert len(all_tokens) == len(GOLDEN_), \
        'Extracted different numbers of tokens'
    for i in range(len(GOLDEN_)):
      assert all_tokens[i].String() == GOLDEN_[i], \
          'Token %d differs: %s != %s' %  (i,
                                           all_tokens[i].String(),
                                           GOLDEN_[i])
    print '%s successful' % sys.argv[0]


if __name__ == '__main__':
  if len(sys.argv) > 1 and sys.argv[1] == 'generate':
    main(True)
  else:
    main()

########NEW FILE########
__FILENAME__ = filter
# -*- coding: utf-8 -*-

## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

"""Class definition for filters for reducing the set of terms to be
considered in a doclist.
"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
"""

import tokens
import xmlhandler
from __init__ import BASE_

MIN_COUNT_ = 3
 
class Filter:
  """Base class for filters that take in a document list and return a
  list with some terms removed.
  """
  def __init__(self, doclist):
    self.doclist_ = doclist

  def InitData(self, args = []):
    return
  
  def Filter(self):
    return

  def Doclist(self):
    return self.doclist_

class FrequencyFilter(Filter):
  """Simple filter based on a minimum frequency count for a term.
  """
  
  def SetMinCount(self, min_count):
    self.min_count_ = min_count

  def Filter(self):
    try:
      self.min_count_
    except AttributeError:
      self.min_count_ = MIN_COUNT_
    counts = {}
    for doc in self.doclist_.Docs():
      for lang in doc.Langs():
        for token_ in lang.Tokens():
          hash_string = token_.EncodeForHash()
          try:
            counts[hash_string] += token_.Count()
          except KeyError:
            counts[hash_string] = token_.Count()
    for key in counts:
      if counts[key] < self.min_count_: counts[key] = None
    for doc in self.doclist_.Docs():
      for lang in doc.Langs():
        ntokens = []
        for token_ in lang.Tokens():
          hash_string = token_.EncodeForHash()
          if counts[hash_string]: ntokens.append(token_)
        lang.SetTokens(ntokens)

########NEW FILE########
__FILENAME__ = filter_unittest
# -*- coding: utf-8 -*-

## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

"""Unit test for filter class.
"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
hollingk@cslu.ogi.edu (Kristy Hollingshead)
"""

import os
import sys
import xml.sax
import unittest
import xmlhandler
import tokens
import filter
from __init__ import BASE_


SOURCE_FILE_ =  '%s/testdata/doctest.xml' % BASE_ 
GOLDEN_FILE_ =  '%s/testdata/doctest_filt.xml' % BASE_ 
TEST_FILE_ = '/tmp/doctest_filt.xml'

def main(output = False):
  doclist = xmlhandler.XmlHandler().Decode(SOURCE_FILE_)
  filter_ = filter.FrequencyFilter(doclist)
  filter_.SetMinCount(2)
  filter_.Filter()
  doclist = filter_.Doclist()
  if output:
    doclist.XmlDump(GOLDEN_FILE_, utf8 = True)
  else:
    doclist.XmlDump(TEST_FILE_, utf8 = True)
    unittest.TestUnitOutputs(sys.argv[0],\
                             GOLDEN_FILE_, TEST_FILE_)

if __name__ == '__main__':
  if len(sys.argv) > 1 and sys.argv[1] == 'generate':
    main(True)
  else:
    main()

########NEW FILE########
__FILENAME__ = japanese_extractor
# -*- coding: utf-8 -*-

## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

"""Japanese katakana extractor
"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
"""

import extractor
import tokens
import chinese_extractor
import Utils.script
from __init__ import BASE_


class KatakanaExtractor(chinese_extractor.EastAsianExtractor):
  """Katataka sequence extractor
  """

  def LineSegment(self, line):
    try: utext = unicode(line.strip(), 'utf-8')
    except TypeError: utext = line.strip()
    word = []
    for u in utext:
      if Utils.script.CharacterToScript(u) == 'Katakana':
        word.append(u.encode('utf-8'))
      else:
        if word and word != ['・']:
          self.tokens_.append(tokens.Token(''.join(word)))
          word = []


########NEW FILE########
__FILENAME__ = makeindex

## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

import sys

print '<html>'
print '<head>'
print '<title>Pydoc for ScriptTranscriber</title>'
print '</head>'
print '<body>'
for line in sys.stdin.readlines():
  html = line.strip()
  print '<a href="%s">%s</a><br>' % (html, html)
print '</body>'
print '</html>'

########NEW FILE########
__FILENAME__ = mEdit
"""String edit distance computation for two strings in WorldBet using
phonetic distance.

Depends upon the file 'phoneFeature2.txt', which should be included in
this distribution.

NOTE THAT THIS CODE DOES NOT FOLLOW THE (GOOGLE) STYLE CONVENTIONS.
"""

__author__ = """syoon9@uiuc.edu (Su-Youn Yoon)
rws@uiuc.edu (Richard Sproat)"""

import sys

_PHONE_FEATURES_FILE = 'phoneFeature2.txt' 


## Insertion, deletion and substitution costs of features.
## If you add a new feature in the phone features file, then
## the substitution cost should also be added here.

FCDic = {
    # Major features

    'c': 20,     # consonant
    's': 20,     # sonorant

    # Manner features
    'n': 5,      # nasal
    'l': 3,      # lateral
    'cnt': 1.5,  # continuous
    'v': 1.5,    # voicing

    # Place features
    'L': 20,     # Labial
    'C': 7,      # Coronal
    'A': 1.5,    # Anterior
    'D': 8,      # Dorsal
    
    # Vowel feature
    'r': 2,      # round
    'h': 2,      # high
    'w': 2,      # low
    'f': 2,      # front
    'b': 2,      # back
    'rh': 3,     # rhotic - r-coloured vowel
    'g': 8,      # glottalized vowel 
    'lg': 8,     # long vowel 
    'p': 3,      # pharyngealized 
    'nas': 3     # nasalized vowel
    }

## List of all features

featList = FCDic.keys()

LClist = ['L', 'C', 'D']
PClist = ['C', 'D'] 
VClist = ['c', 's', 'r', 'h', 'w', 'f', 'b', 'rh', 'g', 'lg']
jlist  = ['tS', 'tSh', 'tS>', 'dZ', 'c', 'ch', 'cC', 'cCh', 'cC>', 'J']
wlist  = ['v', 'f']
sList  = ['w', 'j']
hlist  = ['h', 'x', 'G', 'X', 'K']


def readFile(file):
    """Reads file and returns lines from file.

    Args:
      string: file name

    Returns:
      list: lines from file
    """

    fin = open(file)
    lines = fin.readlines()
    fin.close()
    return lines


def conTable(phonefile):
    """Read in the file of phone definition and set up the mapping
    between each phone and the associated features.

    Args:
      string: name of file of phones

    Returns:
      a list: lines containing phone, feature maps
    """

    lines = readFile(phonefile)
    nlines = []
    llen = len(lines)
    fList = lines[0].strip().split()
    flen = len(fList)
    for i in range(1, llen):
        x = lines[i].strip().split()
        xlen = len(x)
        if flen != xlen-1:
            sys.stderr.write("Error in phone feature file, line %d.\n" % i)
            sys.exit(1)
        out = ''
        ph = x[0]
        for j in range(1, xlen):
            if fList[j-1] in featList:
                out = ph + '\t' + fList[j-1] + '\t' + x[j] 
                nlines.append(out)
    return nlines


def addPhoneFeatureValue(phone, feat, val, dic):
    """Add a phone/feature/value triple to the dictionary.

    Args:
      string: phone
      string: feat
      string: val
      dictionary: dic

    Returns:
      None
    """

    try:
        dic[phone][feat] = int(val)
    except KeyError:
        dic[phone] = {}
        dic[phone][feat] = int(val)


def loadPFDic(fPF):
    """Loads the phonetic feature dictionary. Each phoneme has
    associated features and values, where the values are one of
    [-1, 0, 1].

    Args:
      string: phonetic feature file name

    Returns:
      dictionary: instantiated dictionary
    """

    PFDic = {} 
    lines = conTable(fPF)
    for line in lines:
        x = line.strip().split()
        phone = x[0]
        feat = x[1]
        val = x[2]
        addPhoneFeatureValue(phone, feat, val, PFDic)
    return PFDic


def loadTargetLanguagePhList(fPL):
    """Load just that subset of phones needed for the language pair in
    question. This is desirable since we will compute an all-pairs
    cost matrix for all relevant phones for the language pair in
    question, and this will be smaller than computing this for the
    entire set of phones.

    Args:
      string: file name for phone subset

    Returns:
      list: phone subset
    """

    phList = []
    lines = readFile(fPL)
    for line in lines:
        x = line.strip().split()
        if not '#' in x[0]:
           if not x[0] in phList: phList.append(x[0])
    return phList
   

def calPhoneToPhoneCost(ph1, ph2, PFDic):
    """Calculate the phoneme substitution/insertion/deletion cost
    based on the substitution/insertion/deletion costs for the
    features.

    Args:
      string: phone 1
      string: phone 2
      dictionary: phonetic feature dictionary

    Returns:
      int: sum
    """

    oph1 = ph1
    oph2 = ph2
    ph1 = ph1.replace('_c', '')
    ph2 = ph2.replace('_c', '')
    sum = 1.5
    # Deletion or insertion cost:
    if ph2 =='NA':            
        if PFDic[ph1]['c'] == 1 :	
            sum += 20
            # Coda consonant:
            if oph1.find('_c') > -1:
                sum = sum - 5 
        else :	
            sum += 8
            # If vowel is low add 2 to cost:
            if PFDic[ph1]['h'] != 1:  
                sum += 2
    # Substitution:
    else:		       
        # ph1 and ph2 are not consonants:
        if (PFDic[ph1]['c']!= 1) and (PFDic[ph2]['c']!=1): 
            for feat in VClist:
                if PFDic[ph1][feat]!=0:
                    # Default value - no cost:
                    if PFDic[ph2][feat]!=0:	
                        # Two values are different:
                        if PFDic[ph1][feat] != PFDic[ph2][feat]:	
                            cost = float(FCDic[feat]) 
                            sum += cost
        # Consonant - consonant, consonant - vowel/semivowel:
        else:	
            for feat in PFDic[ph1]:
                if PFDic[ph1][feat]!=0:
                    # Default value - no cost:
                    if PFDic[ph2][feat]!=0:	
                        # Two values are different:
                        if PFDic[ph1][feat] != PFDic[ph2][feat]:	
                            cost = float(FCDic[feat]) 
                            if (ph1 == 'h') or (ph2 =='h'): 
                                # 'h'-exception: there is no specific
                                # place of articulation for 'h'.
                                # h -> into fricatives:
                                if feat == 'cnt': cost = 10 
                                elif feat == 'D': cost = cost - 5
                            if feat in LClist:
                                # Vowel/semivowel - consonant:
				if ((PFDic[ph1]['c']!=1 and ph1!='h')
                                    or (PFDic[ph2]['c']!=1 and ph2!='h')): 
                                    cost = cost - 5
                                # Palatal consonant substitution :
			        elif ((((PFDic[ph1]['C'] ==1) and
                                        (PFDic[ph1]['D'] ==1)) or
                                       ((PFDic[ph2]['C'] ==1) and
                                        (PFDic[ph2]['D'] ==1))) and
                                      (feat!='L')):
                                    cost = cost - 5 
                            sum += cost
    # Semi-vowel penalty:
    if ((ph1 in sList and PFDic[ph2]['c']==-1) or
        (PFDic[ph1]['c']==-1 and ph2 in sList)):
        sum += 5  
    elif (((ph1 == 'j') and (ph2 in jlist)) or
          ((ph2=='j') and (ph1 in jlist))):
        sum = 5
    elif (((ph1 == 'w') and (ph2 in wlist)) or
          ((ph2=='w') and (ph1 in wlist))):
        sum = 5
    # Nasal substitution cost in coda position: since nasal is
    # frequently substituted in coda position, the high substitution 
    # costs of place features are adjusted:
    elif ((PFDic[ph1]['n']==1) and
          (PFDic[ph2]['n']==1) and
          ((oph2.find('_c') > -1) and (oph1.find('_c') > -1))):
        sum = 10 
    return (sum)


def getCDic(fPL, PFDic, writeFile=None):
    """Sets the language (pair) specific phone-to-phone cost
    mappings. Makes the cost table for all possible insertions,
    deletions and substitutions.

    Args:
      string:  file name for phone subset
      dictionary: phonetic feature dictionary
      string: file to write output (default: None)

    Returns:
      dictionary: language-pair specific cost mapping
    """

    CDic =  {}	
    phList = loadTargetLanguagePhList(fPL)
    for ph1 in phList:
        if ph1!='NA':
           for ph2 in phList:
               cost = calPhoneToPhoneCost(ph1, ph2, PFDic)
               key = (ph1, ph2)
               CDic[key] = cost
    if writeFile:
        fout = open(writeFile, 'w')
        for key in CDic:
            fout.write('%s\t%s\t%d\n' % (key[0],  key[1], CDic[key]))
        fout.close()
    return CDic	


def fMin(j, k, dic, CM, sp, tp):
    """Calculate insertion cost.

    Args:
      dictionary: cost dictionary
      dictionary: ...Add explanation
      dictionary: ...Add explanation
      string: source phone
      string: target phone

    Returns:
      int: minimum cost
      string: action
    """

    if j == 0 and k==0: return(0, 'N')
    act = 'I'
    if j> 0:
       cost = dic[(tp, 'NA')]
       Min = CM[j-1][k] + cost
    else: Min = 1000
    # calculate deletion cost
    if k > 0:
      cost = dic[(sp, 'NA')]
      deL = CM[j][k-1] + cost
      if Min > deL:
	 act = 'D'
	 Min = deL
    if j> 0 and k > 0:
       cost = dic[(tp, sp)]
       sub = CM[j-1][k-1] + cost
       if Min > sub:
	  act = 'S'
	  Min = sub
    return (Min, act)


def getIcost(AM, CM, Tnlen, Snlen):
    """Get the minimum edit distance of the initial phonemes.
    Ins/del/subs rarely occur at word-initial position, and the
    pair with the high initial cost is usually incorrect pair. 
    Therefore, double the initial cost.

    Args:
      dictionary: ...Add explanation
      dictionary: ...Add explanation
      int: target length
      int: source length

    Returns:
      int: cost
    """

    state = AM[Tnlen-1][Snlen-1]
    pcost = CM[Tnlen-1][Snlen-1]
    x = Tnlen - 1
    y = Snlen - 1
    while state != 'N':
	  if state == 'S':
	        x = x - 1
	        y = y - 1
    	  elif state == 'I':
	   	x = x - 1
	  elif state == 'D':
	   	y = y - 1
          ## this fails sometimes for unclear reasons:
          try: state = AM[x][y]
          except IndexError: state = 'N'
          try:
              cost = pcost - CM[x][y]
              pcost = CM[x][y]
          except IndexError: state = 'N'
    icost = cost
    return icost
    

def mEdit(srcPronString, tarPronString, CDic, PFDic):
    """Get minimum edit distance cost for two strings.

    Args:
      string: source pronunciation string
      string: target pronunciation string
      dictionary: source-target pair phone dictionary
      dictionary: whole phonetic feature dictionary

    Returns:
      int: edit distance      
    """

    ini = ['NA']
    src = ini + srcPronString.strip().split()
    tar = ini + tarPronString.strip().split()
    Snlen = len(src)
    Tnlen = len(tar)
  
    # For minimum edit distance:
    # record the cost ;
    CM = [] 
    # record the action of minimum cost
    # (deletion: D, insertion:I, substitution:S).
    AM = [] 
            
    for j in range(Tnlen):
	CM.append([])
	AM.append([])
        for k in range(Snlen):
	    CM[j].append([])
	    AM[j].append([])
	
    # Fill CM and AM
    for j in range(Tnlen):
        for k in range(Snlen):
	  # set value of the first column, row as '0'
	  sp = src[k]
	  tp = tar[j]
	  (cost, act) = fMin(j, k, CDic, CM, sp, tp)
	  CM[j][k] = cost
	  AM[j][k] = act

    # calculate the distance between two strings, and compare with threshold.
    icost = getIcost(AM, CM, Tnlen, Snlen)
    # doubling initial cost
    dist = float((CM[Tnlen-1][Snlen-1] + icost) / max(Tnlen-1, Snlen-1)) 

    # In order to solve Festival error - final phoneme. 
    # Obstruent consonants such as [s,z,k...] are not pronounced
    # at word-final position frequently,  but the pronunciation
    # obtained by Festival includes these phonemes. 
    # ex> Phillipines 
    if (PFDic[tar[-1]]['s'] == -1 and
        len(src) > 5 and len(tar) > 5 and
        CM[Tnlen-1][Snlen-1] > CM[Tnlen-2][Snlen-1]): 
        dist = float((CM[Tnlen-2][Snlen-1])/ max(Tnlen-2, Snlen-1))
    return dist 


def init(phone_subset):
    """Initialize dictionaries.

    Args:
      string: phonetic feature file name

    Returns:
      dictionary: source-target pair phone dictionary
      dictionary: whole phonetic feature dictionary
    """

    PFDic = loadPFDic(_PHONE_FEATURES_FILE)
    CDic = getCDic(phone_subset, PFDic)
    return CDic, PFDic

########NEW FILE########
__FILENAME__ = miner
# -*- coding: utf-8 -*-

## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

"""
"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
"""

import sys
import os
import getopt
import xml.sax.handler
import documents
import tokens
import token_comp
import extractor
import chinese_extractor
import thai_extractor
import pronouncer
import morph
import filter
import def_pronouncers
from __init__ import BASE_

DEF_BAD_COST_ = 6.0

DEF_MINCNT_ = 20

USAGE_      = """
  -l/--list=[file list] 
  -b/--base=[directory base] (default: .)
  -x/--xml_dump=[location of xml dump file (default: /tmp/pid.xml)]
  -p/--phonetic_match_dump=[phonetic match dump (def: /tmp/pid.pmatch)]
  -t/--temporal_match_dump=[temporal match dump (def: /tmp/pid.tmatch)]
  -m/--min_count=[minimum corpus count for keeping term (def: %d)]
  -b/--bad_cost=[minimum cost to keep phonetic match (def: %.1f)]
""" % (DEF_MINCNT_, DEF_BAD_COST_)
USAGE_       = "Usage: %s [args]\n  Arguments:" + USAGE_


def LoadData(filelist,
             base='.',
             extractor_=extractor.NameExtractor,
             xdump=None,
             mincnt=DEF_MINCNT_):
  lastgroup = -1
  lastlanguage = ''
  doc = None
  lang = None
  doclist = documents.Doclist()
  xtractr = extractor_()
  sys.stderr.write('Extracting terms...\n')
  fp = open(filelist)
  for line in fp:
    toks = line.split()
    group = int(toks[0])
    language = toks[1]
    files = toks[2:]
    if group != lastgroup:
      if lastgroup > 0:
        assert group == lastgroup + 1,\
            'Failed sanity check: group %d != group %d + 1' % (group, lastgroup)
      doc = documents.Doc()
      doclist.AddDoc(doc)
    if language != lastlanguage:
      if lang:
        lang.CompactTokens()
      lang = tokens.Lang()
      lang.SetId(language)
      doc.AddLang(lang)
    for file in files:
      file = base + '/' + file
      xtractr.InitData()
      xtractr.FileExtract(file)
      for t in xtractr.Tokens():
        lang.AddToken(t)
    lastgroup = group
    lastlanguage = language
    #lines = p.readlines()
  fp.close()
  if mincnt > 0:
    sys.stderr.write('Filtering to remove terms less frequent than %d...\n' %
                     mincnt)
    filter_ = filter.FrequencyFilter(doclist)
    filter_.SetMinCount(mincnt)
    filter_.Filter()
  if xdump:
    sys.stderr.write('Dumping doclist to %s...\n' % xdump)
    doclist.XmlDump(xdump, utf8 = True)    
  return doclist


def Pronounce(doclist, xdump=None):
  cached_prons = {}
  did = 0
  for doc in doclist.Docs():
    for lang in doc.Langs():
      lid = lang.Id()
      sys.stderr.write('Pronouncing doc %d, language %s\n' % (did, lid))
      for t in lang.Tokens():      
        try:
          prons = cached_prons[t.String()]
          for p in prons:
            t.AddPronunciation(p)
        except KeyError:
          for pronouncer_ in def_pronouncers.DefPronouncers(lid):
            p = pronouncer_(t)
            p.Pronounce()
          cached_prons[t.String()] = t.Pronunciations()
    did += 1
  if xdump:
    sys.stderr.write('Dumping doclist to %s...\n' % xdump)
    doclist.XmlDump(xdump, utf8 = True)    


def CompLT(i, j):
  if i < j: return 1
  if i == j: return 0
  if i > j: return -1


def CompGT(i, j):
  if i < j: return -1
  if i == j: return 0
  if i > j: return 1


def Comparator(doclist,
               comparator=token_comp.OldPhoneticDistanceComparator,
               pdump=None,
               bad_cost=DEF_BAD_COST_,
               comp=CompLT):
  matches = {}
  did = 0
  if comparator == token_comp.TimeCorrelator:
    stats = tokens.DocTokenStats(doclist)
  for doc in doclist.Docs():
    k = len(doc.Langs())
    for i in range(k):
      for j in range(i+1, k):
        lang1 = doc.Langs()[i]
        lang2 = doc.Langs()[j]
        sys.stderr.write('Computing comparison for doc %d, l1=%s, l2=%s\n'
                         % (did, lang1.Id(), lang2.Id()))
        for t1 in lang1.Tokens():
          hash1 = t1.EncodeForHash()
          for t2 in lang2.Tokens():
            hash2 = t2.EncodeForHash()
            try:
              result = matches[(hash1, hash2)] ## don't re-calc
            except KeyError:
              if comparator == token_comp.TimeCorrelator:
                comparator_ = comparator(t1, t2, stats)
              else:
                comparator_ = comparator(t1, t2)
              comparator_.ComputeDistance()
              result = comparator_.ComparisonResult()
              matches[(hash1, hash2)] = result
    did += 1
  values = matches.values()
  values.sort(lambda x, y: comp(y.Cost(), x.Cost()))
  if pdump:
    sys.stderr.write('Dumping comparisons to %s...\n' % pdump)
    p = open(pdump, 'w') ## zero out the file
    p.close()
    for v in values:
      if comp(v.Cost(), bad_cost) < 0: break
      v.Print(pdump, 'a')


def Usage(argv):
  sys.stderr.write(USAGE_ % argv[0])
  sys.exit(2)


def main(argv):
  list = ''
  bad_cost = DEF_BAD_COST_
  mincnt = DEF_MINCNT_
  pid = os.getpid()
  xdump = '/tmp/%d.xml' % pid
  pdump = '/tmp/%d.pmatch' % pid
  tdump = None
  base = '.'
  try:
    opts, args = getopt.getopt(argv[1:],
                               "l:b:x:p:t:m:B:",
                               ["list=", "base=",
                                "xml_dump=", "phonetic_match_dump=",
                                "temporal_match_dump=",
                                "min_count=", "bad_cost="])
  except getopt.GetoptError:
    Usage(argv)
  for opt, arg in opts:
    if opt in ("-l", "--list"):
      list = arg
    elif opt in ("-b", "--base"):
      base = arg
    elif opt in ("-x", "--xml_dump"):
      xdump = arg
    elif opt in ("-p", "--phonetic_match_dump"):
      pdump = arg
    elif opt in ("-t", "--temporal_match_dump"):
      tdump = arg
    elif opt in ("-m", "--min_count"):
      mincnt = float(arg)
    elif opt in ("-B", "--bad_cost"):
      bad_cost = float(arg)
  if not list: Usage(argv)
  doclist = LoadData(list,
                     base=base,
                     extractor_=extractor.NameExtractor,
                     xdump=xdump,
                     mincnt=mincnt)
  Pronounce(doclist,
            xdump=xdump)
  Comparator(doclist,
             pdump=pdump,
             bad_cost=bad_cost,
             comp=CompLT)
  if tdump:
    Comparator(doclist,
               pdump=tdump,
               comparator=token_comp.TimeCorrelator,
               bad_cost=0.1 ** 10,
               comp=CompGT)


if __name__ == '__main__':
  main(sys.argv)

########NEW FILE########
__FILENAME__ = morph
## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

"""Definition of a morph analyzer. Defines one base classe:

MorphAnalyzer: class of methods to preprocess a doclist
to produce data for morphological analysis, and populate the morph
fields of the tokens
"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
"""

import Utils.script

DEFAULT_SUBSTRING_LENGTH_ = 5

class MorphAnalyzer:
  """Preprocesses the entire doclist to produce data to use in
  morphological analysis. In general, since morphological analysis may
  depend upon context, or else
  """
  def __init__(self, doclist, lang):
    self.doclist_ = doclist
    self.mapping_table_ = {}
    self.lang_ = lang
    self.initialized_ = False

  def ProcessDoclist(self):
    pass

  def LabelDoclist(self):
    pass

  def Morphs(self, string):
    return ''

class PrefixAnalyzer(MorphAnalyzer):
  """Follow Alex Klementiev's approach and cluster words together that
  share a prefix of 5 or more letters.

  TODO(rws): fix the substring match so it's right
  """

  def Initialize(self, type='prefix'):
    try: self.substring_length_ == DEFAULT_SUBSTRING_LENGTH_
    except AttributeError: self.substring_length_ = DEFAULT_SUBSTRING_LENGTH_
    vocab = {}
    self.morphs_ = {}
    for doc in self.doclist_.Docs():
      for lang in doc.Langs():
        if lang.Id() == self.lang_:
          for token in lang.Tokens():
            str = token.String()
            for k in range(self.substring_length_, len(str) + 1):
              if type == 'prefix': sub = str[:k]
              else: sub = str[-k:]
              sub = Utils.script.Lower(sub)
              try: 
                if str not in vocab[sub]:
                  vocab[sub].append(str)
              except KeyError:
                vocab[sub] = [str]
    for sub in vocab:
      if len(vocab[sub]) > 1: ## otherwise not interesting
        for str in vocab[sub]:
          try:
            if len(self.morphs_[str]) < len(sub):
              self.morphs_[str] = sub
          except KeyError:
            self.morphs_[str] = sub
    self.initialized_ = True

  def SetSubstringLength(self, length=DEFAULT_SUBSTRING_LENGTH_):
    self.substring_length_ = length
  
  def Morphs(self, string):
    try: return self.morphs_[string]
    except AttributeError, KeyError: return ''

  def LabelDoclist(self):
    assert self.initialized_ == True, 'Must Initialize() the analyzer!'
    for doc in self.doclist_.Docs():
      for lang in doc.Langs():
        if lang.Id() == self.lang_:
          for token in lang.Tokens():
            try:
              morph = self.morphs_[token.String()]
              token.SetMorphs([morph])
            except KeyError:
              pass

########NEW FILE########
__FILENAME__ = morph_unittest
# -*- coding: utf-8 -*-

## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

"""Unit test for morph.py
"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
"""

import os
import sys
import xml.sax
import unittest
import tokens
import xmlhandler
import morph
from __init__ import BASE_

SOURCE_FILE_ =  '%s/testdata/morphtest_src.xml' % BASE_ 
GOLDEN_FILE_ =  '%s/testdata/morphtest.xml' % BASE_ 
TEST_FILE_ = '/tmp/morphtest.xml'

def main(output = False):
  parser = xml.sax.make_parser()
  handler = xmlhandler.XmlHandler()
  parser.setContentHandler(handler)
  parser.parse(SOURCE_FILE_)
  doclist = handler.DocList()
  analyzer = morph.PrefixAnalyzer(doclist, 'eng')
  analyzer.Initialize()
  analyzer.LabelDoclist()
  if output:
    doclist.XmlDump(GOLDEN_FILE_, utf8=True)
  else:
    doclist.XmlDump(TEST_FILE_)
    unittest.TestUnitOutputs(sys.argv[0], GOLDEN_FILE_, TEST_FILE_)

if __name__ == '__main__':
  if len(sys.argv) > 1 and sys.argv[1] == 'generate':
    main(True)
  else:
    main()


########NEW FILE########
__FILENAME__ = paper_example
# -*- coding: utf-8 -*-

## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

"""Sample transcription extractor based on the LCTL Thai parallel
data. Also tests Thai prons and alignment.
"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
"""

import sys
import os
import documents
import tokens
import extractor
import chinese_extractor
import pronouncer
import unittest
from __init__ import BASE_

## A tiny example from the ISI parallel data:

ENGLISH_       = '%s/testdata/chinese_test_eng.txt' % BASE_
CHINESE_       = '%s/testdata/chinese_test_chi.txt' % BASE_
XML_FILE_      = '%s/testdata/chinese_test.xml' % BASE_
TMP_XML_FILE_  = '/tmp/chinese_test.xml'
MATCH_FILE_    = '%s/testdata/chinese_test.matches' % BASE_
TMP_MATCH_FILE_ = '/tmp/chinese_test.matches'
BAD_COST_      = 6.0

def LoadData():
  t_extr = chinese_extractor.ChineseExtractor()
  e_extr = extractor.NameExtractor()
  doclist = documents.Doclist()
  doc = documents.Doc()
  doclist.AddDoc(doc)
  #### Chinese
  lang = tokens.Lang()
  lang.SetId('zh')
  doc.AddLang(lang)
  t_extr.FileExtract(CHINESE_)
  lang.SetTokens(t_extr.Tokens())
  lang.CompactTokens()
  for t in lang.Tokens():
    pronouncer_ = pronouncer.HanziPronouncer(t)
    pronouncer_.Pronounce()
  #### English
  lang = tokens.Lang()
  lang.SetId('en')
  doc.AddLang(lang)
  e_extr.FileExtract(ENGLISH_)
  lang.SetTokens(e_extr.Tokens())
  lang.CompactTokens()
  for t in lang.Tokens():
    pronouncer_ = pronouncer.EnglishPronouncer(t)
    pronouncer_.Pronounce()
  return doclist


def main():
  doclist = LoadData()
  doclist.XmlDump(XML_FILE_, utf8 = True)

if __name__ == '__main__':
  main()

########NEW FILE########
__FILENAME__ = perceptron
# -*- coding: utf-8 -*-

## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

"""Definition for a SNoW-based peceptron model.
"""

__author__ = """
ting.qian@rochester.edu (Ting Qian)
"""
import os
import shutil
import snow
import random
import copy

class Perceptron:
  """A Perceptron model
  """  
  def __init__(self, network_file=None):
    """Constructor, with an optional parameter accepting the name of a network file.
    If the network file exists, a trained perceptron will be constructed,
    and is available for testing. If the file does not exist, network file generated
    during training will be written to its content.
    """
    if network_file is not None:
      self.network_file_ = network_file
      if os.path.exists(network_file):
        self.isTrained_ = True
      else:
        self.isTrained_ = False
    else:
      self.isTrained_ = False
      self.network_file_ = 'perceptron.net'

    self.cut_off_value_ = 2
    self.iteration_ = 2
    self.snow_session_ = None
    self.snow_session_retrained_ = False

  def TrainFromFile(self, train_file):
    """Train a perceptron model given input from the file "train_file", which
    should consist of examples formatted as SNoW input (e.g. 1, 232, 421, 121).
    Return True if the model is successfully trained.
    """
    # using PySnow to train the model
    snow_training_args = {'I':train_file,\
                          'F':self.network_file_,\
                          'P':':0,1',\
                          'r':str(self.iteration_),\
                          'e':'count:' + str(self.cut_off_value_),\
                          'g':'-', \
                          'v':'off'
                          }
    snow.train(snow_training_args)

    # flag the perceptron as trained
    self.isTrained_ = True
    return True

  def IncremLearn(self, increm_file):
    """Learn from additional examples, formatted as SNoW input.
    Return True if the model successfully learns new features.
    """
    # using PySnow to test the file with 'i +' argument
    # for incremental learning
    snow_learning_args = {'I': increm_file, \
                          'F': self.network_file_, \
                          'i': '+', \
                          'o': 'accuracy', \
                          'v': 'off'
                          }
    snow.test(snow_learning_args)
    shutil.move(self.network_file_ + '.new', self.network_file_)

    self.snow_session_retrained_ = True

    return True

  def TestActivation(self, example):
    """Test a given example formatted as SNoW input.
    Return a tuple of activated target and activation, in the order as mentioned.
    """
    # snow testing arguments
    snow_test_args = {'F': self.network_file_,\
                      'o': "allactivations",\
                      'v': "off",
                      'P': ":0-1"
                      }

    if self.snow_session_ is None:
      self.snow_session_ = snow.SnowSession(snow.MODE_SERVER, snow_test_args)
      result = self.snow_session_.evaluateExample(example)
    else:
      if self.snow_session_retrained_:
        tmp_session = snow.SnowSession(snow.MODE_SERVER, snow_test_args)
        result = tmp_session.evaluateExample(example)
        self.snow_session_ = tmp_session
        self.snow_session_retrained_ = False
      else:
        result = self.snow_session_.evaluateExample(example)
 
    try: [target, a, b, activation] = result.split('\n')[1].split()
    except IndexError: return (0, 0)

    # ad-hoc case:
    if activation[-1] == '*':
      activation = activation[:-1]

    return (int(target[0]), float(activation))

  def IsTrained(self):
    """Return whether a model is trained or not.
    """
    return self.isTrained_

  def CleanUp(self):
    """Remove network files of the current trained model.
    """
    if self.isTrained_:
      del self.snow_session_
      os.remove(self.network_file_)
      self.isTrained_ = False

class FeatureMap:
  """A feature definition class, implemented as a hash (dictionary).
  Note: for maximum compatibility, please convert all feature contents (as opposed to feature IDs)
  to strings before hashing.
  """
  def __init__(self, feature_map=None):
    """Constructor, with an optional parameter to load an existing feature map.
    """
    # feature_dic contains a hashtable with features as keys
    # feature ids as values
    if feature_map is None:
      self.feature_dic_ = {}
      self.cur_id_ = 10
    else:
      self.feature_dic_ = feature_map
      self.cur_id_ = max(self.feature_dic_.values())

  def AddFeature(self, feature):
    """Add a new feature to the map.
    Return the newly assigned ID of that feature; if the given feature exists,
    return the currently assigned ID.
    """
    if feature not in self.feature_dic_:
      self.cur_id_ += 1
      self.feature_dic_[feature] = self.cur_id_
      return self.cur_id_
    else:
      return self.feature_dic_[feature]

  def GetFeatureDic(self):
    """Return a copy of the hash/dictionary held by a FeatureMap object.
    """
    return self.feature_dic_

  def DumpToFile(self, feature_map_file):
    """Dump the entire feature map to a file whose name is given as the parameter.
    """
    fm_fp = open(feature_map_file, 'w')
    for k, v in self.feature_dic_.iteritems():
      fm_fp.write(k + '\t' + str(v) + '\n')
    fm_fp.close()
    return True
  

########NEW FILE########
__FILENAME__ = perceptron_trainer
# -*- coding: utf-8 -*-

## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

"""A string-based discrinimative model for transcribing between
pairs of source and target tokens
"""

__author__ = """
ting.qian@rochester.edu (Ting Qian)
"""

import perceptron
import random
import os
import tempfile

DEBUG_ = False

def Distance(a, b):
  """Function that calculates the Levenshtein distance between any two
  indexable objects.
  """
  c = {}
  n = len(a); m = len(b)
  
  for i in range(0, n + 1):
    c[i, 0] = i
  for j in range(0, m + 1):
    c[0, j] = j

  for i in range(1, n + 1):
    for j in range(1, m + 1):
      x = c[i - 1, j] + 1
      y = c[i, j - 1] + 1
      if a[i - 1] == b[j - 1]:
        z = c[i - 1, j - 1]
      else:
        z = c[i - 1, j - 1] + 2
      c[i, j] = min(x, y, z)
  return c[n, m] 

class ParallelTrainer:
  """A perceptron class for training a discriminative model from a parallel dictionary.
  """
  def __init__(self, feature_map_file=None, network_file=None):
    """Constructor. Optional arguments are the filenames of the feature map
    and the network file (existing or to be generated).
    If no parameters are given, default names will be used (not recommended).
    """
    if feature_map_file is not None:
      self.feature_map_file_ = feature_map_file
      try: self.feature_map_ = perceptron.FeatureMap(self.LoadFeatureMap(feature_map_file))
      except: self.feature_map_ = perceptron.FeatureMap()
    else:
      self.feature_map_file_ = 'perceptron.featuremap'
      self.feature_map_ = perceptron.FeatureMap()

    if network_file is not None:
      self.snow_p_ = perceptron.Perceptron(network_file)
    else:
      self.snow_p_ = perceptron.Perceptron()

    self.token2_tri_index_ = {}

  def TrigramSubstr(self, str):
    """Return all possible letter trigrams for a string.
    """
    trigram_substrs = []
    str = '_ ' + str + ' _'
    chars = str.split()
    for i in range(len(chars) - 2):
      trigram_substrs.append(chars[i] + chars[i + 1] + chars[i + 2])
    return trigram_substrs

  def Train(self, pos_examples_list):
    """Given a list of positive examples, generate random negative examples,
    and train a perceptron model.
    Input: a list of positive examples in the form of tuples (token1, token2).
    Output: True if the process is successful. Feature map and network file are
    dumped to the current directory.
    """
    # if the peceptron is already trained, warn and abort
    if self.snow_p_.IsTrained():
      if DEBUG_: print 'Perceptron already trained (use Retrain?)'
      return False

    for example in pos_examples_list:
      for tri in self.TrigramSubstr(example[1]):
        try: self.token2_tri_index_[tri].add(example[1])
        except KeyError:
          self.token2_tri_index_[tri] = set()
          self.token2_tri_index_[tri].add(example[1])        
     
    train_examples = []

    # get positive examples
    for pair in pos_examples_list:
      temp_ex = TRExample(pair[0], pair[1], 1)
      for sc in temp_ex.GenerateCouplings():
        temp_ex.AddFeatureID(self.feature_map_.AddFeature(str(sc)))
      train_examples.append(temp_ex)

    # shuffle to create negative examples
    neg_examples_list = self.ShuffleWords(pos_examples_list)
    for pair in neg_examples_list:
      temp_ex = TRExample(pair[0], pair[1], 0)
      for sc in temp_ex.GenerateCouplings():
        temp_ex.AddFeatureID(self.feature_map_.AddFeature(str(sc)))
      train_examples.append(temp_ex)

    # write the training examples to a file
    random.shuffle(train_examples)
    train_file = tempfile.mkstemp()[1]
    net_fp = open(train_file, 'w')
    for ex in train_examples:
      net_fp.write(ex.DisplayFeatureID())
    net_fp.close()

    # train the model
    self.snow_p_.TrainFromFile(train_file)

    try: os.remove(train_file)
    except: pass
    self.DumpFeatureMap()
    
    return True

  def Retrain(self, new_positives):
    """Incrementally learn from new examples (require a trained perceptron)
    Input: a list of positive examples in the form of tuples (token1, token2).
    Output: True if successful, False otherwise. Feature map and network file
    are updated/created.
    """
    # if the perceptron has not been trained, warn and abort
    if not self.snow_p_.IsTrained():
      if DEBUG_: print 'Perceptron is not trained (use Train?)'
      return False
    
    for example in new_positives:
      for tri in self.TrigramSubstr(example[1]):
        try: self.token2_tri_index_[tri].add(example[1])
        except KeyError:
          self.token2_tri_index_[tri] = set()
          self.token2_tri_index_[tri].add(example[1])     
    
    train_examples = []
    
    # get positive examples
    for pair in new_positives:
      temp_ex = TRExample(pair[0], pair[1], 1)
      for sc in temp_ex.GenerateCouplings():
        temp_ex.AddFeatureID(self.feature_map_.AddFeature(str(sc)))
      train_examples.append(temp_ex)

    # shuffle to create negative examples
    new_negatives = self.ShuffleWords(new_positives)
    for pair in new_negatives:
      temp_ex = TRExample(pair[0], pair[1], 0)
      for sc in temp_ex.GenerateCouplings():
        temp_ex.AddFeatureID(self.feature_map_.AddFeature(str(sc)))
      train_examples.append(temp_ex)

    # append the training examples
    learning_file = tempfile.mkstemp()[1]
    random.shuffle(train_examples)
    net_fp = open(learning_file, 'w')
    for ex in train_examples:
      net_fp.write(ex.DisplayFeatureID())
    net_fp.close()

    # retrain the model
    self.snow_p_.IncremLearn(learning_file)

    try: os.remove(learning_file)
    except: pass
    self.DumpFeatureMap()

    return True

  def Evaluate(self, s_token, t_token):
    """Evaluate a pair of test case.
    Return: a tuple of activated target and activation, in the order as mentioned.
    """
    if not self.snow_p_.IsTrained():
      if DEBUG_: print 'Perceptron not trained'
      return False
    
    test_ex = Example(s_token, t_token)
    for sc in test_ex.GenerateCouplings():
      try: test_ex.AddFeatureID(self.feature_map_.GetFeatureDic()[str(sc)])
      except KeyError: pass
    return self.snow_p_.TestActivation(test_ex.DisplayFeatureID())

  def DumpFeatureMap(self):
    """Dump the entire feature to the current directory. Helper method.
    """
    try:
      self.feature_map_.DumpToFile(self.feature_map_file_)
      return True
    except:
      return False

  def LoadFeatureMap(self, feature_map_file):
    """Load a feature map object from a feature map file. Helper method.
    """
    f_map = {}
    fm_fp = open(feature_map_file, 'r')
    for line in fm_fp.readlines():
      [f, fid] = line.strip().split('\t')
      f_map[f] = int(fid)
    return f_map

  def ShuffleWords(self, positives):
    """Create a list of negative examples by mismatching positive ones. Helper method.
    """
    negatives = []
    ps = WordShuffler(positives)
    for size in range(0,4):
      negatives.extend(ps.CreateShuffledList())
    return negatives

  def ShuffleCharacters(self, positives):
    """Create a list of negative examples by shuffling characters in the second token in each pair.
    """
    negatives = []
    for pair in positives:
      chars = pair[1].split()
      random.shuffle(chars)
      new_str = ' '.join(chars)
      negatives.append((pair[0], new_str))
    return negatives

  def NearestNeighbors(self, positives):
    """Create a list of negative examples by matching token1 in each pair with
    four other token2 strings that have shortest editing distance to the original
    token2. Intensive computation.
    """
    negatives = []
    for pair in positives:
      token2 = pair[1]
      candidates = []
      for tri in self.TrigramSubstr(token2):
        try: candidates.extend(self.token2_tri_index_[tri])
        except: pass

      candidates = set(candidates)
      candidates = list(candidates)

      distances = map(lambda x:
                      (x, Distance(x.split(), token2.split())), candidates)
      distances = sorted(distances, lambda x,y: x[1] - y[1])

      for new_str in distances[1:5]:
        negatives.append((pair[0], new_str[0]))
#        print pair[0] + ' <=> ' + new_str[0] + ' : ' + str(new_str[1])
    return negatives

  def CleanUp(self):
    """Remove generated feature maps and network files (i.e. the current
    trained model) from the current directory.
    """
    if self.snow_p_.IsTrained():
      try:
        os.remove(self.feature_map_file_)
      except: pass
      try:
        self.snow_p_.CleanUp()
      except: pass

  def IsTrained(self):
    """Return True if an instance is trained, false otherwise.
    """
    return self.snow_p_.IsTrained()

class WordShuffler:
  """A helper class for shuffling positive examples between columns. Do not use directly.
  """
  def __init__(self, l):
    self.l_ = l
    self.left_els_ = map(lambda x: x[0], self.l_)

  def CreateShuffledList(self):
    shuffled_list = []
    # for each left element, get a different left element,
    # store the new (element, value) tuple/pair in a new list
    for left_e in self.left_els_:
      ind = self.left_els_.index(left_e)
      alt_ind = self.GetAlternativeInd(ind)
      shuffled_list.append((left_e, self.l_[alt_ind][1]))
    return shuffled_list

  def GetAlternativeInd(self, ind):
    alt_ind = ind
    while alt_ind == ind:
      alt_ind = random.randint(0, len(self.left_els_) - 1)
    return alt_ind

class Example:
  """An example, with a pair of source and target tokens,
  and feature set represented as feature IDs.
  """
  def __init__(self, s_token, t_token):
    """Constructor.
    """
    self.s_token_ = s_token
    self.t_token_ = t_token
    self.feature_id_set_ = set()

  def Display(self):
    """Return a string representation of the example.
    """
    return ((self.s_token_, self.t_token_))

  def DisplayFeatureID(self):
    """Return a string representation of the example in feature IDs,
    also a line of SNoW input.
    """
    return ', '.join([str(f) for f in self.feature_id_set_]) + '\n'
  
  def GetSPron(self):
    return self.s_token_

  def GetTPron(self):
    return self.t_token_

  def AddFeatureID(self, f_id):
    """Add a feature ID to this example.
    """
    self.feature_id_set_.add(f_id)

  def GenerateCouplings(self):
    """Return n-gram couplings for the two tokens in this example.
    """
    s_substrs = self.BigramSubstr(self.s_token_)
    t_substrs = self.BigramSubstr(self.t_token_)
    return self.CoupleSubstrs(s_substrs, t_substrs)
  
  def BigramSubstr(self, str):
    """Return all possible letter bigrams for a string.
    """
    bigram_substrs = []
    if not str.find(' '):
      str = ' '.join(list(str))
    str = "_ " + str + " _"
    chars = str.split()
    for i in range(len(chars)-1):
      bigram_substrs.append(chars[i] + chars[i+1])
    return bigram_substrs

  def CoupleSubstrs(self, substrs1,substrs2):
    """Return a set of coupled bigram strings for the current example.
    """
    # coupling set
    c_set = set()
    for i in range(len(substrs1)):
      if (i - 1) >= 0 and (i - 1) < len(substrs2):
        c_set.add((substrs1[i], substrs2[i - 1]))
      if (i) < len(substrs2):
        c_set.add((substrs1[i], substrs2[i]))
      if (i + 1) < len(substrs2):
        c_set.add((substrs1[i], substrs2[i + 1]))
    return c_set

class TRExample(Example):
  """A training example, extending the class Example, with the addition of
  category labels
  """
  def __init__(self, s_token, t_token, cate):
    Example.__init__(self, s_token, t_token)
    self.cate_ = cate

  def DisplayFeatureID(self):
    """Return a string representation of the current example in feature IDS,
    with the category label as the starting feature.
    """
    return str(self.cate_) + ', ' + \
           ', '.join([str(f) for f in self.feature_id_set_]) + \
           '\n'

########NEW FILE########
__FILENAME__ = perceptron_trainer_unittest
# -*- coding: utf-8 -*-

## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

"""Unit test for letter-based perceptron models for parallel dictionaries
"""

__author__ = """
ting.qian@rochester.edu (Ting Qian)
"""

import perceptron_trainer
import os

def GetDictionary():
  training_dict = []

  # read a parallel dictionary into a hash structure
  dict_fp = open("testdata/perceptron.ce.dict.training", "r")
  for line in dict_fp.readlines():
    item = line.strip().split('\t')
    training_dict.append((item[0], item[1]))
  return training_dict

def EvaluateExamples(pt):
  return \
         str(pt.Evaluate("on > r m &", "n u &r m a")) + \
         str(pt.Evaluate("t I k h & n & v I tS", "cC i x a n w o w e i cCh i")) + \
         str(pt.Evaluate("t I k h & n & v I tS", "n u &r m a"))

def main():
  dict = GetDictionary()
  perceptron_trainer.DEBUG_ = True
  # initialize a new perceptron
  pt = perceptron_trainer.ParallelTrainer('2.fm', '2.net')
  
  # train the perceptron
  pt.Train(dict[0:1000])
  first_run = EvaluateExamples(pt)
  print first_run

  # results here should be the same
  second_run = EvaluateExamples(pt)
  print second_run
  
  # learn from new examples
  # produce new results
  pt.Retrain(dict[1001:3000])
  third_run = EvaluateExamples(pt)
  print third_run

  # this result should be the same as the third run
  fourth_run = EvaluateExamples(pt)
  print fourth_run

  # test
  if first_run == second_run and first_run != third_run \
         and third_run == fourth_run:
    print 'unittest successful'
  else:
    print 'unsuccessful'

  # clean up
  pt.CleanUp()

if __name__ == '__main__':
  main()

########NEW FILE########
__FILENAME__ = pronouncer
## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

"""Pronouncer class.

Takes a token as input and calls an appropriate method to fill in the
pronunciation field.
"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
"""

import tokens
import Unitran.unitran
import Utils.chinese
import Utils.kunyomi_new
import Utils.english
import Utils.latin
from __init__ import BASE_

DUMMY_PHONE_ = 'DUM'

UNITRAN_PATH_ = '%s/Unitran' % BASE_
UTILS_PATH_ = '%s/Utils' % BASE_

class Pronouncer:
  def __init__(self, token):
    self.token_ = token

  def InitData(self, data = []):
    return

  def Pronounce(self):
    self.token_.AddPronunciation(self.token_.String())

  def Token(self):
    return self.token_


class UnitranPronouncer(Pronouncer):
  """Uses Unitran to compute the pronunciation of a UTF-8 token. Works
  for any script except Latin and Hanzi/Hanja/Kanji.
  """
  def __init__(self, token):
    self.token_ = token
    self.InitData()
  
  def InitData(self):
    Unitran.unitran.JOINER_ = ' '
    Unitran.unitran.IndicCon= \
        Unitran.unitran.IndicCV('%s/IndicCon.txt' % UNITRAN_PATH_)
    Unitran.unitran.IndicVowel = \
        Unitran.unitran.IndicCV('%s/IndicVowel.txt' % UNITRAN_PATH_)
    Unitran.unitran.IndicComp = \
        Unitran.unitran.IndicTwoPartVowels('%s/IndicTwoPartVowel.txt' % 
                                           UNITRAN_PATH_)
    Unitran.unitran.UpdateTransTable(Unitran.unitran.Tables.TransTable)

  def Pronounce(self):
    pron = Unitran.unitran.ProcToken(self.token_.String())
    if pron and ''.join(pron.split()) != self.token_.String():
      try: pron.encode('ascii')
      except UnicodeDecodeError:
        pron2 = []
        for p in pron.split():
          try: p2 = p.encode('ascii')
          except: p2 = DUMMY_PHONE_
          pron2.append(p2)
        pron = ' '.join(pron2)
      self.token_.AddPronunciation(pron)

class HanziPronouncer(Pronouncer):
  """Mandarin and kunyomi (native Japanese) pronunciations.
  """
  def __init__(self, token):
    self.token_ = token
    self.InitData()
  
  def InitData(self):
    Utils.chinese.LoadMandarinWbTable('%s/Mandarin.wb' % UTILS_PATH_)
    Utils.kunyomi_new.LoadKunyomiWbTable('%s/Kunyomi_new.wb' % UTILS_PATH_)

  def Pronounce(self):
    pronc, successc = Utils.chinese.HanziToWorldBet(self.token_.String())
    pronk, successk = Utils.kunyomi_new.KanjiToWorldBet(self.token_.String())
    if successc:
      try: pronc.encode('ascii')
      except UnicodeDecodeError:
        pronc2 = []
        for p in pronc.split():
          try: p2 = p.encode('ascii')
          except: p2 = DUMMY_PHONE_
          pronc2.append(p2)
        pronc = ' '.join(pronc2)
      self.token_.AddPronunciation(pronc)
    if successk:
      try: pronk.encode('ascii')
      except UnicodeDecodeError:
        pronk2 = []
        for p in pronk.split():
          try: p2 = p.encode('ascii')
          except: p2 = DUMMY_PHONE_
          pronk2.append(p2)
        pronk = ' '.join(pronk2)
      self.token_.AddPronunciation(pronk)

class EnglishPronouncer(Pronouncer):
  """Uses a static list compiled from Festival to look up the word
  """
  def __init__(self, token):
    self.token_ = token
    self.InitData()

  def InitData(self):
    Utils.english.LoadEnglishWbTable('%s/English.wb' % UTILS_PATH_)

  def Pronounce(self):
    pron = Utils.english.EnglishToWorldBet(self.token_.String())
    if pron:
      try: pron.encode('ascii')
      except UnicodeDecodeError:
        pron2 = []
        for p in pron.split():
          try: p2 = p.encode('ascii')
          except: p2 = DUMMY_PHONE_
          pron2.append(p2)
        pron = ' '.join(pron2)
      self.token_.AddPronunciation(pron)

class LatinPronouncer(Pronouncer):
  """Fallback pronouncer for arbitrary latin1
  """
  def __init__(self, token):
    self.token_ = token
    self.InitData()
  
  def InitData(self):
    Utils.latin.LoadLatinWbTable('%s/Latin.wb' % UTILS_PATH_)

  def Pronounce(self):
    pron, success = Utils.latin.LatinToWorldBet(self.token_.String())
    if success:
      try: pron.encode('ascii')
      except UnicodeDecodeError:
        pron2 = []
        for p in pron.split():
          try: p2 = p.encode('ascii')
          except: p2 = DUMMY_PHONE_
          pron2.append(p2)
        pron = ' '.join(pron2)
      self.token_.AddPronunciation(pron)

########NEW FILE########
__FILENAME__ = pronouncer_unittest
# -*- coding: utf-8 -*-

## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

"""Unit test for pronouncer class.
"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
"""

import sys
import tokens
import pronouncer
from __init__ import BASE_

WORDS_ =  [ '﻿ᓵᓕ ᓴᕕᐊᕐᔪ',
            'հայտնում',
            'совместно',
            'πρεσβυτερων',
            ' נצטר',
            'สิริ',
            '曼德尔森',
            '高島屋',
            '동지사',
            'うずまき',
            'ᎠᏓᏔᏍᏘ',
            'cheese',
            'CHEESE',
            'loogoowoogoo',
            'ἄτομος',
            'ადიშის',
            'आदिवासी',
            'ਪੰਜਾਬ',
            'ذِھِنُ',
            'અનુપ',
            'தினபூமி',
            'பரமேஸ்வரன்', ## example involving virama (pulli)
             ]

GOLDEN_FILE_ = '%s/testdata/pronounce_golden.txt' % BASE_
GOLDEN_ = {}


def LoadGolden():
  p = open(GOLDEN_FILE_)
  for line in p:
    line = line.strip()
    word, pron = line.split('\t')
    try: word = unicode(word, 'utf-8')
    except TypeError: pass
    try: pron = unicode(pron, 'utf-8')
    except TypeError: pass
    try:
      GOLDEN_[word].AddPronunciation(pron)
    except KeyError:
      GOLDEN_[word] = tokens.Token(word)
      GOLDEN_[word].AddPronunciation(pron)
  p.close()


def main(output = False):
  if output: file = open(GOLDEN_FILE_, 'w')
  else: LoadGolden()
  for w in WORDS_:
    try: w = unicode(w.strip(), 'utf-8')
    except TypeError: pass
    token_ = tokens.Token(w)
    pronouncer_ = pronouncer.UnitranPronouncer(token_)
    pronouncer_.Pronounce()
    pronouncer_ = pronouncer.HanziPronouncer(token_)
    pronouncer_.Pronounce()
    pronouncer_ = pronouncer.EnglishPronouncer(token_)
    pronouncer_.Pronounce()
    pronouncer_ = pronouncer.LatinPronouncer(token_)
    pronouncer_.Pronounce()
    if output:
      for p in pronouncer_.Token().Pronunciations():
        file.write('%s\t%s\n' % (pronouncer_.Token().String(), p))
    else:
      try:
        string = unicode(pronouncer_.Token().String(), 'utf-8')
      except TypeError:
        string = pronouncer_.Token().String()
      assert string in GOLDEN_, \
          "Can't find string %s in gold standard" % \
            string.encode('utf-8')
      nprons = pronouncer_.Token().Pronunciations()
      gprons = GOLDEN_[string].Pronunciations()
      assert len(nprons) == len(gprons), \
          '# of prons in gold standard differs for %s' % \
            string.encode('utf-8')
      for i in range(len(nprons)):
        assert nprons[i] == gprons[i], \
            'pron %d differs for %s (%s->%s)' % (i,
                                                 string.encode('utf-8'),
                                                 nprons[i],
                                                 gprons[i])
  if output:
    print 'generated %s' % GOLDEN_FILE_
    file.close()
  else:
    print '%s successful' % sys.argv[0]


if __name__ == '__main__':
  if len(sys.argv) > 1 and sys.argv[1] == 'generate':
    main(True)
  else:
    main()

########NEW FILE########
__FILENAME__ = sample
# -*- coding: utf-8 -*-

## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

"""Sample transcription extractor based on the ISI parallel
Chinese/English data.
"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
hollingk@cslu.ogi.edu (Kristy Hollingshead)
"""

import sys
import os
import documents
import tokens
import token_comp
import extractor
import chinese_extractor
import pronouncer
from __init__ import BASE_

## A sample of 10,000 from each:

CHINESE_       = '%s/data/ISI_chi_eng_parallel_corpus.chi' % BASE_
ENGLISH_       = '%s/data/ISI_chi_eng_parallel_corpus.eng' % BASE_
CONFIDENCE_    = '%s/data/ISI_chi_eng_parallel_corpus.score' % BASE_
MINCONFIDENCE_ = 0.90
XML_FILE_      = '%s/data/isi.xml' % BASE_
MATCH_FILE_    = '%s/data/isi.matches' % BASE_
CORR_FILE_    = '%s/data/isi.corr' % BASE_
BAD_COST_      = 6.0

def LoadData():
  mp = open(CHINESE_)
  ep = open(ENGLISH_)
  cp = open(CONFIDENCE_)
  doclist = documents.Doclist()
  while True:
    eline = ep.readline()
    mline = mp.readline()
    cline = cp.readline()
    if not cline: break
    if float(cline.strip()) < MINCONFIDENCE_: continue
    doc = documents.Doc()
    ### Chinese
    extractor_ = chinese_extractor.ChineseExtractor()
    extractor_.InitData()
    extractor_.LineSegment(mline)
    lang = tokens.Lang()
    lang.SetId('zho')
    for t in extractor_.Tokens():
      lang.AddToken(t)
    lang.CompactTokens() ## Combine duplicates
    for t in lang.Tokens():
      pronouncer_ = pronouncer.HanziPronouncer(t)
      pronouncer_.Pronounce()
    doc.AddLang(lang)
    ### English
    extractor_ = extractor.NameExtractor()
    extractor_.InitData()
    extractor_.LineSegment(eline)
    lang = tokens.Lang()
    lang.SetId('eng')
    for t in extractor_.Tokens():
      lang.AddToken(t)
    lang.CompactTokens() ## Combine duplicates
    for t in lang.Tokens():
      pronouncer_ = pronouncer.EnglishPronouncer(t)
      pronouncer_.Pronounce()
      if not t.Pronunciations():
        pronouncer_ = pronouncer.LatinPronouncer(t)
        pronouncer_.Pronounce()
    doc.AddLang(lang)
    doclist.AddDoc(doc)
  mp.close()
  ep.close()
  cp.close()
  return doclist


def ComputePhoneMatches(doclist):
  matches = {}
  for doc in doclist.Docs():
    lang1 = doc.Langs()[0]
    lang2 = doc.Langs()[1]
    for t1 in lang1.Tokens():
      hash1 = t1.EncodeForHash()
      for t2 in lang2.Tokens():
        hash2 = t2.EncodeForHash()
        try: result = matches[(hash1, hash2)] ## don't re-calc
        except KeyError:
          comparator = token_comp.OldPhoneticDistanceComparator(t1, t2)
          comparator.ComputeDistance()
          result = comparator.ComparisonResult()
          matches[(hash1, hash2)] = result
  values = matches.values()
  values.sort(lambda x, y: cmp(x.Cost(), y.Cost()))
  p = open(MATCH_FILE_, 'w') ## zero out the file
  p.close()
  for v in values:
    if v.Cost() > BAD_COST_: break
    v.Print(MATCH_FILE_, 'a')


def ComputeTimeCorrelation(doclist):
  correlates = {}
  stats = tokens.DocTokenStats(doclist)
  for doc in doclist.Docs():
    lang1 = doc.Langs()[0]
    lang2 = doc.Langs()[1]
    for t1 in lang1.Tokens():
      hash1 = t1.EncodeForHash()
      for t2 in lang2.Tokens():
        hash2 = t2.EncodeForHash()
        try: result = correlates[(hash1, hash2)] ## don't re-calc
        except KeyError:
          comparator = token_comp.TimeCorrelator(t1, t2, stats)
          comparator.ComputeDistance()
          result = comparator.ComparisonResult()
          correlates[(hash1, hash2)] = result
  values = correlates.values()
  values.sort(lambda x, y: cmp(x.Cost(), y.Cost()))
  p = open(CORR_FILE_, 'w') ## zero out the file
  p.close()
  for v in values:
    if v.Cost() <= 0.0: continue
    v.Print(CORR_FILE_, 'a')


if __name__ == '__main__':
  doclist = LoadData()
  doclist.XmlDump(XML_FILE_, utf8 = True)
  ComputePhoneMatches(doclist)
  ComputeTimeCorrelation(doclist)
    # this should really only be run for one term at a time!
  

########NEW FILE########
__FILENAME__ = thai_extractor
# -*- coding: utf-8 -*-

## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

"""Extractor for Thai. 

Uses a perceptron-based segmenter trained on the Thai LCTL data from
LDC. However, this tends to oversegment (even though it has 98%
accuracy on the tags...) so we also include the whole space-delimited
token as a candidate.

Something similar should also work for Lao...
"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
"""

import extractor
import tokens
import Utils.script
import snow
from __init__ import BASE_

FEATURES_FILE_ = '%s/Utils/thaifeats.txt' % BASE_
NETWORK_FILE_  = '%s/Utils/thaiseg.net' % BASE_

MIN_CNT_ = 5
MIN_LEN_ = 5
MAX_REASONABLE_SINGLE_WORD_ = 15


class FeatureMap:
  def __init__(self):
    self.table_ = {}
    self.itable_ = {}
    self.curr_label_ = 1
    self.immutable_ = False
  
  def Encode(self, feat):
    try:
      return self.table_[feat]
    except KeyError:
      if self.immutable_: return None
      self.table_[feat] = self.curr_label_
      self.curr_label_ += 1
      return self.table_[feat]

  def Decode(self, val):
    try:
      return self.itable_[val]
    except KeyError:
      return None

  def Dump(self, file):
    keys = self.table_.keys()
    keys.sort(lambda x, y: cmp(self.table_[x], self.table_[y]))
    p = open(file, 'w')
    for k in keys:
      p.write('%s\t%d\n' % (k, self.table_[k]))
    p.close()

  def Load(self, file, immutable = True):
    """By default if we load from a file we assume that the features
    come from an already trained system, so we don't add new features.
    """
    self.immutable_ = immutable
    p = open(file, 'r')
    lines = p.readlines()
    p.close()
    for line in lines:
      (k, v) = line.strip().split('\t')
      v = int(v)
      self.table_[k] = v
      self.itable_[v] = k


def Listify(text):
  """Split out the characters of the text into a list for fast feature
  extraction.
  """
  list = []
  for u in unicode(text, 'utf8'):
    list.append(u.encode('utf8'))
  return list


def FeatureExtract(i, corpus, fmap):
  try: l4 = corpus[i-4]
  except IndexError: l4 = '#'
  try: l3 = corpus[i-3]
  except IndexError: l3 = '#'
  try: l2 = corpus[i-2]
  except IndexError: l2 = '#'
  try: l1 = corpus[i-1]
  except IndexError: l1 = '#'
  try: r1 = corpus[i+1]
  except IndexError: r1 = '#'
  try: r2 = corpus[i+2]
  except IndexError: r2 = '#'
  try: r3 = corpus[i+3]
  except IndexError: r3 = '#'
  try: r4 = corpus[i+4]
  except IndexError: r4 = '#'
  t = corpus[i]
  ifeats = []
  ifeats.append(fmap.Encode('%s%s%s%s_' % (l4,l3,l2,l1)))
  ifeats.append(fmap.Encode('%s%s%s_%s' % (l3,l2,l1,r1)))
  ifeats.append(fmap.Encode('%s%s_%s%s' % (l2,l1,r1,r2)))
  ifeats.append(fmap.Encode('%s_%s%s%s' % (l1,r1,r2,r3)))
  ifeats.append(fmap.Encode('_%s%s%s%s' % (r1,r2,r3,r4)))
  ifeats.append(fmap.Encode('%s%s%s_' % (l3,l2,l1)))
  ifeats.append(fmap.Encode('%s%s_%s' % (l2,l1,r1)))
  ifeats.append(fmap.Encode('%s_%s%s' % (l1,r1,r2)))
  ifeats.append(fmap.Encode('_%s%s%s' % (r1,r2,r3)))
  ifeats.append(fmap.Encode('%s%s_' % (l2,l1)))
  ifeats.append(fmap.Encode('%s_%s' % (l1,r1)))
  ifeats.append(fmap.Encode('_%s%s' % (r1,r2)))
  ifeats.append(fmap.Encode('%s' % t))
  feats = []
  for f in ifeats:
    if f == None: continue
    feats.append(f)
  return feats


class ThaiExtractor(extractor.Extractor):
  """Extractor of potential Thai terms.
  """

  def FileExtract(self, filename):
    """This is like EastAsianExtractor but also keeps tabs on how many
    times each segment occurs in a text.
    """
    fp = open(filename, 'r')
    nlines = []
    for line in fp.readlines():
      line = line.strip()
      if line == '': line = ' ' ## Keep empty line
      nlines.append(line)
    text = ''.join(nlines)
    for line in text.split():
      self.LineSegment(line)
    return self.tokens_

  def LineSegment(self, line):
    """Replace this with the call to SNoW. Segment before a char if it
    is classified as B or I. After if it is classified as I or
    F. Conflicts are:
    
    F M
    F F
    M I
    M B
    B F
    B M
    I B

    """
    try: self.feature_map_
    except AttributeError:
      self.feature_map_ = FeatureMap()
      self.feature_map_.Load(FEATURES_FILE_)
    try: self.snow_session_
    except AttributeError:
      snow_test_args = {'F':NETWORK_FILE_,\
                        'o':"allactivations",\
                        'v':"off"
                        }
      self.snow_session_ = snow.SnowSession(snow.MODE_SERVER,
                                            snow_test_args)
    try: utext = unicode(line.strip(), 'utf-8')
    except TypeError: utext = line.strip()
    segments = utext.split()
    for segment in segments:
      if len(segment) < MAX_REASONABLE_SINGLE_WORD_:
        t = tokens.Token(segment) ## the whole segment too...
        self.tokens_.append(t)
      seglist = Listify(segment.encode('utf8'))
      features = []
      for i in range(len(seglist)):
        feats = ', '.join(map(lambda x: str(x),
                              FeatureExtract(i, seglist,
                                             self.feature_map_))) + ':\n'
        result = self.snow_session_.evaluateExample(feats)
        target, a, b, activation = result.split('\n')[1].split()
        target = int(target[:-1]) ## remove ':'
        if activation[-1] == '*':
          activation = activation[:-1]
        activation = float(activation)
        feature = self.feature_map_.Decode(target)
        features.append((feature, target, activation))
      tok = ''
      for i in range(len(seglist)):
        if features[i][0] == 'B' or features[i][0] == 'F':
          tok += seglist[i]
          t = tokens.Token(tok)
          self.tokens_.append(t)
          tok = ''
        elif features[i][0] == 'I':
          if tok:
            t = tokens.Token(tok)
            self.tokens_.append(t)
            tok = ''
          tok += seglist[i]
        else:
          tok += seglist[i]
      if tok:
        t = tokens.Token(tok)
        self.tokens_.append(t)

########NEW FILE########
__FILENAME__ = thai_unittest
# -*- coding: utf-8 -*-

## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

"""Sample transcription extractor based on the LCTL Thai parallel
data. Also tests Thai prons and alignment.
"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
"""

import sys
import os
import documents
import tokens
import token_comp
import extractor
import thai_extractor
import pronouncer
import unittest
from __init__ import BASE_

## A sample of 10,000 from each:

ENGLISH_       = '%s/testdata/thai_test_eng.txt' % BASE_
THAI_          = '%s/testdata/thai_test_thai.txt' % BASE_
XML_FILE_      = '%s/testdata/thai_test.xml' % BASE_
TMP_XML_FILE_  = '/tmp/thai_test.xml'
MATCH_FILE_    = '%s/testdata/thai_test.matches' % BASE_
TMP_MATCH_FILE_ = '/tmp/thai_test.matches'
BAD_COST_      = 6.0

def LoadData():
  t_extr = thai_extractor.ThaiExtractor()
  e_extr = extractor.NameExtractor()
  doclist = documents.Doclist()
  doc = documents.Doc()
  doclist.AddDoc(doc)
  #### Thai
  lang = tokens.Lang()
  lang.SetId('th')
  doc.AddLang(lang)
  t_extr.FileExtract(THAI_)
  lang.SetTokens(t_extr.Tokens())
  lang.CompactTokens()
  for t in lang.Tokens():
    pronouncer_ = pronouncer.UnitranPronouncer(t)
    pronouncer_.Pronounce()
  #### English
  lang = tokens.Lang()
  lang.SetId('en')
  doc.AddLang(lang)
  e_extr.FileExtract(ENGLISH_)
  lang.SetTokens(e_extr.Tokens())
  lang.CompactTokens()
  for t in lang.Tokens():
    pronouncer_ = pronouncer.EnglishPronouncer(t)
    pronouncer_.Pronounce()
  return doclist


def ComputePhoneMatches(doclist, match_file):
  matches = {}
  for doc in doclist.Docs():
    lang1 = doc.Langs()[0]
    lang2 = doc.Langs()[1]
    for t1 in lang1.Tokens():
      hash1 = t1.EncodeForHash()
      for t2 in lang2.Tokens():
        hash2 = t2.EncodeForHash()
        try: result = matches[(hash1, hash2)] ## don't re-calc
        except KeyError:
          comparator = token_comp.OldPhoneticDistanceComparator(t1, t2)
          comparator.ComputeDistance()
          result = comparator.ComparisonResult()
          matches[(hash1, hash2)] = result
  values = matches.values()
  values.sort(lambda x, y: cmp(x.Cost(), y.Cost()))
  p = open(match_file, 'w') ## zero out the file
  p.close()
  for v in values:
    if v.Cost() > BAD_COST_: break
    v.Print(match_file, 'a')


def main(output = False):
  doclist = LoadData()
  if output:
    doclist.XmlDump(XML_FILE_, utf8 = True)
    ComputePhoneMatches(doclist, MATCH_FILE_)
  else:
    doclist.XmlDump(TMP_XML_FILE_, utf8 = True)
    ComputePhoneMatches(doclist, TMP_MATCH_FILE_)
    unittest.TestUnitOutputs(sys.argv[0] + ': token parsing',\
                             XML_FILE_, TMP_XML_FILE_)
    unittest.TestUnitOutputs(sys.argv[0] + ': string matching',\
                             MATCH_FILE_, TMP_MATCH_FILE_)

    
if __name__ == '__main__':
  if len(sys.argv) > 1 and sys.argv[1] == 'generate':
    main(True)
  else:
    main()

########NEW FILE########
__FILENAME__ = tokens
# -*- coding: utf-8 -*-

## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

"""Definition for tokens, languages, documents and doclists, to store
the results of extraction, and express in XML.

For the XML format see dochandler.py
"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
hollingk@cslu.ogi.edu (Kristy Hollingshead)
"""

import xml.sax.saxutils
from math import sqrt
from __init__ import BASE_
import documents

XML_HEADER_ = '<?xml version="1.0" encoding="UTF-8"?>'
LANG_INDENT_  = ' ' * 4
TOKEN_INDENT_ = ' ' * 6

def SumProd(x, y):
  return sum(map(lambda x, y: x * y, x, y))

class Token:
  """A token is a term extracted from text, with attributes
  count, pronunciation, morphological decomposition
  """
  def __init__(self, string):
    try: self.string_ = string.encode('utf-8')
    except UnicodeDecodeError: self.string_ = string
    self.count_ = 1
    self.morphs_ = []
    self.pronunciations_ = []
    self.frequencies_ = []
    self.langid_ = ''

  def __eq__(self, other):
    skey = self.EncodeForHash()
    okey = other.EncodeForHash()
    return skey == okey

  def __repr__(self):
    return '#<%s %d %s %s %s>' % (self.string_,
                                  self.count_,
                                  self.morphs_,
                                  self.pronunciations_,
                                  self.langid_)

  def XmlEncode(self):
    xml_string_ = '<token count="%d" morphs="%s" prons="%s">%s</token>'
    morphs = ' '.join(self.morphs_)
    morphs = xml.sax.saxutils.escape(morphs)
    prons = ' ; '.join(self.pronunciations_)
    prons = xml.sax.saxutils.escape(prons)
    string_ = xml.sax.saxutils.escape(self.string_)
    xml_result = xml_string_ % (self.count_, morphs, prons, string_)
    return  TOKEN_INDENT_ + xml_result

  def EncodeForHash(self):
    return '%s<%s><%s><%s>' % (self.String(),
                               ' '.join(self.Morphs()),
                               ' '.join(self.Pronunciations()),
                               self.LangId())

  def String(self):
    return self.string_

  def SetCount(self, count):
    self.count_ = count

  def IncrementCount(self, increment = 1):
    self.count_ += increment

  def Count(self):
    return self.count_

  def AddPronunciation(self, pron):
    if pron not in self.pronunciations_:
      try: self.pronunciations_.append(pron.encode('utf-8'))
      except UnicodeDecodeError: self.pronunciations_.append(pron)

  def Pronunciations(self):
    return self.pronunciations_

  def SetMorphs(self, morphs):
    self.morphs_ = []
    for m in morphs:
      try: self.morphs_.append(m.encode('utf-8'))
      except UnicodeDecodeError: self.morphs_.append(m)

  def Morphs(self):
    return self.morphs_

  def SetLangId(self, lang):
    self.langid_ = lang

  def LangId(self):
    return self.langid_

class TokenFreqStats:
  """Holder for token frequency-statistics such as
  relative frequency-counts and variance.
  """
  def __init__(self, tok):
    self.token_ = tok
    self.frequencies_ = []
    self.freqsum_ = 0
    self.freqsumsq_ = 0
    self.variance_ = 0

  def __repr__(self):
    return '#<%s %s %.6f %.6f %.6f>' % (self.token_,
                                        self.frequencies_,
                                        self.freqsum_,
                                        self.freqsumsq_,
                                        self.variance_)

  def Token(self):
    return self.token_

  def Frequencies(self):
    return self.frequencies_

  def AddFrequency(self, f):
    self.frequencies_.append(f)

  def SetFrequencies(self, freq):
    self.frequencies_ = []
    for f in freq:
      self.frequencies_.append(f)

  def NormFrequencies(self):
    self.frequencies_ = [float(f) for f in self.frequencies_]
    sumfreqs = float(sum(self.frequencies_))
    if sumfreqs != 0.0:
      self.frequencies_ = [f/sumfreqs for f in self.frequencies_]

  def CalcFreqStats(self):
    n = len(self.frequencies_)
    self.freqsum_ = float(sum(self.frequencies_))
    self.freqsumsq_ = SumProd(self.frequencies_, self.frequencies_)
    self.variance_ = self.freqsumsq_/n - (self.freqsum_**2)/(n**2)

  def FreqSum(self):
    return self.freqsum_
    
  def FreqVariance(self):
    return self.variance_

class DocTokenStats:
  """Holder for Doclist-specific token statistics, such as frequency
  counts. Also allows for calculation of pairwise comparison metrics
  such as Pearson's correlation.
  """
  def __init__(self, doclist=None):
    if doclist is None:
      self.doclist_ = documents.Doclist()
    else: self.doclist_ = doclist
    self.n_ = len(self.doclist_.Docs())
    self.tokstats_ = {}

  def InitTokenStats(self, tok):
    tstats = TokenFreqStats(tok)
    tfreq = []
    for doc in self.doclist_.Docs():
      c = 0
      for lang in doc.Langs():
        if tok.LangId() != lang.Id(): continue
        tmptok = lang.MatchToken(tok)
        if tmptok is not None:
          c += tmptok.Count()
      tfreq.append(c)
    tstats.SetFrequencies(tfreq)
    tstats.NormFrequencies()
    tstats.CalcFreqStats()
    self.tokstats_[tok.EncodeForHash()] = tstats
    return tstats

  def AddTokenStats(self, tstats):
    tokhash = tstats.Token().EncodeForHash()
    if tokhash not in self.tokstats_:
      self.tokstats_[tokhash] = tstats

  def GetTokenStats(self, tok):
    try: return self.tokstats_[tok.EncodeForHash()]
    except KeyError: return self.InitTokenStats(tok)

  def TokenStats(self):
    return self.tokstats_.values()

  def SetN(self, n):
    self.n_ = n

  def GetN(self):
    return self.n_

  def PearsonsCorrelation(self, token1, token2):
    stats1 = self.GetTokenStats(token1)
    stats2 = self.GetTokenStats(token2)
    freq1 = stats1.Frequencies()
    freq2 = stats2.Frequencies()
    sumxy = sum(map(lambda x, y: x * y, freq1, freq2))
    covxy = sumxy/float(self.n_) - \
            (stats1.FreqSum()*stats2.FreqSum())/float(self.n_**2)
    try:
      rho = covxy/sqrt(stats1.FreqVariance()*stats2.FreqVariance())
    except ZeroDivisionError:
      rho = 0.0
    #print x.String(),y.String(),sumx2,sumy2,varx,vary,sumxy,covxy,rho
    return rho

class Lang:
  """Holder for tokens in a language.
  """
  def __init__(self):
    self.id_ = ''
    self.tokens_ = []

  def XmlEncode(self):
    if len(self.tokens_) == 0: return ''
    xml_string_ = '<lang id="%s">\n%s\n%s</lang>'
    xml_tokens = []
    for token_ in self.Tokens():
      xml_tokens.append(token_.XmlEncode())
    xml_result = xml_string_ % (self.id_, '\n'.join(xml_tokens),
                                LANG_INDENT_)
    return LANG_INDENT_ + xml_result

  def Id(self):
    return self.id_

  def SetId(self, id):
    self.id_ = id.encode('utf-8')

  def Tokens(self):
    return self.tokens_

  def SetTokens(self, tokens):
    self.tokens_ = []
    for t in tokens:
      self.AddToken(t)

  def AddToken(self, token, merge=False):
    """If an identical token already exists in dictionary,
    will merge tokens and cumulate their counts. Checks to
    see that morphology and pronunciations are identical,
    otherwise the tokens will not be merged.
    """
    token.SetLangId(self.id_)
    if not merge:
      self.tokens_.append(token)
    else:
      exists = self.MatchToken(token)
      if exists is None:
        self.tokens_.append(token)
      else:
        exists.IncrementCount(token.Count())

  def MatchToken(self, token):
    try:
      i = self.tokens_.index(token)
      return self.tokens_[i]
    except ValueError:
      return None

  def CompactTokens(self):
    """Merge identical tokens and cumulate their counts. Checks to see
    that morphology and pronunciations are identical, otherwise the
    tokens will not be merged.
    """
    map = {}
    for token_ in self.tokens_:
      hash_string = token_.EncodeForHash()
      try: map[hash_string].append(token_)
      except KeyError: map[hash_string] = [token_]
    ntokens = []
    keys = map.keys()
    keys.sort()
    for k in keys:
      token_ = map[k][0]
      for otoken in map[k][1:]:
        token_.IncrementCount(otoken.Count())
      ntokens.append(token_)
    self.tokens_ = ntokens


########NEW FILE########
__FILENAME__ = token_comp
# -*- coding: utf-8 -*-

## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

"""String-based comparator. Takes two tokens and returns a result that
includes the cost for the comparison. The comparator may be based on
any features of the two tokens, but reasonable features are phoneme
sequences, grapheme sequences (possibly based on a portion of the
morphological decomposition rather than the raw token), combinations
of these, or features derived from them.
"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
hollingk@cslu.ogi.edu (Kristy Hollingshead)
ting.qian@rochester.edu (Ting Qian)
"""

import sys
import tokens
import MinEditDist.mEdit
import perceptron_trainer
from __init__ import BASE_

CDIC_        = None
PFDIC_       = None
PHONE_TABLE_ = {}
DUMMY_PHONE_ = 'DUM'
MEDIT_PATH_ = '%s/MinEditDist' % BASE_

class ComparisonResult:
  """Value returned by TokenComparator. Includes the two tokens, the
  comparison cost, and informative string explaining how the cost was
  achieved.
  """
  def __init__(self, token1, token2):
    self.token1_ = token1
    self.token2_ = token2
    self.cost_ = 0
    self.info_ = ''

  def __repr__(self):
    return \
        '#<comparator: %s <-> %s, %2.4f, "%s">' % (self.token1_.String(),
                                                   self.token2_.String(),
                                                   self.cost_,
                                                   self.info_)

  def Print(self, file=None, mode='w'):
    if file: stream = open(file, mode)
    else: stream = sys.stdout
    info = '%s\t%s\t%f\t%s' % (self.token1_.String(),
                               self.token2_.String(),
                               self.cost_,
                               self.info_)
    #info = info.decode('utf-8')
    stream.write('%s\n' % info)
    if file: stream.close()

  def Cost(self):
    return self.cost_

  def SetCost(self, cost):
    self.cost_ = cost

  def Info(self):
    return self.info_

  def SetInfo(self, info):
    self.info_ = info

class TokenComparator:
  """Base class for token comparators.
  """
  def __init__(self, token1, token2):
    self.token1_ = token1
    self.token2_ = token2

  def InitData(self, data = []):
    return

  def ComparisonResult(self):
    return self.comparison_result_

  def ComputeDistance(self):
    self.comparison_result_ = ComparisonResult(self.token1_, self.token2_)

class OldPhoneticDistanceComparator(TokenComparator):
  """Phonetic distance comparator based on hand-derived features and
  old mEdit code.
  """
  def __init__(self, token1, token2):
    self.token1_ = token1
    self.token2_ = token2
    self.InitData()

  def LoadPhones(self, file):
    p = open(file)
    lines = p.readlines()
    p.close()
    phone_table = {}
    for phone in lines:
      PHONE_TABLE_[phone.strip()] = True

  def ProcessStringforMEditDistance(self, string):
    phones = string.split()
    nphones = []
    for phone in phones:
      if phone in PHONE_TABLE_: nphones.append(phone)
      else: nphones.append(DUMMY_PHONE_)
    return ' '.join(nphones)

  def InitData(self):
    global CDIC_
    global PFDIC_
    MinEditDist.mEdit._PHONE_FEATURES_FILE = \
        '%s/phoneFeature2.txt' % MEDIT_PATH_
    if CDIC_ and PFDIC_ and PHONE_TABLE_:
      pass
    else:
      phon_vocab = '%s/total_c.vocab' % MEDIT_PATH_
      CDIC_, PFDIC_ = MinEditDist.mEdit.init(phon_vocab)
      self.LoadPhones(phon_vocab)

  def ComputeDistance(self):
    bestcost = 100000
    bestinfo = ''
    for trans1 in self.token1_.Pronunciations():
      trans1 = self.ProcessStringforMEditDistance(trans1)
      for trans2 in self.token2_.Pronunciations():
        trans2 = self.ProcessStringforMEditDistance(trans2)
        cost = MinEditDist.mEdit.mEdit(trans1, trans2, CDIC_, PFDIC_)
        if cost < bestcost:
          bestcost = cost
          bestinfo = '%s <-> %s' % (trans1, trans2)
    result = ComparisonResult(self.token1_, self.token2_)
    result.SetCost(bestcost)
    result.SetInfo(bestinfo)
    self.comparison_result_ = result

class TimeCorrelator(TokenComparator):
  """Time distance comparator based on using the Pearson's correlation
  coefficient to calculate the similarity of two terms' frequency
  distribution.
  """
  def __init__(self, token1, token2, doctokstats):
    self.token1_ = token1
    self.token2_ = token2
    self.stats_ = doctokstats

  def ComputeDistance(self):
    bestcost = 0
    bestinfo = ''
    corr = self.stats_.PearsonsCorrelation(self.token1_, self.token2_)
    bestcost = corr
    bestinfo = 'Pearson\'s correlation'
    result = ComparisonResult(self.token1_, self.token2_)
    result.SetCost(bestcost)
    result.SetInfo(bestinfo)
    self.comparison_result_ = result

class SnowPronComparator(TokenComparator):
  """Snow comparator based on using a perceptron's activation given a
  pair of two terms.
  """
  def __init__(self, fm_file=None, net_file=None):
    perceptron_trainer.DEBUG_ = True
    self.snow_ = perceptron_trainer.ParallelTrainer(fm_file, net_file)

  def Train(self, pos_example_list):
    return self.snow_.Train(pos_example_list)

  def LearnFromNewExamples(self, new_example_list):
    self.snow_.Retrain(new_example_list)

  def ComputeDistance(self, token1, token2):
    if self.snow_.IsTrained():
      max_act_value = 0
      max_act_sign = 1
      bestinfo = 'Pronunciation Perceptron'

      for p1 in token1.Pronunciations():
        for p2 in token2.Pronunciations():
          cost = 0
          act_tuple = self.snow_.Evaluate(p1, p2)
          target = act_tuple[0]
          activation = act_tuple[1]
          if target == 0:
            if activation > max_act_value:
              max_act_sign = -1
              max_act_value = activation
          elif target == 1:
            if activation > max_act_value:
              max_act_sign = 1
              max_act_value = activation

      bestcost = max_act_value * max_act_sign
      result = ComparisonResult(token1, token2)
      result.SetCost(bestcost)
      result.SetInfo(bestinfo)
      self.comparison_result_ = result
    else:
      result = ComparisonResult(token1_, token2_)
      result.SetCost(0)
      result.SetInfo('Pronunciation Perceptron')
      self.comparison_result_ = result

  def Forget(self):
    self.snow_.CleanUp()
    
    
    

########NEW FILE########
__FILENAME__ = token_comp_unittest
# -*- coding: utf-8 -*-

## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

"""Unit test for token_comp. Also exercises pronouncer
and doc handler a bit.
"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
hollingk@cslu.ogi.edu (Kristy Hollingshead)
ting.qian@rochester.edu (Ting Qian)
"""

import os
import sys
import unittest
import documents
import tokens
import pronouncer
import token_comp
import auxiliary_comp
from __init__ import BASE_

PAIRS_ = [(u'高島屋', u'Takashimaya'),
          (u'共產黨', u'공산당'),
          (u'Kuomintang', u'國民黨'),
          (u'ᏣᎳᎩ', u'Cherokee'),
          (u'niqitsiavaliriniq', u'ᓂᕿᑦᓯᐊᕙᓕᕆᓂᖅ')
          ]

GOLDEN_FILE_ =  '%s/testdata/token_comp_test.txt' % BASE_ 
TEST_FILE_ = '/tmp/token_comp_test.txt'

def CreateDoclist():
  doclist = documents.Doclist()
  doc = documents.Doc()
  lang = tokens.Lang()
  lang.SetId('eng')
  token_ = tokens.Token('Bush')
  token_.SetCount(1)
  token_.AddPronunciation('b U S')
  token_.SetMorphs(['Bush', "'s"])
  lang.AddToken(token_)
  token_ = tokens.Token('Clinton')
  token_.SetCount(3)
  token_.AddPronunciation('k l I n t & n')
  token_.AddPronunciation('k l I n t > n')
  token_.SetMorphs(['Clinton'])
  lang.AddToken(token_)
  token_ = tokens.Token('Bush')
  token_.SetCount(3)
  token_.AddPronunciation('b U S')
  token_.SetMorphs(['Bush', "'s",])
  lang.AddToken(token_)
  lang.CompactTokens()
  doc.AddLang(lang)
  lang = tokens.Lang()
  lang.SetId('zho')
  token_ = tokens.Token(u'克林頓')
  token_.SetCount(3)
  token_.AddPronunciation('kh & l i n t u n')
  token_.SetMorphs([u'克林頓'])
  lang.AddToken(token_)
  token_ = tokens.Token(u'高島屋')
  token_.SetCount(1)
  token_.AddPronunciation('k a u t a u u')
  token_.AddPronunciation('t A k A s i m A j a')
  lang.AddToken(token_)
  doc.AddLang(lang)
  doclist.AddDoc(doc)
  doc = documents.Doc()
  lang = tokens.Lang()
  lang.SetId('eng')
  token_ = tokens.Token('Clinton')
  token_.SetCount(2)
  token_.AddPronunciation('k l I n t & n')
  token_.SetMorphs(['Clinton'])
  lang.AddToken(token_)
  token_ = tokens.Token('Bush')
  token_.SetCount(3)
  token_.AddPronunciation('b U S')
  token_.SetMorphs(['Bush', "'s"])
  lang.AddToken(token_)
  doc.AddLang(lang)
  lang = tokens.Lang()
  lang.SetId('ara')
  token_ = tokens.Token(u'كلينتون')
  token_.SetCount(3)
  token_.AddPronunciation('k l j n t w n')
  token_.SetMorphs([u'كلينتون'])
  lang.AddToken(token_)
  doc.AddLang(lang)
  doclist.AddDoc(doc)
  return doclist


def TestPronunciations(tok):
  pronouncer_ = pronouncer.UnitranPronouncer(tok)
  pronouncer_.Pronounce()
  pronouncer_ = pronouncer.HanziPronouncer(tok)
  pronouncer_.Pronounce()
  pronouncer_ = pronouncer.EnglishPronouncer(tok)
  pronouncer_.Pronounce()
  if not tok.Pronunciations():
    pronouncer_ = pronouncer.LatinPronouncer(tok)
    pronouncer_.Pronounce()


def TestCorrelations(doclist, outfile):
  correlates = []
  stats = tokens.DocTokenStats(doclist)
  for doc in doclist.Docs():
    for lang1 in doc.Langs():
      for lang2 in doc.Langs():
        for t1 in lang1.Tokens():
          for t2 in lang2.Tokens():
            comparator = token_comp.TimeCorrelator(t1, t2, stats)
            comparator.ComputeDistance()
            result = comparator.ComparisonResult()
            correlates.append(result)
  correlates.sort(lambda x, y: cmp(x.Cost(), y.Cost()))
  for c in correlates:
    if c.Cost() <= 0.0: continue
    c.Print(outfile, 'a')

def TestSnowActivations(doclist, outfile):
  activations = []
  comparator = token_comp.SnowPronComparator('1.fm', '1.network')
  tr_list = []
  tr_fp = open('testdata/perceptron.ce.dict.training', 'r')
  for line in tr_fp.readlines():
    [pron1, pron2] = line.strip().split('\t')
    tr_list.append((pron1, pron2))
  comparator.Train(tr_list[0:2000])
  
  for doc in doclist.Docs():
    for lang1 in doc.Langs():
      for lang2 in doc.Langs():
        for t1 in lang1.Tokens():
          for t2 in lang2.Tokens():
            if t1.LangId() == 'eng' and t2.LangId() == 'zho':
              comparator.ComputeDistance(t1, t2)
              result = comparator.ComparisonResult()
              activations.append(result)

  out = open(outfile, 'a')
  for a in activations:
    a.Print()
    str_repr = str(a)
    if str_repr.split(', ')[1] != '0':
      out.write(str_repr.split(', ')[0] + ': sucessfully activated one target>\n')
    else:
      out.write(str_repr.split(', ')[0] + ': no activation on any target>\n')
  out.close()
  comparator.Forget()
            
def TestAuxiliaryComparators(unitname):
  ## Added tests for Wade-Giles and Pinyin comparators
  t1 = tokens.Token('毛泽东')
  t2 = tokens.Token('周恩来')
  t1py = tokens.Token('Mao Zedong')
  t2py = tokens.Token('Zhou Enlai')
  t1wg = tokens.Token('Mao Tse-tung')
  t2wg = tokens.Token('Chou Enlai')
  comparator = auxiliary_comp.PinyinComparator(t1, t1py)
  comparator.ComputeDistance()
  assert comparator.ComparisonResult().Cost() == auxiliary_comp.MATCH_, \
      '%s should match %s' % (t1.String(), t1py.String())
  comparator = auxiliary_comp.PinyinComparator(t2, t2py)
  comparator.ComputeDistance()
  assert comparator.ComparisonResult().Cost() == auxiliary_comp.MATCH_, \
      '%s should match %s' % (t2.String(), t2py.String())
  comparator = auxiliary_comp.WadeGilesComparator(t1, t1wg)
  comparator.ComputeDistance()
  assert comparator.ComparisonResult().Cost() == auxiliary_comp.MATCH_, \
      '%s should match %s' % (t1.String(), t1wg.String())
  comparator = auxiliary_comp.WadeGilesComparator(t2, t2wg)
  comparator.ComputeDistance()
  assert comparator.ComparisonResult().Cost() == auxiliary_comp.MATCH_, \
      '%s should match %s' % (t2.String(), t2wg.String())
  comparator = auxiliary_comp.WadeGilesComparator(t2, t2py)
  comparator.ComputeDistance()
  assert comparator.ComparisonResult().Cost() == auxiliary_comp.NO_MATCH_, \
      '%s should not match %s' % (t2.String(), t2py.String())
  print '%s (auxiliary tests) successful' % unitname


def main(output = False):
  phonecmps = []
  timecmps = []
  doclist = CreateDoclist()
  for pair in PAIRS_:
    token1 = tokens.Token(pair[0])
    TestPronunciations(token1)
    token2 = tokens.Token(pair[1])
    TestPronunciations(token2)
    comparator = token_comp.OldPhoneticDistanceComparator(token1, token2)
    comparator.ComputeDistance()
    phonecmps.append(comparator)
  if output:
    p = open(GOLDEN_FILE_, 'w')  ## clear golden file
    p.close()
    for pc in phonecmps:
      pc.ComparisonResult().Print(GOLDEN_FILE_, 'a')
    TestCorrelations(doclist, GOLDEN_FILE_)
    TestSnowActivations(doclist, GOLDEN_FILE_)
  else:
    p = open(TEST_FILE_, 'w') ## clear test file
    p.close()
    for pc in phonecmps:
      pc.ComparisonResult().Print(TEST_FILE_, 'a')
    TestCorrelations(doclist, TEST_FILE_)
    TestSnowActivations(doclist, TEST_FILE_)
    unittest.TestUnitOutputs(sys.argv[0] + ' (main test & perceptron test)', \
                             GOLDEN_FILE_, TEST_FILE_)
    TestAuxiliaryComparators(sys.argv[0])

if __name__ == '__main__':
  if len(sys.argv) > 1 and sys.argv[1] == 'generate':
    main(True)
  else:
    main()

########NEW FILE########
__FILENAME__ = mk_sampa_table
# -*- coding: utf-8 -*-

"""Loads Tables.py and produces X_Tables.py, mapping to XSampa rather
than WorldBet.

X_Tables.py.unc will contain unconverted symbols
"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
"""

import sys
sys.path.append('../')
import Tables
import Wb2Xs


def Main():
  newTable = {}
  unconverted = {}
  for u in Tables.TransTable:
    wbstring, utf = Tables.TransTable[u]
    wbs = wbstring.split()
    xslist = []
    for wb in wbs:
      try:
        xs = Wb2Xs.WorldBetToXSampa[wb]
      except KeyError:
        xs = wb
        unconverted[xs] = True
      xslist.append(xs)
    newTable[u] = [' '.join(xslist), utf]
  p = open('X_Tables.py', 'w')
  p.write('# coding=utf-8\n')
  p.write('TransTable = {\n')
  keys = newTable.keys()
  keys.sort()
  for u in keys:
    xstring, utf = newTable[u]
    p.write("    %s : ['%s', '%s'],\n" % (repr(u), xstring, utf))
  p.write('}\n')
  p.close()
  p = open('X_Tables.py.unc', 'w')
  for xs in unconverted:
    p.write('%s\n' % xs)
  p.close()


if __name__ == '__main__':
  Main()



########NEW FILE########
__FILENAME__ = Tables
# coding=utf-8
TransTable = {
    u'\u0600' : ['(NUMBER SIGN)','؀'],
    u'\u0601' : ['(SANAH)','؁'],
    u'\u0602' : ['(FOOTNOTEMARKER)','؂'],
    u'\u0603' : ['(SAPHA)','؃'],
    u'\u060B' : ['(AFGHANI SIGN)','؋'],
    u'\u060C' : ['(COMMA)','،'],
    u'\u060D' : ['(DATE SEPARATOR)','؍'],
    u'\u060E' : ['(POETIC VERSE SIGN)','؎'],
    u'\u060F' : ['(SIGN MISRA)','؏'],
    u'\u0610' : ['(##)','ؐ'],
    u'\u0611' : ['(##)','ؑ'],
    u'\u0612' : ['(##)','ؒ'],
    u'\u0613' : ['(##)','ؓ'],
    u'\u0614' : ['(##)','ؔ'],
    u'\u0615' : ['(##)','ؕ'],
    u'\u061B' : ['(SEMICOLON)','؛'],
    u'\u061E' : ['(TRIPLE DOT)','؞'],
    u'\u061F' : ['(QUESTION MARK)','؟'],
    u'\u0621' : ['(HAMZA)','ء'],
    u'\u0622' : ['?','آ'],
    u'\u0623' : ['?','أ'],
    u'\u0624' : ['w','ؤ'],
    u'\u0625' : ['?','إ'],
    u'\u0626' : ['j','ئ'],
    u'\u0627' : ['j','ا'],
    u'\u0628' : ['b','ب'],
    u'\u0629' : ['t','ة'],
    u'\u062A' : ['t','ت'],
    u'\u062B' : ['T','ث'],
    u'\u062C' : ['dZ','ج'],
    u'\u062D' : ['H','ح'],
    u'\u062E' : ['x','خ'],
    u'\u062F' : ['d','د'],
    u'\u0630' : ['D','ذ'],
    u'\u0631' : ['r','ر'],
    u'\u0632' : ['z','ز'],
    u'\u0633' : ['s','س'],
    u'\u0634' : ['S','ش'],
    u'\u0635' : ['s~','ص'],
    u'\u0636' : ['d~','ض'],
    u'\u0637' : ['t~','ط'],
    u'\u0638' : ['z~','ظ'],
    u'\u0639' : ['hv','ع'],
    u'\u063A' : ['G','غ'],
    u'\u0640' : ['(TATWEEL)','ـ'],
    u'\u0641' : ['f','ف'],
    u'\u0642' : ['q','ق'],
    u'\u0643' : ['k','ك'],
    u'\u0644' : ['l','ل'],
    u'\u0645' : ['m','م'],
    u'\u0646' : ['n','ن'],
    u'\u0647' : ['h','ه'],
    u'\u0648' : ['w','و'],
    u'\u0649' : ['a:','ى'],
    u'\u064A' : ['j','ي'],
    u'\u064B' : ['(FATHATAN)','ً'],
    u'\u064C' : ['(DAMMATAN)','ٌ'],
    u'\u064D' : ['(KASRATAN)','ٍ'],
    u'\u064E' : ['E','َ'],
    u'\u064F' : ['U','ُ'],
    u'\u0650' : ['I','ِ'],
    u'\u0651' : ['(SHADDA)','ّ'],
    u'\u0652' : ['a~','ْ'],
    u'\u0653' : ['(MADDAH ABOVE)','ٓ'],
    u'\u0654' : ['(HAMZA ABOVE)','ٔ'],
    u'\u0655' : ['(HAMZE BELOW)','ٕ'],
    u'\u0656' : ['(SUBSCRIPT ALEF)','ٖ'],
    u'\u0657' : ['(INVERTED DAMMA)','ٗ'],
    u'\u0658' : ['(NOON GHUNNA)','٘'],
    u'\u0659' : ['(ZWARAKAY)','ٙ'],
    u'\u065A' : ['(SMALL V ABOVE)','ٚ'],
    u'\u065B' : ['(INVERTED SMALL V)','ٛ'],
    u'\u065C' : ['(DOT BELOW)','ٜ'],
    u'\u065D' : ['(REVERSED DAMMA)','ٝ'],
    u'\u065E' : ['(FATHA WITH TWO DOTS)','ٞ'],
    u'\u0660' : ['(ZERO)','٠'],
    u'\u0661' : ['(ONE)','١'],
    u'\u0662' : ['(TWO)','٢'],
    u'\u0663' : ['(THREE)','٣'],
    u'\u0664' : ['(FOUR)','٤'],
    u'\u0665' : ['(FIVE)','٥'],
    u'\u0666' : ['(SIX)','٦'],
    u'\u0667' : ['(SEVEN)','٧'],
    u'\u0668' : ['(EIGHT)','٨'],
    u'\u0669' : ['(NINE)','٩'],
    u'\u066A' : ['(PERCENT SIGN)','٪'],
    u'\u066B' : ['(DECIMMAL SEPARATOR)','٫'],
    u'\u066C' : ['(THOUSAND SEPARATOR)','٬'],
    u'\u066D' : ['(ASTERISK)','٭'],
    u'\u066E' : ['b','ٮ'],
    u'\u066F' : ['q','ٯ'],
    u'\u0670' : ['a~','ٰ'],
    u'\u0671' : ['?','ٱ'],
    u'\u0672' : ['?','ٲ'],
    u'\u0673' : ['?','ٳ'],
    u'\u0674' : ['(HIGH HAMZA)','ٴ'],
    u'\u0675' : ['?','ٵ'],
    u'\u0676' : ['w','ٶ'],
    u'\u0677' : ['w','ٷ'],
    u'\u0678' : ['j','ٸ'],
    u'\u0679' : ['tr','ٹ'],
    u'\u067A' : ['tR','ٺ'],
    u'\u067B' : ['t[','ٻ'],
    u'\u067C' : ['tr','ټ'],
    u'\u067D' : ['tr','ٽ'],
    u'\u067E' : ['p','پ'],
    u'\u067F' : ['th','ٿ'],
    u'\u0680' : ['bh','ڀ'],
    u'\u0681' : ['dz','ځ'],
    u'\u0682' : ['dz','ڂ'],
    u'\u0683' : ['n~','ڃ'],
    u'\u0684' : ['f','ڄ'],
    u'\u0685' : ['ts','څ'],
    u'\u0686' : ['tS','چ'],
    u'\u0687' : ['tSh','ڇ'],
    u'\u0688' : ['dr','ڈ'],
    u'\u0689' : ['dr','ډ'],
    u'\u068A' : ['dr','ڊ'],
    u'\u068B' : ['dr','ڋ'],
    u'\u068C' : ['dh','ڌ'],
    u'\u068D' : ['dr','ڍ'],
    u'\u068E' : ['dr','ڎ'],
    u'\u068F' : ['dr','ڏ'],
    u'\u0690' : ['dr','ڐ'],
    u'\u0691' : ['r','ڑ'],
    u'\u0692' : ['r','ڒ'],
    u'\u0693' : ['rr','ړ'],
    u'\u0694' : ['rr','ڔ'],
    u'\u0695' : ['r','ڕ'],
    u'\u0696' : ['r','ږ'],
    u'\u0697' : ['r','ڗ'],
    u'\u0698' : ['Z','ژ'],
    u'\u0699' : ['rr','ڙ'],
    u'\u069A' : ['sr','ښ'],
    u'\u069B' : ['sr','ڛ'],
    u'\u069C' : ['sr','ڜ'],
    u'\u069D' : ['s~','ڝ'],
    u'\u069E' : ['s~','ڞ'],
    u'\u069F' : ['t~','ڟ'],
    u'\u06A0' : ['N','ڠ'],
    u'\u06A1' : ['f','ڡ'],
    u'\u06A2' : ['f','ڢ'],
    u'\u06A3' : ['f','ڣ'],
    u'\u06A4' : ['v','ڤ'],
    u'\u06A5' : ['f','ڥ'],
    u'\u06A6' : ['ph','ڦ'],
    u'\u06A7' : ['f','ڧ'],
    u'\u06A8' : ['f','ڨ'],
    u'\u06A9' : ['k','ک'],
    u'\u06AA' : ['k','ڪ'],
    u'\u06AB' : ['g','ګ'],
    u'\u06AC' : ['k','ڬ'],
    u'\u06AD' : ['n~','ڭ'],
    u'\u06AE' : ['k','ڮ'],
    u'\u06AF' : ['g','گ'],
    u'\u06B0' : ['g','ڰ'],
    u'\u06B1' : ['N','ڱ'],
    u'\u06B2' : ['g','ڲ'],
    u'\u06B3' : ['g','ڳ'],
    u'\u06B4' : ['g','ڴ'],
    u'\u06B5' : ['l','ڵ'],
    u'\u06B6' : ['lr','ڶ'],
    u'\u06B7' : ['l','ڷ'],
    u'\u06B8' : ['l','ڸ'],
    u'\u06B9' : ['n','ڹ'],
    u'\u06BA' : ['n','ں'],
    u'\u06BB' : ['nr','ڻ'],
    u'\u06BC' : ['nr','ڼ'],
    u'\u06BD' : ['nr','ڽ'],
    u'\u06BE' : ['(ASPIRATED)','ھ'],
    u'\u06BF' : ['tS','ڿ'],
    u'\u06C0' : ['h','ۀ'],
    u'\u06C1' : ['h','ہ'],
    u'\u06C2' : ['h','ۂ'],
    u'\u06C3' : ['h','ۃ'],
    u'\u06C4' : ['w','ۄ'],
    u'\u06C5' : ['o','ۅ'],
    u'\u06C6' : ['o','ۆ'],
    u'\u06C7' : ['u:','ۇ'],
    u'\u06C8' : ['u','ۈ'],
    u'\u06C9' : ['u','ۉ'],
    u'\u06CA' : ['q','ۊ'],
    u'\u06CB' : ['v','ۋ'],
    u'\u06CC' : ['j','ی'],
    u'\u06CD' : ['i','ۍ'],
    u'\u06CE' : ['i:','ێ'],
    u'\u06CF' : ['w','ۏ'],
    u'\u06D0' : ['b','ې'],
    u'\u06D1' : ['i:','ۑ'],
    u'\u06D2' : ['e:','ے'],
    u'\u06D3' : ['e:','ۓ'],
    u'\u06D4' : ['(FULL STOP)','۔'],
    u'\u06D5' : ['&','ە'],
    u'\u06D6' : ['(##)','ۖ'],
    u'\u06D7' : ['(##)','ۗ'],
    u'\u06D8' : ['(##)','ۘ'],
    u'\u06D9' : ['(##)','ۙ'],
    u'\u06DA' : ['(##)','ۚ'],
    u'\u06DB' : ['(##)','ۛ'],
    u'\u06DC' : ['(##)','ۜ'],
    u'\u06DD' : ['(##)','۝'],
    u'\u06DE' : ['(##)','۞'],
    u'\u06DF' : ['(##)','۟'],
    u'\u06E0' : ['(##)','۠'],
    u'\u06E1' : ['(##)','ۡ'],
    u'\u06E2' : ['(##)','ۢ'],
    u'\u06E3' : ['(##)','ۣ'],
    u'\u06E4' : ['(##)','ۤ'],
    u'\u06E5' : ['(##)','ۥ'],
    u'\u06E6' : ['(##)','ۦ'],
    u'\u06E7' : ['(##)','ۧ'],
    u'\u06E8' : ['(##)','ۨ'],
    u'\u06E9' : ['(##)','۩'],
    u'\u06EA' : ['(##)','۪'],
    u'\u06EB' : ['(##)','۫'],
    u'\u06EC' : ['(##)','۬'],
    u'\u06ED' : ['(##)','ۭ'],
    u'\u06EE' : ['dr','ۮ'],
    u'\u06EF' : ['rr','ۯ'],
    u'\u06F0' : ['(ZERO)','۰'],
    u'\u06F1' : ['(ONE)','۱'],
    u'\u06F2' : ['(TWO)','۲'],
    u'\u06F3' : ['(THREE)','۳'],
    u'\u06F4' : ['(FOUR)','۴'],
    u'\u06F5' : ['(FIVE)','۵'],
    u'\u06F6' : ['(SIX)','۶'],
    u'\u06F7' : ['(SEVEN)','۷'],
    u'\u06F8' : ['(EIGHT)','۸'],
    u'\u06F9' : ['(NINE)','۹'],
    u'\u06FA' : ['S','ۺ'],
    u'\u06FB' : ['z','ۻ'],
    u'\u06FC' : ['2','ۼ'],
    u'\u06FD' : ['(AMPERSAND)','۽'],
    u'\u06FE' : ['(POSTPOSTION MEN)','۾'],
    u'\u06FF' : ['h','ۿ'],
    u'\uFB50' : ['(##)','ﭐ'],
    u'\uFB51' : ['(##)','ﭑ'],
    u'\uFB52' : ['(##)','ﭒ'],
    u'\uFB53' : ['(##)','ﭓ'],
    u'\uFB54' : ['(##)','ﭔ'],
    u'\uFB55' : ['(##)','ﭕ'],
    u'\uFB56' : ['(##)','ﭖ'],
    u'\uFB57' : ['(##)','ﭗ'],
    u'\uFB58' : ['(##)','ﭘ'],
    u'\uFB59' : ['(##)','ﭙ'],
    u'\uFB5A' : ['(##)','ﭚ'],
    u'\uFB5B' : ['(##)','ﭛ'],
    u'\uFB5C' : ['(##)','ﭜ'],
    u'\uFB5D' : ['(##)','ﭝ'],
    u'\uFB5E' : ['(##)','ﭞ'],
    u'\uFB5F' : ['(##)','ﭟ'],
    u'\uFB60' : ['(##)','ﭠ'],
    u'\uFB61' : ['(##)','ﭡ'],
    u'\uFB62' : ['(##)','ﭢ'],
    u'\uFB63' : ['(##)','ﭣ'],
    u'\uFB64' : ['(##)','ﭤ'],
    u'\uFB65' : ['(##)','ﭥ'],
    u'\uFB66' : ['(##)','ﭦ'],
    u'\uFB67' : ['(##)','ﭧ'],
    u'\uFB68' : ['(##)','ﭨ'],
    u'\uFB69' : ['(##)','ﭩ'],
    u'\uFB6A' : ['(##)','ﭪ'],
    u'\uFB6B' : ['(##)','ﭫ'],
    u'\uFB6C' : ['(##)','ﭬ'],
    u'\uFB6D' : ['(##)','ﭭ'],
    u'\uFB6E' : ['(##)','ﭮ'],
    u'\uFB6F' : ['(##)','ﭯ'],
    u'\uFB70' : ['(##)','ﭰ'],
    u'\uFB71' : ['(##)','ﭱ'],
    u'\uFB72' : ['(##)','ﭲ'],
    u'\uFB73' : ['(##)','ﭳ'],
    u'\uFB74' : ['(##)','ﭴ'],
    u'\uFB75' : ['(##)','ﭵ'],
    u'\uFB76' : ['(##)','ﭶ'],
    u'\uFB77' : ['(##)','ﭷ'],
    u'\uFB78' : ['(##)','ﭸ'],
    u'\uFB79' : ['(##)','ﭹ'],
    u'\uFB7A' : ['(##)','ﭺ'],
    u'\uFB7B' : ['(##)','ﭻ'],
    u'\uFB7C' : ['(##)','ﭼ'],
    u'\uFB7D' : ['(##)','ﭽ'],
    u'\uFB7E' : ['(##)','ﭾ'],
    u'\uFB7F' : ['(##)','ﭿ'],
    u'\uFB80' : ['(##)','ﮀ'],
    u'\uFB81' : ['(##)','ﮁ'],
    u'\uFB82' : ['(##)','ﮂ'],
    u'\uFB83' : ['(##)','ﮃ'],
    u'\uFB84' : ['(##)','ﮄ'],
    u'\uFB85' : ['(##)','ﮅ'],
    u'\uFB86' : ['(##)','ﮆ'],
    u'\uFB87' : ['(##)','ﮇ'],
    u'\uFB88' : ['(##)','ﮈ'],
    u'\uFB89' : ['(##)','ﮉ'],
    u'\uFB8A' : ['(##)','ﮊ'],
    u'\uFB8B' : ['(##)','ﮋ'],
    u'\uFB8C' : ['(##)','ﮌ'],
    u'\uFB8D' : ['(##)','ﮍ'],
    u'\uFB8E' : ['(##)','ﮎ'],
    u'\uFB8F' : ['(##)','ﮏ'],
    u'\uFB90' : ['(##)','ﮐ'],
    u'\uFB91' : ['(##)','ﮑ'],
    u'\uFB92' : ['(##)','ﮒ'],
    u'\uFB93' : ['(##)','ﮓ'],
    u'\uFB94' : ['(##)','ﮔ'],
    u'\uFB95' : ['(##)','ﮕ'],
    u'\uFB96' : ['(##)','ﮖ'],
    u'\uFB97' : ['(##)','ﮗ'],
    u'\uFB98' : ['(##)','ﮘ'],
    u'\uFB99' : ['(##)','ﮙ'],
    u'\uFB9A' : ['(##)','ﮚ'],
    u'\uFB9B' : ['(##)','ﮛ'],
    u'\uFB9C' : ['(##)','ﮜ'],
    u'\uFB9D' : ['(##)','ﮝ'],
    u'\uFB9E' : ['(##)','ﮞ'],
    u'\uFB9F' : ['(##)','ﮟ'],
    u'\uFBA0' : ['(##)','ﮠ'],
    u'\uFBA1' : ['(##)','ﮡ'],
    u'\uFBA2' : ['(##)','ﮢ'],
    u'\uFBA3' : ['(##)','ﮣ'],
    u'\uFBA4' : ['(##)','ﮤ'],
    u'\uFBA5' : ['(##)','ﮥ'],
    u'\uFBA6' : ['(##)','ﮦ'],
    u'\uFBA7' : ['(##)','ﮧ'],
    u'\uFBA8' : ['(##)','ﮨ'],
    u'\uFBA9' : ['(##)','ﮩ'],
    u'\uFBAA' : ['(##)','ﮪ'],
    u'\uFBAB' : ['(##)','ﮫ'],
    u'\uFBAC' : ['(##)','ﮬ'],
    u'\uFBAD' : ['(##)','ﮭ'],
    u'\uFBAE' : ['(##)','ﮮ'],
    u'\uFBAF' : ['(##)','ﮯ'],
    u'\uFBB0' : ['(##)','ﮰ'],
    u'\uFBB1' : ['(##)','ﮱ'],
    u'\uFBD3' : ['(##)','ﯓ'],
    u'\uFBD4' : ['(##)','ﯔ'],
    u'\uFBD5' : ['(##)','ﯕ'],
    u'\uFBD6' : ['(##)','ﯖ'],
    u'\uFBD7' : ['(##)','ﯗ'],
    u'\uFBD8' : ['(##)','ﯘ'],
    u'\uFBD9' : ['(##)','ﯙ'],
    u'\uFBDA' : ['(##)','ﯚ'],
    u'\uFBDB' : ['(##)','ﯛ'],
    u'\uFBDC' : ['(##)','ﯜ'],
    u'\uFBDD' : ['(##)','ﯝ'],
    u'\uFBDE' : ['(##)','ﯞ'],
    u'\uFBDF' : ['(##)','ﯟ'],
    u'\uFBE0' : ['(##)','ﯠ'],
    u'\uFBE1' : ['(##)','ﯡ'],
    u'\uFBE2' : ['(##)','ﯢ'],
    u'\uFBE3' : ['(##)','ﯣ'],
    u'\uFBE4' : ['(##)','ﯤ'],
    u'\uFBE5' : ['(##)','ﯥ'],
    u'\uFBE6' : ['(##)','ﯦ'],
    u'\uFBE7' : ['(##)','ﯧ'],
    u'\uFBE8' : ['(##)','ﯨ'],
    u'\uFBE9' : ['(##)','ﯩ'],
    u'\uFBEA' : ['(##)','ﯪ'],
    u'\uFBEB' : ['(##)','ﯫ'],
    u'\uFBEC' : ['(##)','ﯬ'],
    u'\uFBED' : ['(##)','ﯭ'],
    u'\uFBEE' : ['(##)','ﯮ'],
    u'\uFBEF' : ['(##)','ﯯ'],
    u'\uFBF0' : ['(##)','ﯰ'],
    u'\uFBF1' : ['(##)','ﯱ'],
    u'\uFBF2' : ['(##)','ﯲ'],
    u'\uFBF3' : ['(##)','ﯳ'],
    u'\uFBF4' : ['(##)','ﯴ'],
    u'\uFBF5' : ['(##)','ﯵ'],
    u'\uFBF6' : ['(##)','ﯶ'],
    u'\uFBF7' : ['(##)','ﯷ'],
    u'\uFBF8' : ['(##)','ﯸ'],
    u'\uFBF9' : ['(##)','ﯹ'],
    u'\uFBFA' : ['(##)','ﯺ'],
    u'\uFBFB' : ['(##)','ﯻ'],
    u'\uFBFC' : ['(##)','ﯼ'],
    u'\uFBFD' : ['(##)','ﯽ'],
    u'\uFBFE' : ['(##)','ﯾ'],
    u'\uFBFF' : ['(##)','ﯿ'],
    u'\u0531' : ['A','Ա'],
    u'\u0532' : ['b','Բ'],
    u'\u0533' : ['g','Գ'],
    u'\u0534' : ['d','Դ'],
    u'\u0535' : ['E','Ե'],
    u'\u0536' : ['z','Զ'],
    u'\u0537' : ['e','Է'],
    u'\u0538' : ['>','Ը'],
    u'\u0539' : ['th','Թ'],
    u'\u053A' : ['Z','Ժ'],
    u'\u053B' : ['i','Ի'],
    u'\u053C' : ['l','Լ'],
    u'\u053D' : ['z','Խ'],
    u'\u053E' : ['ts','Ծ'],
    u'\u053F' : ['k','Կ'],
    u'\u0540' : ['h','Հ'],
    u'\u0541' : ['dz','Ձ'],
    u'\u0542' : ['G','Ղ'],
    u'\u0543' : ['tS','Ճ'],
    u'\u0544' : ['m','Մ'],
    u'\u0545' : ['h, j','Յ'],
    u'\u0546' : ['n','Ն'],
    u'\u0547' : ['S','Շ'],
    u'\u0548' : ['o','Ո'],
    u'\u0549' : ['tSh','Չ'],
    u'\u054A' : ['p','Պ'],
    u'\u054B' : ['dZ','Ջ'],
    u'\u054C' : ['r','Ռ'],
    u'\u054D' : ['s','Ս'],
    u'\u054E' : ['v','Վ'],
    u'\u054F' : ['t','Տ'],
    u'\u0550' : ['9','Ր'],
    u'\u0551' : ['tsh','Ց'],
    u'\u0552' : ['v','Ւ'],
    u'\u0553' : ['ph','Փ'],
    u'\u0554' : ['kh','Ք'],
    u'\u0555' : ['o','Օ'],
    u'\u0556' : ['f','Ֆ'],
    u'\u0559' : ['(ML)','ՙ'],
    u'\u055A' : ['(APOSTROPHE)','՚'],
    u'\u055B' : ['(EMPHASIS MARK)','՛'],
    u'\u055C' : ['(EXCLAMATION MARK)','՜'],
    u'\u055D' : ['(COMMA)','՝'],
    u'\u055E' : ['(QUESTION MARK)','՞'],
    u'\u055F' : ['(ABBREVIATION MARK)','՟'],
    u'\u0561' : ['A','ա'],
    u'\u0562' : ['b','բ'],
    u'\u0563' : ['g','գ'],
    u'\u0564' : ['d','դ'],
    u'\u0565' : ['j E','ե'],
    u'\u0566' : ['z','զ'],
    u'\u0567' : ['e','է'],
    u'\u0568' : ['>','ը'],
    u'\u0569' : ['th','թ'],
    u'\u056A' : ['Z','ժ'],
    u'\u056B' : ['i','ի'],
    u'\u056C' : ['l','լ'],
    u'\u056D' : ['x','խ'],
    u'\u056E' : ['ts','ծ'],
    u'\u056F' : ['k','կ'],
    u'\u0570' : ['h','հ'],
    u'\u0571' : ['dz','ձ'],
    u'\u0572' : ['G','ղ'],
    u'\u0573' : ['tS','ճ'],
    u'\u0574' : ['m','մ'],
    u'\u0575' : ['h, j','յ'],
    u'\u0576' : ['n','ն'],
    u'\u0577' : ['S','շ'],
    u'\u0578' : ['o','ո'],
    u'\u0579' : ['tSh','չ'],
    u'\u057A' : ['p','պ'],
    u'\u057B' : ['dZ','ջ'],
    u'\u057C' : ['r','ռ'],
    u'\u057D' : ['s','ս'],
    u'\u057E' : ['v','վ'],
    u'\u057F' : ['t','տ'],
    u'\u0580' : ['9','ր'],
    u'\u0581' : ['tsh','ց'],
    u'\u0582' : ['w','ւ'],
    u'\u0583' : ['ph','փ'],
    u'\u0584' : ['kh','ք'],
    u'\u0585' : ['o','օ'],
    u'\u0586' : ['f','ֆ'],
    u'\u0587' : ['E w','և'],
    u'\u0589' : ['(FULL STOP)','։'],
    u'\u058A' : ['(HYPHEN)','֊'],
    u'\u0981' : ['(CD)','ঁ'],
    u'\u0982' : ['(M)','ং'],
    u'\u0983' : ['(H)','ঃ'],
    u'\u0985' : ['>, o','অ'],
    u'\u0986' : ['A','আ'],
    u'\u0987' : ['i','ই'],
    u'\u0988' : ['i:','ঈ'],
    u'\u0989' : ['u','উ'],
    u'\u098A' : ['u:','ঊ'],
    u'\u098B' : ['9r i','ঋ'],
    u'\u098C' : ['l=','ঌ'],
    u'\u098D' : ['(R:098D)','঍'],
    u'\u098E' : ['(R:098E)','঎'],
    u'\u098F' : ['@, e','এ'],
    u'\u0990' : ['oI','ঐ'],
    u'\u0991' : ['(R:0991)','঑'],
    u'\u0992' : ['(R:0992)','঒'],
    u'\u0993' : ['o','ও'],
    u'\u0994' : ['oU','ঔ'],
    u'\u0995' : ['k A','ক'],
    u'\u0996' : ['kh A','খ'],
    u'\u0997' : ['g A','গ'],
    u'\u0998' : ['gh A','ঘ'],
    u'\u0999' : ['N A','ঙ'],
    u'\u099A' : ['c A','চ'],
    u'\u099B' : ['ch A','ছ'],
    u'\u099C' : ['J A','জ'],
    u'\u099D' : ['Jh A','ঝ'],
    u'\u099E' : ['n~ A','ঞ'],
    u'\u099F' : ['tr A','ট'],
    u'\u09A0' : ['tR A','ঠ'],
    u'\u09A1' : ['dr A','ড'],
    u'\u09A2' : ['dR A','ঢ'],
    u'\u09A3' : ['nr A','ণ'],
    u'\u09A4' : ['t[ A','ত'],
    u'\u09A5' : ['t[_h A','থ'],
    u'\u09A6' : ['d[ A','দ'],
    u'\u09A7' : ['d[_h A','ধ'],
    u'\u09A8' : ['n[ A','ন'],
    u'\u09A9' : ['(R:09A9)','঩'],
    u'\u09AA' : ['p A','প'],
    u'\u09AB' : ['ph A','ফ'],
    u'\u09AC' : ['b A','ব'],
    u'\u09AD' : ['bh A','ভ'],
    u'\u09AE' : ['m A','ম'],
    u'\u09AF' : ['j A','য'],
    u'\u09B0' : ['9r A','র'],
    u'\u09B1' : ['(R:09B1)','঱'],
    u'\u09B2' : ['l A','ল'],
    u'\u09B3' : ['(R:09B3)','঳'],
    u'\u09B4' : ['(R:09B4)','঴'],
    u'\u09B5' : ['(R:09B5)','঵'],
    u'\u09B6' : ['S A','শ'],
    u'\u09B7' : ['S A','ষ'],
    u'\u09B8' : ['s A','স'],
    u'\u09B9' : ['hv A','হ'],
    u'\u09BA' : ['(##)','঺'],
    u'\u09BC' : ['(NUKTA)','়'],
    u'\u09BD' : ['(AVAGRAHA)','ঽ'],
    u'\u09BE' : ['A','া'],
    u'\u09BF' : ['i','ি'],
    u'\u09C0' : ['i:','ী'],
    u'\u09C1' : ['u','ু'],
    u'\u09C2' : ['u:','ূ'],
    u'\u09C3' : ['9r=','ৃ'],
    u'\u09C4' : ['rr=','ৄ'],
    u'\u09C5' : ['(R:09C5)','৅'],
    u'\u09C6' : ['(R:09C6)','৆'],
    u'\u09C7' : ['e','ে'],
    u'\u09C8' : ['oI','ৈ'],
    u'\u09C9' : ['(R:09C9)','৉'],
    u'\u09CA' : ['(R:09CA)','৊'],
    u'\u09CB' : ['o','ো'],
    u'\u09CC' : ['oU','ৌ'],
    u'\u09CD' : ['(P)','্'],
    u'\u09CE' : ['t A','ৎ'],
    u'\u09D7' : ['(oU LENGTH MARK)','ৗ'],
    u'\u09DC' : ['rr A','ড়'],
    u'\u09DD' : ['rh A','ঢ়'],
    u'\u09DE' : ['t A','৞'],
    u'\u09DF' : ['jr A','য়'],
    u'\u09E0' : ['rr=','ৠ'],
    u'\u09E1' : ['l=','ৡ'],
    u'\u09E2' : ['l=','ৢ'],
    u'\u09E3' : ['l=','ৣ'],
    u'\u09E4' : ['(*)','৤'],
    u'\u09E5' : ['(**)','৥'],
    u'\u09E6' : ['(ZERO)','০'],
    u'\u09E7' : ['(ONE)','১'],
    u'\u09E8' : ['(TWO)','২'],
    u'\u09E9' : ['(THREE)','৩'],
    u'\u09EA' : ['(FOUR)','৪'],
    u'\u09EB' : ['(FIVE)','৫'],
    u'\u09EC' : ['(SIX)','৬'],
    u'\u09ED' : ['(SEVEN)','৭'],
    u'\u09EE' : ['(EIGHT)','৮'],
    u'\u09EF' : ['(NINE)','৯'],
    u'\u09F0' : ['9r A','ৰ'],
    u'\u09F1' : ['9r A','ৱ'],
    u'\u09F2' : ['(RUPEE MARK)','৲'],
    u'\u09F3' : ['(RUPEE SIGN)','৳'],
    u'\u09F4' : ['(CURRENCY NUMERATOR 1)','৴'],
    u'\u09F5' : ['(CURRENCY NUMERATOR 2)','৵'],
    u'\u09F6' : ['(CURRENCY NUMERATOR 3)','৶'],
    u'\u09F7' : ['(CURRENCY NUMERATOR 4)','৷'],
    u'\u09F8' : ['(CURRENCY NUMERATOR 1 LESS THAN THE DENOMINATOR)','৸'],
    u'\u09F9' : ['(CURRENCY NUMERATOR 16)','৹'],
    u'\u09FA' : ['(ISSHAR)','৺'],
    u'\u1740' : ['A','ᝀ'],
    u'\u1741' : ['i','ᝁ'],
    u'\u1742' : ['u','ᝂ'],
    u'\u1743' : ['k A','ᝃ'],
    u'\u1744' : ['g A','ᝄ'],
    u'\u1745' : ['N A','ᝅ'],
    u'\u1746' : ['t A','ᝆ'],
    u'\u1747' : ['d A','ᝇ'],
    u'\u1748' : ['n A','ᝈ'],
    u'\u1749' : ['p A','ᝉ'],
    u'\u174A' : ['b A','ᝊ'],
    u'\u174B' : ['m A','ᝋ'],
    u'\u174C' : ['j A','ᝌ'],
    u'\u174D' : ['r A','ᝍ'],
    u'\u174E' : ['l A','ᝎ'],
    u'\u174F' : ['w A','ᝏ'],
    u'\u1750' : ['s A','ᝐ'],
    u'\u1751' : ['h A','ᝑ'],
    u'\u1752' : ['i','ᝒ'],
    u'\u1753' : ['u','ᝓ'],
    u'\u13A0' : ['A','Ꭰ'],
    u'\u13A1' : ['e','Ꭱ'],
    u'\u13A2' : ['i','Ꭲ'],
    u'\u13A3' : ['o','Ꭳ'],
    u'\u13A4' : ['u','Ꭴ'],
    u'\u13A5' : ['&~','Ꭵ'],
    u'\u13A6' : ['k A','Ꭶ'],
    u'\u13A7' : ['kh A','Ꭷ'],
    u'\u13A8' : ['k e','Ꭸ'],
    u'\u13A9' : ['k i','Ꭹ'],
    u'\u13AA' : ['k o','Ꭺ'],
    u'\u13AB' : ['k u','Ꭻ'],
    u'\u13AC' : ['k &~','Ꭼ'],
    u'\u13AD' : ['h A','Ꭽ'],
    u'\u13AE' : ['h e','Ꭾ'],
    u'\u13AF' : ['h i','Ꭿ'],
    u'\u13B0' : ['h o','Ꮀ'],
    u'\u13B1' : ['h u','Ꮁ'],
    u'\u13B2' : ['h &~','Ꮂ'],
    u'\u13B3' : ['l A','Ꮃ'],
    u'\u13B4' : ['l e','Ꮄ'],
    u'\u13B5' : ['l i','Ꮅ'],
    u'\u13B6' : ['l o','Ꮆ'],
    u'\u13B7' : ['l u','Ꮇ'],
    u'\u13B8' : ['l &~','Ꮈ'],
    u'\u13B9' : ['m A','Ꮉ'],
    u'\u13BA' : ['m e','Ꮊ'],
    u'\u13BB' : ['m i','Ꮋ'],
    u'\u13BC' : ['m o','Ꮌ'],
    u'\u13BD' : ['m u','Ꮍ'],
    u'\u13BE' : ['n A','Ꮎ'],
    u'\u13BF' : ['h n A','Ꮏ'],
    u'\u13C0' : ['nh A','Ꮐ'],
    u'\u13C1' : ['n e','Ꮑ'],
    u'\u13C2' : ['n i','Ꮒ'],
    u'\u13C3' : ['n o','Ꮓ'],
    u'\u13C4' : ['n u','Ꮔ'],
    u'\u13C5' : ['n &~','Ꮕ'],
    u'\u13C6' : ['kw A','Ꮖ'],
    u'\u13C7' : ['kw e','Ꮗ'],
    u'\u13C8' : ['kw i','Ꮘ'],
    u'\u13C9' : ['kw o','Ꮙ'],
    u'\u13CA' : ['kw u','Ꮚ'],
    u'\u13CB' : ['kw &~','Ꮛ'],
    u'\u13CC' : ['s A','Ꮜ'],
    u'\u13CD' : ['s','Ꮝ'],
    u'\u13CE' : ['s e','Ꮞ'],
    u'\u13CF' : ['s i','Ꮟ'],
    u'\u13D0' : ['s o','Ꮠ'],
    u'\u13D1' : ['s u','Ꮡ'],
    u'\u13D2' : ['s &~','Ꮢ'],
    u'\u13D3' : ['t A','Ꮣ'],
    u'\u13D4' : ['th A','Ꮤ'],
    u'\u13D5' : ['t e','Ꮥ'],
    u'\u13D6' : ['th e','Ꮦ'],
    u'\u13D7' : ['t i','Ꮧ'],
    u'\u13D8' : ['th i','Ꮨ'],
    u'\u13D9' : ['t o','Ꮩ'],
    u'\u13DA' : ['t u','Ꮪ'],
    u'\u13DB' : ['t &~','Ꮫ'],
    u'\u13DC' : ['tl A','Ꮬ'],
    u'\u13DD' : ['thl A','Ꮭ'],
    u'\u13DE' : ['thl e','Ꮮ'],
    u'\u13DF' : ['thl i','Ꮯ'],
    u'\u13E0' : ['thl o','Ꮰ'],
    u'\u13E1' : ['thl u','Ꮱ'],
    u'\u13E2' : ['thl &~','Ꮲ'],
    u'\u13E3' : ['ts A','Ꮳ'],
    u'\u13E4' : ['ts e','Ꮴ'],
    u'\u13E5' : ['ts i','Ꮵ'],
    u'\u13E6' : ['ts o','Ꮶ'],
    u'\u13E7' : ['ts u','Ꮷ'],
    u'\u13E8' : ['ts &~','Ꮸ'],
    u'\u13E9' : ['w A','Ꮹ'],
    u'\u13EA' : ['w e','Ꮺ'],
    u'\u13EB' : ['w i','Ꮻ'],
    u'\u13EC' : ['w o','Ꮼ'],
    u'\u13ED' : ['w u','Ꮽ'],
    u'\u13EE' : ['w &~','Ꮾ'],
    u'\u13EF' : ['j A','Ꮿ'],
    u'\u13F0' : ['j e','Ᏸ'],
    u'\u13F1' : ['j i','Ᏹ'],
    u'\u13F2' : ['j o','Ᏺ'],
    u'\u13F3' : ['j u','Ᏻ'],
    u'\u13F4' : ['j &~','Ᏼ'],
    u'\u2C80' : ['A','Ⲁ'],
    u'\u2C81' : ['A','ⲁ'],
    u'\u2C82' : ['v','Ⲃ'],
    u'\u2C83' : ['v','ⲃ'],
    u'\u2C84' : ['k','Ⲅ'],
    u'\u2C85' : ['k','ⲅ'],
    u'\u2C86' : ['t','Ⲇ'],
    u'\u2C87' : ['t','ⲇ'],
    u'\u2C88' : ['e','Ⲉ'],
    u'\u2C89' : ['e','ⲉ'],
    u'\u2C8A' : ['(SIX)','Ⲋ'],
    u'\u2C8B' : ['(SIX)','ⲋ'],
    u'\u2C8C' : ['s','Ⲍ'],
    u'\u2C8D' : ['s','ⲍ'],
    u'\u2C8E' : ['e:','Ⲏ'],
    u'\u2C8F' : ['e:','ⲏ'],
    u'\u2C90' : ['th','Ⲑ'],
    u'\u2C91' : ['th','ⲑ'],
    u'\u2C92' : ['i','Ⲓ'],
    u'\u2C93' : ['i','ⲓ'],
    u'\u2C94' : ['k','Ⲕ'],
    u'\u2C95' : ['k','ⲕ'],
    u'\u2C96' : ['l','Ⲗ'],
    u'\u2C97' : ['l','ⲗ'],
    u'\u2C98' : ['m','Ⲙ'],
    u'\u2C99' : ['m','ⲙ'],
    u'\u2C9A' : ['n','Ⲛ'],
    u'\u2C9B' : ['n','ⲛ'],
    u'\u2C9C' : ['k s','Ⲝ'],
    u'\u2C9D' : ['k s','ⲝ'],
    u'\u2C9E' : ['o','Ⲟ'],
    u'\u2C9F' : ['o','ⲟ'],
    u'\u2CA0' : ['p','Ⲡ'],
    u'\u2CA1' : ['p','ⲡ'],
    u'\u2CA2' : ['r','Ⲣ'],
    u'\u2CA3' : ['r','ⲣ'],
    u'\u2CA4' : ['s','Ⲥ'],
    u'\u2CA5' : ['s','ⲥ'],
    u'\u2CA6' : ['t','Ⲧ'],
    u'\u2CA7' : ['t','ⲧ'],
    u'\u2CA8' : ['u','Ⲩ'],
    u'\u2CA9' : ['u','ⲩ'],
    u'\u2CAA' : ['ph','Ⲫ'],
    u'\u2CAB' : ['ph','ⲫ'],
    u'\u2CAC' : ['kh','Ⲭ'],
    u'\u2CAD' : ['kh','ⲭ'],
    u'\u2CAE' : ['p s','Ⲯ'],
    u'\u2CAF' : ['p s','ⲯ'],
    u'\u2CB0' : ['o:','Ⲱ'],
    u'\u2CB1' : ['o:','ⲱ'],
    u'\u2CB2' : ['A','Ⲳ'],
    u'\u2CB3' : ['A','ⲳ'],
    u'\u2CB4' : ['i','Ⲵ'],
    u'\u2CB5' : ['i','ⲵ'],
    u'\u2CB6' : ['e','Ⲷ'],
    u'\u2CB7' : ['e','ⲷ'],
    u'\u2CB8' : ['k','Ⲹ'],
    u'\u2CB9' : ['k','ⲹ'],
    u'\u2CBA' : ['n','Ⲻ'],
    u'\u2CBB' : ['n','ⲻ'],
    u'\u2CBC' : ['n','Ⲽ'],
    u'\u2CBD' : ['n','ⲽ'],
    u'\u2CBE' : ['o:','Ⲿ'],
    u'\u2CBF' : ['o:','ⲿ'],
    u'\u2CC0' : ['s','Ⳁ'],
    u'\u2CC1' : ['s','ⳁ'],
    u'\u2CC2' : ['S','Ⳃ'],
    u'\u2CC3' : ['S','ⳃ'],
    u'\u2CC4' : ['S','Ⳅ'],
    u'\u2CC5' : ['S','ⳅ'],
    u'\u2CC6' : ['s','Ⳇ'],
    u'\u2CC7' : ['s','ⳇ'],
    u'\u2CC8' : ['f','Ⳉ'],
    u'\u2CC9' : ['f','ⳉ'],
    u'\u2CCA' : ['h','Ⳋ'],
    u'\u2CCB' : ['h','ⳋ'],
    u'\u2CCC' : ['h','Ⳍ'],
    u'\u2CCD' : ['h','ⳍ'],
    u'\u2CCE' : ['h','Ⳏ'],
    u'\u2CCF' : ['h','ⳏ'],
    u'\u2CD0' : ['h','Ⳑ'],
    u'\u2CD1' : ['h','ⳑ'],
    u'\u2CD2' : ['ei','Ⳓ'],
    u'\u2CD3' : ['ei','ⳓ'],
    u'\u2CD4' : ['h','Ⳕ'],
    u'\u2CD5' : ['h','ⳕ'],
    u'\u2CD6' : ['dZ','Ⳗ'],
    u'\u2CD7' : ['dZ','ⳗ'],
    u'\u2CD8' : ['dZ','Ⳙ'],
    u'\u2CD9' : ['dZ','ⳙ'],
    u'\u2CDA' : ['s h','Ⳛ'],
    u'\u2CDB' : ['s h','ⳛ'],
    u'\u2CDC' : ['s h','Ⳝ'],
    u'\u2CDD' : ['s h','ⳝ'],
    u'\u2CDE' : ['N i','Ⳟ'],
    u'\u2CDF' : ['N i','ⳟ'],
    u'\u2CE0' : ['nj','Ⳡ'],
    u'\u2CE1' : ['nj','ⳡ'],
    u'\u2CE2' : ['w au','Ⳣ'],
    u'\u2CE3' : ['w au','ⳣ'],
    u'\u2CE4' : ['(KAI)','ⳤ'],
    u'\u2CE5' : ['(MI RO)','⳥'],
    u'\u2CE6' : ['(PI RO)','⳦'],
    u'\u2CE7' : ['(STAUROS)','⳧'],
    u'\u2CE8' : ['(TAU RO)','⳨'],
    u'\u2CE9' : ['(KHI RO)','⳩'],
    u'\u2CEA' : ['(SHIMA SIMA)','⳪'],
    u'\u2CF9' : ['(FULL STOP)','⳹'],
    u'\u2CFA' : ['(DIRECT QUESTION MARK)','⳺'],
    u'\u2CFB' : ['(INDIRECT QUESTION MARK)','⳻'],
    u'\u2CFC' : ['(VERSE DIVIDER)','⳼'],
    u'\u2CFD' : ['(FRACTION ONE HALF)','⳽'],
    u'\u2CFE' : ['(FULL STOP)','⳾'],
    u'\u0400' : ['ie','Ѐ'],
    u'\u0401' : ['io','Ё'],
    u'\u0402' : ['d z}','Ђ'],
    u'\u0403' : ['d z}','Ѓ'],
    u'\u0404' : ['ie','Є'],
    u'\u0405' : ['dz','Ѕ'],
    u'\u0406' : ['i','І'],
    u'\u0407' : ['j','Ї'],
    u'\u0408' : ['j','Ј'],
    u'\u0409' : ['lj','Љ'],
    u'\u040A' : ['n~','Њ'],
    u'\u040B' : ['tSj','Ћ'],
    u'\u040C' : ['kj, tSj','Ќ'],
    u'\u040D' : ['i','Ѝ'],
    u'\u040E' : ['u(','Ў'],
    u'\u040F' : ['dZ','Џ'],
    u'\u0410' : ['A','А'],
    u'\u0411' : ['b','Б'],
    u'\u0412' : ['v','В'],
    u'\u0413' : ['g','Г'],
    u'\u0414' : ['d','Д'],
    u'\u0415' : ['ie','Е'],
    u'\u0416' : ['Z','Ж'],
    u'\u0417' : ['z','З'],
    u'\u0418' : ['i','И'],
    u'\u0419' : ['j','Й'],
    u'\u041A' : ['k','К'],
    u'\u041B' : ['l','Л'],
    u'\u041C' : ['m','М'],
    u'\u041D' : ['n','Н'],
    u'\u041E' : ['>','О'],
    u'\u041F' : ['p','П'],
    u'\u0420' : ['r','Р'],
    u'\u0421' : ['s','С'],
    u'\u0422' : ['t','Т'],
    u'\u0423' : ['u','У'],
    u'\u0424' : ['f','Ф'],
    u'\u0425' : ['x','Х'],
    u'\u0426' : ['ts','Ц'],
    u'\u0427' : ['tSj','Ч'],
    u'\u0428' : ['S','Ш'],
    u'\u0429' : ['Sj tSj','Щ'],
    u'\u042A' : ['(HARD SIGN)','Ъ'],
    u'\u042B' : ['ix','Ы'],
    u'\u042C' : ['(SOFT SIGN)','Ь'],
    u'\u042D' : ['E','Э'],
    u'\u042E' : ['u','Ю'],
    u'\u042F' : ['A','Я'],
    u'\u0430' : ['A','а'],
    u'\u0431' : ['b','б'],
    u'\u0432' : ['v','в'],
    u'\u0433' : ['g','г'],
    u'\u0434' : ['d','д'],
    u'\u0435' : ['ie','е'],
    u'\u0436' : ['Z','ж'],
    u'\u0437' : ['z','з'],
    u'\u0438' : ['i','и'],
    u'\u0439' : ['j','й'],
    u'\u043A' : ['k','к'],
    u'\u043B' : ['l','л'],
    u'\u043C' : ['m','м'],
    u'\u043D' : ['n','н'],
    u'\u043E' : ['>','о'],
    u'\u043F' : ['p','п'],
    u'\u0440' : ['r','р'],
    u'\u0441' : ['s','с'],
    u'\u0442' : ['t','т'],
    u'\u0443' : ['u','у'],
    u'\u0444' : ['f','ф'],
    u'\u0445' : ['x','х'],
    u'\u0446' : ['ts','ц'],
    u'\u0447' : ['tSj','ч'],
    u'\u0448' : ['S','ш'],
    u'\u0449' : ['Sj tSj','щ'],
    u'\u044A' : ['(HARD SIGN)','ъ'],
    u'\u044B' : ['ix','ы'],
    u'\u044C' : ['(SOFT SIGN)','ь'],
    u'\u044D' : ['E','э'],
    u'\u044E' : ['u','ю'],
    u'\u044F' : ['A','я'],
    u'\u0450' : ['E','ѐ'],
    u'\u0451' : ['io','ё'],
    u'\u0452' : ['d Z}','ђ'],
    u'\u0453' : ['d z}','ѓ'],
    u'\u0454' : ['ie','є'],
    u'\u0455' : ['dz','ѕ'],
    u'\u0456' : ['i','і'],
    u'\u0457' : ['j','ї'],
    u'\u0458' : ['j','ј'],
    u'\u0459' : ['lj','љ'],
    u'\u045A' : ['n~','њ'],
    u'\u045B' : ['tSj','ћ'],
    u'\u045C' : ['kj, tSj','ќ'],
    u'\u045D' : ['i','ѝ'],
    u'\u045E' : ['u(','ў'],
    u'\u045F' : ['dZ','џ'],
    u'\u0460' : ['o','Ѡ'],
    u'\u0461' : ['o','ѡ'],
    u'\u0462' : ['@','Ѣ'],
    u'\u0463' : ['@','ѣ'],
    u'\u0464' : ['E','Ѥ'],
    u'\u0465' : ['E','ѥ'],
    u'\u0466' : ['E~','Ѧ'],
    u'\u0467' : ['E~','ѧ'],
    u'\u0468' : ['>~','Ѩ'],
    u'\u0469' : ['>~','ѩ'],
    u'\u046A' : ['E~','Ѫ'],
    u'\u046B' : ['E~','ѫ'],
    u'\u046C' : ['>~','Ѭ'],
    u'\u046D' : ['>~','ѭ'],
    u'\u046E' : ['(INFORMAL ABBREVIATION FOR 1000 PSI)','Ѯ'],
    u'\u046F' : ['(INFORMAL ABBREVIATION FOR 1000 PSI)','ѯ'],
    u'\u0470' : ['p s','Ѱ'],
    u'\u0471' : ['p s','ѱ'],
    u'\u0472' : ['f','Ѳ'],
    u'\u0473' : ['f','ѳ'],
    u'\u0474' : ['i','Ѵ'],
    u'\u0475' : ['i','ѵ'],
    u'\u0476' : ['i','Ѷ'],
    u'\u0477' : ['i','ѷ'],
    u'\u0478' : ['u','Ѹ'],
    u'\u0479' : ['u','ѹ'],
    u'\u047A' : ['o','Ѻ'],
    u'\u047B' : ['o','ѻ'],
    u'\u047C' : ['o','Ѽ'],
    u'\u047D' : ['o','ѽ'],
    u'\u047E' : ['o t[','Ѿ'],
    u'\u047F' : ['o t[','ѿ'],
    u'\u0480' : ['(NINETY)','Ҁ'],
    u'\u0481' : ['(NINETY)','ҁ'],
    u'\u0482' : ['(THOUSAND SIGN)','҂'],
    u'\u0483' : ['(TITLO)','҃'],
    u'\u0484' : ['(PALATALIZATION)','҄'],
    u'\u0485' : ['(DASIA PNEUMATA)','҅'],
    u'\u0486' : ['(PSILI PNEUMATA)','҆'],
    u'\u0488' : ['(HUNDRED THOUSAND SIGN)','҈'],
    u'\u0489' : ['(MILLION SIGN)','҉'],
    u'\u048A' : ['i','Ҋ'],
    u'\u048B' : ['i','ҋ'],
    u'\u048C' : ['(SEMISOFT SIGN)','Ҍ'],
    u'\u048D' : ['(SEMISOFT SIGN)','ҍ'],
    u'\u048E' : ['r','Ҏ'],
    u'\u048F' : ['r','ҏ'],
    u'\u0490' : ['g','Ґ'],
    u'\u0491' : ['g','ґ'],
    u'\u0492' : ['g','Ғ'],
    u'\u0493' : ['g','ғ'],
    u'\u0494' : ['g','Ҕ'],
    u'\u0495' : ['g','ҕ'],
    u'\u0496' : ['Z','Җ'],
    u'\u0497' : ['Z','җ'],
    u'\u0498' : ['z','Ҙ'],
    u'\u0499' : ['z','ҙ'],
    u'\u049A' : ['k','Қ'],
    u'\u049B' : ['k','қ'],
    u'\u049C' : ['k','Ҝ'],
    u'\u049D' : ['k','ҝ'],
    u'\u049E' : ['k','Ҟ'],
    u'\u049F' : ['k','ҟ'],
    u'\u04A0' : ['k','Ҡ'],
    u'\u04A1' : ['k','ҡ'],
    u'\u04A2' : ['n','Ң'],
    u'\u04A3' : ['n','ң'],
    u'\u04A4' : ['n g','Ҥ'],
    u'\u04A5' : ['n g','ҥ'],
    u'\u04A6' : ['p','Ҧ'],
    u'\u04A7' : ['p','ҧ'],
    u'\u04A8' : ['x','Ҩ'],
    u'\u04A9' : ['x','ҩ'],
    u'\u04AA' : ['s','Ҫ'],
    u'\u04AB' : ['s','ҫ'],
    u'\u04AC' : ['t','Ҭ'],
    u'\u04AD' : ['t','ҭ'],
    u'\u04AE' : ['u','Ү'],
    u'\u04AF' : ['u','ү'],
    u'\u04B0' : ['u','Ұ'],
    u'\u04B1' : ['u','ұ'],
    u'\u04B2' : ['x','Ҳ'],
    u'\u04B3' : ['x','ҳ'],
    u'\u04B4' : ['t t s','Ҵ'],
    u'\u04B5' : ['t t s','ҵ'],
    u'\u04B6' : ['tSj','Ҷ'],
    u'\u04B7' : ['tSj','ҷ'],
    u'\u04B8' : ['tSj','Ҹ'],
    u'\u04B9' : ['tSj','ҹ'],
    u'\u04BA' : ['h','Һ'],
    u'\u04BB' : ['h','һ'],
    u'\u04BC' : ['tSj','Ҽ'],
    u'\u04BD' : ['tSj','ҽ'],
    u'\u04BE' : ['tSj','Ҿ'],
    u'\u04BF' : ['tSj','ҿ'],
    u'\u04C0' : ['(ASPIRATION SIGN)','Ӏ'],
    u'\u04C1' : ['Z','Ӂ'],
    u'\u04C2' : ['Z','ӂ'],
    u'\u04C3' : ['k','Ӄ'],
    u'\u04C4' : ['k','ӄ'],
    u'\u04C5' : ['l','Ӆ'],
    u'\u04C6' : ['l','ӆ'],
    u'\u04C7' : ['n','Ӈ'],
    u'\u04C8' : ['n','ӈ'],
    u'\u04C9' : ['n','Ӊ'],
    u'\u04CA' : ['n','ӊ'],
    u'\u04CB' : ['tSj','Ӌ'],
    u'\u04CC' : ['tSj','ӌ'],
    u'\u04CD' : ['m','Ӎ'],
    u'\u04CE' : ['m','ӎ'],
    u'\u04D0' : ['A','Ӑ'],
    u'\u04D1' : ['A','ӑ'],
    u'\u04D2' : ['A','Ӓ'],
    u'\u04D3' : ['A','ӓ'],
    u'\u04D4' : ['A ie','Ӕ'],
    u'\u04D5' : ['A ie','ӕ'],
    u'\u04D6' : ['ie','Ӗ'],
    u'\u04D7' : ['ie','ӗ'],
    u'\u04D8' : ['&','Ә'],
    u'\u04D9' : ['&','ә'],
    u'\u04DA' : ['&','Ӛ'],
    u'\u04DB' : ['&','ӛ'],
    u'\u04DC' : ['Z','Ӝ'],
    u'\u04DD' : ['Z','ӝ'],
    u'\u04DE' : ['z','Ӟ'],
    u'\u04DF' : ['z','ӟ'],
    u'\u04E0' : ['dz','Ӡ'],
    u'\u04E1' : ['dz','ӡ'],
    u'\u04E2' : ['i:','Ӣ'],
    u'\u04E3' : ['i:','ӣ'],
    u'\u04E4' : ['i','Ӥ'],
    u'\u04E5' : ['i','ӥ'],
    u'\u04E6' : ['o','Ӧ'],
    u'\u04E7' : ['o','ӧ'],
    u'\u04E8' : ['o','Ө'],
    u'\u04E9' : ['o','ө'],
    u'\u04EA' : ['o','Ӫ'],
    u'\u04EB' : ['o','ӫ'],
    u'\u04EC' : ['E','Ӭ'],
    u'\u04ED' : ['E','ӭ'],
    u'\u04EE' : ['u:','Ӯ'],
    u'\u04EF' : ['u:','ӯ'],
    u'\u04F0' : ['u','Ӱ'],
    u'\u04F1' : ['u','ӱ'],
    u'\u04F2' : ['u','Ӳ'],
    u'\u04F3' : ['u','ӳ'],
    u'\u04F4' : ['tSj','Ӵ'],
    u'\u04F5' : ['tSj','ӵ'],
    u'\u04F6' : ['g','Ӷ'],
    u'\u04F7' : ['g','ӷ'],
    u'\u04F8' : ['ix','Ӹ'],
    u'\u04F9' : ['ix','ӹ'],
    u'\u0901' : ['(CD)','ँ'],
    u'\u0902' : ['(M)','ं'],
    u'\u0903' : ['(H)','ः'],
    u'\u0904' : ['A','ऄ'],
    u'\u0905' : ['A','अ'],
    u'\u0906' : ['A:','आ'],
    u'\u0907' : ['i','इ'],
    u'\u0908' : ['i:','ई'],
    u'\u0909' : ['u','उ'],
    u'\u090A' : ['u:','ऊ'],
    u'\u090B' : ['9r=','ऋ'],
    u'\u090C' : ['l=','ऌ'],
    u'\u090D' : ['e','ऍ'],
    u'\u090E' : ['e','ऎ'],
    u'\u090F' : ['e','ए'],
    u'\u0910' : ['aI','ऐ'],
    u'\u0911' : ['o','ऑ'],
    u'\u0912' : ['o','ऒ'],
    u'\u0913' : ['o','ओ'],
    u'\u0914' : ['aU','औ'],
    u'\u0915' : ['k A','क'],
    u'\u0916' : ['kh A','ख'],
    u'\u0917' : ['g A','ग'],
    u'\u0918' : ['gh A','घ'],
    u'\u0919' : ['N A','ङ'],
    u'\u091A' : ['c A','च'],
    u'\u091B' : ['ch A','छ'],
    u'\u091C' : ['J A','ज'],
    u'\u091D' : ['Jh A','झ'],
    u'\u091E' : ['n~ A','ञ'],
    u'\u091F' : ['tr A','ट'],
    u'\u0920' : ['tR A','ठ'],
    u'\u0921' : ['dr A','ड'],
    u'\u0922' : ['dR A','ढ'],
    u'\u0923' : ['nr A','ण'],
    u'\u0924' : ['t[ A','त'],
    u'\u0925' : ['t[_h A','थ'],
    u'\u0926' : ['d[ A','द'],
    u'\u0927' : ['d[_h A','ध'],
    u'\u0928' : ['n[ A','न'],
    u'\u0929' : ['n A','ऩ'],
    u'\u092A' : ['p A','प'],
    u'\u092B' : ['ph A','फ'],
    u'\u092C' : ['b A','ब'],
    u'\u092D' : ['bh A','भ'],
    u'\u092E' : ['m A','म'],
    u'\u092F' : ['j A','य'],
    u'\u0930' : ['9r A','र'],
    u'\u0931' : ['9r A','ऱ'],
    u'\u0932' : ['l A','ल'],
    u'\u0933' : ['lr A','ळ'],
    u'\u0934' : ['l A','ऴ'],
    u'\u0935' : ['v A','व'],
    u'\u0936' : ['c} A','श'],
    u'\u0937' : ['sr A','ष'],
    u'\u0938' : ['s A','स'],
    u'\u0939' : ['hv A','ह'],
    u'\u093C' : ['(NUKTA)','़'],
    u'\u093D' : ['(AVAGRAHA)','ऽ'],
    u'\u093E' : ['A:','ा'],
    u'\u093F' : ['i','ि'],
    u'\u0940' : ['i:','ी'],
    u'\u0941' : ['u','ु'],
    u'\u0942' : ['u:','ू'],
    u'\u0943' : ['9r=','ृ'],
    u'\u0944' : ['rr=','ॄ'],
    u'\u0945' : ['e','ॅ'],
    u'\u0946' : ['e','ॆ'],
    u'\u0947' : ['e','े'],
    u'\u0948' : ['aI','ै'],
    u'\u0949' : ['o','ॉ'],
    u'\u094A' : ['o','ॊ'],
    u'\u094B' : ['o','ो'],
    u'\u094C' : ['aU','ौ'],
    u'\u094D' : ['(P)','्'],
    u'\u0950' : ['(OM)','ॐ'],
    u'\u0951' : ['(UDATTA)','॑'],
    u'\u0952' : ['(ADUDATTA)','॒'],
    u'\u0953' : ['(GRAVE ACCENT)','॓'],
    u'\u0954' : ['(ACUTE ACCENT)','॔'],
    u'\u0958' : ['q A','क़'],
    u'\u0959' : ['x A','ख़'],
    u'\u095A' : ['G A','ग़'],
    u'\u095B' : ['z A','ज़'],
    u'\u095C' : ['rr A','ड़'],
    u'\u095D' : ['rrh A','ढ़'],
    u'\u095E' : ['f A','फ़'],
    u'\u095F' : ['j','य़'],
    u'\u0960' : ['rr=','ॠ'],
    u'\u0961' : ['lr=','ॡ'],
    u'\u0962' : ['l=','ॢ'],
    u'\u0963' : ['l=','ॣ'],
    u'\u0964' : ['(*)','।'],
    u'\u0965' : ['(**)','॥'],
    u'\u0966' : ['(ZERO)','०'],
    u'\u0967' : ['(ONE)','१'],
    u'\u0968' : ['(TWO)','२'],
    u'\u0969' : ['(THREE)','३'],
    u'\u096A' : ['(FOUR)','४'],
    u'\u096B' : ['(FIVE)','५'],
    u'\u096C' : ['(SIX)','६'],
    u'\u096D' : ['(SEVEN)','७'],
    u'\u096E' : ['(EIGHT)','८'],
    u'\u096F' : ['(NINE)','९'],
    u'\u0970' : ['(ABBREVIATION)','॰'],
    u'\u097D' : ['?','ॽ'],
    u'\u1200' : ['h 3','ሀ'],
    u'\u1201' : ['h u','ሁ'],
    u'\u1202' : ['h i','ሂ'],
    u'\u1203' : ['h A','ሃ'],
    u'\u1204' : ['h e','ሄ'],
    u'\u1205' : ['h &, h','ህ'],
    u'\u1206' : ['h o','ሆ'],
    u'\u1207' : ['h o 3','ሇ'],
    u'\u1208' : ['l 3','ለ'],
    u'\u1209' : ['l u','ሉ'],
    u'\u120A' : ['l i','ሊ'],
    u'\u120B' : ['l A','ላ'],
    u'\u120C' : ['l e','ሌ'],
    u'\u120D' : ['l &, l','ል'],
    u'\u120E' : ['l o','ሎ'],
    u'\u120F' : ['l u 3','ሏ'],
    u'\u1210' : ['h 3','ሐ'],
    u'\u1211' : ['h u','ሑ'],
    u'\u1212' : ['h i','ሒ'],
    u'\u1213' : ['h A','ሓ'],
    u'\u1214' : ['h e','ሔ'],
    u'\u1215' : ['h &, h','ሕ'],
    u'\u1216' : ['h o','ሖ'],
    u'\u1217' : ['h u 3','ሗ'],
    u'\u1218' : ['m 3','መ'],
    u'\u1219' : ['m u','ሙ'],
    u'\u121A' : ['m i','ሚ'],
    u'\u121B' : ['m A','ማ'],
    u'\u121C' : ['m e','ሜ'],
    u'\u121D' : ['m &, m','ም'],
    u'\u121E' : ['m o','ሞ'],
    u'\u121F' : ['m u 3','ሟ'],
    u'\u1220' : ['s 3','ሠ'],
    u'\u1221' : ['s u','ሡ'],
    u'\u1222' : ['s i','ሢ'],
    u'\u1223' : ['s A','ሣ'],
    u'\u1224' : ['s e','ሤ'],
    u'\u1225' : ['s &, s','ሥ'],
    u'\u1226' : ['s o','ሦ'],
    u'\u1227' : ['s u 3','ሧ'],
    u'\u1228' : ['9r 3','ረ'],
    u'\u1229' : ['9r u','ሩ'],
    u'\u122A' : ['9r i','ሪ'],
    u'\u122B' : ['9r A','ራ'],
    u'\u122C' : ['9r e','ሬ'],
    u'\u122D' : ['9r &, 9r','ር'],
    u'\u122E' : ['9r o','ሮ'],
    u'\u122F' : ['9r u 3','ሯ'],
    u'\u1230' : ['s 3','ሰ'],
    u'\u1231' : ['s u','ሱ'],
    u'\u1232' : ['s i','ሲ'],
    u'\u1233' : ['s A','ሳ'],
    u'\u1234' : ['s e','ሴ'],
    u'\u1235' : ['s &, s','ስ'],
    u'\u1236' : ['s o','ሶ'],
    u'\u1237' : ['s u 3','ሷ'],
    u'\u1238' : ['S 3','ሸ'],
    u'\u1239' : ['S u','ሹ'],
    u'\u123A' : ['S i','ሺ'],
    u'\u123B' : ['S A','ሻ'],
    u'\u123C' : ['S e','ሼ'],
    u'\u123D' : ['S &, S','ሽ'],
    u'\u123E' : ['S o','ሾ'],
    u'\u123F' : ['S u 3','ሿ'],
    u'\u1240' : ['k> 3','ቀ'],
    u'\u1241' : ['k> u','ቁ'],
    u'\u1242' : ['k> i','ቂ'],
    u'\u1243' : ['k> A','ቃ'],
    u'\u1244' : ['k> e','ቄ'],
    u'\u1245' : ['k> &, k>','ቅ'],
    u'\u1246' : ['k> o','ቆ'],
    u'\u1247' : ['k> o 3','ቇ'],
    u'\u1248' : ['k> u 3','ቈ'],
    u'\u124A' : ['k> ui','ቊ'],
    u'\u124B' : ['k> ua','ቋ'],
    u'\u124C' : ['k> u e','ቌ'],
    u'\u124D' : ['k> u&','ቍ'],
    u'\u1250' : ['k> h 3','ቐ'],
    u'\u1251' : ['k> h u','ቑ'],
    u'\u1252' : ['k> h i','ቒ'],
    u'\u1253' : ['k> h A','ቓ'],
    u'\u1254' : ['k> h e','ቔ'],
    u'\u1255' : ['k> h &, k> h','ቕ'],
    u'\u1256' : ['k> h o','ቖ'],
    u'\u125A' : ['k> h ui','ቚ'],
    u'\u125B' : ['k> h ua','ቛ'],
    u'\u125C' : ['k> h u e','ቜ'],
    u'\u125D' : ['k> h u&','ቝ'],
    u'\u1260' : ['b 3','በ'],
    u'\u1261' : ['b u','ቡ'],
    u'\u1262' : ['b i','ቢ'],
    u'\u1263' : ['b A','ባ'],
    u'\u1264' : ['b e','ቤ'],
    u'\u1265' : ['b &, b','ብ'],
    u'\u1266' : ['b o','ቦ'],
    u'\u1267' : ['b u 3','ቧ'],
    u'\u1268' : ['v 3','ቨ'],
    u'\u1269' : ['v u','ቩ'],
    u'\u126A' : ['v i','ቪ'],
    u'\u126B' : ['v A','ቫ'],
    u'\u126C' : ['v e','ቬ'],
    u'\u126D' : ['v &, v','ቭ'],
    u'\u126E' : ['v o','ቮ'],
    u'\u126F' : ['v u 3','ቯ'],
    u'\u1270' : ['t[ 3','ተ'],
    u'\u1271' : ['t[ u','ቱ'],
    u'\u1272' : ['t[ i','ቲ'],
    u'\u1273' : ['t[ A','ታ'],
    u'\u1274' : ['t[ e','ቴ'],
    u'\u1275' : ['t[ &, t[','ት'],
    u'\u1276' : ['t[ o','ቶ'],
    u'\u1277' : ['t[ u 3','ቷ'],
    u'\u1278' : ['tS 3','ቸ'],
    u'\u1279' : ['tS u','ቹ'],
    u'\u127A' : ['tS i','ቺ'],
    u'\u127B' : ['tS A','ቻ'],
    u'\u127C' : ['tS e','ቼ'],
    u'\u127D' : ['tS &, tS','ች'],
    u'\u127E' : ['tS o','ቾ'],
    u'\u127F' : ['tS u 3','ቿ'],
    u'\u1280' : ['h 3','ኀ'],
    u'\u1281' : ['h u','ኁ'],
    u'\u1282' : ['h i','ኂ'],
    u'\u1283' : ['h A','ኃ'],
    u'\u1284' : ['h e','ኄ'],
    u'\u1285' : ['h &, h','ኅ'],
    u'\u1286' : ['h o','ኆ'],
    u'\u1287' : ['h o 3','ኇ'],
    u'\u1288' : ['h u 3','ኈ'],
    u'\u128A' : ['h ui','ኊ'],
    u'\u128B' : ['h ua','ኋ'],
    u'\u128C' : ['h u e','ኌ'],
    u'\u128D' : ['h u&','ኍ'],
    u'\u1290' : ['n 3','ነ'],
    u'\u1291' : ['n u','ኑ'],
    u'\u1292' : ['n i','ኒ'],
    u'\u1293' : ['n A','ና'],
    u'\u1294' : ['n e','ኔ'],
    u'\u1295' : ['n &, n','ን'],
    u'\u1296' : ['n o','ኖ'],
    u'\u1297' : ['n u 3','ኗ'],
    u'\u1298' : ['n~ 3','ኘ'],
    u'\u1299' : ['n~ u','ኙ'],
    u'\u129A' : ['n~ i','ኚ'],
    u'\u129B' : ['n~ A','ኛ'],
    u'\u129C' : ['n~ e','ኜ'],
    u'\u129D' : ['n~ &, n~','ኝ'],
    u'\u129E' : ['n~ o','ኞ'],
    u'\u129F' : ['n~ u 3','ኟ'],
    u'\u12A0' : ['? 3','አ'],
    u'\u12A1' : ['? u','ኡ'],
    u'\u12A2' : ['? i','ኢ'],
    u'\u12A3' : ['? A','ኣ'],
    u'\u12A4' : ['? e','ኤ'],
    u'\u12A5' : ['? &, ?','እ'],
    u'\u12A6' : ['? o','ኦ'],
    u'\u12A7' : ['? u 3','ኧ'],
    u'\u12A8' : ['k 3','ከ'],
    u'\u12A9' : ['k u','ኩ'],
    u'\u12AA' : ['k i','ኪ'],
    u'\u12AB' : ['k A','ካ'],
    u'\u12AC' : ['k e','ኬ'],
    u'\u12AD' : ['k &, k','ክ'],
    u'\u12AE' : ['k o','ኮ'],
    u'\u12AF' : ['k o 3','ኯ'],
    u'\u12B0' : ['k u 3','ኰ'],
    u'\u12B2' : ['k ui','ኲ'],
    u'\u12B3' : ['k ua','ኳ'],
    u'\u12B4' : ['k u e','ኴ'],
    u'\u12B5' : ['k u&','ኵ'],
    u'\u12B8' : ['h 3','ኸ'],
    u'\u12B9' : ['h u','ኹ'],
    u'\u12BA' : ['h i','ኺ'],
    u'\u12BB' : ['h A','ኻ'],
    u'\u12BC' : ['h e','ኼ'],
    u'\u12BD' : ['h &, h','ኽ'],
    u'\u12BE' : ['h o','ኾ'],
    u'\u12C0' : ['h u 3','ዀ'],
    u'\u12C2' : ['h ui','ዂ'],
    u'\u12C3' : ['h ua','ዃ'],
    u'\u12C4' : ['h u e','ዄ'],
    u'\u12C5' : ['h u&','ዅ'],
    u'\u12C8' : ['w 3','ወ'],
    u'\u12C9' : ['w u','ዉ'],
    u'\u12CA' : ['w i','ዊ'],
    u'\u12CB' : ['w A','ዋ'],
    u'\u12CC' : ['w e','ዌ'],
    u'\u12CD' : ['w &, w','ው'],
    u'\u12CE' : ['w o','ዎ'],
    u'\u12CF' : ['w o 3','ዏ'],
    u'\u12D0' : ['? 3','ዐ'],
    u'\u12D1' : ['? u','ዑ'],
    u'\u12D2' : ['? i','ዒ'],
    u'\u12D3' : ['? A','ዓ'],
    u'\u12D4' : ['? e','ዔ'],
    u'\u12D5' : ['? &, ?','ዕ'],
    u'\u12D6' : ['? o','ዖ'],
    u'\u12D8' : ['z 3','ዘ'],
    u'\u12D9' : ['z u','ዙ'],
    u'\u12DA' : ['z i','ዚ'],
    u'\u12DB' : ['z A','ዛ'],
    u'\u12DC' : ['z e','ዜ'],
    u'\u12DD' : ['z &, z','ዝ'],
    u'\u12DE' : ['z o','ዞ'],
    u'\u12DF' : ['z u 3','ዟ'],
    u'\u12E0' : ['Z 3','ዠ'],
    u'\u12E1' : ['Z u','ዡ'],
    u'\u12E2' : ['Z i','ዢ'],
    u'\u12E3' : ['Z A','ዣ'],
    u'\u12E4' : ['Z e','ዤ'],
    u'\u12E5' : ['Z &, Z','ዥ'],
    u'\u12E6' : ['Z o','ዦ'],
    u'\u12E7' : ['Z u 3','ዧ'],
    u'\u12E8' : ['j 3','የ'],
    u'\u12E9' : ['j u','ዩ'],
    u'\u12EA' : ['j i','ዪ'],
    u'\u12EB' : ['j A','ያ'],
    u'\u12EC' : ['j e','ዬ'],
    u'\u12ED' : ['j &, j','ይ'],
    u'\u12EE' : ['j o','ዮ'],
    u'\u12EF' : ['j o 3','ዯ'],
    u'\u12F0' : ['d 3','ደ'],
    u'\u12F1' : ['d[ u','ዱ'],
    u'\u12F2' : ['d[ i','ዲ'],
    u'\u12F3' : ['d[ A','ዳ'],
    u'\u12F4' : ['d[ e','ዴ'],
    u'\u12F5' : ['d[ &, d[','ድ'],
    u'\u12F6' : ['d[ o','ዶ'],
    u'\u12F7' : ['d[ u 3','ዷ'],
    u'\u12F8' : ['d d 3','ዸ'],
    u'\u12F9' : ['d d u','ዹ'],
    u'\u12FA' : ['d d i','ዺ'],
    u'\u12FB' : ['d d A','ዻ'],
    u'\u12FC' : ['d d e','ዼ'],
    u'\u12FD' : ['d d &, d d','ዽ'],
    u'\u12FE' : ['d d o','ዾ'],
    u'\u12FF' : ['d d u 3','ዿ'],
    u'\u1300' : ['dZ 3','ጀ'],
    u'\u1301' : ['dZ u','ጁ'],
    u'\u1302' : ['dZ i','ጂ'],
    u'\u1303' : ['dZ A','ጃ'],
    u'\u1304' : ['dZ e','ጄ'],
    u'\u1305' : ['dZ &, dZ','ጅ'],
    u'\u1306' : ['dZ o','ጆ'],
    u'\u1307' : ['dZ u 3','ጇ'],
    u'\u1308' : ['g 3','ገ'],
    u'\u1309' : ['g u','ጉ'],
    u'\u130A' : ['g i','ጊ'],
    u'\u130B' : ['g A','ጋ'],
    u'\u130C' : ['g e','ጌ'],
    u'\u130D' : ['g &, g','ግ'],
    u'\u130E' : ['g o','ጎ'],
    u'\u130F' : ['g o 3','ጏ'],
    u'\u1310' : ['g u 3','ጐ'],
    u'\u1312' : ['g ui','ጒ'],
    u'\u1313' : ['g ua','ጓ'],
    u'\u1314' : ['g u e','ጔ'],
    u'\u1315' : ['g u&','ጕ'],
    u'\u1318' : ['g g 3','ጘ'],
    u'\u1319' : ['g g u','ጙ'],
    u'\u131A' : ['g g i','ጚ'],
    u'\u131B' : ['g g A','ጛ'],
    u'\u131C' : ['g g e','ጜ'],
    u'\u131D' : ['g g &, g g','ጝ'],
    u'\u131E' : ['g g o','ጞ'],
    u'\u131F' : ['g g ua','ጟ'],
    u'\u1320' : ['t[> 3','ጠ'],
    u'\u1321' : ['t[> u','ጡ'],
    u'\u1322' : ['t[> i','ጢ'],
    u'\u1323' : ['t[> A','ጣ'],
    u'\u1324' : ['t[> e','ጤ'],
    u'\u1325' : ['t[> &, t[>','ጥ'],
    u'\u1326' : ['t[> o','ጦ'],
    u'\u1327' : ['t[> u 3','ጧ'],
    u'\u1328' : ['tS> 3','ጨ'],
    u'\u1329' : ['tS> u','ጩ'],
    u'\u132A' : ['tS> i','ጪ'],
    u'\u132B' : ['tS> A','ጫ'],
    u'\u132C' : ['tS> e','ጬ'],
    u'\u132D' : ['tS> &, tS>','ጭ'],
    u'\u132E' : ['tS> o','ጮ'],
    u'\u132F' : ['tS> u 3','ጯ'],
    u'\u1330' : ['p> 3','ጰ'],
    u'\u1331' : ['p> u','ጱ'],
    u'\u1332' : ['p> i','ጲ'],
    u'\u1333' : ['p> A','ጳ'],
    u'\u1334' : ['p> e','ጴ'],
    u'\u1335' : ['p> &, p>','ጵ'],
    u'\u1336' : ['p> o','ጶ'],
    u'\u1337' : ['p> u 3','ጷ'],
    u'\u1338' : ['ts 3','ጸ'],
    u'\u1339' : ['ts u','ጹ'],
    u'\u133A' : ['ts i','ጺ'],
    u'\u133B' : ['ts A','ጻ'],
    u'\u133C' : ['ts e','ጼ'],
    u'\u133D' : ['ts &, ts','ጽ'],
    u'\u133E' : ['ts o','ጾ'],
    u'\u133F' : ['ts u 3','ጿ'],
    u'\u1340' : ['ts 3','ፀ'],
    u'\u1341' : ['ts u','ፁ'],
    u'\u1342' : ['ts i','ፂ'],
    u'\u1343' : ['ts A','ፃ'],
    u'\u1344' : ['ts e','ፄ'],
    u'\u1345' : ['ts &, ts','ፅ'],
    u'\u1346' : ['ts o','ፆ'],
    u'\u1347' : ['ts u 3','ፇ'],
    u'\u1348' : ['f 3','ፈ'],
    u'\u1349' : ['f u','ፉ'],
    u'\u134A' : ['f i','ፊ'],
    u'\u134B' : ['f A','ፋ'],
    u'\u134C' : ['f e','ፌ'],
    u'\u134D' : ['f &, f','ፍ'],
    u'\u134E' : ['f o','ፎ'],
    u'\u134F' : ['f u 3','ፏ'],
    u'\u1350' : ['p 3','ፐ'],
    u'\u1351' : ['p u','ፑ'],
    u'\u1352' : ['p i','ፒ'],
    u'\u1353' : ['p A','ፓ'],
    u'\u1354' : ['p e','ፔ'],
    u'\u1355' : ['p &, p','ፕ'],
    u'\u1356' : ['p o','ፖ'],
    u'\u1357' : ['p u 3','ፗ'],
    u'\u1358' : ['9rj a','ፘ'],
    u'\u1359' : ['mj a','ፙ'],
    u'\u135A' : ['fj a','ፚ'],
    u'\u135F' : ['(GEMINATION MARK)','፟'],
    u'\u1360' : ['(SECTION MARK)','፠'],
    u'\u1361' : ['(WORDSPACE)','፡'],
    u'\u1362' : ['(FULL STOP)','።'],
    u'\u1363' : ['(COMMA)','፣'],
    u'\u1364' : ['(SEMICOLON)','፤'],
    u'\u1365' : ['(COLON)','፥'],
    u'\u1366' : ['(PREFACE COLON)','፦'],
    u'\u1367' : ['(QUESTION MARK)','፧'],
    u'\u1368' : ['(PARAGRAPH SEPARATOR)','፨'],
    u'\u1369' : ['(ONE)','፩'],
    u'\u136A' : ['(TWO)','፪'],
    u'\u136B' : ['(THREE)','፫'],
    u'\u136C' : ['(FOUR)','፬'],
    u'\u136D' : ['(FIVE)','፭'],
    u'\u136E' : ['(SIX)','፮'],
    u'\u136F' : ['(SEVEN)','፯'],
    u'\u1370' : ['(EIGHT)','፰'],
    u'\u1371' : ['(NINE)','፱'],
    u'\u1372' : ['(TEN)','፲'],
    u'\u1373' : ['(TWENTY)','፳'],
    u'\u1374' : ['(THIRTY)','፴'],
    u'\u1375' : ['(FORTY)','፵'],
    u'\u1376' : ['(FIFTY)','፶'],
    u'\u1377' : ['(SIXTY)','፷'],
    u'\u1378' : ['(SEVENTY)','፸'],
    u'\u1379' : ['(EIGHT)','፹'],
    u'\u137A' : ['(NINETY)','፺'],
    u'\u137B' : ['(HUNDRED)','፻'],
    u'\u137C' : ['(TEN THOUSAND)','፼'],
    u'\u1380' : ['mw 3','ᎀ'],
    u'\u1381' : ['mw i','ᎁ'],
    u'\u1382' : ['mw 3','ᎂ'],
    u'\u1383' : ['mw &','ᎃ'],
    u'\u1384' : ['bw 3','ᎄ'],
    u'\u1385' : ['bw i','ᎅ'],
    u'\u1386' : ['bw e','ᎆ'],
    u'\u1387' : ['bw &','ᎇ'],
    u'\u1388' : ['fw 3','ᎈ'],
    u'\u1389' : ['fw i','ᎉ'],
    u'\u138A' : ['fw e','ᎊ'],
    u'\u138B' : ['fw &','ᎋ'],
    u'\u138C' : ['pw 3','ᎌ'],
    u'\u138D' : ['pw i','ᎍ'],
    u'\u138E' : ['pw e','ᎎ'],
    u'\u138F' : ['pw &','ᎏ'],
    u'\u1390' : ['(YIZET)','᎐'],
    u'\u1391' : ['(DERET)','᎑'],
    u'\u1392' : ['(RIKRIK)','᎒'],
    u'\u1393' : ['(SHORT RIKRIK)','᎓'],
    u'\u1394' : ['(DIFAT)','᎔'],
    u'\u1395' : ['(KENAT)','᎕'],
    u'\u1396' : ['(CHIRET)','᎖'],
    u'\u1397' : ['(HIDET)','᎗'],
    u'\u1398' : ['(DERET-HIDET)','᎘'],
    u'\u1399' : ['(HURT)','᎙'],
    u'\u2D80' : ['l o 3','ⶀ'],
    u'\u2D81' : ['m o 3','ⶁ'],
    u'\u2D82' : ['9r o 3','ⶂ'],
    u'\u2D83' : ['s o 3','ⶃ'],
    u'\u2D84' : ['S o 3','ⶄ'],
    u'\u2D85' : ['b o 3','ⶅ'],
    u'\u2D86' : ['t[ o 3','ⶆ'],
    u'\u2D87' : ['tS o 3','ⶇ'],
    u'\u2D88' : ['n o 3','ⶈ'],
    u'\u2D89' : ['n~ o 3','ⶉ'],
    u'\u2D8A' : ['? o 3','ⶊ'],
    u'\u2D8B' : ['z o 3','ⶋ'],
    u'\u2D8C' : ['d[ o 3','ⶌ'],
    u'\u2D8D' : ['dr o 3','ⶍ'],
    u'\u2D8E' : ['j o 3','ⶎ'],
    u'\u2D8F' : ['t> o 3','ⶏ'],
    u'\u2D90' : ['tS> o 3','ⶐ'],
    u'\u2D91' : ['p> o 3','ⶑ'],
    u'\u2D92' : ['p o 3','ⶒ'],
    u'\u2D93' : ['gr u 3','ⶓ'],
    u'\u2D94' : ['gr ui','ⶔ'],
    u'\u2D95' : ['gr u e','ⶕ'],
    u'\u2D96' : ['gr u&','ⶖ'],
    u'\u2DA0' : ['s s 3','ⶠ'],
    u'\u2DA1' : ['s s u','ⶡ'],
    u'\u2DA2' : ['s s i','ⶢ'],
    u'\u2DA3' : ['s s A','ⶣ'],
    u'\u2DA4' : ['s s e','ⶤ'],
    u'\u2DA5' : ['s s &, s s','ⶥ'],
    u'\u2DA6' : ['s s o','ⶦ'],
    u'\u2DA8' : ['tS tS 3','ⶨ'],
    u'\u2DA9' : ['tS tS u','ⶩ'],
    u'\u2DAA' : ['tS tS i','ⶪ'],
    u'\u2DAB' : ['tS tS A','ⶫ'],
    u'\u2DAC' : ['tS tS e','ⶬ'],
    u'\u2DAD' : ['tS tS &, tS tS','ⶭ'],
    u'\u2DAE' : ['tS tS o','ⶮ'],
    u'\u2DB0' : ['z z 3','ⶰ'],
    u'\u2DB1' : ['z z u','ⶱ'],
    u'\u2DB2' : ['z z i','ⶲ'],
    u'\u2DB3' : ['z z A','ⶳ'],
    u'\u2DB4' : ['z z e','ⶴ'],
    u'\u2DB5' : ['z z &, z z','ⶵ'],
    u'\u2DB6' : ['z z o','ⶶ'],
    u'\u2DB8' : ['tS tSh 3','ⶸ'],
    u'\u2DB9' : ['tS tSh u','ⶹ'],
    u'\u2DBA' : ['tS tSh i','ⶺ'],
    u'\u2DBB' : ['tS tSh A','ⶻ'],
    u'\u2DBC' : ['tS tSh e','ⶼ'],
    u'\u2DBD' : ['tS tSh &, tS tSh','ⶽ'],
    u'\u2DBE' : ['tS tSh o','ⶾ'],
    u'\u2DC0' : ['k>j 3','ⷀ'],
    u'\u2DC1' : ['k>j u','ⷁ'],
    u'\u2DC2' : ['k>j i','ⷂ'],
    u'\u2DC3' : ['k>j A','ⷃ'],
    u'\u2DC4' : ['k>j e','ⷄ'],
    u'\u2DC5' : ['k>j &, k>j','ⷅ'],
    u'\u2DC6' : ['k>j o','ⷆ'],
    u'\u2DC8' : ['kj 3','ⷈ'],
    u'\u2DC9' : ['kj u','ⷉ'],
    u'\u2DCA' : ['kj i','ⷊ'],
    u'\u2DCB' : ['kj A','ⷋ'],
    u'\u2DCC' : ['kj e','ⷌ'],
    u'\u2DCE' : ['kj &, kj','ⷎ'],
    u'\u2DD0' : ['kj o','ⷐ'],
    u'\u2DD1' : ['sj 3','ⷑ'],
    u'\u2DD2' : ['sj u','ⷒ'],
    u'\u2DD3' : ['sj i','ⷓ'],
    u'\u2DD4' : ['sj A','ⷔ'],
    u'\u2DD5' : ['sj &, sj','ⷕ'],
    u'\u2DD6' : ['sj o','ⷖ'],
    u'\u2DD8' : ['gj 3','ⷘ'],
    u'\u2DD9' : ['gj u','ⷙ'],
    u'\u2DDA' : ['gj i','ⷚ'],
    u'\u2DDB' : ['gj A','ⷛ'],
    u'\u2DDC' : ['gj 3','ⷜ'],
    u'\u2DDD' : ['gj &, gj','ⷝ'],
    u'\u2DDE' : ['gj o','ⷞ'],
    u'\u1F00' : ['A','ἀ'],
    u'\u1F01' : ['h A','ἁ'],
    u'\u1F02' : ['A','ἂ'],
    u'\u1F03' : ['h A','ἃ'],
    u'\u1F04' : ['A','ἄ'],
    u'\u1F05' : ['h A','ἅ'],
    u'\u1F06' : ['A','ἆ'],
    u'\u1F07' : ['h A','ἇ'],
    u'\u1F08' : ['A','Ἀ'],
    u'\u1F09' : ['h A','Ἁ'],
    u'\u1F0A' : ['h A','Ἂ'],
    u'\u1F0B' : ['A','Ἃ'],
    u'\u1F0C' : ['A','Ἄ'],
    u'\u1F0D' : ['h A','Ἅ'],
    u'\u1F0E' : ['A','Ἆ'],
    u'\u1F0F' : ['h A','Ἇ'],
    u'\u1F10' : ['e','ἐ'],
    u'\u1F11' : ['h e','ἑ'],
    u'\u1F12' : ['e','ἒ'],
    u'\u1F13' : ['h e','ἓ'],
    u'\u1F14' : ['e','ἔ'],
    u'\u1F15' : ['h e','ἕ'],
    u'\u1F18' : ['e','Ἐ'],
    u'\u1F19' : ['h e','Ἑ'],
    u'\u1F1A' : ['e','Ἒ'],
    u'\u1F1B' : ['h e','Ἓ'],
    u'\u1F1C' : ['e','Ἔ'],
    u'\u1F1D' : ['h e','Ἕ'],
    u'\u1F20' : ['E:','ἠ'],
    u'\u1F21' : ['h E:','ἡ'],
    u'\u1F22' : ['E:','ἢ'],
    u'\u1F23' : ['h E:','ἣ'],
    u'\u1F24' : ['E:','ἤ'],
    u'\u1F25' : ['h E:','ἥ'],
    u'\u1F26' : ['E:','ἦ'],
    u'\u1F27' : ['h E:','ἧ'],
    u'\u1F28' : ['E:','Ἠ'],
    u'\u1F29' : ['h E:','Ἡ'],
    u'\u1F2A' : ['E:','Ἢ'],
    u'\u1F2B' : ['h E:','Ἣ'],
    u'\u1F2C' : ['E:','Ἤ'],
    u'\u1F2D' : ['h E:','Ἥ'],
    u'\u1F2E' : ['E:','Ἦ'],
    u'\u1F2F' : ['h E:','Ἧ'],
    u'\u1F30' : ['i','ἰ'],
    u'\u1F31' : ['h i','ἱ'],
    u'\u1F32' : ['i','ἲ'],
    u'\u1F33' : ['h i','ἳ'],
    u'\u1F34' : ['i','ἴ'],
    u'\u1F35' : ['h i','ἵ'],
    u'\u1F36' : ['i','ἶ'],
    u'\u1F37' : ['h i','ἷ'],
    u'\u1F38' : ['i','Ἰ'],
    u'\u1F39' : ['h i','Ἱ'],
    u'\u1F3A' : ['i','Ἲ'],
    u'\u1F3B' : ['h i','Ἳ'],
    u'\u1F3C' : ['i','Ἴ'],
    u'\u1F3D' : ['h i','Ἵ'],
    u'\u1F3E' : ['i','Ἶ'],
    u'\u1F3F' : ['h i','Ἷ'],
    u'\u1F40' : ['o','ὀ'],
    u'\u1F41' : ['h o','ὁ'],
    u'\u1F42' : ['o','ὂ'],
    u'\u1F43' : ['h o','ὃ'],
    u'\u1F44' : ['o','ὄ'],
    u'\u1F45' : ['h o','ὅ'],
    u'\u1F48' : ['o','Ὀ'],
    u'\u1F49' : ['h o','Ὁ'],
    u'\u1F4A' : ['o','Ὂ'],
    u'\u1F4B' : ['h o','Ὃ'],
    u'\u1F4C' : ['o','Ὄ'],
    u'\u1F4D' : ['h o','Ὅ'],
    u'\u1F50' : ['i','ὐ'],
    u'\u1F51' : ['h i','ὑ'],
    u'\u1F52' : ['i','ὒ'],
    u'\u1F53' : ['h i','ὓ'],
    u'\u1F54' : ['i','ὔ'],
    u'\u1F55' : ['h i','ὕ'],
    u'\u1F56' : ['i','ὖ'],
    u'\u1F57' : ['h i','ὗ'],
    u'\u1F59' : ['i','Ὑ'],
    u'\u1F5B' : ['h i','Ὓ'],
    u'\u1F5D' : ['i','Ὕ'],
    u'\u1F5F' : ['h i','Ὗ'],
    u'\u1F60' : ['o','ὠ'],
    u'\u1F61' : ['h o','ὡ'],
    u'\u1F62' : ['o','ὢ'],
    u'\u1F63' : ['h o','ὣ'],
    u'\u1F64' : ['o','ὤ'],
    u'\u1F65' : ['h o','ὥ'],
    u'\u1F66' : ['o','ὦ'],
    u'\u1F67' : ['h o','ὧ'],
    u'\u1F68' : ['o','Ὠ'],
    u'\u1F69' : ['h o','Ὡ'],
    u'\u1F6A' : ['o','Ὢ'],
    u'\u1F6B' : ['h o','Ὣ'],
    u'\u1F6C' : ['o','Ὤ'],
    u'\u1F6D' : ['h o','Ὥ'],
    u'\u1F6E' : ['o','Ὦ'],
    u'\u1F6F' : ['h o','Ὧ'],
    u'\u1F70' : ['A','ὰ'],
    u'\u1F71' : ['A','ά'],
    u'\u1F72' : ['e','ὲ'],
    u'\u1F73' : ['e','έ'],
    u'\u1F74' : ['E:','ὴ'],
    u'\u1F75' : ['E:','ή'],
    u'\u1F76' : ['i','ὶ'],
    u'\u1F77' : ['i','ί'],
    u'\u1F78' : ['o','ὸ'],
    u'\u1F79' : ['o','ό'],
    u'\u1F7A' : ['u','ὺ'],
    u'\u1F7B' : ['u','ύ'],
    u'\u1F7C' : ['o','ὼ'],
    u'\u1F7D' : ['o','ώ'],
    u'\u1F80' : ['A','ᾀ'],
    u'\u1F81' : ['h A','ᾁ'],
    u'\u1F82' : ['A','ᾂ'],
    u'\u1F83' : ['h A','ᾃ'],
    u'\u1F84' : ['A','ᾄ'],
    u'\u1F85' : ['h A','ᾅ'],
    u'\u1F86' : ['A','ᾆ'],
    u'\u1F87' : ['h A','ᾇ'],
    u'\u1F88' : ['A','ᾈ'],
    u'\u1F89' : ['h A','ᾉ'],
    u'\u1F8A' : ['A','ᾊ'],
    u'\u1F8B' : ['h A','ᾋ'],
    u'\u1F8C' : ['A','ᾌ'],
    u'\u1F8D' : ['h A','ᾍ'],
    u'\u1F8E' : ['A','ᾎ'],
    u'\u1F8F' : ['h A','ᾏ'],
    u'\u1F90' : ['E:','ᾐ'],
    u'\u1F91' : ['h E:','ᾑ'],
    u'\u1F92' : ['E:','ᾒ'],
    u'\u1F93' : ['h E:','ᾓ'],
    u'\u1F94' : ['E:','ᾔ'],
    u'\u1F95' : ['h E:','ᾕ'],
    u'\u1F96' : ['E:','ᾖ'],
    u'\u1F97' : ['h E:','ᾗ'],
    u'\u1F98' : ['E:','ᾘ'],
    u'\u1F99' : ['h E:','ᾙ'],
    u'\u1F9A' : ['E:','ᾚ'],
    u'\u1F9B' : ['h E:','ᾛ'],
    u'\u1F9C' : ['E:','ᾜ'],
    u'\u1F9D' : ['h E:','ᾝ'],
    u'\u1F9E' : ['E:','ᾞ'],
    u'\u1F9F' : ['h E:','ᾟ'],
    u'\u1FA0' : ['o','ᾠ'],
    u'\u1FA1' : ['h o','ᾡ'],
    u'\u1FA2' : ['o','ᾢ'],
    u'\u1FA3' : ['h o','ᾣ'],
    u'\u1FA4' : ['o','ᾤ'],
    u'\u1FA5' : ['h o','ᾥ'],
    u'\u1FA6' : ['o','ᾦ'],
    u'\u1FA7' : ['h o','ᾧ'],
    u'\u1FA8' : ['o','ᾨ'],
    u'\u1FA9' : ['h o','ᾩ'],
    u'\u1FAA' : ['o','ᾪ'],
    u'\u1FAB' : ['h o','ᾫ'],
    u'\u1FAC' : ['o','ᾬ'],
    u'\u1FAD' : ['h o','ᾭ'],
    u'\u1FAE' : ['o','ᾮ'],
    u'\u1FAF' : ['h o','ᾯ'],
    u'\u1FB0' : ['A','ᾰ'],
    u'\u1FB1' : ['A','ᾱ'],
    u'\u1FB2' : ['A','ᾲ'],
    u'\u1FB3' : ['A','ᾳ'],
    u'\u1FB4' : ['A','ᾴ'],
    u'\u1FB6' : ['A','ᾶ'],
    u'\u1FB7' : ['A','ᾷ'],
    u'\u1FB8' : ['A','Ᾰ'],
    u'\u1FB9' : ['A','Ᾱ'],
    u'\u1FBA' : ['A','Ὰ'],
    u'\u1FBB' : ['A','Ά'],
    u'\u1FBC' : ['A','ᾼ'],
    u'\u1FBD' : ['(KORONIS)','᾽'],
    u'\u1FBE' : ['i','ι'],
    u'\u1FBF' : ['(PSILI)','᾿'],
    u'\u1FC0' : ['(PERISPOMENI)','῀'],
    u'\u1FC1' : ['(DIALYTIKA AND PERISPOMENI)','῁'],
    u'\u1FC2' : ['E:','ῂ'],
    u'\u1FC3' : ['E:','ῃ'],
    u'\u1FC4' : ['E:','ῄ'],
    u'\u1FC6' : ['E:','ῆ'],
    u'\u1FC7' : ['E:','ῇ'],
    u'\u1FC8' : ['e','Ὲ'],
    u'\u1FC9' : ['E:','Έ'],
    u'\u1FCA' : ['E:','Ὴ'],
    u'\u1FCB' : ['E:','Ή'],
    u'\u1FCC' : ['E:','ῌ'],
    u'\u1FCD' : ['(PSILI AND VARIA)','῍'],
    u'\u1FCE' : ['(PSILI AND OXIA)','῎'],
    u'\u1FCF' : ['(PSILI AND PERISPOMENI)','῏'],
    u'\u1FD0' : ['i','ῐ'],
    u'\u1FD1' : ['i','ῑ'],
    u'\u1FD2' : ['i','ῒ'],
    u'\u1FD3' : ['i','ΐ'],
    u'\u1FD6' : ['i','ῖ'],
    u'\u1FD7' : ['i','ῗ'],
    u'\u1FD8' : ['i','Ῐ'],
    u'\u1FD9' : ['i','Ῑ'],
    u'\u1FDA' : ['i','Ὶ'],
    u'\u1FDB' : ['i','Ί'],
    u'\u1FDD' : ['  (DASIA AND VARIA)','῝'],
    u'\u1FDE' : ['(DASIA AND OXIA)','῞'],
    u'\u1FDF' : ['(DASIA AND PERISPOMENI)','῟'],
    u'\u1FE0' : ['i','ῠ'],
    u'\u1FE1' : ['i','ῡ'],
    u'\u1FE2' : ['i','ῢ'],
    u'\u1FE3' : ['i','ΰ'],
    u'\u1FE4' : ['r','ῤ'],
    u'\u1FE5' : ['r h','ῥ'],
    u'\u1FE6' : ['i','ῦ'],
    u'\u1FE7' : ['i','ῧ'],
    u'\u1FE8' : ['i','Ῠ'],
    u'\u1FE9' : ['i','Ῡ'],
    u'\u1FEA' : ['i','Ὺ'],
    u'\u1FEB' : ['i','Ύ'],
    u'\u1FEC' : ['r h','Ῥ'],
    u'\u1FED' : ['(DIALYTIKA AND VARIA)','῭'],
    u'\u1FEE' : ['(DIALYTIKA AND OXIA)','΅'],
    u'\u1FEF' : ['(VARIA)','`'],
    u'\u1FF2' : ['o','ῲ'],
    u'\u1FF3' : ['o','ῳ'],
    u'\u1FF4' : ['o','ῴ'],
    u'\u1FF6' : ['o','ῶ'],
    u'\u1FF7' : ['o','ῷ'],
    u'\u1FF8' : ['o','Ὸ'],
    u'\u1FF9' : ['o','Ό'],
    u'\u1FFA' : ['o','Ὼ'],
    u'\u1FFB' : ['o','Ώ'],
    u'\u1FFC' : ['o','ῼ'],
    u'\u1FFD' : ['(OXIA)','´'],
    u'\u1FFE' : ['(DASIA)','῾'],
    u'\u10A0' : ['A','Ⴀ'],
    u'\u10A1' : ['b','Ⴁ'],
    u'\u10A2' : ['g','Ⴂ'],
    u'\u10A3' : ['d','Ⴃ'],
    u'\u10A4' : ['E','Ⴄ'],
    u'\u10A5' : ['v','Ⴅ'],
    u'\u10A6' : ['z','Ⴆ'],
    u'\u10A7' : ['th','Ⴇ'],
    u'\u10A8' : ['i','Ⴈ'],
    u'\u10A9' : ['k>','Ⴉ'],
    u'\u10AA' : ['l','Ⴊ'],
    u'\u10AB' : ['m','Ⴋ'],
    u'\u10AC' : ['n','Ⴌ'],
    u'\u10AD' : ['o','Ⴍ'],
    u'\u10AE' : ['p>','Ⴎ'],
    u'\u10AF' : ['Z','Ⴏ'],
    u'\u10B0' : ['v','Ⴐ'],
    u'\u10B1' : ['s','Ⴑ'],
    u'\u10B2' : ['t>','Ⴒ'],
    u'\u10B3' : ['u','Ⴓ'],
    u'\u10B4' : ['ph','Ⴔ'],
    u'\u10B5' : ['kh','Ⴕ'],
    u'\u10B6' : ['G','Ⴖ'],
    u'\u10B7' : ['q>','Ⴗ'],
    u'\u10B8' : ['S','Ⴘ'],
    u'\u10B9' : ['tS','Ⴙ'],
    u'\u10BA' : ['k','Ⴚ'],
    u'\u10BB' : ['dz','Ⴛ'],
    u'\u10BC' : ['ts>','Ⴜ'],
    u'\u10BD' : ['tS>','Ⴝ'],
    u'\u10BE' : ['x','Ⴞ'],
    u'\u10BF' : ['dZ','Ⴟ'],
    u'\u10C0' : ['h','Ⴠ'],
    u'\u10C1' : ['ej','Ⴡ'],
    u'\u10C2' : ['j','Ⴢ'],
    u'\u10C3' : ['w i','Ⴣ'],
    u'\u10C4' : ['q','Ⴤ'],
    u'\u10C5' : ['oU','Ⴥ'],
    u'\u10D0' : ['A','ა'],
    u'\u10D1' : ['b','ბ'],
    u'\u10D2' : ['g','გ'],
    u'\u10D3' : ['d','დ'],
    u'\u10D4' : ['E','ე'],
    u'\u10D5' : ['v','ვ'],
    u'\u10D6' : ['z','ზ'],
    u'\u10D7' : ['th','თ'],
    u'\u10D8' : ['i','ი'],
    u'\u10D9' : ['k>','კ'],
    u'\u10DA' : ['l','ლ'],
    u'\u10DB' : ['m','მ'],
    u'\u10DC' : ['n','ნ'],
    u'\u10DD' : ['o','ო'],
    u'\u10DE' : ['p>','პ'],
    u'\u10DF' : ['Z','ჟ'],
    u'\u10E0' : ['v','რ'],
    u'\u10E1' : ['s','ს'],
    u'\u10E2' : ['t>','ტ'],
    u'\u10E3' : ['u','უ'],
    u'\u10E4' : ['ph','ფ'],
    u'\u10E5' : ['kh','ქ'],
    u'\u10E6' : ['G','ღ'],
    u'\u10E7' : ['q>','ყ'],
    u'\u10E8' : ['S','შ'],
    u'\u10E9' : ['tS','ჩ'],
    u'\u10EA' : ['k','ც'],
    u'\u10EB' : ['dz','ძ'],
    u'\u10EC' : ['ts>','წ'],
    u'\u10ED' : ['tS>','ჭ'],
    u'\u10EE' : ['x','ხ'],
    u'\u10EF' : ['dZ','ჯ'],
    u'\u10F0' : ['h','ჰ'],
    u'\u10F1' : ['ej','ჱ'],
    u'\u10F2' : ['j','ჲ'],
    u'\u10F3' : ['w i','ჳ'],
    u'\u10F4' : ['q','ჴ'],
    u'\u10F5' : ['oU','ჵ'],
    u'\u10F6' : ['f','ჶ'],
    u'\u10F7' : ['(YN)','ჷ'],
    u'\u10F8' : ['(ELIFI)','ჸ'],
    u'\u10F9' : ['(TURNED GAN)','ჹ'],
    u'\u10FA' : ['(AIN)','ჺ'],
    u'\u10FB' : ['(PARAGRAPH SEPARATOR)','჻'],
    u'\u10FC' : ['(MODIFIER LETTER GEORGIAN NAR)','ჼ'],
    u'\u2D00' : ['A','ⴀ'],
    u'\u2D01' : ['b','ⴁ'],
    u'\u2D02' : ['g','ⴂ'],
    u'\u2D03' : ['d','ⴃ'],
    u'\u2D04' : ['E','ⴄ'],
    u'\u2D05' : ['v','ⴅ'],
    u'\u2D06' : ['z','ⴆ'],
    u'\u2D07' : ['th','ⴇ'],
    u'\u2D08' : ['i','ⴈ'],
    u'\u2D09' : ['k>','ⴉ'],
    u'\u2D0A' : ['l','ⴊ'],
    u'\u2D0B' : ['m','ⴋ'],
    u'\u2D0C' : ['n','ⴌ'],
    u'\u2D0D' : ['o','ⴍ'],
    u'\u2D0E' : ['p>','ⴎ'],
    u'\u2D0F' : ['z','ⴏ'],
    u'\u2D10' : ['v','ⴐ'],
    u'\u2D11' : ['s','ⴑ'],
    u'\u2D12' : ['t>','ⴒ'],
    u'\u2D13' : ['u','ⴓ'],
    u'\u2D14' : ['ph','ⴔ'],
    u'\u2D15' : ['kh','ⴕ'],
    u'\u2D16' : ['G','ⴖ'],
    u'\u2D17' : ['q>','ⴗ'],
    u'\u2D18' : ['S','ⴘ'],
    u'\u2D19' : ['tS','ⴙ'],
    u'\u2D1A' : ['k','ⴚ'],
    u'\u2D1B' : ['dz','ⴛ'],
    u'\u2D1C' : ['ts>','ⴜ'],
    u'\u2D1D' : ['tS>','ⴝ'],
    u'\u2D1E' : ['x','ⴞ'],
    u'\u2D1F' : ['dZ','ⴟ'],
    u'\u2D20' : ['h','ⴠ'],
    u'\u2D21' : ['ej','ⴡ'],
    u'\u2D22' : ['j','ⴢ'],
    u'\u2D23' : ['w i','ⴣ'],
    u'\u2D24' : ['q','ⴤ'],
    u'\u2D25' : ['oU','ⴥ'],
    u'\u0374' : ['(NUMERAL SIGN)','ʹ'],
    u'\u0375' : ['(LOWER NUMERAL SIGN)','͵'],
    u'\u037A' : ['(YPOGEGRAMMENI)','ͺ'],
    u'\u037E' : ['(QUESTION MARK)',';'],
    u'\u0384' : ['(TONOS)','΄'],
    u'\u0385' : ['(DIALYTIKA TONOS)','΅'],
    u'\u0386' : ['A','Ά'],
    u'\u0387' : ['(TELEIA)','·'],
    u'\u0388' : ['e','Έ'],
    u'\u0389' : ['E:','Ή'],
    u'\u038A' : ['i','Ί'],
    u'\u038C' : ['o','Ό'],
    u'\u038E' : ['i','Ύ'],
    u'\u038F' : ['o','Ώ'],
    u'\u0390' : ['i','ΐ'],
    u'\u0391' : ['A','Α'],
    u'\u0392' : ['b','Β'],
    u'\u0393' : ['g','Γ'],
    u'\u0394' : ['d','Δ'],
    u'\u0395' : ['e','Ε'],
    u'\u0396' : ['z','Ζ'],
    u'\u0397' : ['E:','Η'],
    u'\u0398' : ['th','Θ'],
    u'\u0399' : ['i','Ι'],
    u'\u039A' : ['k','Κ'],
    u'\u039B' : ['l','Λ'],
    u'\u039C' : ['m','Μ'],
    u'\u039D' : ['n','Ν'],
    u'\u039E' : ['k s','Ξ'],
    u'\u039F' : ['o','Ο'],
    u'\u03A0' : ['p','Π'],
    u'\u03A1' : ['r','Ρ'],
    u'\u03A3' : ['s','Σ'],
    u'\u03A4' : ['t','Τ'],
    u'\u03A5' : ['i','Υ'],
    u'\u03A6' : ['f','Φ'],
    u'\u03A7' : ['X','Χ'],
    u'\u03A8' : ['p s','Ψ'],
    u'\u03A9' : ['o','Ω'],
    u'\u03AA' : ['i','Ϊ'],
    u'\u03AB' : ['i','Ϋ'],
    u'\u03AC' : ['A','ά'],
    u'\u03AD' : ['e','έ'],
    u'\u03AE' : ['E:','ή'],
    u'\u03AF' : ['i','ί'],
    u'\u03B0' : ['i','ΰ'],
    u'\u03B1' : ['A','α'],
    u'\u03B2' : ['b','β'],
    u'\u03B3' : ['g','γ'],
    u'\u03B4' : ['d','δ'],
    u'\u03B5' : ['e','ε'],
    u'\u03B6' : ['z','ζ'],
    u'\u03B7' : ['E:','η'],
    u'\u03B8' : ['th','θ'],
    u'\u03B9' : ['i','ι'],
    u'\u03BA' : ['k','κ'],
    u'\u03BB' : ['l','λ'],
    u'\u03BC' : ['m','μ'],
    u'\u03BD' : ['n','ν'],
    u'\u03BE' : ['k s','ξ'],
    u'\u03BF' : ['o','ο'],
    u'\u03C0' : ['p','π'],
    u'\u03C1' : ['r','ρ'],
    u'\u03C2' : ['s','ς'],
    u'\u03C3' : ['s','σ'],
    u'\u03C4' : ['t','τ'],
    u'\u03C5' : ['i','υ'],
    u'\u03C6' : ['f','φ'],
    u'\u03C7' : ['X','χ'],
    u'\u03C8' : ['p s','ψ'],
    u'\u03C9' : ['o','ω'],
    u'\u03CA' : ['i','ϊ'],
    u'\u03CB' : ['i','ϋ'],
    u'\u03CC' : ['o','ό'],
    u'\u03CD' : ['i','ύ'],
    u'\u03CE' : ['o','ώ'],
    u'\u03D0' : ['b','ϐ'],
    u'\u03D1' : ['th','ϑ'],
    u'\u03D2' : ['i','ϒ'],
    u'\u03D3' : ['i','ϓ'],
    u'\u03D4' : ['i','ϔ'],
    u'\u03D5' : ['f','ϕ'],
    u'\u03D6' : ['p','ϖ'],
    u'\u03D7' : ['(KAI)','ϗ'],
    u'\u03D8' : ['q','Ϙ'],
    u'\u03D9' : ['q','ϙ'],
    u'\u03DA' : ['s','Ϛ'],
    u'\u03DB' : ['s','ϛ'],
    u'\u03DC' : ['w','Ϝ'],
    u'\u03DD' : ['w','ϝ'],
    u'\u03DE' : ['q','Ϟ'],
    u'\u03DF' : ['q','ϟ'],
    u'\u03E0' : ['(NINE HUNDREDS)','Ϡ'],
    u'\u03E1' : ['(NINE HUNDREDS)','ϡ'],
    u'\u03E2' : ['S','Ϣ'],
    u'\u03E3' : ['S','ϣ'],
    u'\u03E4' : ['f','Ϥ'],
    u'\u03E5' : ['f','ϥ'],
    u'\u03E6' : ['h','Ϧ'],
    u'\u03E7' : ['h','ϧ'],
    u'\u03E8' : ['h','Ϩ'],
    u'\u03E9' : ['h','ϩ'],
    u'\u03EA' : ['dZ','Ϫ'],
    u'\u03EB' : ['dZ','ϫ'],
    u'\u03EC' : ['q','Ϭ'],
    u'\u03ED' : ['q','ϭ'],
    u'\u03EE' : ['t i','Ϯ'],
    u'\u03EF' : ['t i','ϯ'],
    u'\u03F0' : ['k','ϰ'],
    u'\u03F1' : ['r','ϱ'],
    u'\u03F2' : ['s','ϲ'],
    u'\u03F3' : ['(YOT)','ϳ'],
    u'\u03F4' : ['th','ϴ'],
    u'\u03F5' : ['e','ϵ'],
    u'\u03F6' : ['e','϶'],
    u'\u03F7' : ['S','Ϸ'],
    u'\u03F8' : ['S','ϸ'],
    u'\u03F9' : ['s','Ϲ'],
    u'\u03FA' : ['(SAN)','Ϻ'],
    u'\u03FB' : ['(SAN)','ϻ'],
    u'\u03FC' : ['r','ϼ'],
    u'\u03FD' : ['s','Ͻ'],
    u'\u03FE' : ['s','Ͼ'],
    u'\u03FF' : ['s','Ͽ'],
    u'\u0A81' : ['(CD)','ઁ'],
    u'\u0A82' : ['(M)','ં'],
    u'\u0A83' : ['(H)','ઃ'],
    u'\u0A85' : ['A','અ'],
    u'\u0A86' : ['A:','આ'],
    u'\u0A87' : ['i','ઇ'],
    u'\u0A88' : ['i:','ઈ'],
    u'\u0A89' : ['u','ઉ'],
    u'\u0A8A' : ['u:','ઊ'],
    u'\u0A8B' : ['9r u','ઋ'],
    u'\u0A8C' : ['l=','ઌ'],
    u'\u0A8D' : ['e','ઍ'],
    u'\u0A8F' : ['e','એ'],
    u'\u0A90' : ['aI','ઐ'],
    u'\u0A91' : ['o','ઑ'],
    u'\u0A93' : ['o','ઓ'],
    u'\u0A94' : ['aU','ઔ'],
    u'\u0A95' : ['k A','ક'],
    u'\u0A96' : ['kh A','ખ'],
    u'\u0A97' : ['g A','ગ'],
    u'\u0A98' : ['gh A','ઘ'],
    u'\u0A99' : ['N A','ઙ'],
    u'\u0A9A' : ['c A','ચ'],
    u'\u0A9B' : ['ch A','છ'],
    u'\u0A9C' : ['J A','જ'],
    u'\u0A9D' : ['Jh A','ઝ'],
    u'\u0A9E' : ['n~ A','ઞ'],
    u'\u0A9F' : ['tr A','ટ'],
    u'\u0AA0' : ['tR A','ઠ'],
    u'\u0AA1' : ['dr A','ડ'],
    u'\u0AA2' : ['dR A','ઢ'],
    u'\u0AA3' : ['nr A','ણ'],
    u'\u0AA4' : ['t[ A','ત'],
    u'\u0AA5' : ['t[_h A','થ'],
    u'\u0AA6' : ['d[ A','દ'],
    u'\u0AA7' : ['d[_h A','ધ'],
    u'\u0AA8' : ['n[ A','ન'],
    u'\u0AAA' : ['p A','પ'],
    u'\u0AAB' : ['ph A','ફ'],
    u'\u0AAC' : ['b A','બ'],
    u'\u0AAD' : ['bh A','ભ'],
    u'\u0AAE' : ['m A','મ'],
    u'\u0AAF' : ['j A','ય'],
    u'\u0AB0' : ['9r A','ર'],
    u'\u0AB2' : ['l A','લ'],
    u'\u0AB3' : ['lr A','ળ'],
    u'\u0AB5' : ['v A','વ'],
    u'\u0AB6' : ['S A','શ'],
    u'\u0AB7' : ['S A','ષ'],
    u'\u0AB8' : ['s A','સ'],
    u'\u0AB9' : ['hv A','હ'],
    u'\u0ABC' : ['(NUKTA)','઼'],
    u'\u0ABD' : ['(AVAGRAHA)','ઽ'],
    u'\u0ABE' : ['A:','ા'],
    u'\u0ABF' : ['i','િ'],
    u'\u0AC0' : ['i:','ી'],
    u'\u0AC1' : ['u','ુ'],
    u'\u0AC2' : ['u:','ૂ'],
    u'\u0AC3' : ['9r=','ૃ'],
    u'\u0AC4' : ['rr=','ૄ'],
    u'\u0AC5' : ['e','ૅ'],
    u'\u0AC7' : ['e','ે'],
    u'\u0AC8' : ['aI','ૈ'],
    u'\u0AC9' : ['o','ૉ'],
    u'\u0ACB' : ['o','ો'],
    u'\u0ACC' : ['aU','ૌ'],
    u'\u0ACD' : ['(P)','્'],
    u'\u0AD0' : ['(OM)','ૐ'],
    u'\u0AE0' : ['rr=','ૠ'],
    u'\u0AE1' : ['lr=','ૡ'],
    u'\u0AE2' : ['l=','ૢ'],
    u'\u0AE3' : ['lr=','ૣ'],
    u'\u0AE6' : ['(ZERO)','૦'],
    u'\u0AE7' : ['(ONE)','૧'],
    u'\u0AE8' : ['(TWO)','૨'],
    u'\u0AE9' : ['(THREE)','૩'],
    u'\u0AEA' : ['(FOUR)','૪'],
    u'\u0AEB' : ['(FIVE)','૫'],
    u'\u0AEC' : ['(SIX)','૬'],
    u'\u0AED' : ['(SEVEN)','૭'],
    u'\u0AEE' : ['(EIGHT)','૮'],
    u'\u0AEF' : ['(NINE)','૯'],
    u'\u0A00' : ['(CD)','਀'],
    u'\u0A01' : ['(R:0A01)','ਁ'],
    u'\u0A02' : ['(M)','ਂ'],
    u'\u0A03' : ['(H)','ਃ'],
    u'\u0A05' : ['&','ਅ'],
    u'\u0A06' : ['A','ਆ'],
    u'\u0A07' : ['I','ਇ'],
    u'\u0A08' : ['i','ਈ'],
    u'\u0A09' : ['U','ਉ'],
    u'\u0A0A' : ['u','ਊ'],
    u'\u0A0B' : ['(R:0A0B)','਋'],
    u'\u0A0D' : ['(R:0A0D)','਍'],
    u'\u0A0E' : ['(R:0A0E)','਎'],
    u'\u0A0F' : ['e','ਏ'],
    u'\u0A10' : ['aI','ਐ'],
    u'\u0A11' : ['(R:0A11)','਑'],
    u'\u0A12' : ['(R:0A12)','਒'],
    u'\u0A13' : ['o','ਓ'],
    u'\u0A14' : ['aU','ਔ'],
    u'\u0A15' : ['k A','ਕ'],
    u'\u0A16' : ['kh A','ਖ'],
    u'\u0A17' : ['g A','ਗ'],
    u'\u0A18' : ['gh A','ਘ'],
    u'\u0A19' : ['N A','ਙ'],
    u'\u0A1A' : ['c A','ਚ'],
    u'\u0A1B' : ['ch A','ਛ'],
    u'\u0A1C' : ['J A','ਜ'],
    u'\u0A1D' : ['Jh A','ਝ'],
    u'\u0A1E' : ['n~ A','ਞ'],
    u'\u0A1F' : ['tr A','ਟ'],
    u'\u0A20' : ['tR A','ਠ'],
    u'\u0A21' : ['dr A','ਡ'],
    u'\u0A22' : ['dR A','ਢ'],
    u'\u0A23' : ['nr A','ਣ'],
    u'\u0A24' : ['t[ A','ਤ'],
    u'\u0A25' : ['t[_h A','ਥ'],
    u'\u0A26' : ['d[','ਦ'],
    u'\u0A27' : ['d[_h A','ਧ'],
    u'\u0A28' : ['n[','ਨ'],
    u'\u0A29' : ['(R:0A29)','਩'],
    u'\u0A2A' : ['p A','ਪ'],
    u'\u0A2B' : ['ph A','ਫ'],
    u'\u0A2C' : ['b A','ਬ'],
    u'\u0A2D' : ['bh A','ਭ'],
    u'\u0A2E' : ['m A','ਮ'],
    u'\u0A2F' : ['j A','ਯ'],
    u'\u0A30' : ['r A','ਰ'],
    u'\u0A31' : ['(R:0A31)','਱'],
    u'\u0A32' : ['l A','ਲ'],
    u'\u0A33' : ['lr A','ਲ਼'],
    u'\u0A34' : ['(R:0A34)','਴'],
    u'\u0A35' : ['v A','ਵ'],
    u'\u0A36' : ['S A','ਸ਼'],
    u'\u0A37' : ['(R:0A37)','਷'],
    u'\u0A38' : ['s A','ਸ'],
    u'\u0A39' : ['hv A','ਹ'],
    u'\u0A3C' : ['(NUKTA)','਼'],
    u'\u0A3E' : ['A','ਾ'],
    u'\u0A3F' : ['I','ਿ'],
    u'\u0A40' : ['i','ੀ'],
    u'\u0A41' : ['U','ੁ'],
    u'\u0A42' : ['u','ੂ'],
    u'\u0A43' : ['(R:0A43)','੃'],
    u'\u0A45' : ['(R:0A45)','੅'],
    u'\u0A46' : ['(R:0A46)','੆'],
    u'\u0A47' : ['e','ੇ'],
    u'\u0A48' : ['aI','ੈ'],
    u'\u0A49' : ['(R:0A49)','੉'],
    u'\u0A4A' : ['(R:0A4A)','੊'],
    u'\u0A4B' : ['o','ੋ'],
    u'\u0A4C' : ['aU','ੌ'],
    u'\u0A4D' : ['(P)','੍'],
    u'\u0A59' : ['x A','ਖ਼'],
    u'\u0A5A' : ['G A','ਗ਼'],
    u'\u0A5B' : ['z A','ਜ਼'],
    u'\u0A5C' : ['rr A','ੜ'],
    u'\u0A5E' : ['f A','ਫ਼'],
    u'\u0A5F' : ['(R:0A5F)','੟'],
    u'\u0A64' : ['(R:0A64)','੤'],
    u'\u0A66' : ['(ZERO)','੦'],
    u'\u0A67' : ['(ONE)','੧'],
    u'\u0A68' : ['(TWO)','੨'],
    u'\u0A69' : ['(THREE)','੩'],
    u'\u0A6A' : ['(FOUR)','੪'],
    u'\u0A6B' : ['(FIVE)','੫'],
    u'\u0A6C' : ['(SIX)','੬'],
    u'\u0A6D' : ['(SEVEN)','੭'],
    u'\u0A6E' : ['(EIGHT)','੮'],
    u'\u0A6F' : ['(NINE)','੯'],
    u'\u0A70' : ['(TIPPI)','ੰ'],
    u'\u0A71' : ['(ADDAK)','ੱ'],
    u'\u0A72' : ['(IRI)','ੲ'],
    u'\u0A73' : ['(URA)','ੳ'],
    u'\u1720' : ['A','ᜠ'],
    u'\u1721' : ['i','ᜡ'],
    u'\u1722' : ['u','ᜢ'],
    u'\u1723' : ['k A','ᜣ'],
    u'\u1724' : ['g A','ᜤ'],
    u'\u1725' : ['N A','ᜥ'],
    u'\u1726' : ['t A','ᜦ'],
    u'\u1727' : ['d A','ᜧ'],
    u'\u1728' : ['n A','ᜨ'],
    u'\u1729' : ['p A','ᜩ'],
    u'\u172A' : ['b A','ᜪ'],
    u'\u172B' : ['m A','ᜫ'],
    u'\u172C' : ['j A','ᜬ'],
    u'\u172D' : ['r A','ᜭ'],
    u'\u172E' : ['l A','ᜮ'],
    u'\u172F' : ['w A','ᜯ'],
    u'\u1730' : ['s A','ᜰ'],
    u'\u1731' : ['h A','ᜱ'],
    u'\u1732' : ['i','ᜲ'],
    u'\u1733' : ['u','ᜳ'],
    u'\u1734' : ['(P)','᜴'],
    u'\u1735' : ['(SINGLE PUNCTUATION)','᜵'],
    u'\u1736' : ['(DOUBLE PUNCTUATION)','᜶'],
    u'\u0591' : ['(##)','֑'],
    u'\u0592' : ['(##)','֒'],
    u'\u0593' : ['(##)','֓'],
    u'\u0594' : ['(##)','֔'],
    u'\u0595' : ['(##)','֕'],
    u'\u0596' : ['(##)','֖'],
    u'\u0597' : ['(##)','֗'],
    u'\u0598' : ['(##)','֘'],
    u'\u0599' : ['(##)','֙'],
    u'\u059A' : ['(##)','֚'],
    u'\u059B' : ['(##)','֛'],
    u'\u059C' : ['(##)','֜'],
    u'\u059D' : ['(##)','֝'],
    u'\u059E' : ['(##)','֞'],
    u'\u059F' : ['(##)','֟'],
    u'\u05A0' : ['(##)','֠'],
    u'\u05A1' : ['(##)','֡'],
    u'\u05A2' : ['(##)','֢'],
    u'\u05A3' : ['(##)','֣'],
    u'\u05A4' : ['(##)','֤'],
    u'\u05A5' : ['(##)','֥'],
    u'\u05A6' : ['(##)','֦'],
    u'\u05A7' : ['(##)','֧'],
    u'\u05A8' : ['(##)','֨'],
    u'\u05A9' : ['(##)','֩'],
    u'\u05AA' : ['(##)','֪'],
    u'\u05AB' : ['(##)','֫'],
    u'\u05AC' : ['(##)','֬'],
    u'\u05AD' : ['(##)','֭'],
    u'\u05AE' : ['(##)','֮'],
    u'\u05AF' : ['(##)','֯'],
    u'\u05B0' : ['(##)','ְ'],
    u'\u05B1' : ['(##)','ֱ'],
    u'\u05B2' : ['(##)','ֲ'],
    u'\u05B3' : ['(##)','ֳ'],
    u'\u05B4' : ['(##)','ִ'],
    u'\u05B5' : ['(##)','ֵ'],
    u'\u05B6' : ['(##)','ֶ'],
    u'\u05B7' : ['(##)','ַ'],
    u'\u05B8' : ['(##)','ָ'],
    u'\u05B9' : ['(##)','ֹ'],
    u'\u05BB' : ['(##)','ֻ'],
    u'\u05BC' : ['(##)','ּ'],
    u'\u05BD' : ['(##)','ֽ'],
    u'\u05BE' : ['(##)','־'],
    u'\u05BF' : ['(##)','ֿ'],
    u'\u05C0' : ['(##)','׀'],
    u'\u05C1' : ['(##)','ׁ'],
    u'\u05C2' : ['(##)','ׂ'],
    u'\u05C3' : ['(##)','׃'],
    u'\u05C4' : ['(##)','ׄ'],
    u'\u05C5' : ['(##)','ׅ'],
    u'\u05C6' : ['(##)','׆'],
    u'\u05C7' : ['(##)','ׇ'],
    u'\u05D0' : ['?, 7','א'],
    u'\u05D1' : ['b, v','ב'],
    u'\u05D2' : ['g','ג'],
    u'\u05D3' : ['d','ד'],
    u'\u05D4' : ['h','ה'],
    u'\u05D5' : ['v','ו'],
    u'\u05D6' : ['z','ז'],
    u'\u05D7' : ['x','ח'],
    u'\u05D8' : ['t','ט'],
    u'\u05D9' : ['j','י'],
    u'\u05DA' : ['k, x','ך'],
    u'\u05DB' : ['k, x','כ'],
    u'\u05DC' : ['l','ל'],
    u'\u05DD' : ['m','ם'],
    u'\u05DE' : ['m','מ'],
    u'\u05DF' : ['n','ן'],
    u'\u05E0' : ['n','נ'],
    u'\u05E1' : ['s','ס'],
    u'\u05E2' : ['?, 7','ע'],
    u'\u05E3' : ['p, f','ף'],
    u'\u05E4' : ['p, f','פ'],
    u'\u05E5' : ['ts','ץ'],
    u'\u05E6' : ['ts','צ'],
    u'\u05E7' : ['k','ק'],
    u'\u05E8' : ['k','ר'],
    u'\u05E9' : ['s, S','ש'],
    u'\u05EA' : ['t','ת'],
    u'\u05F0' : ['v','װ'],
    u'\u05F1' : ['>j','ױ'],
    u'\u05F2' : ['Ej','ײ'],
    u'\u05F3' : ['(##)','׳'],
    u'\u05F4' : ['(##)','״'],
    u'\uFB00' : ['f f','ﬀ'],
    u'\uFB01' : ['f i','ﬁ'],
    u'\uFB02' : ['f l','ﬂ'],
    u'\uFB03' : ['f f i','ﬃ'],
    u'\uFB04' : ['f f l','ﬄ'],
    u'\uFB05' : ['s t','ﬅ'],
    u'\uFB06' : ['s t','ﬆ'],
    u'\uFB13' : ['m n','ﬓ'],
    u'\uFB14' : ['m h','ﬔ'],
    u'\uFB15' : ['m dZ','ﬕ'],
    u'\uFB16' : ['m n','ﬖ'],
    u'\uFB17' : ['m x','ﬗ'],
    u'\uFB1D' : ['j','יִ'],
    u'\uFB1E' : ['(HEBREW POINT JUDEO-SPANISH VARIKA)','ﬞ'],
    u'\uFB1F' : ['>j','ײַ'],
    u'\uFB20' : ['?, 7','ﬠ'],
    u'\uFB21' : ['?, 7','ﬡ'],
    u'\uFB22' : ['d','ﬢ'],
    u'\uFB23' : ['h','ﬣ'],
    u'\uFB24' : ['k, x','ﬤ'],
    u'\uFB25' : ['l','ﬥ'],
    u'\uFB26' : ['m','ﬦ'],
    u'\uFB27' : ['k','ﬧ'],
    u'\uFB28' : ['t','ﬨ'],
    u'\uFB29' : ['(ALTERNATIVE PLUS SIGN)','﬩'],
    u'\uFB2A' : ['s, S','שׁ'],
    u'\uFB2B' : ['s, S','שׂ'],
    u'\uFB2C' : ['s, S','שּׁ'],
    u'\uFB2D' : ['s, S','שּׂ'],
    u'\uFB2E' : ['?, 7','אַ'],
    u'\uFB2F' : ['?, 7','אָ'],
    u'\uFB30' : ['?, 7','אּ'],
    u'\uFB31' : ['b, v','בּ'],
    u'\uFB32' : ['g','גּ'],
    u'\uFB33' : ['d','דּ'],
    u'\uFB34' : ['h','הּ'],
    u'\uFB35' : ['v','וּ'],
    u'\uFB36' : ['z','זּ'],
    u'\uFB38' : ['t','טּ'],
    u'\uFB39' : ['j','יּ'],
    u'\uFB3A' : ['k, x','ךּ'],
    u'\uFB3B' : ['k, x','כּ'],
    u'\uFB3C' : ['l','לּ'],
    u'\uFB3E' : ['m','מּ'],
    u'\uFB40' : ['n','נּ'],
    u'\uFB41' : ['s','סּ'],
    u'\uFB43' : ['p, f','ףּ'],
    u'\uFB44' : ['p, f','פּ'],
    u'\uFB46' : ['ts','צּ'],
    u'\uFB47' : ['k','קּ'],
    u'\uFB48' : ['k','רּ'],
    u'\uFB49' : ['s, S','שּ'],
    u'\uFB4A' : ['t','תּ'],
    u'\uFB4B' : ['v','וֹ'],
    u'\uFB4C' : ['b, v','בֿ'],
    u'\uFB4D' : ['k, x','כֿ'],
    u'\uFB4E' : ['p, f','פֿ'],
    u'\uFB4F' : ['?, 7','ﭏ'],
    u'\u3041' : ['A','ぁ'],
    u'\u3042' : ['A','あ'],
    u'\u3043' : ['i','ぃ'],
    u'\u3044' : ['i','い'],
    u'\u3045' : ['u','ぅ'],
    u'\u3046' : ['u','う'],
    u'\u3047' : ['e','ぇ'],
    u'\u3048' : ['e','え'],
    u'\u3049' : ['o','ぉ'],
    u'\u304A' : ['o','お'],
    u'\u304B' : ['k A','か'],
    u'\u304C' : ['g A','が'],
    u'\u304D' : ['k i','き'],
    u'\u304E' : ['g i','ぎ'],
    u'\u304F' : ['k u','く'],
    u'\u3050' : ['g u','ぐ'],
    u'\u3051' : ['k e','け'],
    u'\u3052' : ['g e','げ'],
    u'\u3053' : ['k o','こ'],
    u'\u3054' : ['g o','ご'],
    u'\u3055' : ['s A','さ'],
    u'\u3056' : ['z A','ざ'],
    u'\u3057' : ['s i','し'],
    u'\u3058' : ['z i','じ'],
    u'\u3059' : ['s u','す'],
    u'\u305A' : ['z u','ず'],
    u'\u305B' : ['s e','せ'],
    u'\u305C' : ['z e','ぜ'],
    u'\u305D' : ['s o','そ'],
    u'\u305E' : ['z o','ぞ'],
    u'\u305F' : ['t A','た'],
    u'\u3060' : ['d A','だ'],
    u'\u3061' : ['tS i','ち'],
    u'\u3062' : ['z i','ぢ'],
    u'\u3063' : ['ts u','っ'],
    u'\u3064' : ['ts u','つ'],
    u'\u3065' : ['z u','づ'],
    u'\u3066' : ['t e','て'],
    u'\u3067' : ['d e','で'],
    u'\u3068' : ['t o','と'],
    u'\u3069' : ['d o','ど'],
    u'\u306A' : ['n A','な'],
    u'\u306B' : ['n i','に'],
    u'\u306C' : ['n u','ぬ'],
    u'\u306D' : ['n e','ね'],
    u'\u306E' : ['n o','の'],
    u'\u306F' : ['h A','は'],
    u'\u3070' : ['b A','ば'],
    u'\u3071' : ['p A','ぱ'],
    u'\u3072' : ['h i','ひ'],
    u'\u3073' : ['b i','び'],
    u'\u3074' : ['p i','ぴ'],
    u'\u3075' : ['h u','ふ'],
    u'\u3076' : ['b u','ぶ'],
    u'\u3077' : ['p u','ぷ'],
    u'\u3078' : ['h e','へ'],
    u'\u3079' : ['b e','べ'],
    u'\u307A' : ['p e','ぺ'],
    u'\u307B' : ['h o','ほ'],
    u'\u307C' : ['b o','ぼ'],
    u'\u307D' : ['p o','ぽ'],
    u'\u307E' : ['m A','ま'],
    u'\u307F' : ['m i','み'],
    u'\u3080' : ['m u','む'],
    u'\u3081' : ['m e','め'],
    u'\u3082' : ['m o','も'],
    u'\u3083' : ['j a','ゃ'],
    u'\u3084' : ['j a','や'],
    u'\u3085' : ['yu','ゅ'],
    u'\u3086' : ['yu','ゆ'],
    u'\u3087' : ['j o','ょ'],
    u'\u3088' : ['j o','よ'],
    u'\u3089' : ['r A','ら'],
    u'\u308A' : ['r i','り'],
    u'\u308B' : ['r u','る'],
    u'\u308C' : ['r e','れ'],
    u'\u308D' : ['r o','ろ'],
    u'\u308E' : ['w A','ゎ'],
    u'\u308F' : ['w A','わ'],
    u'\u3090' : ['w i','ゐ'],
    u'\u3091' : ['w e','ゑ'],
    u'\u3092' : ['w o','を'],
    u'\u3093' : ['N','ん'],
    u'\u3094' : ['v u','ゔ'],
    u'\u3095' : ['k A','ゕ'],
    u'\u3096' : ['k e','ゖ'],
    u'\u3099' : ['(VOICED)','゙'],
    u'\u309A' : ['(SEMI VOICED)','゚'],
    u'\u309B' : ['(VOICED)','゛'],
    u'\u309C' : ['(SEMI VOICED)','゜'],
    u'\u309D' : ['(ITERATION MARK)','ゝ'],
    u'\u309E' : ['(VOICED ITERATION MARK)','ゞ'],
    u'\u309F' : ['(YORI)','ゟ'],
    u'\u30A0' : ['(EQUAL SIGN)','゠'],
    u'\u30A1' : ['A','ァ'],
    u'\u30A2' : ['A','ア'],
    u'\u30A3' : ['i','ィ'],
    u'\u30A4' : ['i','イ'],
    u'\u30A5' : ['u','ゥ'],
    u'\u30A6' : ['u','ウ'],
    u'\u30A7' : ['e','ェ'],
    u'\u30A8' : ['e','エ'],
    u'\u30A9' : ['o','ォ'],
    u'\u30AA' : ['o','オ'],
    u'\u30AB' : ['k A','カ'],
    u'\u30AC' : ['g A','ガ'],
    u'\u30AD' : ['k i','キ'],
    u'\u30AE' : ['g i','ギ'],
    u'\u30AF' : ['k u','ク'],
    u'\u30B0' : ['g u','グ'],
    u'\u30B1' : ['k e','ケ'],
    u'\u30B2' : ['g e','ゲ'],
    u'\u30B3' : ['k o','コ'],
    u'\u30B4' : ['g o','ゴ'],
    u'\u30B5' : ['s A','サ'],
    u'\u30B6' : ['z A','ザ'],
    u'\u30B7' : ['s i','シ'],
    u'\u30B8' : ['z i','ジ'],
    u'\u30B9' : ['s u','ス'],
    u'\u30BA' : ['z u','ズ'],
    u'\u30BB' : ['s e','セ'],
    u'\u30BC' : ['z e','ゼ'],
    u'\u30BD' : ['s o','ソ'],
    u'\u30BE' : ['z o','ゾ'],
    u'\u30BF' : ['t A','タ'],
    u'\u30C0' : ['d A','ダ'],
    u'\u30C1' : ['tS i','チ'],
    u'\u30C2' : ['z i','ヂ'],
    u'\u30C3' : ['ts u','ッ'],
    u'\u30C4' : ['ts u','ツ'],
    u'\u30C5' : ['z u','ヅ'],
    u'\u30C6' : ['t e','テ'],
    u'\u30C7' : ['d e','デ'],
    u'\u30C8' : ['t o','ト'],
    u'\u30C9' : ['d o','ド'],
    u'\u30CA' : ['n A','ナ'],
    u'\u30CB' : ['n i','ニ'],
    u'\u30CC' : ['n u','ヌ'],
    u'\u30CD' : ['n e','ネ'],
    u'\u30CE' : ['n o','ノ'],
    u'\u30CF' : ['h A','ハ'],
    u'\u30D0' : ['b A','バ'],
    u'\u30D1' : ['p A','パ'],
    u'\u30D2' : ['h i','ヒ'],
    u'\u30D3' : ['b i','ビ'],
    u'\u30D4' : ['p i','ピ'],
    u'\u30D5' : ['h u','フ'],
    u'\u30D6' : ['b u','ブ'],
    u'\u30D7' : ['p u','プ'],
    u'\u30D8' : ['h e','ヘ'],
    u'\u30D9' : ['b e','ベ'],
    u'\u30DA' : ['p e','ペ'],
    u'\u30DB' : ['h o','ホ'],
    u'\u30DC' : ['b o','ボ'],
    u'\u30DD' : ['p o','ポ'],
    u'\u30DE' : ['m A','マ'],
    u'\u30DF' : ['m i','ミ'],
    u'\u30E0' : ['m u','ム'],
    u'\u30E1' : ['m e','メ'],
    u'\u30E2' : ['m o','モ'],
    u'\u30E3' : ['j a','ャ'],
    u'\u30E4' : ['j a','ヤ'],
    u'\u30E5' : ['yu','ュ'],
    u'\u30E6' : ['yu','ユ'],
    u'\u30E7' : ['j o','ョ'],
    u'\u30E8' : ['j o','ヨ'],
    u'\u30E9' : ['r A','ラ'],
    u'\u30EA' : ['r i','リ'],
    u'\u30EB' : ['r u','ル'],
    u'\u30EC' : ['r e','レ'],
    u'\u30ED' : ['r o','ロ'],
    u'\u30EE' : ['w A','ヮ'],
    u'\u30EF' : ['w A','ワ'],
    u'\u30F0' : ['w i','ヰ'],
    u'\u30F1' : ['w e','ヱ'],
    u'\u30F2' : ['w o','ヲ'],
    u'\u30F3' : ['N','ン'],
    u'\u30F4' : ['v u','ヴ'],
    u'\u30F5' : ['k A','ヵ'],
    u'\u30F6' : ['k e','ヶ'],
    u'\u30F7' : ['v A','ヷ'],
    u'\u30F8' : ['v i','ヸ'],
    u'\u30F9' : ['v e','ヹ'],
    u'\u30FA' : ['v o','ヺ'],
    u'\u30FB' : ['(MIDDLE DOT)','・'],
    u'\u30FC' : ['(PROLONGED SOUND MARK)','ー'],
    u'\u30FD' : ['(ITERATION MARK)','ヽ'],
    u'\u30FE' : ['(VOICED ITERATION MARK)','ヾ'],
    u'\u0C81' : ['(R:0C81)','ಁ'],
    u'\u0C82' : ['(M)','ಂ'],
    u'\u0C83' : ['(H)','ಃ'],
    u'\u0C85' : ['>','ಅ'],
    u'\u0C86' : ['>:','ಆ'],
    u'\u0C87' : ['i','ಇ'],
    u'\u0C88' : ['i:','ಈ'],
    u'\u0C89' : ['u','ಉ'],
    u'\u0C8A' : ['u:','ಊ'],
    u'\u0C8B' : ['9r i, 9r u','ಋ'],
    u'\u0C8C' : ['l=','ಌ'],
    u'\u0C8D' : ['(R:0C8D)','಍'],
    u'\u0C8E' : ['e','ಎ'],
    u'\u0C8F' : ['e:','ಏ'],
    u'\u0C90' : ['aI','ಐ'],
    u'\u0C91' : ['(R:0C91)','಑'],
    u'\u0C92' : ['o','ಒ'],
    u'\u0C93' : ['o:','ಓ'],
    u'\u0C94' : ['aU','ಔ'],
    u'\u0C95' : ['k A','ಕ'],
    u'\u0C96' : ['kh A','ಖ'],
    u'\u0C97' : ['g A','ಗ'],
    u'\u0C98' : ['gh A','ಘ'],
    u'\u0C99' : ['N A','ಙ'],
    u'\u0C9A' : ['c A','ಚ'],
    u'\u0C9B' : ['ch A','ಛ'],
    u'\u0C9C' : ['J A','ಜ'],
    u'\u0C9D' : ['Jh A','ಝ'],
    u'\u0C9E' : ['n~ A','ಞ'],
    u'\u0C9F' : ['tr A','ಟ'],
    u'\u0CA0' : ['tR A','ಠ'],
    u'\u0CA1' : ['dr A','ಡ'],
    u'\u0CA2' : ['dR A','ಢ'],
    u'\u0CA3' : ['nr A','ಣ'],
    u'\u0CA4' : ['t[ A','ತ'],
    u'\u0CA5' : ['t[_h A','ಥ'],
    u'\u0CA6' : ['d[ A','ದ'],
    u'\u0CA7' : ['d[_h A','ಧ'],
    u'\u0CA8' : ['n[ A','ನ'],
    u'\u0CA9' : ['(R:0CA9)','಩'],
    u'\u0CAA' : ['p A','ಪ'],
    u'\u0CAB' : ['ph A','ಫ'],
    u'\u0CAC' : ['b A','ಬ'],
    u'\u0CAD' : ['bh A','ಭ'],
    u'\u0CAE' : ['m A','ಮ'],
    u'\u0CAF' : ['j A','ಯ'],
    u'\u0CB0' : ['9r A','ರ'],
    u'\u0CB1' : ['rr A','ಱ'],
    u'\u0CB2' : ['l A','ಲ'],
    u'\u0CB3' : ['lr A','ಳ'],
    u'\u0CB4' : ['(R:0CB4)','಴'],
    u'\u0CB5' : ['v A','ವ'],
    u'\u0CB6' : ['S A','ಶ'],
    u'\u0CB7' : ['S A','ಷ'],
    u'\u0CB8' : ['s A','ಸ'],
    u'\u0CB9' : ['hv A','ಹ'],
    u'\u0CBC' : ['(NUKTA)','಼'],
    u'\u0CBD' : ['(AVAGRAHA)','ಽ'],
    u'\u0CBE' : ['>:','ಾ'],
    u'\u0CBF' : ['i','ಿ'],
    u'\u0CC0' : ['i:','ೀ'],
    u'\u0CC1' : ['u','ು'],
    u'\u0CC2' : ['u:','ೂ'],
    u'\u0CC3' : ['9r=','ೃ'],
    u'\u0CC4' : ['rr=','ೄ'],
    u'\u0CC5' : ['(R:0CC5)','೅'],
    u'\u0CC6' : ['e','ೆ'],
    u'\u0CC7' : ['e:','ೇ'],
    u'\u0CC8' : ['aI','ೈ'],
    u'\u0CC9' : ['(R:0CC9)','೉'],
    u'\u0CCA' : ['o','ೊ'],
    u'\u0CCB' : ['o:','ೋ'],
    u'\u0CCC' : ['aU','ೌ'],
    u'\u0CCD' : ['(P)','್'],
    u'\u0CD5' : ['(LENGTH MARK)','ೕ'],
    u'\u0CD6' : ['(aI LENGTH MARK)','ೖ'],
    u'\u0CDE' : ['f A','ೞ'],
    u'\u0CDF' : ['(R:0CDF)','೟'],
    u'\u0CE0' : ['rr=','ೠ'],
    u'\u0CE1' : ['l=','ೡ'],
    u'\u0CE4' : ['(R:0CE4)','೤'],
    u'\u0CE6' : ['(ZERO)','೦'],
    u'\u0CE7' : ['(ONE)','೧'],
    u'\u0CE8' : ['(TWO)','೨'],
    u'\u0CE9' : ['(THREE)','೩'],
    u'\u0CEA' : ['(FOUR)','೪'],
    u'\u0CEB' : ['(FIVE)','೫'],
    u'\u0CEC' : ['(SIX)','೬'],
    u'\u0CED' : ['(SEVEN)','೭'],
    u'\u0CEE' : ['(EIGHT)','೮'],
    u'\u0CEF' : ['(NINE)','೯'],
    u'\u1780' : ['k A','ក'],
    u'\u1781' : ['kh A','ខ'],
    u'\u1782' : ['k >','គ'],
    u'\u1783' : ['kh >','ឃ'],
    u'\u1784' : ['N >','ង'],
    u'\u1785' : ['c A','ច'],
    u'\u1786' : ['ch A','ឆ'],
    u'\u1787' : ['c >','ជ'],
    u'\u1788' : ['ch >','ឈ'],
    u'\u1789' : ['n~ >','ញ'],
    u'\u178A' : ['dr A','ដ'],
    u'\u178B' : ['th A','ឋ'],
    u'\u178C' : ['dr >','ឌ'],
    u'\u178D' : ['th >','ឍ'],
    u'\u178E' : ['n >','ណ'],
    u'\u178F' : ['t A','ត'],
    u'\u1790' : ['th A','ថ'],
    u'\u1791' : ['t >','ទ'],
    u'\u1792' : ['th >','ធ'],
    u'\u1793' : ['n >','ន'],
    u'\u1794' : ['b A','ប'],
    u'\u1795' : ['ph A','ផ'],
    u'\u1796' : ['p >','ព'],
    u'\u1797' : ['ph >','ភ'],
    u'\u1798' : ['m >','ម'],
    u'\u1799' : ['j >','យ'],
    u'\u179A' : ['r >','រ'],
    u'\u179B' : ['l >','ល'],
    u'\u179C' : ['w >','វ'],
    u'\u179D' : ['sh A','ឝ'],
    u'\u179E' : ['sr A','ឞ'],
    u'\u179F' : ['s A','ស'],
    u'\u17A0' : ['h A','ហ'],
    u'\u17A1' : ['l A','ឡ'],
    u'\u17A2' : ['? A','អ'],
    u'\u17A3' : ['A','ឣ'],
    u'\u17A4' : ['a:','ឤ'],
    u'\u17A5' : ['i','ឥ'],
    u'\u17A6' : ['i:','ឦ'],
    u'\u17A7' : ['o','ឧ'],
    u'\u17A8' : ['& w','ឨ'],
    u'\u17A9' : ['u:','ឩ'],
    u'\u17AA' : ['A w','ឪ'],
    u'\u17AB' : ['r','ឫ'],
    u'\u17AC' : ['r','ឬ'],
    u'\u17AD' : ['l','ឭ'],
    u'\u17AE' : ['l','ឮ'],
    u'\u17AF' : ['E','ឯ'],
    u'\u17B0' : ['a j','ឰ'],
    u'\u17B1' : ['a w','ឱ'],
    u'\u17B2' : ['o:','ឲ'],
    u'\u17B3' : ['> w','ឳ'],
    u'\u17B4' : [' A','឴'],
    u'\u17B5' : ['a:','឵'],
    u'\u17B6' : ['a:','ា'],
    u'\u17B7' : ['i','ិ'],
    u'\u17B8' : ['i:','ី'],
    u'\u17B9' : ['i','ឹ'],
    u'\u17BA' : ['i:','ឺ'],
    u'\u17BB' : ['o','ុ'],
    u'\u17BC' : ['u:','ូ'],
    u'\u17BD' : ['u >','ួ'],
    u'\u17BE' : ['>:','ើ'],
    u'\u17BF' : ['i >','ឿ'],
    u'\u17C0' : ['i >','ៀ'],
    u'\u17C1' : ['e:','េ'],
    u'\u17C2' : ['E:','ែ'],
    u'\u17C3' : ['a j','ៃ'],
    u'\u17C4' : ['o:','ោ'],
    u'\u17C5' : ['a w','ៅ'],
    u'\u17C6' : ['(M)','ំ'],
    u'\u17C7' : ['(A)','ះ'],
    u'\u17C8' : ['(Y)','ៈ'],
    u'\u17C9' : ['(MU)','៉'],
    u'\u17CA' : ['(T)','៊'],
    u'\u17CB' : ['(##)','់'],
    u'\u17CC' : ['(##)','៌'],
    u'\u17CD' : ['(##)','៍'],
    u'\u17CE' : ['(##)','៎'],
    u'\u17CF' : ['(##)','៏'],
    u'\u17D0' : ['(##)','័'],
    u'\u17D1' : ['(##)','៑'],
    u'\u17D2' : ['(##)','្'],
    u'\u17D3' : ['(##)','៓'],
    u'\u17D4' : ['(##)','។'],
    u'\u17D5' : ['(##)','៕'],
    u'\u17D6' : ['(##)','៖'],
    u'\u17D7' : ['(##)','ៗ'],
    u'\u17D8' : ['(##)','៘'],
    u'\u17D9' : ['(##)','៙'],
    u'\u17DA' : ['(##)','៚'],
    u'\u17DB' : ['(CURRENCY SYMBOL)','៛'],
    u'\u17DC' : ['(##)','ៜ'],
    u'\u17DD' : ['(##)','៝'],
    u'\u17E0' : ['(ZERO)','០'],
    u'\u17E1' : ['(ONE)','១'],
    u'\u17E2' : ['(TWO)','២'],
    u'\u17E3' : ['(THREE)','៣'],
    u'\u17E4' : ['(FOUR)','៤'],
    u'\u17E5' : ['(FIVE)','៥'],
    u'\u17E6' : ['(SIX)','៦'],
    u'\u17E7' : ['(SEVEN)','៧'],
    u'\u17E8' : ['(EIGHT)','៨'],
    u'\u17E9' : ['(NINE)','៩'],
    u'\u17F0' : ['(TEN)','៰'],
    u'\u17F1' : ['(##)','៱'],
    u'\u17F2' : ['(##)','៲'],
    u'\u17F3' : ['(##)','៳'],
    u'\u17F4' : ['(##)','៴'],
    u'\u17F5' : ['(##)','៵'],
    u'\u17F6' : ['(##)','៶'],
    u'\u17F7' : ['(##)','៷'],
    u'\u17F8' : ['(##)','៸'],
    u'\u17F9' : ['(##)','៹'],
    u'\u19E0' : ['(##)','᧠'],
    u'\u19E1' : ['(##)','᧡'],
    u'\u19E2' : ['(##)','᧢'],
    u'\u19E3' : ['(##)','᧣'],
    u'\u19E4' : ['(##)','᧤'],
    u'\u19E5' : ['(##)','᧥'],
    u'\u19E6' : ['(##)','᧦'],
    u'\u19E7' : ['(##)','᧧'],
    u'\u19E8' : ['(##)','᧨'],
    u'\u19E9' : ['(##)','᧩'],
    u'\u19EA' : ['(##)','᧪'],
    u'\u19EB' : ['(##)','᧫'],
    u'\u19EC' : ['(##)','᧬'],
    u'\u19ED' : ['(##)','᧭'],
    u'\u19EE' : ['(##)','᧮'],
    u'\u19EF' : ['(##)','᧯'],
    u'\u19F0' : ['(##)','᧰'],
    u'\u19F1' : ['(##)','᧱'],
    u'\u19F2' : ['(##)','᧲'],
    u'\u19F3' : ['(##)','᧳'],
    u'\u19F4' : ['(##)','᧴'],
    u'\u19F5' : ['(##)','᧵'],
    u'\u19F6' : ['(##)','᧶'],
    u'\u19F7' : ['(##)','᧷'],
    u'\u19F8' : ['(##)','᧸'],
    u'\u19F9' : ['(##)','᧹'],
    u'\u19FA' : ['(##)','᧺'],
    u'\u19FB' : ['(##)','᧻'],
    u'\u19FC' : ['(##)','᧼'],
    u'\u19FD' : ['(##)','᧽'],
    u'\u19FE' : ['(##)','᧾'],
    u'\u19FF' : ['(##)','᧿'],
    u'\uAC00' : ['k a','가'],
    u'\uAC01' : ['k a k','각'],
    u'\uAC02' : ['k a k>','갂'],
    u'\uAC03' : ['k a k sh','갃'],
    u'\uAC04' : ['k a n','간'],
    u'\uAC05' : ['k a n tS','갅'],
    u'\uAC06' : ['k a n h','갆'],
    u'\uAC07' : ['k a t','갇'],
    u'\uAC08' : ['k a l','갈'],
    u'\uAC09' : ['k a l k','갉'],
    u'\uAC0A' : ['k a l m','갊'],
    u'\uAC0B' : ['k a l b','갋'],
    u'\uAC0C' : ['k a l sh','갌'],
    u'\uAC0D' : ['k a l th','갍'],
    u'\uAC0E' : ['k a l ph','갎'],
    u'\uAC0F' : ['k a l h','갏'],
    u'\uAC10' : ['k a m','감'],
    u'\uAC11' : ['k a p','갑'],
    u'\uAC12' : ['k a p sh','값'],
    u'\uAC13' : ['k a sh','갓'],
    u'\uAC14' : ['k a s','갔'],
    u'\uAC15' : ['k a N','강'],
    u'\uAC16' : ['k a tS','갖'],
    u'\uAC17' : ['k a tSh','갗'],
    u'\uAC18' : ['k a kh','갘'],
    u'\uAC19' : ['k a th','같'],
    u'\uAC1A' : ['k a ph','갚'],
    u'\uAC1B' : ['k a h','갛'],
    u'\uAC1C' : ['k @','개'],
    u'\uAC1D' : ['k @ k','객'],
    u'\uAC1E' : ['k @ k>','갞'],
    u'\uAC1F' : ['k @ k sh','갟'],
    u'\uAC20' : ['k @ n','갠'],
    u'\uAC21' : ['k @ n tS','갡'],
    u'\uAC22' : ['k @ n h','갢'],
    u'\uAC23' : ['k @ t','갣'],
    u'\uAC24' : ['k @ l','갤'],
    u'\uAC25' : ['k @ l k','갥'],
    u'\uAC26' : ['k @ l m','갦'],
    u'\uAC27' : ['k @ l p','갧'],
    u'\uAC28' : ['k @ l sh','갨'],
    u'\uAC29' : ['k @ l th','갩'],
    u'\uAC2A' : ['k @ l ph','갪'],
    u'\uAC2B' : ['k @ l h','갫'],
    u'\uAC2C' : ['k @ m','갬'],
    u'\uAC2D' : ['k @ p','갭'],
    u'\uAC2E' : ['k @ p sh','갮'],
    u'\uAC2F' : ['k @ sh','갯'],
    u'\uAC30' : ['k @ s','갰'],
    u'\uAC31' : ['k @ N','갱'],
    u'\uAC32' : ['k @ tS','갲'],
    u'\uAC33' : ['k @ tSh','갳'],
    u'\uAC34' : ['k @ kh','갴'],
    u'\uAC35' : ['k @ th','갵'],
    u'\uAC36' : ['k @ ph','갶'],
    u'\uAC37' : ['k @ h','갷'],
    u'\uAC38' : ['k j a','갸'],
    u'\uAC39' : ['k j a k','갹'],
    u'\uAC3A' : ['k j a k>','갺'],
    u'\uAC3B' : ['k j a k sh','갻'],
    u'\uAC3C' : ['k j a n','갼'],
    u'\uAC3D' : ['k j a n tS','갽'],
    u'\uAC3E' : ['k j a n h','갾'],
    u'\uAC3F' : ['k j a t','갿'],
    u'\uAC40' : ['k j a l','걀'],
    u'\uAC41' : ['k j a l k','걁'],
    u'\uAC42' : ['k j a L M','걂'],
    u'\uAC43' : ['k j a l p','걃'],
    u'\uAC44' : ['k j a l sh','걄'],
    u'\uAC45' : ['k j a l th','걅'],
    u'\uAC46' : ['k j a l ph','걆'],
    u'\uAC47' : ['k j a l h','걇'],
    u'\uAC48' : ['k j a m','걈'],
    u'\uAC49' : ['k j a p','걉'],
    u'\uAC4A' : ['k j a p sh','걊'],
    u'\uAC4B' : ['k j a sh','걋'],
    u'\uAC4C' : ['k j a s','걌'],
    u'\uAC4D' : ['k j a N','걍'],
    u'\uAC4E' : ['k j a tS','걎'],
    u'\uAC4F' : ['k j a tSh','걏'],
    u'\uAC50' : ['k j a kh','걐'],
    u'\uAC51' : ['k j a th','걑'],
    u'\uAC52' : ['k j a ph','걒'],
    u'\uAC53' : ['k j a h','걓'],
    u'\uAC54' : ['k j @','걔'],
    u'\uAC55' : ['k j @ k','걕'],
    u'\uAC56' : ['k j @ k>','걖'],
    u'\uAC57' : ['k j @ k sh','걗'],
    u'\uAC58' : ['k j @ n','걘'],
    u'\uAC59' : ['k j @ n tS','걙'],
    u'\uAC5A' : ['k j @ n h','걚'],
    u'\uAC5B' : ['k j @ t','걛'],
    u'\uAC5C' : ['k j @ l','걜'],
    u'\uAC5D' : ['k j @ l k','걝'],
    u'\uAC5E' : ['k j @ l m','걞'],
    u'\uAC5F' : ['k j @ l p','걟'],
    u'\uAC60' : ['k j @ l sh','걠'],
    u'\uAC61' : ['k j @ l th','걡'],
    u'\uAC62' : ['k j @ l ph','걢'],
    u'\uAC63' : ['k j @ l h','걣'],
    u'\uAC64' : ['k j @ m','걤'],
    u'\uAC65' : ['k j @ p','걥'],
    u'\uAC66' : ['k j @ p sh','걦'],
    u'\uAC67' : ['k j @ sh','걧'],
    u'\uAC68' : ['k j @ s','걨'],
    u'\uAC69' : ['k j @ N','걩'],
    u'\uAC6A' : ['k j @ tS','걪'],
    u'\uAC6B' : ['k j @ tSh','걫'],
    u'\uAC6C' : ['k j @ kh','걬'],
    u'\uAC6D' : ['k j @ th','걭'],
    u'\uAC6E' : ['k j @ ph','걮'],
    u'\uAC6F' : ['k j @ h','걯'],
    u'\uAC70' : ['k ^','거'],
    u'\uAC71' : ['k ^ k','걱'],
    u'\uAC72' : ['k ^ k>','걲'],
    u'\uAC73' : ['k ^ k sh','걳'],
    u'\uAC74' : ['k ^ n','건'],
    u'\uAC75' : ['k ^ n tS','걵'],
    u'\uAC76' : ['k ^ n h','걶'],
    u'\uAC77' : ['k ^ t','걷'],
    u'\uAC78' : ['k ^ l','걸'],
    u'\uAC79' : ['k ^ l k','걹'],
    u'\uAC7A' : ['k ^ l m','걺'],
    u'\uAC7B' : ['k ^ l p','걻'],
    u'\uAC7C' : ['k ^ l sh','걼'],
    u'\uAC7D' : ['k ^ l th','걽'],
    u'\uAC7E' : ['k ^ l ph','걾'],
    u'\uAC7F' : ['k ^ l h','걿'],
    u'\uAC80' : ['k ^ m','검'],
    u'\uAC81' : ['k ^ p','겁'],
    u'\uAC82' : ['k ^ p sh','겂'],
    u'\uAC83' : ['k ^ sh','것'],
    u'\uAC84' : ['k ^ s','겄'],
    u'\uAC85' : ['k ^ N','겅'],
    u'\uAC86' : ['k ^ tS','겆'],
    u'\uAC87' : ['k ^ tSh','겇'],
    u'\uAC88' : ['k ^ kh','겈'],
    u'\uAC89' : ['k ^ th','겉'],
    u'\uAC8A' : ['k ^ ph','겊'],
    u'\uAC8B' : ['k ^ h','겋'],
    u'\uAC8C' : ['k e','게'],
    u'\uAC8D' : ['k e k','겍'],
    u'\uAC8E' : ['k e k>','겎'],
    u'\uAC8F' : ['k e k sh','겏'],
    u'\uAC90' : ['k e n','겐'],
    u'\uAC91' : ['k e n tS','겑'],
    u'\uAC92' : ['k e n h','겒'],
    u'\uAC93' : ['k e t','겓'],
    u'\uAC94' : ['k e l','겔'],
    u'\uAC95' : ['k e l k','겕'],
    u'\uAC96' : ['k e l m','겖'],
    u'\uAC97' : ['k e l p','겗'],
    u'\uAC98' : ['k e l sh','겘'],
    u'\uAC99' : ['k e l th','겙'],
    u'\uAC9A' : ['k e l ph','겚'],
    u'\uAC9B' : ['k e l h','겛'],
    u'\uAC9C' : ['k e m','겜'],
    u'\uAC9D' : ['k e p','겝'],
    u'\uAC9E' : ['k e p sh','겞'],
    u'\uAC9F' : ['k e sh','겟'],
    u'\uACA0' : ['k e s','겠'],
    u'\uACA1' : ['k e N','겡'],
    u'\uACA2' : ['k e tS','겢'],
    u'\uACA3' : ['k e tSh','겣'],
    u'\uACA4' : ['k e kh','겤'],
    u'\uACA5' : ['k e th','겥'],
    u'\uACA6' : ['k e ph','겦'],
    u'\uACA7' : ['k e h','겧'],
    u'\uACA8' : ['k j ^','겨'],
    u'\uACA9' : ['k j ^ k','격'],
    u'\uACAA' : ['k j ^ k>','겪'],
    u'\uACAB' : ['k j ^ k sh','겫'],
    u'\uACAC' : ['k j ^ n','견'],
    u'\uACAD' : ['k j ^ n tS','겭'],
    u'\uACAE' : ['k j ^ n h','겮'],
    u'\uACAF' : ['k j ^ t','겯'],
    u'\uACB0' : ['k j ^ l','결'],
    u'\uACB1' : ['k j ^ l k','겱'],
    u'\uACB2' : ['k j ^ l m','겲'],
    u'\uACB3' : ['k j ^ l p','겳'],
    u'\uACB4' : ['k j ^ l sh','겴'],
    u'\uACB5' : ['k j ^ l th','겵'],
    u'\uACB6' : ['k j ^ l ph','겶'],
    u'\uACB7' : ['k j ^ l h','겷'],
    u'\uACB8' : ['k j ^ m','겸'],
    u'\uACB9' : ['k j ^ p','겹'],
    u'\uACBA' : ['k j ^ p sh','겺'],
    u'\uACBB' : ['k j ^ sh','겻'],
    u'\uACBC' : ['k j ^ s','겼'],
    u'\uACBD' : ['k j ^ N','경'],
    u'\uACBE' : ['k j ^ tS','겾'],
    u'\uACBF' : ['k j ^ tSh','겿'],
    u'\uACC1' : ['k j ^ th','곁'],
    u'\uACC2' : ['k j ^ ph','곂'],
    u'\uACC3' : ['k j ^ h','곃'],
    u'\uACC4' : ['k j e','계'],
    u'\uACC5' : ['k j e k','곅'],
    u'\uACC6' : ['k j e k>','곆'],
    u'\uACC7' : ['k j e k sh','곇'],
    u'\uACC8' : ['k j e n','곈'],
    u'\uACC9' : ['k j e n tS','곉'],
    u'\uACCA' : ['k j e n h','곊'],
    u'\uACCB' : ['k j e t','곋'],
    u'\uACCC' : ['k j e l','곌'],
    u'\uACCD' : ['k j e l k','곍'],
    u'\uACCE' : ['k j e l m','곎'],
    u'\uACCF' : ['k j e l p','곏'],
    u'\uACD1' : ['k j e l sh','곑'],
    u'\uACD2' : ['k j e l th','곒'],
    u'\uACD3' : ['k j e l ph','곓'],
    u'\uACD4' : ['k j e m','곔'],
    u'\uACD5' : ['k j e p','곕'],
    u'\uACD6' : ['k j e p sh','곖'],
    u'\uACD7' : ['k j e sh','곗'],
    u'\uACD8' : ['k j e s','곘'],
    u'\uACD9' : ['k j e N','곙'],
    u'\uACDA' : ['k j e tS','곚'],
    u'\uACDB' : ['k j e tSh','곛'],
    u'\uACDC' : ['k j e kh','곜'],
    u'\uACDD' : ['k j e th','곝'],
    u'\uACDE' : ['k j e ph','곞'],
    u'\uACDF' : ['k j e h','곟'],
    u'\uACE0' : ['k o','고'],
    u'\uACE1' : ['k o k','곡'],
    u'\uACE2' : ['k o k>','곢'],
    u'\uACE3' : ['k o k sh','곣'],
    u'\uACE4' : ['k o n','곤'],
    u'\uACE5' : ['k o n tS','곥'],
    u'\uACE6' : ['k o n h','곦'],
    u'\uACE7' : ['k o t','곧'],
    u'\uACE8' : ['k o l','골'],
    u'\uACE9' : ['k o l k','곩'],
    u'\uACEA' : ['k o l m','곪'],
    u'\uACEB' : ['k o l b','곫'],
    u'\uACEC' : ['k o l sh','곬'],
    u'\uACED' : ['k o l th','곭'],
    u'\uACEE' : ['k o l ph','곮'],
    u'\uACEF' : ['k o l h','곯'],
    u'\uACF0' : ['k o m','곰'],
    u'\uACF1' : ['k o p','곱'],
    u'\uACF2' : ['k o p sh','곲'],
    u'\uACF3' : ['k o sh','곳'],
    u'\uACF4' : ['k o s','곴'],
    u'\uACF5' : ['k o N','공'],
    u'\uACF6' : ['k o tS','곶'],
    u'\uACF7' : ['k o tSh','곷'],
    u'\uACF8' : ['k o kh','곸'],
    u'\uACF9' : ['k o th','곹'],
    u'\uACFA' : ['k o ph','곺'],
    u'\uACFB' : ['k o h','곻'],
    u'\uACFC' : ['k w a','과'],
    u'\uACFD' : ['k w a k','곽'],
    u'\uACFE' : ['k w a k>','곾'],
    u'\uACFF' : ['k w a k sh','곿'],
    u'\uAD00' : ['k w a n','관'],
    u'\uAD01' : ['k w a n tS','괁'],
    u'\uAD02' : ['k w a n h','괂'],
    u'\uAD03' : ['k w a t','괃'],
    u'\uAD04' : ['k w a l','괄'],
    u'\uAD05' : ['k w a l k','괅'],
    u'\uAD06' : ['k w a l m','괆'],
    u'\uAD07' : ['k w a l p','괇'],
    u'\uAD08' : ['k w a l sh','괈'],
    u'\uAD09' : ['k w a l th','괉'],
    u'\uAD0A' : ['k w a l ph','괊'],
    u'\uAD0B' : ['k w a l h','괋'],
    u'\uAD0C' : ['k w a m','괌'],
    u'\uAD0D' : ['k w a p','괍'],
    u'\uAD0E' : ['k w a p sh','괎'],
    u'\uAD0F' : ['k w a sh','괏'],
    u'\uAD10' : ['k w a s','괐'],
    u'\uAD11' : ['k w a N','광'],
    u'\uAD12' : ['k w a tS','괒'],
    u'\uAD13' : ['k w a tSh','괓'],
    u'\uAD14' : ['k w a kh','괔'],
    u'\uAD15' : ['k w a th','괕'],
    u'\uAD16' : ['k w a ph','괖'],
    u'\uAD17' : ['k w a h','괗'],
    u'\uAD18' : ['k w @','괘'],
    u'\uAD19' : ['k w @ k','괙'],
    u'\uAD1A' : ['k w @ k>','괚'],
    u'\uAD1B' : ['k w @ k sh','괛'],
    u'\uAD1C' : ['k w @ n','괜'],
    u'\uAD1D' : ['k w @ n tS','괝'],
    u'\uAD1E' : ['k w @ n h','괞'],
    u'\uAD1F' : ['k w @ t','괟'],
    u'\uAD20' : ['k w @ l','괠'],
    u'\uAD21' : ['k w @ l k','괡'],
    u'\uAD22' : ['k w @ l m','괢'],
    u'\uAD23' : ['k w @ l p','괣'],
    u'\uAD24' : ['k w @ l sh','괤'],
    u'\uAD25' : ['k w @ l th','괥'],
    u'\uAD26' : ['k w @ l ph','괦'],
    u'\uAD27' : ['k w @ l h','괧'],
    u'\uAD28' : ['k w @ m','괨'],
    u'\uAD29' : ['k w @ p','괩'],
    u'\uAD2A' : ['k w @ p sh','괪'],
    u'\uAD2B' : ['k w @ sh','괫'],
    u'\uAD2C' : ['k w @ s','괬'],
    u'\uAD2D' : ['k w @ N','괭'],
    u'\uAD2E' : ['k w @ tS','괮'],
    u'\uAD2F' : ['k w @ tSh','괯'],
    u'\uAD30' : ['k w @ kh','괰'],
    u'\uAD31' : ['k w @ th','괱'],
    u'\uAD32' : ['k w @ ph','괲'],
    u'\uAD33' : ['k w @ h','괳'],
    u'\uAD34' : ['k w e','괴'],
    u'\uAD35' : ['k w e k','괵'],
    u'\uAD36' : ['k w e k>','괶'],
    u'\uAD37' : ['k w e k sh','괷'],
    u'\uAD38' : ['k w e n','괸'],
    u'\uAD39' : ['k w e n tS','괹'],
    u'\uAD3A' : ['k w e n h','괺'],
    u'\uAD3B' : ['k w e t','괻'],
    u'\uAD3C' : ['k w e l','괼'],
    u'\uAD3D' : ['k w e l k','괽'],
    u'\uAD3E' : ['k w e l m','괾'],
    u'\uAD3F' : ['k w e l p','괿'],
    u'\uAD40' : ['k w e l sh','굀'],
    u'\uAD41' : ['k w e l th','굁'],
    u'\uAD42' : ['k w e l ph','굂'],
    u'\uAD43' : ['k w e l h','굃'],
    u'\uAD44' : ['k w e m','굄'],
    u'\uAD45' : ['k w e p','굅'],
    u'\uAD46' : ['k w e p sh','굆'],
    u'\uAD47' : ['k w e sh','굇'],
    u'\uAD48' : ['k w e s','굈'],
    u'\uAD49' : ['k w e N','굉'],
    u'\uAD4A' : ['k w e tS','굊'],
    u'\uAD4B' : ['k w e tSh','굋'],
    u'\uAD4C' : ['k w e kh','굌'],
    u'\uAD4D' : ['k w e th','굍'],
    u'\uAD4E' : ['k w e ph','굎'],
    u'\uAD4F' : ['k w e h','굏'],
    u'\uAD50' : ['k j o','교'],
    u'\uAD51' : ['k j o k','굑'],
    u'\uAD52' : ['k j o k>','굒'],
    u'\uAD53' : ['k j o k sh','굓'],
    u'\uAD54' : ['k j o n','굔'],
    u'\uAD55' : ['k j o n tS','굕'],
    u'\uAD56' : ['k j o n h','굖'],
    u'\uAD57' : ['k j o t','굗'],
    u'\uAD58' : ['k j o l','굘'],
    u'\uAD59' : ['k j o l k','굙'],
    u'\uAD5A' : ['k j o l m','굚'],
    u'\uAD5B' : ['k j o l p','굛'],
    u'\uAD5C' : ['k j o l sh','굜'],
    u'\uAD5D' : ['k j o l th','굝'],
    u'\uAD5E' : ['k j o l ph','굞'],
    u'\uAD5F' : ['k j o l h','굟'],
    u'\uAD60' : ['k j o m','굠'],
    u'\uAD61' : ['k j o p','굡'],
    u'\uAD62' : ['k j o p sh','굢'],
    u'\uAD63' : ['k j o sh','굣'],
    u'\uAD64' : ['k j o s','굤'],
    u'\uAD65' : ['k j o N','굥'],
    u'\uAD66' : ['k j o tS','굦'],
    u'\uAD67' : ['k j o tSh','굧'],
    u'\uAD68' : ['k j o kh','굨'],
    u'\uAD69' : ['k j o th','굩'],
    u'\uAD6A' : ['k j o ph','굪'],
    u'\uAD6B' : ['k j o h','굫'],
    u'\uAD6C' : ['k u','구'],
    u'\uAD6D' : ['k u k','국'],
    u'\uAD6E' : ['k u k>','굮'],
    u'\uAD6F' : ['k u k sh','굯'],
    u'\uAD70' : ['k u n','군'],
    u'\uAD71' : ['k u n tS','굱'],
    u'\uAD72' : ['k u n h','굲'],
    u'\uAD73' : ['k u t','굳'],
    u'\uAD74' : ['k u l','굴'],
    u'\uAD75' : ['k u l k','굵'],
    u'\uAD76' : ['k u l m','굶'],
    u'\uAD77' : ['k u l p','굷'],
    u'\uAD78' : ['k u l sh','굸'],
    u'\uAD79' : ['k u l th','굹'],
    u'\uAD7A' : ['k u l ph','굺'],
    u'\uAD7B' : ['k u l h','굻'],
    u'\uAD7C' : ['k u m','굼'],
    u'\uAD7D' : ['k u p','굽'],
    u'\uAD7E' : ['k u p sh','굾'],
    u'\uAD7F' : ['k u sh','굿'],
    u'\uAD80' : ['k u s','궀'],
    u'\uAD81' : ['k u N','궁'],
    u'\uAD82' : ['k u tS','궂'],
    u'\uAD83' : ['k u tSh','궃'],
    u'\uAD84' : ['k u kh','궄'],
    u'\uAD85' : ['k u th','궅'],
    u'\uAD86' : ['k u ph','궆'],
    u'\uAD87' : ['k u h','궇'],
    u'\uAD88' : ['k w ^','궈'],
    u'\uAD89' : ['k w ^ k','궉'],
    u'\uAD8A' : ['k w ^ k>','궊'],
    u'\uAD8B' : ['k w ^ k sh','궋'],
    u'\uAD8C' : ['k w ^ n','권'],
    u'\uAD8D' : ['k w ^ n tS','궍'],
    u'\uAD8E' : ['k w ^ n h','궎'],
    u'\uAD8F' : ['k w ^ t','궏'],
    u'\uAD90' : ['k w ^ l','궐'],
    u'\uAD91' : ['k w ^ l k','궑'],
    u'\uAD92' : ['k w ^ l m','궒'],
    u'\uAD93' : ['k w ^ l p','궓'],
    u'\uAD94' : ['k w ^ l sh','궔'],
    u'\uAD95' : ['k w ^ l th','궕'],
    u'\uAD96' : ['k w ^ l ph','궖'],
    u'\uAD97' : ['k w ^ l h','궗'],
    u'\uAD98' : ['k w ^ m','궘'],
    u'\uAD99' : ['k w ^ p','궙'],
    u'\uAD9A' : ['k w ^ p sh','궚'],
    u'\uAD9B' : ['k w ^ sh','궛'],
    u'\uAD9C' : ['k w ^ s','궜'],
    u'\uAD9D' : ['k w ^ N','궝'],
    u'\uAD9E' : ['k w ^ tS','궞'],
    u'\uAD9F' : ['k w ^ tSh','궟'],
    u'\uADA0' : ['k w ^ kh','궠'],
    u'\uADA1' : ['k w ^ th','궡'],
    u'\uADA2' : ['k w ^ ph','궢'],
    u'\uADA3' : ['k w ^ h','궣'],
    u'\uADA4' : ['k w E','궤'],
    u'\uADA5' : ['k w E k','궥'],
    u'\uADA6' : ['k w E k>','궦'],
    u'\uADA7' : ['k w E k sh','궧'],
    u'\uADA8' : ['k w E n','궨'],
    u'\uADA9' : ['k w E n tS','궩'],
    u'\uADAA' : ['k w E n h','궪'],
    u'\uADAB' : ['k w E t','궫'],
    u'\uADAC' : ['k w E l','궬'],
    u'\uADAD' : ['k w E l k','궭'],
    u'\uADAE' : ['k w E l m','궮'],
    u'\uADAF' : ['k w E l p','궯'],
    u'\uADB0' : ['k w E l sh','궰'],
    u'\uADB1' : ['k w E l th','궱'],
    u'\uADB2' : ['k w E l ph','궲'],
    u'\uADB3' : ['k w E l h','궳'],
    u'\uADB4' : ['k w E m','궴'],
    u'\uADB5' : ['k w E p','궵'],
    u'\uADB6' : ['k w E p sh','궶'],
    u'\uADB7' : ['k w E sh','궷'],
    u'\uADB8' : ['k w E s','궸'],
    u'\uADB9' : ['k w E N','궹'],
    u'\uADBA' : ['k w E tS','궺'],
    u'\uADBB' : ['k w E tSh','궻'],
    u'\uADBC' : ['k w E kh','궼'],
    u'\uADBD' : ['k w E th','궽'],
    u'\uADBE' : ['k w E ph','궾'],
    u'\uADBF' : ['k w E h','궿'],
    u'\uADC0' : ['k 7','귀'],
    u'\uADC1' : ['k 7 k','귁'],
    u'\uADC2' : ['k 7 k>','귂'],
    u'\uADC3' : ['k 7 k sh','귃'],
    u'\uADC4' : ['k 7 n','귄'],
    u'\uADC5' : ['k 7 n tS','귅'],
    u'\uADC6' : ['k 7 n h','귆'],
    u'\uADC7' : ['k 7 t','귇'],
    u'\uADC8' : ['k 7 l','귈'],
    u'\uADC9' : ['k 7 l k','귉'],
    u'\uADCA' : ['k 7 l m','귊'],
    u'\uADCB' : ['k 7 l p','귋'],
    u'\uADCC' : ['k 7 l sh','귌'],
    u'\uADCD' : ['k 7 l th','귍'],
    u'\uADCE' : ['k 7 l ph','귎'],
    u'\uADCF' : ['k 7 l h','귏'],
    u'\uADD0' : ['k 7 m','귐'],
    u'\uADD1' : ['k 7 p','귑'],
    u'\uADD2' : ['k 7 p sh','귒'],
    u'\uADD3' : ['k 7 sh','귓'],
    u'\uADD4' : ['k 7 s','귔'],
    u'\uADD5' : ['k 7 N','귕'],
    u'\uADD6' : ['k 7 tS','귖'],
    u'\uADD7' : ['k 7 tSh','귗'],
    u'\uADD8' : ['k 7 kh','귘'],
    u'\uADD9' : ['k 7 th','귙'],
    u'\uADDA' : ['k 7 ph','귚'],
    u'\uADDB' : ['k 7 h','귛'],
    u'\uADDC' : ['k j u','규'],
    u'\uADDD' : ['k j u k','귝'],
    u'\uADDE' : ['k j u k>','귞'],
    u'\uADDF' : ['k j u k sh','귟'],
    u'\uADE0' : ['k j u n','균'],
    u'\uADE1' : ['k j u n tS','귡'],
    u'\uADE2' : ['k j u n h','귢'],
    u'\uADE3' : ['k j u t','귣'],
    u'\uADE4' : ['k j u l','귤'],
    u'\uADE5' : ['k j u l k','귥'],
    u'\uADE6' : ['k j u l m','귦'],
    u'\uADE7' : ['k j u l p','귧'],
    u'\uADE8' : ['k j u l sh','귨'],
    u'\uADE9' : ['k j u l th','귩'],
    u'\uADEA' : ['k j u l ph','귪'],
    u'\uADEB' : ['k j u l h','귫'],
    u'\uADEC' : ['k j u m','귬'],
    u'\uADED' : ['k j u p','귭'],
    u'\uADEE' : ['k j u p sh','귮'],
    u'\uADEF' : ['k j u sh','귯'],
    u'\uADF0' : ['k j u s','귰'],
    u'\uADF1' : ['k j u N','귱'],
    u'\uADF2' : ['k j u tS','귲'],
    u'\uADF3' : ['k j u tSh','귳'],
    u'\uADF4' : ['k j u kh','귴'],
    u'\uADF5' : ['k j u th','귵'],
    u'\uADF6' : ['k j u ph','귶'],
    u'\uADF7' : ['k j u h','귷'],
    u'\uADF8' : ['k 4','그'],
    u'\uADF9' : ['k 4 k','극'],
    u'\uADFA' : ['k 4 k>','귺'],
    u'\uADFB' : ['k 4 k sh','귻'],
    u'\uADFC' : ['k 4 n','근'],
    u'\uADFD' : ['k 4 n tS','귽'],
    u'\uADFE' : ['k 4 n h','귾'],
    u'\uADFF' : ['k 4 t','귿'],
    u'\uAE00' : ['k 4 l','글'],
    u'\uAE01' : ['k 4 l k','긁'],
    u'\uAE02' : ['k 4 l m','긂'],
    u'\uAE03' : ['k 4 l p','긃'],
    u'\uAE04' : ['k 4 l sh','긄'],
    u'\uAE05' : ['k 4 l th','긅'],
    u'\uAE06' : ['k 4 l ph','긆'],
    u'\uAE07' : ['k 4 l h','긇'],
    u'\uAE08' : ['k 4 m','금'],
    u'\uAE09' : ['k 4 p','급'],
    u'\uAE0A' : ['k 4 p sh','긊'],
    u'\uAE0B' : ['k 4 sh','긋'],
    u'\uAE0C' : ['k 4 s','긌'],
    u'\uAE0D' : ['k 4 N','긍'],
    u'\uAE0E' : ['k 4 tS','긎'],
    u'\uAE0F' : ['k 4 tSh','긏'],
    u'\uAE10' : ['k 4 kh','긐'],
    u'\uAE11' : ['k 4 th','긑'],
    u'\uAE12' : ['k 4 ph','긒'],
    u'\uAE13' : ['k 4 h','긓'],
    u'\uAE14' : ['k 4 j','긔'],
    u'\uAE15' : ['k 4 j k','긕'],
    u'\uAE16' : ['k 4 j k>','긖'],
    u'\uAE17' : ['k 4 j k sh','긗'],
    u'\uAE18' : ['k 4 j n','긘'],
    u'\uAE19' : ['k 4 j n tS','긙'],
    u'\uAE1A' : ['k 4 j n h','긚'],
    u'\uAE1B' : ['k 4 j t','긛'],
    u'\uAE1C' : ['k 4 j l','긜'],
    u'\uAE1D' : ['k 4 j l k','긝'],
    u'\uAE1E' : ['k 4 j l m','긞'],
    u'\uAE1F' : ['k 4 j l p','긟'],
    u'\uAE20' : ['k 4 j l sh','긠'],
    u'\uAE21' : ['k 4 j l th','긡'],
    u'\uAE22' : ['k 4 j l ph','긢'],
    u'\uAE23' : ['k 4 j l h','긣'],
    u'\uAE24' : ['k 4 j m','긤'],
    u'\uAE25' : ['k 4 j p','긥'],
    u'\uAE26' : ['k 4 j p sh','긦'],
    u'\uAE27' : ['k 4 j sh','긧'],
    u'\uAE28' : ['k 4 j s','긨'],
    u'\uAE29' : ['k 4 j N','긩'],
    u'\uAE2A' : ['k 4 j tS','긪'],
    u'\uAE2B' : ['k 4 j tSh','긫'],
    u'\uAE2C' : ['k 4 j kh','긬'],
    u'\uAE2D' : ['k 4 j th','긭'],
    u'\uAE2E' : ['k 4 j ph','긮'],
    u'\uAE2F' : ['k 4 j h','긯'],
    u'\uAE30' : ['k i','기'],
    u'\uAE31' : ['k i k','긱'],
    u'\uAE32' : ['k i k>','긲'],
    u'\uAE33' : ['k i k sh','긳'],
    u'\uAE34' : ['k i n','긴'],
    u'\uAE35' : ['k i n tS','긵'],
    u'\uAE36' : ['k i n h','긶'],
    u'\uAE37' : ['k i t','긷'],
    u'\uAE38' : ['k i l','길'],
    u'\uAE39' : ['k i l k','긹'],
    u'\uAE3A' : ['k i l m','긺'],
    u'\uAE3B' : ['k i l p','긻'],
    u'\uAE3C' : ['k i l sh','긼'],
    u'\uAE3D' : ['k i l th','긽'],
    u'\uAE3E' : ['k i l ph','긾'],
    u'\uAE3F' : ['k i l h','긿'],
    u'\uAE40' : ['k i m','김'],
    u'\uAE41' : ['k i p','깁'],
    u'\uAE42' : ['k i p sh','깂'],
    u'\uAE43' : ['k i sh','깃'],
    u'\uAE44' : ['k i s','깄'],
    u'\uAE45' : ['k i N','깅'],
    u'\uAE46' : ['k i tS','깆'],
    u'\uAE47' : ['k i tSh','깇'],
    u'\uAE48' : ['k i kh','깈'],
    u'\uAE49' : ['k i th','깉'],
    u'\uAE4A' : ['k i ph','깊'],
    u'\uAE4B' : ['k i h','깋'],
    u'\uAE4C' : ['k> a','까'],
    u'\uAE4D' : ['k> a k','깍'],
    u'\uAE4E' : ['k> a k>','깎'],
    u'\uAE4F' : ['k> a k ah','깏'],
    u'\uAE50' : ['k> a n','깐'],
    u'\uAE51' : ['k> a n tS','깑'],
    u'\uAE52' : ['k> a n h','깒'],
    u'\uAE53' : ['k> a t','깓'],
    u'\uAE54' : ['k> a l','깔'],
    u'\uAE55' : ['k> a l k','깕'],
    u'\uAE56' : ['k> a l m','깖'],
    u'\uAE57' : ['k> a l p','깗'],
    u'\uAE58' : ['k> a l sh','깘'],
    u'\uAE59' : ['k> a l th','깙'],
    u'\uAE5A' : ['k> a l ph','깚'],
    u'\uAE5B' : ['k> a l h','깛'],
    u'\uAE5C' : ['k> a m','깜'],
    u'\uAE5D' : ['k> a p','깝'],
    u'\uAE5E' : ['k> a p sh','깞'],
    u'\uAE5F' : ['k> a sh','깟'],
    u'\uAE60' : ['k> a s','깠'],
    u'\uAE61' : ['k> a N','깡'],
    u'\uAE62' : ['k> a tS','깢'],
    u'\uAE63' : ['k> a tSh','깣'],
    u'\uAE64' : ['k> a kh','깤'],
    u'\uAE65' : ['k> a th','깥'],
    u'\uAE66' : ['k> a ph','깦'],
    u'\uAE67' : ['k> a h','깧'],
    u'\uAE68' : ['k> @','깨'],
    u'\uAE69' : ['k> @ k','깩'],
    u'\uAE6A' : ['k> @ k>','깪'],
    u'\uAE6B' : ['k> @ k sh','깫'],
    u'\uAE6C' : ['k> @ n','깬'],
    u'\uAE6D' : ['k> @ n tS','깭'],
    u'\uAE6E' : ['k> @ n h','깮'],
    u'\uAE6F' : ['k> @ t','깯'],
    u'\uAE70' : ['k> @ l','깰'],
    u'\uAE71' : ['k> @ l k','깱'],
    u'\uAE72' : ['k> @ l m','깲'],
    u'\uAE73' : ['k> @ l p','깳'],
    u'\uAE74' : ['k> @ l sh','깴'],
    u'\uAE75' : ['k> @ l th','깵'],
    u'\uAE76' : ['k> @ l ph','깶'],
    u'\uAE77' : ['k> @ l h','깷'],
    u'\uAE78' : ['k> @ m','깸'],
    u'\uAE79' : ['k> @ p','깹'],
    u'\uAE7A' : ['k> @ p sh','깺'],
    u'\uAE7B' : ['k> @ sh','깻'],
    u'\uAE7C' : ['k> @ s','깼'],
    u'\uAE7D' : ['k> @ N','깽'],
    u'\uAE7E' : ['k> @ tS','깾'],
    u'\uAE7F' : ['k> @ tSh','깿'],
    u'\uAE80' : ['k> @ kh','꺀'],
    u'\uAE81' : ['k> @ th','꺁'],
    u'\uAE82' : ['k> @ ph','꺂'],
    u'\uAE83' : ['k> @ h','꺃'],
    u'\uAE84' : ['k> j a','꺄'],
    u'\uAE85' : ['k> j a k','꺅'],
    u'\uAE86' : ['k> j a k>','꺆'],
    u'\uAE87' : ['k> j a k sh','꺇'],
    u'\uAE88' : ['k> j a n','꺈'],
    u'\uAE89' : ['k> j a n tS','꺉'],
    u'\uAE8A' : ['k> j a n h','꺊'],
    u'\uAE8B' : ['k> j a t','꺋'],
    u'\uAE8C' : ['k> j a l','꺌'],
    u'\uAE8D' : ['k> j a l k','꺍'],
    u'\uAE8E' : ['k> j a l m','꺎'],
    u'\uAE8F' : ['k> j a l p','꺏'],
    u'\uAE90' : ['k> j a l sh','꺐'],
    u'\uAE91' : ['k> j a l th','꺑'],
    u'\uAE92' : ['k> j a l ph','꺒'],
    u'\uAE93' : ['k> j a l h','꺓'],
    u'\uAE94' : ['k> j a m','꺔'],
    u'\uAE95' : ['k> j a p','꺕'],
    u'\uAE96' : ['k> j a p sh','꺖'],
    u'\uAE97' : ['k> j a sh','꺗'],
    u'\uAE98' : ['k> j a s','꺘'],
    u'\uAE99' : ['k> j a N','꺙'],
    u'\uAE9A' : ['k> j a tS','꺚'],
    u'\uAE9B' : ['k> j a tSh','꺛'],
    u'\uAE9C' : ['k> j a kh','꺜'],
    u'\uAE9D' : ['k> j a th','꺝'],
    u'\uAE9E' : ['k> j a ph','꺞'],
    u'\uAE9F' : ['k> j a h','꺟'],
    u'\uAEA0' : ['k> j @','꺠'],
    u'\uAEA1' : ['k> j @ k','꺡'],
    u'\uAEA2' : ['k> j @ k>','꺢'],
    u'\uAEA3' : ['k> j @ k sh','꺣'],
    u'\uAEA4' : ['k> j @ n','꺤'],
    u'\uAEA5' : ['k> j @ n tS','꺥'],
    u'\uAEA6' : ['k> j @ n h','꺦'],
    u'\uAEA7' : ['k> j @ t','꺧'],
    u'\uAEA8' : ['k> j @ l','꺨'],
    u'\uAEA9' : ['k> j @ l k','꺩'],
    u'\uAEAA' : ['k> j @ l m','꺪'],
    u'\uAEAB' : ['k> j @ l p','꺫'],
    u'\uAEAC' : ['k> j @ l sh','꺬'],
    u'\uAEAD' : ['k> j @ l th','꺭'],
    u'\uAEAE' : ['k> j @ l ph','꺮'],
    u'\uAEAF' : ['k> j @ l h','꺯'],
    u'\uAEB0' : ['k> j @ m','꺰'],
    u'\uAEB1' : ['k> j @ p','꺱'],
    u'\uAEB2' : ['k> j @ p sh','꺲'],
    u'\uAEB3' : ['k> j @ sh','꺳'],
    u'\uAEB4' : ['k> j @ s','꺴'],
    u'\uAEB5' : ['k> j @ N','꺵'],
    u'\uAEB6' : ['k> j @ tS','꺶'],
    u'\uAEB7' : ['k> j @ tSh','꺷'],
    u'\uAEB8' : ['k> j @ kh','꺸'],
    u'\uAEB9' : ['k> j @ th','꺹'],
    u'\uAEBA' : ['k> j @ ph','꺺'],
    u'\uAEBB' : ['k> j @ h','꺻'],
    u'\uAEBC' : ['k> ^','꺼'],
    u'\uAEBD' : ['k> ^ k','꺽'],
    u'\uAEBE' : ['k> ^ k>','꺾'],
    u'\uAEBF' : ['k> ^ k sh','꺿'],
    u'\uAEC0' : ['k> ^ n','껀'],
    u'\uAEC1' : ['k> ^ n tS','껁'],
    u'\uAEC2' : ['k> ^ n h','껂'],
    u'\uAEC3' : ['k> ^ t','껃'],
    u'\uAEC4' : ['k> ^ l','껄'],
    u'\uAEC5' : ['k> ^ l k','껅'],
    u'\uAEC6' : ['k> ^ l m','껆'],
    u'\uAEC7' : ['k> ^ l p','껇'],
    u'\uAEC8' : ['k> ^ l sh','껈'],
    u'\uAEC9' : ['k> ^ l th','껉'],
    u'\uAECA' : ['k> ^ l ph','껊'],
    u'\uAECB' : ['k> ^ l h','껋'],
    u'\uAECC' : ['k> ^ m','껌'],
    u'\uAECD' : ['k> ^ p','껍'],
    u'\uAECE' : ['k> ^ p sh','껎'],
    u'\uAECF' : ['k> ^ sh','껏'],
    u'\uAED0' : ['k> ^ s','껐'],
    u'\uAED1' : ['k> ^ N','껑'],
    u'\uAED2' : ['k> ^ tS','껒'],
    u'\uAED3' : ['k> ^ tSh','껓'],
    u'\uAED4' : ['k> ^ kh','껔'],
    u'\uAED5' : ['k> ^ th','껕'],
    u'\uAED6' : ['k> ^ ph','껖'],
    u'\uAED7' : ['k> ^ h','껗'],
    u'\uAED8' : ['k> e','께'],
    u'\uAED9' : ['k> e k','껙'],
    u'\uAEDA' : ['k> e k>','껚'],
    u'\uAEDB' : ['k> e k sh','껛'],
    u'\uAEDC' : ['k> e n','껜'],
    u'\uAEDD' : ['k> e n tS','껝'],
    u'\uAEDE' : ['k> e n h','껞'],
    u'\uAEDF' : ['k> e t','껟'],
    u'\uAEE0' : ['k> e l','껠'],
    u'\uAEE1' : ['k> e l k','껡'],
    u'\uAEE2' : ['k> e l m','껢'],
    u'\uAEE3' : ['k> e l p','껣'],
    u'\uAEE4' : ['k> e l sh','껤'],
    u'\uAEE5' : ['k> e l th','껥'],
    u'\uAEE6' : ['k> e l ph','껦'],
    u'\uAEE7' : ['k> e l h','껧'],
    u'\uAEE8' : ['k> e m','껨'],
    u'\uAEE9' : ['k> e p','껩'],
    u'\uAEEA' : ['k> e p sh','껪'],
    u'\uAEEB' : ['k> e sh','껫'],
    u'\uAEEC' : ['k> e s','껬'],
    u'\uAEED' : ['k> e N','껭'],
    u'\uAEEE' : ['k> e tS','껮'],
    u'\uAEEF' : ['k> e tSh','껯'],
    u'\uAEF0' : ['k> e kh','껰'],
    u'\uAEF1' : ['k> e th','껱'],
    u'\uAEF2' : ['k> e ph','껲'],
    u'\uAEF3' : ['k> e h','껳'],
    u'\uAEF4' : ['k> j ^','껴'],
    u'\uAEF5' : ['k> j ^ k','껵'],
    u'\uAEF6' : ['k> j ^ k>','껶'],
    u'\uAEF7' : ['k> j ^ k sh','껷'],
    u'\uAEF8' : ['k> j ^ n','껸'],
    u'\uAEF9' : ['k> j ^ n tS','껹'],
    u'\uAEFA' : ['k> j ^ n h','껺'],
    u'\uAEFB' : ['k> j ^ t','껻'],
    u'\uAEFC' : ['k> j ^ l','껼'],
    u'\uAEFD' : ['k> j ^ l k','껽'],
    u'\uAEFE' : ['k> j ^ l m','껾'],
    u'\uAEFF' : ['k> j ^ l p','껿'],
    u'\uAF00' : ['k> j ^ l sh','꼀'],
    u'\uAF01' : ['k> j ^ l th','꼁'],
    u'\uAF02' : ['k> j ^ l ph','꼂'],
    u'\uAF03' : ['k> j ^ l h','꼃'],
    u'\uAF04' : ['k> j ^ m','꼄'],
    u'\uAF05' : ['k> j ^ p','꼅'],
    u'\uAF06' : ['k> j ^ p sh','꼆'],
    u'\uAF07' : ['k> j ^ sh','꼇'],
    u'\uAF08' : ['k> j ^ s','꼈'],
    u'\uAF09' : ['k> j ^ N','꼉'],
    u'\uAF0A' : ['k> j ^ tS','꼊'],
    u'\uAF0B' : ['k> j ^ tSh','꼋'],
    u'\uAF0C' : ['k> j ^ kh','꼌'],
    u'\uAF0D' : ['k> j ^ th','꼍'],
    u'\uAF0E' : ['k> j ^ ph','꼎'],
    u'\uAF0F' : ['k> j ^ h','꼏'],
    u'\uAF10' : ['k> j e','꼐'],
    u'\uAF11' : ['k> j e k','꼑'],
    u'\uAF12' : ['k> j e k>','꼒'],
    u'\uAF13' : ['k> j e k sh','꼓'],
    u'\uAF14' : ['k> j e n','꼔'],
    u'\uAF15' : ['k> j e n tS','꼕'],
    u'\uAF16' : ['k> j e n h','꼖'],
    u'\uAF17' : ['k> j e t','꼗'],
    u'\uAF18' : ['k> j e l','꼘'],
    u'\uAF19' : ['k> j e l k','꼙'],
    u'\uAF1A' : ['k> j e l m','꼚'],
    u'\uAF1B' : ['k> j e l p','꼛'],
    u'\uAF1C' : ['k> j e l sh','꼜'],
    u'\uAF1D' : ['k> j e l th','꼝'],
    u'\uAF1E' : ['k> j e l ph','꼞'],
    u'\uAF1F' : ['k> j e l h','꼟'],
    u'\uAF20' : ['k> j e m','꼠'],
    u'\uAF21' : ['k> j e p','꼡'],
    u'\uAF22' : ['k> j e p sh','꼢'],
    u'\uAF23' : ['k> j e sh','꼣'],
    u'\uAF24' : ['k> j e s','꼤'],
    u'\uAF25' : ['k> j e N','꼥'],
    u'\uAF26' : ['k> j e tS','꼦'],
    u'\uAF27' : ['k> j e tSh','꼧'],
    u'\uAF28' : ['k> j e kh','꼨'],
    u'\uAF29' : ['k> j e th','꼩'],
    u'\uAF2A' : ['k> j e ph','꼪'],
    u'\uAF2B' : ['k> j e h','꼫'],
    u'\uAF2C' : ['k> o','꼬'],
    u'\uAF2D' : ['k> o k','꼭'],
    u'\uAF2E' : ['k> o k>','꼮'],
    u'\uAF2F' : ['k> o k sh','꼯'],
    u'\uAF30' : ['k> o n','꼰'],
    u'\uAF31' : ['k> o n tS','꼱'],
    u'\uAF32' : ['k> o n h','꼲'],
    u'\uAF33' : ['k> o t','꼳'],
    u'\uAF34' : ['k> o l','꼴'],
    u'\uAF35' : ['k> o l k','꼵'],
    u'\uAF36' : ['k> o l m','꼶'],
    u'\uAF37' : ['k> o l p','꼷'],
    u'\uAF38' : ['k> o l sh','꼸'],
    u'\uAF39' : ['k> o l th','꼹'],
    u'\uAF3A' : ['k> o l ph','꼺'],
    u'\uAF3B' : ['k> o l h','꼻'],
    u'\uAF3C' : ['k> o m','꼼'],
    u'\uAF3D' : ['k> o p','꼽'],
    u'\uAF3E' : ['k> o p sh','꼾'],
    u'\uAF3F' : ['k> o sh','꼿'],
    u'\uAF40' : ['k> o s','꽀'],
    u'\uAF41' : ['k> o N','꽁'],
    u'\uAF42' : ['k> o tS','꽂'],
    u'\uAF43' : ['k> o tSh','꽃'],
    u'\uAF44' : ['k> o kh','꽄'],
    u'\uAF45' : ['k> o th','꽅'],
    u'\uAF46' : ['k> o ph','꽆'],
    u'\uAF47' : ['k> o h','꽇'],
    u'\uAF48' : ['k> w a','꽈'],
    u'\uAF49' : ['k> w a k','꽉'],
    u'\uAF4A' : ['k> w a k>','꽊'],
    u'\uAF4B' : ['k> w a k sh','꽋'],
    u'\uAF4C' : ['k> w a n','꽌'],
    u'\uAF4D' : ['k> w a n tS','꽍'],
    u'\uAF4E' : ['k> w a n h','꽎'],
    u'\uAF4F' : ['k> w a t','꽏'],
    u'\uAF50' : ['k> w a l','꽐'],
    u'\uAF51' : ['k> w a l k','꽑'],
    u'\uAF52' : ['k> w a l m','꽒'],
    u'\uAF53' : ['k> w a l p','꽓'],
    u'\uAF54' : ['k> w a l sh','꽔'],
    u'\uAF55' : ['k> w a l th','꽕'],
    u'\uAF56' : ['k> w a l ph','꽖'],
    u'\uAF57' : ['k> w a l h','꽗'],
    u'\uAF58' : ['k> w a m','꽘'],
    u'\uAF59' : ['k> w a p','꽙'],
    u'\uAF5A' : ['k> w a p sh','꽚'],
    u'\uAF5B' : ['k> w a sh','꽛'],
    u'\uAF5C' : ['k> w a s','꽜'],
    u'\uAF5D' : ['k> w a N','꽝'],
    u'\uAF5E' : ['k> w a tS','꽞'],
    u'\uAF5F' : ['k> w a tSh','꽟'],
    u'\uAF60' : ['k> w a kh','꽠'],
    u'\uAF61' : ['k> w a th','꽡'],
    u'\uAF62' : ['k> w a ph','꽢'],
    u'\uAF63' : ['k> w a h','꽣'],
    u'\uAF64' : ['k> w @','꽤'],
    u'\uAF65' : ['k> w @ k','꽥'],
    u'\uAF66' : ['k> w @ k>','꽦'],
    u'\uAF67' : ['k> w @ k sh','꽧'],
    u'\uAF68' : ['k> w @ n','꽨'],
    u'\uAF69' : ['k> w @ n tS','꽩'],
    u'\uAF6A' : ['k> w @ n h','꽪'],
    u'\uAF6B' : ['k> w @ t','꽫'],
    u'\uAF6C' : ['k> w @ l','꽬'],
    u'\uAF6D' : ['k> w @ l k','꽭'],
    u'\uAF6E' : ['k> w @ l m','꽮'],
    u'\uAF6F' : ['k> w @ l p','꽯'],
    u'\uAF70' : ['k> w @ l sh','꽰'],
    u'\uAF71' : ['k> w @ l th','꽱'],
    u'\uAF72' : ['k> w @ l ph','꽲'],
    u'\uAF73' : ['k> w @ l h','꽳'],
    u'\uAF74' : ['k> w @ m','꽴'],
    u'\uAF75' : ['k> w @ p','꽵'],
    u'\uAF76' : ['k> w @ p sh','꽶'],
    u'\uAF77' : ['k> w @ sh','꽷'],
    u'\uAF78' : ['k> w @ s','꽸'],
    u'\uAF79' : ['k> w @ N','꽹'],
    u'\uAF7A' : ['k> w @ tS','꽺'],
    u'\uAF7B' : ['k> w @ tSh','꽻'],
    u'\uAF7C' : ['k> w @ kh','꽼'],
    u'\uAF7D' : ['k> w @ th','꽽'],
    u'\uAF7E' : ['k> w @ ph','꽾'],
    u'\uAF7F' : ['k> w @ h','꽿'],
    u'\uAF80' : ['k> w e','꾀'],
    u'\uAF81' : ['k> w e k','꾁'],
    u'\uAF82' : ['k> w e k>','꾂'],
    u'\uAF83' : ['k> w e k sh','꾃'],
    u'\uAF84' : ['k> w e n','꾄'],
    u'\uAF85' : ['k> w e n tS','꾅'],
    u'\uAF86' : ['k> w e n h','꾆'],
    u'\uAF87' : ['k> w e t','꾇'],
    u'\uAF88' : ['k> w e l','꾈'],
    u'\uAF89' : ['k> w e l k','꾉'],
    u'\uAF8A' : ['k> w e l m','꾊'],
    u'\uAF8B' : ['k> w e l p','꾋'],
    u'\uAF8C' : ['k> w e l sh','꾌'],
    u'\uAF8D' : ['k> w e l th','꾍'],
    u'\uAF8E' : ['k> w e l ph','꾎'],
    u'\uAF8F' : ['k> w e l h','꾏'],
    u'\uAF90' : ['k> w e m','꾐'],
    u'\uAF91' : ['k> w e p','꾑'],
    u'\uAF92' : ['k> w e p sh','꾒'],
    u'\uAF93' : ['k> w e sh','꾓'],
    u'\uAF94' : ['k> w e s','꾔'],
    u'\uAF95' : ['k> w e N','꾕'],
    u'\uAF96' : ['k> w e tS','꾖'],
    u'\uAF97' : ['k> w e tSh','꾗'],
    u'\uAF98' : ['k> w e kh','꾘'],
    u'\uAF99' : ['k> w e th','꾙'],
    u'\uAF9A' : ['k> w e ph','꾚'],
    u'\uAF9B' : ['k> w e h','꾛'],
    u'\uAF9C' : ['k> j o','꾜'],
    u'\uAF9D' : ['k> j o k','꾝'],
    u'\uAF9E' : ['k> j o k>','꾞'],
    u'\uAF9F' : ['k> j o k sh','꾟'],
    u'\uAFA0' : ['k> j o n','꾠'],
    u'\uAFA1' : ['k> j o n tS','꾡'],
    u'\uAFA2' : ['k> j o n h','꾢'],
    u'\uAFA3' : ['k> j o t','꾣'],
    u'\uAFA4' : ['k> j o l','꾤'],
    u'\uAFA5' : ['k> j o l k','꾥'],
    u'\uAFA6' : ['k> j o l m','꾦'],
    u'\uAFA7' : ['k> j o l p','꾧'],
    u'\uAFA8' : ['k> j o l sh','꾨'],
    u'\uAFA9' : ['k> j o l th','꾩'],
    u'\uAFAA' : ['k> j o l ph','꾪'],
    u'\uAFAB' : ['k> j o l h','꾫'],
    u'\uAFAC' : ['k> j o m','꾬'],
    u'\uAFAD' : ['k> j o p','꾭'],
    u'\uAFAE' : ['k> j o p sh','꾮'],
    u'\uAFAF' : ['k> j o sh','꾯'],
    u'\uAFB0' : ['k> j o s','꾰'],
    u'\uAFB1' : ['k> j o N','꾱'],
    u'\uAFB2' : ['k> j o tS','꾲'],
    u'\uAFB3' : ['k> j o tSh','꾳'],
    u'\uAFB4' : ['k> j o kh','꾴'],
    u'\uAFB5' : ['k> j o th','꾵'],
    u'\uAFB6' : ['k> j o ph','꾶'],
    u'\uAFB7' : ['k> j o h','꾷'],
    u'\uAFB8' : ['k> u','꾸'],
    u'\uAFB9' : ['k> u k','꾹'],
    u'\uAFBA' : ['k> u k>','꾺'],
    u'\uAFBB' : ['k> u k sh','꾻'],
    u'\uAFBC' : ['k> u n','꾼'],
    u'\uAFBD' : ['k> u n tS','꾽'],
    u'\uAFBE' : ['k> u n h','꾾'],
    u'\uAFBF' : ['k> u t','꾿'],
    u'\uAFC0' : ['k> u l','꿀'],
    u'\uAFC1' : ['k> u l k','꿁'],
    u'\uAFC2' : ['k> u l m','꿂'],
    u'\uAFC3' : ['k> u l p','꿃'],
    u'\uAFC4' : ['k> u l sh','꿄'],
    u'\uAFC5' : ['k> u l th','꿅'],
    u'\uAFC6' : ['k> u l ph','꿆'],
    u'\uAFC7' : ['k> u l h','꿇'],
    u'\uAFC8' : ['k> u m','꿈'],
    u'\uAFC9' : ['k> u p','꿉'],
    u'\uAFCA' : ['k> u p sh','꿊'],
    u'\uAFCB' : ['k> u sh','꿋'],
    u'\uAFCC' : ['k> u s','꿌'],
    u'\uAFCD' : ['k> u d','꿍'],
    u'\uAFCE' : ['k> u tS','꿎'],
    u'\uAFCF' : ['k> u tSh','꿏'],
    u'\uAFD0' : ['k> u kh','꿐'],
    u'\uAFD1' : ['k> u th','꿑'],
    u'\uAFD2' : ['k> u ph','꿒'],
    u'\uAFD3' : ['k> u h','꿓'],
    u'\uAFD4' : ['k> w ^','꿔'],
    u'\uAFD5' : ['k> w ^ k','꿕'],
    u'\uAFD6' : ['k> w ^ k>','꿖'],
    u'\uAFD7' : ['k> w ^ k sh','꿗'],
    u'\uAFD8' : ['k> w ^ n','꿘'],
    u'\uAFD9' : ['k> w ^ n tS','꿙'],
    u'\uAFDA' : ['k> w ^ n h','꿚'],
    u'\uAFDB' : ['k> w ^ t','꿛'],
    u'\uAFDC' : ['k> w ^ l','꿜'],
    u'\uAFDD' : ['k> w ^ l k','꿝'],
    u'\uAFDE' : ['k> w ^ l m','꿞'],
    u'\uAFDF' : ['k> w ^ l p','꿟'],
    u'\uAFE0' : ['k> w ^ l sh','꿠'],
    u'\uAFE1' : ['k> w ^ l th','꿡'],
    u'\uAFE2' : ['k> w ^ l ph','꿢'],
    u'\uAFE3' : ['k> w ^ l h','꿣'],
    u'\uAFE4' : ['k> w ^ m','꿤'],
    u'\uAFE5' : ['k> w ^ p','꿥'],
    u'\uAFE6' : ['k> w ^ p sh','꿦'],
    u'\uAFE7' : ['k> w ^ sh','꿧'],
    u'\uAFE8' : ['k> w ^ s','꿨'],
    u'\uAFE9' : ['k> w ^ N','꿩'],
    u'\uAFEA' : ['k> w ^ tS','꿪'],
    u'\uAFEB' : ['k> w ^ tSh','꿫'],
    u'\uAFEC' : ['k> w ^ kh','꿬'],
    u'\uAFED' : ['k> w ^ th','꿭'],
    u'\uAFEE' : ['k> w ^ ph','꿮'],
    u'\uAFEF' : ['k> w ^ h','꿯'],
    u'\uAFF0' : ['k> w E','꿰'],
    u'\uAFF1' : ['k> w E k','꿱'],
    u'\uAFF2' : ['k> w E k>','꿲'],
    u'\uAFF3' : ['k> w E k sh','꿳'],
    u'\uAFF4' : ['k> w E n','꿴'],
    u'\uAFF5' : ['k> w E n tS','꿵'],
    u'\uAFF6' : ['k> w E n h','꿶'],
    u'\uAFF7' : ['k> w E t','꿷'],
    u'\uAFF8' : ['k> w E l','꿸'],
    u'\uAFF9' : ['k> w E l k','꿹'],
    u'\uAFFA' : ['k> w E l m','꿺'],
    u'\uAFFB' : ['k> w E l p','꿻'],
    u'\uAFFC' : ['k> w E l sh','꿼'],
    u'\uAFFD' : ['k> w E l th','꿽'],
    u'\uAFFE' : ['k> w E l ph','꿾'],
    u'\uAFFF' : ['k> w E l h','꿿'],
    u'\uB000' : ['k> w E m','뀀'],
    u'\uB001' : ['k> w E p','뀁'],
    u'\uB002' : ['k> w E p sh','뀂'],
    u'\uB003' : ['k> w E sh','뀃'],
    u'\uB004' : ['k> w E s','뀄'],
    u'\uB005' : ['k> w E N','뀅'],
    u'\uB006' : ['k> w E tS','뀆'],
    u'\uB007' : ['k> w E tSh','뀇'],
    u'\uB008' : ['k> w E kh','뀈'],
    u'\uB009' : ['k> w E th','뀉'],
    u'\uB00A' : ['k> w E ph','뀊'],
    u'\uB00B' : ['k> w E h','뀋'],
    u'\uB00C' : ['k> 7','뀌'],
    u'\uB00D' : ['k> 7 k','뀍'],
    u'\uB00E' : ['k> 7 k>','뀎'],
    u'\uB00F' : ['k> 7 k sh','뀏'],
    u'\uB010' : ['k> 7 n','뀐'],
    u'\uB011' : ['k> 7 n tS','뀑'],
    u'\uB012' : ['k> 7 n h','뀒'],
    u'\uB013' : ['k> 7 t','뀓'],
    u'\uB014' : ['k> 7 l','뀔'],
    u'\uB015' : ['k> 7 l k','뀕'],
    u'\uB016' : ['k> 7 l m','뀖'],
    u'\uB017' : ['k> 7 l p','뀗'],
    u'\uB018' : ['k> 7 l sh','뀘'],
    u'\uB019' : ['k> 7 l th','뀙'],
    u'\uB01A' : ['k> 7 l ph','뀚'],
    u'\uB01B' : ['k> 7 l h','뀛'],
    u'\uB01C' : ['k> 7 m','뀜'],
    u'\uB01D' : ['k> 7 p','뀝'],
    u'\uB01E' : ['k> 7 p sh','뀞'],
    u'\uB01F' : ['k> 7 sh','뀟'],
    u'\uB020' : ['k> 7 s','뀠'],
    u'\uB021' : ['k> 7 N','뀡'],
    u'\uB022' : ['k> 7 tS','뀢'],
    u'\uB023' : ['k> 7 tSh','뀣'],
    u'\uB024' : ['k> 7 kh','뀤'],
    u'\uB025' : ['k> 7 th','뀥'],
    u'\uB026' : ['k> 7 ph','뀦'],
    u'\uB027' : ['k> 7 h','뀧'],
    u'\uB028' : ['k> j u','뀨'],
    u'\uB029' : ['k> j u k','뀩'],
    u'\uB02A' : ['k> j u k>','뀪'],
    u'\uB02B' : ['k> j u k sh','뀫'],
    u'\uB02C' : ['k> j u n','뀬'],
    u'\uB02D' : ['k> j u n tS','뀭'],
    u'\uB02E' : ['k> j u n h','뀮'],
    u'\uB02F' : ['k> j u t','뀯'],
    u'\uB030' : ['k> j u l','뀰'],
    u'\uB031' : ['k> j u l k','뀱'],
    u'\uB032' : ['k> j u l m','뀲'],
    u'\uB033' : ['k> j u l p','뀳'],
    u'\uB034' : ['k> j u l sh','뀴'],
    u'\uB035' : ['k> j u l th','뀵'],
    u'\uB036' : ['k> j u l ph','뀶'],
    u'\uB037' : ['k> j u l h','뀷'],
    u'\uB038' : ['k> j u m','뀸'],
    u'\uB039' : ['k> j u p','뀹'],
    u'\uB03A' : ['k> j u p sh','뀺'],
    u'\uB03B' : ['k> j u sh','뀻'],
    u'\uB03C' : ['k> j u s','뀼'],
    u'\uB03D' : ['k> j u N','뀽'],
    u'\uB03E' : ['k> j u tS','뀾'],
    u'\uB03F' : ['k> j u tSh','뀿'],
    u'\uB040' : ['k> j u kh','끀'],
    u'\uB041' : ['k> j u th','끁'],
    u'\uB042' : ['k> j u ph','끂'],
    u'\uB043' : ['k> j u h','끃'],
    u'\uB044' : ['k> 4','끄'],
    u'\uB045' : ['k> 4 k','끅'],
    u'\uB046' : ['k> 4 k>','끆'],
    u'\uB047' : ['k> 4 k sh','끇'],
    u'\uB048' : ['k> 4 n','끈'],
    u'\uB049' : ['k> 4 n tS','끉'],
    u'\uB04A' : ['k> 4 n h','끊'],
    u'\uB04B' : ['k> 4 t','끋'],
    u'\uB04C' : ['k> 4 l','끌'],
    u'\uB04D' : ['k> 4 l k','끍'],
    u'\uB04E' : ['k> 4 l m','끎'],
    u'\uB04F' : ['k> 4 l p','끏'],
    u'\uB050' : ['k> 4 l sh','끐'],
    u'\uB051' : ['k> 4 l th','끑'],
    u'\uB052' : ['k> 4 l ph','끒'],
    u'\uB053' : ['k> 4 l h','끓'],
    u'\uB054' : ['k> 4 m','끔'],
    u'\uB055' : ['k> 4 p','끕'],
    u'\uB056' : ['k> 4 p sh','끖'],
    u'\uB057' : ['k> 4 sh','끗'],
    u'\uB058' : ['k> 4 s','끘'],
    u'\uB059' : ['k> 4 N','끙'],
    u'\uB05A' : ['k> 4 tS','끚'],
    u'\uB05B' : ['k> 4 tSh','끛'],
    u'\uB05C' : ['k> 4 kh','끜'],
    u'\uB05D' : ['k> 4 th','끝'],
    u'\uB05E' : ['k> 4 ph','끞'],
    u'\uB05F' : ['k> 4 h','끟'],
    u'\uB060' : ['k> 4 j','끠'],
    u'\uB061' : ['k> 4 j k','끡'],
    u'\uB062' : ['k> 4 j k>','끢'],
    u'\uB063' : ['k> 4 j k sh','끣'],
    u'\uB064' : ['k> 4 j n','끤'],
    u'\uB065' : ['k> 4 j n tS','끥'],
    u'\uB066' : ['k> 4 j n h','끦'],
    u'\uB067' : ['k> 4 j t','끧'],
    u'\uB068' : ['k> 4 j l','끨'],
    u'\uB069' : ['k> 4 j l k','끩'],
    u'\uB06A' : ['k> 4 j l m','끪'],
    u'\uB06B' : ['k> 4 j l p','끫'],
    u'\uB06C' : ['k> 4 j l sh','끬'],
    u'\uB06D' : ['k> 4 j l th','끭'],
    u'\uB06E' : ['k> 4 j l ph','끮'],
    u'\uB06F' : ['k> 4 j l h','끯'],
    u'\uB070' : ['k> 4 j m','끰'],
    u'\uB071' : ['k> 4 j p','끱'],
    u'\uB072' : ['k> 4 j p sh','끲'],
    u'\uB073' : ['k> 4 j sh','끳'],
    u'\uB074' : ['k> 4 j s','끴'],
    u'\uB075' : ['k> 4 j N','끵'],
    u'\uB076' : ['k> 4 j tS','끶'],
    u'\uB077' : ['k> 4 j tSh','끷'],
    u'\uB078' : ['k> 4 j kh','끸'],
    u'\uB079' : ['k> 4 j th','끹'],
    u'\uB07A' : ['k> 4 j ph','끺'],
    u'\uB07B' : ['k> 4 j h','끻'],
    u'\uB07C' : ['k> i','끼'],
    u'\uB07D' : ['k> i k','끽'],
    u'\uB07E' : ['k> i k>','끾'],
    u'\uB07F' : ['k> i k sh','끿'],
    u'\uB080' : ['k> i n','낀'],
    u'\uB081' : ['k> i n tS','낁'],
    u'\uB082' : ['k> i n h','낂'],
    u'\uB083' : ['k> i t','낃'],
    u'\uB084' : ['k> i l','낄'],
    u'\uB085' : ['k> i l k','낅'],
    u'\uB086' : ['k> i l m','낆'],
    u'\uB087' : ['k> i l p','낇'],
    u'\uB088' : ['k> i l sh','낈'],
    u'\uB089' : ['k> i l th','낉'],
    u'\uB08A' : ['k> i l ph','낊'],
    u'\uB08B' : ['k> i l h','낋'],
    u'\uB08C' : ['k> i m','낌'],
    u'\uB08D' : ['k> i p','낍'],
    u'\uB08E' : ['k> i p sh','낎'],
    u'\uB08F ' : ['k> i sh','낏 '],
    u'\uB090' : ['k> i s','낐'],
    u'\uB091' : ['k> i N','낑'],
    u'\uB092' : ['k> i tS','낒'],
    u'\uB093' : ['k> i tSh','낓'],
    u'\uB094' : ['k> i kh','낔'],
    u'\uB095' : ['k> i th','낕'],
    u'\uB096' : ['k> i ph','낖'],
    u'\uB097' : ['k> i h','낗'],
    u'\uB098' : ['n a','나'],
    u'\uB099' : ['n a k','낙'],
    u'\uB09A' : ['n a k>','낚'],
    u'\uB09B' : ['n a k sh','낛'],
    u'\uB09C' : ['n a n','난'],
    u'\uB09D' : ['n a n tS','낝'],
    u'\uB09E' : ['n a n h','낞'],
    u'\uB09F' : ['n a t','낟'],
    u'\uB0A0' : ['n a l','날'],
    u'\uB0A1' : ['n a l k','낡'],
    u'\uB0A2' : ['n a l m','낢'],
    u'\uB0A3' : ['n a l p','낣'],
    u'\uB0A4' : ['n a l sh','낤'],
    u'\uB0A5' : ['n a l th','낥'],
    u'\uB0A6' : ['n a l ph','낦'],
    u'\uB0A7' : ['n a l h','낧'],
    u'\uB0A8' : ['n a m','남'],
    u'\uB0A9' : ['n a p','납'],
    u'\uB0AA' : ['n a p sh','낪'],
    u'\uB0AB' : ['n a sh','낫'],
    u'\uB0AC' : ['n a s','났'],
    u'\uB0AD' : ['n a N','낭'],
    u'\uB0AE' : ['n a tS','낮'],
    u'\uB0AF' : ['n a tSh','낯'],
    u'\uB0B0' : ['n a kh','낰'],
    u'\uB0B1' : ['n a th','낱'],
    u'\uB0B2' : ['n a ph','낲'],
    u'\uB0B3' : ['n a h','낳'],
    u'\uB0B4' : ['n @','내'],
    u'\uB0B5' : ['n @ k','낵'],
    u'\uB0B6' : ['n @ k>','낶'],
    u'\uB0B7' : ['n @ k sh','낷'],
    u'\uB0B8' : ['n @ n','낸'],
    u'\uB0B9' : ['n @ n tS','낹'],
    u'\uB0BA' : ['n @ n h','낺'],
    u'\uB0BB' : ['n @ t','낻'],
    u'\uB0BC' : ['n @ l','낼'],
    u'\uB0BD' : ['n @ l k','낽'],
    u'\uB0BE' : ['n @ l m','낾'],
    u'\uB0BF' : ['n @ l p','낿'],
    u'\uB0C0' : ['n @ l sh','냀'],
    u'\uB0C1' : ['n @ l th','냁'],
    u'\uB0C2' : ['n @ l ph','냂'],
    u'\uB0C3' : ['n @ l h','냃'],
    u'\uB0C4' : ['n @ m','냄'],
    u'\uB0C5' : ['n @ p','냅'],
    u'\uB0C6' : ['n @ p sh','냆'],
    u'\uB0C7' : ['n @ sh','냇'],
    u'\uB0C8' : ['n @ s','냈'],
    u'\uB0C9' : ['n @ N','냉'],
    u'\uB0CA' : ['n @ tS','냊'],
    u'\uB0CB' : ['n @ tSh','냋'],
    u'\uB0CC' : ['n @ kh','냌'],
    u'\uB0CD' : ['n @ th','냍'],
    u'\uB0CE' : ['n @ ph','냎'],
    u'\uB0CF' : ['n @ h','냏'],
    u'\uB0D0' : ['n j a','냐'],
    u'\uB0D1' : ['n j a k','냑'],
    u'\uB0D2' : ['n j a k>','냒'],
    u'\uB0D3' : ['n j a k sh','냓'],
    u'\uB0D4' : ['n j a n','냔'],
    u'\uB0D5' : ['n j a n tS','냕'],
    u'\uB0D6' : ['n j a n h','냖'],
    u'\uB0D7' : ['n j a t','냗'],
    u'\uB0D8' : ['n j a l','냘'],
    u'\uB0D9' : ['n j a l k','냙'],
    u'\uB0DA' : ['n j a l m','냚'],
    u'\uB0DB' : ['n j a l p','냛'],
    u'\uB0DC' : ['n j a l sh','냜'],
    u'\uB0DD' : ['n j a l th','냝'],
    u'\uB0DE' : ['n j a l ph','냞'],
    u'\uB0DF' : ['n j a l h','냟'],
    u'\uB0E0' : ['n j a m','냠'],
    u'\uB0E1' : ['n j a p','냡'],
    u'\uB0E2' : ['n j a p sh','냢'],
    u'\uB0E3' : ['n j a sh','냣'],
    u'\uB0E4' : ['n j a s','냤'],
    u'\uB0E5' : ['n j a N','냥'],
    u'\uB0E6' : ['n j a tS','냦'],
    u'\uB0E7' : ['n j a tSh','냧'],
    u'\uB0E8' : ['n j a kh','냨'],
    u'\uB0E9' : ['n j a th','냩'],
    u'\uB0EA' : ['n j a ph','냪'],
    u'\uB0EB' : ['n j a h','냫'],
    u'\uB0EC' : ['n j @','냬'],
    u'\uB0ED' : ['n j @ k','냭'],
    u'\uB0EE' : ['n j @ k>','냮'],
    u'\uB0EF' : ['n j @ k sh','냯'],
    u'\uB0F0' : ['n j @ n','냰'],
    u'\uB0F1' : ['n j @ n tS','냱'],
    u'\uB0F2' : ['n j @ n h','냲'],
    u'\uB0F3' : ['n j @ t','냳'],
    u'\uB0F4' : ['n j @ l','냴'],
    u'\uB0F5' : ['n j @ l k','냵'],
    u'\uB0F6' : ['n j @ l m','냶'],
    u'\uB0F7' : ['n j @ l p','냷'],
    u'\uB0F8' : ['n j @ l sh','냸'],
    u'\uB0F9' : ['n j @ l th','냹'],
    u'\uB0FA' : ['n j @ l ph','냺'],
    u'\uB0FB' : ['n j @ l h','냻'],
    u'\uB0FC' : ['n j @ m','냼'],
    u'\uB0FD' : ['n j @ p','냽'],
    u'\uB0FE' : ['n j @ p sh','냾'],
    u'\uB0FF' : ['n j @ sh','냿'],
    u'\uB100' : ['n j @ s','넀'],
    u'\uB101' : ['n j @ N','넁'],
    u'\uB102' : ['n j @ tS','넂'],
    u'\uB103' : ['n j @ tSh','넃'],
    u'\uB104' : ['n j @ kh','넄'],
    u'\uB105' : ['n j @ th','넅'],
    u'\uB106' : ['n j @ ph','넆'],
    u'\uB107' : ['n j @ h','넇'],
    u'\uB108' : ['n ^','너'],
    u'\uB109' : ['n ^ k','넉'],
    u'\uB10A' : ['n ^ k>','넊'],
    u'\uB10B' : ['n ^ k sh','넋'],
    u'\uB10C' : ['n ^ n','넌'],
    u'\uB10D' : ['n ^ n tS','넍'],
    u'\uB10E' : ['n ^ n h','넎'],
    u'\uB10F' : ['n ^ t','넏'],
    u'\uB110' : ['n ^ l','널'],
    u'\uB111' : ['n ^ l k','넑'],
    u'\uB112' : ['n ^ l m','넒'],
    u'\uB113' : ['n ^ l p','넓'],
    u'\uB114' : ['n ^ l sh','넔'],
    u'\uB115' : ['n ^ l th','넕'],
    u'\uB116' : ['n ^ l ph','넖'],
    u'\uB117' : ['n ^ l h','넗'],
    u'\uB118' : ['n ^ m','넘'],
    u'\uB119' : ['n ^ p','넙'],
    u'\uB11A' : ['n ^ p sh','넚'],
    u'\uB11B' : ['n ^ sh','넛'],
    u'\uB11C' : ['n ^ s','넜'],
    u'\uB11D' : ['n ^ N','넝'],
    u'\uB11E' : ['n ^ tS','넞'],
    u'\uB11F' : ['n ^ tSh','넟'],
    u'\uB120' : ['n ^ kh','넠'],
    u'\uB121' : ['n ^ th','넡'],
    u'\uB122' : ['n ^ ph','넢'],
    u'\uB123' : ['n ^ h','넣'],
    u'\uB124' : ['n e','네'],
    u'\uB125' : ['n e k','넥'],
    u'\uB126' : ['n e k>','넦'],
    u'\uB127' : ['n e k sh','넧'],
    u'\uB128' : ['n e n','넨'],
    u'\uB129' : ['n e n tS','넩'],
    u'\uB12A' : ['n e n h','넪'],
    u'\uB12B' : ['n e t','넫'],
    u'\uB12C' : ['n e l','넬'],
    u'\uB12D' : ['n e l k','넭'],
    u'\uB12E' : ['n e l m','넮'],
    u'\uB12F' : ['n e l p','넯'],
    u'\uB130' : ['n e l sh','넰'],
    u'\uB131' : ['n e l th','넱'],
    u'\uB132' : ['n e l ph','넲'],
    u'\uB133' : ['n e l h','넳'],
    u'\uB134' : ['n e m','넴'],
    u'\uB135' : ['n e p','넵'],
    u'\uB136' : ['n e p sh','넶'],
    u'\uB137' : ['n e sh','넷'],
    u'\uB138' : ['n e s','넸'],
    u'\uB139' : ['n e N','넹'],
    u'\uB13A' : ['n e tS','넺'],
    u'\uB13B' : ['n e tSh','넻'],
    u'\uB13C' : ['n e kh','넼'],
    u'\uB13D' : ['n e th','넽'],
    u'\uB13E' : ['n e ph','넾'],
    u'\uB13F' : ['n e h','넿'],
    u'\uB140' : ['n j ^','녀'],
    u'\uB141' : ['n j ^ k','녁'],
    u'\uB142' : ['n j ^ k>','녂'],
    u'\uB143' : ['n j ^ k sh','녃'],
    u'\uB144' : ['n j ^ n','년'],
    u'\uB145' : ['n j ^ n tS','녅'],
    u'\uB146' : ['n j ^ n h','녆'],
    u'\uB147' : ['n j ^ t','녇'],
    u'\uB148' : ['n j ^ l','녈'],
    u'\uB149' : ['n j ^ l k','녉'],
    u'\uB14A' : ['n j ^ l m','녊'],
    u'\uB14B' : ['n j ^ l p','녋'],
    u'\uB14C' : ['n j ^ l sh','녌'],
    u'\uB14D' : ['n j ^ l th','녍'],
    u'\uB14E' : ['n j ^ l ph','녎'],
    u'\uB14F' : ['n j ^ l h','녏'],
    u'\uB150' : ['n j ^ m','념'],
    u'\uB151' : ['n j ^ p','녑'],
    u'\uB152' : ['n j ^ p sh','녒'],
    u'\uB153' : ['n j ^ sh','녓'],
    u'\uB154' : ['n j ^ s','녔'],
    u'\uB155' : ['n j ^ N','녕'],
    u'\uB156' : ['n j ^ tS','녖'],
    u'\uB157' : ['n j ^ tSh','녗'],
    u'\uB158' : ['n j ^ kh','녘'],
    u'\uB159' : ['n j ^ th','녙'],
    u'\uB15A' : ['n j ^ ph','녚'],
    u'\uB15B' : ['n j ^ h','녛'],
    u'\uB15C' : ['n j e','녜'],
    u'\uB15D' : ['n j e k','녝'],
    u'\uB15E' : ['n j e k>','녞'],
    u'\uB15F' : ['n j e k sh','녟'],
    u'\uB160' : ['n j e n','녠'],
    u'\uB161' : ['n j e n tS','녡'],
    u'\uB162' : ['n j e n h','녢'],
    u'\uB163' : ['n j e t','녣'],
    u'\uB164' : ['n j e l','녤'],
    u'\uB165' : ['n j e l k','녥'],
    u'\uB166' : ['n j e l m','녦'],
    u'\uB167' : ['n j e l p','녧'],
    u'\uB168' : ['n j e l sh','녨'],
    u'\uB169' : ['n j e l th','녩'],
    u'\uB16A' : ['n j e l ph','녪'],
    u'\uB16B' : ['n j e l h','녫'],
    u'\uB16C' : ['n j e m','녬'],
    u'\uB16D' : ['n j e p','녭'],
    u'\uB16E' : ['n j e p sh','녮'],
    u'\uB16F' : ['n j e sh','녯'],
    u'\uB170' : ['n j e s','녰'],
    u'\uB171' : ['n j e N','녱'],
    u'\uB172' : ['n j e tS','녲'],
    u'\uB173' : ['n j e tSh','녳'],
    u'\uB174' : ['n j e kh','녴'],
    u'\uB175' : ['n j e th','녵'],
    u'\uB176' : ['n j e ph','녶'],
    u'\uB177' : ['n j e h','녷'],
    u'\uB178' : ['n o','노'],
    u'\uB179' : ['n o k','녹'],
    u'\uB17A' : ['n o k>','녺'],
    u'\uB17B' : ['n o k sh','녻'],
    u'\uB17C' : ['n o n','논'],
    u'\uB17D' : ['n o n tS','녽'],
    u'\uB17E' : ['n o n h','녾'],
    u'\uB17F' : ['n o t','녿'],
    u'\uB180' : ['n o l','놀'],
    u'\uB181' : ['n o l k','놁'],
    u'\uB182' : ['n o l m','놂'],
    u'\uB183' : ['n o l p','놃'],
    u'\uB184' : ['n o l sh','놄'],
    u'\uB185' : ['n o l th','놅'],
    u'\uB186' : ['n o l ph','놆'],
    u'\uB187' : ['n o l h','놇'],
    u'\uB188' : ['n o m','놈'],
    u'\uB189' : ['n o p','놉'],
    u'\uB18A' : ['n o p sh','놊'],
    u'\uB18B' : ['n o sh','놋'],
    u'\uB18C' : ['n o s','놌'],
    u'\uB18D' : ['n o N','농'],
    u'\uB18E' : ['n o tS','놎'],
    u'\uB18F' : ['n o tSh','놏'],
    u'\uB190' : ['n o kh','놐'],
    u'\uB191' : ['n o th','놑'],
    u'\uB192' : ['n o ph','높'],
    u'\uB193' : ['n o h','놓'],
    u'\uB194' : ['n w a','놔'],
    u'\uB195' : ['n w a k','놕'],
    u'\uB196' : ['n w a k>','놖'],
    u'\uB197' : ['n w a k sh','놗'],
    u'\uB198' : ['n w a n','놘'],
    u'\uB199' : ['n w a n tS','놙'],
    u'\uB19A' : ['n w a n h','놚'],
    u'\uB19B' : ['n w a t','놛'],
    u'\uB19C' : ['n w a l','놜'],
    u'\uB19D' : ['n w a l k','놝'],
    u'\uB19E' : ['n w a l m','놞'],
    u'\uB19F' : ['n w a l p','놟'],
    u'\uB1A0' : ['n w a l sh','놠'],
    u'\uB1A1' : ['n w a l th','놡'],
    u'\uB1A2' : ['n w a l ph','놢'],
    u'\uB1A3' : ['n w a l h','놣'],
    u'\uB1A4' : ['n w a m','놤'],
    u'\uB1A5' : ['n w a p','놥'],
    u'\uB1A6' : ['n w a p sh','놦'],
    u'\uB1A7' : ['n w a sh','놧'],
    u'\uB1A8' : ['n w a s','놨'],
    u'\uB1A9' : ['n w a N','놩'],
    u'\uB1AA' : ['n w a tS','놪'],
    u'\uB1AB' : ['n w a tSh','놫'],
    u'\uB1AC' : ['n w a kh','놬'],
    u'\uB1AD' : ['n w a th','놭'],
    u'\uB1AE' : ['n w a ph','놮'],
    u'\uB1AF' : ['n w a h','놯'],
    u'\uB1B0' : ['n w @','놰'],
    u'\uB1B1' : ['n w @ k','놱'],
    u'\uB1B2' : ['n w @ k>','놲'],
    u'\uB1B3' : ['n w @ k sh','놳'],
    u'\uB1B4' : ['n w @ n','놴'],
    u'\uB1B5' : ['n w @ n tS','놵'],
    u'\uB1B6' : ['n w @ n h','놶'],
    u'\uB1B7' : ['n w @ t','놷'],
    u'\uB1B8' : ['n w @ l','놸'],
    u'\uB1B9' : ['n w @ l k','놹'],
    u'\uB1BA' : ['n w @ l m','놺'],
    u'\uB1BB' : ['n w @ l p','놻'],
    u'\uB1BC' : ['n w @ l sh','놼'],
    u'\uB1BD' : ['n w @ l th','놽'],
    u'\uB1BE' : ['n w @ l ph','놾'],
    u'\uB1BF' : ['n w @ l h','놿'],
    u'\uB1C0' : ['n w @ m','뇀'],
    u'\uB1C1' : ['n w @ p','뇁'],
    u'\uB1C2' : ['n w @ p sh','뇂'],
    u'\uB1C3' : ['n w @ sh','뇃'],
    u'\uB1C4' : ['n w @ s','뇄'],
    u'\uB1C5' : ['n w @ N','뇅'],
    u'\uB1C6' : ['n w @ tS','뇆'],
    u'\uB1C7' : ['n w @ tSh','뇇'],
    u'\uB1C8' : ['n w @ kh','뇈'],
    u'\uB1C9' : ['n w @ th','뇉'],
    u'\uB1CA' : ['n w @ ph','뇊'],
    u'\uB1CB' : ['n w @ h','뇋'],
    u'\uB1CC' : ['n w e','뇌'],
    u'\uB1CD' : ['n w e k','뇍'],
    u'\uB1CE' : ['n w e k>','뇎'],
    u'\uB1CF' : ['n w e k sh','뇏'],
    u'\uB1D0 ' : ['n w e n','뇐 '],
    u'\uB1D1' : ['n w e n tS','뇑'],
    u'\uB1D2' : ['n w e n h','뇒'],
    u'\uB1D3' : ['n w e t','뇓'],
    u'\uB1D4' : ['n w e l','뇔'],
    u'\uB1D5' : ['n w e l k','뇕'],
    u'\uB1D6' : ['n w e l m','뇖'],
    u'\uB1D7' : ['n w e l p','뇗'],
    u'\uB1D8' : ['n w e l sh','뇘'],
    u'\uB1D9' : ['n w e l th','뇙'],
    u'\uB1DA' : ['n w e l ph','뇚'],
    u'\uB1DB' : ['n w e l h','뇛'],
    u'\uB1DC' : ['n w e m','뇜'],
    u'\uB1DD' : ['n w e p','뇝'],
    u'\uB1DE' : ['n w e p sh','뇞'],
    u'\uB1DF' : ['n w e sh','뇟'],
    u'\uB1E0' : ['n w e s','뇠'],
    u'\uB1E1' : ['n w e N','뇡'],
    u'\uB1E2' : ['n w e tS','뇢'],
    u'\uB1E3' : ['n w e tSh','뇣'],
    u'\uB1E4' : ['n w e kh','뇤'],
    u'\uB1E5' : ['n w e th','뇥'],
    u'\uB1E6' : ['n w e ph','뇦'],
    u'\uB1E7' : ['n w e h','뇧'],
    u'\uB1E8' : ['n j o','뇨'],
    u'\uB1E9' : ['n j o k','뇩'],
    u'\uB1EA' : ['n j o k>','뇪'],
    u'\uB1EB' : ['n j o k sh','뇫'],
    u'\uB1EC' : ['n j o n','뇬'],
    u'\uB1ED' : ['n j o n tS','뇭'],
    u'\uB1EE' : ['n j o n h','뇮'],
    u'\uB1EF' : ['n j o t','뇯'],
    u'\uB1F0' : ['n j o l','뇰'],
    u'\uB1F1' : ['n j o l k','뇱'],
    u'\uB1F2' : ['n j o l m','뇲'],
    u'\uB1F3' : ['n j o l p','뇳'],
    u'\uB1F4' : ['n j o l sh','뇴'],
    u'\uB1F5' : ['n j o l th','뇵'],
    u'\uB1F6' : ['n j o l ph','뇶'],
    u'\uB1F7' : ['n j o l h','뇷'],
    u'\uB1F8' : ['n j o m','뇸'],
    u'\uB1F9' : ['n j o p','뇹'],
    u'\uB1FA' : ['n j o p sh','뇺'],
    u'\uB1FB' : ['n j o sh','뇻'],
    u'\uB1FC' : ['n j o s','뇼'],
    u'\uB1FD' : ['n j o N','뇽'],
    u'\uB1FE' : ['n j o tS','뇾'],
    u'\uB1FF' : ['n j o tSh','뇿'],
    u'\uB200' : ['n j o kh','눀'],
    u'\uB201' : ['n j o th','눁'],
    u'\uB202' : ['n j o ph','눂'],
    u'\uB203' : ['n j o h','눃'],
    u'\uB204' : ['n u','누'],
    u'\uB205' : ['n u k','눅'],
    u'\uB206' : ['n u k>','눆'],
    u'\uB207' : ['n u k sh','눇'],
    u'\uB208' : ['n u n','눈'],
    u'\uB209' : ['n u n tS','눉'],
    u'\uB20A' : ['n u n h','눊'],
    u'\uB20B' : ['n u t','눋'],
    u'\uB20C' : ['n u l','눌'],
    u'\uB20D' : ['n u l k','눍'],
    u'\uB20E' : ['n u l m','눎'],
    u'\uB20F' : ['n u l p','눏'],
    u'\uB210' : ['n u l sh','눐'],
    u'\uB211' : ['n u l th','눑'],
    u'\uB212' : ['n u l ph','눒'],
    u'\uB213' : ['n u l h','눓'],
    u'\uB214' : ['n u m','눔'],
    u'\uB215' : ['n u p','눕'],
    u'\uB216' : ['n u p sh','눖'],
    u'\uB217' : ['n u sh','눗'],
    u'\uB218' : ['n u s','눘'],
    u'\uB219' : ['n u N','눙'],
    u'\uB21A' : ['n u tS','눚'],
    u'\uB21B' : ['n u tSh','눛'],
    u'\uB21C' : ['n u kh','눜'],
    u'\uB21D' : ['n u th','눝'],
    u'\uB21E' : ['n u ph','눞'],
    u'\uB21F' : ['n u h','눟'],
    u'\uB220' : ['n w ^','눠'],
    u'\uB221' : ['n w ^ k','눡'],
    u'\uB222' : ['n w ^ k>','눢'],
    u'\uB223' : ['n w ^ k sh','눣'],
    u'\uB224' : ['n w ^ n','눤'],
    u'\uB225' : ['n w ^ n tS','눥'],
    u'\uB226' : ['n w ^ n h','눦'],
    u'\uB227' : ['n w ^ t','눧'],
    u'\uB228' : ['n w ^ l','눨'],
    u'\uB229' : ['n w ^ l k','눩'],
    u'\uB22A' : ['n w ^ l m','눪'],
    u'\uB22B' : ['n w ^ l p','눫'],
    u'\uB22C' : ['n w ^ l sh','눬'],
    u'\uB22D' : ['n w ^ l th','눭'],
    u'\uB22E' : ['n w ^ l ph','눮'],
    u'\uB22F' : ['n w ^ l h','눯'],
    u'\uB230' : ['n w ^ m','눰'],
    u'\uB231' : ['n w ^ p','눱'],
    u'\uB232' : ['n w ^ p sh','눲'],
    u'\uB233' : ['n w ^ sh','눳'],
    u'\uB234' : ['n w ^ s','눴'],
    u'\uB235' : ['n w ^ N','눵'],
    u'\uB236' : ['n w ^ tS','눶'],
    u'\uB237' : ['n w ^ tSh','눷'],
    u'\uB238' : ['n w ^ kh','눸'],
    u'\uB239' : ['n w ^ th','눹'],
    u'\uB23A' : ['n w ^ ph','눺'],
    u'\uB23B' : ['n w ^ h','눻'],
    u'\uB23C' : ['n w E','눼'],
    u'\uB23D' : ['n w E k','눽'],
    u'\uB23E' : ['n w E k>','눾'],
    u'\uB23F' : ['n w E k sh','눿'],
    u'\uB240' : ['n w E n','뉀'],
    u'\uB241' : ['n w E n tS','뉁'],
    u'\uB242' : ['n w E n h','뉂'],
    u'\uB243' : ['n w E t','뉃'],
    u'\uB244' : ['n w E l','뉄'],
    u'\uB245' : ['n w E l k','뉅'],
    u'\uB246' : ['n w E l m','뉆'],
    u'\uB247' : ['n w E l p','뉇'],
    u'\uB248' : ['n w E l sh','뉈'],
    u'\uB249' : ['n w E l th','뉉'],
    u'\uB24A' : ['n w E l ph','뉊'],
    u'\uB24B' : ['n w E l h','뉋'],
    u'\uB24C' : ['n w E m','뉌'],
    u'\uB24D' : ['n w E p','뉍'],
    u'\uB24E' : ['n w E p sh','뉎'],
    u'\uB24F' : ['n w E sh','뉏'],
    u'\uB250' : ['n w E s','뉐'],
    u'\uB251' : ['n w E N','뉑'],
    u'\uB252' : ['n w E tS','뉒'],
    u'\uB253' : ['n w E tSh','뉓'],
    u'\uB254' : ['n w E kh','뉔'],
    u'\uB255' : ['n w E th','뉕'],
    u'\uB256' : ['n w E ph','뉖'],
    u'\uB257' : ['n w E h','뉗'],
    u'\uB258' : ['n 7','뉘'],
    u'\uB259' : ['n 7 k','뉙'],
    u'\uB25A' : ['n 7 k>','뉚'],
    u'\uB25B' : ['n 7 k sh','뉛'],
    u'\uB25C' : ['n 7 n','뉜'],
    u'\uB25D' : ['n 7 n tS','뉝'],
    u'\uB25E' : ['n 7 n h','뉞'],
    u'\uB25F' : ['n 7 t','뉟'],
    u'\uB260' : ['n 7 l','뉠'],
    u'\uB261' : ['n 7 l k','뉡'],
    u'\uB262' : ['n 7 l m','뉢'],
    u'\uB263' : ['n 7 l p','뉣'],
    u'\uB264' : ['n 7 l sh','뉤'],
    u'\uB265' : ['n 7 l th','뉥'],
    u'\uB266' : ['n 7 l ph','뉦'],
    u'\uB267' : ['n 7 l h','뉧'],
    u'\uB268' : ['n 7 m','뉨'],
    u'\uB269' : ['n 7 p','뉩'],
    u'\uB26A' : ['n 7 p sh','뉪'],
    u'\uB26B' : ['n 7 sh','뉫'],
    u'\uB26C' : ['n 7 s','뉬'],
    u'\uB26D' : ['n 7 N','뉭'],
    u'\uB26E' : ['n 7 tS','뉮'],
    u'\uB26F' : ['n 7 tSh','뉯'],
    u'\uB270' : ['n 7 kh','뉰'],
    u'\uB271' : ['n 7 th','뉱'],
    u'\uB272' : ['n 7 ph','뉲'],
    u'\uB273' : ['n 7 h','뉳'],
    u'\uB274' : ['n j u','뉴'],
    u'\uB275' : ['n j u k','뉵'],
    u'\uB276' : ['n j u k>','뉶'],
    u'\uB277' : ['n j u k sh','뉷'],
    u'\uB278' : ['n j u n','뉸'],
    u'\uB279' : ['n j u n tS','뉹'],
    u'\uB27A' : ['n j u n h','뉺'],
    u'\uB27B' : ['n j u t','뉻'],
    u'\uB27C' : ['n j u l','뉼'],
    u'\uB27D' : ['n j u l k','뉽'],
    u'\uB27E' : ['n j u l m','뉾'],
    u'\uB27F' : ['n j u l p','뉿'],
    u'\uB280' : ['n j u l sh','늀'],
    u'\uB281' : ['n j u l th','늁'],
    u'\uB282' : ['n j u l ph','늂'],
    u'\uB283' : ['n j u l h','늃'],
    u'\uB284' : ['n j u m','늄'],
    u'\uB285' : ['n j u p','늅'],
    u'\uB286' : ['n j u p sh','늆'],
    u'\uB287' : ['n j u sh','늇'],
    u'\uB288' : ['n j u s','늈'],
    u'\uB289' : ['n j u N','늉'],
    u'\uB28A' : ['n j u tS','늊'],
    u'\uB28B' : ['n j u tSh','늋'],
    u'\uB28C' : ['n j u kh','늌'],
    u'\uB28D' : ['n j u th','늍'],
    u'\uB28E' : ['n j u ph','늎'],
    u'\uB28F' : ['n j u h','늏'],
    u'\uB290' : ['n 4','느'],
    u'\uB291' : ['n 4 k','늑'],
    u'\uB292' : ['n 4 k>','늒'],
    u'\uB293' : ['n 4 k sh','늓'],
    u'\uB294' : ['n 4 n','는'],
    u'\uB295' : ['n 4 n tS','늕'],
    u'\uB296' : ['n 4 nh','늖'],
    u'\uB297' : ['n 4 t','늗'],
    u'\uB298' : ['n 4 l','늘'],
    u'\uB299' : ['n 4 l k','늙'],
    u'\uB29A' : ['n 4 l m','늚'],
    u'\uB29B' : ['n 4 l p','늛'],
    u'\uB29C' : ['n 4 l sh','늜'],
    u'\uB29D' : ['n 4 l th','늝'],
    u'\uB29E' : ['n 4 l ph','늞'],
    u'\uB29F' : ['n 4 l h','늟'],
    u'\uB2A0' : ['n 4 m','늠'],
    u'\uB2A1' : ['n 4 p','늡'],
    u'\uB2A2' : ['n 4 p sh','늢'],
    u'\uB2A3' : ['n 4 sh','늣'],
    u'\uB2A4' : ['n 4 s','늤'],
    u'\uB2A5' : ['n 4 N','능'],
    u'\uB2A6' : ['n 4 tS','늦'],
    u'\uB2A7' : ['n 4 tSh','늧'],
    u'\uB2A8' : ['n 4 kh','늨'],
    u'\uB2A9' : ['n 4 th','늩'],
    u'\uB2AA' : ['n 4 ph','늪'],
    u'\uB2AB' : ['n 4 h','늫'],
    u'\uB2AC' : ['n 4 j','늬'],
    u'\uB2AD' : ['n 4 j k','늭'],
    u'\uB2AE' : ['n 4 j k>','늮'],
    u'\uB2AF' : ['n 4 j k sh','늯'],
    u'\uB2B0' : ['n 4 j n','늰'],
    u'\uB2B1' : ['n 4 j n tS','늱'],
    u'\uB2B2' : ['n 4 j n h','늲'],
    u'\uB2B3' : ['n 4 j t','늳'],
    u'\uB2B4' : ['n 4 j l','늴'],
    u'\uB2B5' : ['n 4 j l k','늵'],
    u'\uB2B6' : ['n 4 j l m','늶'],
    u'\uB2B7' : ['n 4 j l p','늷'],
    u'\uB2B8' : ['n 4 j l sh','늸'],
    u'\uB2B9' : ['n 4 j l th','늹'],
    u'\uB2BA' : ['n 4 j l ph','늺'],
    u'\uB2BB' : ['n 4 j l h','늻'],
    u'\uB2BC' : ['n 4 j m','늼'],
    u'\uB2BD' : ['n 4 j p','늽'],
    u'\uB2BE' : ['n 4 j p sh','늾'],
    u'\uB2BF' : ['n 4 j sh','늿'],
    u'\uB2C0' : ['n 4 j s','닀'],
    u'\uB2C1' : ['n 4 j N','닁'],
    u'\uB2C2' : ['n 4 j tS','닂'],
    u'\uB2C3' : ['n 4 j tSh','닃'],
    u'\uB2C4' : ['n 4 j kh','닄'],
    u'\uB2C5' : ['n 4 j th','닅'],
    u'\uB2C6' : ['n 4 j ph','닆'],
    u'\uB2C7' : ['n 4 j h','닇'],
    u'\uB2C8' : ['n i','니'],
    u'\uB2C9' : ['n i k','닉'],
    u'\uB2CA' : ['n i k>','닊'],
    u'\uB2CB' : ['n i k sh','닋'],
    u'\uB2CC' : ['n i n','닌'],
    u'\uB2CD' : ['n i n tS','닍'],
    u'\uB2CE' : ['n i n h','닎'],
    u'\uB2CF' : ['n i t','닏'],
    u'\uB2D0' : ['n i l','닐'],
    u'\uB2D1' : ['n i l k','닑'],
    u'\uB2D2' : ['n i l m','닒'],
    u'\uB2D3' : ['n i l p','닓'],
    u'\uB2D4' : ['n i l sh','닔'],
    u'\uB2D5' : ['n i l th','닕'],
    u'\uB2D6' : ['n i l ph','닖'],
    u'\uB2D7' : ['n i l h','닗'],
    u'\uB2D8' : ['n i m','님'],
    u'\uB2D9' : ['n i p','닙'],
    u'\uB2DA' : ['n i p sh','닚'],
    u'\uB2DB' : ['n i sh','닛'],
    u'\uB2DC' : ['n i s','닜'],
    u'\uB2DD' : ['n i N','닝'],
    u'\uB2DE' : ['n i tS','닞'],
    u'\uB2DF' : ['n i tSh','닟'],
    u'\uB2E0' : ['n i kh','닠'],
    u'\uB2E1' : ['n i th','닡'],
    u'\uB2E2' : ['n i ph','닢'],
    u'\uB2E3' : ['n i h','닣'],
    u'\uB2E4' : ['t a','다'],
    u'\uB2E5' : ['t a k','닥'],
    u'\uB2E6' : ['t a k>','닦'],
    u'\uB2E7' : ['t a k sh','닧'],
    u'\uB2E8' : ['t a n','단'],
    u'\uB2E9' : ['t a n tS','닩'],
    u'\uB2EA' : ['t a n h','닪'],
    u'\uB2EB' : ['t a t','닫'],
    u'\uB2EC' : ['t a l','달'],
    u'\uB2ED' : ['t a l k','닭'],
    u'\uB2EE' : ['t a l m','닮'],
    u'\uB2EF' : ['t a l b','닯'],
    u'\uB2F0' : ['t a l sh','닰'],
    u'\uB2F1' : ['t a l th','닱'],
    u'\uB2F2' : ['t a l ph','닲'],
    u'\uB2F3' : ['t a l h','닳'],
    u'\uB2F4' : ['t a m','담'],
    u'\uB2F5' : ['t a p','답'],
    u'\uB2F6' : ['t a p sh','닶'],
    u'\uB2F7' : ['t a sh','닷'],
    u'\uB2F8' : ['t a s','닸'],
    u'\uB2F9' : ['t a N','당'],
    u'\uB2FA' : ['t a tS','닺'],
    u'\uB2FB' : ['t a tSh','닻'],
    u'\uB2FC' : ['t a kh','닼'],
    u'\uB2FD' : ['t a th','닽'],
    u'\uB2FE' : ['t a ph','닾'],
    u'\uB2FF' : ['t a h','닿'],
    u'\uB300' : ['t @','대'],
    u'\uB301' : ['t @ k','댁'],
    u'\uB302' : ['t @ k>','댂'],
    u'\uB303' : ['t @ k sh','댃'],
    u'\uB304' : ['t @ n','댄'],
    u'\uB305' : ['t @ n tS','댅'],
    u'\uB306' : ['t @ n h','댆'],
    u'\uB307' : ['t @ t','댇'],
    u'\uB308' : ['t @ l','댈'],
    u'\uB309' : ['t @ l k','댉'],
    u'\uB30A' : ['t @ l m','댊'],
    u'\uB30B' : ['t @ l p','댋'],
    u'\uB30C' : ['t @ l sh','댌'],
    u'\uB30D' : ['t @ l th','댍'],
    u'\uB30E' : ['t @ l ph','댎'],
    u'\uB30F' : ['t @ l h','댏'],
    u'\uB310' : ['t @ m','댐'],
    u'\uB311' : ['t @ p','댑'],
    u'\uB312' : ['t @ p sh','댒'],
    u'\uB313' : ['t @ sh','댓'],
    u'\uB314' : ['t @ s','댔'],
    u'\uB315' : ['t @ N','댕'],
    u'\uB316' : ['t @ tS','댖'],
    u'\uB317' : ['t @ tSh','댗'],
    u'\uB318' : ['t @ kh','댘'],
    u'\uB319' : ['t @ th','댙'],
    u'\uB31A' : ['t @ ph','댚'],
    u'\uB31B' : ['t @ h','댛'],
    u'\uB31C' : ['t j a','댜'],
    u'\uB31D' : ['t j a k','댝'],
    u'\uB31E' : ['t j a k>','댞'],
    u'\uB31F' : ['t j a k sh','댟'],
    u'\uB320' : ['t j a n','댠'],
    u'\uB321' : ['t j a n tS','댡'],
    u'\uB322' : ['t j a n h','댢'],
    u'\uB323' : ['t j a t','댣'],
    u'\uB324' : ['t j a l','댤'],
    u'\uB325' : ['t j a l k','댥'],
    u'\uB326' : ['t j a l m','댦'],
    u'\uB327' : ['t j a l p','댧'],
    u'\uB328' : ['t j a l sh','댨'],
    u'\uB329' : ['t j a l th','댩'],
    u'\uB32A' : ['t j a l ph','댪'],
    u'\uB32B' : ['t j a l h','댫'],
    u'\uB32C' : ['t j a m','댬'],
    u'\uB32D' : ['t j a p','댭'],
    u'\uB32E' : ['t j a p sh','댮'],
    u'\uB32F' : ['t j a sh','댯'],
    u'\uB330' : ['t j a s','댰'],
    u'\uB331' : ['t j a N','댱'],
    u'\uB332' : ['t j a tS','댲'],
    u'\uB333' : ['t j a tSh','댳'],
    u'\uB334' : ['t j a kh','댴'],
    u'\uB335' : ['t j a th','댵'],
    u'\uB336' : ['t j a ph','댶'],
    u'\uB337' : ['t j a h','댷'],
    u'\uB338' : ['t j @','댸'],
    u'\uB339' : ['t j @ k','댹'],
    u'\uB33A' : ['t j @ k>','댺'],
    u'\uB33B' : ['t j @ k sh','댻'],
    u'\uB33C' : ['t j @ n','댼'],
    u'\uB33D' : ['t j @ n tS','댽'],
    u'\uB33E' : ['t j @ n h','댾'],
    u'\uB33F' : ['t j @ t','댿'],
    u'\uB340' : ['t j @ l','덀'],
    u'\uB341' : ['t j @ l k','덁'],
    u'\uB342' : ['t j @ l m','덂'],
    u'\uB343' : ['t j @ l p','덃'],
    u'\uB344' : ['t j @ l sh','덄'],
    u'\uB345' : ['t j @ l th','덅'],
    u'\uB346' : ['t j @ l ph','덆'],
    u'\uB347' : ['t j @ l h','덇'],
    u'\uB348' : ['t j @ m','덈'],
    u'\uB349' : ['t j @ p','덉'],
    u'\uB34A' : ['t j @ p sh','덊'],
    u'\uB34B' : ['t j @ sh','덋'],
    u'\uB34C' : ['t j @ s','덌'],
    u'\uB34D' : ['t j @ N','덍'],
    u'\uB34E' : ['t j @ tS','덎'],
    u'\uB34F' : ['t j @ tSh','덏'],
    u'\uB350' : ['t j @ kh','덐'],
    u'\uB351' : ['t j @ th','덑'],
    u'\uB352' : ['t j @ ph','덒'],
    u'\uB353' : ['t j @ h','덓'],
    u'\uB354' : ['t ^','더'],
    u'\uB355' : ['t ^ k','덕'],
    u'\uB356' : ['t ^ k>','덖'],
    u'\uB357' : ['t ^ k sh','덗'],
    u'\uB358' : ['t ^ n','던'],
    u'\uB359' : ['t ^ n tS','덙'],
    u'\uB35A' : ['t ^ n h','덚'],
    u'\uB35B' : ['t ^ t','덛'],
    u'\uB35C' : ['t ^ l','덜'],
    u'\uB35D' : ['t ^ l k','덝'],
    u'\uB35E' : ['t ^ l m','덞'],
    u'\uB35F' : ['t ^ l p','덟'],
    u'\uB360' : ['t ^ l sh','덠'],
    u'\uB361' : ['t ^ l th','덡'],
    u'\uB362' : ['t ^ l ph','덢'],
    u'\uB363' : ['t ^ l h','덣'],
    u'\uB364' : ['t ^ m','덤'],
    u'\uB365' : ['t ^ p','덥'],
    u'\uB366' : ['t ^ p sh','덦'],
    u'\uB367' : ['t ^ sh','덧'],
    u'\uB368' : ['t ^ s','덨'],
    u'\uB369' : ['t ^ N','덩'],
    u'\uB36A' : ['t ^ tS','덪'],
    u'\uB36B' : ['t ^ tSh','덫'],
    u'\uB36C' : ['t ^ kh','덬'],
    u'\uB36D' : ['t ^ th','덭'],
    u'\uB36E' : ['t ^ ph','덮'],
    u'\uB36F' : ['t ^ h','덯'],
    u'\uB370' : ['t e','데'],
    u'\uB371' : ['t e k','덱'],
    u'\uB372' : ['t e k>','덲'],
    u'\uB373' : ['t e k sh','덳'],
    u'\uB374' : ['t e n','덴'],
    u'\uB375' : ['t e n tS','덵'],
    u'\uB376' : ['t e n h','덶'],
    u'\uB377' : ['t e t','덷'],
    u'\uB378' : ['t e l','델'],
    u'\uB379' : ['t e l k','덹'],
    u'\uB37A' : ['t e l m','덺'],
    u'\uB37B' : ['t e l p','덻'],
    u'\uB37C' : ['t e l sh','덼'],
    u'\uB37D' : ['t e l th','덽'],
    u'\uB37E' : ['t e l ph','덾'],
    u'\uB37F' : ['t e l h','덿'],
    u'\uB380' : ['t e m','뎀'],
    u'\uB381' : ['t e p','뎁'],
    u'\uB382' : ['t e p sh','뎂'],
    u'\uB383' : ['t e sh','뎃'],
    u'\uB384' : ['t e s','뎄'],
    u'\uB385' : ['t e N','뎅'],
    u'\uB386' : ['t e tS','뎆'],
    u'\uB387' : ['t e tSh','뎇'],
    u'\uB388' : ['t e kh','뎈'],
    u'\uB389' : ['t e th','뎉'],
    u'\uB38A' : ['t e ph','뎊'],
    u'\uB38B' : ['t e h','뎋'],
    u'\uB38C' : ['t j ^','뎌'],
    u'\uB38D' : ['t j ^ k','뎍'],
    u'\uB38E' : ['t j ^ k>','뎎'],
    u'\uB38F' : ['t j ^ k sh','뎏'],
    u'\uB390' : ['t j ^ n','뎐'],
    u'\uB391' : ['t j ^ n tS','뎑'],
    u'\uB392' : ['t j ^ n h','뎒'],
    u'\uB393' : ['t j ^ t','뎓'],
    u'\uB394' : ['t j ^ l','뎔'],
    u'\uB395' : ['t j ^ l k','뎕'],
    u'\uB396' : ['t j ^ l m','뎖'],
    u'\uB397' : ['t j ^ l p','뎗'],
    u'\uB398' : ['t j ^ l sh','뎘'],
    u'\uB399' : ['t j ^ l th','뎙'],
    u'\uB39A' : ['t j ^ l ph','뎚'],
    u'\uB39B' : ['t j ^ l h','뎛'],
    u'\uB39C' : ['t j ^ m','뎜'],
    u'\uB39D' : ['t j ^ p','뎝'],
    u'\uB39E' : ['t j ^ p sh','뎞'],
    u'\uB39F' : ['t j ^ sh','뎟'],
    u'\uB3A0' : ['t j ^ s','뎠'],
    u'\uB3A1' : ['t j ^ N','뎡'],
    u'\uB3A2' : ['t j ^ tS','뎢'],
    u'\uB3A3' : ['t j ^ tSh','뎣'],
    u'\uB3A4' : ['t j ^ kh','뎤'],
    u'\uB3A5' : ['t j ^ th','뎥'],
    u'\uB3A6' : ['t j ^ ph','뎦'],
    u'\uB3A7' : ['t j ^ h','뎧'],
    u'\uB3A8 ' : ['t j e','뎨 '],
    u'\uB3A9' : ['t j e k','뎩'],
    u'\uB3AA' : ['t j e k>','뎪'],
    u'\uB3AB' : ['t j e k sh','뎫'],
    u'\uB3AC' : ['t j e n','뎬'],
    u'\uB3AD' : ['t j e n tS','뎭'],
    u'\uB3AE' : ['t j e n h','뎮'],
    u'\uB3AF' : ['t j e t','뎯'],
    u'\uB3B0' : ['t j e l','뎰'],
    u'\uB3B1' : ['t j e l k','뎱'],
    u'\uB3B2' : ['t j e l m','뎲'],
    u'\uB3B3' : ['t j e l p','뎳'],
    u'\uB3B4' : ['t j e l sh','뎴'],
    u'\uB3B5' : ['t j e l th','뎵'],
    u'\uB3B6' : ['t j e l ph','뎶'],
    u'\uB3B7' : ['t j e l h','뎷'],
    u'\uB3B8' : ['t j e m','뎸'],
    u'\uB3B9' : ['t j e p','뎹'],
    u'\uB3BA' : ['t j e p sh','뎺'],
    u'\uB3BB' : ['t j e sh','뎻'],
    u'\uB3BC' : ['t j e s','뎼'],
    u'\uB3BD' : ['t j e N','뎽'],
    u'\uB3BE' : ['t j e tS','뎾'],
    u'\uB3BF' : ['t j e tSh','뎿'],
    u'\uB3C0' : ['t j e kh','돀'],
    u'\uB3C1' : ['t j e th','돁'],
    u'\uB3C2' : ['t j e ph','돂'],
    u'\uB3C3' : ['t j e h','돃'],
    u'\uB3C4' : ['t o','도'],
    u'\uB3C5' : ['t o k','독'],
    u'\uB3C6' : ['t o k>','돆'],
    u'\uB3C7' : ['t o k sh','돇'],
    u'\uB3C8' : ['t o n','돈'],
    u'\uB3C9' : ['t o n tS','돉'],
    u'\uB3CA' : ['t o n h','돊'],
    u'\uB3CB' : ['t o t','돋'],
    u'\uB3CC' : ['t o l','돌'],
    u'\uB3CD' : ['t o l k','돍'],
    u'\uB3CE' : ['t o l m','돎'],
    u'\uB3CF' : ['t o l p','돏'],
    u'\uB3D0' : ['t o l sh','돐'],
    u'\uB3D1' : ['t o l th','돑'],
    u'\uB3D2' : ['t o l ph','돒'],
    u'\uB3D3' : ['t o l h','돓'],
    u'\uB3D4' : ['t o m','돔'],
    u'\uB3D5' : ['t o p','돕'],
    u'\uB3D6' : ['t o p sh','돖'],
    u'\uB3D7' : ['t o sh','돗'],
    u'\uB3D8' : ['t o s','돘'],
    u'\uB3D9' : ['t o N','동'],
    u'\uB3DA' : ['t o tS','돚'],
    u'\uB3DB' : ['t o tSh','돛'],
    u'\uB3DC' : ['t o kh','돜'],
    u'\uB3DD' : ['t o th','돝'],
    u'\uB3DE' : ['t o ph','돞'],
    u'\uB3DF' : ['t o h','돟'],
    u'\uB3E0' : ['t w a','돠'],
    u'\uB3E1' : ['t w a k','돡'],
    u'\uB3E2' : ['t w a k>','돢'],
    u'\uB3E3' : ['t w a k sh','돣'],
    u'\uB3E4' : ['t w a n','돤'],
    u'\uB3E5' : ['t w a n tS','돥'],
    u'\uB3E6' : ['t w a n h','돦'],
    u'\uB3E7' : ['t w a t','돧'],
    u'\uB3E8' : ['t w a l','돨'],
    u'\uB3E9' : ['t w a l k','돩'],
    u'\uB3EA' : ['t w a l m','돪'],
    u'\uB3EB' : ['t w a l p','돫'],
    u'\uB3EC' : ['t w a l sh','돬'],
    u'\uB3ED' : ['t w a l th','돭'],
    u'\uB3EE' : ['t w a l ph','돮'],
    u'\uB3EF' : ['t w a l h','돯'],
    u'\uB3F0' : ['t w a m','돰'],
    u'\uB3F1' : ['t w a p','돱'],
    u'\uB3F2' : ['t w a p sh','돲'],
    u'\uB3F3' : ['t w a sh','돳'],
    u'\uB3F4' : ['t w a s','돴'],
    u'\uB3F5' : ['t w a N','돵'],
    u'\uB3F6' : ['t w a tS','돶'],
    u'\uB3F7' : ['t w a tSh','돷'],
    u'\uB3F8' : ['t w a kh','돸'],
    u'\uB3F9' : ['t w a th','돹'],
    u'\uB3FA' : ['t w a ph','돺'],
    u'\uB3FB' : ['t w a h','돻'],
    u'\uB3FC' : ['t w @','돼'],
    u'\uB3FD' : ['t w @ k','돽'],
    u'\uB3FE' : ['t w @ k>','돾'],
    u'\uB3FF' : ['t w @ k sh','돿'],
    u'\uB400' : ['t w @ n','됀'],
    u'\uB401' : ['t w @ n tS','됁'],
    u'\uB402' : ['t w @ n h','됂'],
    u'\uB403' : ['t w @ t','됃'],
    u'\uB404' : ['t w @ l','됄'],
    u'\uB405' : ['t w @ l k','됅'],
    u'\uB406' : ['t w @ l m','됆'],
    u'\uB407' : ['t w @ l p','됇'],
    u'\uB408' : ['t w @ l sh','됈'],
    u'\uB409' : ['t w @ l th','됉'],
    u'\uB40A' : ['t w @ l ph','됊'],
    u'\uB40B' : ['t w @ l h','됋'],
    u'\uB40C' : ['t w @ m','됌'],
    u'\uB40D' : ['t w @ p','됍'],
    u'\uB40E' : ['t w @ p sh','됎'],
    u'\uB40F' : ['t w @ sh','됏'],
    u'\uB410' : ['t w @ s','됐'],
    u'\uB411' : ['t w @ N','됑'],
    u'\uB412' : ['t w @ tS','됒'],
    u'\uB413' : ['t w @ tSh','됓'],
    u'\uB414' : ['t w @ kh','됔'],
    u'\uB415' : ['t w @ th','됕'],
    u'\uB416' : ['t w @ ph','됖'],
    u'\uB417' : ['t w @ h','됗'],
    u'\uB418' : ['t w e','되'],
    u'\uB419' : ['t w e k','됙'],
    u'\uB41A' : ['t w e k>','됚'],
    u'\uB41B' : ['t w e k sh','됛'],
    u'\uB41C' : ['t w e n','된'],
    u'\uB41D' : ['t w e n tS','됝'],
    u'\uB41E' : ['t w e n h','됞'],
    u'\uB41F' : ['t w e t','됟'],
    u'\uB420' : ['t w e l','될'],
    u'\uB421' : ['t w e l k','됡'],
    u'\uB422' : ['t w e l m','됢'],
    u'\uB423' : ['t w e l p','됣'],
    u'\uB424' : ['t w e l sh','됤'],
    u'\uB425' : ['t w e l th','됥'],
    u'\uB426' : ['t w e l ph','됦'],
    u'\uB427' : ['t w e l h','됧'],
    u'\uB428' : ['t w e m','됨'],
    u'\uB429' : ['t w e p','됩'],
    u'\uB42A' : ['t w e p sh','됪'],
    u'\uB42B' : ['t w e sh','됫'],
    u'\uB42C' : ['t w e s','됬'],
    u'\uB42D' : ['t w e N','됭'],
    u'\uB42E' : ['t w e tS','됮'],
    u'\uB42F' : ['t w e tSh','됯'],
    u'\uB430' : ['t w e kh','됰'],
    u'\uB431' : ['t w e th','됱'],
    u'\uB432' : ['t w e ph','됲'],
    u'\uB433' : ['t w e h','됳'],
    u'\uB434' : ['t j o','됴'],
    u'\uB435' : ['t j o k','됵'],
    u'\uB436' : ['t j o k>','됶'],
    u'\uB437' : ['t j o k sh','됷'],
    u'\uB438' : ['t j o n','됸'],
    u'\uB439' : ['t j o n tS','됹'],
    u'\uB43A' : ['t j o n h','됺'],
    u'\uB43B' : ['t j o t','됻'],
    u'\uB43C' : ['t j o l','됼'],
    u'\uB43D' : ['t j o l k','됽'],
    u'\uB43E' : ['t j o l m','됾'],
    u'\uB43F' : ['t j o l p','됿'],
    u'\uB440' : ['t j o l sh','둀'],
    u'\uB441' : ['t j o l th','둁'],
    u'\uB442' : ['t j o l ph','둂'],
    u'\uB443' : ['t j o l h','둃'],
    u'\uB444' : ['t j o m','둄'],
    u'\uB445' : ['t j o p','둅'],
    u'\uB446' : ['t j o p sh','둆'],
    u'\uB447' : ['t j o sh','둇'],
    u'\uB448' : ['t j o s','둈'],
    u'\uB449' : ['t j o N','둉'],
    u'\uB44A' : ['t j o tS','둊'],
    u'\uB44B' : ['t j o tSh','둋'],
    u'\uB44C' : ['t j o kh','둌'],
    u'\uB44D' : ['t j o th','둍'],
    u'\uB44E' : ['t j o ph','둎'],
    u'\uB44F' : ['t j o h','둏'],
    u'\uB450' : ['t u','두'],
    u'\uB451' : ['t u k','둑'],
    u'\uB452' : ['t u k>','둒'],
    u'\uB453' : ['t u k sh','둓'],
    u'\uB454' : ['t u n','둔'],
    u'\uB455' : ['t u n tS','둕'],
    u'\uB456' : ['t u n h','둖'],
    u'\uB457' : ['t u t','둗'],
    u'\uB458' : ['t u l','둘'],
    u'\uB459' : ['t u l k','둙'],
    u'\uB45A' : ['t u l m','둚'],
    u'\uB45B' : ['t u l p','둛'],
    u'\uB45C' : ['t u l sh','둜'],
    u'\uB45D' : ['t u l th','둝'],
    u'\uB45E' : ['t u l ph','둞'],
    u'\uB45F' : ['t u l h','둟'],
    u'\uB460' : ['t u m','둠'],
    u'\uB461' : ['t u p','둡'],
    u'\uB462' : ['t u p sh','둢'],
    u'\uB463' : ['t u sh','둣'],
    u'\uB464' : ['t u s','둤'],
    u'\uB465' : ['t u N','둥'],
    u'\uB466' : ['t u tS','둦'],
    u'\uB467' : ['t u tSh','둧'],
    u'\uB468' : ['t u kh','둨'],
    u'\uB469' : ['t u th','둩'],
    u'\uB46A' : ['t u ph','둪'],
    u'\uB46B' : ['t u h','둫'],
    u'\uB46C' : ['t w ^','둬'],
    u'\uB46D' : ['t w ^ k','둭'],
    u'\uB46E' : ['t w ^ k>','둮'],
    u'\uB46F' : ['t w ^ k sh','둯'],
    u'\uB470' : ['t w ^ n','둰'],
    u'\uB471' : ['t w ^ n tS','둱'],
    u'\uB472' : ['t w ^ n h','둲'],
    u'\uB473' : ['t w ^ t','둳'],
    u'\uB474' : ['t w ^ l','둴'],
    u'\uB475' : ['t w ^ l k','둵'],
    u'\uB476' : ['t w ^ l m','둶'],
    u'\uB477' : ['t w ^ l p','둷'],
    u'\uB478' : ['t w ^ l sh','둸'],
    u'\uB479' : ['t w ^ l th','둹'],
    u'\uB47A' : ['t w ^ l ph','둺'],
    u'\uB47B' : ['t w ^ l h','둻'],
    u'\uB47C' : ['t w ^ m','둼'],
    u'\uB47D' : ['t w ^ p','둽'],
    u'\uB47E' : ['t w ^ p sh','둾'],
    u'\uB47F' : ['t w ^ sh','둿'],
    u'\uB480' : ['t w ^ s','뒀'],
    u'\uB481' : ['t w ^ N','뒁'],
    u'\uB482' : ['t w ^ tS','뒂'],
    u'\uB483' : ['t w ^ tSh','뒃'],
    u'\uB484' : ['t w ^ kh','뒄'],
    u'\uB485' : ['t w ^ th','뒅'],
    u'\uB486' : ['t w ^ ph','뒆'],
    u'\uB487' : ['t w ^ h','뒇'],
    u'\uB488' : ['t w E','뒈'],
    u'\uB489' : ['t w E k','뒉'],
    u'\uB48A' : ['t w E k>','뒊'],
    u'\uB48B' : ['t w E k sh','뒋'],
    u'\uB48C' : ['t w E n','뒌'],
    u'\uB48D' : ['t w E n tS','뒍'],
    u'\uB48E' : ['t w E n h','뒎'],
    u'\uB48F' : ['t w E t','뒏'],
    u'\uB490' : ['t w E l','뒐'],
    u'\uB491' : ['t w E l k','뒑'],
    u'\uB492' : ['t w E l m','뒒'],
    u'\uB493' : ['t w E l p','뒓'],
    u'\uB494' : ['t w E l sh','뒔'],
    u'\uB495' : ['t w E l th','뒕'],
    u'\uB496' : ['t w E l ph','뒖'],
    u'\uB497' : ['t w E l h','뒗'],
    u'\uB498' : ['t w E m','뒘'],
    u'\uB499' : ['t w E p','뒙'],
    u'\uB49A' : ['t w E p sh','뒚'],
    u'\uB49B' : ['t w E sh','뒛'],
    u'\uB49C' : ['t w E s','뒜'],
    u'\uB49D' : ['t w E N','뒝'],
    u'\uB49E' : ['t w E tS','뒞'],
    u'\uB49F' : ['t w E tSh','뒟'],
    u'\uB4A0' : ['t w E kh','뒠'],
    u'\uB4A1' : ['t w E th','뒡'],
    u'\uB4A2' : ['t w E ph','뒢'],
    u'\uB4A3' : ['t w E h','뒣'],
    u'\uB4A4' : ['t 7','뒤'],
    u'\uB4A5' : ['t 7 k','뒥'],
    u'\uB4A6' : ['t 7 k>','뒦'],
    u'\uB4A7' : ['t 7 k sh','뒧'],
    u'\uB4A8' : ['t 7 n','뒨'],
    u'\uB4A9' : ['t 7 n tS','뒩'],
    u'\uB4AA' : ['t 7 n h','뒪'],
    u'\uB4AB' : ['t 7 t','뒫'],
    u'\uB4AC' : ['t 7 l','뒬'],
    u'\uB4AD' : ['t 7 l k','뒭'],
    u'\uB4AE' : ['t 7 l m','뒮'],
    u'\uB4AF' : ['t 7 l p','뒯'],
    u'\uB4B0' : ['t 7 l sh','뒰'],
    u'\uB4B1' : ['t 7 l th','뒱'],
    u'\uB4B2' : ['t 7 l ph','뒲'],
    u'\uB4B3' : ['t 7 l h','뒳'],
    u'\uB4B4' : ['t 7 m','뒴'],
    u'\uB4B5' : ['t 7 p','뒵'],
    u'\uB4B6' : ['t 7 p sh','뒶'],
    u'\uB4B7' : ['t 7 sh','뒷'],
    u'\uB4B8' : ['t 7 s','뒸'],
    u'\uB4B9' : ['t 7 N','뒹'],
    u'\uB4BA' : ['t 7 tS','뒺'],
    u'\uB4BB' : ['t 7 tSh','뒻'],
    u'\uB4BC' : ['t 7 kh','뒼'],
    u'\uB4BD' : ['t 7 th','뒽'],
    u'\uB4BE' : ['t 7 ph','뒾'],
    u'\uB4BF' : ['t 7 h','뒿'],
    u'\uB4C0' : ['t j u','듀'],
    u'\uB4C1' : ['t j u k','듁'],
    u'\uB4C2' : ['t j u k>','듂'],
    u'\uB4C3' : ['t j u k sh','듃'],
    u'\uB4C4' : ['t j u n','듄'],
    u'\uB4C5' : ['t j u n tS','듅'],
    u'\uB4C6' : ['t j u n h','듆'],
    u'\uB4C7' : ['t j u t','듇'],
    u'\uB4C8' : ['t j u l','듈'],
    u'\uB4C9' : ['t j u l k','듉'],
    u'\uB4CA' : ['t j u l m','듊'],
    u'\uB4CB' : ['t j u l p','듋'],
    u'\uB4CC' : ['t j u l sh','듌'],
    u'\uB4CD' : ['t j u l th','듍'],
    u'\uB4CE' : ['t j u l ph','듎'],
    u'\uB4CF' : ['t j u l h','듏'],
    u'\uB4D0' : ['t j u m','듐'],
    u'\uB4D1' : ['t j u p','듑'],
    u'\uB4D2' : ['t j u p sh','듒'],
    u'\uB4D3' : ['t j u sh','듓'],
    u'\uB4D4' : ['t j u s','듔'],
    u'\uB4D5' : ['t j u N','듕'],
    u'\uB4D6' : ['t j u tS','듖'],
    u'\uB4D7' : ['t j u tSh','듗'],
    u'\uB4D8' : ['t j u kh','듘'],
    u'\uB4D9' : ['t j u th','듙'],
    u'\uB4DA' : ['t j u ph','듚'],
    u'\uB4DB' : ['t j u h','듛'],
    u'\uB4DC' : ['t 4','드'],
    u'\uB4DD' : ['t 4 k','득'],
    u'\uB4DE' : ['t 4 k>','듞'],
    u'\uB4DF' : ['t 4 k sh','듟'],
    u'\uB4E0' : ['t 4 n','든'],
    u'\uB4E1' : ['t 4 n tS','듡'],
    u'\uB4E2' : ['t 4 n h','듢'],
    u'\uB4E3' : ['t 4 t','듣'],
    u'\uB4E4' : ['t 4 l','들'],
    u'\uB4E5' : ['t 4 l k','듥'],
    u'\uB4E6' : ['t 4 l m','듦'],
    u'\uB4E7' : ['t 4 l p','듧'],
    u'\uB4E8' : ['t 4 l sh','듨'],
    u'\uB4E9' : ['t 4 l th','듩'],
    u'\uB4EA' : ['t 4 l ph','듪'],
    u'\uB4EB' : ['t 4 l h','듫'],
    u'\uB4EC' : ['t 4 m','듬'],
    u'\uB4ED' : ['t 4 p','듭'],
    u'\uB4EE' : ['t 4 p sh','듮'],
    u'\uB4EF' : ['t 4 sh','듯'],
    u'\uB4F0' : ['t 4 s','듰'],
    u'\uB4F1' : ['t 4 N','등'],
    u'\uB4F2' : ['t 4 tS','듲'],
    u'\uB4F3' : ['t 4 tSh','듳'],
    u'\uB4F4' : ['t 4 kh','듴'],
    u'\uB4F5' : ['t 4 th','듵'],
    u'\uB4F6' : ['t 4 ph','듶'],
    u'\uB4F7' : ['t 4 h','듷'],
    u'\uB4F8' : ['t 4 j','듸'],
    u'\uB4F9' : ['t 4 j k','듹'],
    u'\uB4FA' : ['t 4 j k>','듺'],
    u'\uB4FB' : ['t 4 j k sh','듻'],
    u'\uB4FC' : ['t 4 j n','듼'],
    u'\uB4FD' : ['t 4 j n tS','듽'],
    u'\uB4FE' : ['t 4 j n h','듾'],
    u'\uB4FF' : ['t 4 j t','듿'],
    u'\uB500' : ['t 4 j l','딀'],
    u'\uB501' : ['t 4 j l k','딁'],
    u'\uB502' : ['t 4 j l m','딂'],
    u'\uB503' : ['t 4 j l p','딃'],
    u'\uB504' : ['t 4 j l sh','딄'],
    u'\uB505' : ['t 4 j l th','딅'],
    u'\uB506' : ['t 4 j l ph','딆'],
    u'\uB507' : ['t 4 j l h','딇'],
    u'\uB508' : ['t 4 j m','딈'],
    u'\uB509' : ['t 4 j p','딉'],
    u'\uB50A' : ['t 4 j p sh','딊'],
    u'\uB50B' : ['t 4 j sh','딋'],
    u'\uB50C' : ['t 4 j s','딌'],
    u'\uB50D' : ['t 4 j N','딍'],
    u'\uB50E' : ['t 4 j tS','딎'],
    u'\uB50F' : ['t 4 j tSh','딏'],
    u'\uB510' : ['t 4 j kh','딐'],
    u'\uB511' : ['t 4 j th','딑'],
    u'\uB512' : ['t 4 j ph','딒'],
    u'\uB513' : ['t 4 j h','딓'],
    u'\uB514' : ['t i','디'],
    u'\uB515' : ['t i k','딕'],
    u'\uB516' : ['t i k>','딖'],
    u'\uB517' : ['t i k sh','딗'],
    u'\uB518' : ['t i n','딘'],
    u'\uB519' : ['t i n tS','딙'],
    u'\uB51A' : ['t i n h','딚'],
    u'\uB51B' : ['t i t','딛'],
    u'\uB51C' : ['t i l','딜'],
    u'\uB51D' : ['t i l k','딝'],
    u'\uB51E' : ['t i l m','딞'],
    u'\uB51F' : ['t i l p','딟'],
    u'\uB520' : ['t i l sh','딠'],
    u'\uB521' : ['t i l th','딡'],
    u'\uB522' : ['t i l ph','딢'],
    u'\uB523' : ['t i l h','딣'],
    u'\uB524' : ['t i m','딤'],
    u'\uB525' : ['t i p','딥'],
    u'\uB526' : ['t i p sh','딦'],
    u'\uB527' : ['t i sh','딧'],
    u'\uB528' : ['t i s','딨'],
    u'\uB529' : ['t i N','딩'],
    u'\uB52A' : ['t i tS','딪'],
    u'\uB52B' : ['t i tSh','딫'],
    u'\uB52C' : ['t i kh','딬'],
    u'\uB52D' : ['t i th','딭'],
    u'\uB52E' : ['t i ph','딮'],
    u'\uB52F' : ['t i h','딯'],
    u'\uB530' : ['t> a','따'],
    u'\uB531' : ['t> a k','딱'],
    u'\uB532' : ['t> a k>','딲'],
    u'\uB533' : ['t> a k sh','딳'],
    u'\uB534' : ['t> a n','딴'],
    u'\uB535' : ['t> a n tS','딵'],
    u'\uB536' : ['t> a n h','딶'],
    u'\uB537' : ['t> a t','딷'],
    u'\uB538' : ['t> a l','딸'],
    u'\uB539' : ['t> a l k','딹'],
    u'\uB53A' : ['t> a l m','딺'],
    u'\uB53B' : ['t> a l p','딻'],
    u'\uB53C' : ['t> a l sh','딼'],
    u'\uB53D' : ['t> a l th','딽'],
    u'\uB53E' : ['t> a l ph','딾'],
    u'\uB53F' : ['t> a l h','딿'],
    u'\uB540' : ['t> a m','땀'],
    u'\uB541' : ['t> a p','땁'],
    u'\uB542' : ['t> a p sh','땂'],
    u'\uB543' : ['t> a sh','땃'],
    u'\uB544' : ['t> a s','땄'],
    u'\uB545' : ['t> a N','땅'],
    u'\uB546' : ['t> a tS','땆'],
    u'\uB547' : ['t> a tSh','땇'],
    u'\uB548' : ['t> a kh','땈'],
    u'\uB549' : ['t> a th','땉'],
    u'\uB54A' : ['t> a ph','땊'],
    u'\uB54B' : ['t> a h','땋'],
    u'\uB54C' : ['t> @','때'],
    u'\uB54D' : ['t> @ k','땍'],
    u'\uB54E' : ['t> @ k>','땎'],
    u'\uB54F' : ['t> @ k sh','땏'],
    u'\uB550' : ['t> @ n','땐'],
    u'\uB551' : ['t> @ n tS','땑'],
    u'\uB552' : ['t> @ n h','땒'],
    u'\uB553' : ['t> @ t','땓'],
    u'\uB554' : ['t> @ l','땔'],
    u'\uB555' : ['t> @ l k','땕'],
    u'\uB556' : ['t> @ l m','땖'],
    u'\uB557' : ['t> @ l p','땗'],
    u'\uB558' : ['t> @ l sh','땘'],
    u'\uB559' : ['t> @ l th','땙'],
    u'\uB55A' : ['t> @ l ph','땚'],
    u'\uB55B' : ['t> @ l h','땛'],
    u'\uB55C' : ['t> @ m','땜'],
    u'\uB55D' : ['t> @ p','땝'],
    u'\uB55E' : ['t> @ p sh','땞'],
    u'\uB55F ' : ['t> @ sh','땟 '],
    u'\uB560' : ['t> @ s','땠'],
    u'\uB561' : ['t> @ N','땡'],
    u'\uB562' : ['t> @ tS','땢'],
    u'\uB563' : ['t> @ tSh','땣'],
    u'\uB564' : ['t> @ kh','땤'],
    u'\uB565' : ['t> @ th','땥'],
    u'\uB566' : ['t> @ ph','땦'],
    u'\uB567' : ['t> @ h','땧'],
    u'\uB568' : ['t> j a','땨'],
    u'\uB569' : ['t> j a k','땩'],
    u'\uB56A' : ['t> j a k>','땪'],
    u'\uB56B' : ['t> j a k sh','땫'],
    u'\uB56C' : ['t> j a n','땬'],
    u'\uB56D' : ['t> j a n tS','땭'],
    u'\uB56E' : ['t> j a n h','땮'],
    u'\uB56F' : ['t> j a t','땯'],
    u'\uB570' : ['t> j a l','땰'],
    u'\uB571' : ['t> j a l k','땱'],
    u'\uB572' : ['t> j a l m','땲'],
    u'\uB573' : ['t> j a l p','땳'],
    u'\uB574' : ['t> j a l sh','땴'],
    u'\uB575' : ['t> j a l th','땵'],
    u'\uB576' : ['t> j a l ph','땶'],
    u'\uB577' : ['t> j a l h','땷'],
    u'\uB578' : ['t> j a m','땸'],
    u'\uB579' : ['t> j a p','땹'],
    u'\uB57A' : ['t> j a p sh','땺'],
    u'\uB57B' : ['t> j a sh','땻'],
    u'\uB57C' : ['t> j a s','땼'],
    u'\uB57D' : ['t> j a N','땽'],
    u'\uB57E' : ['t> j a tS','땾'],
    u'\uB57F' : ['t> j a tSh','땿'],
    u'\uB580' : ['t> j a kh','떀'],
    u'\uB581' : ['t> j a th','떁'],
    u'\uB582' : ['t> j a ph','떂'],
    u'\uB583' : ['t> j a h','떃'],
    u'\uB584' : ['t> j @','떄'],
    u'\uB585' : ['t> j @ k','떅'],
    u'\uB586' : ['t> j @ k>','떆'],
    u'\uB587' : ['t> j @ k sh','떇'],
    u'\uB588' : ['t> j @ n','떈'],
    u'\uB589' : ['t> j @ n tS','떉'],
    u'\uB58A' : ['t> j @ n h','떊'],
    u'\uB58B' : ['t> j @ t','떋'],
    u'\uB58C' : ['t> j @ l','떌'],
    u'\uB58D' : ['t> j @ l k','떍'],
    u'\uB58E' : ['t> j @ l m','떎'],
    u'\uB58F' : ['t> j @ l p','떏'],
    u'\uB590' : ['t> j @ l sh','떐'],
    u'\uB591' : ['t> j @ l th','떑'],
    u'\uB592' : ['t> j @ l ph','떒'],
    u'\uB593' : ['t> j @ l h','떓'],
    u'\uB594' : ['t> j @ m','떔'],
    u'\uB595' : ['t> j @ p','떕'],
    u'\uB596' : ['t> j @ p sh','떖'],
    u'\uB597' : ['t> j @ sh','떗'],
    u'\uB598' : ['t> j @ s','떘'],
    u'\uB599' : ['t> j @ N','떙'],
    u'\uB59A' : ['t> j @ tS','떚'],
    u'\uB59B' : ['t> j @ tSh','떛'],
    u'\uB59C' : ['t> j @ kh','떜'],
    u'\uB59D' : ['t> j @ th','떝'],
    u'\uB59E' : ['t> j @ ph','떞'],
    u'\uB59F' : ['t> j @ h','떟'],
    u'\uB5A0' : ['t> ^','떠'],
    u'\uB5A1' : ['t> ^ k','떡'],
    u'\uB5A2' : ['t> ^ k>','떢'],
    u'\uB5A3' : ['t> ^ k sh','떣'],
    u'\uB5A4' : ['t> ^ n','떤'],
    u'\uB5A5' : ['t> ^ n tS','떥'],
    u'\uB5A6' : ['t> ^ n h','떦'],
    u'\uB5A7' : ['t> ^ t','떧'],
    u'\uB5A8' : ['t> ^ l','떨'],
    u'\uB5A9' : ['t> ^ l k','떩'],
    u'\uB5AA' : ['t> ^ l m','떪'],
    u'\uB5AB' : ['t> ^ l p','떫'],
    u'\uB5AC' : ['t> ^ l sh','떬'],
    u'\uB5AD' : ['t> ^ l th','떭'],
    u'\uB5AE' : ['t> ^ l ph','떮'],
    u'\uB5AF' : ['t> ^ l h','떯'],
    u'\uB5B0' : ['t> ^ m','떰'],
    u'\uB5B1' : ['t> ^ p','떱'],
    u'\uB5B2' : ['t> ^ p sh','떲'],
    u'\uB5B3' : ['t> ^ sh','떳'],
    u'\uB5B4' : ['t> ^ s','떴'],
    u'\uB5B5' : ['t> ^ N','떵'],
    u'\uB5B6' : ['t> ^ tS','떶'],
    u'\uB5B7' : ['t> ^ tSh','떷'],
    u'\uB5B8' : ['t> ^ kh','떸'],
    u'\uB5B9' : ['t> ^ th','떹'],
    u'\uB5BA' : ['t> ^ ph','떺'],
    u'\uB5BB' : ['t> ^ h','떻'],
    u'\uB5BC' : ['t> e','떼'],
    u'\uB5BD' : ['t> e k','떽'],
    u'\uB5BE' : ['t> e k>','떾'],
    u'\uB5BF' : ['t> e k sh','떿'],
    u'\uB5C0' : ['t> e n','뗀'],
    u'\uB5C1' : ['t> e n tS','뗁'],
    u'\uB5C2' : ['t> e n h','뗂'],
    u'\uB5C3' : ['t> e t','뗃'],
    u'\uB5C4' : ['t> e l','뗄'],
    u'\uB5C5' : ['t> e l k','뗅'],
    u'\uB5C6' : ['t> e l m','뗆'],
    u'\uB5C7' : ['t> e l p','뗇'],
    u'\uB5C8' : ['t> e l sh','뗈'],
    u'\uB5C9' : ['t> e l th','뗉'],
    u'\uB5CA' : ['t> e l ph','뗊'],
    u'\uB5CB' : ['t> e l h','뗋'],
    u'\uB5CC' : ['t> e m','뗌'],
    u'\uB5CD' : ['t> e p','뗍'],
    u'\uB5CE' : ['t> e p sh','뗎'],
    u'\uB5CF' : ['t> e sh','뗏'],
    u'\uB5D0' : ['t> e s','뗐'],
    u'\uB5D1' : ['t> e N','뗑'],
    u'\uB5D2' : ['t> e tS','뗒'],
    u'\uB5D3' : ['t> e tSh','뗓'],
    u'\uB5D4' : ['t> e kh','뗔'],
    u'\uB5D5' : ['t> e th','뗕'],
    u'\uB5D6' : ['t> e ph','뗖'],
    u'\uB5D7' : ['t> e h','뗗'],
    u'\uB5D8' : ['t> j ^','뗘'],
    u'\uB5D9' : ['t> j ^ k','뗙'],
    u'\uB5DA' : ['t> j ^ k>','뗚'],
    u'\uB5DB' : ['t> j ^ k sh','뗛'],
    u'\uB5DC' : ['t> j ^ n','뗜'],
    u'\uB5DD' : ['t> j ^ n tS','뗝'],
    u'\uB5DE' : ['t> j ^ n h','뗞'],
    u'\uB5DF' : ['t> j ^ t','뗟'],
    u'\uB5E0' : ['t> j ^ l','뗠'],
    u'\uB5E1' : ['t> j ^ l k','뗡'],
    u'\uB5E2' : ['t> j ^ l m','뗢'],
    u'\uB5E3' : ['t> j ^ l p','뗣'],
    u'\uB5E4' : ['t> j ^ l sh','뗤'],
    u'\uB5E5' : ['t> j ^ l th','뗥'],
    u'\uB5E6' : ['t> j ^ l ph','뗦'],
    u'\uB5E7' : ['t> j ^ l h','뗧'],
    u'\uB5E8' : ['t> j ^ m','뗨'],
    u'\uB5E9' : ['t> j ^ p','뗩'],
    u'\uB5EA' : ['t> j ^ p sh','뗪'],
    u'\uB5EB' : ['t> j ^ sh','뗫'],
    u'\uB5EC' : ['t> j ^ s','뗬'],
    u'\uB5ED' : ['t> j ^ N','뗭'],
    u'\uB5EE' : ['t> j ^ tS','뗮'],
    u'\uB5EF' : ['t> j ^ tSh','뗯'],
    u'\uB5F0' : ['t> j ^ kh','뗰'],
    u'\uB5F1' : ['t> j ^ th','뗱'],
    u'\uB5F2' : ['t> j ^ ph','뗲'],
    u'\uB5F3' : ['t> j ^ h','뗳'],
    u'\uB5F4' : ['t> j e','뗴'],
    u'\uB5F5' : ['t> j e k','뗵'],
    u'\uB5F6' : ['t> j e k>','뗶'],
    u'\uB5F7' : ['t> j e k sh','뗷'],
    u'\uB5F8' : ['t> j e n','뗸'],
    u'\uB5F9' : ['t> j e n tS','뗹'],
    u'\uB5FA' : ['t> j e n h','뗺'],
    u'\uB5FB' : ['t> j e t','뗻'],
    u'\uB5FC' : ['t> j e l','뗼'],
    u'\uB5FD' : ['t> j e l k','뗽'],
    u'\uB5FE' : ['t> j e l m','뗾'],
    u'\uB5FF' : ['t> j e l p','뗿'],
    u'\uB600' : ['t> j e l sh','똀'],
    u'\uB601' : ['t> j e l th','똁'],
    u'\uB602' : ['t> j e l ph','똂'],
    u'\uB603' : ['t> j e lh','똃'],
    u'\uB604' : ['t> j e m','똄'],
    u'\uB605' : ['t> j e p','똅'],
    u'\uB606' : ['t> j e p sh','똆'],
    u'\uB607' : ['t> j e sh','똇'],
    u'\uB608' : ['t> j e s','똈'],
    u'\uB609' : ['t> j e N','똉'],
    u'\uB60A' : ['t> j e tS','똊'],
    u'\uB60B' : ['t> j e tSh','똋'],
    u'\uB60C' : ['t> j e kh','똌'],
    u'\uB60D' : ['t> j e th','똍'],
    u'\uB60E' : ['t> j e ph','똎'],
    u'\uB60F' : ['t> j e h','똏'],
    u'\uB610' : ['t> o','또'],
    u'\uB611' : ['t> o k','똑'],
    u'\uB612' : ['t> o k>','똒'],
    u'\uB613' : ['t> o k sh','똓'],
    u'\uB614' : ['t> o n','똔'],
    u'\uB615' : ['t> o n tS','똕'],
    u'\uB616' : ['t> o n h','똖'],
    u'\uB617' : ['t> o t','똗'],
    u'\uB618' : ['t> o l','똘'],
    u'\uB619' : ['t> o l k','똙'],
    u'\uB61A' : ['t> o l m','똚'],
    u'\uB61B' : ['t> o l p','똛'],
    u'\uB61C' : ['t> o l sh','똜'],
    u'\uB61D' : ['t> o l th','똝'],
    u'\uB61E' : ['t> o l ph','똞'],
    u'\uB61F' : ['t> o l h','똟'],
    u'\uB620' : ['t> o m','똠'],
    u'\uB621' : ['t> o p','똡'],
    u'\uB622' : ['t> o p sh','똢'],
    u'\uB623' : ['t> o sh','똣'],
    u'\uB624' : ['t> o s','똤'],
    u'\uB625' : ['t> o N','똥'],
    u'\uB626' : ['t> o tS','똦'],
    u'\uB627' : ['t> o tSh','똧'],
    u'\uB628' : ['t> o kh','똨'],
    u'\uB629' : ['t> o th','똩'],
    u'\uB62A' : ['t> o ph','똪'],
    u'\uB62B' : ['t> o h','똫'],
    u'\uB62C' : ['t> w a','똬'],
    u'\uB62D' : ['t> w a k','똭'],
    u'\uB62E' : ['t> w a k>','똮'],
    u'\uB62F' : ['t> w a k sh','똯'],
    u'\uB630' : ['t> w a n','똰'],
    u'\uB631' : ['t> w a n tS','똱'],
    u'\uB632' : ['t> w a nh','똲'],
    u'\uB633' : ['t> w a t','똳'],
    u'\uB634' : ['t> w a l','똴'],
    u'\uB635' : ['t> w a l k','똵'],
    u'\uB636' : ['t> w a l m','똶'],
    u'\uB637' : ['t> w a l p','똷'],
    u'\uB638' : ['t> w a l sh','똸'],
    u'\uB639' : ['t> w a l th','똹'],
    u'\uB63A' : ['t> w a l ph','똺'],
    u'\uB63B' : ['t> w a l h','똻'],
    u'\uB63C' : ['t> w a m','똼'],
    u'\uB63D' : ['t> w a p','똽'],
    u'\uB63E' : ['t> w a p sh','똾'],
    u'\uB63F' : ['t> w a sh','똿'],
    u'\uB640' : ['t> w a s','뙀'],
    u'\uB641' : ['t> w a N','뙁'],
    u'\uB642' : ['t> w a tS','뙂'],
    u'\uB643' : ['t> w a tSh','뙃'],
    u'\uB644' : ['t> w a kh','뙄'],
    u'\uB645' : ['t> w a th','뙅'],
    u'\uB646' : ['t> w a ph','뙆'],
    u'\uB647' : ['t> w a h','뙇'],
    u'\uB648' : ['t> w @','뙈'],
    u'\uB649' : ['t> w @ k','뙉'],
    u'\uB64A' : ['t> w @ k>','뙊'],
    u'\uB64B' : ['t> w @ k sh','뙋'],
    u'\uB64C' : ['t> w @ n','뙌'],
    u'\uB64D' : ['t> w @ n tS','뙍'],
    u'\uB64E' : ['t> w @ n h','뙎'],
    u'\uB64F' : ['t> w @ t','뙏'],
    u'\uB650' : ['t> w @ l','뙐'],
    u'\uB651' : ['t> w @ l k','뙑'],
    u'\uB652' : ['t> w @ l m','뙒'],
    u'\uB653' : ['t> w @ l p','뙓'],
    u'\uB654' : ['t> w @ l sh','뙔'],
    u'\uB655' : ['t> w @ l th','뙕'],
    u'\uB656' : ['t> w @ l ph','뙖'],
    u'\uB657' : ['t> w @ l h','뙗'],
    u'\uB658' : ['t> w @ m','뙘'],
    u'\uB659' : ['t> w @ p','뙙'],
    u'\uB65A' : ['t> w @ p sh','뙚'],
    u'\uB65B' : ['t> w @ sh','뙛'],
    u'\uB65C' : ['t> w @ s','뙜'],
    u'\uB65D' : ['t> w @ N','뙝'],
    u'\uB65E' : ['t> w @ tS','뙞'],
    u'\uB65F' : ['t> w @ tSh','뙟'],
    u'\uB660' : ['t> w @ kh','뙠'],
    u'\uB661' : ['t> w @ th','뙡'],
    u'\uB662' : ['t> w @ ph','뙢'],
    u'\uB663' : ['t> w @ h','뙣'],
    u'\uB664' : ['t> w e','뙤'],
    u'\uB665' : ['t> w e k','뙥'],
    u'\uB666' : ['t> w e k>','뙦'],
    u'\uB667' : ['t> w e k sh','뙧'],
    u'\uB668' : ['t> w e n','뙨'],
    u'\uB669' : ['t> w e n tS','뙩'],
    u'\uB66A' : ['t> w e n h','뙪'],
    u'\uB66B' : ['t> w e t','뙫'],
    u'\uB66C' : ['t> w e l','뙬'],
    u'\uB66D' : ['t> w e l k','뙭'],
    u'\uB66E' : ['t> w e l m','뙮'],
    u'\uB66F' : ['t> w e l p','뙯'],
    u'\uB670' : ['t> w e l sh','뙰'],
    u'\uB671' : ['t> w e l th','뙱'],
    u'\uB672' : ['t> w e l ph','뙲'],
    u'\uB673' : ['t> w e l h','뙳'],
    u'\uB674' : ['t> w e m','뙴'],
    u'\uB675' : ['t> w e p','뙵'],
    u'\uB676' : ['t> w e p sh','뙶'],
    u'\uB677' : ['t> w e sh','뙷'],
    u'\uB678' : ['t> w e s','뙸'],
    u'\uB679' : ['t> w e N','뙹'],
    u'\uB67A' : ['t> w e tS','뙺'],
    u'\uB67B' : ['t> w e tSh','뙻'],
    u'\uB67C' : ['t> w e kh','뙼'],
    u'\uB67D' : ['t> w e th','뙽'],
    u'\uB67E' : ['t> w e ph','뙾'],
    u'\uB67F' : ['t> w e h','뙿'],
    u'\uB680' : ['t> j o','뚀'],
    u'\uB681' : ['t> j o k','뚁'],
    u'\uB682' : ['t> j o k>','뚂'],
    u'\uB683' : ['t> j o k sh','뚃'],
    u'\uB684' : ['t> j o n','뚄'],
    u'\uB685' : ['t> j o n tS','뚅'],
    u'\uB686' : ['t> j o n h','뚆'],
    u'\uB687' : ['t> j o t','뚇'],
    u'\uB688' : ['t> j o l','뚈'],
    u'\uB689' : ['t> j o l k','뚉'],
    u'\uB68A' : ['t> j o l m','뚊'],
    u'\uB68B' : ['t> j o l p','뚋'],
    u'\uB68C' : ['t> j o l sh','뚌'],
    u'\uB68D' : ['t> j o l th','뚍'],
    u'\uB68E' : ['t> j o l ph','뚎'],
    u'\uB68F' : ['t> j o l h','뚏'],
    u'\uB690' : ['t> j o m','뚐'],
    u'\uB691' : ['t> j o p','뚑'],
    u'\uB692' : ['t> j o p sh','뚒'],
    u'\uB693' : ['t> j o sh','뚓'],
    u'\uB694' : ['t> j o s','뚔'],
    u'\uB695' : ['t> j o N','뚕'],
    u'\uB696' : ['t> j o tS','뚖'],
    u'\uB697' : ['t> j o tSh','뚗'],
    u'\uB698' : ['t> j o kh','뚘'],
    u'\uB699' : ['t> j o th','뚙'],
    u'\uB69A' : ['t> j o ph','뚚'],
    u'\uB69B' : ['t> j o h','뚛'],
    u'\uB69C' : ['t> u','뚜'],
    u'\uB69D' : ['t> u k','뚝'],
    u'\uB69E' : ['t> u k>','뚞'],
    u'\uB69F' : ['t> u k sh','뚟'],
    u'\uB6A0' : ['t> u n','뚠'],
    u'\uB6A1' : ['t> u n tS','뚡'],
    u'\uB6A2' : ['t> u n h','뚢'],
    u'\uB6A3' : ['t> u t','뚣'],
    u'\uB6A4' : ['t> u l','뚤'],
    u'\uB6A5' : ['t> u l k','뚥'],
    u'\uB6A6' : ['t> u l m','뚦'],
    u'\uB6A7' : ['t> u l p','뚧'],
    u'\uB6A8' : ['t> u l sh','뚨'],
    u'\uB6A9' : ['t> u l th','뚩'],
    u'\uB6AA' : ['t> u l ph','뚪'],
    u'\uB6AB' : ['t> u l h','뚫'],
    u'\uB6AC' : ['t> u m','뚬'],
    u'\uB6AD' : ['t> u p','뚭'],
    u'\uB6AE' : ['t> u p sh','뚮'],
    u'\uB6AF' : ['t> u sh','뚯'],
    u'\uB6B0' : ['t> u s','뚰'],
    u'\uB6B1' : ['t> u N','뚱'],
    u'\uB6B2' : ['t> u tS','뚲'],
    u'\uB6B3' : ['t> u tSh','뚳'],
    u'\uB6B4' : ['t> u kh','뚴'],
    u'\uB6B5' : ['t> u th','뚵'],
    u'\uB6B6' : ['t> u ph','뚶'],
    u'\uB6B7' : ['t> u h','뚷'],
    u'\uB6B8' : ['t> w ^','뚸'],
    u'\uB6B9' : ['t> w ^ k','뚹'],
    u'\uB6BA' : ['t> w ^ k>','뚺'],
    u'\uB6BB' : ['t> w ^ k sh','뚻'],
    u'\uB6BC' : ['t> w ^ n','뚼'],
    u'\uB6BD' : ['t> w ^ n tS','뚽'],
    u'\uB6BE' : ['t> w ^ n h','뚾'],
    u'\uB6BF' : ['t> w ^ t','뚿'],
    u'\uB6C0' : ['t> w ^ l','뛀'],
    u'\uB6C1' : ['t> w ^ l k','뛁'],
    u'\uB6C2' : ['t> w ^ l m','뛂'],
    u'\uB6C3' : ['t> w ^ l p','뛃'],
    u'\uB6C4' : ['t> w ^ l sh','뛄'],
    u'\uB6C5' : ['t> w ^ l th','뛅'],
    u'\uB6C6' : ['t> w ^ l ph','뛆'],
    u'\uB6C7' : ['t> w ^ l h','뛇'],
    u'\uB6C8' : ['t> w ^ m','뛈'],
    u'\uB6C9' : ['t> w ^ p','뛉'],
    u'\uB6CA' : ['t> w ^ p sh','뛊'],
    u'\uB6CB' : ['t> w ^ sh','뛋'],
    u'\uB6CC' : ['t> w ^ s','뛌'],
    u'\uB6CD ' : ['t> w ^ N','뛍 '],
    u'\uB6CE' : ['t> w ^ tS','뛎'],
    u'\uB6CF' : ['t> w ^ tSh','뛏'],
    u'\uB6D0' : ['t> w ^ kh','뛐'],
    u'\uB6D1' : ['t> w ^ th','뛑'],
    u'\uB6D2' : ['t> w ^ ph','뛒'],
    u'\uB6D3' : ['t> w ^ h','뛓'],
    u'\uB6D4' : ['t> w E','뛔'],
    u'\uB6D5' : ['t> w E k','뛕'],
    u'\uB6D6' : ['t> w E k>','뛖'],
    u'\uB6D7' : ['t> w E k sh','뛗'],
    u'\uB6D8' : ['t> w E n','뛘'],
    u'\uB6D9' : ['t> w E n tS','뛙'],
    u'\uB6DA' : ['t> w E n h','뛚'],
    u'\uB6DB' : ['t> w E t','뛛'],
    u'\uB6DC' : ['t> w E l','뛜'],
    u'\uB6DD' : ['t> w E l k','뛝'],
    u'\uB6DE' : ['t> w E l m','뛞'],
    u'\uB6DF' : ['t> w E l p','뛟'],
    u'\uB6E0' : ['t> w E l sh','뛠'],
    u'\uB6E1' : ['t> w E l th','뛡'],
    u'\uB6E2' : ['t> w E l ph','뛢'],
    u'\uB6E3' : ['t> w E l h','뛣'],
    u'\uB6E4' : ['t> w E m','뛤'],
    u'\uB6E5' : ['t> w E p','뛥'],
    u'\uB6E6' : ['t> w E p sh','뛦'],
    u'\uB6E7' : ['t> w E sh','뛧'],
    u'\uB6E8' : ['t> w E s','뛨'],
    u'\uB6E9' : ['t> w E N','뛩'],
    u'\uB6EA' : ['t> w E tS','뛪'],
    u'\uB6EB' : ['t> w E tSh','뛫'],
    u'\uB6EC' : ['t> w E kh','뛬'],
    u'\uB6ED' : ['t> w E th','뛭'],
    u'\uB6EE' : ['t> w E ph','뛮'],
    u'\uB6EF' : ['t> w E h','뛯'],
    u'\uB6F0' : ['t> 7','뛰'],
    u'\uB6F1' : ['t> 7 k','뛱'],
    u'\uB6F2' : ['t> 7 k>','뛲'],
    u'\uB6F3' : ['t> 7 k sh','뛳'],
    u'\uB6F4' : ['t> 7 n','뛴'],
    u'\uB6F5' : ['t> 7 n tS','뛵'],
    u'\uB6F6' : ['t> 7 n h','뛶'],
    u'\uB6F7' : ['t> 7 t','뛷'],
    u'\uB6F8' : ['t> 7 l','뛸'],
    u'\uB6F9' : ['t> 7 l k','뛹'],
    u'\uB6FA' : ['t> 7 l m','뛺'],
    u'\uB6FB' : ['t> 7 l p','뛻'],
    u'\uB6FC' : ['t> 7 l sh','뛼'],
    u'\uB6FD' : ['t> 7 l th','뛽'],
    u'\uB6FE' : ['t> 7 l ph','뛾'],
    u'\uB6FF' : ['t> 7 lh','뛿'],
    u'\uB700' : ['t> 7 m','뜀'],
    u'\uB701' : ['t> 7 p','뜁'],
    u'\uB702' : ['t> 7 p sh','뜂'],
    u'\uB703' : ['t> 7 sh','뜃'],
    u'\uB704' : ['t> 7 s','뜄'],
    u'\uB705' : ['t> 7 N','뜅'],
    u'\uB706' : ['t> 7 tS','뜆'],
    u'\uB707' : ['t> 7 tSh','뜇'],
    u'\uB708' : ['t> 7 kh','뜈'],
    u'\uB709' : ['t> 7 th','뜉'],
    u'\uB70A' : ['t> 7 ph','뜊'],
    u'\uB70B' : ['t> 7 h','뜋'],
    u'\uB70C' : ['t> j u','뜌'],
    u'\uB70D' : ['t> j u k','뜍'],
    u'\uB70E' : ['t> j u k>','뜎'],
    u'\uB70F' : ['t> j u k sh','뜏'],
    u'\uB710' : ['t> j u n','뜐'],
    u'\uB711' : ['t> j u n tS','뜑'],
    u'\uB712' : ['t> j u n h','뜒'],
    u'\uB713' : ['t> j u t','뜓'],
    u'\uB714' : ['t> j u l','뜔'],
    u'\uB715' : ['t> j u l k','뜕'],
    u'\uB716' : ['t> j u l m','뜖'],
    u'\uB717' : ['t> j u l p','뜗'],
    u'\uB718' : ['t> j u l sh','뜘'],
    u'\uB719' : ['t> j u l th','뜙'],
    u'\uB71A' : ['t> j u l ph','뜚'],
    u'\uB71B' : ['t> j u l h','뜛'],
    u'\uB71C' : ['t> j u m','뜜'],
    u'\uB71D' : ['t> j u p','뜝'],
    u'\uB71E' : ['t> j u p sh','뜞'],
    u'\uB71F' : ['t> j u sh','뜟'],
    u'\uB720' : ['t> j u s','뜠'],
    u'\uB721' : ['t> j u N','뜡'],
    u'\uB722' : ['t> j u tS','뜢'],
    u'\uB723' : ['t> j u tSh','뜣'],
    u'\uB724' : ['t> j u kh','뜤'],
    u'\uB725' : ['t> j u th','뜥'],
    u'\uB726' : ['t> j u ph','뜦'],
    u'\uB727' : ['t> j u h','뜧'],
    u'\uB728' : ['t> 4','뜨'],
    u'\uB729' : ['t> 4 k','뜩'],
    u'\uB72A' : ['t> 4 k>','뜪'],
    u'\uB72B' : ['t> 4 k sh','뜫'],
    u'\uB72C' : ['t> 4 n','뜬'],
    u'\uB72D' : ['t> 4 n tS','뜭'],
    u'\uB72E' : ['t> 4 n h','뜮'],
    u'\uB72F' : ['t> 4 t','뜯'],
    u'\uB730' : ['t> 4 l','뜰'],
    u'\uB731' : ['t> 4 l k','뜱'],
    u'\uB732' : ['t> 4 l m','뜲'],
    u'\uB733' : ['t> 4 l p','뜳'],
    u'\uB734' : ['t> 4 l sh','뜴'],
    u'\uB735' : ['t> 4 l th','뜵'],
    u'\uB736' : ['t> 4 l ph','뜶'],
    u'\uB737' : ['t> 4 l h','뜷'],
    u'\uB738' : ['t> 4 m','뜸'],
    u'\uB739' : ['t> 4 p','뜹'],
    u'\uB73A' : ['t> 4 p sh','뜺'],
    u'\uB73B' : ['t> 4 sh','뜻'],
    u'\uB73C' : ['t> 4 s','뜼'],
    u'\uB73D' : ['t> 4 N','뜽'],
    u'\uB73E' : ['t> 4 tS','뜾'],
    u'\uB73F' : ['t> 4 tSh','뜿'],
    u'\uB740' : ['t> 4 kh','띀'],
    u'\uB741' : ['t> 4 th','띁'],
    u'\uB742' : ['t> 4 ph','띂'],
    u'\uB743' : ['t> 4 h','띃'],
    u'\uB744' : ['t> 4 j','띄'],
    u'\uB745' : ['t> 4 j k','띅'],
    u'\uB746' : ['t> 4 j k>','띆'],
    u'\uB747' : ['t> 4 j k sh','띇'],
    u'\uB748' : ['t> 4 j n','띈'],
    u'\uB749' : ['t> 4 j n tS','띉'],
    u'\uB74A' : ['t> 4 j n h','띊'],
    u'\uB74B' : ['t> 4 j t','띋'],
    u'\uB74C' : ['t> 4 j l','띌'],
    u'\uB74D' : ['t> 4 j l k','띍'],
    u'\uB74E' : ['t> 4 j l m','띎'],
    u'\uB74F' : ['t> 4 j l p','띏'],
    u'\uB750' : ['t> 4 j l sh','띐'],
    u'\uB751' : ['t> 4 j l th','띑'],
    u'\uB752' : ['t> 4 j l ph','띒'],
    u'\uB753' : ['t> 4 j l h','띓'],
    u'\uB754' : ['t> 4 j m','띔'],
    u'\uB755' : ['t> 4 j p','띕'],
    u'\uB756' : ['t> 4 j p sh','띖'],
    u'\uB757' : ['t> 4 j sh','띗'],
    u'\uB758' : ['t> 4 j s','띘'],
    u'\uB759' : ['t> 4 j N','띙'],
    u'\uB75A' : ['t> 4 j tS','띚'],
    u'\uB75B' : ['t> 4 j tSh','띛'],
    u'\uB75C' : ['t> 4 j kh','띜'],
    u'\uB75D' : ['t> 4 j th','띝'],
    u'\uB75E' : ['t> 4 j ph','띞'],
    u'\uB75F' : ['t> 4 j h','띟'],
    u'\uB760' : ['t> i','띠'],
    u'\uB761' : ['t> i k','띡'],
    u'\uB762' : ['t> i k>','띢'],
    u'\uB763' : ['t> i k sh','띣'],
    u'\uB764' : ['t> i n','띤'],
    u'\uB765' : ['t> i n tS','띥'],
    u'\uB766' : ['t> i n h','띦'],
    u'\uB767' : ['t> i t','띧'],
    u'\uB768' : ['t> i l','띨'],
    u'\uB769' : ['t> i l k','띩'],
    u'\uB76A' : ['t> i l m','띪'],
    u'\uB76B' : ['t> i l p','띫'],
    u'\uB76C' : ['t> i l sh','띬'],
    u'\uB76D' : ['t> i l th','띭'],
    u'\uB76E' : ['t> i l ph','띮'],
    u'\uB76F' : ['t> i l h','띯'],
    u'\uB770' : ['t> i m','띰'],
    u'\uB771' : ['t> i p','띱'],
    u'\uB772' : ['t> i p sh','띲'],
    u'\uB773' : ['t> i sh','띳'],
    u'\uB774' : ['t> i s','띴'],
    u'\uB775' : ['t> i N','띵'],
    u'\uB776' : ['t> i tS','띶'],
    u'\uB777' : ['t> i tSh','띷'],
    u'\uB778' : ['t> i kh','띸'],
    u'\uB779' : ['t> i th','띹'],
    u'\uB77A' : ['t> i ph','띺'],
    u'\uB77B' : ['t> i h','띻'],
    u'\uB77C' : ['r a','라'],
    u'\uB77D' : ['r a k','락'],
    u'\uB77E' : ['r a k>','띾'],
    u'\uB77F' : ['r a k sh','띿'],
    u'\uB780' : ['r a n','란'],
    u'\uB781' : ['r a n tS','랁'],
    u'\uB782' : ['r a n h','랂'],
    u'\uB783' : ['r a t','랃'],
    u'\uB784' : ['r a l','랄'],
    u'\uB785' : ['r a l k','랅'],
    u'\uB786' : ['r a l m','랆'],
    u'\uB787' : ['r a l p','랇'],
    u'\uB788' : ['r a l sh','랈'],
    u'\uB789' : ['r a l th','랉'],
    u'\uB78A' : ['r a l ph','랊'],
    u'\uB78B' : ['r a l h','랋'],
    u'\uB78C' : ['r a m','람'],
    u'\uB78D' : ['r a p','랍'],
    u'\uB78E' : ['r a p sh','랎'],
    u'\uB78F' : ['r a sh','랏'],
    u'\uB790' : ['r a s','랐'],
    u'\uB791' : ['r a N','랑'],
    u'\uB792' : ['r a tS','랒'],
    u'\uB793' : ['r a tSh','랓'],
    u'\uB794' : ['r a kh','랔'],
    u'\uB795' : ['r a th','랕'],
    u'\uB796' : ['r a ph','랖'],
    u'\uB797' : ['r a h','랗'],
    u'\uB798' : ['r @','래'],
    u'\uB799' : ['r @ k','랙'],
    u'\uB79A' : ['r @ k>','랚'],
    u'\uB79B' : ['r @ k sh','랛'],
    u'\uB79C' : ['r @ n','랜'],
    u'\uB79D' : ['r @ n tS','랝'],
    u'\uB79E' : ['r @ n h','랞'],
    u'\uB79F' : ['r @ t','랟'],
    u'\uB7A0' : ['r @ l','랠'],
    u'\uB7A1' : ['r @ l k','랡'],
    u'\uB7A2' : ['r @ l m','랢'],
    u'\uB7A3' : ['r @ l p','랣'],
    u'\uB7A4' : ['r @ l sh','랤'],
    u'\uB7A5' : ['r @ l th','랥'],
    u'\uB7A6' : ['r @ l ph','랦'],
    u'\uB7A7' : ['r @ lh','랧'],
    u'\uB7A8' : ['r @ m','램'],
    u'\uB7A9' : ['r @ p','랩'],
    u'\uB7AA' : ['r @ p sh','랪'],
    u'\uB7AB' : ['r @ sh','랫'],
    u'\uB7AC' : ['r @ s','랬'],
    u'\uB7AD' : ['r @ N','랭'],
    u'\uB7AE' : ['r @ tS','랮'],
    u'\uB7AF' : ['r @ tSh','랯'],
    u'\uB7B0' : ['r @ kh','랰'],
    u'\uB7B1' : ['r @ th','랱'],
    u'\uB7B2' : ['r @ ph','랲'],
    u'\uB7B3' : ['r @ h','랳'],
    u'\uB7B4' : ['r j a','랴'],
    u'\uB7B5' : ['r j a k','략'],
    u'\uB7B6' : ['r j a k>','랶'],
    u'\uB7B7' : ['r j a k sh','랷'],
    u'\uB7B8' : ['r j a n','랸'],
    u'\uB7B9' : ['r j a n tS','랹'],
    u'\uB7BA' : ['r j a n h','랺'],
    u'\uB7BB' : ['r j a t','랻'],
    u'\uB7BC' : ['r j a l','랼'],
    u'\uB7BD' : ['r j a l k','랽'],
    u'\uB7BE' : ['r j a l m','랾'],
    u'\uB7BF' : ['r j a l p','랿'],
    u'\uB7C0' : ['r j a l sh','럀'],
    u'\uB7C1' : ['r j a l th','럁'],
    u'\uB7C2' : ['r j a l ph','럂'],
    u'\uB7C3' : ['r j a l h','럃'],
    u'\uB7C4' : ['r j a m','럄'],
    u'\uB7C5' : ['r j a p','럅'],
    u'\uB7C6' : ['r j a p sh','럆'],
    u'\uB7C7' : ['r j a sh','럇'],
    u'\uB7C8' : ['r j a s','럈'],
    u'\uB7C9' : ['r j a N','량'],
    u'\uB7CA' : ['r j a tS','럊'],
    u'\uB7CB' : ['r j a tSh','럋'],
    u'\uB7CC' : ['r j a kh','럌'],
    u'\uB7CD' : ['r j a th','럍'],
    u'\uB7CE' : ['r j a ph','럎'],
    u'\uB7CF' : ['r j a h','럏'],
    u'\uB7D0' : ['r j @','럐'],
    u'\uB7D1' : ['r j @ k','럑'],
    u'\uB7D2' : ['r j @ k>','럒'],
    u'\uB7D3' : ['r j @ k sh','럓'],
    u'\uB7D4' : ['r j @ n','럔'],
    u'\uB7D5' : ['r j @ n tS','럕'],
    u'\uB7D6' : ['r j @ n h','럖'],
    u'\uB7D7' : ['r j @ t','럗'],
    u'\uB7D8' : ['r j @ l','럘'],
    u'\uB7D9' : ['r j @ l','럙'],
    u'\uB7DA' : ['r j @ l m','럚'],
    u'\uB7DB' : ['r j @ l p','럛'],
    u'\uB7DC' : ['r j @ l sh','럜'],
    u'\uB7DD' : ['r j @ l th','럝'],
    u'\uB7DE' : ['r j @ l ph','럞'],
    u'\uB7DF' : ['r j @ l h','럟'],
    u'\uB7E0' : ['r j @ m','럠'],
    u'\uB7E1' : ['r j @ p','럡'],
    u'\uB7E2' : ['r j @ p sh','럢'],
    u'\uB7E3' : ['r j @ sh','럣'],
    u'\uB7E4' : ['r j @ s','럤'],
    u'\uB7E5' : ['r j @ N','럥'],
    u'\uB7E6' : ['r j @ tS','럦'],
    u'\uB7E7' : ['r j @ tSh','럧'],
    u'\uB7E8' : ['r j @ kh','럨'],
    u'\uB7E9' : ['r j @ th','럩'],
    u'\uB7EA' : ['r j @ ph','럪'],
    u'\uB7EB' : ['r j @ h','럫'],
    u'\uB7EC' : ['r ^','러'],
    u'\uB7ED' : ['r ^ k','럭'],
    u'\uB7EE' : ['r ^ k>','럮'],
    u'\uB7EF' : ['r ^ k sh','럯'],
    u'\uB7F0' : ['r ^ n','런'],
    u'\uB7F1' : ['r ^ n tS','럱'],
    u'\uB7F2' : ['r ^ n h','럲'],
    u'\uB7F3' : ['r ^ t','럳'],
    u'\uB7F4' : ['r ^ l','럴'],
    u'\uB7F5' : ['r ^ l k','럵'],
    u'\uB7F6' : ['r ^ l m','럶'],
    u'\uB7F7' : ['r ^ l p','럷'],
    u'\uB7F8' : ['r ^ l sh','럸'],
    u'\uB7F9' : ['r ^ l th','럹'],
    u'\uB7FA' : ['r ^ l ph','럺'],
    u'\uB7FB' : ['r ^ l h','럻'],
    u'\uB7FC' : ['r ^ m','럼'],
    u'\uB7FD' : ['r ^ p','럽'],
    u'\uB7FE' : ['r ^ p sh','럾'],
    u'\uB7FF' : ['r ^ sh','럿'],
    u'\uB800' : ['r ^ s','렀'],
    u'\uB801' : ['r ^ N','렁'],
    u'\uB802' : ['r ^ tS','렂'],
    u'\uB803' : ['r ^ tSh','렃'],
    u'\uB804' : ['r ^ kh','렄'],
    u'\uB805' : ['r ^ th','렅'],
    u'\uB806' : ['r ^ ph','렆'],
    u'\uB807' : ['r ^ h','렇'],
    u'\uB808' : ['r e','레'],
    u'\uB809' : ['r e k','렉'],
    u'\uB80A' : ['r e k>','렊'],
    u'\uB80B' : ['r e k sh','렋'],
    u'\uB80C' : ['r e n','렌'],
    u'\uB80D' : ['r e n tS','렍'],
    u'\uB80E' : ['r e n h','렎'],
    u'\uB80F' : ['r e t','렏'],
    u'\uB810' : ['r e l','렐'],
    u'\uB811' : ['r e l k','렑'],
    u'\uB812' : ['r e l m','렒'],
    u'\uB813' : ['r e l p','렓'],
    u'\uB814' : ['r e l sh','렔'],
    u'\uB815' : ['r e l th','렕'],
    u'\uB816' : ['r e l ph','렖'],
    u'\uB817' : ['r e l h','렗'],
    u'\uB818' : ['r e m','렘'],
    u'\uB819' : ['r e p','렙'],
    u'\uB81A' : ['r e p sh','렚'],
    u'\uB81B' : ['r e sh','렛'],
    u'\uB81C' : ['r e s','렜'],
    u'\uB81D' : ['r e N','렝'],
    u'\uB81E' : ['r e tS','렞'],
    u'\uB81F' : ['r e tSh','렟'],
    u'\uB820' : ['r e kh','렠'],
    u'\uB821' : ['r e th','렡'],
    u'\uB822' : ['r e ph','렢'],
    u'\uB823' : ['r e h','렣'],
    u'\uB824' : ['r j ^','려'],
    u'\uB825' : ['r j ^ k','력'],
    u'\uB826' : ['r j ^ k>','렦'],
    u'\uB827' : ['r j ^ k sh','렧'],
    u'\uB828' : ['r j ^ n','련'],
    u'\uB829' : ['r j ^ n tS','렩'],
    u'\uB82A' : ['r j ^ n h','렪'],
    u'\uB82B' : ['r j ^ t','렫'],
    u'\uB82C' : ['r j ^ l','렬'],
    u'\uB82D' : ['r j ^ l k','렭'],
    u'\uB82E' : ['r j ^ l m','렮'],
    u'\uB82F' : ['r j ^ l p','렯'],
    u'\uB830' : ['r j ^ l sh','렰'],
    u'\uB831' : ['r j ^ l th','렱'],
    u'\uB832' : ['r j ^ l ph','렲'],
    u'\uB833' : ['r j ^ l h','렳'],
    u'\uB834' : ['r j ^ m','렴'],
    u'\uB835' : ['r j ^ p','렵'],
    u'\uB836' : ['r j ^ p sh','렶'],
    u'\uB837' : ['r j ^ sh','렷'],
    u'\uB838' : ['r j ^ s','렸'],
    u'\uB839' : ['r j ^ N','령'],
    u'\uB83A' : ['r j ^ tS','렺'],
    u'\uB83B' : ['r j ^ tSh','렻'],
    u'\uB83C' : ['r j ^ kh','렼'],
    u'\uB83D' : ['r j ^ th','렽'],
    u'\uB83E' : ['r j ^ ph','렾'],
    u'\uB83F' : ['r j ^ h','렿'],
    u'\uB840' : ['r j e','례'],
    u'\uB841' : ['r j e k','롁'],
    u'\uB842' : ['r j e k>','롂'],
    u'\uB843' : ['r j e k sh','롃'],
    u'\uB844' : ['r j e n','롄'],
    u'\uB845' : ['r j e n tS','롅'],
    u'\uB846' : ['r j e n h','롆'],
    u'\uB847' : ['r j e t','롇'],
    u'\uB848' : ['r j e l','롈'],
    u'\uB849' : ['r j e l k','롉'],
    u'\uB84A' : ['r j e l m','롊'],
    u'\uB84B' : ['r j e l p','롋'],
    u'\uB84C' : ['r j e l sh','롌'],
    u'\uB84D' : ['r j e l th','롍'],
    u'\uB84E' : ['r j e l ph','롎'],
    u'\uB84F' : ['r j e l h','롏'],
    u'\uB850' : ['r j e m','롐'],
    u'\uB851' : ['r j e p','롑'],
    u'\uB852' : ['r j e p sh','롒'],
    u'\uB853' : ['r j e sh','롓'],
    u'\uB854' : ['r j e s','롔'],
    u'\uB855' : ['r j e N','롕'],
    u'\uB856' : ['r j e tS','롖'],
    u'\uB857' : ['r j e tSh','롗'],
    u'\uB858' : ['r j e kh','롘'],
    u'\uB859' : ['r j e th','롙'],
    u'\uB85A' : ['r j e ph','롚'],
    u'\uB85B' : ['r j e h','롛'],
    u'\uB85C' : ['r o','로'],
    u'\uB85D' : ['r o k','록'],
    u'\uB85E' : ['r o k>','롞'],
    u'\uB85F' : ['r o k sh','롟'],
    u'\uB860' : ['r o n','론'],
    u'\uB861' : ['r o n tS','롡'],
    u'\uB862' : ['r o n h','롢'],
    u'\uB863' : ['r o t','롣'],
    u'\uB864' : ['r o l','롤'],
    u'\uB865' : ['r o l k','롥'],
    u'\uB866' : ['r o l m','롦'],
    u'\uB867' : ['r o l p','롧'],
    u'\uB868' : ['r o l sh','롨'],
    u'\uB869' : ['r o l th','롩'],
    u'\uB86A' : ['r o l ph','롪'],
    u'\uB86B' : ['r o l h','롫'],
    u'\uB86C' : ['r o m','롬'],
    u'\uB86D' : ['r o p','롭'],
    u'\uB86E' : ['r o p sh','롮'],
    u'\uB86F' : ['r o sh','롯'],
    u'\uB870' : ['r o s','롰'],
    u'\uB871' : ['r o N','롱'],
    u'\uB872' : ['r o tS','롲'],
    u'\uB873' : ['r o tSh','롳'],
    u'\uB874' : ['r o kh','롴'],
    u'\uB875' : ['r o th','롵'],
    u'\uB876' : ['r o ph','롶'],
    u'\uB877' : ['r o h','롷'],
    u'\uB878' : ['r w a','롸'],
    u'\uB879' : ['r w a k','롹'],
    u'\uB87A' : ['r w a k>','롺'],
    u'\uB87B' : ['r w a k sh','롻'],
    u'\uB87C' : ['r w a n','롼'],
    u'\uB87D' : ['r w a n tS','롽'],
    u'\uB87E' : ['r w a n h','롾'],
    u'\uB87F' : ['r w a t','롿'],
    u'\uB880' : ['r w a l','뢀'],
    u'\uB881' : ['r w a l k','뢁'],
    u'\uB882' : ['r w a l m','뢂'],
    u'\uB883' : ['r w a l p','뢃'],
    u'\uB884' : ['r w a l sh','뢄'],
    u'\uB885' : ['r w a l th','뢅'],
    u'\uB886' : ['r w a l ph','뢆'],
    u'\uB887' : ['r w a l h','뢇'],
    u'\uB888' : ['r w a m','뢈'],
    u'\uB889' : ['r w a p','뢉'],
    u'\uB88A' : ['r w a p sh','뢊'],
    u'\uB88B' : ['r w a sh','뢋'],
    u'\uB88C' : ['r w a s','뢌'],
    u'\uB88D' : ['r w a N','뢍'],
    u'\uB88E' : ['r w a tS','뢎'],
    u'\uB88F' : ['r w a tSh','뢏'],
    u'\uB890' : ['r w a kh','뢐'],
    u'\uB891' : ['r w a th','뢑'],
    u'\uB892' : ['r w a ph','뢒'],
    u'\uB893' : ['r w a h','뢓'],
    u'\uB894' : ['r w @','뢔'],
    u'\uB895' : ['r w @ k','뢕'],
    u'\uB896' : ['r w @ k>','뢖'],
    u'\uB897' : ['r w @ k sh','뢗'],
    u'\uB898' : ['r w @ n','뢘'],
    u'\uB899' : ['r w @ n tS','뢙'],
    u'\uB89A' : ['r w @ n h','뢚'],
    u'\uB89B' : ['r w @ t','뢛'],
    u'\uB89C' : ['r w @ l','뢜'],
    u'\uB89D' : ['r w @ l k','뢝'],
    u'\uB89E' : ['r w @ l m','뢞'],
    u'\uB89F' : ['r w @ l p','뢟'],
    u'\uB8A0' : ['r w @ l sh','뢠'],
    u'\uB8A1' : ['r w @ l th','뢡'],
    u'\uB8A2' : ['r w @ l ph','뢢'],
    u'\uB8A3' : ['r w @ l h','뢣'],
    u'\uB8A4' : ['r w @ m','뢤'],
    u'\uB8A5' : ['r w @ p','뢥'],
    u'\uB8A6' : ['r w @ p sh','뢦'],
    u'\uB8A7' : ['r w @ sh','뢧'],
    u'\uB8A8' : ['r w @ s','뢨'],
    u'\uB8A9' : ['r w @ N','뢩'],
    u'\uB8AA' : ['r w @ tS','뢪'],
    u'\uB8AB' : ['r w @ tSh','뢫'],
    u'\uB8AC' : ['r w @ kh','뢬'],
    u'\uB8AD' : ['r w @ th','뢭'],
    u'\uB8AE' : ['r w @ ph','뢮'],
    u'\uB8AF' : ['r w @ h','뢯'],
    u'\uB8B0' : ['r w e','뢰'],
    u'\uB8B1' : ['r w e k','뢱'],
    u'\uB8B2' : ['r w e k>','뢲'],
    u'\uB8B3' : ['r w e k sh','뢳'],
    u'\uB8B4' : ['r w e n','뢴'],
    u'\uB8B5' : ['r w e n tS','뢵'],
    u'\uB8B6' : ['r w e n h','뢶'],
    u'\uB8B7' : ['r w e t','뢷'],
    u'\uB8B8' : ['r w e l','뢸'],
    u'\uB8B9' : ['r w e l k','뢹'],
    u'\uB8BA' : ['r w e l m','뢺'],
    u'\uB8BB' : ['r w e l p','뢻'],
    u'\uB8BC' : ['r w e l sh','뢼'],
    u'\uB8BD' : ['r w e l th','뢽'],
    u'\uB8BE' : ['r w e l ph','뢾'],
    u'\uB8BF' : ['r w e l h','뢿'],
    u'\uB8C0' : ['r w e m','룀'],
    u'\uB8C1' : ['r w e p','룁'],
    u'\uB8C2' : ['r w e p sh','룂'],
    u'\uB8C3' : ['r w e sh','룃'],
    u'\uB8C4' : ['r w e s','룄'],
    u'\uB8C5' : ['r w e N','룅'],
    u'\uB8C6' : ['r w e tS','룆'],
    u'\uB8C7' : ['r w e tSh','룇'],
    u'\uB8C8' : ['r w e kh','룈'],
    u'\uB8C9' : ['r w e th','룉'],
    u'\uB8CA' : ['r w e ph','룊'],
    u'\uB8CB' : ['r w e h','룋'],
    u'\uB8CC' : ['r j o','료'],
    u'\uB8CD' : ['r j o k','룍'],
    u'\uB8CE' : ['r j o k>','룎'],
    u'\uB8CF' : ['r j o k sh','룏'],
    u'\uB8D0' : ['r j o n','룐'],
    u'\uB8D1' : ['r j o n tS','룑'],
    u'\uB8D2' : ['r j o n h','룒'],
    u'\uB8D3' : ['r j o t','룓'],
    u'\uB8D4' : ['r j o l','룔'],
    u'\uB8D5' : ['r j o l k','룕'],
    u'\uB8D6' : ['r j o l m','룖'],
    u'\uB8D7' : ['r j o l p','룗'],
    u'\uB8D8' : ['r j o l sh','룘'],
    u'\uB8D9' : ['r j o l th','룙'],
    u'\uB8DA' : ['r j o l ph','룚'],
    u'\uB8DB' : ['r j o l h','룛'],
    u'\uB8DC' : ['r j o m','룜'],
    u'\uB8DD' : ['r j o p','룝'],
    u'\uB8DE' : ['r j o p sh','룞'],
    u'\uB8DF' : ['r j o sh','룟'],
    u'\uB8E0' : ['r j o s','룠'],
    u'\uB8E1' : ['r j o N','룡'],
    u'\uB8E2' : ['r j o tS','룢'],
    u'\uB8E3' : ['r j o tSh','룣'],
    u'\uB8E4' : ['r j o kh','룤'],
    u'\uB8E5' : ['r j o th','룥'],
    u'\uB8E6' : ['r j o ph','룦'],
    u'\uB8E7' : ['r j o h','룧'],
    u'\uB8E8' : ['r u','루'],
    u'\uB8E9' : ['r u k','룩'],
    u'\uB8EA' : ['r u k>','룪'],
    u'\uB8EB' : ['r u k sh','룫'],
    u'\uB8EC' : ['r u n','룬'],
    u'\uB8ED' : ['r u n tS','룭'],
    u'\uB8EE' : ['r u n h','룮'],
    u'\uB8EF' : ['r u t','룯'],
    u'\uB8F0' : ['r u l','룰'],
    u'\uB8F1' : ['r u l k','룱'],
    u'\uB8F2' : ['r u l m','룲'],
    u'\uB8F3' : ['r u l p','룳'],
    u'\uB8F4' : ['r u l sh','룴'],
    u'\uB8F5' : ['r u l th','룵'],
    u'\uB8F6' : ['r u l ph','룶'],
    u'\uB8F7' : ['r u l h','룷'],
    u'\uB8F8' : ['r u m','룸'],
    u'\uB8F9' : ['r u p','룹'],
    u'\uB8FA' : ['r u p sh','룺'],
    u'\uB8FB' : ['r u sh','룻'],
    u'\uB8FC' : ['r u s','룼'],
    u'\uB8FD' : ['r u N','룽'],
    u'\uB8FE' : ['r u tS','룾'],
    u'\uB8FF' : ['r u tSh','룿'],
    u'\uB900' : ['r u kh','뤀'],
    u'\uB901' : ['r u th','뤁'],
    u'\uB902' : ['r u ph','뤂'],
    u'\uB903' : ['r u h','뤃'],
    u'\uB904' : ['r w ^','뤄'],
    u'\uB905' : ['r w ^ k','뤅'],
    u'\uB906' : ['r w ^ k>','뤆'],
    u'\uB907' : ['r w ^ k sh','뤇'],
    u'\uB908' : ['r w ^ n','뤈'],
    u'\uB909' : ['r w ^ n tS','뤉'],
    u'\uB90A' : ['r w ^ n h','뤊'],
    u'\uB90B' : ['r w ^ t','뤋'],
    u'\uB90C' : ['r w ^ l','뤌'],
    u'\uB90D' : ['r w ^ l k','뤍'],
    u'\uB90E' : ['r w ^ l m','뤎'],
    u'\uB90F' : ['r w ^ l p','뤏'],
    u'\uB910' : ['r w ^ l sh','뤐'],
    u'\uB911' : ['r w ^ l th','뤑'],
    u'\uB912' : ['r w ^ l ph','뤒'],
    u'\uB913' : ['r w ^ l h','뤓'],
    u'\uB914' : ['r w ^ m','뤔'],
    u'\uB915' : ['r w ^ p','뤕'],
    u'\uB916' : ['r w ^ p sh','뤖'],
    u'\uB917' : ['r w ^ sh','뤗'],
    u'\uB918' : ['r w ^ s','뤘'],
    u'\uB919' : ['r w ^ N','뤙'],
    u'\uB91A' : ['r w ^ tS','뤚'],
    u'\uB91B' : ['r w ^ tSh','뤛'],
    u'\uB91C' : ['r w ^ kh','뤜'],
    u'\uB91D' : ['r w ^ th','뤝'],
    u'\uB91E' : ['r w ^ ph','뤞'],
    u'\uB91F' : ['r w ^ h','뤟'],
    u'\uB920' : ['r w E','뤠'],
    u'\uB921' : ['r w E k','뤡'],
    u'\uB922' : ['r w E k>','뤢'],
    u'\uB923' : ['r w E k sh','뤣'],
    u'\uB924' : ['r w E n','뤤'],
    u'\uB925' : ['r w E n tS','뤥'],
    u'\uB926' : ['r w E n h','뤦'],
    u'\uB927' : ['r w E t','뤧'],
    u'\uB928' : ['r w E l','뤨'],
    u'\uB929' : ['r w E l k','뤩'],
    u'\uB92A' : ['r w E l m','뤪'],
    u'\uB92B' : ['r w E l p','뤫'],
    u'\uB92C' : ['r w E l sh','뤬'],
    u'\uB92D' : ['r w E l th','뤭'],
    u'\uB92E' : ['r w E l ph','뤮'],
    u'\uB92F' : ['r w E l h','뤯'],
    u'\uB930' : ['r w E m','뤰'],
    u'\uB931' : ['r w E p','뤱'],
    u'\uB932' : ['r w E p sh','뤲'],
    u'\uB933' : ['r w E sh','뤳'],
    u'\uB934' : ['r w E s','뤴'],
    u'\uB935' : ['r w E N','뤵'],
    u'\uB936' : ['r w E tS','뤶'],
    u'\uB937' : ['r w E tSh','뤷'],
    u'\uB938' : ['r w E kh','뤸'],
    u'\uB939' : ['r w E th','뤹'],
    u'\uB93A' : ['r w E ph','뤺'],
    u'\uB93B' : ['r w E h','뤻'],
    u'\uB93C' : ['r 7','뤼'],
    u'\uB93D' : ['r 7 k','뤽'],
    u'\uB93E' : ['r 7 k>','뤾'],
    u'\uB93F' : ['r 7 k sh','뤿'],
    u'\uB940' : ['r 7 n','륀'],
    u'\uB941' : ['r 7 n tS','륁'],
    u'\uB942' : ['r 7 n h','륂'],
    u'\uB943' : ['r 7 t','륃'],
    u'\uB944' : ['r 7 l','륄'],
    u'\uB945' : ['r 7 l k','륅'],
    u'\uB946' : ['r 7 l m','륆'],
    u'\uB947' : ['r 7 l p','륇'],
    u'\uB948' : ['r 7 l sh','륈'],
    u'\uB949' : ['r 7 l th','륉'],
    u'\uB94A' : ['r 7 l ph','륊'],
    u'\uB94B' : ['r 7 l h','륋'],
    u'\uB94C' : ['r 7 m','륌'],
    u'\uB94D' : ['r 7 p','륍'],
    u'\uB94E' : ['r 7 p sh','륎'],
    u'\uB94F' : ['r 7 sh','륏'],
    u'\uB950' : ['r 7 s','륐'],
    u'\uB951' : ['r 7 N','륑'],
    u'\uB952' : ['r 7 tS','륒'],
    u'\uB953' : ['r 7 tSh','륓'],
    u'\uB954' : ['r 7 kh','륔'],
    u'\uB955' : ['r 7 th','륕'],
    u'\uB956' : ['r 7 ph','륖'],
    u'\uB957' : ['r 7 h','륗'],
    u'\uB958' : ['r j u','류'],
    u'\uB959' : ['r j u k','륙'],
    u'\uB95A' : ['r j u k>','륚'],
    u'\uB95B' : ['r j u k sh','륛'],
    u'\uB95C' : ['r j u k n','륜'],
    u'\uB95D' : ['r j u n tS','륝'],
    u'\uB95E' : ['r j u n h','륞'],
    u'\uB95F' : ['r j u t','륟'],
    u'\uB960' : ['r j u l','률'],
    u'\uB961' : ['r j u l k','륡'],
    u'\uB962' : ['r j u l m','륢'],
    u'\uB963' : ['r j u l p','륣'],
    u'\uB964' : ['r j u l sh','륤'],
    u'\uB965' : ['r j u l th','륥'],
    u'\uB966' : ['r j u l ph','륦'],
    u'\uB967' : ['r j u l h','륧'],
    u'\uB968' : ['r j u m','륨'],
    u'\uB969' : ['r j u p','륩'],
    u'\uB96A' : ['r j u p sh','륪'],
    u'\uB96B' : ['r j u sh','륫'],
    u'\uB96C' : ['r j u s','륬'],
    u'\uB96D' : ['r j u N','륭'],
    u'\uB96E' : ['r j u tS','륮'],
    u'\uB96F' : ['r j u tSh','륯'],
    u'\uB970' : ['r j u kh','륰'],
    u'\uB971' : ['r j u th','륱'],
    u'\uB972' : ['r j u ph','륲'],
    u'\uB973' : ['r j u h','륳'],
    u'\uB974' : ['r 4','르'],
    u'\uB975' : ['r 4 k','륵'],
    u'\uB976' : ['r 4 k>','륶'],
    u'\uB977' : ['r 4 k sh','륷'],
    u'\uB978' : ['r 4 n','른'],
    u'\uB979' : ['r 4 n tS','륹'],
    u'\uB97A' : ['r 4 n h','륺'],
    u'\uB97B' : ['r 4 t','륻'],
    u'\uB97C' : ['r 4 l','를'],
    u'\uB97D' : ['r 4 l k','륽'],
    u'\uB97E' : ['r 4 l m','륾'],
    u'\uB97F' : ['r 4 l p','륿'],
    u'\uB980' : ['r 4 l sh','릀'],
    u'\uB981' : ['r 4 l th','릁'],
    u'\uB982' : ['r 4 l ph','릂'],
    u'\uB983' : ['r 4 l h','릃'],
    u'\uB984' : ['r 4 m','름'],
    u'\uB985' : ['r 4 p','릅'],
    u'\uB986' : ['r 4 p sh','릆'],
    u'\uB987' : ['r 4 sh','릇'],
    u'\uB988' : ['r 4 s','릈'],
    u'\uB989' : ['r 4 N','릉'],
    u'\uB98A' : ['r 4 tS','릊'],
    u'\uB98B' : ['r 4 tSh','릋'],
    u'\uB98C' : ['r 4 kh','릌'],
    u'\uB98D' : ['r 4 th','릍'],
    u'\uB98E' : ['r 4 ph','릎'],
    u'\uB98F' : ['r 4 h','릏'],
    u'\uB990' : ['r 4 j','릐'],
    u'\uB991' : ['r 4 j k','릑'],
    u'\uB992' : ['r 4 j k>','릒'],
    u'\uB993' : ['r 4 j k sh','릓'],
    u'\uB994' : ['r 4 j n','릔'],
    u'\uB995' : ['r 4 j n tS','릕'],
    u'\uB996' : ['r 4 j n h','릖'],
    u'\uB997' : ['r 4 j t','릗'],
    u'\uB998' : ['r 4 j l','릘'],
    u'\uB999' : ['r 4 j l k','릙'],
    u'\uB99A' : ['r 4 j l m','릚'],
    u'\uB99B' : ['r 4 j l p','릛'],
    u'\uB99C' : ['r 4 j l sh','릜'],
    u'\uB99D' : ['r 4 j l th','릝'],
    u'\uB99E' : ['r 4 j l ph','릞'],
    u'\uB99F' : ['r 4 j l h','릟'],
    u'\uB9A0' : ['r 4 j m','릠'],
    u'\uB9A1' : ['r 4 j p','릡'],
    u'\uB9A2' : ['r 4 j p sh','릢'],
    u'\uB9A3' : ['r 4 j sh','릣'],
    u'\uB9A4' : ['r 4 j s','릤'],
    u'\uB9A5' : ['r 4 j N','릥'],
    u'\uB9A6' : ['r 4 j tS','릦'],
    u'\uB9A7' : ['r 4 j tSh','릧'],
    u'\uB9A8' : ['r 4 j kh','릨'],
    u'\uB9A9' : ['r 4 j th','릩'],
    u'\uB9AA' : ['r 4 j ph','릪'],
    u'\uB9AB' : ['r 4 j h','릫'],
    u'\uB9AC' : ['r i','리'],
    u'\uB9AD' : ['r i k','릭'],
    u'\uB9AE' : ['r i k>','릮'],
    u'\uB9AF' : ['r i k sh','릯'],
    u'\uB9B0' : ['r i n','린'],
    u'\uB9B1' : ['r i n tS','릱'],
    u'\uB9B2' : ['r i n h','릲'],
    u'\uB9B3' : ['r i t','릳'],
    u'\uB9B4' : ['r i l','릴'],
    u'\uB9B5' : ['r i l k','릵'],
    u'\uB9B6' : ['r i l m','릶'],
    u'\uB9B7' : ['r i l p','릷'],
    u'\uB9B8' : ['r i l sh','릸'],
    u'\uB9B9' : ['r i l th','릹'],
    u'\uB9BA' : ['r i l ph','릺'],
    u'\uB9BB' : ['r i l h','릻'],
    u'\uB9BC' : ['r i m','림'],
    u'\uB9BD' : ['r i p','립'],
    u'\uB9BE' : ['r i p sh','릾'],
    u'\uB9BF' : ['r i sh','릿'],
    u'\uB9C0' : ['r i s','맀'],
    u'\uB9C1' : ['r i N','링'],
    u'\uB9C2' : ['r i tS','맂'],
    u'\uB9C3' : ['r i tSh','맃'],
    u'\uB9C4' : ['r i kh','맄'],
    u'\uB9C5' : ['r i th','맅'],
    u'\uB9C6' : ['r i ph','맆'],
    u'\uB9C7' : ['r i h','맇'],
    u'\uB9C8' : ['m a','마'],
    u'\uB9C9' : ['m a k','막'],
    u'\uB9CA' : ['m a k>','맊'],
    u'\uB9CB' : ['m a k sh','맋'],
    u'\uB9CC' : ['m a n','만'],
    u'\uB9CD' : ['m a n tS','맍'],
    u'\uB9CE' : ['m a n h','많'],
    u'\uB9CF' : ['m a t','맏'],
    u'\uB9D0' : ['m a l','말'],
    u'\uB9D1' : ['m a l k','맑'],
    u'\uB9D2' : ['m a l m','맒'],
    u'\uB9D3' : ['m a l p','맓'],
    u'\uB9D4' : ['m a l sh','맔'],
    u'\uB9D5' : ['m a l th','맕'],
    u'\uB9D6' : ['m a l ph','맖'],
    u'\uB9D7' : ['m a l h','맗'],
    u'\uB9D8' : ['m a m','맘'],
    u'\uB9D9' : ['m a p','맙'],
    u'\uB9DA' : ['m a p sh','맚'],
    u'\uB9DB' : ['m a sh','맛'],
    u'\uB9DC' : ['m a s','맜'],
    u'\uB9DD' : ['m a N','망'],
    u'\uB9DE' : ['m a tS','맞'],
    u'\uB9DF' : ['m a tSh','맟'],
    u'\uB9E0' : ['m a kh','맠'],
    u'\uB9E1' : ['m a th','맡'],
    u'\uB9E2' : ['m a ph','맢'],
    u'\uB9E3' : ['m a h','맣'],
    u'\uB9E4' : ['m @','매'],
    u'\uB9E5' : ['m @ k','맥'],
    u'\uB9E6' : ['m @ k>','맦'],
    u'\uB9E7' : ['m @ k sh','맧'],
    u'\uB9E8' : ['m @ n','맨'],
    u'\uB9E9' : ['m @ n tS','맩'],
    u'\uB9EA' : ['m @ n h','맪'],
    u'\uB9EB' : ['m @ t','맫'],
    u'\uB9EC' : ['m @ l','맬'],
    u'\uB9ED' : ['m @ l k','맭'],
    u'\uB9EE' : ['m @ l m','맮'],
    u'\uB9EF' : ['m @ l p','맯'],
    u'\uB9F0' : ['m @ l sh','맰'],
    u'\uB9F1' : ['m @ l th','맱'],
    u'\uB9F2' : ['m @ l ph','맲'],
    u'\uB9F3' : ['m @ l h','맳'],
    u'\uB9F4' : ['m @ m','맴'],
    u'\uB9F5' : ['m @ p','맵'],
    u'\uB9F6' : ['m @ p sh','맶'],
    u'\uB9F7' : ['m @ sh','맷'],
    u'\uB9F8' : ['m @ s','맸'],
    u'\uB9F9' : ['m @ N','맹'],
    u'\uB9FA' : ['m @ tS','맺'],
    u'\uB9FB' : ['m @ tSh','맻'],
    u'\uB9FC' : ['m @ kh','맼'],
    u'\uB9FD' : ['m @ th','맽'],
    u'\uB9FE' : ['m @ ph','맾'],
    u'\uB9FF' : ['m @ h','맿'],
    u'\uBA00' : ['m j a','먀'],
    u'\uBA01' : ['m j a k','먁'],
    u'\uBA02' : ['m j a k>','먂'],
    u'\uBA03' : ['m j a k sh','먃'],
    u'\uBA04' : ['m j a n','먄'],
    u'\uBA05' : ['m j a n tS','먅'],
    u'\uBA06' : ['m j a n h','먆'],
    u'\uBA07' : ['m j a t','먇'],
    u'\uBA08' : ['m j a l','먈'],
    u'\uBA09' : ['m j a l k','먉'],
    u'\uBA0A' : ['m j a l m','먊'],
    u'\uBA0B' : ['m j a l p','먋'],
    u'\uBA0C' : ['m j a l sh','먌'],
    u'\uBA0D' : ['m j a l th','먍'],
    u'\uBA0E' : ['m j a l ph','먎'],
    u'\uBA0F' : ['m j a l h','먏'],
    u'\uBA10' : ['m j a m','먐'],
    u'\uBA11' : ['m j a p','먑'],
    u'\uBA12' : ['m j a p sh','먒'],
    u'\uBA13' : ['m j a sh','먓'],
    u'\uBA14' : ['m j a s','먔'],
    u'\uBA15' : ['m j a N','먕'],
    u'\uBA16' : ['m j a tS','먖'],
    u'\uBA17' : ['m j a tSh','먗'],
    u'\uBA18' : ['m j a kh','먘'],
    u'\uBA19' : ['m j a th','먙'],
    u'\uBA1A' : ['m j a ph','먚'],
    u'\uBA1B' : ['m j a h','먛'],
    u'\uBA1C' : ['m j @','먜'],
    u'\uBA1D' : ['m j @ k','먝'],
    u'\uBA1E' : ['m j @ k>','먞'],
    u'\uBA1F' : ['m j @ k sh','먟'],
    u'\uBA20' : ['m j @ n','먠'],
    u'\uBA21' : ['m j @ n tS','먡'],
    u'\uBA22' : ['m j @ n h','먢'],
    u'\uBA23' : ['m j @ t','먣'],
    u'\uBA24' : ['m j @ l','먤'],
    u'\uBA25' : ['m j @ l k','먥'],
    u'\uBA26' : ['m j @ l m','먦'],
    u'\uBA27' : ['m j @ l p','먧'],
    u'\uBA28' : ['m j @ l sh','먨'],
    u'\uBA29' : ['m j @ l th','먩'],
    u'\uBA2A' : ['m j @ l ph','먪'],
    u'\uBA2B' : ['m j @ l h','먫'],
    u'\uBA2C' : ['m j @ m','먬'],
    u'\uBA2D' : ['m j @ p','먭'],
    u'\uBA2E' : ['m j @ p sh','먮'],
    u'\uBA2F' : ['m j @ sh','먯'],
    u'\uBA30' : ['m j @ s','먰'],
    u'\uBA31' : ['m j @ N','먱'],
    u'\uBA32' : ['m j @ tS','먲'],
    u'\uBA33' : ['m j @ tSh','먳'],
    u'\uBA34' : ['m j @ kh','먴'],
    u'\uBA35' : ['m j @ th','먵'],
    u'\uBA36' : ['m j @ ph','먶'],
    u'\uBA37' : ['m j @ h','먷'],
    u'\uBA38' : ['m ^','머'],
    u'\uBA39' : ['m ^ k','먹'],
    u'\uBA3A' : ['m ^ k>','먺'],
    u'\uBA3B' : ['m ^ k sh','먻'],
    u'\uBA3C' : ['m ^ n','먼'],
    u'\uBA3D' : ['m ^ n tS','먽'],
    u'\uBA3E' : ['m ^ n h','먾'],
    u'\uBA3F' : ['m ^ t','먿'],
    u'\uBA40' : ['m ^ l','멀'],
    u'\uBA41' : ['m ^ l k','멁'],
    u'\uBA42' : ['m ^ l m','멂'],
    u'\uBA43' : ['m ^ l p','멃'],
    u'\uBA44' : ['m ^ l sh','멄'],
    u'\uBA45' : ['m ^ l th','멅'],
    u'\uBA46' : ['m ^ l ph','멆'],
    u'\uBA47' : ['m ^ l h','멇'],
    u'\uBA48' : ['m ^ m','멈'],
    u'\uBA49' : ['m ^ p','멉'],
    u'\uBA4A' : ['m ^ p sh','멊'],
    u'\uBA4B' : ['m ^ sh','멋'],
    u'\uBA4C' : ['m ^ s','멌'],
    u'\uBA4D' : ['m ^ N','멍'],
    u'\uBA4E' : ['m ^ tS','멎'],
    u'\uBA4F' : ['m ^ tSh','멏'],
    u'\uBA50' : ['m ^ kh','멐'],
    u'\uBA51' : ['m ^ th','멑'],
    u'\uBA52' : ['m ^ ph','멒'],
    u'\uBA53' : ['m ^ h','멓'],
    u'\uBA54' : ['m e','메'],
    u'\uBA55' : ['m e k','멕'],
    u'\uBA56' : ['m e k>','멖'],
    u'\uBA57' : ['m e k sh','멗'],
    u'\uBA58' : ['m e n','멘'],
    u'\uBA59' : ['m e n tS','멙'],
    u'\uBA5A' : ['m e n h','멚'],
    u'\uBA5B' : ['m e t','멛'],
    u'\uBA5C' : ['m e l','멜'],
    u'\uBA5D' : ['m e l k','멝'],
    u'\uBA5E' : ['m e l m','멞'],
    u'\uBA5F' : ['m e l p','멟'],
    u'\uBA60' : ['m e l sh','멠'],
    u'\uBA61' : ['m e l th','멡'],
    u'\uBA62' : ['m e l ph','멢'],
    u'\uBA63' : ['m e l h','멣'],
    u'\uBA64' : ['m e m','멤'],
    u'\uBA65' : ['m e p','멥'],
    u'\uBA66' : ['m e p sh','멦'],
    u'\uBA67' : ['m e sh','멧'],
    u'\uBA68' : ['m e s','멨'],
    u'\uBA69' : ['m e N','멩'],
    u'\uBA6A' : ['m e tS','멪'],
    u'\uBA6B' : ['m e tSh','멫'],
    u'\uBA6C' : ['m e kh','멬'],
    u'\uBA6D' : ['m e th','멭'],
    u'\uBA6E' : ['m e ph','멮'],
    u'\uBA6F' : ['m e h','멯'],
    u'\uBA70' : ['m j ^','며'],
    u'\uBA71' : ['m j ^ k','멱'],
    u'\uBA72' : ['m j ^ k>','멲'],
    u'\uBA73' : ['m j ^ k sh','멳'],
    u'\uBA74' : ['m j ^ n','면'],
    u'\uBA75' : ['m j ^ n tS','멵'],
    u'\uBA76' : ['m j ^ n h','멶'],
    u'\uBA77' : ['m j ^ t','멷'],
    u'\uBA78' : ['m j ^ l','멸'],
    u'\uBA79' : ['m j ^ l k','멹'],
    u'\uBA7A' : ['m j ^ l m','멺'],
    u'\uBA7B' : ['m j ^ l p','멻'],
    u'\uBA7C' : ['m j ^ l sh','멼'],
    u'\uBA7D' : ['m j ^ l th','멽'],
    u'\uBA7E' : ['m j ^ l ph','멾'],
    u'\uBA7F' : ['m j ^ l h','멿'],
    u'\uBA80' : ['m j ^ m','몀'],
    u'\uBA81' : ['m j ^ p','몁'],
    u'\uBA82' : ['m j ^ p sh','몂'],
    u'\uBA83' : ['m j ^ sh','몃'],
    u'\uBA84' : ['m j ^ s','몄'],
    u'\uBA85' : ['m j ^ N','명'],
    u'\uBA86' : ['m j ^ tS','몆'],
    u'\uBA87' : ['m j ^ tSh','몇'],
    u'\uBA88' : ['m j ^ kh','몈'],
    u'\uBA89' : ['m j ^ th','몉'],
    u'\uBA8A' : ['m j ^ ph','몊'],
    u'\uBA8B' : ['m j ^ h','몋'],
    u'\uBA8C' : ['m j e','몌'],
    u'\uBA8D' : ['m j e k','몍'],
    u'\uBA8E' : ['m j e k>','몎'],
    u'\uBA8F' : ['m j e k sh','몏'],
    u'\uBA90' : ['m j e n','몐'],
    u'\uBA91' : ['m j e n tS','몑'],
    u'\uBA92' : ['m j e n h','몒'],
    u'\uBA93' : ['m j e t','몓'],
    u'\uBA94' : ['m j e l','몔'],
    u'\uBA95' : ['m j e l k','몕'],
    u'\uBA96' : ['m j e l m','몖'],
    u'\uBA97' : ['m j e l p','몗'],
    u'\uBA98' : ['m j e l sh','몘'],
    u'\uBA99' : ['m j e l th','몙'],
    u'\uBA9A' : ['m j e l ph','몚'],
    u'\uBA9B' : ['m j e l h','몛'],
    u'\uBA9C' : ['m j e m','몜'],
    u'\uBA9D' : ['m j e p','몝'],
    u'\uBA9E' : ['m j e p sh','몞'],
    u'\uBA9F' : ['m j e sh','몟'],
    u'\uBAA0' : ['m j e s','몠'],
    u'\uBAA1' : ['m j e N','몡'],
    u'\uBAA2' : ['m j e tS','몢'],
    u'\uBAA3' : ['m j e tSh','몣'],
    u'\uBAA4' : ['m j e kh','몤'],
    u'\uBAA5' : ['m j e th','몥'],
    u'\uBAA6' : ['m j e ph','몦'],
    u'\uBAA7' : ['m j e h','몧'],
    u'\uBAA8' : ['m o','모'],
    u'\uBAA9' : ['m o k','목'],
    u'\uBAAA' : ['m o k>','몪'],
    u'\uBAAB' : ['m o k sh','몫'],
    u'\uBAAC' : ['m o n','몬'],
    u'\uBAAD' : ['m o n tS','몭'],
    u'\uBAAE' : ['m o n h','몮'],
    u'\uBAAF' : ['m o t','몯'],
    u'\uBAB0' : ['m o l','몰'],
    u'\uBAB1' : ['m o l k','몱'],
    u'\uBAB2' : ['m o l m','몲'],
    u'\uBAB3' : ['m o l p','몳'],
    u'\uBAB4' : ['m o l sh','몴'],
    u'\uBAB5' : ['m o l th','몵'],
    u'\uBAB6' : ['m o l ph','몶'],
    u'\uBAB7' : ['m o l h','몷'],
    u'\uBAB8' : ['m o m','몸'],
    u'\uBAB9' : ['m o p','몹'],
    u'\uBABA' : ['m o p sh','몺'],
    u'\uBABB' : ['m o sh','못'],
    u'\uBABC' : ['m o s','몼'],
    u'\uBABD' : ['m o N','몽'],
    u'\uBABE' : ['m o tS','몾'],
    u'\uBABF' : ['m o tSh','몿'],
    u'\uBAC0' : ['m o kh','뫀'],
    u'\uBAC1' : ['m o th','뫁'],
    u'\uBAC2' : ['m o ph','뫂'],
    u'\uBAC3' : ['m o h','뫃'],
    u'\uBAC4' : ['m w a','뫄'],
    u'\uBAC5' : ['m w a k','뫅'],
    u'\uBAC6' : ['m w a k>','뫆'],
    u'\uBAC7' : ['m w a k sh','뫇'],
    u'\uBAC8' : ['m w a n','뫈'],
    u'\uBAC9' : ['m w a n tS','뫉'],
    u'\uBACA' : ['m w a n h','뫊'],
    u'\uBACB' : ['m w a t','뫋'],
    u'\uBACC' : ['m w a l','뫌'],
    u'\uBACD' : ['m w a l k','뫍'],
    u'\uBACE' : ['m w a l m','뫎'],
    u'\uBACF' : ['m w a l p','뫏'],
    u'\uBAD0' : ['m w a l sh','뫐'],
    u'\uBAD1' : ['m w a l th','뫑'],
    u'\uBAD2' : ['m w a l ph','뫒'],
    u'\uBAD3' : ['m w a l h','뫓'],
    u'\uBAD4' : ['m w a m','뫔'],
    u'\uBAD5' : ['m w a p','뫕'],
    u'\uBAD6' : ['m w a p sh','뫖'],
    u'\uBAD7' : ['m w a sh','뫗'],
    u'\uBAD8' : ['m w a s','뫘'],
    u'\uBAD9' : ['m w a N','뫙'],
    u'\uBADA' : ['m w a tS','뫚'],
    u'\uBADB' : ['m w a tSh','뫛'],
    u'\uBADC' : ['m w a kh','뫜'],
    u'\uBADD' : ['m w a th','뫝'],
    u'\uBADE' : ['m w a ph','뫞'],
    u'\uBADF' : ['m w a h','뫟'],
    u'\uBAE0' : ['m w @','뫠'],
    u'\uBAE1' : ['m w @ k','뫡'],
    u'\uBAE2' : ['m w @ k>','뫢'],
    u'\uBAE3' : ['m w @ k sh','뫣'],
    u'\uBAE4' : ['m w @ n','뫤'],
    u'\uBAE5' : ['m w @ n tS','뫥'],
    u'\uBAE6' : ['m w @ n h','뫦'],
    u'\uBAE7' : ['m w @ t','뫧'],
    u'\uBAE8' : ['m w @ l','뫨'],
    u'\uBAE9' : ['m w @ l k','뫩'],
    u'\uBAEA' : ['m w @ l m','뫪'],
    u'\uBAEB' : ['m w @ l p','뫫'],
    u'\uBAEC' : ['m w @ l sh','뫬'],
    u'\uBAED' : ['m w @ l th','뫭'],
    u'\uBAEE' : ['m w @ l ph','뫮'],
    u'\uBAEF' : ['m w @ l h','뫯'],
    u'\uBAF0' : ['m w @ m','뫰'],
    u'\uBAF1' : ['m w @ p','뫱'],
    u'\uBAF2' : ['m w @ p sh','뫲'],
    u'\uBAF3' : ['m w @ sh','뫳'],
    u'\uBAF4' : ['m w @ s','뫴'],
    u'\uBAF5' : ['m w @ N','뫵'],
    u'\uBAF6' : ['m w @ tS','뫶'],
    u'\uBAF7' : ['m w @ tSh','뫷'],
    u'\uBAF8' : ['m w @ kh','뫸'],
    u'\uBAF9' : ['m w @ th','뫹'],
    u'\uBAFA' : ['m w @ ph','뫺'],
    u'\uBAFB' : ['m w @ h','뫻'],
    u'\uBAFC' : ['m w e','뫼'],
    u'\uBAFD' : ['m w e k','뫽'],
    u'\uBAFE' : ['m w e k>','뫾'],
    u'\uBAFF' : ['m w e k sh','뫿'],
    u'\uBB00' : ['m w e n','묀'],
    u'\uBB01' : ['m w e n tS','묁'],
    u'\uBB02' : ['m w e n h','묂'],
    u'\uBB03' : ['m w e t','묃'],
    u'\uBB04' : ['m w e l','묄'],
    u'\uBB05' : ['m w e l k','묅'],
    u'\uBB06' : ['m w e l m','묆'],
    u'\uBB07' : ['m w e l p','묇'],
    u'\uBB08' : ['m w e l sh','묈'],
    u'\uBB09' : ['m w e l th','묉'],
    u'\uBB0A' : ['m w e l ph','묊'],
    u'\uBB0B' : ['m w e l h','묋'],
    u'\uBB0C' : ['m w e m','묌'],
    u'\uBB0D' : ['m w e p','묍'],
    u'\uBB0E' : ['m w e p sh','묎'],
    u'\uBB0F' : ['m w e sh','묏'],
    u'\uBB10' : ['m w e s','묐'],
    u'\uBB11' : ['m w e N','묑'],
    u'\uBB12' : ['m w e tS','묒'],
    u'\uBB13' : ['m w e tSh','묓'],
    u'\uBB14' : ['m w e kh','묔'],
    u'\uBB15' : ['m w e th','묕'],
    u'\uBB16' : ['m w e ph','묖'],
    u'\uBB17' : ['m w e h','묗'],
    u'\uBB18' : ['m j o','묘'],
    u'\uBB19' : ['m j o k','묙'],
    u'\uBB1A' : ['m j o k>','묚'],
    u'\uBB1B' : ['m j o k sh','묛'],
    u'\uBB1C' : ['m j o n','묜'],
    u'\uBB1D' : ['m j o n tS','묝'],
    u'\uBB1E' : ['m j o n h','묞'],
    u'\uBB1F' : ['m j o t','묟'],
    u'\uBB20' : ['m j o l','묠'],
    u'\uBB21' : ['m j o l k','묡'],
    u'\uBB22' : ['m j o l m','묢'],
    u'\uBB23' : ['m j o l p','묣'],
    u'\uBB24' : ['m j o l sh','묤'],
    u'\uBB25' : ['m j o l th','묥'],
    u'\uBB26' : ['m j o l ph','묦'],
    u'\uBB27' : ['m j o l h','묧'],
    u'\uBB28' : ['m j o m','묨'],
    u'\uBB29' : ['m j o p','묩'],
    u'\uBB2A' : ['m j o p sh','묪'],
    u'\uBB2B' : ['m j o sh','묫'],
    u'\uBB2C' : ['m j o s','묬'],
    u'\uBB2D' : ['m j o N','묭'],
    u'\uBB2E' : ['m j o tS','묮'],
    u'\uBB2F' : ['m j o tSh','묯'],
    u'\uBB30' : ['m j o kh','묰'],
    u'\uBB31' : ['m j o th','묱'],
    u'\uBB32' : ['m j o ph','묲'],
    u'\uBB33' : ['m j o h','묳'],
    u'\uBB34' : ['m u','무'],
    u'\uBB35' : ['m u k','묵'],
    u'\uBB36' : ['m u k>','묶'],
    u'\uBB37' : ['m u k sh','묷'],
    u'\uBB38' : ['m u n','문'],
    u'\uBB39' : ['m u n tS','묹'],
    u'\uBB3A' : ['m u n h','묺'],
    u'\uBB3B' : ['m u t','묻'],
    u'\uBB3C' : ['m u l','물'],
    u'\uBB3D' : ['m u l k','묽'],
    u'\uBB3E' : ['m u l m','묾'],
    u'\uBB3F' : ['m u l p','묿'],
    u'\uBB40' : ['m u l sh','뭀'],
    u'\uBB41' : ['m u l th','뭁'],
    u'\uBB42' : ['m u l ph','뭂'],
    u'\uBB43' : ['m u l h','뭃'],
    u'\uBB44' : ['m u m','뭄'],
    u'\uBB45' : ['m u p','뭅'],
    u'\uBB46' : ['m u p sh','뭆'],
    u'\uBB47' : ['m u sh','뭇'],
    u'\uBB48' : ['m u s','뭈'],
    u'\uBB49' : ['m u N','뭉'],
    u'\uBB4A' : ['m u tS','뭊'],
    u'\uBB4B' : ['m u tSh','뭋'],
    u'\uBB4C' : ['m u kh','뭌'],
    u'\uBB4D' : ['m u th','뭍'],
    u'\uBB4E' : ['m u ph','뭎'],
    u'\uBB4F' : ['m u h','뭏'],
    u'\uBB50' : ['m w ^','뭐'],
    u'\uBB51' : ['m w ^ k','뭑'],
    u'\uBB52' : ['m w ^ k>','뭒'],
    u'\uBB53' : ['m w ^ k sh','뭓'],
    u'\uBB54' : ['m w ^ n','뭔'],
    u'\uBB55' : ['m w ^ n tS','뭕'],
    u'\uBB56' : ['m w ^ n h','뭖'],
    u'\uBB57' : ['m w ^ t','뭗'],
    u'\uBB58' : ['m w ^ l','뭘'],
    u'\uBB59' : ['m w ^ l k','뭙'],
    u'\uBB5A' : ['m w ^ l m','뭚'],
    u'\uBB5B' : ['m w ^ l p','뭛'],
    u'\uBB5C' : ['m w ^ l sh','뭜'],
    u'\uBB5D' : ['m w ^ l th','뭝'],
    u'\uBB5E' : ['m w ^ l ph','뭞'],
    u'\uBB5F' : ['m w ^ l h','뭟'],
    u'\uBB60' : ['m w ^ m','뭠'],
    u'\uBB61' : ['m w ^ p','뭡'],
    u'\uBB62' : ['m w ^ p sh','뭢'],
    u'\uBB63' : ['m w ^ sh','뭣'],
    u'\uBB64' : ['m w ^ s','뭤'],
    u'\uBB65' : ['m w ^ N','뭥'],
    u'\uBB66' : ['m w ^ tS','뭦'],
    u'\uBB67' : ['m w ^ tSh','뭧'],
    u'\uBB68' : ['m w ^ kh','뭨'],
    u'\uBB69' : ['m w ^ th','뭩'],
    u'\uBB6A' : ['m w ^ ph','뭪'],
    u'\uBB6B' : ['m w ^ h','뭫'],
    u'\uBB6C' : ['m w E','뭬'],
    u'\uBB6D' : ['m w E k','뭭'],
    u'\uBB6E' : ['m w E k>','뭮'],
    u'\uBB6F' : ['m w E k sh','뭯'],
    u'\uBB70' : ['m w E n','뭰'],
    u'\uBB71' : ['m w E n tS','뭱'],
    u'\uBB72' : ['m w E n h','뭲'],
    u'\uBB73' : ['m w E t','뭳'],
    u'\uBB74' : ['m w E l','뭴'],
    u'\uBB75' : ['m w E l k','뭵'],
    u'\uBB76' : ['m w E l m','뭶'],
    u'\uBB77' : ['m w E l p','뭷'],
    u'\uBB78' : ['m w E l sh','뭸'],
    u'\uBB79' : ['m w E l th','뭹'],
    u'\uBB7A' : ['m w E l ph','뭺'],
    u'\uBB7B' : ['m w E l h','뭻'],
    u'\uBB7C' : ['m w E m','뭼'],
    u'\uBB7D' : ['m w E p','뭽'],
    u'\uBB7E' : ['m w E p sh','뭾'],
    u'\uBB7F' : ['m w E sh','뭿'],
    u'\uBB80' : ['m w E s','뮀'],
    u'\uBB81' : ['m w E N','뮁'],
    u'\uBB82' : ['m w E tS','뮂'],
    u'\uBB83' : ['m w E tSh','뮃'],
    u'\uBB84' : ['m w E kh','뮄'],
    u'\uBB85' : ['m w E th','뮅'],
    u'\uBB86' : ['m w E ph','뮆'],
    u'\uBB87' : ['m w E h','뮇'],
    u'\uBB88' : ['m 7','뮈'],
    u'\uBB89' : ['m 7 k','뮉'],
    u'\uBB8A' : ['m 7 k>','뮊'],
    u'\uBB8B' : ['m 7 k sh','뮋'],
    u'\uBB8C' : ['m 7 n','뮌'],
    u'\uBB8D' : ['m 7 n tS','뮍'],
    u'\uBB8E' : ['m 7 n h','뮎'],
    u'\uBB8F' : ['m 7 t','뮏'],
    u'\uBB90' : ['m 7 l','뮐'],
    u'\uBB91' : ['m 7 l k','뮑'],
    u'\uBB92' : ['m 7 l m','뮒'],
    u'\uBB93' : ['m 7 l p','뮓'],
    u'\uBB94' : ['m 7 l sh','뮔'],
    u'\uBB95' : ['m 7 l th','뮕'],
    u'\uBB96' : ['m 7 l ph','뮖'],
    u'\uBB97' : ['m 7 l h','뮗'],
    u'\uBB98' : ['m 7 m','뮘'],
    u'\uBB99' : ['m 7 p','뮙'],
    u'\uBB9A' : ['m 7 p sh','뮚'],
    u'\uBB9B' : ['m 7 sh','뮛'],
    u'\uBB9C' : ['m 7 s','뮜'],
    u'\uBB9D' : ['m 7 N','뮝'],
    u'\uBB9E' : ['m 7 tS','뮞'],
    u'\uBB9F' : ['m 7 tSh','뮟'],
    u'\uBBA0' : ['m 7 kh','뮠'],
    u'\uBBA1' : ['m 7 th','뮡'],
    u'\uBBA2' : ['m 7 ph','뮢'],
    u'\uBBA3' : ['m 7 h','뮣'],
    u'\uBBA4' : ['m j u','뮤'],
    u'\uBBA5' : ['m j u k','뮥'],
    u'\uBBA6' : ['m j u k>','뮦'],
    u'\uBBA7' : ['m j u k sh','뮧'],
    u'\uBBA8' : ['m j u n','뮨'],
    u'\uBBA9' : ['m j u n tS','뮩'],
    u'\uBBAA' : ['m j u n h','뮪'],
    u'\uBBAB' : ['m j u t','뮫'],
    u'\uBBAC' : ['m j u l','뮬'],
    u'\uBBAD' : ['m j u l k','뮭'],
    u'\uBBAE' : ['m j u l m','뮮'],
    u'\uBBAF' : ['m j u l p','뮯'],
    u'\uBBB0' : ['m j u l sh','뮰'],
    u'\uBBB1' : ['m j u l th','뮱'],
    u'\uBBB2' : ['m j u l ph','뮲'],
    u'\uBBB3' : ['m j u l h','뮳'],
    u'\uBBB4' : ['m j u m','뮴'],
    u'\uBBB5' : ['m j u p','뮵'],
    u'\uBBB6' : ['m j u p sh','뮶'],
    u'\uBBB7' : ['m j u sh','뮷'],
    u'\uBBB8' : ['m j u s','뮸'],
    u'\uBBB9' : ['m j u N','뮹'],
    u'\uBBBA' : ['m j u tS','뮺'],
    u'\uBBBB' : ['m j u tSh','뮻'],
    u'\uBBBC' : ['m j u kh','뮼'],
    u'\uBBBD' : ['m j u th','뮽'],
    u'\uBBBE' : ['m j u ph','뮾'],
    u'\uBBBF' : ['m j u h','뮿'],
    u'\uBBC0' : ['m 4','므'],
    u'\uBBC1' : ['m 4 k','믁'],
    u'\uBBC2' : ['m 4 k>','믂'],
    u'\uBBC3' : ['m 4 k sh','믃'],
    u'\uBBC4' : ['m 4 n','믄'],
    u'\uBBC5' : ['m 4 n tS','믅'],
    u'\uBBC6' : ['m 4 n h','믆'],
    u'\uBBC7' : ['m 4 t','믇'],
    u'\uBBC8' : ['m 4 l','믈'],
    u'\uBBC9' : ['m 4 l k','믉'],
    u'\uBBCA' : ['m 4 l m','믊'],
    u'\uBBCB' : ['m 4 l p','믋'],
    u'\uBBCC' : ['m 4 l sh','믌'],
    u'\uBBCD' : ['m 4 l th','믍'],
    u'\uBBCE' : ['m 4 l ph','믎'],
    u'\uBBCF' : ['m 4 l h','믏'],
    u'\uBBD0' : ['m 4 m','믐'],
    u'\uBBD1' : ['m 4 p','믑'],
    u'\uBBD2' : ['m 4 p sh','믒'],
    u'\uBBD3' : ['m 4 sh','믓'],
    u'\uBBD4' : ['m 4 s','믔'],
    u'\uBBD5' : ['m 4 N','믕'],
    u'\uBBD6' : ['m 4 tS','믖'],
    u'\uBBD7' : ['m 4 tSh','믗'],
    u'\uBBD8' : ['m 4 kh','믘'],
    u'\uBBD9' : ['m 4 th','믙'],
    u'\uBBDA' : ['m 4 ph','믚'],
    u'\uBBDB' : ['m 4 h','믛'],
    u'\uBBDC' : ['m 4 j','믜'],
    u'\uBBDD' : ['m 4 j k','믝'],
    u'\uBBDE' : ['m 4 j k>','믞'],
    u'\uBBDF' : ['m 4 j k sh','믟'],
    u'\uBBE0' : ['m 4 j n','믠'],
    u'\uBBE1' : ['m 4 j n tS','믡'],
    u'\uBBE2' : ['m 4 j n h','믢'],
    u'\uBBE3' : ['m 4 j t','믣'],
    u'\uBBE4' : ['m 4 j l','믤'],
    u'\uBBE5' : ['m 4 j l k','믥'],
    u'\uBBE6' : ['m 4 j l m','믦'],
    u'\uBBE7' : ['m 4 j l p','믧'],
    u'\uBBE8' : ['m 4 j l sh','믨'],
    u'\uBBE9' : ['m 4 j l th','믩'],
    u'\uBBEA' : ['m 4 j l ph','믪'],
    u'\uBBEB' : ['m 4 j l h','믫'],
    u'\uBBEC' : ['m 4 j m','믬'],
    u'\uBBED' : ['m 4 j p','믭'],
    u'\uBBEE' : ['m 4 j p sh','믮'],
    u'\uBBEF' : ['m 4 j sh','믯'],
    u'\uBBF0' : ['m 4 j s','믰'],
    u'\uBBF1' : ['m 4 j N','믱'],
    u'\uBBF2' : ['m 4 j tS','믲'],
    u'\uBBF3' : ['m 4 j tSh','믳'],
    u'\uBBF4' : ['m 4 j kh','믴'],
    u'\uBBF5' : ['m 4 j th','믵'],
    u'\uBBF6' : ['m 4 j ph','믶'],
    u'\uBBF7' : ['m 4 j h','믷'],
    u'\uBBF8' : ['m i','미'],
    u'\uBBF9' : ['m i k','믹'],
    u'\uBBFA' : ['m i k>','믺'],
    u'\uBBFB' : ['m i k sh','믻'],
    u'\uBBFC' : ['m i n','민'],
    u'\uBBFD' : ['m i n tS','믽'],
    u'\uBBFE' : ['m i n h','믾'],
    u'\uBBFF' : ['m i t','믿'],
    u'\uBC00' : ['m i l','밀'],
    u'\uBC01' : ['m i l k','밁'],
    u'\uBC02' : ['m i l m','밂'],
    u'\uBC03' : ['m i l p','밃'],
    u'\uBC04' : ['m i l sh','밄'],
    u'\uBC05' : ['m i l th','밅'],
    u'\uBC06' : ['m i l ph','밆'],
    u'\uBC07' : ['m i l h','밇'],
    u'\uBC08' : ['m i m','밈'],
    u'\uBC09' : ['m i p','밉'],
    u'\uBC0A' : ['m i p sh','밊'],
    u'\uBC0B' : ['m i sh','밋'],
    u'\uBC0C' : ['m i s','밌'],
    u'\uBC0D' : ['m i N','밍'],
    u'\uBC0E' : ['m i tS','밎'],
    u'\uBC0F' : ['m i tSh','및'],
    u'\uBC10' : ['m i kh','밐'],
    u'\uBC11' : ['m i th','밑'],
    u'\uBC12' : ['m i ph','밒'],
    u'\uBC13' : ['m i h','밓'],
    u'\uBC14' : ['p a','바'],
    u'\uBC15' : ['p a k','박'],
    u'\uBC16' : ['p a k>','밖'],
    u'\uBC17' : ['p a k sh','밗'],
    u'\uBC18' : ['p a n','반'],
    u'\uBC19' : ['p a n tS','밙'],
    u'\uBC1A' : ['p a n h','밚'],
    u'\uBC1B' : ['p a t','받'],
    u'\uBC1C' : ['p a l','발'],
    u'\uBC1D' : ['p a l k','밝'],
    u'\uBC1E' : ['p a l m','밞'],
    u'\uBC1F' : ['p a l p','밟'],
    u'\uBC20' : ['p a l sh','밠'],
    u'\uBC21' : ['p a l th','밡'],
    u'\uBC22' : ['p a l ph','밢'],
    u'\uBC23' : ['p a l h','밣'],
    u'\uBC24' : ['p a m','밤'],
    u'\uBC25' : ['p a p','밥'],
    u'\uBC26' : ['p a p sh','밦'],
    u'\uBC27' : ['p a sh','밧'],
    u'\uBC28' : ['p a s','밨'],
    u'\uBC29' : ['p a N','방'],
    u'\uBC2A' : ['p a tS','밪'],
    u'\uBC2B' : ['p a tSh','밫'],
    u'\uBC2C' : ['p a kh','밬'],
    u'\uBC2D' : ['p a th','밭'],
    u'\uBC2E' : ['p a ph','밮'],
    u'\uBC2F' : ['p a h','밯'],
    u'\uBC30' : ['p @','배'],
    u'\uBC31' : ['p @ k','백'],
    u'\uBC32' : ['p @ k>','밲'],
    u'\uBC33' : ['p @ k sh','밳'],
    u'\uBC34' : ['p @ n','밴'],
    u'\uBC35' : ['p @ n tS','밵'],
    u'\uBC36' : ['p @ n h','밶'],
    u'\uBC37' : ['p @ t','밷'],
    u'\uBC38' : ['p @ l','밸'],
    u'\uBC39' : ['p @ l k','밹'],
    u'\uBC3A' : ['p @ l m','밺'],
    u'\uBC3B' : ['p @ l p','밻'],
    u'\uBC3C' : ['p @ l sh','밼'],
    u'\uBC3D' : ['p @ l th','밽'],
    u'\uBC3E' : ['p @ l ph','밾'],
    u'\uBC3F' : ['p @ l h','밿'],
    u'\uBC40' : ['p @ m','뱀'],
    u'\uBC41' : ['p @ p','뱁'],
    u'\uBC42' : ['p @ p sh','뱂'],
    u'\uBC43' : ['p @ sh','뱃'],
    u'\uBC44' : ['p @ s','뱄'],
    u'\uBC45' : ['p @ N','뱅'],
    u'\uBC46' : ['p @ tS','뱆'],
    u'\uBC47' : ['p @ tSh','뱇'],
    u'\uBC48' : ['p @ kh','뱈'],
    u'\uBC49' : ['p @ th','뱉'],
    u'\uBC4A' : ['p @ ph','뱊'],
    u'\uBC4B' : ['p @ h','뱋'],
    u'\uBC4C' : ['p j a','뱌'],
    u'\uBC4D' : ['p j a k','뱍'],
    u'\uBC4E' : ['p j a k>','뱎'],
    u'\uBC4F' : ['p j a k sh','뱏'],
    u'\uBC50' : ['p j a n','뱐'],
    u'\uBC51' : ['p j a n tS','뱑'],
    u'\uBC52' : ['p j a n h','뱒'],
    u'\uBC53' : ['p j a t','뱓'],
    u'\uBC54' : ['p j a l','뱔'],
    u'\uBC55' : ['p j a l k','뱕'],
    u'\uBC56' : ['p j a l m','뱖'],
    u'\uBC57' : ['p j a l p','뱗'],
    u'\uBC58' : ['p j a l sh','뱘'],
    u'\uBC59' : ['p j a l th','뱙'],
    u'\uBC5A' : ['p j a l ph','뱚'],
    u'\uBC5B' : ['p j a lh','뱛'],
    u'\uBC5C' : ['p j a m','뱜'],
    u'\uBC5D' : ['p j a p','뱝'],
    u'\uBC5E' : ['p j a p sh','뱞'],
    u'\uBC5F' : ['p j a sh','뱟'],
    u'\uBC60' : ['p j a s','뱠'],
    u'\uBC61' : ['p j a N','뱡'],
    u'\uBC62' : ['p j a tS','뱢'],
    u'\uBC63' : ['p j a tSh','뱣'],
    u'\uBC64' : ['p j a kh','뱤'],
    u'\uBC65' : ['p j a th','뱥'],
    u'\uBC66' : ['p j a ph','뱦'],
    u'\uBC67' : ['p j a h','뱧'],
    u'\uBC68' : ['p j @','뱨'],
    u'\uBC69' : ['p j @ k','뱩'],
    u'\uBC6A' : ['p j @ k>','뱪'],
    u'\uBC6B' : ['p j @ k sh','뱫'],
    u'\uBC6C' : ['p j @ n','뱬'],
    u'\uBC6D' : ['p j @ n tS','뱭'],
    u'\uBC6E' : ['p j @ n h','뱮'],
    u'\uBC6F' : ['p j @ t','뱯'],
    u'\uBC70' : ['p j @ l','뱰'],
    u'\uBC71' : ['p j @ l k','뱱'],
    u'\uBC72' : ['p j @ l m','뱲'],
    u'\uBC73' : ['p j @ l p','뱳'],
    u'\uBC74' : ['p j @ l sh','뱴'],
    u'\uBC75' : ['p j @ l th','뱵'],
    u'\uBC76' : ['p j @ l ph','뱶'],
    u'\uBC77' : ['p j @ l h','뱷'],
    u'\uBC78' : ['p j @ m','뱸'],
    u'\uBC79' : ['p j @ p','뱹'],
    u'\uBC7A' : ['p j @ p sh','뱺'],
    u'\uBC7B' : ['p j @ sh','뱻'],
    u'\uBC7C' : ['p j @ s','뱼'],
    u'\uBC7D' : ['p j @ N','뱽'],
    u'\uBC7E' : ['p j @ tS','뱾'],
    u'\uBC7F' : ['p j @ tSh','뱿'],
    u'\uBC80' : ['p j @ kh','벀'],
    u'\uBC81' : ['p j @ th','벁'],
    u'\uBC82' : ['p j @ ph','벂'],
    u'\uBC83' : ['p j @ h','벃'],
    u'\uBC84' : ['p ^','버'],
    u'\uBC85' : ['p ^ k','벅'],
    u'\uBC86' : ['p ^ k>','벆'],
    u'\uBC87' : ['p ^ k sh','벇'],
    u'\uBC88' : ['p ^ n','번'],
    u'\uBC89' : ['p ^ n tS','벉'],
    u'\uBC8A' : ['p ^ n h','벊'],
    u'\uBC8B' : ['p ^ t','벋'],
    u'\uBC8C' : ['p ^ l','벌'],
    u'\uBC8D' : ['p ^ l k','벍'],
    u'\uBC8E' : ['p ^ l m','벎'],
    u'\uBC8F' : ['p ^ l p','벏'],
    u'\uBC90' : ['p ^ l sh','벐'],
    u'\uBC91' : ['p ^ l th','벑'],
    u'\uBC92' : ['p ^ l ph','벒'],
    u'\uBC93' : ['p ^ l h','벓'],
    u'\uBC94' : ['p ^ m','범'],
    u'\uBC95' : ['p ^ p','법'],
    u'\uBC96' : ['p ^ p sh','벖'],
    u'\uBC97' : ['p ^ sh','벗'],
    u'\uBC98' : ['p ^ s','벘'],
    u'\uBC99' : ['p ^ N','벙'],
    u'\uBC9A' : ['p ^ tS','벚'],
    u'\uBC9B' : ['p ^ tSh','벛'],
    u'\uBC9C' : ['p ^ kh','벜'],
    u'\uBC9D' : ['p ^ th','벝'],
    u'\uBC9E' : ['p ^ ph','벞'],
    u'\uBC9F' : ['p ^ h','벟'],
    u'\uBCA0' : ['p e','베'],
    u'\uBCA1' : ['p e k','벡'],
    u'\uBCA2' : ['p e k>','벢'],
    u'\uBCA3' : ['p e k sh','벣'],
    u'\uBCA4' : ['p e n','벤'],
    u'\uBCA5' : ['p e n tS','벥'],
    u'\uBCA6' : ['p e n h','벦'],
    u'\uBCA7' : ['p e t','벧'],
    u'\uBCA8' : ['p e l','벨'],
    u'\uBCA9' : ['p e l k','벩'],
    u'\uBCAA' : ['p e l m','벪'],
    u'\uBCAB' : ['p e l p','벫'],
    u'\uBCAC' : ['p e l sh','벬'],
    u'\uBCAD' : ['p e l th','벭'],
    u'\uBCAE' : ['p e l ph','벮'],
    u'\uBCAF' : ['p e l h','벯'],
    u'\uBCB0' : ['p e m','벰'],
    u'\uBCB1' : ['p e p','벱'],
    u'\uBCB2' : ['p e p sh','벲'],
    u'\uBCB3' : ['p e sh','벳'],
    u'\uBCB4' : ['p e s','벴'],
    u'\uBCB5' : ['p e N','벵'],
    u'\uBCB6' : ['p e tS','벶'],
    u'\uBCB7' : ['p e tSh','벷'],
    u'\uBCB8' : ['p e kh','벸'],
    u'\uBCB9' : ['p e th','벹'],
    u'\uBCBA' : ['p e ph','벺'],
    u'\uBCBB' : ['p e h','벻'],
    u'\uBCBC' : ['p j ^','벼'],
    u'\uBCBD' : ['p j ^ k','벽'],
    u'\uBCBE' : ['p j ^ k>','벾'],
    u'\uBCBF' : ['p j ^ k sh','벿'],
    u'\uBCC0' : ['p j ^ n','변'],
    u'\uBCC1' : ['p j ^ n tS','볁'],
    u'\uBCC2' : ['p j ^ n h','볂'],
    u'\uBCC3' : ['p j ^ t','볃'],
    u'\uBCC4' : ['p j ^ l','별'],
    u'\uBCC5' : ['p j ^ l k','볅'],
    u'\uBCC6' : ['p j ^ l m','볆'],
    u'\uBCC7' : ['p j ^ l p','볇'],
    u'\uBCC8' : ['p j ^ l sh','볈'],
    u'\uBCC9' : ['p j ^ l th','볉'],
    u'\uBCCA' : ['p j ^ l ph','볊'],
    u'\uBCCB' : ['p j ^ l h','볋'],
    u'\uBCCC' : ['p j ^ m','볌'],
    u'\uBCCD' : ['p j ^ p','볍'],
    u'\uBCCE' : ['p j ^ p sh','볎'],
    u'\uBCCF' : ['p j ^ sh','볏'],
    u'\uBCD0' : ['p j ^ s','볐'],
    u'\uBCD1' : ['p j ^ N','병'],
    u'\uBCD2' : ['p j ^ tS','볒'],
    u'\uBCD3' : ['p j ^ tSh','볓'],
    u'\uBCD4' : ['p j ^ kh','볔'],
    u'\uBCD5' : ['p j ^ th','볕'],
    u'\uBCD6' : ['p j ^ ph','볖'],
    u'\uBCD7' : ['p j ^ h','볗'],
    u'\uBCD8' : ['p j e','볘'],
    u'\uBCD9' : ['p j e k','볙'],
    u'\uBCDA' : ['p j e k>','볚'],
    u'\uBCDB' : ['p j e k sh','볛'],
    u'\uBCDC' : ['p j e n','볜'],
    u'\uBCDD' : ['p j e n tS','볝'],
    u'\uBCDE' : ['p j e n h','볞'],
    u'\uBCDF' : ['p j e t','볟'],
    u'\uBCE0' : ['p j e l','볠'],
    u'\uBCE1' : ['p j e l k','볡'],
    u'\uBCE2' : ['p j e l m','볢'],
    u'\uBCE3' : ['p j e l p','볣'],
    u'\uBCE4' : ['p j e l sh','볤'],
    u'\uBCE5' : ['p j e l th','볥'],
    u'\uBCE6' : ['p j e l ph','볦'],
    u'\uBCE7' : ['p j e l h','볧'],
    u'\uBCE8' : ['p j e m','볨'],
    u'\uBCE9' : ['p j e p','볩'],
    u'\uBCEA' : ['p j e p sh','볪'],
    u'\uBCEB' : ['p j e sh','볫'],
    u'\uBCEC' : ['p j e s','볬'],
    u'\uBCED' : ['p j e N','볭'],
    u'\uBCEE' : ['p j e tS','볮'],
    u'\uBCEF' : ['p j e tSh','볯'],
    u'\uBCF0' : ['p j e kh','볰'],
    u'\uBCF1' : ['p j e th','볱'],
    u'\uBCF2' : ['p j e ph','볲'],
    u'\uBCF3' : ['p j e h','볳'],
    u'\uBCF4' : ['p o','보'],
    u'\uBCF5' : ['p o k','복'],
    u'\uBCF6' : ['p o k>','볶'],
    u'\uBCF7' : ['p o k sh','볷'],
    u'\uBCF8' : ['p o n','본'],
    u'\uBCF9' : ['p o n tS','볹'],
    u'\uBCFA' : ['p o n h','볺'],
    u'\uBCFB' : ['p o t','볻'],
    u'\uBCFC' : ['p o l','볼'],
    u'\uBCFD' : ['p o l k','볽'],
    u'\uBCFE' : ['p o l m','볾'],
    u'\uBCFF' : ['p o l p','볿'],
    u'\uBD00' : ['p o l sh','봀'],
    u'\uBD01' : ['p o l th','봁'],
    u'\uBD02' : ['p o l ph','봂'],
    u'\uBD03' : ['p o l h','봃'],
    u'\uBD04' : ['p o m','봄'],
    u'\uBD05' : ['p o p','봅'],
    u'\uBD06' : ['p o p sh','봆'],
    u'\uBD07' : ['p o sh','봇'],
    u'\uBD08' : ['p o s','봈'],
    u'\uBD09' : ['p o N','봉'],
    u'\uBD0A' : ['p o tS','봊'],
    u'\uBD0B' : ['p o tSh','봋'],
    u'\uBD0C' : ['p o kh','봌'],
    u'\uBD0D' : ['p o th','봍'],
    u'\uBD0E' : ['p o ph','봎'],
    u'\uBD0F' : ['p o h','봏'],
    u'\uBD10' : ['p w a','봐'],
    u'\uBD11' : ['p w a k','봑'],
    u'\uBD12' : ['p w a k>','봒'],
    u'\uBD13' : ['p w a k sh','봓'],
    u'\uBD14' : ['p w a n','봔'],
    u'\uBD15' : ['p w a n tS','봕'],
    u'\uBD16' : ['p w a nh','봖'],
    u'\uBD17' : ['p w a t','봗'],
    u'\uBD18' : ['p w a l','봘'],
    u'\uBD19' : ['p w a l k','봙'],
    u'\uBD1A' : ['p w a l m','봚'],
    u'\uBD1B' : ['p w a l p','봛'],
    u'\uBD1C' : ['p w a l sh','봜'],
    u'\uBD1D' : ['p w a l th','봝'],
    u'\uBD1E' : ['p w a l ph','봞'],
    u'\uBD1F' : ['p w a l h','봟'],
    u'\uBD20' : ['p w a m','봠'],
    u'\uBD21' : ['p w a p','봡'],
    u'\uBD22' : ['p w a p sh','봢'],
    u'\uBD23' : ['p w a sh','봣'],
    u'\uBD24' : ['p w a s','봤'],
    u'\uBD25 ' : ['p w a N','봥 '],
    u'\uBD26' : ['p w a tS','봦'],
    u'\uBD27' : ['p w a tSh','봧'],
    u'\uBD28' : ['p w a kh','봨'],
    u'\uBD29' : ['p w a th','봩'],
    u'\uBD2A' : ['p w a ph','봪'],
    u'\uBD2B' : ['p w a h','봫'],
    u'\uBD2C' : ['p w @','봬'],
    u'\uBD2D' : ['p w @ k','봭'],
    u'\uBD2E' : ['p w @ k>','봮'],
    u'\uBD2F' : ['p w @ k sh','봯'],
    u'\uBD30' : ['p w @ n','봰'],
    u'\uBD31' : ['p w @ n tS','봱'],
    u'\uBD32' : ['p w @ n h','봲'],
    u'\uBD33' : ['p w @ t','봳'],
    u'\uBD34' : ['p w @ l','봴'],
    u'\uBD35' : ['p w @ l k','봵'],
    u'\uBD36' : ['p w @ l m','봶'],
    u'\uBD37' : ['p w @ l p','봷'],
    u'\uBD38' : ['p w @ l sh','봸'],
    u'\uBD39' : ['p w @ l th','봹'],
    u'\uBD3A' : ['p w @ l ph','봺'],
    u'\uBD3B' : ['p w @ l h','봻'],
    u'\uBD3C' : ['p w @ m','봼'],
    u'\uBD3D' : ['p w @ p','봽'],
    u'\uBD3E' : ['p w @ p sh','봾'],
    u'\uBD3F' : ['p w @ sh','봿'],
    u'\uBD40' : ['p w @ s','뵀'],
    u'\uBD41' : ['p w @ N','뵁'],
    u'\uBD42' : ['p w @ tS','뵂'],
    u'\uBD43' : ['p w @ tSh','뵃'],
    u'\uBD44' : ['p w @ kh','뵄'],
    u'\uBD45' : ['p w @ th','뵅'],
    u'\uBD46' : ['p w @ ph','뵆'],
    u'\uBD47' : ['p w @ h','뵇'],
    u'\uBD48' : ['p w e','뵈'],
    u'\uBD49' : ['p w e k','뵉'],
    u'\uBD4A' : ['p w e k>','뵊'],
    u'\uBD4B' : ['p w e k sh','뵋'],
    u'\uBD4C' : ['p w e n','뵌'],
    u'\uBD4D' : ['p w e n tS','뵍'],
    u'\uBD4E' : ['p w e n h','뵎'],
    u'\uBD4F' : ['p w e t','뵏'],
    u'\uBD50' : ['p w e l','뵐'],
    u'\uBD51' : ['p w e l k','뵑'],
    u'\uBD52' : ['p w e l m','뵒'],
    u'\uBD53' : ['p w e l p','뵓'],
    u'\uBD54' : ['p w e l sh','뵔'],
    u'\uBD55' : ['p w e l th','뵕'],
    u'\uBD56' : ['p w e l ph','뵖'],
    u'\uBD57' : ['p w e l h','뵗'],
    u'\uBD58' : ['p w e m','뵘'],
    u'\uBD59' : ['p w e p','뵙'],
    u'\uBD5A' : ['p w e p sh','뵚'],
    u'\uBD5B' : ['p w e sh','뵛'],
    u'\uBD5C' : ['p w e s','뵜'],
    u'\uBD5D' : ['p w e N','뵝'],
    u'\uBD5E' : ['p w e tS','뵞'],
    u'\uBD5F' : ['p w e tSh','뵟'],
    u'\uBD60' : ['p w e kh','뵠'],
    u'\uBD61' : ['p w e th','뵡'],
    u'\uBD62' : ['p w e ph','뵢'],
    u'\uBD63' : ['p w e h','뵣'],
    u'\uBD64' : ['p j o','뵤'],
    u'\uBD65' : ['p j o k','뵥'],
    u'\uBD66' : ['p j o k>','뵦'],
    u'\uBD67' : ['p j o k sh','뵧'],
    u'\uBD68' : ['p j o n','뵨'],
    u'\uBD69' : ['p j o n tS','뵩'],
    u'\uBD6A' : ['p j o n h','뵪'],
    u'\uBD6B' : ['p j o t','뵫'],
    u'\uBD6C' : ['p j o l','뵬'],
    u'\uBD6D' : ['p j o l k','뵭'],
    u'\uBD6E' : ['p j o l m','뵮'],
    u'\uBD6F' : ['p j o l p','뵯'],
    u'\uBD70' : ['p j o l sh','뵰'],
    u'\uBD71' : ['p j o l th','뵱'],
    u'\uBD72' : ['p j o l ph','뵲'],
    u'\uBD73' : ['p j o l h','뵳'],
    u'\uBD74' : ['p j o m','뵴'],
    u'\uBD75' : ['p j o p','뵵'],
    u'\uBD76' : ['p j o p sh','뵶'],
    u'\uBD77' : ['p j o sh','뵷'],
    u'\uBD78' : ['p j o s','뵸'],
    u'\uBD79' : ['p j o N','뵹'],
    u'\uBD7A' : ['p j o tS','뵺'],
    u'\uBD7B' : ['p j o tSh','뵻'],
    u'\uBD7C' : ['p j o kh','뵼'],
    u'\uBD7D' : ['p j o th','뵽'],
    u'\uBD7E' : ['p j o ph','뵾'],
    u'\uBD7F' : ['p j o h','뵿'],
    u'\uBD80' : ['p u','부'],
    u'\uBD81' : ['p u k','북'],
    u'\uBD82' : ['p u k>','붂'],
    u'\uBD83' : ['p u k sh','붃'],
    u'\uBD84' : ['p u n','분'],
    u'\uBD85' : ['p u n tS','붅'],
    u'\uBD86' : ['p u n h','붆'],
    u'\uBD87' : ['p u t','붇'],
    u'\uBD88' : ['p u l','불'],
    u'\uBD89' : ['p u l k','붉'],
    u'\uBD8A' : ['p u l m','붊'],
    u'\uBD8B' : ['p u l p','붋'],
    u'\uBD8C' : ['p u l sh','붌'],
    u'\uBD8D' : ['p u l th','붍'],
    u'\uBD8E' : ['p u l ph','붎'],
    u'\uBD8F' : ['p u l h','붏'],
    u'\uBD90' : ['p u m','붐'],
    u'\uBD91' : ['p u p','붑'],
    u'\uBD92' : ['p u p sh','붒'],
    u'\uBD93' : ['p u sh','붓'],
    u'\uBD94' : ['p u s','붔'],
    u'\uBD95' : ['p u N','붕'],
    u'\uBD96' : ['p u tS','붖'],
    u'\uBD97' : ['p u tSh','붗'],
    u'\uBD98' : ['p u kh','붘'],
    u'\uBD99' : ['p u th','붙'],
    u'\uBD9A' : ['p u ph','붚'],
    u'\uBD9B' : ['p u h','붛'],
    u'\uBD9C' : ['p w ^','붜'],
    u'\uBD9D' : ['p w ^ k','붝'],
    u'\uBD9E' : ['p w ^ k>','붞'],
    u'\uBD9F' : ['p w ^ k sh','붟'],
    u'\uBDA0' : ['p w ^ n','붠'],
    u'\uBDA1' : ['p w ^ n tS','붡'],
    u'\uBDA2' : ['p w ^ n h','붢'],
    u'\uBDA3' : ['p w ^ t','붣'],
    u'\uBDA4' : ['p w ^ l','붤'],
    u'\uBDA5' : ['p w ^ l k','붥'],
    u'\uBDA6' : ['p w ^ l m','붦'],
    u'\uBDA7' : ['p w ^ l p','붧'],
    u'\uBDA8' : ['p w ^ l sh','붨'],
    u'\uBDA9' : ['p w ^ l th','붩'],
    u'\uBDAA' : ['p w ^ l ph','붪'],
    u'\uBDAB' : ['p w ^ l h','붫'],
    u'\uBDAC' : ['p w ^ m','붬'],
    u'\uBDAD' : ['p w ^ p','붭'],
    u'\uBDAE' : ['p w ^ p sh','붮'],
    u'\uBDAF' : ['p w ^ sh','붯'],
    u'\uBDB0' : ['p w ^ s','붰'],
    u'\uBDB1' : ['p w ^ N','붱'],
    u'\uBDB2' : ['p w ^ tS','붲'],
    u'\uBDB3' : ['p w ^ tSh','붳'],
    u'\uBDB4' : ['p w ^ kh','붴'],
    u'\uBDB5' : ['p w ^ th','붵'],
    u'\uBDB6' : ['p w ^ ph','붶'],
    u'\uBDB7' : ['p w ^ h','붷'],
    u'\uBDB8' : ['p w E','붸'],
    u'\uBDB9' : ['p w E k','붹'],
    u'\uBDBA' : ['p w E k>','붺'],
    u'\uBDBB' : ['p w E k sh','붻'],
    u'\uBDBC' : ['p w E n','붼'],
    u'\uBDBD' : ['p w E n tS','붽'],
    u'\uBDBE' : ['p w E n h','붾'],
    u'\uBDBF' : ['p w E t','붿'],
    u'\uBDC0' : ['p w E l','뷀'],
    u'\uBDC1' : ['p w E l k','뷁'],
    u'\uBDC2' : ['p w E l m','뷂'],
    u'\uBDC3' : ['p w E l p','뷃'],
    u'\uBDC4' : ['p w E l sh','뷄'],
    u'\uBDC5' : ['p w E l th','뷅'],
    u'\uBDC6' : ['p w E l ph','뷆'],
    u'\uBDC7' : ['p w E l h','뷇'],
    u'\uBDC8' : ['p w E m','뷈'],
    u'\uBDC9' : ['p w E p','뷉'],
    u'\uBDCA' : ['p w E p sh','뷊'],
    u'\uBDCB' : ['p w E sh','뷋'],
    u'\uBDCC' : ['p w E s','뷌'],
    u'\uBDCD' : ['p w E N','뷍'],
    u'\uBDCE' : ['p w E tS','뷎'],
    u'\uBDCF' : ['p w E tSh','뷏'],
    u'\uBDD0' : ['p w E kh','뷐'],
    u'\uBDD1' : ['p w E th','뷑'],
    u'\uBDD2' : ['p w E ph','뷒'],
    u'\uBDD3' : ['p w E h','뷓'],
    u'\uBDD4' : ['p 7','뷔'],
    u'\uBDD5' : ['p 7 k','뷕'],
    u'\uBDD6' : ['p 7 k>','뷖'],
    u'\uBDD7' : ['p 7 k sh','뷗'],
    u'\uBDD8' : ['p 7 n','뷘'],
    u'\uBDD9' : ['p 7 n tS','뷙'],
    u'\uBDDA' : ['p 7 n h','뷚'],
    u'\uBDDB' : ['p 7 t','뷛'],
    u'\uBDDC' : ['p 7 l','뷜'],
    u'\uBDDD' : ['p 7 l k','뷝'],
    u'\uBDDE' : ['p 7 l m','뷞'],
    u'\uBDDF' : ['p 7 l p','뷟'],
    u'\uBDE0' : ['p 7 l sh','뷠'],
    u'\uBDE1' : ['p 7 l th','뷡'],
    u'\uBDE2' : ['p 7 l ph','뷢'],
    u'\uBDE3' : ['p 7 l s','뷣'],
    u'\uBDE4' : ['p 7 m','뷤'],
    u'\uBDE5' : ['p 7 p','뷥'],
    u'\uBDE6' : ['p 7 p sh','뷦'],
    u'\uBDE7' : ['p 7 sh','뷧'],
    u'\uBDE8' : ['p 7 s','뷨'],
    u'\uBDE9' : ['p 7 N','뷩'],
    u'\uBDEA' : ['p 7 tS','뷪'],
    u'\uBDEB' : ['p 7 tSh','뷫'],
    u'\uBDEC' : ['p 7 kh','뷬'],
    u'\uBDED' : ['p 7 th','뷭'],
    u'\uBDEE' : ['p 7 ph','뷮'],
    u'\uBDEF' : ['p 7 h','뷯'],
    u'\uBDF0' : ['p j u','뷰'],
    u'\uBDF1' : ['p j u k','뷱'],
    u'\uBDF2' : ['p j u k>','뷲'],
    u'\uBDF3' : ['p j u k sh','뷳'],
    u'\uBDF4' : ['p j u n','뷴'],
    u'\uBDF5' : ['p j u n tS','뷵'],
    u'\uBDF6' : ['p j u n h','뷶'],
    u'\uBDF7' : ['p j u t','뷷'],
    u'\uBDF8' : ['p j u l','뷸'],
    u'\uBDF9' : ['p j u l k','뷹'],
    u'\uBDFA' : ['p j u l m','뷺'],
    u'\uBDFB' : ['p j u l p','뷻'],
    u'\uBDFC' : ['p j u l sh','뷼'],
    u'\uBDFD' : ['p j u l th','뷽'],
    u'\uBDFE' : ['p j u l ph','뷾'],
    u'\uBDFF' : ['p j u l h','뷿'],
    u'\uBE00' : ['p j u m','븀'],
    u'\uBE01' : ['p j u p','븁'],
    u'\uBE02' : ['p j u p sh','븂'],
    u'\uBE03' : ['p j u sh','븃'],
    u'\uBE04' : ['p j u s','븄'],
    u'\uBE05' : ['p j u N','븅'],
    u'\uBE06' : ['p j u tS','븆'],
    u'\uBE07' : ['p j u tSh','븇'],
    u'\uBE08' : ['p j u kh','븈'],
    u'\uBE09' : ['p j u th','븉'],
    u'\uBE0A' : ['p j u ph','븊'],
    u'\uBE0B' : ['p j u h','븋'],
    u'\uBE0C' : ['p 4','브'],
    u'\uBE0D' : ['p 4 k','븍'],
    u'\uBE0E' : ['p 4 k>','븎'],
    u'\uBE0F' : ['p 4 k sh','븏'],
    u'\uBE10' : ['p 4 n','븐'],
    u'\uBE11' : ['p 4 n tS','븑'],
    u'\uBE12' : ['p 4 n h','븒'],
    u'\uBE13' : ['p 4 t','븓'],
    u'\uBE14' : ['p 4 l','블'],
    u'\uBE15' : ['p 4 l k','븕'],
    u'\uBE16' : ['p 4 l m','븖'],
    u'\uBE17' : ['p 4 l p','븗'],
    u'\uBE18' : ['p 4 l sh','븘'],
    u'\uBE19' : ['p 4 l th','븙'],
    u'\uBE1A' : ['p 4 l ph','븚'],
    u'\uBE1B' : ['p 4 l h','븛'],
    u'\uBE1C' : ['p 4 m','븜'],
    u'\uBE1D' : ['p 4 p','븝'],
    u'\uBE1E' : ['p 4 p sh','븞'],
    u'\uBE1F' : ['p 4 sh','븟'],
    u'\uBE20' : ['p 4 s','븠'],
    u'\uBE21' : ['p 4 N','븡'],
    u'\uBE22' : ['p 4 tS','븢'],
    u'\uBE23' : ['p 4 tSh','븣'],
    u'\uBE24' : ['p 4 kh','븤'],
    u'\uBE25' : ['p 4 th','븥'],
    u'\uBE26' : ['p 4 ph','븦'],
    u'\uBE27' : ['p 4 h','븧'],
    u'\uBE28' : ['p 4 j','븨'],
    u'\uBE29' : ['p 4 j k','븩'],
    u'\uBE2A' : ['p 4 j k>','븪'],
    u'\uBE2B' : ['p 4 j k sh','븫'],
    u'\uBE2C' : ['p 4 j n','븬'],
    u'\uBE2D' : ['p 4 j n tS','븭'],
    u'\uBE2E' : ['p 4 j n h','븮'],
    u'\uBE2F' : ['p 4 j t','븯'],
    u'\uBE30' : ['p 4 j l','븰'],
    u'\uBE31' : ['p 4 j l k','븱'],
    u'\uBE32' : ['p 4 j l m','븲'],
    u'\uBE33' : ['p 4 j l p','븳'],
    u'\uBE34' : ['p 4 j l sh','븴'],
    u'\uBE35' : ['p 4 j l th','븵'],
    u'\uBE36' : ['p 4 j l ph','븶'],
    u'\uBE37' : ['p 4 j l h','븷'],
    u'\uBE38' : ['p 4 j m','븸'],
    u'\uBE39' : ['p 4 j p','븹'],
    u'\uBE3A' : ['p 4 j p sh','븺'],
    u'\uBE3B' : ['p 4 j sh','븻'],
    u'\uBE3C' : ['p 4 j s','븼'],
    u'\uBE3D' : ['p 4 j N','븽'],
    u'\uBE3E' : ['p 4 j tS','븾'],
    u'\uBE3F' : ['p 4 j tSh','븿'],
    u'\uBE40' : ['p 4 j kh','빀'],
    u'\uBE41' : ['p 4 j th','빁'],
    u'\uBE42' : ['p 4 j ph','빂'],
    u'\uBE43' : ['p 4 j h','빃'],
    u'\uBE44' : ['p i','비'],
    u'\uBE45' : ['p i k','빅'],
    u'\uBE46' : ['p i k>','빆'],
    u'\uBE47' : ['p i k sh','빇'],
    u'\uBE48' : ['p i n','빈'],
    u'\uBE49' : ['p i n tS','빉'],
    u'\uBE4A' : ['p i n h','빊'],
    u'\uBE4B' : ['p i t','빋'],
    u'\uBE4C' : ['p i l','빌'],
    u'\uBE4D' : ['p i l k','빍'],
    u'\uBE4E' : ['p i l m','빎'],
    u'\uBE4F' : ['p i l p','빏'],
    u'\uBE50' : ['p i l sh','빐'],
    u'\uBE51' : ['p i l th','빑'],
    u'\uBE52' : ['p i l ph','빒'],
    u'\uBE53' : ['p i l h','빓'],
    u'\uBE54' : ['p i m','빔'],
    u'\uBE55' : ['p i p','빕'],
    u'\uBE56' : ['p i p sh','빖'],
    u'\uBE57' : ['p i sh','빗'],
    u'\uBE58' : ['p i s','빘'],
    u'\uBE59' : ['p i N','빙'],
    u'\uBE5A' : ['p i tS','빚'],
    u'\uBE5B' : ['p i tSh','빛'],
    u'\uBE5C' : ['p i kh','빜'],
    u'\uBE5D' : ['p i th','빝'],
    u'\uBE5E' : ['p i ph','빞'],
    u'\uBE5F' : ['p i h','빟'],
    u'\uBE60' : ['p> a','빠'],
    u'\uBE61' : ['p> a k','빡'],
    u'\uBE62' : ['p> a k>','빢'],
    u'\uBE63' : ['p> a k sh','빣'],
    u'\uBE64' : ['p> a n','빤'],
    u'\uBE65' : ['p> a n tS','빥'],
    u'\uBE66' : ['p> a n h','빦'],
    u'\uBE67' : ['p> a t','빧'],
    u'\uBE68' : ['p> a l','빨'],
    u'\uBE69' : ['p> a l k','빩'],
    u'\uBE6A' : ['p> a l m','빪'],
    u'\uBE6B' : ['p> a l p','빫'],
    u'\uBE6C' : ['p> a l sh','빬'],
    u'\uBE6D' : ['p> a l th','빭'],
    u'\uBE6E' : ['p> a l ph','빮'],
    u'\uBE6F' : ['p> a l h','빯'],
    u'\uBE70' : ['p> a m','빰'],
    u'\uBE71' : ['p> a p','빱'],
    u'\uBE72' : ['p> a p sh','빲'],
    u'\uBE73' : ['p> a sh','빳'],
    u'\uBE74' : ['p> a s','빴'],
    u'\uBE75' : ['p> a N','빵'],
    u'\uBE76' : ['p> a tS','빶'],
    u'\uBE77' : ['p> a tSh','빷'],
    u'\uBE78' : ['p> a kh','빸'],
    u'\uBE79' : ['p> a th','빹'],
    u'\uBE7A' : ['p> a ph','빺'],
    u'\uBE7B' : ['p> a h','빻'],
    u'\uBE7C' : ['p> @','빼'],
    u'\uBE7D' : ['p> @ k','빽'],
    u'\uBE7E' : ['p> @ k>','빾'],
    u'\uBE7F' : ['p> @ k sh','빿'],
    u'\uBE80' : ['p> @ n','뺀'],
    u'\uBE81' : ['p> @ n tS','뺁'],
    u'\uBE82' : ['p> @ n h','뺂'],
    u'\uBE83' : ['p> @ t','뺃'],
    u'\uBE84' : ['p> @ l','뺄'],
    u'\uBE85' : ['p> @ l k','뺅'],
    u'\uBE86' : ['p> @ l m','뺆'],
    u'\uBE87' : ['p> @ l p','뺇'],
    u'\uBE88' : ['p> @ l sh','뺈'],
    u'\uBE89' : ['p> @ l th','뺉'],
    u'\uBE8A' : ['p> @ l ph','뺊'],
    u'\uBE8B' : ['p> @ l h','뺋'],
    u'\uBE8C' : ['p> @ m','뺌'],
    u'\uBE8D' : ['p> @ p','뺍'],
    u'\uBE8E' : ['p> @ p sh','뺎'],
    u'\uBE8F' : ['p> @ sh','뺏'],
    u'\uBE90' : ['p> @ s','뺐'],
    u'\uBE91' : ['p> @ N','뺑'],
    u'\uBE92' : ['p> @ tS','뺒'],
    u'\uBE93' : ['p> @ tSh','뺓'],
    u'\uBE94' : ['p> @ kh','뺔'],
    u'\uBE95' : ['p> @ th','뺕'],
    u'\uBE96' : ['p> @ ph','뺖'],
    u'\uBE97' : ['p> @ h','뺗'],
    u'\uBE98' : ['p> j a','뺘'],
    u'\uBE99' : ['p> j a k','뺙'],
    u'\uBE9A' : ['p> j a k>','뺚'],
    u'\uBE9B' : ['p> j a k sh','뺛'],
    u'\uBE9C' : ['p> j a n','뺜'],
    u'\uBE9D' : ['p> j a n tS','뺝'],
    u'\uBE9E' : ['p> j a n h','뺞'],
    u'\uBE9F' : ['p> j a t','뺟'],
    u'\uBEA0' : ['p> j a l','뺠'],
    u'\uBEA1' : ['p> j a l k','뺡'],
    u'\uBEA2' : ['p> j a l m','뺢'],
    u'\uBEA3' : ['p> j a l p','뺣'],
    u'\uBEA4' : ['p> j a l sh','뺤'],
    u'\uBEA5' : ['p> j a l th','뺥'],
    u'\uBEA6' : ['p> j a l ph','뺦'],
    u'\uBEA7' : ['p> j a l h','뺧'],
    u'\uBEA8' : ['p> j a m','뺨'],
    u'\uBEA9' : ['p> j a p','뺩'],
    u'\uBEAA' : ['p> j a p sh','뺪'],
    u'\uBEAB' : ['p> j a sh','뺫'],
    u'\uBEAC' : ['p> j a s','뺬'],
    u'\uBEAD' : ['p> j a N','뺭'],
    u'\uBEAE' : ['p> j a tS','뺮'],
    u'\uBEAF' : ['p> j a tSh','뺯'],
    u'\uBEB0' : ['p> j a kh','뺰'],
    u'\uBEB1' : ['p> j a th','뺱'],
    u'\uBEB2' : ['p> j a ph','뺲'],
    u'\uBEB3' : ['p> j a h','뺳'],
    u'\uBEB4' : ['p> j @','뺴'],
    u'\uBEB5' : ['p> j @ k','뺵'],
    u'\uBEB6' : ['p> j @ k>','뺶'],
    u'\uBEB7' : ['p> j @ k sh','뺷'],
    u'\uBEB8' : ['p> j @ n','뺸'],
    u'\uBEB9' : ['p> j @ n tS','뺹'],
    u'\uBEBA' : ['p> j @ n h','뺺'],
    u'\uBEBB' : ['p> j @ t','뺻'],
    u'\uBEBC' : ['p> j @ l','뺼'],
    u'\uBEBD' : ['p> j @ l k','뺽'],
    u'\uBEBE' : ['p> j @ l m','뺾'],
    u'\uBEBF' : ['p> j @ l p','뺿'],
    u'\uBEC0' : ['p> j @ l sh','뻀'],
    u'\uBEC1' : ['p> j @ l th','뻁'],
    u'\uBEC2' : ['p> j @ l ph','뻂'],
    u'\uBEC3' : ['p> j @ l h','뻃'],
    u'\uBEC4' : ['p> j @ m','뻄'],
    u'\uBEC5' : ['p> j @ p','뻅'],
    u'\uBEC6' : ['p> j @ p sh','뻆'],
    u'\uBEC7' : ['p> j @ sh','뻇'],
    u'\uBEC8' : ['p> j @ s','뻈'],
    u'\uBEC9' : ['p> j @ N','뻉'],
    u'\uBECA' : ['p> j @ tS','뻊'],
    u'\uBECB' : ['p> j @ tSh','뻋'],
    u'\uBECC' : ['p> j @ kh','뻌'],
    u'\uBECD' : ['p> j @ th','뻍'],
    u'\uBECE' : ['p> j @ ph','뻎'],
    u'\uBECF' : ['p> j @ h','뻏'],
    u'\uBED0' : ['p> ^','뻐'],
    u'\uBED1' : ['p> ^ k','뻑'],
    u'\uBED2' : ['p> ^ k>','뻒'],
    u'\uBED3' : ['p> ^ k sh','뻓'],
    u'\uBED4' : ['p> ^ n','뻔'],
    u'\uBED5' : ['p> ^ n tS','뻕'],
    u'\uBED6' : ['p> ^ n h','뻖'],
    u'\uBED7' : ['p> ^ t','뻗'],
    u'\uBED8' : ['p> ^ l','뻘'],
    u'\uBED9' : ['p> ^ l k','뻙'],
    u'\uBEDA' : ['p> ^ l m','뻚'],
    u'\uBEDB' : ['p> ^ l p','뻛'],
    u'\uBEDC' : ['p> ^ l sh','뻜'],
    u'\uBEDD' : ['p> ^ l th','뻝'],
    u'\uBEDE' : ['p> ^ l ph','뻞'],
    u'\uBEDF' : ['p> ^ l h','뻟'],
    u'\uBEE0' : ['p> ^ m','뻠'],
    u'\uBEE1' : ['p> ^ p','뻡'],
    u'\uBEE2' : ['p> ^ p sh','뻢'],
    u'\uBEE3' : ['p> ^ sh','뻣'],
    u'\uBEE4' : ['p> ^ s','뻤'],
    u'\uBEE5' : ['p> ^ N','뻥'],
    u'\uBEE6' : ['p> ^ tS','뻦'],
    u'\uBEE7' : ['p> ^ tSh','뻧'],
    u'\uBEE8' : ['p> ^ kh','뻨'],
    u'\uBEE9' : ['p> ^ th','뻩'],
    u'\uBEEA' : ['p> ^ ph','뻪'],
    u'\uBEEB' : ['p> ^ h','뻫'],
    u'\uBEEC' : ['p> e','뻬'],
    u'\uBEED' : ['p> e k','뻭'],
    u'\uBEEE' : ['p> e k>','뻮'],
    u'\uBEEF' : ['p> e k sh','뻯'],
    u'\uBEF0' : ['p> e n','뻰'],
    u'\uBEF1' : ['p> e n tS','뻱'],
    u'\uBEF2' : ['p> e n h','뻲'],
    u'\uBEF3' : ['p> e t','뻳'],
    u'\uBEF4' : ['p> e l','뻴'],
    u'\uBEF5' : ['p> e l k','뻵'],
    u'\uBEF6' : ['p> e l m','뻶'],
    u'\uBEF7' : ['p> e l p','뻷'],
    u'\uBEF8' : ['p> e l sh','뻸'],
    u'\uBEF9' : ['p> e l th','뻹'],
    u'\uBEFA' : ['p> e l ph','뻺'],
    u'\uBEFB' : ['p> e l h','뻻'],
    u'\uBEFC' : ['p> e m','뻼'],
    u'\uBEFD' : ['p> e p','뻽'],
    u'\uBEFE' : ['p> e p sh','뻾'],
    u'\uBEFF' : ['p> e sh','뻿'],
    u'\uBF00' : ['p> e s','뼀'],
    u'\uBF01' : ['p> e N','뼁'],
    u'\uBF02' : ['p> e tS','뼂'],
    u'\uBF03' : ['p> e tSh','뼃'],
    u'\uBF04' : ['p> e kh','뼄'],
    u'\uBF05' : ['p> e th','뼅'],
    u'\uBF06' : ['p> e ph','뼆'],
    u'\uBF07' : ['p> e h','뼇'],
    u'\uBF08' : ['p> j ^','뼈'],
    u'\uBF09' : ['p> j ^ k','뼉'],
    u'\uBF0A' : ['p> j ^ k>','뼊'],
    u'\uBF0B' : ['p> j ^ k sh','뼋'],
    u'\uBF0C' : ['p> j ^ n','뼌'],
    u'\uBF0D' : ['p> j ^ n tS','뼍'],
    u'\uBF0E' : ['p> j ^ n h','뼎'],
    u'\uBF0F' : ['p> j ^ t','뼏'],
    u'\uBF10' : ['p> j ^ l','뼐'],
    u'\uBF11' : ['p> j ^ l k','뼑'],
    u'\uBF12' : ['p> j ^ l m','뼒'],
    u'\uBF13' : ['p> j ^ l p','뼓'],
    u'\uBF14' : ['p> j ^ l sh','뼔'],
    u'\uBF15' : ['p> j ^ l th','뼕'],
    u'\uBF16' : ['p> j ^ l ph','뼖'],
    u'\uBF17' : ['p> j ^ l h','뼗'],
    u'\uBF18' : ['p> j ^ m','뼘'],
    u'\uBF19' : ['p> j ^ p','뼙'],
    u'\uBF1A' : ['p> j ^ p sh','뼚'],
    u'\uBF1B' : ['p> j ^ sh','뼛'],
    u'\uBF1C' : ['p> j ^ s','뼜'],
    u'\uBF1D' : ['p> j ^ N','뼝'],
    u'\uBF1E' : ['p> j ^ tS','뼞'],
    u'\uBF1F' : ['p> j ^ tSh','뼟'],
    u'\uBF20' : ['p> j ^ kh','뼠'],
    u'\uBF21' : ['p> j ^ th','뼡'],
    u'\uBF22' : ['p> j ^ ph','뼢'],
    u'\uBF23' : ['p> j ^ h','뼣'],
    u'\uBF24' : ['p> j e','뼤'],
    u'\uBF25' : ['p> j e k','뼥'],
    u'\uBF26' : ['p> j e k>','뼦'],
    u'\uBF27' : ['p> j e k sh','뼧'],
    u'\uBF28' : ['p> j e n','뼨'],
    u'\uBF29' : ['p> j e n tS','뼩'],
    u'\uBF2A' : ['p> j e n h','뼪'],
    u'\uBF2B' : ['p> j e t','뼫'],
    u'\uBF2C' : ['p> j e l','뼬'],
    u'\uBF2D' : ['p> j e l k','뼭'],
    u'\uBF2E' : ['p> j e l m','뼮'],
    u'\uBF2F' : ['p> j e l p','뼯'],
    u'\uBF30' : ['p> j e l sh','뼰'],
    u'\uBF31' : ['p> j e l th','뼱'],
    u'\uBF32' : ['p> j e l ph','뼲'],
    u'\uBF33' : ['p> j e l h','뼳'],
    u'\uBF34' : ['p> j e m','뼴'],
    u'\uBF35' : ['p> j e p','뼵'],
    u'\uBF36' : ['p> j e p sh','뼶'],
    u'\uBF37' : ['p> j e sh','뼷'],
    u'\uBF38' : ['p> j e s','뼸'],
    u'\uBF39' : ['p> j e N','뼹'],
    u'\uBF3A' : ['p> j e tS','뼺'],
    u'\uBF3B' : ['p> j e tSh','뼻'],
    u'\uBF3C' : ['p> j e kh','뼼'],
    u'\uBF3D' : ['p> j e th','뼽'],
    u'\uBF3E' : ['p> j e ph','뼾'],
    u'\uBF3F' : ['p> j e h','뼿'],
    u'\uBF40' : ['p> o','뽀'],
    u'\uBF41' : ['p> o k','뽁'],
    u'\uBF42' : ['p> o k>','뽂'],
    u'\uBF43' : ['p> o k sh','뽃'],
    u'\uBF44' : ['p> o n','뽄'],
    u'\uBF45' : ['p> o n tS','뽅'],
    u'\uBF46' : ['p> o n h','뽆'],
    u'\uBF47' : ['p> o t','뽇'],
    u'\uBF48' : ['p> o l','뽈'],
    u'\uBF49' : ['p> o l k','뽉'],
    u'\uBF4A' : ['p> o l m','뽊'],
    u'\uBF4B' : ['p> o l p','뽋'],
    u'\uBF4C' : ['p> o l sh','뽌'],
    u'\uBF4D' : ['p> o l th','뽍'],
    u'\uBF4E' : ['p> o l ph','뽎'],
    u'\uBF4F' : ['p> o l h','뽏'],
    u'\uBF50' : ['p> o m','뽐'],
    u'\uBF51' : ['p> o p','뽑'],
    u'\uBF52' : ['p> o p sh','뽒'],
    u'\uBF53' : ['p> o sh','뽓'],
    u'\uBF54' : ['p> o s','뽔'],
    u'\uBF55' : ['p> o N','뽕'],
    u'\uBF56' : ['p> o tS','뽖'],
    u'\uBF57' : ['p> o tSh','뽗'],
    u'\uBF58' : ['p> o kh','뽘'],
    u'\uBF59' : ['p> o th','뽙'],
    u'\uBF5A' : ['p> o ph','뽚'],
    u'\uBF5B' : ['p> o h','뽛'],
    u'\uBF5C' : ['p> w a','뽜'],
    u'\uBF5D' : ['p> w a k','뽝'],
    u'\uBF5E' : ['p> w a k>','뽞'],
    u'\uBF5F' : ['p> w a k sh','뽟'],
    u'\uBF60' : ['p> w a n','뽠'],
    u'\uBF61' : ['p> w a n tS','뽡'],
    u'\uBF62' : ['p> w a n h','뽢'],
    u'\uBF63' : ['p> w a t','뽣'],
    u'\uBF64' : ['p> w a l','뽤'],
    u'\uBF65' : ['p> w a l k','뽥'],
    u'\uBF66' : ['p> w a l m','뽦'],
    u'\uBF67' : ['p> w a l p','뽧'],
    u'\uBF68' : ['p> w a l sh','뽨'],
    u'\uBF69' : ['p> w a l th','뽩'],
    u'\uBF6A' : ['p> w a l ph','뽪'],
    u'\uBF6B' : ['p> w a l h','뽫'],
    u'\uBF6C' : ['p> w a m','뽬'],
    u'\uBF6D' : ['p> w a p','뽭'],
    u'\uBF6E' : ['p> w a p sh','뽮'],
    u'\uBF6F' : ['p> w a sh','뽯'],
    u'\uBF70 ' : ['p> w a s','뽰 '],
    u'\uBF71' : ['p> w a N','뽱'],
    u'\uBF72' : ['p> w a tS','뽲'],
    u'\uBF73' : ['p> w a tSh','뽳'],
    u'\uBF74' : ['p> w a kh','뽴'],
    u'\uBF75' : ['p> w a th','뽵'],
    u'\uBF76' : ['p> w a ph','뽶'],
    u'\uBF77' : ['p> w a h','뽷'],
    u'\uBF78' : ['p> w @','뽸'],
    u'\uBF79' : ['p> w @ k','뽹'],
    u'\uBF7A' : ['p> w @ k>','뽺'],
    u'\uBF7B' : ['p> w @ k sh','뽻'],
    u'\uBF7C' : ['p> w @ n','뽼'],
    u'\uBF7D' : ['p> w @ n tS','뽽'],
    u'\uBF7E' : ['p> w @ n h','뽾'],
    u'\uBF7F' : ['p> w @ t','뽿'],
    u'\uBF80' : ['p> w @ l','뾀'],
    u'\uBF81' : ['p> w @ l k','뾁'],
    u'\uBF82' : ['p> w @ l m','뾂'],
    u'\uBF83' : ['p> w @ l p','뾃'],
    u'\uBF84' : ['p> w @ l sh','뾄'],
    u'\uBF85' : ['p> w @ l th','뾅'],
    u'\uBF86' : ['p> w @ l ph','뾆'],
    u'\uBF87' : ['p> w @ l h','뾇'],
    u'\uBF88' : ['p> w @ m','뾈'],
    u'\uBF89' : ['p> w @ p','뾉'],
    u'\uBF8A' : ['p> w @ p sh','뾊'],
    u'\uBF8B' : ['p> w @ sh','뾋'],
    u'\uBF8C' : ['p> w @ s','뾌'],
    u'\uBF8D' : ['p> w @ N','뾍'],
    u'\uBF8E' : ['p> w @ tS','뾎'],
    u'\uBF8F' : ['p> w @ tSh','뾏'],
    u'\uBF90' : ['p> w @ kh','뾐'],
    u'\uBF91' : ['p> w @ th','뾑'],
    u'\uBF92' : ['p> w @ ph','뾒'],
    u'\uBF93' : ['p> w @ h','뾓'],
    u'\uBF94' : ['p> w e','뾔'],
    u'\uBF95' : ['p> w e k','뾕'],
    u'\uBF96' : ['p> w e k>','뾖'],
    u'\uBF97' : ['p> w e k sh','뾗'],
    u'\uBF98' : ['p> w e n','뾘'],
    u'\uBF99' : ['p> w e n tS','뾙'],
    u'\uBF9A' : ['p> w e n h','뾚'],
    u'\uBF9B' : ['p> w e t','뾛'],
    u'\uBF9C' : ['p> w e l','뾜'],
    u'\uBF9D' : ['p> w e l k','뾝'],
    u'\uBF9E' : ['p> w e l m','뾞'],
    u'\uBF9F' : ['p> w e l p','뾟'],
    u'\uBFA0' : ['p> w e l sh','뾠'],
    u'\uBFA1' : ['p> w e l th','뾡'],
    u'\uBFA2' : ['p> w e l ph','뾢'],
    u'\uBFA3' : ['p> w e l h','뾣'],
    u'\uBFA4' : ['p> w e m','뾤'],
    u'\uBFA5' : ['p> w e p','뾥'],
    u'\uBFA6' : ['p> w e p sh','뾦'],
    u'\uBFA7' : ['p> w e sh','뾧'],
    u'\uBFA8' : ['p> w e s','뾨'],
    u'\uBFA9' : ['p> w e N','뾩'],
    u'\uBFAA' : ['p> w e tS','뾪'],
    u'\uBFAB' : ['p> w e tSh','뾫'],
    u'\uBFAC' : ['p> w e kh','뾬'],
    u'\uBFAD' : ['p> w e th','뾭'],
    u'\uBFAE' : ['p> w e ph','뾮'],
    u'\uBFAF' : ['p> w e h','뾯'],
    u'\uBFB0' : ['p> j o','뾰'],
    u'\uBFB1' : ['p> j o k','뾱'],
    u'\uBFB2' : ['p> j o k>','뾲'],
    u'\uBFB3' : ['p> j o k sh','뾳'],
    u'\uBFB4' : ['p> j o n','뾴'],
    u'\uBFB5' : ['p> j o n tS','뾵'],
    u'\uBFB6' : ['p> j o n h','뾶'],
    u'\uBFB7' : ['p> j o t','뾷'],
    u'\uBFB8' : ['p> j o l','뾸'],
    u'\uBFB9' : ['p> j o l k','뾹'],
    u'\uBFBA' : ['p> j o l m','뾺'],
    u'\uBFBB' : ['p> j o l p','뾻'],
    u'\uBFBC' : ['p> j o l sh','뾼'],
    u'\uBFBD' : ['p> j o l th','뾽'],
    u'\uBFBE' : ['p> j o l ph','뾾'],
    u'\uBFBF' : ['p> j o l h','뾿'],
    u'\uBFC0' : ['p> j o m','뿀'],
    u'\uBFC1' : ['p> j o p','뿁'],
    u'\uBFC2' : ['p> j o p sh','뿂'],
    u'\uBFC3' : ['p> j o sh','뿃'],
    u'\uBFC4' : ['p> j o s','뿄'],
    u'\uBFC5' : ['p> j o N','뿅'],
    u'\uBFC6' : ['p> j o tS','뿆'],
    u'\uBFC7' : ['p> j o tSh','뿇'],
    u'\uBFC8' : ['p> j o kh','뿈'],
    u'\uBFC9' : ['p> j o th','뿉'],
    u'\uBFCA' : ['p> j o ph','뿊'],
    u'\uBFCB' : ['p> j o h','뿋'],
    u'\uBFCC' : ['p> u','뿌'],
    u'\uBFCD' : ['p> u k','뿍'],
    u'\uBFCE' : ['p> u k>','뿎'],
    u'\uBFCF' : ['p> u k sh','뿏'],
    u'\uBFD0' : ['p> u n','뿐'],
    u'\uBFD1' : ['p> u n tS','뿑'],
    u'\uBFD2' : ['p> u n h','뿒'],
    u'\uBFD3' : ['p> u t','뿓'],
    u'\uBFD4' : ['p> u l','뿔'],
    u'\uBFD5' : ['p> u l k','뿕'],
    u'\uBFD6' : ['p> u l m','뿖'],
    u'\uBFD7' : ['p> u l p','뿗'],
    u'\uBFD8' : ['p> u l sh','뿘'],
    u'\uBFD9' : ['p> u l th','뿙'],
    u'\uBFDA' : ['p> u l ph','뿚'],
    u'\uBFDB' : ['p> u l h','뿛'],
    u'\uBFDC' : ['p> u m','뿜'],
    u'\uBFDD' : ['p> u p','뿝'],
    u'\uBFDE' : ['p> u p sh','뿞'],
    u'\uBFDF' : ['p> u sh','뿟'],
    u'\uBFE0' : ['p> u s','뿠'],
    u'\uBFE1' : ['p> u N','뿡'],
    u'\uBFE2' : ['p> u tS','뿢'],
    u'\uBFE3' : ['p> u tSh','뿣'],
    u'\uBFE4' : ['p> u kh','뿤'],
    u'\uBFE5' : ['p> u th','뿥'],
    u'\uBFE6' : ['p> u ph','뿦'],
    u'\uBFE7' : ['p> u h','뿧'],
    u'\uBFE8' : ['p> w ^','뿨'],
    u'\uBFE9' : ['p> w ^ k','뿩'],
    u'\uBFEA' : ['p> w ^ k>','뿪'],
    u'\uBFEB' : ['p> w ^ k sh','뿫'],
    u'\uBFEC' : ['p> w ^ n','뿬'],
    u'\uBFED' : ['p> w ^ n tS','뿭'],
    u'\uBFEE' : ['p> w ^ n h','뿮'],
    u'\uBFEF' : ['p> w ^ t','뿯'],
    u'\uBFF0' : ['p> w ^ l','뿰'],
    u'\uBFF1' : ['p> w ^ l k','뿱'],
    u'\uBFF2' : ['p> w ^ l m','뿲'],
    u'\uBFF3' : ['p> w ^ l p','뿳'],
    u'\uBFF4' : ['p> w ^ l sh','뿴'],
    u'\uBFF5' : ['p> w ^ l th','뿵'],
    u'\uBFF6' : ['p> w ^ l ph','뿶'],
    u'\uBFF7' : ['p> w ^ l h','뿷'],
    u'\uBFF8' : ['p> w ^ m','뿸'],
    u'\uBFF9' : ['p> w ^ p','뿹'],
    u'\uBFFA' : ['p> w ^ p sh','뿺'],
    u'\uBFFB' : ['p> w ^ sh','뿻'],
    u'\uBFFC' : ['p> w ^ s','뿼'],
    u'\uBFFD' : ['p> w ^ N','뿽'],
    u'\uBFFE' : ['p> w ^ tS','뿾'],
    u'\uBFFF' : ['p> w ^ tSh','뿿'],
    u'\uC000' : ['p> w ^ kh','쀀'],
    u'\uC001' : ['p> w ^ th','쀁'],
    u'\uC002' : ['p> w ^ ph','쀂'],
    u'\uC003' : ['p> w ^ h','쀃'],
    u'\uC004' : ['p> w E','쀄'],
    u'\uC005' : ['p> w E k','쀅'],
    u'\uC006' : ['p> w E k>','쀆'],
    u'\uC007' : ['p> w E k sh','쀇'],
    u'\uC008' : ['p> w E n','쀈'],
    u'\uC009' : ['p> w E n tS','쀉'],
    u'\uC00A' : ['p> w E n h','쀊'],
    u'\uC00B' : ['p> w E t','쀋'],
    u'\uC00C' : ['p> w E l','쀌'],
    u'\uC00D' : ['p> w E l k','쀍'],
    u'\uC00E' : ['p> w E l m','쀎'],
    u'\uC00F' : ['p> w E l p','쀏'],
    u'\uC010' : ['p> w E l sh','쀐'],
    u'\uC011' : ['p> w E l th','쀑'],
    u'\uC012' : ['p> w E l ph','쀒'],
    u'\uC013' : ['p> w E l h','쀓'],
    u'\uC014' : ['p> w E m','쀔'],
    u'\uC015' : ['p> w E p','쀕'],
    u'\uC016' : ['p> w E p sh','쀖'],
    u'\uC017' : ['p> w E sh','쀗'],
    u'\uC018' : ['p> w E s','쀘'],
    u'\uC019' : ['p> w E N','쀙'],
    u'\uC01A' : ['p> w E tS','쀚'],
    u'\uC01B' : ['p> w E tSh','쀛'],
    u'\uC01C' : ['p> w E kh','쀜'],
    u'\uC01D' : ['p> w E th','쀝'],
    u'\uC01E' : ['p> w E ph','쀞'],
    u'\uC01F' : ['p> w E h','쀟'],
    u'\uC020' : ['p> 7','쀠'],
    u'\uC021' : ['p> 7 k','쀡'],
    u'\uC022' : ['p> 7 k>','쀢'],
    u'\uC023' : ['p> 7 k sh','쀣'],
    u'\uC024' : ['p> 7 n','쀤'],
    u'\uC025' : ['p> 7 n tS','쀥'],
    u'\uC026' : ['p> 7 n h','쀦'],
    u'\uC027' : ['p> 7 t','쀧'],
    u'\uC028' : ['p> 7 l','쀨'],
    u'\uC029' : ['p> 7 l k','쀩'],
    u'\uC02A' : ['p> 7 l m','쀪'],
    u'\uC02B' : ['p> 7 l p','쀫'],
    u'\uC02C' : ['p> 7 l sh','쀬'],
    u'\uC02D' : ['p> 7 l th','쀭'],
    u'\uC02E' : ['p> 7 l ph','쀮'],
    u'\uC02F' : ['p> 7 l h','쀯'],
    u'\uC030' : ['p> 7 m','쀰'],
    u'\uC031' : ['p> 7 p','쀱'],
    u'\uC032' : ['p> 7 p sh','쀲'],
    u'\uC033' : ['p> 7 sh','쀳'],
    u'\uC034' : ['p> 7 s','쀴'],
    u'\uC035' : ['p> 7 N','쀵'],
    u'\uC036' : ['p> 7 tS','쀶'],
    u'\uC037' : ['p> 7 tSh','쀷'],
    u'\uC038' : ['p> 7 kh','쀸'],
    u'\uC039' : ['p> 7 th','쀹'],
    u'\uC03A' : ['p> 7 ph','쀺'],
    u'\uC03B' : ['p> 7 h','쀻'],
    u'\uC03C' : ['p> j u','쀼'],
    u'\uC03D' : ['p> j u k','쀽'],
    u'\uC03E' : ['p> j u k>','쀾'],
    u'\uC03F' : ['p> j u k sh','쀿'],
    u'\uC040' : ['p> j u n','쁀'],
    u'\uC041' : ['p> j u n tS','쁁'],
    u'\uC042' : ['p> j u n h','쁂'],
    u'\uC043' : ['p> j u t','쁃'],
    u'\uC044' : ['p> j u l','쁄'],
    u'\uC045' : ['p> j u l k','쁅'],
    u'\uC046' : ['p> j u l m','쁆'],
    u'\uC047' : ['p> j u l p','쁇'],
    u'\uC048' : ['p> j u l sh','쁈'],
    u'\uC049' : ['p> j u l th','쁉'],
    u'\uC04A' : ['p> j u l ph','쁊'],
    u'\uC04B' : ['p> j u l h','쁋'],
    u'\uC04C' : ['p> j u m','쁌'],
    u'\uC04D' : ['p> j u p','쁍'],
    u'\uC04E' : ['p> j u p sh','쁎'],
    u'\uC04F' : ['p> j u sh','쁏'],
    u'\uC050' : ['p> j u s','쁐'],
    u'\uC051' : ['p> j u N','쁑'],
    u'\uC052' : ['p> j u tS','쁒'],
    u'\uC053' : ['p> j u tSh','쁓'],
    u'\uC054' : ['p> j u kh','쁔'],
    u'\uC055' : ['p> j u th','쁕'],
    u'\uC056' : ['p> j u ph','쁖'],
    u'\uC057' : ['p> j u h','쁗'],
    u'\uC058' : ['p> 4','쁘'],
    u'\uC059' : ['p> 4 k','쁙'],
    u'\uC05A' : ['p> 4 k>','쁚'],
    u'\uC05B' : ['p> 4 k sh','쁛'],
    u'\uC05C' : ['p> 4 n','쁜'],
    u'\uC05D' : ['p> 4 n tS','쁝'],
    u'\uC05E' : ['p> 4 n h','쁞'],
    u'\uC05F' : ['p> 4 t','쁟'],
    u'\uC060' : ['p> 4 l','쁠'],
    u'\uC061' : ['p> 4 l k','쁡'],
    u'\uC062' : ['p> 4 l m','쁢'],
    u'\uC063' : ['p> 4 l p','쁣'],
    u'\uC064' : ['p> 4 l sh','쁤'],
    u'\uC065' : ['p> 4 l th','쁥'],
    u'\uC066' : ['p> 4 l ph','쁦'],
    u'\uC067' : ['p> 4 l h','쁧'],
    u'\uC068' : ['p> 4 m','쁨'],
    u'\uC069' : ['p> 4 p','쁩'],
    u'\uC06A' : ['p> 4 p sh','쁪'],
    u'\uC06B' : ['p> 4 sh','쁫'],
    u'\uC06C' : ['p> 4 s','쁬'],
    u'\uC06D' : ['p> 4 N','쁭'],
    u'\uC06E' : ['p> 4 tS','쁮'],
    u'\uC06F' : ['p> 4 tSh','쁯'],
    u'\uC070' : ['p> 4 kh','쁰'],
    u'\uC071' : ['p> 4 th','쁱'],
    u'\uC072' : ['p> 4 ph','쁲'],
    u'\uC073' : ['p> 4 h','쁳'],
    u'\uC074' : ['p> 4 j','쁴'],
    u'\uC075' : ['p> 4 j k','쁵'],
    u'\uC076' : ['p> 4 j k>','쁶'],
    u'\uC077' : ['p> 4 j k sh','쁷'],
    u'\uC078' : ['p> 4 j n','쁸'],
    u'\uC079' : ['p> 4 j n tS','쁹'],
    u'\uC07A' : ['p> 4 j n h','쁺'],
    u'\uC07B' : ['p> 4 j t','쁻'],
    u'\uC07C' : ['p> 4 j l','쁼'],
    u'\uC07D' : ['p> 4 j l k','쁽'],
    u'\uC07E' : ['p> 4 j l m','쁾'],
    u'\uC07F' : ['p> 4 j l p','쁿'],
    u'\uC080' : ['p> 4 j l sh','삀'],
    u'\uC081' : ['p> 4 j l th','삁'],
    u'\uC082' : ['p> 4 j l ph','삂'],
    u'\uC083' : ['p> 4 j l h','삃'],
    u'\uC084' : ['p> 4 j m','삄'],
    u'\uC085' : ['p> 4 j p','삅'],
    u'\uC086' : ['p> 4 j p sh','삆'],
    u'\uC087' : ['p> 4 j sh','삇'],
    u'\uC088' : ['p> 4 j s','삈'],
    u'\uC089' : ['p> 4 j N','삉'],
    u'\uC08A' : ['p> 4 j tS','삊'],
    u'\uC08B' : ['p> 4 j tSh','삋'],
    u'\uC08C' : ['p> 4 j kh','삌'],
    u'\uC08D' : ['p> 4 j th','삍'],
    u'\uC08E' : ['p> 4 j ph','삎'],
    u'\uC08F' : ['p> 4 j h','삏'],
    u'\uC090' : ['p> i','삐'],
    u'\uC091' : ['p> i k','삑'],
    u'\uC092' : ['p> i k>','삒'],
    u'\uC093' : ['p> i k sh','삓'],
    u'\uC094' : ['p> i n','삔'],
    u'\uC095' : ['p> i n tS','삕'],
    u'\uC096' : ['p> i n h','삖'],
    u'\uC097' : ['p> i t','삗'],
    u'\uC098' : ['p> i l','삘'],
    u'\uC099' : ['p> i l k','삙'],
    u'\uC09A' : ['p> i l m','삚'],
    u'\uC09B' : ['p> i l p','삛'],
    u'\uC09C' : ['p> i l sh','삜'],
    u'\uC09D' : ['p> i l th','삝'],
    u'\uC09E' : ['p> i l ph','삞'],
    u'\uC09F' : ['p> i l h','삟'],
    u'\uC0A0' : ['p> i m','삠'],
    u'\uC0A1' : ['p> i p','삡'],
    u'\uC0A2' : ['p> i p sh','삢'],
    u'\uC0A3' : ['p> i sh','삣'],
    u'\uC0A4' : ['p> i s','삤'],
    u'\uC0A5' : ['p> i N','삥'],
    u'\uC0A6' : ['p> i tS','삦'],
    u'\uC0A7' : ['p> i tSh','삧'],
    u'\uC0A8' : ['p> i kh','삨'],
    u'\uC0A9' : ['p> i th','삩'],
    u'\uC0AA' : ['p> i ph','삪'],
    u'\uC0AB' : ['p> i h','삫'],
    u'\uC0AC' : ['sh a','사'],
    u'\uC0AD' : ['sh a k','삭'],
    u'\uC0AE' : ['sh a k>','삮'],
    u'\uC0AF' : ['sh a k sh','삯'],
    u'\uC0B0' : ['sh a n','산'],
    u'\uC0B1' : ['sh a n tS','삱'],
    u'\uC0B2' : ['sh a n h','삲'],
    u'\uC0B3' : ['sh a t','삳'],
    u'\uC0B4' : ['sh a l','살'],
    u'\uC0B5' : ['sh a l k','삵'],
    u'\uC0B6' : ['sh a l m','삶'],
    u'\uC0B7' : ['sh a l p','삷'],
    u'\uC0B8' : ['sh a l sh','삸'],
    u'\uC0B9' : ['sh a l th','삹'],
    u'\uC0BA' : ['sh a l ph','삺'],
    u'\uC0BB' : ['sh a l h','삻'],
    u'\uC0BC' : ['sh a m','삼'],
    u'\uC0BD' : ['sh a p','삽'],
    u'\uC0BE' : ['sh a p sh','삾'],
    u'\uC0BF' : ['sh a sh','삿'],
    u'\uC0C0' : ['sh a s','샀'],
    u'\uC0C1' : ['sh a N','상'],
    u'\uC0C2' : ['sh a tS','샂'],
    u'\uC0C3' : ['sh a tSh','샃'],
    u'\uC0C4' : ['sh a kh','샄'],
    u'\uC0C5' : ['sh a th','샅'],
    u'\uC0C6' : ['sh a ph','샆'],
    u'\uC0C7' : ['sh a h','샇'],
    u'\uC0C8' : ['sh @','새'],
    u'\uC0C9' : ['sh @ k','색'],
    u'\uC0CA' : ['sh @ k>','샊'],
    u'\uC0CB' : ['sh @ k sh','샋'],
    u'\uC0CC' : ['sh @ n','샌'],
    u'\uC0CD' : ['sh @ n tS','샍'],
    u'\uC0CE' : ['sh @ n h','샎'],
    u'\uC0CF' : ['sh @ t','샏'],
    u'\uC0D0' : ['sh @ l','샐'],
    u'\uC0D1' : ['sh @ l k','샑'],
    u'\uC0D2' : ['sh @ l m','샒'],
    u'\uC0D3' : ['sh @ l p','샓'],
    u'\uC0D4' : ['sh @ l sh','샔'],
    u'\uC0D5' : ['sh @ l th','샕'],
    u'\uC0D6' : ['sh @ l ph','샖'],
    u'\uC0D7' : ['sh @ l h','샗'],
    u'\uC0D8' : ['sh @ m','샘'],
    u'\uC0D9' : ['sh @ p','샙'],
    u'\uC0DA' : ['sh @ p sh','샚'],
    u'\uC0DB' : ['sh @ sh','샛'],
    u'\uC0DC' : ['sh @ s','샜'],
    u'\uC0DD' : ['sh @ N','생'],
    u'\uC0DE' : ['sh @ tS','샞'],
    u'\uC0DF' : ['sh @ tSh','샟'],
    u'\uC0E0' : ['sh @ kh','샠'],
    u'\uC0E1' : ['sh @ th','샡'],
    u'\uC0E2' : ['sh @ ph','샢'],
    u'\uC0E3' : ['sh @ h','샣'],
    u'\uC0E4' : ['sh j a','샤'],
    u'\uC0E5' : ['sh j a k','샥'],
    u'\uC0E6' : ['sh j a k>','샦'],
    u'\uC0E7' : ['sh j a k sh','샧'],
    u'\uC0E8' : ['sh j a n','샨'],
    u'\uC0E9' : ['sh j a n tS','샩'],
    u'\uC0EA' : ['sh j a n h','샪'],
    u'\uC0EB' : ['sh j a t','샫'],
    u'\uC0EC' : ['sh j a l','샬'],
    u'\uC0ED' : ['sh j a l k','샭'],
    u'\uC0EE' : ['sh j a l m','샮'],
    u'\uC0EF' : ['sh j a l p','샯'],
    u'\uC0F0' : ['sh j a l sh','샰'],
    u'\uC0F1' : ['sh j a l th','샱'],
    u'\uC0F2' : ['sh j a l ph','샲'],
    u'\uC0F3' : ['sh j a l h','샳'],
    u'\uC0F4' : ['sh j a m','샴'],
    u'\uC0F5' : ['sh j a p','샵'],
    u'\uC0F6' : ['sh j a p sh','샶'],
    u'\uC0F7' : ['sh j a sh','샷'],
    u'\uC0F8' : ['sh j a s','샸'],
    u'\uC0F9' : ['sh j a N','샹'],
    u'\uC0FA' : ['sh j a tS','샺'],
    u'\uC0FB' : ['sh j a tSh','샻'],
    u'\uC0FC' : ['sh j a kh','샼'],
    u'\uC0FD' : ['sh j a th','샽'],
    u'\uC0FE' : ['sh j a ph','샾'],
    u'\uC0FF' : ['sh j a h','샿'],
    u'\uC100' : ['sh j @','섀'],
    u'\uC101' : ['sh j @ k','섁'],
    u'\uC102' : ['sh j @ k>','섂'],
    u'\uC103' : ['sh j @ k sh','섃'],
    u'\uC104' : ['sh j @ n','섄'],
    u'\uC105' : ['sh j @ n tS','섅'],
    u'\uC106' : ['sh j @ n h','섆'],
    u'\uC107' : ['sh j @ t','섇'],
    u'\uC108' : ['sh j @ l','섈'],
    u'\uC109' : ['sh j @ l k','섉'],
    u'\uC10A' : ['sh j @ l m','섊'],
    u'\uC10B' : ['sh j @ l p','섋'],
    u'\uC10C' : ['sh j @ l sh','섌'],
    u'\uC10D' : ['sh j @ l th','섍'],
    u'\uC10E' : ['sh j @ l ph','섎'],
    u'\uC10F' : ['sh j @ l h','섏'],
    u'\uC110' : ['sh j @ m','섐'],
    u'\uC111' : ['sh j @ p','섑'],
    u'\uC112' : ['sh j @ p sh','섒'],
    u'\uC113' : ['sh j @ sh','섓'],
    u'\uC114' : ['sh j @ s','섔'],
    u'\uC115' : ['sh j @ N','섕'],
    u'\uC116' : ['sh j @ tS','섖'],
    u'\uC117' : ['sh j @ tSh','섗'],
    u'\uC118' : ['sh j @ kh','섘'],
    u'\uC119' : ['sh j @ th','섙'],
    u'\uC11A' : ['sh j @ ph','섚'],
    u'\uC11B' : ['sh j @ h','섛'],
    u'\uC11C' : ['sh ^','서'],
    u'\uC11D' : ['sh ^ k','석'],
    u'\uC11E' : ['sh ^ k>','섞'],
    u'\uC11F' : ['sh ^ k sh','섟'],
    u'\uC120' : ['sh ^ n','선'],
    u'\uC121' : ['sh ^ n tS','섡'],
    u'\uC122' : ['sh ^ n h','섢'],
    u'\uC123' : ['sh ^ t','섣'],
    u'\uC124' : ['sh ^ l','설'],
    u'\uC125' : ['sh ^ l k','섥'],
    u'\uC126' : ['sh ^ l m','섦'],
    u'\uC127' : ['sh ^ l p','섧'],
    u'\uC128' : ['sh ^ l sh','섨'],
    u'\uC129' : ['sh ^ l th','섩'],
    u'\uC12A' : ['sh ^ l ph','섪'],
    u'\uC12B' : ['sh ^ l h','섫'],
    u'\uC12C' : ['sh ^ m','섬'],
    u'\uC12D' : ['sh ^ p','섭'],
    u'\uC12E' : ['sh ^ p sh','섮'],
    u'\uC12F' : ['sh ^ sh','섯'],
    u'\uC130' : ['sh ^ s','섰'],
    u'\uC131' : ['sh ^ N','성'],
    u'\uC132' : ['sh ^ tS','섲'],
    u'\uC133' : ['sh ^ tSh','섳'],
    u'\uC134' : ['sh ^ kh','섴'],
    u'\uC135' : ['sh ^ th','섵'],
    u'\uC136' : ['sh ^ ph','섶'],
    u'\uC137' : ['sh ^ h','섷'],
    u'\uC138' : ['sh e','세'],
    u'\uC139' : ['sh e k','섹'],
    u'\uC13A' : ['sh e k>','섺'],
    u'\uC13B' : ['sh e k sh','섻'],
    u'\uC13C' : ['sh e n','센'],
    u'\uC13D' : ['sh e n tS','섽'],
    u'\uC13E' : ['sh e n h','섾'],
    u'\uC13F' : ['sh e t','섿'],
    u'\uC140' : ['sh e l','셀'],
    u'\uC141' : ['sh e l k','셁'],
    u'\uC142' : ['sh e l m','셂'],
    u'\uC143' : ['sh e l p','셃'],
    u'\uC144' : ['sh e l sh','셄'],
    u'\uC145' : ['sh e l th','셅'],
    u'\uC146' : ['sh e l ph','셆'],
    u'\uC147' : ['sh e l h','셇'],
    u'\uC148' : ['sh e m','셈'],
    u'\uC149' : ['sh e p','셉'],
    u'\uC14A' : ['sh e p sh','셊'],
    u'\uC14B' : ['sh e sh','셋'],
    u'\uC14C' : ['sh e s','셌'],
    u'\uC14D' : ['sh e N','셍'],
    u'\uC14E' : ['sh e tS','셎'],
    u'\uC14F' : ['sh e tSh','셏'],
    u'\uC150' : ['sh e kh','셐'],
    u'\uC151' : ['sh e th','셑'],
    u'\uC152' : ['sh e ph','셒'],
    u'\uC153' : ['sh e h','셓'],
    u'\uC154' : ['sh j ^','셔'],
    u'\uC155' : ['sh j ^ k','셕'],
    u'\uC156' : ['sh j ^ k>','셖'],
    u'\uC157' : ['sh j ^ k sh','셗'],
    u'\uC158' : ['sh j ^ n','션'],
    u'\uC159' : ['sh j ^ n tS','셙'],
    u'\uC15A' : ['sh j ^ n h','셚'],
    u'\uC15B' : ['sh j ^ t','셛'],
    u'\uC15C' : ['sh j ^ l','셜'],
    u'\uC15D' : ['sh j ^ l k','셝'],
    u'\uC15E' : ['sh j ^ l m','셞'],
    u'\uC15F' : ['sh j ^ l p','셟'],
    u'\uC160' : ['sh j ^ l sh','셠'],
    u'\uC161' : ['sh j ^ l th','셡'],
    u'\uC162' : ['sh j ^ l ph','셢'],
    u'\uC163' : ['sh j ^ l h','셣'],
    u'\uC164' : ['sh j ^ m','셤'],
    u'\uC165' : ['sh j ^ p','셥'],
    u'\uC166' : ['sh j ^ p sh','셦'],
    u'\uC167' : ['sh j ^ sh','셧'],
    u'\uC168' : ['sh j ^ s','셨'],
    u'\uC169' : ['sh j ^ N','셩'],
    u'\uC16A' : ['sh j ^ tS','셪'],
    u'\uC16B' : ['sh j ^ tSh','셫'],
    u'\uC16C' : ['sh j ^ kh','셬'],
    u'\uC16D' : ['sh j ^ th','셭'],
    u'\uC16E' : ['sh j ^ ph','셮'],
    u'\uC16F' : ['sh j ^ h','셯'],
    u'\uC170' : ['sh j e','셰'],
    u'\uC171' : ['sh j e k','셱'],
    u'\uC172' : ['sh j e k>','셲'],
    u'\uC173' : ['sh j e k sh','셳'],
    u'\uC174' : ['sh j e n','셴'],
    u'\uC175' : ['sh j e n tS','셵'],
    u'\uC176' : ['sh j e n h','셶'],
    u'\uC177' : ['sh j e t','셷'],
    u'\uC178' : ['sh j e l','셸'],
    u'\uC179' : ['sh j e l k','셹'],
    u'\uC17A' : ['sh j e l m','셺'],
    u'\uC17B' : ['sh j e l p','셻'],
    u'\uC17C' : ['sh j e l sh','셼'],
    u'\uC17D' : ['sh j e l th','셽'],
    u'\uC17E' : ['sh j e l ph','셾'],
    u'\uC17F' : ['sh j e l h','셿'],
    u'\uC180' : ['sh j e m','솀'],
    u'\uC181' : ['sh j e p','솁'],
    u'\uC182' : ['sh j e p sh','솂'],
    u'\uC183' : ['sh j e sh','솃'],
    u'\uC184' : ['sh j e s','솄'],
    u'\uC185' : ['sh j e N','솅'],
    u'\uC186' : ['sh j e tS','솆'],
    u'\uC187' : ['sh j e tSh','솇'],
    u'\uC188' : ['sh j e kh','솈'],
    u'\uC189' : ['sh j e th','솉'],
    u'\uC18A' : ['sh j e ph','솊'],
    u'\uC18B' : ['sh j e h','솋'],
    u'\uC18C' : ['sh o','소'],
    u'\uC18D' : ['sh o k','속'],
    u'\uC18E' : ['sh o k>','솎'],
    u'\uC18F' : ['sh o k sh','솏'],
    u'\uC190' : ['sh o n','손'],
    u'\uC191' : ['sh o n tS','솑'],
    u'\uC192' : ['sh o n h','솒'],
    u'\uC193' : ['sh o t','솓'],
    u'\uC194' : ['sh o l','솔'],
    u'\uC195' : ['sh o l k','솕'],
    u'\uC196' : ['sh o l m','솖'],
    u'\uC197' : ['sh o l p','솗'],
    u'\uC198' : ['sh o l sh','솘'],
    u'\uC199' : ['sh o l th','솙'],
    u'\uC19A' : ['sh o l ph','솚'],
    u'\uC19B' : ['sh o l h','솛'],
    u'\uC19C' : ['sh o m','솜'],
    u'\uC19D' : ['sh o p','솝'],
    u'\uC19E' : ['sh o p sh','솞'],
    u'\uC19F' : ['sh o sh','솟'],
    u'\uC1A0' : ['sh o s','솠'],
    u'\uC1A1' : ['sh o N','송'],
    u'\uC1A2' : ['sh o tS','솢'],
    u'\uC1A3' : ['sh o tSh','솣'],
    u'\uC1A4' : ['sh o kh','솤'],
    u'\uC1A5' : ['sh o th','솥'],
    u'\uC1A6' : ['sh o ph','솦'],
    u'\uC1A7' : ['sh o h','솧'],
    u'\uC1A8' : ['sh w a','솨'],
    u'\uC1A9' : ['sh w a k','솩'],
    u'\uC1AA' : ['sh w a k>','솪'],
    u'\uC1AB' : ['sh w a k sh','솫'],
    u'\uC1AC' : ['sh w a n','솬'],
    u'\uC1AD' : ['sh w a n tS','솭'],
    u'\uC1AE' : ['sh w a n h','솮'],
    u'\uC1AF' : ['sh w a t','솯'],
    u'\uC1B0' : ['sh w a l','솰'],
    u'\uC1B1' : ['sh w a l k','솱'],
    u'\uC1B2' : ['sh w a l m','솲'],
    u'\uC1B3' : ['sh w a l p','솳'],
    u'\uC1B4' : ['sh w a l sh','솴'],
    u'\uC1B5' : ['sh w a l th','솵'],
    u'\uC1B6' : ['sh w a l ph','솶'],
    u'\uC1B7' : ['sh w a l h','솷'],
    u'\uC1B8' : ['sh w a m','솸'],
    u'\uC1B9' : ['sh w a p','솹'],
    u'\uC1BA' : ['sh w a p sh','솺'],
    u'\uC1BB' : ['sh w a sh','솻'],
    u'\uC1BC' : ['sh w a s','솼'],
    u'\uC1BD' : ['sh w a N','솽'],
    u'\uC1BE' : ['sh w a tS','솾'],
    u'\uC1BF' : ['sh w a tSh','솿'],
    u'\uC1C0' : ['sh w a kh','쇀'],
    u'\uC1C1' : ['sh w a th','쇁'],
    u'\uC1C2' : ['sh w a ph','쇂'],
    u'\uC1C3' : ['sh w a h','쇃'],
    u'\uC1C4' : ['sh w @','쇄'],
    u'\uC1C5' : ['sh w @ k','쇅'],
    u'\uC1C6' : ['sh w @ k>','쇆'],
    u'\uC1C7' : ['sh w @ k sh','쇇'],
    u'\uC1C8' : ['sh w @ n','쇈'],
    u'\uC1C9' : ['sh w @ n tS','쇉'],
    u'\uC1CA' : ['sh w @ n h','쇊'],
    u'\uC1CB' : ['sh w @ t','쇋'],
    u'\uC1CC' : ['sh w @ l','쇌'],
    u'\uC1CD' : ['sh w @ l k','쇍'],
    u'\uC1CE' : ['sh w @ l m','쇎'],
    u'\uC1CF' : ['sh w @ l p','쇏'],
    u'\uC1D0' : ['sh w @ l sh','쇐'],
    u'\uC1D1' : ['sh w @ l th','쇑'],
    u'\uC1D2' : ['sh w @ l ph','쇒'],
    u'\uC1D3' : ['sh w @ l h','쇓'],
    u'\uC1D4' : ['sh w @ m','쇔'],
    u'\uC1D5' : ['sh w @ p','쇕'],
    u'\uC1D6' : ['sh w @ p sh','쇖'],
    u'\uC1D7' : ['sh w @ sh','쇗'],
    u'\uC1D8' : ['sh w @ s','쇘'],
    u'\uC1D9' : ['sh w @ N','쇙'],
    u'\uC1DA' : ['sh w @ tS','쇚'],
    u'\uC1DB' : ['sh w @ tSh','쇛'],
    u'\uC1DC' : ['sh w @ kh','쇜'],
    u'\uC1DD' : ['sh w @ th','쇝'],
    u'\uC1DE' : ['sh w @ ph','쇞'],
    u'\uC1DF' : ['sh w @ h','쇟'],
    u'\uC1E0' : ['sh w e','쇠'],
    u'\uC1E1' : ['sh w e k','쇡'],
    u'\uC1E2' : ['sh w e k>','쇢'],
    u'\uC1E3' : ['sh w e k sh','쇣'],
    u'\uC1E4' : ['sh w e n','쇤'],
    u'\uC1E5' : ['sh w e n tS','쇥'],
    u'\uC1E6' : ['sh w e n h','쇦'],
    u'\uC1E7' : ['sh w e t','쇧'],
    u'\uC1E8' : ['sh w e l','쇨'],
    u'\uC1E9' : ['sh w e l k','쇩'],
    u'\uC1EA' : ['sh w e l m','쇪'],
    u'\uC1EB' : ['sh w e l p','쇫'],
    u'\uC1EC' : ['sh w e l sh','쇬'],
    u'\uC1ED' : ['sh w e l th','쇭'],
    u'\uC1EE' : ['sh w e l ph','쇮'],
    u'\uC1EF' : ['sh w e l h','쇯'],
    u'\uC1F0' : ['sh w e m','쇰'],
    u'\uC1F1' : ['sh w e p','쇱'],
    u'\uC1F2' : ['sh w e p sh','쇲'],
    u'\uC1F3' : ['sh w e sh','쇳'],
    u'\uC1F4' : ['sh w e s','쇴'],
    u'\uC1F5' : ['sh w e N','쇵'],
    u'\uC1F6' : ['sh w e tS','쇶'],
    u'\uC1F7' : ['sh w e tSh','쇷'],
    u'\uC1F8' : ['sh w e kh','쇸'],
    u'\uC1F9' : ['sh w e th','쇹'],
    u'\uC1FA' : ['sh w e ph','쇺'],
    u'\uC1FB' : ['sh w e h','쇻'],
    u'\uC1FC' : ['sh j o','쇼'],
    u'\uC1FD' : ['sh j o k','쇽'],
    u'\uC1FE' : ['sh j o k>','쇾'],
    u'\uC1FF' : ['sh j o k sh','쇿'],
    u'\uC200' : ['sh j o n','숀'],
    u'\uC201' : ['sh j o n tS','숁'],
    u'\uC202' : ['sh j o n h','숂'],
    u'\uC203' : ['sh j o t','숃'],
    u'\uC204' : ['sh j o l','숄'],
    u'\uC205' : ['sh j o l k','숅'],
    u'\uC206' : ['sh j o l m','숆'],
    u'\uC207' : ['sh j o l p','숇'],
    u'\uC208' : ['sh j o l sh','숈'],
    u'\uC209' : ['sh j o l th','숉'],
    u'\uC20A' : ['sh j o l ph','숊'],
    u'\uC20B' : ['sh j o l h','숋'],
    u'\uC20C' : ['sh j o m','숌'],
    u'\uC20D' : ['sh j o p','숍'],
    u'\uC20E' : ['sh j o p sh','숎'],
    u'\uC20F' : ['sh j o sh','숏'],
    u'\uC210' : ['sh j o s','숐'],
    u'\uC211' : ['sh j o N','숑'],
    u'\uC212' : ['sh j o tS','숒'],
    u'\uC213' : ['sh j o tSh','숓'],
    u'\uC214' : ['sh j o kh','숔'],
    u'\uC215' : ['sh j o th','숕'],
    u'\uC216' : ['sh j o ph','숖'],
    u'\uC217' : ['sh j o h','숗'],
    u'\uC218' : ['sh u','수'],
    u'\uC219' : ['sh u k','숙'],
    u'\uC21A' : ['sh u k>','숚'],
    u'\uC21B' : ['sh u k sh','숛'],
    u'\uC21C' : ['sh u n','순'],
    u'\uC21D' : ['sh u n tS','숝'],
    u'\uC21E' : ['sh u n h','숞'],
    u'\uC21F' : ['sh u t','숟'],
    u'\uC220' : ['sh u l','술'],
    u'\uC221' : ['sh u l k','숡'],
    u'\uC222' : ['sh u l m','숢'],
    u'\uC223' : ['sh u l p','숣'],
    u'\uC224' : ['sh u l sh','숤'],
    u'\uC225' : ['sh u l th','숥'],
    u'\uC226' : ['sh u l ph','숦'],
    u'\uC227' : ['sh u l h','숧'],
    u'\uC228' : ['sh u m','숨'],
    u'\uC229' : ['sh u p','숩'],
    u'\uC22A' : ['sh u p sh','숪'],
    u'\uC22B' : ['sh u sh','숫'],
    u'\uC22C' : ['sh u s','숬'],
    u'\uC22D' : ['sh u N','숭'],
    u'\uC22E' : ['sh u tS','숮'],
    u'\uC22F' : ['sh u tSh','숯'],
    u'\uC230' : ['sh u kh','숰'],
    u'\uC231' : ['sh u th','숱'],
    u'\uC232' : ['sh u ph','숲'],
    u'\uC233' : ['sh u h','숳'],
    u'\uC234' : ['sh w ^','숴'],
    u'\uC235' : ['sh w ^ k','숵'],
    u'\uC236' : ['sh w ^ k>','숶'],
    u'\uC237' : ['sh w ^ k sh','숷'],
    u'\uC238' : ['sh w ^ n','숸'],
    u'\uC239' : ['sh w ^ n tS','숹'],
    u'\uC23A' : ['sh w ^ n h','숺'],
    u'\uC23B' : ['sh w ^ t','숻'],
    u'\uC23C' : ['sh w ^ l','숼'],
    u'\uC23D' : ['sh w ^ l k','숽'],
    u'\uC23E' : ['sh w ^ l m','숾'],
    u'\uC23F' : ['sh w ^ l p','숿'],
    u'\uC240' : ['sh w ^ l sh','쉀'],
    u'\uC241' : ['sh w ^ l th','쉁'],
    u'\uC242' : ['sh w ^ l ph','쉂'],
    u'\uC243' : ['sh w ^ l h','쉃'],
    u'\uC244' : ['sh w ^ m','쉄'],
    u'\uC245' : ['sh w ^ p','쉅'],
    u'\uC246' : ['sh w ^ p sh','쉆'],
    u'\uC247' : ['sh w ^ sh','쉇'],
    u'\uC248' : ['sh w ^ s','쉈'],
    u'\uC249' : ['sh w ^ N','쉉'],
    u'\uC24A' : ['sh w ^ tS','쉊'],
    u'\uC24B' : ['sh w ^ tSh','쉋'],
    u'\uC24C' : ['sh w ^ kh','쉌'],
    u'\uC24D' : ['sh w ^ th','쉍'],
    u'\uC24E' : ['sh w ^ ph','쉎'],
    u'\uC24F' : ['sh w ^ h','쉏'],
    u'\uC250' : ['sh w E','쉐'],
    u'\uC251' : ['sh w E k','쉑'],
    u'\uC252' : ['sh w E k>','쉒'],
    u'\uC253' : ['sh w E k sh','쉓'],
    u'\uC254' : ['sh w E n','쉔'],
    u'\uC255' : ['sh w E n tS','쉕'],
    u'\uC256' : ['sh w E n h','쉖'],
    u'\uC257' : ['sh w E t','쉗'],
    u'\uC258' : ['sh w E l','쉘'],
    u'\uC259' : ['sh w E l k','쉙'],
    u'\uC25A' : ['sh w E l m','쉚'],
    u'\uC25B' : ['sh w E l p','쉛'],
    u'\uC25C' : ['sh w E l sh','쉜'],
    u'\uC25D' : ['sh w E l th','쉝'],
    u'\uC25E' : ['sh w E l ph','쉞'],
    u'\uC25F' : ['sh w E l h','쉟'],
    u'\uC260' : ['sh w E m','쉠'],
    u'\uC261' : ['sh w E p','쉡'],
    u'\uC262' : ['sh w E p sh','쉢'],
    u'\uC263' : ['sh w E sh','쉣'],
    u'\uC264' : ['sh w E s','쉤'],
    u'\uC265' : ['sh w E N','쉥'],
    u'\uC266' : ['sh w E tS','쉦'],
    u'\uC267' : ['sh w E tSh','쉧'],
    u'\uC268' : ['sh w E kh','쉨'],
    u'\uC269' : ['sh w E th','쉩'],
    u'\uC26A' : ['sh w E ph','쉪'],
    u'\uC26B' : ['sh w E h','쉫'],
    u'\uC26C' : ['sh 7','쉬'],
    u'\uC26D' : ['sh 7 k','쉭'],
    u'\uC26E' : ['sh 7 k>','쉮'],
    u'\uC26F' : ['sh 7 k sh','쉯'],
    u'\uC270' : ['sh 7 n','쉰'],
    u'\uC271' : ['sh 7 n tS','쉱'],
    u'\uC272' : ['sh 7 n h','쉲'],
    u'\uC273' : ['sh 7 t','쉳'],
    u'\uC274' : ['sh 7 l','쉴'],
    u'\uC275' : ['sh 7 l k','쉵'],
    u'\uC276' : ['sh 7 l m','쉶'],
    u'\uC277' : ['sh 7 l p','쉷'],
    u'\uC278' : ['sh 7 l sh','쉸'],
    u'\uC279' : ['sh 7 l th','쉹'],
    u'\uC27A' : ['sh 7 l ph','쉺'],
    u'\uC27B' : ['sh 7 l h','쉻'],
    u'\uC27C' : ['sh 7 m','쉼'],
    u'\uC27D' : ['sh 7 p','쉽'],
    u'\uC27E' : ['sh 7 p sh','쉾'],
    u'\uC27F' : ['sh 7 sh','쉿'],
    u'\uC280' : ['sh 7 s','슀'],
    u'\uC281' : ['sh 7 N','슁'],
    u'\uC282' : ['sh 7 tS','슂'],
    u'\uC283' : ['sh 7 tSh','슃'],
    u'\uC284' : ['sh 7 kh','슄'],
    u'\uC285' : ['sh 7 th','슅'],
    u'\uC286' : ['sh 7 ph','슆'],
    u'\uC287' : ['sh 7 h','슇'],
    u'\uC288' : ['sh j u','슈'],
    u'\uC289' : ['sh j u k','슉'],
    u'\uC28A' : ['sh j u k>','슊'],
    u'\uC28B' : ['sh j u k sh','슋'],
    u'\uC28C' : ['sh j u n','슌'],
    u'\uC28D' : ['sh j u n tS','슍'],
    u'\uC28E' : ['sh j u n h','슎'],
    u'\uC28F' : ['sh j u t','슏'],
    u'\uC290' : ['sh j u l','슐'],
    u'\uC291' : ['sh j u l k','슑'],
    u'\uC292' : ['sh j u l m','슒'],
    u'\uC293' : ['sh j u l p','슓'],
    u'\uC294' : ['sh j u l sh','슔'],
    u'\uC295' : ['sh j u l th','슕'],
    u'\uC296' : ['sh j u l ph','슖'],
    u'\uC297' : ['sh j u l h','슗'],
    u'\uC298' : ['sh j u m','슘'],
    u'\uC299' : ['sh j u p','슙'],
    u'\uC29A' : ['sh j u p sh','슚'],
    u'\uC29B' : ['sh j u sh','슛'],
    u'\uC29C' : ['sh j u s','슜'],
    u'\uC29D' : ['sh j u N','슝'],
    u'\uC29E' : ['sh j u tS','슞'],
    u'\uC29F' : ['sh j u tSh','슟'],
    u'\uC2A0' : ['sh j u kh','슠'],
    u'\uC2A1' : ['sh j u th','슡'],
    u'\uC2A2' : ['sh j u ph','슢'],
    u'\uC2A3' : ['sh j u h','슣'],
    u'\uC2A4' : ['sh 4','스'],
    u'\uC2A5' : ['sh 4 k','슥'],
    u'\uC2A6' : ['sh 4 k>','슦'],
    u'\uC2A7' : ['sh 4 k sh','슧'],
    u'\uC2A8' : ['sh 4 n','슨'],
    u'\uC2A9' : ['sh 4 n tS','슩'],
    u'\uC2AA' : ['sh 4 n h','슪'],
    u'\uC2AB' : ['sh 4 t','슫'],
    u'\uC2AC' : ['sh 4 l','슬'],
    u'\uC2AD' : ['sh 4 l k','슭'],
    u'\uC2AE' : ['sh 4 l m','슮'],
    u'\uC2AF' : ['sh 4 l p','슯'],
    u'\uC2B0' : ['sh 4 l sh','슰'],
    u'\uC2B1' : ['sh 4 l th','슱'],
    u'\uC2B2' : ['sh 4 l ph','슲'],
    u'\uC2B3' : ['sh 4 l h','슳'],
    u'\uC2B4' : ['sh 4 m','슴'],
    u'\uC2B5' : ['sh 4 p','습'],
    u'\uC2B6' : ['sh 4 p sh','슶'],
    u'\uC2B7' : ['sh 4 sh','슷'],
    u'\uC2B8' : ['sh 4 s','슸'],
    u'\uC2B9' : ['sh 4 N','승'],
    u'\uC2BA' : ['sh 4 tS','슺'],
    u'\uC2BB' : ['sh 4 tSh','슻'],
    u'\uC2BC' : ['sh 4 kh','슼'],
    u'\uC2BD' : ['sh 4 th','슽'],
    u'\uC2BE' : ['sh 4 ph','슾'],
    u'\uC2BF' : ['sh 4 h','슿'],
    u'\uC2C0' : ['sh 4 j','싀'],
    u'\uC2C1' : ['sh 4 j k','싁'],
    u'\uC2C2' : ['sh 4 j k>','싂'],
    u'\uC2C3' : ['sh 4 j k sh','싃'],
    u'\uC2C4' : ['sh 4 j n','싄'],
    u'\uC2C5' : ['sh 4 j n tS','싅'],
    u'\uC2C6' : ['sh 4 j n h','싆'],
    u'\uC2C7' : ['sh 4 j t','싇'],
    u'\uC2C8' : ['sh 4 j l','싈'],
    u'\uC2C9' : ['sh 4 j l k','싉'],
    u'\uC2CA' : ['sh 4 j l m','싊'],
    u'\uC2CB' : ['sh 4 j l p','싋'],
    u'\uC2CC' : ['sh 4 j l sh','싌'],
    u'\uC2CD' : ['sh 4 j l th','싍'],
    u'\uC2CE' : ['sh 4 j l ph','싎'],
    u'\uC2CF' : ['sh 4 j l h','싏'],
    u'\uC2D0' : ['sh 4 j m','싐'],
    u'\uC2D1' : ['sh 4 j p','싑'],
    u'\uC2D2' : ['sh 4 j p sh','싒'],
    u'\uC2D3' : ['sh 4 j sh','싓'],
    u'\uC2D4' : ['sh 4 j s','싔'],
    u'\uC2D5' : ['sh 4 j N','싕'],
    u'\uC2D6' : ['sh 4 j tS','싖'],
    u'\uC2D7' : ['sh 4 j tSh','싗'],
    u'\uC2D8' : ['sh 4 j kh','싘'],
    u'\uC2D9' : ['sh 4 j th','싙'],
    u'\uC2DA' : ['sh 4 j ph','싚'],
    u'\uC2DB' : ['sh 4 j h','싛'],
    u'\uC2DC' : ['sh i','시'],
    u'\uC2DD' : ['sh i k','식'],
    u'\uC2DE' : ['sh i k>','싞'],
    u'\uC2DF' : ['sh i k sh','싟'],
    u'\uC2E0' : ['sh i n','신'],
    u'\uC2E1' : ['sh i n tS','싡'],
    u'\uC2E2' : ['sh i n h','싢'],
    u'\uC2E3' : ['sh i t','싣'],
    u'\uC2E4' : ['sh i l','실'],
    u'\uC2E5' : ['sh i l k','싥'],
    u'\uC2E6' : ['sh i l m','싦'],
    u'\uC2E7' : ['sh i l p','싧'],
    u'\uC2E8' : ['sh i l sh','싨'],
    u'\uC2E9' : ['sh i l th','싩'],
    u'\uC2EA' : ['sh i l ph','싪'],
    u'\uC2EB' : ['sh i l h','싫'],
    u'\uC2EC' : ['sh i m','심'],
    u'\uC2ED' : ['sh i p','십'],
    u'\uC2EE' : ['sh i p sh','싮'],
    u'\uC2EF' : ['sh i sh','싯'],
    u'\uC2F0' : ['sh i s','싰'],
    u'\uC2F1' : ['sh i N','싱'],
    u'\uC2F2' : ['sh i tS','싲'],
    u'\uC2F3' : ['sh i tSh','싳'],
    u'\uC2F4' : ['sh i kh','싴'],
    u'\uC2F5' : ['sh i th','싵'],
    u'\uC2F6' : ['sh i ph','싶'],
    u'\uC2F7' : ['sh i h','싷'],
    u'\uC2F8' : ['s a','싸'],
    u'\uC2F9' : ['s a k','싹'],
    u'\uC2FA' : ['s a k>','싺'],
    u'\uC2FB' : ['s a k sh','싻'],
    u'\uC2FC' : ['s a n','싼'],
    u'\uC2FD' : ['s a n tS','싽'],
    u'\uC2FE' : ['s a n h','싾'],
    u'\uC2FF' : ['s a t','싿'],
    u'\uC300' : ['s a l','쌀'],
    u'\uC301' : ['s a l k','쌁'],
    u'\uC302' : ['s a l m','쌂'],
    u'\uC303' : ['s a l p','쌃'],
    u'\uC304' : ['s a l sh','쌄'],
    u'\uC305' : ['s a l th','쌅'],
    u'\uC306' : ['s a l ph','쌆'],
    u'\uC307' : ['s a l h','쌇'],
    u'\uC308' : ['s a m','쌈'],
    u'\uC309' : ['s a p','쌉'],
    u'\uC30A' : ['s a p sh','쌊'],
    u'\uC30B' : ['s a sh','쌋'],
    u'\uC30C' : ['s a s','쌌'],
    u'\uC30D' : ['s a N','쌍'],
    u'\uC30E' : ['s a tS','쌎'],
    u'\uC30F' : ['s a tSh','쌏'],
    u'\uC310' : ['s a kh','쌐'],
    u'\uC311' : ['s a th','쌑'],
    u'\uC312' : ['s a ph','쌒'],
    u'\uC313' : ['s a h','쌓'],
    u'\uC314' : ['s @','쌔'],
    u'\uC315' : ['s @ k','쌕'],
    u'\uC316' : ['s @ k>','쌖'],
    u'\uC317' : ['s @ k sh','쌗'],
    u'\uC318' : ['s @ n','쌘'],
    u'\uC319' : ['s @ n tS','쌙'],
    u'\uC31A' : ['s @ n h','쌚'],
    u'\uC31B' : ['s @ t','쌛'],
    u'\uC31C' : ['s @ l','쌜'],
    u'\uC31D' : ['s @ l k','쌝'],
    u'\uC31E' : ['s @ l m','쌞'],
    u'\uC31F' : ['s @ l p','쌟'],
    u'\uC320' : ['s @ l sh','쌠'],
    u'\uC321' : ['s @ l th','쌡'],
    u'\uC322' : ['s @ l ph','쌢'],
    u'\uC323' : ['s @ l h','쌣'],
    u'\uC324' : ['s @ m','쌤'],
    u'\uC325' : ['s @ p','쌥'],
    u'\uC326' : ['s @ p sh','쌦'],
    u'\uC327' : ['s @ sh','쌧'],
    u'\uC328' : ['s @ s','쌨'],
    u'\uC329' : ['s @ N','쌩'],
    u'\uC32A' : ['s @ tS','쌪'],
    u'\uC32B' : ['s @ tSh','쌫'],
    u'\uC32C' : ['s @ kh','쌬'],
    u'\uC32D' : ['s @ th','쌭'],
    u'\uC32E' : ['s @ ph','쌮'],
    u'\uC32F' : ['s @ h','쌯'],
    u'\uC330' : ['s j a','쌰'],
    u'\uC331' : ['s j a k','쌱'],
    u'\uC332' : ['s j a k>','쌲'],
    u'\uC333' : ['s j a k sh','쌳'],
    u'\uC334' : ['s j a n','쌴'],
    u'\uC335' : ['s j a n tS','쌵'],
    u'\uC336' : ['s j a n h','쌶'],
    u'\uC337' : ['s j a t','쌷'],
    u'\uC338' : ['s j a l','쌸'],
    u'\uC339' : ['s j a l k','쌹'],
    u'\uC33A' : ['s j a l m','쌺'],
    u'\uC33B' : ['s j a l p','쌻'],
    u'\uC33C' : ['s j a l sh','쌼'],
    u'\uC33D' : ['s j a l th','쌽'],
    u'\uC33E' : ['s j a l ph','쌾'],
    u'\uC33F' : ['s j a l s','쌿'],
    u'\uC340' : ['s j a m','썀'],
    u'\uC341' : ['s j a p','썁'],
    u'\uC342' : ['s j a p sh','썂'],
    u'\uC343' : ['s j a sh','썃'],
    u'\uC344' : ['s j a s','썄'],
    u'\uC345' : ['s j a N','썅'],
    u'\uC346' : ['s j a tS','썆'],
    u'\uC347' : ['s j a tSh','썇'],
    u'\uC348' : ['s j a kh','썈'],
    u'\uC349' : ['s j a th','썉'],
    u'\uC34A' : ['s j a ph','썊'],
    u'\uC34B' : ['s j a h','썋'],
    u'\uC34C' : ['s j @','썌'],
    u'\uC34D' : ['s j @ k','썍'],
    u'\uC34E' : ['s j @ k>','썎'],
    u'\uC34F' : ['s j @ k sh','썏'],
    u'\uC350' : ['s j @ n','썐'],
    u'\uC351' : ['s j @ n tS','썑'],
    u'\uC352' : ['s j @ n h','썒'],
    u'\uC353' : ['s j @ t','썓'],
    u'\uC354' : ['s j @ l','썔'],
    u'\uC355' : ['s j @ l k','썕'],
    u'\uC356' : ['s j @ l m','썖'],
    u'\uC357' : ['s j @ l p','썗'],
    u'\uC358' : ['s j @ l sh','썘'],
    u'\uC359' : ['s j @ l th','썙'],
    u'\uC35A' : ['s j @ l ph','썚'],
    u'\uC35B' : ['s j @ l h','썛'],
    u'\uC35C' : ['s j @ m','썜'],
    u'\uC35D' : ['s j @ p','썝'],
    u'\uC35E' : ['s j @ p sh','썞'],
    u'\uC35F' : ['s j @ sh','썟'],
    u'\uC360' : ['s j @ s','썠'],
    u'\uC361' : ['s j @ N','썡'],
    u'\uC362' : ['s j @ tS','썢'],
    u'\uC363' : ['s j @ tSh','썣'],
    u'\uC364' : ['s j @ kh','썤'],
    u'\uC365' : ['s j @ th','썥'],
    u'\uC366' : ['s j @ ph','썦'],
    u'\uC367' : ['s j @ h','썧'],
    u'\uC368' : ['s ^','써'],
    u'\uC369' : ['s ^ k','썩'],
    u'\uC36A' : ['s ^ k>','썪'],
    u'\uC36B' : ['s ^ k sh','썫'],
    u'\uC36C' : ['s ^ n','썬'],
    u'\uC36D' : ['s ^ n tS','썭'],
    u'\uC36E' : ['s ^ n h','썮'],
    u'\uC36F' : ['s ^ t','썯'],
    u'\uC370' : ['s ^ l','썰'],
    u'\uC371' : ['s ^ l k','썱'],
    u'\uC372' : ['s ^ l m','썲'],
    u'\uC373' : ['s ^ l p','썳'],
    u'\uC374' : ['s ^ l sh','썴'],
    u'\uC375' : ['s ^ l th','썵'],
    u'\uC376' : ['s ^ l ph','썶'],
    u'\uC377' : ['s ^ l h','썷'],
    u'\uC378' : ['s ^ m','썸'],
    u'\uC379' : ['s ^ p','썹'],
    u'\uC37A' : ['s ^ p sh','썺'],
    u'\uC37B' : ['s ^ sh','썻'],
    u'\uC37C' : ['s ^ s','썼'],
    u'\uC37D' : ['s ^ N','썽'],
    u'\uC37E' : ['s ^ tS','썾'],
    u'\uC37F' : ['s ^ tSh','썿'],
    u'\uC380' : ['s ^ kh','쎀'],
    u'\uC381' : ['s ^ th','쎁'],
    u'\uC382' : ['s ^ ph','쎂'],
    u'\uC383' : ['s ^ h','쎃'],
    u'\uC384' : ['s e','쎄'],
    u'\uC385' : ['s e k','쎅'],
    u'\uC386' : ['s e k>','쎆'],
    u'\uC387' : ['s e k sh','쎇'],
    u'\uC388' : ['s e n','쎈'],
    u'\uC389' : ['s e n tS','쎉'],
    u'\uC38A' : ['s e n h','쎊'],
    u'\uC38B' : ['s e t','쎋'],
    u'\uC38C' : ['s e l','쎌'],
    u'\uC38D' : ['s e l k','쎍'],
    u'\uC38E' : ['s e l m','쎎'],
    u'\uC38F' : ['s e l p','쎏'],
    u'\uC390' : ['s e l sh','쎐'],
    u'\uC391' : ['s e l th','쎑'],
    u'\uC392' : ['s e l ph','쎒'],
    u'\uC393' : ['s e l h','쎓'],
    u'\uC394' : ['s e m','쎔'],
    u'\uC395' : ['s e p','쎕'],
    u'\uC396' : ['s e p sh','쎖'],
    u'\uC397' : ['s e sh','쎗'],
    u'\uC398' : ['s e s','쎘'],
    u'\uC399' : ['s e N','쎙'],
    u'\uC39A' : ['s e tS','쎚'],
    u'\uC39B' : ['s e tSh','쎛'],
    u'\uC39C' : ['s e kh','쎜'],
    u'\uC39D' : ['s e th','쎝'],
    u'\uC39E' : ['s e ph','쎞'],
    u'\uC39F' : ['s e h','쎟'],
    u'\uC3A0' : ['s j ^','쎠'],
    u'\uC3A1' : ['s j ^ k','쎡'],
    u'\uC3A2' : ['s j ^ k>','쎢'],
    u'\uC3A3' : ['s j ^ k sh','쎣'],
    u'\uC3A4' : ['s j ^ n','쎤'],
    u'\uC3A5' : ['s j ^ n tS','쎥'],
    u'\uC3A6' : ['s j ^ nh','쎦'],
    u'\uC3A7' : ['s j ^ t','쎧'],
    u'\uC3A8' : ['s j ^ l','쎨'],
    u'\uC3A9' : ['s j ^ l k','쎩'],
    u'\uC3AA' : ['s j ^ l m','쎪'],
    u'\uC3AB' : ['s j ^ l p','쎫'],
    u'\uC3AC' : ['s j ^ l sh','쎬'],
    u'\uC3AD' : ['s j ^ l th','쎭'],
    u'\uC3AE' : ['s j ^ l ph','쎮'],
    u'\uC3AF' : ['s j ^ l h','쎯'],
    u'\uC3B0' : ['s j ^ m','쎰'],
    u'\uC3B1' : ['s j ^ p','쎱'],
    u'\uC3B2' : ['s j ^ p sh','쎲'],
    u'\uC3B3' : ['s j ^ sh','쎳'],
    u'\uC3B4' : ['s j ^ s','쎴'],
    u'\uC3B5' : ['s j ^ N','쎵'],
    u'\uC3B6' : ['s j ^ tS','쎶'],
    u'\uC3B7' : ['s j ^ tSh','쎷'],
    u'\uC3B8' : ['s j ^ kh','쎸'],
    u'\uC3B9' : ['s j ^ th','쎹'],
    u'\uC3BA' : ['s j ^ ph','쎺'],
    u'\uC3BB' : ['s j ^ h','쎻'],
    u'\uC3BC' : ['s j e','쎼'],
    u'\uC3BD' : ['s j e k','쎽'],
    u'\uC3BE' : ['s j e k>','쎾'],
    u'\uC3BF' : ['s j e k sh','쎿'],
    u'\uC3C0' : ['s j e n','쏀'],
    u'\uC3C1' : ['s j e n tS','쏁'],
    u'\uC3C2' : ['s j e n h','쏂'],
    u'\uC3C3' : ['s j e t','쏃'],
    u'\uC3C4' : ['s j e l','쏄'],
    u'\uC3C5' : ['s j e l k','쏅'],
    u'\uC3C6' : ['s j e l m','쏆'],
    u'\uC3C7' : ['s j e l p','쏇'],
    u'\uC3C8' : ['s j e l sh','쏈'],
    u'\uC3C9' : ['s j e l th','쏉'],
    u'\uC3CA' : ['s j e l ph','쏊'],
    u'\uC3CB' : ['s j e l h','쏋'],
    u'\uC3CC' : ['s j e m','쏌'],
    u'\uC3CD' : ['s j e p','쏍'],
    u'\uC3CE' : ['s j e p sh','쏎'],
    u'\uC3CF' : ['s j e sh','쏏'],
    u'\uC3D0' : ['s j e s','쏐'],
    u'\uC3D1' : ['s j e N','쏑'],
    u'\uC3D2' : ['s j e tS','쏒'],
    u'\uC3D3' : ['s j e tSh','쏓'],
    u'\uC3D4' : ['s j e kh','쏔'],
    u'\uC3D5' : ['s j e th','쏕'],
    u'\uC3D6' : ['s j e ph','쏖'],
    u'\uC3D7' : ['s j e h','쏗'],
    u'\uC3D8' : ['s o','쏘'],
    u'\uC3D9' : ['s o k','쏙'],
    u'\uC3DA' : ['s o k>','쏚'],
    u'\uC3DB' : ['s o k sh','쏛'],
    u'\uC3DC' : ['s o n','쏜'],
    u'\uC3DD' : ['s o n tS','쏝'],
    u'\uC3DE' : ['s o n h','쏞'],
    u'\uC3DF' : ['s o t','쏟'],
    u'\uC3E0' : ['s o l','쏠'],
    u'\uC3E1' : ['s o l k','쏡'],
    u'\uC3E2' : ['s o l m','쏢'],
    u'\uC3E3' : ['s o l p','쏣'],
    u'\uC3E4' : ['s o l sh','쏤'],
    u'\uC3E5' : ['s o l th','쏥'],
    u'\uC3E6' : ['s o l ph','쏦'],
    u'\uC3E7' : ['s o l h','쏧'],
    u'\uC3E8' : ['s o m','쏨'],
    u'\uC3E9' : ['s o p','쏩'],
    u'\uC3EA' : ['s o p sh','쏪'],
    u'\uC3EB' : ['s o sh','쏫'],
    u'\uC3EC' : ['s o s','쏬'],
    u'\uC3ED' : ['s o N','쏭'],
    u'\uC3EE' : ['s o tS','쏮'],
    u'\uC3EF' : ['s o tSh','쏯'],
    u'\uC3F0' : ['s o kh','쏰'],
    u'\uC3F1' : ['s o th','쏱'],
    u'\uC3F2' : ['s o ph','쏲'],
    u'\uC3F3' : ['s o h','쏳'],
    u'\uC3F4' : ['s w a','쏴'],
    u'\uC3F5' : ['s w a k','쏵'],
    u'\uC3F6' : ['s w a k>','쏶'],
    u'\uC3F7' : ['s w a k sh','쏷'],
    u'\uC3F8' : ['s w a n','쏸'],
    u'\uC3F9' : ['s w a n tS','쏹'],
    u'\uC3FA' : ['s w a n h','쏺'],
    u'\uC3FB' : ['s w a t','쏻'],
    u'\uC3FC' : ['s w a l','쏼'],
    u'\uC3FD' : ['s w a l k','쏽'],
    u'\uC3FE' : ['s w a l m','쏾'],
    u'\uC3FF' : ['s w a l p','쏿'],
    u'\uC400' : ['s w a l sh','쐀'],
    u'\uC401' : ['s w a l th','쐁'],
    u'\uC402' : ['s w a l ph','쐂'],
    u'\uC403' : ['s w a l h','쐃'],
    u'\uC404' : ['s w a m','쐄'],
    u'\uC405' : ['s w a p','쐅'],
    u'\uC406' : ['s w a p sh','쐆'],
    u'\uC407' : ['s w a sh','쐇'],
    u'\uC408' : ['s w a s','쐈'],
    u'\uC409' : ['s w a N','쐉'],
    u'\uC40A' : ['s w a tS','쐊'],
    u'\uC40B' : ['s w a tSh','쐋'],
    u'\uC40C' : ['s w a kh','쐌'],
    u'\uC40D' : ['s w a th','쐍'],
    u'\uC40E' : ['s w a ph','쐎'],
    u'\uC40F' : ['s w a h','쐏'],
    u'\uC410' : ['s w @','쐐'],
    u'\uC411' : ['s w @ k','쐑'],
    u'\uC412' : ['s w @ k>','쐒'],
    u'\uC413' : ['s w @ k sh','쐓'],
    u'\uC414' : ['s w @ n','쐔'],
    u'\uC415' : ['s w @ n tS','쐕'],
    u'\uC416' : ['s w @ n h','쐖'],
    u'\uC417' : ['s w @ t','쐗'],
    u'\uC418' : ['s w @ l','쐘'],
    u'\uC419' : ['s w @ l k','쐙'],
    u'\uC41A' : ['s w @ l m','쐚'],
    u'\uC41B' : ['s w @ l p','쐛'],
    u'\uC41C' : ['s w @ l sh','쐜'],
    u'\uC41D' : ['s w @ l th','쐝'],
    u'\uC41E' : ['s w @ l ph','쐞'],
    u'\uC41F' : ['s w @ l h','쐟'],
    u'\uC420' : ['s w @ m','쐠'],
    u'\uC421' : ['s w @ p','쐡'],
    u'\uC422' : ['s w @ p sh','쐢'],
    u'\uC423' : ['s w @ sh','쐣'],
    u'\uC424' : ['s w @ s','쐤'],
    u'\uC425' : ['s w @ N','쐥'],
    u'\uC426' : ['s w @ tS','쐦'],
    u'\uC427' : ['s w @ tSh','쐧'],
    u'\uC428' : ['s w @ kh','쐨'],
    u'\uC429' : ['s w @ th','쐩'],
    u'\uC42A' : ['s w @ ph','쐪'],
    u'\uC42B' : ['s w @ h','쐫'],
    u'\uC42C' : ['s w e','쐬'],
    u'\uC42D' : ['s w e k','쐭'],
    u'\uC42E' : ['s w e k>','쐮'],
    u'\uC42F' : ['s w e k sh','쐯'],
    u'\uC430' : ['s w e n','쐰'],
    u'\uC431' : ['s w e n tS','쐱'],
    u'\uC432' : ['s w e nh','쐲'],
    u'\uC433' : ['s w e t','쐳'],
    u'\uC434' : ['s w e l','쐴'],
    u'\uC435' : ['s w e l k','쐵'],
    u'\uC436' : ['s w e l m','쐶'],
    u'\uC437' : ['s w e l p','쐷'],
    u'\uC438' : ['s w e l sh','쐸'],
    u'\uC439' : ['s w e l th','쐹'],
    u'\uC43A' : ['s w e l ph','쐺'],
    u'\uC43B' : ['s w e l h','쐻'],
    u'\uC43C' : ['s w e m','쐼'],
    u'\uC43D' : ['s w e p','쐽'],
    u'\uC43E' : ['s w e p sh','쐾'],
    u'\uC43F' : ['s w e sh','쐿'],
    u'\uC440' : ['s w e s','쑀'],
    u'\uC441' : ['s w e N','쑁'],
    u'\uC442' : ['s w e tS','쑂'],
    u'\uC443' : ['s w e tSh','쑃'],
    u'\uC444' : ['s w e kh','쑄'],
    u'\uC445' : ['s w e th','쑅'],
    u'\uC446' : ['s w e ph','쑆'],
    u'\uC447' : ['s w e h','쑇'],
    u'\uC448' : ['s j o','쑈'],
    u'\uC449' : ['s j o k','쑉'],
    u'\uC44A' : ['s j o k>','쑊'],
    u'\uC44B' : ['s j o k sh','쑋'],
    u'\uC44C' : ['s j o n','쑌'],
    u'\uC44D' : ['s j o n tS','쑍'],
    u'\uC44E' : ['s j o n h','쑎'],
    u'\uC44F' : ['s j o t','쑏'],
    u'\uC450' : ['s j o l','쑐'],
    u'\uC451' : ['s j o l k','쑑'],
    u'\uC452' : ['s j o l m','쑒'],
    u'\uC453' : ['s j o l p','쑓'],
    u'\uC454' : ['s j o l sh','쑔'],
    u'\uC455' : ['s j o l th','쑕'],
    u'\uC456' : ['s j o l ph','쑖'],
    u'\uC457' : ['s j o l h','쑗'],
    u'\uC458' : ['s j o m','쑘'],
    u'\uC459' : ['s j o p','쑙'],
    u'\uC45A' : ['s j o p sh','쑚'],
    u'\uC45B' : ['s j o sh','쑛'],
    u'\uC45C' : ['s j o s','쑜'],
    u'\uC45D' : ['s j o N','쑝'],
    u'\uC45E' : ['s j o tS','쑞'],
    u'\uC45F' : ['s j o tSh','쑟'],
    u'\uC460' : ['s j o kh','쑠'],
    u'\uC461' : ['s j o th','쑡'],
    u'\uC462' : ['s j o ph','쑢'],
    u'\uC463' : ['s j o h','쑣'],
    u'\uC464' : ['s u','쑤'],
    u'\uC465' : ['s u k','쑥'],
    u'\uC466' : ['s u k>','쑦'],
    u'\uC467' : ['s u k sh','쑧'],
    u'\uC468' : ['s u n','쑨'],
    u'\uC469' : ['s u n tS','쑩'],
    u'\uC46A' : ['s u n h','쑪'],
    u'\uC46B' : ['s u t','쑫'],
    u'\uC46C' : ['s u l','쑬'],
    u'\uC46D' : ['s u l k','쑭'],
    u'\uC46E' : ['s u l m','쑮'],
    u'\uC46F' : ['s u l p','쑯'],
    u'\uC470' : ['s u l sh','쑰'],
    u'\uC471' : ['s u l th','쑱'],
    u'\uC472' : ['s u l ph','쑲'],
    u'\uC473' : ['s u l h','쑳'],
    u'\uC474' : ['s u m','쑴'],
    u'\uC475' : ['s u p','쑵'],
    u'\uC476' : ['s u p sh','쑶'],
    u'\uC477' : ['s u sh','쑷'],
    u'\uC478' : ['s u s','쑸'],
    u'\uC479' : ['s u N','쑹'],
    u'\uC47A' : ['s u tS','쑺'],
    u'\uC47B' : ['s u tSh','쑻'],
    u'\uC47C' : ['s u kh','쑼'],
    u'\uC47D' : ['s u th','쑽'],
    u'\uC47E' : ['s u ph','쑾'],
    u'\uC47F' : ['s u h','쑿'],
    u'\uC480' : ['s w ^','쒀'],
    u'\uC481' : ['s w ^ k','쒁'],
    u'\uC482' : ['s w ^ k>','쒂'],
    u'\uC483' : ['s w ^ k sh','쒃'],
    u'\uC484' : ['s w ^ n','쒄'],
    u'\uC485' : ['s w ^ n tS','쒅'],
    u'\uC486' : ['s w ^ n h','쒆'],
    u'\uC487' : ['s w ^ t','쒇'],
    u'\uC488' : ['s w ^ l','쒈'],
    u'\uC489' : ['s w ^ l k','쒉'],
    u'\uC48A' : ['s w ^ l m','쒊'],
    u'\uC48B' : ['s w ^ l p','쒋'],
    u'\uC48C' : ['s w ^ l sh','쒌'],
    u'\uC48D' : ['s w ^ l th','쒍'],
    u'\uC48E' : ['s w ^ l ph','쒎'],
    u'\uC48F' : ['s w ^ l h','쒏'],
    u'\uC490' : ['s w ^ m','쒐'],
    u'\uC491' : ['s w ^ p','쒑'],
    u'\uC492' : ['s w ^ p sh','쒒'],
    u'\uC493' : ['s w ^ sh','쒓'],
    u'\uC494' : ['s w ^ s','쒔'],
    u'\uC495' : ['s w ^ N','쒕'],
    u'\uC496' : ['s w ^ tS','쒖'],
    u'\uC497' : ['s w ^ tSh','쒗'],
    u'\uC498' : ['s w ^ kh','쒘'],
    u'\uC499' : ['s w ^ th','쒙'],
    u'\uC49A' : ['s w ^ ph','쒚'],
    u'\uC49B' : ['s w ^ h','쒛'],
    u'\uC49C' : ['s w E','쒜'],
    u'\uC49D' : ['s w E k','쒝'],
    u'\uC49E' : ['s w E k>','쒞'],
    u'\uC49F' : ['s w E k sh','쒟'],
    u'\uC4A0' : ['s w E n','쒠'],
    u'\uC4A1' : ['s w E n tS','쒡'],
    u'\uC4A2' : ['s w E n h','쒢'],
    u'\uC4A3' : ['s w E t','쒣'],
    u'\uC4A4' : ['s w E l','쒤'],
    u'\uC4A5' : ['s w E l k','쒥'],
    u'\uC4A6' : ['s w E l m','쒦'],
    u'\uC4A7' : ['s w E l p','쒧'],
    u'\uC4A8' : ['s w E l sh','쒨'],
    u'\uC4A9' : ['s w E l th','쒩'],
    u'\uC4AA' : ['s w E l ph','쒪'],
    u'\uC4AB' : ['s w E l h','쒫'],
    u'\uC4AC' : ['s w E m','쒬'],
    u'\uC4AD' : ['s w E p','쒭'],
    u'\uC4AE' : ['s w E p sh','쒮'],
    u'\uC4AF' : ['s w E sh','쒯'],
    u'\uC4B0' : ['s w E s','쒰'],
    u'\uC4B1' : ['s w E N','쒱'],
    u'\uC4B2' : ['s w E tS','쒲'],
    u'\uC4B3' : ['s w E tSh','쒳'],
    u'\uC4B4' : ['s w E kh','쒴'],
    u'\uC4B5' : ['s w E th','쒵'],
    u'\uC4B6' : ['s w E ph','쒶'],
    u'\uC4B7' : ['s w E h','쒷'],
    u'\uC4B8' : ['s 7','쒸'],
    u'\uC4B9' : ['s 7 k','쒹'],
    u'\uC4BA' : ['s 7 k>','쒺'],
    u'\uC4BB' : ['s 7 k sh','쒻'],
    u'\uC4BC' : ['s 7 n','쒼'],
    u'\uC4BD' : ['s 7 n tS','쒽'],
    u'\uC4BE' : ['s 7 n h','쒾'],
    u'\uC4BF' : ['s 7 t','쒿'],
    u'\uC4C0' : ['s 7 l','쓀'],
    u'\uC4C1' : ['s 7 l k','쓁'],
    u'\uC4C2' : ['s 7 l m','쓂'],
    u'\uC4C3' : ['s 7 l p','쓃'],
    u'\uC4C4' : ['s 7 l sh','쓄'],
    u'\uC4C5' : ['s 7 l th','쓅'],
    u'\uC4C6' : ['s 7 l ph','쓆'],
    u'\uC4C7' : ['s 7 l h','쓇'],
    u'\uC4C8' : ['s 7 m','쓈'],
    u'\uC4C9' : ['s 7 p','쓉'],
    u'\uC4CA' : ['s 7 p sh','쓊'],
    u'\uC4CB' : ['s 7 sh','쓋'],
    u'\uC4CC' : ['s 7 s','쓌'],
    u'\uC4CD' : ['s 7 N','쓍'],
    u'\uC4CE' : ['s 7 tS','쓎'],
    u'\uC4CF' : ['s 7 tSh','쓏'],
    u'\uC4D0' : ['s 7 kh','쓐'],
    u'\uC4D1' : ['s 7 th','쓑'],
    u'\uC4D2' : ['s 7 ph','쓒'],
    u'\uC4D3' : ['s 7 h','쓓'],
    u'\uC4D4' : ['s j u','쓔'],
    u'\uC4D5' : ['s j u k','쓕'],
    u'\uC4D6' : ['s j u k>','쓖'],
    u'\uC4D7' : ['s j u k sh','쓗'],
    u'\uC4D8' : ['s j u n','쓘'],
    u'\uC4D9' : ['s j u n tS','쓙'],
    u'\uC4DA' : ['s j u n h','쓚'],
    u'\uC4DB' : ['s j u t','쓛'],
    u'\uC4DC' : ['s j u l','쓜'],
    u'\uC4DD' : ['s j u l','쓝'],
    u'\uC4DE' : ['s j u l m','쓞'],
    u'\uC4DF' : ['s j u l p','쓟'],
    u'\uC4E0' : ['s j u l sh','쓠'],
    u'\uC4E1' : ['s j u l th','쓡'],
    u'\uC4E2' : ['s j u l ph','쓢'],
    u'\uC4E3' : ['s j u l h','쓣'],
    u'\uC4E4' : ['s j u m','쓤'],
    u'\uC4E5' : ['s j u p','쓥'],
    u'\uC4E7' : ['s j u sh','쓧'],
    u'\uC4E8' : ['s j u s','쓨'],
    u'\uC4E9' : ['s j u N','쓩'],
    u'\uC4EA' : ['s j u tS','쓪'],
    u'\uC4EB' : ['s j u tSh','쓫'],
    u'\uC4EC' : ['s j u kh','쓬'],
    u'\uC4ED' : ['s j u th','쓭'],
    u'\uC4EE' : ['s j u ph','쓮'],
    u'\uC4EF' : ['s j u h','쓯'],
    u'\uC4F0' : ['s 4','쓰'],
    u'\uC4F1' : ['s 4 k','쓱'],
    u'\uC4F2' : ['s 4 k>','쓲'],
    u'\uC4F3' : ['s 4 k sh','쓳'],
    u'\uC4F4' : ['s 4 n','쓴'],
    u'\uC4F5' : ['s 4 n tS','쓵'],
    u'\uC4F6' : ['s 4 n h','쓶'],
    u'\uC4F7' : ['s 4 t','쓷'],
    u'\uC4F8' : ['s 4 l','쓸'],
    u'\uC4F9' : ['s 4 l k','쓹'],
    u'\uC4FA' : ['s 4 l m','쓺'],
    u'\uC4FB' : ['s 4 l p','쓻'],
    u'\uC4FC' : ['s 4 l sh','쓼'],
    u'\uC4FD' : ['s 4 l th','쓽'],
    u'\uC4FE' : ['s 4 l ph','쓾'],
    u'\uC4FF' : ['s 4 l h','쓿'],
    u'\uC500' : ['s 4 m','씀'],
    u'\uC501' : ['s 4 p','씁'],
    u'\uC502' : ['s 4 p sh','씂'],
    u'\uC503' : ['s 4 sh','씃'],
    u'\uC504' : ['s 4 s','씄'],
    u'\uC505' : ['s 4 N','씅'],
    u'\uC506' : ['s 4 tS','씆'],
    u'\uC507' : ['s 4 tSh','씇'],
    u'\uC508' : ['s 4 kh','씈'],
    u'\uC509' : ['s 4 th','씉'],
    u'\uC50A' : ['s 4 ph','씊'],
    u'\uC50B' : ['s 4 h','씋'],
    u'\uC50C' : ['s 4 j','씌'],
    u'\uC50D' : ['s 4 j k','씍'],
    u'\uC50E' : ['s 4 j k>','씎'],
    u'\uC50F' : ['s 4 j k sh','씏'],
    u'\uC510' : ['s 4 j n','씐'],
    u'\uC511' : ['s 4 j n tS','씑'],
    u'\uC512' : ['s 4 j n h','씒'],
    u'\uC513' : ['s 4 j t','씓'],
    u'\uC514' : ['s 4 j l','씔'],
    u'\uC515' : ['s 4 j l k','씕'],
    u'\uC516' : ['s 4 j l m','씖'],
    u'\uC517' : ['s 4 j l p','씗'],
    u'\uC518' : ['s 4 j l sh','씘'],
    u'\uC519' : ['s 4 j l th','씙'],
    u'\uC51A' : ['s 4 j l ph','씚'],
    u'\uC51B' : ['s 4 j l h','씛'],
    u'\uC51C' : ['s 4 j m','씜'],
    u'\uC51D' : ['s 4 j p','씝'],
    u'\uC51E' : ['s 4 j p sh','씞'],
    u'\uC51F' : ['s 4 j sh','씟'],
    u'\uC520' : ['s 4 j s','씠'],
    u'\uC521' : ['s 4 j N','씡'],
    u'\uC522' : ['s 4 j tS','씢'],
    u'\uC523' : ['s 4 j tSh','씣'],
    u'\uC524' : ['s 4 j kh','씤'],
    u'\uC525' : ['s 4 j th','씥'],
    u'\uC526' : ['s 4 j ph','씦'],
    u'\uC527' : ['s 4 j h','씧'],
    u'\uC528' : ['s i','씨'],
    u'\uC529' : ['s i k','씩'],
    u'\uC52A' : ['s i k>','씪'],
    u'\uC52B' : ['s i k sh','씫'],
    u'\uC52C' : ['s i n','씬'],
    u'\uC52D' : ['s i n tS','씭'],
    u'\uC52E' : ['s i n h','씮'],
    u'\uC52F' : ['s i t','씯'],
    u'\uC530' : ['s i l','씰'],
    u'\uC531' : ['s i l k','씱'],
    u'\uC532' : ['s i l m','씲'],
    u'\uC533' : ['s i l p','씳'],
    u'\uC534' : ['s i l sh','씴'],
    u'\uC535' : ['s i l th','씵'],
    u'\uC536' : ['s i l ph','씶'],
    u'\uC537' : ['s i l h','씷'],
    u'\uC538' : ['s i m','씸'],
    u'\uC539' : ['s i p','씹'],
    u'\uC53A' : ['s i p sh','씺'],
    u'\uC53B' : ['s i sh','씻'],
    u'\uC53C' : ['s i s','씼'],
    u'\uC53D' : ['s i N','씽'],
    u'\uC53E' : ['s i tS','씾'],
    u'\uC53F' : ['s i tSh','씿'],
    u'\uC540' : ['s i kh','앀'],
    u'\uC541' : ['s i th','앁'],
    u'\uC542' : ['s i ph','앂'],
    u'\uC543' : ['s i h','앃'],
    u'\uC544' : ['a','아'],
    u'\uC545' : ['a k','악'],
    u'\uC546' : ['a k>','앆'],
    u'\uC547' : ['a k sh','앇'],
    u'\uC548' : ['a n','안'],
    u'\uC549' : ['a n tS','앉'],
    u'\uC54A' : ['a n h','않'],
    u'\uC54B' : ['a t','앋'],
    u'\uC54C' : ['a l','알'],
    u'\uC54D' : ['a l k','앍'],
    u'\uC54E' : ['a l m','앎'],
    u'\uC54F' : ['a l p','앏'],
    u'\uC550' : ['a l sh','앐'],
    u'\uC551' : ['a l th','앑'],
    u'\uC552' : ['a l ph','앒'],
    u'\uC553' : ['a l h','앓'],
    u'\uC554' : ['a m','암'],
    u'\uC555' : ['a p','압'],
    u'\uC556' : ['a p sh','앖'],
    u'\uC557' : ['a sh','앗'],
    u'\uC558' : ['a s','았'],
    u'\uC559' : ['a N','앙'],
    u'\uC55A' : ['a tS','앚'],
    u'\uC55B' : ['a tSh','앛'],
    u'\uC55C' : ['a kh','앜'],
    u'\uC55D' : ['a th','앝'],
    u'\uC55E' : ['a ph','앞'],
    u'\uC55F' : ['a h','앟'],
    u'\uC560' : ['@','애'],
    u'\uC561' : ['@ k','액'],
    u'\uC562' : ['@ k>','앢'],
    u'\uC563' : ['@ k sh','앣'],
    u'\uC564' : ['@ n','앤'],
    u'\uC565' : ['@ n tS','앥'],
    u'\uC566' : ['@ n h','앦'],
    u'\uC567' : ['@ t','앧'],
    u'\uC568' : ['@ l','앨'],
    u'\uC569' : ['@ l k','앩'],
    u'\uC56A' : ['@ l m','앪'],
    u'\uC56B' : ['@ l p','앫'],
    u'\uC56C' : ['@ l sh','앬'],
    u'\uC56D' : ['@ l th','앭'],
    u'\uC56E' : ['@ l ph','앮'],
    u'\uC56F' : ['@ l h','앯'],
    u'\uC570' : ['@ m','앰'],
    u'\uC571' : ['@ p','앱'],
    u'\uC572' : ['@ p sh','앲'],
    u'\uC573' : ['@ sh','앳'],
    u'\uC574' : ['@ s','앴'],
    u'\uC575' : ['@ N','앵'],
    u'\uC576' : ['@ tS','앶'],
    u'\uC577' : ['@ tSh','앷'],
    u'\uC578' : ['@ kh','앸'],
    u'\uC579' : ['@ th','앹'],
    u'\uC57A' : ['@ ph','앺'],
    u'\uC57B' : ['@ h','앻'],
    u'\uC57C' : ['j a','야'],
    u'\uC57D' : ['j a k','약'],
    u'\uC57E' : ['j a k>','앾'],
    u'\uC57F' : ['j a k sh','앿'],
    u'\uC580' : ['j a n','얀'],
    u'\uC581' : ['j a n tS','얁'],
    u'\uC582' : ['j a n h','얂'],
    u'\uC583' : ['j a t','얃'],
    u'\uC584' : ['j a l','얄'],
    u'\uC585' : ['j a l k','얅'],
    u'\uC586' : ['j a l m','얆'],
    u'\uC587' : ['j a l p','얇'],
    u'\uC588' : ['j a l sh','얈'],
    u'\uC589' : ['j a l th','얉'],
    u'\uC58A' : ['j a l ph','얊'],
    u'\uC58B' : ['j a l h','얋'],
    u'\uC58C' : ['j a m','얌'],
    u'\uC58D' : ['j a p','얍'],
    u'\uC58E' : ['j a p sh','얎'],
    u'\uC58F' : ['j a sh','얏'],
    u'\uC590' : ['j a s','얐'],
    u'\uC591' : ['j a N','양'],
    u'\uC592' : ['j a tS','얒'],
    u'\uC593' : ['j a tSh','얓'],
    u'\uC594' : ['j a kh','얔'],
    u'\uC595' : ['j a th','얕'],
    u'\uC596' : ['j a ph','얖'],
    u'\uC597' : ['j a h','얗'],
    u'\uC598' : ['j @','얘'],
    u'\uC599' : ['j @ k','얙'],
    u'\uC59A' : ['j @ k>','얚'],
    u'\uC59B' : ['j @ k sh','얛'],
    u'\uC59C' : ['j @ n','얜'],
    u'\uC59D' : ['j @ n tS','얝'],
    u'\uC59E' : ['j @ n h','얞'],
    u'\uC59F' : ['j @ t','얟'],
    u'\uC5A0' : ['j @ l','얠'],
    u'\uC5A1' : ['j @ l k','얡'],
    u'\uC5A2' : ['j @ l m','얢'],
    u'\uC5A3' : ['j @ l p','얣'],
    u'\uC5A4' : ['j @ l sh','얤'],
    u'\uC5A5' : ['j @ l th','얥'],
    u'\uC5A6' : ['j @ l ph','얦'],
    u'\uC5A7' : ['j @ l h','얧'],
    u'\uC5A8' : ['j @ m','얨'],
    u'\uC5A9' : ['j @ p','얩'],
    u'\uC5AA' : ['j @ p sh','얪'],
    u'\uC5AB' : ['j @ sh','얫'],
    u'\uC5AC' : ['j @ s','얬'],
    u'\uC5AD' : ['j @ N','얭'],
    u'\uC5AE' : ['j @ tS','얮'],
    u'\uC5AF' : ['j @ tSh','얯'],
    u'\uC5B0' : ['j @ kh','얰'],
    u'\uC5B1' : ['j @ th','얱'],
    u'\uC5B2' : ['j @ ph','얲'],
    u'\uC5B3' : ['j @ h','얳'],
    u'\uC5B4' : ['^','어'],
    u'\uC5B5' : ['^ k','억'],
    u'\uC5B6' : ['^ k>','얶'],
    u'\uC5B7' : ['^ k sh','얷'],
    u'\uC5B8' : ['^ n','언'],
    u'\uC5B9' : ['^ n tS','얹'],
    u'\uC5BA' : ['^ n h','얺'],
    u'\uC5BB' : ['^ t','얻'],
    u'\uC5BC' : ['^ l','얼'],
    u'\uC5BD' : ['^ l k','얽'],
    u'\uC5BE' : ['^ l m','얾'],
    u'\uC5BF' : ['^ l p','얿'],
    u'\uC5C0' : ['^ l sh','엀'],
    u'\uC5C1' : ['^ l th','엁'],
    u'\uC5C2' : ['^ l ph','엂'],
    u'\uC5C3' : ['^ l h','엃'],
    u'\uC5C4' : ['^ m','엄'],
    u'\uC5C5' : ['^ p','업'],
    u'\uC5C6' : ['^ p sh','없'],
    u'\uC5C7' : ['^ sh','엇'],
    u'\uC5C8' : ['^ s','었'],
    u'\uC5C9' : ['^ N','엉'],
    u'\uC5CA' : ['^ tS','엊'],
    u'\uC5CB' : ['^ tSh','엋'],
    u'\uC5CC' : ['^ kh','엌'],
    u'\uC5CD' : ['^ th','엍'],
    u'\uC5CE' : ['^ ph','엎'],
    u'\uC5CF' : ['^ h','엏'],
    u'\uC5D0' : ['e','에'],
    u'\uC5D1' : ['e k','엑'],
    u'\uC5D2' : ['e k>','엒'],
    u'\uC5D3' : ['e k sh','엓'],
    u'\uC5D4' : ['e n','엔'],
    u'\uC5D5' : ['e n tS','엕'],
    u'\uC5D6' : ['e n h','엖'],
    u'\uC5D7' : ['e t','엗'],
    u'\uC5D8' : ['e l','엘'],
    u'\uC5D9' : ['e l k','엙'],
    u'\uC5DA' : ['e l m','엚'],
    u'\uC5DB' : ['e l p','엛'],
    u'\uC5DC' : ['e l sh','엜'],
    u'\uC5DD' : ['e l th','엝'],
    u'\uC5DE' : ['e l ph','엞'],
    u'\uC5DF' : ['e l h','엟'],
    u'\uC5E0' : ['e m','엠'],
    u'\uC5E1' : ['e p','엡'],
    u'\uC5E2' : ['e p sh','엢'],
    u'\uC5E3' : ['e sh','엣'],
    u'\uC5E4' : ['e s','엤'],
    u'\uC5E5' : ['e N','엥'],
    u'\uC5E6' : ['e tS','엦'],
    u'\uC5E7' : ['e tSh','엧'],
    u'\uC5E8' : ['e kh','엨'],
    u'\uC5E9' : ['e th','엩'],
    u'\uC5EA' : ['e ph','엪'],
    u'\uC5EB' : ['e h','엫'],
    u'\uC5EC' : ['j ^','여'],
    u'\uC5ED' : ['j ^ k','역'],
    u'\uC5EE' : ['j ^ k>','엮'],
    u'\uC5EF' : ['j ^ k sh','엯'],
    u'\uC5F0' : ['j ^ n','연'],
    u'\uC5F1' : ['j ^ n tS','엱'],
    u'\uC5F2' : ['j ^ n h','엲'],
    u'\uC5F3' : ['j ^ t','엳'],
    u'\uC5F4' : ['j ^ l','열'],
    u'\uC5F5' : ['j ^ l k','엵'],
    u'\uC5F6' : ['j ^ l m','엶'],
    u'\uC5F7' : ['j ^ l p','엷'],
    u'\uC5F8' : ['j ^ l sh','엸'],
    u'\uC5F9' : ['j ^ l th','엹'],
    u'\uC5FA' : ['j ^ l ph','엺'],
    u'\uC5FB' : ['j ^ l h','엻'],
    u'\uC5FC' : ['j ^ m','염'],
    u'\uC5FD' : ['j ^ p','엽'],
    u'\uC5FE' : ['j ^ p sh','엾'],
    u'\uC5FF' : ['j ^ sh','엿'],
    u'\uC600' : ['j ^ s','였'],
    u'\uC601' : ['j ^ N','영'],
    u'\uC602' : ['j ^ tS','옂'],
    u'\uC603' : ['j ^ tSh','옃'],
    u'\uC604' : ['j ^ kh','옄'],
    u'\uC605' : ['j ^ th','옅'],
    u'\uC606' : ['j ^ ph','옆'],
    u'\uC607' : ['j ^ h','옇'],
    u'\uC608' : ['j e','예'],
    u'\uC609' : ['j e k','옉'],
    u'\uC60A' : ['j e k>','옊'],
    u'\uC60B' : ['j e k sh','옋'],
    u'\uC60C' : ['j e n','옌'],
    u'\uC60D' : ['j e n tS','옍'],
    u'\uC60E' : ['j e n h','옎'],
    u'\uC60F' : ['j e t','옏'],
    u'\uC610' : ['j e l','옐'],
    u'\uC611' : ['j e l k','옑'],
    u'\uC612' : ['j e l m','옒'],
    u'\uC613' : ['j e l p','옓'],
    u'\uC614' : ['j e l sh','옔'],
    u'\uC615' : ['j e l th','옕'],
    u'\uC616' : ['j e l ph','옖'],
    u'\uC617' : ['j e l h','옗'],
    u'\uC618' : ['j e m','옘'],
    u'\uC619' : ['j e p','옙'],
    u'\uC61A' : ['j e p sh','옚'],
    u'\uC61B' : ['j e sh','옛'],
    u'\uC61C' : ['j e s','옜'],
    u'\uC61D' : ['j e N','옝'],
    u'\uC61E' : ['j e tS','옞'],
    u'\uC61F' : ['j e tSh','옟'],
    u'\uC620' : ['j e kh','옠'],
    u'\uC621' : ['j e th','옡'],
    u'\uC622' : ['j e ph','옢'],
    u'\uC623' : ['j e h','옣'],
    u'\uC624' : ['o','오'],
    u'\uC625' : ['o k','옥'],
    u'\uC626' : ['o k>','옦'],
    u'\uC627' : ['o k sh','옧'],
    u'\uC628' : ['o n','온'],
    u'\uC629' : ['o n tS','옩'],
    u'\uC62A' : ['o n h','옪'],
    u'\uC62B' : ['o t','옫'],
    u'\uC62C' : ['o l','올'],
    u'\uC62D' : ['o l k','옭'],
    u'\uC62E' : ['o l m','옮'],
    u'\uC62F' : ['o l p','옯'],
    u'\uC630' : ['o l sh','옰'],
    u'\uC631' : ['o l th','옱'],
    u'\uC632' : ['o l ph','옲'],
    u'\uC633' : ['o l h','옳'],
    u'\uC634' : ['o m','옴'],
    u'\uC635' : ['o p','옵'],
    u'\uC636' : ['o p sh','옶'],
    u'\uC637' : ['o sh','옷'],
    u'\uC638' : ['o s','옸'],
    u'\uC639' : ['o N','옹'],
    u'\uC63A' : ['o tS','옺'],
    u'\uC63B' : ['o tSh','옻'],
    u'\uC63C' : ['o kh','옼'],
    u'\uC63D' : ['o th','옽'],
    u'\uC63E' : ['o ph','옾'],
    u'\uC63F' : ['o h','옿'],
    u'\uC640' : ['w a','와'],
    u'\uC641' : ['w a k','왁'],
    u'\uC642' : ['w a k>','왂'],
    u'\uC643' : ['w a k sh','왃'],
    u'\uC644' : ['w a n','완'],
    u'\uC645' : ['w a n tS','왅'],
    u'\uC646' : ['w a n h','왆'],
    u'\uC647' : ['w a t','왇'],
    u'\uC648' : ['w a l','왈'],
    u'\uC649' : ['w a l k','왉'],
    u'\uC64A' : ['w a l m','왊'],
    u'\uC64B' : ['w a l p','왋'],
    u'\uC64C' : ['w a l sh','왌'],
    u'\uC64D' : ['w a l th','왍'],
    u'\uC64E' : ['w a l ph','왎'],
    u'\uC64F' : ['w a l h','왏'],
    u'\uC650' : ['w a m','왐'],
    u'\uC651' : ['w a p','왑'],
    u'\uC652' : ['w a p sh','왒'],
    u'\uC653' : ['w a sh','왓'],
    u'\uC654' : ['w a s','왔'],
    u'\uC655' : ['w a N','왕'],
    u'\uC656' : ['w a tS','왖'],
    u'\uC657' : ['w a tSh','왗'],
    u'\uC658' : ['w a kh','왘'],
    u'\uC659' : ['w a th','왙'],
    u'\uC65A' : ['w a ph','왚'],
    u'\uC65B' : ['w a h','왛'],
    u'\uC65C' : ['w @','왜'],
    u'\uC65D' : ['w @ k','왝'],
    u'\uC65E' : ['w @ k>','왞'],
    u'\uC65F' : ['w @ k sh','왟'],
    u'\uC660' : ['w @ n','왠'],
    u'\uC661' : ['w @ n tS','왡'],
    u'\uC662' : ['w @ n h','왢'],
    u'\uC663' : ['w @ t','왣'],
    u'\uC664' : ['w @ l','왤'],
    u'\uC665' : ['w @ l k','왥'],
    u'\uC666' : ['w @ l m','왦'],
    u'\uC667' : ['w @ l p','왧'],
    u'\uC668' : ['w @ l sh','왨'],
    u'\uC669' : ['w @ l th','왩'],
    u'\uC66A' : ['w @ l ph','왪'],
    u'\uC66B' : ['w @ l h','왫'],
    u'\uC66C' : ['w @ m','왬'],
    u'\uC66D' : ['w @ p','왭'],
    u'\uC66E' : ['w @ p sh','왮'],
    u'\uC66F' : ['w @ sh','왯'],
    u'\uC670' : ['w @ s','왰'],
    u'\uC671' : ['w @ N','왱'],
    u'\uC672' : ['w @ tS','왲'],
    u'\uC673' : ['w @ tSh','왳'],
    u'\uC674' : ['w @ kh','왴'],
    u'\uC675' : ['w @ th','왵'],
    u'\uC676' : ['w @ ph','왶'],
    u'\uC677' : ['w @ h','왷'],
    u'\uC678' : ['w e','외'],
    u'\uC679' : ['w e k','왹'],
    u'\uC67A' : ['w e k>','왺'],
    u'\uC67B' : ['w e k sh','왻'],
    u'\uC67C' : ['w e n','왼'],
    u'\uC67D' : ['w e n tS','왽'],
    u'\uC67E' : ['w e n h','왾'],
    u'\uC67F' : ['w e t','왿'],
    u'\uC680' : ['w e l','욀'],
    u'\uC681' : ['w e l k','욁'],
    u'\uC682' : ['w e l m','욂'],
    u'\uC683' : ['w e l p','욃'],
    u'\uC684' : ['w e l sh','욄'],
    u'\uC685' : ['w e l th','욅'],
    u'\uC686' : ['w e l ph','욆'],
    u'\uC687' : ['w e l h','욇'],
    u'\uC688' : ['w e m','욈'],
    u'\uC689' : ['w e p','욉'],
    u'\uC68A' : ['w e p sh','욊'],
    u'\uC68B' : ['w e sh','욋'],
    u'\uC68C' : ['w e s','욌'],
    u'\uC68D' : ['w e N','욍'],
    u'\uC68E' : ['w e tS','욎'],
    u'\uC68F' : ['w e tSh','욏'],
    u'\uC690' : ['w e kh','욐'],
    u'\uC691' : ['w e th','욑'],
    u'\uC692' : ['w e ph','욒'],
    u'\uC693' : ['w e h','욓'],
    u'\uC694' : ['j o','요'],
    u'\uC695' : ['j o k','욕'],
    u'\uC696' : ['j o k>','욖'],
    u'\uC697' : ['j o k sh','욗'],
    u'\uC698' : ['j o n','욘'],
    u'\uC699' : ['j o n tS','욙'],
    u'\uC69A' : ['j o n h','욚'],
    u'\uC69B' : ['j o t','욛'],
    u'\uC69C' : ['j o l','욜'],
    u'\uC69D' : ['j o l k','욝'],
    u'\uC69E' : ['j o l m','욞'],
    u'\uC69F' : ['j o l p','욟'],
    u'\uC6A0' : ['j o l sh','욠'],
    u'\uC6A1' : ['j o l th','욡'],
    u'\uC6A2' : ['j o l ph','욢'],
    u'\uC6A3' : ['j o l h','욣'],
    u'\uC6A4' : ['j o m','욤'],
    u'\uC6A5' : ['j o p','욥'],
    u'\uC6A6' : ['j o p sh','욦'],
    u'\uC6A7' : ['j o sh','욧'],
    u'\uC6A8' : ['j o s','욨'],
    u'\uC6A9' : ['j o N','용'],
    u'\uC6AA' : ['j o tS','욪'],
    u'\uC6AB' : ['j o tSh','욫'],
    u'\uC6AC' : ['j o kh','욬'],
    u'\uC6AD' : ['j o th','욭'],
    u'\uC6AE' : ['j o ph','욮'],
    u'\uC6AF' : ['j o h','욯'],
    u'\uC6B0' : ['u','우'],
    u'\uC6B1' : ['u k','욱'],
    u'\uC6B2' : ['u k>','욲'],
    u'\uC6B3' : ['u k sh','욳'],
    u'\uC6B4' : ['u n','운'],
    u'\uC6B5' : ['u n tS','욵'],
    u'\uC6B6' : ['u n h','욶'],
    u'\uC6B7' : ['u t','욷'],
    u'\uC6B8' : ['u l','울'],
    u'\uC6B9' : ['u l k','욹'],
    u'\uC6BA' : ['u l m','욺'],
    u'\uC6BB' : ['u l p','욻'],
    u'\uC6BC' : ['u l sh','욼'],
    u'\uC6BD' : ['u l th','욽'],
    u'\uC6BE' : ['u l ph','욾'],
    u'\uC6BF' : ['u l h','욿'],
    u'\uC6C0' : ['u m','움'],
    u'\uC6C1' : ['u p','웁'],
    u'\uC6C2' : ['u p sh','웂'],
    u'\uC6C3' : ['u sh','웃'],
    u'\uC6C4' : ['u s','웄'],
    u'\uC6C5' : ['u N','웅'],
    u'\uC6C6' : ['u tS','웆'],
    u'\uC6C7' : ['u tSh','웇'],
    u'\uC6C8' : ['u kh','웈'],
    u'\uC6C9' : ['u th','웉'],
    u'\uC6CA' : ['u ph','웊'],
    u'\uC6CB' : ['u h','웋'],
    u'\uC6CC' : ['w ^','워'],
    u'\uC6CD' : ['w ^ k','웍'],
    u'\uC6CE' : ['w ^ k>','웎'],
    u'\uC6CF' : ['w ^ k sh','웏'],
    u'\uC6D0' : ['w ^ n','원'],
    u'\uC6D1' : ['w ^ n tS','웑'],
    u'\uC6D2' : ['w ^ n h','웒'],
    u'\uC6D3' : ['w ^ t','웓'],
    u'\uC6D4' : ['w ^ l','월'],
    u'\uC6D5' : ['w ^ l k','웕'],
    u'\uC6D6' : ['w ^ l m','웖'],
    u'\uC6D7' : ['w ^ l p','웗'],
    u'\uC6D8' : ['w ^ l sh','웘'],
    u'\uC6D9' : ['w ^ l th','웙'],
    u'\uC6DA' : ['w ^ l ph','웚'],
    u'\uC6DB' : ['w ^ l h','웛'],
    u'\uC6DC' : ['w ^ m','웜'],
    u'\uC6DD' : ['w ^ p','웝'],
    u'\uC6DE' : ['w ^ p sh','웞'],
    u'\uC6DF' : ['w ^ sh','웟'],
    u'\uC6E0' : ['w ^ s','웠'],
    u'\uC6E1' : ['w ^ N','웡'],
    u'\uC6E2' : ['w ^ tS','웢'],
    u'\uC6E3' : ['w ^ tSh','웣'],
    u'\uC6E4' : ['w ^ kh','웤'],
    u'\uC6E5' : ['w ^ th','웥'],
    u'\uC6E6' : ['w ^ ph','웦'],
    u'\uC6E7' : ['w ^ h','웧'],
    u'\uC6E8' : ['w E','웨'],
    u'\uC6E9' : ['w E k','웩'],
    u'\uC6EA' : ['w E k>','웪'],
    u'\uC6EB' : ['w E k sh','웫'],
    u'\uC6EC' : ['w E n','웬'],
    u'\uC6ED' : ['w E n tS','웭'],
    u'\uC6EE' : ['w E n h','웮'],
    u'\uC6EF' : ['w E t','웯'],
    u'\uC6F0' : ['w E l','웰'],
    u'\uC6F1' : ['w E l k','웱'],
    u'\uC6F2' : ['w E l m','웲'],
    u'\uC6F3' : ['w E l p','웳'],
    u'\uC6F4' : ['w E l sh','웴'],
    u'\uC6F5' : ['w E l th','웵'],
    u'\uC6F6' : ['w E l ph','웶'],
    u'\uC6F7' : ['w E l h','웷'],
    u'\uC6F8' : ['w E m','웸'],
    u'\uC6F9' : ['w E p','웹'],
    u'\uC6FA' : ['w E p sh','웺'],
    u'\uC6FB' : ['w E sh','웻'],
    u'\uC6FC' : ['w E s','웼'],
    u'\uC6FD' : ['w E N','웽'],
    u'\uC6FE' : ['w E tS','웾'],
    u'\uC6FF' : ['w E tSh','웿'],
    u'\uC700' : ['w E kh','윀'],
    u'\uC701' : ['w E th','윁'],
    u'\uC702' : ['w E ph','윂'],
    u'\uC703' : ['w E h','윃'],
    u'\uC704' : ['7','위'],
    u'\uC705' : ['7 k','윅'],
    u'\uC706' : ['7 k>','윆'],
    u'\uC707' : ['7 k sh','윇'],
    u'\uC708' : ['7 n','윈'],
    u'\uC709' : ['7 n tS','윉'],
    u'\uC70A' : ['7 n h','윊'],
    u'\uC70B' : ['7 t','윋'],
    u'\uC70C' : ['7 l','윌'],
    u'\uC70D' : ['7 l k','윍'],
    u'\uC70E' : ['7 l m','윎'],
    u'\uC70F' : ['7 l p','윏'],
    u'\uC710' : ['7 l sh','윐'],
    u'\uC711' : ['7 l th','윑'],
    u'\uC712' : ['7 l ph','윒'],
    u'\uC713' : ['7 l h','윓'],
    u'\uC714' : ['7 m','윔'],
    u'\uC715' : ['7 p','윕'],
    u'\uC716' : ['7 p sh','윖'],
    u'\uC717' : ['7 sh','윗'],
    u'\uC718' : ['7 s','윘'],
    u'\uC719' : ['7 N','윙'],
    u'\uC71A' : ['7 tS','윚'],
    u'\uC71B' : ['7 tSh','윛'],
    u'\uC71C' : ['7 kh','윜'],
    u'\uC71D' : ['7 th','윝'],
    u'\uC71E' : ['7 ph','윞'],
    u'\uC71F' : ['7 h','윟'],
    u'\uC720' : ['j u','유'],
    u'\uC721' : ['j u k','육'],
    u'\uC722' : ['j u k>','윢'],
    u'\uC723' : ['j u k sh','윣'],
    u'\uC724' : ['j u n','윤'],
    u'\uC725' : ['j u n tS','윥'],
    u'\uC726' : ['j u n h','윦'],
    u'\uC727' : ['j u t','윧'],
    u'\uC728' : ['j u l','율'],
    u'\uC729' : ['j u l k','윩'],
    u'\uC72A' : ['j u l m','윪'],
    u'\uC72B' : ['j u l p','윫'],
    u'\uC72C' : ['j u l sh','윬'],
    u'\uC72D' : ['j u l th','윭'],
    u'\uC72E' : ['j u l ph','윮'],
    u'\uC72F' : ['j u l h','윯'],
    u'\uC730' : ['j u m','윰'],
    u'\uC731' : ['j u p','윱'],
    u'\uC732' : ['j u p sh','윲'],
    u'\uC733' : ['j u sh','윳'],
    u'\uC734' : ['j u s','윴'],
    u'\uC735' : ['j u N','융'],
    u'\uC736' : ['j u tS','윶'],
    u'\uC737' : ['j u tSh','윷'],
    u'\uC738' : ['j u kh','윸'],
    u'\uC739' : ['j u th','윹'],
    u'\uC73A' : ['j u ph','윺'],
    u'\uC73B' : ['j u h','윻'],
    u'\uC73C' : ['4','으'],
    u'\uC73D' : ['4 k','윽'],
    u'\uC73E' : ['4 k>','윾'],
    u'\uC73F' : ['4 k sh','윿'],
    u'\uC740' : ['4 n','은'],
    u'\uC741' : ['4 n tS','읁'],
    u'\uC742' : ['4 nh','읂'],
    u'\uC743' : ['4 t','읃'],
    u'\uC744' : ['4 l','을'],
    u'\uC745' : ['4 l k','읅'],
    u'\uC746' : ['4 l m','읆'],
    u'\uC747' : ['4 l p','읇'],
    u'\uC748' : ['4 l sh','읈'],
    u'\uC749' : ['4 l th','읉'],
    u'\uC74A' : ['4 l ph','읊'],
    u'\uC74B' : ['4 l h','읋'],
    u'\uC74C' : ['4 m','음'],
    u'\uC74D' : ['4 p','읍'],
    u'\uC74E' : ['4 p sh','읎'],
    u'\uC74F' : ['4 sh','읏'],
    u'\uC750' : ['4 s','읐'],
    u'\uC751' : ['4 N','응'],
    u'\uC752' : ['4 tS','읒'],
    u'\uC753' : ['4 tSh','읓'],
    u'\uC754' : ['4 kh','읔'],
    u'\uC755' : ['4 th','읕'],
    u'\uC756' : ['4 ph','읖'],
    u'\uC757' : ['4 h','읗'],
    u'\uC758' : ['4 j','의'],
    u'\uC759' : ['4 j k','읙'],
    u'\uC75A' : ['4 j k>','읚'],
    u'\uC75B' : ['4 j k sh','읛'],
    u'\uC75C' : ['4 j n','읜'],
    u'\uC75D' : ['4 j n tS','읝'],
    u'\uC75E' : ['4 j n h','읞'],
    u'\uC75F' : ['4 j t','읟'],
    u'\uC760' : ['4 j l','읠'],
    u'\uC761' : ['4 j l k','읡'],
    u'\uC762' : ['4 j l m','읢'],
    u'\uC763' : ['4 j l p','읣'],
    u'\uC764' : ['4 j l sh','읤'],
    u'\uC765' : ['4 j l th','읥'],
    u'\uC766' : ['4 j l ph','읦'],
    u'\uC767' : ['4 j l h','읧'],
    u'\uC768' : ['4 j m','읨'],
    u'\uC769' : ['4 j p','읩'],
    u'\uC76A' : ['4 j p sh','읪'],
    u'\uC76B' : ['4 j sh','읫'],
    u'\uC76C' : ['4 j s','읬'],
    u'\uC76D' : ['4 j N','읭'],
    u'\uC76E' : ['4 j tS','읮'],
    u'\uC76F' : ['4 j tSh','읯'],
    u'\uC770' : ['4 j kh','읰'],
    u'\uC771' : ['4 j th','읱'],
    u'\uC772' : ['4 j ph','읲'],
    u'\uC773' : ['4 j h','읳'],
    u'\uC774' : ['i','이'],
    u'\uC775' : ['i k','익'],
    u'\uC776' : ['i k>','읶'],
    u'\uC777' : ['i k sh','읷'],
    u'\uC778' : ['i n','인'],
    u'\uC779' : ['i n tS','읹'],
    u'\uC77A' : ['i n h','읺'],
    u'\uC77B' : ['i t','읻'],
    u'\uC77C' : ['i l','일'],
    u'\uC77D' : ['i l k','읽'],
    u'\uC77E' : ['i l m','읾'],
    u'\uC77F' : ['i l p','읿'],
    u'\uC780' : ['i l sh','잀'],
    u'\uC781' : ['i l th','잁'],
    u'\uC782' : ['i l ph','잂'],
    u'\uC783' : ['i l h','잃'],
    u'\uC784' : ['i m','임'],
    u'\uC785' : ['i p','입'],
    u'\uC786' : ['i p sh','잆'],
    u'\uC787' : ['i sh','잇'],
    u'\uC788' : ['i s','있'],
    u'\uC789' : ['i N','잉'],
    u'\uC78A' : ['i tS','잊'],
    u'\uC78B' : ['i tSh','잋'],
    u'\uC78C' : ['i kh','잌'],
    u'\uC78D' : ['i th','잍'],
    u'\uC78E' : ['i ph','잎'],
    u'\uC78F' : ['i h','잏'],
    u'\uC790' : ['tS a','자'],
    u'\uC791' : ['tS a k','작'],
    u'\uC792' : ['tS a k>','잒'],
    u'\uC793' : ['tS a k sh','잓'],
    u'\uC794' : ['tS a n','잔'],
    u'\uC795' : ['tS a n tS','잕'],
    u'\uC796' : ['tS a n h','잖'],
    u'\uC797' : ['tS a t','잗'],
    u'\uC798' : ['tS a l','잘'],
    u'\uC799' : ['tS a l k','잙'],
    u'\uC79A' : ['tS a l m','잚'],
    u'\uC79B' : ['tS a l p','잛'],
    u'\uC79C' : ['tS a l sh','잜'],
    u'\uC79D' : ['tS a l th','잝'],
    u'\uC79E' : ['tS a l ph','잞'],
    u'\uC79F' : ['tS a l h','잟'],
    u'\uC7A0' : ['tS a m','잠'],
    u'\uC7A1' : ['tS a p','잡'],
    u'\uC7A2' : ['tS a p sh','잢'],
    u'\uC7A3' : ['tS a sh','잣'],
    u'\uC7A4' : ['tS a s','잤'],
    u'\uC7A5' : ['tS a N','장'],
    u'\uC7A6' : ['tS a tS','잦'],
    u'\uC7A7' : ['tS a tSh','잧'],
    u'\uC7A8' : ['tS a kh','잨'],
    u'\uC7A9' : ['tS a th','잩'],
    u'\uC7AA' : ['tS a ph','잪'],
    u'\uC7AB' : ['tS a h','잫'],
    u'\uC7AC' : ['tS @','재'],
    u'\uC7AD' : ['tS @ k','잭'],
    u'\uC7AE' : ['tS @ k>','잮'],
    u'\uC7AF' : ['tS @ k sh','잯'],
    u'\uC7B0' : ['tS @ n','잰'],
    u'\uC7B1' : ['tS @ n tS','잱'],
    u'\uC7B2' : ['tS @ n h','잲'],
    u'\uC7B3' : ['tS @ t','잳'],
    u'\uC7B4' : ['tS @ l','잴'],
    u'\uC7B5' : ['tS @ l k','잵'],
    u'\uC7B6' : ['tS @ l m','잶'],
    u'\uC7B7' : ['tS @ l p','잷'],
    u'\uC7B8' : ['tS @ l sh','잸'],
    u'\uC7B9' : ['tS @ l th','잹'],
    u'\uC7BA' : ['tS @ l ph','잺'],
    u'\uC7BB' : ['tS @ l h','잻'],
    u'\uC7BC' : ['tS @ m','잼'],
    u'\uC7BD' : ['tS @ p','잽'],
    u'\uC7BE' : ['tS @ p sh','잾'],
    u'\uC7BF' : ['tS @ sh','잿'],
    u'\uC7C0' : ['tS @ s','쟀'],
    u'\uC7C1' : ['tS @ N','쟁'],
    u'\uC7C2' : ['tS @ tS','쟂'],
    u'\uC7C3' : ['tS @ tSh','쟃'],
    u'\uC7C4' : ['tS @ kh','쟄'],
    u'\uC7C5' : ['tS @ th','쟅'],
    u'\uC7C6' : ['tS @ ph','쟆'],
    u'\uC7C7' : ['tS @ h','쟇'],
    u'\uC7C8' : ['tS j a','쟈'],
    u'\uC7C9' : ['tS j a k','쟉'],
    u'\uC7CA' : ['tS j a k>','쟊'],
    u'\uC7CB' : ['tS j a k sh','쟋'],
    u'\uC7CC' : ['tS j a n','쟌'],
    u'\uC7CD' : ['tS j a n tS','쟍'],
    u'\uC7CE' : ['tS j a n h','쟎'],
    u'\uC7CF' : ['tS j a t','쟏'],
    u'\uC7D0' : ['tS j a l','쟐'],
    u'\uC7D1' : ['tS j a l k','쟑'],
    u'\uC7D2' : ['tS j a l m','쟒'],
    u'\uC7D3' : ['tS j a l p','쟓'],
    u'\uC7D4' : ['tS j a l sh','쟔'],
    u'\uC7D5' : ['tS j a l th','쟕'],
    u'\uC7D6' : ['tS j a l ph','쟖'],
    u'\uC7D7' : ['tS j a l h','쟗'],
    u'\uC7D8' : ['tS j a m','쟘'],
    u'\uC7D9' : ['tS j a p','쟙'],
    u'\uC7DA' : ['tS j a p sh','쟚'],
    u'\uC7DB' : ['tS j a sh','쟛'],
    u'\uC7DC' : ['tS j a s','쟜'],
    u'\uC7DD' : ['tS j a N','쟝'],
    u'\uC7DE' : ['tS j a tS','쟞'],
    u'\uC7DF' : ['tS j a tSh','쟟'],
    u'\uC7E0' : ['tS j a kh','쟠'],
    u'\uC7E1' : ['tS j a th','쟡'],
    u'\uC7E2' : ['tS j a ph','쟢'],
    u'\uC7E3' : ['tS j a h','쟣'],
    u'\uC7E4' : ['tS j @','쟤'],
    u'\uC7E5' : ['tS j @ k','쟥'],
    u'\uC7E6' : ['tS j @ k>','쟦'],
    u'\uC7E7' : ['tS j @ k sh','쟧'],
    u'\uC7E8' : ['tS j @ n','쟨'],
    u'\uC7E9' : ['tS j @ n tS','쟩'],
    u'\uC7EA' : ['tS j @ n h','쟪'],
    u'\uC7EB' : ['tS j @ t','쟫'],
    u'\uC7EC' : ['tS j @ l','쟬'],
    u'\uC7ED' : ['tS j @ l k','쟭'],
    u'\uC7EE' : ['tS j @ l m','쟮'],
    u'\uC7EF' : ['tS j @ l p','쟯'],
    u'\uC7F0' : ['tS j @ l sh','쟰'],
    u'\uC7F1' : ['tS j @ l th','쟱'],
    u'\uC7F2' : ['tS j @ l ph','쟲'],
    u'\uC7F3' : ['tS j @ l h','쟳'],
    u'\uC7F4' : ['tS j @ m','쟴'],
    u'\uC7F5' : ['tS j @ p','쟵'],
    u'\uC7F6' : ['tS j @ p sh','쟶'],
    u'\uC7F7' : ['tS j @ sh','쟷'],
    u'\uC7F8' : ['tS j @ s','쟸'],
    u'\uC7F9' : ['tS j @ N','쟹'],
    u'\uC7FA' : ['tS j @ tS','쟺'],
    u'\uC7FB' : ['tS j @ tSh','쟻'],
    u'\uC7FC' : ['tS j @ kh','쟼'],
    u'\uC7FD' : ['tS j @ th','쟽'],
    u'\uC7FE' : ['tS j @ ph','쟾'],
    u'\uC7FF' : ['tS j @ h','쟿'],
    u'\uC800' : ['tS ^','저'],
    u'\uC801' : ['tS ^ k','적'],
    u'\uC802' : ['tS ^ k>','젂'],
    u'\uC803' : ['tS ^ k sh','젃'],
    u'\uC804' : ['tS ^ n','전'],
    u'\uC805' : ['tS ^ n tS','젅'],
    u'\uC806' : ['tS ^ n h','젆'],
    u'\uC807' : ['tS ^ t','젇'],
    u'\uC808' : ['tS ^ l','절'],
    u'\uC809' : ['tS ^ l k','젉'],
    u'\uC80A' : ['tS ^ l m','젊'],
    u'\uC80B' : ['tS ^ l p','젋'],
    u'\uC80C' : ['tS ^ l sh','젌'],
    u'\uC80D' : ['tS ^ l th','젍'],
    u'\uC80E' : ['tS ^ l ph','젎'],
    u'\uC80F' : ['tS ^ l h','젏'],
    u'\uC810' : ['tS ^ m','점'],
    u'\uC811' : ['tS ^ p','접'],
    u'\uC812' : ['tS ^ p sh','젒'],
    u'\uC813' : ['tS ^ sh','젓'],
    u'\uC814' : ['tS ^ s','젔'],
    u'\uC815' : ['tS ^ N','정'],
    u'\uC816' : ['tS ^ tS','젖'],
    u'\uC817' : ['tS ^ tSh','젗'],
    u'\uC818' : ['tS ^ kh','젘'],
    u'\uC819' : ['tS ^ th','젙'],
    u'\uC81A' : ['tS ^ ph','젚'],
    u'\uC81B' : ['tS ^ h','젛'],
    u'\uC81C' : ['tS e','제'],
    u'\uC81D' : ['tS e k','젝'],
    u'\uC81E' : ['tS e k>','젞'],
    u'\uC81F' : ['tS e k sh','젟'],
    u'\uC820' : ['tS e n','젠'],
    u'\uC821' : ['tS e n tS','젡'],
    u'\uC822' : ['tS e n h','젢'],
    u'\uC823' : ['tS e t','젣'],
    u'\uC824' : ['tS e l','젤'],
    u'\uC825' : ['tS e l k','젥'],
    u'\uC826' : ['tS e l m','젦'],
    u'\uC827' : ['tS e l p','젧'],
    u'\uC828' : ['tS e l sh','젨'],
    u'\uC829' : ['tS e l th','젩'],
    u'\uC82A' : ['tS e l ph','젪'],
    u'\uC82B' : ['tS e l h','젫'],
    u'\uC82C' : ['tS e m','젬'],
    u'\uC82D' : ['tS e p','젭'],
    u'\uC82E' : ['tS e p sh','젮'],
    u'\uC82F' : ['tS e sh','젯'],
    u'\uC830' : ['tS e s','젰'],
    u'\uC831' : ['tS e N','젱'],
    u'\uC832' : ['tS e tS','젲'],
    u'\uC833' : ['tS e tSh','젳'],
    u'\uC834' : ['tS e kh','젴'],
    u'\uC835' : ['tS e th','젵'],
    u'\uC836' : ['tS e ph','젶'],
    u'\uC837' : ['tS e h','젷'],
    u'\uC838' : ['tS j ^','져'],
    u'\uC839' : ['tS j ^ k','젹'],
    u'\uC83A' : ['tS j ^ k>','젺'],
    u'\uC83B' : ['tS j ^ k sh','젻'],
    u'\uC83C' : ['tS j ^ n','젼'],
    u'\uC83D' : ['tS j ^ n tS','젽'],
    u'\uC83E' : ['tS j ^ n h','젾'],
    u'\uC83F' : ['tS j ^ t','젿'],
    u'\uC840' : ['tS j ^ l','졀'],
    u'\uC841' : ['tS j ^ l k','졁'],
    u'\uC842' : ['tS j ^ l m','졂'],
    u'\uC843' : ['tS j ^ l p','졃'],
    u'\uC844' : ['tS j ^ l sh','졄'],
    u'\uC845' : ['tS j ^ l th','졅'],
    u'\uC846' : ['tS j ^ l ph','졆'],
    u'\uC847' : ['tS j ^ l h','졇'],
    u'\uC848' : ['tS j ^ m','졈'],
    u'\uC849' : ['tS j ^ p','졉'],
    u'\uC84A' : ['tS j ^ p sh','졊'],
    u'\uC84B' : ['tS j ^ sh','졋'],
    u'\uC84C' : ['tS j ^ s','졌'],
    u'\uC84D' : ['tS j ^ N','졍'],
    u'\uC84E' : ['tS j ^ tS','졎'],
    u'\uC84F' : ['tS j ^ tSh','졏'],
    u'\uC850' : ['tS j ^ kh','졐'],
    u'\uC851' : ['tS j ^ th','졑'],
    u'\uC852' : ['tS j ^ ph','졒'],
    u'\uC853' : ['tS j ^ h','졓'],
    u'\uC854' : ['tS j e','졔'],
    u'\uC855' : ['tS j e k','졕'],
    u'\uC856' : ['tS j e k>','졖'],
    u'\uC857' : ['tS j e k sh','졗'],
    u'\uC858' : ['tS j e n','졘'],
    u'\uC859' : ['tS j e n tS','졙'],
    u'\uC85A' : ['tS j e n h','졚'],
    u'\uC85B' : ['tS j e t','졛'],
    u'\uC85C' : ['tS j e l','졜'],
    u'\uC85D' : ['tS j e l k','졝'],
    u'\uC85E' : ['tS j e l m','졞'],
    u'\uC85F' : ['tS j e l p','졟'],
    u'\uC860' : ['tS j e l sh','졠'],
    u'\uC861' : ['tS j e l th','졡'],
    u'\uC862' : ['tS j e l ph','졢'],
    u'\uC863' : ['tS j e l h','졣'],
    u'\uC864' : ['tS j e m','졤'],
    u'\uC865' : ['tS j e p','졥'],
    u'\uC866' : ['tS j e p sh','졦'],
    u'\uC867' : ['tS j e sh','졧'],
    u'\uC868' : ['tS j e s','졨'],
    u'\uC869' : ['tS j e N','졩'],
    u'\uC86A' : ['tS j e tS','졪'],
    u'\uC86B' : ['tS j e tSh','졫'],
    u'\uC86C' : ['tS j e kh','졬'],
    u'\uC86D' : ['tS j e th','졭'],
    u'\uC86E' : ['tS j e ph','졮'],
    u'\uC86F' : ['tS j e h','졯'],
    u'\uC870' : ['tS o','조'],
    u'\uC871' : ['tS o k','족'],
    u'\uC872' : ['tS o k>','졲'],
    u'\uC873' : ['tS o k sh','졳'],
    u'\uC874' : ['tS o n','존'],
    u'\uC875' : ['tS o n tS','졵'],
    u'\uC876' : ['tS o n h','졶'],
    u'\uC877' : ['tS o t','졷'],
    u'\uC878' : ['tS o l','졸'],
    u'\uC879' : ['tS o l k','졹'],
    u'\uC87A' : ['tS o l m','졺'],
    u'\uC87B' : ['tS o l p','졻'],
    u'\uC87C' : ['tS o l sh','졼'],
    u'\uC87D' : ['tS o l th','졽'],
    u'\uC87E' : ['tS o l ph','졾'],
    u'\uC87F' : ['tS o l h','졿'],
    u'\uC880' : ['tS o m','좀'],
    u'\uC881' : ['tS o p','좁'],
    u'\uC882' : ['tS o p sh','좂'],
    u'\uC883' : ['tS o sh','좃'],
    u'\uC884' : ['tS o s','좄'],
    u'\uC885' : ['tS o N','종'],
    u'\uC886' : ['tS o tS','좆'],
    u'\uC887' : ['tS o tSh','좇'],
    u'\uC888' : ['tS o kh','좈'],
    u'\uC889' : ['tS o th','좉'],
    u'\uC88A' : ['tS o ph','좊'],
    u'\uC88B' : ['tS o h','좋'],
    u'\uC88C' : ['tS w a','좌'],
    u'\uC88D' : ['tS w a k','좍'],
    u'\uC88E' : ['tS w a k>','좎'],
    u'\uC88F' : ['tS w a k sh','좏'],
    u'\uC890' : ['tS w a n','좐'],
    u'\uC891' : ['tS w a n tS','좑'],
    u'\uC892' : ['tS w a n h','좒'],
    u'\uC893' : ['tS w a t','좓'],
    u'\uC894' : ['tS w a l','좔'],
    u'\uC895' : ['tS w a l k','좕'],
    u'\uC896' : ['tS w a l m','좖'],
    u'\uC897' : ['tS w a l p','좗'],
    u'\uC898' : ['tS w a l sh','좘'],
    u'\uC899' : ['tS w a l th','좙'],
    u'\uC89A' : ['tS w a l ph','좚'],
    u'\uC89B' : ['tS w a l h','좛'],
    u'\uC89C' : ['tS w a m','좜'],
    u'\uC89D' : ['tS w a p','좝'],
    u'\uC89E' : ['tS w a p sh','좞'],
    u'\uC89F' : ['tS w a sh','좟'],
    u'\uC8A0' : ['tS w a s','좠'],
    u'\uC8A1' : ['tS w a N','좡'],
    u'\uC8A2' : ['tS w a tS','좢'],
    u'\uC8A3' : ['tS w a tSh','좣'],
    u'\uC8A4' : ['tS w a kh','좤'],
    u'\uC8A5' : ['tS w a th','좥'],
    u'\uC8A6' : ['tS w a ph','좦'],
    u'\uC8A7' : ['tS w a h','좧'],
    u'\uC8A8' : ['tS w @','좨'],
    u'\uC8A9' : ['tS w @ k','좩'],
    u'\uC8AA' : ['tS w @ k>','좪'],
    u'\uC8AB' : ['tS w @ k sh','좫'],
    u'\uC8AC' : ['tS w @ n','좬'],
    u'\uC8AD' : ['tS w @ n tS','좭'],
    u'\uC8AE' : ['tS w @ n h','좮'],
    u'\uC8AF' : ['tS w @ t','좯'],
    u'\uC8B0' : ['tS w @ l','좰'],
    u'\uC8B1' : ['tS w @ l k','좱'],
    u'\uC8B2' : ['tS w @ l m','좲'],
    u'\uC8B3' : ['tS w @ l p','좳'],
    u'\uC8B4' : ['tS w @ l sh','좴'],
    u'\uC8B5' : ['tS w @ l th','좵'],
    u'\uC8B6' : ['tS w @ l ph','좶'],
    u'\uC8B7' : ['tS w @ l h','좷'],
    u'\uC8B8' : ['tS w @ m','좸'],
    u'\uC8B9' : ['tS w @ p','좹'],
    u'\uC8BA' : ['tS w @ p sh','좺'],
    u'\uC8BB' : ['tS w @ sh','좻'],
    u'\uC8BC' : ['tS w @ s','좼'],
    u'\uC8BD' : ['tS w @ N','좽'],
    u'\uC8BE' : ['tS w @ tS','좾'],
    u'\uC8BF' : ['tS w @ tSh','좿'],
    u'\uC8C0' : ['tS w @ kh','죀'],
    u'\uC8C1' : ['tS w @ th','죁'],
    u'\uC8C2' : ['tS w @ ph','죂'],
    u'\uC8C3' : ['tS w @ h','죃'],
    u'\uC8C4' : ['tS w e','죄'],
    u'\uC8C5' : ['tS w e k','죅'],
    u'\uC8C6' : ['tS w e k>','죆'],
    u'\uC8C7' : ['tS w e k sh','죇'],
    u'\uC8C8' : ['tS w e n','죈'],
    u'\uC8C9' : ['tS w e n tS','죉'],
    u'\uC8CA' : ['tS w e n h','죊'],
    u'\uC8CB' : ['tS w e t','죋'],
    u'\uC8CC' : ['tS w e l','죌'],
    u'\uC8CD' : ['tS w e l k','죍'],
    u'\uC8CE' : ['tS w e l m','죎'],
    u'\uC8CF' : ['tS w e l p','죏'],
    u'\uC8D0' : ['tS w e l sh','죐'],
    u'\uC8D1' : ['tS w e l th','죑'],
    u'\uC8D2' : ['tS w e l ph','죒'],
    u'\uC8D3' : ['tS w e l h','죓'],
    u'\uC8D4' : ['tS w e m','죔'],
    u'\uC8D5' : ['tS w e p','죕'],
    u'\uC8D6' : ['tS w e p sh','죖'],
    u'\uC8D7' : ['tS w e sh','죗'],
    u'\uC8D8' : ['tS w e s','죘'],
    u'\uC8D9' : ['tS w e N','죙'],
    u'\uC8DA' : ['tS w e tS','죚'],
    u'\uC8DB' : ['tS w e tSh','죛'],
    u'\uC8DC' : ['tS w e kh','죜'],
    u'\uC8DD' : ['tS w e th','죝'],
    u'\uC8DE' : ['tS w e ph','죞'],
    u'\uC8DF' : ['tS w e h','죟'],
    u'\uC8E0' : ['tS j o','죠'],
    u'\uC8E1' : ['tS j o k','죡'],
    u'\uC8E2' : ['tS j o k>','죢'],
    u'\uC8E3' : ['tS j o k sh','죣'],
    u'\uC8E4' : ['tS j o n','죤'],
    u'\uC8E5' : ['tS j o n tS','죥'],
    u'\uC8E6' : ['tS j o n h','죦'],
    u'\uC8E7' : ['tS j o t','죧'],
    u'\uC8E8' : ['tS j o l','죨'],
    u'\uC8E9' : ['tS j o l k','죩'],
    u'\uC8EA' : ['tS j o l m','죪'],
    u'\uC8EB' : ['tS j o l p','죫'],
    u'\uC8EC' : ['tS j o l sh','죬'],
    u'\uC8ED' : ['tS j o l th','죭'],
    u'\uC8EE' : ['tS j o l ph','죮'],
    u'\uC8EF' : ['tS j o l h','죯'],
    u'\uC8F0' : ['tS j o m','죰'],
    u'\uC8F1' : ['tS j o p','죱'],
    u'\uC8F2' : ['tS j o p sh','죲'],
    u'\uC8F3' : ['tS j o sh','죳'],
    u'\uC8F4' : ['tS j o s','죴'],
    u'\uC8F5' : ['tS j o N','죵'],
    u'\uC8F6' : ['tS j o tS','죶'],
    u'\uC8F7' : ['tS j o tSh','죷'],
    u'\uC8F8' : ['tS j o kh','죸'],
    u'\uC8F9' : ['tS j o th','죹'],
    u'\uC8FA' : ['tS j o ph','죺'],
    u'\uC8FB' : ['tS j o h','죻'],
    u'\uC8FC' : ['tS u','주'],
    u'\uC8FD' : ['tS u k','죽'],
    u'\uC8FE' : ['tS u k>','죾'],
    u'\uC8FF' : ['tS u k sh','죿'],
    u'\uC900' : ['tS u n','준'],
    u'\uC901' : ['tS u n tS','줁'],
    u'\uC902' : ['tS u n h','줂'],
    u'\uC903' : ['tS u t','줃'],
    u'\uC904' : ['tS u l','줄'],
    u'\uC905' : ['tS u l k','줅'],
    u'\uC906' : ['tS u l m','줆'],
    u'\uC907' : ['tS u l p','줇'],
    u'\uC908' : ['tS u l sh','줈'],
    u'\uC909' : ['tS u l th','줉'],
    u'\uC90A' : ['tS u l ph','줊'],
    u'\uC90B' : ['tS u l h','줋'],
    u'\uC90C' : ['tS u m','줌'],
    u'\uC90D' : ['tS u p','줍'],
    u'\uC90E' : ['tS u p sh','줎'],
    u'\uC90F' : ['tS u sh','줏'],
    u'\uC910' : ['tS u s','줐'],
    u'\uC911' : ['tS u N','중'],
    u'\uC912' : ['tS u tS','줒'],
    u'\uC913' : ['tS u tSh','줓'],
    u'\uC914' : ['tS u kh','줔'],
    u'\uC915' : ['tS u th','줕'],
    u'\uC916' : ['tS u ph','줖'],
    u'\uC917' : ['tS u h','줗'],
    u'\uC918' : ['tS w ^','줘'],
    u'\uC919' : ['tS w ^ k','줙'],
    u'\uC91A' : ['tS w ^ k>','줚'],
    u'\uC91B' : ['tS w ^ k sh','줛'],
    u'\uC91C' : ['tS w ^ n','줜'],
    u'\uC91D' : ['tS w ^ n tS','줝'],
    u'\uC91E' : ['tS w ^ n h','줞'],
    u'\uC91F' : ['tS w ^ t','줟'],
    u'\uC920' : ['tS w ^ l','줠'],
    u'\uC921' : ['tS w ^ l k','줡'],
    u'\uC922' : ['tS w ^ l m','줢'],
    u'\uC923' : ['tS w ^ l p','줣'],
    u'\uC924' : ['tS w ^ l sh','줤'],
    u'\uC925' : ['tS w ^ l th','줥'],
    u'\uC926' : ['tS w ^ l ph','줦'],
    u'\uC927' : ['tS w ^ l h','줧'],
    u'\uC928' : ['tS w ^ m','줨'],
    u'\uC929' : ['tS w ^ p','줩'],
    u'\uC92A' : ['tS w ^ p sh','줪'],
    u'\uC92B' : ['tS w ^ sh','줫'],
    u'\uC92C' : ['tS w ^ s','줬'],
    u'\uC92D' : ['tS w ^ N','줭'],
    u'\uC92E' : ['tS w ^ tS','줮'],
    u'\uC92F' : ['tS w ^ tSh','줯'],
    u'\uC930' : ['tS w ^ kh','줰'],
    u'\uC931' : ['tS w ^ th','줱'],
    u'\uC932' : ['tS w ^ ph','줲'],
    u'\uC933' : ['tS w ^ h','줳'],
    u'\uC934' : ['tS w E','줴'],
    u'\uC935' : ['tS w E k','줵'],
    u'\uC936' : ['tS w E k>','줶'],
    u'\uC937' : ['tS w E k sh','줷'],
    u'\uC938' : ['tS w E n','줸'],
    u'\uC939' : ['tS w E n tS','줹'],
    u'\uC93A' : ['tS w E n h','줺'],
    u'\uC93B' : ['tS w E t','줻'],
    u'\uC93C' : ['tS w E l','줼'],
    u'\uC93D' : ['tS w E l k','줽'],
    u'\uC93E' : ['tS w E l m','줾'],
    u'\uC93F' : ['tS w E l p','줿'],
    u'\uC940' : ['tS w E l sh','쥀'],
    u'\uC941' : ['tS w E l th','쥁'],
    u'\uC942' : ['tS w E l ph','쥂'],
    u'\uC943' : ['tS w E l h','쥃'],
    u'\uC944' : ['tS w E m','쥄'],
    u'\uC945' : ['tS w E p','쥅'],
    u'\uC946' : ['tS w E p sh','쥆'],
    u'\uC947' : ['tS w E sh','쥇'],
    u'\uC948' : ['tS w E s','쥈'],
    u'\uC949' : ['tS w E N','쥉'],
    u'\uC94A' : ['tS w E tS','쥊'],
    u'\uC94B' : ['tS w E tSh','쥋'],
    u'\uC94C' : ['tS w E kh','쥌'],
    u'\uC94D' : ['tS w E th','쥍'],
    u'\uC94E' : ['tS w E ph','쥎'],
    u'\uC94F' : ['tS w E h','쥏'],
    u'\uC950' : ['tS 7','쥐'],
    u'\uC951' : ['tS 7 k','쥑'],
    u'\uC952' : ['tS 7 k>','쥒'],
    u'\uC953' : ['tS 7 k sh','쥓'],
    u'\uC954' : ['tS 7 n','쥔'],
    u'\uC955' : ['tS 7 n tS','쥕'],
    u'\uC956' : ['tS 7 n h','쥖'],
    u'\uC957' : ['tS 7 t','쥗'],
    u'\uC958' : ['tS 7 l','쥘'],
    u'\uC959' : ['tS 7 l k','쥙'],
    u'\uC95A' : ['tS 7 l m','쥚'],
    u'\uC95B' : ['tS 7 l p','쥛'],
    u'\uC95C' : ['tS 7 l sh','쥜'],
    u'\uC95D' : ['tS 7 l th','쥝'],
    u'\uC95E' : ['tS 7 l ph','쥞'],
    u'\uC95F' : ['tS 7 l h','쥟'],
    u'\uC960' : ['tS 7 m','쥠'],
    u'\uC961' : ['tS 7 p','쥡'],
    u'\uC962' : ['tS 7 p sh','쥢'],
    u'\uC963' : ['tS 7 sh','쥣'],
    u'\uC964' : ['tS 7 s','쥤'],
    u'\uC965' : ['tS 7 N','쥥'],
    u'\uC966' : ['tS 7 tS','쥦'],
    u'\uC967' : ['tS 7 tSh','쥧'],
    u'\uC968' : ['tS 7 kh','쥨'],
    u'\uC969' : ['tS 7 th','쥩'],
    u'\uC96A' : ['tS 7 ph','쥪'],
    u'\uC96B' : ['tS 7 h','쥫'],
    u'\uC96C' : ['tS j u','쥬'],
    u'\uC96D' : ['tS j u k','쥭'],
    u'\uC96E' : ['tS j u k>','쥮'],
    u'\uC96F' : ['tS j u k sh','쥯'],
    u'\uC970' : ['tS j u n','쥰'],
    u'\uC971' : ['tS j u n tS','쥱'],
    u'\uC972' : ['tS j u n h','쥲'],
    u'\uC973' : ['tS j u t','쥳'],
    u'\uC974' : ['tS j u l','쥴'],
    u'\uC975' : ['tS j u l k','쥵'],
    u'\uC976' : ['tS j u l m','쥶'],
    u'\uC977' : ['tS j u l p','쥷'],
    u'\uC978' : ['tS j u l sh','쥸'],
    u'\uC979' : ['tS j u l th','쥹'],
    u'\uC97A' : ['tS j u l ph','쥺'],
    u'\uC97B' : ['tS j u l h','쥻'],
    u'\uC97C' : ['tS j u m','쥼'],
    u'\uC97D' : ['tS j u p','쥽'],
    u'\uC97E' : ['tS j u p sh','쥾'],
    u'\uC97F' : ['tS j u sh','쥿'],
    u'\uC980' : ['tS j u s','즀'],
    u'\uC981' : ['tS j u N','즁'],
    u'\uC982' : ['tS j u tS','즂'],
    u'\uC983' : ['tS j u tSh','즃'],
    u'\uC984' : ['tS j u kh','즄'],
    u'\uC985' : ['tS j u th','즅'],
    u'\uC986' : ['tS j u ph','즆'],
    u'\uC987' : ['tS j u h','즇'],
    u'\uC988' : ['tS 4','즈'],
    u'\uC989' : ['tS 4 k','즉'],
    u'\uC98A' : ['tS 4 k>','즊'],
    u'\uC98B' : ['tS 4 k sh','즋'],
    u'\uC98C' : ['tS 4 n','즌'],
    u'\uC98D' : ['tS 4 n tS','즍'],
    u'\uC98E' : ['tS 4 n h','즎'],
    u'\uC98F' : ['tS 4 t','즏'],
    u'\uC990' : ['tS 4 l','즐'],
    u'\uC991' : ['tS 4 l k','즑'],
    u'\uC992' : ['tS 4 l m','즒'],
    u'\uC993' : ['tS 4 l p','즓'],
    u'\uC994' : ['tS 4 l sh','즔'],
    u'\uC995' : ['tS 4 l th','즕'],
    u'\uC996' : ['tS 4 l ph','즖'],
    u'\uC997' : ['tS 4 l h','즗'],
    u'\uC998' : ['tS 4 m','즘'],
    u'\uC999' : ['tS 4 p','즙'],
    u'\uC99A' : ['tS 4 p sh','즚'],
    u'\uC99B' : ['tS 4 sh','즛'],
    u'\uC99C' : ['tS 4 s','즜'],
    u'\uC99D' : ['tS 4 N','증'],
    u'\uC99E' : ['tS 4 tS','즞'],
    u'\uC99F' : ['tS 4 tSh','즟'],
    u'\uC9A0' : ['tS 4 kh','즠'],
    u'\uC9A1' : ['tS 4 th','즡'],
    u'\uC9A2' : ['tS 4 ph','즢'],
    u'\uC9A3' : ['tS 4 h','즣'],
    u'\uC9A4' : ['tS 4 j','즤'],
    u'\uC9A5' : ['tS 4 j k','즥'],
    u'\uC9A6' : ['tS 4 j k>','즦'],
    u'\uC9A7' : ['tS 4 j k sh','즧'],
    u'\uC9A8' : ['tS 4 j n','즨'],
    u'\uC9A9' : ['tS 4 j n tS','즩'],
    u'\uC9AA' : ['tS 4 j n h','즪'],
    u'\uC9AB' : ['tS 4 j t','즫'],
    u'\uC9AC' : ['tS 4 j l','즬'],
    u'\uC9AD' : ['tS 4 j l k','즭'],
    u'\uC9AE' : ['tS 4 j l m','즮'],
    u'\uC9AF' : ['tS 4 j l p','즯'],
    u'\uC9B0' : ['tS 4 j l sh','즰'],
    u'\uC9B1' : ['tS 4 j l th','즱'],
    u'\uC9B2' : ['tS 4 j l ph','즲'],
    u'\uC9B3' : ['tS 4 j l h','즳'],
    u'\uC9B4' : ['tS 4 j m','즴'],
    u'\uC9B5' : ['tS 4 j p','즵'],
    u'\uC9B6' : ['tS 4 j p sh','즶'],
    u'\uC9B7' : ['tS 4 j sh','즷'],
    u'\uC9B8' : ['tS 4 j s','즸'],
    u'\uC9B9' : ['tS 4 j N','즹'],
    u'\uC9BA' : ['tS 4 j tS','즺'],
    u'\uC9BB' : ['tS 4 j tSh','즻'],
    u'\uC9BC' : ['tS 4 j kh','즼'],
    u'\uC9BD' : ['tS 4 j th','즽'],
    u'\uC9BE' : ['tS 4 j ph','즾'],
    u'\uC9BF' : ['tS 4 j h','즿'],
    u'\uC9C0' : ['tS i','지'],
    u'\uC9C1' : ['tS i k','직'],
    u'\uC9C2' : ['tS i k>','짂'],
    u'\uC9C3' : ['tS i k sh','짃'],
    u'\uC9C4' : ['tS i n','진'],
    u'\uC9C5' : ['tS i n tS','짅'],
    u'\uC9C6' : ['tS i n h','짆'],
    u'\uC9C7' : ['tS i t','짇'],
    u'\uC9C8' : ['tS i l','질'],
    u'\uC9C9' : ['tS i l k','짉'],
    u'\uC9CA' : ['tS i l m','짊'],
    u'\uC9CB' : ['tS i l p','짋'],
    u'\uC9CC' : ['tS i l sh','짌'],
    u'\uC9CD' : ['tS i l th','짍'],
    u'\uC9CE' : ['tS i l ph','짎'],
    u'\uC9CF' : ['tS i l h','짏'],
    u'\uC9D0' : ['tS i m','짐'],
    u'\uC9D1' : ['tS i p','집'],
    u'\uC9D2' : ['tS i p sh','짒'],
    u'\uC9D3' : ['tS i sh','짓'],
    u'\uC9D4' : ['tS i s','짔'],
    u'\uC9D5' : ['tS i N','징'],
    u'\uC9D6' : ['tS i tS','짖'],
    u'\uC9D7' : ['tS i tSh','짗'],
    u'\uC9D8' : ['tS i kh','짘'],
    u'\uC9D9' : ['tS i th','짙'],
    u'\uC9DA' : ['tS i ph','짚'],
    u'\uC9DB' : ['tS i h','짛'],
    u'\uC9DC' : ['tS> a','짜'],
    u'\uC9DD' : ['tS> a k','짝'],
    u'\uC9DE' : ['tS> a k>','짞'],
    u'\uC9DF' : ['tS> a k sh','짟'],
    u'\uC9E0' : ['tS> a n','짠'],
    u'\uC9E1' : ['tS> a n tS','짡'],
    u'\uC9E2' : ['tS> a n h','짢'],
    u'\uC9E3' : ['tS> a t','짣'],
    u'\uC9E4' : ['tS> a l','짤'],
    u'\uC9E5' : ['tS> a l k','짥'],
    u'\uC9E6' : ['tS> a l m','짦'],
    u'\uC9E7' : ['tS> a l p','짧'],
    u'\uC9E8' : ['tS> a l sh','짨'],
    u'\uC9E9' : ['tS> a l th','짩'],
    u'\uC9EA' : ['tS> a l ph','짪'],
    u'\uC9EB' : ['tS> a l h','짫'],
    u'\uC9EC' : ['tS> a m','짬'],
    u'\uC9ED' : ['tS> a p','짭'],
    u'\uC9EE' : ['tS> a p sh','짮'],
    u'\uC9EF' : ['tS> a sh','짯'],
    u'\uC9F0' : ['tS> a s','짰'],
    u'\uC9F1' : ['tS> a N','짱'],
    u'\uC9F2' : ['tS> a tS','짲'],
    u'\uC9F3' : ['tS> a tSh','짳'],
    u'\uC9F4' : ['tS> a kh','짴'],
    u'\uC9F5' : ['tS> a th','짵'],
    u'\uC9F6' : ['tS> a ph','짶'],
    u'\uC9F7' : ['tS> a h','짷'],
    u'\uC9F8' : ['tS> @','째'],
    u'\uC9F9' : ['tS> @ k','짹'],
    u'\uC9FA' : ['tS> @ k>','짺'],
    u'\uC9FB' : ['tS> @ k sh','짻'],
    u'\uC9FC' : ['tS> @ n','짼'],
    u'\uC9FD' : ['tS> @ n tS','짽'],
    u'\uC9FE' : ['tS> @ n h','짾'],
    u'\uC9FF' : ['tS> @ t','짿'],
    u'\uCA00' : ['tS> @ l','쨀'],
    u'\uCA01' : ['tS> @ l k','쨁'],
    u'\uCA02' : ['tS> @ l m','쨂'],
    u'\uCA03' : ['tS> @ l p','쨃'],
    u'\uCA04' : ['tS> @ l sh','쨄'],
    u'\uCA05' : ['tS> @ l th','쨅'],
    u'\uCA06' : ['tS> @ l ph','쨆'],
    u'\uCA07' : ['tS> @ l h','쨇'],
    u'\uCA08' : ['tS> @ m','쨈'],
    u'\uCA09' : ['tS> @ p','쨉'],
    u'\uCA0A' : ['tS> @ p sh','쨊'],
    u'\uCA0B' : ['tS> @ sh','쨋'],
    u'\uCA0C' : ['tS> @ s','쨌'],
    u'\uCA0D' : ['tS> @ N','쨍'],
    u'\uCA0E' : ['tS> @ tS','쨎'],
    u'\uCA0F' : ['tS> @ tSh','쨏'],
    u'\uCA10' : ['tS> @ kh','쨐'],
    u'\uCA11' : ['tS> @ th','쨑'],
    u'\uCA12' : ['tS> @ ph','쨒'],
    u'\uCA13' : ['tS> @ h','쨓'],
    u'\uCA14' : ['tS> j a','쨔'],
    u'\uCA15' : ['tS> j a k','쨕'],
    u'\uCA16' : ['tS> j a k>','쨖'],
    u'\uCA17' : ['tS> j a k sh','쨗'],
    u'\uCA18' : ['tS> j a n','쨘'],
    u'\uCA19' : ['tS> j a n tS','쨙'],
    u'\uCA1A' : ['tS> j a n h','쨚'],
    u'\uCA1B' : ['tS> j a t','쨛'],
    u'\uCA1C' : ['tS> j a l','쨜'],
    u'\uCA1D' : ['tS> j a l k','쨝'],
    u'\uCA1E' : ['tS> j a l m','쨞'],
    u'\uCA1F' : ['tS> j a l p','쨟'],
    u'\uCA20' : ['tS> j a l sh','쨠'],
    u'\uCA21' : ['tS> j a l th','쨡'],
    u'\uCA22' : ['tS> j a l ph','쨢'],
    u'\uCA23' : ['tS> j a l h','쨣'],
    u'\uCA24' : ['tS> j a m','쨤'],
    u'\uCA25' : ['tS> j a p','쨥'],
    u'\uCA26' : ['tS> j a p sh','쨦'],
    u'\uCA27' : ['tS> j a sh','쨧'],
    u'\uCA28' : ['tS> j a s','쨨'],
    u'\uCA29' : ['tS> j a N','쨩'],
    u'\uCA2A' : ['tS> j a tS','쨪'],
    u'\uCA2B' : ['tS> j a tSh','쨫'],
    u'\uCA2C' : ['tS> j a kh','쨬'],
    u'\uCA2D' : ['tS> j a th','쨭'],
    u'\uCA2E' : ['tS> j a ph','쨮'],
    u'\uCA2F' : ['tS> j a h','쨯'],
    u'\uCA30' : ['tS> j @','쨰'],
    u'\uCA31' : ['tS> j @ k','쨱'],
    u'\uCA32' : ['tS> j @ k>','쨲'],
    u'\uCA33' : ['tS> j @ k sh','쨳'],
    u'\uCA34' : ['tS> j @ n','쨴'],
    u'\uCA35' : ['tS> j @ n tS','쨵'],
    u'\uCA36' : ['tS> j @ n h','쨶'],
    u'\uCA37' : ['tS> j @ t','쨷'],
    u'\uCA38' : ['tS> j @ l','쨸'],
    u'\uCA39' : ['tS> j @ l k','쨹'],
    u'\uCA3A' : ['tS> j @ l m','쨺'],
    u'\uCA3B' : ['tS> j @ l p','쨻'],
    u'\uCA3C' : ['tS> j @ l sh','쨼'],
    u'\uCA3D' : ['tS> j @ l th','쨽'],
    u'\uCA3E' : ['tS> j @ l ph','쨾'],
    u'\uCA3F' : ['tS> j @ l h','쨿'],
    u'\uCA40' : ['tS> j @ m','쩀'],
    u'\uCA41' : ['tS> j @ p','쩁'],
    u'\uCA42' : ['tS> j @ p sh','쩂'],
    u'\uCA43' : ['tS> j @ sh','쩃'],
    u'\uCA44' : ['tS> j @ s','쩄'],
    u'\uCA45' : ['tS> j @ N','쩅'],
    u'\uCA46' : ['tS> j @ tS','쩆'],
    u'\uCA47' : ['tS> j @ tSh','쩇'],
    u'\uCA48' : ['tS> j @ kh','쩈'],
    u'\uCA49' : ['tS> j @ th','쩉'],
    u'\uCA4A' : ['tS> j @ ph','쩊'],
    u'\uCA4B' : ['tS> j @ h','쩋'],
    u'\uCA4C' : ['tS> ^','쩌'],
    u'\uCA4D' : ['tS> ^ k','쩍'],
    u'\uCA4E' : ['tS> ^ k>','쩎'],
    u'\uCA4F' : ['tS> ^ k sh','쩏'],
    u'\uCA50' : ['tS> ^ n','쩐'],
    u'\uCA51' : ['tS> ^ n tS','쩑'],
    u'\uCA52' : ['tS> ^ n h','쩒'],
    u'\uCA53' : ['tS> ^ t','쩓'],
    u'\uCA54' : ['tS> ^ l','쩔'],
    u'\uCA55' : ['tS> ^ l k','쩕'],
    u'\uCA56' : ['tS> ^ l m','쩖'],
    u'\uCA57' : ['tS> ^ l p','쩗'],
    u'\uCA58' : ['tS> ^ l sh','쩘'],
    u'\uCA59' : ['tS> ^ l th','쩙'],
    u'\uCA5A' : ['tS> ^ l ph','쩚'],
    u'\uCA5B' : ['tS> ^ l h','쩛'],
    u'\uCA5C' : ['tS> ^ m','쩜'],
    u'\uCA5D' : ['tS> ^ p','쩝'],
    u'\uCA5E' : ['tS> ^ p sh','쩞'],
    u'\uCA5F' : ['tS> ^ sh','쩟'],
    u'\uCA60' : ['tS> ^ s','쩠'],
    u'\uCA61' : ['tS> ^ N','쩡'],
    u'\uCA62' : ['tS> ^ tS','쩢'],
    u'\uCA63' : ['tS> ^ tSh','쩣'],
    u'\uCA64' : ['tS> ^ kh','쩤'],
    u'\uCA65' : ['tS> ^ th','쩥'],
    u'\uCA66' : ['tS> ^ ph','쩦'],
    u'\uCA67' : ['tS> ^ h','쩧'],
    u'\uCA68' : ['tS> e','쩨'],
    u'\uCA69' : ['tS> e k','쩩'],
    u'\uCA6A' : ['tS> e k>','쩪'],
    u'\uCA6B' : ['tS> e k sh','쩫'],
    u'\uCA6C' : ['tS> e n','쩬'],
    u'\uCA6D' : ['tS> e n tS','쩭'],
    u'\uCA6E' : ['tS> e n h','쩮'],
    u'\uCA6F' : ['tS> e t','쩯'],
    u'\uCA70' : ['tS> e l','쩰'],
    u'\uCA71' : ['tS> e l k','쩱'],
    u'\uCA72' : ['tS> e l m','쩲'],
    u'\uCA73' : ['tS> e l p','쩳'],
    u'\uCA74' : ['tS> e l sh','쩴'],
    u'\uCA75' : ['tS> e l th','쩵'],
    u'\uCA76' : ['tS> e l ph','쩶'],
    u'\uCA77' : ['tS> e l h','쩷'],
    u'\uCA78' : ['tS> e m','쩸'],
    u'\uCA79' : ['tS> e p','쩹'],
    u'\uCA7A' : ['tS> e p sh','쩺'],
    u'\uCA7B' : ['tS> e sh','쩻'],
    u'\uCA7C' : ['tS> e s','쩼'],
    u'\uCA7D' : ['tS> e N','쩽'],
    u'\uCA7E' : ['tS> e tS','쩾'],
    u'\uCA7F' : ['tS> e tSh','쩿'],
    u'\uCA80' : ['tS> e kh','쪀'],
    u'\uCA81' : ['tS> e th','쪁'],
    u'\uCA82' : ['tS> e ph','쪂'],
    u'\uCA83' : ['tS> e h','쪃'],
    u'\uCA84' : ['tS> j ^','쪄'],
    u'\uCA85' : ['tS> j ^ k','쪅'],
    u'\uCA86' : ['tS> j ^ k>','쪆'],
    u'\uCA87' : ['tS> j ^ k sh','쪇'],
    u'\uCA88' : ['tS> j ^ n','쪈'],
    u'\uCA89' : ['tS> j ^ n tS','쪉'],
    u'\uCA8A' : ['tS> j ^ n h','쪊'],
    u'\uCA8B' : ['tS> j ^ t','쪋'],
    u'\uCA8C' : ['tS> j ^ l','쪌'],
    u'\uCA8D' : ['tS> j ^ l k','쪍'],
    u'\uCA8E' : ['tS> j ^ l m','쪎'],
    u'\uCA8F' : ['tS> j ^ l p','쪏'],
    u'\uCA90' : ['tS> j ^ l sh','쪐'],
    u'\uCA91' : ['tS> j ^ l th','쪑'],
    u'\uCA92' : ['tS> j ^ l ph','쪒'],
    u'\uCA93' : ['tS> j ^ l h','쪓'],
    u'\uCA94' : ['tS> j ^ m','쪔'],
    u'\uCA95' : ['tS> j ^ p','쪕'],
    u'\uCA96' : ['tS> j ^ p sh','쪖'],
    u'\uCA97' : ['tS> j ^ sh','쪗'],
    u'\uCA98' : ['tS> j ^ s','쪘'],
    u'\uCA99' : ['tS> j ^ N','쪙'],
    u'\uCA9A' : ['tS> j ^ tS','쪚'],
    u'\uCA9B' : ['tS> j ^ tSh','쪛'],
    u'\uCA9C' : ['tS> j ^ kh','쪜'],
    u'\uCA9D' : ['tS> j ^ th','쪝'],
    u'\uCA9E' : ['tS> j ^ ph','쪞'],
    u'\uCA9F' : ['tS> j ^ h','쪟'],
    u'\uCAA0' : ['tS> j e','쪠'],
    u'\uCAA1' : ['tS> j e k','쪡'],
    u'\uCAA2' : ['tS> j e k>','쪢'],
    u'\uCAA3' : ['tS> j e k sh','쪣'],
    u'\uCAA4' : ['tS> j e n','쪤'],
    u'\uCAA5' : ['tS> j e n tS','쪥'],
    u'\uCAA6' : ['tS> j e n h','쪦'],
    u'\uCAA7' : ['tS> j e t','쪧'],
    u'\uCAA8' : ['tS> j e l','쪨'],
    u'\uCAA9' : ['tS> j e l k','쪩'],
    u'\uCAAA' : ['tS> j e l m','쪪'],
    u'\uCAAB' : ['tS> j e l p','쪫'],
    u'\uCAAC' : ['tS> j e l sh','쪬'],
    u'\uCAAD' : ['tS> j e l th','쪭'],
    u'\uCAAE' : ['tS> j e l ph','쪮'],
    u'\uCAAF' : ['tS> j e l h','쪯'],
    u'\uCAB0' : ['tS> j e m','쪰'],
    u'\uCAB1' : ['tS> j e p','쪱'],
    u'\uCAB2' : ['tS> j e p sh','쪲'],
    u'\uCAB3' : ['tS> j e sh','쪳'],
    u'\uCAB4' : ['tS> j e s','쪴'],
    u'\uCAB5' : ['tS> j e N','쪵'],
    u'\uCAB6' : ['tS> j e tS','쪶'],
    u'\uCAB7' : ['tS> j e tSh','쪷'],
    u'\uCAB8' : ['tS> j e kh','쪸'],
    u'\uCAB9' : ['tS> j e th','쪹'],
    u'\uCABA' : ['tS> j e ph','쪺'],
    u'\uCABB' : ['tS> j e h','쪻'],
    u'\uCABC' : ['tS> o','쪼'],
    u'\uCABD' : ['tS> o k','쪽'],
    u'\uCABE' : ['tS> o k>','쪾'],
    u'\uCABF' : ['tS> o k sh','쪿'],
    u'\uCAC0' : ['tS> o n','쫀'],
    u'\uCAC1' : ['tS> o n tS','쫁'],
    u'\uCAC2' : ['tS> o n h','쫂'],
    u'\uCAC3' : ['tS> o t','쫃'],
    u'\uCAC4' : ['tS> o l','쫄'],
    u'\uCAC5' : ['tS> o l k','쫅'],
    u'\uCAC6' : ['tS> o l m','쫆'],
    u'\uCAC7' : ['tS> o l p','쫇'],
    u'\uCAC8' : ['tS> o l sh','쫈'],
    u'\uCAC9' : ['tS> o l th','쫉'],
    u'\uCACA' : ['tS> o l ph','쫊'],
    u'\uCACB' : ['tS> o l h','쫋'],
    u'\uCACC' : ['tS> o m','쫌'],
    u'\uCACD' : ['tS> o p','쫍'],
    u'\uCACE' : ['tS> o p sh','쫎'],
    u'\uCACF' : ['tS> o sh','쫏'],
    u'\uCAD0' : ['tS> o s','쫐'],
    u'\uCAD1' : ['tS> o N','쫑'],
    u'\uCAD2' : ['tS> o tS','쫒'],
    u'\uCAD3' : ['tS> o tSh','쫓'],
    u'\uCAD4' : ['tS> o kh','쫔'],
    u'\uCAD5' : ['tS> o th','쫕'],
    u'\uCAD6' : ['tS> o ph','쫖'],
    u'\uCAD7' : ['tS> o h','쫗'],
    u'\uCAD8' : ['tS> w a','쫘'],
    u'\uCAD9' : ['tS> w a k','쫙'],
    u'\uCADA' : ['tS> w a k>','쫚'],
    u'\uCADB' : ['tS> w a k sh','쫛'],
    u'\uCADC' : ['tS> w a n','쫜'],
    u'\uCADD' : ['tS> w a n tS','쫝'],
    u'\uCADE' : ['tS> w a n h','쫞'],
    u'\uCADF' : ['tS> w a t','쫟'],
    u'\uCAE0' : ['tS> w a l','쫠'],
    u'\uCAE1' : ['tS> w a l k','쫡'],
    u'\uCAE2' : ['tS> w a l m','쫢'],
    u'\uCAE3' : ['tS> w a l p','쫣'],
    u'\uCAE4' : ['tS> w a l sh','쫤'],
    u'\uCAE5' : ['tS> w a l th','쫥'],
    u'\uCAE6' : ['tS> w a l ph','쫦'],
    u'\uCAE7' : ['tS> w a l h','쫧'],
    u'\uCAE8' : ['tS> w a m','쫨'],
    u'\uCAE9' : ['tS> w a p','쫩'],
    u'\uCAEA' : ['tS> w a p sh','쫪'],
    u'\uCAEB' : ['tS> w a sh','쫫'],
    u'\uCAEC' : ['tS> w a s','쫬'],
    u'\uCAED' : ['tS> w a N','쫭'],
    u'\uCAEE' : ['tS> w a tS','쫮'],
    u'\uCAEF' : ['tS> w a tSh','쫯'],
    u'\uCAF0' : ['tS> w a kh','쫰'],
    u'\uCAF1' : ['tS> w a th','쫱'],
    u'\uCAF2' : ['tS> w a ph','쫲'],
    u'\uCAF3' : ['tS> w a h','쫳'],
    u'\uCAF4' : ['tS> w @','쫴'],
    u'\uCAF5' : ['tS> w @ k','쫵'],
    u'\uCAF6' : ['tS> w @ k>','쫶'],
    u'\uCAF7' : ['tS> w @ k sh','쫷'],
    u'\uCAF8' : ['tS> w @ n','쫸'],
    u'\uCAF9' : ['tS> w @ n tS','쫹'],
    u'\uCAFA' : ['tS> w @ n h','쫺'],
    u'\uCAFB' : ['tS> w @ t','쫻'],
    u'\uCAFC' : ['tS> w @ l','쫼'],
    u'\uCAFD' : ['tS> w @ l k','쫽'],
    u'\uCAFE' : ['tS> w @ l m','쫾'],
    u'\uCAFF' : ['tS> w @ l p','쫿'],
    u'\uCB00' : ['tS> w @ l sh','쬀'],
    u'\uCB01' : ['tS> w @ l th','쬁'],
    u'\uCB02' : ['tS> w @ l ph','쬂'],
    u'\uCB03' : ['tS> w @ l h','쬃'],
    u'\uCB04' : ['tS> w @ m','쬄'],
    u'\uCB05' : ['tS> w @ p','쬅'],
    u'\uCB06' : ['tS> w @ p sh','쬆'],
    u'\uCB07' : ['tS> w @ sh','쬇'],
    u'\uCB08' : ['tS> w @ s','쬈'],
    u'\uCB09' : ['tS> w @ N','쬉'],
    u'\uCB0A' : ['tS> w @ tS','쬊'],
    u'\uCB0B' : ['tS> w @ tSh','쬋'],
    u'\uCB0C' : ['tS> w @ kh','쬌'],
    u'\uCB0D' : ['tS> w @ th','쬍'],
    u'\uCB0E' : ['tS> w @ ph','쬎'],
    u'\uCB0F' : ['tS> w @ h','쬏'],
    u'\uCB10' : ['tS> w e','쬐'],
    u'\uCB11' : ['tS> w e k','쬑'],
    u'\uCB12' : ['tS> w e k>','쬒'],
    u'\uCB13' : ['tS> w e k sh','쬓'],
    u'\uCB14' : ['tS> w e n','쬔'],
    u'\uCB15' : ['tS> w e n tS','쬕'],
    u'\uCB16' : ['tS> w e n h','쬖'],
    u'\uCB17' : ['tS> w e t','쬗'],
    u'\uCB18' : ['tS> w e l','쬘'],
    u'\uCB19' : ['tS> w e l k','쬙'],
    u'\uCB1A' : ['tS> w e l m','쬚'],
    u'\uCB1B' : ['tS> w e l p','쬛'],
    u'\uCB1C' : ['tS> w e l sh','쬜'],
    u'\uCB1D' : ['tS> w e l th','쬝'],
    u'\uCB1E' : ['tS> w e l ph','쬞'],
    u'\uCB1F' : ['tS> w e l h','쬟'],
    u'\uCB20' : ['tS> w e m','쬠'],
    u'\uCB21' : ['tS> w e p','쬡'],
    u'\uCB22' : ['tS> w e p sh','쬢'],
    u'\uCB23' : ['tS> w e sh','쬣'],
    u'\uCB24' : ['tS> w e s','쬤'],
    u'\uCB25' : ['tS> w e N','쬥'],
    u'\uCB26' : ['tS> w e tS','쬦'],
    u'\uCB27' : ['tS> w e tSh','쬧'],
    u'\uCB28' : ['tS> w e kh','쬨'],
    u'\uCB29' : ['tS> w e th','쬩'],
    u'\uCB2A' : ['tS> w e ph','쬪'],
    u'\uCB2B' : ['tS> w e h','쬫'],
    u'\uCB2C' : ['tS> j o','쬬'],
    u'\uCB2D' : ['tS> j o k','쬭'],
    u'\uCB2E' : ['tS> j o k>','쬮'],
    u'\uCB2F' : ['tS> j o k sh','쬯'],
    u'\uCB30' : ['tS> j o n','쬰'],
    u'\uCB31' : ['tS> j o n tS','쬱'],
    u'\uCB32' : ['tS> j o n h','쬲'],
    u'\uCB33' : ['tS> j o t','쬳'],
    u'\uCB34' : ['tS> j o l','쬴'],
    u'\uCB35' : ['tS> j o l k','쬵'],
    u'\uCB36' : ['tS> j o l m','쬶'],
    u'\uCB37' : ['tS> j o l p','쬷'],
    u'\uCB38' : ['tS> j o l sh','쬸'],
    u'\uCB39' : ['tS> j o l th','쬹'],
    u'\uCB3A' : ['tS> j o l ph','쬺'],
    u'\uCB3B' : ['tS> j o l h','쬻'],
    u'\uCB3C' : ['tS> j o m','쬼'],
    u'\uCB3D' : ['tS> j o p','쬽'],
    u'\uCB3E' : ['tS> j o p sh','쬾'],
    u'\uCB3F' : ['tS> j o sh','쬿'],
    u'\uCB40' : ['tS> j o s','쭀'],
    u'\uCB41' : ['tS> j o N','쭁'],
    u'\uCB42' : ['tS> j o tS','쭂'],
    u'\uCB43' : ['tS> j o tSh','쭃'],
    u'\uCB44' : ['tS> j o kh','쭄'],
    u'\uCB45' : ['tS> j o th','쭅'],
    u'\uCB46' : ['tS> j o ph','쭆'],
    u'\uCB47' : ['tS> j o h','쭇'],
    u'\uCB48' : ['tS> u','쭈'],
    u'\uCB49' : ['tS> u k','쭉'],
    u'\uCB4A' : ['tS> u k>','쭊'],
    u'\uCB4B' : ['tS> u k sh','쭋'],
    u'\uCB4C' : ['tS> u n','쭌'],
    u'\uCB4D' : ['tS> u n tS','쭍'],
    u'\uCB4E' : ['tS> u n h','쭎'],
    u'\uCB4F' : ['tS> u t','쭏'],
    u'\uCB50' : ['tS> u l','쭐'],
    u'\uCB51' : ['tS> u l k','쭑'],
    u'\uCB52' : ['tS> u l m','쭒'],
    u'\uCB53' : ['tS> u l p','쭓'],
    u'\uCB54' : ['tS> u l sh','쭔'],
    u'\uCB55' : ['tS> u l th','쭕'],
    u'\uCB56' : ['tS> u l ph','쭖'],
    u'\uCB57' : ['tS> u l h','쭗'],
    u'\uCB58' : ['tS> u m','쭘'],
    u'\uCB59' : ['tS> u p','쭙'],
    u'\uCB5A' : ['tS> u p sh','쭚'],
    u'\uCB5B' : ['tS> u sh','쭛'],
    u'\uCB5C' : ['tS> u s','쭜'],
    u'\uCB5D' : ['tS> u N','쭝'],
    u'\uCB5E' : ['tS> u tS','쭞'],
    u'\uCB5F' : ['tS> u tSh','쭟'],
    u'\uCB60' : ['tS> u kh','쭠'],
    u'\uCB61' : ['tS> u th','쭡'],
    u'\uCB62' : ['tS> u ph','쭢'],
    u'\uCB63' : ['tS> u h','쭣'],
    u'\uCB64' : ['tS> w ^','쭤'],
    u'\uCB65' : ['tS> w ^ k','쭥'],
    u'\uCB66' : ['tS> w ^ k>','쭦'],
    u'\uCB67' : ['tS> w ^ k sh','쭧'],
    u'\uCB68' : ['tS> w ^ n','쭨'],
    u'\uCB69' : ['tS> w ^ n tS','쭩'],
    u'\uCB6A' : ['tS> w ^ n h','쭪'],
    u'\uCB6B' : ['tS> w ^ t','쭫'],
    u'\uCB6C' : ['tS> w ^ l','쭬'],
    u'\uCB6D' : ['tS> w ^ l k','쭭'],
    u'\uCB6E' : ['tS> w ^ l m','쭮'],
    u'\uCB6F' : ['tS> w ^ l p','쭯'],
    u'\uCB70' : ['tS> w ^ l sh','쭰'],
    u'\uCB71' : ['tS> w ^ l th','쭱'],
    u'\uCB72' : ['tS> w ^ l ph','쭲'],
    u'\uCB73' : ['tS> w ^ l h','쭳'],
    u'\uCB74' : ['tS> w ^ m','쭴'],
    u'\uCB75' : ['tS> w ^ p','쭵'],
    u'\uCB76' : ['tS> w ^ p sh','쭶'],
    u'\uCB77' : ['tS> w ^ sh','쭷'],
    u'\uCB78' : ['tS> w ^ s','쭸'],
    u'\uCB79' : ['tS> w ^ N','쭹'],
    u'\uCB7A' : ['tS> w ^ tS','쭺'],
    u'\uCB7B' : ['tS> w ^ tSh','쭻'],
    u'\uCB7C' : ['tS> w ^ kh','쭼'],
    u'\uCB7D' : ['tS> w ^ th','쭽'],
    u'\uCB7E' : ['tS> w ^ ph','쭾'],
    u'\uCB7F' : ['tS> w ^ h','쭿'],
    u'\uCB80' : ['tS> w E','쮀'],
    u'\uCB81' : ['tS> w E k','쮁'],
    u'\uCB82' : ['tS> w E k>','쮂'],
    u'\uCB83' : ['tS> w E k sh','쮃'],
    u'\uCB84' : ['tS> w E n','쮄'],
    u'\uCB85' : ['tS> w E n tS','쮅'],
    u'\uCB86' : ['tS> w E n h','쮆'],
    u'\uCB87' : ['tS> w E t','쮇'],
    u'\uCB88' : ['tS> w E l','쮈'],
    u'\uCB89' : ['tS> w E l k','쮉'],
    u'\uCB8A' : ['tS> w E l m','쮊'],
    u'\uCB8B' : ['tS> w E l p','쮋'],
    u'\uCB8C' : ['tS> w E l sh','쮌'],
    u'\uCB8D' : ['tS> w E l th','쮍'],
    u'\uCB8E' : ['tS> w E l ph','쮎'],
    u'\uCB8F' : ['tS> w E l h','쮏'],
    u'\uCB90' : ['tS> w E m','쮐'],
    u'\uCB91' : ['tS> w E p','쮑'],
    u'\uCB92' : ['tS> w E p sh','쮒'],
    u'\uCB93' : ['tS> w E sh','쮓'],
    u'\uCB94' : ['tS> w E s','쮔'],
    u'\uCB95' : ['tS> w E N','쮕'],
    u'\uCB96' : ['tS> w E tS','쮖'],
    u'\uCB97' : ['tS> w E tSh','쮗'],
    u'\uCB98' : ['tS> w E kh','쮘'],
    u'\uCB99' : ['tS> w E th','쮙'],
    u'\uCB9A' : ['tS> w E ph','쮚'],
    u'\uCB9B' : ['tS> w E h','쮛'],
    u'\uCB9C' : ['tS> 7','쮜'],
    u'\uCB9D' : ['tS> 7 k','쮝'],
    u'\uCB9E' : ['tS> 7 k>','쮞'],
    u'\uCB9F' : ['tS> 7 k sh','쮟'],
    u'\uCBA0' : ['tS> 7 n','쮠'],
    u'\uCBA1' : ['tS> 7 n tS','쮡'],
    u'\uCBA2' : ['tS> 7 n h','쮢'],
    u'\uCBA3' : ['tS> 7 t','쮣'],
    u'\uCBA4' : ['tS> 7 l','쮤'],
    u'\uCBA5' : ['tS> 7 l k','쮥'],
    u'\uCBA6' : ['tS> 7 l m','쮦'],
    u'\uCBA7' : ['tS> 7 l p','쮧'],
    u'\uCBA8' : ['tS> 7 l sh','쮨'],
    u'\uCBA9' : ['tS> 7 l th','쮩'],
    u'\uCBAA' : ['tS> 7 l ph','쮪'],
    u'\uCBAB' : ['tS> 7 l h','쮫'],
    u'\uCBAC' : ['tS> 7 m','쮬'],
    u'\uCBAD' : ['tS> 7 p','쮭'],
    u'\uCBAE' : ['tS> 7 p sh','쮮'],
    u'\uCBAF' : ['tS> 7 sh','쮯'],
    u'\uCBB0' : ['tS> 7 s','쮰'],
    u'\uCBB1' : ['tS> 7 N','쮱'],
    u'\uCBB2' : ['tS> 7 tS','쮲'],
    u'\uCBB3' : ['tS> 7 tSh','쮳'],
    u'\uCBB4' : ['tS> 7 kh','쮴'],
    u'\uCBB5' : ['tS> 7 th','쮵'],
    u'\uCBB6' : ['tS> 7 ph','쮶'],
    u'\uCBB7' : ['tS> 7 h','쮷'],
    u'\uCBB8' : ['tS> j u','쮸'],
    u'\uCBB9' : ['tS> j u k','쮹'],
    u'\uCBBA' : ['tS> j u k>','쮺'],
    u'\uCBBB' : ['tS> j u k sh','쮻'],
    u'\uCBBC' : ['tS> j u n','쮼'],
    u'\uCBBD' : ['tS> j u n tS','쮽'],
    u'\uCBBE' : ['tS> j u n h','쮾'],
    u'\uCBBF' : ['tS> j u t','쮿'],
    u'\uCBC0' : ['tS> j u l','쯀'],
    u'\uCBC1' : ['tS> j u l k','쯁'],
    u'\uCBC2' : ['tS> j u l m','쯂'],
    u'\uCBC3' : ['tS> j u l p','쯃'],
    u'\uCBC4' : ['tS> j u l sh','쯄'],
    u'\uCBC5' : ['tS> j u l th','쯅'],
    u'\uCBC6' : ['tS> j u l ph','쯆'],
    u'\uCBC7' : ['tS> j u l h','쯇'],
    u'\uCBC8' : ['tS> j u m','쯈'],
    u'\uCBC9' : ['tS> j u p','쯉'],
    u'\uCBCA' : ['tS> j u p sh','쯊'],
    u'\uCBCB' : ['tS> j u sh','쯋'],
    u'\uCBCC' : ['tS> j u s','쯌'],
    u'\uCBCD' : ['tS> j u N','쯍'],
    u'\uCBCE' : ['tS> j u tS','쯎'],
    u'\uCBCF' : ['tS> j u tSh','쯏'],
    u'\uCBD0' : ['tS> j u kh','쯐'],
    u'\uCBD1' : ['tS> j u th','쯑'],
    u'\uCBD2' : ['tS> j u ph','쯒'],
    u'\uCBD3' : ['tS> j u h','쯓'],
    u'\uCBD4' : ['tS> 4','쯔'],
    u'\uCBD5' : ['tS> 4 k','쯕'],
    u'\uCBD6' : ['tS> 4 k>','쯖'],
    u'\uCBD7' : ['tS> 4 k s','쯗'],
    u'\uCBD8' : ['tS> 4 n','쯘'],
    u'\uCBD9' : ['tS> 4 n tS','쯙'],
    u'\uCBDA' : ['tS> 4 n h','쯚'],
    u'\uCBDB' : ['tS> 4 t','쯛'],
    u'\uCBDC' : ['tS> 4 l','쯜'],
    u'\uCBDD' : ['tS> 4 l k','쯝'],
    u'\uCBDE' : ['tS> 4 l m','쯞'],
    u'\uCBDF' : ['tS> 4 l p','쯟'],
    u'\uCBE0' : ['tS> 4 l sh','쯠'],
    u'\uCBE1' : ['tS> 4 l th','쯡'],
    u'\uCBE2' : ['tS> 4 l ph','쯢'],
    u'\uCBE3' : ['tS> 4 l h','쯣'],
    u'\uCBE4' : ['tS> 4 m','쯤'],
    u'\uCBE5' : ['tS> 4 p','쯥'],
    u'\uCBE6' : ['tS> 4 p sh','쯦'],
    u'\uCBE7' : ['tS> 4 sh','쯧'],
    u'\uCBE8' : ['tS> 4 s','쯨'],
    u'\uCBE9' : ['tS> 4 N','쯩'],
    u'\uCBEA' : ['tS> 4 tS','쯪'],
    u'\uCBEB' : ['tS> 4 tSh','쯫'],
    u'\uCBEC' : ['tS> 4 kh','쯬'],
    u'\uCBED' : ['tS> 4 th','쯭'],
    u'\uCBEE' : ['tS> 4 ph','쯮'],
    u'\uCBEF' : ['tS> 4 h','쯯'],
    u'\uCBF0' : ['tS> 4 j','쯰'],
    u'\uCBF1' : ['tS> 4 j k','쯱'],
    u'\uCBF2' : ['tS> 4 j k>','쯲'],
    u'\uCBF3' : ['tS> 4 j k sh','쯳'],
    u'\uCBF4' : ['tS> 4 j n','쯴'],
    u'\uCBF5' : ['tS> 4 j n tS','쯵'],
    u'\uCBF6' : ['tS> 4 j n h','쯶'],
    u'\uCBF7' : ['tS> 4 j t','쯷'],
    u'\uCBF8' : ['tS> 4 j l','쯸'],
    u'\uCBF9' : ['tS> 4 j l k','쯹'],
    u'\uCBFA' : ['tS> 4 j l m','쯺'],
    u'\uCBFB' : ['tS> 4 j l p','쯻'],
    u'\uCBFC' : ['tS> 4 j l sh','쯼'],
    u'\uCBFD' : ['tS> 4 j l th','쯽'],
    u'\uCBFE' : ['tS> 4 j l ph','쯾'],
    u'\uCBFF' : ['tS> 4 j l h','쯿'],
    u'\uCC00' : ['tS> 4 j m','찀'],
    u'\uCC01' : ['tS> 4 j p','찁'],
    u'\uCC02' : ['tS> 4 j p sh','찂'],
    u'\uCC03' : ['tS> 4 j sh','찃'],
    u'\uCC04' : ['tS> 4 j s','찄'],
    u'\uCC05' : ['tS> 4 j N','찅'],
    u'\uCC06' : ['tS> 4 j tS','찆'],
    u'\uCC07' : ['tS> 4 j tSh','찇'],
    u'\uCC08' : ['tS> 4 j kh','찈'],
    u'\uCC09' : ['tS> 4 j th','찉'],
    u'\uCC0A' : ['tS> 4 j ph','찊'],
    u'\uCC0B' : ['tS> 4 j h','찋'],
    u'\uCC0C' : ['tS> i','찌'],
    u'\uCC0D' : ['tS> i k','찍'],
    u'\uCC0E' : ['tS> i k>','찎'],
    u'\uCC0F' : ['tS> i k sh','찏'],
    u'\uCC10' : ['tS> i n','찐'],
    u'\uCC11' : ['tS> i n tS','찑'],
    u'\uCC12' : ['tS> i n h','찒'],
    u'\uCC13' : ['tS> i t','찓'],
    u'\uCC14' : ['tS> i l','찔'],
    u'\uCC15' : ['tS> i l k','찕'],
    u'\uCC16' : ['tS> i l m','찖'],
    u'\uCC17' : ['tS> i l p','찗'],
    u'\uCC18' : ['tS> i l sh','찘'],
    u'\uCC19' : ['tS> i l th','찙'],
    u'\uCC1A' : ['tS> i l ph','찚'],
    u'\uCC1B' : ['tS> i l h','찛'],
    u'\uCC1C' : ['tS> i m','찜'],
    u'\uCC1D' : ['tS> i p','찝'],
    u'\uCC1E' : ['tS> i p sh','찞'],
    u'\uCC1F' : ['tS> i sh','찟'],
    u'\uCC20' : ['tS> i s','찠'],
    u'\uCC21' : ['tS> i N','찡'],
    u'\uCC22' : ['tS> i tS','찢'],
    u'\uCC23' : ['tS> i tSh','찣'],
    u'\uCC24' : ['tS> i kh','찤'],
    u'\uCC25' : ['tS> i th','찥'],
    u'\uCC26' : ['tS> i ph','찦'],
    u'\uCC27' : ['tS> i h','찧'],
    u'\uCC28' : ['tSh a','차'],
    u'\uCC29' : ['tSh a k','착'],
    u'\uCC2A' : ['tSh a k>','찪'],
    u'\uCC2B' : ['tSh a k sh','찫'],
    u'\uCC2C' : ['tSh a n','찬'],
    u'\uCC2D' : ['tSh a n tS','찭'],
    u'\uCC2E' : ['tSh a n h','찮'],
    u'\uCC2F' : ['tSh a t','찯'],
    u'\uCC30' : ['tSh a l','찰'],
    u'\uCC31' : ['tSh a l k','찱'],
    u'\uCC32' : ['tSh a l m','찲'],
    u'\uCC33' : ['tSh a l p','찳'],
    u'\uCC34' : ['tSh a l sh','찴'],
    u'\uCC35' : ['tSh a l th','찵'],
    u'\uCC36' : ['tSh a l ph','찶'],
    u'\uCC37' : ['tSh a l h','찷'],
    u'\uCC38' : ['tSh a m','참'],
    u'\uCC39' : ['tSh a p','찹'],
    u'\uCC3A' : ['tSh a p sh','찺'],
    u'\uCC3B' : ['tSh a sh','찻'],
    u'\uCC3C' : ['tSh a s','찼'],
    u'\uCC3D' : ['tSh a N','창'],
    u'\uCC3E' : ['tSh a tS','찾'],
    u'\uCC3F' : ['tSh a tSh','찿'],
    u'\uCC40' : ['tSh a kh','챀'],
    u'\uCC41' : ['tSh a th','챁'],
    u'\uCC42' : ['tSh a ph','챂'],
    u'\uCC43' : ['tSh a h','챃'],
    u'\uCC44' : ['tSh @','채'],
    u'\uCC45' : ['tSh @ k','책'],
    u'\uCC46' : ['tSh @ k>','챆'],
    u'\uCC47' : ['tSh @ k sh','챇'],
    u'\uCC48' : ['tSh @ n','챈'],
    u'\uCC49' : ['tSh @ n tS','챉'],
    u'\uCC4A' : ['tSh @ n h','챊'],
    u'\uCC4B' : ['tSh @ t','챋'],
    u'\uCC4C' : ['tSh @ l','챌'],
    u'\uCC4D' : ['tSh @ l k','챍'],
    u'\uCC4E' : ['tSh @ l m','챎'],
    u'\uCC4F' : ['tSh @ l p','챏'],
    u'\uCC50' : ['tSh @ l sh','챐'],
    u'\uCC51' : ['tSh @ l th','챑'],
    u'\uCC52' : ['tSh @ l ph','챒'],
    u'\uCC53' : ['tSh @ l h','챓'],
    u'\uCC54' : ['tSh @ m','챔'],
    u'\uCC55' : ['tSh @ p','챕'],
    u'\uCC56' : ['tSh @ p sh','챖'],
    u'\uCC57' : ['tSh @ sh','챗'],
    u'\uCC58' : ['tSh @ s','챘'],
    u'\uCC59' : ['tSh @ N','챙'],
    u'\uCC5A' : ['tSh @ tS','챚'],
    u'\uCC5B' : ['tSh @ tSh','챛'],
    u'\uCC5C' : ['tSh @ kh','챜'],
    u'\uCC5D' : ['tSh @ th','챝'],
    u'\uCC5E' : ['tSh @ ph','챞'],
    u'\uCC5F' : ['tSh @ h','챟'],
    u'\uCC60' : ['tSh j a','챠'],
    u'\uCC61' : ['tSh j a k','챡'],
    u'\uCC62' : ['tSh j a k>','챢'],
    u'\uCC63' : ['tSh j a k sh','챣'],
    u'\uCC64' : ['tSh j a n','챤'],
    u'\uCC65' : ['tSh j a n tS','챥'],
    u'\uCC66' : ['tSh j a n h','챦'],
    u'\uCC67' : ['tSh j a t','챧'],
    u'\uCC68' : ['tSh j a l','챨'],
    u'\uCC69' : ['tSh j a l k','챩'],
    u'\uCC6A' : ['tSh j a l m','챪'],
    u'\uCC6B' : ['tSh j a l p','챫'],
    u'\uCC6C' : ['tSh j a l sh','챬'],
    u'\uCC6D' : ['tSh j a l th','챭'],
    u'\uCC6E' : ['tSh j a l ph','챮'],
    u'\uCC6F' : ['tSh j a l h','챯'],
    u'\uCC70' : ['tSh j a m','챰'],
    u'\uCC71' : ['tSh j a p','챱'],
    u'\uCC72' : ['tSh j a p sh','챲'],
    u'\uCC73' : ['tSh j a sh','챳'],
    u'\uCC74' : ['tSh j a s','챴'],
    u'\uCC75' : ['tSh j a N','챵'],
    u'\uCC76' : ['tSh j a tS','챶'],
    u'\uCC77' : ['tSh j a tSh','챷'],
    u'\uCC78' : ['tSh j a kh','챸'],
    u'\uCC79' : ['tSh j a th','챹'],
    u'\uCC7A' : ['tSh j a ph','챺'],
    u'\uCC7B' : ['tSh j a h','챻'],
    u'\uCC7C' : ['tSh j @','챼'],
    u'\uCC7D' : ['tSh j @ k','챽'],
    u'\uCC7E' : ['tSh j @ k>','챾'],
    u'\uCC7F' : ['tSh j @ k sh','챿'],
    u'\uCC80' : ['tSh j @ n','첀'],
    u'\uCC81' : ['tSh j @ n tS','첁'],
    u'\uCC82' : ['tSh j @ n h','첂'],
    u'\uCC83' : ['tSh j @ t','첃'],
    u'\uCC84' : ['tSh j @ l','첄'],
    u'\uCC85' : ['tSh j @ l k','첅'],
    u'\uCC86' : ['tSh j @ l m','첆'],
    u'\uCC87' : ['tSh j @ l p','첇'],
    u'\uCC88' : ['tSh j @ l sh','첈'],
    u'\uCC89' : ['tSh j @ l th','첉'],
    u'\uCC8A' : ['tSh j @ l ph','첊'],
    u'\uCC8B' : ['tSh j @ l h','첋'],
    u'\uCC8C' : ['tSh j @ m','첌'],
    u'\uCC8D' : ['tSh j @ p','첍'],
    u'\uCC8E' : ['tSh j @ p sh','첎'],
    u'\uCC8F' : ['tSh j @ sh','첏'],
    u'\uCC90' : ['tSh j @ s','첐'],
    u'\uCC91' : ['tSh j @ N','첑'],
    u'\uCC92' : ['tSh j @ tS','첒'],
    u'\uCC93' : ['tSh j @ tSh','첓'],
    u'\uCC94' : ['tSh j @ kh','첔'],
    u'\uCC95' : ['tSh j @ th','첕'],
    u'\uCC96' : ['tSh j @ ph','첖'],
    u'\uCC97' : ['tSh j @ h','첗'],
    u'\uCC98' : ['tSh ^','처'],
    u'\uCC99' : ['tSh ^ k','척'],
    u'\uCC9A' : ['tSh ^ k>','첚'],
    u'\uCC9B' : ['tSh ^ k sh','첛'],
    u'\uCC9C' : ['tSh ^ n','천'],
    u'\uCC9D' : ['tSh ^ n tS','첝'],
    u'\uCC9E' : ['tSh ^ n h','첞'],
    u'\uCC9F' : ['tSh ^ t','첟'],
    u'\uCCA0' : ['tSh ^ l','철'],
    u'\uCCA1' : ['tSh ^ l k','첡'],
    u'\uCCA2' : ['tSh ^ l m','첢'],
    u'\uCCA3' : ['tSh ^ l p','첣'],
    u'\uCCA4' : ['tSh ^ l sh','첤'],
    u'\uCCA5' : ['tSh ^ l th','첥'],
    u'\uCCA6' : ['tSh ^ l ph','첦'],
    u'\uCCA7' : ['tSh ^ l h','첧'],
    u'\uCCA8' : ['tSh ^ m','첨'],
    u'\uCCA9' : ['tSh ^ p','첩'],
    u'\uCCAA' : ['tSh ^ p sh','첪'],
    u'\uCCAB' : ['tSh ^ sh','첫'],
    u'\uCCAC' : ['tSh ^ s','첬'],
    u'\uCCAD' : ['tSh ^ N','청'],
    u'\uCCAE' : ['tSh ^ tS','첮'],
    u'\uCCAF' : ['tSh ^ tSh','첯'],
    u'\uCCB0' : ['tSh ^ kh','첰'],
    u'\uCCB1' : ['tSh ^ th','첱'],
    u'\uCCB2' : ['tSh ^ ph','첲'],
    u'\uCCB3' : ['tSh ^ h','첳'],
    u'\uCCB4' : ['tSh e','체'],
    u'\uCCB5' : ['tSh e k','첵'],
    u'\uCCB6' : ['tSh e k>','첶'],
    u'\uCCB7' : ['tSh e k sh','첷'],
    u'\uCCB8' : ['tSh e n','첸'],
    u'\uCCB9' : ['tSh e n tS','첹'],
    u'\uCCBA' : ['tSh e n h','첺'],
    u'\uCCBB' : ['tSh e t','첻'],
    u'\uCCBC' : ['tSh e l','첼'],
    u'\uCCBD' : ['tSh e l k','첽'],
    u'\uCCBE' : ['tSh e l m','첾'],
    u'\uCCBF' : ['tSh e l p','첿'],
    u'\uCCC0' : ['tSh e l sh','쳀'],
    u'\uCCC1' : ['tSh e l th','쳁'],
    u'\uCCC2' : ['tSh e l ph','쳂'],
    u'\uCCC3' : ['tSh e l h','쳃'],
    u'\uCCC4' : ['tSh e m','쳄'],
    u'\uCCC5' : ['tSh e p','쳅'],
    u'\uCCC6' : ['tSh e p sh','쳆'],
    u'\uCCC7' : ['tSh e sh','쳇'],
    u'\uCCC8' : ['tSh e s','쳈'],
    u'\uCCC9' : ['tSh e N','쳉'],
    u'\uCCCA' : ['tSh e tS','쳊'],
    u'\uCCCB' : ['tSh e tSh','쳋'],
    u'\uCCCC' : ['tSh e kh','쳌'],
    u'\uCCCD' : ['tSh e th','쳍'],
    u'\uCCCE' : ['tSh e ph','쳎'],
    u'\uCCCF' : ['tSh e h','쳏'],
    u'\uCCD0' : ['tSh j ^','쳐'],
    u'\uCCD1' : ['tSh j ^ k','쳑'],
    u'\uCCD2' : ['tSh j ^ k>','쳒'],
    u'\uCCD3' : ['tSh j ^ k sh','쳓'],
    u'\uCCD4' : ['tSh j ^ n','쳔'],
    u'\uCCD5' : ['tSh j ^ n tS','쳕'],
    u'\uCCD6' : ['tSh j ^ n h','쳖'],
    u'\uCCD7' : ['tSh j ^ t','쳗'],
    u'\uCCD8' : ['tSh j ^ l','쳘'],
    u'\uCCD9' : ['tSh j ^ l k','쳙'],
    u'\uCCDA' : ['tSh j ^ l m','쳚'],
    u'\uCCDB' : ['tSh j ^ l p','쳛'],
    u'\uCCDC' : ['tSh j ^ l sh','쳜'],
    u'\uCCDD' : ['tSh j ^ l th','쳝'],
    u'\uCCDE' : ['tSh j ^ l ph','쳞'],
    u'\uCCDF' : ['tSh j ^ l h','쳟'],
    u'\uCCE0' : ['tSh j ^ m','쳠'],
    u'\uCCE1' : ['tSh j ^ p','쳡'],
    u'\uCCE2' : ['tSh j ^ p sh','쳢'],
    u'\uCCE3' : ['tSh j ^ sh','쳣'],
    u'\uCCE4' : ['tSh j ^ s','쳤'],
    u'\uCCE5' : ['tSh j ^ N','쳥'],
    u'\uCCE6' : ['tSh j ^ tS','쳦'],
    u'\uCCE7' : ['tSh j ^ tSh','쳧'],
    u'\uCCE8' : ['tSh j ^ kh','쳨'],
    u'\uCCE9' : ['tSh j ^ th','쳩'],
    u'\uCCEA' : ['tSh j ^ ph','쳪'],
    u'\uCCEB' : ['tSh j ^ h','쳫'],
    u'\uCCEC' : ['tSh j e','쳬'],
    u'\uCCED' : ['tSh j e k','쳭'],
    u'\uCCEE' : ['tSh j e k>','쳮'],
    u'\uCCEF' : ['tSh j e k sh','쳯'],
    u'\uCCF0' : ['tSh j e n','쳰'],
    u'\uCCF1' : ['tSh j e n tS','쳱'],
    u'\uCCF2' : ['tSh j e nh','쳲'],
    u'\uCCF3' : ['tSh j e t','쳳'],
    u'\uCCF4' : ['tSh j e l','쳴'],
    u'\uCCF5' : ['tSh j e l k','쳵'],
    u'\uCCF6' : ['tSh j e l m','쳶'],
    u'\uCCF7' : ['tSh j e l p','쳷'],
    u'\uCCF8' : ['tSh j e l sh','쳸'],
    u'\uCCF9' : ['tSh j e l th','쳹'],
    u'\uCCFA' : ['tSh j e l ph','쳺'],
    u'\uCCFB' : ['tSh j e l h','쳻'],
    u'\uCCFC' : ['tSh j e m','쳼'],
    u'\uCCFD' : ['tSh j e p','쳽'],
    u'\uCCFE' : ['tSh j e p sh','쳾'],
    u'\uCCFF' : ['tSh j e sh','쳿'],
    u'\uCD00' : ['tSh j e s','촀'],
    u'\uCD01' : ['tSh j e N','촁'],
    u'\uCD02' : ['tSh j e tS','촂'],
    u'\uCD03' : ['tSh j e tSh','촃'],
    u'\uCD04' : ['tSh j e kh','촄'],
    u'\uCD05' : ['tSh j e th','촅'],
    u'\uCD06' : ['tSh j e ph','촆'],
    u'\uCD07' : ['tSh j e h','촇'],
    u'\uCD08' : ['tSh o','초'],
    u'\uCD09' : ['tSh o k','촉'],
    u'\uCD0A' : ['tSh o k>','촊'],
    u'\uCD0B' : ['tSh o k sh','촋'],
    u'\uCD0C' : ['tSh o n','촌'],
    u'\uCD0D' : ['tSh o n tS','촍'],
    u'\uCD0E' : ['tSh o n h','촎'],
    u'\uCD0F' : ['tSh o t','촏'],
    u'\uCD10' : ['tSh o l','촐'],
    u'\uCD11' : ['tSh o l k','촑'],
    u'\uCD12' : ['tSh o l m','촒'],
    u'\uCD13' : ['tSh o l p','촓'],
    u'\uCD14' : ['tSh o l sh','촔'],
    u'\uCD15' : ['tSh o l th','촕'],
    u'\uCD16' : ['tSh o l ph','촖'],
    u'\uCD17' : ['tSh o l h','촗'],
    u'\uCD18' : ['tSh o m','촘'],
    u'\uCD19' : ['tSh o p','촙'],
    u'\uCD1A' : ['tSh o p sh','촚'],
    u'\uCD1B' : ['tSh o sh','촛'],
    u'\uCD1C' : ['tSh o s','촜'],
    u'\uCD1D' : ['tSh o N','총'],
    u'\uCD1E' : ['tSh o tS','촞'],
    u'\uCD1F' : ['tSh o tSh','촟'],
    u'\uCD20' : ['tSh o kh','촠'],
    u'\uCD21' : ['tSh o th','촡'],
    u'\uCD22' : ['tSh o ph','촢'],
    u'\uCD23' : ['tSh o h','촣'],
    u'\uCD24' : ['tSh w a','촤'],
    u'\uCD25' : ['tSh w a k','촥'],
    u'\uCD26' : ['tSh w a k>','촦'],
    u'\uCD27' : ['tSh w a k sh','촧'],
    u'\uCD28' : ['tSh w a n','촨'],
    u'\uCD29' : ['tSh w a n tS','촩'],
    u'\uCD2A' : ['tSh w a n h','촪'],
    u'\uCD2B' : ['tSh w a t','촫'],
    u'\uCD2C' : ['tSh w a l','촬'],
    u'\uCD2D' : ['tSh w a l k','촭'],
    u'\uCD2E' : ['tSh w a l m','촮'],
    u'\uCD2F' : ['tSh w a l p','촯'],
    u'\uCD30' : ['tSh w a l sh','촰'],
    u'\uCD31' : ['tSh w a l th','촱'],
    u'\uCD32' : ['tSh w a l ph','촲'],
    u'\uCD33' : ['tSh w a l h','촳'],
    u'\uCD34' : ['tSh w a m','촴'],
    u'\uCD35' : ['tSh w a p','촵'],
    u'\uCD36' : ['tSh w a p sh','촶'],
    u'\uCD37' : ['tSh w a sh','촷'],
    u'\uCD38' : ['tSh w a s','촸'],
    u'\uCD39' : ['tSh w a N','촹'],
    u'\uCD3A' : ['tSh w a tS','촺'],
    u'\uCD3B' : ['tSh w a tSh','촻'],
    u'\uCD3C' : ['tSh w a kh','촼'],
    u'\uCD3D' : ['tSh w a th','촽'],
    u'\uCD3E' : ['tSh w a ph','촾'],
    u'\uCD3F' : ['tSh w a h','촿'],
    u'\uCD40' : ['tSh w @','쵀'],
    u'\uCD41' : ['tSh w @ k','쵁'],
    u'\uCD42' : ['tSh w @ k>','쵂'],
    u'\uCD43' : ['tSh w @ k sh','쵃'],
    u'\uCD44' : ['tSh w @ n','쵄'],
    u'\uCD45' : ['tSh w @ n tS','쵅'],
    u'\uCD46' : ['tSh w @ n h','쵆'],
    u'\uCD47' : ['tSh w @ t','쵇'],
    u'\uCD48' : ['tSh w @ l','쵈'],
    u'\uCD49' : ['tSh w @ l k','쵉'],
    u'\uCD4A' : ['tSh w @ l m','쵊'],
    u'\uCD4B' : ['tSh w @ l p','쵋'],
    u'\uCD4C' : ['tSh w @ l sh','쵌'],
    u'\uCD4D' : ['tSh w @ l th','쵍'],
    u'\uCD4E' : ['tSh w @ l ph','쵎'],
    u'\uCD4F' : ['tSh w @ l h','쵏'],
    u'\uCD50' : ['tSh w @ m','쵐'],
    u'\uCD51' : ['tSh w @ p','쵑'],
    u'\uCD52' : ['tSh w @ p sh','쵒'],
    u'\uCD53' : ['tSh w @ sh','쵓'],
    u'\uCD54' : ['tSh w @ s','쵔'],
    u'\uCD55' : ['tSh w @ N','쵕'],
    u'\uCD56' : ['tSh w @ tS','쵖'],
    u'\uCD57' : ['tSh w @ tSh','쵗'],
    u'\uCD58' : ['tSh w @ kh','쵘'],
    u'\uCD59' : ['tSh w @ th','쵙'],
    u'\uCD5A' : ['tSh w @ ph','쵚'],
    u'\uCD5B' : ['tSh w @ h','쵛'],
    u'\uCD5C' : ['tSh w e','최'],
    u'\uCD5D' : ['tSh w e k','쵝'],
    u'\uCD5E' : ['tSh w e k>','쵞'],
    u'\uCD5F' : ['tSh w e k sh','쵟'],
    u'\uCD60' : ['tSh w e n','쵠'],
    u'\uCD61' : ['tSh w e n tS','쵡'],
    u'\uCD62' : ['tSh w e n h','쵢'],
    u'\uCD63' : ['tSh w e t','쵣'],
    u'\uCD64' : ['tSh w e l','쵤'],
    u'\uCD65' : ['tSh w e l k','쵥'],
    u'\uCD66' : ['tSh w e l m','쵦'],
    u'\uCD67' : ['tSh w e l p','쵧'],
    u'\uCD68' : ['tSh w e l sh','쵨'],
    u'\uCD69' : ['tSh w e l th','쵩'],
    u'\uCD6A' : ['tSh w e l ph','쵪'],
    u'\uCD6B' : ['tSh w e l h','쵫'],
    u'\uCD6C' : ['tSh w e m','쵬'],
    u'\uCD6D' : ['tSh w e p','쵭'],
    u'\uCD6E' : ['tSh w e p sh','쵮'],
    u'\uCD6F' : ['tSh w e sh','쵯'],
    u'\uCD70' : ['tSh w e s','쵰'],
    u'\uCD71' : ['tSh w e N','쵱'],
    u'\uCD72' : ['tSh w e tS','쵲'],
    u'\uCD73' : ['tSh w e tSh','쵳'],
    u'\uCD74' : ['tSh w e kh','쵴'],
    u'\uCD75' : ['tSh w e th','쵵'],
    u'\uCD76' : ['tSh w e ph','쵶'],
    u'\uCD77' : ['tSh w e h','쵷'],
    u'\uCD78' : ['tSh j o','쵸'],
    u'\uCD79' : ['tSh j o k','쵹'],
    u'\uCD7A' : ['tSh j o k>','쵺'],
    u'\uCD7B' : ['tSh j o k sh','쵻'],
    u'\uCD7C' : ['tSh j o n','쵼'],
    u'\uCD7D' : ['tSh j o n tS','쵽'],
    u'\uCD7E' : ['tSh j o n h','쵾'],
    u'\uCD7F' : ['tSh j o t','쵿'],
    u'\uCD80' : ['tSh j o l','춀'],
    u'\uCD81' : ['tSh j o l k','춁'],
    u'\uCD82' : ['tSh j o l m','춂'],
    u'\uCD83' : ['tSh j o l p','춃'],
    u'\uCD84' : ['tSh j o l sh','춄'],
    u'\uCD85' : ['tSh j o l th','춅'],
    u'\uCD86' : ['tSh j o l ph','춆'],
    u'\uCD87' : ['tSh j o l h','춇'],
    u'\uCD88' : ['tSh j o m','춈'],
    u'\uCD89' : ['tSh j o p','춉'],
    u'\uCD8A' : ['tSh j o p sh','춊'],
    u'\uCD8B' : ['tSh j o sh','춋'],
    u'\uCD8C' : ['tSh j o s','춌'],
    u'\uCD8D' : ['tSh j o N','춍'],
    u'\uCD8E' : ['tSh j o tS','춎'],
    u'\uCD8F' : ['tSh j o tSh','춏'],
    u'\uCD90' : ['tSh j o kh','춐'],
    u'\uCD91' : ['tSh j o th','춑'],
    u'\uCD92' : ['tSh j o ph','춒'],
    u'\uCD93' : ['tSh j o h','춓'],
    u'\uCD94' : ['tSh u','추'],
    u'\uCD95' : ['tSh u k','축'],
    u'\uCD96' : ['tSh u k>','춖'],
    u'\uCD97' : ['tSh u k sh','춗'],
    u'\uCD98' : ['tSh u n','춘'],
    u'\uCD99' : ['tSh u n tS','춙'],
    u'\uCD9A' : ['tSh u n h','춚'],
    u'\uCD9B' : ['tSh u t','춛'],
    u'\uCD9C' : ['tSh u l','출'],
    u'\uCD9D' : ['tSh u l k','춝'],
    u'\uCD9E' : ['tSh u l m','춞'],
    u'\uCD9F' : ['tSh u l p','춟'],
    u'\uCDA0' : ['tSh u l sh','춠'],
    u'\uCDA1' : ['tSh u l th','춡'],
    u'\uCDA2' : ['tSh u l ph','춢'],
    u'\uCDA3' : ['tSh u l h','춣'],
    u'\uCDA4' : ['tSh u m','춤'],
    u'\uCDA5' : ['tSh u p','춥'],
    u'\uCDA6' : ['tSh u p sh','춦'],
    u'\uCDA7' : ['tSh u sh','춧'],
    u'\uCDA8' : ['tSh u s','춨'],
    u'\uCDA9' : ['tSh u N','충'],
    u'\uCDAA' : ['tSh u tS','춪'],
    u'\uCDAB' : ['tSh u tSh','춫'],
    u'\uCDAC' : ['tSh u kh','춬'],
    u'\uCDAD' : ['tSh u th','춭'],
    u'\uCDAE' : ['tSh u ph','춮'],
    u'\uCDAF' : ['tSh u h','춯'],
    u'\uCDB0' : ['tSh w ^','춰'],
    u'\uCDB1' : ['tSh w ^ k','춱'],
    u'\uCDB2' : ['tSh w ^ k>','춲'],
    u'\uCDB3' : ['tSh w ^ k sh','춳'],
    u'\uCDB4' : ['tSh w ^ n','춴'],
    u'\uCDB5' : ['tSh w ^ n tS','춵'],
    u'\uCDB6' : ['tSh w ^ n h','춶'],
    u'\uCDB7' : ['tSh w ^ t','춷'],
    u'\uCDB8' : ['tSh w ^ l','춸'],
    u'\uCDB9' : ['tSh w ^ l k','춹'],
    u'\uCDBA' : ['tSh w ^ l m','춺'],
    u'\uCDBB' : ['tSh w ^ l p','춻'],
    u'\uCDBC' : ['tSh w ^ l sh','춼'],
    u'\uCDBD' : ['tSh w ^ l th','춽'],
    u'\uCDBE' : ['tSh w ^ l ph','춾'],
    u'\uCDBF' : ['tSh w ^ l h','춿'],
    u'\uCDC0' : ['tSh w ^ m','췀'],
    u'\uCDC1' : ['tSh w ^ p','췁'],
    u'\uCDC2' : ['tSh w ^ p sh','췂'],
    u'\uCDC3' : ['tSh w ^ sh','췃'],
    u'\uCDC4' : ['tSh w ^ s','췄'],
    u'\uCDC5' : ['tSh w ^ N','췅'],
    u'\uCDC6' : ['tSh w ^ tS','췆'],
    u'\uCDC7' : ['tSh w ^ tSh','췇'],
    u'\uCDC8' : ['tSh w ^ kh','췈'],
    u'\uCDC9' : ['tSh w ^ th','췉'],
    u'\uCDCA' : ['tSh w ^ ph','췊'],
    u'\uCDCB' : ['tSh w ^ h','췋'],
    u'\uCDCC' : ['tSh w E','췌'],
    u'\uCDCD' : ['tSh w E k','췍'],
    u'\uCDCE' : ['tSh w E k>','췎'],
    u'\uCDCF' : ['tSh w E k sh','췏'],
    u'\uCDD0' : ['tSh w E n','췐'],
    u'\uCDD1' : ['tSh w E n tS','췑'],
    u'\uCDD2' : ['tSh w E n h','췒'],
    u'\uCDD3' : ['tSh w E t','췓'],
    u'\uCDD4' : ['tSh w E l','췔'],
    u'\uCDD5' : ['tSh w E l k','췕'],
    u'\uCDD6' : ['tSh w E l m','췖'],
    u'\uCDD7' : ['tSh w E l p','췗'],
    u'\uCDD8' : ['tSh w E l sh','췘'],
    u'\uCDD9' : ['tSh w E l th','췙'],
    u'\uCDDA' : ['tSh w E l ph','췚'],
    u'\uCDDB' : ['tSh w E l h','췛'],
    u'\uCDDC' : ['tSh w E m','췜'],
    u'\uCDDD' : ['tSh w E p','췝'],
    u'\uCDDE' : ['tSh w E p sh','췞'],
    u'\uCDDF' : ['tSh w E sh','췟'],
    u'\uCDE0' : ['tSh w E s','췠'],
    u'\uCDE1' : ['tSh w E N','췡'],
    u'\uCDE2' : ['tSh w E tS','췢'],
    u'\uCDE3' : ['tSh w E tSh','췣'],
    u'\uCDE4' : ['tSh w E kh','췤'],
    u'\uCDE5' : ['tSh w E th','췥'],
    u'\uCDE6' : ['tSh w E ph','췦'],
    u'\uCDE7' : ['tSh w E h','췧'],
    u'\uCDE8' : ['tSh 7','취'],
    u'\uCDE9' : ['tSh 7 k','췩'],
    u'\uCDEA' : ['tSh 7 k>','췪'],
    u'\uCDEB' : ['tSh 7 k sh','췫'],
    u'\uCDEC' : ['tSh 7 n','췬'],
    u'\uCDED' : ['tSh 7 n tS','췭'],
    u'\uCDEE' : ['tSh 7 n h','췮'],
    u'\uCDEF' : ['tSh 7 t','췯'],
    u'\uCDF0' : ['tSh 7 l','췰'],
    u'\uCDF1' : ['tSh 7 l k','췱'],
    u'\uCDF2' : ['tSh 7 l m','췲'],
    u'\uCDF3' : ['tSh 7 l p','췳'],
    u'\uCDF4' : ['tSh 7 l sh','췴'],
    u'\uCDF5' : ['tSh 7 l th','췵'],
    u'\uCDF6' : ['tSh 7 l ph','췶'],
    u'\uCDF7' : ['tSh 7 l h','췷'],
    u'\uCDF8' : ['tSh 7 m','췸'],
    u'\uCDF9' : ['tSh 7 p','췹'],
    u'\uCDFA' : ['tSh 7 p sh','췺'],
    u'\uCDFB' : ['tSh 7 sh','췻'],
    u'\uCDFC' : ['tSh 7 s','췼'],
    u'\uCDFD' : ['tSh 7 N','췽'],
    u'\uCDFE' : ['tSh 7 tS','췾'],
    u'\uCDFF' : ['tSh 7 tSh','췿'],
    u'\uCE00' : ['tSh 7 kh','츀'],
    u'\uCE01' : ['tSh 7 th','츁'],
    u'\uCE02' : ['tSh 7 ph','츂'],
    u'\uCE03' : ['tSh 7 h','츃'],
    u'\uCE04' : ['tSh j u','츄'],
    u'\uCE05' : ['tSh j u k','츅'],
    u'\uCE06' : ['tSh j u k>','츆'],
    u'\uCE07' : ['tSh j u k sh','츇'],
    u'\uCE08' : ['tSh j u n','츈'],
    u'\uCE09' : ['tSh j u n tS','츉'],
    u'\uCE0A' : ['tSh j u n h','츊'],
    u'\uCE0B' : ['tSh j u t','츋'],
    u'\uCE0C' : ['tSh j u l','츌'],
    u'\uCE0D' : ['tSh j u l k','츍'],
    u'\uCE0E' : ['tSh j u l m','츎'],
    u'\uCE0F' : ['tSh j u l p','츏'],
    u'\uCE10' : ['tSh j u l sh','츐'],
    u'\uCE11' : ['tSh j u l th','츑'],
    u'\uCE12' : ['tSh j u l ph','츒'],
    u'\uCE13' : ['tSh j u l h','츓'],
    u'\uCE14' : ['tSh j u m','츔'],
    u'\uCE15' : ['tSh j u p','츕'],
    u'\uCE16' : ['tSh j u p sh','츖'],
    u'\uCE17' : ['tSh j u sh','츗'],
    u'\uCE18' : ['tSh j u s','츘'],
    u'\uCE19' : ['tSh j u N','츙'],
    u'\uCE1A' : ['tSh j u tS','츚'],
    u'\uCE1B' : ['tSh j u tSh','츛'],
    u'\uCE1C' : ['tSh j u kh','츜'],
    u'\uCE1D' : ['tSh j u th','츝'],
    u'\uCE1E' : ['tSh j u ph','츞'],
    u'\uCE1F' : ['tSh j u h','츟'],
    u'\uCE20' : ['tSh 4','츠'],
    u'\uCE21' : ['tSh 4 k','측'],
    u'\uCE22' : ['tSh 4 k>','츢'],
    u'\uCE23' : ['tSh 4 k sh','츣'],
    u'\uCE24' : ['tSh 4 n','츤'],
    u'\uCE25' : ['tSh 4 n tS','츥'],
    u'\uCE26' : ['tSh 4 n h','츦'],
    u'\uCE27' : ['tSh 4 t','츧'],
    u'\uCE28' : ['tSh 4 l','츨'],
    u'\uCE29' : ['tSh 4 l k','츩'],
    u'\uCE2A' : ['tSh 4 l m','츪'],
    u'\uCE2B' : ['tSh 4 l p','츫'],
    u'\uCE2C' : ['tSh 4 l sh','츬'],
    u'\uCE2D' : ['tSh 4 l th','츭'],
    u'\uCE2E' : ['tSh 4 l ph','츮'],
    u'\uCE2F' : ['tSh 4 l h','츯'],
    u'\uCE30' : ['tSh 4 m','츰'],
    u'\uCE31' : ['tSh 4 p','츱'],
    u'\uCE32' : ['tSh 4 p sh','츲'],
    u'\uCE33' : ['tSh 4 sh','츳'],
    u'\uCE34' : ['tSh 4 s','츴'],
    u'\uCE35' : ['tSh 4 N','층'],
    u'\uCE36' : ['tSh 4 tS','츶'],
    u'\uCE37' : ['tSh 4 tSh','츷'],
    u'\uCE38' : ['tSh 4 kh','츸'],
    u'\uCE39' : ['tSh 4 th','츹'],
    u'\uCE3A' : ['tSh 4 ph','츺'],
    u'\uCE3B' : ['tSh 4 h','츻'],
    u'\uCE3C' : ['tSh 4 j','츼'],
    u'\uCE3D' : ['tSh 4 j k','츽'],
    u'\uCE3E' : ['tSh 4 j k>','츾'],
    u'\uCE3F' : ['tSh 4 j k sh','츿'],
    u'\uCE40' : ['tSh 4 j n','칀'],
    u'\uCE41' : ['tSh 4 j n tS','칁'],
    u'\uCE42' : ['tSh 4 j n h','칂'],
    u'\uCE43' : ['tSh 4 j t','칃'],
    u'\uCE44' : ['tSh 4 j l','칄'],
    u'\uCE45' : ['tSh 4 j l k','칅'],
    u'\uCE46' : ['tSh 4 j l m','칆'],
    u'\uCE47' : ['tSh 4 j l p','칇'],
    u'\uCE48' : ['tSh 4 j l sh','칈'],
    u'\uCE49' : ['tSh 4 j l th','칉'],
    u'\uCE4A' : ['tSh 4 j l ph','칊'],
    u'\uCE4B' : ['tSh 4 j l h','칋'],
    u'\uCE4C' : ['tSh 4 j m','칌'],
    u'\uCE4D' : ['tSh 4 j p','칍'],
    u'\uCE4E' : ['tSh 4 j p sh','칎'],
    u'\uCE4F' : ['tSh 4 j sh','칏'],
    u'\uCE50' : ['tSh 4 j s','칐'],
    u'\uCE51' : ['tSh 4 j N','칑'],
    u'\uCE52' : ['tSh 4 j tS','칒'],
    u'\uCE53' : ['tSh 4 j tSh','칓'],
    u'\uCE54' : ['tSh 4 j kh','칔'],
    u'\uCE55' : ['tSh 4 j th','칕'],
    u'\uCE56' : ['tSh 4 j ph','칖'],
    u'\uCE57' : ['tSh 4 j h','칗'],
    u'\uCE58' : ['tSh i','치'],
    u'\uCE59' : ['tSh i k','칙'],
    u'\uCE5A' : ['tSh i k>','칚'],
    u'\uCE5B' : ['tSh i k sh','칛'],
    u'\uCE5C' : ['tSh i n','친'],
    u'\uCE5D' : ['tSh i n tS','칝'],
    u'\uCE5E' : ['tSh i n h','칞'],
    u'\uCE5F' : ['tSh i t','칟'],
    u'\uCE60' : ['tSh i l','칠'],
    u'\uCE61' : ['tSh i l k','칡'],
    u'\uCE62' : ['tSh i l m','칢'],
    u'\uCE63' : ['tSh i l p','칣'],
    u'\uCE64' : ['tSh i l sh','칤'],
    u'\uCE65' : ['tSh i l th','칥'],
    u'\uCE66' : ['tSh i l ph','칦'],
    u'\uCE67' : ['tSh i l h','칧'],
    u'\uCE68' : ['tSh i m','침'],
    u'\uCE69' : ['tSh i p','칩'],
    u'\uCE6A' : ['tSh i p sh','칪'],
    u'\uCE6B' : ['tSh i sh','칫'],
    u'\uCE6C' : ['tSh i s','칬'],
    u'\uCE6D' : ['tSh i N','칭'],
    u'\uCE6E' : ['tSh i tS','칮'],
    u'\uCE6F' : ['tSh i tSh','칯'],
    u'\uCE70' : ['tSh i kh','칰'],
    u'\uCE71' : ['tSh i th','칱'],
    u'\uCE72' : ['tSh i ph','칲'],
    u'\uCE73' : ['tSh i h','칳'],
    u'\uCE74' : ['kh a','카'],
    u'\uCE75' : ['kh a k','칵'],
    u'\uCE76' : ['kh a k>','칶'],
    u'\uCE77' : ['kh a k sh','칷'],
    u'\uCE78' : ['kh a n','칸'],
    u'\uCE79' : ['kh a n tS','칹'],
    u'\uCE7A' : ['kh a n h','칺'],
    u'\uCE7B' : ['kh a t','칻'],
    u'\uCE7C' : ['kh a l','칼'],
    u'\uCE7D' : ['kh a l k','칽'],
    u'\uCE7E' : ['kh a l m','칾'],
    u'\uCE7F' : ['kh a l p','칿'],
    u'\uCE80' : ['kh a l sh','캀'],
    u'\uCE81' : ['kh a l th','캁'],
    u'\uCE82' : ['kh a l ph','캂'],
    u'\uCE83' : ['kh a l h','캃'],
    u'\uCE84' : ['kh a m','캄'],
    u'\uCE85' : ['kh a p','캅'],
    u'\uCE86' : ['kh a p sh','캆'],
    u'\uCE87' : ['kh a sh','캇'],
    u'\uCE88' : ['kh a s','캈'],
    u'\uCE89' : ['kh a N','캉'],
    u'\uCE8A' : ['kh a tS','캊'],
    u'\uCE8B' : ['kh a tSh','캋'],
    u'\uCE8C' : ['kh a kh','캌'],
    u'\uCE8D' : ['kh a th','캍'],
    u'\uCE8E' : ['kh a ph','캎'],
    u'\uCE8F' : ['kh a h','캏'],
    u'\uCE90' : ['kh @','캐'],
    u'\uCE91' : ['kh @ k','캑'],
    u'\uCE92' : ['kh @ k>','캒'],
    u'\uCE93' : ['kh @ k sh','캓'],
    u'\uCE94' : ['kh @ n','캔'],
    u'\uCE95' : ['kh @ n tS','캕'],
    u'\uCE96' : ['kh @ n h','캖'],
    u'\uCE97' : ['kh @ t','캗'],
    u'\uCE98' : ['kh @ l','캘'],
    u'\uCE99' : ['kh @ l k','캙'],
    u'\uCE9A' : ['kh @ l m','캚'],
    u'\uCE9B' : ['kh @ l p','캛'],
    u'\uCE9C' : ['kh @ l sh','캜'],
    u'\uCE9D' : ['kh @ l th','캝'],
    u'\uCE9E' : ['kh @ l ph','캞'],
    u'\uCE9F' : ['kh @ l h','캟'],
    u'\uCEA0' : ['kh @ m','캠'],
    u'\uCEA1' : ['kh @ p','캡'],
    u'\uCEA2' : ['kh @ p sh','캢'],
    u'\uCEA3' : ['kh @ sh','캣'],
    u'\uCEA4' : ['kh @ s','캤'],
    u'\uCEA5' : ['kh @ N','캥'],
    u'\uCEA6' : ['kh @ tS','캦'],
    u'\uCEA7' : ['kh @ tSh','캧'],
    u'\uCEA8' : ['kh @ kh','캨'],
    u'\uCEA9' : ['kh @ th','캩'],
    u'\uCEAA' : ['kh @ ph','캪'],
    u'\uCEAB' : ['kh @ h','캫'],
    u'\uCEAC' : ['kh j a','캬'],
    u'\uCEAD' : ['kh j a k','캭'],
    u'\uCEAE' : ['kh j a k>','캮'],
    u'\uCEAF' : ['kh j a k sh','캯'],
    u'\uCEB0' : ['kh j a n','캰'],
    u'\uCEB1' : ['kh j a n tS','캱'],
    u'\uCEB2' : ['kh j a n h','캲'],
    u'\uCEB3' : ['kh j a t','캳'],
    u'\uCEB4' : ['kh j a l','캴'],
    u'\uCEB5' : ['kh j a l k','캵'],
    u'\uCEB6' : ['kh j a l m','캶'],
    u'\uCEB7' : ['kh j a l p','캷'],
    u'\uCEB8' : ['kh j a l sh','캸'],
    u'\uCEB9' : ['kh j a l th','캹'],
    u'\uCEBA' : ['kh j a l ph','캺'],
    u'\uCEBB' : ['kh j a l h','캻'],
    u'\uCEBC' : ['kh j a m','캼'],
    u'\uCEBD' : ['kh j a p','캽'],
    u'\uCEBE' : ['kh j a p sh','캾'],
    u'\uCEBF' : ['kh j a sh','캿'],
    u'\uCEC0' : ['kh j a s','컀'],
    u'\uCEC1' : ['kh j a N','컁'],
    u'\uCEC2' : ['kh j a tS','컂'],
    u'\uCEC3' : ['kh j a tSh','컃'],
    u'\uCEC4' : ['kh j a kh','컄'],
    u'\uCEC5' : ['kh j a th','컅'],
    u'\uCEC6' : ['kh j a ph','컆'],
    u'\uCEC7' : ['kh j a h','컇'],
    u'\uCEC8' : ['kh j @','컈'],
    u'\uCEC9' : ['kh j @ k','컉'],
    u'\uCECA' : ['kh j @ k>','컊'],
    u'\uCECB' : ['kh j @ k sh','컋'],
    u'\uCECC' : ['kh j @ n','컌'],
    u'\uCECD' : ['kh j @ n tS','컍'],
    u'\uCECE' : ['kh j @ n h','컎'],
    u'\uCECF' : ['kh j @ t','컏'],
    u'\uCED0' : ['kh j @ l','컐'],
    u'\uCED1' : ['kh j @ l k','컑'],
    u'\uCED2' : ['kh j @ l m','컒'],
    u'\uCED3' : ['kh j @ l p','컓'],
    u'\uCED4' : ['kh j @ l sh','컔'],
    u'\uCED5' : ['kh j @ l th','컕'],
    u'\uCED6' : ['kh j @ l ph','컖'],
    u'\uCED7' : ['kh j @ l h','컗'],
    u'\uCED8' : ['kh j @ m','컘'],
    u'\uCED9' : ['kh j @ p','컙'],
    u'\uCEDA' : ['kh j @ p sh','컚'],
    u'\uCEDB' : ['kh j @ sh','컛'],
    u'\uCEDC' : ['kh j @ s','컜'],
    u'\uCEDD' : ['kh j @ N','컝'],
    u'\uCEDE' : ['kh j @ tS','컞'],
    u'\uCEDF' : ['kh j @ tSh','컟'],
    u'\uCEE0' : ['kh j @ kh','컠'],
    u'\uCEE1' : ['kh j @ th','컡'],
    u'\uCEE2' : ['kh j @ ph','컢'],
    u'\uCEE3' : ['kh j @ h','컣'],
    u'\uCEE4' : ['kh ^','커'],
    u'\uCEE5' : ['kh ^ k','컥'],
    u'\uCEE6' : ['kh ^ k>','컦'],
    u'\uCEE7' : ['kh ^ k sh','컧'],
    u'\uCEE8' : ['kh ^ n','컨'],
    u'\uCEE9' : ['kh ^ n tS','컩'],
    u'\uCEEA' : ['kh ^ n h','컪'],
    u'\uCEEB' : ['kh ^ t','컫'],
    u'\uCEEC' : ['kh ^ l','컬'],
    u'\uCEED' : ['kh ^ l k','컭'],
    u'\uCEEE' : ['kh ^ l m','컮'],
    u'\uCEEF' : ['kh ^ l p','컯'],
    u'\uCEF0' : ['kh ^ l sh','컰'],
    u'\uCEF1' : ['kh ^ l th','컱'],
    u'\uCEF2' : ['kh ^ l ph','컲'],
    u'\uCEF3' : ['kh ^ l h','컳'],
    u'\uCEF4' : ['kh ^ m','컴'],
    u'\uCEF5' : ['kh ^ p','컵'],
    u'\uCEF6' : ['kh ^ p sh','컶'],
    u'\uCEF7' : ['kh ^ sh','컷'],
    u'\uCEF8' : ['kh ^ s','컸'],
    u'\uCEF9' : ['kh ^ N','컹'],
    u'\uCEFA' : ['kh ^ tS','컺'],
    u'\uCEFB' : ['kh ^ tSh','컻'],
    u'\uCEFC' : ['kh ^ kh','컼'],
    u'\uCEFD' : ['kh ^ th','컽'],
    u'\uCEFE' : ['kh ^ ph','컾'],
    u'\uCEFF' : ['kh ^ h','컿'],
    u'\uCF00' : ['kh e','케'],
    u'\uCF01' : ['kh e k','켁'],
    u'\uCF02' : ['kh e k>','켂'],
    u'\uCF03' : ['kh e k sh','켃'],
    u'\uCF04' : ['kh e n','켄'],
    u'\uCF05' : ['kh e n tS','켅'],
    u'\uCF06' : ['kh e n h','켆'],
    u'\uCF07' : ['kh e t','켇'],
    u'\uCF08' : ['kh e l','켈'],
    u'\uCF09' : ['kh e l k','켉'],
    u'\uCF0A' : ['kh e l m','켊'],
    u'\uCF0B' : ['kh e l p','켋'],
    u'\uCF0C' : ['kh e l sh','켌'],
    u'\uCF0D' : ['kh e l th','켍'],
    u'\uCF0E' : ['kh e l ph','켎'],
    u'\uCF0F' : ['kh e l h','켏'],
    u'\uCF10' : ['kh e m','켐'],
    u'\uCF11' : ['kh e p','켑'],
    u'\uCF12' : ['kh e p sh','켒'],
    u'\uCF13' : ['kh e sh','켓'],
    u'\uCF14' : ['kh e s','켔'],
    u'\uCF15' : ['kh e N','켕'],
    u'\uCF16' : ['kh e tS','켖'],
    u'\uCF17' : ['kh e tSh','켗'],
    u'\uCF18' : ['kh e kh','켘'],
    u'\uCF19' : ['kh e th','켙'],
    u'\uCF1A' : ['kh e ph','켚'],
    u'\uCF1B' : ['kh e h','켛'],
    u'\uCF1C' : ['kh j ^','켜'],
    u'\uCF1D' : ['kh j ^ k','켝'],
    u'\uCF1E' : ['kh j ^ k>','켞'],
    u'\uCF1F' : ['kh j ^ k sh','켟'],
    u'\uCF20' : ['kh j ^ n','켠'],
    u'\uCF21' : ['kh j ^ n tS','켡'],
    u'\uCF22' : ['kh j ^ n h','켢'],
    u'\uCF23' : ['kh j ^ t','켣'],
    u'\uCF24' : ['kh j ^ l','켤'],
    u'\uCF25' : ['kh j ^ l k','켥'],
    u'\uCF26' : ['kh j ^ l m','켦'],
    u'\uCF27' : ['kh j ^ l p','켧'],
    u'\uCF28' : ['kh j ^ l sh','켨'],
    u'\uCF29' : ['kh j ^ l th','켩'],
    u'\uCF2A' : ['kh j ^ l ph','켪'],
    u'\uCF2B' : ['kh j ^ l h','켫'],
    u'\uCF2C' : ['kh j ^ m','켬'],
    u'\uCF2D' : ['kh j ^ p','켭'],
    u'\uCF2E' : ['kh j ^ p sh','켮'],
    u'\uCF2F' : ['kh j ^ sh','켯'],
    u'\uCF30' : ['kh j ^ s','켰'],
    u'\uCF31' : ['kh j ^ N','켱'],
    u'\uCF32' : ['kh j ^ tS','켲'],
    u'\uCF33' : ['kh j ^ tSh','켳'],
    u'\uCF34' : ['kh j ^ kh','켴'],
    u'\uCF35' : ['kh j ^ th','켵'],
    u'\uCF36' : ['kh j ^ ph','켶'],
    u'\uCF37' : ['kh j ^ h','켷'],
    u'\uCF38' : ['kh j e','켸'],
    u'\uCF39' : ['kh j e k','켹'],
    u'\uCF3A' : ['kh j e k>','켺'],
    u'\uCF3B' : ['kh j e k sh','켻'],
    u'\uCF3C' : ['kh j e n','켼'],
    u'\uCF3D' : ['kh j e n tS','켽'],
    u'\uCF3E' : ['kh j e n h','켾'],
    u'\uCF3F' : ['kh j e t','켿'],
    u'\uCF40' : ['kh j e l','콀'],
    u'\uCF41' : ['kh j e l k','콁'],
    u'\uCF42' : ['kh j e l m','콂'],
    u'\uCF43' : ['kh j e l p','콃'],
    u'\uCF44' : ['kh j e l sh','콄'],
    u'\uCF45' : ['kh j e l th','콅'],
    u'\uCF46' : ['kh j e l ph','콆'],
    u'\uCF47' : ['kh j e l h','콇'],
    u'\uCF48' : ['kh j e m','콈'],
    u'\uCF49' : ['kh j e p','콉'],
    u'\uCF4A' : ['kh j e p sh','콊'],
    u'\uCF4B' : ['kh j e sh','콋'],
    u'\uCF4C' : ['kh j e s','콌'],
    u'\uCF4D' : ['kh j e N','콍'],
    u'\uCF4E' : ['kh j e tS','콎'],
    u'\uCF4F' : ['kh j e tSh','콏'],
    u'\uCF50' : ['kh j e kh','콐'],
    u'\uCF51' : ['kh j e th','콑'],
    u'\uCF52' : ['kh j e ph','콒'],
    u'\uCF53' : ['kh j e h','콓'],
    u'\uCF54' : ['kh o','코'],
    u'\uCF55' : ['kh o k','콕'],
    u'\uCF56' : ['kh o k>','콖'],
    u'\uCF57' : ['kh o k sh','콗'],
    u'\uCF58' : ['kh o n','콘'],
    u'\uCF59' : ['kh o n tS','콙'],
    u'\uCF5A' : ['kh o n h','콚'],
    u'\uCF5B' : ['kh o t','콛'],
    u'\uCF5C' : ['kh o l','콜'],
    u'\uCF5D' : ['kh o l k','콝'],
    u'\uCF5E' : ['kh o l m','콞'],
    u'\uCF5F' : ['kh o l p','콟'],
    u'\uCF60' : ['kh o l sh','콠'],
    u'\uCF61' : ['kh o l th','콡'],
    u'\uCF62' : ['kh o l ph','콢'],
    u'\uCF63' : ['kh o l h','콣'],
    u'\uCF64' : ['kh o m','콤'],
    u'\uCF65' : ['kh o p','콥'],
    u'\uCF66' : ['kh o p sh','콦'],
    u'\uCF67' : ['kh o sh','콧'],
    u'\uCF68' : ['kh o s','콨'],
    u'\uCF69' : ['kh o N','콩'],
    u'\uCF6A' : ['kh o tS','콪'],
    u'\uCF6B' : ['kh o tSh','콫'],
    u'\uCF6C' : ['kh o kh','콬'],
    u'\uCF6D' : ['kh o th','콭'],
    u'\uCF6E' : ['kh o ph','콮'],
    u'\uCF6F' : ['kh o h','콯'],
    u'\uCF70' : ['kh w a','콰'],
    u'\uCF71' : ['kh w a k','콱'],
    u'\uCF72' : ['kh w a k>','콲'],
    u'\uCF73' : ['kh w a k sh','콳'],
    u'\uCF74' : ['kh w a n','콴'],
    u'\uCF75' : ['kh w a n tS','콵'],
    u'\uCF76' : ['kh w a n h','콶'],
    u'\uCF77' : ['kh w a t','콷'],
    u'\uCF78' : ['kh w a l','콸'],
    u'\uCF79' : ['kh w a l k','콹'],
    u'\uCF7A' : ['kh w a l m','콺'],
    u'\uCF7B' : ['kh w a l p','콻'],
    u'\uCF7C' : ['kh w a l sh','콼'],
    u'\uCF7D' : ['kh w a l th','콽'],
    u'\uCF7E' : ['kh w a l ph','콾'],
    u'\uCF7F' : ['kh w a l h','콿'],
    u'\uCF80' : ['kh w a m','쾀'],
    u'\uCF81' : ['kh w a p','쾁'],
    u'\uCF82' : ['kh w a p sh','쾂'],
    u'\uCF83' : ['kh w a sh','쾃'],
    u'\uCF84' : ['kh w a s','쾄'],
    u'\uCF85' : ['kh w a N','쾅'],
    u'\uCF86' : ['kh w a tS','쾆'],
    u'\uCF87' : ['kh w a tSh','쾇'],
    u'\uCF88' : ['kh w a kh','쾈'],
    u'\uCF89' : ['kh w a th','쾉'],
    u'\uCF8A' : ['kh w a ph','쾊'],
    u'\uCF8B' : ['kh w a h','쾋'],
    u'\uCF8C' : ['kh w @','쾌'],
    u'\uCF8D' : ['kh w @ k','쾍'],
    u'\uCF8E' : ['kh w @ k>','쾎'],
    u'\uCF8F' : ['kh w @ k sh','쾏'],
    u'\uCF90' : ['kh w @ n','쾐'],
    u'\uCF91' : ['kh w @ n tS','쾑'],
    u'\uCF92' : ['kh w @ n h','쾒'],
    u'\uCF93' : ['kh w @ t','쾓'],
    u'\uCF94' : ['kh w @ l','쾔'],
    u'\uCF95' : ['kh w @ l k','쾕'],
    u'\uCF96' : ['kh w @ l m','쾖'],
    u'\uCF97' : ['kh w @ l p','쾗'],
    u'\uCF98' : ['kh w @ l sh','쾘'],
    u'\uCF99' : ['kh w @ l th','쾙'],
    u'\uCF9A' : ['kh w @ l ph','쾚'],
    u'\uCF9B' : ['kh w @ l h','쾛'],
    u'\uCF9C' : ['kh w @ m','쾜'],
    u'\uCF9D' : ['kh w @ p','쾝'],
    u'\uCF9E' : ['kh w @ p sh','쾞'],
    u'\uCF9F' : ['kh w @ sh','쾟'],
    u'\uCFA0' : ['kh w @ s','쾠'],
    u'\uCFA1' : ['kh w @ N','쾡'],
    u'\uCFA2' : ['kh w @ tS','쾢'],
    u'\uCFA3' : ['kh w @ tSh','쾣'],
    u'\uCFA4' : ['kh w @ kh','쾤'],
    u'\uCFA5' : ['kh w @ th','쾥'],
    u'\uCFA6' : ['kh w @ ph','쾦'],
    u'\uCFA7' : ['kh w @ h','쾧'],
    u'\uCFA8' : ['kh w e','쾨'],
    u'\uCFA9' : ['kh w e k','쾩'],
    u'\uCFAA' : ['kh w e k>','쾪'],
    u'\uCFAB' : ['kh w e k sh','쾫'],
    u'\uCFAC' : ['kh w e n','쾬'],
    u'\uCFAD' : ['kh w e n tS','쾭'],
    u'\uCFAE' : ['kh w e n h','쾮'],
    u'\uCFAF' : ['kh w e t','쾯'],
    u'\uCFB0' : ['kh w e l','쾰'],
    u'\uCFB1' : ['kh w e l k','쾱'],
    u'\uCFB2' : ['kh w e l m','쾲'],
    u'\uCFB3' : ['kh w e l p','쾳'],
    u'\uCFB4' : ['kh w e l sh','쾴'],
    u'\uCFB5' : ['kh w e l th','쾵'],
    u'\uCFB6' : ['kh w e l ph','쾶'],
    u'\uCFB7' : ['kh w e l h','쾷'],
    u'\uCFB8' : ['kh w e m','쾸'],
    u'\uCFB9' : ['kh w e p','쾹'],
    u'\uCFBA' : ['kh w e p sh','쾺'],
    u'\uCFBB' : ['kh w e sh','쾻'],
    u'\uCFBC' : ['kh w e s','쾼'],
    u'\uCFBD' : ['kh w e N','쾽'],
    u'\uCFBE' : ['kh w e tS','쾾'],
    u'\uCFBF' : ['kh w e tSh','쾿'],
    u'\uCFC0' : ['kh w e kh','쿀'],
    u'\uCFC1' : ['kh w e th','쿁'],
    u'\uCFC2' : ['kh w e ph','쿂'],
    u'\uCFC3' : ['kh w e h','쿃'],
    u'\uCFC4' : ['kh j o','쿄'],
    u'\uCFC5' : ['kh j o k','쿅'],
    u'\uCFC6' : ['kh j o k>','쿆'],
    u'\uCFC7' : ['kh j o k sh','쿇'],
    u'\uCFC8' : ['kh j o n','쿈'],
    u'\uCFC9' : ['kh j o n tS','쿉'],
    u'\uCFCA' : ['kh j o n h','쿊'],
    u'\uCFCB' : ['kh j o t','쿋'],
    u'\uCFCC' : ['kh j o l','쿌'],
    u'\uCFCD' : ['kh j o l k','쿍'],
    u'\uCFCE' : ['kh j o l m','쿎'],
    u'\uCFCF' : ['kh j o l p','쿏'],
    u'\uCFD0' : ['kh j o l sh','쿐'],
    u'\uCFD1' : ['kh j o l th','쿑'],
    u'\uCFD2' : ['kh j o l ph','쿒'],
    u'\uCFD3' : ['kh j o l h','쿓'],
    u'\uCFD4' : ['kh j o m','쿔'],
    u'\uCFD5' : ['kh j o p','쿕'],
    u'\uCFD6' : ['kh j o p sh','쿖'],
    u'\uCFD7' : ['kh j o sh','쿗'],
    u'\uCFD8' : ['kh j o s','쿘'],
    u'\uCFD9' : ['kh j o N','쿙'],
    u'\uCFDA' : ['kh j o tS','쿚'],
    u'\uCFDB' : ['kh j o tSh','쿛'],
    u'\uCFDC' : ['kh j o kh','쿜'],
    u'\uCFDD' : ['kh j o th','쿝'],
    u'\uCFDE' : ['kh j o ph','쿞'],
    u'\uCFDF' : ['kh j o h','쿟'],
    u'\uCFE0' : ['kh u','쿠'],
    u'\uCFE1' : ['kh u k','쿡'],
    u'\uCFE2' : ['kh u k>','쿢'],
    u'\uCFE3' : ['kh u k sh','쿣'],
    u'\uCFE4' : ['kh u n','쿤'],
    u'\uCFE5' : ['kh u n tS','쿥'],
    u'\uCFE6' : ['kh u n h','쿦'],
    u'\uCFE7' : ['kh u t','쿧'],
    u'\uCFE8' : ['kh u l','쿨'],
    u'\uCFE9' : ['kh u l k','쿩'],
    u'\uCFEA' : ['kh u l m','쿪'],
    u'\uCFEB' : ['kh u l p','쿫'],
    u'\uCFEC' : ['kh u l sh','쿬'],
    u'\uCFED' : ['kh u l th','쿭'],
    u'\uCFEE' : ['kh u l ph','쿮'],
    u'\uCFEF' : ['kh u l h','쿯'],
    u'\uCFF0' : ['kh u m','쿰'],
    u'\uCFF1' : ['kh u p','쿱'],
    u'\uCFF2' : ['kh u p sh','쿲'],
    u'\uCFF3' : ['kh u sh','쿳'],
    u'\uCFF4' : ['kh u s','쿴'],
    u'\uCFF5' : ['kh u N','쿵'],
    u'\uCFF6' : ['kh u tS','쿶'],
    u'\uCFF7' : ['kh u tSh','쿷'],
    u'\uCFF8' : ['kh u kh','쿸'],
    u'\uCFF9' : ['kh u th','쿹'],
    u'\uCFFA' : ['kh u ph','쿺'],
    u'\uCFFB' : ['kh u h','쿻'],
    u'\uCFFC' : ['kh w ^','쿼'],
    u'\uCFFD' : ['kh w ^ k','쿽'],
    u'\uCFFE' : ['kh w ^ k>','쿾'],
    u'\uCFFF' : ['kh w ^ k sh','쿿'],
    u'\uD000' : ['kh w ^ n','퀀'],
    u'\uD001' : ['kh w ^ n tS','퀁'],
    u'\uD002' : ['kh w ^ n h','퀂'],
    u'\uD003' : ['kh w ^ t','퀃'],
    u'\uD004' : ['kh w ^ l','퀄'],
    u'\uD005' : ['kh w ^ l k','퀅'],
    u'\uD006' : ['kh w ^ l m','퀆'],
    u'\uD007' : ['kh w ^ l p','퀇'],
    u'\uD008' : ['kh w ^ l sh','퀈'],
    u'\uD009' : ['kh w ^ l th','퀉'],
    u'\uD00A' : ['kh w ^ l ph','퀊'],
    u'\uD00B' : ['kh w ^ l h','퀋'],
    u'\uD00C' : ['kh w ^ m','퀌'],
    u'\uD00D' : ['kh w ^ p','퀍'],
    u'\uD00E' : ['kh w ^ p sh','퀎'],
    u'\uD00F' : ['kh w ^ sh','퀏'],
    u'\uD010' : ['kh w ^ s','퀐'],
    u'\uD011' : ['kh w ^ N','퀑'],
    u'\uD012' : ['kh w ^ tS','퀒'],
    u'\uD013' : ['kh w ^ tSh','퀓'],
    u'\uD014' : ['kh w ^ kh','퀔'],
    u'\uD015' : ['kh w ^ th','퀕'],
    u'\uD016' : ['kh w ^ ph','퀖'],
    u'\uD017' : ['kh w ^ h','퀗'],
    u'\uD018' : ['kh w E','퀘'],
    u'\uD019' : ['kh w E k','퀙'],
    u'\uD01A' : ['kh w E k>','퀚'],
    u'\uD01B' : ['kh w E k sh','퀛'],
    u'\uD01C' : ['kh w E n','퀜'],
    u'\uD01D' : ['kh w E n tS','퀝'],
    u'\uD01E' : ['kh w E n h','퀞'],
    u'\uD01F' : ['kh w E t','퀟'],
    u'\uD020' : ['kh w E l','퀠'],
    u'\uD021' : ['kh w E l k','퀡'],
    u'\uD022' : ['kh w E l m','퀢'],
    u'\uD023' : ['kh w E l p','퀣'],
    u'\uD024' : ['kh w E l sh','퀤'],
    u'\uD025' : ['kh w E l th','퀥'],
    u'\uD026' : ['kh w E l ph','퀦'],
    u'\uD027' : ['kh w E l h','퀧'],
    u'\uD028' : ['kh w E m','퀨'],
    u'\uD029' : ['kh w E p','퀩'],
    u'\uD02A' : ['kh w E p sh','퀪'],
    u'\uD02B' : ['kh w E sh','퀫'],
    u'\uD02C' : ['kh w E s','퀬'],
    u'\uD02D' : ['kh w E N','퀭'],
    u'\uD02E' : ['kh w E tS','퀮'],
    u'\uD02F' : ['kh w E tSh','퀯'],
    u'\uD030' : ['kh w E kh','퀰'],
    u'\uD031' : ['kh w E th','퀱'],
    u'\uD032' : ['kh w E ph','퀲'],
    u'\uD033' : ['kh w E h','퀳'],
    u'\uD034' : ['kh 7','퀴'],
    u'\uD035' : ['kh 7 k','퀵'],
    u'\uD036' : ['kh 7 k>','퀶'],
    u'\uD037' : ['kh 7 k sh','퀷'],
    u'\uD038' : ['kh 7 n','퀸'],
    u'\uD039' : ['kh 7 n tS','퀹'],
    u'\uD03A' : ['kh 7 n h','퀺'],
    u'\uD03B' : ['kh 7 t','퀻'],
    u'\uD03C' : ['kh 7 l','퀼'],
    u'\uD03D' : ['kh 7 l k','퀽'],
    u'\uD03E' : ['kh 7 l m','퀾'],
    u'\uD03F' : ['kh 7 l p','퀿'],
    u'\uD040' : ['kh 7 l sh','큀'],
    u'\uD041' : ['kh 7 l th','큁'],
    u'\uD042' : ['kh 7 l ph','큂'],
    u'\uD043' : ['kh 7 l h','큃'],
    u'\uD044' : ['kh 7 m','큄'],
    u'\uD045' : ['kh 7 p','큅'],
    u'\uD046' : ['kh 7 p sh','큆'],
    u'\uD047' : ['kh 7 sh','큇'],
    u'\uD048' : ['kh 7 s','큈'],
    u'\uD049' : ['kh 7 N','큉'],
    u'\uD04A' : ['kh 7 tS','큊'],
    u'\uD04B' : ['kh 7 tSh','큋'],
    u'\uD04C' : ['kh 7 kh','큌'],
    u'\uD04D' : ['kh 7 th','큍'],
    u'\uD04E' : ['kh 7 ph','큎'],
    u'\uD04F' : ['kh 7 h','큏'],
    u'\uD050' : ['kh j u','큐'],
    u'\uD051' : ['kh j u k','큑'],
    u'\uD052' : ['kh j u k>','큒'],
    u'\uD053' : ['kh j u k sh','큓'],
    u'\uD054' : ['kh j u n','큔'],
    u'\uD055' : ['kh j u n tS','큕'],
    u'\uD056' : ['kh j u n h','큖'],
    u'\uD057' : ['kh j u t','큗'],
    u'\uD058' : ['kh j u l','큘'],
    u'\uD059' : ['kh j u l k','큙'],
    u'\uD05A' : ['kh j u l m','큚'],
    u'\uD05B' : ['kh j u l p','큛'],
    u'\uD05C' : ['kh j u l sh','큜'],
    u'\uD05D' : ['kh j u l th','큝'],
    u'\uD05E' : ['kh j u l ph','큞'],
    u'\uD05F' : ['kh j u l h','큟'],
    u'\uD060' : ['kh j u m','큠'],
    u'\uD061' : ['kh j u p','큡'],
    u'\uD062' : ['kh j u p sh','큢'],
    u'\uD063' : ['kh j u sh','큣'],
    u'\uD064' : ['kh j u s','큤'],
    u'\uD065' : ['kh j u N','큥'],
    u'\uD066' : ['kh j u tS','큦'],
    u'\uD067' : ['kh j u tSh','큧'],
    u'\uD068' : ['kh j u kh','큨'],
    u'\uD069' : ['kh j u th','큩'],
    u'\uD06A' : ['kh j u ph','큪'],
    u'\uD06B' : ['kh j u h','큫'],
    u'\uD06C' : ['kh 4','크'],
    u'\uD06D' : ['kh 4 k','큭'],
    u'\uD06E' : ['kh 4 k>','큮'],
    u'\uD06F' : ['kh 4 k sh','큯'],
    u'\uD070' : ['kh 4 n','큰'],
    u'\uD071' : ['kh 4 n tS','큱'],
    u'\uD072' : ['kh 4 n h','큲'],
    u'\uD073' : ['kh 4 t','큳'],
    u'\uD074' : ['kh 4 l','클'],
    u'\uD075' : ['kh 4 l k','큵'],
    u'\uD076' : ['kh 4 l m','큶'],
    u'\uD077' : ['kh 4 l p','큷'],
    u'\uD078' : ['kh 4 l sh','큸'],
    u'\uD079' : ['kh 4 l th','큹'],
    u'\uD07A' : ['kh 4 l ph','큺'],
    u'\uD07B' : ['kh 4 l h','큻'],
    u'\uD07C' : ['kh 4 m','큼'],
    u'\uD07D' : ['kh 4 p','큽'],
    u'\uD07E' : ['kh 4 p sh','큾'],
    u'\uD07F' : ['kh 4 sh','큿'],
    u'\uD080' : ['kh 4 s','킀'],
    u'\uD081' : ['kh 4 N','킁'],
    u'\uD082' : ['kh 4 tS','킂'],
    u'\uD083' : ['kh 4 tSh','킃'],
    u'\uD084' : ['kh 4 kh','킄'],
    u'\uD085' : ['kh 4 th','킅'],
    u'\uD086' : ['kh 4 ph','킆'],
    u'\uD087' : ['kh 4 h','킇'],
    u'\uD088' : ['kh 4 j','킈'],
    u'\uD089' : ['kh 4 j k','킉'],
    u'\uD08A' : ['kh 4 j k>','킊'],
    u'\uD08B' : ['kh 4 j k sh','킋'],
    u'\uD08C' : ['kh 4 j n','킌'],
    u'\uD08D' : ['kh 4 j n tS','킍'],
    u'\uD08E' : ['kh 4 j n h','킎'],
    u'\uD08F' : ['kh 4 j t','킏'],
    u'\uD090' : ['kh 4 j l','킐'],
    u'\uD091' : ['kh 4 j l k','킑'],
    u'\uD092' : ['kh 4 j l m','킒'],
    u'\uD093' : ['kh 4 j l p','킓'],
    u'\uD094' : ['kh 4 j l sh','킔'],
    u'\uD095' : ['kh 4 j l th','킕'],
    u'\uD096' : ['kh 4 j l ph','킖'],
    u'\uD097' : ['kh 4 j l h','킗'],
    u'\uD098' : ['kh 4 j m','킘'],
    u'\uD099' : ['kh 4 j p','킙'],
    u'\uD09A' : ['kh 4 j p sh','킚'],
    u'\uD09B' : ['kh 4 j sh','킛'],
    u'\uD09C' : ['kh 4 j s','킜'],
    u'\uD09D' : ['kh 4 j N','킝'],
    u'\uD09E' : ['kh 4 j tS','킞'],
    u'\uD09F' : ['kh 4 j tSh','킟'],
    u'\uD0A0' : ['kh 4 j kh','킠'],
    u'\uD0A1' : ['kh 4 j th','킡'],
    u'\uD0A2' : ['kh 4 j ph','킢'],
    u'\uD0A3' : ['kh 4 j h','킣'],
    u'\uD0A4' : ['kh i','키'],
    u'\uD0A5' : ['kh i k','킥'],
    u'\uD0A6' : ['kh i k>','킦'],
    u'\uD0A7' : ['kh i k sh','킧'],
    u'\uD0A8' : ['kh i n','킨'],
    u'\uD0A9' : ['kh i n tS','킩'],
    u'\uD0AA' : ['kh i n h','킪'],
    u'\uD0AB' : ['kh i t','킫'],
    u'\uD0AC' : ['kh i l','킬'],
    u'\uD0AD' : ['kh i l k','킭'],
    u'\uD0AE' : ['kh i l m','킮'],
    u'\uD0AF' : ['kh i l p','킯'],
    u'\uD0B0' : ['kh i l sh','킰'],
    u'\uD0B1' : ['kh i l th','킱'],
    u'\uD0B2' : ['kh i l ph','킲'],
    u'\uD0B3' : ['kh i l h','킳'],
    u'\uD0B4' : ['kh i m','킴'],
    u'\uD0B5' : ['kh i p','킵'],
    u'\uD0B6' : ['kh i p sh','킶'],
    u'\uD0B7' : ['kh i sh','킷'],
    u'\uD0B8' : ['kh i s','킸'],
    u'\uD0B9' : ['kh i N','킹'],
    u'\uD0BA' : ['kh i tS','킺'],
    u'\uD0BB' : ['kh i tSh','킻'],
    u'\uD0BC' : ['kh i kh','킼'],
    u'\uD0BD' : ['kh i th','킽'],
    u'\uD0BE' : ['kh i ph','킾'],
    u'\uD0BF' : ['kh i h','킿'],
    u'\uD0C0' : ['th a','타'],
    u'\uD0C1' : ['th a k','탁'],
    u'\uD0C2' : ['th a k>','탂'],
    u'\uD0C3' : ['th a k sh','탃'],
    u'\uD0C4' : ['th a n','탄'],
    u'\uD0C5' : ['th a n tS','탅'],
    u'\uD0C6' : ['th a n h','탆'],
    u'\uD0C7' : ['th a t','탇'],
    u'\uD0C8' : ['th a l','탈'],
    u'\uD0C9' : ['th a l k','탉'],
    u'\uD0CA' : ['th a l m','탊'],
    u'\uD0CB' : ['th a l p','탋'],
    u'\uD0CC' : ['th a l sh','탌'],
    u'\uD0CD' : ['th a l th','탍'],
    u'\uD0CE' : ['th a l ph','탎'],
    u'\uD0CF' : ['th a l h','탏'],
    u'\uD0D0' : ['th a m','탐'],
    u'\uD0D1' : ['th a p','탑'],
    u'\uD0D2' : ['th a p sh','탒'],
    u'\uD0D3' : ['th a sh','탓'],
    u'\uD0D4' : ['th a s','탔'],
    u'\uD0D5' : ['th a N','탕'],
    u'\uD0D6' : ['th a tS','탖'],
    u'\uD0D7' : ['th a tSh','탗'],
    u'\uD0D8' : ['th a kh','탘'],
    u'\uD0D9' : ['th a th','탙'],
    u'\uD0DA' : ['th a ph','탚'],
    u'\uD0DB' : ['th a h','탛'],
    u'\uD0DC' : ['th @','태'],
    u'\uD0DD' : ['th @ k','택'],
    u'\uD0DE' : ['th @ k>','탞'],
    u'\uD0DF' : ['th @ k sh','탟'],
    u'\uD0E0' : ['th @ n','탠'],
    u'\uD0E1' : ['th @ n tS','탡'],
    u'\uD0E2' : ['th @ n h','탢'],
    u'\uD0E3' : ['th @ t','탣'],
    u'\uD0E4' : ['th @ l','탤'],
    u'\uD0E5' : ['th @ l k','탥'],
    u'\uD0E6' : ['th @ l m','탦'],
    u'\uD0E7' : ['th @ l p','탧'],
    u'\uD0E8' : ['th @ l sh','탨'],
    u'\uD0E9' : ['th @ l th','탩'],
    u'\uD0EA' : ['th @ l ph','탪'],
    u'\uD0EB' : ['th @ l h','탫'],
    u'\uD0EC' : ['th @ m','탬'],
    u'\uD0ED' : ['th @ p','탭'],
    u'\uD0EE' : ['th @ p sh','탮'],
    u'\uD0EF' : ['th @ sh','탯'],
    u'\uD0F0' : ['th @ s','탰'],
    u'\uD0F1' : ['th @ N','탱'],
    u'\uD0F2' : ['th @ tS','탲'],
    u'\uD0F3' : ['th @ tSh','탳'],
    u'\uD0F4' : ['th @ kh','탴'],
    u'\uD0F5' : ['th @ th','탵'],
    u'\uD0F6' : ['th @ ph','탶'],
    u'\uD0F7' : ['th @ h','탷'],
    u'\uD0F8' : ['th j a','탸'],
    u'\uD0F9' : ['th j a k','탹'],
    u'\uD0FA' : ['th j a k>','탺'],
    u'\uD0FB' : ['th j a k sh','탻'],
    u'\uD0FC' : ['th j a n','탼'],
    u'\uD0FD' : ['th j a n tS','탽'],
    u'\uD0FE' : ['th j a n h','탾'],
    u'\uD0FF' : ['th j a t','탿'],
    u'\uD100' : ['th j a l','턀'],
    u'\uD101' : ['th j a l k','턁'],
    u'\uD102' : ['th j a l m','턂'],
    u'\uD103' : ['th j a l p','턃'],
    u'\uD104' : ['th j a l sh','턄'],
    u'\uD105' : ['th j a l th','턅'],
    u'\uD106' : ['th j a l ph','턆'],
    u'\uD107' : ['th j a l h','턇'],
    u'\uD108' : ['th j a m','턈'],
    u'\uD109' : ['th j a p','턉'],
    u'\uD10A' : ['th j a p sh','턊'],
    u'\uD10B' : ['th j a sh','턋'],
    u'\uD10C' : ['th j a s','턌'],
    u'\uD10D' : ['th j a N','턍'],
    u'\uD10E' : ['th j a tS','턎'],
    u'\uD10F' : ['th j a tSh','턏'],
    u'\uD110' : ['th j a kh','턐'],
    u'\uD111' : ['th j a th','턑'],
    u'\uD112' : ['th j a ph','턒'],
    u'\uD113' : ['th j a h','턓'],
    u'\uD114' : ['th j @','턔'],
    u'\uD115' : ['th j @ k','턕'],
    u'\uD116' : ['th j @ k>','턖'],
    u'\uD117' : ['th j @ k sh','턗'],
    u'\uD118' : ['th j @ n','턘'],
    u'\uD119' : ['th j @ n tS','턙'],
    u'\uD11A' : ['th j @ n h','턚'],
    u'\uD11B' : ['th j @ t','턛'],
    u'\uD11C' : ['th j @ l','턜'],
    u'\uD11D' : ['th j @ l k','턝'],
    u'\uD11E' : ['th j @ l m','턞'],
    u'\uD11F' : ['th j @ l p','턟'],
    u'\uD120' : ['th j @ l sh','턠'],
    u'\uD121' : ['th j @ l th','턡'],
    u'\uD122' : ['th j @ l ph','턢'],
    u'\uD123' : ['th j @ l h','턣'],
    u'\uD124' : ['th j @ m','턤'],
    u'\uD125' : ['th j @ p','턥'],
    u'\uD126' : ['th j @ p sh','턦'],
    u'\uD127' : ['th j @ sh','턧'],
    u'\uD128' : ['th j @ s','턨'],
    u'\uD129' : ['th j @ N','턩'],
    u'\uD12A' : ['th j @ tS','턪'],
    u'\uD12B' : ['th j @ tSh','턫'],
    u'\uD12C' : ['th j @ kh','턬'],
    u'\uD12D' : ['th j @ th','턭'],
    u'\uD12E' : ['th j @ ph','턮'],
    u'\uD12F' : ['th j @ h','턯'],
    u'\uD130' : ['th ^','터'],
    u'\uD131' : ['th ^ k','턱'],
    u'\uD132' : ['th ^ k>','턲'],
    u'\uD133' : ['th ^ k sh','턳'],
    u'\uD134' : ['th ^ n','턴'],
    u'\uD135' : ['th ^ n tS','턵'],
    u'\uD136' : ['th ^ n h','턶'],
    u'\uD137' : ['th ^ t','턷'],
    u'\uD138' : ['th ^ l','털'],
    u'\uD139' : ['th ^ l k','턹'],
    u'\uD13A' : ['th ^ l m','턺'],
    u'\uD13B' : ['th ^ l p','턻'],
    u'\uD13C' : ['th ^ l sh','턼'],
    u'\uD13D' : ['th ^ l th','턽'],
    u'\uD13E' : ['th ^ l ph','턾'],
    u'\uD13F' : ['th ^ l h','턿'],
    u'\uD140' : ['th ^ m','텀'],
    u'\uD141' : ['th ^ p','텁'],
    u'\uD142' : ['th ^ p sh','텂'],
    u'\uD143' : ['th ^ sh','텃'],
    u'\uD144' : ['th ^ s','텄'],
    u'\uD145' : ['th ^ N','텅'],
    u'\uD146' : ['th ^ tS','텆'],
    u'\uD147' : ['th ^ tSh','텇'],
    u'\uD148' : ['th ^ kh','텈'],
    u'\uD149' : ['th ^ th','텉'],
    u'\uD14A' : ['th ^ ph','텊'],
    u'\uD14B' : ['th ^ h','텋'],
    u'\uD14C' : ['th e','테'],
    u'\uD14D' : ['th e k','텍'],
    u'\uD14E' : ['th e k>','텎'],
    u'\uD14F' : ['th e k sh','텏'],
    u'\uD150' : ['th e n','텐'],
    u'\uD151' : ['th e n tS','텑'],
    u'\uD152' : ['th e n h','텒'],
    u'\uD153' : ['th e t','텓'],
    u'\uD154' : ['th e l','텔'],
    u'\uD155' : ['th e l k','텕'],
    u'\uD156' : ['th e l m','텖'],
    u'\uD157' : ['th e l p','텗'],
    u'\uD158' : ['th e l sh','텘'],
    u'\uD159' : ['th e l th','텙'],
    u'\uD15A' : ['th e l ph','텚'],
    u'\uD15B' : ['th e l h','텛'],
    u'\uD15C' : ['th e m','템'],
    u'\uD15D' : ['th e p','텝'],
    u'\uD15E' : ['th e p sh','텞'],
    u'\uD15F' : ['th e sh','텟'],
    u'\uD160' : ['th e s','텠'],
    u'\uD161' : ['th e N','텡'],
    u'\uD162' : ['th e tS','텢'],
    u'\uD163' : ['th e tSh','텣'],
    u'\uD164' : ['th e kh','텤'],
    u'\uD165' : ['th e th','텥'],
    u'\uD166' : ['th e ph','텦'],
    u'\uD167' : ['th e h','텧'],
    u'\uD168' : ['th j ^','텨'],
    u'\uD169' : ['th j ^ k','텩'],
    u'\uD16A' : ['th j ^ k>','텪'],
    u'\uD16B' : ['th j ^ k sh','텫'],
    u'\uD16C' : ['th j ^ n','텬'],
    u'\uD16D' : ['th j ^ n tS','텭'],
    u'\uD16E' : ['th j ^ n h','텮'],
    u'\uD16F' : ['th j ^ t','텯'],
    u'\uD170' : ['th j ^ l','텰'],
    u'\uD171' : ['th j ^ l k','텱'],
    u'\uD172' : ['th j ^ l m','텲'],
    u'\uD173' : ['th j ^ l p','텳'],
    u'\uD174' : ['th j ^ l sh','텴'],
    u'\uD175' : ['th j ^ l th','텵'],
    u'\uD176' : ['th j ^ l ph','텶'],
    u'\uD177' : ['th j ^ l h','텷'],
    u'\uD178' : ['th j ^ m','텸'],
    u'\uD179' : ['th j ^ p','텹'],
    u'\uD17A' : ['th j ^ p sh','텺'],
    u'\uD17B' : ['th j ^ sh','텻'],
    u'\uD17C' : ['th j ^ s','텼'],
    u'\uD17D' : ['th j ^ N','텽'],
    u'\uD17E' : ['th j ^ tS','텾'],
    u'\uD17F' : ['th j ^ tSh','텿'],
    u'\uD180' : ['th j ^ kh','톀'],
    u'\uD181' : ['th j ^ th','톁'],
    u'\uD182' : ['th j ^ ph','톂'],
    u'\uD183' : ['th j ^ h','톃'],
    u'\uD184' : ['th j e','톄'],
    u'\uD185' : ['th j e k','톅'],
    u'\uD186' : ['th j e k>','톆'],
    u'\uD187' : ['th j e k sh','톇'],
    u'\uD188' : ['th j e n','톈'],
    u'\uD189' : ['th j e n tS','톉'],
    u'\uD18A' : ['th j e n h','톊'],
    u'\uD18B' : ['th j e t','톋'],
    u'\uD18C' : ['th j e l','톌'],
    u'\uD18D' : ['th j e l k','톍'],
    u'\uD18E' : ['th j e l m','톎'],
    u'\uD18F' : ['th j e l p','톏'],
    u'\uD190' : ['th j e l sh','톐'],
    u'\uD191' : ['th j e l th','톑'],
    u'\uD192' : ['th j e l ph','톒'],
    u'\uD193' : ['th j e l h','톓'],
    u'\uD194' : ['th j e m','톔'],
    u'\uD195' : ['th j e p','톕'],
    u'\uD196' : ['th j e p sh','톖'],
    u'\uD197' : ['th j e sh','톗'],
    u'\uD198' : ['th j e s','톘'],
    u'\uD199' : ['th j e N','톙'],
    u'\uD19A' : ['th j e tS','톚'],
    u'\uD19B' : ['th j e tSh','톛'],
    u'\uD19C' : ['th j e kh','톜'],
    u'\uD19D' : ['th j e th','톝'],
    u'\uD19E' : ['th j e ph','톞'],
    u'\uD19F' : ['th j e h','톟'],
    u'\uD1A0' : ['th o','토'],
    u'\uD1A1' : ['th o k','톡'],
    u'\uD1A2' : ['th o k>','톢'],
    u'\uD1A3' : ['th o k sh','톣'],
    u'\uD1A4' : ['th o n','톤'],
    u'\uD1A5' : ['th o n tS','톥'],
    u'\uD1A6' : ['th o n h','톦'],
    u'\uD1A7' : ['th o t','톧'],
    u'\uD1A8' : ['th o l','톨'],
    u'\uD1A9' : ['th o l k','톩'],
    u'\uD1AA' : ['th o l m','톪'],
    u'\uD1AB' : ['th o l p','톫'],
    u'\uD1AC' : ['th o l sh','톬'],
    u'\uD1AD' : ['th o l th','톭'],
    u'\uD1AE' : ['th o l ph','톮'],
    u'\uD1AF' : ['th o l h','톯'],
    u'\uD1B0' : ['th o m','톰'],
    u'\uD1B1' : ['th o p','톱'],
    u'\uD1B2' : ['th o p sh','톲'],
    u'\uD1B3' : ['th o sh','톳'],
    u'\uD1B4' : ['th o s','톴'],
    u'\uD1B5' : ['th o N','통'],
    u'\uD1B6' : ['th o tS','톶'],
    u'\uD1B7' : ['th o tSh','톷'],
    u'\uD1B8' : ['th o kh','톸'],
    u'\uD1B9' : ['th o th','톹'],
    u'\uD1BA' : ['th o ph','톺'],
    u'\uD1BB' : ['th o h','톻'],
    u'\uD1BC' : ['th w a','톼'],
    u'\uD1BD' : ['th w a k','톽'],
    u'\uD1BE' : ['th w a k>','톾'],
    u'\uD1BF' : ['th w a k sh','톿'],
    u'\uD1C0' : ['th w a n','퇀'],
    u'\uD1C1' : ['th w a n tS','퇁'],
    u'\uD1C2' : ['th w a n h','퇂'],
    u'\uD1C3' : ['th w a t','퇃'],
    u'\uD1C4' : ['th w a l','퇄'],
    u'\uD1C5' : ['th w a l k','퇅'],
    u'\uD1C6' : ['th w a l m','퇆'],
    u'\uD1C7' : ['th w a l p','퇇'],
    u'\uD1C8' : ['th w a l sh','퇈'],
    u'\uD1C9' : ['th w a l th','퇉'],
    u'\uD1CA' : ['th w a l ph','퇊'],
    u'\uD1CB' : ['th w a l h','퇋'],
    u'\uD1CC' : ['th w a m','퇌'],
    u'\uD1CD' : ['th w a p','퇍'],
    u'\uD1CE' : ['th w a p sh','퇎'],
    u'\uD1CF' : ['th w a sh','퇏'],
    u'\uD1D0' : ['th w a s','퇐'],
    u'\uD1D1' : ['th w a N','퇑'],
    u'\uD1D2' : ['th w a tS','퇒'],
    u'\uD1D3' : ['th w a tSh','퇓'],
    u'\uD1D4' : ['th w a kh','퇔'],
    u'\uD1D5' : ['th w a th','퇕'],
    u'\uD1D6' : ['th w a ph','퇖'],
    u'\uD1D7' : ['th w a h','퇗'],
    u'\uD1D8' : ['th w @','퇘'],
    u'\uD1D9' : ['th w @ k','퇙'],
    u'\uD1DA' : ['th w @ k>','퇚'],
    u'\uD1DB' : ['th w @ k sh','퇛'],
    u'\uD1DC' : ['th w @ n','퇜'],
    u'\uD1DD' : ['th w @ n tS','퇝'],
    u'\uD1DE' : ['th w @ n h','퇞'],
    u'\uD1DF' : ['th w @ t','퇟'],
    u'\uD1E0' : ['th w @ l','퇠'],
    u'\uD1E1' : ['th w @ l k','퇡'],
    u'\uD1E2' : ['th w @ l m','퇢'],
    u'\uD1E3' : ['th w @ l p','퇣'],
    u'\uD1E4' : ['th w @ l sh','퇤'],
    u'\uD1E5' : ['th w @ l th','퇥'],
    u'\uD1E6' : ['th w @ l ph','퇦'],
    u'\uD1E7' : ['th w @ l h','퇧'],
    u'\uD1E8' : ['th w @ m','퇨'],
    u'\uD1E9' : ['th w @ p','퇩'],
    u'\uD1EA' : ['th w @ p sh','퇪'],
    u'\uD1EB' : ['th w @ sh','퇫'],
    u'\uD1EC' : ['th w @ s','퇬'],
    u'\uD1ED' : ['th w @ N','퇭'],
    u'\uD1EE' : ['th w @ tS','퇮'],
    u'\uD1EF' : ['th w @ tSh','퇯'],
    u'\uD1F0' : ['th w @ kh','퇰'],
    u'\uD1F1' : ['th w @ th','퇱'],
    u'\uD1F2' : ['th w @ ph','퇲'],
    u'\uD1F3' : ['th w @ h','퇳'],
    u'\uD1F4' : ['th w e','퇴'],
    u'\uD1F5' : ['th w e k','퇵'],
    u'\uD1F6' : ['th w e k>','퇶'],
    u'\uD1F7' : ['th w e k sh','퇷'],
    u'\uD1F8' : ['th w e n','퇸'],
    u'\uD1F9' : ['th w e n tS','퇹'],
    u'\uD1FA' : ['th w e n h','퇺'],
    u'\uD1FB' : ['th w e t','퇻'],
    u'\uD1FC' : ['th w e l','퇼'],
    u'\uD1FD' : ['th w e l k','퇽'],
    u'\uD1FE' : ['th w e l m','퇾'],
    u'\uD1FF' : ['th w e l p','퇿'],
    u'\uD200' : ['th w e l sh','툀'],
    u'\uD201' : ['th w e l th','툁'],
    u'\uD202' : ['th w e l ph','툂'],
    u'\uD203' : ['th w e l h','툃'],
    u'\uD204' : ['th w e m','툄'],
    u'\uD205' : ['th w e p','툅'],
    u'\uD206' : ['th w e p sh','툆'],
    u'\uD207' : ['th w e sh','툇'],
    u'\uD208' : ['th w e s','툈'],
    u'\uD209' : ['th w e N','툉'],
    u'\uD20A' : ['th w e tS','툊'],
    u'\uD20B' : ['th w e tSh','툋'],
    u'\uD20C' : ['th w e kh','툌'],
    u'\uD20D' : ['th w e th','툍'],
    u'\uD20E' : ['th w e ph','툎'],
    u'\uD20F' : ['th w e h','툏'],
    u'\uD210' : ['th j o','툐'],
    u'\uD211' : ['th j o k','툑'],
    u'\uD212' : ['th j o k>','툒'],
    u'\uD213' : ['th j o k sh','툓'],
    u'\uD214' : ['th j o n','툔'],
    u'\uD215' : ['th j o n tS','툕'],
    u'\uD216' : ['th j o n h','툖'],
    u'\uD217' : ['th j o t','툗'],
    u'\uD218' : ['th j o l','툘'],
    u'\uD219' : ['th j o l k','툙'],
    u'\uD21A' : ['th j o l m','툚'],
    u'\uD21B' : ['th j o l p','툛'],
    u'\uD21C' : ['th j o l sh','툜'],
    u'\uD21D' : ['th j o l th','툝'],
    u'\uD21E' : ['th j o l ph','툞'],
    u'\uD21F' : ['th j o l h','툟'],
    u'\uD220' : ['th j o m','툠'],
    u'\uD221' : ['th j o p','툡'],
    u'\uD222' : ['th j o p sh','툢'],
    u'\uD223' : ['th j o sh','툣'],
    u'\uD224' : ['th j o s','툤'],
    u'\uD225' : ['th j o N','툥'],
    u'\uD226' : ['th j o tS','툦'],
    u'\uD227' : ['th j o tSh','툧'],
    u'\uD228' : ['th j o kh','툨'],
    u'\uD229' : ['th j o th','툩'],
    u'\uD22A' : ['th j o ph','툪'],
    u'\uD22B' : ['th j o h','툫'],
    u'\uD22C' : ['th u','투'],
    u'\uD22D' : ['th u k','툭'],
    u'\uD22E' : ['th u k>','툮'],
    u'\uD22F' : ['th u k sh','툯'],
    u'\uD230' : ['th u n','툰'],
    u'\uD231' : ['th u n tS','툱'],
    u'\uD232' : ['th u n h','툲'],
    u'\uD233' : ['th u t','툳'],
    u'\uD234' : ['th u l','툴'],
    u'\uD235' : ['th u l k','툵'],
    u'\uD236' : ['th u l m','툶'],
    u'\uD237' : ['th u l p','툷'],
    u'\uD238' : ['th u l sh','툸'],
    u'\uD239' : ['th u l th','툹'],
    u'\uD23A' : ['th u l ph','툺'],
    u'\uD23B' : ['th u l h','툻'],
    u'\uD23C' : ['th u m','툼'],
    u'\uD23D' : ['th u p','툽'],
    u'\uD23E' : ['th u p sh','툾'],
    u'\uD23F' : ['th u sh','툿'],
    u'\uD240' : ['th u s','퉀'],
    u'\uD241' : ['th u N','퉁'],
    u'\uD242' : ['th u tS','퉂'],
    u'\uD243' : ['th u tSh','퉃'],
    u'\uD244' : ['th u kh','퉄'],
    u'\uD245' : ['th u th','퉅'],
    u'\uD246' : ['th u ph','퉆'],
    u'\uD247' : ['th u h','퉇'],
    u'\uD248' : ['th w ^','퉈'],
    u'\uD249' : ['th w ^ k','퉉'],
    u'\uD24A' : ['th w ^ k>','퉊'],
    u'\uD24B' : ['th w ^ k sh','퉋'],
    u'\uD24C' : ['th w ^ n','퉌'],
    u'\uD24D' : ['th w ^ n tS','퉍'],
    u'\uD24E' : ['th w ^ n h','퉎'],
    u'\uD24F' : ['th w ^ t','퉏'],
    u'\uD250' : ['th w ^ l','퉐'],
    u'\uD251' : ['th w ^ l k','퉑'],
    u'\uD252' : ['th w ^ l m','퉒'],
    u'\uD253' : ['th w ^ l p','퉓'],
    u'\uD254' : ['th w ^ l sh','퉔'],
    u'\uD255' : ['th w ^ l th','퉕'],
    u'\uD256' : ['th w ^ l ph','퉖'],
    u'\uD257' : ['th w ^ l h','퉗'],
    u'\uD258' : ['th w ^ m','퉘'],
    u'\uD259' : ['th w ^ p','퉙'],
    u'\uD25A' : ['th w ^ p sh','퉚'],
    u'\uD25B' : ['th w ^ sh','퉛'],
    u'\uD25C' : ['th w ^ s','퉜'],
    u'\uD25D' : ['th w ^ N','퉝'],
    u'\uD25E' : ['th w ^ tS','퉞'],
    u'\uD25F' : ['th w ^ tSh','퉟'],
    u'\uD260' : ['th w ^ kh','퉠'],
    u'\uD261' : ['th w ^ th','퉡'],
    u'\uD262' : ['th w ^ ph','퉢'],
    u'\uD263' : ['th w ^ h','퉣'],
    u'\uD264' : ['th w E','퉤'],
    u'\uD265' : ['th w E k','퉥'],
    u'\uD266' : ['th w E k>','퉦'],
    u'\uD267' : ['th w E k sh','퉧'],
    u'\uD268' : ['th w E n','퉨'],
    u'\uD269' : ['th w E n tS','퉩'],
    u'\uD26A' : ['th w E n h','퉪'],
    u'\uD26B' : ['th w E t','퉫'],
    u'\uD26C' : ['th w E l','퉬'],
    u'\uD26D' : ['th w E l k','퉭'],
    u'\uD26E' : ['th w E l m','퉮'],
    u'\uD26F' : ['th w E l p','퉯'],
    u'\uD270' : ['th w E l sh','퉰'],
    u'\uD271' : ['th w E l th','퉱'],
    u'\uD272' : ['th w E l ph','퉲'],
    u'\uD273' : ['th w E l h','퉳'],
    u'\uD274' : ['th w E m','퉴'],
    u'\uD275' : ['th w E p','퉵'],
    u'\uD276' : ['th w E p sh','퉶'],
    u'\uD277' : ['th w E sh','퉷'],
    u'\uD278' : ['th w E s','퉸'],
    u'\uD279' : ['th w E N','퉹'],
    u'\uD27A' : ['th w E tS','퉺'],
    u'\uD27B' : ['th w E tSh','퉻'],
    u'\uD27C' : ['th w E kh','퉼'],
    u'\uD27D' : ['th w E th','퉽'],
    u'\uD27E' : ['th w E ph','퉾'],
    u'\uD27F' : ['th w E h','퉿'],
    u'\uD280' : ['th 7','튀'],
    u'\uD281' : ['th 7 k','튁'],
    u'\uD282' : ['th 7 k>','튂'],
    u'\uD283' : ['th 7 k sh','튃'],
    u'\uD284' : ['th 7 n','튄'],
    u'\uD285' : ['th 7 n tS','튅'],
    u'\uD286' : ['th 7 n h','튆'],
    u'\uD287' : ['th 7 t','튇'],
    u'\uD288' : ['th 7 l','튈'],
    u'\uD289' : ['th 7 l k','튉'],
    u'\uD28A' : ['th 7 l m','튊'],
    u'\uD28B' : ['th 7 l p','튋'],
    u'\uD28C' : ['th 7 l sh','튌'],
    u'\uD28D' : ['th 7 l th','튍'],
    u'\uD28E' : ['th 7 l ph','튎'],
    u'\uD28F' : ['th 7 l h','튏'],
    u'\uD290' : ['th 7 m','튐'],
    u'\uD291' : ['th 7 p','튑'],
    u'\uD292' : ['th 7 p sh','튒'],
    u'\uD293' : ['th 7 sh','튓'],
    u'\uD294' : ['th 7 s','튔'],
    u'\uD295' : ['th 7 N','튕'],
    u'\uD296' : ['th 7 tS','튖'],
    u'\uD297' : ['th 7 tSh','튗'],
    u'\uD298' : ['th 7 kh','튘'],
    u'\uD299' : ['th 7 th','튙'],
    u'\uD29A' : ['th 7 ph','튚'],
    u'\uD29B' : ['th 7 h','튛'],
    u'\uD29C' : ['th j u','튜'],
    u'\uD29D' : ['th j u k','튝'],
    u'\uD29E' : ['th j u k>','튞'],
    u'\uD29F' : ['th j u k sh','튟'],
    u'\uD2A0' : ['th j u n','튠'],
    u'\uD2A1' : ['th j u n tS','튡'],
    u'\uD2A2' : ['th j u n h','튢'],
    u'\uD2A3' : ['th j u t','튣'],
    u'\uD2A4' : ['th j u l','튤'],
    u'\uD2A5' : ['th j u l k','튥'],
    u'\uD2A6' : ['th j u l m','튦'],
    u'\uD2A7' : ['th j u l p','튧'],
    u'\uD2A8' : ['th j u l sh','튨'],
    u'\uD2A9' : ['th j u l th','튩'],
    u'\uD2AA' : ['th j u l ph','튪'],
    u'\uD2AB' : ['th j u l h','튫'],
    u'\uD2AC' : ['th j u m','튬'],
    u'\uD2AD' : ['th j u p','튭'],
    u'\uD2AE' : ['th j u p sh','튮'],
    u'\uD2AF' : ['th j u sh','튯'],
    u'\uD2B0' : ['th j u s','튰'],
    u'\uD2B1' : ['th j u N','튱'],
    u'\uD2B2' : ['th j u tS','튲'],
    u'\uD2B3' : ['th j u tSh','튳'],
    u'\uD2B4' : ['th j u kh','튴'],
    u'\uD2B5' : ['th j u th','튵'],
    u'\uD2B6' : ['th j u ph','튶'],
    u'\uD2B7' : ['th j u h','튷'],
    u'\uD2B8' : ['th 4','트'],
    u'\uD2B9' : ['th 4 k','특'],
    u'\uD2BA' : ['th 4 k>','튺'],
    u'\uD2BB' : ['th 4 k sh','튻'],
    u'\uD2BC' : ['th 4 n','튼'],
    u'\uD2BD' : ['th 4 n tS','튽'],
    u'\uD2BE' : ['th 4 n h','튾'],
    u'\uD2BF' : ['th 4 t','튿'],
    u'\uD2C0' : ['th 4 l','틀'],
    u'\uD2C1' : ['th 4 l k','틁'],
    u'\uD2C2' : ['th 4 l m','틂'],
    u'\uD2C3' : ['th 4 l p','틃'],
    u'\uD2C4' : ['th 4 l sh','틄'],
    u'\uD2C5' : ['th 4 l th','틅'],
    u'\uD2C6' : ['th 4 l ph','틆'],
    u'\uD2C7' : ['th 4 l h','틇'],
    u'\uD2C8' : ['th 4 m','틈'],
    u'\uD2C9' : ['th 4 p','틉'],
    u'\uD2CA' : ['th 4 p sh','틊'],
    u'\uD2CB' : ['th 4 sh','틋'],
    u'\uD2CC' : ['th 4 s','틌'],
    u'\uD2CD' : ['th 4 N','틍'],
    u'\uD2CE' : ['th 4 tS','틎'],
    u'\uD2CF' : ['th 4 tSh','틏'],
    u'\uD2D0' : ['th 4 kh','틐'],
    u'\uD2D1' : ['th 4 th','틑'],
    u'\uD2D2' : ['th 4 ph','틒'],
    u'\uD2D3' : ['th 4 h','틓'],
    u'\uD2D4' : ['th 4 j','틔'],
    u'\uD2D5' : ['th 4 j k','틕'],
    u'\uD2D6' : ['th 4 j k>','틖'],
    u'\uD2D7' : ['th 4 j k sh','틗'],
    u'\uD2D8' : ['th 4 j n','틘'],
    u'\uD2D9' : ['th 4 j n tS','틙'],
    u'\uD2DA' : ['th 4 j n h','틚'],
    u'\uD2DB' : ['th 4 j t','틛'],
    u'\uD2DC' : ['th 4 j l','틜'],
    u'\uD2DD' : ['th 4 j l k','틝'],
    u'\uD2DE' : ['th 4 j l m','틞'],
    u'\uD2DF' : ['th 4 j l p','틟'],
    u'\uD2E0' : ['th 4 j l sh','틠'],
    u'\uD2E1' : ['th 4 j l th','틡'],
    u'\uD2E2' : ['th 4 j l ph','틢'],
    u'\uD2E3' : ['th 4 j l h','틣'],
    u'\uD2E4' : ['th 4 j m','틤'],
    u'\uD2E5' : ['th 4 j p','틥'],
    u'\uD2E6' : ['th 4 j p sh','틦'],
    u'\uD2E7' : ['th 4 j sh','틧'],
    u'\uD2E8' : ['th 4 j s','틨'],
    u'\uD2E9' : ['th 4 j N','틩'],
    u'\uD2EA' : ['th 4 j tS','틪'],
    u'\uD2EB' : ['th 4 j tSh','틫'],
    u'\uD2EC' : ['th 4 j kh','틬'],
    u'\uD2ED' : ['th 4 j th','틭'],
    u'\uD2EE' : ['th 4 j ph','틮'],
    u'\uD2EF' : ['th 4 j h','틯'],
    u'\uD2F0' : ['th i','티'],
    u'\uD2F1' : ['th i k','틱'],
    u'\uD2F2' : ['th i k>','틲'],
    u'\uD2F3' : ['th i k sh','틳'],
    u'\uD2F4' : ['th i n','틴'],
    u'\uD2F5' : ['th i n tS','틵'],
    u'\uD2F6' : ['th i n h','틶'],
    u'\uD2F7' : ['th i t','틷'],
    u'\uD2F8' : ['th i l','틸'],
    u'\uD2F9' : ['th i l k','틹'],
    u'\uD2FA' : ['th i l m','틺'],
    u'\uD2FB' : ['th i l p','틻'],
    u'\uD2FC' : ['th i l sh','틼'],
    u'\uD2FD' : ['th i l th','틽'],
    u'\uD2FE' : ['th i l ph','틾'],
    u'\uD2FF' : ['th i l h','틿'],
    u'\uD300' : ['th i m','팀'],
    u'\uD301' : ['th i p','팁'],
    u'\uD302' : ['th i p sh','팂'],
    u'\uD303' : ['th i sh','팃'],
    u'\uD304' : ['th i s','팄'],
    u'\uD305' : ['th i N','팅'],
    u'\uD306' : ['th i tS','팆'],
    u'\uD307' : ['th i tSh','팇'],
    u'\uD308' : ['th i kh','팈'],
    u'\uD309' : ['th i th','팉'],
    u'\uD30A' : ['th i ph','팊'],
    u'\uD30B' : ['th i h','팋'],
    u'\uD30C' : ['ph a','파'],
    u'\uD30D' : ['ph a k','팍'],
    u'\uD30E' : ['ph a k>','팎'],
    u'\uD30F' : ['ph a k sh','팏'],
    u'\uD310' : ['ph a n','판'],
    u'\uD311' : ['ph a n tS','팑'],
    u'\uD312' : ['ph a n h','팒'],
    u'\uD313' : ['ph a t','팓'],
    u'\uD314' : ['ph a l','팔'],
    u'\uD315' : ['ph a l k','팕'],
    u'\uD316' : ['ph a l m','팖'],
    u'\uD317' : ['ph a l p','팗'],
    u'\uD318' : ['ph a l sh','팘'],
    u'\uD319' : ['ph a l th','팙'],
    u'\uD31A' : ['ph a l ph','팚'],
    u'\uD31B' : ['ph a l h','팛'],
    u'\uD31C' : ['ph a m','팜'],
    u'\uD31D' : ['ph a p','팝'],
    u'\uD31E' : ['ph a p sh','팞'],
    u'\uD31F' : ['ph a sh','팟'],
    u'\uD320' : ['ph a s','팠'],
    u'\uD321' : ['ph a N','팡'],
    u'\uD322' : ['ph a tS','팢'],
    u'\uD323' : ['ph a tSh','팣'],
    u'\uD324' : ['ph a kh','팤'],
    u'\uD325' : ['ph a th','팥'],
    u'\uD326' : ['ph a ph','팦'],
    u'\uD327' : ['ph a h','팧'],
    u'\uD328' : ['ph @','패'],
    u'\uD329' : ['ph @ k','팩'],
    u'\uD32A' : ['ph @ k>','팪'],
    u'\uD32B' : ['ph @ k sh','팫'],
    u'\uD32C' : ['ph @ n','팬'],
    u'\uD32D' : ['ph @ n tS','팭'],
    u'\uD32E' : ['ph @ n h','팮'],
    u'\uD32F' : ['ph @ t','팯'],
    u'\uD330' : ['ph @ l','팰'],
    u'\uD331' : ['ph @ l k','팱'],
    u'\uD332' : ['ph @ l m','팲'],
    u'\uD333' : ['ph @ l p','팳'],
    u'\uD334' : ['ph @ l sh','팴'],
    u'\uD335' : ['ph @ l th','팵'],
    u'\uD336' : ['ph @ l ph','팶'],
    u'\uD337' : ['ph @ l h','팷'],
    u'\uD338' : ['ph @ m','팸'],
    u'\uD339' : ['ph @ p','팹'],
    u'\uD33A' : ['ph @ p sh','팺'],
    u'\uD33B' : ['ph @ sh','팻'],
    u'\uD33C' : ['ph @ s','팼'],
    u'\uD33D' : ['ph @ N','팽'],
    u'\uD33E' : ['ph @ tS','팾'],
    u'\uD33F' : ['ph @ tSh','팿'],
    u'\uD340' : ['ph @ kh','퍀'],
    u'\uD341' : ['ph @ th','퍁'],
    u'\uD342' : ['ph @ ph','퍂'],
    u'\uD343' : ['ph @ h','퍃'],
    u'\uD344' : ['ph j a','퍄'],
    u'\uD345' : ['ph j a k','퍅'],
    u'\uD346' : ['ph j a k>','퍆'],
    u'\uD347' : ['ph j a k sh','퍇'],
    u'\uD348' : ['ph j a n','퍈'],
    u'\uD349' : ['ph j a n tS','퍉'],
    u'\uD34A' : ['ph j a n h','퍊'],
    u'\uD34B' : ['ph j a t','퍋'],
    u'\uD34C' : ['ph j a l','퍌'],
    u'\uD34D' : ['ph j a l k','퍍'],
    u'\uD34E' : ['ph j a l m','퍎'],
    u'\uD34F' : ['ph j a l p','퍏'],
    u'\uD350' : ['ph j a l sh','퍐'],
    u'\uD351' : ['ph j a l th','퍑'],
    u'\uD352' : ['ph j a l ph','퍒'],
    u'\uD353' : ['ph j a l h','퍓'],
    u'\uD354' : ['ph j a m','퍔'],
    u'\uD355' : ['ph j a p','퍕'],
    u'\uD356' : ['ph j a p sh','퍖'],
    u'\uD357' : ['ph j a sh','퍗'],
    u'\uD358' : ['ph j a s','퍘'],
    u'\uD359' : ['ph j a N','퍙'],
    u'\uD35A' : ['ph j a tS','퍚'],
    u'\uD35B' : ['ph j a tSh','퍛'],
    u'\uD35C' : ['ph j a kh','퍜'],
    u'\uD35D' : ['ph j a th','퍝'],
    u'\uD35E' : ['ph j a ph','퍞'],
    u'\uD35F' : ['ph j a h','퍟'],
    u'\uD360' : ['ph j @','퍠'],
    u'\uD361' : ['ph j @ k','퍡'],
    u'\uD362' : ['ph j @ k>','퍢'],
    u'\uD363' : ['ph j @ k sh','퍣'],
    u'\uD364' : ['ph j @ n','퍤'],
    u'\uD365' : ['ph j @ n tS','퍥'],
    u'\uD366' : ['ph j @ n h','퍦'],
    u'\uD367' : ['ph j @ t','퍧'],
    u'\uD368' : ['ph j @ l','퍨'],
    u'\uD369' : ['ph j @ l k','퍩'],
    u'\uD36A' : ['ph j @ l m','퍪'],
    u'\uD36B' : ['ph j @ l p','퍫'],
    u'\uD36C' : ['ph j @ l sh','퍬'],
    u'\uD36D' : ['ph j @ l th','퍭'],
    u'\uD36E' : ['ph j @ l ph','퍮'],
    u'\uD36F' : ['ph j @ l h','퍯'],
    u'\uD370' : ['ph j @ m','퍰'],
    u'\uD371' : ['ph j @ p','퍱'],
    u'\uD372' : ['ph j @ p sh','퍲'],
    u'\uD373' : ['ph j @ sh','퍳'],
    u'\uD374' : ['ph j @ s','퍴'],
    u'\uD375' : ['ph j @ N','퍵'],
    u'\uD376' : ['ph j @ tS','퍶'],
    u'\uD377' : ['ph j @ tSh','퍷'],
    u'\uD378' : ['ph j @ kh','퍸'],
    u'\uD379' : ['ph j @ th','퍹'],
    u'\uD37A' : ['ph j @ ph','퍺'],
    u'\uD37B' : ['ph j @ h','퍻'],
    u'\uD37C' : ['ph ^','퍼'],
    u'\uD37D' : ['ph ^ k','퍽'],
    u'\uD37E' : ['ph ^ k>','퍾'],
    u'\uD37F' : ['ph ^ k sh','퍿'],
    u'\uD380' : ['ph ^ n','펀'],
    u'\uD381' : ['ph ^ n tS','펁'],
    u'\uD382' : ['ph ^ n h','펂'],
    u'\uD383' : ['ph ^ t','펃'],
    u'\uD384' : ['ph ^ l','펄'],
    u'\uD385' : ['ph ^ l k','펅'],
    u'\uD386' : ['ph ^ l m','펆'],
    u'\uD387' : ['ph ^ l p','펇'],
    u'\uD388' : ['ph ^ l sh','펈'],
    u'\uD389' : ['ph ^ l th','펉'],
    u'\uD38A' : ['ph ^ l ph','펊'],
    u'\uD38B' : ['ph ^ l h','펋'],
    u'\uD38C' : ['ph ^ m','펌'],
    u'\uD38D' : ['ph ^ p','펍'],
    u'\uD38E' : ['ph ^ p sh','펎'],
    u'\uD38F' : ['ph ^ sh','펏'],
    u'\uD390' : ['ph ^ s','펐'],
    u'\uD391' : ['ph ^ N','펑'],
    u'\uD392' : ['ph ^ tS','펒'],
    u'\uD393' : ['ph ^ tSh','펓'],
    u'\uD394' : ['ph ^ kh','펔'],
    u'\uD395' : ['ph ^ th','펕'],
    u'\uD396' : ['ph ^ ph','펖'],
    u'\uD397' : ['ph ^ h','펗'],
    u'\uD398' : ['ph e','페'],
    u'\uD399' : ['ph e k','펙'],
    u'\uD39A' : ['ph e k>','펚'],
    u'\uD39B' : ['ph e k sh','펛'],
    u'\uD39C' : ['ph e n','펜'],
    u'\uD39D' : ['ph e n tS','펝'],
    u'\uD39E' : ['ph e n h','펞'],
    u'\uD39F' : ['ph e t','펟'],
    u'\uD3A0' : ['ph e l','펠'],
    u'\uD3A1' : ['ph e l k','펡'],
    u'\uD3A2' : ['ph e l m','펢'],
    u'\uD3A3' : ['ph e l p','펣'],
    u'\uD3A4' : ['ph e l sh','펤'],
    u'\uD3A5' : ['ph e l th','펥'],
    u'\uD3A6' : ['ph e l ph','펦'],
    u'\uD3A7' : ['ph e l h','펧'],
    u'\uD3A8' : ['ph e m','펨'],
    u'\uD3A9' : ['ph e p','펩'],
    u'\uD3AA' : ['ph e p sh','펪'],
    u'\uD3AB' : ['ph e sh','펫'],
    u'\uD3AC' : ['ph e s','펬'],
    u'\uD3AD' : ['ph e N','펭'],
    u'\uD3AE' : ['ph e tS','펮'],
    u'\uD3AF' : ['ph e tSh','펯'],
    u'\uD3B0' : ['ph e kh','펰'],
    u'\uD3B1' : ['ph e th','펱'],
    u'\uD3B2' : ['ph e ph','펲'],
    u'\uD3B3' : ['ph e h','펳'],
    u'\uD3B4' : ['ph j ^','펴'],
    u'\uD3B5' : ['ph j ^ k','펵'],
    u'\uD3B6' : ['ph j ^ k>','펶'],
    u'\uD3B7' : ['ph j ^ k sh','펷'],
    u'\uD3B8' : ['ph j ^ n','편'],
    u'\uD3B9' : ['ph j ^ n tS','펹'],
    u'\uD3BA' : ['ph j ^ n h','펺'],
    u'\uD3BB' : ['ph j ^ t','펻'],
    u'\uD3BC' : ['ph j ^ l','펼'],
    u'\uD3BD' : ['ph j ^ l k','펽'],
    u'\uD3BE' : ['ph j ^ l m','펾'],
    u'\uD3BF' : ['ph j ^ l p','펿'],
    u'\uD3C0' : ['ph j ^ l sh','폀'],
    u'\uD3C1' : ['ph j ^ l th','폁'],
    u'\uD3C2' : ['ph j ^ l ph','폂'],
    u'\uD3C3' : ['ph j ^ l h','폃'],
    u'\uD3C4' : ['ph j ^ m','폄'],
    u'\uD3C5' : ['ph j ^ p','폅'],
    u'\uD3C6' : ['ph j ^ p sh','폆'],
    u'\uD3C7' : ['ph j ^ sh','폇'],
    u'\uD3C8' : ['ph j ^ s','폈'],
    u'\uD3C9' : ['ph j ^ N','평'],
    u'\uD3CA' : ['ph j ^ tS','폊'],
    u'\uD3CB' : ['ph j ^ tSh','폋'],
    u'\uD3CC' : ['ph j ^ kh','폌'],
    u'\uD3CD' : ['ph j ^ th','폍'],
    u'\uD3CE' : ['ph j ^ ph','폎'],
    u'\uD3CF' : ['ph j ^ h','폏'],
    u'\uD3D0' : ['ph j e','폐'],
    u'\uD3D1' : ['ph j e k','폑'],
    u'\uD3D2' : ['ph j e k>','폒'],
    u'\uD3D3' : ['ph j e k sh','폓'],
    u'\uD3D4' : ['ph j e n','폔'],
    u'\uD3D5' : ['ph j e n tS','폕'],
    u'\uD3D6' : ['ph j e n h','폖'],
    u'\uD3D7' : ['ph j e t','폗'],
    u'\uD3D8' : ['ph j e l','폘'],
    u'\uD3D9' : ['ph j e l k','폙'],
    u'\uD3DA' : ['ph j e l m','폚'],
    u'\uD3DB' : ['ph j e l p','폛'],
    u'\uD3DC' : ['ph j e l sh','폜'],
    u'\uD3DD' : ['ph j e l th','폝'],
    u'\uD3DE' : ['ph j e l ph','폞'],
    u'\uD3DF' : ['ph j e l h','폟'],
    u'\uD3E0' : ['ph j e m','폠'],
    u'\uD3E1' : ['ph j e p','폡'],
    u'\uD3E2' : ['ph j e p sh','폢'],
    u'\uD3E3' : ['ph j e sh','폣'],
    u'\uD3E4' : ['ph j e s','폤'],
    u'\uD3E5' : ['ph j e N','폥'],
    u'\uD3E6' : ['ph j e tS','폦'],
    u'\uD3E7' : ['ph j e tSh','폧'],
    u'\uD3E8' : ['ph j e kh','폨'],
    u'\uD3E9' : ['ph j e th','폩'],
    u'\uD3EA' : ['ph j e ph','폪'],
    u'\uD3EB' : ['ph j e h','폫'],
    u'\uD3EC' : ['ph o','포'],
    u'\uD3ED' : ['ph o k','폭'],
    u'\uD3EE' : ['ph o k>','폮'],
    u'\uD3EF' : ['ph o k sh','폯'],
    u'\uD3F0' : ['ph o n','폰'],
    u'\uD3F1' : ['ph o n tS','폱'],
    u'\uD3F2' : ['ph o n h','폲'],
    u'\uD3F3' : ['ph o t','폳'],
    u'\uD3F4' : ['ph o l','폴'],
    u'\uD3F5' : ['ph o l k','폵'],
    u'\uD3F6' : ['ph o l m','폶'],
    u'\uD3F7' : ['ph o l p','폷'],
    u'\uD3F8' : ['ph o l sh','폸'],
    u'\uD3F9' : ['ph o l th','폹'],
    u'\uD3FA' : ['ph o l ph','폺'],
    u'\uD3FB' : ['ph o l h','폻'],
    u'\uD3FC' : ['ph o m','폼'],
    u'\uD3FD' : ['ph o p','폽'],
    u'\uD3FE' : ['ph o p sh','폾'],
    u'\uD3FF' : ['ph o sh','폿'],
    u'\uD400' : ['ph o s','퐀'],
    u'\uD401' : ['ph o N','퐁'],
    u'\uD402' : ['ph o tS','퐂'],
    u'\uD403' : ['ph o tSh','퐃'],
    u'\uD404' : ['ph o kh','퐄'],
    u'\uD405' : ['ph o th','퐅'],
    u'\uD406' : ['ph o ph','퐆'],
    u'\uD407' : ['ph o h','퐇'],
    u'\uD408' : ['ph w a','퐈'],
    u'\uD409' : ['ph w a k','퐉'],
    u'\uD40A' : ['ph w a k>','퐊'],
    u'\uD40B' : ['ph w a k sh','퐋'],
    u'\uD40C' : ['ph w a n','퐌'],
    u'\uD40D' : ['ph w a n tS','퐍'],
    u'\uD40E' : ['ph w a n h','퐎'],
    u'\uD40F' : ['ph w a t','퐏'],
    u'\uD410' : ['ph w a l','퐐'],
    u'\uD411' : ['ph w a l k','퐑'],
    u'\uD412' : ['ph w a l m','퐒'],
    u'\uD413' : ['ph w a l p','퐓'],
    u'\uD414' : ['ph w a l sh','퐔'],
    u'\uD415' : ['ph w a l th','퐕'],
    u'\uD416' : ['ph w a l ph','퐖'],
    u'\uD417' : ['ph w a l h','퐗'],
    u'\uD418' : ['ph w a m','퐘'],
    u'\uD419' : ['ph w a p','퐙'],
    u'\uD41A' : ['ph w a p sh','퐚'],
    u'\uD41B' : ['ph w a sh','퐛'],
    u'\uD41C' : ['ph w a s','퐜'],
    u'\uD41D' : ['ph w a N','퐝'],
    u'\uD41E' : ['ph w a tS','퐞'],
    u'\uD41F' : ['ph w a tSh','퐟'],
    u'\uD420' : ['ph w a kh','퐠'],
    u'\uD421' : ['ph w a th','퐡'],
    u'\uD422' : ['ph w a ph','퐢'],
    u'\uD423' : ['ph w a h','퐣'],
    u'\uD424' : ['ph w @','퐤'],
    u'\uD425' : ['ph w @ k','퐥'],
    u'\uD426' : ['ph w @ k>','퐦'],
    u'\uD427' : ['ph w @ k sh','퐧'],
    u'\uD428' : ['ph w @ n','퐨'],
    u'\uD429' : ['ph w @ n tS','퐩'],
    u'\uD42A' : ['ph w @ n h','퐪'],
    u'\uD42B' : ['ph w @ t','퐫'],
    u'\uD42C' : ['ph w @ l','퐬'],
    u'\uD42D' : ['ph w @ l k','퐭'],
    u'\uD42E' : ['ph w @ l m','퐮'],
    u'\uD42F' : ['ph w @ l p','퐯'],
    u'\uD430' : ['ph w @ l sh','퐰'],
    u'\uD431' : ['ph w @ l th','퐱'],
    u'\uD432' : ['ph w @ l ph','퐲'],
    u'\uD433' : ['ph w @ l h','퐳'],
    u'\uD434' : ['ph w @ m','퐴'],
    u'\uD435' : ['ph w @ p','퐵'],
    u'\uD436' : ['ph w @ p sh','퐶'],
    u'\uD437' : ['ph w @ sh','퐷'],
    u'\uD438' : ['ph w @ s','퐸'],
    u'\uD439' : ['ph w @ N','퐹'],
    u'\uD43A' : ['ph w @ tS','퐺'],
    u'\uD43B' : ['ph w @ tSh','퐻'],
    u'\uD43C' : ['ph w @ kh','퐼'],
    u'\uD43D' : ['ph w @ th','퐽'],
    u'\uD43E' : ['ph w @ ph','퐾'],
    u'\uD43F' : ['ph w @ h','퐿'],
    u'\uD440' : ['ph w e','푀'],
    u'\uD441' : ['ph w e k','푁'],
    u'\uD442' : ['ph w e k>','푂'],
    u'\uD443' : ['ph w e k sh','푃'],
    u'\uD444' : ['ph w e n','푄'],
    u'\uD445' : ['ph w e n tS','푅'],
    u'\uD446' : ['ph w e n h','푆'],
    u'\uD447' : ['ph w e t','푇'],
    u'\uD448' : ['ph w e l','푈'],
    u'\uD449' : ['ph w e l k','푉'],
    u'\uD44A' : ['ph w e l m','푊'],
    u'\uD44B' : ['ph w e l p','푋'],
    u'\uD44C' : ['ph w e l sh','푌'],
    u'\uD44D' : ['ph w e l th','푍'],
    u'\uD44E' : ['ph w e l ph','푎'],
    u'\uD44F' : ['ph w e l h','푏'],
    u'\uD450' : ['ph w e m','푐'],
    u'\uD451' : ['ph w e p','푑'],
    u'\uD452' : ['ph w e p sh','푒'],
    u'\uD453' : ['ph w e sh','푓'],
    u'\uD454' : ['ph w e s','푔'],
    u'\uD455' : ['ph w e N','푕'],
    u'\uD456' : ['ph w e tS','푖'],
    u'\uD457' : ['ph w e tSh','푗'],
    u'\uD458' : ['ph w e kh','푘'],
    u'\uD459' : ['ph w e th','푙'],
    u'\uD45A' : ['ph w e ph','푚'],
    u'\uD45B' : ['ph w e h','푛'],
    u'\uD45C' : ['ph j o','표'],
    u'\uD45D' : ['ph j o k','푝'],
    u'\uD45E' : ['ph j o k>','푞'],
    u'\uD45F' : ['ph j o k sh','푟'],
    u'\uD460' : ['ph j o n','푠'],
    u'\uD461' : ['ph j o n tS','푡'],
    u'\uD462' : ['ph j o n h','푢'],
    u'\uD463' : ['ph j o t','푣'],
    u'\uD464' : ['ph j o l','푤'],
    u'\uD465' : ['ph j o l k','푥'],
    u'\uD466' : ['ph j o l m','푦'],
    u'\uD467' : ['ph j o l p','푧'],
    u'\uD468' : ['ph j o l sh','푨'],
    u'\uD469' : ['ph j o l th','푩'],
    u'\uD46A' : ['ph j o l ph','푪'],
    u'\uD46B' : ['ph j o l h','푫'],
    u'\uD46C' : ['ph j o m','푬'],
    u'\uD46D' : ['ph j o p','푭'],
    u'\uD46E' : ['ph j o p sh','푮'],
    u'\uD46F' : ['ph j o sh','푯'],
    u'\uD470' : ['ph j o s','푰'],
    u'\uD471' : ['ph j o N','푱'],
    u'\uD472' : ['ph j o tS','푲'],
    u'\uD473' : ['ph j o tSh','푳'],
    u'\uD474' : ['ph j o kh','푴'],
    u'\uD475' : ['ph j o th','푵'],
    u'\uD476' : ['ph j o ph','푶'],
    u'\uD477' : ['ph j o h','푷'],
    u'\uD478' : ['ph u','푸'],
    u'\uD479' : ['ph u k','푹'],
    u'\uD47A' : ['ph u k>','푺'],
    u'\uD47B' : ['ph u k sh','푻'],
    u'\uD47C' : ['ph u n','푼'],
    u'\uD47D' : ['ph u n tS','푽'],
    u'\uD47E' : ['ph u n h','푾'],
    u'\uD47F' : ['ph u t','푿'],
    u'\uD480' : ['ph u l','풀'],
    u'\uD481' : ['ph u l k','풁'],
    u'\uD482' : ['ph u l m','풂'],
    u'\uD483' : ['ph u l p','풃'],
    u'\uD484' : ['ph u l sh','풄'],
    u'\uD485' : ['ph u l th','풅'],
    u'\uD486' : ['ph u l ph','풆'],
    u'\uD487' : ['ph u l h','풇'],
    u'\uD488' : ['ph u m','품'],
    u'\uD489' : ['ph u p','풉'],
    u'\uD48A' : ['ph u p sh','풊'],
    u'\uD48B' : ['ph u sh','풋'],
    u'\uD48C' : ['ph u s','풌'],
    u'\uD48D' : ['ph u N','풍'],
    u'\uD48E' : ['ph u tS','풎'],
    u'\uD48F' : ['ph u tSh','풏'],
    u'\uD490' : ['ph u kh','풐'],
    u'\uD491' : ['ph u th','풑'],
    u'\uD492' : ['ph u ph','풒'],
    u'\uD493' : ['ph u h','풓'],
    u'\uD494' : ['ph w ^','풔'],
    u'\uD495' : ['ph w ^ k','풕'],
    u'\uD496' : ['ph w ^ k>','풖'],
    u'\uD497' : ['ph w ^ k sh','풗'],
    u'\uD498' : ['ph w ^ n','풘'],
    u'\uD499' : ['ph w ^ n tS','풙'],
    u'\uD49A' : ['ph w ^ n h','풚'],
    u'\uD49B' : ['ph w ^ t','풛'],
    u'\uD49C' : ['ph w ^ l','풜'],
    u'\uD49D' : ['ph w ^ l k','풝'],
    u'\uD49E' : ['ph w ^ l m','풞'],
    u'\uD49F' : ['ph w ^ l p','풟'],
    u'\uD4A0' : ['ph w ^ l sh','풠'],
    u'\uD4A1' : ['ph w ^ l th','풡'],
    u'\uD4A2' : ['ph w ^ l ph','풢'],
    u'\uD4A3' : ['ph w ^ l h','풣'],
    u'\uD4A4' : ['ph w ^ m','풤'],
    u'\uD4A5' : ['ph w ^ p','풥'],
    u'\uD4A6' : ['ph w ^ p sh','풦'],
    u'\uD4A7' : ['ph w ^ sh','풧'],
    u'\uD4A8' : ['ph w ^ s','풨'],
    u'\uD4A9' : ['ph w ^ N','풩'],
    u'\uD4AA' : ['ph w ^ tS','풪'],
    u'\uD4AB' : ['ph w ^ tSh','풫'],
    u'\uD4AC' : ['ph w ^ kh','풬'],
    u'\uD4AD' : ['ph w ^ th','풭'],
    u'\uD4AE' : ['ph w ^ ph','풮'],
    u'\uD4AF' : ['ph w ^ h','풯'],
    u'\uD4B0' : ['ph w E','풰'],
    u'\uD4B1' : ['ph w E k','풱'],
    u'\uD4B2' : ['ph w E k>','풲'],
    u'\uD4B3' : ['ph w E k sh','풳'],
    u'\uD4B4' : ['ph w E n','풴'],
    u'\uD4B5' : ['ph w E n tS','풵'],
    u'\uD4B6' : ['ph w E n h','풶'],
    u'\uD4B7' : ['ph w E t','풷'],
    u'\uD4B8' : ['ph w E l','풸'],
    u'\uD4B9' : ['ph w E l k','풹'],
    u'\uD4BA' : ['ph w E l m','풺'],
    u'\uD4BB' : ['ph w E l p','풻'],
    u'\uD4BC' : ['ph w E l sh','풼'],
    u'\uD4BD' : ['ph w E l th','풽'],
    u'\uD4BE' : ['ph w E l ph','풾'],
    u'\uD4BF' : ['ph w E l h','풿'],
    u'\uD4C0' : ['ph w E m','퓀'],
    u'\uD4C1' : ['ph w E p','퓁'],
    u'\uD4C2' : ['ph w E p sh','퓂'],
    u'\uD4C3' : ['ph w E sh','퓃'],
    u'\uD4C4' : ['ph w E s','퓄'],
    u'\uD4C5' : ['ph w E N','퓅'],
    u'\uD4C6' : ['ph w E tS','퓆'],
    u'\uD4C7' : ['ph w E tSh','퓇'],
    u'\uD4C8' : ['ph w E kh','퓈'],
    u'\uD4C9' : ['ph w E th','퓉'],
    u'\uD4CA' : ['ph w E ph','퓊'],
    u'\uD4CB' : ['ph w E h','퓋'],
    u'\uD4CC' : ['ph 7','퓌'],
    u'\uD4CD' : ['ph 7 k','퓍'],
    u'\uD4CE' : ['ph 7 k>','퓎'],
    u'\uD4CF' : ['ph 7 k sh','퓏'],
    u'\uD4D0' : ['ph 7 n','퓐'],
    u'\uD4D1' : ['ph 7 n tS','퓑'],
    u'\uD4D2' : ['ph 7 n h','퓒'],
    u'\uD4D3' : ['ph 7 t','퓓'],
    u'\uD4D4' : ['ph 7 l','퓔'],
    u'\uD4D5' : ['ph 7 l k','퓕'],
    u'\uD4D6' : ['ph 7 l m','퓖'],
    u'\uD4D7' : ['ph 7 l p','퓗'],
    u'\uD4D8' : ['ph 7 l sh','퓘'],
    u'\uD4D9' : ['ph 7 l th','퓙'],
    u'\uD4DA' : ['ph 7 l ph','퓚'],
    u'\uD4DB' : ['ph 7 l h','퓛'],
    u'\uD4DC' : ['ph 7 m','퓜'],
    u'\uD4DD' : ['ph 7 p','퓝'],
    u'\uD4DE' : ['ph 7 p sh','퓞'],
    u'\uD4DF' : ['ph 7 sh','퓟'],
    u'\uD4E0' : ['ph 7 s','퓠'],
    u'\uD4E1' : ['ph 7 N','퓡'],
    u'\uD4E2' : ['ph 7 tS','퓢'],
    u'\uD4E3' : ['ph 7 tSh','퓣'],
    u'\uD4E4' : ['ph 7 kh','퓤'],
    u'\uD4E5' : ['ph 7 th','퓥'],
    u'\uD4E6' : ['ph 7 ph','퓦'],
    u'\uD4E7' : ['ph 7 h','퓧'],
    u'\uD4E8' : ['ph j u','퓨'],
    u'\uD4E9' : ['ph j u k','퓩'],
    u'\uD4EA' : ['ph j u k>','퓪'],
    u'\uD4EB' : ['ph j u k sh','퓫'],
    u'\uD4EC' : ['ph j u n','퓬'],
    u'\uD4ED' : ['ph j u n tS','퓭'],
    u'\uD4EE' : ['ph j u n h','퓮'],
    u'\uD4EF' : ['ph j u t','퓯'],
    u'\uD4F0' : ['ph j u l','퓰'],
    u'\uD4F1' : ['ph j u l k','퓱'],
    u'\uD4F2' : ['ph j u l m','퓲'],
    u'\uD4F3' : ['ph j u l p','퓳'],
    u'\uD4F4' : ['ph j u l sh','퓴'],
    u'\uD4F5' : ['ph j u l th','퓵'],
    u'\uD4F6' : ['ph j u l ph','퓶'],
    u'\uD4F7' : ['ph j u l h','퓷'],
    u'\uD4F8' : ['ph j u m','퓸'],
    u'\uD4F9' : ['ph j u p','퓹'],
    u'\uD4FA' : ['ph j u p sh','퓺'],
    u'\uD4FB' : ['ph j u sh','퓻'],
    u'\uD4FC' : ['ph j u s','퓼'],
    u'\uD4FD' : ['ph j u N','퓽'],
    u'\uD4FE' : ['ph j u tS','퓾'],
    u'\uD4FF' : ['ph j u tSh','퓿'],
    u'\uD500' : ['ph j u kh','픀'],
    u'\uD501' : ['ph j u th','픁'],
    u'\uD502' : ['ph j u ph','픂'],
    u'\uD503' : ['ph j u h','픃'],
    u'\uD504' : ['ph 4','프'],
    u'\uD505' : ['ph 4 k','픅'],
    u'\uD506' : ['ph 4 k>','픆'],
    u'\uD507' : ['ph 4 k sh','픇'],
    u'\uD508' : ['ph 4 n','픈'],
    u'\uD509' : ['ph 4 n tS','픉'],
    u'\uD50A' : ['ph 4 n h','픊'],
    u'\uD50B' : ['ph 4 t','픋'],
    u'\uD50C' : ['ph 4 l','플'],
    u'\uD50D' : ['ph 4 l k','픍'],
    u'\uD50E' : ['ph 4 l m','픎'],
    u'\uD50F' : ['ph 4 l p','픏'],
    u'\uD510' : ['ph 4 l sh','픐'],
    u'\uD511' : ['ph 4 l th','픑'],
    u'\uD512' : ['ph 4 l ph','픒'],
    u'\uD513' : ['ph 4 l h','픓'],
    u'\uD514' : ['ph 4 m','픔'],
    u'\uD515' : ['ph 4 p','픕'],
    u'\uD516' : ['ph 4 p sh','픖'],
    u'\uD517' : ['ph 4 sh','픗'],
    u'\uD518' : ['ph 4 s','픘'],
    u'\uD519' : ['ph 4 N','픙'],
    u'\uD51A' : ['ph 4 tS','픚'],
    u'\uD51B' : ['ph 4 tSh','픛'],
    u'\uD51C' : ['ph 4 kh','픜'],
    u'\uD51D' : ['ph 4 th','픝'],
    u'\uD51E' : ['ph 4 ph','픞'],
    u'\uD51F' : ['ph 4 h','픟'],
    u'\uD520' : ['ph 4 j','픠'],
    u'\uD521' : ['ph 4 j k','픡'],
    u'\uD522' : ['ph 4 j k>','픢'],
    u'\uD523' : ['ph 4 j k sh','픣'],
    u'\uD524' : ['ph 4 j n','픤'],
    u'\uD525' : ['ph 4 j n tS','픥'],
    u'\uD526' : ['ph 4 j n h','픦'],
    u'\uD527' : ['ph 4 j t','픧'],
    u'\uD528' : ['ph 4 j l','픨'],
    u'\uD529' : ['ph 4 j l k','픩'],
    u'\uD52A' : ['ph 4 j l m','픪'],
    u'\uD52B' : ['ph 4 j l p','픫'],
    u'\uD52C' : ['ph 4 j l sh','픬'],
    u'\uD52D' : ['ph 4 j l th','픭'],
    u'\uD52E' : ['ph 4 j l ph','픮'],
    u'\uD52F' : ['ph 4 j l h','픯'],
    u'\uD530' : ['ph 4 j m','픰'],
    u'\uD531' : ['ph 4 j p','픱'],
    u'\uD532' : ['ph 4 j p sh','픲'],
    u'\uD533' : ['ph 4 j sh','픳'],
    u'\uD534' : ['ph 4 j s','픴'],
    u'\uD535' : ['ph 4 j N','픵'],
    u'\uD536' : ['ph 4 j tS','픶'],
    u'\uD537' : ['ph 4 j tSh','픷'],
    u'\uD538' : ['ph 4 j kh','픸'],
    u'\uD539' : ['ph 4 j th','픹'],
    u'\uD53A' : ['ph 4 j ph','픺'],
    u'\uD53B' : ['ph 4 j h','픻'],
    u'\uD53C' : ['ph i','피'],
    u'\uD53D' : ['ph i k','픽'],
    u'\uD53E' : ['ph i k>','픾'],
    u'\uD53F' : ['ph i k sh','픿'],
    u'\uD540' : ['ph i n','핀'],
    u'\uD541' : ['ph i n tS','핁'],
    u'\uD542' : ['ph i n h','핂'],
    u'\uD543' : ['ph i t','핃'],
    u'\uD544' : ['ph i l','필'],
    u'\uD545' : ['ph i l k','핅'],
    u'\uD546' : ['ph i l m','핆'],
    u'\uD547' : ['ph i l p','핇'],
    u'\uD548' : ['ph i l sh','핈'],
    u'\uD549' : ['ph i l th','핉'],
    u'\uD54A' : ['ph i l ph','핊'],
    u'\uD54B' : ['ph i l h','핋'],
    u'\uD54C' : ['ph i m','핌'],
    u'\uD54D' : ['ph i p','핍'],
    u'\uD54E' : ['ph i p sh','핎'],
    u'\uD54F' : ['ph i sh','핏'],
    u'\uD550' : ['ph i s','핐'],
    u'\uD551' : ['ph i N','핑'],
    u'\uD552' : ['ph i tS','핒'],
    u'\uD553' : ['ph i tSh','핓'],
    u'\uD554' : ['ph i kh','핔'],
    u'\uD555' : ['ph i th','핕'],
    u'\uD556' : ['ph i ph','핖'],
    u'\uD557' : ['ph i h','핗'],
    u'\uD558' : ['h a','하'],
    u'\uD559' : ['h a k','학'],
    u'\uD55A' : ['h a k>','핚'],
    u'\uD55B' : ['h a k sh','핛'],
    u'\uD55C' : ['h a n','한'],
    u'\uD55D' : ['h a n tS','핝'],
    u'\uD55E' : ['h a n h','핞'],
    u'\uD55F' : ['h a t','핟'],
    u'\uD560' : ['h a l','할'],
    u'\uD561' : ['h a l k','핡'],
    u'\uD562' : ['h a l m','핢'],
    u'\uD563' : ['h a l p','핣'],
    u'\uD564' : ['h a l sh','핤'],
    u'\uD565' : ['h a l th','핥'],
    u'\uD566' : ['h a l ph','핦'],
    u'\uD567' : ['h a l h','핧'],
    u'\uD568' : ['h a m','함'],
    u'\uD569' : ['h a p','합'],
    u'\uD56A' : ['h a p sh','핪'],
    u'\uD56B' : ['h a sh','핫'],
    u'\uD56C' : ['h a s','핬'],
    u'\uD56D' : ['h a N','항'],
    u'\uD56E' : ['h a tS','핮'],
    u'\uD56F' : ['h a tSh','핯'],
    u'\uD570' : ['h a kh','핰'],
    u'\uD571' : ['h a th','핱'],
    u'\uD572' : ['h a ph','핲'],
    u'\uD573' : ['h a h','핳'],
    u'\uD574' : ['h @','해'],
    u'\uD575' : ['h @ k','핵'],
    u'\uD576' : ['h @ k>','핶'],
    u'\uD577' : ['h @ k sh','핷'],
    u'\uD578' : ['h @ n','핸'],
    u'\uD579' : ['h @ n tS','핹'],
    u'\uD57A' : ['h @ n h','핺'],
    u'\uD57B' : ['h @ t','핻'],
    u'\uD57C' : ['h @ l','핼'],
    u'\uD57D' : ['h @ l k','핽'],
    u'\uD57E' : ['h @ l m','핾'],
    u'\uD57F' : ['h @ l p','핿'],
    u'\uD580' : ['h @ l sh','햀'],
    u'\uD581' : ['h @ l th','햁'],
    u'\uD582' : ['h @ l ph','햂'],
    u'\uD583' : ['h @ l h','햃'],
    u'\uD584' : ['h @ m','햄'],
    u'\uD585' : ['h @ p','햅'],
    u'\uD586' : ['h @ p sh','햆'],
    u'\uD587' : ['h @ sh','햇'],
    u'\uD588' : ['h @ s','했'],
    u'\uD589' : ['h @ N','행'],
    u'\uD58A' : ['h @ tS','햊'],
    u'\uD58B' : ['h @ tSh','햋'],
    u'\uD58C' : ['h @ kh','햌'],
    u'\uD58D' : ['h @ th','햍'],
    u'\uD58E' : ['h @ ph','햎'],
    u'\uD58F' : ['h @ h','햏'],
    u'\uD590' : ['h j a','햐'],
    u'\uD591' : ['h j a k','햑'],
    u'\uD592' : ['h j a k>','햒'],
    u'\uD593' : ['h j a k sh','햓'],
    u'\uD594' : ['h j a n','햔'],
    u'\uD595' : ['h j a n tS','햕'],
    u'\uD596' : ['h j a n h','햖'],
    u'\uD597' : ['h j a t','햗'],
    u'\uD598' : ['h j a l','햘'],
    u'\uD599' : ['h j a l k','햙'],
    u'\uD59A' : ['h j a l m','햚'],
    u'\uD59B' : ['h j a l p','햛'],
    u'\uD59C' : ['h j a l sh','햜'],
    u'\uD59D' : ['h j a l th','햝'],
    u'\uD59E' : ['h j a l ph','햞'],
    u'\uD59F' : ['h j a l h','햟'],
    u'\uD5A0' : ['h j a m','햠'],
    u'\uD5A1' : ['h j a p','햡'],
    u'\uD5A2' : ['h j a p sh','햢'],
    u'\uD5A3' : ['h j a sh','햣'],
    u'\uD5A4' : ['h j a s','햤'],
    u'\uD5A5' : ['h j a N','향'],
    u'\uD5A6' : ['h j a tS','햦'],
    u'\uD5A7' : ['h j a tSh','햧'],
    u'\uD5A8' : ['h j a kh','햨'],
    u'\uD5A9' : ['h j a th','햩'],
    u'\uD5AA' : ['h j a ph','햪'],
    u'\uD5AB' : ['h j a h','햫'],
    u'\uD5AC' : ['h j @','햬'],
    u'\uD5AD' : ['h j @ k','햭'],
    u'\uD5AE' : ['h j @ k>','햮'],
    u'\uD5AF' : ['h j @ k sh','햯'],
    u'\uD5B0' : ['h j @ n','햰'],
    u'\uD5B1' : ['h j @ n tS','햱'],
    u'\uD5B2' : ['h j @ n h','햲'],
    u'\uD5B3' : ['h j @ t','햳'],
    u'\uD5B4' : ['h j @ l','햴'],
    u'\uD5B5' : ['h j @ l k','햵'],
    u'\uD5B6' : ['h j @ l m','햶'],
    u'\uD5B7' : ['h j @ l p','햷'],
    u'\uD5B8' : ['h j @ l sh','햸'],
    u'\uD5B9' : ['h j @ l th','햹'],
    u'\uD5BA' : ['h j @ l ph','햺'],
    u'\uD5BB' : ['h j @ l h','햻'],
    u'\uD5BC' : ['h j @ m','햼'],
    u'\uD5BD' : ['h j @ p','햽'],
    u'\uD5BE' : ['h j @ p sh','햾'],
    u'\uD5BF' : ['h j @ sh','햿'],
    u'\uD5C0' : ['h j @ s','헀'],
    u'\uD5C1' : ['h j @ N','헁'],
    u'\uD5C2' : ['h j @ tS','헂'],
    u'\uD5C3' : ['h j @ tSh','헃'],
    u'\uD5C4' : ['h j @ kh','헄'],
    u'\uD5C5' : ['h j @ th','헅'],
    u'\uD5C6' : ['h j @ ph','헆'],
    u'\uD5C7' : ['h j @ h','헇'],
    u'\uD5C8' : ['h ^','허'],
    u'\uD5C9' : ['h ^ k','헉'],
    u'\uD5CA' : ['h ^ k>','헊'],
    u'\uD5CB' : ['h ^ k sh','헋'],
    u'\uD5CC' : ['h ^ n','헌'],
    u'\uD5CD' : ['h ^ n tS','헍'],
    u'\uD5CE' : ['h ^ n h','헎'],
    u'\uD5CF' : ['h ^ t','헏'],
    u'\uD5D0' : ['h ^ l','헐'],
    u'\uD5D1' : ['h ^ l k','헑'],
    u'\uD5D2' : ['h ^ l m','헒'],
    u'\uD5D3' : ['h ^ l p','헓'],
    u'\uD5D4' : ['h ^ l sh','헔'],
    u'\uD5D5' : ['h ^ l th','헕'],
    u'\uD5D6' : ['h ^ l ph','헖'],
    u'\uD5D7' : ['h ^ l h','헗'],
    u'\uD5D8' : ['h ^ m','험'],
    u'\uD5D9' : ['h ^ p','헙'],
    u'\uD5DA' : ['h ^ p sh','헚'],
    u'\uD5DB' : ['h ^ sh','헛'],
    u'\uD5DC' : ['h ^ s','헜'],
    u'\uD5DD' : ['h ^ N','헝'],
    u'\uD5DE' : ['h ^ tS','헞'],
    u'\uD5DF' : ['h ^ tSh','헟'],
    u'\uD5E0' : ['h ^ kh','헠'],
    u'\uD5E1' : ['h ^ th','헡'],
    u'\uD5E2' : ['h ^ ph','헢'],
    u'\uD5E3' : ['h ^ h','헣'],
    u'\uD5E4' : ['h e','헤'],
    u'\uD5E5' : ['h e k','헥'],
    u'\uD5E6' : ['h e k>','헦'],
    u'\uD5E7' : ['h e k sh','헧'],
    u'\uD5E8' : ['h e n','헨'],
    u'\uD5E9' : ['h e n tS','헩'],
    u'\uD5EA' : ['h e n h','헪'],
    u'\uD5EB' : ['h e t','헫'],
    u'\uD5EC' : ['h e l','헬'],
    u'\uD5ED' : ['h e l k','헭'],
    u'\uD5EE' : ['h e l m','헮'],
    u'\uD5EF' : ['h e l p','헯'],
    u'\uD5F0' : ['h e l sh','헰'],
    u'\uD5F1' : ['h e l th','헱'],
    u'\uD5F2' : ['h e l ph','헲'],
    u'\uD5F3' : ['h e l h','헳'],
    u'\uD5F4' : ['h e m','헴'],
    u'\uD5F5' : ['h e p','헵'],
    u'\uD5F6' : ['h e p sh','헶'],
    u'\uD5F7' : ['h e sh','헷'],
    u'\uD5F8' : ['h e s','헸'],
    u'\uD5F9' : ['h e N','헹'],
    u'\uD5FA' : ['h e tS','헺'],
    u'\uD5FB' : ['h e tSh','헻'],
    u'\uD5FC' : ['h e kh','헼'],
    u'\uD5FD' : ['h e th','헽'],
    u'\uD5FE' : ['h e ph','헾'],
    u'\uD5FF' : ['h e h','헿'],
    u'\uD600' : ['h j ^','혀'],
    u'\uD601' : ['h j ^ k','혁'],
    u'\uD602' : ['h j ^ k>','혂'],
    u'\uD603' : ['h j ^ k sh','혃'],
    u'\uD604' : ['h j ^ n','현'],
    u'\uD605' : ['h j ^ n tS','혅'],
    u'\uD606' : ['h j ^ n h','혆'],
    u'\uD607' : ['h j ^ t','혇'],
    u'\uD608' : ['h j ^ l','혈'],
    u'\uD609' : ['h j ^ l k','혉'],
    u'\uD60A' : ['h j ^ l m','혊'],
    u'\uD60B' : ['h j ^ l p','혋'],
    u'\uD60C' : ['h j ^ l sh','혌'],
    u'\uD60D' : ['h j ^ l th','혍'],
    u'\uD60E' : ['h j ^ l ph','혎'],
    u'\uD60F' : ['h j ^ l h','혏'],
    u'\uD610' : ['h j ^ m','혐'],
    u'\uD611' : ['h j ^ p','협'],
    u'\uD612' : ['h j ^ p sh','혒'],
    u'\uD613' : ['h j ^ sh','혓'],
    u'\uD614' : ['h j ^ s','혔'],
    u'\uD615' : ['h j ^ N','형'],
    u'\uD616' : ['h j ^ tS','혖'],
    u'\uD617' : ['h j ^ tSh','혗'],
    u'\uD618' : ['h j ^ kh','혘'],
    u'\uD619' : ['h j ^ th','혙'],
    u'\uD61A' : ['h j ^ ph','혚'],
    u'\uD61B' : ['h j ^ h','혛'],
    u'\uD61C' : ['h j e','혜'],
    u'\uD61D' : ['h j e k','혝'],
    u'\uD61E' : ['h j e k>','혞'],
    u'\uD61F' : ['h j e k sh','혟'],
    u'\uD620' : ['h j e n','혠'],
    u'\uD621' : ['h j e n tS','혡'],
    u'\uD622' : ['h j e n h','혢'],
    u'\uD623' : ['h j e t','혣'],
    u'\uD624' : ['h j e l','혤'],
    u'\uD625' : ['h j e l k','혥'],
    u'\uD626' : ['h j e l m','혦'],
    u'\uD627' : ['h j e l p','혧'],
    u'\uD628' : ['h j e l sh','혨'],
    u'\uD629' : ['h j e l th','혩'],
    u'\uD62A' : ['h j e l ph','혪'],
    u'\uD62B' : ['h j e l h','혫'],
    u'\uD62C' : ['h j e m','혬'],
    u'\uD62D' : ['h j e p','혭'],
    u'\uD62E' : ['h j e p sh','혮'],
    u'\uD62F' : ['h j e sh','혯'],
    u'\uD630' : ['h j e s','혰'],
    u'\uD631' : ['h j e N','혱'],
    u'\uD632' : ['h j e tS','혲'],
    u'\uD633' : ['h j e tSh','혳'],
    u'\uD634' : ['h j e kh','혴'],
    u'\uD635' : ['h j e th','혵'],
    u'\uD636' : ['h j e ph','혶'],
    u'\uD637' : ['h j e h','혷'],
    u'\uD638' : ['h o','호'],
    u'\uD639' : ['h o k','혹'],
    u'\uD63A' : ['h o k>','혺'],
    u'\uD63B' : ['h o k sh','혻'],
    u'\uD63C' : ['h o n','혼'],
    u'\uD63D' : ['h o n tS','혽'],
    u'\uD63E' : ['h o n h','혾'],
    u'\uD63F' : ['h o t','혿'],
    u'\uD640' : ['h o l','홀'],
    u'\uD641' : ['h o l k','홁'],
    u'\uD642' : ['h o l m','홂'],
    u'\uD643' : ['h o l p','홃'],
    u'\uD644' : ['h o l sh','홄'],
    u'\uD645' : ['h o l th','홅'],
    u'\uD646' : ['h o l ph','홆'],
    u'\uD647' : ['h o l h','홇'],
    u'\uD648' : ['h o m','홈'],
    u'\uD649' : ['h o p','홉'],
    u'\uD64A' : ['h o p sh','홊'],
    u'\uD64B' : ['h o sh','홋'],
    u'\uD64C' : ['h o s','홌'],
    u'\uD64D' : ['h o N','홍'],
    u'\uD64E' : ['h o tS','홎'],
    u'\uD64F' : ['h o tSh','홏'],
    u'\uD650' : ['h o kh','홐'],
    u'\uD651' : ['h o th','홑'],
    u'\uD652' : ['h o ph','홒'],
    u'\uD653' : ['h o h','홓'],
    u'\uD654' : ['h w a','화'],
    u'\uD655' : ['h w a k','확'],
    u'\uD656' : ['h w a k>','홖'],
    u'\uD657' : ['h w a k sh','홗'],
    u'\uD658' : ['h w a n','환'],
    u'\uD659' : ['h w a n tS','홙'],
    u'\uD65A' : ['h w a n h','홚'],
    u'\uD65B' : ['h w a t','홛'],
    u'\uD65C' : ['h w a l','활'],
    u'\uD65D' : ['h w a l k','홝'],
    u'\uD65E' : ['h w a l m','홞'],
    u'\uD65F' : ['h w a l p','홟'],
    u'\uD660' : ['h w a l sh','홠'],
    u'\uD661' : ['h w a l th','홡'],
    u'\uD662' : ['h w a l ph','홢'],
    u'\uD663' : ['h w a l h','홣'],
    u'\uD664' : ['h w a m','홤'],
    u'\uD665' : ['h w a p','홥'],
    u'\uD666' : ['h w a p sh','홦'],
    u'\uD667' : ['h w a sh','홧'],
    u'\uD668' : ['h w a s','홨'],
    u'\uD669' : ['h w a N','황'],
    u'\uD66A' : ['h w a tS','홪'],
    u'\uD66B' : ['h w a tSh','홫'],
    u'\uD66C' : ['h w a kh','홬'],
    u'\uD66D' : ['h w a th','홭'],
    u'\uD66E' : ['h w a ph','홮'],
    u'\uD66F' : ['h w a h','홯'],
    u'\uD670' : ['h w @','홰'],
    u'\uD671' : ['h w @ k','홱'],
    u'\uD672' : ['h w @ k>','홲'],
    u'\uD673' : ['h w @ k sh','홳'],
    u'\uD674' : ['h w @ n','홴'],
    u'\uD675' : ['h w @ n tS','홵'],
    u'\uD676' : ['h w @ n h','홶'],
    u'\uD677' : ['h w @ t','홷'],
    u'\uD678' : ['h w @ l','홸'],
    u'\uD679' : ['h w @ l k','홹'],
    u'\uD67A' : ['h w @ l m','홺'],
    u'\uD67B' : ['h w @ l p','홻'],
    u'\uD67C' : ['h w @ l sh','홼'],
    u'\uD67D' : ['h w @ l th','홽'],
    u'\uD67E' : ['h w @ l ph','홾'],
    u'\uD67F' : ['h w @ l h','홿'],
    u'\uD680' : ['h w @ m','횀'],
    u'\uD681' : ['h w @ p','횁'],
    u'\uD682' : ['h w @ p sh','횂'],
    u'\uD683' : ['h w @ sh','횃'],
    u'\uD684' : ['h w @ s','횄'],
    u'\uD685' : ['h w @ N','횅'],
    u'\uD686' : ['h w @ tS','횆'],
    u'\uD687' : ['h w @ tSh','횇'],
    u'\uD688' : ['h w @ kh','횈'],
    u'\uD689' : ['h w @ th','횉'],
    u'\uD68A' : ['h w @ ph','횊'],
    u'\uD68B' : ['h w @ h','횋'],
    u'\uD68C' : ['h w e','회'],
    u'\uD68D' : ['h w e k','획'],
    u'\uD68E' : ['h w e k>','횎'],
    u'\uD68F' : ['h w e k sh','횏'],
    u'\uD690' : ['h w e n','횐'],
    u'\uD691' : ['h w e n tS','횑'],
    u'\uD692' : ['h w e n h','횒'],
    u'\uD693' : ['h w e t','횓'],
    u'\uD694' : ['h w e l','횔'],
    u'\uD695' : ['h w e l k','횕'],
    u'\uD696' : ['h w e l m','횖'],
    u'\uD697' : ['h w e l p','횗'],
    u'\uD698' : ['h w e l sh','횘'],
    u'\uD699' : ['h w e l th','횙'],
    u'\uD69A' : ['h w e l ph','횚'],
    u'\uD69B' : ['h w e l h','횛'],
    u'\uD69C' : ['h w e m','횜'],
    u'\uD69D' : ['h w e p','횝'],
    u'\uD69E' : ['h w e p sh','횞'],
    u'\uD69F' : ['h w e sh','횟'],
    u'\uD6A0' : ['h w e s','횠'],
    u'\uD6A1' : ['h w e N','횡'],
    u'\uD6A2' : ['h w e tS','횢'],
    u'\uD6A3' : ['h w e tSh','횣'],
    u'\uD6A4' : ['h w e kh','횤'],
    u'\uD6A5' : ['h w e th','횥'],
    u'\uD6A6' : ['h w e ph','횦'],
    u'\uD6A7' : ['h w e h','횧'],
    u'\uD6A8' : ['h j o','효'],
    u'\uD6A9' : ['h j o k','횩'],
    u'\uD6AA' : ['h j o k>','횪'],
    u'\uD6AB' : ['h j o k sh','횫'],
    u'\uD6AC' : ['h j o n','횬'],
    u'\uD6AD' : ['h j o n tS','횭'],
    u'\uD6AE' : ['h j o n h','횮'],
    u'\uD6AF' : ['h j o t','횯'],
    u'\uD6B0' : ['h j o l','횰'],
    u'\uD6B1' : ['h j o l k','횱'],
    u'\uD6B2' : ['h j o l m','횲'],
    u'\uD6B3' : ['h j o l p','횳'],
    u'\uD6B4' : ['h j o l sh','횴'],
    u'\uD6B5' : ['h j o l th','횵'],
    u'\uD6B6' : ['h j o l ph','횶'],
    u'\uD6B7' : ['h j o l h','횷'],
    u'\uD6B8' : ['h j o m','횸'],
    u'\uD6B9' : ['h j o p','횹'],
    u'\uD6BA' : ['h j o p sh','횺'],
    u'\uD6BB' : ['h j o sh','횻'],
    u'\uD6BC' : ['h j o s','횼'],
    u'\uD6BD' : ['h j o N','횽'],
    u'\uD6BE' : ['h j o tS','횾'],
    u'\uD6BF' : ['h j o tSh','횿'],
    u'\uD6C0' : ['h j o kh','훀'],
    u'\uD6C1' : ['h j o th','훁'],
    u'\uD6C2' : ['h j o ph','훂'],
    u'\uD6C3' : ['h j o h','훃'],
    u'\uD6C4' : ['h u','후'],
    u'\uD6C5' : ['h u k','훅'],
    u'\uD6C6' : ['h u k>','훆'],
    u'\uD6C7' : ['h u k sh','훇'],
    u'\uD6C8' : ['h u n','훈'],
    u'\uD6C9' : ['h u n tS','훉'],
    u'\uD6CA' : ['h u n h','훊'],
    u'\uD6CB' : ['h u t','훋'],
    u'\uD6CC' : ['h u l','훌'],
    u'\uD6CD' : ['h u l k','훍'],
    u'\uD6CE' : ['h u l m','훎'],
    u'\uD6CF' : ['h u l p','훏'],
    u'\uD6D0' : ['h u l sh','훐'],
    u'\uD6D1' : ['h u l th','훑'],
    u'\uD6D2' : ['h u l ph','훒'],
    u'\uD6D3' : ['h u l h','훓'],
    u'\uD6D4' : ['h u m','훔'],
    u'\uD6D5' : ['h u p','훕'],
    u'\uD6D6' : ['h u p sh','훖'],
    u'\uD6D7' : ['h u sh','훗'],
    u'\uD6D8' : ['h u s','훘'],
    u'\uD6D9' : ['h u N','훙'],
    u'\uD6DA' : ['h u tS','훚'],
    u'\uD6DB' : ['h u tSh','훛'],
    u'\uD6DC' : ['h u kh','훜'],
    u'\uD6DD' : ['h u th','훝'],
    u'\uD6DE' : ['h u ph','훞'],
    u'\uD6DF' : ['h u h','훟'],
    u'\uD6E0' : ['h w ^','훠'],
    u'\uD6E1' : ['h w ^ k','훡'],
    u'\uD6E2' : ['h w ^ k>','훢'],
    u'\uD6E3' : ['h w ^ k sh','훣'],
    u'\uD6E4' : ['h w ^ n','훤'],
    u'\uD6E5' : ['h w ^ n tS','훥'],
    u'\uD6E6' : ['h w ^ n h','훦'],
    u'\uD6E7' : ['h w ^ t','훧'],
    u'\uD6E8' : ['h w ^ l','훨'],
    u'\uD6E9' : ['h w ^ l k','훩'],
    u'\uD6EA' : ['h w ^ l m','훪'],
    u'\uD6EB' : ['h w ^ l p','훫'],
    u'\uD6EC' : ['h w ^ l sh','훬'],
    u'\uD6ED' : ['h w ^ l th','훭'],
    u'\uD6EE' : ['h w ^ l ph','훮'],
    u'\uD6EF' : ['h w ^ l h','훯'],
    u'\uD6F0' : ['h w ^ m','훰'],
    u'\uD6F1' : ['h w ^ p','훱'],
    u'\uD6F2' : ['h w ^ p sh','훲'],
    u'\uD6F3' : ['h w ^ sh','훳'],
    u'\uD6F4' : ['h w ^ s','훴'],
    u'\uD6F5' : ['h w ^ N','훵'],
    u'\uD6F6' : ['h w ^ tS','훶'],
    u'\uD6F7' : ['h w ^ tSh','훷'],
    u'\uD6F8' : ['h w ^ kh','훸'],
    u'\uD6F9' : ['h w ^ th','훹'],
    u'\uD6FA' : ['h w ^ ph','훺'],
    u'\uD6FB' : ['h w ^ h','훻'],
    u'\uD6FC' : ['h w E','훼'],
    u'\uD6FD' : ['h w E k','훽'],
    u'\uD6FE' : ['h w E k>','훾'],
    u'\uD6FF' : ['h w E k sh','훿'],
    u'\uD700' : ['h w E n','휀'],
    u'\uD701' : ['h w E n tS','휁'],
    u'\uD702' : ['h w E n h','휂'],
    u'\uD703' : ['h w E t','휃'],
    u'\uD704' : ['h w E l','휄'],
    u'\uD705' : ['h w E l k','휅'],
    u'\uD706' : ['h w E l m','휆'],
    u'\uD707' : ['h w E l p','휇'],
    u'\uD708' : ['h w E l sh','휈'],
    u'\uD709' : ['h w E l th','휉'],
    u'\uD70A' : ['h w E l ph','휊'],
    u'\uD70B' : ['h w E l h','휋'],
    u'\uD70C' : ['h w E m','휌'],
    u'\uD70D' : ['h w E p','휍'],
    u'\uD70E' : ['h w E p sh','휎'],
    u'\uD70F' : ['h w E sh','휏'],
    u'\uD710' : ['h w E s','휐'],
    u'\uD711' : ['h w E N','휑'],
    u'\uD712' : ['h w E tS','휒'],
    u'\uD713' : ['h w E tSh','휓'],
    u'\uD714' : ['h w E kh','휔'],
    u'\uD715' : ['h w E th','휕'],
    u'\uD716' : ['h w E ph','휖'],
    u'\uD717' : ['h w E h','휗'],
    u'\uD718' : ['h 7','휘'],
    u'\uD719' : ['h 7 k','휙'],
    u'\uD71A' : ['h 7 k>','휚'],
    u'\uD71B' : ['h 7 k sh','휛'],
    u'\uD71C' : ['h 7 n','휜'],
    u'\uD71D' : ['h 7 n tS','휝'],
    u'\uD71E' : ['h 7 n h','휞'],
    u'\uD71F' : ['h 7 t','휟'],
    u'\uD720' : ['h 7 l','휠'],
    u'\uD721' : ['h 7 l k','휡'],
    u'\uD722' : ['h 7 l m','휢'],
    u'\uD723' : ['h 7 l p','휣'],
    u'\uD724' : ['h 7 l sh','휤'],
    u'\uD725' : ['h 7 l th','휥'],
    u'\uD726' : ['h 7 l ph','휦'],
    u'\uD727' : ['h 7 l h','휧'],
    u'\uD728' : ['h 7 m','휨'],
    u'\uD729' : ['h 7 p','휩'],
    u'\uD72A' : ['h 7 p sh','휪'],
    u'\uD72B' : ['h 7 sh','휫'],
    u'\uD72C' : ['h 7 s','휬'],
    u'\uD72D' : ['h 7 N','휭'],
    u'\uD72E' : ['h 7 tS','휮'],
    u'\uD72F' : ['h 7 tSh','휯'],
    u'\uD730' : ['h 7 kh','휰'],
    u'\uD731' : ['h 7 th','휱'],
    u'\uD732' : ['h 7 ph','휲'],
    u'\uD733' : ['h 7 h','휳'],
    u'\uD734' : ['h j u','휴'],
    u'\uD735' : ['h j u k','휵'],
    u'\uD736' : ['h j u k>','휶'],
    u'\uD737' : ['h j u k sh','휷'],
    u'\uD738' : ['h j u n','휸'],
    u'\uD739' : ['h j u n tS','휹'],
    u'\uD73A' : ['h j u n h','휺'],
    u'\uD73B' : ['h j u t','휻'],
    u'\uD73C' : ['h j u l','휼'],
    u'\uD73D' : ['h j u l k','휽'],
    u'\uD73E' : ['h j u l m','휾'],
    u'\uD73F' : ['h j u l p','휿'],
    u'\uD740' : ['h j u l sh','흀'],
    u'\uD741' : ['h j u l th','흁'],
    u'\uD742' : ['h j u l ph','흂'],
    u'\uD743' : ['h j u l h','흃'],
    u'\uD744' : ['h j u m','흄'],
    u'\uD745' : ['h j u p','흅'],
    u'\uD746' : ['h j u p sh','흆'],
    u'\uD747' : ['h j u sh','흇'],
    u'\uD748' : ['h j u s','흈'],
    u'\uD749' : ['h j u N','흉'],
    u'\uD74A' : ['h j u tS','흊'],
    u'\uD74B' : ['h j u tSh','흋'],
    u'\uD74C' : ['h j u kh','흌'],
    u'\uD74D' : ['h j u th','흍'],
    u'\uD74E' : ['h j u ph','흎'],
    u'\uD74F' : ['h j u h','흏'],
    u'\uD750' : ['h 4','흐'],
    u'\uD751' : ['h 4 k','흑'],
    u'\uD752' : ['h 4 k>','흒'],
    u'\uD753' : ['h 4 k sh','흓'],
    u'\uD754' : ['h 4 n','흔'],
    u'\uD755' : ['h 4 n tS','흕'],
    u'\uD756' : ['h 4 n h','흖'],
    u'\uD757' : ['h 4 t','흗'],
    u'\uD758' : ['h 4 l','흘'],
    u'\uD759' : ['h 4 l k','흙'],
    u'\uD75A' : ['h 4 l m','흚'],
    u'\uD75B' : ['h 4 l p','흛'],
    u'\uD75C' : ['h 4 l sh','흜'],
    u'\uD75D' : ['h 4 l th','흝'],
    u'\uD75E' : ['h 4 l ph','흞'],
    u'\uD75F' : ['h 4 l h','흟'],
    u'\uD760' : ['h 4 m','흠'],
    u'\uD761' : ['h 4 p','흡'],
    u'\uD762' : ['h 4 p sh','흢'],
    u'\uD763' : ['h 4 sh','흣'],
    u'\uD764' : ['h 4 s','흤'],
    u'\uD765' : ['h 4 N','흥'],
    u'\uD766' : ['h 4 tS','흦'],
    u'\uD767' : ['h 4 tSh','흧'],
    u'\uD768' : ['h 4 kh','흨'],
    u'\uD769' : ['h 4 th','흩'],
    u'\uD76A' : ['h 4 ph','흪'],
    u'\uD76B' : ['h 4 h','흫'],
    u'\uD76C' : ['h 4 j','희'],
    u'\uD76D' : ['h 4 j k','흭'],
    u'\uD76E' : ['h 4 j k>','흮'],
    u'\uD76F' : ['h 4 j k sh','흯'],
    u'\uD770' : ['h 4 j n','흰'],
    u'\uD771' : ['h 4 j n tS','흱'],
    u'\uD772' : ['h 4 j n h','흲'],
    u'\uD773' : ['h 4 j t','흳'],
    u'\uD774' : ['h 4 j l','흴'],
    u'\uD775' : ['h 4 j l k','흵'],
    u'\uD776' : ['h 4 j l m','흶'],
    u'\uD777' : ['h 4 j l p','흷'],
    u'\uD778' : ['h 4 j l sh','흸'],
    u'\uD779' : ['h 4 j l th','흹'],
    u'\uD77A' : ['h 4 j l ph','흺'],
    u'\uD77B' : ['h 4 j l h','흻'],
    u'\uD77C' : ['h 4 j m','흼'],
    u'\uD77D' : ['h 4 j p','흽'],
    u'\uD77E' : ['h 4 j p sh','흾'],
    u'\uD77F' : ['h 4 j sh','흿'],
    u'\uD780' : ['h 4 j s','힀'],
    u'\uD781' : ['h 4 j N','힁'],
    u'\uD782' : ['h 4 j tS','힂'],
    u'\uD783' : ['h 4 j tSh','힃'],
    u'\uD784' : ['h 4 j kh','힄'],
    u'\uD785' : ['h 4 j th','힅'],
    u'\uD786' : ['h 4 j ph','힆'],
    u'\uD787' : ['h 4 j h','힇'],
    u'\uD788' : ['h i','히'],
    u'\uD789' : ['h i k','힉'],
    u'\uD78A' : ['h i k>','힊'],
    u'\uD78B' : ['h i k sh','힋'],
    u'\uD78C' : ['h i n','힌'],
    u'\uD78D' : ['h i n tS','힍'],
    u'\uD78E' : ['h i n h','힎'],
    u'\uD78F' : ['h i t','힏'],
    u'\uD790' : ['h i l','힐'],
    u'\uD791' : ['h i l k','힑'],
    u'\uD792' : ['h i l m','힒'],
    u'\uD793' : ['h i l p','힓'],
    u'\uD794' : ['h i l sh','힔'],
    u'\uD795' : ['h i l th','힕'],
    u'\uD796' : ['h i l ph','힖'],
    u'\uD797' : ['h i l h','힗'],
    u'\uD798' : ['h i m','힘'],
    u'\uD799' : ['h i p','힙'],
    u'\uD79A' : ['h i p sh','힚'],
    u'\uD79B' : ['h i sh','힛'],
    u'\uD79C' : ['h i s','힜'],
    u'\uD79D' : ['h i N','힝'],
    u'\uD79E' : ['h i tS','힞'],
    u'\uD79F' : ['h i tSh','힟'],
    u'\uD7A0' : ['h i kh','힠'],
    u'\uD7A1' : ['h i th','힡'],
    u'\uD7A2' : ['h i ph','힢'],
    u'\uD7A3' : ['h i h','힣'],
    u'\u1100' : ['k','ᄀ'],
    u'\u1101' : ['k>','ᄁ'],
    u'\u1102' : ['n','ᄂ'],
    u'\u1103' : ['t','ᄃ'],
    u'\u1104' : ['t>','ᄄ'],
    u'\u1105' : ['r','ᄅ'],
    u'\u1106' : ['m','ᄆ'],
    u'\u1107' : ['p','ᄇ'],
    u'\u1108' : ['p>','ᄈ'],
    u'\u1109' : ['sh','ᄉ'],
    u'\u110A' : ['s','ᄊ'],
    u'\u110B' : ['(SILENT)','ᄋ'],
    u'\u110C' : ['tS','ᄌ'],
    u'\u110D' : ['tS>','ᄍ'],
    u'\u110E' : ['tSh','ᄎ'],
    u'\u110F' : ['kh','ᄏ'],
    u'\u1110' : ['th','ᄐ'],
    u'\u1112' : ['h','ᄒ'],
    u'\u113C' : ['sh','ᄼ'],
    u'\u113D' : ['s','ᄽ'],
    u'\u113E' : ['sh','ᄾ'],
    u'\u113F' : ['s','ᄿ'],
    u'\u114C' : ['(SILENT)','ᅌ'],
    u'\u114E' : ['tS','ᅎ'],
    u'\u114F' : ['tS>','ᅏ'],
    u'\u1150' : ['tS','ᅐ'],
    u'\u1151' : ['tS>','ᅑ'],
    u'\u1154' : ['tSh','ᅔ'],
    u'\u1155' : ['tSh','ᅕ'],
    u'\u115F' : ['(CHOSEONG FILLER)','ᅟ'],
    u'\u1160' : ['(JUNGSEONG FILLER)','ᅠ'],
    u'\u1161' : ['a','ᅡ'],
    u'\u1162' : ['@','ᅢ'],
    u'\u1163' : ['j a','ᅣ'],
    u'\u1164' : ['j @','ᅤ'],
    u'\u1165' : ['^','ᅥ'],
    u'\u1166' : ['e','ᅦ'],
    u'\u1167' : ['j ^','ᅧ'],
    u'\u1168' : ['j e','ᅨ'],
    u'\u1169' : ['o','ᅩ'],
    u'\u116A' : ['w a','ᅪ'],
    u'\u116B' : ['w @','ᅫ'],
    u'\u116C' : ['w e','ᅬ'],
    u'\u116D' : ['j o','ᅭ'],
    u'\u116E' : ['u','ᅮ'],
    u'\u116F' : ['w ^','ᅯ'],
    u'\u1170' : ['w E','ᅰ'],
    u'\u1171' : ['7','ᅱ'],
    u'\u1172' : ['j u','ᅲ'],
    u'\u1173' : ['4','ᅳ'],
    u'\u1174' : ['4 j','ᅴ'],
    u'\u1175' : ['i','ᅵ'],
    u'\u11A8' : ['k','ᆨ'],
    u'\u11A9' : ['k>','ᆩ'],
    u'\u11AA' : ['k sh','ᆪ'],
    u'\u11AB' : ['n','ᆫ'],
    u'\u11AC' : ['n tS','ᆬ'],
    u'\u11AD' : ['n h','ᆭ'],
    u'\u11AE' : ['t','ᆮ'],
    u'\u11AF' : ['l','ᆯ'],
    u'\u11B0' : ['l k','ᆰ'],
    u'\u11B1' : ['l m','ᆱ'],
    u'\u11B2' : ['l p','ᆲ'],
    u'\u11B3' : ['l sh','ᆳ'],
    u'\u11B4' : ['l th','ᆴ'],
    u'\u11B5' : ['l ph','ᆵ'],
    u'\u11B6' : ['l h','ᆶ'],
    u'\u11B7' : ['m','ᆷ'],
    u'\u11B8' : ['p','ᆸ'],
    u'\u11B9' : ['p sh','ᆹ'],
    u'\u11BA' : ['sh','ᆺ'],
    u'\u11BB' : ['s','ᆻ'],
    u'\u11BC' : ['N','ᆼ'],
    u'\u11BD' : ['tS','ᆽ'],
    u'\u11BE' : ['tSh','ᆾ'],
    u'\u11BF' : ['kh','ᆿ'],
    u'\u11C0' : ['th','ᇀ'],
    u'\u11C1' : ['ph','ᇁ'],
    u'\u11C2' : ['h','ᇂ'],
    u'\u11F0' : ['N','ᇰ'],
    u'\u3131' : ['k','ㄱ'],
    u'\u3132' : ['k>','ㄲ'],
    u'\u3133' : ['k sh','ㄳ'],
    u'\u3134' : ['n','ㄴ'],
    u'\u3135' : ['n tS','ㄵ'],
    u'\u3136' : ['n h','ㄶ'],
    u'\u3137' : ['t','ㄷ'],
    u'\u3138' : ['t>','ㄸ'],
    u'\u3139' : ['r','ㄹ'],
    u'\u313A' : ['l k','ㄺ'],
    u'\u313B' : ['l m','ㄻ'],
    u'\u313C' : ['l b','ㄼ'],
    u'\u313D' : ['l sh','ㄽ'],
    u'\u313E' : ['l th','ㄾ'],
    u'\u313F' : ['l ph','ㄿ'],
    u'\u3140' : ['l h','ㅀ'],
    u'\u3141' : ['m','ㅁ'],
    u'\u3142' : ['p','ㅂ'],
    u'\u3143' : ['p>','ㅃ'],
    u'\u3144' : ['b sh','ㅄ'],
    u'\u3145' : ['sh','ㅅ'],
    u'\u3146' : ['s','ㅆ'],
    u'\u3147' : ['(SILENT)','ㅇ'],
    u'\u3148' : ['tS','ㅈ'],
    u'\u3149' : ['tS>','ㅉ'],
    u'\u314A' : ['tSh','ㅊ'],
    u'\u314B' : ['kh','ㅋ'],
    u'\u314C' : ['th','ㅌ'],
    u'\u314D' : ['ph','ㅍ'],
    u'\u314E' : ['h','ㅎ'],
    u'\u314F' : ['a','ㅏ'],
    u'\u3150' : ['@','ㅐ'],
    u'\u3151' : ['j a','ㅑ'],
    u'\u3152' : ['J @','ㅒ'],
    u'\u3153' : ['^','ㅓ'],
    u'\u3154' : ['e','ㅔ'],
    u'\u3155' : ['j ^','ㅕ'],
    u'\u3156' : ['j e','ㅖ'],
    u'\u3157' : ['o','ㅗ'],
    u'\u3158' : ['w a','ㅘ'],
    u'\u3159' : ['w @','ㅙ'],
    u'\u315A' : ['w e','ㅚ'],
    u'\u315B' : ['j o','ㅛ'],
    u'\u315C' : ['u','ㅜ'],
    u'\u315D' : ['w ^','ㅝ'],
    u'\u315E' : ['w E','ㅞ'],
    u'\u315F' : ['7','ㅟ'],
    u'\u3160' : ['j u','ㅠ'],
    u'\u3161' : ['4','ㅡ'],
    u'\u3162' : ['4 j','ㅢ'],
    u'\u3163' : ['i','ㅣ'],
    u'\u3164' : ['(HANGUL FILLER)','ㅤ'],
    u'\u0E81' : ['k, k','ກ'],
    u'\u0E82' : ['kh, k','ຂ'],
    u'\u0E84' : ['kh, k','ຄ'],
    u'\u0E87' : ['N, N','ງ'],
    u'\u0E88' : ['c, t','ຈ'],
    u'\u0E8A' : ['s, t','ຊ'],
    u'\u0E8D' : ['j, n','ຍ'],
    u'\u0E94' : ['d, t','ດ'],
    u'\u0E95' : ['t, t','ຕ'],
    u'\u0E96' : ['th, t','ຖ'],
    u'\u0E97' : ['th, t','ທ'],
    u'\u0E99' : ['n, n','ນ'],
    u'\u0E9A' : ['b, p','ບ'],
    u'\u0E9B' : ['p, p','ປ'],
    u'\u0E9C' : ['ph, p','ຜ'],
    u'\u0E9D' : ['f','ຝ'],
    u'\u0E9E' : ['ph, p','ພ'],
    u'\u0E9F' : ['f, p','ຟ'],
    u'\u0EA1' : ['m, m','ມ'],
    u'\u0EA2' : ['n~, j','ຢ'],
    u'\u0EA3' : ['h, n','ຣ'],
    u'\u0EA5' : ['l, n','ລ'],
    u'\u0EA7' : ['w, w','ວ'],
    u'\u0EAA' : ['s, t','ສ'],
    u'\u0EAB' : ['h','ຫ'],
    u'\u0EAD' : ['(SILENT)','ອ'],
    u'\u0EAE' : ['h','ຮ'],
    u'\u0EAF' : ['(ELLIPSIS)','ຯ'],
    u'\u0EB0' : ['A','ະ'],
    u'\u0EB1' : ['A','ັ'],
    u'\u0EB2' : ['A:','າ'],
    u'\u0EB3' : ['A m','ຳ'],
    u'\u0EB4' : ['i','ິ'],
    u'\u0EB5' : ['i:','ີ'],
    u'\u0EB6' : ['4','ຶ'],
    u'\u0EB7' : ['4:','ື'],
    u'\u0EB8' : ['u','ຸ'],
    u'\u0EB9' : ['u:','ູ'],
    u'\u0EBB' : ['(MAI KON)','ົ'],
    u'\u0EBC' : ['(LO)','ຼ'],
    u'\u0EBD' : ['(NYO)','ຽ'],
    u'\u0EC0' : ['e:','ເ'],
    u'\u0EC1' : ['@','ແ'],
    u'\u0EC2' : ['o','ໂ'],
    u'\u0EC3' : ['aI','ໃ'],
    u'\u0EC4' : ['aI','ໄ'],
    u'\u0EC6' : ['(KO LA)','ໆ'],
    u'\u0EC8' : ['(MAI EK)','່'],
    u'\u0EC9' : ['(MAI THO)','້'],
    u'\u0ECA' : ['(MAI TI)','໊'],
    u'\u0ECB' : ['(MAI CATAWA)','໋'],
    u'\u0ECC' : ['(CANCELLATION MARK)','໌'],
    u'\u0ECD' : ['(NIGGAHITA)','ໍ'],
    u'\u0ED0' : ['(ZERO)','໐'],
    u'\u0ED1' : ['(ONE)','໑'],
    u'\u0ED2' : ['(TWO)','໒'],
    u'\u0ED3' : ['(THREE)','໓'],
    u'\u0ED4' : ['(FOUR)','໔'],
    u'\u0ED5' : ['(FIVE)','໕'],
    u'\u0ED6' : ['(SIX)','໖'],
    u'\u0ED7' : ['(SEVEN)','໗'],
    u'\u0ED8' : ['(EIGHT)','໘'],
    u'\u0ED9' : ['(NINE)','໙'],
    u'\u0EDC' : ['h n','ໜ'],
    u'\u0EDD' : ['h m','ໝ'],
    u'\u1900' : ['(VOWEL CARRIER)','ᤀ'],
    u'\u1901' : ['k A','ᤁ'],
    u'\u1902' : ['kh A','ᤂ'],
    u'\u1903' : ['g A','ᤃ'],
    u'\u1904' : ['gh A','ᤄ'],
    u'\u1905' : ['N A','ᤅ'],
    u'\u1906' : ['c A','ᤆ'],
    u'\u1907' : ['ch A','ᤇ'],
    u'\u1908' : ['J A','ᤈ'],
    u'\u1909' : ['Jh a','ᤉ'],
    u'\u190A' : ['j Ar','ᤊ'],
    u'\u190B' : ['t[ A','ᤋ'],
    u'\u190C' : ['t[_h A','ᤌ'],
    u'\u190D' : ['d[ A','ᤍ'],
    u'\u190E' : ['d[_h A','ᤎ'],
    u'\u190F' : ['n[ A','ᤏ'],
    u'\u1910' : ['p A','ᤐ'],
    u'\u1911' : ['ph A','ᤑ'],
    u'\u1912' : ['b A','ᤒ'],
    u'\u1913' : ['bh A','ᤓ'],
    u'\u1914' : ['m A','ᤔ'],
    u'\u1915' : ['j A','ᤕ'],
    u'\u1916' : ['9r A','ᤖ'],
    u'\u1917' : ['l A','ᤗ'],
    u'\u1918' : ['w A','ᤘ'],
    u'\u1919' : ['S A','ᤙ'],
    u'\u191A' : ['S A','ᤚ'],
    u'\u191B' : ['s A','ᤛ'],
    u'\u191C' : ['h A','ᤜ'],
    u'\u1920' : ['A','ᤠ'],
    u'\u1921' : ['i','ᤡ'],
    u'\u1922' : ['u','ᤢ'],
    u'\u1923' : ['e:','ᤣ'],
    u'\u1924' : ['aI','ᤤ'],
    u'\u1925' : ['o:','ᤥ'],
    u'\u1926' : ['aU','ᤦ'],
    u'\u1927' : ['e','ᤧ'],
    u'\u1928' : ['o','ᤨ'],
    u'\u1929' : ['j A','ᤩ'],
    u'\u192A' : ['9r A','ᤪ'],
    u'\u192B' : ['w A','ᤫ'],
    u'\u1930' : ['k A','ᤰ'],
    u'\u1931' : ['N A','ᤱ'],
    u'\u1932' : ['M A','ᤲ'],
    u'\u1933' : ['t[ A','ᤳ'],
    u'\u1934' : ['n[ A','ᤴ'],
    u'\u1935' : ['p A','ᤵ'],
    u'\u1936' : ['m A','ᤶ'],
    u'\u1937' : ['9r A','ᤷ'],
    u'\u1938' : ['l A','ᤸ'],
    u'\u1939' : ['(MUKPHRENG)','᤹'],
    u'\u193A' : ['(KEMPHRENG)','᤺'],
    u'\u193B' : ['(P)','᤻'],
    u'\u1940' : ['(LOO)','᥀'],
    u'\u1944' : ['(EXCLAMATION MARK)','᥄'],
    u'\u1945' : ['(QUESTION MARK)','᥅'],
    u'\u1946' : ['(ZERO)','᥆'],
    u'\u1947' : ['(ONE)','᥇'],
    u'\u1948' : ['(TWO)','᥈'],
    u'\u1949' : ['(THREE)','᥉'],
    u'\u194A' : ['(FOUR)','᥊'],
    u'\u194B' : ['(FIVE)','᥋'],
    u'\u194C' : ['(SIX)','᥌'],
    u'\u194D' : ['(SEVEN)','᥍'],
    u'\u194E' : ['(EIGHT)','᥎'],
    u'\u194F' : ['(NINE)','᥏'],
    u'\u0D01' : ['(R:0D01)','ഁ'],
    u'\u0D02' : ['(M)','ം'],
    u'\u0D03' : ['(H)','ഃ'],
    u'\u0D05' : ['A','അ'],
    u'\u0D06' : ['A:','ആ'],
    u'\u0D07' : ['i','ഇ'],
    u'\u0D08' : ['i:','ഈ'],
    u'\u0D09' : ['u','ഉ'],
    u'\u0D0A' : ['u:','ഊ'],
    u'\u0D0B' : ['9r ix','ഋ'],
    u'\u0D0C' : ['l=','ഌ'],
    u'\u0D0D' : ['(R:0D0D)','഍'],
    u'\u0D0E' : ['e','എ'],
    u'\u0D0F' : ['e:','ഏ'],
    u'\u0D10' : ['aI','ഐ'],
    u'\u0D11' : ['(R:0D11)','഑'],
    u'\u0D12' : ['o','ഒ'],
    u'\u0D13' : ['o:','ഓ'],
    u'\u0D14' : ['aU','ഔ'],
    u'\u0D15' : ['k A','ക'],
    u'\u0D16' : ['kh A','ഖ'],
    u'\u0D17' : ['g A','ഗ'],
    u'\u0D18' : ['gh A','ഘ'],
    u'\u0D19' : ['N A','ങ'],
    u'\u0D1A' : ['c A','ച'],
    u'\u0D1B' : ['ch A','ഛ'],
    u'\u0D1C' : ['J A','ജ'],
    u'\u0D1D' : ['Jh A','ഝ'],
    u'\u0D1E' : ['n~ A','ഞ'],
    u'\u0D1F' : ['tr A','ട'],
    u'\u0D20' : ['tR A','ഠ'],
    u'\u0D21' : ['dr A','ഡ'],
    u'\u0D22' : ['dR A','ഢ'],
    u'\u0D23' : ['nr A','ണ'],
    u'\u0D24' : ['t[ A','ത'],
    u'\u0D25' : ['t[_h A','ഥ'],
    u'\u0D26' : ['d[ A','ദ'],
    u'\u0D27' : ['d[_h A','ധ'],
    u'\u0D28' : ['n[ A, n A','ന'],
    u'\u0D29' : ['(R:0D29)','ഩ'],
    u'\u0D2A' : ['p A','പ'],
    u'\u0D2B' : ['ph A','ഫ'],
    u'\u0D2C' : ['b A','ബ'],
    u'\u0D2D' : ['bh A','ഭ'],
    u'\u0D2E' : ['m A','മ'],
    u'\u0D2F' : ['j A','യ'],
    u'\u0D30' : ['9r[ A','ര'],
    u'\u0D31' : ['9r A','റ'],
    u'\u0D32' : ['l A','ല'],
    u'\u0D33' : ['lr A','ള'],
    u'\u0D34' : ['l A','ഴ'],
    u'\u0D35' : ['v A','വ'],
    u'\u0D36' : ['c} A','ശ'],
    u'\u0D37' : ['S A','ഷ'],
    u'\u0D38' : ['s A','സ'],
    u'\u0D39' : ['hv A','ഹ'],
    u'\u0D3E' : ['A:','ാ'],
    u'\u0D3F' : ['i','ി'],
    u'\u0D40' : ['i:','ീ'],
    u'\u0D41' : ['u','ു'],
    u'\u0D42' : ['u:','ൂ'],
    u'\u0D43' : ['9r=','ൃ'],
    u'\u0D45' : ['(R:0D45)','൅'],
    u'\u0D46' : ['e','െ'],
    u'\u0D47' : ['e:','േ'],
    u'\u0D48' : ['aI','ൈ'],
    u'\u0D49' : ['(R:0D49)','൉'],
    u'\u0D4A' : ['o','ൊ'],
    u'\u0D4B' : ['o:','ോ'],
    u'\u0D4C' : ['aU','ൌ'],
    u'\u0D4D' : ['(P)','്'],
    u'\u0D57' : ['(aU LENGTH MARK)','ൗ'],
    u'\u0D5F' : ['(R:0D5F)','ൟ'],
    u'\u0D60' : ['rr=','ൠ'],
    u'\u0D61' : ['lr=','ൡ'],
    u'\u0D64' : ['(R:0D64)','൤'],
    u'\u0D66' : ['(ZERO)','൦'],
    u'\u0D67' : ['(ONE)','൧'],
    u'\u0D68' : ['(TWO)','൨'],
    u'\u0D69' : ['(THREE)','൩'],
    u'\u0D6A' : ['(FOUR)','൪'],
    u'\u0D6B' : ['(FIVE)','൫'],
    u'\u0D6C' : ['(SIX)','൬'],
    u'\u0D6D' : ['(SEVEN)','൭'],
    u'\u0D6E' : ['(EIGHT)','൮'],
    u'\u0D6F' : ['(NINE)','൯'],
    u'\uFEFF' : ['(BOM)','﻿'],
    u'\u0080' : ['(##)',''],
    u'\u0081' : ['(##)',''],
    u'\u0082' : ['(##)',''],
    u'\u0083' : ['(##)',''],
    u'\u0084' : ['(##)',''],
    u'\u0085' : ['(##)',''],
    u'\u0086' : ['(##)',''],
    u'\u0087' : ['(##)',''],
    u'\u0088' : ['(##)',''],
    u'\u0089' : ['(##)',''],
    u'\u008A' : ['(##)',''],
    u'\u008B' : ['(##)',''],
    u'\u008C' : ['(##)',''],
    u'\u008D' : ['(##)',''],
    u'\u008E' : ['(##)',''],
    u'\u008F' : ['(##)',''],
    u'\u0090' : ['(##)',''],
    u'\u0091' : ['(##)',''],
    u'\u0092' : ['(##)',''],
    u'\u0093' : ['(##)',''],
    u'\u0094' : ['(##)',''],
    u'\u0095' : ['(##)',''],
    u'\u0096' : ['(##)',''],
    u'\u0097' : ['(##)',''],
    u'\u0098' : ['(##)',''],
    u'\u0099' : ['(##)',''],
    u'\u009A' : ['(##)',''],
    u'\u009B' : ['(##)',''],
    u'\u009C' : ['(##)',''],
    u'\u009D' : ['(##)',''],
    u'\u009E' : ['(##)',''],
    u'\u009F' : ['(##)',''],
    u'\u00A0' : ['(##)',' '],
    u'\u00A1' : ['(##)','¡'],
    u'\u00A2' : ['(##)','¢'],
    u'\u00A3' : ['(##)','£'],
    u'\u00A4' : ['(##)','¤'],
    u'\u00A5' : ['(##)','¥'],
    u'\u00A6' : ['(##)','¦'],
    u'\u00A7' : ['(##)','§'],
    u'\u00A8' : ['(##)','¨'],
    u'\u00A9' : ['(##)','©'],
    u'\u00AA' : ['(##)','ª'],
    u'\u00AB' : ['(##)','«'],
    u'\u00AC' : ['(##)','¬'],
    u'\u00AD' : ['(##)','­'],
    u'\u00AE' : ['(##)','®'],
    u'\u00AF' : ['(##)','¯'],
    u'\u00B0' : ['(##)','°'],
    u'\u00B1' : ['(##)','±'],
    u'\u00B2' : ['(##)','²'],
    u'\u00B3' : ['(##)','³'],
    u'\u00B4' : ['(##)','´'],
    u'\u00B5' : ['(##)','µ'],
    u'\u00B6' : ['(##)','¶'],
    u'\u00B7' : ['(##)','·'],
    u'\u00B8' : ['(##)','¸'],
    u'\u00B9' : ['(##)','¹'],
    u'\u00BA' : ['(##)','º'],
    u'\u00BB' : ['(##)','»'],
    u'\u00BC' : ['(##)','¼'],
    u'\u00BD' : ['(##)','½'],
    u'\u00BE' : ['(##)','¾'],
    u'\u00BF' : ['(##)','¿'],
    u'\u00C0' : ['(##)','À'],
    u'\u00C1' : ['(##)','Á'],
    u'\u00C2' : ['(##)','Â'],
    u'\u00C3' : ['(##)','Ã'],
    u'\u00C4' : ['(##)','Ä'],
    u'\u00C5' : ['(##)','Å'],
    u'\u00C6' : ['(##)','Æ'],
    u'\u00C7' : ['(##)','Ç'],
    u'\u00C8' : ['(##)','È'],
    u'\u00C9' : ['(##)','É'],
    u'\u00CA' : ['(##)','Ê'],
    u'\u00CB' : ['(##)','Ë'],
    u'\u00CC' : ['(##)','Ì'],
    u'\u00CD' : ['(##)','Í'],
    u'\u00CE' : ['(##)','Î'],
    u'\u00CF' : ['(##)','Ï'],
    u'\u00D0' : ['(##)','Ð'],
    u'\u00D1' : ['(##)','Ñ'],
    u'\u00D2' : ['(##)','Ò'],
    u'\u00D3' : ['(##)','Ó'],
    u'\u00D4' : ['(##)','Ô'],
    u'\u00D5' : ['(##)','Õ'],
    u'\u00D6' : ['(##)','Ö'],
    u'\u00D7' : ['(##)','×'],
    u'\u00D8' : ['(##)','Ø'],
    u'\u00D9' : ['(##)','Ù'],
    u'\u00DA' : ['(##)','Ú'],
    u'\u00DB' : ['(##)','Û'],
    u'\u00DC' : ['(##)','Ü'],
    u'\u00DD' : ['(##)','Ý'],
    u'\u00DE' : ['(##)','Þ'],
    u'\u00DF' : ['(##)','ß'],
    u'\u00E0' : ['(##)','à'],
    u'\u00E1' : ['(##)','á'],
    u'\u00E2' : ['(##)','â'],
    u'\u00E3' : ['(##)','ã'],
    u'\u00E4' : ['(##)','ä'],
    u'\u00E5' : ['(##)','å'],
    u'\u00E6' : ['(##)','æ'],
    u'\u00E7' : ['(##)','ç'],
    u'\u00E8' : ['(##)','è'],
    u'\u00E9' : ['(##)','é'],
    u'\u00EA' : ['(##)','ê'],
    u'\u00EB' : ['(##)','ë'],
    u'\u00EC' : ['(##)','ì'],
    u'\u00ED' : ['(##)','í'],
    u'\u00EE' : ['(##)','î'],
    u'\u00EF' : ['(##)','ï'],
    u'\u00F0' : ['(##)','ð'],
    u'\u00F1' : ['(##)','ñ'],
    u'\u00F2' : ['(##)','ò'],
    u'\u00F3' : ['(##)','ó'],
    u'\u00F4' : ['(##)','ô'],
    u'\u00F5' : ['(##)','õ'],
    u'\u00F6' : ['(##)','ö'],
    u'\u00F7' : ['(##)','÷'],
    u'\u00F8' : ['(##)','ø'],
    u'\u00F9' : ['(##)','ù'],
    u'\u00FA' : ['(##)','ú'],
    u'\u00FB' : ['(##)','û'],
    u'\u00FC' : ['(##)','ü'],
    u'\u00FD' : ['(##)','ý'],
    u'\u00FE' : ['(##)','þ'],
    u'\u00FF' : ['(##)','ÿ'],
    u'\u0100' : ['(##)','Ā'],
    u'\u0101' : ['(##)','ā'],
    u'\u0102' : ['(##)','Ă'],
    u'\u0103' : ['(##)','ă'],
    u'\u0104' : ['(##)','Ą'],
    u'\u0105' : ['(##)','ą'],
    u'\u0106' : ['(##)','Ć'],
    u'\u0107' : ['(##)','ć'],
    u'\u0108' : ['(##)','Ĉ'],
    u'\u0109' : ['(##)','ĉ'],
    u'\u010A' : ['(##)','Ċ'],
    u'\u010B' : ['(##)','ċ'],
    u'\u010C' : ['(##)','Č'],
    u'\u010D' : ['(##)','č'],
    u'\u010E' : ['(##)','Ď'],
    u'\u010F' : ['(##)','ď'],
    u'\u0110' : ['(##)','Đ'],
    u'\u0111' : ['(##)','đ'],
    u'\u0112' : ['(##)','Ē'],
    u'\u0113' : ['(##)','ē'],
    u'\u0114' : ['(##)','Ĕ'],
    u'\u0115' : ['(##)','ĕ'],
    u'\u0116' : ['(##)','Ė'],
    u'\u0117' : ['(##)','ė'],
    u'\u0118' : ['(##)','Ę'],
    u'\u0119' : ['(##)','ę'],
    u'\u011A' : ['(##)','Ě'],
    u'\u011B' : ['(##)','ě'],
    u'\u011C' : ['(##)','Ĝ'],
    u'\u011D' : ['(##)','ĝ'],
    u'\u011E' : ['(##)','Ğ'],
    u'\u011F' : ['(##)','ğ'],
    u'\u0120' : ['(##)','Ġ'],
    u'\u0121' : ['(##)','ġ'],
    u'\u0122' : ['(##)','Ģ'],
    u'\u0123' : ['(##)','ģ'],
    u'\u0124' : ['(##)','Ĥ'],
    u'\u0125' : ['(##)','ĥ'],
    u'\u0126' : ['(##)','Ħ'],
    u'\u0127' : ['(##)','ħ'],
    u'\u0128' : ['(##)','Ĩ'],
    u'\u0129' : ['(##)','ĩ'],
    u'\u012A' : ['(##)','Ī'],
    u'\u012B' : ['(##)','ī'],
    u'\u012C' : ['(##)','Ĭ'],
    u'\u012D' : ['(##)','ĭ'],
    u'\u012E' : ['(##)','Į'],
    u'\u012F' : ['(##)','į'],
    u'\u0130' : ['(##)','İ'],
    u'\u0131' : ['(##)','ı'],
    u'\u0132' : ['(##)','Ĳ'],
    u'\u0133' : ['(##)','ĳ'],
    u'\u0134' : ['(##)','Ĵ'],
    u'\u0135' : ['(##)','ĵ'],
    u'\u0136' : ['(##)','Ķ'],
    u'\u0137' : ['(##)','ķ'],
    u'\u0138' : ['(##)','ĸ'],
    u'\u0139' : ['(##)','Ĺ'],
    u'\u013A' : ['(##)','ĺ'],
    u'\u013B' : ['(##)','Ļ'],
    u'\u013C' : ['(##)','ļ'],
    u'\u013D' : ['(##)','Ľ'],
    u'\u013E' : ['(##)','ľ'],
    u'\u013F' : ['(##)','Ŀ'],
    u'\u0140' : ['(##)','ŀ'],
    u'\u0141' : ['(##)','Ł'],
    u'\u0142' : ['(##)','ł'],
    u'\u0143' : ['(##)','Ń'],
    u'\u0144' : ['(##)','ń'],
    u'\u0145' : ['(##)','Ņ'],
    u'\u0146' : ['(##)','ņ'],
    u'\u0147' : ['(##)','Ň'],
    u'\u0148' : ['(##)','ň'],
    u'\u0149' : ['(##)','ŉ'],
    u'\u014A' : ['(##)','Ŋ'],
    u'\u014B' : ['(##)','ŋ'],
    u'\u014C' : ['(##)','Ō'],
    u'\u014D' : ['(##)','ō'],
    u'\u014E' : ['(##)','Ŏ'],
    u'\u014F' : ['(##)','ŏ'],
    u'\u0150' : ['(##)','Ő'],
    u'\u0151' : ['(##)','ő'],
    u'\u0152' : ['(##)','Œ'],
    u'\u0153' : ['(##)','œ'],
    u'\u0154' : ['(##)','Ŕ'],
    u'\u0155' : ['(##)','ŕ'],
    u'\u0156' : ['(##)','Ŗ'],
    u'\u0157' : ['(##)','ŗ'],
    u'\u0158' : ['(##)','Ř'],
    u'\u0159' : ['(##)','ř'],
    u'\u015A' : ['(##)','Ś'],
    u'\u015B' : ['(##)','ś'],
    u'\u015C' : ['(##)','Ŝ'],
    u'\u015D' : ['(##)','ŝ'],
    u'\u015E' : ['(##)','Ş'],
    u'\u015F' : ['(##)','ş'],
    u'\u0160' : ['(##)','Š'],
    u'\u0161' : ['(##)','š'],
    u'\u0162' : ['(##)','Ţ'],
    u'\u0163' : ['(##)','ţ'],
    u'\u0164' : ['(##)','Ť'],
    u'\u0165' : ['(##)','ť'],
    u'\u0166' : ['(##)','Ŧ'],
    u'\u0167' : ['(##)','ŧ'],
    u'\u0168' : ['(##)','Ũ'],
    u'\u0169' : ['(##)','ũ'],
    u'\u016A' : ['(##)','Ū'],
    u'\u016B' : ['(##)','ū'],
    u'\u016C' : ['(##)','Ŭ'],
    u'\u016D' : ['(##)','ŭ'],
    u'\u016E' : ['(##)','Ů'],
    u'\u016F' : ['(##)','ů'],
    u'\u0170' : ['(##)','Ű'],
    u'\u0171' : ['(##)','ű'],
    u'\u0172' : ['(##)','Ų'],
    u'\u0173' : ['(##)','ų'],
    u'\u0174' : ['(##)','Ŵ'],
    u'\u0175' : ['(##)','ŵ'],
    u'\u0176' : ['(##)','Ŷ'],
    u'\u0177' : ['(##)','ŷ'],
    u'\u0178' : ['(##)','Ÿ'],
    u'\u0179' : ['(##)','Ź'],
    u'\u017A' : ['(##)','ź'],
    u'\u017B' : ['(##)','Ż'],
    u'\u017C' : ['(##)','ż'],
    u'\u017D' : ['(##)','Ž'],
    u'\u017E' : ['(##)','ž'],
    u'\u017F' : ['(##)','ſ'],
    u'\u0180' : ['(##)','ƀ'],
    u'\u0181' : ['(##)','Ɓ'],
    u'\u0182' : ['(##)','Ƃ'],
    u'\u0183' : ['(##)','ƃ'],
    u'\u0184' : ['(##)','Ƅ'],
    u'\u0185' : ['(##)','ƅ'],
    u'\u0186' : ['(##)','Ɔ'],
    u'\u0187' : ['(##)','Ƈ'],
    u'\u0188' : ['(##)','ƈ'],
    u'\u0189' : ['(##)','Ɖ'],
    u'\u018A' : ['(##)','Ɗ'],
    u'\u018B' : ['(##)','Ƌ'],
    u'\u018C' : ['(##)','ƌ'],
    u'\u018D' : ['(##)','ƍ'],
    u'\u018E' : ['(##)','Ǝ'],
    u'\u018F' : ['(##)','Ə'],
    u'\u0190' : ['(##)','Ɛ'],
    u'\u0191' : ['(##)','Ƒ'],
    u'\u0192' : ['(##)','ƒ'],
    u'\u0193' : ['(##)','Ɠ'],
    u'\u0194' : ['(##)','Ɣ'],
    u'\u0195' : ['(##)','ƕ'],
    u'\u0196' : ['(##)','Ɩ'],
    u'\u0197' : ['(##)','Ɨ'],
    u'\u0198' : ['(##)','Ƙ'],
    u'\u0199' : ['(##)','ƙ'],
    u'\u019A' : ['(##)','ƚ'],
    u'\u019B' : ['(##)','ƛ'],
    u'\u019C' : ['(##)','Ɯ'],
    u'\u019D' : ['(##)','Ɲ'],
    u'\u019E' : ['(##)','ƞ'],
    u'\u019F' : ['(##)','Ɵ'],
    u'\u01A0' : ['(##)','Ơ'],
    u'\u01A1' : ['(##)','ơ'],
    u'\u01A2' : ['(##)','Ƣ'],
    u'\u01A3' : ['(##)','ƣ'],
    u'\u01A4' : ['(##)','Ƥ'],
    u'\u01A5' : ['(##)','ƥ'],
    u'\u01A6' : ['(##)','Ʀ'],
    u'\u01A7' : ['(##)','Ƨ'],
    u'\u01A8' : ['(##)','ƨ'],
    u'\u01A9' : ['(##)','Ʃ'],
    u'\u01AA' : ['(##)','ƪ'],
    u'\u01AB' : ['(##)','ƫ'],
    u'\u01AC' : ['(##)','Ƭ'],
    u'\u01AD' : ['(##)','ƭ'],
    u'\u01AE' : ['(##)','Ʈ'],
    u'\u01AF' : ['(##)','Ư'],
    u'\u01B0' : ['(##)','ư'],
    u'\u01B1' : ['(##)','Ʊ'],
    u'\u01B2' : ['(##)','Ʋ'],
    u'\u01B3' : ['(##)','Ƴ'],
    u'\u01B4' : ['(##)','ƴ'],
    u'\u01B5' : ['(##)','Ƶ'],
    u'\u01B6' : ['(##)','ƶ'],
    u'\u01B7' : ['(##)','Ʒ'],
    u'\u01B8' : ['(##)','Ƹ'],
    u'\u01B9' : ['(##)','ƹ'],
    u'\u01BA' : ['(##)','ƺ'],
    u'\u01BB' : ['(##)','ƻ'],
    u'\u01BC' : ['(##)','Ƽ'],
    u'\u01BD' : ['(##)','ƽ'],
    u'\u01BE' : ['(##)','ƾ'],
    u'\u01BF' : ['(##)','ƿ'],
    u'\u01C0' : ['(##)','ǀ'],
    u'\u01C1' : ['(##)','ǁ'],
    u'\u01C2' : ['(##)','ǂ'],
    u'\u01C3' : ['(##)','ǃ'],
    u'\u01C4' : ['(##)','Ǆ'],
    u'\u01C5' : ['(##)','ǅ'],
    u'\u01C6' : ['(##)','ǆ'],
    u'\u01C7' : ['(##)','Ǉ'],
    u'\u01C8' : ['(##)','ǈ'],
    u'\u01C9' : ['(##)','ǉ'],
    u'\u01CA' : ['(##)','Ǌ'],
    u'\u01CB' : ['(##)','ǋ'],
    u'\u01CC' : ['(##)','ǌ'],
    u'\u01CD' : ['(##)','Ǎ'],
    u'\u01CE' : ['(##)','ǎ'],
    u'\u01CF' : ['(##)','Ǐ'],
    u'\u01D0' : ['(##)','ǐ'],
    u'\u01D1' : ['(##)','Ǒ'],
    u'\u01D2' : ['(##)','ǒ'],
    u'\u01D3' : ['(##)','Ǔ'],
    u'\u01D4' : ['(##)','ǔ'],
    u'\u01D5' : ['(##)','Ǖ'],
    u'\u01D6' : ['(##)','ǖ'],
    u'\u01D7' : ['(##)','Ǘ'],
    u'\u01D8' : ['(##)','ǘ'],
    u'\u01D9' : ['(##)','Ǚ'],
    u'\u01DA' : ['(##)','ǚ'],
    u'\u01DB' : ['(##)','Ǜ'],
    u'\u01DC' : ['(##)','ǜ'],
    u'\u01DD' : ['(##)','ǝ'],
    u'\u01DE' : ['(##)','Ǟ'],
    u'\u01DF' : ['(##)','ǟ'],
    u'\u01E0' : ['(##)','Ǡ'],
    u'\u01E1' : ['(##)','ǡ'],
    u'\u01E2' : ['(##)','Ǣ'],
    u'\u01E3' : ['(##)','ǣ'],
    u'\u01E4' : ['(##)','Ǥ'],
    u'\u01E5' : ['(##)','ǥ'],
    u'\u01E6' : ['(##)','Ǧ'],
    u'\u01E7' : ['(##)','ǧ'],
    u'\u01E8' : ['(##)','Ǩ'],
    u'\u01E9' : ['(##)','ǩ'],
    u'\u01EA' : ['(##)','Ǫ'],
    u'\u01EB' : ['(##)','ǫ'],
    u'\u01EC' : ['(##)','Ǭ'],
    u'\u01ED' : ['(##)','ǭ'],
    u'\u01EE' : ['(##)','Ǯ'],
    u'\u01EF' : ['(##)','ǯ'],
    u'\u01F0' : ['(##)','ǰ'],
    u'\u01E1' : ['(##)','ǡ'],
    u'\u01E2' : ['(##)','Ǣ'],
    u'\u01E3' : ['(##)','ǣ'],
    u'\u01E4' : ['(##)','Ǥ'],
    u'\u01E5' : ['(##)','ǥ'],
    u'\u01E6' : ['(##)','Ǧ'],
    u'\u01E7' : ['(##)','ǧ'],
    u'\u01E8' : ['(##)','Ǩ'],
    u'\u01E9' : ['(##)','ǩ'],
    u'\u01EA' : ['(##)','Ǫ'],
    u'\u01EB' : ['(##)','ǫ'],
    u'\u01EC' : ['(##)','Ǭ'],
    u'\u01ED' : ['(##)','ǭ'],
    u'\u01EE' : ['(##)','Ǯ'],
    u'\u01EF' : ['(##)','ǯ'],
    u'\u0200' : ['(##)','Ȁ'],
    u'\u0201' : ['(##)','ȁ'],
    u'\u0202' : ['(##)','Ȃ'],
    u'\u0203' : ['(##)','ȃ'],
    u'\u0204' : ['(##)','Ȅ'],
    u'\u0205' : ['(##)','ȅ'],
    u'\u0206' : ['(##)','Ȇ'],
    u'\u0207' : ['(##)','ȇ'],
    u'\u0208' : ['(##)','Ȉ'],
    u'\u0209' : ['(##)','ȉ'],
    u'\u020A' : ['(##)','Ȋ'],
    u'\u020B' : ['(##)','ȋ'],
    u'\u020C' : ['(##)','Ȍ'],
    u'\u020D' : ['(##)','ȍ'],
    u'\u020E' : ['(##)','Ȏ'],
    u'\u020F' : ['(##)','ȏ'],
    u'\u0210' : ['(##)','Ȑ'],
    u'\u0211' : ['(##)','ȑ'],
    u'\u0212' : ['(##)','Ȓ'],
    u'\u0213' : ['(##)','ȓ'],
    u'\u0214' : ['(##)','Ȕ'],
    u'\u0215' : ['(##)','ȕ'],
    u'\u0216' : ['(##)','Ȗ'],
    u'\u0217' : ['(##)','ȗ'],
    u'\u0218' : ['(##)','Ș'],
    u'\u0219' : ['(##)','ș'],
    u'\u021A' : ['(##)','Ț'],
    u'\u021B' : ['(##)','ț'],
    u'\u021C' : ['(##)','Ȝ'],
    u'\u021D' : ['(##)','ȝ'],
    u'\u021E' : ['(##)','Ȟ'],
    u'\u021F' : ['(##)','ȟ'],
    u'\u0220' : ['(##)','Ƞ'],
    u'\u0221' : ['(##)','ȡ'],
    u'\u0222' : ['(##)','Ȣ'],
    u'\u0223' : ['(##)','ȣ'],
    u'\u0224' : ['(##)','Ȥ'],
    u'\u0225' : ['(##)','ȥ'],
    u'\u0226' : ['(##)','Ȧ'],
    u'\u0227' : ['(##)','ȧ'],
    u'\u0228' : ['(##)','Ȩ'],
    u'\u0229' : ['(##)','ȩ'],
    u'\u022A' : ['(##)','Ȫ'],
    u'\u022B' : ['(##)','ȫ'],
    u'\u022C' : ['(##)','Ȭ'],
    u'\u022D' : ['(##)','ȭ'],
    u'\u022E' : ['(##)','Ȯ'],
    u'\u022F' : ['(##)','ȯ'],
    u'\u0230' : ['(##)','Ȱ'],
    u'\u0231' : ['(##)','ȱ'],
    u'\u0232' : ['(##)','Ȳ'],
    u'\u0233' : ['(##)','ȳ'],
    u'\u0234' : ['(##)','ȴ'],
    u'\u0235' : ['(##)','ȵ'],
    u'\u0236' : ['(##)','ȶ'],
    u'\u0237' : ['(##)','ȷ'],
    u'\u0238' : ['(##)','ȸ'],
    u'\u0239' : ['(##)','ȹ'],
    u'\u023A' : ['(##)','Ⱥ'],
    u'\u023B' : ['(##)','Ȼ'],
    u'\u023C' : ['(##)','ȼ'],
    u'\u023D' : ['(##)','Ƚ'],
    u'\u023E' : ['(##)','Ⱦ'],
    u'\u023F' : ['(##)','ȿ'],
    u'\u0240' : ['(##)','ɀ'],
    u'\u0241' : ['(##)','Ɂ'],
    u'\u0242' : ['(##)','ɂ'],
    u'\u0243' : ['(##)','Ƀ'],
    u'\u0244' : ['(##)','Ʉ'],
    u'\u0245' : ['(##)','Ʌ'],
    u'\u0246' : ['(##)','Ɇ'],
    u'\u0247' : ['(##)','ɇ'],
    u'\u0248' : ['(##)','Ɉ'],
    u'\u0249' : ['(##)','ɉ'],
    u'\u024A' : ['(##)','Ɋ'],
    u'\u024B' : ['(##)','ɋ'],
    u'\u024C' : ['(##)','Ɍ'],
    u'\u024D' : ['(##)','ɍ'],
    u'\u024E' : ['(##)','Ɏ'],
    u'\u024F' : ['(##)','ɏ'],
    u'\u2C60' : ['(##)','Ⱡ'],
    u'\u2C61' : ['(##)','ⱡ'],
    u'\u2C62' : ['(##)','Ɫ'],
    u'\u2C63' : ['(##)','Ᵽ'],
    u'\u2C64' : ['(##)','Ɽ'],
    u'\u2C65' : ['(##)','ⱥ'],
    u'\u2C66' : ['(##)','ⱦ'],
    u'\u2C67' : ['(##)','Ⱨ'],
    u'\u2C68' : ['(##)','ⱨ'],
    u'\u2C69' : ['(##)','Ⱪ'],
    u'\u2C6A' : ['(##)','ⱪ'],
    u'\u2C6B' : ['(##)','Ⱬ'],
    u'\u2C6C' : ['(##)','ⱬ'],
    u'\u2C74' : ['(##)','ⱴ'],
    u'\u2C75' : ['(##)','Ⱶ'],
    u'\u2C76' : ['(##)','ⱶ'],
    u'\u2C77' : ['(##)','ⱷ'],
    u'\uA720' : ['(##)','꜠'],
    u'\uA721' : ['(##)','꜡'],
    u'\u2000' : ['(##)',' '],
    u'\u2001' : ['(##)',' '],
    u'\u2002' : ['(##)',' '],
    u'\u2003' : ['(##)',' '],
    u'\u2004' : ['(##)',' '],
    u'\u2005' : ['(##)',' '],
    u'\u2006' : ['(##)',' '],
    u'\u2007' : ['(##)',' '],
    u'\u2008' : ['(##)',' '],
    u'\u2009' : ['(##)',' '],
    u'\u200A' : ['(##)',' '],
    u'\u200B' : ['(##)','​'],
    u'\u200C' : ['(##)','‌'],
    u'\u200D' : ['(##)','‍'],
    u'\u200E' : ['(##)','‎'],
    u'\u200F' : ['(##)','‏'],
    u'\u2010' : ['(##)','‐'],
    u'\u2011' : ['(##)','‑'],
    u'\u2012' : ['(##)','‒'],
    u'\u2013' : ['(##)','–'],
    u'\u2014' : ['(##)','—'],
    u'\u2015' : ['(##)','―'],
    u'\u2016' : ['(##)','‖'],
    u'\u2017' : ['(##)','‗'],
    u'\u2018' : ['(##)','‘'],
    u'\u2019' : ['(##)','’'],
    u'\u201A' : ['(##)','‚'],
    u'\u201B' : ['(##)','‛'],
    u'\u201C' : ['(##)','“'],
    u'\u201D' : ['(##)','”'],
    u'\u201E' : ['(##)','„'],
    u'\u201F' : ['(##)','‟'],
    u'\u2020' : ['(##)','†'],
    u'\u2021' : ['(##)','‡'],
    u'\u2022' : ['(##)','•'],
    u'\u2023' : ['(##)','‣'],
    u'\u2024' : ['(##)','․'],
    u'\u2025' : ['(##)','‥'],
    u'\u2026' : ['(##)','…'],
    u'\u2027' : ['(##)','‧'],
    u'\u2028' : ['(##)',' '],
    u'\u2029' : ['(##)',' '],
    u'\u202A' : ['(##)','‪'],
    u'\u202B' : ['(##)','‫'],
    u'\u202C' : ['(##)','‬'],
    u'\u202D' : ['(##)','‭'],
    u'\u202E' : ['(##)','‮'],
    u'\u202F' : ['(##)',' '],
    u'\u2030' : ['(##)','‰'],
    u'\u2031' : ['(##)','‱'],
    u'\u2032' : ['(##)','′'],
    u'\u2033' : ['(##)','″'],
    u'\u2034' : ['(##)','‴'],
    u'\u2035' : ['(##)','‵'],
    u'\u2036' : ['(##)','‶'],
    u'\u2037' : ['(##)','‷'],
    u'\u2038' : ['(##)','‸'],
    u'\u2039' : ['(##)','‹'],
    u'\u203A' : ['(##)','›'],
    u'\u203B' : ['(##)','※'],
    u'\u203C' : ['(##)','‼'],
    u'\u203D' : ['(##)','‽'],
    u'\u203E' : ['(##)','‾'],
    u'\u203F' : ['(##)','‿'],
    u'\u2040' : ['(##)','⁀'],
    u'\u2041' : ['(##)','⁁'],
    u'\u2042' : ['(##)','⁂'],
    u'\u2043' : ['(##)','⁃'],
    u'\u2044' : ['(##)','⁄'],
    u'\u2045' : ['(##)','⁅'],
    u'\u2046' : ['(##)','⁆'],
    u'\u2047' : ['(##)','⁇'],
    u'\u2048' : ['(##)','⁈'],
    u'\u2049' : ['(##)','⁉'],
    u'\u204A' : ['(##)','⁊'],
    u'\u204B' : ['(##)','⁋'],
    u'\u204C' : ['(##)','⁌'],
    u'\u204D' : ['(##)','⁍'],
    u'\u204E' : ['(##)','⁎'],
    u'\u204F' : ['(##)','⁏'],
    u'\u2050' : ['(##)','⁐'],
    u'\u2051' : ['(##)','⁑'],
    u'\u2052' : ['(##)','⁒'],
    u'\u2053' : ['(##)','⁓'],
    u'\u2054' : ['(##)','⁔'],
    u'\u2055' : ['(##)','⁕'],
    u'\u2056' : ['(##)','⁖'],
    u'\u2057' : ['(##)','⁗'],
    u'\u2058' : ['(##)','⁘'],
    u'\u2059' : ['(##)','⁙'],
    u'\u205A' : ['(##)','⁚'],
    u'\u205B' : ['(##)','⁛'],
    u'\u205C' : ['(##)','⁜'],
    u'\u205D' : ['(##)','⁝'],
    u'\u205E' : ['(##)','⁞'],
    u'\u205F' : ['(##)',' '],
    u'\u2060' : ['(##)','⁠'],
    u'\u2061' : ['(##)','⁡'],
    u'\u2062' : ['(##)','⁢'],
    u'\u2063' : ['(##)','⁣'],
    u'\u2064' : ['(##)','⁤'],
    u'\u2065' : ['(##)','⁥'],
    u'\u2066' : ['(##)','⁦'],
    u'\u2067' : ['(##)','⁧'],
    u'\u2068' : ['(##)','⁨'],
    u'\u2069' : ['(##)','⁩'],
    u'\u206A' : ['(##)','⁪'],
    u'\u206B' : ['(##)','⁫'],
    u'\u206C' : ['(##)','⁬'],
    u'\u206D' : ['(##)','⁭'],
    u'\u206E' : ['(##)','⁮'],
    u'\u206F' : ['(##)','⁯'],
    u'\u2100' : ['(##)','℀'],
    u'\u2101' : ['(##)','℁'],
    u'\u2102' : ['(##)','ℂ'],
    u'\u2103' : ['(##)','℃'],
    u'\u2104' : ['(##)','℄'],
    u'\u2105' : ['(##)','℅'],
    u'\u2106' : ['(##)','℆'],
    u'\u2107' : ['(##)','ℇ'],
    u'\u2108' : ['(##)','℈'],
    u'\u2109' : ['(##)','℉'],
    u'\u210A' : ['(##)','ℊ'],
    u'\u210B' : ['(##)','ℋ'],
    u'\u210C' : ['(##)','ℌ'],
    u'\u210D' : ['(##)','ℍ'],
    u'\u210E' : ['(##)','ℎ'],
    u'\u210F' : ['(##)','ℏ'],
    u'\u2110' : ['(##)','ℐ'],
    u'\u2111' : ['(##)','ℑ'],
    u'\u2112' : ['(##)','ℒ'],
    u'\u2113' : ['(##)','ℓ'],
    u'\u2114' : ['(##)','℔'],
    u'\u2115' : ['(##)','ℕ'],
    u'\u2116' : ['(##)','№'],
    u'\u2117' : ['(##)','℗'],
    u'\u2118' : ['(##)','℘'],
    u'\u2119' : ['(##)','ℙ'],
    u'\u211A' : ['(##)','ℚ'],
    u'\u211B' : ['(##)','ℛ'],
    u'\u211C' : ['(##)','ℜ'],
    u'\u211D' : ['(##)','ℝ'],
    u'\u211E' : ['(##)','℞'],
    u'\u211F' : ['(##)','℟'],
    u'\u2120' : ['(##)','℠'],
    u'\u2121' : ['(##)','℡'],
    u'\u2122' : ['(##)','™'],
    u'\u2123' : ['(##)','℣'],
    u'\u2124' : ['(##)','ℤ'],
    u'\u2125' : ['(##)','℥'],
    u'\u2126' : ['(##)','Ω'],
    u'\u2127' : ['(##)','℧'],
    u'\u2128' : ['(##)','ℨ'],
    u'\u2129' : ['(##)','℩'],
    u'\u212A' : ['(##)','K'],
    u'\u212B' : ['(##)','Å'],
    u'\u212C' : ['(##)','ℬ'],
    u'\u212D' : ['(##)','ℭ'],
    u'\u212E' : ['(##)','℮'],
    u'\u212F' : ['(##)','ℯ'],
    u'\u2130' : ['(##)','ℰ'],
    u'\u2131' : ['(##)','ℱ'],
    u'\u2132' : ['(##)','Ⅎ'],
    u'\u2133' : ['(##)','ℳ'],
    u'\u2134' : ['(##)','ℴ'],
    u'\u2135' : ['(##)','ℵ'],
    u'\u2136' : ['(##)','ℶ'],
    u'\u2137' : ['(##)','ℷ'],
    u'\u2138' : ['(##)','ℸ'],
    u'\u2139' : ['(##)','ℹ'],
    u'\u213A' : ['(##)','℺'],
    u'\u213B' : ['(##)','℻'],
    u'\u213C' : ['(##)','ℼ'],
    u'\u213D' : ['(##)','ℽ'],
    u'\u213E' : ['(##)','ℾ'],
    u'\u213F' : ['(##)','ℿ'],
    u'\u2140' : ['(##)','⅀'],
    u'\u2141' : ['(##)','⅁'],
    u'\u2142' : ['(##)','⅂'],
    u'\u2143' : ['(##)','⅃'],
    u'\u2144' : ['(##)','⅄'],
    u'\u2145' : ['(##)','ⅅ'],
    u'\u2146' : ['(##)','ⅆ'],
    u'\u2147' : ['(##)','ⅇ'],
    u'\u2148' : ['(##)','ⅈ'],
    u'\u2149' : ['(##)','ⅉ'],
    u'\u214A' : ['(##)','⅊'],
    u'\u214B' : ['(##)','⅋'],
    u'\u214C' : ['(##)','⅌'],
    u'\u214D' : ['(##)','⅍'],
    u'\u214E' : ['(##)','ⅎ'],
    u'\u214F' : ['(##)','⅏'],
    u'\uFE50' : ['(##)','﹐'],
    u'\uFE51' : ['(##)','﹑'],
    u'\uFE52' : ['(##)','﹒'],
    u'\uFE54' : ['(##)','﹔'],
    u'\uFE55' : ['(##)','﹕'],
    u'\uFE56' : ['(##)','﹖'],
    u'\uFE57' : ['(##)','﹗'],
    u'\uFE58' : ['(##)','﹘'],
    u'\uFE59' : ['(##)','﹙'],
    u'\uFE50' : ['(##)','﹐'],
    u'\uFE5A' : ['(##)','﹚'],
    u'\uFE5B' : ['(##)','﹛'],
    u'\uFE5C' : ['(##)','﹜'],
    u'\uFE5D' : ['(##)','﹝'],
    u'\uFE5E' : ['(##)','﹞'],
    u'\uFE5F' : ['(##)','﹟'],
    u'\uFE60' : ['(##)','﹠'],
    u'\uFE61' : ['(##)','﹡'],
    u'\uFE62' : ['(##)','﹢'],
    u'\uFE63' : ['(##)','﹣'],
    u'\uFE64' : ['(##)','﹤'],
    u'\uFE65' : ['(##)','﹥'],
    u'\uFE66' : ['(##)','﹦'],
    u'\uFE68' : ['(##)','﹨'],
    u'\uFE69' : ['(##)','﹩'],
    u'\uFE6A' : ['(##)','﹪'],
    u'\uFE6B' : ['(##)','﹫'],
    u'\u1800' : ['(##)','᠀'],
    u'\u1801' : ['(##)','᠁'],
    u'\u1802' : ['(##)','᠂'],
    u'\u1803' : ['(##)','᠃'],
    u'\u1804' : ['(##)','᠄'],
    u'\u1805' : ['(##)','᠅'],
    u'\u1806' : ['(##)','᠆'],
    u'\u1807' : ['(##)','᠇'],
    u'\u1808' : ['(##)','᠈'],
    u'\u1809' : ['(##)','᠉'],
    u'\u180A' : ['(##)','᠊'],
    u'\u180B' : ['(##)','᠋'],
    u'\u180C' : ['(##)','᠌'],
    u'\u180D' : ['(##)','᠍'],
    u'\u180E' : ['(##)','᠎'],
    u'\u1810' : ['(ZERO)','᠐'],
    u'\u1811' : ['(ONE)','᠑'],
    u'\u1812' : ['(TWO)','᠒'],
    u'\u1813' : ['(THREE)','᠓'],
    u'\u1814' : ['(FOUR)','᠔'],
    u'\u1815' : ['(FIVE)','᠕'],
    u'\u1816' : ['(SIX)','᠖'],
    u'\u1817' : ['(SEVEN)','᠗'],
    u'\u1818' : ['(EIGHT)','᠘'],
    u'\u1819' : ['(NINE)','᠙'],
    u'\u1820' : ['A','ᠠ'],
    u'\u1821' : ['E','ᠡ'],
    u'\u1822' : ['i','ᠢ'],
    u'\u1823' : ['>','ᠣ'],
    u'\u1824' : ['u','ᠤ'],
    u'\u1825' : ['o','ᠥ'],
    u'\u1826' : ['u','ᠦ'],
    u'\u1827' : ['E:','ᠧ'],
    u'\u1828' : ['n A','ᠨ'],
    u'\u1829' : ['N','ᠩ'],
    u'\u182A' : ['b A','ᠪ'],
    u'\u182B' : ['p A','ᠫ'],
    u'\u182C' : ['x A','ᠬ'],
    u'\u182D' : ['g A','ᠭ'],
    u'\u182E' : ['m A','ᠮ'],
    u'\u182F' : ['l A','ᠯ'],
    u'\u1830' : ['s A','ᠰ'],
    u'\u1831' : ['S A','ᠱ'],
    u'\u1832' : ['t A','ᠲ'],
    u'\u1833' : ['d A','ᠳ'],
    u'\u1834' : ['tSj A','ᠴ'],
    u'\u1835' : ['Z A','ᠵ'],
    u'\u1836' : ['j A','ᠶ'],
    u'\u1837' : ['r A','ᠷ'],
    u'\u1838' : ['v A','ᠸ'],
    u'\u1839' : ['f A','ᠹ'],
    u'\u183A' : ['x A','ᠺ'],
    u'\u183B' : ['k A','ᠻ'],
    u'\u183C' : ['ts A','ᠼ'],
    u'\u183D' : ['z A','ᠽ'],
    u'\u183E' : ['x A:','ᠾ'],
    u'\u183F' : ['Z A','ᠿ'],
    u'\u1840' : ['hl A','ᡀ'],
    u'\u1841' : ['S A','ᡁ'],
    u'\u1842' : ['tS j','ᡂ'],
    u'\u1843' : ['(:)','ᡃ'],
    u'\u1844' : ['E','ᡄ'],
    u'\u1845' : ['i','ᡅ'],
    u'\u1846' : ['o','ᡆ'],
    u'\u1847' : ['u','ᡇ'],
    u'\u1848' : ['o','ᡈ'],
    u'\u1849' : ['u','ᡉ'],
    u'\u184A' : ['N','ᡊ'],
    u'\u184B' : ['b A','ᡋ'],
    u'\u184C' : ['p A','ᡌ'],
    u'\u184D' : ['x A','ᡍ'],
    u'\u184E' : ['g A','ᡎ'],
    u'\u184F' : ['m A','ᡏ'],
    u'\u1850' : ['t A','ᡐ'],
    u'\u1851' : ['d A','ᡑ'],
    u'\u1852' : ['tSj A','ᡒ'],
    u'\u1853' : ['Z A','ᡓ'],
    u'\u1854' : ['ts A','ᡔ'],
    u'\u1855' : ['j A','ᡕ'],
    u'\u1856' : ['w A','ᡖ'],
    u'\u1857' : ['k A','ᡗ'],
    u'\u1858' : ['g A:','ᡘ'],
    u'\u1859' : ['x A:','ᡙ'],
    u'\u185A' : ['Z A','ᡚ'],
    u'\u185B' : ['n A','ᡛ'],
    u'\u185C' : ['dz A','ᡜ'],
    u'\u185D' : ['E','ᡝ'],
    u'\u185E' : ['i','ᡞ'],
    u'\u185F' : ['j','ᡟ'],
    u'\u1860' : ['u','ᡠ'],
    u'\u1861' : ['u','ᡡ'],
    u'\u1862' : ['N','ᡢ'],
    u'\u1863' : ['k A','ᡣ'],
    u'\u1864' : ['g A','ᡤ'],
    u'\u1865' : ['x A','ᡥ'],
    u'\u1866' : ['p A','ᡦ'],
    u'\u1867' : ['S A','ᡧ'],
    u'\u1868' : ['t A','ᡨ'],
    u'\u1869' : ['d A','ᡩ'],
    u'\u186A' : ['Z A','ᡪ'],
    u'\u186B' : ['f A','ᡫ'],
    u'\u186C' : ['g A:','ᡬ'],
    u'\u186D' : ['x A:','ᡭ'],
    u'\u186E' : ['ts A','ᡮ'],
    u'\u186F' : ['z A','ᡯ'],
    u'\u1870' : ['r A:','ᡰ'],
    u'\u1871' : ['tSj A','ᡱ'],
    u'\u1872' : ['S A','ᡲ'],
    u'\u1873' : ['i','ᡳ'],
    u'\u1874' : ['k A','ᡴ'],
    u'\u1875' : ['f A','ᡵ'],
    u'\u1876' : ['S A','ᡶ'],
    u'\u1877' : ['(##)','ᡷ'],
    u'\u1880' : ['(##)','ᢀ'],
    u'\u1881' : ['(##)','ᢁ'],
    u'\u1882' : ['(##)','ᢂ'],
    u'\u1883' : ['(##)','ᢃ'],
    u'\u1884' : ['(##)','ᢄ'],
    u'\u1885' : ['(##)','ᢅ'],
    u'\u1886' : ['(##)','ᢆ'],
    u'\u1887' : ['A','ᢇ'],
    u'\u1888' : ['i','ᢈ'],
    u'\u1889' : ['k A','ᢉ'],
    u'\u188A' : ['N A','ᢊ'],
    u'\u188B' : ['tS A','ᢋ'],
    u'\u188C' : ['t t A','ᢌ'],
    u'\u188D' : ['t th A','ᢍ'],
    u'\u188E' : ['t t A','ᢎ'],
    u'\u188F' : ['n n A','ᢏ'],
    u'\u1890' : ['t A','ᢐ'],
    u'\u1891' : ['t A','ᢑ'],
    u'\u1892' : ['p A','ᢒ'],
    u'\u1893' : ['ph A','ᢓ'],
    u'\u1894' : ['S S A','ᢔ'],
    u'\u1895' : ['S A','ᢕ'],
    u'\u1896' : ['z A','ᢖ'],
    u'\u1897' : ['Ah','ᢗ'],
    u'\u1898' : ['t A','ᢘ'],
    u'\u1899' : ['S A','ᢙ'],
    u'\u189A' : ['Gh A','ᢚ'],
    u'\u189B' : ['N A','ᢛ'],
    u'\u189C' : ['tS A','ᢜ'],
    u'\u189D' : ['zh A','ᢝ'],
    u'\u189E' : ['t t A','ᢞ'],
    u'\u189F' : ['th A','ᢟ'],
    u'\u18A0' : ['t A','ᢠ'],
    u'\u18A1' : ['th A','ᢡ'],
    u'\u18A2' : ['S S A','ᢢ'],
    u'\u18A3' : ['tSj A','ᢣ'],
    u'\u18A4' : ['S A','ᢤ'],
    u'\u18A5' : ['s A','ᢥ'],
    u'\u18A6' : ['u','ᢦ'],
    u'\u18A7' : ['j A','ᢧ'],
    u'\u18A8' : ['(BHA)','ᢨ'],
    u'\u18A9' : ['(DAGALGA)','ᢩ'],
    u'\u1000' : ['k A','က'],
    u'\u1001' : ['kh A','ခ'],
    u'\u1002' : ['g A','ဂ'],
    u'\u1003' : ['g A','ဃ'],
    u'\u1004' : ['N A','င'],
    u'\u1005' : ['s A','စ'],
    u'\u1006' : ['sh A','ဆ'],
    u'\u1007' : ['z A','ဇ'],
    u'\u1008' : ['z A','ဈ'],
    u'\u1009' : ['n~ A','ဉ'],
    u'\u100A' : ['n~ A','ည'],
    u'\u100B' : ['tr A','ဋ'],
    u'\u100C' : ['trh A','ဌ'],
    u'\u100D' : ['d A','ဍ'],
    u'\u100E' : ['d A','ဎ'],
    u'\u100F' : ['n A','ဏ'],
    u'\u1010' : ['tr A','တ'],
    u'\u1011' : ['trh A','ထ'],
    u'\u1012' : ['d A','ဒ'],
    u'\u1013' : ['d A','ဓ'],
    u'\u1014' : ['n A','န'],
    u'\u1015' : ['p A','ပ'],
    u'\u1016' : ['ph A','ဖ'],
    u'\u1017' : ['b A','ဗ'],
    u'\u1018' : ['bh A','ဘ'],
    u'\u1019' : ['m A','မ'],
    u'\u101A' : ['j A','ယ'],
    u'\u101B' : ['j A','ရ'],
    u'\u101C' : ['l A','လ'],
    u'\u101D' : ['w A','ဝ'],
    u'\u101E' : ['T A','သ'],
    u'\u101F' : ['h A','ဟ'],
    u'\u1020' : ['l A','ဠ'],
    u'\u1021' : ['A, ?','အ'],
    u'\u1023' : ['i','ဣ'],
    u'\u1024' : ['i','ဤ'],
    u'\u1025' : ['u','ဥ'],
    u'\u1026' : ['u','ဦ'],
    u'\u1027' : ['e','ဧ'],
    u'\u1029' : ['o','ဩ'],
    u'\u102A' : ['>','ဪ'],
    u'\u102C' : ['A','ာ'],
    u'\u102D' : ['i','ိ'],
    u'\u102E' : ['i','ီ'],
    u'\u102F' : ['u','ု'],
    u'\u1030' : ['u','ူ'],
    u'\u1031' : ['e','ေ'],
    u'\u1032' : ['E','ဲ'],
    u'\u1036' : ['(M)','ံ'],
    u'\u1037' : ['(TONE MARK)','့'],
    u'\u1038' : ['(H)','း'],
    u'\u1039' : ['(P)','္'],
    u'\u1040' : ['(ZERO)','၀'],
    u'\u1041' : ['(ONE)','၁'],
    u'\u1042' : ['(TWO)','၂'],
    u'\u1043' : ['(THREE)','၃'],
    u'\u1044' : ['(FOUR)','၄'],
    u'\u1045' : ['(FIVE)','၅'],
    u'\u1046' : ['(SIX)','၆'],
    u'\u1047' : ['(SEVEN)','၇'],
    u'\u1048' : ['(EIGHT)','၈'],
    u'\u1049' : ['(NINE)','၉'],
    u'\u104A' : ['(DANDA)','၊'],
    u'\u104B' : ['(DOUBLE DANDA)','။'],
    u'\u104C' : ['(LOCATIVE)','၌'],
    u'\u104D' : ['(COMPLETED)','၍'],
    u'\u104E' : ['(AFOREMENTIONED)','၎'],
    u'\u104F' : ['(GENETIVE)','၏'],
    u'\u1050' : ['sh','ၐ'],
    u'\u1051' : ['sr','ၑ'],
    u'\u1052' : ['r=','ၒ'],
    u'\u1053' : ['rr=','ၓ'],
    u'\u1054' : ['l=','ၔ'],
    u'\u1055' : ['l=','ၕ'],
    u'\u1056' : ['r=','ၖ'],
    u'\u1057' : ['rr=','ၗ'],
    u'\u1058' : ['l=','ၘ'],
    u'\u1059' : ['l=','ၙ'],
    u'\u0B01' : ['(CD)','ଁ'],
    u'\u0B02' : ['(M)','ଂ'],
    u'\u0B03' : ['(H)','ଃ'],
    u'\u0B05' : ['>','ଅ'],
    u'\u0B06' : ['A','ଆ'],
    u'\u0B07' : ['i','ଇ'],
    u'\u0B08' : ['i:','ଈ'],
    u'\u0B09' : ['u','ଉ'],
    u'\u0B0A' : ['u:','ଊ'],
    u'\u0B0B' : ['9r=','ଋ'],
    u'\u0B0C' : ['l=','ଌ'],
    u'\u0B0E' : ['(R:0B0E)','଎'],
    u'\u0B0F' : ['e','ଏ'],
    u'\u0B10' : ['>I','ଐ'],
    u'\u0B13' : ['o','ଓ'],
    u'\u0B14' : ['aU','ଔ'],
    u'\u0B15' : ['k A','କ'],
    u'\u0B16' : ['kh A','ଖ'],
    u'\u0B17' : ['g A','ଗ'],
    u'\u0B18' : ['gh A','ଘ'],
    u'\u0B19' : ['N A','ଙ'],
    u'\u0B1A' : ['c A','ଚ'],
    u'\u0B1B' : ['ch A','ଛ'],
    u'\u0B1C' : ['J A','ଜ'],
    u'\u0B1D' : ['Jh A','ଝ'],
    u'\u0B1E' : ['n~ A','ଞ'],
    u'\u0B1F' : ['tr A','ଟ'],
    u'\u0B20' : ['tR A','ଠ'],
    u'\u0B21' : ['dr A','ଡ'],
    u'\u0B22' : ['dR A','ଢ'],
    u'\u0B23' : ['nr A','ଣ'],
    u'\u0B24' : ['t[ A','ତ'],
    u'\u0B25' : ['t[_h A','ଥ'],
    u'\u0B26' : ['d[ A','ଦ'],
    u'\u0B27' : ['d[_h A','ଧ'],
    u'\u0B28' : ['n[ A','ନ'],
    u'\u0B29' : ['(R:0B29)','଩'],
    u'\u0B2A' : ['p A','ପ'],
    u'\u0B2B' : ['ph A','ଫ'],
    u'\u0B2C' : ['b A','ବ'],
    u'\u0B2D' : ['bh A','ଭ'],
    u'\u0B2E' : ['m A','ମ'],
    u'\u0B2F' : ['j A','ଯ'],
    u'\u0B30' : ['9r A','ର'],
    u'\u0B31' : ['(R:0B31)','଱'],
    u'\u0B32' : ['l A','ଲ'],
    u'\u0B33' : ['lr A','ଳ'],
    u'\u0B34' : ['(R:0B34)','଴'],
    u'\u0B35' : ['v A','ଵ'],
    u'\u0B36' : ['S A','ଶ'],
    u'\u0B37' : ['sr A','ଷ'],
    u'\u0B38' : ['s A','ସ'],
    u'\u0B39' : ['hv A','ହ'],
    u'\u0B3C' : ['(NUKTA)','଼'],
    u'\u0B3D' : ['(AVAGRAHA)','ଽ'],
    u'\u0B3E' : ['A','ା'],
    u'\u0B3F' : ['i','ି'],
    u'\u0B40' : ['i:','ୀ'],
    u'\u0B41' : ['u','ୁ'],
    u'\u0B42' : ['u:','ୂ'],
    u'\u0B43' : ['9r=','ୃ'],
    u'\u0B45' : ['(R:0B45)','୅'],
    u'\u0B46' : ['(R:0B46)','୆'],
    u'\u0B47' : ['e','େ'],
    u'\u0B48' : ['>I','ୈ'],
    u'\u0B49' : ['(R:0B49)','୉'],
    u'\u0B4B' : ['o','ୋ'],
    u'\u0B4C' : ['aU','ୌ'],
    u'\u0B4D' : ['(P)','୍'],
    u'\u0B56' : ['(>I LENGTH MARK)','ୖ'],
    u'\u0B57' : ['(aU LENGTH MARK)','ୗ'],
    u'\u0B5C' : ['9r: A','ଡ଼'],
    u'\u0B5D' : ['9rh A','ଢ଼'],
    u'\u0B5F' : ['j A','ୟ'],
    u'\u0B60' : ['rr=','ୠ'],
    u'\u0B61' : ['lr=','ୡ'],
    u'\u0B64' : ['(*)','୤'],
    u'\u0B65' : ['(**)','୥'],
    u'\u0B66' : ['(ZERO)','୦'],
    u'\u0B67' : ['(ONE)','୧'],
    u'\u0B68' : ['(TWO)','୨'],
    u'\u0B69' : ['(THREE)','୩'],
    u'\u0B6A' : ['(FOUR)','୪'],
    u'\u0B6B' : ['(FIVE)','୫'],
    u'\u0B6C' : ['(SIX)','୬'],
    u'\u0B6D' : ['(SEVEN)','୭'],
    u'\u0B6E' : ['(EIGHT)','୮'],
    u'\u0B6F' : ['(NINE)','୯'],
    u'\u0B70' : ['(ISSHAR)','୰'],
    u'\u0B71' : ['w A','ୱ'],
    u'\u0D82' : ['(M)','ං'],
    u'\u0D83' : ['(H)','ඃ'],
    u'\u0D85' : ['A, &','අ'],
    u'\u0D86' : ['A:, A','ආ'],
    u'\u0D87' : ['@','ඇ'],
    u'\u0D88' : ['@:','ඈ'],
    u'\u0D89' : ['i','ඉ'],
    u'\u0D8A' : ['i:','ඊ'],
    u'\u0D8B' : ['u','උ'],
    u'\u0D8C' : ['u:','ඌ'],
    u'\u0D8D' : ['9r i, 9r u','ඍ'],
    u'\u0D8E' : ['9r i:, 9r u:','ඎ'],
    u'\u0D8F' : ['l=','ඏ'],
    u'\u0D90' : ['l=','ඐ'],
    u'\u0D91' : ['e','එ'],
    u'\u0D92' : ['e:','ඒ'],
    u'\u0D93' : ['aI','ඓ'],
    u'\u0D94' : ['o','ඔ'],
    u'\u0D95' : ['o:','ඕ'],
    u'\u0D96' : ['aU','ඖ'],
    u'\u0D9A' : ['k A','ක'],
    u'\u0D9B' : ['kh A','ඛ'],
    u'\u0D9C' : ['g A','ග'],
    u'\u0D9D' : ['gh A','ඝ'],
    u'\u0D9E' : ['N A','ඞ'],
    u'\u0D9F' : ['N A','ඟ'],
    u'\u0DA0' : ['c A','ච'],
    u'\u0DA1' : ['ch A','ඡ'],
    u'\u0DA2' : ['J A','ජ'],
    u'\u0DA3' : ['Jh A','ඣ'],
    u'\u0DA4' : ['n~ A','ඤ'],
    u'\u0DA5' : ['J n~ A','ඥ'],
    u'\u0DA6' : ['n~ J A','ඦ'],
    u'\u0DA7' : ['tr A','ට'],
    u'\u0DA8' : ['tR A','ඨ'],
    u'\u0DA9' : ['dr A','ඩ'],
    u'\u0DAA' : ['dR A','ඪ'],
    u'\u0DAB' : ['nr A','ණ'],
    u'\u0DAC' : ['nr dr A','ඬ'],
    u'\u0DAD' : ['t[ A','ත'],
    u'\u0DAE' : ['t[_h A','ථ'],
    u'\u0DAF' : ['d[ A','ද'],
    u'\u0DB0' : ['d[_h A','ධ'],
    u'\u0DB1' : ['n[ A','න'],
    u'\u0DB3' : ['n d A','ඳ'],
    u'\u0DB4' : ['p A','ප'],
    u'\u0DB5' : ['ph A','ඵ'],
    u'\u0DB6' : ['b A','බ'],
    u'\u0DB7' : ['bh A','භ'],
    u'\u0DB8' : ['m A','ම'],
    u'\u0DB9' : ['m b A','ඹ'],
    u'\u0DBA' : ['j A','ය'],
    u'\u0DBB' : ['9r A','ර'],
    u'\u0DBD' : ['l A','ල'],
    u'\u0DC0' : ['v A','ව'],
    u'\u0DC1' : ['S A','ශ'],
    u'\u0DC2' : ['S A','ෂ'],
    u'\u0DC3' : ['s A','ස'],
    u'\u0DC4' : ['hv A','හ'],
    u'\u0DC5' : ['lr A','ළ'],
    u'\u0DC6' : ['f A','ෆ'],
    u'\u0DCA' : ['(P)','්'],
    u'\u0DCF' : ['A:','ා'],
    u'\u0DD0' : ['@','ැ'],
    u'\u0DD1' : ['@:','ෑ'],
    u'\u0DD2' : ['i','ි'],
    u'\u0DD3' : ['i:','ී'],
    u'\u0DD4' : ['u','ු'],
    u'\u0DD6' : ['u:','ූ'],
    u'\u0DD8' : ['9r=','ෘ'],
    u'\u0DD9' : ['e','ෙ'],
    u'\u0DDA' : ['e:','ේ'],
    u'\u0DDB' : ['aI','ෛ'],
    u'\u0DDC' : ['o','ො'],
    u'\u0DDD' : ['o:','ෝ'],
    u'\u0DDE' : ['aU','ෞ'],
    u'\u0DDF' : ['l=','ෟ'],
    u'\u0DF2' : ['rr=','ෲ'],
    u'\u0DF3' : ['l=','ෳ'],
    u'\u0DF4' : ['(PUNCTUATION)','෴'],
    u'\uA800' : ['A','ꠀ'],
    u'\uA801' : ['i','ꠁ'],
    u'\uA802' : ['(DVIAVARA)','ꠂ'],
    u'\uA803' : ['u','ꠃ'],
    u'\uA804' : ['e','ꠄ'],
    u'\uA805' : ['o','ꠅ'],
    u'\uA806' : ['(HASANTA)','꠆'],
    u'\uA807' : ['k A','ꠇ'],
    u'\uA808' : ['kh A','ꠈ'],
    u'\uA809' : ['g A','ꠉ'],
    u'\uA80A' : ['gh A','ꠊ'],
    u'\uA80B' : ['m A','ꠋ'],
    u'\uA80C' : ['c A','ꠌ'],
    u'\uA80D' : ['ch A','ꠍ'],
    u'\uA80E' : ['J A','ꠎ'],
    u'\uA80F' : ['Jh A','ꠏ'],
    u'\uA810' : ['tr A','ꠐ'],
    u'\uA811' : ['tR A','ꠑ'],
    u'\uA812' : ['dr A','ꠒ'],
    u'\uA813' : ['dR A','ꠓ'],
    u'\uA814' : ['t[ A','ꠔ'],
    u'\uA815' : ['t[_h A','ꠕ'],
    u'\uA816' : ['d[ A','ꠖ'],
    u'\uA817' : ['d[_h A','ꠗ'],
    u'\uA818' : ['n[ A','ꠘ'],
    u'\uA819' : ['p A','ꠙ'],
    u'\uA81A' : ['ph A','ꠚ'],
    u'\uA81B' : ['b A','ꠛ'],
    u'\uA81C' : ['bh A','ꠜ'],
    u'\uA81D' : ['m A','ꠝ'],
    u'\uA81E' : ['9r A','ꠞ'],
    u'\uA81F' : ['l A','ꠟ'],
    u'\uA820' : ['9r: A','ꠠ'],
    u'\uA821' : ['s A','ꠡ'],
    u'\uA822' : ['h A','ꠢ'],
    u'\uA823' : ['A','ꠣ'],
    u'\uA824' : ['i','ꠤ'],
    u'\uA825' : ['u','ꠥ'],
    u'\uA826' : ['e','ꠦ'],
    u'\uA827' : ['o:','ꠧ'],
    u'\uA828' : ['(POETRY MARK1)','꠨'],
    u'\uA829' : ['(POETRY MARK2)','꠩'],
    u'\uA82A' : ['(POETRY MARK3)','꠪'],
    u'\u0700' : ['(##)','܀'],
    u'\u0701' : ['(##)','܁'],
    u'\u0702' : ['(##)','܂'],
    u'\u0703' : ['(##)','܃'],
    u'\u0704' : ['(##)','܄'],
    u'\u0705' : ['(##)','܅'],
    u'\u0706' : ['(##)','܆'],
    u'\u0707' : ['(##)','܇'],
    u'\u0708' : ['(##)','܈'],
    u'\u0709' : ['(##)','܉'],
    u'\u070A' : ['(##)','܊'],
    u'\u070B' : ['(##)','܋'],
    u'\u070C' : ['(##)','܌'],
    u'\u070D' : ['(##)','܍'],
    u'\u070F' : ['(##)','܏'],
    u'\u0710' : ['?','ܐ'],
    u'\u0711' : ['?','ܑ'],
    u'\u0712' : ['b, v','ܒ'],
    u'\u0713' : ['g, G','ܓ'],
    u'\u0714' : ['g, G','ܔ'],
    u'\u0715' : ['d, D','ܕ'],
    u'\u0716' : ['d, D','ܖ'],
    u'\u0717' : ['h','ܗ'],
    u'\u0718' : ['w','ܘ'],
    u'\u0719' : ['z','ܙ'],
    u'\u071A' : ['HH','ܚ'],
    u'\u071B' : ['t~','ܛ'],
    u'\u071C' : ['t~','ܜ'],
    u'\u071D' : ['j','ܝ'],
    u'\u071E' : ['j h','ܞ'],
    u'\u071F' : ['k, x','ܟ'],
    u'\u0720' : ['l','ܠ'],
    u'\u0721' : ['m','ܡ'],
    u'\u0722' : ['n','ܢ'],
    u'\u0723' : ['s','ܣ'],
    u'\u0724' : ['s','ܤ'],
    u'\u0725' : ['HH_v','ܥ'],
    u'\u0726' : ['p, f','ܦ'],
    u'\u0727' : ['p, f','ܧ'],
    u'\u0728' : ['s~','ܨ'],
    u'\u0729' : ['q','ܩ'],
    u'\u072A' : ['r','ܪ'],
    u'\u072B' : ['S','ܫ'],
    u'\u072C' : ['t','ܬ'],
    u'\u072D' : ['b, v','ܭ'],
    u'\u072E' : ['g, G','ܮ'],
    u'\u072F' : ['d, D','ܯ'],
    u'\u0730' : ['(##)','ܰ'],
    u'\u0731' : ['(##)','ܱ'],
    u'\u0732' : ['(##)','ܲ'],
    u'\u0733' : ['(##)','ܳ'],
    u'\u0734' : ['(##)','ܴ'],
    u'\u0735' : ['(##)','ܵ'],
    u'\u0736' : ['(##)','ܶ'],
    u'\u0737' : ['(##)','ܷ'],
    u'\u0738' : ['(##)','ܸ'],
    u'\u0739' : ['(##)','ܹ'],
    u'\u073A' : ['(##)','ܺ'],
    u'\u073B' : ['(##)','ܻ'],
    u'\u073C' : ['(##)','ܼ'],
    u'\u073D' : ['(##)','ܽ'],
    u'\u073E' : ['(##)','ܾ'],
    u'\u073F' : ['(##)','ܿ'],
    u'\u0740' : ['(##)','݀'],
    u'\u0741' : ['(##)','݁'],
    u'\u0742' : ['(##)','݂'],
    u'\u0743' : ['(##)','݃'],
    u'\u0744' : ['(##)','݄'],
    u'\u0745' : ['(##)','݅'],
    u'\u0746' : ['(##)','݆'],
    u'\u0747' : ['(##)','݇'],
    u'\u0748' : ['(##)','݈'],
    u'\u0749' : ['(##)','݉'],
    u'\u074A' : ['(##)','݊'],
    u'\u074D' : ['zh','ݍ'],
    u'\u074E' : ['kh','ݎ'],
    u'\u074F' : ['f e','ݏ'],
    u'\u074F' : ['(##)','ݏ'],
    u'\u1700' : ['A','ᜀ'],
    u'\u1701' : ['e, i','ᜁ'],
    u'\u1702' : ['o, u','ᜂ'],
    u'\u1703' : ['k A','ᜃ'],
    u'\u1704' : ['g A','ᜄ'],
    u'\u1705' : ['N A','ᜅ'],
    u'\u1706' : ['t A','ᜆ'],
    u'\u1707' : ['d A','ᜇ'],
    u'\u1708' : ['n A','ᜈ'],
    u'\u1709' : ['p A','ᜉ'],
    u'\u170A' : ['b A','ᜊ'],
    u'\u170B' : ['m A','ᜋ'],
    u'\u170C' : ['j A','ᜌ'],
    u'\u170E' : ['l A','ᜎ'],
    u'\u170F' : ['w A','ᜏ'],
    u'\u1710' : ['s A','ᜐ'],
    u'\u1711' : ['n A','ᜑ'],
    u'\u1712' : ['i','ᜒ'],
    u'\u1713' : ['u','ᜓ'],
    u'\u1714' : ['(P)','᜔'],
    u'\u1760' : ['A','ᝠ'],
    u'\u1761' : ['i','ᝡ'],
    u'\u1762' : ['u','ᝢ'],
    u'\u1763' : ['k A','ᝣ'],
    u'\u1764' : ['g A','ᝤ'],
    u'\u1765' : ['N A','ᝥ'],
    u'\u1766' : ['t A','ᝦ'],
    u'\u1767' : ['d A','ᝧ'],
    u'\u1768' : ['n A','ᝨ'],
    u'\u1769' : ['p A','ᝩ'],
    u'\u176A' : ['b A','ᝪ'],
    u'\u176B' : ['m A','ᝫ'],
    u'\u176C' : ['j A','ᝬ'],
    u'\u176E' : ['l A','ᝮ'],
    u'\u176F' : ['w A','ᝯ'],
    u'\u1770' : ['s A','ᝰ'],
    u'\u1772' : ['i','ᝲ'],
    u'\u1773' : ['u','ᝳ'],
    u'\u0B81' : ['(R:0B81)','஁'],
    u'\u0B82' : ['(M)','ஂ'],
    u'\u0B83' : ['(H)','ஃ'],
    u'\u0B85' : ['>','அ'],
    u'\u0B86' : ['A:','ஆ'],
    u'\u0B87' : ['i','இ'],
    u'\u0B88' : ['i:','ஈ'],
    u'\u0B89' : ['u','உ'],
    u'\u0B8A' : ['u:','ஊ'],
    u'\u0B8B' : ['(R:0B8B)','஋'],
    u'\u0B8D' : ['(R:0B8D)','஍'],
    u'\u0B8E' : ['e','எ'],
    u'\u0B8F' : ['e:','ஏ'],
    u'\u0B90' : ['>I','ஐ'],
    u'\u0B91' : ['(R:0B91)','஑'],
    u'\u0B92' : ['o','ஒ'],
    u'\u0B93' : ['o:','ஓ'],
    u'\u0B94' : ['aU','ஔ'],
    u'\u0B95' : ['k A, g A, x A, G A, h A','க'],
    u'\u0B96' : ['(R:0B96)','஖'],
    u'\u0B97' : ['(R:0B97)','஗'],
    u'\u0B98' : ['(R:0B98)','஘'],
    u'\u0B99' : ['N A','ங'],
    u'\u0B9A' : ['c A, J A, S A, s A','ச'],
    u'\u0B9B' : ['(R:0B9B)','஛'],
    u'\u0B9C' : ['J A','ஜ'],
    u'\u0B9D' : ['(R:0B9D)','஝'],
    u'\u0B9E' : ['n~ A','ஞ'],
    u'\u0B9F' : ['tr A, dr A, rr A','ட'],
    u'\u0BA0' : ['(R:0BA0)','஠'],
    u'\u0BA1' : ['(R:0BA1)','஡'],
    u'\u0BA2' : ['(R:0BA2)','஢'],
    u'\u0BA3' : ['nr A','ண'],
    u'\u0BA4' : ['t[ A, d[ A, D A','த'],
    u'\u0BA5' : ['(R:0BA5)','஥'],
    u'\u0BA6' : ['(R:0BA6)','஦'],
    u'\u0BA7' : ['(R:0BA7)','஧'],
    u'\u0BA8' : ['n[ A','ந'],
    u'\u0BA9' : ['n A','ன'],
    u'\u0BAA' : ['p A, b A, V A','ப'],
    u'\u0BAB' : ['(R:0BAB)','஫'],
    u'\u0BAC' : ['(R:0BAC)','஬'],
    u'\u0BAE' : ['m A','ம'],
    u'\u0BAF' : ['j A','ய'],
    u'\u0BB0' : ['9r[ A','ர'],
    u'\u0BB1' : ['9r A, t A, d A','ற'],
    u'\u0BB2' : ['l A','ல'],
    u'\u0BB3' : ['lr A','ள'],
    u'\u0BB4' : ['l A','ழ'],
    u'\u0BB5' : ['v A','வ'],
    u'\u0BB6' : ['S A','ஶ'],
    u'\u0BB7' : ['sr A','ஷ'],
    u'\u0BB8' : ['s A','ஸ'],
    u'\u0BB9' : ['h A','ஹ'],
    u'\u0BBE' : ['A:','ா'],
    u'\u0BBF' : ['i','ி'],
    u'\u0BC0' : ['i:','ீ'],
    u'\u0BC1' : ['u','ு'],
    u'\u0BC2' : ['u:','ூ'],
    u'\u0BC3' : ['(R:0BC3)','௃'],
    u'\u0BC5' : ['(R:0BC5)','௅'],
    u'\u0BC6' : ['e','ெ'],
    u'\u0BC7' : ['e:','ே'],
    u'\u0BC8' : ['>I','ை'],
    u'\u0BC9' : ['(R:0BC9)','௉'],
    u'\u0BCA' : ['o','ொ'],
    u'\u0BCB' : ['o:','ோ'],
    u'\u0BCC' : ['aU','ௌ'],
    u'\u0BCD' : ['(P)','்'],
    u'\u0BD7' : ['(>I LENGTH MARK)','ௗ'],
    u'\u0BDF' : ['(R:0BDF)','௟'],
    u'\u0BE4' : ['(R:0BE4)','௤'],
    u'\u0BE6' : ['(ZERO)','௦'],
    u'\u0BE7' : ['(ONE)','௧'],
    u'\u0BE8' : ['(TWO)','௨'],
    u'\u0BE9' : ['(THREE)','௩'],
    u'\u0BEA' : ['(FOUR)','௪'],
    u'\u0BEB' : ['(FIVE)','௫'],
    u'\u0BEC' : ['(SIX)','௬'],
    u'\u0BED' : ['(SEVEN)','௭'],
    u'\u0BEE' : ['(EIGHT)','௮'],
    u'\u0BEF' : ['(NINE)','௯'],
    u'\u0BF0' : ['(TEN)','௰'],
    u'\u0BF1' : ['(HUNDREAD)','௱'],
    u'\u0BF2' : ['(THOUSAND)','௲'],
    u'\u0BF3' : ['(DAY)','௳'],
    u'\u0BF4' : ['(MONTH)','௴'],
    u'\u0BF5' : ['(YEAR)','௵'],
    u'\u0BF6' : ['(DEBIT)','௶'],
    u'\u0BF7' : ['(CREDIT)','௷'],
    u'\u0BF8' : ['(AS ABOVE)','௸'],
    u'\u0BF9' : ['(RUPEE)','௹'],
    u'\u0BFA' : ['(NUMBER)','௺'],
    u'\u0C01' : ['(CD)','ఁ'],
    u'\u0C02' : ['(M)','ం'],
    u'\u0C03' : ['(H)','ః'],
    u'\u0C05' : ['>','అ'],
    u'\u0C06' : ['>:','ఆ'],
    u'\u0C07' : ['i','ఇ'],
    u'\u0C08' : ['i:','ఈ'],
    u'\u0C09' : ['u','ఉ'],
    u'\u0C0A' : ['u:','ఊ'],
    u'\u0C0B' : ['9r i, 9r u','ఋ'],
    u'\u0C0C' : ['l=','ఌ'],
    u'\u0C0D' : ['(R:0C0D)','఍'],
    u'\u0C0E' : ['e','ఎ'],
    u'\u0C0F' : ['e:','ఏ'],
    u'\u0C10' : ['aI','ఐ'],
    u'\u0C12' : ['o','ఒ'],
    u'\u0C13' : ['o:','ఓ'],
    u'\u0C14' : ['aU','ఔ'],
    u'\u0C15' : ['k A','క'],
    u'\u0C16' : ['kh A','ఖ'],
    u'\u0C17' : ['g A','గ'],
    u'\u0C18' : ['gh A','ఘ'],
    u'\u0C19' : ['N A','ఙ'],
    u'\u0C1A' : ['c A','చ'],
    u'\u0C1B' : ['ch A','ఛ'],
    u'\u0C1C' : ['J A','జ'],
    u'\u0C1D' : ['Jh A','ఝ'],
    u'\u0C1E' : ['n~ A','ఞ'],
    u'\u0C1F' : ['tr A','ట'],
    u'\u0C20' : ['tR A','ఠ'],
    u'\u0C21' : ['dr A','డ'],
    u'\u0C22' : ['dR A','ఢ'],
    u'\u0C23' : ['nr A','ణ'],
    u'\u0C24' : ['t[ A','త'],
    u'\u0C25' : ['t[_h A','థ'],
    u'\u0C26' : ['d[ A','ద'],
    u'\u0C27' : ['d[_h A','ధ'],
    u'\u0C28' : ['n[ A','న'],
    u'\u0C29' : ['(R:0C29)','఩'],
    u'\u0C2A' : ['p A','ప'],
    u'\u0C2B' : ['ph A','ఫ'],
    u'\u0C2C' : ['b A','బ'],
    u'\u0C2D' : ['bh A','భ'],
    u'\u0C2E' : ['m A','మ'],
    u'\u0C2F' : ['j A','య'],
    u'\u0C30' : ['9r A','ర'],
    u'\u0C31' : ['9r: A','ఱ'],
    u'\u0C32' : ['l A','ల'],
    u'\u0C33' : ['lr A','ళ'],
    u'\u0C34' : ['(R:0C34)','ఴ'],
    u'\u0C35' : ['v A','వ'],
    u'\u0C36' : ['S A','శ'],
    u'\u0C37' : ['S A','ష'],
    u'\u0C38' : ['s A','స'],
    u'\u0C39' : ['hv A','హ'],
    u'\u0C3E' : ['>:','ా'],
    u'\u0C3F' : ['i','ి'],
    u'\u0C40' : ['i:','ీ'],
    u'\u0C41' : ['u','ు'],
    u'\u0C42' : ['u:','ూ'],
    u'\u0C43' : ['9r=','ృ'],
    u'\u0C44' : ['rr=','ౄ'],
    u'\u0C45' : ['(R:0C45)','౅'],
    u'\u0C46' : ['e','ె'],
    u'\u0C47' : ['e:','ే'],
    u'\u0C48' : ['aI','ై'],
    u'\u0C49' : ['(R:0C49)','౉'],
    u'\u0C4A' : ['o','ొ'],
    u'\u0C4B' : ['o:','ో'],
    u'\u0C4C' : ['aU','ౌ'],
    u'\u0C4D' : ['(P)','్'],
    u'\u0C55' : ['(LENGTH MARK)','ౕ'],
    u'\u0C56' : ['(aI LENGTH MARK)','ౖ'],
    u'\u0C5F' : ['(R:0C5F)','౟'],
    u'\u0C60' : ['rr=','ౠ'],
    u'\u0C61' : ['lr=','ౡ'],
    u'\u0C66' : ['(ZERO)','౦'],
    u'\u0C67' : ['(ONE)','౧'],
    u'\u0C68' : ['(TWO)','౨'],
    u'\u0C69' : ['(THREE)','౩'],
    u'\u0C6A' : ['(FOUR)','౪'],
    u'\u0C6B' : ['(FIVE)','౫'],
    u'\u0C6C' : ['(SIX)','౬'],
    u'\u0C6D' : ['(SEVEN)','౭'],
    u'\u0C6E' : ['(EIGHT)','౮'],
    u'\u0C6F' : ['(NINE)','౯'],
    u'\u0780' : ['h','ހ'],
    u'\u0781' : ['sr','ށ'],
    u'\u0782' : ['n','ނ'],
    u'\u0783' : ['r','ރ'],
    u'\u0784' : ['b','ބ'],
    u'\u0785' : ['lr','ޅ'],
    u'\u0786' : ['k','ކ'],
    u'\u0787' : ['(SILENT)','އ'],
    u'\u0788' : ['v','ވ'],
    u'\u0789' : ['m','މ'],
    u'\u078A' : ['f','ފ'],
    u'\u078B' : ['d','ދ'],
    u'\u078C' : ['t','ތ'],
    u'\u078D' : ['l','ލ'],
    u'\u078E' : ['g','ގ'],
    u'\u078F' : ['n~','ޏ'],
    u'\u0790' : ['s','ސ'],
    u'\u0791' : ['dr','ޑ'],
    u'\u0792' : ['z','ޒ'],
    u'\u0793' : ['tr','ޓ'],
    u'\u0794' : ['j','ޔ'],
    u'\u0795' : ['p','ޕ'],
    u'\u0796' : ['j-','ޖ'],
    u'\u0797' : ['c','ޗ'],
    u'\u0798' : ['T','ޘ'],
    u'\u0799' : ['D','ޙ'],
    u'\u079A' : ['HH','ޚ'],
    u'\u079B' : ['x','ޛ'],
    u'\u079C' : ['z','ޜ'],
    u'\u079D' : ['S','ޝ'],
    u'\u079E' : ['sr','ޞ'],
    u'\u079F' : ['dr','ޟ'],
    u'\u07A0' : ['tr','ޠ'],
    u'\u07A1' : ['zr','ޡ'],
    u'\u07A2' : ['HH_v','ޢ'],
    u'\u07A3' : ['G','ޣ'],
    u'\u07A4' : ['q','ޤ'],
    u'\u07A5' : ['w','ޥ'],
    u'\u07A6' : ['A','ަ'],
    u'\u07A7' : ['A:','ާ'],
    u'\u07A8' : ['i','ި'],
    u'\u07A9' : ['i:','ީ'],
    u'\u07AA' : ['u','ު'],
    u'\u07AB' : ['u:','ޫ'],
    u'\u07AC' : ['e','ެ'],
    u'\u07AD' : ['e:','ޭ'],
    u'\u07AE' : ['o','ޮ'],
    u'\u07AF' : ['o:','ޯ'],
    u'\u07B0' : ['?','ް'],
    u'\u07B1' : ['n','ޱ'],
    u'\u0E01' : ['k','ก'],
    u'\u0E02' : ['kh, k','ข'],
    u'\u0E03' : ['kh','ฃ'],
    u'\u0E04' : ['kh, k','ค'],
    u'\u0E05' : ['kh','ฅ'],
    u'\u0E06' : ['kh, k','ฆ'],
    u'\u0E07' : ['N','ง'],
    u'\u0E08' : ['c, t','จ'],
    u'\u0E09' : ['ch','ฉ'],
    u'\u0E0A' : ['ch, t','ช'],
    u'\u0E0B' : ['s, t','ซ'],
    u'\u0E0C' : ['ch, t','ฌ'],
    u'\u0E0D' : ['j, n','ญ'],
    u'\u0E0E' : ['d, t','ฎ'],
    u'\u0E0F' : ['t','ฏ'],
    u'\u0E10' : ['th, t','ฐ'],
    u'\u0E11' : ['th, t','ฑ'],
    u'\u0E12' : ['th, t','ฒ'],
    u'\u0E13' : ['n','ณ'],
    u'\u0E14' : ['d, t','ด'],
    u'\u0E15' : ['t','ต'],
    u'\u0E16' : ['th, t','ถ'],
    u'\u0E17' : ['th, t','ท'],
    u'\u0E18' : ['th, t','ธ'],
    u'\u0E19' : ['n','น'],
    u'\u0E1A' : ['b, p','บ'],
    u'\u0E1B' : ['p','ป'],
    u'\u0E1C' : ['ph, p','ผ'],
    u'\u0E1D' : ['f','ฝ'],
    u'\u0E1E' : ['ph, p','พ'],
    u'\u0E1F' : ['f, p','ฟ'],
    u'\u0E20' : ['ph, p','ภ'],
    u'\u0E21' : ['m','ม'],
    u'\u0E22' : ['j','ย'],
    u'\u0E23' : ['r, n','ร'],
    u'\u0E24' : ['r 4','ฤ'],
    u'\u0E25' : ['l, n','ล'],
    u'\u0E26' : ['l 4','ฦ'],
    u'\u0E27' : ['w','ว'],
    u'\u0E28' : ['s, t','ศ'],
    u'\u0E29' : ['s, t','ษ'],
    u'\u0E2A' : ['s, t','ส'],
    u'\u0E2B' : ['h','ห'],
    u'\u0E2C' : ['l, n','ฬ'],
    u'\u0E2D' : ['?','อ'],
    u'\u0E2E' : ['h','ฮ'],
    u'\u0E2F' : ['(ELLIPSIS, ABBREVIATION)','ฯ'],
    u'\u0E30' : ['A','ะ'],
    u'\u0E31' : ['A','ั'],
    u'\u0E32' : ['A:','า'],
    u'\u0E33' : ['A m','ำ'],
    u'\u0E34' : ['i','ิ'],
    u'\u0E35' : ['i:','ี'],
    u'\u0E36' : ['4','ึ'],
    u'\u0E37' : ['4:','ื'],
    u'\u0E38' : ['u','ุ'],
    u'\u0E39' : ['u:','ู'],
    u'\u0E3A' : ['(P)','ฺ'],
    u'\u0E3F' : ['(CURRENCY SYMBOL BAHT)','฿'],
    u'\u0E40' : ['e:','เ'],
    u'\u0E41' : ['@','แ'],
    u'\u0E42' : ['o','โ'],
    u'\u0E43' : ['aI','ใ'],
    u'\u0E44' : ['aI','ไ'],
    u'\u0E45' : ['(PHINTHU)','ๅ'],
    u'\u0E46' : ['(MAIYAMOK)','ๆ'],
    u'\u0E47' : ['(MAITAIKHU)','็'],
    u'\u0E48' : ['(MAI EK)','่'],
    u'\u0E49' : ['(MAI THO)','้'],
    u'\u0E4A' : ['(MAI TRI)','๊'],
    u'\u0E4B' : ['(MAI CHATTAWA)','๋'],
    u'\u0E4C' : ['(THANTHAKHAT)','์'],
    u'\u0E4D' : ['(NIKHHIT)','ํ'],
    u'\u0E4E' : ['(YAMAKKAN)','๎'],
    u'\u0E4F' : ['(FONGMAN)','๏'],
    u'\u0E50' : ['(ZERO)','๐'],
    u'\u0E51' : ['(ONE)','๑'],
    u'\u0E52' : ['(TWO)','๒'],
    u'\u0E53' : ['(THREE)','๓'],
    u'\u0E54' : ['(FOUR)','๔'],
    u'\u0E55' : ['(FIVE)','๕'],
    u'\u0E56' : ['(SIX)','๖'],
    u'\u0E57' : ['(SEVEN)','๗'],
    u'\u0E58' : ['(EIGHT)','๘'],
    u'\u0E59' : ['(NINE)','๙'],
    u'\u0E5A' : ['(ANGKHANKHU)','๚'],
    u'\u0E5B' : ['(KHOMUT)','๛'],
    u'\u0F00' : ['(OM)','ༀ'],
    u'\u0F01' : ['(##)','༁'],
    u'\u0F02' : ['(##)','༂'],
    u'\u0F03' : ['(##)','༃'],
    u'\u0F04' : ['(##)','༄'],
    u'\u0F05' : ['(##)','༅'],
    u'\u0F06' : ['(##)','༆'],
    u'\u0F07' : ['(##)','༇'],
    u'\u0F08' : ['(##)','༈'],
    u'\u0F09' : ['(##)','༉'],
    u'\u0F0A' : ['(##)','༊'],
    u'\u0F0B' : ['(##)','་'],
    u'\u0F0C' : ['(##)','༌'],
    u'\u0F0D' : ['(##)','།'],
    u'\u0F0E' : ['(##)','༎'],
    u'\u0F0F' : ['(##)','༏'],
    u'\u0F10' : ['(##)','༐'],
    u'\u0F11' : ['(##)','༑'],
    u'\u0F12' : ['(##)','༒'],
    u'\u0F13' : ['(##)','༓'],
    u'\u0F14' : ['(##)','༔'],
    u'\u0F15' : ['(##)','༕'],
    u'\u0F16' : ['(##)','༖'],
    u'\u0F17' : ['(##)','༗'],
    u'\u0F18' : ['(##)','༘'],
    u'\u0F19' : ['(##)','༙'],
    u'\u0F1A' : ['(##)','༚'],
    u'\u0F1B' : ['(##)','༛'],
    u'\u0F1C' : ['(##)','༜'],
    u'\u0F1D' : ['(##)','༝'],
    u'\u0F1E' : ['(##)','༞'],
    u'\u0F1F' : ['(##)','༟'],
    u'\u0F20' : ['(ZERO)','༠'],
    u'\u0F21' : ['(ONE)','༡'],
    u'\u0F22' : ['(TWO)','༢'],
    u'\u0F23' : ['(THREE)','༣'],
    u'\u0F24' : ['(FOUR)','༤'],
    u'\u0F25' : ['(FIVE)','༥'],
    u'\u0F26' : ['(SIX)','༦'],
    u'\u0F27' : ['(SEVEN)','༧'],
    u'\u0F28' : ['(EIGHT)','༨'],
    u'\u0F29' : ['(NINE)','༩'],
    u'\u0F2A' : ['(HALF ONE)','༪'],
    u'\u0F2B' : ['(HALF TWO)','༫'],
    u'\u0F2C' : ['(HALF THREE)','༬'],
    u'\u0F2D' : ['(HALF FOUR)','༭'],
    u'\u0F2E' : ['(HALF FIVE)','༮'],
    u'\u0F2F' : ['(HALF SIX)','༯'],
    u'\u0F30' : ['(HALF SEVEN)','༰'],
    u'\u0F31' : ['(HALF EIGHT)','༱'],
    u'\u0F32' : ['(HALF NINE)','༲'],
    u'\u0F33' : ['(HALF ZERO)','༳'],
    u'\u0F34' : ['(##)','༴'],
    u'\u0F35' : ['(##)','༵'],
    u'\u0F36' : ['(##)','༶'],
    u'\u0F37' : ['(##)','༷'],
    u'\u0F38' : ['(##)','༸'],
    u'\u0F39' : ['(##)','༹'],
    u'\u0F3A' : ['(##)','༺'],
    u'\u0F3B' : ['(##)','༻'],
    u'\u0F3C' : ['(##)','༼'],
    u'\u0F3D' : ['(##)','༽'],
    u'\u0F3E' : ['(##)','༾'],
    u'\u0F3F' : ['(##)','༿'],
    u'\u0F40' : ['k A','ཀ'],
    u'\u0F41' : ['kh A','ཁ'],
    u'\u0F42' : ['k A','ག'],
    u'\u0F43' : ['kh A','གྷ'],
    u'\u0F44' : ['N A','ང'],
    u'\u0F45' : ['tS A','ཅ'],
    u'\u0F46' : ['tSh A','ཆ'],
    u'\u0F47' : ['tS A','ཇ'],
    u'\u0F49' : ['n~ A','ཉ'],
    u'\u0F4A' : ['t t A','ཊ'],
    u'\u0F4B' : ['t th A','ཋ'],
    u'\u0F4C' : ['t t A','ཌ'],
    u'\u0F4D' : ['t th A','ཌྷ'],
    u'\u0F4E' : ['n n A','ཎ'],
    u'\u0F4F' : ['t A','ཏ'],
    u'\u0F50' : ['th A','ཐ'],
    u'\u0F51' : ['t A','ད'],
    u'\u0F52' : ['th A','དྷ'],
    u'\u0F53' : ['n A','ན'],
    u'\u0F54' : ['p A','པ'],
    u'\u0F55' : ['ph A','ཕ'],
    u'\u0F56' : ['p A','བ'],
    u'\u0F57' : ['ph A','བྷ'],
    u'\u0F58' : ['m A','མ'],
    u'\u0F59' : ['ts A','ཙ'],
    u'\u0F5A' : ['tsh A','ཚ'],
    u'\u0F5B' : ['ts A','ཛ'],
    u'\u0F5C' : ['tsh A','ཛྷ'],
    u'\u0F5D' : ['w A','ཝ'],
    u'\u0F5E' : ['S A','ཞ'],
    u'\u0F5F' : ['s A','ཟ'],
    u'\u0F60' : ['A','འ'],
    u'\u0F61' : ['j A','ཡ'],
    u'\u0F62' : ['r A','ར'],
    u'\u0F63' : ['l A','ལ'],
    u'\u0F64' : ['S A','ཤ'],
    u'\u0F65' : ['S S A','ཥ'],
    u'\u0F66' : ['s A','ས'],
    u'\u0F67' : ['h A','ཧ'],
    u'\u0F68' : ['A','ཨ'],
    u'\u0F69' : ['k S S A','ཀྵ'],
    u'\u0F6A' : ['r A','ཪ'],
    u'\u0F71' : ['A:','ཱ'],
    u'\u0F72' : ['i','ི'],
    u'\u0F73' : ['i:','ཱི'],
    u'\u0F74' : ['u','ུ'],
    u'\u0F75' : ['u:','ཱུ'],
    u'\u0F76' : ['r=','ྲྀ'],
    u'\u0F77' : ['rr=','ཷ'],
    u'\u0F78' : ['l=','ླྀ'],
    u'\u0F79' : ['l=','ཹ'],
    u'\u0F7A' : ['e','ེ'],
    u'\u0F7B' : ['e:','ཻ'],
    u'\u0F7C' : ['o','ོ'],
    u'\u0F7D' : ['o:','ཽ'],
    u'\u0F7E' : ['(M)','ཾ'],
    u'\u0F7F' : ['(H)','ཿ'],
    u'\u0F80' : ['i','ྀ'],
    u'\u0F81' : ['i:','ཱྀ'],
    u'\u0F82' : ['(##)','ྂ'],
    u'\u0F83' : ['(##)','ྃ'],
    u'\u0F84' : ['(##)','྄'],
    u'\u0F85' : ['(##)','྅'],
    u'\u0F86' : ['(##)','྆'],
    u'\u0F87' : ['(##)','྇'],
    u'\u0F88' : ['(##)','ྈ'],
    u'\u0F89' : ['(##)','ྉ'],
    u'\u0F8A' : ['(##)','ྊ'],
    u'\u0F8B' : ['(##)','ྋ'],
    u'\u0F90' : ['k A','ྐ'],
    u'\u0F91' : ['kh A','ྑ'],
    u'\u0F92' : ['k A','ྒ'],
    u'\u0F93' : ['kh A','ྒྷ'],
    u'\u0F94' : ['N A','ྔ'],
    u'\u0F95' : ['tS A','ྕ'],
    u'\u0F96' : ['tSh A','ྖ'],
    u'\u0F97' : ['tS A','ྗ'],
    u'\u0F99' : ['n~ A','ྙ'],
    u'\u0F9A' : ['t t A','ྚ'],
    u'\u0F9B' : ['t th A','ྛ'],
    u'\u0F9C' : ['t t A','ྜ'],
    u'\u0F9D' : ['t th A','ྜྷ'],
    u'\u0F9E' : ['n n A','ྞ'],
    u'\u0F9F' : ['t A','ྟ'],
    u'\u0FA0' : ['th A','ྠ'],
    u'\u0FA1' : ['t A','ྡ'],
    u'\u0FA2' : ['th A','ྡྷ'],
    u'\u0FA3' : ['n A','ྣ'],
    u'\u0FA4' : ['p A','ྤ'],
    u'\u0FA5' : ['ph A','ྥ'],
    u'\u0FA6' : ['p A','ྦ'],
    u'\u0FA7' : ['ph A','ྦྷ'],
    u'\u0FA8' : ['m A','ྨ'],
    u'\u0FA9' : ['ts A','ྩ'],
    u'\u0FAA' : ['tsh A','ྪ'],
    u'\u0FAB' : ['ts A','ྫ'],
    u'\u0FAC' : ['tsh A','ྫྷ'],
    u'\u0FAD' : ['w A','ྭ'],
    u'\u0FAE' : ['S A','ྮ'],
    u'\u0FAF' : ['s A','ྯ'],
    u'\u0FB0' : ['A','ྰ'],
    u'\u0FB1' : ['j A','ྱ'],
    u'\u0FB2' : ['r A','ྲ'],
    u'\u0FB3' : ['l A','ླ'],
    u'\u0FB4' : ['S A','ྴ'],
    u'\u0FB5' : ['S S A','ྵ'],
    u'\u0FB6' : ['s A','ྶ'],
    u'\u0FB7' : ['h A','ྷ'],
    u'\u0FB8' : ['A','ྸ'],
    u'\u0FB9' : ['k S S A','ྐྵ'],
    u'\u0FBA' : ['w A','ྺ'],
    u'\u0FBB' : ['j A','ྻ'],
    u'\u0FBC' : ['r A','ྼ'],
    u'\u0FBE' : ['(REFRAIN)','྾'],
    u'\u0FBF' : ['(REFERENCE MARK)','྿'],
    u'\u0FC0' : ['(HEAVY DRUM BEAT)','࿀'],
    u'\u0FC1' : ['(LIGHT DRUM BEAT)','࿁'],
    u'\u0FC2' : ['(SMALL TIBETAN HAND DRUM)','࿂'],
    u'\u0FC3' : ['(TIBETAN CYMBAL)','࿃'],
    u'\u0FC4' : ['(##)','࿄'],
    u'\u0FC5' : ['(##)','࿅'],
    u'\u0FC6' : ['(##)','࿆'],
    u'\u0FC7' : ['(##)','࿇'],
    u'\u0FC8' : ['(##)','࿈'],
    u'\u0FC9' : ['(##)','࿉'],
    u'\u0FCA' : ['(##)','࿊'],
    u'\u0FCB' : ['(##)','࿋'],
    u'\u0FCC' : ['(##)','࿌'],
    u'\u0FCF' : ['(##)','࿏'],
    u'\u0FD0' : ['(##)','࿐'],
    u'\u0FD1' : ['(##)','࿑'],
    u'\u2D30' : ['a','ⴰ'],
    u'\u2D31' : ['b','ⴱ'],
    u'\u2D32' : ['bh','ⴲ'],
    u'\u2D33' : ['g','ⴳ'],
    u'\u2D34' : ['gh','ⴴ'],
    u'\u2D35' : ['dj','ⴵ'],
    u'\u2D36' : ['dj','ⴶ'],
    u'\u2D37' : ['d','ⴷ'],
    u'\u2D38' : ['dh','ⴸ'],
    u'\u2D39' : ['d d','ⴹ'],
    u'\u2D3A' : ['d dh','ⴺ'],
    u'\u2D3B' : ['e','ⴻ'],
    u'\u2D3C' : ['f','ⴼ'],
    u'\u2D3D' : ['k','ⴽ'],
    u'\u2D3E' : ['k','ⴾ'],
    u'\u2D3F' : ['kh','ⴿ'],
    u'\u2D40' : ['b','ⵀ'],
    u'\u2D41' : ['h','ⵁ'],
    u'\u2D42' : ['h','ⵂ'],
    u'\u2D43' : ['h','ⵃ'],
    u'\u2D44' : ['@','ⵄ'],
    u'\u2D45' : ['kh','ⵅ'],
    u'\u2D46' : ['q','ⵆ'],
    u'\u2D47' : ['q','ⵇ'],
    u'\u2D48' : ['i','ⵈ'],
    u'\u2D49' : ['z','ⵉ'],
    u'\u2D4A' : ['z','ⵊ'],
    u'\u2D4B' : ['z','ⵋ'],
    u'\u2D4C' : ['z','ⵌ'],
    u'\u2D4D' : ['l','ⵍ'],
    u'\u2D4E' : ['m','ⵎ'],
    u'\u2D4F' : ['n','ⵏ'],
    u'\u2D50' : ['nj','ⵐ'],
    u'\u2D51' : ['N','ⵑ'],
    u'\u2D52' : ['p','ⵒ'],
    u'\u2D53' : ['u','ⵓ'],
    u'\u2D54' : ['r','ⵔ'],
    u'\u2D55' : ['rr','ⵕ'],
    u'\u2D56' : ['gh','ⵖ'],
    u'\u2D57' : ['gh','ⵗ'],
    u'\u2D58' : ['gh','ⵘ'],
    u'\u2D59' : ['s','ⵙ'],
    u'\u2D5A' : ['s s','ⵚ'],
    u'\u2D5B' : ['sh','ⵛ'],
    u'\u2D5C' : ['t','ⵜ'],
    u'\u2D5D' : ['th','ⵝ'],
    u'\u2D5E' : ['ch','ⵞ'],
    u'\u2D5F' : ['t t','ⵟ'],
    u'\u2D60' : ['v','ⵠ'],
    u'\u2D61' : ['w','ⵡ'],
    u'\u2D62' : ['j','ⵢ'],
    u'\u2D63' : ['z','ⵣ'],
    u'\u2D64' : ['z','ⵤ'],
    u'\u2D65' : ['z z','ⵥ'],
    u'\u2D6F' : ['w','ⵯ'],
    u'\u1401' : ['aI','ᐁ'],
    u'\u1402' : ['A: i','ᐂ'],
    u'\u1403' : ['i','ᐃ'],
    u'\u1404' : ['i:','ᐄ'],
    u'\u1405' : ['u','ᐅ'],
    u'\u1406' : ['u:','ᐆ'],
    u'\u1407' : ['o:','ᐇ'],
    u'\u1408' : ['e:','ᐈ'],
    u'\u1409' : ['i','ᐉ'],
    u'\u140A' : ['A','ᐊ'],
    u'\u140B' : ['A:','ᐋ'],
    u'\u140C' : ['w e','ᐌ'],
    u'\u140D' : ['w e','ᐍ'],
    u'\u140E' : ['w i','ᐎ'],
    u'\u140F' : ['w i','ᐏ'],
    u'\u1410' : ['w i:','ᐐ'],
    u'\u1411' : ['w i:','ᐑ'],
    u'\u1412' : ['w o','ᐒ'],
    u'\u1413' : ['w o','ᐓ'],
    u'\u1414' : ['w o:','ᐔ'],
    u'\u1415' : ['w o:','ᐕ'],
    u'\u1416' : ['w o:','ᐖ'],
    u'\u1417' : ['w A','ᐗ'],
    u'\u1418' : ['w A','ᐘ'],
    u'\u1419' : ['w A:','ᐙ'],
    u'\u141A' : ['w A:','ᐚ'],
    u'\u141B' : ['w A:','ᐛ'],
    u'\u141C' : ['A:','ᐜ'],
    u'\u141D' : ['w','ᐝ'],
    u'\u141E' : ['?','ᐞ'],
    u'\u141F' : ['?','ᐟ'],
    u'\u1420' : ['k','ᐠ'],
    u'\u1421' : ['S','ᐡ'],
    u'\u1422' : ['s','ᐢ'],
    u'\u1423' : ['n','ᐣ'],
    u'\u1424' : ['w','ᐤ'],
    u'\u1425' : ['t t','ᐥ'],
    u'\u1426' : ['h','ᐦ'],
    u'\u1427' : ['w','ᐧ'],
    u'\u1428' : ['G','ᐨ'],
    u'\u1429' : ['n','ᐩ'],
    u'\u142A' : ['l','ᐪ'],
    u'\u142B' : ['e n','ᐫ'],
    u'\u142C' : ['i n','ᐬ'],
    u'\u142D' : ['o n','ᐭ'],
    u'\u142E' : ['a n','ᐮ'],
    u'\u142F' : ['p aI','ᐯ'],
    u'\u1430' : ['p A: i','ᐰ'],
    u'\u1431' : ['p i','ᐱ'],
    u'\u1432' : ['p i:','ᐲ'],
    u'\u1433' : ['p u','ᐳ'],
    u'\u1434' : ['p u:','ᐴ'],
    u'\u1435' : ['p o:','ᐵ'],
    u'\u1436' : ['h e:','ᐶ'],
    u'\u1437' : ['h i','ᐷ'],
    u'\u1438' : ['p A','ᐸ'],
    u'\u1439' : ['p A:','ᐹ'],
    u'\u143A' : ['pw e','ᐺ'],
    u'\u143B' : ['pw e','ᐻ'],
    u'\u143C' : ['pw i','ᐼ'],
    u'\u143D' : ['pw i','ᐽ'],
    u'\u143E' : ['pw i:','ᐾ'],
    u'\u143F' : ['pw i:','ᐿ'],
    u'\u1440' : ['pw o','ᑀ'],
    u'\u1441' : ['pw o','ᑁ'],
    u'\u1442' : ['pw o:','ᑂ'],
    u'\u1443' : ['pw o:','ᑃ'],
    u'\u1444' : ['pw A','ᑄ'],
    u'\u1445' : ['pw A','ᑅ'],
    u'\u1446' : ['pw A:','ᑆ'],
    u'\u1447' : ['pw A:','ᑇ'],
    u'\u1448' : ['pw A:','ᑈ'],
    u'\u1449' : ['p','ᑉ'],
    u'\u144A' : ['h','ᑊ'],
    u'\u144B' : ['t aI','ᑋ'],
    u'\u144C' : ['t A: i','ᑌ'],
    u'\u144D' : ['t i','ᑍ'],
    u'\u144E' : ['t i','ᑎ'],
    u'\u144F' : ['t i:','ᑏ'],
    u'\u1450' : ['t u','ᑐ'],
    u'\u1451' : ['t u:','ᑑ'],
    u'\u1452' : ['t o:','ᑒ'],
    u'\u1453' : ['d e:','ᑓ'],
    u'\u1454' : ['d i','ᑔ'],
    u'\u1455' : ['t A','ᑕ'],
    u'\u1456' : ['t A:','ᑖ'],
    u'\u1457' : ['tw e','ᑗ'],
    u'\u1458' : ['tw i','ᑘ'],
    u'\u1459' : ['tw i','ᑙ'],
    u'\u145A' : ['tw i:','ᑚ'],
    u'\u145B' : ['tw i:','ᑛ'],
    u'\u145C' : ['tw i:','ᑜ'],
    u'\u145D' : ['tw o','ᑝ'],
    u'\u145E' : ['tw o','ᑞ'],
    u'\u145F' : ['tw o:','ᑟ'],
    u'\u1460' : ['tw o:','ᑠ'],
    u'\u1461' : ['tw A','ᑡ'],
    u'\u1462' : ['tw A','ᑢ'],
    u'\u1463' : ['tw A:','ᑣ'],
    u'\u1464' : ['tw A:','ᑤ'],
    u'\u1465' : ['tw A:','ᑥ'],
    u'\u1466' : ['t','ᑦ'],
    u'\u1467' : ['t t e','ᑧ'],
    u'\u1468' : ['t t i','ᑨ'],
    u'\u1469' : ['t t o','ᑩ'],
    u'\u146A' : ['t t A','ᑪ'],
    u'\u146B' : ['k aI','ᑫ'],
    u'\u146C' : ['k A: i','ᑬ'],
    u'\u146D' : ['k i','ᑭ'],
    u'\u146E' : ['k i:','ᑮ'],
    u'\u146F' : ['k u','ᑯ'],
    u'\u1470' : ['k u:','ᑰ'],
    u'\u1471' : ['k o:','ᑱ'],
    u'\u1472' : ['k A','ᑲ'],
    u'\u1473' : ['k A:','ᑳ'],
    u'\u1474' : ['kw e','ᑴ'],
    u'\u1475' : ['kw e','ᑵ'],
    u'\u1476' : ['kw i','ᑶ'],
    u'\u1477' : ['kw i','ᑷ'],
    u'\u1478' : ['kw i:','ᑸ'],
    u'\u1479' : ['kw i:','ᑹ'],
    u'\u147A' : ['kw o','ᑺ'],
    u'\u147B' : ['kw o','ᑻ'],
    u'\u147C' : ['kw o:','ᑼ'],
    u'\u147D' : ['kw o:','ᑽ'],
    u'\u147E' : ['kw A','ᑾ'],
    u'\u147F' : ['kw A','ᑿ'],
    u'\u1480' : ['kw A','ᒀ'],
    u'\u1481' : ['kw A:','ᒁ'],
    u'\u1482' : ['kw A:','ᒂ'],
    u'\u1483' : ['k','ᒃ'],
    u'\u1484' : ['kw','ᒄ'],
    u'\u1485' : ['k eh','ᒅ'],
    u'\u1486' : ['k ih','ᒆ'],
    u'\u1487' : ['k oh','ᒇ'],
    u'\u1488' : ['k ah','ᒈ'],
    u'\u1489' : ['G aI','ᒉ'],
    u'\u148A' : ['G A: i','ᒊ'],
    u'\u148B' : ['G i','ᒋ'],
    u'\u148C' : ['G i:','ᒌ'],
    u'\u148D' : ['G u','ᒍ'],
    u'\u148E' : ['G u:','ᒎ'],
    u'\u148F' : ['G o:','ᒏ'],
    u'\u1490' : ['G A','ᒐ'],
    u'\u1491' : ['G A:','ᒑ'],
    u'\u1492' : ['Gw e','ᒒ'],
    u'\u1493' : ['Gw e','ᒓ'],
    u'\u1494' : ['Gw i','ᒔ'],
    u'\u1495' : ['Gw i','ᒕ'],
    u'\u1496' : ['Gw i:','ᒖ'],
    u'\u1497' : ['Gw i:','ᒗ'],
    u'\u1498' : ['Gw o','ᒘ'],
    u'\u1499' : ['Gw o','ᒙ'],
    u'\u149A' : ['Gw o:','ᒚ'],
    u'\u149B' : ['Gw o:','ᒛ'],
    u'\u149C' : ['Gw A','ᒜ'],
    u'\u149D' : ['Gw A','ᒝ'],
    u'\u149E' : ['Gw A:','ᒞ'],
    u'\u149F' : ['Gw A:','ᒟ'],
    u'\u14A0' : ['Gw A:','ᒠ'],
    u'\u14A1' : ['G','ᒡ'],
    u'\u14A2' : ['D','ᒢ'],
    u'\u14A3' : ['m aI','ᒣ'],
    u'\u14A4' : ['m A: i','ᒤ'],
    u'\u14A5' : ['m i','ᒥ'],
    u'\u14A6' : ['m i:','ᒦ'],
    u'\u14A7' : ['m u','ᒧ'],
    u'\u14A8' : ['m u:','ᒨ'],
    u'\u14A9' : ['m o:','ᒩ'],
    u'\u14AA' : ['m A','ᒪ'],
    u'\u14AB' : ['m A:','ᒫ'],
    u'\u14AC' : ['mw e','ᒬ'],
    u'\u14AD' : ['mw e','ᒭ'],
    u'\u14AE' : ['mw i','ᒮ'],
    u'\u14AF' : ['mw i','ᒯ'],
    u'\u14B0' : ['mw i:','ᒰ'],
    u'\u14B1' : ['mw i:','ᒱ'],
    u'\u14B2' : ['mw o','ᒲ'],
    u'\u14B3' : ['mw o','ᒳ'],
    u'\u14B4' : ['mw o:','ᒴ'],
    u'\u14B5' : ['mw o:','ᒵ'],
    u'\u14B6' : ['mw A','ᒶ'],
    u'\u14B7' : ['mw A','ᒷ'],
    u'\u14B8' : ['mw A:','ᒸ'],
    u'\u14B9' : ['mw A:','ᒹ'],
    u'\u14BA' : ['mw A:','ᒺ'],
    u'\u14BB' : ['m','ᒻ'],
    u'\u14BC' : ['m','ᒼ'],
    u'\u14BD' : ['mh','ᒽ'],
    u'\u14BE' : ['m','ᒾ'],
    u'\u14BF' : ['m','ᒿ'],
    u'\u14C0' : ['n aI','ᓀ'],
    u'\u14C1' : ['n A: i','ᓁ'],
    u'\u14C2' : ['n i','ᓂ'],
    u'\u14C3' : ['n i:','ᓃ'],
    u'\u14C4' : ['n u','ᓄ'],
    u'\u14C5' : ['n u:','ᓅ'],
    u'\u14C6' : ['n o:','ᓆ'],
    u'\u14C7' : ['n A','ᓇ'],
    u'\u14C8' : ['n A:','ᓈ'],
    u'\u14C9' : ['nw e','ᓉ'],
    u'\u14CA' : ['nw e','ᓊ'],
    u'\u14CB' : ['nw A','ᓋ'],
    u'\u14CC' : ['nw A','ᓌ'],
    u'\u14CD' : ['nw A:','ᓍ'],
    u'\u14CE' : ['nw A:','ᓎ'],
    u'\u14CF' : ['nw A:','ᓏ'],
    u'\u14D0' : ['n','ᓐ'],
    u'\u14D1' : ['N','ᓑ'],
    u'\u14D2' : ['nh','ᓒ'],
    u'\u14D3' : ['l aI','ᓓ'],
    u'\u14D4' : ['l A: i','ᓔ'],
    u'\u14D5' : ['l i','ᓕ'],
    u'\u14D6' : ['l i:','ᓖ'],
    u'\u14D7' : ['l u','ᓗ'],
    u'\u14D8' : ['l u:','ᓘ'],
    u'\u14D9' : ['l o:','ᓙ'],
    u'\u14DA' : ['l A','ᓚ'],
    u'\u14DB' : ['l A:','ᓛ'],
    u'\u14DC' : ['lw e','ᓜ'],
    u'\u14DD' : ['lw e','ᓝ'],
    u'\u14DE' : ['lw i','ᓞ'],
    u'\u14DF' : ['lw i','ᓟ'],
    u'\u14E0' : ['lw i:','ᓠ'],
    u'\u14E1' : ['lw i:','ᓡ'],
    u'\u14E2' : ['lw o','ᓢ'],
    u'\u14E3' : ['lw o','ᓣ'],
    u'\u14E4' : ['lw o:','ᓤ'],
    u'\u14E5' : ['lw o:','ᓥ'],
    u'\u14E6' : ['lw A','ᓦ'],
    u'\u14E7' : ['lw A','ᓧ'],
    u'\u14E8' : ['lw A:','ᓨ'],
    u'\u14E9' : ['lw A:','ᓩ'],
    u'\u14EA' : ['l','ᓪ'],
    u'\u14EB' : ['l','ᓫ'],
    u'\u14EC' : ['l','ᓬ'],
    u'\u14ED' : ['s aI','ᓭ'],
    u'\u14EE' : ['s A: i','ᓮ'],
    u'\u14EF' : ['s i','ᓯ'],
    u'\u14F0' : ['s i:','ᓰ'],
    u'\u14F1' : ['s u','ᓱ'],
    u'\u14F2' : ['s u:','ᓲ'],
    u'\u14F3' : ['s o:','ᓳ'],
    u'\u14F4' : ['s A','ᓴ'],
    u'\u14F5' : ['s A:','ᓵ'],
    u'\u14F6' : ['sw e','ᓶ'],
    u'\u14F7' : ['sw e','ᓷ'],
    u'\u14F8' : ['sw i','ᓸ'],
    u'\u14F9' : ['sw i','ᓹ'],
    u'\u14FA' : ['sw i:','ᓺ'],
    u'\u14FB' : ['sw i:','ᓻ'],
    u'\u14FC' : ['sw o','ᓼ'],
    u'\u14FD' : ['sw o','ᓽ'],
    u'\u14FE' : ['sw o:','ᓾ'],
    u'\u14FF' : ['sw o:','ᓿ'],
    u'\u1500' : ['sw A','ᔀ'],
    u'\u1501' : ['sw A','ᔁ'],
    u'\u1502' : ['sw A:','ᔂ'],
    u'\u1503' : ['sw A:','ᔃ'],
    u'\u1504' : ['sw A:','ᔄ'],
    u'\u1505' : ['s','ᔅ'],
    u'\u1506' : ['s','ᔆ'],
    u'\u1507' : ['sw','ᔇ'],
    u'\u1508' : ['s','ᔈ'],
    u'\u1509' : ['s k','ᔉ'],
    u'\u150A' : ['s kw','ᔊ'],
    u'\u150B' : ['s w','ᔋ'],
    u'\u150C' : ['s pw A','ᔌ'],
    u'\u150D' : ['s tw A','ᔍ'],
    u'\u150E' : ['s kw A','ᔎ'],
    u'\u150F' : ['s cw A','ᔏ'],
    u'\u1510' : ['S e','ᔐ'],
    u'\u1511' : ['S i','ᔑ'],
    u'\u1512' : ['S i:','ᔒ'],
    u'\u1513' : ['S o','ᔓ'],
    u'\u1514' : ['S o:','ᔔ'],
    u'\u1515' : ['S A','ᔕ'],
    u'\u1516' : ['S A:','ᔖ'],
    u'\u1517' : ['Sw e','ᔗ'],
    u'\u1518' : ['Sw e','ᔘ'],
    u'\u1519' : ['Sw i','ᔙ'],
    u'\u151A' : ['Sw i','ᔚ'],
    u'\u151B' : ['Sw i:','ᔛ'],
    u'\u151C' : ['Sw i:','ᔜ'],
    u'\u151D' : ['Sw o','ᔝ'],
    u'\u151E' : ['Sw o','ᔞ'],
    u'\u151F' : ['Sw o:','ᔟ'],
    u'\u1520' : ['Sw o:','ᔠ'],
    u'\u1521' : ['Sw A','ᔡ'],
    u'\u1522' : ['Sw A','ᔢ'],
    u'\u1523' : ['Sw A:','ᔣ'],
    u'\u1524' : ['Sw A:','ᔤ'],
    u'\u1525' : ['S','ᔥ'],
    u'\u1526' : ['j aI','ᔦ'],
    u'\u1527' : ['j A: i','ᔧ'],
    u'\u1528' : ['j i','ᔨ'],
    u'\u1529' : ['j i:','ᔩ'],
    u'\u152A' : ['j u','ᔪ'],
    u'\u152B' : ['j u:','ᔫ'],
    u'\u152C' : ['j o:','ᔬ'],
    u'\u152D' : ['j A','ᔭ'],
    u'\u152E' : ['j A:','ᔮ'],
    u'\u152F' : ['jw e','ᔯ'],
    u'\u1530' : ['jw e','ᔰ'],
    u'\u1531' : ['jw i','ᔱ'],
    u'\u1532' : ['jw i','ᔲ'],
    u'\u1533' : ['jw i:','ᔳ'],
    u'\u1534' : ['jw i:','ᔴ'],
    u'\u1535' : ['jw o','ᔵ'],
    u'\u1536' : ['jw o','ᔶ'],
    u'\u1537' : ['jw o:','ᔷ'],
    u'\u1538' : ['jw o:','ᔸ'],
    u'\u1539' : ['jw A','ᔹ'],
    u'\u153A' : ['jw A','ᔺ'],
    u'\u153B' : ['jw A:','ᔻ'],
    u'\u153C' : ['jw A:','ᔼ'],
    u'\u153D' : ['jw A:','ᔽ'],
    u'\u153E' : ['j','ᔾ'],
    u'\u153F' : ['j','ᔿ'],
    u'\u1540' : ['j','ᕀ'],
    u'\u1541' : ['j i','ᕁ'],
    u'\u1542' : ['r aI','ᕂ'],
    u'\u1543' : ['r e','ᕃ'],
    u'\u1544' : ['l e','ᕄ'],
    u'\u1545' : ['r A: i','ᕅ'],
    u'\u1546' : ['r i','ᕆ'],
    u'\u1547' : ['r i:','ᕇ'],
    u'\u1548' : ['r u','ᕈ'],
    u'\u1549' : ['r u:','ᕉ'],
    u'\u154A' : ['l o','ᕊ'],
    u'\u154B' : ['r A','ᕋ'],
    u'\u154C' : ['r A:','ᕌ'],
    u'\u154D' : ['l a','ᕍ'],
    u'\u154E' : ['rw A:','ᕎ'],
    u'\u154F' : ['rw A:','ᕏ'],
    u'\u1550' : ['r','ᕐ'],
    u'\u1551' : ['r','ᕑ'],
    u'\u1552' : ['r','ᕒ'],
    u'\u1553' : ['v aI','ᕓ'],
    u'\u1554' : ['v A: i','ᕔ'],
    u'\u1555' : ['v i','ᕕ'],
    u'\u1556' : ['v i:','ᕖ'],
    u'\u1557' : ['v o','ᕗ'],
    u'\u1558' : ['v o:','ᕘ'],
    u'\u1559' : ['v A','ᕙ'],
    u'\u155A' : ['v A:','ᕚ'],
    u'\u155B' : ['vw A:','ᕛ'],
    u'\u155C' : ['vw A:','ᕜ'],
    u'\u155D' : ['v','ᕝ'],
    u'\u155E' : ['D e','ᕞ'],
    u'\u155F' : ['D e','ᕟ'],
    u'\u1560' : ['D i','ᕠ'],
    u'\u1561' : ['D i','ᕡ'],
    u'\u1562' : ['D i:','ᕢ'],
    u'\u1563' : ['D i:','ᕣ'],
    u'\u1564' : ['D o','ᕤ'],
    u'\u1565' : ['D o:','ᕥ'],
    u'\u1566' : ['D A','ᕦ'],
    u'\u1567' : ['D A:','ᕧ'],
    u'\u1568' : ['Dw A:','ᕨ'],
    u'\u1569' : ['Dw A:','ᕩ'],
    u'\u156A' : ['D','ᕪ'],
    u'\u156B' : ['D e','ᕫ'],
    u'\u156C' : ['D i','ᕬ'],
    u'\u156D' : ['D o','ᕭ'],
    u'\u156E' : ['D A','ᕮ'],
    u'\u156F' : ['D','ᕯ'],
    u'\u1570' : ['tj e','ᕰ'],
    u'\u1571' : ['tj i','ᕱ'],
    u'\u1572' : ['tj o','ᕲ'],
    u'\u1573' : ['tj A','ᕳ'],
    u'\u1574' : ['h e','ᕴ'],
    u'\u1575' : ['h i','ᕵ'],
    u'\u1576' : ['h i:','ᕶ'],
    u'\u1577' : ['h o','ᕷ'],
    u'\u1578' : ['h o:','ᕸ'],
    u'\u1579' : ['h A','ᕹ'],
    u'\u157A' : ['h A:','ᕺ'],
    u'\u157B' : ['h','ᕻ'],
    u'\u157C' : ['h','ᕼ'],
    u'\u157D' : ['h k','ᕽ'],
    u'\u157E' : ['q A: i','ᕾ'],
    u'\u157F' : ['q i','ᕿ'],
    u'\u1580' : ['q i:','ᖀ'],
    u'\u1581' : ['q u','ᖁ'],
    u'\u1582' : ['q u:','ᖂ'],
    u'\u1583' : ['q A','ᖃ'],
    u'\u1584' : ['q A:','ᖄ'],
    u'\u1585' : ['q','ᖅ'],
    u'\u1586' : ['th l e','ᖆ'],
    u'\u1587' : ['th l i','ᖇ'],
    u'\u1588' : ['th l o','ᖈ'],
    u'\u1589' : ['th l A','ᖉ'],
    u'\u158A' : ['r e','ᖊ'],
    u'\u158B' : ['r i','ᖋ'],
    u'\u158C' : ['r o','ᖌ'],
    u'\u158D' : ['r A','ᖍ'],
    u'\u158E' : ['N aI','ᖎ'],
    u'\u158F' : ['N i','ᖏ'],
    u'\u1590' : ['N i:','ᖐ'],
    u'\u1591' : ['N u','ᖑ'],
    u'\u1592' : ['N u:','ᖒ'],
    u'\u1593' : ['N A','ᖓ'],
    u'\u1594' : ['N A:','ᖔ'],
    u'\u1595' : ['N','ᖕ'],
    u'\u1596' : ['N','ᖖ'],
    u'\u1597' : ['S e','ᖗ'],
    u'\u1598' : ['S i','ᖘ'],
    u'\u1599' : ['S o','ᖙ'],
    u'\u159A' : ['S A','ᖚ'],
    u'\u159B' : ['D e','ᖛ'],
    u'\u159C' : ['D i','ᖜ'],
    u'\u159D' : ['D o','ᖝ'],
    u'\u159E' : ['D A','ᖞ'],
    u'\u159F' : ['D','ᖟ'],
    u'\u15A0' : ['hl i','ᖠ'],
    u'\u15A1' : ['hl i:','ᖡ'],
    u'\u15A2' : ['hl u','ᖢ'],
    u'\u15A3' : ['hl u:','ᖣ'],
    u'\u15A4' : ['hl A','ᖤ'],
    u'\u15A5' : ['hl A:','ᖥ'],
    u'\u15A6' : ['hl','ᖦ'],
    u'\u15A7' : ['D e','ᖧ'],
    u'\u15A8' : ['D i','ᖨ'],
    u'\u15A9' : ['D i:','ᖩ'],
    u'\u15AA' : ['D o','ᖪ'],
    u'\u15AB' : ['D o:','ᖫ'],
    u'\u15AC' : ['D A','ᖬ'],
    u'\u15AD' : ['D A:','ᖭ'],
    u'\u15AE' : ['D','ᖮ'],
    u'\u15AF' : ['b','ᖯ'],
    u'\u15B0' : ['e','ᖰ'],
    u'\u15B1' : ['i','ᖱ'],
    u'\u15B2' : ['o','ᖲ'],
    u'\u15B3' : ['A','ᖳ'],
    u'\u15B4' : ['w e','ᖴ'],
    u'\u15B5' : ['w i','ᖵ'],
    u'\u15B6' : ['w o','ᖶ'],
    u'\u15B7' : ['w A','ᖷ'],
    u'\u15B8' : ['n e','ᖸ'],
    u'\u15B9' : ['n i','ᖹ'],
    u'\u15BA' : ['n o','ᖺ'],
    u'\u15BB' : ['n A','ᖻ'],
    u'\u15BC' : ['k e','ᖼ'],
    u'\u15BD' : ['k i','ᖽ'],
    u'\u15BE' : ['k o','ᖾ'],
    u'\u15BF' : ['k A','ᖿ'],
    u'\u15C0' : ['h e','ᗀ'],
    u'\u15C1' : ['h i','ᗁ'],
    u'\u15C2' : ['h o','ᗂ'],
    u'\u15C3' : ['h A','ᗃ'],
    u'\u15C4' : ['Gh u','ᗄ'],
    u'\u15C5' : ['Gh o','ᗅ'],
    u'\u15C6' : ['Gh e','ᗆ'],
    u'\u15C7' : ['Gh e:','ᗇ'],
    u'\u15C8' : ['Gh i','ᗈ'],
    u'\u15C9' : ['Gh A','ᗉ'],
    u'\u15CA' : ['r u','ᗊ'],
    u'\u15CB' : ['r o','ᗋ'],
    u'\u15CC' : ['r e','ᗌ'],
    u'\u15CD' : ['r e:','ᗍ'],
    u'\u15CE' : ['r i','ᗎ'],
    u'\u15CF' : ['r A','ᗏ'],
    u'\u15D0' : ['w u','ᗐ'],
    u'\u15D1' : ['w o','ᗑ'],
    u'\u15D2' : ['w e','ᗒ'],
    u'\u15D3' : ['w e:','ᗓ'],
    u'\u15D4' : ['w i','ᗔ'],
    u'\u15D5' : ['w A','ᗕ'],
    u'\u15D6' : ['hw u','ᗖ'],
    u'\u15D7' : ['hw o','ᗗ'],
    u'\u15D8' : ['hw e','ᗘ'],
    u'\u15D9' : ['hw e:','ᗙ'],
    u'\u15DA' : ['hw i','ᗚ'],
    u'\u15DB' : ['hw A','ᗛ'],
    u'\u15DC' : ['D u','ᗜ'],
    u'\u15DD' : ['D o','ᗝ'],
    u'\u15DE' : ['D e','ᗞ'],
    u'\u15DF' : ['D e:','ᗟ'],
    u'\u15E0' : ['D i','ᗠ'],
    u'\u15E1' : ['D A','ᗡ'],
    u'\u15E2' : ['t t u','ᗢ'],
    u'\u15E3' : ['t t o','ᗣ'],
    u'\u15E4' : ['t t e','ᗤ'],
    u'\u15E5' : ['t t e:','ᗥ'],
    u'\u15E6' : ['t t i','ᗦ'],
    u'\u15E7' : ['t t A','ᗧ'],
    u'\u15E8' : ['p u','ᗨ'],
    u'\u15E9' : ['p o','ᗩ'],
    u'\u15EA' : ['p e','ᗪ'],
    u'\u15EB' : ['p e:','ᗫ'],
    u'\u15EC' : ['p i','ᗬ'],
    u'\u15ED' : ['p A','ᗭ'],
    u'\u15EE' : ['p','ᗮ'],
    u'\u15EF' : ['G u','ᗯ'],
    u'\u15F0' : ['G o','ᗰ'],
    u'\u15F1' : ['G e','ᗱ'],
    u'\u15F2' : ['G e:','ᗲ'],
    u'\u15F3' : ['G i','ᗳ'],
    u'\u15F4' : ['G A','ᗴ'],
    u'\u15F5' : ['kh u','ᗵ'],
    u'\u15F6' : ['kh o','ᗶ'],
    u'\u15F7' : ['kh e','ᗷ'],
    u'\u15F8' : ['kh e:','ᗸ'],
    u'\u15F9' : ['kh i','ᗹ'],
    u'\u15FA' : ['kh A','ᗺ'],
    u'\u15FB' : ['k k u','ᗻ'],
    u'\u15FC' : ['k k o','ᗼ'],
    u'\u15FD' : ['k k e','ᗽ'],
    u'\u15FE' : ['k k e:','ᗾ'],
    u'\u15FF' : ['k k i','ᗿ'],
    u'\u1600' : ['k k A','ᘀ'],
    u'\u1601' : ['k k','ᘁ'],
    u'\u1602' : ['n u','ᘂ'],
    u'\u1603' : ['n o','ᘃ'],
    u'\u1604' : ['n e','ᘄ'],
    u'\u1605' : ['n e:','ᘅ'],
    u'\u1606' : ['n i','ᘆ'],
    u'\u1607' : ['n A','ᘇ'],
    u'\u1608' : ['m u','ᘈ'],
    u'\u1609' : ['m o','ᘉ'],
    u'\u160A' : ['m e','ᘊ'],
    u'\u160B' : ['m e:','ᘋ'],
    u'\u160C' : ['m i','ᘌ'],
    u'\u160D' : ['m A','ᘍ'],
    u'\u160E' : ['j u','ᘎ'],
    u'\u160F' : ['j o','ᘏ'],
    u'\u1610' : ['j e','ᘐ'],
    u'\u1611' : ['j e:','ᘑ'],
    u'\u1612' : ['j i','ᘒ'],
    u'\u1613' : ['j A','ᘓ'],
    u'\u1614' : ['z u','ᘔ'],
    u'\u1615' : ['z u','ᘕ'],
    u'\u1616' : ['z o','ᘖ'],
    u'\u1617' : ['z e','ᘗ'],
    u'\u1618' : ['z e:','ᘘ'],
    u'\u1619' : ['z i','ᘙ'],
    u'\u161A' : ['z i','ᘚ'],
    u'\u161B' : ['z A','ᘛ'],
    u'\u161C' : ['z z u','ᘜ'],
    u'\u161D' : ['z z o','ᘝ'],
    u'\u161E' : ['z z e','ᘞ'],
    u'\u161F' : ['z z e:','ᘟ'],
    u'\u1620' : ['z z i','ᘠ'],
    u'\u1621' : ['z z A','ᘡ'],
    u'\u1622' : ['l u','ᘢ'],
    u'\u1623' : ['l o','ᘣ'],
    u'\u1624' : ['l e','ᘤ'],
    u'\u1625' : ['l e:','ᘥ'],
    u'\u1626' : ['l i','ᘦ'],
    u'\u1627' : ['l A','ᘧ'],
    u'\u1628' : ['d l u','ᘨ'],
    u'\u1629' : ['d l o','ᘩ'],
    u'\u162A' : ['d l e','ᘪ'],
    u'\u162B' : ['d l e:','ᘫ'],
    u'\u162C' : ['d l i','ᘬ'],
    u'\u162D' : ['d l A','ᘭ'],
    u'\u162E' : ['hl u','ᘮ'],
    u'\u162F' : ['hl o','ᘯ'],
    u'\u1630' : ['hl e','ᘰ'],
    u'\u1631' : ['hl e:','ᘱ'],
    u'\u1632' : ['hl i','ᘲ'],
    u'\u1633' : ['hl A','ᘳ'],
    u'\u1634' : ['th l u','ᘴ'],
    u'\u1635' : ['th l o','ᘵ'],
    u'\u1636' : ['th l e','ᘶ'],
    u'\u1637' : ['th l e:','ᘷ'],
    u'\u1638' : ['th l i','ᘸ'],
    u'\u1639' : ['th l A','ᘹ'],
    u'\u163A' : ['t l u','ᘺ'],
    u'\u163B' : ['t l o','ᘻ'],
    u'\u163C' : ['t l e','ᘼ'],
    u'\u163D' : ['t l e:','ᘽ'],
    u'\u163E' : ['t l i','ᘾ'],
    u'\u163F' : ['t l A','ᘿ'],
    u'\u1640' : ['z u','ᙀ'],
    u'\u1641' : ['z o','ᙁ'],
    u'\u1642' : ['z e','ᙂ'],
    u'\u1643' : ['z e:','ᙃ'],
    u'\u1644' : ['z i','ᙄ'],
    u'\u1645' : ['z A','ᙅ'],
    u'\u1646' : ['z','ᙆ'],
    u'\u1647' : ['z','ᙇ'],
    u'\u1648' : ['dz u','ᙈ'],
    u'\u1649' : ['dz o','ᙉ'],
    u'\u164A' : ['dz e','ᙊ'],
    u'\u164B' : ['dz e:','ᙋ'],
    u'\u164C' : ['dz i','ᙌ'],
    u'\u164D' : ['dz A','ᙍ'],
    u'\u164E' : ['s u','ᙎ'],
    u'\u164F' : ['s o','ᙏ'],
    u'\u1650' : ['s e','ᙐ'],
    u'\u1651' : ['s e:','ᙑ'],
    u'\u1652' : ['s i','ᙒ'],
    u'\u1653' : ['s A','ᙓ'],
    u'\u1654' : ['S u','ᙔ'],
    u'\u1655' : ['S o','ᙕ'],
    u'\u1656' : ['S e','ᙖ'],
    u'\u1657' : ['S e:','ᙗ'],
    u'\u1658' : ['S i','ᙘ'],
    u'\u1659' : ['S A','ᙙ'],
    u'\u165A' : ['S','ᙚ'],
    u'\u165B' : ['ts u','ᙛ'],
    u'\u165C' : ['ts o','ᙜ'],
    u'\u165D' : ['ts e','ᙝ'],
    u'\u165E' : ['ts e:','ᙞ'],
    u'\u165F' : ['ts i','ᙟ'],
    u'\u1660' : ['ts A','ᙠ'],
    u'\u1661' : ['Gh u','ᙡ'],
    u'\u1662' : ['Gh o','ᙢ'],
    u'\u1663' : ['Gh e','ᙣ'],
    u'\u1664' : ['Gh e:','ᙤ'],
    u'\u1665' : ['Gh i','ᙥ'],
    u'\u1666' : ['Gh A','ᙦ'],
    u'\u1667' : ['ts u','ᙧ'],
    u'\u1668' : ['ts o','ᙨ'],
    u'\u1669' : ['ts e','ᙩ'],
    u'\u166A' : ['ts e:','ᙪ'],
    u'\u166B' : ['ts i','ᙫ'],
    u'\u166C' : ['ts A','ᙬ'],
    u'\u166D' : ['(SYMBOL TO DENOTE CHRIST)','᙭'],
    u'\u166E' : ['(FULL STOP)','᙮'],
    u'\u166F' : ['q aI','ᙯ'],
    u'\u1670' : ['N aI','ᙰ'],
    u'\u1671' : ['N i','ᙱ'],
    u'\u1672' : ['N i:','ᙲ'],
    u'\u1673' : ['N o','ᙳ'],
    u'\u1674' : ['N o:','ᙴ'],
    u'\u1675' : ['N A','ᙵ'],
    u'\u1676' : ['N A:','ᙶ'],
    u'\uA000' : ['i','ꀀ'],
    u'\uA001' : ['i','ꀁ'],
    u'\uA002' : ['i','ꀂ'],
    u'\uA003' : ['i','ꀃ'],
    u'\uA004' : ['e','ꀄ'],
    u'\uA005' : ['e','ꀅ'],
    u'\uA006' : ['e','ꀆ'],
    u'\uA007' : ['e','ꀇ'],
    u'\uA008' : ['A','ꀈ'],
    u'\uA009' : ['A','ꀉ'],
    u'\uA00A' : ['A','ꀊ'],
    u'\uA00B' : ['A','ꀋ'],
    u'\uA00C' : ['>','ꀌ'],
    u'\uA00D' : ['>','ꀍ'],
    u'\uA00E' : ['>','ꀎ'],
    u'\uA00F' : ['o','ꀏ'],
    u'\uA010' : ['o','ꀐ'],
    u'\uA011' : ['o','ꀑ'],
    u'\uA012' : ['o','ꀒ'],
    u'\uA013' : ['&','ꀓ'],
    u'\uA014' : ['&','ꀔ'],
    u'\uA015' : ['G u','ꀕ'],
    u'\uA016' : ['p i','ꀖ'],
    u'\uA017' : ['p i','ꀗ'],
    u'\uA018' : ['p i','ꀘ'],
    u'\uA019' : ['p i','ꀙ'],
    u'\uA01A' : ['p e','ꀚ'],
    u'\uA01B' : ['p e','ꀛ'],
    u'\uA01C' : ['p e','ꀜ'],
    u'\uA01D' : ['p e','ꀝ'],
    u'\uA01E' : ['p A','ꀞ'],
    u'\uA01F' : ['p A','ꀟ'],
    u'\uA020' : ['p A','ꀠ'],
    u'\uA021' : ['p A','ꀡ'],
    u'\uA022' : ['p >','ꀢ'],
    u'\uA023' : ['p >','ꀣ'],
    u'\uA024' : ['p >','ꀤ'],
    u'\uA025' : ['p o','ꀥ'],
    u'\uA026' : ['p o','ꀦ'],
    u'\uA027' : ['p o','ꀧ'],
    u'\uA028' : ['p o','ꀨ'],
    u'\uA029' : ['p &','ꀩ'],
    u'\uA02A' : ['p &','ꀪ'],
    u'\uA02B' : ['p &','ꀫ'],
    u'\uA02C' : ['p u','ꀬ'],
    u'\uA02D' : ['p u','ꀭ'],
    u'\uA02E' : ['p u','ꀮ'],
    u'\uA02F' : ['p u','ꀯ'],
    u'\uA030' : ['p u','ꀰ'],
    u'\uA031' : ['p u','ꀱ'],
    u'\uA032' : ['p l','ꀲ'],
    u'\uA033' : ['p l','ꀳ'],
    u'\uA034' : ['p l','ꀴ'],
    u'\uA035' : ['p l','ꀵ'],
    u'\uA036' : ['p i','ꀶ'],
    u'\uA037' : ['p i','ꀷ'],
    u'\uA038' : ['ph i','ꀸ'],
    u'\uA039' : ['ph i','ꀹ'],
    u'\uA03A' : ['ph i','ꀺ'],
    u'\uA03B' : ['ph i','ꀻ'],
    u'\uA03C' : ['ph e','ꀼ'],
    u'\uA03D' : ['ph e','ꀽ'],
    u'\uA03E' : ['ph e','ꀾ'],
    u'\uA03F' : ['ph A','ꀿ'],
    u'\uA040' : ['ph A','ꁀ'],
    u'\uA041' : ['ph A','ꁁ'],
    u'\uA042' : ['ph A','ꁂ'],
    u'\uA043' : ['ph >','ꁃ'],
    u'\uA044' : ['ph >','ꁄ'],
    u'\uA045' : ['ph >','ꁅ'],
    u'\uA046' : ['ph o','ꁆ'],
    u'\uA047' : ['ph o','ꁇ'],
    u'\uA048' : ['ph o','ꁈ'],
    u'\uA049' : ['ph o','ꁉ'],
    u'\uA04A' : ['ph u','ꁊ'],
    u'\uA04B' : ['ph u','ꁋ'],
    u'\uA04C' : ['ph u','ꁌ'],
    u'\uA04D' : ['ph u','ꁍ'],
    u'\uA04E' : ['ph u','ꁎ'],
    u'\uA04F' : ['ph u','ꁏ'],
    u'\uA050' : ['ph l','ꁐ'],
    u'\uA051' : ['ph l','ꁑ'],
    u'\uA052' : ['ph l','ꁒ'],
    u'\uA053' : ['ph l','ꁓ'],
    u'\uA054' : ['ph i','ꁔ'],
    u'\uA055' : ['ph i','ꁕ'],
    u'\uA056' : ['b i','ꁖ'],
    u'\uA057' : ['b i','ꁗ'],
    u'\uA058' : ['b i','ꁘ'],
    u'\uA059' : ['b i','ꁙ'],
    u'\uA05A' : ['b e','ꁚ'],
    u'\uA05B' : ['b e','ꁛ'],
    u'\uA05C' : ['b e','ꁜ'],
    u'\uA05D' : ['b e','ꁝ'],
    u'\uA05E' : ['b A','ꁞ'],
    u'\uA05F' : ['b A','ꁟ'],
    u'\uA060' : ['b A','ꁠ'],
    u'\uA061' : ['b A','ꁡ'],
    u'\uA062' : ['b >','ꁢ'],
    u'\uA063' : ['b >','ꁣ'],
    u'\uA064' : ['b >','ꁤ'],
    u'\uA065' : ['b o','ꁥ'],
    u'\uA066' : ['b o','ꁦ'],
    u'\uA067' : ['b o','ꁧ'],
    u'\uA068' : ['b o','ꁨ'],
    u'\uA069' : ['b &','ꁩ'],
    u'\uA06A' : ['b &','ꁪ'],
    u'\uA06B' : ['b &','ꁫ'],
    u'\uA06C' : ['b u','ꁬ'],
    u'\uA06D' : ['b u','ꁭ'],
    u'\uA06E' : ['b u','ꁮ'],
    u'\uA06F' : ['b u','ꁯ'],
    u'\uA070' : ['b u','ꁰ'],
    u'\uA071' : ['b u','ꁱ'],
    u'\uA072' : ['b l','ꁲ'],
    u'\uA073' : ['b l','ꁳ'],
    u'\uA074' : ['b l','ꁴ'],
    u'\uA075' : ['b l','ꁵ'],
    u'\uA076' : ['m b i','ꁶ'],
    u'\uA077' : ['m b i','ꁷ'],
    u'\uA078' : ['m b i','ꁸ'],
    u'\uA079' : ['m b i','ꁹ'],
    u'\uA07A' : ['m b e','ꁺ'],
    u'\uA07B' : ['m b e','ꁻ'],
    u'\uA07C' : ['m b e','ꁼ'],
    u'\uA07D' : ['m b A','ꁽ'],
    u'\uA07E' : ['m b A','ꁾ'],
    u'\uA07F' : ['m b A','ꁿ'],
    u'\uA080' : ['m b A','ꂀ'],
    u'\uA081' : ['m b o','ꂁ'],
    u'\uA082' : ['m b o','ꂂ'],
    u'\uA083' : ['m b o','ꂃ'],
    u'\uA084' : ['m b o','ꂄ'],
    u'\uA085' : ['m b u','ꂅ'],
    u'\uA086' : ['m b u','ꂆ'],
    u'\uA087' : ['m b u','ꂇ'],
    u'\uA088' : ['m b u','ꂈ'],
    u'\uA089' : ['m b u','ꂉ'],
    u'\uA08A' : ['m b u','ꂊ'],
    u'\uA08B' : ['m b l','ꂋ'],
    u'\uA08C' : ['m b l','ꂌ'],
    u'\uA08D' : ['m b l','ꂍ'],
    u'\uA08E' : ['m b l','ꂎ'],
    u'\uA08F' : ['m b i','ꂏ'],
    u'\uA090' : ['m b i','ꂐ'],
    u'\uA091' : ['m0 i','ꂑ'],
    u'\uA092' : ['m0 i','ꂒ'],
    u'\uA093' : ['m0 i','ꂓ'],
    u'\uA094' : ['m0 i','ꂔ'],
    u'\uA095' : ['m0 e','ꂕ'],
    u'\uA096' : ['m0 e','ꂖ'],
    u'\uA097' : ['m0 e','ꂗ'],
    u'\uA098' : ['m0 A','ꂘ'],
    u'\uA099' : ['m0 A','ꂙ'],
    u'\uA09A' : ['m0 A','ꂚ'],
    u'\uA09B' : ['m0 A','ꂛ'],
    u'\uA09C' : ['m0 >','ꂜ'],
    u'\uA09D' : ['m0 >','ꂝ'],
    u'\uA09E' : ['m0 >','ꂞ'],
    u'\uA09F' : ['m0 o','ꂟ'],
    u'\uA0A0' : ['m0 o','ꂠ'],
    u'\uA0A1' : ['m0 o','ꂡ'],
    u'\uA0A2' : ['m0 o','ꂢ'],
    u'\uA0A3' : ['m0 u','ꂣ'],
    u'\uA0A4' : ['m0 u','ꂤ'],
    u'\uA0A5' : ['m0 u','ꂥ'],
    u'\uA0A6' : ['m0 u','ꂦ'],
    u'\uA0A7' : ['m0 u','ꂧ'],
    u'\uA0A8' : ['m0 u','ꂨ'],
    u'\uA0A9' : ['m0 l','ꂩ'],
    u'\uA0AA' : ['m0 l','ꂪ'],
    u'\uA0AB' : ['m0 l','ꂫ'],
    u'\uA0AC' : ['m0 i','ꂬ'],
    u'\uA0AD' : ['m0 i','ꂭ'],
    u'\uA0AE' : ['m i','ꂮ'],
    u'\uA0AF' : ['m i','ꂯ'],
    u'\uA0B0' : ['m i','ꂰ'],
    u'\uA0B1' : ['m i','ꂱ'],
    u'\uA0B2' : ['m e','ꂲ'],
    u'\uA0B3' : ['m e','ꂳ'],
    u'\uA0B4' : ['m e','ꂴ'],
    u'\uA0B5' : ['m A','ꂵ'],
    u'\uA0B6' : ['m A','ꂶ'],
    u'\uA0B7' : ['m A','ꂷ'],
    u'\uA0B8' : ['m A','ꂸ'],
    u'\uA0B9' : ['m >','ꂹ'],
    u'\uA0BA' : ['m >','ꂺ'],
    u'\uA0BB' : ['m >','ꂻ'],
    u'\uA0BC' : ['m >','ꂼ'],
    u'\uA0BD' : ['m o','ꂽ'],
    u'\uA0BE' : ['m o','ꂾ'],
    u'\uA0BF' : ['m o','ꂿ'],
    u'\uA0C0' : ['m o','ꃀ'],
    u'\uA0C1' : ['m &','ꃁ'],
    u'\uA0C2' : ['m &','ꃂ'],
    u'\uA0C3' : ['m u','ꃃ'],
    u'\uA0C4' : ['m u','ꃄ'],
    u'\uA0C5' : ['m u','ꃅ'],
    u'\uA0C6' : ['m u','ꃆ'],
    u'\uA0C7' : ['m u','ꃇ'],
    u'\uA0C8' : ['m u','ꃈ'],
    u'\uA0C9' : ['m l','ꃉ'],
    u'\uA0CA' : ['m l','ꃊ'],
    u'\uA0CB' : ['m l','ꃋ'],
    u'\uA0CC' : ['m l','ꃌ'],
    u'\uA0CD' : ['f i','ꃍ'],
    u'\uA0CE' : ['f i','ꃎ'],
    u'\uA0CF' : ['f i','ꃏ'],
    u'\uA0D0' : ['f i','ꃐ'],
    u'\uA0D1' : ['f A','ꃑ'],
    u'\uA0D2' : ['f A','ꃒ'],
    u'\uA0D3' : ['f A','ꃓ'],
    u'\uA0D4' : ['f A','ꃔ'],
    u'\uA0D5' : ['f o','ꃕ'],
    u'\uA0D6' : ['f o','ꃖ'],
    u'\uA0D7' : ['f o','ꃗ'],
    u'\uA0D8' : ['f u','ꃘ'],
    u'\uA0D9' : ['f u','ꃙ'],
    u'\uA0DA' : ['f u','ꃚ'],
    u'\uA0DB' : ['f u','ꃛ'],
    u'\uA0DC' : ['f u','ꃜ'],
    u'\uA0DD' : ['f u','ꃝ'],
    u'\uA0DE' : ['f l','ꃞ'],
    u'\uA0DF' : ['f l','ꃟ'],
    u'\uA0E0' : ['f l','ꃠ'],
    u'\uA0E1' : ['f l','ꃡ'],
    u'\uA0E2' : ['v i','ꃢ'],
    u'\uA0E3' : ['v i','ꃣ'],
    u'\uA0E4' : ['v i','ꃤ'],
    u'\uA0E5' : ['v i','ꃥ'],
    u'\uA0E6' : ['v e','ꃦ'],
    u'\uA0E7' : ['v e','ꃧ'],
    u'\uA0E8' : ['v e','ꃨ'],
    u'\uA0E9' : ['v e','ꃩ'],
    u'\uA0EA' : ['v A','ꃪ'],
    u'\uA0EB' : ['v A','ꃫ'],
    u'\uA0EC' : ['v A','ꃬ'],
    u'\uA0ED' : ['v A','ꃭ'],
    u'\uA0EE' : ['v o','ꃮ'],
    u'\uA0EF' : ['v o','ꃯ'],
    u'\uA0F0' : ['v o','ꃰ'],
    u'\uA0F1' : ['v o','ꃱ'],
    u'\uA0F2' : ['v &','ꃲ'],
    u'\uA0F3' : ['v &','ꃳ'],
    u'\uA0F4' : ['v u','ꃴ'],
    u'\uA0F5' : ['v u','ꃵ'],
    u'\uA0F6' : ['v u','ꃶ'],
    u'\uA0F7' : ['v u','ꃷ'],
    u'\uA0F8' : ['v u','ꃸ'],
    u'\uA0F9' : ['v u','ꃹ'],
    u'\uA0FA' : ['v l','ꃺ'],
    u'\uA0FB' : ['v l','ꃻ'],
    u'\uA0FC' : ['v l','ꃼ'],
    u'\uA0FD' : ['v l','ꃽ'],
    u'\uA0FE' : ['v i','ꃾ'],
    u'\uA0FF' : ['v i','ꃿ'],
    u'\uA100' : ['t i','ꄀ'],
    u'\uA101' : ['t i','ꄁ'],
    u'\uA102' : ['t i','ꄂ'],
    u'\uA103' : ['t i','ꄃ'],
    u'\uA104' : ['t e','ꄄ'],
    u'\uA105' : ['t e','ꄅ'],
    u'\uA106' : ['t e','ꄆ'],
    u'\uA107' : ['t A','ꄇ'],
    u'\uA108' : ['t A','ꄈ'],
    u'\uA109' : ['t A','ꄉ'],
    u'\uA10A' : ['t A','ꄊ'],
    u'\uA10B' : ['t >','ꄋ'],
    u'\uA10C' : ['t >','ꄌ'],
    u'\uA10D' : ['t o','ꄍ'],
    u'\uA10E' : ['t o','ꄎ'],
    u'\uA10F' : ['t o','ꄏ'],
    u'\uA110' : ['t o','ꄐ'],
    u'\uA111' : ['t &','ꄑ'],
    u'\uA112' : ['t &','ꄒ'],
    u'\uA113' : ['t &','ꄓ'],
    u'\uA114' : ['t u','ꄔ'],
    u'\uA115' : ['t u','ꄕ'],
    u'\uA116' : ['t u','ꄖ'],
    u'\uA117' : ['t u','ꄗ'],
    u'\uA118' : ['t u','ꄘ'],
    u'\uA119' : ['t u','ꄙ'],
    u'\uA11A' : ['th i','ꄚ'],
    u'\uA11B' : ['th i','ꄛ'],
    u'\uA11C' : ['th i','ꄜ'],
    u'\uA11D' : ['th i','ꄝ'],
    u'\uA11E' : ['th e','ꄞ'],
    u'\uA11F' : ['th e','ꄟ'],
    u'\uA120' : ['th e','ꄠ'],
    u'\uA121' : ['th A','ꄡ'],
    u'\uA122' : ['th A','ꄢ'],
    u'\uA123' : ['th A','ꄣ'],
    u'\uA124' : ['th A','ꄤ'],
    u'\uA125' : ['th >','ꄥ'],
    u'\uA126' : ['th >','ꄦ'],
    u'\uA127' : ['th >','ꄧ'],
    u'\uA128' : ['th >','ꄨ'],
    u'\uA129' : ['th o','ꄩ'],
    u'\uA12A' : ['th o','ꄪ'],
    u'\uA12B' : ['th o','ꄫ'],
    u'\uA12C' : ['th o','ꄬ'],
    u'\uA12D' : ['th &','ꄭ'],
    u'\uA12E' : ['th &','ꄮ'],
    u'\uA12F' : ['th &','ꄯ'],
    u'\uA130' : ['th u','ꄰ'],
    u'\uA131' : ['th u','ꄱ'],
    u'\uA132' : ['th u','ꄲ'],
    u'\uA133' : ['th u','ꄳ'],
    u'\uA134' : ['th u','ꄴ'],
    u'\uA135' : ['th u','ꄵ'],
    u'\uA136' : ['d i','ꄶ'],
    u'\uA137' : ['d i','ꄷ'],
    u'\uA138' : ['d i','ꄸ'],
    u'\uA139' : ['d i','ꄹ'],
    u'\uA13A' : ['d e','ꄺ'],
    u'\uA13B' : ['d e','ꄻ'],
    u'\uA13C' : ['d e','ꄼ'],
    u'\uA13D' : ['d A','ꄽ'],
    u'\uA13E' : ['d A','ꄾ'],
    u'\uA13F' : ['d A','ꄿ'],
    u'\uA140' : ['d A','ꅀ'],
    u'\uA141' : ['d >','ꅁ'],
    u'\uA142' : ['d >','ꅂ'],
    u'\uA143' : ['d >','ꅃ'],
    u'\uA144' : ['d o','ꅄ'],
    u'\uA145' : ['d o','ꅅ'],
    u'\uA146' : ['d o','ꅆ'],
    u'\uA147' : ['d o','ꅇ'],
    u'\uA148' : ['d &','ꅈ'],
    u'\uA149' : ['d &','ꅉ'],
    u'\uA14A' : ['d &','ꅊ'],
    u'\uA14B' : ['d u','ꅋ'],
    u'\uA14C' : ['d u','ꅌ'],
    u'\uA14D' : ['d u','ꅍ'],
    u'\uA14E' : ['d u','ꅎ'],
    u'\uA14F' : ['d u','ꅏ'],
    u'\uA150' : ['d u','ꅐ'],
    u'\uA151' : ['n d i','ꅑ'],
    u'\uA152' : ['n d i','ꅒ'],
    u'\uA153' : ['n d i','ꅓ'],
    u'\uA154' : ['n d i','ꅔ'],
    u'\uA155' : ['n d e','ꅕ'],
    u'\uA156' : ['n d e','ꅖ'],
    u'\uA157' : ['n d A','ꅗ'],
    u'\uA158' : ['n d A','ꅘ'],
    u'\uA159' : ['n d A','ꅙ'],
    u'\uA15A' : ['n d A','ꅚ'],
    u'\uA15B' : ['n d o','ꅛ'],
    u'\uA15C' : ['n d o','ꅜ'],
    u'\uA15D' : ['n d o','ꅝ'],
    u'\uA15E' : ['n d o','ꅞ'],
    u'\uA15F' : ['n d &','ꅟ'],
    u'\uA160' : ['n d &','ꅠ'],
    u'\uA161' : ['n d &','ꅡ'],
    u'\uA162' : ['n d u','ꅢ'],
    u'\uA163' : ['n d u','ꅣ'],
    u'\uA164' : ['n d u','ꅤ'],
    u'\uA165' : ['n d u','ꅥ'],
    u'\uA166' : ['n d u','ꅦ'],
    u'\uA167' : ['n d u','ꅧ'],
    u'\uA168' : ['n0 i','ꅨ'],
    u'\uA169' : ['n0 i','ꅩ'],
    u'\uA16A' : ['n0 i','ꅪ'],
    u'\uA16B' : ['n0 i','ꅫ'],
    u'\uA16C' : ['n0 e','ꅬ'],
    u'\uA16D' : ['n0 e','ꅭ'],
    u'\uA16E' : ['n0 e','ꅮ'],
    u'\uA16F' : ['n0 e','ꅯ'],
    u'\uA170' : ['n0 A','ꅰ'],
    u'\uA171' : ['n0 A','ꅱ'],
    u'\uA172' : ['n0 A','ꅲ'],
    u'\uA173' : ['n0 A','ꅳ'],
    u'\uA174' : ['n0 >','ꅴ'],
    u'\uA175' : ['n0 >','ꅵ'],
    u'\uA176' : ['n0 o','ꅶ'],
    u'\uA177' : ['n0 o','ꅷ'],
    u'\uA178' : ['n0 o','ꅸ'],
    u'\uA179' : ['n0 &','ꅹ'],
    u'\uA17A' : ['n0 &','ꅺ'],
    u'\uA17B' : ['n0 &','ꅻ'],
    u'\uA17C' : ['n0 u','ꅼ'],
    u'\uA17D' : ['n i','ꅽ'],
    u'\uA17E' : ['n i','ꅾ'],
    u'\uA17F' : ['n i','ꅿ'],
    u'\uA180' : ['n i','ꆀ'],
    u'\uA181' : ['n e','ꆁ'],
    u'\uA182' : ['n e','ꆂ'],
    u'\uA183' : ['n e','ꆃ'],
    u'\uA184' : ['n A','ꆄ'],
    u'\uA185' : ['n A','ꆅ'],
    u'\uA186' : ['n A','ꆆ'],
    u'\uA187' : ['n >','ꆇ'],
    u'\uA188' : ['n >','ꆈ'],
    u'\uA189' : ['n >','ꆉ'],
    u'\uA18A' : ['n o','ꆊ'],
    u'\uA18B' : ['n o','ꆋ'],
    u'\uA18C' : ['n o','ꆌ'],
    u'\uA18D' : ['n o','ꆍ'],
    u'\uA18E' : ['n &','ꆎ'],
    u'\uA18F' : ['n &','ꆏ'],
    u'\uA190' : ['n &','ꆐ'],
    u'\uA191' : ['n u','ꆑ'],
    u'\uA192' : ['n u','ꆒ'],
    u'\uA193' : ['n u','ꆓ'],
    u'\uA194' : ['n u','ꆔ'],
    u'\uA195' : ['n u','ꆕ'],
    u'\uA196' : ['n u','ꆖ'],
    u'\uA197' : ['hl i','ꆗ'],
    u'\uA198' : ['hl i','ꆘ'],
    u'\uA199' : ['hl i','ꆙ'],
    u'\uA19A' : ['hl i','ꆚ'],
    u'\uA19B' : ['hl e','ꆛ'],
    u'\uA19C' : ['hl e','ꆜ'],
    u'\uA19D' : ['hl e','ꆝ'],
    u'\uA19E' : ['hl A','ꆞ'],
    u'\uA19F' : ['hl A','ꆟ'],
    u'\uA1A0' : ['hl A','ꆠ'],
    u'\uA1A1' : ['hl A','ꆡ'],
    u'\uA1A2' : ['hl >','ꆢ'],
    u'\uA1A3' : ['hl >','ꆣ'],
    u'\uA1A4' : ['hl >','ꆤ'],
    u'\uA1A5' : ['hl o','ꆥ'],
    u'\uA1A6' : ['hl o','ꆦ'],
    u'\uA1A7' : ['hl o','ꆧ'],
    u'\uA1A8' : ['hl &','ꆨ'],
    u'\uA1A9' : ['hl &','ꆩ'],
    u'\uA1AA' : ['hl &','ꆪ'],
    u'\uA1AB' : ['hl u','ꆫ'],
    u'\uA1AC' : ['hl u','ꆬ'],
    u'\uA1AD' : ['hl u','ꆭ'],
    u'\uA1AE' : ['hl u','ꆮ'],
    u'\uA1AF' : ['hl u','ꆯ'],
    u'\uA1B0' : ['hl u','ꆰ'],
    u'\uA1B1' : ['hl l','ꆱ'],
    u'\uA1B2' : ['hl l','ꆲ'],
    u'\uA1B3' : ['hl l','ꆳ'],
    u'\uA1B4' : ['hl l','ꆴ'],
    u'\uA1B5' : ['hl i','ꆵ'],
    u'\uA1B6' : ['hl i','ꆶ'],
    u'\uA1B7' : ['l i','ꆷ'],
    u'\uA1B8' : ['l i','ꆸ'],
    u'\uA1B9' : ['l i','ꆹ'],
    u'\uA1BA' : ['l i','ꆺ'],
    u'\uA1BB' : ['l e','ꆻ'],
    u'\uA1BC' : ['l e','ꆼ'],
    u'\uA1BD' : ['l e','ꆽ'],
    u'\uA1BE' : ['l e','ꆾ'],
    u'\uA1BF' : ['l A','ꆿ'],
    u'\uA1C0' : ['l A','ꇀ'],
    u'\uA1C1' : ['l A','ꇁ'],
    u'\uA1C2' : ['l A','ꇂ'],
    u'\uA1C3' : ['l >','ꇃ'],
    u'\uA1C4' : ['l >','ꇄ'],
    u'\uA1C5' : ['l >','ꇅ'],
    u'\uA1C6' : ['l >','ꇆ'],
    u'\uA1C7' : ['l o','ꇇ'],
    u'\uA1C8' : ['l o','ꇈ'],
    u'\uA1C9' : ['l o','ꇉ'],
    u'\uA1CA' : ['l o','ꇊ'],
    u'\uA1CB' : ['l &','ꇋ'],
    u'\uA1CC' : ['l &','ꇌ'],
    u'\uA1CD' : ['l &','ꇍ'],
    u'\uA1CE' : ['l u','ꇎ'],
    u'\uA1CF' : ['l u','ꇏ'],
    u'\uA1D0' : ['l u','ꇐ'],
    u'\uA1D1' : ['l u','ꇑ'],
    u'\uA1D2' : ['l u','ꇒ'],
    u'\uA1D3' : ['l u','ꇓ'],
    u'\uA1D4' : ['l l','ꇔ'],
    u'\uA1D5' : ['l l','ꇕ'],
    u'\uA1D6' : ['l l','ꇖ'],
    u'\uA1D7' : ['l l','ꇗ'],
    u'\uA1D8' : ['l i','ꇘ'],
    u'\uA1D9' : ['l i','ꇙ'],
    u'\uA1DA' : ['k i','ꇚ'],
    u'\uA1DB' : ['k i','ꇛ'],
    u'\uA1DC' : ['k i','ꇜ'],
    u'\uA1DD' : ['k i','ꇝ'],
    u'\uA1DE' : ['k e','ꇞ'],
    u'\uA1DF' : ['k e','ꇟ'],
    u'\uA1E0' : ['k e','ꇠ'],
    u'\uA1E1' : ['k e','ꇡ'],
    u'\uA1E2' : ['k A','ꇢ'],
    u'\uA1E3' : ['k A','ꇣ'],
    u'\uA1E4' : ['k A','ꇤ'],
    u'\uA1E5' : ['k A','ꇥ'],
    u'\uA1E6' : ['k >','ꇦ'],
    u'\uA1E7' : ['k >','ꇧ'],
    u'\uA1E8' : ['k >','ꇨ'],
    u'\uA1E9' : ['k >','ꇩ'],
    u'\uA1EA' : ['k o','ꇪ'],
    u'\uA1EB' : ['k o','ꇫ'],
    u'\uA1EC' : ['k o','ꇬ'],
    u'\uA1ED' : ['k o','ꇭ'],
    u'\uA1EE' : ['k &','ꇮ'],
    u'\uA1EF' : ['k &','ꇯ'],
    u'\uA1F0' : ['k &','ꇰ'],
    u'\uA1F1' : ['k &','ꇱ'],
    u'\uA1F2' : ['k u','ꇲ'],
    u'\uA1F3' : ['k u','ꇳ'],
    u'\uA1F4' : ['k u','ꇴ'],
    u'\uA1F5' : ['k u','ꇵ'],
    u'\uA1F6' : ['k u','ꇶ'],
    u'\uA1F7' : ['k u','ꇷ'],
    u'\uA1F8' : ['kh i','ꇸ'],
    u'\uA1F9' : ['kh i','ꇹ'],
    u'\uA1FA' : ['kh i','ꇺ'],
    u'\uA1FB' : ['kh i','ꇻ'],
    u'\uA1FC' : ['kh e','ꇼ'],
    u'\uA1FD' : ['kh e','ꇽ'],
    u'\uA1FE' : ['kh e','ꇾ'],
    u'\uA1FF' : ['kh A','ꇿ'],
    u'\uA200' : ['kh A','ꈀ'],
    u'\uA201' : ['kh A','ꈁ'],
    u'\uA202' : ['kh A','ꈂ'],
    u'\uA203' : ['kh >','ꈃ'],
    u'\uA204' : ['kh >','ꈄ'],
    u'\uA205' : ['kh >','ꈅ'],
    u'\uA206' : ['kh o','ꈆ'],
    u'\uA207' : ['kh o','ꈇ'],
    u'\uA208' : ['kh o','ꈈ'],
    u'\uA209' : ['kh o','ꈉ'],
    u'\uA20A' : ['kh &','ꈊ'],
    u'\uA20B' : ['kh &','ꈋ'],
    u'\uA20C' : ['kh &','ꈌ'],
    u'\uA20D' : ['kh &','ꈍ'],
    u'\uA20E' : ['kh u','ꈎ'],
    u'\uA20F' : ['kh u','ꈏ'],
    u'\uA210' : ['kh u','ꈐ'],
    u'\uA211' : ['kh u','ꈑ'],
    u'\uA212' : ['kh u','ꈒ'],
    u'\uA213' : ['kh u','ꈓ'],
    u'\uA214' : ['g i','ꈔ'],
    u'\uA215' : ['g i','ꈕ'],
    u'\uA216' : ['g i','ꈖ'],
    u'\uA217' : ['g e','ꈗ'],
    u'\uA218' : ['g e','ꈘ'],
    u'\uA219' : ['g e','ꈙ'],
    u'\uA21A' : ['g A','ꈚ'],
    u'\uA21B' : ['g A','ꈛ'],
    u'\uA21C' : ['g A','ꈜ'],
    u'\uA21D' : ['g A','ꈝ'],
    u'\uA21E' : ['g >','ꈞ'],
    u'\uA21F' : ['g >','ꈟ'],
    u'\uA220' : ['g >','ꈠ'],
    u'\uA221' : ['g >','ꈡ'],
    u'\uA222' : ['g o','ꈢ'],
    u'\uA223' : ['g o','ꈣ'],
    u'\uA224' : ['g o','ꈤ'],
    u'\uA225' : ['g o','ꈥ'],
    u'\uA226' : ['g &','ꈦ'],
    u'\uA227' : ['g &','ꈧ'],
    u'\uA228' : ['g &','ꈨ'],
    u'\uA229' : ['g &','ꈩ'],
    u'\uA22A' : ['g u','ꈪ'],
    u'\uA22B' : ['g u','ꈫ'],
    u'\uA22C' : ['g u','ꈬ'],
    u'\uA22D' : ['g u','ꈭ'],
    u'\uA22E' : ['g u','ꈮ'],
    u'\uA22F' : ['g u','ꈯ'],
    u'\uA230' : ['N g e','ꈰ'],
    u'\uA231' : ['N g e','ꈱ'],
    u'\uA232' : ['N g A','ꈲ'],
    u'\uA233' : ['N g A','ꈳ'],
    u'\uA234' : ['N g A','ꈴ'],
    u'\uA235' : ['N g A','ꈵ'],
    u'\uA236' : ['N g >','ꈶ'],
    u'\uA237' : ['N g >','ꈷ'],
    u'\uA238' : ['N g >','ꈸ'],
    u'\uA239' : ['N g o','ꈹ'],
    u'\uA23A' : ['N g o','ꈺ'],
    u'\uA23B' : ['N g o','ꈻ'],
    u'\uA23C' : ['N g o','ꈼ'],
    u'\uA23D' : ['N g &','ꈽ'],
    u'\uA23E' : ['N g &','ꈾ'],
    u'\uA23F' : ['N g &','ꈿ'],
    u'\uA240' : ['N g u','ꉀ'],
    u'\uA241' : ['N g u','ꉁ'],
    u'\uA242' : ['N g u','ꉂ'],
    u'\uA243' : ['N g u','ꉃ'],
    u'\uA244' : ['N g u','ꉄ'],
    u'\uA245' : ['N g u','ꉅ'],
    u'\uA246' : ['h i','ꉆ'],
    u'\uA247' : ['h i','ꉇ'],
    u'\uA248' : ['h i','ꉈ'],
    u'\uA249' : ['h i','ꉉ'],
    u'\uA24A' : ['h e','ꉊ'],
    u'\uA24B' : ['h e','ꉋ'],
    u'\uA24C' : ['h e','ꉌ'],
    u'\uA24D' : ['h e','ꉍ'],
    u'\uA24E' : ['h A','ꉎ'],
    u'\uA24F' : ['h A','ꉏ'],
    u'\uA250' : ['h A','ꉐ'],
    u'\uA251' : ['h A','ꉑ'],
    u'\uA252' : ['h >','ꉒ'],
    u'\uA253' : ['h >','ꉓ'],
    u'\uA254' : ['h >','ꉔ'],
    u'\uA255' : ['h >','ꉕ'],
    u'\uA256' : ['h o','ꉖ'],
    u'\uA257' : ['h o','ꉗ'],
    u'\uA258' : ['h o','ꉘ'],
    u'\uA259' : ['h o','ꉙ'],
    u'\uA25A' : ['h &','ꉚ'],
    u'\uA25B' : ['h &','ꉛ'],
    u'\uA25C' : ['h &','ꉜ'],
    u'\uA25D' : ['N e','ꉝ'],
    u'\uA25E' : ['N e','ꉞ'],
    u'\uA25F' : ['N e','ꉟ'],
    u'\uA260' : ['N A','ꉠ'],
    u'\uA261' : ['N A','ꉡ'],
    u'\uA262' : ['N A','ꉢ'],
    u'\uA263' : ['N A','ꉣ'],
    u'\uA264' : ['N >','ꉤ'],
    u'\uA265' : ['N >','ꉥ'],
    u'\uA266' : ['N >','ꉦ'],
    u'\uA267' : ['N o','ꉧ'],
    u'\uA268' : ['N o','ꉨ'],
    u'\uA269' : ['N o','ꉩ'],
    u'\uA26A' : ['N o','ꉪ'],
    u'\uA26B' : ['N &','ꉫ'],
    u'\uA26C' : ['N &','ꉬ'],
    u'\uA26D' : ['N &','ꉭ'],
    u'\uA26E' : ['x i','ꉮ'],
    u'\uA26F' : ['X e','ꉯ'],
    u'\uA270' : ['x e','ꉰ'],
    u'\uA271' : ['x A','ꉱ'],
    u'\uA272' : ['x A','ꉲ'],
    u'\uA273' : ['x A','ꉳ'],
    u'\uA274' : ['x A','ꉴ'],
    u'\uA275' : ['x >','ꉵ'],
    u'\uA276' : ['x >','ꉶ'],
    u'\uA277' : ['x >','ꉷ'],
    u'\uA278' : ['x >','ꉸ'],
    u'\uA279' : ['x o','ꉹ'],
    u'\uA27A' : ['x o','ꉺ'],
    u'\uA27B' : ['x o','ꉻ'],
    u'\uA27C' : ['x o','ꉼ'],
    u'\uA27D' : ['x &','ꉽ'],
    u'\uA27E' : ['x &','ꉾ'],
    u'\uA27F' : ['x &','ꉿ'],
    u'\uA280' : ['G A','ꊀ'],
    u'\uA281' : ['G A','ꊁ'],
    u'\uA282' : ['G A','ꊂ'],
    u'\uA283' : ['G A','ꊃ'],
    u'\uA284' : ['G >','ꊄ'],
    u'\uA285' : ['G >','ꊅ'],
    u'\uA286' : ['G >','ꊆ'],
    u'\uA287' : ['G o','ꊇ'],
    u'\uA288' : ['G o','ꊈ'],
    u'\uA289' : ['G o','ꊉ'],
    u'\uA28A' : ['G &','ꊊ'],
    u'\uA28B' : ['G &','ꊋ'],
    u'\uA28C' : ['G &','ꊌ'],
    u'\uA28D' : ['ts i','ꊍ'],
    u'\uA28E' : ['ts i','ꊎ'],
    u'\uA28F' : ['ts i','ꊏ'],
    u'\uA290' : ['ts i','ꊐ'],
    u'\uA291' : ['ts e','ꊑ'],
    u'\uA292' : ['ts e','ꊒ'],
    u'\uA293' : ['ts e','ꊓ'],
    u'\uA294' : ['ts A','ꊔ'],
    u'\uA295' : ['ts A','ꊕ'],
    u'\uA296' : ['ts A','ꊖ'],
    u'\uA297' : ['ts A','ꊗ'],
    u'\uA298' : ['ts >','ꊘ'],
    u'\uA299' : ['ts >','ꊙ'],
    u'\uA29A' : ['ts >','ꊚ'],
    u'\uA29B' : ['ts o','ꊛ'],
    u'\uA29C' : ['ts o','ꊜ'],
    u'\uA29D' : ['ts o','ꊝ'],
    u'\uA29E' : ['ts o','ꊞ'],
    u'\uA29F' : ['ts &','ꊟ'],
    u'\uA2A0' : ['ts &','ꊠ'],
    u'\uA2A1' : ['ts &','ꊡ'],
    u'\uA2A2' : ['ts u','ꊢ'],
    u'\uA2A3' : ['ts u','ꊣ'],
    u'\uA2A4' : ['ts u','ꊤ'],
    u'\uA2A5' : ['ts u','ꊥ'],
    u'\uA2A6' : ['ts u','ꊦ'],
    u'\uA2A7' : ['ts u','ꊧ'],
    u'\uA2A8' : ['ts l','ꊨ'],
    u'\uA2A9' : ['ts l','ꊩ'],
    u'\uA2AA' : ['ts l','ꊪ'],
    u'\uA2AB' : ['ts l','ꊫ'],
    u'\uA2AC' : ['ts i','ꊬ'],
    u'\uA2AD' : ['ts i','ꊭ'],
    u'\uA2AE' : ['tsh i','ꊮ'],
    u'\uA2AF' : ['tsh i','ꊯ'],
    u'\uA2B0' : ['tsh i','ꊰ'],
    u'\uA2B1' : ['tsh i','ꊱ'],
    u'\uA2B2' : ['tsh e','ꊲ'],
    u'\uA2B3' : ['tsh e','ꊳ'],
    u'\uA2B4' : ['tsh e','ꊴ'],
    u'\uA2B5' : ['tsh e','ꊵ'],
    u'\uA2B6' : ['tsh A','ꊶ'],
    u'\uA2B7' : ['tsh A','ꊷ'],
    u'\uA2B8' : ['tsh A','ꊸ'],
    u'\uA2B9' : ['tsh A','ꊹ'],
    u'\uA2BA' : ['tsh >','ꊺ'],
    u'\uA2BB' : ['tsh >','ꊻ'],
    u'\uA2BC' : ['tsh >','ꊼ'],
    u'\uA2BD' : ['tsh o','ꊽ'],
    u'\uA2BE' : ['tsh o','ꊾ'],
    u'\uA2BF' : ['tsh o','ꊿ'],
    u'\uA2C0' : ['tsh o','ꋀ'],
    u'\uA2C1' : ['tsh &','ꋁ'],
    u'\uA2C2' : ['tsh &','ꋂ'],
    u'\uA2C3' : ['tsh &','ꋃ'],
    u'\uA2C4' : ['tsh u','ꋄ'],
    u'\uA2C5' : ['tsh u','ꋅ'],
    u'\uA2C6' : ['tsh u','ꋆ'],
    u'\uA2C7' : ['tsh u','ꋇ'],
    u'\uA2C8' : ['tsh u','ꋈ'],
    u'\uA2C9' : ['tsh u','ꋉ'],
    u'\uA2CA' : ['tsh l','ꋊ'],
    u'\uA2CB' : ['tsh l','ꋋ'],
    u'\uA2CC' : ['tsh l','ꋌ'],
    u'\uA2CD' : ['tsh l','ꋍ'],
    u'\uA2CE' : ['tsh i','ꋎ'],
    u'\uA2CF' : ['tsh i','ꋏ'],
    u'\uA2D0' : ['dz i','ꋐ'],
    u'\uA2D1' : ['dz i','ꋑ'],
    u'\uA2D2' : ['dz i','ꋒ'],
    u'\uA2D3' : ['dz i','ꋓ'],
    u'\uA2D4' : ['dz e','ꋔ'],
    u'\uA2D5' : ['dz e','ꋕ'],
    u'\uA2D6' : ['dz e','ꋖ'],
    u'\uA2D7' : ['dz e','ꋗ'],
    u'\uA2D8' : ['dz A','ꋘ'],
    u'\uA2D9' : ['dz A','ꋙ'],
    u'\uA2DA' : ['dz A','ꋚ'],
    u'\uA2DB' : ['dz A','ꋛ'],
    u'\uA2DC' : ['dz o','ꋜ'],
    u'\uA2DD' : ['dz o','ꋝ'],
    u'\uA2DE' : ['dz o','ꋞ'],
    u'\uA2DF' : ['dz &','ꋟ'],
    u'\uA2E0' : ['dz &','ꋠ'],
    u'\uA2E1' : ['dz &','ꋡ'],
    u'\uA2E2' : ['dz u','ꋢ'],
    u'\uA2E3' : ['dz u','ꋣ'],
    u'\uA2E4' : ['dz u','ꋤ'],
    u'\uA2E5' : ['dz u','ꋥ'],
    u'\uA2E6' : ['dz u','ꋦ'],
    u'\uA2E7' : ['dz l','ꋧ'],
    u'\uA2E8' : ['dz l','ꋨ'],
    u'\uA2E9' : ['dz l','ꋩ'],
    u'\uA2EA' : ['dz l','ꋪ'],
    u'\uA2EB' : ['dz i','ꋫ'],
    u'\uA2EC' : ['dz i','ꋬ'],
    u'\uA2ED' : ['n dz i','ꋭ'],
    u'\uA2EE' : ['n dz i','ꋮ'],
    u'\uA2EF' : ['n dz i','ꋯ'],
    u'\uA2F0' : ['n dz i','ꋰ'],
    u'\uA2F1' : ['n dz e','ꋱ'],
    u'\uA2F2' : ['n dz e','ꋲ'],
    u'\uA2F3' : ['n dz e','ꋳ'],
    u'\uA2F4' : ['n dz A','ꋴ'],
    u'\uA2F5' : ['n dz A','ꋵ'],
    u'\uA2F6' : ['n dz A','ꋶ'],
    u'\uA2F7' : ['n dz A','ꋷ'],
    u'\uA2F8' : ['n dz >','ꋸ'],
    u'\uA2F9' : ['n dz >','ꋹ'],
    u'\uA2FA' : ['n dz o','ꋺ'],
    u'\uA2FB' : ['n dz o','ꋻ'],
    u'\uA2FC' : ['n dz &','ꋼ'],
    u'\uA2FD' : ['n dz &','ꋽ'],
    u'\uA2FE' : ['n dz u','ꋾ'],
    u'\uA2FF' : ['n dz u','ꋿ'],
    u'\uA300' : ['n dz u','ꌀ'],
    u'\uA301' : ['n dz u','ꌁ'],
    u'\uA302' : ['n dz u','ꌂ'],
    u'\uA303' : ['n dz l','ꌃ'],
    u'\uA304' : ['n dz l','ꌄ'],
    u'\uA305' : ['n dz l','ꌅ'],
    u'\uA306' : ['n dz l','ꌆ'],
    u'\uA307' : ['n dz i','ꌇ'],
    u'\uA308' : ['n dz i','ꌈ'],
    u'\uA309' : ['s i','ꌉ'],
    u'\uA30A' : ['s i','ꌊ'],
    u'\uA30B' : ['s i','ꌋ'],
    u'\uA30C' : ['s i','ꌌ'],
    u'\uA30D' : ['s e','ꌍ'],
    u'\uA30E' : ['s e','ꌎ'],
    u'\uA30F' : ['s e','ꌏ'],
    u'\uA310' : ['s A','ꌐ'],
    u'\uA311' : ['s A','ꌑ'],
    u'\uA312' : ['s A','ꌒ'],
    u'\uA313' : ['s A','ꌓ'],
    u'\uA314' : ['s >','ꌔ'],
    u'\uA315' : ['s >','ꌕ'],
    u'\uA316' : ['s >','ꌖ'],
    u'\uA317' : ['s o','ꌗ'],
    u'\uA318' : ['s o','ꌘ'],
    u'\uA319' : ['s o','ꌙ'],
    u'\uA31A' : ['s o','ꌚ'],
    u'\uA31B' : ['s &','ꌛ'],
    u'\uA31C' : ['s &','ꌜ'],
    u'\uA31D' : ['s &','ꌝ'],
    u'\uA31E' : ['s u','ꌞ'],
    u'\uA31F' : ['s u','ꌟ'],
    u'\uA320' : ['s u','ꌠ'],
    u'\uA321' : ['s u','ꌡ'],
    u'\uA322' : ['s u','ꌢ'],
    u'\uA323' : ['s u','ꌣ'],
    u'\uA324' : ['s l','ꌤ'],
    u'\uA325' : ['s l','ꌥ'],
    u'\uA326' : ['s l','ꌦ'],
    u'\uA327' : ['s l','ꌧ'],
    u'\uA328' : ['s i','ꌨ'],
    u'\uA329' : ['s i','ꌩ'],
    u'\uA32A ' : ['z i','ꌪ '],
    u'\uA32B' : ['z i','ꌫ'],
    u'\uA32C' : ['z i','ꌬ'],
    u'\uA32D' : ['z i','ꌭ'],
    u'\uA32E' : ['z e','ꌮ'],
    u'\uA32F' : ['z e','ꌯ'],
    u'\uA330' : ['z e','ꌰ'],
    u'\uA331' : ['z A','ꌱ'],
    u'\uA332' : ['z A','ꌲ'],
    u'\uA333' : ['z A','ꌳ'],
    u'\uA334' : ['z A','ꌴ'],
    u'\uA335' : ['z o','ꌵ'],
    u'\uA336' : ['z o','ꌶ'],
    u'\uA337' : ['z o','ꌷ'],
    u'\uA338' : ['z o','ꌸ'],
    u'\uA339' : ['z &','ꌹ'],
    u'\uA33A' : ['z &','ꌺ'],
    u'\uA33B' : ['z &','ꌻ'],
    u'\uA33C' : ['z u','ꌼ'],
    u'\uA33D' : ['z u','ꌽ'],
    u'\uA33E' : ['z u','ꌾ'],
    u'\uA33F' : ['z u','ꌿ'],
    u'\uA340' : ['z l','ꍀ'],
    u'\uA341' : ['z l','ꍁ'],
    u'\uA342' : ['z l','ꍂ'],
    u'\uA343' : ['z l','ꍃ'],
    u'\uA344' : ['z i','ꍄ'],
    u'\uA345' : ['z i','ꍅ'],
    u'\uA346' : ['ts_r A','ꍆ'],
    u'\uA347' : ['ts_r A','ꍇ'],
    u'\uA348' : ['ts_r A','ꍈ'],
    u'\uA349' : ['ts_r A','ꍉ'],
    u'\uA34A' : ['ts_r >','ꍊ'],
    u'\uA34B' : ['ts_r >','ꍋ'],
    u'\uA34C' : ['ts_r >','ꍌ'],
    u'\uA34D' : ['ts_r o','ꍍ'],
    u'\uA34E' : ['ts_r o','ꍎ'],
    u'\uA34F' : ['ts_r o','ꍏ'],
    u'\uA350' : ['ts_r o','ꍐ'],
    u'\uA351' : ['ts_r &','ꍑ'],
    u'\uA352' : ['ts_r &','ꍒ'],
    u'\uA353' : ['ts_r &','ꍓ'],
    u'\uA354' : ['ts_r &','ꍔ'],
    u'\uA355' : ['ts_r u','ꍕ'],
    u'\uA356' : ['ts_r u','ꍖ'],
    u'\uA357' : ['ts_r u','ꍗ'],
    u'\uA358' : ['ts_r u','ꍘ'],
    u'\uA359' : ['ts_r u','ꍙ'],
    u'\uA35A' : ['ts_r u','ꍚ'],
    u'\uA35B' : ['ts_r l','ꍛ'],
    u'\uA35C' : ['ts_r l','ꍜ'],
    u'\uA35D' : ['ts_r l','ꍝ'],
    u'\uA35E' : ['ts_r l','ꍞ'],
    u'\uA35F' : ['ts_r i','ꍟ'],
    u'\uA360' : ['ts_r i','ꍠ'],
    u'\uA361' : ['ts_rh A','ꍡ'],
    u'\uA362' : ['ts_rh A','ꍢ'],
    u'\uA363' : ['ts_rh A','ꍣ'],
    u'\uA364' : ['ts_rh A','ꍤ'],
    u'\uA365' : ['ts_rh >','ꍥ'],
    u'\uA366' : ['ts_rh >','ꍦ'],
    u'\uA367' : ['ts_rh >','ꍧ'],
    u'\uA368' : ['ts_rh >','ꍨ'],
    u'\uA369' : ['ts_rh o','ꍩ'],
    u'\uA36A' : ['ts_rh o','ꍪ'],
    u'\uA36B' : ['ts_rh o','ꍫ'],
    u'\uA36C' : ['ts_rh o','ꍬ'],
    u'\uA36D' : ['ts_rh &','ꍭ'],
    u'\uA36E' : ['ts_rh &','ꍮ'],
    u'\uA36F' : ['ts_rh &','ꍯ'],
    u'\uA370' : ['ts_rh &','ꍰ'],
    u'\uA371' : ['ts_rh u','ꍱ'],
    u'\uA372' : ['ts_rh u','ꍲ'],
    u'\uA373' : ['ts_rh u','ꍳ'],
    u'\uA374' : ['ts_rh u','ꍴ'],
    u'\uA375' : ['ts_rh u','ꍵ'],
    u'\uA376' : ['ts_rh l','ꍶ'],
    u'\uA377' : ['ts_rh l','ꍷ'],
    u'\uA378' : ['ts_rh l','ꍸ'],
    u'\uA379' : ['ts_rh l','ꍹ'],
    u'\uA37A' : ['ts_rh i','ꍺ'],
    u'\uA37B' : ['ts_rh i','ꍻ'],
    u'\uA37C' : ['dzr A','ꍼ'],
    u'\uA37D' : ['dzr A','ꍽ'],
    u'\uA37E' : ['dzr >','ꍾ'],
    u'\uA37F' : ['dzr >','ꍿ'],
    u'\uA380' : ['dzr o','ꎀ'],
    u'\uA381' : ['dzr o','ꎁ'],
    u'\uA382' : ['dzr o','ꎂ'],
    u'\uA383' : ['dzr o','ꎃ'],
    u'\uA384' : ['dzr &','ꎄ'],
    u'\uA385' : ['dzr &','ꎅ'],
    u'\uA386' : ['dzr &','ꎆ'],
    u'\uA387' : ['dzr &','ꎇ'],
    u'\uA388' : ['dzr u','ꎈ'],
    u'\uA389' : ['dzr u','ꎉ'],
    u'\uA38A' : ['dzr u','ꎊ'],
    u'\uA38B' : ['dzr u','ꎋ'],
    u'\uA38C' : ['dzr u','ꎌ'],
    u'\uA38D' : ['dzr u','ꎍ'],
    u'\uA38E' : ['dzr l','ꎎ'],
    u'\uA38F' : ['dzr l','ꎏ'],
    u'\uA390' : ['dzr l','ꎐ'],
    u'\uA391' : ['dzr l','ꎑ'],
    u'\uA392' : ['dzr i','ꎒ'],
    u'\uA393' : ['dzr i','ꎓ'],
    u'\uA394' : ['n dzr A','ꎔ'],
    u'\uA395' : ['n dzr A','ꎕ'],
    u'\uA396' : ['n dzr A','ꎖ'],
    u'\uA397' : ['n dzr A','ꎗ'],
    u'\uA398' : ['n dzr o','ꎘ'],
    u'\uA399' : ['n dzr o','ꎙ'],
    u'\uA39A' : ['n dzr o','ꎚ'],
    u'\uA39B' : ['n dzr &','ꎛ'],
    u'\uA39C' : ['n dzr &','ꎜ'],
    u'\uA39D' : ['n dzr &','ꎝ'],
    u'\uA39E' : ['n dzr &','ꎞ'],
    u'\uA39F' : ['n dzr u','ꎟ'],
    u'\uA3A0' : ['n dzr u','ꎠ'],
    u'\uA3A1' : ['n dzr u','ꎡ'],
    u'\uA3A2' : ['n dzr u','ꎢ'],
    u'\uA3A3' : ['n dzr u','ꎣ'],
    u'\uA3A4' : ['n dzr u','ꎤ'],
    u'\uA3A5' : ['n dzr l','ꎥ'],
    u'\uA3A6' : ['n dzr l','ꎦ'],
    u'\uA3A7' : ['n dzr l','ꎧ'],
    u'\uA3A8' : ['n dzr l','ꎨ'],
    u'\uA3A9' : ['n dzr i','ꎩ'],
    u'\uA3AA' : ['n dzr i','ꎪ'],
    u'\uA3AB' : ['sr A','ꎫ'],
    u'\uA3AC' : ['sr A','ꎬ'],
    u'\uA3AD' : ['sr A','ꎭ'],
    u'\uA3AE' : ['sr A','ꎮ'],
    u'\uA3AF' : ['sr >','ꎯ'],
    u'\uA3B0' : ['sr >','ꎰ'],
    u'\uA3B1' : ['sr >','ꎱ'],
    u'\uA3B2' : ['sr o','ꎲ'],
    u'\uA3B3' : ['sr o','ꎳ'],
    u'\uA3B4' : ['sr o','ꎴ'],
    u'\uA3B5' : ['sr o','ꎵ'],
    u'\uA3B6' : ['sr &','ꎶ'],
    u'\uA3B7' : ['sr &','ꎷ'],
    u'\uA3B8' : ['sr &','ꎸ'],
    u'\uA3B9' : ['sr &','ꎹ'],
    u'\uA3BA' : ['sr u','ꎺ'],
    u'\uA3BB' : ['sr u','ꎻ'],
    u'\uA3BC' : ['sr u','ꎼ'],
    u'\uA3BD' : ['sr u','ꎽ'],
    u'\uA3BE' : ['sr u','ꎾ'],
    u'\uA3BF' : ['sr u','ꎿ'],
    u'\uA3C0' : ['sr l','ꏀ'],
    u'\uA3C1' : ['sr l','ꏁ'],
    u'\uA3C2' : ['sr l','ꏂ'],
    u'\uA3C3' : ['sr l','ꏃ'],
    u'\uA3C4' : ['sr i','ꏄ'],
    u'\uA3C5' : ['sr i','ꏅ'],
    u'\uA3C6' : ['zr A','ꏆ'],
    u'\uA3C7' : ['zr A','ꏇ'],
    u'\uA3C8' : ['zr A','ꏈ'],
    u'\uA3C9' : ['zr A','ꏉ'],
    u'\uA3CA' : ['zr >','ꏊ'],
    u'\uA3CB' : ['zr >','ꏋ'],
    u'\uA3CC' : ['zr >','ꏌ'],
    u'\uA3CD' : ['zr o','ꏍ'],
    u'\uA3CE' : ['zr o','ꏎ'],
    u'\uA3CF' : ['zr o','ꏏ'],
    u'\uA3D0' : ['zr o','ꏐ'],
    u'\uA3D1' : ['zr &','ꏑ'],
    u'\uA3D2' : ['zr &','ꏒ'],
    u'\uA3D3' : ['zr &','ꏓ'],
    u'\uA3D4' : ['zr u','ꏔ'],
    u'\uA3D5' : ['zr u','ꏕ'],
    u'\uA3D6' : ['zr u','ꏖ'],
    u'\uA3D7' : ['zr u','ꏗ'],
    u'\uA3D8' : ['zr u','ꏘ'],
    u'\uA3D9' : ['zr u','ꏙ'],
    u'\uA3DA' : ['zr l','ꏚ'],
    u'\uA3DB' : ['zr l','ꏛ'],
    u'\uA3DC' : ['zr l','ꏜ'],
    u'\uA3DD' : ['zr l','ꏝ'],
    u'\uA3DE' : ['zr i','ꏞ'],
    u'\uA3DF' : ['zr i','ꏟ'],
    u'\uA3E0' : ['t c} i','ꏠ'],
    u'\uA3E1' : ['t c} i','ꏡ'],
    u'\uA3E2' : ['t c} i','ꏢ'],
    u'\uA3E3' : ['t c} i','ꏣ'],
    u'\uA3E4' : ['t c} e','ꏤ'],
    u'\uA3E5' : ['t c} e','ꏥ'],
    u'\uA3E6' : ['t c} e','ꏦ'],
    u'\uA3E7' : ['t c} e','ꏧ'],
    u'\uA3E8' : ['t c} >','ꏨ'],
    u'\uA3E9' : ['t c} >','ꏩ'],
    u'\uA3EA' : ['t c} >','ꏪ'],
    u'\uA3EB' : ['t c} >','ꏫ'],
    u'\uA3EC' : ['t c} o','ꏬ'],
    u'\uA3ED' : ['t c} o','ꏭ'],
    u'\uA3EE' : ['t c} o','ꏮ'],
    u'\uA3EF' : ['t c} o','ꏯ'],
    u'\uA3F0' : ['t c} u','ꏰ'],
    u'\uA3F1' : ['t c} u','ꏱ'],
    u'\uA3F2' : ['t c} u','ꏲ'],
    u'\uA3F3' : ['t c} u','ꏳ'],
    u'\uA3F4' : ['t c} u','ꏴ'],
    u'\uA3F5' : ['t c} u','ꏵ'],
    u'\uA3F6' : ['t c} l','ꏶ'],
    u'\uA3F7' : ['t c} l','ꏷ'],
    u'\uA3F8' : ['t c} l','ꏸ'],
    u'\uA3F9' : ['t c} l','ꏹ'],
    u'\uA3FA' : ['t c} i','ꏺ'],
    u'\uA3FB' : ['t c} i','ꏻ'],
    u'\uA3FC' : ['t c}h i','ꏼ'],
    u'\uA3FD' : ['t c}h i','ꏽ'],
    u'\uA3FE' : ['t c}h i','ꏾ'],
    u'\uA3FF' : ['t c}h i','ꏿ'],
    u'\uA400' : ['t c}h e','ꐀ'],
    u'\uA401' : ['t c}h e','ꐁ'],
    u'\uA402' : ['t c}h e','ꐂ'],
    u'\uA403' : ['t c}h e','ꐃ'],
    u'\uA404' : ['t c}h >','ꐄ'],
    u'\uA405' : ['t c}h >','ꐅ'],
    u'\uA406' : ['t c}h >','ꐆ'],
    u'\uA407' : ['t c}h >','ꐇ'],
    u'\uA408' : ['t c}h o','ꐈ'],
    u'\uA409' : ['t c}h o','ꐉ'],
    u'\uA40A' : ['t c}h o','ꐊ'],
    u'\uA40B' : ['t c}h o','ꐋ'],
    u'\uA40C' : ['t c}h u','ꐌ'],
    u'\uA40D' : ['t c}h u','ꐍ'],
    u'\uA40E' : ['t c}h u','ꐎ'],
    u'\uA40F' : ['t c}h u','ꐏ'],
    u'\uA410' : ['t c}h u','ꐐ'],
    u'\uA411' : ['t c}h u','ꐑ'],
    u'\uA412' : ['t c}h l','ꐒ'],
    u'\uA413' : ['t c}h l','ꐓ'],
    u'\uA414' : ['t c}h l','ꐔ'],
    u'\uA415' : ['t c}h l','ꐕ'],
    u'\uA416' : ['t c}h i','ꐖ'],
    u'\uA417' : ['t c}h i','ꐗ'],
    u'\uA418' : ['d z} i','ꐘ'],
    u'\uA419' : ['d z} i','ꐙ'],
    u'\uA41A' : ['d z} i','ꐚ'],
    u'\uA41B' : ['d z} i','ꐛ'],
    u'\uA41C' : ['d z} e','ꐜ'],
    u'\uA41D' : ['d z} e','ꐝ'],
    u'\uA41E' : ['d z} e','ꐞ'],
    u'\uA41F' : ['d z} e','ꐟ'],
    u'\uA420' : ['d z} >','ꐠ'],
    u'\uA421' : ['d z} >','ꐡ'],
    u'\uA422' : ['d z} >','ꐢ'],
    u'\uA423' : ['d z} o','ꐣ'],
    u'\uA424' : ['d z} o','ꐤ'],
    u'\uA425' : ['d z} o','ꐥ'],
    u'\uA426' : ['d z} o','ꐦ'],
    u'\uA427' : ['d z} u','ꐧ'],
    u'\uA428' : ['d z} u','ꐨ'],
    u'\uA429' : ['d z} u','ꐩ'],
    u'\uA42A' : ['d z} u','ꐪ'],
    u'\uA42B' : ['d z} u','ꐫ'],
    u'\uA42C' : ['d z} u','ꐬ'],
    u'\uA42D' : ['d z} l','ꐭ'],
    u'\uA42E' : ['d z} l','ꐮ'],
    u'\uA42F' : ['d z} l','ꐯ'],
    u'\uA430' : ['d z} l','ꐰ'],
    u'\uA431' : ['n d z} i','ꐱ'],
    u'\uA432' : ['n d z} i','ꐲ'],
    u'\uA433' : ['n d z} i','ꐳ'],
    u'\uA434' : ['n d z} i','ꐴ'],
    u'\uA435' : ['n d z} e','ꐵ'],
    u'\uA436' : ['n d z} e','ꐶ'],
    u'\uA437' : ['n d z} e','ꐷ'],
    u'\uA438' : ['n d z} e','ꐸ'],
    u'\uA439' : ['n d z} >','ꐹ'],
    u'\uA43A' : ['n d z} >','ꐺ'],
    u'\uA43B' : ['n d z} o','ꐻ'],
    u'\uA43C' : ['n d z} o','ꐼ'],
    u'\uA43D' : ['n d z} o','ꐽ'],
    u'\uA43E' : ['n d z} o','ꐾ'],
    u'\uA43F' : ['n d z} u','ꐿ'],
    u'\uA440' : ['n d z} u','ꑀ'],
    u'\uA441' : ['n d z} u','ꑁ'],
    u'\uA442' : ['n d z} u','ꑂ'],
    u'\uA443' : ['n d z} u','ꑃ'],
    u'\uA444' : ['n d z} l','ꑄ'],
    u'\uA445' : ['n d z} l','ꑅ'],
    u'\uA446' : ['n d z} l','ꑆ'],
    u'\uA447' : ['n d z} l','ꑇ'],
    u'\uA448' : ['n d z} i','ꑈ'],
    u'\uA449' : ['n d z} i','ꑉ'],
    u'\uA44A' : ['nr i','ꑊ'],
    u'\uA44B' : ['nr i','ꑋ'],
    u'\uA44C' : ['nr i','ꑌ'],
    u'\uA44D' : ['nr i','ꑍ'],
    u'\uA44E' : ['nr e','ꑎ'],
    u'\uA44F' : ['nr e','ꑏ'],
    u'\uA450' : ['nr e','ꑐ'],
    u'\uA451' : ['nr e','ꑑ'],
    u'\uA452' : ['nr >','ꑒ'],
    u'\uA453' : ['nr >','ꑓ'],
    u'\uA454' : ['nr >','ꑔ'],
    u'\uA455' : ['nr o','ꑕ'],
    u'\uA456' : ['nr o','ꑖ'],
    u'\uA457' : ['nr o','ꑗ'],
    u'\uA458' : ['nr o','ꑘ'],
    u'\uA459' : ['nr u','ꑙ'],
    u'\uA45A' : ['nr u','ꑚ'],
    u'\uA45B' : ['nr u','ꑛ'],
    u'\uA45C' : ['nr u','ꑜ'],
    u'\uA45D' : ['c} i','ꑝ'],
    u'\uA45E' : ['c} i','ꑞ'],
    u'\uA45F' : ['c} i','ꑟ'],
    u'\uA460' : ['c} i','ꑠ'],
    u'\uA461' : ['c} e','ꑡ'],
    u'\uA462' : ['c} e','ꑢ'],
    u'\uA463' : ['c} e','ꑣ'],
    u'\uA464' : ['c} e','ꑤ'],
    u'\uA465' : ['c} >','ꑥ'],
    u'\uA466' : ['c} >','ꑦ'],
    u'\uA467' : ['c} o','ꑧ'],
    u'\uA468' : ['c} o','ꑨ'],
    u'\uA469' : ['c} o','ꑩ'],
    u'\uA46A' : ['c} o','ꑪ'],
    u'\uA46B' : ['c} l','ꑫ'],
    u'\uA46C' : ['c} l','ꑬ'],
    u'\uA46D' : ['c} l','ꑭ'],
    u'\uA46E' : ['c} l','ꑮ'],
    u'\uA46F' : ['c} i','ꑯ'],
    u'\uA470' : ['c} i','ꑰ'],
    u'\uA471' : ['z} i','ꑱ'],
    u'\uA472' : ['z} i','ꑲ'],
    u'\uA473' : ['z} i','ꑳ'],
    u'\uA474' : ['z} i','ꑴ'],
    u'\uA475' : ['z} e','ꑵ'],
    u'\uA476' : ['z} e','ꑶ'],
    u'\uA477' : ['z} e','ꑷ'],
    u'\uA478' : ['z} e','ꑸ'],
    u'\uA479' : ['z} >','ꑹ'],
    u'\uA47A' : ['z} >','ꑺ'],
    u'\uA47B' : ['z} >','ꑻ'],
    u'\uA47C' : ['z} >','ꑼ'],
    u'\uA47D' : ['z} o','ꑽ'],
    u'\uA47E' : ['z} o','ꑾ'],
    u'\uA47F' : ['z} o','ꑿ'],
    u'\uA480' : ['z} o','ꒀ'],
    u'\uA481' : ['z} u','ꒁ'],
    u'\uA482' : ['z} u','ꒂ'],
    u'\uA483' : ['z} u','ꒃ'],
    u'\uA484' : ['z} u','ꒄ'],
    u'\uA485' : ['z} u','ꒅ'],
    u'\uA486' : ['z} u','ꒆ'],
    u'\uA487' : ['z} l','ꒇ'],
    u'\uA488' : ['z} l','ꒈ'],
    u'\uA489' : ['z} l','ꒉ'],
    u'\uA48A' : ['z} l','ꒊ'],
    u'\uA48B' : ['z} i','ꒋ'],
    u'\uA48C' : ['z} i','ꒌ'],
    u'\uA490' : ['t c}h o','꒐'],
    u'\uA491' : ['l i','꒑'],
    u'\uA492' : ['kh i','꒒'],
    u'\uA493' : ['nr i','꒓'],
    u'\uA494' : ['tsh l','꒔'],
    u'\uA495' : ['z i','꒕'],
    u'\uA496' : ['g o','꒖'],
    u'\uA497' : ['k e','꒗'],
    u'\uA498' : ['m i','꒘'],
    u'\uA499' : ['h i','꒙'],
    u'\uA49A' : ['l l','꒚'],
    u'\uA49B' : ['b u','꒛'],
    u'\uA49C' : ['m o','꒜'],
    u'\uA49D' : ['z} o','꒝'],
    u'\uA49E' : ['ph u','꒞'],
    u'\uA49F' : ['h >','꒟'],
    u'\uA4A0' : ['th A','꒠'],
    u'\uA4A1' : ['k A','꒡'],
    u'\uA4A2' : ['ts u','꒢'],
    u'\uA4A3' : ['tsh l','꒣'],
    u'\uA4A4' : ['d u','꒤'],
    u'\uA4A5' : ['p u','꒥'],
    u'\uA4A6' : ['g u','꒦'],
    u'\uA4A7' : ['nr o','꒧'],
    u'\uA4A8' : ['th u','꒨'],
    u'\uA4A9' : ['o','꒩'],
    u'\uA4AA' : ['d z} u','꒪'],
    u'\uA4AB' : ['ts o','꒫'],
    u'\uA4AC' : ['p l','꒬'],
    u'\uA4AD' : ['m0 o','꒭'],
    u'\uA4AE' : ['z} i','꒮'],
    u'\uA4AF' : ['v u','꒯'],
    u'\uA4B0' : ['sr l','꒰'],
    u'\uA4B1' : ['v e','꒱'],
    u'\uA4B2' : ['ts A','꒲'],
    u'\uA4B3' : ['t c} o','꒳'],
    u'\uA4B4' : ['n dz u','꒴'],
    u'\uA4B5' : ['d z} l','꒵'],
    u'\uA4B6' : ['k o','꒶'],
    u'\uA4B7' : ['d z} e','꒷'],
    u'\uA4B8' : ['G o','꒸'],
    u'\uA4B9' : ['t u','꒹'],
    u'\uA4BA' : ['sr u','꒺'],
    u'\uA4BB' : ['l e','꒻'],
    u'\uA4BC' : ['tsh l','꒼'],
    u'\uA4BD' : ['tsh >','꒽'],
    u'\uA4BE' : ['tsh i','꒾'],
    u'\uA4BF' : ['h o','꒿'],
    u'\uA4C0' : ['sr A','꓀'],
    u'\uA4C1' : ['ts u','꓁'],
    u'\uA4C2' : ['sr o','꓂'],
    u'\uA4C3' : ['ts_rh e','꓃'],
    u'\uA4C4' : ['dz e','꓄'],
    u'\uA4C5' : ['m b e','꓅'],
    u'\uA4C6' : ['kh e','꓆'],
    }

########NEW FILE########
__FILENAME__ = thaifix
# coding=utf-8

"""thaifix

For backwards compatibility reasons, Unicode decided on the lovely
property of having Thai left-catenating vowels appear in their
*visual* rather than their *logical* order. This necessitates some
jiggery-pokery to reorder these after consonant sequences.

Basically Thai can have consonant clusters that include a stop
followed by a liquid (l, r), including cases where the liquid (r) is
not pronounced.

Also possible are "h" and "?" (empty consonant) followed by a
sonorant.

A prefixed vowel (PV) must skip over a single consonant, or one of
these sequences.

See http://www.thai-language.com/ref/double_consonants

"""
__author__ = """
rws@uiuc.edu (Richard Sproat)
"""

ThaiTable = {
  u'\u0E01' : ["C", "ก", "THAI CHARACTER KO KAI"],
  u'\u0E02' : ["C", "ข", "THAI CHARACTER KHO KHAI"],
  u'\u0E03' : ["C", "ฃ", "THAI CHARACTER KHO KHUAT"],
  u'\u0E04' : ["C", "ค", "THAI CHARACTER KHO KHWAI"],
  u'\u0E05' : ["C", "ฅ", "THAI CHARACTER KHO KHON"],
  u'\u0E06' : ["C", "ฆ", "THAI CHARACTER KHO RAKHANG"],
  u'\u0E07' : ["C", "ง", "THAI CHARACTER NGO NGU"],
  u'\u0E08' : ["C", "จ", "THAI CHARACTER CHO CHAN"],
  u'\u0E09' : ["C", "ฉ", "THAI CHARACTER CHO CHING"],
  u'\u0E0A' : ["C", "ช", "THAI CHARACTER CHO CHANG"],
  u'\u0E0B' : ["C", "ซ", "THAI CHARACTER SO SO"],
  u'\u0E0C' : ["C", "ฌ", "THAI CHARACTER CHO CHOE"],
  u'\u0E0D' : ["S", "ญ", "THAI CHARACTER YO YING"],
  u'\u0E0E' : ["C", "ฎ", "THAI CHARACTER DO CHADA"],
  u'\u0E0F' : ["C", "ฏ", "THAI CHARACTER TO PATAK"],
  u'\u0E10' : ["C", "ฐ", "THAI CHARACTER THO THAN"],
  u'\u0E11' : ["C", "ฑ", "THAI CHARACTER THO NANGMONTHO"],
  u'\u0E12' : ["C", "ฒ", "THAI CHARACTER THO PHUTHAO"],
  u'\u0E13' : ["S", "ณ", "THAI CHARACTER NO NEN"],
  u'\u0E14' : ["C", "ด", "THAI CHARACTER DO DEK"],
  u'\u0E15' : ["C", "ต", "THAI CHARACTER TO TAO"],
  u'\u0E16' : ["C", "ถ", "THAI CHARACTER THO THUNG"],
  u'\u0E17' : ["C", "ท", "THAI CHARACTER THO THAHAN"],
  u'\u0E18' : ["C", "ธ", "THAI CHARACTER THO THONG"],
  u'\u0E19' : ["S", "น", "THAI CHARACTER NO NU"],
  u'\u0E1A' : ["C", "บ", "THAI CHARACTER BO BAIMAI"],
  u'\u0E1B' : ["C", "ป", "THAI CHARACTER PO PLA"],
  u'\u0E1C' : ["C", "ผ", "THAI CHARACTER PHO PHUNG"],
  u'\u0E1D' : ["C", "ฝ", "THAI CHARACTER FO FA"],
  u'\u0E1E' : ["C", "พ", "THAI CHARACTER PHO PHAN"],
  u'\u0E1F' : ["C", "ฟ", "THAI CHARACTER FO FAN"],
  u'\u0E20' : ["C", "ภ", "THAI CHARACTER PHO SAMPHAO"],
  u'\u0E21' : ["S", "ม", "THAI CHARACTER MO MA"],
  u'\u0E22' : ["C", "ย", "THAI CHARACTER YO YAK"],
  u'\u0E23' : ["L", "ร", "THAI CHARACTER RO RUA"],
  u'\u0E24' : ["C", "ฤ", "THAI CHARACTER RU"],
  u'\u0E25' : ["L", "ล", "THAI CHARACTER LO LING"],
  u'\u0E26' : ["C", "ฦ", "THAI CHARACTER LU"],
  u'\u0E27' : ["S", "ว", "THAI CHARACTER WO WAEN"],
  u'\u0E28' : ["C", "ศ", "THAI CHARACTER SO SALA"],
  u'\u0E29' : ["C", "ษ", "THAI CHARACTER SO RUSI"],
  u'\u0E2A' : ["C", "ส", "THAI CHARACTER SO SUA"],
  u'\u0E2B' : ["H", "ห", "THAI CHARACTER HO HIP"],
  u'\u0E2C' : ["C", "ฬ", "THAI CHARACTER LO CHULA"],
  u'\u0E2D' : ["H", "อ", "THAI CHARACTER O ANG"],
  u'\u0E2E' : ["C", "ฮ", "THAI CHARACTER HO NOKHUK"],
  u'\u0E2F' : ["V", "กฯ", "THAI CHARACTER PAIYANNOI (combined with ko kai (ก))"],
  u'\u0E30' : ["V", "กะ", "THAI CHARACTER SARA A (combined with ko kai (ก))"],
  u'\u0E31' : ["V", "กั", "THAI CHARACTER MAI HAN-AKAT (combined with ko kai (ก))"],
  u'\u0E32' : ["V", "กา", "THAI CHARACTER SARA AA (combined with ko kai (ก))"],
  u'\u0E33' : ["V", "กำ", "THAI CHARACTER SARA AM (combined with ko kai (ก))"],
  u'\u0E34' : ["V", "กิ", "THAI CHARACTER SARA I (combined with ko kai (ก))"],
  u'\u0E35' : ["V", "กี", "THAI CHARACTER SARA II (combined with ko kai (ก))"],
  u'\u0E36' : ["V", "กึ", "THAI CHARACTER SARA UE (combined with ko kai (ก))"],
  u'\u0E37' : ["V", "กื", "THAI CHARACTER SARA UEE (combined with ko kai (ก))"],
  u'\u0E38' : ["V", "กุ", "THAI CHARACTER SARA U (combined with ko kai (ก))"],
  u'\u0E39' : ["V", "กู", "THAI CHARACTER SARA UU (combined with ko kai (ก))"],
  u'\u0E3A' : ["V", "กฺ", "THAI CHARACTER PHINTHU (combined with ko kai (ก))"],
  u'\u0E40' : ["PV", "กเ", "THAI CHARACTER SARA E (combined with ko kai (ก))"],
  u'\u0E41' : ["PV", "กแ", "THAI CHARACTER SARA AE (combined with ko kai (ก))"],
  u'\u0E42' : ["PV", "กโ", "THAI CHARACTER SARA O (combined with ko kai (ก))"],
  u'\u0E43' : ["PV", "กใ", "THAI CHARACTER SARA AI MAIMUAN (combined with ko kai (ก))"],
  u'\u0E44' : ["PV", "กไ", "THAI CHARACTER SARA AI MAIMALAI (combined with ko kai (ก))"],
  u'\u0E45' : ["V", "กๅ", "THAI CHARACTER LAKKHANGYAO (combined with ko kai (ก))"],
  u'\u0E46' : ["V", "กๆ", "THAI CHARACTER MAIYAMOK (combined with ko kai (ก))"],
  u'\u0E47' : ["V", "ก็", "THAI CHARACTER MAITAIKHU (combined with ko kai (ก))"],
  u'\u0E48' : ["V", "ก่", "THAI CHARACTER MAI EK (combined with ko kai (ก))"],
  u'\u0E49' : ["V", "ก้", "THAI CHARACTER MAI THO (combined with ko kai (ก))"],
  u'\u0E4A' : ["V", "ก๊", "THAI CHARACTER MAI TRI (combined with ko kai (ก))"],
  u'\u0E4B' : ["V", "ก๋", "THAI CHARACTER MAI CHATTAWA (combined with ko kai (ก))"],
  u'\u0E4C' : ["V", "ก์", "THAI CHARACTER THANTHAKHAT (combined with ko kai (ก))"],
  u'\u0E4D' : ["V", "กํ", "THAI CHARACTER NIKHAHIT (combined with ko kai (ก))"],
}


PATTERNS_ = [('PV', 'C', 'L'),
             ('PV', 'H', 'S'),
             ('PV', 'C'),
             ('PV', 'L'),
             ('PV', 'S'),
             ('PV', 'H'),]


def ThaiFix(string):
  try: ustring = unicode(string, 'utf8')
  except UnicodeDecodeError: ustring = string
  i = 0
  length = len(ustring)
  nustring = []
  while i < length:
    c1 = c2 = c3 = ''
    try: 
      u1 = ustring[i]
      c1 = ThaiTable[u1][0]
    except (KeyError, IndexError): pass
    try:
      u2 = ustring[i+1]
      c2 = ThaiTable[u2][0]
    except (KeyError, IndexError): pass
    try:
      u3 = ustring[i+2]
      c3 = ThaiTable[u3][0]
    except (KeyError, IndexError): pass
    if (c1, c2, c3) in PATTERNS_:
      nustring += [u2, u3, u1]
      i += 3
    elif (c1, c2) in PATTERNS_:
      nustring += [u2, u1]
      i += 2
    else:
      nustring += [u1]
      i += 1
  return ''.join(nustring).encode('utf8')

########NEW FILE########
__FILENAME__ = unitran
"""UTF-8 to WorldBet/XSampa decoder

The main complexity here involves Indic systems, which have inherent
vowels and (many of which have) two-part vowels.

Indic systems are handled by globals IndicCon, IndicVowel,
IndicComp. If a sequence is a combination of a consonant and a vowel,
the inherent vowel is removed from the preceding consonant. If a
sequence is two compound-vowel portions, these are combined.

Virama (cancellation sign) is handled in a similar way: it cancels out
the inherent vowel, but in this case it deletes itself
"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
kkim36@uiuc.edu (Kyoung-young Kim)
"""

import sys
import getopt
import Tables
import X_Tables
import thaifix

JOINER_ = ''
TransTable = {}
VIRAMA_ = '(P)'


def UpdateTransTable(table):
  """Update local module's TransTable with data from table.
  """
  for k in table: TransTable[k] = table[k]


def IndicCV(file):
  """Load Indic-specific CV data
  """
  Indic = []
  p = open(file)
  for line in p.readlines():
    line = line.split('\n')
    unis = "u'\u%s'"%line[0]
    uni = eval(unis)
    Indic.append(uni)
  return Indic


def IndicTwoPartVowels(file):
  """Load Indic-specific two-part vowel data
  """
  vowels = []
  p = open(file)
  for line in p.readlines():
    line = line.split('\t')
    prev = "u'\u%s'"%line[0]
    cur = "u'\u%s'"%line[1]
    comp = "u'\u%s'"%line[2].strip('\n')
    pre = eval(prev)
    curr = eval(cur)
    comps = eval(comp)
    vowel = [pre, curr, comps]
    vowels.append(vowel)
  return vowels
    

def RemoveSpacesFromDescriptor(token):
  """Remove JOINER_ from the likes of "(POETIC VERSE SIGN)"
  """
  if not token: return token
  if token[0] == '(' and token[-1] == ')':
    token = token.replace(JOINER_, '_')
  return token


def ProcToken (token):
  """Process individual text token, however defined, converting
  from utf-8 to phonetic encoding.
  """
  new = ''
  prev = None
  token = thaifix.ThaiFix(token)
  try: utoken = unicode(token.strip() + ' ', 'utf8')
  except UnicodeDecodeError: return token
  for c in utoken:
    if prev:
      bigram = prev + c
      if c in IndicVowel and prev in IndicCon:
        try:
          T = TransTable[prev][0].split(',')
          ntoken = JOINER_.join(T[0].strip('A').strip('>').split())
          ntoken = RemoveSpacesFromDescriptor(ntoken)
          new = new + JOINER_ + ntoken
        except KeyError:
          new = new + JOINER_ + prev.encode('utf-8')
      else:
        ## TODO: fix this sequential search with something more
        ## efficient:
        for i in range(len(IndicComp)):
          if bigram == IndicComp[i][0]+IndicComp[i][1]:
            prev = IndicComp[i][2]
            c = None
        try:
          T = TransTable[prev][0].split(',')
          ntoken = JOINER_.join(T[0].split())
          ntoken = RemoveSpacesFromDescriptor(ntoken)
          if ntoken != VIRAMA_:
            new = new + JOINER_ + ntoken
        except KeyError:
          new = new + JOINER_ + prev.encode('utf-8')
    prev = c
  return new


def TransFile (infile=None,outfile=None):
  """Top-level function to translate utf-8 file into phonetic transcription
  """
  if infile == None: instream = sys.stdin
  else: instream = open(infile)
  if outfile == None: outstream = sys.stdout
  else: outstream = open(outfile,'w')
  for line in instream.readlines(): outstream.write(ProcToken(line) + '\n')
  instream.close()
  outstream.close()


def Usage(argv):
  sys.stderr.write('Usage: %s [-x/--x-sampa] (files)\n' % argv[0])


def main(argv):
  global IndicCon, IndicVowel, IndicComp
  global JOINER_
  xsampa = False
  try:
    opts, args = getopt.getopt(argv[1:], "xj:", ["x-sampa", "joiner="])
  except getopt.GetoptError:
    Usage(argv)
    sys.exit(2)
  for opt, arg in opts:
    if opt in ("-x", "--x-sampa"):
      xsampa = True
    elif opt in ("-j", "--joiner"):
      JOINER_ = arg
  if xsampa:
    UpdateTransTable(X_Tables.TransTable)
  else:
    UpdateTransTable(Tables.TransTable)
  IndicCon= IndicCV('IndicCon.txt')
  IndicVowel = IndicCV('IndicVowel.txt')
  IndicComp = IndicTwoPartVowels('IndicTwoPartVowel.txt')
  if len(args) > 0:
    for file in args:
      TransFile(file, file + '.out')
  else:
    TransFile()


if __name__ == '__main__':
    main(sys.argv)

########NEW FILE########
__FILENAME__ = gentable
"""Given a list of the form

1401    Ai
1402    A: i
1403    i
1404    i:
1405    u
1406    u:
1407    o:

where the lefthand column is the Unicode codepoint and the second to
the last column is the WorldBet translation, produce entries for a table of the
form:

TransTable = {
  unicode : [worldbet, utf-8],
  unicode : [worldbet, utf-8],
  unicode : [worldbet, utf-8],
  ...
}

"""

__author__ = "rws@uiuc.edu (Richard Sproat)"

import sys, re

sys.stdout = open('Tables.py', 'w')

print '# coding=utf-8'

print "TransTable = {"

for line in sys.stdin.readlines():
  if not '#' in line:
    line = line.strip().split('\t')
    if len(line[0]) > 1:
      if len(line) == 1: worldbet = '(##)'
      else: worldbet = line[1]
      unistring = "u'\u%s'" % line[0]
      uni = eval(unistring)
      utf8 = uni.encode('utf8')
      print "    %s : ['%s','%s']," % (unistring, worldbet, utf8)
print "    }"

########NEW FILE########
__FILENAME__ = Wb2Xs
# -*- coding: utf-8 -*-

"""Map between WorldBet and XSampa. This table is still not complete.
"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
"""

WorldBetToXSampa = {
  'a' : 'a',
  'b' : 'b',
  'bc' : 'b_}',
  'bh' : 'b_h',
  'br' : 'b`',
  'bR' : 'b`_h',
  'bn' : 'b_n',
  'b>' : 'b_>',
  'b<' : 'b_<',
  'bv' : 'bv',
  'bv_c' : 'bv_}',
  'bV' : 'bB',
  'bV_c' : 'bB_}',
  'c' : 'c',
  'cc' : 'c_}',
  'ch' : 'c_h',
  'cp' : 'cp',
  'cp_c' : 'cp_}',
  'cp_h' : 'cp_h',
  'cC' : 'cC',
  'cC_c' : 'cC_}',
  'd' : 'd',
  'dc' : 'd_}',
  'dh' : 'd_h',
  'dr' : 'd`',
  'dR' : 'd`_h',
  'dn' : 'd_n',
  'd>' : 'd_>',
  'd<' : 'd_<',
  'd[' : 'd_d',
  'd[_c' : 'd_d_}',
  'd[_h' : 'd_d_h',
  'd[_<' : 'd_d_<',
  'dz' : 'dz',
  'dz_c' : 'dz_}',
  'dZ' : 'dZ',
  'dZ_c' : 'dZ_}',
  'dZ_:' : 'dZ:',
  'e' : 'e',
  'e~' : 'e_~',
  'f' : 'f',
  'g' : 'g',
  'gc' : 'g_}',
  'gh' : 'g_h',
  'gr' : 'g`',
  'gR' : 'g`_h',
  'gn' : 'g_n',
  'g>' : 'g_>',
  'g<' : 'g_<',
  'gb' : 'gb',
  'gb_c' : 'gb_}',
  'gb_h' : 'gb_h',
  'gG' : 'gG',
  'gG_c' : 'gG_}',
  'h' : 'h',
  'hv' : 'h\\\\',
  'i' : 'i',
  'i:' : 'i:',
  'i~' : 'i_~',
  'j' : 'j',
  'j-' : 'j\\\\',
  'k' : 'k',
  'kc' : 'k_}',
  'kh' : 'k_h',
  'kR' : 'k`_h',
  'kn' : 'k_n',
  'k>' : 'k_>',
  'k<' : 'k_<',
  'kp' : 'kp',
  'kp_c' : 'kp_}',
  'kp_h' : 'kp_h',
  'kx' : 'kx',
  'kx_c' : 'kx_}',
  'l' : 'l',
  'l=' : 'l_=',
  'lr' : 'l`',
  'l(' : 'l\\\\',
  'm' : 'm',
  'm=' : 'm_=',
  'n' : 'n',
  'n=' : 'n_=',
  'n[' : 'n_d',
  'nj' : 'n_j',
  'n~' : 'n_~',
  'nr' : 'n"',
  'o' : 'o',
  'o~' : 'o_~',
  'p' : 'p',
  'pr' : 'p`',
  'pR' : 'p`_h',
  'pc' : 'p_}',
  'ph' : 'p_h',
  'pn' : 'p_n',
  'p>' : 'p_>',
  'p<' : 'p_<',
  'pf' : 'pf',
  'pf_c' : 'pf_}',
  'pF' : 'pp\\\\',
  'pF_c' : 'pp\\\\_}',
  'F' : 'p\\\\',
  'q' : 'q',
  'qc' : 'q_}',
  'qh' : 'q_h',
  'q<' : 'q_<',
  'qX' : 'qX',
  'qX_c' : 'qX_}',
  'r' : 'r',
  'rr' : 'r`',
  '9' : 'r\\\\',
  '9r' : 'r\\\\`',
  's' : 's',
  'sr' : 's`',
  'c}' : 's\\\\',
  't' : 't',
  'tc' : 't_}',
  'tr' : 't`',
  'tR' : 't`_h',
  'th' : 't_h',
  'tn' : 't_n',
  't>' : 't_>',
  't<' : 't_<',
  't[' : 't_d',
  't[_c' : 't_d_}',
  't[_h' : 't_d_h',
  't[_n' : 't_d_n',
  't[_<' : 't_d_<',
  't[_>' : 't_d_>',
  'ts' : 'ts',
  'ts_c' : 'ts_}',
  'ts_r' : 'ts`',
  'ts_r_c' : 'ts`_}',
  'tS' : 'tS',
  'tS_c' : 'tS_}',
  'tS:' : 'tS:',
  'u' : 'u',
  'u~' : 'u_~',
  'v' : 'v',
  'V[' : 'v\\\\',
  'w' : 'w',
  'x' : 'x',
  'y' : 'y',
  'z' : 'z',
  'z}' : 'z\\\\',
  'zr' : 'z`',
  'A' : 'A',
  'A~' : 'A_~',
  'V' : 'B',
  'B' : 'B\\\\',
  'C' : 'C',
  'D' : 'D',
  'E' : 'E',
  'E~' : 'E_~',
  'M' : 'F',
  'G' : 'G',
  'Q' : 'G\\\\',
  'Q<' : 'G\\\\_<',
  'Qh' : 'G\\\\_h',
  'jw' : 'H',
  'I' : 'I',
  'I~' : 'I_~',
  'I:' : 'I:',
  'Ix' : 'I\\\\',
  'n~' : 'J',
  'J' : 'J\\\\',
  'Jc' : 'J\\\\_}',
  'Jh' : 'J\\\\_h',
  'J<' : 'J\\\\_<',
  'Jj' : 'J\\\\j',
  'Jj_c' : 'J\\\\j_}',
  'Jb' : 'J\\\\b',
  'Jb_c' : 'J\\\\b_}',
  'Jb_h' : 'J\\\\b_h',
  'hl' : 'K',
  'Zl' : 'K\\\\',
  'L' : 'L',
  'Lg' : 'L\\\\',
  '4' : 'M',
  '4)' : 'M\\\\',
  '4~' : 'M_~',
  'N' : 'N',
  'N=' : 'N_=',
  'Nq' : 'N\\\\',
  '>' : 'O',
  '>~' : 'O_~',
  'p|' : 'O\\\\',
  '5' : 'Q',
  'K' : 'R',
  'QK' : 'G\\\\R',
  'QK_c' : 'G\\\\R_}',
  'R' : 'R\\\\',
  'S' : 'S',
  'T' : 'T',
  'U' : 'U',
  'U~' : 'U_~',
  '^' : 'V',
  'W' : 'W',
  'X' : 'X',
  'HH' : 'X\\\\',
  'HH_v' : 'X\\\\_v',
  'Y' : 'Y',
  'Z' : 'Z',
  '&' : '@',
  '&~' : '@_~',
  '@' : '{',
  '@~' : '{_~',
  'ux' : '}',
  'ix' : '1',
  '7' : '2',
  '3' : '3',
  'r(' : '4',
  'ax' : '6',
  'ax~' : '6_~',
  '2' : '7',
  '8' : '9',
  '8~' : '9_~',
  '6' : '&',
  '?' : '?',
  '|' : '\\\\',
  'l|' : '\\\\|\\\\',
  'c|' : '=\\\\',
  'ia' : 'ia',
  'ie' : 'ie',
  'i5' : 'iQ',
  'iax' : 'i6',
  'yax' : 'y6',
  '7ax' : '26',
  'Eax' : 'E6',
  'eax' : 'e6',
  'io' : 'io',
  'iu' : 'iu',
  'i4' : 'iM',
  'i&' : 'i@',
  'yu' : 'yu',
  'ie' : 'ie',
  'ei' : 'ei',
  'eI' : 'eI',
  'eo' : 'eo',
  'eu' : 'eu',
  'e&' : 'e@',
  'Ei' : 'Ei',
  'E5' : 'EQ',
  'Eu' : 'Eu',
  '@5' : '{Q',
  '@u' : '{u',
  '>i' : 'Oi',
  '>I' : 'OI',
  '>Y' : 'OY',
  '>&' : 'O@',
  '8y' : '9y',
  '8u' : '9u',
  'ai' : 'ai',
  'aI' : 'aI',
  'ae' : 'ae',
  'eax' : 'e6',
  'aU' : 'aU',
  'a4' : 'aM',
  'ao' : 'ao',
  'au' : 'au',
  'Au' : 'Au',
  'oi' : 'oi',
  'oI' : 'oI',
  'oU' : 'oU',
  'oa' : 'oa',
  'oax' : 'o6',
  'ou' : 'ou',
  'o5' : 'oQ',
  'ui' : 'ui',
  'ua' : 'ua',
  'uax' : 'u6',
  'u5' : 'uQ',
  'uo' : 'uo',
  'u&' : 'u@',
  '4i' : 'Mi',
  '4a' : 'Ma',
  '&u' : '@u',
  ':' : ':',
  'X' : '_"',
  '+' : '_+',
  '-' : '_-',
  '0' : '_0',
  '>' : '_>',
  '<' : '_<',
  '=' : '_=',
  '!' : '_?\\\\',
  'c' : '_}',
  'r' : '`',
  '~' : '_~',
  ']' : '_a',
  'e_l' : '_B',
  '[' : '_d',
  '2' : '_G',
  'e_7' : '_H',
  'h' : '_h',
  'j' : '_j',
  '?' : '_k',
  'e_3' : '_L',
  'l' : '_l',
  'e_5' : 'M',
  '}' : '_m',
  '{' : '_N',
  'n' : '_n',
  '(w)' : '_O',
  '\\\\' : '_o',
  '^' : '_r',
  'e_9' : '_T',
  'Hv' : '_t',
  'v' : '_v',
  'w' : '_w',
  '(' : '_X',
  'x' : '_x',
  }

########NEW FILE########
__FILENAME__ = X_Tables
# coding=utf-8
TransTable = {
    u'\x80' : ['(##)', ''],
    u'\x81' : ['(##)', ''],
    u'\x82' : ['(##)', ''],
    u'\x83' : ['(##)', ''],
    u'\x84' : ['(##)', ''],
    u'\x85' : ['(##)', ''],
    u'\x86' : ['(##)', ''],
    u'\x87' : ['(##)', ''],
    u'\x88' : ['(##)', ''],
    u'\x89' : ['(##)', ''],
    u'\x8a' : ['(##)', ''],
    u'\x8b' : ['(##)', ''],
    u'\x8c' : ['(##)', ''],
    u'\x8d' : ['(##)', ''],
    u'\x8e' : ['(##)', ''],
    u'\x8f' : ['(##)', ''],
    u'\x90' : ['(##)', ''],
    u'\x91' : ['(##)', ''],
    u'\x92' : ['(##)', ''],
    u'\x93' : ['(##)', ''],
    u'\x94' : ['(##)', ''],
    u'\x95' : ['(##)', ''],
    u'\x96' : ['(##)', ''],
    u'\x97' : ['(##)', ''],
    u'\x98' : ['(##)', ''],
    u'\x99' : ['(##)', ''],
    u'\x9a' : ['(##)', ''],
    u'\x9b' : ['(##)', ''],
    u'\x9c' : ['(##)', ''],
    u'\x9d' : ['(##)', ''],
    u'\x9e' : ['(##)', ''],
    u'\x9f' : ['(##)', ''],
    u'\xa0' : ['(##)', ' '],
    u'\xa1' : ['(##)', '¡'],
    u'\xa2' : ['(##)', '¢'],
    u'\xa3' : ['(##)', '£'],
    u'\xa4' : ['(##)', '¤'],
    u'\xa5' : ['(##)', '¥'],
    u'\xa6' : ['(##)', '¦'],
    u'\xa7' : ['(##)', '§'],
    u'\xa8' : ['(##)', '¨'],
    u'\xa9' : ['(##)', '©'],
    u'\xaa' : ['(##)', 'ª'],
    u'\xab' : ['(##)', '«'],
    u'\xac' : ['(##)', '¬'],
    u'\xad' : ['(##)', '­'],
    u'\xae' : ['(##)', '®'],
    u'\xaf' : ['(##)', '¯'],
    u'\xb0' : ['(##)', '°'],
    u'\xb1' : ['(##)', '±'],
    u'\xb2' : ['(##)', '²'],
    u'\xb3' : ['(##)', '³'],
    u'\xb4' : ['(##)', '´'],
    u'\xb5' : ['(##)', 'µ'],
    u'\xb6' : ['(##)', '¶'],
    u'\xb7' : ['(##)', '·'],
    u'\xb8' : ['(##)', '¸'],
    u'\xb9' : ['(##)', '¹'],
    u'\xba' : ['(##)', 'º'],
    u'\xbb' : ['(##)', '»'],
    u'\xbc' : ['(##)', '¼'],
    u'\xbd' : ['(##)', '½'],
    u'\xbe' : ['(##)', '¾'],
    u'\xbf' : ['(##)', '¿'],
    u'\xc0' : ['(##)', 'À'],
    u'\xc1' : ['(##)', 'Á'],
    u'\xc2' : ['(##)', 'Â'],
    u'\xc3' : ['(##)', 'Ã'],
    u'\xc4' : ['(##)', 'Ä'],
    u'\xc5' : ['(##)', 'Å'],
    u'\xc6' : ['(##)', 'Æ'],
    u'\xc7' : ['(##)', 'Ç'],
    u'\xc8' : ['(##)', 'È'],
    u'\xc9' : ['(##)', 'É'],
    u'\xca' : ['(##)', 'Ê'],
    u'\xcb' : ['(##)', 'Ë'],
    u'\xcc' : ['(##)', 'Ì'],
    u'\xcd' : ['(##)', 'Í'],
    u'\xce' : ['(##)', 'Î'],
    u'\xcf' : ['(##)', 'Ï'],
    u'\xd0' : ['(##)', 'Ð'],
    u'\xd1' : ['(##)', 'Ñ'],
    u'\xd2' : ['(##)', 'Ò'],
    u'\xd3' : ['(##)', 'Ó'],
    u'\xd4' : ['(##)', 'Ô'],
    u'\xd5' : ['(##)', 'Õ'],
    u'\xd6' : ['(##)', 'Ö'],
    u'\xd7' : ['(##)', '×'],
    u'\xd8' : ['(##)', 'Ø'],
    u'\xd9' : ['(##)', 'Ù'],
    u'\xda' : ['(##)', 'Ú'],
    u'\xdb' : ['(##)', 'Û'],
    u'\xdc' : ['(##)', 'Ü'],
    u'\xdd' : ['(##)', 'Ý'],
    u'\xde' : ['(##)', 'Þ'],
    u'\xdf' : ['(##)', 'ß'],
    u'\xe0' : ['(##)', 'à'],
    u'\xe1' : ['(##)', 'á'],
    u'\xe2' : ['(##)', 'â'],
    u'\xe3' : ['(##)', 'ã'],
    u'\xe4' : ['(##)', 'ä'],
    u'\xe5' : ['(##)', 'å'],
    u'\xe6' : ['(##)', 'æ'],
    u'\xe7' : ['(##)', 'ç'],
    u'\xe8' : ['(##)', 'è'],
    u'\xe9' : ['(##)', 'é'],
    u'\xea' : ['(##)', 'ê'],
    u'\xeb' : ['(##)', 'ë'],
    u'\xec' : ['(##)', 'ì'],
    u'\xed' : ['(##)', 'í'],
    u'\xee' : ['(##)', 'î'],
    u'\xef' : ['(##)', 'ï'],
    u'\xf0' : ['(##)', 'ð'],
    u'\xf1' : ['(##)', 'ñ'],
    u'\xf2' : ['(##)', 'ò'],
    u'\xf3' : ['(##)', 'ó'],
    u'\xf4' : ['(##)', 'ô'],
    u'\xf5' : ['(##)', 'õ'],
    u'\xf6' : ['(##)', 'ö'],
    u'\xf7' : ['(##)', '÷'],
    u'\xf8' : ['(##)', 'ø'],
    u'\xf9' : ['(##)', 'ù'],
    u'\xfa' : ['(##)', 'ú'],
    u'\xfb' : ['(##)', 'û'],
    u'\xfc' : ['(##)', 'ü'],
    u'\xfd' : ['(##)', 'ý'],
    u'\xfe' : ['(##)', 'þ'],
    u'\xff' : ['(##)', 'ÿ'],
    u'\u0100' : ['(##)', 'Ā'],
    u'\u0101' : ['(##)', 'ā'],
    u'\u0102' : ['(##)', 'Ă'],
    u'\u0103' : ['(##)', 'ă'],
    u'\u0104' : ['(##)', 'Ą'],
    u'\u0105' : ['(##)', 'ą'],
    u'\u0106' : ['(##)', 'Ć'],
    u'\u0107' : ['(##)', 'ć'],
    u'\u0108' : ['(##)', 'Ĉ'],
    u'\u0109' : ['(##)', 'ĉ'],
    u'\u010a' : ['(##)', 'Ċ'],
    u'\u010b' : ['(##)', 'ċ'],
    u'\u010c' : ['(##)', 'Č'],
    u'\u010d' : ['(##)', 'č'],
    u'\u010e' : ['(##)', 'Ď'],
    u'\u010f' : ['(##)', 'ď'],
    u'\u0110' : ['(##)', 'Đ'],
    u'\u0111' : ['(##)', 'đ'],
    u'\u0112' : ['(##)', 'Ē'],
    u'\u0113' : ['(##)', 'ē'],
    u'\u0114' : ['(##)', 'Ĕ'],
    u'\u0115' : ['(##)', 'ĕ'],
    u'\u0116' : ['(##)', 'Ė'],
    u'\u0117' : ['(##)', 'ė'],
    u'\u0118' : ['(##)', 'Ę'],
    u'\u0119' : ['(##)', 'ę'],
    u'\u011a' : ['(##)', 'Ě'],
    u'\u011b' : ['(##)', 'ě'],
    u'\u011c' : ['(##)', 'Ĝ'],
    u'\u011d' : ['(##)', 'ĝ'],
    u'\u011e' : ['(##)', 'Ğ'],
    u'\u011f' : ['(##)', 'ğ'],
    u'\u0120' : ['(##)', 'Ġ'],
    u'\u0121' : ['(##)', 'ġ'],
    u'\u0122' : ['(##)', 'Ģ'],
    u'\u0123' : ['(##)', 'ģ'],
    u'\u0124' : ['(##)', 'Ĥ'],
    u'\u0125' : ['(##)', 'ĥ'],
    u'\u0126' : ['(##)', 'Ħ'],
    u'\u0127' : ['(##)', 'ħ'],
    u'\u0128' : ['(##)', 'Ĩ'],
    u'\u0129' : ['(##)', 'ĩ'],
    u'\u012a' : ['(##)', 'Ī'],
    u'\u012b' : ['(##)', 'ī'],
    u'\u012c' : ['(##)', 'Ĭ'],
    u'\u012d' : ['(##)', 'ĭ'],
    u'\u012e' : ['(##)', 'Į'],
    u'\u012f' : ['(##)', 'į'],
    u'\u0130' : ['(##)', 'İ'],
    u'\u0131' : ['(##)', 'ı'],
    u'\u0132' : ['(##)', 'Ĳ'],
    u'\u0133' : ['(##)', 'ĳ'],
    u'\u0134' : ['(##)', 'Ĵ'],
    u'\u0135' : ['(##)', 'ĵ'],
    u'\u0136' : ['(##)', 'Ķ'],
    u'\u0137' : ['(##)', 'ķ'],
    u'\u0138' : ['(##)', 'ĸ'],
    u'\u0139' : ['(##)', 'Ĺ'],
    u'\u013a' : ['(##)', 'ĺ'],
    u'\u013b' : ['(##)', 'Ļ'],
    u'\u013c' : ['(##)', 'ļ'],
    u'\u013d' : ['(##)', 'Ľ'],
    u'\u013e' : ['(##)', 'ľ'],
    u'\u013f' : ['(##)', 'Ŀ'],
    u'\u0140' : ['(##)', 'ŀ'],
    u'\u0141' : ['(##)', 'Ł'],
    u'\u0142' : ['(##)', 'ł'],
    u'\u0143' : ['(##)', 'Ń'],
    u'\u0144' : ['(##)', 'ń'],
    u'\u0145' : ['(##)', 'Ņ'],
    u'\u0146' : ['(##)', 'ņ'],
    u'\u0147' : ['(##)', 'Ň'],
    u'\u0148' : ['(##)', 'ň'],
    u'\u0149' : ['(##)', 'ŉ'],
    u'\u014a' : ['(##)', 'Ŋ'],
    u'\u014b' : ['(##)', 'ŋ'],
    u'\u014c' : ['(##)', 'Ō'],
    u'\u014d' : ['(##)', 'ō'],
    u'\u014e' : ['(##)', 'Ŏ'],
    u'\u014f' : ['(##)', 'ŏ'],
    u'\u0150' : ['(##)', 'Ő'],
    u'\u0151' : ['(##)', 'ő'],
    u'\u0152' : ['(##)', 'Œ'],
    u'\u0153' : ['(##)', 'œ'],
    u'\u0154' : ['(##)', 'Ŕ'],
    u'\u0155' : ['(##)', 'ŕ'],
    u'\u0156' : ['(##)', 'Ŗ'],
    u'\u0157' : ['(##)', 'ŗ'],
    u'\u0158' : ['(##)', 'Ř'],
    u'\u0159' : ['(##)', 'ř'],
    u'\u015a' : ['(##)', 'Ś'],
    u'\u015b' : ['(##)', 'ś'],
    u'\u015c' : ['(##)', 'Ŝ'],
    u'\u015d' : ['(##)', 'ŝ'],
    u'\u015e' : ['(##)', 'Ş'],
    u'\u015f' : ['(##)', 'ş'],
    u'\u0160' : ['(##)', 'Š'],
    u'\u0161' : ['(##)', 'š'],
    u'\u0162' : ['(##)', 'Ţ'],
    u'\u0163' : ['(##)', 'ţ'],
    u'\u0164' : ['(##)', 'Ť'],
    u'\u0165' : ['(##)', 'ť'],
    u'\u0166' : ['(##)', 'Ŧ'],
    u'\u0167' : ['(##)', 'ŧ'],
    u'\u0168' : ['(##)', 'Ũ'],
    u'\u0169' : ['(##)', 'ũ'],
    u'\u016a' : ['(##)', 'Ū'],
    u'\u016b' : ['(##)', 'ū'],
    u'\u016c' : ['(##)', 'Ŭ'],
    u'\u016d' : ['(##)', 'ŭ'],
    u'\u016e' : ['(##)', 'Ů'],
    u'\u016f' : ['(##)', 'ů'],
    u'\u0170' : ['(##)', 'Ű'],
    u'\u0171' : ['(##)', 'ű'],
    u'\u0172' : ['(##)', 'Ų'],
    u'\u0173' : ['(##)', 'ų'],
    u'\u0174' : ['(##)', 'Ŵ'],
    u'\u0175' : ['(##)', 'ŵ'],
    u'\u0176' : ['(##)', 'Ŷ'],
    u'\u0177' : ['(##)', 'ŷ'],
    u'\u0178' : ['(##)', 'Ÿ'],
    u'\u0179' : ['(##)', 'Ź'],
    u'\u017a' : ['(##)', 'ź'],
    u'\u017b' : ['(##)', 'Ż'],
    u'\u017c' : ['(##)', 'ż'],
    u'\u017d' : ['(##)', 'Ž'],
    u'\u017e' : ['(##)', 'ž'],
    u'\u017f' : ['(##)', 'ſ'],
    u'\u0180' : ['(##)', 'ƀ'],
    u'\u0181' : ['(##)', 'Ɓ'],
    u'\u0182' : ['(##)', 'Ƃ'],
    u'\u0183' : ['(##)', 'ƃ'],
    u'\u0184' : ['(##)', 'Ƅ'],
    u'\u0185' : ['(##)', 'ƅ'],
    u'\u0186' : ['(##)', 'Ɔ'],
    u'\u0187' : ['(##)', 'Ƈ'],
    u'\u0188' : ['(##)', 'ƈ'],
    u'\u0189' : ['(##)', 'Ɖ'],
    u'\u018a' : ['(##)', 'Ɗ'],
    u'\u018b' : ['(##)', 'Ƌ'],
    u'\u018c' : ['(##)', 'ƌ'],
    u'\u018d' : ['(##)', 'ƍ'],
    u'\u018e' : ['(##)', 'Ǝ'],
    u'\u018f' : ['(##)', 'Ə'],
    u'\u0190' : ['(##)', 'Ɛ'],
    u'\u0191' : ['(##)', 'Ƒ'],
    u'\u0192' : ['(##)', 'ƒ'],
    u'\u0193' : ['(##)', 'Ɠ'],
    u'\u0194' : ['(##)', 'Ɣ'],
    u'\u0195' : ['(##)', 'ƕ'],
    u'\u0196' : ['(##)', 'Ɩ'],
    u'\u0197' : ['(##)', 'Ɨ'],
    u'\u0198' : ['(##)', 'Ƙ'],
    u'\u0199' : ['(##)', 'ƙ'],
    u'\u019a' : ['(##)', 'ƚ'],
    u'\u019b' : ['(##)', 'ƛ'],
    u'\u019c' : ['(##)', 'Ɯ'],
    u'\u019d' : ['(##)', 'Ɲ'],
    u'\u019e' : ['(##)', 'ƞ'],
    u'\u019f' : ['(##)', 'Ɵ'],
    u'\u01a0' : ['(##)', 'Ơ'],
    u'\u01a1' : ['(##)', 'ơ'],
    u'\u01a2' : ['(##)', 'Ƣ'],
    u'\u01a3' : ['(##)', 'ƣ'],
    u'\u01a4' : ['(##)', 'Ƥ'],
    u'\u01a5' : ['(##)', 'ƥ'],
    u'\u01a6' : ['(##)', 'Ʀ'],
    u'\u01a7' : ['(##)', 'Ƨ'],
    u'\u01a8' : ['(##)', 'ƨ'],
    u'\u01a9' : ['(##)', 'Ʃ'],
    u'\u01aa' : ['(##)', 'ƪ'],
    u'\u01ab' : ['(##)', 'ƫ'],
    u'\u01ac' : ['(##)', 'Ƭ'],
    u'\u01ad' : ['(##)', 'ƭ'],
    u'\u01ae' : ['(##)', 'Ʈ'],
    u'\u01af' : ['(##)', 'Ư'],
    u'\u01b0' : ['(##)', 'ư'],
    u'\u01b1' : ['(##)', 'Ʊ'],
    u'\u01b2' : ['(##)', 'Ʋ'],
    u'\u01b3' : ['(##)', 'Ƴ'],
    u'\u01b4' : ['(##)', 'ƴ'],
    u'\u01b5' : ['(##)', 'Ƶ'],
    u'\u01b6' : ['(##)', 'ƶ'],
    u'\u01b7' : ['(##)', 'Ʒ'],
    u'\u01b8' : ['(##)', 'Ƹ'],
    u'\u01b9' : ['(##)', 'ƹ'],
    u'\u01ba' : ['(##)', 'ƺ'],
    u'\u01bb' : ['(##)', 'ƻ'],
    u'\u01bc' : ['(##)', 'Ƽ'],
    u'\u01bd' : ['(##)', 'ƽ'],
    u'\u01be' : ['(##)', 'ƾ'],
    u'\u01bf' : ['(##)', 'ƿ'],
    u'\u01c0' : ['(##)', 'ǀ'],
    u'\u01c1' : ['(##)', 'ǁ'],
    u'\u01c2' : ['(##)', 'ǂ'],
    u'\u01c3' : ['(##)', 'ǃ'],
    u'\u01c4' : ['(##)', 'Ǆ'],
    u'\u01c5' : ['(##)', 'ǅ'],
    u'\u01c6' : ['(##)', 'ǆ'],
    u'\u01c7' : ['(##)', 'Ǉ'],
    u'\u01c8' : ['(##)', 'ǈ'],
    u'\u01c9' : ['(##)', 'ǉ'],
    u'\u01ca' : ['(##)', 'Ǌ'],
    u'\u01cb' : ['(##)', 'ǋ'],
    u'\u01cc' : ['(##)', 'ǌ'],
    u'\u01cd' : ['(##)', 'Ǎ'],
    u'\u01ce' : ['(##)', 'ǎ'],
    u'\u01cf' : ['(##)', 'Ǐ'],
    u'\u01d0' : ['(##)', 'ǐ'],
    u'\u01d1' : ['(##)', 'Ǒ'],
    u'\u01d2' : ['(##)', 'ǒ'],
    u'\u01d3' : ['(##)', 'Ǔ'],
    u'\u01d4' : ['(##)', 'ǔ'],
    u'\u01d5' : ['(##)', 'Ǖ'],
    u'\u01d6' : ['(##)', 'ǖ'],
    u'\u01d7' : ['(##)', 'Ǘ'],
    u'\u01d8' : ['(##)', 'ǘ'],
    u'\u01d9' : ['(##)', 'Ǚ'],
    u'\u01da' : ['(##)', 'ǚ'],
    u'\u01db' : ['(##)', 'Ǜ'],
    u'\u01dc' : ['(##)', 'ǜ'],
    u'\u01dd' : ['(##)', 'ǝ'],
    u'\u01de' : ['(##)', 'Ǟ'],
    u'\u01df' : ['(##)', 'ǟ'],
    u'\u01e0' : ['(##)', 'Ǡ'],
    u'\u01e1' : ['(##)', 'ǡ'],
    u'\u01e2' : ['(##)', 'Ǣ'],
    u'\u01e3' : ['(##)', 'ǣ'],
    u'\u01e4' : ['(##)', 'Ǥ'],
    u'\u01e5' : ['(##)', 'ǥ'],
    u'\u01e6' : ['(##)', 'Ǧ'],
    u'\u01e7' : ['(##)', 'ǧ'],
    u'\u01e8' : ['(##)', 'Ǩ'],
    u'\u01e9' : ['(##)', 'ǩ'],
    u'\u01ea' : ['(##)', 'Ǫ'],
    u'\u01eb' : ['(##)', 'ǫ'],
    u'\u01ec' : ['(##)', 'Ǭ'],
    u'\u01ed' : ['(##)', 'ǭ'],
    u'\u01ee' : ['(##)', 'Ǯ'],
    u'\u01ef' : ['(##)', 'ǯ'],
    u'\u01f0' : ['(##)', 'ǰ'],
    u'\u0200' : ['(##)', 'Ȁ'],
    u'\u0201' : ['(##)', 'ȁ'],
    u'\u0202' : ['(##)', 'Ȃ'],
    u'\u0203' : ['(##)', 'ȃ'],
    u'\u0204' : ['(##)', 'Ȅ'],
    u'\u0205' : ['(##)', 'ȅ'],
    u'\u0206' : ['(##)', 'Ȇ'],
    u'\u0207' : ['(##)', 'ȇ'],
    u'\u0208' : ['(##)', 'Ȉ'],
    u'\u0209' : ['(##)', 'ȉ'],
    u'\u020a' : ['(##)', 'Ȋ'],
    u'\u020b' : ['(##)', 'ȋ'],
    u'\u020c' : ['(##)', 'Ȍ'],
    u'\u020d' : ['(##)', 'ȍ'],
    u'\u020e' : ['(##)', 'Ȏ'],
    u'\u020f' : ['(##)', 'ȏ'],
    u'\u0210' : ['(##)', 'Ȑ'],
    u'\u0211' : ['(##)', 'ȑ'],
    u'\u0212' : ['(##)', 'Ȓ'],
    u'\u0213' : ['(##)', 'ȓ'],
    u'\u0214' : ['(##)', 'Ȕ'],
    u'\u0215' : ['(##)', 'ȕ'],
    u'\u0216' : ['(##)', 'Ȗ'],
    u'\u0217' : ['(##)', 'ȗ'],
    u'\u0218' : ['(##)', 'Ș'],
    u'\u0219' : ['(##)', 'ș'],
    u'\u021a' : ['(##)', 'Ț'],
    u'\u021b' : ['(##)', 'ț'],
    u'\u021c' : ['(##)', 'Ȝ'],
    u'\u021d' : ['(##)', 'ȝ'],
    u'\u021e' : ['(##)', 'Ȟ'],
    u'\u021f' : ['(##)', 'ȟ'],
    u'\u0220' : ['(##)', 'Ƞ'],
    u'\u0221' : ['(##)', 'ȡ'],
    u'\u0222' : ['(##)', 'Ȣ'],
    u'\u0223' : ['(##)', 'ȣ'],
    u'\u0224' : ['(##)', 'Ȥ'],
    u'\u0225' : ['(##)', 'ȥ'],
    u'\u0226' : ['(##)', 'Ȧ'],
    u'\u0227' : ['(##)', 'ȧ'],
    u'\u0228' : ['(##)', 'Ȩ'],
    u'\u0229' : ['(##)', 'ȩ'],
    u'\u022a' : ['(##)', 'Ȫ'],
    u'\u022b' : ['(##)', 'ȫ'],
    u'\u022c' : ['(##)', 'Ȭ'],
    u'\u022d' : ['(##)', 'ȭ'],
    u'\u022e' : ['(##)', 'Ȯ'],
    u'\u022f' : ['(##)', 'ȯ'],
    u'\u0230' : ['(##)', 'Ȱ'],
    u'\u0231' : ['(##)', 'ȱ'],
    u'\u0232' : ['(##)', 'Ȳ'],
    u'\u0233' : ['(##)', 'ȳ'],
    u'\u0234' : ['(##)', 'ȴ'],
    u'\u0235' : ['(##)', 'ȵ'],
    u'\u0236' : ['(##)', 'ȶ'],
    u'\u0237' : ['(##)', 'ȷ'],
    u'\u0238' : ['(##)', 'ȸ'],
    u'\u0239' : ['(##)', 'ȹ'],
    u'\u023a' : ['(##)', 'Ⱥ'],
    u'\u023b' : ['(##)', 'Ȼ'],
    u'\u023c' : ['(##)', 'ȼ'],
    u'\u023d' : ['(##)', 'Ƚ'],
    u'\u023e' : ['(##)', 'Ⱦ'],
    u'\u023f' : ['(##)', 'ȿ'],
    u'\u0240' : ['(##)', 'ɀ'],
    u'\u0241' : ['(##)', 'Ɂ'],
    u'\u0242' : ['(##)', 'ɂ'],
    u'\u0243' : ['(##)', 'Ƀ'],
    u'\u0244' : ['(##)', 'Ʉ'],
    u'\u0245' : ['(##)', 'Ʌ'],
    u'\u0246' : ['(##)', 'Ɇ'],
    u'\u0247' : ['(##)', 'ɇ'],
    u'\u0248' : ['(##)', 'Ɉ'],
    u'\u0249' : ['(##)', 'ɉ'],
    u'\u024a' : ['(##)', 'Ɋ'],
    u'\u024b' : ['(##)', 'ɋ'],
    u'\u024c' : ['(##)', 'Ɍ'],
    u'\u024d' : ['(##)', 'ɍ'],
    u'\u024e' : ['(##)', 'Ɏ'],
    u'\u024f' : ['(##)', 'ɏ'],
    u'\u0374' : ['(NUMERAL SIGN)', 'ʹ'],
    u'\u0375' : ['(LOWER NUMERAL SIGN)', '͵'],
    u'\u037a' : ['(YPOGEGRAMMENI)', 'ͺ'],
    u'\u037e' : ['(QUESTION MARK)', ';'],
    u'\u0384' : ['(TONOS)', '΄'],
    u'\u0385' : ['(DIALYTIKA TONOS)', '΅'],
    u'\u0386' : ['A', 'Ά'],
    u'\u0387' : ['(TELEIA)', '·'],
    u'\u0388' : ['e', 'Έ'],
    u'\u0389' : ['E:', 'Ή'],
    u'\u038a' : ['i', 'Ί'],
    u'\u038c' : ['o', 'Ό'],
    u'\u038e' : ['i', 'Ύ'],
    u'\u038f' : ['o', 'Ώ'],
    u'\u0390' : ['i', 'ΐ'],
    u'\u0391' : ['A', 'Α'],
    u'\u0392' : ['b', 'Β'],
    u'\u0393' : ['g', 'Γ'],
    u'\u0394' : ['d', 'Δ'],
    u'\u0395' : ['e', 'Ε'],
    u'\u0396' : ['z', 'Ζ'],
    u'\u0397' : ['E:', 'Η'],
    u'\u0398' : ['t_h', 'Θ'],
    u'\u0399' : ['i', 'Ι'],
    u'\u039a' : ['k', 'Κ'],
    u'\u039b' : ['_l', 'Λ'],
    u'\u039c' : ['m', 'Μ'],
    u'\u039d' : ['_n', 'Ν'],
    u'\u039e' : ['k s', 'Ξ'],
    u'\u039f' : ['o', 'Ο'],
    u'\u03a0' : ['p', 'Π'],
    u'\u03a1' : ['`', 'Ρ'],
    u'\u03a3' : ['s', 'Σ'],
    u'\u03a4' : ['t', 'Τ'],
    u'\u03a5' : ['i', 'Υ'],
    u'\u03a6' : ['f', 'Φ'],
    u'\u03a7' : ['_"', 'Χ'],
    u'\u03a8' : ['p s', 'Ψ'],
    u'\u03a9' : ['o', 'Ω'],
    u'\u03aa' : ['i', 'Ϊ'],
    u'\u03ab' : ['i', 'Ϋ'],
    u'\u03ac' : ['A', 'ά'],
    u'\u03ad' : ['e', 'έ'],
    u'\u03ae' : ['E:', 'ή'],
    u'\u03af' : ['i', 'ί'],
    u'\u03b0' : ['i', 'ΰ'],
    u'\u03b1' : ['A', 'α'],
    u'\u03b2' : ['b', 'β'],
    u'\u03b3' : ['g', 'γ'],
    u'\u03b4' : ['d', 'δ'],
    u'\u03b5' : ['e', 'ε'],
    u'\u03b6' : ['z', 'ζ'],
    u'\u03b7' : ['E:', 'η'],
    u'\u03b8' : ['t_h', 'θ'],
    u'\u03b9' : ['i', 'ι'],
    u'\u03ba' : ['k', 'κ'],
    u'\u03bb' : ['_l', 'λ'],
    u'\u03bc' : ['m', 'μ'],
    u'\u03bd' : ['_n', 'ν'],
    u'\u03be' : ['k s', 'ξ'],
    u'\u03bf' : ['o', 'ο'],
    u'\u03c0' : ['p', 'π'],
    u'\u03c1' : ['`', 'ρ'],
    u'\u03c2' : ['s', 'ς'],
    u'\u03c3' : ['s', 'σ'],
    u'\u03c4' : ['t', 'τ'],
    u'\u03c5' : ['i', 'υ'],
    u'\u03c6' : ['f', 'φ'],
    u'\u03c7' : ['_"', 'χ'],
    u'\u03c8' : ['p s', 'ψ'],
    u'\u03c9' : ['o', 'ω'],
    u'\u03ca' : ['i', 'ϊ'],
    u'\u03cb' : ['i', 'ϋ'],
    u'\u03cc' : ['o', 'ό'],
    u'\u03cd' : ['i', 'ύ'],
    u'\u03ce' : ['o', 'ώ'],
    u'\u03d0' : ['b', 'ϐ'],
    u'\u03d1' : ['t_h', 'ϑ'],
    u'\u03d2' : ['i', 'ϒ'],
    u'\u03d3' : ['i', 'ϓ'],
    u'\u03d4' : ['i', 'ϔ'],
    u'\u03d5' : ['f', 'ϕ'],
    u'\u03d6' : ['p', 'ϖ'],
    u'\u03d7' : ['(KAI)', 'ϗ'],
    u'\u03d8' : ['q', 'Ϙ'],
    u'\u03d9' : ['q', 'ϙ'],
    u'\u03da' : ['s', 'Ϛ'],
    u'\u03db' : ['s', 'ϛ'],
    u'\u03dc' : ['_w', 'Ϝ'],
    u'\u03dd' : ['_w', 'ϝ'],
    u'\u03de' : ['q', 'Ϟ'],
    u'\u03df' : ['q', 'ϟ'],
    u'\u03e0' : ['(NINE HUNDREDS)', 'Ϡ'],
    u'\u03e1' : ['(NINE HUNDREDS)', 'ϡ'],
    u'\u03e2' : ['S', 'Ϣ'],
    u'\u03e3' : ['S', 'ϣ'],
    u'\u03e4' : ['f', 'Ϥ'],
    u'\u03e5' : ['f', 'ϥ'],
    u'\u03e6' : ['_h', 'Ϧ'],
    u'\u03e7' : ['_h', 'ϧ'],
    u'\u03e8' : ['_h', 'Ϩ'],
    u'\u03e9' : ['_h', 'ϩ'],
    u'\u03ea' : ['dZ', 'Ϫ'],
    u'\u03eb' : ['dZ', 'ϫ'],
    u'\u03ec' : ['q', 'Ϭ'],
    u'\u03ed' : ['q', 'ϭ'],
    u'\u03ee' : ['t i', 'Ϯ'],
    u'\u03ef' : ['t i', 'ϯ'],
    u'\u03f0' : ['k', 'ϰ'],
    u'\u03f1' : ['`', 'ϱ'],
    u'\u03f2' : ['s', 'ϲ'],
    u'\u03f3' : ['(YOT)', 'ϳ'],
    u'\u03f4' : ['t_h', 'ϴ'],
    u'\u03f5' : ['e', 'ϵ'],
    u'\u03f6' : ['e', '϶'],
    u'\u03f7' : ['S', 'Ϸ'],
    u'\u03f8' : ['S', 'ϸ'],
    u'\u03f9' : ['s', 'Ϲ'],
    u'\u03fa' : ['(SAN)', 'Ϻ'],
    u'\u03fb' : ['(SAN)', 'ϻ'],
    u'\u03fc' : ['`', 'ϼ'],
    u'\u03fd' : ['s', 'Ͻ'],
    u'\u03fe' : ['s', 'Ͼ'],
    u'\u03ff' : ['s', 'Ͽ'],
    u'\u0400' : ['ie', 'Ѐ'],
    u'\u0401' : ['io', 'Ё'],
    u'\u0402' : ['d z\\', 'Ђ'],
    u'\u0403' : ['d z\\', 'Ѓ'],
    u'\u0404' : ['ie', 'Є'],
    u'\u0405' : ['dz', 'Ѕ'],
    u'\u0406' : ['i', 'І'],
    u'\u0407' : ['_j', 'Ї'],
    u'\u0408' : ['_j', 'Ј'],
    u'\u0409' : ['lj', 'Љ'],
    u'\u040a' : ['J', 'Њ'],
    u'\u040b' : ['tSj', 'Ћ'],
    u'\u040c' : ['kj, tSj', 'Ќ'],
    u'\u040d' : ['i', 'Ѝ'],
    u'\u040e' : ['u(', 'Ў'],
    u'\u040f' : ['dZ', 'Џ'],
    u'\u0410' : ['A', 'А'],
    u'\u0411' : ['b', 'Б'],
    u'\u0412' : ['_v', 'В'],
    u'\u0413' : ['g', 'Г'],
    u'\u0414' : ['d', 'Д'],
    u'\u0415' : ['ie', 'Е'],
    u'\u0416' : ['Z', 'Ж'],
    u'\u0417' : ['z', 'З'],
    u'\u0418' : ['i', 'И'],
    u'\u0419' : ['_j', 'Й'],
    u'\u041a' : ['k', 'К'],
    u'\u041b' : ['_l', 'Л'],
    u'\u041c' : ['m', 'М'],
    u'\u041d' : ['_n', 'Н'],
    u'\u041e' : ['_>', 'О'],
    u'\u041f' : ['p', 'П'],
    u'\u0420' : ['`', 'Р'],
    u'\u0421' : ['s', 'С'],
    u'\u0422' : ['t', 'Т'],
    u'\u0423' : ['u', 'У'],
    u'\u0424' : ['f', 'Ф'],
    u'\u0425' : ['_x', 'Х'],
    u'\u0426' : ['ts', 'Ц'],
    u'\u0427' : ['tSj', 'Ч'],
    u'\u0428' : ['S', 'Ш'],
    u'\u0429' : ['Sj tSj', 'Щ'],
    u'\u042a' : ['(HARD SIGN)', 'Ъ'],
    u'\u042b' : ['1', 'Ы'],
    u'\u042c' : ['(SOFT SIGN)', 'Ь'],
    u'\u042d' : ['E', 'Э'],
    u'\u042e' : ['u', 'Ю'],
    u'\u042f' : ['A', 'Я'],
    u'\u0430' : ['A', 'а'],
    u'\u0431' : ['b', 'б'],
    u'\u0432' : ['_v', 'в'],
    u'\u0433' : ['g', 'г'],
    u'\u0434' : ['d', 'д'],
    u'\u0435' : ['ie', 'е'],
    u'\u0436' : ['Z', 'ж'],
    u'\u0437' : ['z', 'з'],
    u'\u0438' : ['i', 'и'],
    u'\u0439' : ['_j', 'й'],
    u'\u043a' : ['k', 'к'],
    u'\u043b' : ['_l', 'л'],
    u'\u043c' : ['m', 'м'],
    u'\u043d' : ['_n', 'н'],
    u'\u043e' : ['_>', 'о'],
    u'\u043f' : ['p', 'п'],
    u'\u0440' : ['`', 'р'],
    u'\u0441' : ['s', 'с'],
    u'\u0442' : ['t', 'т'],
    u'\u0443' : ['u', 'у'],
    u'\u0444' : ['f', 'ф'],
    u'\u0445' : ['_x', 'х'],
    u'\u0446' : ['ts', 'ц'],
    u'\u0447' : ['tSj', 'ч'],
    u'\u0448' : ['S', 'ш'],
    u'\u0449' : ['Sj tSj', 'щ'],
    u'\u044a' : ['(HARD SIGN)', 'ъ'],
    u'\u044b' : ['1', 'ы'],
    u'\u044c' : ['(SOFT SIGN)', 'ь'],
    u'\u044d' : ['E', 'э'],
    u'\u044e' : ['u', 'ю'],
    u'\u044f' : ['A', 'я'],
    u'\u0450' : ['E', 'ѐ'],
    u'\u0451' : ['io', 'ё'],
    u'\u0452' : ['d Z}', 'ђ'],
    u'\u0453' : ['d z\\', 'ѓ'],
    u'\u0454' : ['ie', 'є'],
    u'\u0455' : ['dz', 'ѕ'],
    u'\u0456' : ['i', 'і'],
    u'\u0457' : ['_j', 'ї'],
    u'\u0458' : ['_j', 'ј'],
    u'\u0459' : ['lj', 'љ'],
    u'\u045a' : ['J', 'њ'],
    u'\u045b' : ['tSj', 'ћ'],
    u'\u045c' : ['kj, tSj', 'ќ'],
    u'\u045d' : ['i', 'ѝ'],
    u'\u045e' : ['u(', 'ў'],
    u'\u045f' : ['dZ', 'џ'],
    u'\u0460' : ['o', 'Ѡ'],
    u'\u0461' : ['o', 'ѡ'],
    u'\u0462' : ['{', 'Ѣ'],
    u'\u0463' : ['{', 'ѣ'],
    u'\u0464' : ['E', 'Ѥ'],
    u'\u0465' : ['E', 'ѥ'],
    u'\u0466' : ['E_~', 'Ѧ'],
    u'\u0467' : ['E_~', 'ѧ'],
    u'\u0468' : ['O_~', 'Ѩ'],
    u'\u0469' : ['O_~', 'ѩ'],
    u'\u046a' : ['E_~', 'Ѫ'],
    u'\u046b' : ['E_~', 'ѫ'],
    u'\u046c' : ['O_~', 'Ѭ'],
    u'\u046d' : ['O_~', 'ѭ'],
    u'\u046e' : ['(INFORMAL ABBREVIATION FOR 1000 PSI)', 'Ѯ'],
    u'\u046f' : ['(INFORMAL ABBREVIATION FOR 1000 PSI)', 'ѯ'],
    u'\u0470' : ['p s', 'Ѱ'],
    u'\u0471' : ['p s', 'ѱ'],
    u'\u0472' : ['f', 'Ѳ'],
    u'\u0473' : ['f', 'ѳ'],
    u'\u0474' : ['i', 'Ѵ'],
    u'\u0475' : ['i', 'ѵ'],
    u'\u0476' : ['i', 'Ѷ'],
    u'\u0477' : ['i', 'ѷ'],
    u'\u0478' : ['u', 'Ѹ'],
    u'\u0479' : ['u', 'ѹ'],
    u'\u047a' : ['o', 'Ѻ'],
    u'\u047b' : ['o', 'ѻ'],
    u'\u047c' : ['o', 'Ѽ'],
    u'\u047d' : ['o', 'ѽ'],
    u'\u047e' : ['o t_d', 'Ѿ'],
    u'\u047f' : ['o t_d', 'ѿ'],
    u'\u0480' : ['(NINETY)', 'Ҁ'],
    u'\u0481' : ['(NINETY)', 'ҁ'],
    u'\u0482' : ['(THOUSAND SIGN)', '҂'],
    u'\u0483' : ['(TITLO)', '҃'],
    u'\u0484' : ['(PALATALIZATION)', '҄'],
    u'\u0485' : ['(DASIA PNEUMATA)', '҅'],
    u'\u0486' : ['(PSILI PNEUMATA)', '҆'],
    u'\u0488' : ['(HUNDRED THOUSAND SIGN)', '҈'],
    u'\u0489' : ['(MILLION SIGN)', '҉'],
    u'\u048a' : ['i', 'Ҋ'],
    u'\u048b' : ['i', 'ҋ'],
    u'\u048c' : ['(SEMISOFT SIGN)', 'Ҍ'],
    u'\u048d' : ['(SEMISOFT SIGN)', 'ҍ'],
    u'\u048e' : ['`', 'Ҏ'],
    u'\u048f' : ['`', 'ҏ'],
    u'\u0490' : ['g', 'Ґ'],
    u'\u0491' : ['g', 'ґ'],
    u'\u0492' : ['g', 'Ғ'],
    u'\u0493' : ['g', 'ғ'],
    u'\u0494' : ['g', 'Ҕ'],
    u'\u0495' : ['g', 'ҕ'],
    u'\u0496' : ['Z', 'Җ'],
    u'\u0497' : ['Z', 'җ'],
    u'\u0498' : ['z', 'Ҙ'],
    u'\u0499' : ['z', 'ҙ'],
    u'\u049a' : ['k', 'Қ'],
    u'\u049b' : ['k', 'қ'],
    u'\u049c' : ['k', 'Ҝ'],
    u'\u049d' : ['k', 'ҝ'],
    u'\u049e' : ['k', 'Ҟ'],
    u'\u049f' : ['k', 'ҟ'],
    u'\u04a0' : ['k', 'Ҡ'],
    u'\u04a1' : ['k', 'ҡ'],
    u'\u04a2' : ['_n', 'Ң'],
    u'\u04a3' : ['_n', 'ң'],
    u'\u04a4' : ['_n g', 'Ҥ'],
    u'\u04a5' : ['_n g', 'ҥ'],
    u'\u04a6' : ['p', 'Ҧ'],
    u'\u04a7' : ['p', 'ҧ'],
    u'\u04a8' : ['_x', 'Ҩ'],
    u'\u04a9' : ['_x', 'ҩ'],
    u'\u04aa' : ['s', 'Ҫ'],
    u'\u04ab' : ['s', 'ҫ'],
    u'\u04ac' : ['t', 'Ҭ'],
    u'\u04ad' : ['t', 'ҭ'],
    u'\u04ae' : ['u', 'Ү'],
    u'\u04af' : ['u', 'ү'],
    u'\u04b0' : ['u', 'Ұ'],
    u'\u04b1' : ['u', 'ұ'],
    u'\u04b2' : ['_x', 'Ҳ'],
    u'\u04b3' : ['_x', 'ҳ'],
    u'\u04b4' : ['t t s', 'Ҵ'],
    u'\u04b5' : ['t t s', 'ҵ'],
    u'\u04b6' : ['tSj', 'Ҷ'],
    u'\u04b7' : ['tSj', 'ҷ'],
    u'\u04b8' : ['tSj', 'Ҹ'],
    u'\u04b9' : ['tSj', 'ҹ'],
    u'\u04ba' : ['_h', 'Һ'],
    u'\u04bb' : ['_h', 'һ'],
    u'\u04bc' : ['tSj', 'Ҽ'],
    u'\u04bd' : ['tSj', 'ҽ'],
    u'\u04be' : ['tSj', 'Ҿ'],
    u'\u04bf' : ['tSj', 'ҿ'],
    u'\u04c0' : ['(ASPIRATION SIGN)', 'Ӏ'],
    u'\u04c1' : ['Z', 'Ӂ'],
    u'\u04c2' : ['Z', 'ӂ'],
    u'\u04c3' : ['k', 'Ӄ'],
    u'\u04c4' : ['k', 'ӄ'],
    u'\u04c5' : ['_l', 'Ӆ'],
    u'\u04c6' : ['_l', 'ӆ'],
    u'\u04c7' : ['_n', 'Ӈ'],
    u'\u04c8' : ['_n', 'ӈ'],
    u'\u04c9' : ['_n', 'Ӊ'],
    u'\u04ca' : ['_n', 'ӊ'],
    u'\u04cb' : ['tSj', 'Ӌ'],
    u'\u04cc' : ['tSj', 'ӌ'],
    u'\u04cd' : ['m', 'Ӎ'],
    u'\u04ce' : ['m', 'ӎ'],
    u'\u04d0' : ['A', 'Ӑ'],
    u'\u04d1' : ['A', 'ӑ'],
    u'\u04d2' : ['A', 'Ӓ'],
    u'\u04d3' : ['A', 'ӓ'],
    u'\u04d4' : ['A ie', 'Ӕ'],
    u'\u04d5' : ['A ie', 'ӕ'],
    u'\u04d6' : ['ie', 'Ӗ'],
    u'\u04d7' : ['ie', 'ӗ'],
    u'\u04d8' : ['@', 'Ә'],
    u'\u04d9' : ['@', 'ә'],
    u'\u04da' : ['@', 'Ӛ'],
    u'\u04db' : ['@', 'ӛ'],
    u'\u04dc' : ['Z', 'Ӝ'],
    u'\u04dd' : ['Z', 'ӝ'],
    u'\u04de' : ['z', 'Ӟ'],
    u'\u04df' : ['z', 'ӟ'],
    u'\u04e0' : ['dz', 'Ӡ'],
    u'\u04e1' : ['dz', 'ӡ'],
    u'\u04e2' : ['i:', 'Ӣ'],
    u'\u04e3' : ['i:', 'ӣ'],
    u'\u04e4' : ['i', 'Ӥ'],
    u'\u04e5' : ['i', 'ӥ'],
    u'\u04e6' : ['o', 'Ӧ'],
    u'\u04e7' : ['o', 'ӧ'],
    u'\u04e8' : ['o', 'Ө'],
    u'\u04e9' : ['o', 'ө'],
    u'\u04ea' : ['o', 'Ӫ'],
    u'\u04eb' : ['o', 'ӫ'],
    u'\u04ec' : ['E', 'Ӭ'],
    u'\u04ed' : ['E', 'ӭ'],
    u'\u04ee' : ['u:', 'Ӯ'],
    u'\u04ef' : ['u:', 'ӯ'],
    u'\u04f0' : ['u', 'Ӱ'],
    u'\u04f1' : ['u', 'ӱ'],
    u'\u04f2' : ['u', 'Ӳ'],
    u'\u04f3' : ['u', 'ӳ'],
    u'\u04f4' : ['tSj', 'Ӵ'],
    u'\u04f5' : ['tSj', 'ӵ'],
    u'\u04f6' : ['g', 'Ӷ'],
    u'\u04f7' : ['g', 'ӷ'],
    u'\u04f8' : ['1', 'Ӹ'],
    u'\u04f9' : ['1', 'ӹ'],
    u'\u0531' : ['A', 'Ա'],
    u'\u0532' : ['b', 'Բ'],
    u'\u0533' : ['g', 'Գ'],
    u'\u0534' : ['d', 'Դ'],
    u'\u0535' : ['E', 'Ե'],
    u'\u0536' : ['z', 'Զ'],
    u'\u0537' : ['e', 'Է'],
    u'\u0538' : ['_>', 'Ը'],
    u'\u0539' : ['t_h', 'Թ'],
    u'\u053a' : ['Z', 'Ժ'],
    u'\u053b' : ['i', 'Ի'],
    u'\u053c' : ['_l', 'Լ'],
    u'\u053d' : ['z', 'Խ'],
    u'\u053e' : ['ts', 'Ծ'],
    u'\u053f' : ['k', 'Կ'],
    u'\u0540' : ['_h', 'Հ'],
    u'\u0541' : ['dz', 'Ձ'],
    u'\u0542' : ['G', 'Ղ'],
    u'\u0543' : ['tS', 'Ճ'],
    u'\u0544' : ['m', 'Մ'],
    u'\u0545' : ['h, _j', 'Յ'],
    u'\u0546' : ['_n', 'Ն'],
    u'\u0547' : ['S', 'Շ'],
    u'\u0548' : ['o', 'Ո'],
    u'\u0549' : ['tSh', 'Չ'],
    u'\u054a' : ['p', 'Պ'],
    u'\u054b' : ['dZ', 'Ջ'],
    u'\u054c' : ['`', 'Ռ'],
    u'\u054d' : ['s', 'Ս'],
    u'\u054e' : ['_v', 'Վ'],
    u'\u054f' : ['t', 'Տ'],
    u'\u0550' : ['r\\', 'Ր'],
    u'\u0551' : ['tsh', 'Ց'],
    u'\u0552' : ['_v', 'Ւ'],
    u'\u0553' : ['p_h', 'Փ'],
    u'\u0554' : ['k_h', 'Ք'],
    u'\u0555' : ['o', 'Օ'],
    u'\u0556' : ['f', 'Ֆ'],
    u'\u0559' : ['(ML)', 'ՙ'],
    u'\u055a' : ['(APOSTROPHE)', '՚'],
    u'\u055b' : ['(EMPHASIS MARK)', '՛'],
    u'\u055c' : ['(EXCLAMATION MARK)', '՜'],
    u'\u055d' : ['(COMMA)', '՝'],
    u'\u055e' : ['(QUESTION MARK)', '՞'],
    u'\u055f' : ['(ABBREVIATION MARK)', '՟'],
    u'\u0561' : ['A', 'ա'],
    u'\u0562' : ['b', 'բ'],
    u'\u0563' : ['g', 'գ'],
    u'\u0564' : ['d', 'դ'],
    u'\u0565' : ['_j E', 'ե'],
    u'\u0566' : ['z', 'զ'],
    u'\u0567' : ['e', 'է'],
    u'\u0568' : ['_>', 'ը'],
    u'\u0569' : ['t_h', 'թ'],
    u'\u056a' : ['Z', 'ժ'],
    u'\u056b' : ['i', 'ի'],
    u'\u056c' : ['_l', 'լ'],
    u'\u056d' : ['_x', 'խ'],
    u'\u056e' : ['ts', 'ծ'],
    u'\u056f' : ['k', 'կ'],
    u'\u0570' : ['_h', 'հ'],
    u'\u0571' : ['dz', 'ձ'],
    u'\u0572' : ['G', 'ղ'],
    u'\u0573' : ['tS', 'ճ'],
    u'\u0574' : ['m', 'մ'],
    u'\u0575' : ['h, _j', 'յ'],
    u'\u0576' : ['_n', 'ն'],
    u'\u0577' : ['S', 'շ'],
    u'\u0578' : ['o', 'ո'],
    u'\u0579' : ['tSh', 'չ'],
    u'\u057a' : ['p', 'պ'],
    u'\u057b' : ['dZ', 'ջ'],
    u'\u057c' : ['`', 'ռ'],
    u'\u057d' : ['s', 'ս'],
    u'\u057e' : ['_v', 'վ'],
    u'\u057f' : ['t', 'տ'],
    u'\u0580' : ['r\\', 'ր'],
    u'\u0581' : ['tsh', 'ց'],
    u'\u0582' : ['_w', 'ւ'],
    u'\u0583' : ['p_h', 'փ'],
    u'\u0584' : ['k_h', 'ք'],
    u'\u0585' : ['o', 'օ'],
    u'\u0586' : ['f', 'ֆ'],
    u'\u0587' : ['E _w', 'և'],
    u'\u0589' : ['(FULL STOP)', '։'],
    u'\u058a' : ['(HYPHEN)', '֊'],
    u'\u0591' : ['(##)', '֑'],
    u'\u0592' : ['(##)', '֒'],
    u'\u0593' : ['(##)', '֓'],
    u'\u0594' : ['(##)', '֔'],
    u'\u0595' : ['(##)', '֕'],
    u'\u0596' : ['(##)', '֖'],
    u'\u0597' : ['(##)', '֗'],
    u'\u0598' : ['(##)', '֘'],
    u'\u0599' : ['(##)', '֙'],
    u'\u059a' : ['(##)', '֚'],
    u'\u059b' : ['(##)', '֛'],
    u'\u059c' : ['(##)', '֜'],
    u'\u059d' : ['(##)', '֝'],
    u'\u059e' : ['(##)', '֞'],
    u'\u059f' : ['(##)', '֟'],
    u'\u05a0' : ['(##)', '֠'],
    u'\u05a1' : ['(##)', '֡'],
    u'\u05a2' : ['(##)', '֢'],
    u'\u05a3' : ['(##)', '֣'],
    u'\u05a4' : ['(##)', '֤'],
    u'\u05a5' : ['(##)', '֥'],
    u'\u05a6' : ['(##)', '֦'],
    u'\u05a7' : ['(##)', '֧'],
    u'\u05a8' : ['(##)', '֨'],
    u'\u05a9' : ['(##)', '֩'],
    u'\u05aa' : ['(##)', '֪'],
    u'\u05ab' : ['(##)', '֫'],
    u'\u05ac' : ['(##)', '֬'],
    u'\u05ad' : ['(##)', '֭'],
    u'\u05ae' : ['(##)', '֮'],
    u'\u05af' : ['(##)', '֯'],
    u'\u05b0' : ['(##)', 'ְ'],
    u'\u05b1' : ['(##)', 'ֱ'],
    u'\u05b2' : ['(##)', 'ֲ'],
    u'\u05b3' : ['(##)', 'ֳ'],
    u'\u05b4' : ['(##)', 'ִ'],
    u'\u05b5' : ['(##)', 'ֵ'],
    u'\u05b6' : ['(##)', 'ֶ'],
    u'\u05b7' : ['(##)', 'ַ'],
    u'\u05b8' : ['(##)', 'ָ'],
    u'\u05b9' : ['(##)', 'ֹ'],
    u'\u05bb' : ['(##)', 'ֻ'],
    u'\u05bc' : ['(##)', 'ּ'],
    u'\u05bd' : ['(##)', 'ֽ'],
    u'\u05be' : ['(##)', '־'],
    u'\u05bf' : ['(##)', 'ֿ'],
    u'\u05c0' : ['(##)', '׀'],
    u'\u05c1' : ['(##)', 'ׁ'],
    u'\u05c2' : ['(##)', 'ׂ'],
    u'\u05c3' : ['(##)', '׃'],
    u'\u05c4' : ['(##)', 'ׄ'],
    u'\u05c5' : ['(##)', 'ׅ'],
    u'\u05c6' : ['(##)', '׆'],
    u'\u05c7' : ['(##)', 'ׇ'],
    u'\u05d0' : ['?, 2', 'א'],
    u'\u05d1' : ['b, _v', 'ב'],
    u'\u05d2' : ['g', 'ג'],
    u'\u05d3' : ['d', 'ד'],
    u'\u05d4' : ['_h', 'ה'],
    u'\u05d5' : ['_v', 'ו'],
    u'\u05d6' : ['z', 'ז'],
    u'\u05d7' : ['_x', 'ח'],
    u'\u05d8' : ['t', 'ט'],
    u'\u05d9' : ['_j', 'י'],
    u'\u05da' : ['k, _x', 'ך'],
    u'\u05db' : ['k, _x', 'כ'],
    u'\u05dc' : ['_l', 'ל'],
    u'\u05dd' : ['m', 'ם'],
    u'\u05de' : ['m', 'מ'],
    u'\u05df' : ['_n', 'ן'],
    u'\u05e0' : ['_n', 'נ'],
    u'\u05e1' : ['s', 'ס'],
    u'\u05e2' : ['?, 2', 'ע'],
    u'\u05e3' : ['p, f', 'ף'],
    u'\u05e4' : ['p, f', 'פ'],
    u'\u05e5' : ['ts', 'ץ'],
    u'\u05e6' : ['ts', 'צ'],
    u'\u05e7' : ['k', 'ק'],
    u'\u05e8' : ['k', 'ר'],
    u'\u05e9' : ['s, S', 'ש'],
    u'\u05ea' : ['t', 'ת'],
    u'\u05f0' : ['_v', 'װ'],
    u'\u05f1' : ['>j', 'ױ'],
    u'\u05f2' : ['Ej', 'ײ'],
    u'\u05f3' : ['(##)', '׳'],
    u'\u05f4' : ['(##)', '״'],
    u'\u0600' : ['(NUMBER SIGN)', '؀'],
    u'\u0601' : ['(SANAH)', '؁'],
    u'\u0602' : ['(FOOTNOTEMARKER)', '؂'],
    u'\u0603' : ['(SAPHA)', '؃'],
    u'\u060b' : ['(AFGHANI SIGN)', '؋'],
    u'\u060c' : ['(COMMA)', '،'],
    u'\u060d' : ['(DATE SEPARATOR)', '؍'],
    u'\u060e' : ['(POETIC VERSE SIGN)', '؎'],
    u'\u060f' : ['(SIGN MISRA)', '؏'],
    u'\u0610' : ['(##)', 'ؐ'],
    u'\u0611' : ['(##)', 'ؑ'],
    u'\u0612' : ['(##)', 'ؒ'],
    u'\u0613' : ['(##)', 'ؓ'],
    u'\u0614' : ['(##)', 'ؔ'],
    u'\u0615' : ['(##)', 'ؕ'],
    u'\u061b' : ['(SEMICOLON)', '؛'],
    u'\u061e' : ['(TRIPLE DOT)', '؞'],
    u'\u061f' : ['(QUESTION MARK)', '؟'],
    u'\u0621' : ['(HAMZA)', 'ء'],
    u'\u0622' : ['_k', 'آ'],
    u'\u0623' : ['_k', 'أ'],
    u'\u0624' : ['_w', 'ؤ'],
    u'\u0625' : ['_k', 'إ'],
    u'\u0626' : ['_j', 'ئ'],
    u'\u0627' : ['_j', 'ا'],
    u'\u0628' : ['b', 'ب'],
    u'\u0629' : ['t', 'ة'],
    u'\u062a' : ['t', 'ت'],
    u'\u062b' : ['T', 'ث'],
    u'\u062c' : ['dZ', 'ج'],
    u'\u062d' : ['H', 'ح'],
    u'\u062e' : ['_x', 'خ'],
    u'\u062f' : ['d', 'د'],
    u'\u0630' : ['D', 'ذ'],
    u'\u0631' : ['`', 'ر'],
    u'\u0632' : ['z', 'ز'],
    u'\u0633' : ['s', 'س'],
    u'\u0634' : ['S', 'ش'],
    u'\u0635' : ['s~', 'ص'],
    u'\u0636' : ['d~', 'ض'],
    u'\u0637' : ['t~', 'ط'],
    u'\u0638' : ['z~', 'ظ'],
    u'\u0639' : ['h\\', 'ع'],
    u'\u063a' : ['G', 'غ'],
    u'\u0640' : ['(TATWEEL)', 'ـ'],
    u'\u0641' : ['f', 'ف'],
    u'\u0642' : ['q', 'ق'],
    u'\u0643' : ['k', 'ك'],
    u'\u0644' : ['_l', 'ل'],
    u'\u0645' : ['m', 'م'],
    u'\u0646' : ['_n', 'ن'],
    u'\u0647' : ['_h', 'ه'],
    u'\u0648' : ['_w', 'و'],
    u'\u0649' : ['a:', 'ى'],
    u'\u064a' : ['_j', 'ي'],
    u'\u064b' : ['(FATHATAN)', 'ً'],
    u'\u064c' : ['(DAMMATAN)', 'ٌ'],
    u'\u064d' : ['(KASRATAN)', 'ٍ'],
    u'\u064e' : ['E', 'َ'],
    u'\u064f' : ['U', 'ُ'],
    u'\u0650' : ['I', 'ِ'],
    u'\u0651' : ['(SHADDA)', 'ّ'],
    u'\u0652' : ['a~', 'ْ'],
    u'\u0653' : ['(MADDAH ABOVE)', 'ٓ'],
    u'\u0654' : ['(HAMZA ABOVE)', 'ٔ'],
    u'\u0655' : ['(HAMZE BELOW)', 'ٕ'],
    u'\u0656' : ['(SUBSCRIPT ALEF)', 'ٖ'],
    u'\u0657' : ['(INVERTED DAMMA)', 'ٗ'],
    u'\u0658' : ['(NOON GHUNNA)', '٘'],
    u'\u0659' : ['(ZWARAKAY)', 'ٙ'],
    u'\u065a' : ['(SMALL B ABOVE)', 'ٚ'],
    u'\u065b' : ['(INVERTED SMALL V)', 'ٛ'],
    u'\u065c' : ['(DOT BELOW)', 'ٜ'],
    u'\u065d' : ['(REVERSED DAMMA)', 'ٝ'],
    u'\u065e' : ['(FATHA WITH TWO DOTS)', 'ٞ'],
    u'\u0660' : ['(ZERO)', '٠'],
    u'\u0661' : ['(ONE)', '١'],
    u'\u0662' : ['(TWO)', '٢'],
    u'\u0663' : ['(THREE)', '٣'],
    u'\u0664' : ['(FOUR)', '٤'],
    u'\u0665' : ['(FIVE)', '٥'],
    u'\u0666' : ['(SIX)', '٦'],
    u'\u0667' : ['(SEVEN)', '٧'],
    u'\u0668' : ['(EIGHT)', '٨'],
    u'\u0669' : ['(NINE)', '٩'],
    u'\u066a' : ['(PERCENT SIGN)', '٪'],
    u'\u066b' : ['(DECIMMAL SEPARATOR)', '٫'],
    u'\u066c' : ['(THOUSAND SEPARATOR)', '٬'],
    u'\u066d' : ['(ASTERISK)', '٭'],
    u'\u066e' : ['b', 'ٮ'],
    u'\u066f' : ['q', 'ٯ'],
    u'\u0670' : ['a~', 'ٰ'],
    u'\u0671' : ['_k', 'ٱ'],
    u'\u0672' : ['_k', 'ٲ'],
    u'\u0673' : ['_k', 'ٳ'],
    u'\u0674' : ['(HIGH HAMZA)', 'ٴ'],
    u'\u0675' : ['_k', 'ٵ'],
    u'\u0676' : ['_w', 'ٶ'],
    u'\u0677' : ['_w', 'ٷ'],
    u'\u0678' : ['_j', 'ٸ'],
    u'\u0679' : ['t`', 'ٹ'],
    u'\u067a' : ['t`_h', 'ٺ'],
    u'\u067b' : ['t_d', 'ٻ'],
    u'\u067c' : ['t`', 'ټ'],
    u'\u067d' : ['t`', 'ٽ'],
    u'\u067e' : ['p', 'پ'],
    u'\u067f' : ['t_h', 'ٿ'],
    u'\u0680' : ['b_h', 'ڀ'],
    u'\u0681' : ['dz', 'ځ'],
    u'\u0682' : ['dz', 'ڂ'],
    u'\u0683' : ['J', 'ڃ'],
    u'\u0684' : ['f', 'ڄ'],
    u'\u0685' : ['ts', 'څ'],
    u'\u0686' : ['tS', 'چ'],
    u'\u0687' : ['tSh', 'ڇ'],
    u'\u0688' : ['d`', 'ڈ'],
    u'\u0689' : ['d`', 'ډ'],
    u'\u068a' : ['d`', 'ڊ'],
    u'\u068b' : ['d`', 'ڋ'],
    u'\u068c' : ['d_h', 'ڌ'],
    u'\u068d' : ['d`', 'ڍ'],
    u'\u068e' : ['d`', 'ڎ'],
    u'\u068f' : ['d`', 'ڏ'],
    u'\u0690' : ['d`', 'ڐ'],
    u'\u0691' : ['`', 'ڑ'],
    u'\u0692' : ['`', 'ڒ'],
    u'\u0693' : ['r`', 'ړ'],
    u'\u0694' : ['r`', 'ڔ'],
    u'\u0695' : ['`', 'ڕ'],
    u'\u0696' : ['`', 'ږ'],
    u'\u0697' : ['`', 'ڗ'],
    u'\u0698' : ['Z', 'ژ'],
    u'\u0699' : ['r`', 'ڙ'],
    u'\u069a' : ['s`', 'ښ'],
    u'\u069b' : ['s`', 'ڛ'],
    u'\u069c' : ['s`', 'ڜ'],
    u'\u069d' : ['s~', 'ڝ'],
    u'\u069e' : ['s~', 'ڞ'],
    u'\u069f' : ['t~', 'ڟ'],
    u'\u06a0' : ['N', 'ڠ'],
    u'\u06a1' : ['f', 'ڡ'],
    u'\u06a2' : ['f', 'ڢ'],
    u'\u06a3' : ['f', 'ڣ'],
    u'\u06a4' : ['_v', 'ڤ'],
    u'\u06a5' : ['f', 'ڥ'],
    u'\u06a6' : ['p_h', 'ڦ'],
    u'\u06a7' : ['f', 'ڧ'],
    u'\u06a8' : ['f', 'ڨ'],
    u'\u06a9' : ['k', 'ک'],
    u'\u06aa' : ['k', 'ڪ'],
    u'\u06ab' : ['g', 'ګ'],
    u'\u06ac' : ['k', 'ڬ'],
    u'\u06ad' : ['J', 'ڭ'],
    u'\u06ae' : ['k', 'ڮ'],
    u'\u06af' : ['g', 'گ'],
    u'\u06b0' : ['g', 'ڰ'],
    u'\u06b1' : ['N', 'ڱ'],
    u'\u06b2' : ['g', 'ڲ'],
    u'\u06b3' : ['g', 'ڳ'],
    u'\u06b4' : ['g', 'ڴ'],
    u'\u06b5' : ['_l', 'ڵ'],
    u'\u06b6' : ['l`', 'ڶ'],
    u'\u06b7' : ['_l', 'ڷ'],
    u'\u06b8' : ['_l', 'ڸ'],
    u'\u06b9' : ['_n', 'ڹ'],
    u'\u06ba' : ['_n', 'ں'],
    u'\u06bb' : ['n"', 'ڻ'],
    u'\u06bc' : ['n"', 'ڼ'],
    u'\u06bd' : ['n"', 'ڽ'],
    u'\u06be' : ['(ASPIRATED)', 'ھ'],
    u'\u06bf' : ['tS', 'ڿ'],
    u'\u06c0' : ['_h', 'ۀ'],
    u'\u06c1' : ['_h', 'ہ'],
    u'\u06c2' : ['_h', 'ۂ'],
    u'\u06c3' : ['_h', 'ۃ'],
    u'\u06c4' : ['_w', 'ۄ'],
    u'\u06c5' : ['o', 'ۅ'],
    u'\u06c6' : ['o', 'ۆ'],
    u'\u06c7' : ['u:', 'ۇ'],
    u'\u06c8' : ['u', 'ۈ'],
    u'\u06c9' : ['u', 'ۉ'],
    u'\u06ca' : ['q', 'ۊ'],
    u'\u06cb' : ['_v', 'ۋ'],
    u'\u06cc' : ['_j', 'ی'],
    u'\u06cd' : ['i', 'ۍ'],
    u'\u06ce' : ['i:', 'ێ'],
    u'\u06cf' : ['_w', 'ۏ'],
    u'\u06d0' : ['b', 'ې'],
    u'\u06d1' : ['i:', 'ۑ'],
    u'\u06d2' : ['e:', 'ے'],
    u'\u06d3' : ['e:', 'ۓ'],
    u'\u06d4' : ['(FULL STOP)', '۔'],
    u'\u06d5' : ['@', 'ە'],
    u'\u06d6' : ['(##)', 'ۖ'],
    u'\u06d7' : ['(##)', 'ۗ'],
    u'\u06d8' : ['(##)', 'ۘ'],
    u'\u06d9' : ['(##)', 'ۙ'],
    u'\u06da' : ['(##)', 'ۚ'],
    u'\u06db' : ['(##)', 'ۛ'],
    u'\u06dc' : ['(##)', 'ۜ'],
    u'\u06dd' : ['(##)', '۝'],
    u'\u06de' : ['(##)', '۞'],
    u'\u06df' : ['(##)', '۟'],
    u'\u06e0' : ['(##)', '۠'],
    u'\u06e1' : ['(##)', 'ۡ'],
    u'\u06e2' : ['(##)', 'ۢ'],
    u'\u06e3' : ['(##)', 'ۣ'],
    u'\u06e4' : ['(##)', 'ۤ'],
    u'\u06e5' : ['(##)', 'ۥ'],
    u'\u06e6' : ['(##)', 'ۦ'],
    u'\u06e7' : ['(##)', 'ۧ'],
    u'\u06e8' : ['(##)', 'ۨ'],
    u'\u06e9' : ['(##)', '۩'],
    u'\u06ea' : ['(##)', '۪'],
    u'\u06eb' : ['(##)', '۫'],
    u'\u06ec' : ['(##)', '۬'],
    u'\u06ed' : ['(##)', 'ۭ'],
    u'\u06ee' : ['d`', 'ۮ'],
    u'\u06ef' : ['r`', 'ۯ'],
    u'\u06f0' : ['(ZERO)', '۰'],
    u'\u06f1' : ['(ONE)', '۱'],
    u'\u06f2' : ['(TWO)', '۲'],
    u'\u06f3' : ['(THREE)', '۳'],
    u'\u06f4' : ['(FOUR)', '۴'],
    u'\u06f5' : ['(FIVE)', '۵'],
    u'\u06f6' : ['(SIX)', '۶'],
    u'\u06f7' : ['(SEVEN)', '۷'],
    u'\u06f8' : ['(EIGHT)', '۸'],
    u'\u06f9' : ['(NINE)', '۹'],
    u'\u06fa' : ['S', 'ۺ'],
    u'\u06fb' : ['z', 'ۻ'],
    u'\u06fc' : ['_G', 'ۼ'],
    u'\u06fd' : ['(AMPERSAND)', '۽'],
    u'\u06fe' : ['(POSTPOSTION MEN)', '۾'],
    u'\u06ff' : ['_h', 'ۿ'],
    u'\u0700' : ['(##)', '܀'],
    u'\u0701' : ['(##)', '܁'],
    u'\u0702' : ['(##)', '܂'],
    u'\u0703' : ['(##)', '܃'],
    u'\u0704' : ['(##)', '܄'],
    u'\u0705' : ['(##)', '܅'],
    u'\u0706' : ['(##)', '܆'],
    u'\u0707' : ['(##)', '܇'],
    u'\u0708' : ['(##)', '܈'],
    u'\u0709' : ['(##)', '܉'],
    u'\u070a' : ['(##)', '܊'],
    u'\u070b' : ['(##)', '܋'],
    u'\u070c' : ['(##)', '܌'],
    u'\u070d' : ['(##)', '܍'],
    u'\u070f' : ['(##)', '܏'],
    u'\u0710' : ['_k', 'ܐ'],
    u'\u0711' : ['_k', 'ܑ'],
    u'\u0712' : ['b, _v', 'ܒ'],
    u'\u0713' : ['g, G', 'ܓ'],
    u'\u0714' : ['g, G', 'ܔ'],
    u'\u0715' : ['d, D', 'ܕ'],
    u'\u0716' : ['d, D', 'ܖ'],
    u'\u0717' : ['_h', 'ܗ'],
    u'\u0718' : ['_w', 'ܘ'],
    u'\u0719' : ['z', 'ܙ'],
    u'\u071a' : ['X\\', 'ܚ'],
    u'\u071b' : ['t~', 'ܛ'],
    u'\u071c' : ['t~', 'ܜ'],
    u'\u071d' : ['_j', 'ܝ'],
    u'\u071e' : ['_j _h', 'ܞ'],
    u'\u071f' : ['k, _x', 'ܟ'],
    u'\u0720' : ['_l', 'ܠ'],
    u'\u0721' : ['m', 'ܡ'],
    u'\u0722' : ['_n', 'ܢ'],
    u'\u0723' : ['s', 'ܣ'],
    u'\u0724' : ['s', 'ܤ'],
    u'\u0725' : ['X\\_v', 'ܥ'],
    u'\u0726' : ['p, f', 'ܦ'],
    u'\u0727' : ['p, f', 'ܧ'],
    u'\u0728' : ['s~', 'ܨ'],
    u'\u0729' : ['q', 'ܩ'],
    u'\u072a' : ['`', 'ܪ'],
    u'\u072b' : ['S', 'ܫ'],
    u'\u072c' : ['t', 'ܬ'],
    u'\u072d' : ['b, _v', 'ܭ'],
    u'\u072e' : ['g, G', 'ܮ'],
    u'\u072f' : ['d, D', 'ܯ'],
    u'\u0730' : ['(##)', 'ܰ'],
    u'\u0731' : ['(##)', 'ܱ'],
    u'\u0732' : ['(##)', 'ܲ'],
    u'\u0733' : ['(##)', 'ܳ'],
    u'\u0734' : ['(##)', 'ܴ'],
    u'\u0735' : ['(##)', 'ܵ'],
    u'\u0736' : ['(##)', 'ܶ'],
    u'\u0737' : ['(##)', 'ܷ'],
    u'\u0738' : ['(##)', 'ܸ'],
    u'\u0739' : ['(##)', 'ܹ'],
    u'\u073a' : ['(##)', 'ܺ'],
    u'\u073b' : ['(##)', 'ܻ'],
    u'\u073c' : ['(##)', 'ܼ'],
    u'\u073d' : ['(##)', 'ܽ'],
    u'\u073e' : ['(##)', 'ܾ'],
    u'\u073f' : ['(##)', 'ܿ'],
    u'\u0740' : ['(##)', '݀'],
    u'\u0741' : ['(##)', '݁'],
    u'\u0742' : ['(##)', '݂'],
    u'\u0743' : ['(##)', '݃'],
    u'\u0744' : ['(##)', '݄'],
    u'\u0745' : ['(##)', '݅'],
    u'\u0746' : ['(##)', '݆'],
    u'\u0747' : ['(##)', '݇'],
    u'\u0748' : ['(##)', '݈'],
    u'\u0749' : ['(##)', '݉'],
    u'\u074a' : ['(##)', '݊'],
    u'\u074d' : ['zh', 'ݍ'],
    u'\u074e' : ['k_h', 'ݎ'],
    u'\u074f' : ['(##)', 'ݏ'],
    u'\u0780' : ['_h', 'ހ'],
    u'\u0781' : ['s`', 'ށ'],
    u'\u0782' : ['_n', 'ނ'],
    u'\u0783' : ['`', 'ރ'],
    u'\u0784' : ['b', 'ބ'],
    u'\u0785' : ['l`', 'ޅ'],
    u'\u0786' : ['k', 'ކ'],
    u'\u0787' : ['(SILENT)', 'އ'],
    u'\u0788' : ['_v', 'ވ'],
    u'\u0789' : ['m', 'މ'],
    u'\u078a' : ['f', 'ފ'],
    u'\u078b' : ['d', 'ދ'],
    u'\u078c' : ['t', 'ތ'],
    u'\u078d' : ['_l', 'ލ'],
    u'\u078e' : ['g', 'ގ'],
    u'\u078f' : ['J', 'ޏ'],
    u'\u0790' : ['s', 'ސ'],
    u'\u0791' : ['d`', 'ޑ'],
    u'\u0792' : ['z', 'ޒ'],
    u'\u0793' : ['t`', 'ޓ'],
    u'\u0794' : ['_j', 'ޔ'],
    u'\u0795' : ['p', 'ޕ'],
    u'\u0796' : ['j\\', 'ޖ'],
    u'\u0797' : ['_}', 'ޗ'],
    u'\u0798' : ['T', 'ޘ'],
    u'\u0799' : ['D', 'ޙ'],
    u'\u079a' : ['X\\', 'ޚ'],
    u'\u079b' : ['_x', 'ޛ'],
    u'\u079c' : ['z', 'ޜ'],
    u'\u079d' : ['S', 'ޝ'],
    u'\u079e' : ['s`', 'ޞ'],
    u'\u079f' : ['d`', 'ޟ'],
    u'\u07a0' : ['t`', 'ޠ'],
    u'\u07a1' : ['z`', 'ޡ'],
    u'\u07a2' : ['X\\_v', 'ޢ'],
    u'\u07a3' : ['G', 'ޣ'],
    u'\u07a4' : ['q', 'ޤ'],
    u'\u07a5' : ['_w', 'ޥ'],
    u'\u07a6' : ['A', 'ަ'],
    u'\u07a7' : ['A:', 'ާ'],
    u'\u07a8' : ['i', 'ި'],
    u'\u07a9' : ['i:', 'ީ'],
    u'\u07aa' : ['u', 'ު'],
    u'\u07ab' : ['u:', 'ޫ'],
    u'\u07ac' : ['e', 'ެ'],
    u'\u07ad' : ['e:', 'ޭ'],
    u'\u07ae' : ['o', 'ޮ'],
    u'\u07af' : ['o:', 'ޯ'],
    u'\u07b0' : ['_k', 'ް'],
    u'\u07b1' : ['_n', 'ޱ'],
    u'\u0901' : ['(CD)', 'ँ'],
    u'\u0902' : ['(M)', 'ं'],
    u'\u0903' : ['(H)', 'ः'],
    u'\u0904' : ['A', 'ऄ'],
    u'\u0905' : ['A', 'अ'],
    u'\u0906' : ['A:', 'आ'],
    u'\u0907' : ['i', 'इ'],
    u'\u0908' : ['i:', 'ई'],
    u'\u0909' : ['u', 'उ'],
    u'\u090a' : ['u:', 'ऊ'],
    u'\u090b' : ['9r=', 'ऋ'],
    u'\u090c' : ['l_=', 'ऌ'],
    u'\u090d' : ['e', 'ऍ'],
    u'\u090e' : ['e', 'ऎ'],
    u'\u090f' : ['e', 'ए'],
    u'\u0910' : ['aI', 'ऐ'],
    u'\u0911' : ['o', 'ऑ'],
    u'\u0912' : ['o', 'ऒ'],
    u'\u0913' : ['o', 'ओ'],
    u'\u0914' : ['aU', 'औ'],
    u'\u0915' : ['k A', 'क'],
    u'\u0916' : ['k_h A', 'ख'],
    u'\u0917' : ['g A', 'ग'],
    u'\u0918' : ['g_h A', 'घ'],
    u'\u0919' : ['N A', 'ङ'],
    u'\u091a' : ['_} A', 'च'],
    u'\u091b' : ['c_h A', 'छ'],
    u'\u091c' : ['J\\ A', 'ज'],
    u'\u091d' : ['J\\_h A', 'झ'],
    u'\u091e' : ['J A', 'ञ'],
    u'\u091f' : ['t` A', 'ट'],
    u'\u0920' : ['t`_h A', 'ठ'],
    u'\u0921' : ['d` A', 'ड'],
    u'\u0922' : ['d`_h A', 'ढ'],
    u'\u0923' : ['n" A', 'ण'],
    u'\u0924' : ['t_d A', 'त'],
    u'\u0925' : ['t_d_h A', 'थ'],
    u'\u0926' : ['d_d A', 'द'],
    u'\u0927' : ['d_d_h A', 'ध'],
    u'\u0928' : ['n_d A', 'न'],
    u'\u0929' : ['_n A', 'ऩ'],
    u'\u092a' : ['p A', 'प'],
    u'\u092b' : ['p_h A', 'फ'],
    u'\u092c' : ['b A', 'ब'],
    u'\u092d' : ['b_h A', 'भ'],
    u'\u092e' : ['m A', 'म'],
    u'\u092f' : ['_j A', 'य'],
    u'\u0930' : ['r\\` A', 'र'],
    u'\u0931' : ['r\\` A', 'ऱ'],
    u'\u0932' : ['_l A', 'ल'],
    u'\u0933' : ['l` A', 'ळ'],
    u'\u0934' : ['_l A', 'ऴ'],
    u'\u0935' : ['_v A', 'व'],
    u'\u0936' : ['s\\ A', 'श'],
    u'\u0937' : ['s` A', 'ष'],
    u'\u0938' : ['s A', 'स'],
    u'\u0939' : ['h\\ A', 'ह'],
    u'\u093c' : ['(NUKTA)', '़'],
    u'\u093d' : ['(AVAGRAHA)', 'ऽ'],
    u'\u093e' : ['A:', 'ा'],
    u'\u093f' : ['i', 'ि'],
    u'\u0940' : ['i:', 'ी'],
    u'\u0941' : ['u', 'ु'],
    u'\u0942' : ['u:', 'ू'],
    u'\u0943' : ['9r=', 'ृ'],
    u'\u0944' : ['rr=', 'ॄ'],
    u'\u0945' : ['e', 'ॅ'],
    u'\u0946' : ['e', 'ॆ'],
    u'\u0947' : ['e', 'े'],
    u'\u0948' : ['aI', 'ै'],
    u'\u0949' : ['o', 'ॉ'],
    u'\u094a' : ['o', 'ॊ'],
    u'\u094b' : ['o', 'ो'],
    u'\u094c' : ['aU', 'ौ'],
    u'\u094d' : ['(P)', '्'],
    u'\u0950' : ['(OM)', 'ॐ'],
    u'\u0951' : ['(UDATTA)', '॑'],
    u'\u0952' : ['(ADUDATTA)', '॒'],
    u'\u0953' : ['(GRAVE ACCENT)', '॓'],
    u'\u0954' : ['(ACUTE ACCENT)', '॔'],
    u'\u0958' : ['q A', 'क़'],
    u'\u0959' : ['_x A', 'ख़'],
    u'\u095a' : ['G A', 'ग़'],
    u'\u095b' : ['z A', 'ज़'],
    u'\u095c' : ['r` A', 'ड़'],
    u'\u095d' : ['rrh A', 'ढ़'],
    u'\u095e' : ['f A', 'फ़'],
    u'\u095f' : ['_j', 'य़'],
    u'\u0960' : ['rr=', 'ॠ'],
    u'\u0961' : ['lr=', 'ॡ'],
    u'\u0962' : ['l_=', 'ॢ'],
    u'\u0963' : ['l_=', 'ॣ'],
    u'\u0964' : ['(*)', '।'],
    u'\u0965' : ['(**)', '॥'],
    u'\u0966' : ['(ZERO)', '०'],
    u'\u0967' : ['(ONE)', '१'],
    u'\u0968' : ['(TWO)', '२'],
    u'\u0969' : ['(THREE)', '३'],
    u'\u096a' : ['(FOUR)', '४'],
    u'\u096b' : ['(FIVE)', '५'],
    u'\u096c' : ['(SIX)', '६'],
    u'\u096d' : ['(SEVEN)', '७'],
    u'\u096e' : ['(EIGHT)', '८'],
    u'\u096f' : ['(NINE)', '९'],
    u'\u0970' : ['(ABBREVIATION)', '॰'],
    u'\u097d' : ['_k', 'ॽ'],
    u'\u0981' : ['(CD)', 'ঁ'],
    u'\u0982' : ['(M)', 'ং'],
    u'\u0983' : ['(H)', 'ঃ'],
    u'\u0985' : ['>, o', 'অ'],
    u'\u0986' : ['A', 'আ'],
    u'\u0987' : ['i', 'ই'],
    u'\u0988' : ['i:', 'ঈ'],
    u'\u0989' : ['u', 'উ'],
    u'\u098a' : ['u:', 'ঊ'],
    u'\u098b' : ['r\\` i', 'ঋ'],
    u'\u098c' : ['l_=', 'ঌ'],
    u'\u098d' : ['(R:098D)', '঍'],
    u'\u098e' : ['(R:098E)', '঎'],
    u'\u098f' : ['@, e', 'এ'],
    u'\u0990' : ['oI', 'ঐ'],
    u'\u0991' : ['(R:0991)', '঑'],
    u'\u0992' : ['(R:0992)', '঒'],
    u'\u0993' : ['o', 'ও'],
    u'\u0994' : ['oU', 'ঔ'],
    u'\u0995' : ['k A', 'ক'],
    u'\u0996' : ['k_h A', 'খ'],
    u'\u0997' : ['g A', 'গ'],
    u'\u0998' : ['g_h A', 'ঘ'],
    u'\u0999' : ['N A', 'ঙ'],
    u'\u099a' : ['_} A', 'চ'],
    u'\u099b' : ['c_h A', 'ছ'],
    u'\u099c' : ['J\\ A', 'জ'],
    u'\u099d' : ['J\\_h A', 'ঝ'],
    u'\u099e' : ['J A', 'ঞ'],
    u'\u099f' : ['t` A', 'ট'],
    u'\u09a0' : ['t`_h A', 'ঠ'],
    u'\u09a1' : ['d` A', 'ড'],
    u'\u09a2' : ['d`_h A', 'ঢ'],
    u'\u09a3' : ['n" A', 'ণ'],
    u'\u09a4' : ['t_d A', 'ত'],
    u'\u09a5' : ['t_d_h A', 'থ'],
    u'\u09a6' : ['d_d A', 'দ'],
    u'\u09a7' : ['d_d_h A', 'ধ'],
    u'\u09a8' : ['n_d A', 'ন'],
    u'\u09a9' : ['(R:09A9)', '঩'],
    u'\u09aa' : ['p A', 'প'],
    u'\u09ab' : ['p_h A', 'ফ'],
    u'\u09ac' : ['b A', 'ব'],
    u'\u09ad' : ['b_h A', 'ভ'],
    u'\u09ae' : ['m A', 'ম'],
    u'\u09af' : ['_j A', 'য'],
    u'\u09b0' : ['r\\` A', 'র'],
    u'\u09b1' : ['(R:09B1)', '঱'],
    u'\u09b2' : ['_l A', 'ল'],
    u'\u09b3' : ['(R:09B3)', '঳'],
    u'\u09b4' : ['(R:09B4)', '঴'],
    u'\u09b5' : ['(R:09B5)', '঵'],
    u'\u09b6' : ['S A', 'শ'],
    u'\u09b7' : ['S A', 'ষ'],
    u'\u09b8' : ['s A', 'স'],
    u'\u09b9' : ['h\\ A', 'হ'],
    u'\u09ba' : ['(##)', '঺'],
    u'\u09bc' : ['(NUKTA)', '়'],
    u'\u09bd' : ['(AVAGRAHA)', 'ঽ'],
    u'\u09be' : ['A', 'া'],
    u'\u09bf' : ['i', 'ি'],
    u'\u09c0' : ['i:', 'ী'],
    u'\u09c1' : ['u', 'ু'],
    u'\u09c2' : ['u:', 'ূ'],
    u'\u09c3' : ['9r=', 'ৃ'],
    u'\u09c4' : ['rr=', 'ৄ'],
    u'\u09c5' : ['(R:09C5)', '৅'],
    u'\u09c6' : ['(R:09C6)', '৆'],
    u'\u09c7' : ['e', 'ে'],
    u'\u09c8' : ['oI', 'ৈ'],
    u'\u09c9' : ['(R:09C9)', '৉'],
    u'\u09ca' : ['(R:09CA)', '৊'],
    u'\u09cb' : ['o', 'ো'],
    u'\u09cc' : ['oU', 'ৌ'],
    u'\u09cd' : ['(P)', '্'],
    u'\u09ce' : ['t A', 'ৎ'],
    u'\u09d7' : ['(oU LENGTH MARK)', 'ৗ'],
    u'\u09dc' : ['r` A', 'ড়'],
    u'\u09dd' : ['rh A', 'ঢ়'],
    u'\u09de' : ['t A', '৞'],
    u'\u09df' : ['jr A', 'য়'],
    u'\u09e0' : ['rr=', 'ৠ'],
    u'\u09e1' : ['l_=', 'ৡ'],
    u'\u09e2' : ['l_=', 'ৢ'],
    u'\u09e3' : ['l_=', 'ৣ'],
    u'\u09e4' : ['(*)', '৤'],
    u'\u09e5' : ['(**)', '৥'],
    u'\u09e6' : ['(ZERO)', '০'],
    u'\u09e7' : ['(ONE)', '১'],
    u'\u09e8' : ['(TWO)', '২'],
    u'\u09e9' : ['(THREE)', '৩'],
    u'\u09ea' : ['(FOUR)', '৪'],
    u'\u09eb' : ['(FIVE)', '৫'],
    u'\u09ec' : ['(SIX)', '৬'],
    u'\u09ed' : ['(SEVEN)', '৭'],
    u'\u09ee' : ['(EIGHT)', '৮'],
    u'\u09ef' : ['(NINE)', '৯'],
    u'\u09f0' : ['r\\` A', 'ৰ'],
    u'\u09f1' : ['r\\` A', 'ৱ'],
    u'\u09f2' : ['(RUPEE MARK)', '৲'],
    u'\u09f3' : ['(RUPEE SIGN)', '৳'],
    u'\u09f4' : ['(CURRENCY NUMERATOR 1)', '৴'],
    u'\u09f5' : ['(CURRENCY NUMERATOR 2)', '৵'],
    u'\u09f6' : ['(CURRENCY NUMERATOR 3)', '৶'],
    u'\u09f7' : ['(CURRENCY NUMERATOR M\\', '৷'],
    u'\u09f8' : ['(CURRENCY NUMERATOR 1 LESS THAN THE DENOMINATOR)', '৸'],
    u'\u09f9' : ['(CURRENCY NUMERATOR 16)', '৹'],
    u'\u09fa' : ['(ISSHAR)', '৺'],
    u'\u0a00' : ['(CD)', '਀'],
    u'\u0a01' : ['(R:0A01)', 'ਁ'],
    u'\u0a02' : ['(M)', 'ਂ'],
    u'\u0a03' : ['(H)', 'ਃ'],
    u'\u0a05' : ['@', 'ਅ'],
    u'\u0a06' : ['A', 'ਆ'],
    u'\u0a07' : ['I', 'ਇ'],
    u'\u0a08' : ['i', 'ਈ'],
    u'\u0a09' : ['U', 'ਉ'],
    u'\u0a0a' : ['u', 'ਊ'],
    u'\u0a0b' : ['(R:0A0B)', '਋'],
    u'\u0a0d' : ['(R:0A0D)', '਍'],
    u'\u0a0e' : ['(R:0A0E)', '਎'],
    u'\u0a0f' : ['e', 'ਏ'],
    u'\u0a10' : ['aI', 'ਐ'],
    u'\u0a11' : ['(R:0A11)', '਑'],
    u'\u0a12' : ['(R:0A12)', '਒'],
    u'\u0a13' : ['o', 'ਓ'],
    u'\u0a14' : ['aU', 'ਔ'],
    u'\u0a15' : ['k A', 'ਕ'],
    u'\u0a16' : ['k_h A', 'ਖ'],
    u'\u0a17' : ['g A', 'ਗ'],
    u'\u0a18' : ['g_h A', 'ਘ'],
    u'\u0a19' : ['N A', 'ਙ'],
    u'\u0a1a' : ['_} A', 'ਚ'],
    u'\u0a1b' : ['c_h A', 'ਛ'],
    u'\u0a1c' : ['J\\ A', 'ਜ'],
    u'\u0a1d' : ['J\\_h A', 'ਝ'],
    u'\u0a1e' : ['J A', 'ਞ'],
    u'\u0a1f' : ['t` A', 'ਟ'],
    u'\u0a20' : ['t`_h A', 'ਠ'],
    u'\u0a21' : ['d` A', 'ਡ'],
    u'\u0a22' : ['d`_h A', 'ਢ'],
    u'\u0a23' : ['n" A', 'ਣ'],
    u'\u0a24' : ['t_d A', 'ਤ'],
    u'\u0a25' : ['t_d_h A', 'ਥ'],
    u'\u0a26' : ['d_d', 'ਦ'],
    u'\u0a27' : ['d_d_h A', 'ਧ'],
    u'\u0a28' : ['n_d', 'ਨ'],
    u'\u0a29' : ['(R:0A29)', '਩'],
    u'\u0a2a' : ['p A', 'ਪ'],
    u'\u0a2b' : ['p_h A', 'ਫ'],
    u'\u0a2c' : ['b A', 'ਬ'],
    u'\u0a2d' : ['b_h A', 'ਭ'],
    u'\u0a2e' : ['m A', 'ਮ'],
    u'\u0a2f' : ['_j A', 'ਯ'],
    u'\u0a30' : ['` A', 'ਰ'],
    u'\u0a31' : ['(R:0A31)', '਱'],
    u'\u0a32' : ['_l A', 'ਲ'],
    u'\u0a33' : ['l` A', 'ਲ਼'],
    u'\u0a34' : ['(R:0A34)', '਴'],
    u'\u0a35' : ['_v A', 'ਵ'],
    u'\u0a36' : ['S A', 'ਸ਼'],
    u'\u0a37' : ['(R:0A37)', '਷'],
    u'\u0a38' : ['s A', 'ਸ'],
    u'\u0a39' : ['h\\ A', 'ਹ'],
    u'\u0a3c' : ['(NUKTA)', '਼'],
    u'\u0a3e' : ['A', 'ਾ'],
    u'\u0a3f' : ['I', 'ਿ'],
    u'\u0a40' : ['i', 'ੀ'],
    u'\u0a41' : ['U', 'ੁ'],
    u'\u0a42' : ['u', 'ੂ'],
    u'\u0a43' : ['(R:0A43)', '੃'],
    u'\u0a45' : ['(R:0A45)', '੅'],
    u'\u0a46' : ['(R:0A46)', '੆'],
    u'\u0a47' : ['e', 'ੇ'],
    u'\u0a48' : ['aI', 'ੈ'],
    u'\u0a49' : ['(R:0A49)', '੉'],
    u'\u0a4a' : ['(R:0A4A)', '੊'],
    u'\u0a4b' : ['o', 'ੋ'],
    u'\u0a4c' : ['aU', 'ੌ'],
    u'\u0a4d' : ['(P)', '੍'],
    u'\u0a59' : ['_x A', 'ਖ਼'],
    u'\u0a5a' : ['G A', 'ਗ਼'],
    u'\u0a5b' : ['z A', 'ਜ਼'],
    u'\u0a5c' : ['r` A', 'ੜ'],
    u'\u0a5e' : ['f A', 'ਫ਼'],
    u'\u0a5f' : ['(R:0A5F)', '੟'],
    u'\u0a64' : ['(R:0A64)', '੤'],
    u'\u0a66' : ['(ZERO)', '੦'],
    u'\u0a67' : ['(ONE)', '੧'],
    u'\u0a68' : ['(TWO)', '੨'],
    u'\u0a69' : ['(THREE)', '੩'],
    u'\u0a6a' : ['(FOUR)', '੪'],
    u'\u0a6b' : ['(FIVE)', '੫'],
    u'\u0a6c' : ['(SIX)', '੬'],
    u'\u0a6d' : ['(SEVEN)', '੭'],
    u'\u0a6e' : ['(EIGHT)', '੮'],
    u'\u0a6f' : ['(NINE)', '੯'],
    u'\u0a70' : ['(TIPPI)', 'ੰ'],
    u'\u0a71' : ['(ADDAK)', 'ੱ'],
    u'\u0a72' : ['(IRI)', 'ੲ'],
    u'\u0a73' : ['(URA)', 'ੳ'],
    u'\u0a81' : ['(CD)', 'ઁ'],
    u'\u0a82' : ['(M)', 'ં'],
    u'\u0a83' : ['(H)', 'ઃ'],
    u'\u0a85' : ['A', 'અ'],
    u'\u0a86' : ['A:', 'આ'],
    u'\u0a87' : ['i', 'ઇ'],
    u'\u0a88' : ['i:', 'ઈ'],
    u'\u0a89' : ['u', 'ઉ'],
    u'\u0a8a' : ['u:', 'ઊ'],
    u'\u0a8b' : ['r\\` u', 'ઋ'],
    u'\u0a8c' : ['l_=', 'ઌ'],
    u'\u0a8d' : ['e', 'ઍ'],
    u'\u0a8f' : ['e', 'એ'],
    u'\u0a90' : ['aI', 'ઐ'],
    u'\u0a91' : ['o', 'ઑ'],
    u'\u0a93' : ['o', 'ઓ'],
    u'\u0a94' : ['aU', 'ઔ'],
    u'\u0a95' : ['k A', 'ક'],
    u'\u0a96' : ['k_h A', 'ખ'],
    u'\u0a97' : ['g A', 'ગ'],
    u'\u0a98' : ['g_h A', 'ઘ'],
    u'\u0a99' : ['N A', 'ઙ'],
    u'\u0a9a' : ['_} A', 'ચ'],
    u'\u0a9b' : ['c_h A', 'છ'],
    u'\u0a9c' : ['J\\ A', 'જ'],
    u'\u0a9d' : ['J\\_h A', 'ઝ'],
    u'\u0a9e' : ['J A', 'ઞ'],
    u'\u0a9f' : ['t` A', 'ટ'],
    u'\u0aa0' : ['t`_h A', 'ઠ'],
    u'\u0aa1' : ['d` A', 'ડ'],
    u'\u0aa2' : ['d`_h A', 'ઢ'],
    u'\u0aa3' : ['n" A', 'ણ'],
    u'\u0aa4' : ['t_d A', 'ત'],
    u'\u0aa5' : ['t_d_h A', 'થ'],
    u'\u0aa6' : ['d_d A', 'દ'],
    u'\u0aa7' : ['d_d_h A', 'ધ'],
    u'\u0aa8' : ['n_d A', 'ન'],
    u'\u0aaa' : ['p A', 'પ'],
    u'\u0aab' : ['p_h A', 'ફ'],
    u'\u0aac' : ['b A', 'બ'],
    u'\u0aad' : ['b_h A', 'ભ'],
    u'\u0aae' : ['m A', 'મ'],
    u'\u0aaf' : ['_j A', 'ય'],
    u'\u0ab0' : ['r\\` A', 'ર'],
    u'\u0ab2' : ['_l A', 'લ'],
    u'\u0ab3' : ['l` A', 'ળ'],
    u'\u0ab5' : ['_v A', 'વ'],
    u'\u0ab6' : ['S A', 'શ'],
    u'\u0ab7' : ['S A', 'ષ'],
    u'\u0ab8' : ['s A', 'સ'],
    u'\u0ab9' : ['h\\ A', 'હ'],
    u'\u0abc' : ['(NUKTA)', '઼'],
    u'\u0abd' : ['(AVAGRAHA)', 'ઽ'],
    u'\u0abe' : ['A:', 'ા'],
    u'\u0abf' : ['i', 'િ'],
    u'\u0ac0' : ['i:', 'ી'],
    u'\u0ac1' : ['u', 'ુ'],
    u'\u0ac2' : ['u:', 'ૂ'],
    u'\u0ac3' : ['9r=', 'ૃ'],
    u'\u0ac4' : ['rr=', 'ૄ'],
    u'\u0ac5' : ['e', 'ૅ'],
    u'\u0ac7' : ['e', 'ે'],
    u'\u0ac8' : ['aI', 'ૈ'],
    u'\u0ac9' : ['o', 'ૉ'],
    u'\u0acb' : ['o', 'ો'],
    u'\u0acc' : ['aU', 'ૌ'],
    u'\u0acd' : ['(P)', '્'],
    u'\u0ad0' : ['(OM)', 'ૐ'],
    u'\u0ae0' : ['rr=', 'ૠ'],
    u'\u0ae1' : ['lr=', 'ૡ'],
    u'\u0ae2' : ['l_=', 'ૢ'],
    u'\u0ae3' : ['lr=', 'ૣ'],
    u'\u0ae6' : ['(ZERO)', '૦'],
    u'\u0ae7' : ['(ONE)', '૧'],
    u'\u0ae8' : ['(TWO)', '૨'],
    u'\u0ae9' : ['(THREE)', '૩'],
    u'\u0aea' : ['(FOUR)', '૪'],
    u'\u0aeb' : ['(FIVE)', '૫'],
    u'\u0aec' : ['(SIX)', '૬'],
    u'\u0aed' : ['(SEVEN)', '૭'],
    u'\u0aee' : ['(EIGHT)', '૮'],
    u'\u0aef' : ['(NINE)', '૯'],
    u'\u0b01' : ['(CD)', 'ଁ'],
    u'\u0b02' : ['(M)', 'ଂ'],
    u'\u0b03' : ['(H)', 'ଃ'],
    u'\u0b05' : ['_>', 'ଅ'],
    u'\u0b06' : ['A', 'ଆ'],
    u'\u0b07' : ['i', 'ଇ'],
    u'\u0b08' : ['i:', 'ଈ'],
    u'\u0b09' : ['u', 'ଉ'],
    u'\u0b0a' : ['u:', 'ଊ'],
    u'\u0b0b' : ['9r=', 'ଋ'],
    u'\u0b0c' : ['l_=', 'ଌ'],
    u'\u0b0e' : ['(R:0B0E)', '଎'],
    u'\u0b0f' : ['e', 'ଏ'],
    u'\u0b10' : ['OI', 'ଐ'],
    u'\u0b13' : ['o', 'ଓ'],
    u'\u0b14' : ['aU', 'ଔ'],
    u'\u0b15' : ['k A', 'କ'],
    u'\u0b16' : ['k_h A', 'ଖ'],
    u'\u0b17' : ['g A', 'ଗ'],
    u'\u0b18' : ['g_h A', 'ଘ'],
    u'\u0b19' : ['N A', 'ଙ'],
    u'\u0b1a' : ['_} A', 'ଚ'],
    u'\u0b1b' : ['c_h A', 'ଛ'],
    u'\u0b1c' : ['J\\ A', 'ଜ'],
    u'\u0b1d' : ['J\\_h A', 'ଝ'],
    u'\u0b1e' : ['J A', 'ଞ'],
    u'\u0b1f' : ['t` A', 'ଟ'],
    u'\u0b20' : ['t`_h A', 'ଠ'],
    u'\u0b21' : ['d` A', 'ଡ'],
    u'\u0b22' : ['d`_h A', 'ଢ'],
    u'\u0b23' : ['n" A', 'ଣ'],
    u'\u0b24' : ['t_d A', 'ତ'],
    u'\u0b25' : ['t_d_h A', 'ଥ'],
    u'\u0b26' : ['d_d A', 'ଦ'],
    u'\u0b27' : ['d_d_h A', 'ଧ'],
    u'\u0b28' : ['n_d A', 'ନ'],
    u'\u0b29' : ['(R:0B29)', '଩'],
    u'\u0b2a' : ['p A', 'ପ'],
    u'\u0b2b' : ['p_h A', 'ଫ'],
    u'\u0b2c' : ['b A', 'ବ'],
    u'\u0b2d' : ['b_h A', 'ଭ'],
    u'\u0b2e' : ['m A', 'ମ'],
    u'\u0b2f' : ['_j A', 'ଯ'],
    u'\u0b30' : ['r\\` A', 'ର'],
    u'\u0b31' : ['(R:0B31)', '଱'],
    u'\u0b32' : ['_l A', 'ଲ'],
    u'\u0b33' : ['l` A', 'ଳ'],
    u'\u0b34' : ['(R:0B34)', '଴'],
    u'\u0b35' : ['_v A', 'ଵ'],
    u'\u0b36' : ['S A', 'ଶ'],
    u'\u0b37' : ['s` A', 'ଷ'],
    u'\u0b38' : ['s A', 'ସ'],
    u'\u0b39' : ['h\\ A', 'ହ'],
    u'\u0b3c' : ['(NUKTA)', '଼'],
    u'\u0b3d' : ['(AVAGRAHA)', 'ଽ'],
    u'\u0b3e' : ['A', 'ା'],
    u'\u0b3f' : ['i', 'ି'],
    u'\u0b40' : ['i:', 'ୀ'],
    u'\u0b41' : ['u', 'ୁ'],
    u'\u0b42' : ['u:', 'ୂ'],
    u'\u0b43' : ['9r=', 'ୃ'],
    u'\u0b45' : ['(R:0B45)', '୅'],
    u'\u0b46' : ['(R:0B46)', '୆'],
    u'\u0b47' : ['e', 'େ'],
    u'\u0b48' : ['OI', 'ୈ'],
    u'\u0b49' : ['(R:0B49)', '୉'],
    u'\u0b4b' : ['o', 'ୋ'],
    u'\u0b4c' : ['aU', 'ୌ'],
    u'\u0b4d' : ['(P)', '୍'],
    u'\u0b56' : ['(>I LENGTH MARK)', 'ୖ'],
    u'\u0b57' : ['(aU LENGTH MARK)', 'ୗ'],
    u'\u0b5c' : ['9r: A', 'ଡ଼'],
    u'\u0b5d' : ['9rh A', 'ଢ଼'],
    u'\u0b5f' : ['_j A', 'ୟ'],
    u'\u0b60' : ['rr=', 'ୠ'],
    u'\u0b61' : ['lr=', 'ୡ'],
    u'\u0b64' : ['(*)', '୤'],
    u'\u0b65' : ['(**)', '୥'],
    u'\u0b66' : ['(ZERO)', '୦'],
    u'\u0b67' : ['(ONE)', '୧'],
    u'\u0b68' : ['(TWO)', '୨'],
    u'\u0b69' : ['(THREE)', '୩'],
    u'\u0b6a' : ['(FOUR)', '୪'],
    u'\u0b6b' : ['(FIVE)', '୫'],
    u'\u0b6c' : ['(SIX)', '୬'],
    u'\u0b6d' : ['(SEVEN)', '୭'],
    u'\u0b6e' : ['(EIGHT)', '୮'],
    u'\u0b6f' : ['(NINE)', '୯'],
    u'\u0b70' : ['(ISSHAR)', '୰'],
    u'\u0b71' : ['_w A', 'ୱ'],
    u'\u0b81' : ['(R:0B81)', '஁'],
    u'\u0b82' : ['(M)', 'ஂ'],
    u'\u0b83' : ['(H)', 'ஃ'],
    u'\u0b85' : ['_>', 'அ'],
    u'\u0b86' : ['A:', 'ஆ'],
    u'\u0b87' : ['i', 'இ'],
    u'\u0b88' : ['i:', 'ஈ'],
    u'\u0b89' : ['u', 'உ'],
    u'\u0b8a' : ['u:', 'ஊ'],
    u'\u0b8b' : ['(R:0B8B)', '஋'],
    u'\u0b8d' : ['(R:0B8D)', '஍'],
    u'\u0b8e' : ['e', 'எ'],
    u'\u0b8f' : ['e:', 'ஏ'],
    u'\u0b90' : ['OI', 'ஐ'],
    u'\u0b91' : ['(R:0B91)', '஑'],
    u'\u0b92' : ['o', 'ஒ'],
    u'\u0b93' : ['o:', 'ஓ'],
    u'\u0b94' : ['aU', 'ஔ'],
    u'\u0b95' : ['k A, g A, _x A, G A, _h A', 'க'],
    u'\u0b96' : ['(R:0B96)', '஖'],
    u'\u0b97' : ['(R:0B97)', '஗'],
    u'\u0b98' : ['(R:0B98)', '஘'],
    u'\u0b99' : ['N A', 'ங'],
    u'\u0b9a' : ['_} A, J\\ A, S A, s A', 'ச'],
    u'\u0b9b' : ['(R:0B9B)', '஛'],
    u'\u0b9c' : ['J\\ A', 'ஜ'],
    u'\u0b9d' : ['(R:0B9D)', '஝'],
    u'\u0b9e' : ['J A', 'ஞ'],
    u'\u0b9f' : ['t` A, d` A, r` A', 'ட'],
    u'\u0ba0' : ['(R:0BA0)', '஠'],
    u'\u0ba1' : ['(R:0BA1)', '஡'],
    u'\u0ba2' : ['(R:0BA2)', '஢'],
    u'\u0ba3' : ['n" A', 'ண'],
    u'\u0ba4' : ['t_d A, d_d A, D A', 'த'],
    u'\u0ba5' : ['(R:0BA5)', '஥'],
    u'\u0ba6' : ['(R:0BA6)', '஦'],
    u'\u0ba7' : ['(R:0BA7)', '஧'],
    u'\u0ba8' : ['n_d A', 'ந'],
    u'\u0ba9' : ['_n A', 'ன'],
    u'\u0baa' : ['p A, b A, B A', 'ப'],
    u'\u0bab' : ['(R:0BAB)', '஫'],
    u'\u0bac' : ['(R:0BAC)', '஬'],
    u'\u0bae' : ['m A', 'ம'],
    u'\u0baf' : ['_j A', 'ய'],
    u'\u0bb0' : ['9r[ A', 'ர'],
    u'\u0bb1' : ['r\\` A, t A, d A', 'ற'],
    u'\u0bb2' : ['_l A', 'ல'],
    u'\u0bb3' : ['l` A', 'ள'],
    u'\u0bb4' : ['_l A', 'ழ'],
    u'\u0bb5' : ['_v A', 'வ'],
    u'\u0bb6' : ['S A', 'ஶ'],
    u'\u0bb7' : ['s` A', 'ஷ'],
    u'\u0bb8' : ['s A', 'ஸ'],
    u'\u0bb9' : ['_h A', 'ஹ'],
    u'\u0bbe' : ['A:', 'ா'],
    u'\u0bbf' : ['i', 'ி'],
    u'\u0bc0' : ['i:', 'ீ'],
    u'\u0bc1' : ['u', 'ு'],
    u'\u0bc2' : ['u:', 'ூ'],
    u'\u0bc3' : ['(R:0BC3)', '௃'],
    u'\u0bc5' : ['(R:0BC5)', '௅'],
    u'\u0bc6' : ['e', 'ெ'],
    u'\u0bc7' : ['e:', 'ே'],
    u'\u0bc8' : ['OI', 'ை'],
    u'\u0bc9' : ['(R:0BC9)', '௉'],
    u'\u0bca' : ['o', 'ொ'],
    u'\u0bcb' : ['o:', 'ோ'],
    u'\u0bcc' : ['aU', 'ௌ'],
    u'\u0bcd' : ['(P)', '்'],
    u'\u0bd7' : ['(>I LENGTH MARK)', 'ௗ'],
    u'\u0bdf' : ['(R:0BDF)', '௟'],
    u'\u0be4' : ['(R:0BE4)', '௤'],
    u'\u0be6' : ['(ZERO)', '௦'],
    u'\u0be7' : ['(ONE)', '௧'],
    u'\u0be8' : ['(TWO)', '௨'],
    u'\u0be9' : ['(THREE)', '௩'],
    u'\u0bea' : ['(FOUR)', '௪'],
    u'\u0beb' : ['(FIVE)', '௫'],
    u'\u0bec' : ['(SIX)', '௬'],
    u'\u0bed' : ['(SEVEN)', '௭'],
    u'\u0bee' : ['(EIGHT)', '௮'],
    u'\u0bef' : ['(NINE)', '௯'],
    u'\u0bf0' : ['(TEN)', '௰'],
    u'\u0bf1' : ['(HUNDREAD)', '௱'],
    u'\u0bf2' : ['(THOUSAND)', '௲'],
    u'\u0bf3' : ['(DAY)', '௳'],
    u'\u0bf4' : ['(MONTH)', '௴'],
    u'\u0bf5' : ['(YEAR)', '௵'],
    u'\u0bf6' : ['(DEBIT)', '௶'],
    u'\u0bf7' : ['(CREDIT)', '௷'],
    u'\u0bf8' : ['(AS ABOVE)', '௸'],
    u'\u0bf9' : ['(RUPEE)', '௹'],
    u'\u0bfa' : ['(NUMBER)', '௺'],
    u'\u0c01' : ['(CD)', 'ఁ'],
    u'\u0c02' : ['(M)', 'ం'],
    u'\u0c03' : ['(H)', 'ః'],
    u'\u0c05' : ['_>', 'అ'],
    u'\u0c06' : ['>:', 'ఆ'],
    u'\u0c07' : ['i', 'ఇ'],
    u'\u0c08' : ['i:', 'ఈ'],
    u'\u0c09' : ['u', 'ఉ'],
    u'\u0c0a' : ['u:', 'ఊ'],
    u'\u0c0b' : ['r\\` i, r\\` u', 'ఋ'],
    u'\u0c0c' : ['l_=', 'ఌ'],
    u'\u0c0d' : ['(R:0C0D)', '఍'],
    u'\u0c0e' : ['e', 'ఎ'],
    u'\u0c0f' : ['e:', 'ఏ'],
    u'\u0c10' : ['aI', 'ఐ'],
    u'\u0c12' : ['o', 'ఒ'],
    u'\u0c13' : ['o:', 'ఓ'],
    u'\u0c14' : ['aU', 'ఔ'],
    u'\u0c15' : ['k A', 'క'],
    u'\u0c16' : ['k_h A', 'ఖ'],
    u'\u0c17' : ['g A', 'గ'],
    u'\u0c18' : ['g_h A', 'ఘ'],
    u'\u0c19' : ['N A', 'ఙ'],
    u'\u0c1a' : ['_} A', 'చ'],
    u'\u0c1b' : ['c_h A', 'ఛ'],
    u'\u0c1c' : ['J\\ A', 'జ'],
    u'\u0c1d' : ['J\\_h A', 'ఝ'],
    u'\u0c1e' : ['J A', 'ఞ'],
    u'\u0c1f' : ['t` A', 'ట'],
    u'\u0c20' : ['t`_h A', 'ఠ'],
    u'\u0c21' : ['d` A', 'డ'],
    u'\u0c22' : ['d`_h A', 'ఢ'],
    u'\u0c23' : ['n" A', 'ణ'],
    u'\u0c24' : ['t_d A', 'త'],
    u'\u0c25' : ['t_d_h A', 'థ'],
    u'\u0c26' : ['d_d A', 'ద'],
    u'\u0c27' : ['d_d_h A', 'ధ'],
    u'\u0c28' : ['n_d A', 'న'],
    u'\u0c29' : ['(R:0C29)', '఩'],
    u'\u0c2a' : ['p A', 'ప'],
    u'\u0c2b' : ['p_h A', 'ఫ'],
    u'\u0c2c' : ['b A', 'బ'],
    u'\u0c2d' : ['b_h A', 'భ'],
    u'\u0c2e' : ['m A', 'మ'],
    u'\u0c2f' : ['_j A', 'య'],
    u'\u0c30' : ['r\\` A', 'ర'],
    u'\u0c31' : ['9r: A', 'ఱ'],
    u'\u0c32' : ['_l A', 'ల'],
    u'\u0c33' : ['l` A', 'ళ'],
    u'\u0c34' : ['(R:0C34)', 'ఴ'],
    u'\u0c35' : ['_v A', 'వ'],
    u'\u0c36' : ['S A', 'శ'],
    u'\u0c37' : ['S A', 'ష'],
    u'\u0c38' : ['s A', 'స'],
    u'\u0c39' : ['h\\ A', 'హ'],
    u'\u0c3e' : ['>:', 'ా'],
    u'\u0c3f' : ['i', 'ి'],
    u'\u0c40' : ['i:', 'ీ'],
    u'\u0c41' : ['u', 'ు'],
    u'\u0c42' : ['u:', 'ూ'],
    u'\u0c43' : ['9r=', 'ృ'],
    u'\u0c44' : ['rr=', 'ౄ'],
    u'\u0c45' : ['(R:0C45)', '౅'],
    u'\u0c46' : ['e', 'ె'],
    u'\u0c47' : ['e:', 'ే'],
    u'\u0c48' : ['aI', 'ై'],
    u'\u0c49' : ['(R:0C49)', '౉'],
    u'\u0c4a' : ['o', 'ొ'],
    u'\u0c4b' : ['o:', 'ో'],
    u'\u0c4c' : ['aU', 'ౌ'],
    u'\u0c4d' : ['(P)', '్'],
    u'\u0c55' : ['(LENGTH MARK)', 'ౕ'],
    u'\u0c56' : ['(aI LENGTH MARK)', 'ౖ'],
    u'\u0c5f' : ['(R:0C5F)', '౟'],
    u'\u0c60' : ['rr=', 'ౠ'],
    u'\u0c61' : ['lr=', 'ౡ'],
    u'\u0c66' : ['(ZERO)', '౦'],
    u'\u0c67' : ['(ONE)', '౧'],
    u'\u0c68' : ['(TWO)', '౨'],
    u'\u0c69' : ['(THREE)', '౩'],
    u'\u0c6a' : ['(FOUR)', '౪'],
    u'\u0c6b' : ['(FIVE)', '౫'],
    u'\u0c6c' : ['(SIX)', '౬'],
    u'\u0c6d' : ['(SEVEN)', '౭'],
    u'\u0c6e' : ['(EIGHT)', '౮'],
    u'\u0c6f' : ['(NINE)', '౯'],
    u'\u0c81' : ['(R:0C81)', 'ಁ'],
    u'\u0c82' : ['(M)', 'ಂ'],
    u'\u0c83' : ['(H)', 'ಃ'],
    u'\u0c85' : ['_>', 'ಅ'],
    u'\u0c86' : ['>:', 'ಆ'],
    u'\u0c87' : ['i', 'ಇ'],
    u'\u0c88' : ['i:', 'ಈ'],
    u'\u0c89' : ['u', 'ಉ'],
    u'\u0c8a' : ['u:', 'ಊ'],
    u'\u0c8b' : ['r\\` i, r\\` u', 'ಋ'],
    u'\u0c8c' : ['l_=', 'ಌ'],
    u'\u0c8d' : ['(R:0C8D)', '಍'],
    u'\u0c8e' : ['e', 'ಎ'],
    u'\u0c8f' : ['e:', 'ಏ'],
    u'\u0c90' : ['aI', 'ಐ'],
    u'\u0c91' : ['(R:0C91)', '಑'],
    u'\u0c92' : ['o', 'ಒ'],
    u'\u0c93' : ['o:', 'ಓ'],
    u'\u0c94' : ['aU', 'ಔ'],
    u'\u0c95' : ['k A', 'ಕ'],
    u'\u0c96' : ['k_h A', 'ಖ'],
    u'\u0c97' : ['g A', 'ಗ'],
    u'\u0c98' : ['g_h A', 'ಘ'],
    u'\u0c99' : ['N A', 'ಙ'],
    u'\u0c9a' : ['_} A', 'ಚ'],
    u'\u0c9b' : ['c_h A', 'ಛ'],
    u'\u0c9c' : ['J\\ A', 'ಜ'],
    u'\u0c9d' : ['J\\_h A', 'ಝ'],
    u'\u0c9e' : ['J A', 'ಞ'],
    u'\u0c9f' : ['t` A', 'ಟ'],
    u'\u0ca0' : ['t`_h A', 'ಠ'],
    u'\u0ca1' : ['d` A', 'ಡ'],
    u'\u0ca2' : ['d`_h A', 'ಢ'],
    u'\u0ca3' : ['n" A', 'ಣ'],
    u'\u0ca4' : ['t_d A', 'ತ'],
    u'\u0ca5' : ['t_d_h A', 'ಥ'],
    u'\u0ca6' : ['d_d A', 'ದ'],
    u'\u0ca7' : ['d_d_h A', 'ಧ'],
    u'\u0ca8' : ['n_d A', 'ನ'],
    u'\u0ca9' : ['(R:0CA9)', '಩'],
    u'\u0caa' : ['p A', 'ಪ'],
    u'\u0cab' : ['p_h A', 'ಫ'],
    u'\u0cac' : ['b A', 'ಬ'],
    u'\u0cad' : ['b_h A', 'ಭ'],
    u'\u0cae' : ['m A', 'ಮ'],
    u'\u0caf' : ['_j A', 'ಯ'],
    u'\u0cb0' : ['r\\` A', 'ರ'],
    u'\u0cb1' : ['r` A', 'ಱ'],
    u'\u0cb2' : ['_l A', 'ಲ'],
    u'\u0cb3' : ['l` A', 'ಳ'],
    u'\u0cb4' : ['(R:0CB4)', '಴'],
    u'\u0cb5' : ['_v A', 'ವ'],
    u'\u0cb6' : ['S A', 'ಶ'],
    u'\u0cb7' : ['S A', 'ಷ'],
    u'\u0cb8' : ['s A', 'ಸ'],
    u'\u0cb9' : ['h\\ A', 'ಹ'],
    u'\u0cbc' : ['(NUKTA)', '಼'],
    u'\u0cbd' : ['(AVAGRAHA)', 'ಽ'],
    u'\u0cbe' : ['>:', 'ಾ'],
    u'\u0cbf' : ['i', 'ಿ'],
    u'\u0cc0' : ['i:', 'ೀ'],
    u'\u0cc1' : ['u', 'ು'],
    u'\u0cc2' : ['u:', 'ೂ'],
    u'\u0cc3' : ['9r=', 'ೃ'],
    u'\u0cc4' : ['rr=', 'ೄ'],
    u'\u0cc5' : ['(R:0CC5)', '೅'],
    u'\u0cc6' : ['e', 'ೆ'],
    u'\u0cc7' : ['e:', 'ೇ'],
    u'\u0cc8' : ['aI', 'ೈ'],
    u'\u0cc9' : ['(R:0CC9)', '೉'],
    u'\u0cca' : ['o', 'ೊ'],
    u'\u0ccb' : ['o:', 'ೋ'],
    u'\u0ccc' : ['aU', 'ೌ'],
    u'\u0ccd' : ['(P)', '್'],
    u'\u0cd5' : ['(LENGTH MARK)', 'ೕ'],
    u'\u0cd6' : ['(aI LENGTH MARK)', 'ೖ'],
    u'\u0cde' : ['f A', 'ೞ'],
    u'\u0cdf' : ['(R:0CDF)', '೟'],
    u'\u0ce0' : ['rr=', 'ೠ'],
    u'\u0ce1' : ['l_=', 'ೡ'],
    u'\u0ce4' : ['(R:0CE4)', '೤'],
    u'\u0ce6' : ['(ZERO)', '೦'],
    u'\u0ce7' : ['(ONE)', '೧'],
    u'\u0ce8' : ['(TWO)', '೨'],
    u'\u0ce9' : ['(THREE)', '೩'],
    u'\u0cea' : ['(FOUR)', '೪'],
    u'\u0ceb' : ['(FIVE)', '೫'],
    u'\u0cec' : ['(SIX)', '೬'],
    u'\u0ced' : ['(SEVEN)', '೭'],
    u'\u0cee' : ['(EIGHT)', '೮'],
    u'\u0cef' : ['(NINE)', '೯'],
    u'\u0d01' : ['(R:0D01)', 'ഁ'],
    u'\u0d02' : ['(M)', 'ം'],
    u'\u0d03' : ['(H)', 'ഃ'],
    u'\u0d05' : ['A', 'അ'],
    u'\u0d06' : ['A:', 'ആ'],
    u'\u0d07' : ['i', 'ഇ'],
    u'\u0d08' : ['i:', 'ഈ'],
    u'\u0d09' : ['u', 'ഉ'],
    u'\u0d0a' : ['u:', 'ഊ'],
    u'\u0d0b' : ['r\\` 1', 'ഋ'],
    u'\u0d0c' : ['l_=', 'ഌ'],
    u'\u0d0d' : ['(R:0D0D)', '഍'],
    u'\u0d0e' : ['e', 'എ'],
    u'\u0d0f' : ['e:', 'ഏ'],
    u'\u0d10' : ['aI', 'ഐ'],
    u'\u0d11' : ['(R:0D11)', '഑'],
    u'\u0d12' : ['o', 'ഒ'],
    u'\u0d13' : ['o:', 'ഓ'],
    u'\u0d14' : ['aU', 'ഔ'],
    u'\u0d15' : ['k A', 'ക'],
    u'\u0d16' : ['k_h A', 'ഖ'],
    u'\u0d17' : ['g A', 'ഗ'],
    u'\u0d18' : ['g_h A', 'ഘ'],
    u'\u0d19' : ['N A', 'ങ'],
    u'\u0d1a' : ['_} A', 'ച'],
    u'\u0d1b' : ['c_h A', 'ഛ'],
    u'\u0d1c' : ['J\\ A', 'ജ'],
    u'\u0d1d' : ['J\\_h A', 'ഝ'],
    u'\u0d1e' : ['J A', 'ഞ'],
    u'\u0d1f' : ['t` A', 'ട'],
    u'\u0d20' : ['t`_h A', 'ഠ'],
    u'\u0d21' : ['d` A', 'ഡ'],
    u'\u0d22' : ['d`_h A', 'ഢ'],
    u'\u0d23' : ['n" A', 'ണ'],
    u'\u0d24' : ['t_d A', 'ത'],
    u'\u0d25' : ['t_d_h A', 'ഥ'],
    u'\u0d26' : ['d_d A', 'ദ'],
    u'\u0d27' : ['d_d_h A', 'ധ'],
    u'\u0d28' : ['n_d A, _n A', 'ന'],
    u'\u0d29' : ['(R:0D29)', 'ഩ'],
    u'\u0d2a' : ['p A', 'പ'],
    u'\u0d2b' : ['p_h A', 'ഫ'],
    u'\u0d2c' : ['b A', 'ബ'],
    u'\u0d2d' : ['b_h A', 'ഭ'],
    u'\u0d2e' : ['m A', 'മ'],
    u'\u0d2f' : ['_j A', 'യ'],
    u'\u0d30' : ['9r[ A', 'ര'],
    u'\u0d31' : ['r\\` A', 'റ'],
    u'\u0d32' : ['_l A', 'ല'],
    u'\u0d33' : ['l` A', 'ള'],
    u'\u0d34' : ['_l A', 'ഴ'],
    u'\u0d35' : ['_v A', 'വ'],
    u'\u0d36' : ['s\\ A', 'ശ'],
    u'\u0d37' : ['S A', 'ഷ'],
    u'\u0d38' : ['s A', 'സ'],
    u'\u0d39' : ['h\\ A', 'ഹ'],
    u'\u0d3e' : ['A:', 'ാ'],
    u'\u0d3f' : ['i', 'ി'],
    u'\u0d40' : ['i:', 'ീ'],
    u'\u0d41' : ['u', 'ു'],
    u'\u0d42' : ['u:', 'ൂ'],
    u'\u0d43' : ['9r=', 'ൃ'],
    u'\u0d45' : ['(R:0D45)', '൅'],
    u'\u0d46' : ['e', 'െ'],
    u'\u0d47' : ['e:', 'േ'],
    u'\u0d48' : ['aI', 'ൈ'],
    u'\u0d49' : ['(R:0D49)', '൉'],
    u'\u0d4a' : ['o', 'ൊ'],
    u'\u0d4b' : ['o:', 'ോ'],
    u'\u0d4c' : ['aU', 'ൌ'],
    u'\u0d4d' : ['(P)', '്'],
    u'\u0d57' : ['(aU LENGTH MARK)', 'ൗ'],
    u'\u0d5f' : ['(R:0D5F)', 'ൟ'],
    u'\u0d60' : ['rr=', 'ൠ'],
    u'\u0d61' : ['lr=', 'ൡ'],
    u'\u0d64' : ['(R:0D64)', '൤'],
    u'\u0d66' : ['(ZERO)', '൦'],
    u'\u0d67' : ['(ONE)', '൧'],
    u'\u0d68' : ['(TWO)', '൨'],
    u'\u0d69' : ['(THREE)', '൩'],
    u'\u0d6a' : ['(FOUR)', '൪'],
    u'\u0d6b' : ['(FIVE)', '൫'],
    u'\u0d6c' : ['(SIX)', '൬'],
    u'\u0d6d' : ['(SEVEN)', '൭'],
    u'\u0d6e' : ['(EIGHT)', '൮'],
    u'\u0d6f' : ['(NINE)', '൯'],
    u'\u0d82' : ['(M)', 'ං'],
    u'\u0d83' : ['(H)', 'ඃ'],
    u'\u0d85' : ['A, @', 'අ'],
    u'\u0d86' : ['A:, A', 'ආ'],
    u'\u0d87' : ['{', 'ඇ'],
    u'\u0d88' : ['@:', 'ඈ'],
    u'\u0d89' : ['i', 'ඉ'],
    u'\u0d8a' : ['i:', 'ඊ'],
    u'\u0d8b' : ['u', 'උ'],
    u'\u0d8c' : ['u:', 'ඌ'],
    u'\u0d8d' : ['r\\` i, r\\` u', 'ඍ'],
    u'\u0d8e' : ['r\\` i:, r\\` u:', 'ඎ'],
    u'\u0d8f' : ['l_=', 'ඏ'],
    u'\u0d90' : ['l_=', 'ඐ'],
    u'\u0d91' : ['e', 'එ'],
    u'\u0d92' : ['e:', 'ඒ'],
    u'\u0d93' : ['aI', 'ඓ'],
    u'\u0d94' : ['o', 'ඔ'],
    u'\u0d95' : ['o:', 'ඕ'],
    u'\u0d96' : ['aU', 'ඖ'],
    u'\u0d9a' : ['k A', 'ක'],
    u'\u0d9b' : ['k_h A', 'ඛ'],
    u'\u0d9c' : ['g A', 'ග'],
    u'\u0d9d' : ['g_h A', 'ඝ'],
    u'\u0d9e' : ['N A', 'ඞ'],
    u'\u0d9f' : ['N A', 'ඟ'],
    u'\u0da0' : ['_} A', 'ච'],
    u'\u0da1' : ['c_h A', 'ඡ'],
    u'\u0da2' : ['J\\ A', 'ජ'],
    u'\u0da3' : ['J\\_h A', 'ඣ'],
    u'\u0da4' : ['J A', 'ඤ'],
    u'\u0da5' : ['J\\ J A', 'ඥ'],
    u'\u0da6' : ['J J\\ A', 'ඦ'],
    u'\u0da7' : ['t` A', 'ට'],
    u'\u0da8' : ['t`_h A', 'ඨ'],
    u'\u0da9' : ['d` A', 'ඩ'],
    u'\u0daa' : ['d`_h A', 'ඪ'],
    u'\u0dab' : ['n" A', 'ණ'],
    u'\u0dac' : ['n" d` A', 'ඬ'],
    u'\u0dad' : ['t_d A', 'ත'],
    u'\u0dae' : ['t_d_h A', 'ථ'],
    u'\u0daf' : ['d_d A', 'ද'],
    u'\u0db0' : ['d_d_h A', 'ධ'],
    u'\u0db1' : ['n_d A', 'න'],
    u'\u0db3' : ['_n d A', 'ඳ'],
    u'\u0db4' : ['p A', 'ප'],
    u'\u0db5' : ['p_h A', 'ඵ'],
    u'\u0db6' : ['b A', 'බ'],
    u'\u0db7' : ['b_h A', 'භ'],
    u'\u0db8' : ['m A', 'ම'],
    u'\u0db9' : ['m b A', 'ඹ'],
    u'\u0dba' : ['_j A', 'ය'],
    u'\u0dbb' : ['r\\` A', 'ර'],
    u'\u0dbd' : ['_l A', 'ල'],
    u'\u0dc0' : ['_v A', 'ව'],
    u'\u0dc1' : ['S A', 'ශ'],
    u'\u0dc2' : ['S A', 'ෂ'],
    u'\u0dc3' : ['s A', 'ස'],
    u'\u0dc4' : ['h\\ A', 'හ'],
    u'\u0dc5' : ['l` A', 'ළ'],
    u'\u0dc6' : ['f A', 'ෆ'],
    u'\u0dca' : ['(P)', '්'],
    u'\u0dcf' : ['A:', 'ා'],
    u'\u0dd0' : ['{', 'ැ'],
    u'\u0dd1' : ['@:', 'ෑ'],
    u'\u0dd2' : ['i', 'ි'],
    u'\u0dd3' : ['i:', 'ී'],
    u'\u0dd4' : ['u', 'ු'],
    u'\u0dd6' : ['u:', 'ූ'],
    u'\u0dd8' : ['9r=', 'ෘ'],
    u'\u0dd9' : ['e', 'ෙ'],
    u'\u0dda' : ['e:', 'ේ'],
    u'\u0ddb' : ['aI', 'ෛ'],
    u'\u0ddc' : ['o', 'ො'],
    u'\u0ddd' : ['o:', 'ෝ'],
    u'\u0dde' : ['aU', 'ෞ'],
    u'\u0ddf' : ['l_=', 'ෟ'],
    u'\u0df2' : ['rr=', 'ෲ'],
    u'\u0df3' : ['l_=', 'ෳ'],
    u'\u0df4' : ['(PUNCTUATION)', '෴'],
    u'\u0e01' : ['k', 'ก'],
    u'\u0e02' : ['kh, k', 'ข'],
    u'\u0e03' : ['k_h', 'ฃ'],
    u'\u0e04' : ['kh, k', 'ค'],
    u'\u0e05' : ['k_h', 'ฅ'],
    u'\u0e06' : ['kh, k', 'ฆ'],
    u'\u0e07' : ['N', 'ง'],
    u'\u0e08' : ['c, t', 'จ'],
    u'\u0e09' : ['c_h', 'ฉ'],
    u'\u0e0a' : ['ch, t', 'ช'],
    u'\u0e0b' : ['s, t', 'ซ'],
    u'\u0e0c' : ['ch, t', 'ฌ'],
    u'\u0e0d' : ['j, _n', 'ญ'],
    u'\u0e0e' : ['d, t', 'ฎ'],
    u'\u0e0f' : ['t', 'ฏ'],
    u'\u0e10' : ['th, t', 'ฐ'],
    u'\u0e11' : ['th, t', 'ฑ'],
    u'\u0e12' : ['th, t', 'ฒ'],
    u'\u0e13' : ['_n', 'ณ'],
    u'\u0e14' : ['d, t', 'ด'],
    u'\u0e15' : ['t', 'ต'],
    u'\u0e16' : ['th, t', 'ถ'],
    u'\u0e17' : ['th, t', 'ท'],
    u'\u0e18' : ['th, t', 'ธ'],
    u'\u0e19' : ['_n', 'น'],
    u'\u0e1a' : ['b, p', 'บ'],
    u'\u0e1b' : ['p', 'ป'],
    u'\u0e1c' : ['ph, p', 'ผ'],
    u'\u0e1d' : ['f', 'ฝ'],
    u'\u0e1e' : ['ph, p', 'พ'],
    u'\u0e1f' : ['f, p', 'ฟ'],
    u'\u0e20' : ['ph, p', 'ภ'],
    u'\u0e21' : ['m', 'ม'],
    u'\u0e22' : ['_j', 'ย'],
    u'\u0e23' : ['r, _n', 'ร'],
    u'\u0e24' : ['` M', 'ฤ'],
    u'\u0e25' : ['l, _n', 'ล'],
    u'\u0e26' : ['_l M', 'ฦ'],
    u'\u0e27' : ['_w', 'ว'],
    u'\u0e28' : ['s, t', 'ศ'],
    u'\u0e29' : ['s, t', 'ษ'],
    u'\u0e2a' : ['s, t', 'ส'],
    u'\u0e2b' : ['_h', 'ห'],
    u'\u0e2c' : ['l, _n', 'ฬ'],
    u'\u0e2d' : ['_k', 'อ'],
    u'\u0e2e' : ['_h', 'ฮ'],
    u'\u0e2f' : ['(ELLIPSIS, ABBREVIATION)', 'ฯ'],
    u'\u0e30' : ['A', 'ะ'],
    u'\u0e31' : ['A', 'ั'],
    u'\u0e32' : ['A:', 'า'],
    u'\u0e33' : ['A m', 'ำ'],
    u'\u0e34' : ['i', 'ิ'],
    u'\u0e35' : ['i:', 'ี'],
    u'\u0e36' : ['M', 'ึ'],
    u'\u0e37' : ['4:', 'ื'],
    u'\u0e38' : ['u', 'ุ'],
    u'\u0e39' : ['u:', 'ู'],
    u'\u0e3a' : ['(P)', 'ฺ'],
    u'\u0e3f' : ['(CURRENCY SYMBOL BAHT)', '฿'],
    u'\u0e40' : ['e:', 'เ'],
    u'\u0e41' : ['{', 'แ'],
    u'\u0e42' : ['o', 'โ'],
    u'\u0e43' : ['aI', 'ใ'],
    u'\u0e44' : ['aI', 'ไ'],
    u'\u0e45' : ['(PHINTHU)', 'ๅ'],
    u'\u0e46' : ['(MAIYAMOK)', 'ๆ'],
    u'\u0e47' : ['(MAITAIKHU)', '็'],
    u'\u0e48' : ['(MAI EK)', '่'],
    u'\u0e49' : ['(MAI THO)', '้'],
    u'\u0e4a' : ['(MAI TRI)', '๊'],
    u'\u0e4b' : ['(MAI CHATTAWA)', '๋'],
    u'\u0e4c' : ['(THANTHAKHAT)', '์'],
    u'\u0e4d' : ['(NIKHHIT)', 'ํ'],
    u'\u0e4e' : ['(YAMAKKAN)', '๎'],
    u'\u0e4f' : ['(FONGMAN)', '๏'],
    u'\u0e50' : ['(ZERO)', '๐'],
    u'\u0e51' : ['(ONE)', '๑'],
    u'\u0e52' : ['(TWO)', '๒'],
    u'\u0e53' : ['(THREE)', '๓'],
    u'\u0e54' : ['(FOUR)', '๔'],
    u'\u0e55' : ['(FIVE)', '๕'],
    u'\u0e56' : ['(SIX)', '๖'],
    u'\u0e57' : ['(SEVEN)', '๗'],
    u'\u0e58' : ['(EIGHT)', '๘'],
    u'\u0e59' : ['(NINE)', '๙'],
    u'\u0e5a' : ['(ANGKHANKHU)', '๚'],
    u'\u0e5b' : ['(KHOMUT)', '๛'],
    u'\u0e81' : ['k, k', 'ກ'],
    u'\u0e82' : ['kh, k', 'ຂ'],
    u'\u0e84' : ['kh, k', 'ຄ'],
    u'\u0e87' : ['N, N', 'ງ'],
    u'\u0e88' : ['c, t', 'ຈ'],
    u'\u0e8a' : ['s, t', 'ຊ'],
    u'\u0e8d' : ['j, _n', 'ຍ'],
    u'\u0e94' : ['d, t', 'ດ'],
    u'\u0e95' : ['t, t', 'ຕ'],
    u'\u0e96' : ['th, t', 'ຖ'],
    u'\u0e97' : ['th, t', 'ທ'],
    u'\u0e99' : ['n, _n', 'ນ'],
    u'\u0e9a' : ['b, p', 'ບ'],
    u'\u0e9b' : ['p, p', 'ປ'],
    u'\u0e9c' : ['ph, p', 'ຜ'],
    u'\u0e9d' : ['f', 'ຝ'],
    u'\u0e9e' : ['ph, p', 'ພ'],
    u'\u0e9f' : ['f, p', 'ຟ'],
    u'\u0ea1' : ['m, m', 'ມ'],
    u'\u0ea2' : ['n~, _j', 'ຢ'],
    u'\u0ea3' : ['h, _n', 'ຣ'],
    u'\u0ea5' : ['l, _n', 'ລ'],
    u'\u0ea7' : ['w, _w', 'ວ'],
    u'\u0eaa' : ['s, t', 'ສ'],
    u'\u0eab' : ['_h', 'ຫ'],
    u'\u0ead' : ['(SILENT)', 'ອ'],
    u'\u0eae' : ['_h', 'ຮ'],
    u'\u0eaf' : ['(ELLIPSIS)', 'ຯ'],
    u'\u0eb0' : ['A', 'ະ'],
    u'\u0eb1' : ['A', 'ັ'],
    u'\u0eb2' : ['A:', 'າ'],
    u'\u0eb3' : ['A m', 'ຳ'],
    u'\u0eb4' : ['i', 'ິ'],
    u'\u0eb5' : ['i:', 'ີ'],
    u'\u0eb6' : ['M', 'ຶ'],
    u'\u0eb7' : ['4:', 'ື'],
    u'\u0eb8' : ['u', 'ຸ'],
    u'\u0eb9' : ['u:', 'ູ'],
    u'\u0ebb' : ['(MAI KON)', 'ົ'],
    u'\u0ebc' : ['(LO)', 'ຼ'],
    u'\u0ebd' : ['(NYO)', 'ຽ'],
    u'\u0ec0' : ['e:', 'ເ'],
    u'\u0ec1' : ['{', 'ແ'],
    u'\u0ec2' : ['o', 'ໂ'],
    u'\u0ec3' : ['aI', 'ໃ'],
    u'\u0ec4' : ['aI', 'ໄ'],
    u'\u0ec6' : ['(KO LA)', 'ໆ'],
    u'\u0ec8' : ['(MAI EK)', '່'],
    u'\u0ec9' : ['(MAI THO)', '້'],
    u'\u0eca' : ['(MAI TI)', '໊'],
    u'\u0ecb' : ['(MAI CATAWA)', '໋'],
    u'\u0ecc' : ['(CANCELLATION MARK)', '໌'],
    u'\u0ecd' : ['(NIGGAHITA)', 'ໍ'],
    u'\u0ed0' : ['(ZERO)', '໐'],
    u'\u0ed1' : ['(ONE)', '໑'],
    u'\u0ed2' : ['(TWO)', '໒'],
    u'\u0ed3' : ['(THREE)', '໓'],
    u'\u0ed4' : ['(FOUR)', '໔'],
    u'\u0ed5' : ['(FIVE)', '໕'],
    u'\u0ed6' : ['(SIX)', '໖'],
    u'\u0ed7' : ['(SEVEN)', '໗'],
    u'\u0ed8' : ['(EIGHT)', '໘'],
    u'\u0ed9' : ['(NINE)', '໙'],
    u'\u0edc' : ['_h _n', 'ໜ'],
    u'\u0edd' : ['_h m', 'ໝ'],
    u'\u0f00' : ['(OM)', 'ༀ'],
    u'\u0f01' : ['(##)', '༁'],
    u'\u0f02' : ['(##)', '༂'],
    u'\u0f03' : ['(##)', '༃'],
    u'\u0f04' : ['(##)', '༄'],
    u'\u0f05' : ['(##)', '༅'],
    u'\u0f06' : ['(##)', '༆'],
    u'\u0f07' : ['(##)', '༇'],
    u'\u0f08' : ['(##)', '༈'],
    u'\u0f09' : ['(##)', '༉'],
    u'\u0f0a' : ['(##)', '༊'],
    u'\u0f0b' : ['(##)', '་'],
    u'\u0f0c' : ['(##)', '༌'],
    u'\u0f0d' : ['(##)', '།'],
    u'\u0f0e' : ['(##)', '༎'],
    u'\u0f0f' : ['(##)', '༏'],
    u'\u0f10' : ['(##)', '༐'],
    u'\u0f11' : ['(##)', '༑'],
    u'\u0f12' : ['(##)', '༒'],
    u'\u0f13' : ['(##)', '༓'],
    u'\u0f14' : ['(##)', '༔'],
    u'\u0f15' : ['(##)', '༕'],
    u'\u0f16' : ['(##)', '༖'],
    u'\u0f17' : ['(##)', '༗'],
    u'\u0f18' : ['(##)', '༘'],
    u'\u0f19' : ['(##)', '༙'],
    u'\u0f1a' : ['(##)', '༚'],
    u'\u0f1b' : ['(##)', '༛'],
    u'\u0f1c' : ['(##)', '༜'],
    u'\u0f1d' : ['(##)', '༝'],
    u'\u0f1e' : ['(##)', '༞'],
    u'\u0f1f' : ['(##)', '༟'],
    u'\u0f20' : ['(ZERO)', '༠'],
    u'\u0f21' : ['(ONE)', '༡'],
    u'\u0f22' : ['(TWO)', '༢'],
    u'\u0f23' : ['(THREE)', '༣'],
    u'\u0f24' : ['(FOUR)', '༤'],
    u'\u0f25' : ['(FIVE)', '༥'],
    u'\u0f26' : ['(SIX)', '༦'],
    u'\u0f27' : ['(SEVEN)', '༧'],
    u'\u0f28' : ['(EIGHT)', '༨'],
    u'\u0f29' : ['(NINE)', '༩'],
    u'\u0f2a' : ['(HALF ONE)', '༪'],
    u'\u0f2b' : ['(HALF TWO)', '༫'],
    u'\u0f2c' : ['(HALF THREE)', '༬'],
    u'\u0f2d' : ['(HALF FOUR)', '༭'],
    u'\u0f2e' : ['(HALF FIVE)', '༮'],
    u'\u0f2f' : ['(HALF SIX)', '༯'],
    u'\u0f30' : ['(HALF SEVEN)', '༰'],
    u'\u0f31' : ['(HALF EIGHT)', '༱'],
    u'\u0f32' : ['(HALF NINE)', '༲'],
    u'\u0f33' : ['(HALF ZERO)', '༳'],
    u'\u0f34' : ['(##)', '༴'],
    u'\u0f35' : ['(##)', '༵'],
    u'\u0f36' : ['(##)', '༶'],
    u'\u0f37' : ['(##)', '༷'],
    u'\u0f38' : ['(##)', '༸'],
    u'\u0f39' : ['(##)', '༹'],
    u'\u0f3a' : ['(##)', '༺'],
    u'\u0f3b' : ['(##)', '༻'],
    u'\u0f3c' : ['(##)', '༼'],
    u'\u0f3d' : ['(##)', '༽'],
    u'\u0f3e' : ['(##)', '༾'],
    u'\u0f3f' : ['(##)', '༿'],
    u'\u0f40' : ['k A', 'ཀ'],
    u'\u0f41' : ['k_h A', 'ཁ'],
    u'\u0f42' : ['k A', 'ག'],
    u'\u0f43' : ['k_h A', 'གྷ'],
    u'\u0f44' : ['N A', 'ང'],
    u'\u0f45' : ['tS A', 'ཅ'],
    u'\u0f46' : ['tSh A', 'ཆ'],
    u'\u0f47' : ['tS A', 'ཇ'],
    u'\u0f49' : ['J A', 'ཉ'],
    u'\u0f4a' : ['t t A', 'ཊ'],
    u'\u0f4b' : ['t t_h A', 'ཋ'],
    u'\u0f4c' : ['t t A', 'ཌ'],
    u'\u0f4d' : ['t t_h A', 'ཌྷ'],
    u'\u0f4e' : ['_n _n A', 'ཎ'],
    u'\u0f4f' : ['t A', 'ཏ'],
    u'\u0f50' : ['t_h A', 'ཐ'],
    u'\u0f51' : ['t A', 'ད'],
    u'\u0f52' : ['t_h A', 'དྷ'],
    u'\u0f53' : ['_n A', 'ན'],
    u'\u0f54' : ['p A', 'པ'],
    u'\u0f55' : ['p_h A', 'ཕ'],
    u'\u0f56' : ['p A', 'བ'],
    u'\u0f57' : ['p_h A', 'བྷ'],
    u'\u0f58' : ['m A', 'མ'],
    u'\u0f59' : ['ts A', 'ཙ'],
    u'\u0f5a' : ['tsh A', 'ཚ'],
    u'\u0f5b' : ['ts A', 'ཛ'],
    u'\u0f5c' : ['tsh A', 'ཛྷ'],
    u'\u0f5d' : ['_w A', 'ཝ'],
    u'\u0f5e' : ['S A', 'ཞ'],
    u'\u0f5f' : ['s A', 'ཟ'],
    u'\u0f60' : ['A', 'འ'],
    u'\u0f61' : ['_j A', 'ཡ'],
    u'\u0f62' : ['` A', 'ར'],
    u'\u0f63' : ['_l A', 'ལ'],
    u'\u0f64' : ['S A', 'ཤ'],
    u'\u0f65' : ['S S A', 'ཥ'],
    u'\u0f66' : ['s A', 'ས'],
    u'\u0f67' : ['_h A', 'ཧ'],
    u'\u0f68' : ['A', 'ཨ'],
    u'\u0f69' : ['k S S A', 'ཀྵ'],
    u'\u0f6a' : ['` A', 'ཪ'],
    u'\u0f71' : ['A:', 'ཱ'],
    u'\u0f72' : ['i', 'ི'],
    u'\u0f73' : ['i:', 'ཱི'],
    u'\u0f74' : ['u', 'ུ'],
    u'\u0f75' : ['u:', 'ཱུ'],
    u'\u0f76' : ['r=', 'ྲྀ'],
    u'\u0f77' : ['rr=', 'ཷ'],
    u'\u0f78' : ['l_=', 'ླྀ'],
    u'\u0f79' : ['l_=', 'ཹ'],
    u'\u0f7a' : ['e', 'ེ'],
    u'\u0f7b' : ['e:', 'ཻ'],
    u'\u0f7c' : ['o', 'ོ'],
    u'\u0f7d' : ['o:', 'ཽ'],
    u'\u0f7e' : ['(M)', 'ཾ'],
    u'\u0f7f' : ['(H)', 'ཿ'],
    u'\u0f80' : ['i', 'ྀ'],
    u'\u0f81' : ['i:', 'ཱྀ'],
    u'\u0f82' : ['(##)', 'ྂ'],
    u'\u0f83' : ['(##)', 'ྃ'],
    u'\u0f84' : ['(##)', '྄'],
    u'\u0f85' : ['(##)', '྅'],
    u'\u0f86' : ['(##)', '྆'],
    u'\u0f87' : ['(##)', '྇'],
    u'\u0f88' : ['(##)', 'ྈ'],
    u'\u0f89' : ['(##)', 'ྉ'],
    u'\u0f8a' : ['(##)', 'ྊ'],
    u'\u0f8b' : ['(##)', 'ྋ'],
    u'\u0f90' : ['k A', 'ྐ'],
    u'\u0f91' : ['k_h A', 'ྑ'],
    u'\u0f92' : ['k A', 'ྒ'],
    u'\u0f93' : ['k_h A', 'ྒྷ'],
    u'\u0f94' : ['N A', 'ྔ'],
    u'\u0f95' : ['tS A', 'ྕ'],
    u'\u0f96' : ['tSh A', 'ྖ'],
    u'\u0f97' : ['tS A', 'ྗ'],
    u'\u0f99' : ['J A', 'ྙ'],
    u'\u0f9a' : ['t t A', 'ྚ'],
    u'\u0f9b' : ['t t_h A', 'ྛ'],
    u'\u0f9c' : ['t t A', 'ྜ'],
    u'\u0f9d' : ['t t_h A', 'ྜྷ'],
    u'\u0f9e' : ['_n _n A', 'ྞ'],
    u'\u0f9f' : ['t A', 'ྟ'],
    u'\u0fa0' : ['t_h A', 'ྠ'],
    u'\u0fa1' : ['t A', 'ྡ'],
    u'\u0fa2' : ['t_h A', 'ྡྷ'],
    u'\u0fa3' : ['_n A', 'ྣ'],
    u'\u0fa4' : ['p A', 'ྤ'],
    u'\u0fa5' : ['p_h A', 'ྥ'],
    u'\u0fa6' : ['p A', 'ྦ'],
    u'\u0fa7' : ['p_h A', 'ྦྷ'],
    u'\u0fa8' : ['m A', 'ྨ'],
    u'\u0fa9' : ['ts A', 'ྩ'],
    u'\u0faa' : ['tsh A', 'ྪ'],
    u'\u0fab' : ['ts A', 'ྫ'],
    u'\u0fac' : ['tsh A', 'ྫྷ'],
    u'\u0fad' : ['_w A', 'ྭ'],
    u'\u0fae' : ['S A', 'ྮ'],
    u'\u0faf' : ['s A', 'ྯ'],
    u'\u0fb0' : ['A', 'ྰ'],
    u'\u0fb1' : ['_j A', 'ྱ'],
    u'\u0fb2' : ['` A', 'ྲ'],
    u'\u0fb3' : ['_l A', 'ླ'],
    u'\u0fb4' : ['S A', 'ྴ'],
    u'\u0fb5' : ['S S A', 'ྵ'],
    u'\u0fb6' : ['s A', 'ྶ'],
    u'\u0fb7' : ['_h A', 'ྷ'],
    u'\u0fb8' : ['A', 'ྸ'],
    u'\u0fb9' : ['k S S A', 'ྐྵ'],
    u'\u0fba' : ['_w A', 'ྺ'],
    u'\u0fbb' : ['_j A', 'ྻ'],
    u'\u0fbc' : ['` A', 'ྼ'],
    u'\u0fbe' : ['(REFRAIN)', '྾'],
    u'\u0fbf' : ['(REFERENCE MARK)', '྿'],
    u'\u0fc0' : ['(HEAVY DRUM BEAT)', '࿀'],
    u'\u0fc1' : ['(LIGHT DRUM BEAT)', '࿁'],
    u'\u0fc2' : ['(SMALL TIBETAN HAND DRUM)', '࿂'],
    u'\u0fc3' : ['(TIBETAN CYMBAL)', '࿃'],
    u'\u0fc4' : ['(##)', '࿄'],
    u'\u0fc5' : ['(##)', '࿅'],
    u'\u0fc6' : ['(##)', '࿆'],
    u'\u0fc7' : ['(##)', '࿇'],
    u'\u0fc8' : ['(##)', '࿈'],
    u'\u0fc9' : ['(##)', '࿉'],
    u'\u0fca' : ['(##)', '࿊'],
    u'\u0fcb' : ['(##)', '࿋'],
    u'\u0fcc' : ['(##)', '࿌'],
    u'\u0fcf' : ['(##)', '࿏'],
    u'\u0fd0' : ['(##)', '࿐'],
    u'\u0fd1' : ['(##)', '࿑'],
    u'\u1000' : ['k A', 'က'],
    u'\u1001' : ['k_h A', 'ခ'],
    u'\u1002' : ['g A', 'ဂ'],
    u'\u1003' : ['g A', 'ဃ'],
    u'\u1004' : ['N A', 'င'],
    u'\u1005' : ['s A', 'စ'],
    u'\u1006' : ['sh A', 'ဆ'],
    u'\u1007' : ['z A', 'ဇ'],
    u'\u1008' : ['z A', 'ဈ'],
    u'\u1009' : ['J A', 'ဉ'],
    u'\u100a' : ['J A', 'ည'],
    u'\u100b' : ['t` A', 'ဋ'],
    u'\u100c' : ['trh A', 'ဌ'],
    u'\u100d' : ['d A', 'ဍ'],
    u'\u100e' : ['d A', 'ဎ'],
    u'\u100f' : ['_n A', 'ဏ'],
    u'\u1010' : ['t` A', 'တ'],
    u'\u1011' : ['trh A', 'ထ'],
    u'\u1012' : ['d A', 'ဒ'],
    u'\u1013' : ['d A', 'ဓ'],
    u'\u1014' : ['_n A', 'န'],
    u'\u1015' : ['p A', 'ပ'],
    u'\u1016' : ['p_h A', 'ဖ'],
    u'\u1017' : ['b A', 'ဗ'],
    u'\u1018' : ['b_h A', 'ဘ'],
    u'\u1019' : ['m A', 'မ'],
    u'\u101a' : ['_j A', 'ယ'],
    u'\u101b' : ['_j A', 'ရ'],
    u'\u101c' : ['_l A', 'လ'],
    u'\u101d' : ['_w A', 'ဝ'],
    u'\u101e' : ['T A', 'သ'],
    u'\u101f' : ['_h A', 'ဟ'],
    u'\u1020' : ['_l A', 'ဠ'],
    u'\u1021' : ['A, _k', 'အ'],
    u'\u1023' : ['i', 'ဣ'],
    u'\u1024' : ['i', 'ဤ'],
    u'\u1025' : ['u', 'ဥ'],
    u'\u1026' : ['u', 'ဦ'],
    u'\u1027' : ['e', 'ဧ'],
    u'\u1029' : ['o', 'ဩ'],
    u'\u102a' : ['_>', 'ဪ'],
    u'\u102c' : ['A', 'ာ'],
    u'\u102d' : ['i', 'ိ'],
    u'\u102e' : ['i', 'ီ'],
    u'\u102f' : ['u', 'ု'],
    u'\u1030' : ['u', 'ူ'],
    u'\u1031' : ['e', 'ေ'],
    u'\u1032' : ['E', 'ဲ'],
    u'\u1036' : ['(M)', 'ံ'],
    u'\u1037' : ['(TONE MARK)', '့'],
    u'\u1038' : ['(H)', 'း'],
    u'\u1039' : ['(P)', '္'],
    u'\u1040' : ['(ZERO)', '၀'],
    u'\u1041' : ['(ONE)', '၁'],
    u'\u1042' : ['(TWO)', '၂'],
    u'\u1043' : ['(THREE)', '၃'],
    u'\u1044' : ['(FOUR)', '၄'],
    u'\u1045' : ['(FIVE)', '၅'],
    u'\u1046' : ['(SIX)', '၆'],
    u'\u1047' : ['(SEVEN)', '၇'],
    u'\u1048' : ['(EIGHT)', '၈'],
    u'\u1049' : ['(NINE)', '၉'],
    u'\u104a' : ['(DANDA)', '၊'],
    u'\u104b' : ['(DOUBLE DANDA)', '။'],
    u'\u104c' : ['(LOCATIVE)', '၌'],
    u'\u104d' : ['(COMPLETED)', '၍'],
    u'\u104e' : ['(AFOREMENTIONED)', '၎'],
    u'\u104f' : ['(GENETIVE)', '၏'],
    u'\u1050' : ['sh', 'ၐ'],
    u'\u1051' : ['s`', 'ၑ'],
    u'\u1052' : ['r=', 'ၒ'],
    u'\u1053' : ['rr=', 'ၓ'],
    u'\u1054' : ['l_=', 'ၔ'],
    u'\u1055' : ['l_=', 'ၕ'],
    u'\u1056' : ['r=', 'ၖ'],
    u'\u1057' : ['rr=', 'ၗ'],
    u'\u1058' : ['l_=', 'ၘ'],
    u'\u1059' : ['l_=', 'ၙ'],
    u'\u10a0' : ['A', 'Ⴀ'],
    u'\u10a1' : ['b', 'Ⴁ'],
    u'\u10a2' : ['g', 'Ⴂ'],
    u'\u10a3' : ['d', 'Ⴃ'],
    u'\u10a4' : ['E', 'Ⴄ'],
    u'\u10a5' : ['_v', 'Ⴅ'],
    u'\u10a6' : ['z', 'Ⴆ'],
    u'\u10a7' : ['t_h', 'Ⴇ'],
    u'\u10a8' : ['i', 'Ⴈ'],
    u'\u10a9' : ['k_>', 'Ⴉ'],
    u'\u10aa' : ['_l', 'Ⴊ'],
    u'\u10ab' : ['m', 'Ⴋ'],
    u'\u10ac' : ['_n', 'Ⴌ'],
    u'\u10ad' : ['o', 'Ⴍ'],
    u'\u10ae' : ['p_>', 'Ⴎ'],
    u'\u10af' : ['Z', 'Ⴏ'],
    u'\u10b0' : ['_v', 'Ⴐ'],
    u'\u10b1' : ['s', 'Ⴑ'],
    u'\u10b2' : ['t_>', 'Ⴒ'],
    u'\u10b3' : ['u', 'Ⴓ'],
    u'\u10b4' : ['p_h', 'Ⴔ'],
    u'\u10b5' : ['k_h', 'Ⴕ'],
    u'\u10b6' : ['G', 'Ⴖ'],
    u'\u10b7' : ['q>', 'Ⴗ'],
    u'\u10b8' : ['S', 'Ⴘ'],
    u'\u10b9' : ['tS', 'Ⴙ'],
    u'\u10ba' : ['k', 'Ⴚ'],
    u'\u10bb' : ['dz', 'Ⴛ'],
    u'\u10bc' : ['ts>', 'Ⴜ'],
    u'\u10bd' : ['tS>', 'Ⴝ'],
    u'\u10be' : ['_x', 'Ⴞ'],
    u'\u10bf' : ['dZ', 'Ⴟ'],
    u'\u10c0' : ['_h', 'Ⴠ'],
    u'\u10c1' : ['ej', 'Ⴡ'],
    u'\u10c2' : ['_j', 'Ⴢ'],
    u'\u10c3' : ['_w i', 'Ⴣ'],
    u'\u10c4' : ['q', 'Ⴤ'],
    u'\u10c5' : ['oU', 'Ⴥ'],
    u'\u10d0' : ['A', 'ა'],
    u'\u10d1' : ['b', 'ბ'],
    u'\u10d2' : ['g', 'გ'],
    u'\u10d3' : ['d', 'დ'],
    u'\u10d4' : ['E', 'ე'],
    u'\u10d5' : ['_v', 'ვ'],
    u'\u10d6' : ['z', 'ზ'],
    u'\u10d7' : ['t_h', 'თ'],
    u'\u10d8' : ['i', 'ი'],
    u'\u10d9' : ['k_>', 'კ'],
    u'\u10da' : ['_l', 'ლ'],
    u'\u10db' : ['m', 'მ'],
    u'\u10dc' : ['_n', 'ნ'],
    u'\u10dd' : ['o', 'ო'],
    u'\u10de' : ['p_>', 'პ'],
    u'\u10df' : ['Z', 'ჟ'],
    u'\u10e0' : ['_v', 'რ'],
    u'\u10e1' : ['s', 'ს'],
    u'\u10e2' : ['t_>', 'ტ'],
    u'\u10e3' : ['u', 'უ'],
    u'\u10e4' : ['p_h', 'ფ'],
    u'\u10e5' : ['k_h', 'ქ'],
    u'\u10e6' : ['G', 'ღ'],
    u'\u10e7' : ['q>', 'ყ'],
    u'\u10e8' : ['S', 'შ'],
    u'\u10e9' : ['tS', 'ჩ'],
    u'\u10ea' : ['k', 'ც'],
    u'\u10eb' : ['dz', 'ძ'],
    u'\u10ec' : ['ts>', 'წ'],
    u'\u10ed' : ['tS>', 'ჭ'],
    u'\u10ee' : ['_x', 'ხ'],
    u'\u10ef' : ['dZ', 'ჯ'],
    u'\u10f0' : ['_h', 'ჰ'],
    u'\u10f1' : ['ej', 'ჱ'],
    u'\u10f2' : ['_j', 'ჲ'],
    u'\u10f3' : ['_w i', 'ჳ'],
    u'\u10f4' : ['q', 'ჴ'],
    u'\u10f5' : ['oU', 'ჵ'],
    u'\u10f6' : ['f', 'ჶ'],
    u'\u10f7' : ['(YN)', 'ჷ'],
    u'\u10f8' : ['(ELIFI)', 'ჸ'],
    u'\u10f9' : ['(TURNED GAN)', 'ჹ'],
    u'\u10fa' : ['(AIN)', 'ჺ'],
    u'\u10fb' : ['(PARAGRAPH SEPARATOR)', '჻'],
    u'\u10fc' : ['(MODIFIER LETTER GEORGIAN NAR)', 'ჼ'],
    u'\u1100' : ['k', 'ᄀ'],
    u'\u1101' : ['k_>', 'ᄁ'],
    u'\u1102' : ['_n', 'ᄂ'],
    u'\u1103' : ['t', 'ᄃ'],
    u'\u1104' : ['t_>', 'ᄄ'],
    u'\u1105' : ['`', 'ᄅ'],
    u'\u1106' : ['m', 'ᄆ'],
    u'\u1107' : ['p', 'ᄇ'],
    u'\u1108' : ['p_>', 'ᄈ'],
    u'\u1109' : ['sh', 'ᄉ'],
    u'\u110a' : ['s', 'ᄊ'],
    u'\u110b' : ['(SILENT)', 'ᄋ'],
    u'\u110c' : ['tS', 'ᄌ'],
    u'\u110d' : ['tS>', 'ᄍ'],
    u'\u110e' : ['tSh', 'ᄎ'],
    u'\u110f' : ['k_h', 'ᄏ'],
    u'\u1110' : ['t_h', 'ᄐ'],
    u'\u1112' : ['_h', 'ᄒ'],
    u'\u113c' : ['sh', 'ᄼ'],
    u'\u113d' : ['s', 'ᄽ'],
    u'\u113e' : ['sh', 'ᄾ'],
    u'\u113f' : ['s', 'ᄿ'],
    u'\u114c' : ['(SILENT)', 'ᅌ'],
    u'\u114e' : ['tS', 'ᅎ'],
    u'\u114f' : ['tS>', 'ᅏ'],
    u'\u1150' : ['tS', 'ᅐ'],
    u'\u1151' : ['tS>', 'ᅑ'],
    u'\u1154' : ['tSh', 'ᅔ'],
    u'\u1155' : ['tSh', 'ᅕ'],
    u'\u115f' : ['(CHOSEONG FILLER)', 'ᅟ'],
    u'\u1160' : ['(JUNGSEONG FILLER)', 'ᅠ'],
    u'\u1161' : ['a', 'ᅡ'],
    u'\u1162' : ['{', 'ᅢ'],
    u'\u1163' : ['_j a', 'ᅣ'],
    u'\u1164' : ['_j {', 'ᅤ'],
    u'\u1165' : ['_r', 'ᅥ'],
    u'\u1166' : ['e', 'ᅦ'],
    u'\u1167' : ['_j _r', 'ᅧ'],
    u'\u1168' : ['_j e', 'ᅨ'],
    u'\u1169' : ['o', 'ᅩ'],
    u'\u116a' : ['_w a', 'ᅪ'],
    u'\u116b' : ['_w {', 'ᅫ'],
    u'\u116c' : ['_w e', 'ᅬ'],
    u'\u116d' : ['_j o', 'ᅭ'],
    u'\u116e' : ['u', 'ᅮ'],
    u'\u116f' : ['_w _r', 'ᅯ'],
    u'\u1170' : ['_w E', 'ᅰ'],
    u'\u1171' : ['2', 'ᅱ'],
    u'\u1172' : ['_j u', 'ᅲ'],
    u'\u1173' : ['M', 'ᅳ'],
    u'\u1174' : ['M _j', 'ᅴ'],
    u'\u1175' : ['i', 'ᅵ'],
    u'\u11a8' : ['k', 'ᆨ'],
    u'\u11a9' : ['k_>', 'ᆩ'],
    u'\u11aa' : ['k sh', 'ᆪ'],
    u'\u11ab' : ['_n', 'ᆫ'],
    u'\u11ac' : ['_n tS', 'ᆬ'],
    u'\u11ad' : ['_n _h', 'ᆭ'],
    u'\u11ae' : ['t', 'ᆮ'],
    u'\u11af' : ['_l', 'ᆯ'],
    u'\u11b0' : ['_l k', 'ᆰ'],
    u'\u11b1' : ['_l m', 'ᆱ'],
    u'\u11b2' : ['_l p', 'ᆲ'],
    u'\u11b3' : ['_l sh', 'ᆳ'],
    u'\u11b4' : ['_l t_h', 'ᆴ'],
    u'\u11b5' : ['_l p_h', 'ᆵ'],
    u'\u11b6' : ['_l _h', 'ᆶ'],
    u'\u11b7' : ['m', 'ᆷ'],
    u'\u11b8' : ['p', 'ᆸ'],
    u'\u11b9' : ['p sh', 'ᆹ'],
    u'\u11ba' : ['sh', 'ᆺ'],
    u'\u11bb' : ['s', 'ᆻ'],
    u'\u11bc' : ['N', 'ᆼ'],
    u'\u11bd' : ['tS', 'ᆽ'],
    u'\u11be' : ['tSh', 'ᆾ'],
    u'\u11bf' : ['k_h', 'ᆿ'],
    u'\u11c0' : ['t_h', 'ᇀ'],
    u'\u11c1' : ['p_h', 'ᇁ'],
    u'\u11c2' : ['_h', 'ᇂ'],
    u'\u11f0' : ['N', 'ᇰ'],
    u'\u1200' : ['_h 3', 'ሀ'],
    u'\u1201' : ['_h u', 'ሁ'],
    u'\u1202' : ['_h i', 'ሂ'],
    u'\u1203' : ['_h A', 'ሃ'],
    u'\u1204' : ['_h e', 'ሄ'],
    u'\u1205' : ['_h &, _h', 'ህ'],
    u'\u1206' : ['_h o', 'ሆ'],
    u'\u1207' : ['_h o 3', 'ሇ'],
    u'\u1208' : ['_l 3', 'ለ'],
    u'\u1209' : ['_l u', 'ሉ'],
    u'\u120a' : ['_l i', 'ሊ'],
    u'\u120b' : ['_l A', 'ላ'],
    u'\u120c' : ['_l e', 'ሌ'],
    u'\u120d' : ['_l &, _l', 'ል'],
    u'\u120e' : ['_l o', 'ሎ'],
    u'\u120f' : ['_l u 3', 'ሏ'],
    u'\u1210' : ['_h 3', 'ሐ'],
    u'\u1211' : ['_h u', 'ሑ'],
    u'\u1212' : ['_h i', 'ሒ'],
    u'\u1213' : ['_h A', 'ሓ'],
    u'\u1214' : ['_h e', 'ሔ'],
    u'\u1215' : ['_h &, _h', 'ሕ'],
    u'\u1216' : ['_h o', 'ሖ'],
    u'\u1217' : ['_h u 3', 'ሗ'],
    u'\u1218' : ['m 3', 'መ'],
    u'\u1219' : ['m u', 'ሙ'],
    u'\u121a' : ['m i', 'ሚ'],
    u'\u121b' : ['m A', 'ማ'],
    u'\u121c' : ['m e', 'ሜ'],
    u'\u121d' : ['m &, m', 'ም'],
    u'\u121e' : ['m o', 'ሞ'],
    u'\u121f' : ['m u 3', 'ሟ'],
    u'\u1220' : ['s 3', 'ሠ'],
    u'\u1221' : ['s u', 'ሡ'],
    u'\u1222' : ['s i', 'ሢ'],
    u'\u1223' : ['s A', 'ሣ'],
    u'\u1224' : ['s e', 'ሤ'],
    u'\u1225' : ['s &, s', 'ሥ'],
    u'\u1226' : ['s o', 'ሦ'],
    u'\u1227' : ['s u 3', 'ሧ'],
    u'\u1228' : ['r\\` 3', 'ረ'],
    u'\u1229' : ['r\\` u', 'ሩ'],
    u'\u122a' : ['r\\` i', 'ሪ'],
    u'\u122b' : ['r\\` A', 'ራ'],
    u'\u122c' : ['r\\` e', 'ሬ'],
    u'\u122d' : ['r\\` &, r\\`', 'ር'],
    u'\u122e' : ['r\\` o', 'ሮ'],
    u'\u122f' : ['r\\` u 3', 'ሯ'],
    u'\u1230' : ['s 3', 'ሰ'],
    u'\u1231' : ['s u', 'ሱ'],
    u'\u1232' : ['s i', 'ሲ'],
    u'\u1233' : ['s A', 'ሳ'],
    u'\u1234' : ['s e', 'ሴ'],
    u'\u1235' : ['s &, s', 'ስ'],
    u'\u1236' : ['s o', 'ሶ'],
    u'\u1237' : ['s u 3', 'ሷ'],
    u'\u1238' : ['S 3', 'ሸ'],
    u'\u1239' : ['S u', 'ሹ'],
    u'\u123a' : ['S i', 'ሺ'],
    u'\u123b' : ['S A', 'ሻ'],
    u'\u123c' : ['S e', 'ሼ'],
    u'\u123d' : ['S &, S', 'ሽ'],
    u'\u123e' : ['S o', 'ሾ'],
    u'\u123f' : ['S u 3', 'ሿ'],
    u'\u1240' : ['k_> 3', 'ቀ'],
    u'\u1241' : ['k_> u', 'ቁ'],
    u'\u1242' : ['k_> i', 'ቂ'],
    u'\u1243' : ['k_> A', 'ቃ'],
    u'\u1244' : ['k_> e', 'ቄ'],
    u'\u1245' : ['k_> &, k_>', 'ቅ'],
    u'\u1246' : ['k_> o', 'ቆ'],
    u'\u1247' : ['k_> o 3', 'ቇ'],
    u'\u1248' : ['k_> u 3', 'ቈ'],
    u'\u124a' : ['k_> ui', 'ቊ'],
    u'\u124b' : ['k_> ua', 'ቋ'],
    u'\u124c' : ['k_> u e', 'ቌ'],
    u'\u124d' : ['k_> u@', 'ቍ'],
    u'\u1250' : ['k_> _h 3', 'ቐ'],
    u'\u1251' : ['k_> _h u', 'ቑ'],
    u'\u1252' : ['k_> _h i', 'ቒ'],
    u'\u1253' : ['k_> _h A', 'ቓ'],
    u'\u1254' : ['k_> _h e', 'ቔ'],
    u'\u1255' : ['k_> _h &, k_> _h', 'ቕ'],
    u'\u1256' : ['k_> _h o', 'ቖ'],
    u'\u125a' : ['k_> _h ui', 'ቚ'],
    u'\u125b' : ['k_> _h ua', 'ቛ'],
    u'\u125c' : ['k_> _h u e', 'ቜ'],
    u'\u125d' : ['k_> _h u@', 'ቝ'],
    u'\u1260' : ['b 3', 'በ'],
    u'\u1261' : ['b u', 'ቡ'],
    u'\u1262' : ['b i', 'ቢ'],
    u'\u1263' : ['b A', 'ባ'],
    u'\u1264' : ['b e', 'ቤ'],
    u'\u1265' : ['b &, b', 'ብ'],
    u'\u1266' : ['b o', 'ቦ'],
    u'\u1267' : ['b u 3', 'ቧ'],
    u'\u1268' : ['_v 3', 'ቨ'],
    u'\u1269' : ['_v u', 'ቩ'],
    u'\u126a' : ['_v i', 'ቪ'],
    u'\u126b' : ['_v A', 'ቫ'],
    u'\u126c' : ['_v e', 'ቬ'],
    u'\u126d' : ['_v &, _v', 'ቭ'],
    u'\u126e' : ['_v o', 'ቮ'],
    u'\u126f' : ['_v u 3', 'ቯ'],
    u'\u1270' : ['t_d 3', 'ተ'],
    u'\u1271' : ['t_d u', 'ቱ'],
    u'\u1272' : ['t_d i', 'ቲ'],
    u'\u1273' : ['t_d A', 'ታ'],
    u'\u1274' : ['t_d e', 'ቴ'],
    u'\u1275' : ['t_d &, t_d', 'ት'],
    u'\u1276' : ['t_d o', 'ቶ'],
    u'\u1277' : ['t_d u 3', 'ቷ'],
    u'\u1278' : ['tS 3', 'ቸ'],
    u'\u1279' : ['tS u', 'ቹ'],
    u'\u127a' : ['tS i', 'ቺ'],
    u'\u127b' : ['tS A', 'ቻ'],
    u'\u127c' : ['tS e', 'ቼ'],
    u'\u127d' : ['tS &, tS', 'ች'],
    u'\u127e' : ['tS o', 'ቾ'],
    u'\u127f' : ['tS u 3', 'ቿ'],
    u'\u1280' : ['_h 3', 'ኀ'],
    u'\u1281' : ['_h u', 'ኁ'],
    u'\u1282' : ['_h i', 'ኂ'],
    u'\u1283' : ['_h A', 'ኃ'],
    u'\u1284' : ['_h e', 'ኄ'],
    u'\u1285' : ['_h &, _h', 'ኅ'],
    u'\u1286' : ['_h o', 'ኆ'],
    u'\u1287' : ['_h o 3', 'ኇ'],
    u'\u1288' : ['_h u 3', 'ኈ'],
    u'\u128a' : ['_h ui', 'ኊ'],
    u'\u128b' : ['_h ua', 'ኋ'],
    u'\u128c' : ['_h u e', 'ኌ'],
    u'\u128d' : ['_h u@', 'ኍ'],
    u'\u1290' : ['_n 3', 'ነ'],
    u'\u1291' : ['_n u', 'ኑ'],
    u'\u1292' : ['_n i', 'ኒ'],
    u'\u1293' : ['_n A', 'ና'],
    u'\u1294' : ['_n e', 'ኔ'],
    u'\u1295' : ['_n &, _n', 'ን'],
    u'\u1296' : ['_n o', 'ኖ'],
    u'\u1297' : ['_n u 3', 'ኗ'],
    u'\u1298' : ['J 3', 'ኘ'],
    u'\u1299' : ['J u', 'ኙ'],
    u'\u129a' : ['J i', 'ኚ'],
    u'\u129b' : ['J A', 'ኛ'],
    u'\u129c' : ['J e', 'ኜ'],
    u'\u129d' : ['J &, J', 'ኝ'],
    u'\u129e' : ['J o', 'ኞ'],
    u'\u129f' : ['J u 3', 'ኟ'],
    u'\u12a0' : ['_k 3', 'አ'],
    u'\u12a1' : ['_k u', 'ኡ'],
    u'\u12a2' : ['_k i', 'ኢ'],
    u'\u12a3' : ['_k A', 'ኣ'],
    u'\u12a4' : ['_k e', 'ኤ'],
    u'\u12a5' : ['_k &, _k', 'እ'],
    u'\u12a6' : ['_k o', 'ኦ'],
    u'\u12a7' : ['_k u 3', 'ኧ'],
    u'\u12a8' : ['k 3', 'ከ'],
    u'\u12a9' : ['k u', 'ኩ'],
    u'\u12aa' : ['k i', 'ኪ'],
    u'\u12ab' : ['k A', 'ካ'],
    u'\u12ac' : ['k e', 'ኬ'],
    u'\u12ad' : ['k &, k', 'ክ'],
    u'\u12ae' : ['k o', 'ኮ'],
    u'\u12af' : ['k o 3', 'ኯ'],
    u'\u12b0' : ['k u 3', 'ኰ'],
    u'\u12b2' : ['k ui', 'ኲ'],
    u'\u12b3' : ['k ua', 'ኳ'],
    u'\u12b4' : ['k u e', 'ኴ'],
    u'\u12b5' : ['k u@', 'ኵ'],
    u'\u12b8' : ['_h 3', 'ኸ'],
    u'\u12b9' : ['_h u', 'ኹ'],
    u'\u12ba' : ['_h i', 'ኺ'],
    u'\u12bb' : ['_h A', 'ኻ'],
    u'\u12bc' : ['_h e', 'ኼ'],
    u'\u12bd' : ['_h &, _h', 'ኽ'],
    u'\u12be' : ['_h o', 'ኾ'],
    u'\u12c0' : ['_h u 3', 'ዀ'],
    u'\u12c2' : ['_h ui', 'ዂ'],
    u'\u12c3' : ['_h ua', 'ዃ'],
    u'\u12c4' : ['_h u e', 'ዄ'],
    u'\u12c5' : ['_h u@', 'ዅ'],
    u'\u12c8' : ['_w 3', 'ወ'],
    u'\u12c9' : ['_w u', 'ዉ'],
    u'\u12ca' : ['_w i', 'ዊ'],
    u'\u12cb' : ['_w A', 'ዋ'],
    u'\u12cc' : ['_w e', 'ዌ'],
    u'\u12cd' : ['_w &, _w', 'ው'],
    u'\u12ce' : ['_w o', 'ዎ'],
    u'\u12cf' : ['_w o 3', 'ዏ'],
    u'\u12d0' : ['_k 3', 'ዐ'],
    u'\u12d1' : ['_k u', 'ዑ'],
    u'\u12d2' : ['_k i', 'ዒ'],
    u'\u12d3' : ['_k A', 'ዓ'],
    u'\u12d4' : ['_k e', 'ዔ'],
    u'\u12d5' : ['_k &, _k', 'ዕ'],
    u'\u12d6' : ['_k o', 'ዖ'],
    u'\u12d8' : ['z 3', 'ዘ'],
    u'\u12d9' : ['z u', 'ዙ'],
    u'\u12da' : ['z i', 'ዚ'],
    u'\u12db' : ['z A', 'ዛ'],
    u'\u12dc' : ['z e', 'ዜ'],
    u'\u12dd' : ['z &, z', 'ዝ'],
    u'\u12de' : ['z o', 'ዞ'],
    u'\u12df' : ['z u 3', 'ዟ'],
    u'\u12e0' : ['Z 3', 'ዠ'],
    u'\u12e1' : ['Z u', 'ዡ'],
    u'\u12e2' : ['Z i', 'ዢ'],
    u'\u12e3' : ['Z A', 'ዣ'],
    u'\u12e4' : ['Z e', 'ዤ'],
    u'\u12e5' : ['Z &, Z', 'ዥ'],
    u'\u12e6' : ['Z o', 'ዦ'],
    u'\u12e7' : ['Z u 3', 'ዧ'],
    u'\u12e8' : ['_j 3', 'የ'],
    u'\u12e9' : ['_j u', 'ዩ'],
    u'\u12ea' : ['_j i', 'ዪ'],
    u'\u12eb' : ['_j A', 'ያ'],
    u'\u12ec' : ['_j e', 'ዬ'],
    u'\u12ed' : ['_j &, _j', 'ይ'],
    u'\u12ee' : ['_j o', 'ዮ'],
    u'\u12ef' : ['_j o 3', 'ዯ'],
    u'\u12f0' : ['d 3', 'ደ'],
    u'\u12f1' : ['d_d u', 'ዱ'],
    u'\u12f2' : ['d_d i', 'ዲ'],
    u'\u12f3' : ['d_d A', 'ዳ'],
    u'\u12f4' : ['d_d e', 'ዴ'],
    u'\u12f5' : ['d_d &, d_d', 'ድ'],
    u'\u12f6' : ['d_d o', 'ዶ'],
    u'\u12f7' : ['d_d u 3', 'ዷ'],
    u'\u12f8' : ['d d 3', 'ዸ'],
    u'\u12f9' : ['d d u', 'ዹ'],
    u'\u12fa' : ['d d i', 'ዺ'],
    u'\u12fb' : ['d d A', 'ዻ'],
    u'\u12fc' : ['d d e', 'ዼ'],
    u'\u12fd' : ['d d &, d d', 'ዽ'],
    u'\u12fe' : ['d d o', 'ዾ'],
    u'\u12ff' : ['d d u 3', 'ዿ'],
    u'\u1300' : ['dZ 3', 'ጀ'],
    u'\u1301' : ['dZ u', 'ጁ'],
    u'\u1302' : ['dZ i', 'ጂ'],
    u'\u1303' : ['dZ A', 'ጃ'],
    u'\u1304' : ['dZ e', 'ጄ'],
    u'\u1305' : ['dZ &, dZ', 'ጅ'],
    u'\u1306' : ['dZ o', 'ጆ'],
    u'\u1307' : ['dZ u 3', 'ጇ'],
    u'\u1308' : ['g 3', 'ገ'],
    u'\u1309' : ['g u', 'ጉ'],
    u'\u130a' : ['g i', 'ጊ'],
    u'\u130b' : ['g A', 'ጋ'],
    u'\u130c' : ['g e', 'ጌ'],
    u'\u130d' : ['g &, g', 'ግ'],
    u'\u130e' : ['g o', 'ጎ'],
    u'\u130f' : ['g o 3', 'ጏ'],
    u'\u1310' : ['g u 3', 'ጐ'],
    u'\u1312' : ['g ui', 'ጒ'],
    u'\u1313' : ['g ua', 'ጓ'],
    u'\u1314' : ['g u e', 'ጔ'],
    u'\u1315' : ['g u@', 'ጕ'],
    u'\u1318' : ['g g 3', 'ጘ'],
    u'\u1319' : ['g g u', 'ጙ'],
    u'\u131a' : ['g g i', 'ጚ'],
    u'\u131b' : ['g g A', 'ጛ'],
    u'\u131c' : ['g g e', 'ጜ'],
    u'\u131d' : ['g g &, g g', 'ጝ'],
    u'\u131e' : ['g g o', 'ጞ'],
    u'\u131f' : ['g g ua', 'ጟ'],
    u'\u1320' : ['t[> 3', 'ጠ'],
    u'\u1321' : ['t[> u', 'ጡ'],
    u'\u1322' : ['t[> i', 'ጢ'],
    u'\u1323' : ['t[> A', 'ጣ'],
    u'\u1324' : ['t[> e', 'ጤ'],
    u'\u1325' : ['t[> &, t[>', 'ጥ'],
    u'\u1326' : ['t[> o', 'ጦ'],
    u'\u1327' : ['t[> u 3', 'ጧ'],
    u'\u1328' : ['tS> 3', 'ጨ'],
    u'\u1329' : ['tS> u', 'ጩ'],
    u'\u132a' : ['tS> i', 'ጪ'],
    u'\u132b' : ['tS> A', 'ጫ'],
    u'\u132c' : ['tS> e', 'ጬ'],
    u'\u132d' : ['tS> &, tS>', 'ጭ'],
    u'\u132e' : ['tS> o', 'ጮ'],
    u'\u132f' : ['tS> u 3', 'ጯ'],
    u'\u1330' : ['p_> 3', 'ጰ'],
    u'\u1331' : ['p_> u', 'ጱ'],
    u'\u1332' : ['p_> i', 'ጲ'],
    u'\u1333' : ['p_> A', 'ጳ'],
    u'\u1334' : ['p_> e', 'ጴ'],
    u'\u1335' : ['p_> &, p_>', 'ጵ'],
    u'\u1336' : ['p_> o', 'ጶ'],
    u'\u1337' : ['p_> u 3', 'ጷ'],
    u'\u1338' : ['ts 3', 'ጸ'],
    u'\u1339' : ['ts u', 'ጹ'],
    u'\u133a' : ['ts i', 'ጺ'],
    u'\u133b' : ['ts A', 'ጻ'],
    u'\u133c' : ['ts e', 'ጼ'],
    u'\u133d' : ['ts &, ts', 'ጽ'],
    u'\u133e' : ['ts o', 'ጾ'],
    u'\u133f' : ['ts u 3', 'ጿ'],
    u'\u1340' : ['ts 3', 'ፀ'],
    u'\u1341' : ['ts u', 'ፁ'],
    u'\u1342' : ['ts i', 'ፂ'],
    u'\u1343' : ['ts A', 'ፃ'],
    u'\u1344' : ['ts e', 'ፄ'],
    u'\u1345' : ['ts &, ts', 'ፅ'],
    u'\u1346' : ['ts o', 'ፆ'],
    u'\u1347' : ['ts u 3', 'ፇ'],
    u'\u1348' : ['f 3', 'ፈ'],
    u'\u1349' : ['f u', 'ፉ'],
    u'\u134a' : ['f i', 'ፊ'],
    u'\u134b' : ['f A', 'ፋ'],
    u'\u134c' : ['f e', 'ፌ'],
    u'\u134d' : ['f &, f', 'ፍ'],
    u'\u134e' : ['f o', 'ፎ'],
    u'\u134f' : ['f u 3', 'ፏ'],
    u'\u1350' : ['p 3', 'ፐ'],
    u'\u1351' : ['p u', 'ፑ'],
    u'\u1352' : ['p i', 'ፒ'],
    u'\u1353' : ['p A', 'ፓ'],
    u'\u1354' : ['p e', 'ፔ'],
    u'\u1355' : ['p &, p', 'ፕ'],
    u'\u1356' : ['p o', 'ፖ'],
    u'\u1357' : ['p u 3', 'ፗ'],
    u'\u1358' : ['9rj a', 'ፘ'],
    u'\u1359' : ['mj a', 'ፙ'],
    u'\u135a' : ['fj a', 'ፚ'],
    u'\u135f' : ['(GEMINATION MARK)', '፟'],
    u'\u1360' : ['(SECTION MARK)', '፠'],
    u'\u1361' : ['(WORDSPACE)', '፡'],
    u'\u1362' : ['(FULL STOP)', '።'],
    u'\u1363' : ['(COMMA)', '፣'],
    u'\u1364' : ['(SEMICOLON)', '፤'],
    u'\u1365' : ['(COLON)', '፥'],
    u'\u1366' : ['(PREFACE COLON)', '፦'],
    u'\u1367' : ['(QUESTION MARK)', '፧'],
    u'\u1368' : ['(PARAGRAPH SEPARATOR)', '፨'],
    u'\u1369' : ['(ONE)', '፩'],
    u'\u136a' : ['(TWO)', '፪'],
    u'\u136b' : ['(THREE)', '፫'],
    u'\u136c' : ['(FOUR)', '፬'],
    u'\u136d' : ['(FIVE)', '፭'],
    u'\u136e' : ['(SIX)', '፮'],
    u'\u136f' : ['(SEVEN)', '፯'],
    u'\u1370' : ['(EIGHT)', '፰'],
    u'\u1371' : ['(NINE)', '፱'],
    u'\u1372' : ['(TEN)', '፲'],
    u'\u1373' : ['(TWENTY)', '፳'],
    u'\u1374' : ['(THIRTY)', '፴'],
    u'\u1375' : ['(FORTY)', '፵'],
    u'\u1376' : ['(FIFTY)', '፶'],
    u'\u1377' : ['(SIXTY)', '፷'],
    u'\u1378' : ['(SEVENTY)', '፸'],
    u'\u1379' : ['(EIGHT)', '፹'],
    u'\u137a' : ['(NINETY)', '፺'],
    u'\u137b' : ['(HUNDRED)', '፻'],
    u'\u137c' : ['(TEN THOUSAND)', '፼'],
    u'\u1380' : ['mw 3', 'ᎀ'],
    u'\u1381' : ['mw i', 'ᎁ'],
    u'\u1382' : ['mw 3', 'ᎂ'],
    u'\u1383' : ['mw @', 'ᎃ'],
    u'\u1384' : ['bw 3', 'ᎄ'],
    u'\u1385' : ['bw i', 'ᎅ'],
    u'\u1386' : ['bw e', 'ᎆ'],
    u'\u1387' : ['bw @', 'ᎇ'],
    u'\u1388' : ['fw 3', 'ᎈ'],
    u'\u1389' : ['fw i', 'ᎉ'],
    u'\u138a' : ['fw e', 'ᎊ'],
    u'\u138b' : ['fw @', 'ᎋ'],
    u'\u138c' : ['pw 3', 'ᎌ'],
    u'\u138d' : ['pw i', 'ᎍ'],
    u'\u138e' : ['pw e', 'ᎎ'],
    u'\u138f' : ['pw @', 'ᎏ'],
    u'\u1390' : ['(YIZET)', '᎐'],
    u'\u1391' : ['(DERET)', '᎑'],
    u'\u1392' : ['(RIKRIK)', '᎒'],
    u'\u1393' : ['(SHORT RIKRIK)', '᎓'],
    u'\u1394' : ['(DIFAT)', '᎔'],
    u'\u1395' : ['(KENAT)', '᎕'],
    u'\u1396' : ['(CHIRET)', '᎖'],
    u'\u1397' : ['(HIDET)', '᎗'],
    u'\u1398' : ['(DERET-HIDET)', '᎘'],
    u'\u1399' : ['(HURT)', '᎙'],
    u'\u13a0' : ['A', 'Ꭰ'],
    u'\u13a1' : ['e', 'Ꭱ'],
    u'\u13a2' : ['i', 'Ꭲ'],
    u'\u13a3' : ['o', 'Ꭳ'],
    u'\u13a4' : ['u', 'Ꭴ'],
    u'\u13a5' : ['@_~', 'Ꭵ'],
    u'\u13a6' : ['k A', 'Ꭶ'],
    u'\u13a7' : ['k_h A', 'Ꭷ'],
    u'\u13a8' : ['k e', 'Ꭸ'],
    u'\u13a9' : ['k i', 'Ꭹ'],
    u'\u13aa' : ['k o', 'Ꭺ'],
    u'\u13ab' : ['k u', 'Ꭻ'],
    u'\u13ac' : ['k @_~', 'Ꭼ'],
    u'\u13ad' : ['_h A', 'Ꭽ'],
    u'\u13ae' : ['_h e', 'Ꭾ'],
    u'\u13af' : ['_h i', 'Ꭿ'],
    u'\u13b0' : ['_h o', 'Ꮀ'],
    u'\u13b1' : ['_h u', 'Ꮁ'],
    u'\u13b2' : ['_h @_~', 'Ꮂ'],
    u'\u13b3' : ['_l A', 'Ꮃ'],
    u'\u13b4' : ['_l e', 'Ꮄ'],
    u'\u13b5' : ['_l i', 'Ꮅ'],
    u'\u13b6' : ['_l o', 'Ꮆ'],
    u'\u13b7' : ['_l u', 'Ꮇ'],
    u'\u13b8' : ['_l @_~', 'Ꮈ'],
    u'\u13b9' : ['m A', 'Ꮉ'],
    u'\u13ba' : ['m e', 'Ꮊ'],
    u'\u13bb' : ['m i', 'Ꮋ'],
    u'\u13bc' : ['m o', 'Ꮌ'],
    u'\u13bd' : ['m u', 'Ꮍ'],
    u'\u13be' : ['_n A', 'Ꮎ'],
    u'\u13bf' : ['_h _n A', 'Ꮏ'],
    u'\u13c0' : ['nh A', 'Ꮐ'],
    u'\u13c1' : ['_n e', 'Ꮑ'],
    u'\u13c2' : ['_n i', 'Ꮒ'],
    u'\u13c3' : ['_n o', 'Ꮓ'],
    u'\u13c4' : ['_n u', 'Ꮔ'],
    u'\u13c5' : ['_n @_~', 'Ꮕ'],
    u'\u13c6' : ['kw A', 'Ꮖ'],
    u'\u13c7' : ['kw e', 'Ꮗ'],
    u'\u13c8' : ['kw i', 'Ꮘ'],
    u'\u13c9' : ['kw o', 'Ꮙ'],
    u'\u13ca' : ['kw u', 'Ꮚ'],
    u'\u13cb' : ['kw @_~', 'Ꮛ'],
    u'\u13cc' : ['s A', 'Ꮜ'],
    u'\u13cd' : ['s', 'Ꮝ'],
    u'\u13ce' : ['s e', 'Ꮞ'],
    u'\u13cf' : ['s i', 'Ꮟ'],
    u'\u13d0' : ['s o', 'Ꮠ'],
    u'\u13d1' : ['s u', 'Ꮡ'],
    u'\u13d2' : ['s @_~', 'Ꮢ'],
    u'\u13d3' : ['t A', 'Ꮣ'],
    u'\u13d4' : ['t_h A', 'Ꮤ'],
    u'\u13d5' : ['t e', 'Ꮥ'],
    u'\u13d6' : ['t_h e', 'Ꮦ'],
    u'\u13d7' : ['t i', 'Ꮧ'],
    u'\u13d8' : ['t_h i', 'Ꮨ'],
    u'\u13d9' : ['t o', 'Ꮩ'],
    u'\u13da' : ['t u', 'Ꮪ'],
    u'\u13db' : ['t @_~', 'Ꮫ'],
    u'\u13dc' : ['tl A', 'Ꮬ'],
    u'\u13dd' : ['thl A', 'Ꮭ'],
    u'\u13de' : ['thl e', 'Ꮮ'],
    u'\u13df' : ['thl i', 'Ꮯ'],
    u'\u13e0' : ['thl o', 'Ꮰ'],
    u'\u13e1' : ['thl u', 'Ꮱ'],
    u'\u13e2' : ['thl @_~', 'Ꮲ'],
    u'\u13e3' : ['ts A', 'Ꮳ'],
    u'\u13e4' : ['ts e', 'Ꮴ'],
    u'\u13e5' : ['ts i', 'Ꮵ'],
    u'\u13e6' : ['ts o', 'Ꮶ'],
    u'\u13e7' : ['ts u', 'Ꮷ'],
    u'\u13e8' : ['ts @_~', 'Ꮸ'],
    u'\u13e9' : ['_w A', 'Ꮹ'],
    u'\u13ea' : ['_w e', 'Ꮺ'],
    u'\u13eb' : ['_w i', 'Ꮻ'],
    u'\u13ec' : ['_w o', 'Ꮼ'],
    u'\u13ed' : ['_w u', 'Ꮽ'],
    u'\u13ee' : ['_w @_~', 'Ꮾ'],
    u'\u13ef' : ['_j A', 'Ꮿ'],
    u'\u13f0' : ['_j e', 'Ᏸ'],
    u'\u13f1' : ['_j i', 'Ᏹ'],
    u'\u13f2' : ['_j o', 'Ᏺ'],
    u'\u13f3' : ['_j u', 'Ᏻ'],
    u'\u13f4' : ['_j @_~', 'Ᏼ'],
    u'\u1401' : ['aI', 'ᐁ'],
    u'\u1402' : ['A: i', 'ᐂ'],
    u'\u1403' : ['i', 'ᐃ'],
    u'\u1404' : ['i:', 'ᐄ'],
    u'\u1405' : ['u', 'ᐅ'],
    u'\u1406' : ['u:', 'ᐆ'],
    u'\u1407' : ['o:', 'ᐇ'],
    u'\u1408' : ['e:', 'ᐈ'],
    u'\u1409' : ['i', 'ᐉ'],
    u'\u140a' : ['A', 'ᐊ'],
    u'\u140b' : ['A:', 'ᐋ'],
    u'\u140c' : ['_w e', 'ᐌ'],
    u'\u140d' : ['_w e', 'ᐍ'],
    u'\u140e' : ['_w i', 'ᐎ'],
    u'\u140f' : ['_w i', 'ᐏ'],
    u'\u1410' : ['_w i:', 'ᐐ'],
    u'\u1411' : ['_w i:', 'ᐑ'],
    u'\u1412' : ['_w o', 'ᐒ'],
    u'\u1413' : ['_w o', 'ᐓ'],
    u'\u1414' : ['_w o:', 'ᐔ'],
    u'\u1415' : ['_w o:', 'ᐕ'],
    u'\u1416' : ['_w o:', 'ᐖ'],
    u'\u1417' : ['_w A', 'ᐗ'],
    u'\u1418' : ['_w A', 'ᐘ'],
    u'\u1419' : ['_w A:', 'ᐙ'],
    u'\u141a' : ['_w A:', 'ᐚ'],
    u'\u141b' : ['_w A:', 'ᐛ'],
    u'\u141c' : ['A:', 'ᐜ'],
    u'\u141d' : ['_w', 'ᐝ'],
    u'\u141e' : ['_k', 'ᐞ'],
    u'\u141f' : ['_k', 'ᐟ'],
    u'\u1420' : ['k', 'ᐠ'],
    u'\u1421' : ['S', 'ᐡ'],
    u'\u1422' : ['s', 'ᐢ'],
    u'\u1423' : ['_n', 'ᐣ'],
    u'\u1424' : ['_w', 'ᐤ'],
    u'\u1425' : ['t t', 'ᐥ'],
    u'\u1426' : ['_h', 'ᐦ'],
    u'\u1427' : ['_w', 'ᐧ'],
    u'\u1428' : ['G', 'ᐨ'],
    u'\u1429' : ['_n', 'ᐩ'],
    u'\u142a' : ['_l', 'ᐪ'],
    u'\u142b' : ['e _n', 'ᐫ'],
    u'\u142c' : ['i _n', 'ᐬ'],
    u'\u142d' : ['o _n', 'ᐭ'],
    u'\u142e' : ['a _n', 'ᐮ'],
    u'\u142f' : ['p aI', 'ᐯ'],
    u'\u1430' : ['p A: i', 'ᐰ'],
    u'\u1431' : ['p i', 'ᐱ'],
    u'\u1432' : ['p i:', 'ᐲ'],
    u'\u1433' : ['p u', 'ᐳ'],
    u'\u1434' : ['p u:', 'ᐴ'],
    u'\u1435' : ['p o:', 'ᐵ'],
    u'\u1436' : ['_h e:', 'ᐶ'],
    u'\u1437' : ['_h i', 'ᐷ'],
    u'\u1438' : ['p A', 'ᐸ'],
    u'\u1439' : ['p A:', 'ᐹ'],
    u'\u143a' : ['pw e', 'ᐺ'],
    u'\u143b' : ['pw e', 'ᐻ'],
    u'\u143c' : ['pw i', 'ᐼ'],
    u'\u143d' : ['pw i', 'ᐽ'],
    u'\u143e' : ['pw i:', 'ᐾ'],
    u'\u143f' : ['pw i:', 'ᐿ'],
    u'\u1440' : ['pw o', 'ᑀ'],
    u'\u1441' : ['pw o', 'ᑁ'],
    u'\u1442' : ['pw o:', 'ᑂ'],
    u'\u1443' : ['pw o:', 'ᑃ'],
    u'\u1444' : ['pw A', 'ᑄ'],
    u'\u1445' : ['pw A', 'ᑅ'],
    u'\u1446' : ['pw A:', 'ᑆ'],
    u'\u1447' : ['pw A:', 'ᑇ'],
    u'\u1448' : ['pw A:', 'ᑈ'],
    u'\u1449' : ['p', 'ᑉ'],
    u'\u144a' : ['_h', 'ᑊ'],
    u'\u144b' : ['t aI', 'ᑋ'],
    u'\u144c' : ['t A: i', 'ᑌ'],
    u'\u144d' : ['t i', 'ᑍ'],
    u'\u144e' : ['t i', 'ᑎ'],
    u'\u144f' : ['t i:', 'ᑏ'],
    u'\u1450' : ['t u', 'ᑐ'],
    u'\u1451' : ['t u:', 'ᑑ'],
    u'\u1452' : ['t o:', 'ᑒ'],
    u'\u1453' : ['d e:', 'ᑓ'],
    u'\u1454' : ['d i', 'ᑔ'],
    u'\u1455' : ['t A', 'ᑕ'],
    u'\u1456' : ['t A:', 'ᑖ'],
    u'\u1457' : ['tw e', 'ᑗ'],
    u'\u1458' : ['tw i', 'ᑘ'],
    u'\u1459' : ['tw i', 'ᑙ'],
    u'\u145a' : ['tw i:', 'ᑚ'],
    u'\u145b' : ['tw i:', 'ᑛ'],
    u'\u145c' : ['tw i:', 'ᑜ'],
    u'\u145d' : ['tw o', 'ᑝ'],
    u'\u145e' : ['tw o', 'ᑞ'],
    u'\u145f' : ['tw o:', 'ᑟ'],
    u'\u1460' : ['tw o:', 'ᑠ'],
    u'\u1461' : ['tw A', 'ᑡ'],
    u'\u1462' : ['tw A', 'ᑢ'],
    u'\u1463' : ['tw A:', 'ᑣ'],
    u'\u1464' : ['tw A:', 'ᑤ'],
    u'\u1465' : ['tw A:', 'ᑥ'],
    u'\u1466' : ['t', 'ᑦ'],
    u'\u1467' : ['t t e', 'ᑧ'],
    u'\u1468' : ['t t i', 'ᑨ'],
    u'\u1469' : ['t t o', 'ᑩ'],
    u'\u146a' : ['t t A', 'ᑪ'],
    u'\u146b' : ['k aI', 'ᑫ'],
    u'\u146c' : ['k A: i', 'ᑬ'],
    u'\u146d' : ['k i', 'ᑭ'],
    u'\u146e' : ['k i:', 'ᑮ'],
    u'\u146f' : ['k u', 'ᑯ'],
    u'\u1470' : ['k u:', 'ᑰ'],
    u'\u1471' : ['k o:', 'ᑱ'],
    u'\u1472' : ['k A', 'ᑲ'],
    u'\u1473' : ['k A:', 'ᑳ'],
    u'\u1474' : ['kw e', 'ᑴ'],
    u'\u1475' : ['kw e', 'ᑵ'],
    u'\u1476' : ['kw i', 'ᑶ'],
    u'\u1477' : ['kw i', 'ᑷ'],
    u'\u1478' : ['kw i:', 'ᑸ'],
    u'\u1479' : ['kw i:', 'ᑹ'],
    u'\u147a' : ['kw o', 'ᑺ'],
    u'\u147b' : ['kw o', 'ᑻ'],
    u'\u147c' : ['kw o:', 'ᑼ'],
    u'\u147d' : ['kw o:', 'ᑽ'],
    u'\u147e' : ['kw A', 'ᑾ'],
    u'\u147f' : ['kw A', 'ᑿ'],
    u'\u1480' : ['kw A', 'ᒀ'],
    u'\u1481' : ['kw A:', 'ᒁ'],
    u'\u1482' : ['kw A:', 'ᒂ'],
    u'\u1483' : ['k', 'ᒃ'],
    u'\u1484' : ['kw', 'ᒄ'],
    u'\u1485' : ['k eh', 'ᒅ'],
    u'\u1486' : ['k ih', 'ᒆ'],
    u'\u1487' : ['k oh', 'ᒇ'],
    u'\u1488' : ['k ah', 'ᒈ'],
    u'\u1489' : ['G aI', 'ᒉ'],
    u'\u148a' : ['G A: i', 'ᒊ'],
    u'\u148b' : ['G i', 'ᒋ'],
    u'\u148c' : ['G i:', 'ᒌ'],
    u'\u148d' : ['G u', 'ᒍ'],
    u'\u148e' : ['G u:', 'ᒎ'],
    u'\u148f' : ['G o:', 'ᒏ'],
    u'\u1490' : ['G A', 'ᒐ'],
    u'\u1491' : ['G A:', 'ᒑ'],
    u'\u1492' : ['Gw e', 'ᒒ'],
    u'\u1493' : ['Gw e', 'ᒓ'],
    u'\u1494' : ['Gw i', 'ᒔ'],
    u'\u1495' : ['Gw i', 'ᒕ'],
    u'\u1496' : ['Gw i:', 'ᒖ'],
    u'\u1497' : ['Gw i:', 'ᒗ'],
    u'\u1498' : ['Gw o', 'ᒘ'],
    u'\u1499' : ['Gw o', 'ᒙ'],
    u'\u149a' : ['Gw o:', 'ᒚ'],
    u'\u149b' : ['Gw o:', 'ᒛ'],
    u'\u149c' : ['Gw A', 'ᒜ'],
    u'\u149d' : ['Gw A', 'ᒝ'],
    u'\u149e' : ['Gw A:', 'ᒞ'],
    u'\u149f' : ['Gw A:', 'ᒟ'],
    u'\u14a0' : ['Gw A:', 'ᒠ'],
    u'\u14a1' : ['G', 'ᒡ'],
    u'\u14a2' : ['D', 'ᒢ'],
    u'\u14a3' : ['m aI', 'ᒣ'],
    u'\u14a4' : ['m A: i', 'ᒤ'],
    u'\u14a5' : ['m i', 'ᒥ'],
    u'\u14a6' : ['m i:', 'ᒦ'],
    u'\u14a7' : ['m u', 'ᒧ'],
    u'\u14a8' : ['m u:', 'ᒨ'],
    u'\u14a9' : ['m o:', 'ᒩ'],
    u'\u14aa' : ['m A', 'ᒪ'],
    u'\u14ab' : ['m A:', 'ᒫ'],
    u'\u14ac' : ['mw e', 'ᒬ'],
    u'\u14ad' : ['mw e', 'ᒭ'],
    u'\u14ae' : ['mw i', 'ᒮ'],
    u'\u14af' : ['mw i', 'ᒯ'],
    u'\u14b0' : ['mw i:', 'ᒰ'],
    u'\u14b1' : ['mw i:', 'ᒱ'],
    u'\u14b2' : ['mw o', 'ᒲ'],
    u'\u14b3' : ['mw o', 'ᒳ'],
    u'\u14b4' : ['mw o:', 'ᒴ'],
    u'\u14b5' : ['mw o:', 'ᒵ'],
    u'\u14b6' : ['mw A', 'ᒶ'],
    u'\u14b7' : ['mw A', 'ᒷ'],
    u'\u14b8' : ['mw A:', 'ᒸ'],
    u'\u14b9' : ['mw A:', 'ᒹ'],
    u'\u14ba' : ['mw A:', 'ᒺ'],
    u'\u14bb' : ['m', 'ᒻ'],
    u'\u14bc' : ['m', 'ᒼ'],
    u'\u14bd' : ['mh', 'ᒽ'],
    u'\u14be' : ['m', 'ᒾ'],
    u'\u14bf' : ['m', 'ᒿ'],
    u'\u14c0' : ['_n aI', 'ᓀ'],
    u'\u14c1' : ['_n A: i', 'ᓁ'],
    u'\u14c2' : ['_n i', 'ᓂ'],
    u'\u14c3' : ['_n i:', 'ᓃ'],
    u'\u14c4' : ['_n u', 'ᓄ'],
    u'\u14c5' : ['_n u:', 'ᓅ'],
    u'\u14c6' : ['_n o:', 'ᓆ'],
    u'\u14c7' : ['_n A', 'ᓇ'],
    u'\u14c8' : ['_n A:', 'ᓈ'],
    u'\u14c9' : ['nw e', 'ᓉ'],
    u'\u14ca' : ['nw e', 'ᓊ'],
    u'\u14cb' : ['nw A', 'ᓋ'],
    u'\u14cc' : ['nw A', 'ᓌ'],
    u'\u14cd' : ['nw A:', 'ᓍ'],
    u'\u14ce' : ['nw A:', 'ᓎ'],
    u'\u14cf' : ['nw A:', 'ᓏ'],
    u'\u14d0' : ['_n', 'ᓐ'],
    u'\u14d1' : ['N', 'ᓑ'],
    u'\u14d2' : ['nh', 'ᓒ'],
    u'\u14d3' : ['_l aI', 'ᓓ'],
    u'\u14d4' : ['_l A: i', 'ᓔ'],
    u'\u14d5' : ['_l i', 'ᓕ'],
    u'\u14d6' : ['_l i:', 'ᓖ'],
    u'\u14d7' : ['_l u', 'ᓗ'],
    u'\u14d8' : ['_l u:', 'ᓘ'],
    u'\u14d9' : ['_l o:', 'ᓙ'],
    u'\u14da' : ['_l A', 'ᓚ'],
    u'\u14db' : ['_l A:', 'ᓛ'],
    u'\u14dc' : ['lw e', 'ᓜ'],
    u'\u14dd' : ['lw e', 'ᓝ'],
    u'\u14de' : ['lw i', 'ᓞ'],
    u'\u14df' : ['lw i', 'ᓟ'],
    u'\u14e0' : ['lw i:', 'ᓠ'],
    u'\u14e1' : ['lw i:', 'ᓡ'],
    u'\u14e2' : ['lw o', 'ᓢ'],
    u'\u14e3' : ['lw o', 'ᓣ'],
    u'\u14e4' : ['lw o:', 'ᓤ'],
    u'\u14e5' : ['lw o:', 'ᓥ'],
    u'\u14e6' : ['lw A', 'ᓦ'],
    u'\u14e7' : ['lw A', 'ᓧ'],
    u'\u14e8' : ['lw A:', 'ᓨ'],
    u'\u14e9' : ['lw A:', 'ᓩ'],
    u'\u14ea' : ['_l', 'ᓪ'],
    u'\u14eb' : ['_l', 'ᓫ'],
    u'\u14ec' : ['_l', 'ᓬ'],
    u'\u14ed' : ['s aI', 'ᓭ'],
    u'\u14ee' : ['s A: i', 'ᓮ'],
    u'\u14ef' : ['s i', 'ᓯ'],
    u'\u14f0' : ['s i:', 'ᓰ'],
    u'\u14f1' : ['s u', 'ᓱ'],
    u'\u14f2' : ['s u:', 'ᓲ'],
    u'\u14f3' : ['s o:', 'ᓳ'],
    u'\u14f4' : ['s A', 'ᓴ'],
    u'\u14f5' : ['s A:', 'ᓵ'],
    u'\u14f6' : ['sw e', 'ᓶ'],
    u'\u14f7' : ['sw e', 'ᓷ'],
    u'\u14f8' : ['sw i', 'ᓸ'],
    u'\u14f9' : ['sw i', 'ᓹ'],
    u'\u14fa' : ['sw i:', 'ᓺ'],
    u'\u14fb' : ['sw i:', 'ᓻ'],
    u'\u14fc' : ['sw o', 'ᓼ'],
    u'\u14fd' : ['sw o', 'ᓽ'],
    u'\u14fe' : ['sw o:', 'ᓾ'],
    u'\u14ff' : ['sw o:', 'ᓿ'],
    u'\u1500' : ['sw A', 'ᔀ'],
    u'\u1501' : ['sw A', 'ᔁ'],
    u'\u1502' : ['sw A:', 'ᔂ'],
    u'\u1503' : ['sw A:', 'ᔃ'],
    u'\u1504' : ['sw A:', 'ᔄ'],
    u'\u1505' : ['s', 'ᔅ'],
    u'\u1506' : ['s', 'ᔆ'],
    u'\u1507' : ['sw', 'ᔇ'],
    u'\u1508' : ['s', 'ᔈ'],
    u'\u1509' : ['s k', 'ᔉ'],
    u'\u150a' : ['s kw', 'ᔊ'],
    u'\u150b' : ['s _w', 'ᔋ'],
    u'\u150c' : ['s pw A', 'ᔌ'],
    u'\u150d' : ['s tw A', 'ᔍ'],
    u'\u150e' : ['s kw A', 'ᔎ'],
    u'\u150f' : ['s cw A', 'ᔏ'],
    u'\u1510' : ['S e', 'ᔐ'],
    u'\u1511' : ['S i', 'ᔑ'],
    u'\u1512' : ['S i:', 'ᔒ'],
    u'\u1513' : ['S o', 'ᔓ'],
    u'\u1514' : ['S o:', 'ᔔ'],
    u'\u1515' : ['S A', 'ᔕ'],
    u'\u1516' : ['S A:', 'ᔖ'],
    u'\u1517' : ['Sw e', 'ᔗ'],
    u'\u1518' : ['Sw e', 'ᔘ'],
    u'\u1519' : ['Sw i', 'ᔙ'],
    u'\u151a' : ['Sw i', 'ᔚ'],
    u'\u151b' : ['Sw i:', 'ᔛ'],
    u'\u151c' : ['Sw i:', 'ᔜ'],
    u'\u151d' : ['Sw o', 'ᔝ'],
    u'\u151e' : ['Sw o', 'ᔞ'],
    u'\u151f' : ['Sw o:', 'ᔟ'],
    u'\u1520' : ['Sw o:', 'ᔠ'],
    u'\u1521' : ['Sw A', 'ᔡ'],
    u'\u1522' : ['Sw A', 'ᔢ'],
    u'\u1523' : ['Sw A:', 'ᔣ'],
    u'\u1524' : ['Sw A:', 'ᔤ'],
    u'\u1525' : ['S', 'ᔥ'],
    u'\u1526' : ['_j aI', 'ᔦ'],
    u'\u1527' : ['_j A: i', 'ᔧ'],
    u'\u1528' : ['_j i', 'ᔨ'],
    u'\u1529' : ['_j i:', 'ᔩ'],
    u'\u152a' : ['_j u', 'ᔪ'],
    u'\u152b' : ['_j u:', 'ᔫ'],
    u'\u152c' : ['_j o:', 'ᔬ'],
    u'\u152d' : ['_j A', 'ᔭ'],
    u'\u152e' : ['_j A:', 'ᔮ'],
    u'\u152f' : ['H e', 'ᔯ'],
    u'\u1530' : ['H e', 'ᔰ'],
    u'\u1531' : ['H i', 'ᔱ'],
    u'\u1532' : ['H i', 'ᔲ'],
    u'\u1533' : ['H i:', 'ᔳ'],
    u'\u1534' : ['H i:', 'ᔴ'],
    u'\u1535' : ['H o', 'ᔵ'],
    u'\u1536' : ['H o', 'ᔶ'],
    u'\u1537' : ['H o:', 'ᔷ'],
    u'\u1538' : ['H o:', 'ᔸ'],
    u'\u1539' : ['H A', 'ᔹ'],
    u'\u153a' : ['H A', 'ᔺ'],
    u'\u153b' : ['H A:', 'ᔻ'],
    u'\u153c' : ['H A:', 'ᔼ'],
    u'\u153d' : ['H A:', 'ᔽ'],
    u'\u153e' : ['_j', 'ᔾ'],
    u'\u153f' : ['_j', 'ᔿ'],
    u'\u1540' : ['_j', 'ᕀ'],
    u'\u1541' : ['_j i', 'ᕁ'],
    u'\u1542' : ['` aI', 'ᕂ'],
    u'\u1543' : ['` e', 'ᕃ'],
    u'\u1544' : ['_l e', 'ᕄ'],
    u'\u1545' : ['` A: i', 'ᕅ'],
    u'\u1546' : ['` i', 'ᕆ'],
    u'\u1547' : ['` i:', 'ᕇ'],
    u'\u1548' : ['` u', 'ᕈ'],
    u'\u1549' : ['` u:', 'ᕉ'],
    u'\u154a' : ['_l o', 'ᕊ'],
    u'\u154b' : ['` A', 'ᕋ'],
    u'\u154c' : ['` A:', 'ᕌ'],
    u'\u154d' : ['_l a', 'ᕍ'],
    u'\u154e' : ['rw A:', 'ᕎ'],
    u'\u154f' : ['rw A:', 'ᕏ'],
    u'\u1550' : ['`', 'ᕐ'],
    u'\u1551' : ['`', 'ᕑ'],
    u'\u1552' : ['`', 'ᕒ'],
    u'\u1553' : ['_v aI', 'ᕓ'],
    u'\u1554' : ['_v A: i', 'ᕔ'],
    u'\u1555' : ['_v i', 'ᕕ'],
    u'\u1556' : ['_v i:', 'ᕖ'],
    u'\u1557' : ['_v o', 'ᕗ'],
    u'\u1558' : ['_v o:', 'ᕘ'],
    u'\u1559' : ['_v A', 'ᕙ'],
    u'\u155a' : ['_v A:', 'ᕚ'],
    u'\u155b' : ['vw A:', 'ᕛ'],
    u'\u155c' : ['vw A:', 'ᕜ'],
    u'\u155d' : ['_v', 'ᕝ'],
    u'\u155e' : ['D e', 'ᕞ'],
    u'\u155f' : ['D e', 'ᕟ'],
    u'\u1560' : ['D i', 'ᕠ'],
    u'\u1561' : ['D i', 'ᕡ'],
    u'\u1562' : ['D i:', 'ᕢ'],
    u'\u1563' : ['D i:', 'ᕣ'],
    u'\u1564' : ['D o', 'ᕤ'],
    u'\u1565' : ['D o:', 'ᕥ'],
    u'\u1566' : ['D A', 'ᕦ'],
    u'\u1567' : ['D A:', 'ᕧ'],
    u'\u1568' : ['Dw A:', 'ᕨ'],
    u'\u1569' : ['Dw A:', 'ᕩ'],
    u'\u156a' : ['D', 'ᕪ'],
    u'\u156b' : ['D e', 'ᕫ'],
    u'\u156c' : ['D i', 'ᕬ'],
    u'\u156d' : ['D o', 'ᕭ'],
    u'\u156e' : ['D A', 'ᕮ'],
    u'\u156f' : ['D', 'ᕯ'],
    u'\u1570' : ['tj e', 'ᕰ'],
    u'\u1571' : ['tj i', 'ᕱ'],
    u'\u1572' : ['tj o', 'ᕲ'],
    u'\u1573' : ['tj A', 'ᕳ'],
    u'\u1574' : ['_h e', 'ᕴ'],
    u'\u1575' : ['_h i', 'ᕵ'],
    u'\u1576' : ['_h i:', 'ᕶ'],
    u'\u1577' : ['_h o', 'ᕷ'],
    u'\u1578' : ['_h o:', 'ᕸ'],
    u'\u1579' : ['_h A', 'ᕹ'],
    u'\u157a' : ['_h A:', 'ᕺ'],
    u'\u157b' : ['_h', 'ᕻ'],
    u'\u157c' : ['_h', 'ᕼ'],
    u'\u157d' : ['_h k', 'ᕽ'],
    u'\u157e' : ['q A: i', 'ᕾ'],
    u'\u157f' : ['q i', 'ᕿ'],
    u'\u1580' : ['q i:', 'ᖀ'],
    u'\u1581' : ['q u', 'ᖁ'],
    u'\u1582' : ['q u:', 'ᖂ'],
    u'\u1583' : ['q A', 'ᖃ'],
    u'\u1584' : ['q A:', 'ᖄ'],
    u'\u1585' : ['q', 'ᖅ'],
    u'\u1586' : ['t_h _l e', 'ᖆ'],
    u'\u1587' : ['t_h _l i', 'ᖇ'],
    u'\u1588' : ['t_h _l o', 'ᖈ'],
    u'\u1589' : ['t_h _l A', 'ᖉ'],
    u'\u158a' : ['` e', 'ᖊ'],
    u'\u158b' : ['` i', 'ᖋ'],
    u'\u158c' : ['` o', 'ᖌ'],
    u'\u158d' : ['` A', 'ᖍ'],
    u'\u158e' : ['N aI', 'ᖎ'],
    u'\u158f' : ['N i', 'ᖏ'],
    u'\u1590' : ['N i:', 'ᖐ'],
    u'\u1591' : ['N u', 'ᖑ'],
    u'\u1592' : ['N u:', 'ᖒ'],
    u'\u1593' : ['N A', 'ᖓ'],
    u'\u1594' : ['N A:', 'ᖔ'],
    u'\u1595' : ['N', 'ᖕ'],
    u'\u1596' : ['N', 'ᖖ'],
    u'\u1597' : ['S e', 'ᖗ'],
    u'\u1598' : ['S i', 'ᖘ'],
    u'\u1599' : ['S o', 'ᖙ'],
    u'\u159a' : ['S A', 'ᖚ'],
    u'\u159b' : ['D e', 'ᖛ'],
    u'\u159c' : ['D i', 'ᖜ'],
    u'\u159d' : ['D o', 'ᖝ'],
    u'\u159e' : ['D A', 'ᖞ'],
    u'\u159f' : ['D', 'ᖟ'],
    u'\u15a0' : ['K i', 'ᖠ'],
    u'\u15a1' : ['K i:', 'ᖡ'],
    u'\u15a2' : ['K u', 'ᖢ'],
    u'\u15a3' : ['K u:', 'ᖣ'],
    u'\u15a4' : ['K A', 'ᖤ'],
    u'\u15a5' : ['K A:', 'ᖥ'],
    u'\u15a6' : ['K', 'ᖦ'],
    u'\u15a7' : ['D e', 'ᖧ'],
    u'\u15a8' : ['D i', 'ᖨ'],
    u'\u15a9' : ['D i:', 'ᖩ'],
    u'\u15aa' : ['D o', 'ᖪ'],
    u'\u15ab' : ['D o:', 'ᖫ'],
    u'\u15ac' : ['D A', 'ᖬ'],
    u'\u15ad' : ['D A:', 'ᖭ'],
    u'\u15ae' : ['D', 'ᖮ'],
    u'\u15af' : ['b', 'ᖯ'],
    u'\u15b0' : ['e', 'ᖰ'],
    u'\u15b1' : ['i', 'ᖱ'],
    u'\u15b2' : ['o', 'ᖲ'],
    u'\u15b3' : ['A', 'ᖳ'],
    u'\u15b4' : ['_w e', 'ᖴ'],
    u'\u15b5' : ['_w i', 'ᖵ'],
    u'\u15b6' : ['_w o', 'ᖶ'],
    u'\u15b7' : ['_w A', 'ᖷ'],
    u'\u15b8' : ['_n e', 'ᖸ'],
    u'\u15b9' : ['_n i', 'ᖹ'],
    u'\u15ba' : ['_n o', 'ᖺ'],
    u'\u15bb' : ['_n A', 'ᖻ'],
    u'\u15bc' : ['k e', 'ᖼ'],
    u'\u15bd' : ['k i', 'ᖽ'],
    u'\u15be' : ['k o', 'ᖾ'],
    u'\u15bf' : ['k A', 'ᖿ'],
    u'\u15c0' : ['_h e', 'ᗀ'],
    u'\u15c1' : ['_h i', 'ᗁ'],
    u'\u15c2' : ['_h o', 'ᗂ'],
    u'\u15c3' : ['_h A', 'ᗃ'],
    u'\u15c4' : ['Gh u', 'ᗄ'],
    u'\u15c5' : ['Gh o', 'ᗅ'],
    u'\u15c6' : ['Gh e', 'ᗆ'],
    u'\u15c7' : ['Gh e:', 'ᗇ'],
    u'\u15c8' : ['Gh i', 'ᗈ'],
    u'\u15c9' : ['Gh A', 'ᗉ'],
    u'\u15ca' : ['` u', 'ᗊ'],
    u'\u15cb' : ['` o', 'ᗋ'],
    u'\u15cc' : ['` e', 'ᗌ'],
    u'\u15cd' : ['` e:', 'ᗍ'],
    u'\u15ce' : ['` i', 'ᗎ'],
    u'\u15cf' : ['` A', 'ᗏ'],
    u'\u15d0' : ['_w u', 'ᗐ'],
    u'\u15d1' : ['_w o', 'ᗑ'],
    u'\u15d2' : ['_w e', 'ᗒ'],
    u'\u15d3' : ['_w e:', 'ᗓ'],
    u'\u15d4' : ['_w i', 'ᗔ'],
    u'\u15d5' : ['_w A', 'ᗕ'],
    u'\u15d6' : ['hw u', 'ᗖ'],
    u'\u15d7' : ['hw o', 'ᗗ'],
    u'\u15d8' : ['hw e', 'ᗘ'],
    u'\u15d9' : ['hw e:', 'ᗙ'],
    u'\u15da' : ['hw i', 'ᗚ'],
    u'\u15db' : ['hw A', 'ᗛ'],
    u'\u15dc' : ['D u', 'ᗜ'],
    u'\u15dd' : ['D o', 'ᗝ'],
    u'\u15de' : ['D e', 'ᗞ'],
    u'\u15df' : ['D e:', 'ᗟ'],
    u'\u15e0' : ['D i', 'ᗠ'],
    u'\u15e1' : ['D A', 'ᗡ'],
    u'\u15e2' : ['t t u', 'ᗢ'],
    u'\u15e3' : ['t t o', 'ᗣ'],
    u'\u15e4' : ['t t e', 'ᗤ'],
    u'\u15e5' : ['t t e:', 'ᗥ'],
    u'\u15e6' : ['t t i', 'ᗦ'],
    u'\u15e7' : ['t t A', 'ᗧ'],
    u'\u15e8' : ['p u', 'ᗨ'],
    u'\u15e9' : ['p o', 'ᗩ'],
    u'\u15ea' : ['p e', 'ᗪ'],
    u'\u15eb' : ['p e:', 'ᗫ'],
    u'\u15ec' : ['p i', 'ᗬ'],
    u'\u15ed' : ['p A', 'ᗭ'],
    u'\u15ee' : ['p', 'ᗮ'],
    u'\u15ef' : ['G u', 'ᗯ'],
    u'\u15f0' : ['G o', 'ᗰ'],
    u'\u15f1' : ['G e', 'ᗱ'],
    u'\u15f2' : ['G e:', 'ᗲ'],
    u'\u15f3' : ['G i', 'ᗳ'],
    u'\u15f4' : ['G A', 'ᗴ'],
    u'\u15f5' : ['k_h u', 'ᗵ'],
    u'\u15f6' : ['k_h o', 'ᗶ'],
    u'\u15f7' : ['k_h e', 'ᗷ'],
    u'\u15f8' : ['k_h e:', 'ᗸ'],
    u'\u15f9' : ['k_h i', 'ᗹ'],
    u'\u15fa' : ['k_h A', 'ᗺ'],
    u'\u15fb' : ['k k u', 'ᗻ'],
    u'\u15fc' : ['k k o', 'ᗼ'],
    u'\u15fd' : ['k k e', 'ᗽ'],
    u'\u15fe' : ['k k e:', 'ᗾ'],
    u'\u15ff' : ['k k i', 'ᗿ'],
    u'\u1600' : ['k k A', 'ᘀ'],
    u'\u1601' : ['k k', 'ᘁ'],
    u'\u1602' : ['_n u', 'ᘂ'],
    u'\u1603' : ['_n o', 'ᘃ'],
    u'\u1604' : ['_n e', 'ᘄ'],
    u'\u1605' : ['_n e:', 'ᘅ'],
    u'\u1606' : ['_n i', 'ᘆ'],
    u'\u1607' : ['_n A', 'ᘇ'],
    u'\u1608' : ['m u', 'ᘈ'],
    u'\u1609' : ['m o', 'ᘉ'],
    u'\u160a' : ['m e', 'ᘊ'],
    u'\u160b' : ['m e:', 'ᘋ'],
    u'\u160c' : ['m i', 'ᘌ'],
    u'\u160d' : ['m A', 'ᘍ'],
    u'\u160e' : ['_j u', 'ᘎ'],
    u'\u160f' : ['_j o', 'ᘏ'],
    u'\u1610' : ['_j e', 'ᘐ'],
    u'\u1611' : ['_j e:', 'ᘑ'],
    u'\u1612' : ['_j i', 'ᘒ'],
    u'\u1613' : ['_j A', 'ᘓ'],
    u'\u1614' : ['z u', 'ᘔ'],
    u'\u1615' : ['z u', 'ᘕ'],
    u'\u1616' : ['z o', 'ᘖ'],
    u'\u1617' : ['z e', 'ᘗ'],
    u'\u1618' : ['z e:', 'ᘘ'],
    u'\u1619' : ['z i', 'ᘙ'],
    u'\u161a' : ['z i', 'ᘚ'],
    u'\u161b' : ['z A', 'ᘛ'],
    u'\u161c' : ['z z u', 'ᘜ'],
    u'\u161d' : ['z z o', 'ᘝ'],
    u'\u161e' : ['z z e', 'ᘞ'],
    u'\u161f' : ['z z e:', 'ᘟ'],
    u'\u1620' : ['z z i', 'ᘠ'],
    u'\u1621' : ['z z A', 'ᘡ'],
    u'\u1622' : ['_l u', 'ᘢ'],
    u'\u1623' : ['_l o', 'ᘣ'],
    u'\u1624' : ['_l e', 'ᘤ'],
    u'\u1625' : ['_l e:', 'ᘥ'],
    u'\u1626' : ['_l i', 'ᘦ'],
    u'\u1627' : ['_l A', 'ᘧ'],
    u'\u1628' : ['d _l u', 'ᘨ'],
    u'\u1629' : ['d _l o', 'ᘩ'],
    u'\u162a' : ['d _l e', 'ᘪ'],
    u'\u162b' : ['d _l e:', 'ᘫ'],
    u'\u162c' : ['d _l i', 'ᘬ'],
    u'\u162d' : ['d _l A', 'ᘭ'],
    u'\u162e' : ['K u', 'ᘮ'],
    u'\u162f' : ['K o', 'ᘯ'],
    u'\u1630' : ['K e', 'ᘰ'],
    u'\u1631' : ['K e:', 'ᘱ'],
    u'\u1632' : ['K i', 'ᘲ'],
    u'\u1633' : ['K A', 'ᘳ'],
    u'\u1634' : ['t_h _l u', 'ᘴ'],
    u'\u1635' : ['t_h _l o', 'ᘵ'],
    u'\u1636' : ['t_h _l e', 'ᘶ'],
    u'\u1637' : ['t_h _l e:', 'ᘷ'],
    u'\u1638' : ['t_h _l i', 'ᘸ'],
    u'\u1639' : ['t_h _l A', 'ᘹ'],
    u'\u163a' : ['t _l u', 'ᘺ'],
    u'\u163b' : ['t _l o', 'ᘻ'],
    u'\u163c' : ['t _l e', 'ᘼ'],
    u'\u163d' : ['t _l e:', 'ᘽ'],
    u'\u163e' : ['t _l i', 'ᘾ'],
    u'\u163f' : ['t _l A', 'ᘿ'],
    u'\u1640' : ['z u', 'ᙀ'],
    u'\u1641' : ['z o', 'ᙁ'],
    u'\u1642' : ['z e', 'ᙂ'],
    u'\u1643' : ['z e:', 'ᙃ'],
    u'\u1644' : ['z i', 'ᙄ'],
    u'\u1645' : ['z A', 'ᙅ'],
    u'\u1646' : ['z', 'ᙆ'],
    u'\u1647' : ['z', 'ᙇ'],
    u'\u1648' : ['dz u', 'ᙈ'],
    u'\u1649' : ['dz o', 'ᙉ'],
    u'\u164a' : ['dz e', 'ᙊ'],
    u'\u164b' : ['dz e:', 'ᙋ'],
    u'\u164c' : ['dz i', 'ᙌ'],
    u'\u164d' : ['dz A', 'ᙍ'],
    u'\u164e' : ['s u', 'ᙎ'],
    u'\u164f' : ['s o', 'ᙏ'],
    u'\u1650' : ['s e', 'ᙐ'],
    u'\u1651' : ['s e:', 'ᙑ'],
    u'\u1652' : ['s i', 'ᙒ'],
    u'\u1653' : ['s A', 'ᙓ'],
    u'\u1654' : ['S u', 'ᙔ'],
    u'\u1655' : ['S o', 'ᙕ'],
    u'\u1656' : ['S e', 'ᙖ'],
    u'\u1657' : ['S e:', 'ᙗ'],
    u'\u1658' : ['S i', 'ᙘ'],
    u'\u1659' : ['S A', 'ᙙ'],
    u'\u165a' : ['S', 'ᙚ'],
    u'\u165b' : ['ts u', 'ᙛ'],
    u'\u165c' : ['ts o', 'ᙜ'],
    u'\u165d' : ['ts e', 'ᙝ'],
    u'\u165e' : ['ts e:', 'ᙞ'],
    u'\u165f' : ['ts i', 'ᙟ'],
    u'\u1660' : ['ts A', 'ᙠ'],
    u'\u1661' : ['Gh u', 'ᙡ'],
    u'\u1662' : ['Gh o', 'ᙢ'],
    u'\u1663' : ['Gh e', 'ᙣ'],
    u'\u1664' : ['Gh e:', 'ᙤ'],
    u'\u1665' : ['Gh i', 'ᙥ'],
    u'\u1666' : ['Gh A', 'ᙦ'],
    u'\u1667' : ['ts u', 'ᙧ'],
    u'\u1668' : ['ts o', 'ᙨ'],
    u'\u1669' : ['ts e', 'ᙩ'],
    u'\u166a' : ['ts e:', 'ᙪ'],
    u'\u166b' : ['ts i', 'ᙫ'],
    u'\u166c' : ['ts A', 'ᙬ'],
    u'\u166d' : ['(SYMBOL TO DENOTE CHRIST)', '᙭'],
    u'\u166e' : ['(FULL STOP)', '᙮'],
    u'\u166f' : ['q aI', 'ᙯ'],
    u'\u1670' : ['N aI', 'ᙰ'],
    u'\u1671' : ['N i', 'ᙱ'],
    u'\u1672' : ['N i:', 'ᙲ'],
    u'\u1673' : ['N o', 'ᙳ'],
    u'\u1674' : ['N o:', 'ᙴ'],
    u'\u1675' : ['N A', 'ᙵ'],
    u'\u1676' : ['N A:', 'ᙶ'],
    u'\u1700' : ['A', 'ᜀ'],
    u'\u1701' : ['e, i', 'ᜁ'],
    u'\u1702' : ['o, u', 'ᜂ'],
    u'\u1703' : ['k A', 'ᜃ'],
    u'\u1704' : ['g A', 'ᜄ'],
    u'\u1705' : ['N A', 'ᜅ'],
    u'\u1706' : ['t A', 'ᜆ'],
    u'\u1707' : ['d A', 'ᜇ'],
    u'\u1708' : ['_n A', 'ᜈ'],
    u'\u1709' : ['p A', 'ᜉ'],
    u'\u170a' : ['b A', 'ᜊ'],
    u'\u170b' : ['m A', 'ᜋ'],
    u'\u170c' : ['_j A', 'ᜌ'],
    u'\u170e' : ['_l A', 'ᜎ'],
    u'\u170f' : ['_w A', 'ᜏ'],
    u'\u1710' : ['s A', 'ᜐ'],
    u'\u1711' : ['_n A', 'ᜑ'],
    u'\u1712' : ['i', 'ᜒ'],
    u'\u1713' : ['u', 'ᜓ'],
    u'\u1714' : ['(P)', '᜔'],
    u'\u1720' : ['A', 'ᜠ'],
    u'\u1721' : ['i', 'ᜡ'],
    u'\u1722' : ['u', 'ᜢ'],
    u'\u1723' : ['k A', 'ᜣ'],
    u'\u1724' : ['g A', 'ᜤ'],
    u'\u1725' : ['N A', 'ᜥ'],
    u'\u1726' : ['t A', 'ᜦ'],
    u'\u1727' : ['d A', 'ᜧ'],
    u'\u1728' : ['_n A', 'ᜨ'],
    u'\u1729' : ['p A', 'ᜩ'],
    u'\u172a' : ['b A', 'ᜪ'],
    u'\u172b' : ['m A', 'ᜫ'],
    u'\u172c' : ['_j A', 'ᜬ'],
    u'\u172d' : ['` A', 'ᜭ'],
    u'\u172e' : ['_l A', 'ᜮ'],
    u'\u172f' : ['_w A', 'ᜯ'],
    u'\u1730' : ['s A', 'ᜰ'],
    u'\u1731' : ['_h A', 'ᜱ'],
    u'\u1732' : ['i', 'ᜲ'],
    u'\u1733' : ['u', 'ᜳ'],
    u'\u1734' : ['(P)', '᜴'],
    u'\u1735' : ['(SINGLE PUNCTUATION)', '᜵'],
    u'\u1736' : ['(DOUBLE PUNCTUATION)', '᜶'],
    u'\u1740' : ['A', 'ᝀ'],
    u'\u1741' : ['i', 'ᝁ'],
    u'\u1742' : ['u', 'ᝂ'],
    u'\u1743' : ['k A', 'ᝃ'],
    u'\u1744' : ['g A', 'ᝄ'],
    u'\u1745' : ['N A', 'ᝅ'],
    u'\u1746' : ['t A', 'ᝆ'],
    u'\u1747' : ['d A', 'ᝇ'],
    u'\u1748' : ['_n A', 'ᝈ'],
    u'\u1749' : ['p A', 'ᝉ'],
    u'\u174a' : ['b A', 'ᝊ'],
    u'\u174b' : ['m A', 'ᝋ'],
    u'\u174c' : ['_j A', 'ᝌ'],
    u'\u174d' : ['` A', 'ᝍ'],
    u'\u174e' : ['_l A', 'ᝎ'],
    u'\u174f' : ['_w A', 'ᝏ'],
    u'\u1750' : ['s A', 'ᝐ'],
    u'\u1751' : ['_h A', 'ᝑ'],
    u'\u1752' : ['i', 'ᝒ'],
    u'\u1753' : ['u', 'ᝓ'],
    u'\u1760' : ['A', 'ᝠ'],
    u'\u1761' : ['i', 'ᝡ'],
    u'\u1762' : ['u', 'ᝢ'],
    u'\u1763' : ['k A', 'ᝣ'],
    u'\u1764' : ['g A', 'ᝤ'],
    u'\u1765' : ['N A', 'ᝥ'],
    u'\u1766' : ['t A', 'ᝦ'],
    u'\u1767' : ['d A', 'ᝧ'],
    u'\u1768' : ['_n A', 'ᝨ'],
    u'\u1769' : ['p A', 'ᝩ'],
    u'\u176a' : ['b A', 'ᝪ'],
    u'\u176b' : ['m A', 'ᝫ'],
    u'\u176c' : ['_j A', 'ᝬ'],
    u'\u176e' : ['_l A', 'ᝮ'],
    u'\u176f' : ['_w A', 'ᝯ'],
    u'\u1770' : ['s A', 'ᝰ'],
    u'\u1772' : ['i', 'ᝲ'],
    u'\u1773' : ['u', 'ᝳ'],
    u'\u1780' : ['k A', 'ក'],
    u'\u1781' : ['k_h A', 'ខ'],
    u'\u1782' : ['k _>', 'គ'],
    u'\u1783' : ['k_h _>', 'ឃ'],
    u'\u1784' : ['N _>', 'ង'],
    u'\u1785' : ['_} A', 'ច'],
    u'\u1786' : ['c_h A', 'ឆ'],
    u'\u1787' : ['_} _>', 'ជ'],
    u'\u1788' : ['c_h _>', 'ឈ'],
    u'\u1789' : ['J _>', 'ញ'],
    u'\u178a' : ['d` A', 'ដ'],
    u'\u178b' : ['t_h A', 'ឋ'],
    u'\u178c' : ['d` _>', 'ឌ'],
    u'\u178d' : ['t_h _>', 'ឍ'],
    u'\u178e' : ['_n _>', 'ណ'],
    u'\u178f' : ['t A', 'ត'],
    u'\u1790' : ['t_h A', 'ថ'],
    u'\u1791' : ['t _>', 'ទ'],
    u'\u1792' : ['t_h _>', 'ធ'],
    u'\u1793' : ['_n _>', 'ន'],
    u'\u1794' : ['b A', 'ប'],
    u'\u1795' : ['p_h A', 'ផ'],
    u'\u1796' : ['p _>', 'ព'],
    u'\u1797' : ['p_h _>', 'ភ'],
    u'\u1798' : ['m _>', 'ម'],
    u'\u1799' : ['_j _>', 'យ'],
    u'\u179a' : ['` _>', 'រ'],
    u'\u179b' : ['_l _>', 'ល'],
    u'\u179c' : ['_w _>', 'វ'],
    u'\u179d' : ['sh A', 'ឝ'],
    u'\u179e' : ['s` A', 'ឞ'],
    u'\u179f' : ['s A', 'ស'],
    u'\u17a0' : ['_h A', 'ហ'],
    u'\u17a1' : ['_l A', 'ឡ'],
    u'\u17a2' : ['_k A', 'អ'],
    u'\u17a3' : ['A', 'ឣ'],
    u'\u17a4' : ['a:', 'ឤ'],
    u'\u17a5' : ['i', 'ឥ'],
    u'\u17a6' : ['i:', 'ឦ'],
    u'\u17a7' : ['o', 'ឧ'],
    u'\u17a8' : ['@ _w', 'ឨ'],
    u'\u17a9' : ['u:', 'ឩ'],
    u'\u17aa' : ['A _w', 'ឪ'],
    u'\u17ab' : ['`', 'ឫ'],
    u'\u17ac' : ['`', 'ឬ'],
    u'\u17ad' : ['_l', 'ឭ'],
    u'\u17ae' : ['_l', 'ឮ'],
    u'\u17af' : ['E', 'ឯ'],
    u'\u17b0' : ['a _j', 'ឰ'],
    u'\u17b1' : ['a _w', 'ឱ'],
    u'\u17b2' : ['o:', 'ឲ'],
    u'\u17b3' : ['_> _w', 'ឳ'],
    u'\u17b4' : ['A', '឴'],
    u'\u17b5' : ['a:', '឵'],
    u'\u17b6' : ['a:', 'ា'],
    u'\u17b7' : ['i', 'ិ'],
    u'\u17b8' : ['i:', 'ី'],
    u'\u17b9' : ['i', 'ឹ'],
    u'\u17ba' : ['i:', 'ឺ'],
    u'\u17bb' : ['o', 'ុ'],
    u'\u17bc' : ['u:', 'ូ'],
    u'\u17bd' : ['u _>', 'ួ'],
    u'\u17be' : ['>:', 'ើ'],
    u'\u17bf' : ['i _>', 'ឿ'],
    u'\u17c0' : ['i _>', 'ៀ'],
    u'\u17c1' : ['e:', 'េ'],
    u'\u17c2' : ['E:', 'ែ'],
    u'\u17c3' : ['a _j', 'ៃ'],
    u'\u17c4' : ['o:', 'ោ'],
    u'\u17c5' : ['a _w', 'ៅ'],
    u'\u17c6' : ['(M)', 'ំ'],
    u'\u17c7' : ['(A)', 'ះ'],
    u'\u17c8' : ['(Y)', 'ៈ'],
    u'\u17c9' : ['(MU)', '៉'],
    u'\u17ca' : ['(T)', '៊'],
    u'\u17cb' : ['(##)', '់'],
    u'\u17cc' : ['(##)', '៌'],
    u'\u17cd' : ['(##)', '៍'],
    u'\u17ce' : ['(##)', '៎'],
    u'\u17cf' : ['(##)', '៏'],
    u'\u17d0' : ['(##)', '័'],
    u'\u17d1' : ['(##)', '៑'],
    u'\u17d2' : ['(##)', '្'],
    u'\u17d3' : ['(##)', '៓'],
    u'\u17d4' : ['(##)', '។'],
    u'\u17d5' : ['(##)', '៕'],
    u'\u17d6' : ['(##)', '៖'],
    u'\u17d7' : ['(##)', 'ៗ'],
    u'\u17d8' : ['(##)', '៘'],
    u'\u17d9' : ['(##)', '៙'],
    u'\u17da' : ['(##)', '៚'],
    u'\u17db' : ['(CURRENCY SYMBOL)', '៛'],
    u'\u17dc' : ['(##)', 'ៜ'],
    u'\u17dd' : ['(##)', '៝'],
    u'\u17e0' : ['(ZERO)', '០'],
    u'\u17e1' : ['(ONE)', '១'],
    u'\u17e2' : ['(TWO)', '២'],
    u'\u17e3' : ['(THREE)', '៣'],
    u'\u17e4' : ['(FOUR)', '៤'],
    u'\u17e5' : ['(FIVE)', '៥'],
    u'\u17e6' : ['(SIX)', '៦'],
    u'\u17e7' : ['(SEVEN)', '៧'],
    u'\u17e8' : ['(EIGHT)', '៨'],
    u'\u17e9' : ['(NINE)', '៩'],
    u'\u17f0' : ['(TEN)', '៰'],
    u'\u17f1' : ['(##)', '៱'],
    u'\u17f2' : ['(##)', '៲'],
    u'\u17f3' : ['(##)', '៳'],
    u'\u17f4' : ['(##)', '៴'],
    u'\u17f5' : ['(##)', '៵'],
    u'\u17f6' : ['(##)', '៶'],
    u'\u17f7' : ['(##)', '៷'],
    u'\u17f8' : ['(##)', '៸'],
    u'\u17f9' : ['(##)', '៹'],
    u'\u1800' : ['(##)', '᠀'],
    u'\u1801' : ['(##)', '᠁'],
    u'\u1802' : ['(##)', '᠂'],
    u'\u1803' : ['(##)', '᠃'],
    u'\u1804' : ['(##)', '᠄'],
    u'\u1805' : ['(##)', '᠅'],
    u'\u1806' : ['(##)', '᠆'],
    u'\u1807' : ['(##)', '᠇'],
    u'\u1808' : ['(##)', '᠈'],
    u'\u1809' : ['(##)', '᠉'],
    u'\u180a' : ['(##)', '᠊'],
    u'\u180b' : ['(##)', '᠋'],
    u'\u180c' : ['(##)', '᠌'],
    u'\u180d' : ['(##)', '᠍'],
    u'\u180e' : ['(##)', '᠎'],
    u'\u1810' : ['(ZERO)', '᠐'],
    u'\u1811' : ['(ONE)', '᠑'],
    u'\u1812' : ['(TWO)', '᠒'],
    u'\u1813' : ['(THREE)', '᠓'],
    u'\u1814' : ['(FOUR)', '᠔'],
    u'\u1815' : ['(FIVE)', '᠕'],
    u'\u1816' : ['(SIX)', '᠖'],
    u'\u1817' : ['(SEVEN)', '᠗'],
    u'\u1818' : ['(EIGHT)', '᠘'],
    u'\u1819' : ['(NINE)', '᠙'],
    u'\u1820' : ['A', 'ᠠ'],
    u'\u1821' : ['E', 'ᠡ'],
    u'\u1822' : ['i', 'ᠢ'],
    u'\u1823' : ['_>', 'ᠣ'],
    u'\u1824' : ['u', 'ᠤ'],
    u'\u1825' : ['o', 'ᠥ'],
    u'\u1826' : ['u', 'ᠦ'],
    u'\u1827' : ['E:', 'ᠧ'],
    u'\u1828' : ['_n A', 'ᠨ'],
    u'\u1829' : ['N', 'ᠩ'],
    u'\u182a' : ['b A', 'ᠪ'],
    u'\u182b' : ['p A', 'ᠫ'],
    u'\u182c' : ['_x A', 'ᠬ'],
    u'\u182d' : ['g A', 'ᠭ'],
    u'\u182e' : ['m A', 'ᠮ'],
    u'\u182f' : ['_l A', 'ᠯ'],
    u'\u1830' : ['s A', 'ᠰ'],
    u'\u1831' : ['S A', 'ᠱ'],
    u'\u1832' : ['t A', 'ᠲ'],
    u'\u1833' : ['d A', 'ᠳ'],
    u'\u1834' : ['tSj A', 'ᠴ'],
    u'\u1835' : ['Z A', 'ᠵ'],
    u'\u1836' : ['_j A', 'ᠶ'],
    u'\u1837' : ['` A', 'ᠷ'],
    u'\u1838' : ['_v A', 'ᠸ'],
    u'\u1839' : ['f A', 'ᠹ'],
    u'\u183a' : ['_x A', 'ᠺ'],
    u'\u183b' : ['k A', 'ᠻ'],
    u'\u183c' : ['ts A', 'ᠼ'],
    u'\u183d' : ['z A', 'ᠽ'],
    u'\u183e' : ['_x A:', 'ᠾ'],
    u'\u183f' : ['Z A', 'ᠿ'],
    u'\u1840' : ['K A', 'ᡀ'],
    u'\u1841' : ['S A', 'ᡁ'],
    u'\u1842' : ['tS _j', 'ᡂ'],
    u'\u1843' : ['(:)', 'ᡃ'],
    u'\u1844' : ['E', 'ᡄ'],
    u'\u1845' : ['i', 'ᡅ'],
    u'\u1846' : ['o', 'ᡆ'],
    u'\u1847' : ['u', 'ᡇ'],
    u'\u1848' : ['o', 'ᡈ'],
    u'\u1849' : ['u', 'ᡉ'],
    u'\u184a' : ['N', 'ᡊ'],
    u'\u184b' : ['b A', 'ᡋ'],
    u'\u184c' : ['p A', 'ᡌ'],
    u'\u184d' : ['_x A', 'ᡍ'],
    u'\u184e' : ['g A', 'ᡎ'],
    u'\u184f' : ['m A', 'ᡏ'],
    u'\u1850' : ['t A', 'ᡐ'],
    u'\u1851' : ['d A', 'ᡑ'],
    u'\u1852' : ['tSj A', 'ᡒ'],
    u'\u1853' : ['Z A', 'ᡓ'],
    u'\u1854' : ['ts A', 'ᡔ'],
    u'\u1855' : ['_j A', 'ᡕ'],
    u'\u1856' : ['_w A', 'ᡖ'],
    u'\u1857' : ['k A', 'ᡗ'],
    u'\u1858' : ['g A:', 'ᡘ'],
    u'\u1859' : ['_x A:', 'ᡙ'],
    u'\u185a' : ['Z A', 'ᡚ'],
    u'\u185b' : ['_n A', 'ᡛ'],
    u'\u185c' : ['dz A', 'ᡜ'],
    u'\u185d' : ['E', 'ᡝ'],
    u'\u185e' : ['i', 'ᡞ'],
    u'\u185f' : ['_j', 'ᡟ'],
    u'\u1860' : ['u', 'ᡠ'],
    u'\u1861' : ['u', 'ᡡ'],
    u'\u1862' : ['N', 'ᡢ'],
    u'\u1863' : ['k A', 'ᡣ'],
    u'\u1864' : ['g A', 'ᡤ'],
    u'\u1865' : ['_x A', 'ᡥ'],
    u'\u1866' : ['p A', 'ᡦ'],
    u'\u1867' : ['S A', 'ᡧ'],
    u'\u1868' : ['t A', 'ᡨ'],
    u'\u1869' : ['d A', 'ᡩ'],
    u'\u186a' : ['Z A', 'ᡪ'],
    u'\u186b' : ['f A', 'ᡫ'],
    u'\u186c' : ['g A:', 'ᡬ'],
    u'\u186d' : ['_x A:', 'ᡭ'],
    u'\u186e' : ['ts A', 'ᡮ'],
    u'\u186f' : ['z A', 'ᡯ'],
    u'\u1870' : ['` A:', 'ᡰ'],
    u'\u1871' : ['tSj A', 'ᡱ'],
    u'\u1872' : ['S A', 'ᡲ'],
    u'\u1873' : ['i', 'ᡳ'],
    u'\u1874' : ['k A', 'ᡴ'],
    u'\u1875' : ['f A', 'ᡵ'],
    u'\u1876' : ['S A', 'ᡶ'],
    u'\u1877' : ['(##)', 'ᡷ'],
    u'\u1880' : ['(##)', 'ᢀ'],
    u'\u1881' : ['(##)', 'ᢁ'],
    u'\u1882' : ['(##)', 'ᢂ'],
    u'\u1883' : ['(##)', 'ᢃ'],
    u'\u1884' : ['(##)', 'ᢄ'],
    u'\u1885' : ['(##)', 'ᢅ'],
    u'\u1886' : ['(##)', 'ᢆ'],
    u'\u1887' : ['A', 'ᢇ'],
    u'\u1888' : ['i', 'ᢈ'],
    u'\u1889' : ['k A', 'ᢉ'],
    u'\u188a' : ['N A', 'ᢊ'],
    u'\u188b' : ['tS A', 'ᢋ'],
    u'\u188c' : ['t t A', 'ᢌ'],
    u'\u188d' : ['t t_h A', 'ᢍ'],
    u'\u188e' : ['t t A', 'ᢎ'],
    u'\u188f' : ['_n _n A', 'ᢏ'],
    u'\u1890' : ['t A', 'ᢐ'],
    u'\u1891' : ['t A', 'ᢑ'],
    u'\u1892' : ['p A', 'ᢒ'],
    u'\u1893' : ['p_h A', 'ᢓ'],
    u'\u1894' : ['S S A', 'ᢔ'],
    u'\u1895' : ['S A', 'ᢕ'],
    u'\u1896' : ['z A', 'ᢖ'],
    u'\u1897' : ['Ah', 'ᢗ'],
    u'\u1898' : ['t A', 'ᢘ'],
    u'\u1899' : ['S A', 'ᢙ'],
    u'\u189a' : ['Gh A', 'ᢚ'],
    u'\u189b' : ['N A', 'ᢛ'],
    u'\u189c' : ['tS A', 'ᢜ'],
    u'\u189d' : ['zh A', 'ᢝ'],
    u'\u189e' : ['t t A', 'ᢞ'],
    u'\u189f' : ['t_h A', 'ᢟ'],
    u'\u18a0' : ['t A', 'ᢠ'],
    u'\u18a1' : ['t_h A', 'ᢡ'],
    u'\u18a2' : ['S S A', 'ᢢ'],
    u'\u18a3' : ['tSj A', 'ᢣ'],
    u'\u18a4' : ['S A', 'ᢤ'],
    u'\u18a5' : ['s A', 'ᢥ'],
    u'\u18a6' : ['u', 'ᢦ'],
    u'\u18a7' : ['_j A', 'ᢧ'],
    u'\u18a8' : ['(BHA)', 'ᢨ'],
    u'\u18a9' : ['(DAGALGA)', 'ᢩ'],
    u'\u1900' : ['(VOWEL CARRIER)', 'ᤀ'],
    u'\u1901' : ['k A', 'ᤁ'],
    u'\u1902' : ['k_h A', 'ᤂ'],
    u'\u1903' : ['g A', 'ᤃ'],
    u'\u1904' : ['g_h A', 'ᤄ'],
    u'\u1905' : ['N A', 'ᤅ'],
    u'\u1906' : ['_} A', 'ᤆ'],
    u'\u1907' : ['c_h A', 'ᤇ'],
    u'\u1908' : ['J\\ A', 'ᤈ'],
    u'\u1909' : ['J\\_h a', 'ᤉ'],
    u'\u190a' : ['_j Ar', 'ᤊ'],
    u'\u190b' : ['t_d A', 'ᤋ'],
    u'\u190c' : ['t_d_h A', 'ᤌ'],
    u'\u190d' : ['d_d A', 'ᤍ'],
    u'\u190e' : ['d_d_h A', 'ᤎ'],
    u'\u190f' : ['n_d A', 'ᤏ'],
    u'\u1910' : ['p A', 'ᤐ'],
    u'\u1911' : ['p_h A', 'ᤑ'],
    u'\u1912' : ['b A', 'ᤒ'],
    u'\u1913' : ['b_h A', 'ᤓ'],
    u'\u1914' : ['m A', 'ᤔ'],
    u'\u1915' : ['_j A', 'ᤕ'],
    u'\u1916' : ['r\\` A', 'ᤖ'],
    u'\u1917' : ['_l A', 'ᤗ'],
    u'\u1918' : ['_w A', 'ᤘ'],
    u'\u1919' : ['S A', 'ᤙ'],
    u'\u191a' : ['S A', 'ᤚ'],
    u'\u191b' : ['s A', 'ᤛ'],
    u'\u191c' : ['_h A', 'ᤜ'],
    u'\u1920' : ['A', 'ᤠ'],
    u'\u1921' : ['i', 'ᤡ'],
    u'\u1922' : ['u', 'ᤢ'],
    u'\u1923' : ['e:', 'ᤣ'],
    u'\u1924' : ['aI', 'ᤤ'],
    u'\u1925' : ['o:', 'ᤥ'],
    u'\u1926' : ['aU', 'ᤦ'],
    u'\u1927' : ['e', 'ᤧ'],
    u'\u1928' : ['o', 'ᤨ'],
    u'\u1929' : ['_j A', 'ᤩ'],
    u'\u192a' : ['r\\` A', 'ᤪ'],
    u'\u192b' : ['_w A', 'ᤫ'],
    u'\u1930' : ['k A', 'ᤰ'],
    u'\u1931' : ['N A', 'ᤱ'],
    u'\u1932' : ['F A', 'ᤲ'],
    u'\u1933' : ['t_d A', 'ᤳ'],
    u'\u1934' : ['n_d A', 'ᤴ'],
    u'\u1935' : ['p A', 'ᤵ'],
    u'\u1936' : ['m A', 'ᤶ'],
    u'\u1937' : ['r\\` A', 'ᤷ'],
    u'\u1938' : ['_l A', 'ᤸ'],
    u'\u1939' : ['(MUKPHRENG)', '᤹'],
    u'\u193a' : ['(KEMPHRENG)', '᤺'],
    u'\u193b' : ['(P)', '᤻'],
    u'\u1940' : ['(LOO)', '᥀'],
    u'\u1944' : ['(EXCLAMATION MARK)', '᥄'],
    u'\u1945' : ['(QUESTION MARK)', '᥅'],
    u'\u1946' : ['(ZERO)', '᥆'],
    u'\u1947' : ['(ONE)', '᥇'],
    u'\u1948' : ['(TWO)', '᥈'],
    u'\u1949' : ['(THREE)', '᥉'],
    u'\u194a' : ['(FOUR)', '᥊'],
    u'\u194b' : ['(FIVE)', '᥋'],
    u'\u194c' : ['(SIX)', '᥌'],
    u'\u194d' : ['(SEVEN)', '᥍'],
    u'\u194e' : ['(EIGHT)', '᥎'],
    u'\u194f' : ['(NINE)', '᥏'],
    u'\u19e0' : ['(##)', '᧠'],
    u'\u19e1' : ['(##)', '᧡'],
    u'\u19e2' : ['(##)', '᧢'],
    u'\u19e3' : ['(##)', '᧣'],
    u'\u19e4' : ['(##)', '᧤'],
    u'\u19e5' : ['(##)', '᧥'],
    u'\u19e6' : ['(##)', '᧦'],
    u'\u19e7' : ['(##)', '᧧'],
    u'\u19e8' : ['(##)', '᧨'],
    u'\u19e9' : ['(##)', '᧩'],
    u'\u19ea' : ['(##)', '᧪'],
    u'\u19eb' : ['(##)', '᧫'],
    u'\u19ec' : ['(##)', '᧬'],
    u'\u19ed' : ['(##)', '᧭'],
    u'\u19ee' : ['(##)', '᧮'],
    u'\u19ef' : ['(##)', '᧯'],
    u'\u19f0' : ['(##)', '᧰'],
    u'\u19f1' : ['(##)', '᧱'],
    u'\u19f2' : ['(##)', '᧲'],
    u'\u19f3' : ['(##)', '᧳'],
    u'\u19f4' : ['(##)', '᧴'],
    u'\u19f5' : ['(##)', '᧵'],
    u'\u19f6' : ['(##)', '᧶'],
    u'\u19f7' : ['(##)', '᧷'],
    u'\u19f8' : ['(##)', '᧸'],
    u'\u19f9' : ['(##)', '᧹'],
    u'\u19fa' : ['(##)', '᧺'],
    u'\u19fb' : ['(##)', '᧻'],
    u'\u19fc' : ['(##)', '᧼'],
    u'\u19fd' : ['(##)', '᧽'],
    u'\u19fe' : ['(##)', '᧾'],
    u'\u19ff' : ['(##)', '᧿'],
    u'\u1f00' : ['A', 'ἀ'],
    u'\u1f01' : ['_h A', 'ἁ'],
    u'\u1f02' : ['A', 'ἂ'],
    u'\u1f03' : ['_h A', 'ἃ'],
    u'\u1f04' : ['A', 'ἄ'],
    u'\u1f05' : ['_h A', 'ἅ'],
    u'\u1f06' : ['A', 'ἆ'],
    u'\u1f07' : ['_h A', 'ἇ'],
    u'\u1f08' : ['A', 'Ἀ'],
    u'\u1f09' : ['_h A', 'Ἁ'],
    u'\u1f0a' : ['_h A', 'Ἂ'],
    u'\u1f0b' : ['A', 'Ἃ'],
    u'\u1f0c' : ['A', 'Ἄ'],
    u'\u1f0d' : ['_h A', 'Ἅ'],
    u'\u1f0e' : ['A', 'Ἆ'],
    u'\u1f0f' : ['_h A', 'Ἇ'],
    u'\u1f10' : ['e', 'ἐ'],
    u'\u1f11' : ['_h e', 'ἑ'],
    u'\u1f12' : ['e', 'ἒ'],
    u'\u1f13' : ['_h e', 'ἓ'],
    u'\u1f14' : ['e', 'ἔ'],
    u'\u1f15' : ['_h e', 'ἕ'],
    u'\u1f18' : ['e', 'Ἐ'],
    u'\u1f19' : ['_h e', 'Ἑ'],
    u'\u1f1a' : ['e', 'Ἒ'],
    u'\u1f1b' : ['_h e', 'Ἓ'],
    u'\u1f1c' : ['e', 'Ἔ'],
    u'\u1f1d' : ['_h e', 'Ἕ'],
    u'\u1f20' : ['E:', 'ἠ'],
    u'\u1f21' : ['_h E:', 'ἡ'],
    u'\u1f22' : ['E:', 'ἢ'],
    u'\u1f23' : ['_h E:', 'ἣ'],
    u'\u1f24' : ['E:', 'ἤ'],
    u'\u1f25' : ['_h E:', 'ἥ'],
    u'\u1f26' : ['E:', 'ἦ'],
    u'\u1f27' : ['_h E:', 'ἧ'],
    u'\u1f28' : ['E:', 'Ἠ'],
    u'\u1f29' : ['_h E:', 'Ἡ'],
    u'\u1f2a' : ['E:', 'Ἢ'],
    u'\u1f2b' : ['_h E:', 'Ἣ'],
    u'\u1f2c' : ['E:', 'Ἤ'],
    u'\u1f2d' : ['_h E:', 'Ἥ'],
    u'\u1f2e' : ['E:', 'Ἦ'],
    u'\u1f2f' : ['_h E:', 'Ἧ'],
    u'\u1f30' : ['i', 'ἰ'],
    u'\u1f31' : ['_h i', 'ἱ'],
    u'\u1f32' : ['i', 'ἲ'],
    u'\u1f33' : ['_h i', 'ἳ'],
    u'\u1f34' : ['i', 'ἴ'],
    u'\u1f35' : ['_h i', 'ἵ'],
    u'\u1f36' : ['i', 'ἶ'],
    u'\u1f37' : ['_h i', 'ἷ'],
    u'\u1f38' : ['i', 'Ἰ'],
    u'\u1f39' : ['_h i', 'Ἱ'],
    u'\u1f3a' : ['i', 'Ἲ'],
    u'\u1f3b' : ['_h i', 'Ἳ'],
    u'\u1f3c' : ['i', 'Ἴ'],
    u'\u1f3d' : ['_h i', 'Ἵ'],
    u'\u1f3e' : ['i', 'Ἶ'],
    u'\u1f3f' : ['_h i', 'Ἷ'],
    u'\u1f40' : ['o', 'ὀ'],
    u'\u1f41' : ['_h o', 'ὁ'],
    u'\u1f42' : ['o', 'ὂ'],
    u'\u1f43' : ['_h o', 'ὃ'],
    u'\u1f44' : ['o', 'ὄ'],
    u'\u1f45' : ['_h o', 'ὅ'],
    u'\u1f48' : ['o', 'Ὀ'],
    u'\u1f49' : ['_h o', 'Ὁ'],
    u'\u1f4a' : ['o', 'Ὂ'],
    u'\u1f4b' : ['_h o', 'Ὃ'],
    u'\u1f4c' : ['o', 'Ὄ'],
    u'\u1f4d' : ['_h o', 'Ὅ'],
    u'\u1f50' : ['i', 'ὐ'],
    u'\u1f51' : ['_h i', 'ὑ'],
    u'\u1f52' : ['i', 'ὒ'],
    u'\u1f53' : ['_h i', 'ὓ'],
    u'\u1f54' : ['i', 'ὔ'],
    u'\u1f55' : ['_h i', 'ὕ'],
    u'\u1f56' : ['i', 'ὖ'],
    u'\u1f57' : ['_h i', 'ὗ'],
    u'\u1f59' : ['i', 'Ὑ'],
    u'\u1f5b' : ['_h i', 'Ὓ'],
    u'\u1f5d' : ['i', 'Ὕ'],
    u'\u1f5f' : ['_h i', 'Ὗ'],
    u'\u1f60' : ['o', 'ὠ'],
    u'\u1f61' : ['_h o', 'ὡ'],
    u'\u1f62' : ['o', 'ὢ'],
    u'\u1f63' : ['_h o', 'ὣ'],
    u'\u1f64' : ['o', 'ὤ'],
    u'\u1f65' : ['_h o', 'ὥ'],
    u'\u1f66' : ['o', 'ὦ'],
    u'\u1f67' : ['_h o', 'ὧ'],
    u'\u1f68' : ['o', 'Ὠ'],
    u'\u1f69' : ['_h o', 'Ὡ'],
    u'\u1f6a' : ['o', 'Ὢ'],
    u'\u1f6b' : ['_h o', 'Ὣ'],
    u'\u1f6c' : ['o', 'Ὤ'],
    u'\u1f6d' : ['_h o', 'Ὥ'],
    u'\u1f6e' : ['o', 'Ὦ'],
    u'\u1f6f' : ['_h o', 'Ὧ'],
    u'\u1f70' : ['A', 'ὰ'],
    u'\u1f71' : ['A', 'ά'],
    u'\u1f72' : ['e', 'ὲ'],
    u'\u1f73' : ['e', 'έ'],
    u'\u1f74' : ['E:', 'ὴ'],
    u'\u1f75' : ['E:', 'ή'],
    u'\u1f76' : ['i', 'ὶ'],
    u'\u1f77' : ['i', 'ί'],
    u'\u1f78' : ['o', 'ὸ'],
    u'\u1f79' : ['o', 'ό'],
    u'\u1f7a' : ['u', 'ὺ'],
    u'\u1f7b' : ['u', 'ύ'],
    u'\u1f7c' : ['o', 'ὼ'],
    u'\u1f7d' : ['o', 'ώ'],
    u'\u1f80' : ['A', 'ᾀ'],
    u'\u1f81' : ['_h A', 'ᾁ'],
    u'\u1f82' : ['A', 'ᾂ'],
    u'\u1f83' : ['_h A', 'ᾃ'],
    u'\u1f84' : ['A', 'ᾄ'],
    u'\u1f85' : ['_h A', 'ᾅ'],
    u'\u1f86' : ['A', 'ᾆ'],
    u'\u1f87' : ['_h A', 'ᾇ'],
    u'\u1f88' : ['A', 'ᾈ'],
    u'\u1f89' : ['_h A', 'ᾉ'],
    u'\u1f8a' : ['A', 'ᾊ'],
    u'\u1f8b' : ['_h A', 'ᾋ'],
    u'\u1f8c' : ['A', 'ᾌ'],
    u'\u1f8d' : ['_h A', 'ᾍ'],
    u'\u1f8e' : ['A', 'ᾎ'],
    u'\u1f8f' : ['_h A', 'ᾏ'],
    u'\u1f90' : ['E:', 'ᾐ'],
    u'\u1f91' : ['_h E:', 'ᾑ'],
    u'\u1f92' : ['E:', 'ᾒ'],
    u'\u1f93' : ['_h E:', 'ᾓ'],
    u'\u1f94' : ['E:', 'ᾔ'],
    u'\u1f95' : ['_h E:', 'ᾕ'],
    u'\u1f96' : ['E:', 'ᾖ'],
    u'\u1f97' : ['_h E:', 'ᾗ'],
    u'\u1f98' : ['E:', 'ᾘ'],
    u'\u1f99' : ['_h E:', 'ᾙ'],
    u'\u1f9a' : ['E:', 'ᾚ'],
    u'\u1f9b' : ['_h E:', 'ᾛ'],
    u'\u1f9c' : ['E:', 'ᾜ'],
    u'\u1f9d' : ['_h E:', 'ᾝ'],
    u'\u1f9e' : ['E:', 'ᾞ'],
    u'\u1f9f' : ['_h E:', 'ᾟ'],
    u'\u1fa0' : ['o', 'ᾠ'],
    u'\u1fa1' : ['_h o', 'ᾡ'],
    u'\u1fa2' : ['o', 'ᾢ'],
    u'\u1fa3' : ['_h o', 'ᾣ'],
    u'\u1fa4' : ['o', 'ᾤ'],
    u'\u1fa5' : ['_h o', 'ᾥ'],
    u'\u1fa6' : ['o', 'ᾦ'],
    u'\u1fa7' : ['_h o', 'ᾧ'],
    u'\u1fa8' : ['o', 'ᾨ'],
    u'\u1fa9' : ['_h o', 'ᾩ'],
    u'\u1faa' : ['o', 'ᾪ'],
    u'\u1fab' : ['_h o', 'ᾫ'],
    u'\u1fac' : ['o', 'ᾬ'],
    u'\u1fad' : ['_h o', 'ᾭ'],
    u'\u1fae' : ['o', 'ᾮ'],
    u'\u1faf' : ['_h o', 'ᾯ'],
    u'\u1fb0' : ['A', 'ᾰ'],
    u'\u1fb1' : ['A', 'ᾱ'],
    u'\u1fb2' : ['A', 'ᾲ'],
    u'\u1fb3' : ['A', 'ᾳ'],
    u'\u1fb4' : ['A', 'ᾴ'],
    u'\u1fb6' : ['A', 'ᾶ'],
    u'\u1fb7' : ['A', 'ᾷ'],
    u'\u1fb8' : ['A', 'Ᾰ'],
    u'\u1fb9' : ['A', 'Ᾱ'],
    u'\u1fba' : ['A', 'Ὰ'],
    u'\u1fbb' : ['A', 'Ά'],
    u'\u1fbc' : ['A', 'ᾼ'],
    u'\u1fbd' : ['(KORONIS)', '᾽'],
    u'\u1fbe' : ['i', 'ι'],
    u'\u1fbf' : ['(PSILI)', '᾿'],
    u'\u1fc0' : ['(PERISPOMENI)', '῀'],
    u'\u1fc1' : ['(DIALYTIKA AND PERISPOMENI)', '῁'],
    u'\u1fc2' : ['E:', 'ῂ'],
    u'\u1fc3' : ['E:', 'ῃ'],
    u'\u1fc4' : ['E:', 'ῄ'],
    u'\u1fc6' : ['E:', 'ῆ'],
    u'\u1fc7' : ['E:', 'ῇ'],
    u'\u1fc8' : ['e', 'Ὲ'],
    u'\u1fc9' : ['E:', 'Έ'],
    u'\u1fca' : ['E:', 'Ὴ'],
    u'\u1fcb' : ['E:', 'Ή'],
    u'\u1fcc' : ['E:', 'ῌ'],
    u'\u1fcd' : ['(PSILI AND VARIA)', '῍'],
    u'\u1fce' : ['(PSILI AND OXIA)', '῎'],
    u'\u1fcf' : ['(PSILI AND PERISPOMENI)', '῏'],
    u'\u1fd0' : ['i', 'ῐ'],
    u'\u1fd1' : ['i', 'ῑ'],
    u'\u1fd2' : ['i', 'ῒ'],
    u'\u1fd3' : ['i', 'ΐ'],
    u'\u1fd6' : ['i', 'ῖ'],
    u'\u1fd7' : ['i', 'ῗ'],
    u'\u1fd8' : ['i', 'Ῐ'],
    u'\u1fd9' : ['i', 'Ῑ'],
    u'\u1fda' : ['i', 'Ὶ'],
    u'\u1fdb' : ['i', 'Ί'],
    u'\u1fdd' : ['(DASIA AND VARIA)', '῝'],
    u'\u1fde' : ['(DASIA AND OXIA)', '῞'],
    u'\u1fdf' : ['(DASIA AND PERISPOMENI)', '῟'],
    u'\u1fe0' : ['i', 'ῠ'],
    u'\u1fe1' : ['i', 'ῡ'],
    u'\u1fe2' : ['i', 'ῢ'],
    u'\u1fe3' : ['i', 'ΰ'],
    u'\u1fe4' : ['`', 'ῤ'],
    u'\u1fe5' : ['` _h', 'ῥ'],
    u'\u1fe6' : ['i', 'ῦ'],
    u'\u1fe7' : ['i', 'ῧ'],
    u'\u1fe8' : ['i', 'Ῠ'],
    u'\u1fe9' : ['i', 'Ῡ'],
    u'\u1fea' : ['i', 'Ὺ'],
    u'\u1feb' : ['i', 'Ύ'],
    u'\u1fec' : ['` _h', 'Ῥ'],
    u'\u1fed' : ['(DIALYTIKA AND VARIA)', '῭'],
    u'\u1fee' : ['(DIALYTIKA AND OXIA)', '΅'],
    u'\u1fef' : ['(VARIA)', '`'],
    u'\u1ff2' : ['o', 'ῲ'],
    u'\u1ff3' : ['o', 'ῳ'],
    u'\u1ff4' : ['o', 'ῴ'],
    u'\u1ff6' : ['o', 'ῶ'],
    u'\u1ff7' : ['o', 'ῷ'],
    u'\u1ff8' : ['o', 'Ὸ'],
    u'\u1ff9' : ['o', 'Ό'],
    u'\u1ffa' : ['o', 'Ὼ'],
    u'\u1ffb' : ['o', 'Ώ'],
    u'\u1ffc' : ['o', 'ῼ'],
    u'\u1ffd' : ['(OXIA)', '´'],
    u'\u1ffe' : ['(DASIA)', '῾'],
    u'\u2000' : ['(##)', ' '],
    u'\u2001' : ['(##)', ' '],
    u'\u2002' : ['(##)', ' '],
    u'\u2003' : ['(##)', ' '],
    u'\u2004' : ['(##)', ' '],
    u'\u2005' : ['(##)', ' '],
    u'\u2006' : ['(##)', ' '],
    u'\u2007' : ['(##)', ' '],
    u'\u2008' : ['(##)', ' '],
    u'\u2009' : ['(##)', ' '],
    u'\u200a' : ['(##)', ' '],
    u'\u200b' : ['(##)', '​'],
    u'\u200c' : ['(##)', '‌'],
    u'\u200d' : ['(##)', '‍'],
    u'\u200e' : ['(##)', '‎'],
    u'\u200f' : ['(##)', '‏'],
    u'\u2010' : ['(##)', '‐'],
    u'\u2011' : ['(##)', '‑'],
    u'\u2012' : ['(##)', '‒'],
    u'\u2013' : ['(##)', '–'],
    u'\u2014' : ['(##)', '—'],
    u'\u2015' : ['(##)', '―'],
    u'\u2016' : ['(##)', '‖'],
    u'\u2017' : ['(##)', '‗'],
    u'\u2018' : ['(##)', '‘'],
    u'\u2019' : ['(##)', '’'],
    u'\u201a' : ['(##)', '‚'],
    u'\u201b' : ['(##)', '‛'],
    u'\u201c' : ['(##)', '“'],
    u'\u201d' : ['(##)', '”'],
    u'\u201e' : ['(##)', '„'],
    u'\u201f' : ['(##)', '‟'],
    u'\u2020' : ['(##)', '†'],
    u'\u2021' : ['(##)', '‡'],
    u'\u2022' : ['(##)', '•'],
    u'\u2023' : ['(##)', '‣'],
    u'\u2024' : ['(##)', '․'],
    u'\u2025' : ['(##)', '‥'],
    u'\u2026' : ['(##)', '…'],
    u'\u2027' : ['(##)', '‧'],
    u'\u2028' : ['(##)', ' '],
    u'\u2029' : ['(##)', ' '],
    u'\u202a' : ['(##)', '‪'],
    u'\u202b' : ['(##)', '‫'],
    u'\u202c' : ['(##)', '‬'],
    u'\u202d' : ['(##)', '‭'],
    u'\u202e' : ['(##)', '‮'],
    u'\u202f' : ['(##)', ' '],
    u'\u2030' : ['(##)', '‰'],
    u'\u2031' : ['(##)', '‱'],
    u'\u2032' : ['(##)', '′'],
    u'\u2033' : ['(##)', '″'],
    u'\u2034' : ['(##)', '‴'],
    u'\u2035' : ['(##)', '‵'],
    u'\u2036' : ['(##)', '‶'],
    u'\u2037' : ['(##)', '‷'],
    u'\u2038' : ['(##)', '‸'],
    u'\u2039' : ['(##)', '‹'],
    u'\u203a' : ['(##)', '›'],
    u'\u203b' : ['(##)', '※'],
    u'\u203c' : ['(##)', '‼'],
    u'\u203d' : ['(##)', '‽'],
    u'\u203e' : ['(##)', '‾'],
    u'\u203f' : ['(##)', '‿'],
    u'\u2040' : ['(##)', '⁀'],
    u'\u2041' : ['(##)', '⁁'],
    u'\u2042' : ['(##)', '⁂'],
    u'\u2043' : ['(##)', '⁃'],
    u'\u2044' : ['(##)', '⁄'],
    u'\u2045' : ['(##)', '⁅'],
    u'\u2046' : ['(##)', '⁆'],
    u'\u2047' : ['(##)', '⁇'],
    u'\u2048' : ['(##)', '⁈'],
    u'\u2049' : ['(##)', '⁉'],
    u'\u204a' : ['(##)', '⁊'],
    u'\u204b' : ['(##)', '⁋'],
    u'\u204c' : ['(##)', '⁌'],
    u'\u204d' : ['(##)', '⁍'],
    u'\u204e' : ['(##)', '⁎'],
    u'\u204f' : ['(##)', '⁏'],
    u'\u2050' : ['(##)', '⁐'],
    u'\u2051' : ['(##)', '⁑'],
    u'\u2052' : ['(##)', '⁒'],
    u'\u2053' : ['(##)', '⁓'],
    u'\u2054' : ['(##)', '⁔'],
    u'\u2055' : ['(##)', '⁕'],
    u'\u2056' : ['(##)', '⁖'],
    u'\u2057' : ['(##)', '⁗'],
    u'\u2058' : ['(##)', '⁘'],
    u'\u2059' : ['(##)', '⁙'],
    u'\u205a' : ['(##)', '⁚'],
    u'\u205b' : ['(##)', '⁛'],
    u'\u205c' : ['(##)', '⁜'],
    u'\u205d' : ['(##)', '⁝'],
    u'\u205e' : ['(##)', '⁞'],
    u'\u205f' : ['(##)', ' '],
    u'\u2060' : ['(##)', '⁠'],
    u'\u2061' : ['(##)', '⁡'],
    u'\u2062' : ['(##)', '⁢'],
    u'\u2063' : ['(##)', '⁣'],
    u'\u2064' : ['(##)', '⁤'],
    u'\u2065' : ['(##)', '⁥'],
    u'\u2066' : ['(##)', '⁦'],
    u'\u2067' : ['(##)', '⁧'],
    u'\u2068' : ['(##)', '⁨'],
    u'\u2069' : ['(##)', '⁩'],
    u'\u206a' : ['(##)', '⁪'],
    u'\u206b' : ['(##)', '⁫'],
    u'\u206c' : ['(##)', '⁬'],
    u'\u206d' : ['(##)', '⁭'],
    u'\u206e' : ['(##)', '⁮'],
    u'\u206f' : ['(##)', '⁯'],
    u'\u2100' : ['(##)', '℀'],
    u'\u2101' : ['(##)', '℁'],
    u'\u2102' : ['(##)', 'ℂ'],
    u'\u2103' : ['(##)', '℃'],
    u'\u2104' : ['(##)', '℄'],
    u'\u2105' : ['(##)', '℅'],
    u'\u2106' : ['(##)', '℆'],
    u'\u2107' : ['(##)', 'ℇ'],
    u'\u2108' : ['(##)', '℈'],
    u'\u2109' : ['(##)', '℉'],
    u'\u210a' : ['(##)', 'ℊ'],
    u'\u210b' : ['(##)', 'ℋ'],
    u'\u210c' : ['(##)', 'ℌ'],
    u'\u210d' : ['(##)', 'ℍ'],
    u'\u210e' : ['(##)', 'ℎ'],
    u'\u210f' : ['(##)', 'ℏ'],
    u'\u2110' : ['(##)', 'ℐ'],
    u'\u2111' : ['(##)', 'ℑ'],
    u'\u2112' : ['(##)', 'ℒ'],
    u'\u2113' : ['(##)', 'ℓ'],
    u'\u2114' : ['(##)', '℔'],
    u'\u2115' : ['(##)', 'ℕ'],
    u'\u2116' : ['(##)', '№'],
    u'\u2117' : ['(##)', '℗'],
    u'\u2118' : ['(##)', '℘'],
    u'\u2119' : ['(##)', 'ℙ'],
    u'\u211a' : ['(##)', 'ℚ'],
    u'\u211b' : ['(##)', 'ℛ'],
    u'\u211c' : ['(##)', 'ℜ'],
    u'\u211d' : ['(##)', 'ℝ'],
    u'\u211e' : ['(##)', '℞'],
    u'\u211f' : ['(##)', '℟'],
    u'\u2120' : ['(##)', '℠'],
    u'\u2121' : ['(##)', '℡'],
    u'\u2122' : ['(##)', '™'],
    u'\u2123' : ['(##)', '℣'],
    u'\u2124' : ['(##)', 'ℤ'],
    u'\u2125' : ['(##)', '℥'],
    u'\u2126' : ['(##)', 'Ω'],
    u'\u2127' : ['(##)', '℧'],
    u'\u2128' : ['(##)', 'ℨ'],
    u'\u2129' : ['(##)', '℩'],
    u'\u212a' : ['(##)', 'K'],
    u'\u212b' : ['(##)', 'Å'],
    u'\u212c' : ['(##)', 'ℬ'],
    u'\u212d' : ['(##)', 'ℭ'],
    u'\u212e' : ['(##)', '℮'],
    u'\u212f' : ['(##)', 'ℯ'],
    u'\u2130' : ['(##)', 'ℰ'],
    u'\u2131' : ['(##)', 'ℱ'],
    u'\u2132' : ['(##)', 'Ⅎ'],
    u'\u2133' : ['(##)', 'ℳ'],
    u'\u2134' : ['(##)', 'ℴ'],
    u'\u2135' : ['(##)', 'ℵ'],
    u'\u2136' : ['(##)', 'ℶ'],
    u'\u2137' : ['(##)', 'ℷ'],
    u'\u2138' : ['(##)', 'ℸ'],
    u'\u2139' : ['(##)', 'ℹ'],
    u'\u213a' : ['(##)', '℺'],
    u'\u213b' : ['(##)', '℻'],
    u'\u213c' : ['(##)', 'ℼ'],
    u'\u213d' : ['(##)', 'ℽ'],
    u'\u213e' : ['(##)', 'ℾ'],
    u'\u213f' : ['(##)', 'ℿ'],
    u'\u2140' : ['(##)', '⅀'],
    u'\u2141' : ['(##)', '⅁'],
    u'\u2142' : ['(##)', '⅂'],
    u'\u2143' : ['(##)', '⅃'],
    u'\u2144' : ['(##)', '⅄'],
    u'\u2145' : ['(##)', 'ⅅ'],
    u'\u2146' : ['(##)', 'ⅆ'],
    u'\u2147' : ['(##)', 'ⅇ'],
    u'\u2148' : ['(##)', 'ⅈ'],
    u'\u2149' : ['(##)', 'ⅉ'],
    u'\u214a' : ['(##)', '⅊'],
    u'\u214b' : ['(##)', '⅋'],
    u'\u214c' : ['(##)', '⅌'],
    u'\u214d' : ['(##)', '⅍'],
    u'\u214e' : ['(##)', 'ⅎ'],
    u'\u214f' : ['(##)', '⅏'],
    u'\u2c60' : ['(##)', 'Ⱡ'],
    u'\u2c61' : ['(##)', 'ⱡ'],
    u'\u2c62' : ['(##)', 'Ɫ'],
    u'\u2c63' : ['(##)', 'Ᵽ'],
    u'\u2c64' : ['(##)', 'Ɽ'],
    u'\u2c65' : ['(##)', 'ⱥ'],
    u'\u2c66' : ['(##)', 'ⱦ'],
    u'\u2c67' : ['(##)', 'Ⱨ'],
    u'\u2c68' : ['(##)', 'ⱨ'],
    u'\u2c69' : ['(##)', 'Ⱪ'],
    u'\u2c6a' : ['(##)', 'ⱪ'],
    u'\u2c6b' : ['(##)', 'Ⱬ'],
    u'\u2c6c' : ['(##)', 'ⱬ'],
    u'\u2c74' : ['(##)', 'ⱴ'],
    u'\u2c75' : ['(##)', 'Ⱶ'],
    u'\u2c76' : ['(##)', 'ⱶ'],
    u'\u2c77' : ['(##)', 'ⱷ'],
    u'\u2c80' : ['A', 'Ⲁ'],
    u'\u2c81' : ['A', 'ⲁ'],
    u'\u2c82' : ['_v', 'Ⲃ'],
    u'\u2c83' : ['_v', 'ⲃ'],
    u'\u2c84' : ['k', 'Ⲅ'],
    u'\u2c85' : ['k', 'ⲅ'],
    u'\u2c86' : ['t', 'Ⲇ'],
    u'\u2c87' : ['t', 'ⲇ'],
    u'\u2c88' : ['e', 'Ⲉ'],
    u'\u2c89' : ['e', 'ⲉ'],
    u'\u2c8a' : ['(SIX)', 'Ⲋ'],
    u'\u2c8b' : ['(SIX)', 'ⲋ'],
    u'\u2c8c' : ['s', 'Ⲍ'],
    u'\u2c8d' : ['s', 'ⲍ'],
    u'\u2c8e' : ['e:', 'Ⲏ'],
    u'\u2c8f' : ['e:', 'ⲏ'],
    u'\u2c90' : ['t_h', 'Ⲑ'],
    u'\u2c91' : ['t_h', 'ⲑ'],
    u'\u2c92' : ['i', 'Ⲓ'],
    u'\u2c93' : ['i', 'ⲓ'],
    u'\u2c94' : ['k', 'Ⲕ'],
    u'\u2c95' : ['k', 'ⲕ'],
    u'\u2c96' : ['_l', 'Ⲗ'],
    u'\u2c97' : ['_l', 'ⲗ'],
    u'\u2c98' : ['m', 'Ⲙ'],
    u'\u2c99' : ['m', 'ⲙ'],
    u'\u2c9a' : ['_n', 'Ⲛ'],
    u'\u2c9b' : ['_n', 'ⲛ'],
    u'\u2c9c' : ['k s', 'Ⲝ'],
    u'\u2c9d' : ['k s', 'ⲝ'],
    u'\u2c9e' : ['o', 'Ⲟ'],
    u'\u2c9f' : ['o', 'ⲟ'],
    u'\u2ca0' : ['p', 'Ⲡ'],
    u'\u2ca1' : ['p', 'ⲡ'],
    u'\u2ca2' : ['`', 'Ⲣ'],
    u'\u2ca3' : ['`', 'ⲣ'],
    u'\u2ca4' : ['s', 'Ⲥ'],
    u'\u2ca5' : ['s', 'ⲥ'],
    u'\u2ca6' : ['t', 'Ⲧ'],
    u'\u2ca7' : ['t', 'ⲧ'],
    u'\u2ca8' : ['u', 'Ⲩ'],
    u'\u2ca9' : ['u', 'ⲩ'],
    u'\u2caa' : ['p_h', 'Ⲫ'],
    u'\u2cab' : ['p_h', 'ⲫ'],
    u'\u2cac' : ['k_h', 'Ⲭ'],
    u'\u2cad' : ['k_h', 'ⲭ'],
    u'\u2cae' : ['p s', 'Ⲯ'],
    u'\u2caf' : ['p s', 'ⲯ'],
    u'\u2cb0' : ['o:', 'Ⲱ'],
    u'\u2cb1' : ['o:', 'ⲱ'],
    u'\u2cb2' : ['A', 'Ⲳ'],
    u'\u2cb3' : ['A', 'ⲳ'],
    u'\u2cb4' : ['i', 'Ⲵ'],
    u'\u2cb5' : ['i', 'ⲵ'],
    u'\u2cb6' : ['e', 'Ⲷ'],
    u'\u2cb7' : ['e', 'ⲷ'],
    u'\u2cb8' : ['k', 'Ⲹ'],
    u'\u2cb9' : ['k', 'ⲹ'],
    u'\u2cba' : ['_n', 'Ⲻ'],
    u'\u2cbb' : ['_n', 'ⲻ'],
    u'\u2cbc' : ['_n', 'Ⲽ'],
    u'\u2cbd' : ['_n', 'ⲽ'],
    u'\u2cbe' : ['o:', 'Ⲿ'],
    u'\u2cbf' : ['o:', 'ⲿ'],
    u'\u2cc0' : ['s', 'Ⳁ'],
    u'\u2cc1' : ['s', 'ⳁ'],
    u'\u2cc2' : ['S', 'Ⳃ'],
    u'\u2cc3' : ['S', 'ⳃ'],
    u'\u2cc4' : ['S', 'Ⳅ'],
    u'\u2cc5' : ['S', 'ⳅ'],
    u'\u2cc6' : ['s', 'Ⳇ'],
    u'\u2cc7' : ['s', 'ⳇ'],
    u'\u2cc8' : ['f', 'Ⳉ'],
    u'\u2cc9' : ['f', 'ⳉ'],
    u'\u2cca' : ['_h', 'Ⳋ'],
    u'\u2ccb' : ['_h', 'ⳋ'],
    u'\u2ccc' : ['_h', 'Ⳍ'],
    u'\u2ccd' : ['_h', 'ⳍ'],
    u'\u2cce' : ['_h', 'Ⳏ'],
    u'\u2ccf' : ['_h', 'ⳏ'],
    u'\u2cd0' : ['_h', 'Ⳑ'],
    u'\u2cd1' : ['_h', 'ⳑ'],
    u'\u2cd2' : ['ei', 'Ⳓ'],
    u'\u2cd3' : ['ei', 'ⳓ'],
    u'\u2cd4' : ['_h', 'Ⳕ'],
    u'\u2cd5' : ['_h', 'ⳕ'],
    u'\u2cd6' : ['dZ', 'Ⳗ'],
    u'\u2cd7' : ['dZ', 'ⳗ'],
    u'\u2cd8' : ['dZ', 'Ⳙ'],
    u'\u2cd9' : ['dZ', 'ⳙ'],
    u'\u2cda' : ['s _h', 'Ⳛ'],
    u'\u2cdb' : ['s _h', 'ⳛ'],
    u'\u2cdc' : ['s _h', 'Ⳝ'],
    u'\u2cdd' : ['s _h', 'ⳝ'],
    u'\u2cde' : ['N i', 'Ⳟ'],
    u'\u2cdf' : ['N i', 'ⳟ'],
    u'\u2ce0' : ['n_j', 'Ⳡ'],
    u'\u2ce1' : ['n_j', 'ⳡ'],
    u'\u2ce2' : ['_w au', 'Ⳣ'],
    u'\u2ce3' : ['_w au', 'ⳣ'],
    u'\u2ce4' : ['(KAI)', 'ⳤ'],
    u'\u2ce5' : ['(MI RO)', '⳥'],
    u'\u2ce6' : ['(PI RO)', '⳦'],
    u'\u2ce7' : ['(STAUROS)', '⳧'],
    u'\u2ce8' : ['(TAU RO)', '⳨'],
    u'\u2ce9' : ['(KHI RO)', '⳩'],
    u'\u2cea' : ['(SHIMA SIMA)', '⳪'],
    u'\u2cf9' : ['(FULL STOP)', '⳹'],
    u'\u2cfa' : ['(DIRECT QUESTION MARK)', '⳺'],
    u'\u2cfb' : ['(INDIRECT QUESTION MARK)', '⳻'],
    u'\u2cfc' : ['(VERSE DIVIDER)', '⳼'],
    u'\u2cfd' : ['(FRACTION ONE HALF)', '⳽'],
    u'\u2cfe' : ['(FULL STOP)', '⳾'],
    u'\u2d00' : ['A', 'ⴀ'],
    u'\u2d01' : ['b', 'ⴁ'],
    u'\u2d02' : ['g', 'ⴂ'],
    u'\u2d03' : ['d', 'ⴃ'],
    u'\u2d04' : ['E', 'ⴄ'],
    u'\u2d05' : ['_v', 'ⴅ'],
    u'\u2d06' : ['z', 'ⴆ'],
    u'\u2d07' : ['t_h', 'ⴇ'],
    u'\u2d08' : ['i', 'ⴈ'],
    u'\u2d09' : ['k_>', 'ⴉ'],
    u'\u2d0a' : ['_l', 'ⴊ'],
    u'\u2d0b' : ['m', 'ⴋ'],
    u'\u2d0c' : ['_n', 'ⴌ'],
    u'\u2d0d' : ['o', 'ⴍ'],
    u'\u2d0e' : ['p_>', 'ⴎ'],
    u'\u2d0f' : ['z', 'ⴏ'],
    u'\u2d10' : ['_v', 'ⴐ'],
    u'\u2d11' : ['s', 'ⴑ'],
    u'\u2d12' : ['t_>', 'ⴒ'],
    u'\u2d13' : ['u', 'ⴓ'],
    u'\u2d14' : ['p_h', 'ⴔ'],
    u'\u2d15' : ['k_h', 'ⴕ'],
    u'\u2d16' : ['G', 'ⴖ'],
    u'\u2d17' : ['q>', 'ⴗ'],
    u'\u2d18' : ['S', 'ⴘ'],
    u'\u2d19' : ['tS', 'ⴙ'],
    u'\u2d1a' : ['k', 'ⴚ'],
    u'\u2d1b' : ['dz', 'ⴛ'],
    u'\u2d1c' : ['ts>', 'ⴜ'],
    u'\u2d1d' : ['tS>', 'ⴝ'],
    u'\u2d1e' : ['_x', 'ⴞ'],
    u'\u2d1f' : ['dZ', 'ⴟ'],
    u'\u2d20' : ['_h', 'ⴠ'],
    u'\u2d21' : ['ej', 'ⴡ'],
    u'\u2d22' : ['_j', 'ⴢ'],
    u'\u2d23' : ['_w i', 'ⴣ'],
    u'\u2d24' : ['q', 'ⴤ'],
    u'\u2d25' : ['oU', 'ⴥ'],
    u'\u2d30' : ['a', 'ⴰ'],
    u'\u2d31' : ['b', 'ⴱ'],
    u'\u2d32' : ['b_h', 'ⴲ'],
    u'\u2d33' : ['g', 'ⴳ'],
    u'\u2d34' : ['g_h', 'ⴴ'],
    u'\u2d35' : ['dj', 'ⴵ'],
    u'\u2d36' : ['dj', 'ⴶ'],
    u'\u2d37' : ['d', 'ⴷ'],
    u'\u2d38' : ['d_h', 'ⴸ'],
    u'\u2d39' : ['d d', 'ⴹ'],
    u'\u2d3a' : ['d d_h', 'ⴺ'],
    u'\u2d3b' : ['e', 'ⴻ'],
    u'\u2d3c' : ['f', 'ⴼ'],
    u'\u2d3d' : ['k', 'ⴽ'],
    u'\u2d3e' : ['k', 'ⴾ'],
    u'\u2d3f' : ['k_h', 'ⴿ'],
    u'\u2d40' : ['b', 'ⵀ'],
    u'\u2d41' : ['_h', 'ⵁ'],
    u'\u2d42' : ['_h', 'ⵂ'],
    u'\u2d43' : ['_h', 'ⵃ'],
    u'\u2d44' : ['{', 'ⵄ'],
    u'\u2d45' : ['k_h', 'ⵅ'],
    u'\u2d46' : ['q', 'ⵆ'],
    u'\u2d47' : ['q', 'ⵇ'],
    u'\u2d48' : ['i', 'ⵈ'],
    u'\u2d49' : ['z', 'ⵉ'],
    u'\u2d4a' : ['z', 'ⵊ'],
    u'\u2d4b' : ['z', 'ⵋ'],
    u'\u2d4c' : ['z', 'ⵌ'],
    u'\u2d4d' : ['_l', 'ⵍ'],
    u'\u2d4e' : ['m', 'ⵎ'],
    u'\u2d4f' : ['_n', 'ⵏ'],
    u'\u2d50' : ['n_j', 'ⵐ'],
    u'\u2d51' : ['N', 'ⵑ'],
    u'\u2d52' : ['p', 'ⵒ'],
    u'\u2d53' : ['u', 'ⵓ'],
    u'\u2d54' : ['`', 'ⵔ'],
    u'\u2d55' : ['r`', 'ⵕ'],
    u'\u2d56' : ['g_h', 'ⵖ'],
    u'\u2d57' : ['g_h', 'ⵗ'],
    u'\u2d58' : ['g_h', 'ⵘ'],
    u'\u2d59' : ['s', 'ⵙ'],
    u'\u2d5a' : ['s s', 'ⵚ'],
    u'\u2d5b' : ['sh', 'ⵛ'],
    u'\u2d5c' : ['t', 'ⵜ'],
    u'\u2d5d' : ['t_h', 'ⵝ'],
    u'\u2d5e' : ['c_h', 'ⵞ'],
    u'\u2d5f' : ['t t', 'ⵟ'],
    u'\u2d60' : ['_v', 'ⵠ'],
    u'\u2d61' : ['_w', 'ⵡ'],
    u'\u2d62' : ['_j', 'ⵢ'],
    u'\u2d63' : ['z', 'ⵣ'],
    u'\u2d64' : ['z', 'ⵤ'],
    u'\u2d65' : ['z z', 'ⵥ'],
    u'\u2d6f' : ['_w', 'ⵯ'],
    u'\u2d80' : ['_l o 3', 'ⶀ'],
    u'\u2d81' : ['m o 3', 'ⶁ'],
    u'\u2d82' : ['r\\` o 3', 'ⶂ'],
    u'\u2d83' : ['s o 3', 'ⶃ'],
    u'\u2d84' : ['S o 3', 'ⶄ'],
    u'\u2d85' : ['b o 3', 'ⶅ'],
    u'\u2d86' : ['t_d o 3', 'ⶆ'],
    u'\u2d87' : ['tS o 3', 'ⶇ'],
    u'\u2d88' : ['_n o 3', 'ⶈ'],
    u'\u2d89' : ['J o 3', 'ⶉ'],
    u'\u2d8a' : ['_k o 3', 'ⶊ'],
    u'\u2d8b' : ['z o 3', 'ⶋ'],
    u'\u2d8c' : ['d_d o 3', 'ⶌ'],
    u'\u2d8d' : ['d` o 3', 'ⶍ'],
    u'\u2d8e' : ['_j o 3', 'ⶎ'],
    u'\u2d8f' : ['t_> o 3', 'ⶏ'],
    u'\u2d90' : ['tS> o 3', 'ⶐ'],
    u'\u2d91' : ['p_> o 3', 'ⶑ'],
    u'\u2d92' : ['p o 3', 'ⶒ'],
    u'\u2d93' : ['g` u 3', 'ⶓ'],
    u'\u2d94' : ['g` ui', 'ⶔ'],
    u'\u2d95' : ['g` u e', 'ⶕ'],
    u'\u2d96' : ['g` u@', 'ⶖ'],
    u'\u2da0' : ['s s 3', 'ⶠ'],
    u'\u2da1' : ['s s u', 'ⶡ'],
    u'\u2da2' : ['s s i', 'ⶢ'],
    u'\u2da3' : ['s s A', 'ⶣ'],
    u'\u2da4' : ['s s e', 'ⶤ'],
    u'\u2da5' : ['s s &, s s', 'ⶥ'],
    u'\u2da6' : ['s s o', 'ⶦ'],
    u'\u2da8' : ['tS tS 3', 'ⶨ'],
    u'\u2da9' : ['tS tS u', 'ⶩ'],
    u'\u2daa' : ['tS tS i', 'ⶪ'],
    u'\u2dab' : ['tS tS A', 'ⶫ'],
    u'\u2dac' : ['tS tS e', 'ⶬ'],
    u'\u2dad' : ['tS tS &, tS tS', 'ⶭ'],
    u'\u2dae' : ['tS tS o', 'ⶮ'],
    u'\u2db0' : ['z z 3', 'ⶰ'],
    u'\u2db1' : ['z z u', 'ⶱ'],
    u'\u2db2' : ['z z i', 'ⶲ'],
    u'\u2db3' : ['z z A', 'ⶳ'],
    u'\u2db4' : ['z z e', 'ⶴ'],
    u'\u2db5' : ['z z &, z z', 'ⶵ'],
    u'\u2db6' : ['z z o', 'ⶶ'],
    u'\u2db8' : ['tS tSh 3', 'ⶸ'],
    u'\u2db9' : ['tS tSh u', 'ⶹ'],
    u'\u2dba' : ['tS tSh i', 'ⶺ'],
    u'\u2dbb' : ['tS tSh A', 'ⶻ'],
    u'\u2dbc' : ['tS tSh e', 'ⶼ'],
    u'\u2dbd' : ['tS tSh &, tS tSh', 'ⶽ'],
    u'\u2dbe' : ['tS tSh o', 'ⶾ'],
    u'\u2dc0' : ['k>j 3', 'ⷀ'],
    u'\u2dc1' : ['k>j u', 'ⷁ'],
    u'\u2dc2' : ['k>j i', 'ⷂ'],
    u'\u2dc3' : ['k>j A', 'ⷃ'],
    u'\u2dc4' : ['k>j e', 'ⷄ'],
    u'\u2dc5' : ['k>j &, k>j', 'ⷅ'],
    u'\u2dc6' : ['k>j o', 'ⷆ'],
    u'\u2dc8' : ['kj 3', 'ⷈ'],
    u'\u2dc9' : ['kj u', 'ⷉ'],
    u'\u2dca' : ['kj i', 'ⷊ'],
    u'\u2dcb' : ['kj A', 'ⷋ'],
    u'\u2dcc' : ['kj e', 'ⷌ'],
    u'\u2dce' : ['kj &, kj', 'ⷎ'],
    u'\u2dd0' : ['kj o', 'ⷐ'],
    u'\u2dd1' : ['sj 3', 'ⷑ'],
    u'\u2dd2' : ['sj u', 'ⷒ'],
    u'\u2dd3' : ['sj i', 'ⷓ'],
    u'\u2dd4' : ['sj A', 'ⷔ'],
    u'\u2dd5' : ['sj &, sj', 'ⷕ'],
    u'\u2dd6' : ['sj o', 'ⷖ'],
    u'\u2dd8' : ['gj 3', 'ⷘ'],
    u'\u2dd9' : ['gj u', 'ⷙ'],
    u'\u2dda' : ['gj i', 'ⷚ'],
    u'\u2ddb' : ['gj A', 'ⷛ'],
    u'\u2ddc' : ['gj 3', 'ⷜ'],
    u'\u2ddd' : ['gj &, gj', 'ⷝ'],
    u'\u2dde' : ['gj o', 'ⷞ'],
    u'\u3041' : ['A', 'ぁ'],
    u'\u3042' : ['A', 'あ'],
    u'\u3043' : ['i', 'ぃ'],
    u'\u3044' : ['i', 'い'],
    u'\u3045' : ['u', 'ぅ'],
    u'\u3046' : ['u', 'う'],
    u'\u3047' : ['e', 'ぇ'],
    u'\u3048' : ['e', 'え'],
    u'\u3049' : ['o', 'ぉ'],
    u'\u304a' : ['o', 'お'],
    u'\u304b' : ['k A', 'か'],
    u'\u304c' : ['g A', 'が'],
    u'\u304d' : ['k i', 'き'],
    u'\u304e' : ['g i', 'ぎ'],
    u'\u304f' : ['k u', 'く'],
    u'\u3050' : ['g u', 'ぐ'],
    u'\u3051' : ['k e', 'け'],
    u'\u3052' : ['g e', 'げ'],
    u'\u3053' : ['k o', 'こ'],
    u'\u3054' : ['g o', 'ご'],
    u'\u3055' : ['s A', 'さ'],
    u'\u3056' : ['z A', 'ざ'],
    u'\u3057' : ['s i', 'し'],
    u'\u3058' : ['z i', 'じ'],
    u'\u3059' : ['s u', 'す'],
    u'\u305a' : ['z u', 'ず'],
    u'\u305b' : ['s e', 'せ'],
    u'\u305c' : ['z e', 'ぜ'],
    u'\u305d' : ['s o', 'そ'],
    u'\u305e' : ['z o', 'ぞ'],
    u'\u305f' : ['t A', 'た'],
    u'\u3060' : ['d A', 'だ'],
    u'\u3061' : ['tS i', 'ち'],
    u'\u3062' : ['z i', 'ぢ'],
    u'\u3063' : ['ts u', 'っ'],
    u'\u3064' : ['ts u', 'つ'],
    u'\u3065' : ['z u', 'づ'],
    u'\u3066' : ['t e', 'て'],
    u'\u3067' : ['d e', 'で'],
    u'\u3068' : ['t o', 'と'],
    u'\u3069' : ['d o', 'ど'],
    u'\u306a' : ['_n A', 'な'],
    u'\u306b' : ['_n i', 'に'],
    u'\u306c' : ['_n u', 'ぬ'],
    u'\u306d' : ['_n e', 'ね'],
    u'\u306e' : ['_n o', 'の'],
    u'\u306f' : ['_h A', 'は'],
    u'\u3070' : ['b A', 'ば'],
    u'\u3071' : ['p A', 'ぱ'],
    u'\u3072' : ['_h i', 'ひ'],
    u'\u3073' : ['b i', 'び'],
    u'\u3074' : ['p i', 'ぴ'],
    u'\u3075' : ['_h u', 'ふ'],
    u'\u3076' : ['b u', 'ぶ'],
    u'\u3077' : ['p u', 'ぷ'],
    u'\u3078' : ['_h e', 'へ'],
    u'\u3079' : ['b e', 'べ'],
    u'\u307a' : ['p e', 'ぺ'],
    u'\u307b' : ['_h o', 'ほ'],
    u'\u307c' : ['b o', 'ぼ'],
    u'\u307d' : ['p o', 'ぽ'],
    u'\u307e' : ['m A', 'ま'],
    u'\u307f' : ['m i', 'み'],
    u'\u3080' : ['m u', 'む'],
    u'\u3081' : ['m e', 'め'],
    u'\u3082' : ['m o', 'も'],
    u'\u3083' : ['_j a', 'ゃ'],
    u'\u3084' : ['_j a', 'や'],
    u'\u3085' : ['yu', 'ゅ'],
    u'\u3086' : ['yu', 'ゆ'],
    u'\u3087' : ['_j o', 'ょ'],
    u'\u3088' : ['_j o', 'よ'],
    u'\u3089' : ['` A', 'ら'],
    u'\u308a' : ['` i', 'り'],
    u'\u308b' : ['` u', 'る'],
    u'\u308c' : ['` e', 'れ'],
    u'\u308d' : ['` o', 'ろ'],
    u'\u308e' : ['_w A', 'ゎ'],
    u'\u308f' : ['_w A', 'わ'],
    u'\u3090' : ['_w i', 'ゐ'],
    u'\u3091' : ['_w e', 'ゑ'],
    u'\u3092' : ['_w o', 'を'],
    u'\u3093' : ['N', 'ん'],
    u'\u3094' : ['_v u', 'ゔ'],
    u'\u3095' : ['k A', 'ゕ'],
    u'\u3096' : ['k e', 'ゖ'],
    u'\u3099' : ['(VOICED)', '゙'],
    u'\u309a' : ['(SEMI VOICED)', '゚'],
    u'\u309b' : ['(VOICED)', '゛'],
    u'\u309c' : ['(SEMI VOICED)', '゜'],
    u'\u309d' : ['(ITERATION MARK)', 'ゝ'],
    u'\u309e' : ['(VOICED ITERATION MARK)', 'ゞ'],
    u'\u309f' : ['(YORI)', 'ゟ'],
    u'\u30a0' : ['(EQUAL SIGN)', '゠'],
    u'\u30a1' : ['A', 'ァ'],
    u'\u30a2' : ['A', 'ア'],
    u'\u30a3' : ['i', 'ィ'],
    u'\u30a4' : ['i', 'イ'],
    u'\u30a5' : ['u', 'ゥ'],
    u'\u30a6' : ['u', 'ウ'],
    u'\u30a7' : ['e', 'ェ'],
    u'\u30a8' : ['e', 'エ'],
    u'\u30a9' : ['o', 'ォ'],
    u'\u30aa' : ['o', 'オ'],
    u'\u30ab' : ['k A', 'カ'],
    u'\u30ac' : ['g A', 'ガ'],
    u'\u30ad' : ['k i', 'キ'],
    u'\u30ae' : ['g i', 'ギ'],
    u'\u30af' : ['k u', 'ク'],
    u'\u30b0' : ['g u', 'グ'],
    u'\u30b1' : ['k e', 'ケ'],
    u'\u30b2' : ['g e', 'ゲ'],
    u'\u30b3' : ['k o', 'コ'],
    u'\u30b4' : ['g o', 'ゴ'],
    u'\u30b5' : ['s A', 'サ'],
    u'\u30b6' : ['z A', 'ザ'],
    u'\u30b7' : ['s i', 'シ'],
    u'\u30b8' : ['z i', 'ジ'],
    u'\u30b9' : ['s u', 'ス'],
    u'\u30ba' : ['z u', 'ズ'],
    u'\u30bb' : ['s e', 'セ'],
    u'\u30bc' : ['z e', 'ゼ'],
    u'\u30bd' : ['s o', 'ソ'],
    u'\u30be' : ['z o', 'ゾ'],
    u'\u30bf' : ['t A', 'タ'],
    u'\u30c0' : ['d A', 'ダ'],
    u'\u30c1' : ['tS i', 'チ'],
    u'\u30c2' : ['z i', 'ヂ'],
    u'\u30c3' : ['ts u', 'ッ'],
    u'\u30c4' : ['ts u', 'ツ'],
    u'\u30c5' : ['z u', 'ヅ'],
    u'\u30c6' : ['t e', 'テ'],
    u'\u30c7' : ['d e', 'デ'],
    u'\u30c8' : ['t o', 'ト'],
    u'\u30c9' : ['d o', 'ド'],
    u'\u30ca' : ['_n A', 'ナ'],
    u'\u30cb' : ['_n i', 'ニ'],
    u'\u30cc' : ['_n u', 'ヌ'],
    u'\u30cd' : ['_n e', 'ネ'],
    u'\u30ce' : ['_n o', 'ノ'],
    u'\u30cf' : ['_h A', 'ハ'],
    u'\u30d0' : ['b A', 'バ'],
    u'\u30d1' : ['p A', 'パ'],
    u'\u30d2' : ['_h i', 'ヒ'],
    u'\u30d3' : ['b i', 'ビ'],
    u'\u30d4' : ['p i', 'ピ'],
    u'\u30d5' : ['_h u', 'フ'],
    u'\u30d6' : ['b u', 'ブ'],
    u'\u30d7' : ['p u', 'プ'],
    u'\u30d8' : ['_h e', 'ヘ'],
    u'\u30d9' : ['b e', 'ベ'],
    u'\u30da' : ['p e', 'ペ'],
    u'\u30db' : ['_h o', 'ホ'],
    u'\u30dc' : ['b o', 'ボ'],
    u'\u30dd' : ['p o', 'ポ'],
    u'\u30de' : ['m A', 'マ'],
    u'\u30df' : ['m i', 'ミ'],
    u'\u30e0' : ['m u', 'ム'],
    u'\u30e1' : ['m e', 'メ'],
    u'\u30e2' : ['m o', 'モ'],
    u'\u30e3' : ['_j a', 'ャ'],
    u'\u30e4' : ['_j a', 'ヤ'],
    u'\u30e5' : ['yu', 'ュ'],
    u'\u30e6' : ['yu', 'ユ'],
    u'\u30e7' : ['_j o', 'ョ'],
    u'\u30e8' : ['_j o', 'ヨ'],
    u'\u30e9' : ['` A', 'ラ'],
    u'\u30ea' : ['` i', 'リ'],
    u'\u30eb' : ['` u', 'ル'],
    u'\u30ec' : ['` e', 'レ'],
    u'\u30ed' : ['` o', 'ロ'],
    u'\u30ee' : ['_w A', 'ヮ'],
    u'\u30ef' : ['_w A', 'ワ'],
    u'\u30f0' : ['_w i', 'ヰ'],
    u'\u30f1' : ['_w e', 'ヱ'],
    u'\u30f2' : ['_w o', 'ヲ'],
    u'\u30f3' : ['N', 'ン'],
    u'\u30f4' : ['_v u', 'ヴ'],
    u'\u30f5' : ['k A', 'ヵ'],
    u'\u30f6' : ['k e', 'ヶ'],
    u'\u30f7' : ['_v A', 'ヷ'],
    u'\u30f8' : ['_v i', 'ヸ'],
    u'\u30f9' : ['_v e', 'ヹ'],
    u'\u30fa' : ['_v o', 'ヺ'],
    u'\u30fb' : ['(MIDDLE DOT)', '・'],
    u'\u30fc' : ['(PROLONGED SOUND MARK)', 'ー'],
    u'\u30fd' : ['(ITERATION MARK)', 'ヽ'],
    u'\u30fe' : ['(VOICED ITERATION MARK)', 'ヾ'],
    u'\u3131' : ['k', 'ㄱ'],
    u'\u3132' : ['k_>', 'ㄲ'],
    u'\u3133' : ['k sh', 'ㄳ'],
    u'\u3134' : ['_n', 'ㄴ'],
    u'\u3135' : ['_n tS', 'ㄵ'],
    u'\u3136' : ['_n _h', 'ㄶ'],
    u'\u3137' : ['t', 'ㄷ'],
    u'\u3138' : ['t_>', 'ㄸ'],
    u'\u3139' : ['`', 'ㄹ'],
    u'\u313a' : ['_l k', 'ㄺ'],
    u'\u313b' : ['_l m', 'ㄻ'],
    u'\u313c' : ['_l b', 'ㄼ'],
    u'\u313d' : ['_l sh', 'ㄽ'],
    u'\u313e' : ['_l t_h', 'ㄾ'],
    u'\u313f' : ['_l p_h', 'ㄿ'],
    u'\u3140' : ['_l _h', 'ㅀ'],
    u'\u3141' : ['m', 'ㅁ'],
    u'\u3142' : ['p', 'ㅂ'],
    u'\u3143' : ['p_>', 'ㅃ'],
    u'\u3144' : ['b sh', 'ㅄ'],
    u'\u3145' : ['sh', 'ㅅ'],
    u'\u3146' : ['s', 'ㅆ'],
    u'\u3147' : ['(SILENT)', 'ㅇ'],
    u'\u3148' : ['tS', 'ㅈ'],
    u'\u3149' : ['tS>', 'ㅉ'],
    u'\u314a' : ['tSh', 'ㅊ'],
    u'\u314b' : ['k_h', 'ㅋ'],
    u'\u314c' : ['t_h', 'ㅌ'],
    u'\u314d' : ['p_h', 'ㅍ'],
    u'\u314e' : ['_h', 'ㅎ'],
    u'\u314f' : ['a', 'ㅏ'],
    u'\u3150' : ['{', 'ㅐ'],
    u'\u3151' : ['_j a', 'ㅑ'],
    u'\u3152' : ['J\\ {', 'ㅒ'],
    u'\u3153' : ['_r', 'ㅓ'],
    u'\u3154' : ['e', 'ㅔ'],
    u'\u3155' : ['_j _r', 'ㅕ'],
    u'\u3156' : ['_j e', 'ㅖ'],
    u'\u3157' : ['o', 'ㅗ'],
    u'\u3158' : ['_w a', 'ㅘ'],
    u'\u3159' : ['_w {', 'ㅙ'],
    u'\u315a' : ['_w e', 'ㅚ'],
    u'\u315b' : ['_j o', 'ㅛ'],
    u'\u315c' : ['u', 'ㅜ'],
    u'\u315d' : ['_w _r', 'ㅝ'],
    u'\u315e' : ['_w E', 'ㅞ'],
    u'\u315f' : ['2', 'ㅟ'],
    u'\u3160' : ['_j u', 'ㅠ'],
    u'\u3161' : ['M', 'ㅡ'],
    u'\u3162' : ['M _j', 'ㅢ'],
    u'\u3163' : ['i', 'ㅣ'],
    u'\u3164' : ['(HANGUL FILLER)', 'ㅤ'],
    u'\ua000' : ['i', 'ꀀ'],
    u'\ua001' : ['i', 'ꀁ'],
    u'\ua002' : ['i', 'ꀂ'],
    u'\ua003' : ['i', 'ꀃ'],
    u'\ua004' : ['e', 'ꀄ'],
    u'\ua005' : ['e', 'ꀅ'],
    u'\ua006' : ['e', 'ꀆ'],
    u'\ua007' : ['e', 'ꀇ'],
    u'\ua008' : ['A', 'ꀈ'],
    u'\ua009' : ['A', 'ꀉ'],
    u'\ua00a' : ['A', 'ꀊ'],
    u'\ua00b' : ['A', 'ꀋ'],
    u'\ua00c' : ['_>', 'ꀌ'],
    u'\ua00d' : ['_>', 'ꀍ'],
    u'\ua00e' : ['_>', 'ꀎ'],
    u'\ua00f' : ['o', 'ꀏ'],
    u'\ua010' : ['o', 'ꀐ'],
    u'\ua011' : ['o', 'ꀑ'],
    u'\ua012' : ['o', 'ꀒ'],
    u'\ua013' : ['@', 'ꀓ'],
    u'\ua014' : ['@', 'ꀔ'],
    u'\ua015' : ['G u', 'ꀕ'],
    u'\ua016' : ['p i', 'ꀖ'],
    u'\ua017' : ['p i', 'ꀗ'],
    u'\ua018' : ['p i', 'ꀘ'],
    u'\ua019' : ['p i', 'ꀙ'],
    u'\ua01a' : ['p e', 'ꀚ'],
    u'\ua01b' : ['p e', 'ꀛ'],
    u'\ua01c' : ['p e', 'ꀜ'],
    u'\ua01d' : ['p e', 'ꀝ'],
    u'\ua01e' : ['p A', 'ꀞ'],
    u'\ua01f' : ['p A', 'ꀟ'],
    u'\ua020' : ['p A', 'ꀠ'],
    u'\ua021' : ['p A', 'ꀡ'],
    u'\ua022' : ['p _>', 'ꀢ'],
    u'\ua023' : ['p _>', 'ꀣ'],
    u'\ua024' : ['p _>', 'ꀤ'],
    u'\ua025' : ['p o', 'ꀥ'],
    u'\ua026' : ['p o', 'ꀦ'],
    u'\ua027' : ['p o', 'ꀧ'],
    u'\ua028' : ['p o', 'ꀨ'],
    u'\ua029' : ['p @', 'ꀩ'],
    u'\ua02a' : ['p @', 'ꀪ'],
    u'\ua02b' : ['p @', 'ꀫ'],
    u'\ua02c' : ['p u', 'ꀬ'],
    u'\ua02d' : ['p u', 'ꀭ'],
    u'\ua02e' : ['p u', 'ꀮ'],
    u'\ua02f' : ['p u', 'ꀯ'],
    u'\ua030' : ['p u', 'ꀰ'],
    u'\ua031' : ['p u', 'ꀱ'],
    u'\ua032' : ['p _l', 'ꀲ'],
    u'\ua033' : ['p _l', 'ꀳ'],
    u'\ua034' : ['p _l', 'ꀴ'],
    u'\ua035' : ['p _l', 'ꀵ'],
    u'\ua036' : ['p i', 'ꀶ'],
    u'\ua037' : ['p i', 'ꀷ'],
    u'\ua038' : ['p_h i', 'ꀸ'],
    u'\ua039' : ['p_h i', 'ꀹ'],
    u'\ua03a' : ['p_h i', 'ꀺ'],
    u'\ua03b' : ['p_h i', 'ꀻ'],
    u'\ua03c' : ['p_h e', 'ꀼ'],
    u'\ua03d' : ['p_h e', 'ꀽ'],
    u'\ua03e' : ['p_h e', 'ꀾ'],
    u'\ua03f' : ['p_h A', 'ꀿ'],
    u'\ua040' : ['p_h A', 'ꁀ'],
    u'\ua041' : ['p_h A', 'ꁁ'],
    u'\ua042' : ['p_h A', 'ꁂ'],
    u'\ua043' : ['p_h _>', 'ꁃ'],
    u'\ua044' : ['p_h _>', 'ꁄ'],
    u'\ua045' : ['p_h _>', 'ꁅ'],
    u'\ua046' : ['p_h o', 'ꁆ'],
    u'\ua047' : ['p_h o', 'ꁇ'],
    u'\ua048' : ['p_h o', 'ꁈ'],
    u'\ua049' : ['p_h o', 'ꁉ'],
    u'\ua04a' : ['p_h u', 'ꁊ'],
    u'\ua04b' : ['p_h u', 'ꁋ'],
    u'\ua04c' : ['p_h u', 'ꁌ'],
    u'\ua04d' : ['p_h u', 'ꁍ'],
    u'\ua04e' : ['p_h u', 'ꁎ'],
    u'\ua04f' : ['p_h u', 'ꁏ'],
    u'\ua050' : ['p_h _l', 'ꁐ'],
    u'\ua051' : ['p_h _l', 'ꁑ'],
    u'\ua052' : ['p_h _l', 'ꁒ'],
    u'\ua053' : ['p_h _l', 'ꁓ'],
    u'\ua054' : ['p_h i', 'ꁔ'],
    u'\ua055' : ['p_h i', 'ꁕ'],
    u'\ua056' : ['b i', 'ꁖ'],
    u'\ua057' : ['b i', 'ꁗ'],
    u'\ua058' : ['b i', 'ꁘ'],
    u'\ua059' : ['b i', 'ꁙ'],
    u'\ua05a' : ['b e', 'ꁚ'],
    u'\ua05b' : ['b e', 'ꁛ'],
    u'\ua05c' : ['b e', 'ꁜ'],
    u'\ua05d' : ['b e', 'ꁝ'],
    u'\ua05e' : ['b A', 'ꁞ'],
    u'\ua05f' : ['b A', 'ꁟ'],
    u'\ua060' : ['b A', 'ꁠ'],
    u'\ua061' : ['b A', 'ꁡ'],
    u'\ua062' : ['b _>', 'ꁢ'],
    u'\ua063' : ['b _>', 'ꁣ'],
    u'\ua064' : ['b _>', 'ꁤ'],
    u'\ua065' : ['b o', 'ꁥ'],
    u'\ua066' : ['b o', 'ꁦ'],
    u'\ua067' : ['b o', 'ꁧ'],
    u'\ua068' : ['b o', 'ꁨ'],
    u'\ua069' : ['b @', 'ꁩ'],
    u'\ua06a' : ['b @', 'ꁪ'],
    u'\ua06b' : ['b @', 'ꁫ'],
    u'\ua06c' : ['b u', 'ꁬ'],
    u'\ua06d' : ['b u', 'ꁭ'],
    u'\ua06e' : ['b u', 'ꁮ'],
    u'\ua06f' : ['b u', 'ꁯ'],
    u'\ua070' : ['b u', 'ꁰ'],
    u'\ua071' : ['b u', 'ꁱ'],
    u'\ua072' : ['b _l', 'ꁲ'],
    u'\ua073' : ['b _l', 'ꁳ'],
    u'\ua074' : ['b _l', 'ꁴ'],
    u'\ua075' : ['b _l', 'ꁵ'],
    u'\ua076' : ['m b i', 'ꁶ'],
    u'\ua077' : ['m b i', 'ꁷ'],
    u'\ua078' : ['m b i', 'ꁸ'],
    u'\ua079' : ['m b i', 'ꁹ'],
    u'\ua07a' : ['m b e', 'ꁺ'],
    u'\ua07b' : ['m b e', 'ꁻ'],
    u'\ua07c' : ['m b e', 'ꁼ'],
    u'\ua07d' : ['m b A', 'ꁽ'],
    u'\ua07e' : ['m b A', 'ꁾ'],
    u'\ua07f' : ['m b A', 'ꁿ'],
    u'\ua080' : ['m b A', 'ꂀ'],
    u'\ua081' : ['m b o', 'ꂁ'],
    u'\ua082' : ['m b o', 'ꂂ'],
    u'\ua083' : ['m b o', 'ꂃ'],
    u'\ua084' : ['m b o', 'ꂄ'],
    u'\ua085' : ['m b u', 'ꂅ'],
    u'\ua086' : ['m b u', 'ꂆ'],
    u'\ua087' : ['m b u', 'ꂇ'],
    u'\ua088' : ['m b u', 'ꂈ'],
    u'\ua089' : ['m b u', 'ꂉ'],
    u'\ua08a' : ['m b u', 'ꂊ'],
    u'\ua08b' : ['m b _l', 'ꂋ'],
    u'\ua08c' : ['m b _l', 'ꂌ'],
    u'\ua08d' : ['m b _l', 'ꂍ'],
    u'\ua08e' : ['m b _l', 'ꂎ'],
    u'\ua08f' : ['m b i', 'ꂏ'],
    u'\ua090' : ['m b i', 'ꂐ'],
    u'\ua091' : ['m0 i', 'ꂑ'],
    u'\ua092' : ['m0 i', 'ꂒ'],
    u'\ua093' : ['m0 i', 'ꂓ'],
    u'\ua094' : ['m0 i', 'ꂔ'],
    u'\ua095' : ['m0 e', 'ꂕ'],
    u'\ua096' : ['m0 e', 'ꂖ'],
    u'\ua097' : ['m0 e', 'ꂗ'],
    u'\ua098' : ['m0 A', 'ꂘ'],
    u'\ua099' : ['m0 A', 'ꂙ'],
    u'\ua09a' : ['m0 A', 'ꂚ'],
    u'\ua09b' : ['m0 A', 'ꂛ'],
    u'\ua09c' : ['m0 _>', 'ꂜ'],
    u'\ua09d' : ['m0 _>', 'ꂝ'],
    u'\ua09e' : ['m0 _>', 'ꂞ'],
    u'\ua09f' : ['m0 o', 'ꂟ'],
    u'\ua0a0' : ['m0 o', 'ꂠ'],
    u'\ua0a1' : ['m0 o', 'ꂡ'],
    u'\ua0a2' : ['m0 o', 'ꂢ'],
    u'\ua0a3' : ['m0 u', 'ꂣ'],
    u'\ua0a4' : ['m0 u', 'ꂤ'],
    u'\ua0a5' : ['m0 u', 'ꂥ'],
    u'\ua0a6' : ['m0 u', 'ꂦ'],
    u'\ua0a7' : ['m0 u', 'ꂧ'],
    u'\ua0a8' : ['m0 u', 'ꂨ'],
    u'\ua0a9' : ['m0 _l', 'ꂩ'],
    u'\ua0aa' : ['m0 _l', 'ꂪ'],
    u'\ua0ab' : ['m0 _l', 'ꂫ'],
    u'\ua0ac' : ['m0 i', 'ꂬ'],
    u'\ua0ad' : ['m0 i', 'ꂭ'],
    u'\ua0ae' : ['m i', 'ꂮ'],
    u'\ua0af' : ['m i', 'ꂯ'],
    u'\ua0b0' : ['m i', 'ꂰ'],
    u'\ua0b1' : ['m i', 'ꂱ'],
    u'\ua0b2' : ['m e', 'ꂲ'],
    u'\ua0b3' : ['m e', 'ꂳ'],
    u'\ua0b4' : ['m e', 'ꂴ'],
    u'\ua0b5' : ['m A', 'ꂵ'],
    u'\ua0b6' : ['m A', 'ꂶ'],
    u'\ua0b7' : ['m A', 'ꂷ'],
    u'\ua0b8' : ['m A', 'ꂸ'],
    u'\ua0b9' : ['m _>', 'ꂹ'],
    u'\ua0ba' : ['m _>', 'ꂺ'],
    u'\ua0bb' : ['m _>', 'ꂻ'],
    u'\ua0bc' : ['m _>', 'ꂼ'],
    u'\ua0bd' : ['m o', 'ꂽ'],
    u'\ua0be' : ['m o', 'ꂾ'],
    u'\ua0bf' : ['m o', 'ꂿ'],
    u'\ua0c0' : ['m o', 'ꃀ'],
    u'\ua0c1' : ['m @', 'ꃁ'],
    u'\ua0c2' : ['m @', 'ꃂ'],
    u'\ua0c3' : ['m u', 'ꃃ'],
    u'\ua0c4' : ['m u', 'ꃄ'],
    u'\ua0c5' : ['m u', 'ꃅ'],
    u'\ua0c6' : ['m u', 'ꃆ'],
    u'\ua0c7' : ['m u', 'ꃇ'],
    u'\ua0c8' : ['m u', 'ꃈ'],
    u'\ua0c9' : ['m _l', 'ꃉ'],
    u'\ua0ca' : ['m _l', 'ꃊ'],
    u'\ua0cb' : ['m _l', 'ꃋ'],
    u'\ua0cc' : ['m _l', 'ꃌ'],
    u'\ua0cd' : ['f i', 'ꃍ'],
    u'\ua0ce' : ['f i', 'ꃎ'],
    u'\ua0cf' : ['f i', 'ꃏ'],
    u'\ua0d0' : ['f i', 'ꃐ'],
    u'\ua0d1' : ['f A', 'ꃑ'],
    u'\ua0d2' : ['f A', 'ꃒ'],
    u'\ua0d3' : ['f A', 'ꃓ'],
    u'\ua0d4' : ['f A', 'ꃔ'],
    u'\ua0d5' : ['f o', 'ꃕ'],
    u'\ua0d6' : ['f o', 'ꃖ'],
    u'\ua0d7' : ['f o', 'ꃗ'],
    u'\ua0d8' : ['f u', 'ꃘ'],
    u'\ua0d9' : ['f u', 'ꃙ'],
    u'\ua0da' : ['f u', 'ꃚ'],
    u'\ua0db' : ['f u', 'ꃛ'],
    u'\ua0dc' : ['f u', 'ꃜ'],
    u'\ua0dd' : ['f u', 'ꃝ'],
    u'\ua0de' : ['f _l', 'ꃞ'],
    u'\ua0df' : ['f _l', 'ꃟ'],
    u'\ua0e0' : ['f _l', 'ꃠ'],
    u'\ua0e1' : ['f _l', 'ꃡ'],
    u'\ua0e2' : ['_v i', 'ꃢ'],
    u'\ua0e3' : ['_v i', 'ꃣ'],
    u'\ua0e4' : ['_v i', 'ꃤ'],
    u'\ua0e5' : ['_v i', 'ꃥ'],
    u'\ua0e6' : ['_v e', 'ꃦ'],
    u'\ua0e7' : ['_v e', 'ꃧ'],
    u'\ua0e8' : ['_v e', 'ꃨ'],
    u'\ua0e9' : ['_v e', 'ꃩ'],
    u'\ua0ea' : ['_v A', 'ꃪ'],
    u'\ua0eb' : ['_v A', 'ꃫ'],
    u'\ua0ec' : ['_v A', 'ꃬ'],
    u'\ua0ed' : ['_v A', 'ꃭ'],
    u'\ua0ee' : ['_v o', 'ꃮ'],
    u'\ua0ef' : ['_v o', 'ꃯ'],
    u'\ua0f0' : ['_v o', 'ꃰ'],
    u'\ua0f1' : ['_v o', 'ꃱ'],
    u'\ua0f2' : ['_v @', 'ꃲ'],
    u'\ua0f3' : ['_v @', 'ꃳ'],
    u'\ua0f4' : ['_v u', 'ꃴ'],
    u'\ua0f5' : ['_v u', 'ꃵ'],
    u'\ua0f6' : ['_v u', 'ꃶ'],
    u'\ua0f7' : ['_v u', 'ꃷ'],
    u'\ua0f8' : ['_v u', 'ꃸ'],
    u'\ua0f9' : ['_v u', 'ꃹ'],
    u'\ua0fa' : ['_v _l', 'ꃺ'],
    u'\ua0fb' : ['_v _l', 'ꃻ'],
    u'\ua0fc' : ['_v _l', 'ꃼ'],
    u'\ua0fd' : ['_v _l', 'ꃽ'],
    u'\ua0fe' : ['_v i', 'ꃾ'],
    u'\ua0ff' : ['_v i', 'ꃿ'],
    u'\ua100' : ['t i', 'ꄀ'],
    u'\ua101' : ['t i', 'ꄁ'],
    u'\ua102' : ['t i', 'ꄂ'],
    u'\ua103' : ['t i', 'ꄃ'],
    u'\ua104' : ['t e', 'ꄄ'],
    u'\ua105' : ['t e', 'ꄅ'],
    u'\ua106' : ['t e', 'ꄆ'],
    u'\ua107' : ['t A', 'ꄇ'],
    u'\ua108' : ['t A', 'ꄈ'],
    u'\ua109' : ['t A', 'ꄉ'],
    u'\ua10a' : ['t A', 'ꄊ'],
    u'\ua10b' : ['t _>', 'ꄋ'],
    u'\ua10c' : ['t _>', 'ꄌ'],
    u'\ua10d' : ['t o', 'ꄍ'],
    u'\ua10e' : ['t o', 'ꄎ'],
    u'\ua10f' : ['t o', 'ꄏ'],
    u'\ua110' : ['t o', 'ꄐ'],
    u'\ua111' : ['t @', 'ꄑ'],
    u'\ua112' : ['t @', 'ꄒ'],
    u'\ua113' : ['t @', 'ꄓ'],
    u'\ua114' : ['t u', 'ꄔ'],
    u'\ua115' : ['t u', 'ꄕ'],
    u'\ua116' : ['t u', 'ꄖ'],
    u'\ua117' : ['t u', 'ꄗ'],
    u'\ua118' : ['t u', 'ꄘ'],
    u'\ua119' : ['t u', 'ꄙ'],
    u'\ua11a' : ['t_h i', 'ꄚ'],
    u'\ua11b' : ['t_h i', 'ꄛ'],
    u'\ua11c' : ['t_h i', 'ꄜ'],
    u'\ua11d' : ['t_h i', 'ꄝ'],
    u'\ua11e' : ['t_h e', 'ꄞ'],
    u'\ua11f' : ['t_h e', 'ꄟ'],
    u'\ua120' : ['t_h e', 'ꄠ'],
    u'\ua121' : ['t_h A', 'ꄡ'],
    u'\ua122' : ['t_h A', 'ꄢ'],
    u'\ua123' : ['t_h A', 'ꄣ'],
    u'\ua124' : ['t_h A', 'ꄤ'],
    u'\ua125' : ['t_h _>', 'ꄥ'],
    u'\ua126' : ['t_h _>', 'ꄦ'],
    u'\ua127' : ['t_h _>', 'ꄧ'],
    u'\ua128' : ['t_h _>', 'ꄨ'],
    u'\ua129' : ['t_h o', 'ꄩ'],
    u'\ua12a' : ['t_h o', 'ꄪ'],
    u'\ua12b' : ['t_h o', 'ꄫ'],
    u'\ua12c' : ['t_h o', 'ꄬ'],
    u'\ua12d' : ['t_h @', 'ꄭ'],
    u'\ua12e' : ['t_h @', 'ꄮ'],
    u'\ua12f' : ['t_h @', 'ꄯ'],
    u'\ua130' : ['t_h u', 'ꄰ'],
    u'\ua131' : ['t_h u', 'ꄱ'],
    u'\ua132' : ['t_h u', 'ꄲ'],
    u'\ua133' : ['t_h u', 'ꄳ'],
    u'\ua134' : ['t_h u', 'ꄴ'],
    u'\ua135' : ['t_h u', 'ꄵ'],
    u'\ua136' : ['d i', 'ꄶ'],
    u'\ua137' : ['d i', 'ꄷ'],
    u'\ua138' : ['d i', 'ꄸ'],
    u'\ua139' : ['d i', 'ꄹ'],
    u'\ua13a' : ['d e', 'ꄺ'],
    u'\ua13b' : ['d e', 'ꄻ'],
    u'\ua13c' : ['d e', 'ꄼ'],
    u'\ua13d' : ['d A', 'ꄽ'],
    u'\ua13e' : ['d A', 'ꄾ'],
    u'\ua13f' : ['d A', 'ꄿ'],
    u'\ua140' : ['d A', 'ꅀ'],
    u'\ua141' : ['d _>', 'ꅁ'],
    u'\ua142' : ['d _>', 'ꅂ'],
    u'\ua143' : ['d _>', 'ꅃ'],
    u'\ua144' : ['d o', 'ꅄ'],
    u'\ua145' : ['d o', 'ꅅ'],
    u'\ua146' : ['d o', 'ꅆ'],
    u'\ua147' : ['d o', 'ꅇ'],
    u'\ua148' : ['d @', 'ꅈ'],
    u'\ua149' : ['d @', 'ꅉ'],
    u'\ua14a' : ['d @', 'ꅊ'],
    u'\ua14b' : ['d u', 'ꅋ'],
    u'\ua14c' : ['d u', 'ꅌ'],
    u'\ua14d' : ['d u', 'ꅍ'],
    u'\ua14e' : ['d u', 'ꅎ'],
    u'\ua14f' : ['d u', 'ꅏ'],
    u'\ua150' : ['d u', 'ꅐ'],
    u'\ua151' : ['_n d i', 'ꅑ'],
    u'\ua152' : ['_n d i', 'ꅒ'],
    u'\ua153' : ['_n d i', 'ꅓ'],
    u'\ua154' : ['_n d i', 'ꅔ'],
    u'\ua155' : ['_n d e', 'ꅕ'],
    u'\ua156' : ['_n d e', 'ꅖ'],
    u'\ua157' : ['_n d A', 'ꅗ'],
    u'\ua158' : ['_n d A', 'ꅘ'],
    u'\ua159' : ['_n d A', 'ꅙ'],
    u'\ua15a' : ['_n d A', 'ꅚ'],
    u'\ua15b' : ['_n d o', 'ꅛ'],
    u'\ua15c' : ['_n d o', 'ꅜ'],
    u'\ua15d' : ['_n d o', 'ꅝ'],
    u'\ua15e' : ['_n d o', 'ꅞ'],
    u'\ua15f' : ['_n d @', 'ꅟ'],
    u'\ua160' : ['_n d @', 'ꅠ'],
    u'\ua161' : ['_n d @', 'ꅡ'],
    u'\ua162' : ['_n d u', 'ꅢ'],
    u'\ua163' : ['_n d u', 'ꅣ'],
    u'\ua164' : ['_n d u', 'ꅤ'],
    u'\ua165' : ['_n d u', 'ꅥ'],
    u'\ua166' : ['_n d u', 'ꅦ'],
    u'\ua167' : ['_n d u', 'ꅧ'],
    u'\ua168' : ['n0 i', 'ꅨ'],
    u'\ua169' : ['n0 i', 'ꅩ'],
    u'\ua16a' : ['n0 i', 'ꅪ'],
    u'\ua16b' : ['n0 i', 'ꅫ'],
    u'\ua16c' : ['n0 e', 'ꅬ'],
    u'\ua16d' : ['n0 e', 'ꅭ'],
    u'\ua16e' : ['n0 e', 'ꅮ'],
    u'\ua16f' : ['n0 e', 'ꅯ'],
    u'\ua170' : ['n0 A', 'ꅰ'],
    u'\ua171' : ['n0 A', 'ꅱ'],
    u'\ua172' : ['n0 A', 'ꅲ'],
    u'\ua173' : ['n0 A', 'ꅳ'],
    u'\ua174' : ['n0 _>', 'ꅴ'],
    u'\ua175' : ['n0 _>', 'ꅵ'],
    u'\ua176' : ['n0 o', 'ꅶ'],
    u'\ua177' : ['n0 o', 'ꅷ'],
    u'\ua178' : ['n0 o', 'ꅸ'],
    u'\ua179' : ['n0 @', 'ꅹ'],
    u'\ua17a' : ['n0 @', 'ꅺ'],
    u'\ua17b' : ['n0 @', 'ꅻ'],
    u'\ua17c' : ['n0 u', 'ꅼ'],
    u'\ua17d' : ['_n i', 'ꅽ'],
    u'\ua17e' : ['_n i', 'ꅾ'],
    u'\ua17f' : ['_n i', 'ꅿ'],
    u'\ua180' : ['_n i', 'ꆀ'],
    u'\ua181' : ['_n e', 'ꆁ'],
    u'\ua182' : ['_n e', 'ꆂ'],
    u'\ua183' : ['_n e', 'ꆃ'],
    u'\ua184' : ['_n A', 'ꆄ'],
    u'\ua185' : ['_n A', 'ꆅ'],
    u'\ua186' : ['_n A', 'ꆆ'],
    u'\ua187' : ['_n _>', 'ꆇ'],
    u'\ua188' : ['_n _>', 'ꆈ'],
    u'\ua189' : ['_n _>', 'ꆉ'],
    u'\ua18a' : ['_n o', 'ꆊ'],
    u'\ua18b' : ['_n o', 'ꆋ'],
    u'\ua18c' : ['_n o', 'ꆌ'],
    u'\ua18d' : ['_n o', 'ꆍ'],
    u'\ua18e' : ['_n @', 'ꆎ'],
    u'\ua18f' : ['_n @', 'ꆏ'],
    u'\ua190' : ['_n @', 'ꆐ'],
    u'\ua191' : ['_n u', 'ꆑ'],
    u'\ua192' : ['_n u', 'ꆒ'],
    u'\ua193' : ['_n u', 'ꆓ'],
    u'\ua194' : ['_n u', 'ꆔ'],
    u'\ua195' : ['_n u', 'ꆕ'],
    u'\ua196' : ['_n u', 'ꆖ'],
    u'\ua197' : ['K i', 'ꆗ'],
    u'\ua198' : ['K i', 'ꆘ'],
    u'\ua199' : ['K i', 'ꆙ'],
    u'\ua19a' : ['K i', 'ꆚ'],
    u'\ua19b' : ['K e', 'ꆛ'],
    u'\ua19c' : ['K e', 'ꆜ'],
    u'\ua19d' : ['K e', 'ꆝ'],
    u'\ua19e' : ['K A', 'ꆞ'],
    u'\ua19f' : ['K A', 'ꆟ'],
    u'\ua1a0' : ['K A', 'ꆠ'],
    u'\ua1a1' : ['K A', 'ꆡ'],
    u'\ua1a2' : ['K _>', 'ꆢ'],
    u'\ua1a3' : ['K _>', 'ꆣ'],
    u'\ua1a4' : ['K _>', 'ꆤ'],
    u'\ua1a5' : ['K o', 'ꆥ'],
    u'\ua1a6' : ['K o', 'ꆦ'],
    u'\ua1a7' : ['K o', 'ꆧ'],
    u'\ua1a8' : ['K @', 'ꆨ'],
    u'\ua1a9' : ['K @', 'ꆩ'],
    u'\ua1aa' : ['K @', 'ꆪ'],
    u'\ua1ab' : ['K u', 'ꆫ'],
    u'\ua1ac' : ['K u', 'ꆬ'],
    u'\ua1ad' : ['K u', 'ꆭ'],
    u'\ua1ae' : ['K u', 'ꆮ'],
    u'\ua1af' : ['K u', 'ꆯ'],
    u'\ua1b0' : ['K u', 'ꆰ'],
    u'\ua1b1' : ['K _l', 'ꆱ'],
    u'\ua1b2' : ['K _l', 'ꆲ'],
    u'\ua1b3' : ['K _l', 'ꆳ'],
    u'\ua1b4' : ['K _l', 'ꆴ'],
    u'\ua1b5' : ['K i', 'ꆵ'],
    u'\ua1b6' : ['K i', 'ꆶ'],
    u'\ua1b7' : ['_l i', 'ꆷ'],
    u'\ua1b8' : ['_l i', 'ꆸ'],
    u'\ua1b9' : ['_l i', 'ꆹ'],
    u'\ua1ba' : ['_l i', 'ꆺ'],
    u'\ua1bb' : ['_l e', 'ꆻ'],
    u'\ua1bc' : ['_l e', 'ꆼ'],
    u'\ua1bd' : ['_l e', 'ꆽ'],
    u'\ua1be' : ['_l e', 'ꆾ'],
    u'\ua1bf' : ['_l A', 'ꆿ'],
    u'\ua1c0' : ['_l A', 'ꇀ'],
    u'\ua1c1' : ['_l A', 'ꇁ'],
    u'\ua1c2' : ['_l A', 'ꇂ'],
    u'\ua1c3' : ['_l _>', 'ꇃ'],
    u'\ua1c4' : ['_l _>', 'ꇄ'],
    u'\ua1c5' : ['_l _>', 'ꇅ'],
    u'\ua1c6' : ['_l _>', 'ꇆ'],
    u'\ua1c7' : ['_l o', 'ꇇ'],
    u'\ua1c8' : ['_l o', 'ꇈ'],
    u'\ua1c9' : ['_l o', 'ꇉ'],
    u'\ua1ca' : ['_l o', 'ꇊ'],
    u'\ua1cb' : ['_l @', 'ꇋ'],
    u'\ua1cc' : ['_l @', 'ꇌ'],
    u'\ua1cd' : ['_l @', 'ꇍ'],
    u'\ua1ce' : ['_l u', 'ꇎ'],
    u'\ua1cf' : ['_l u', 'ꇏ'],
    u'\ua1d0' : ['_l u', 'ꇐ'],
    u'\ua1d1' : ['_l u', 'ꇑ'],
    u'\ua1d2' : ['_l u', 'ꇒ'],
    u'\ua1d3' : ['_l u', 'ꇓ'],
    u'\ua1d4' : ['_l _l', 'ꇔ'],
    u'\ua1d5' : ['_l _l', 'ꇕ'],
    u'\ua1d6' : ['_l _l', 'ꇖ'],
    u'\ua1d7' : ['_l _l', 'ꇗ'],
    u'\ua1d8' : ['_l i', 'ꇘ'],
    u'\ua1d9' : ['_l i', 'ꇙ'],
    u'\ua1da' : ['k i', 'ꇚ'],
    u'\ua1db' : ['k i', 'ꇛ'],
    u'\ua1dc' : ['k i', 'ꇜ'],
    u'\ua1dd' : ['k i', 'ꇝ'],
    u'\ua1de' : ['k e', 'ꇞ'],
    u'\ua1df' : ['k e', 'ꇟ'],
    u'\ua1e0' : ['k e', 'ꇠ'],
    u'\ua1e1' : ['k e', 'ꇡ'],
    u'\ua1e2' : ['k A', 'ꇢ'],
    u'\ua1e3' : ['k A', 'ꇣ'],
    u'\ua1e4' : ['k A', 'ꇤ'],
    u'\ua1e5' : ['k A', 'ꇥ'],
    u'\ua1e6' : ['k _>', 'ꇦ'],
    u'\ua1e7' : ['k _>', 'ꇧ'],
    u'\ua1e8' : ['k _>', 'ꇨ'],
    u'\ua1e9' : ['k _>', 'ꇩ'],
    u'\ua1ea' : ['k o', 'ꇪ'],
    u'\ua1eb' : ['k o', 'ꇫ'],
    u'\ua1ec' : ['k o', 'ꇬ'],
    u'\ua1ed' : ['k o', 'ꇭ'],
    u'\ua1ee' : ['k @', 'ꇮ'],
    u'\ua1ef' : ['k @', 'ꇯ'],
    u'\ua1f0' : ['k @', 'ꇰ'],
    u'\ua1f1' : ['k @', 'ꇱ'],
    u'\ua1f2' : ['k u', 'ꇲ'],
    u'\ua1f3' : ['k u', 'ꇳ'],
    u'\ua1f4' : ['k u', 'ꇴ'],
    u'\ua1f5' : ['k u', 'ꇵ'],
    u'\ua1f6' : ['k u', 'ꇶ'],
    u'\ua1f7' : ['k u', 'ꇷ'],
    u'\ua1f8' : ['k_h i', 'ꇸ'],
    u'\ua1f9' : ['k_h i', 'ꇹ'],
    u'\ua1fa' : ['k_h i', 'ꇺ'],
    u'\ua1fb' : ['k_h i', 'ꇻ'],
    u'\ua1fc' : ['k_h e', 'ꇼ'],
    u'\ua1fd' : ['k_h e', 'ꇽ'],
    u'\ua1fe' : ['k_h e', 'ꇾ'],
    u'\ua1ff' : ['k_h A', 'ꇿ'],
    u'\ua200' : ['k_h A', 'ꈀ'],
    u'\ua201' : ['k_h A', 'ꈁ'],
    u'\ua202' : ['k_h A', 'ꈂ'],
    u'\ua203' : ['k_h _>', 'ꈃ'],
    u'\ua204' : ['k_h _>', 'ꈄ'],
    u'\ua205' : ['k_h _>', 'ꈅ'],
    u'\ua206' : ['k_h o', 'ꈆ'],
    u'\ua207' : ['k_h o', 'ꈇ'],
    u'\ua208' : ['k_h o', 'ꈈ'],
    u'\ua209' : ['k_h o', 'ꈉ'],
    u'\ua20a' : ['k_h @', 'ꈊ'],
    u'\ua20b' : ['k_h @', 'ꈋ'],
    u'\ua20c' : ['k_h @', 'ꈌ'],
    u'\ua20d' : ['k_h @', 'ꈍ'],
    u'\ua20e' : ['k_h u', 'ꈎ'],
    u'\ua20f' : ['k_h u', 'ꈏ'],
    u'\ua210' : ['k_h u', 'ꈐ'],
    u'\ua211' : ['k_h u', 'ꈑ'],
    u'\ua212' : ['k_h u', 'ꈒ'],
    u'\ua213' : ['k_h u', 'ꈓ'],
    u'\ua214' : ['g i', 'ꈔ'],
    u'\ua215' : ['g i', 'ꈕ'],
    u'\ua216' : ['g i', 'ꈖ'],
    u'\ua217' : ['g e', 'ꈗ'],
    u'\ua218' : ['g e', 'ꈘ'],
    u'\ua219' : ['g e', 'ꈙ'],
    u'\ua21a' : ['g A', 'ꈚ'],
    u'\ua21b' : ['g A', 'ꈛ'],
    u'\ua21c' : ['g A', 'ꈜ'],
    u'\ua21d' : ['g A', 'ꈝ'],
    u'\ua21e' : ['g _>', 'ꈞ'],
    u'\ua21f' : ['g _>', 'ꈟ'],
    u'\ua220' : ['g _>', 'ꈠ'],
    u'\ua221' : ['g _>', 'ꈡ'],
    u'\ua222' : ['g o', 'ꈢ'],
    u'\ua223' : ['g o', 'ꈣ'],
    u'\ua224' : ['g o', 'ꈤ'],
    u'\ua225' : ['g o', 'ꈥ'],
    u'\ua226' : ['g @', 'ꈦ'],
    u'\ua227' : ['g @', 'ꈧ'],
    u'\ua228' : ['g @', 'ꈨ'],
    u'\ua229' : ['g @', 'ꈩ'],
    u'\ua22a' : ['g u', 'ꈪ'],
    u'\ua22b' : ['g u', 'ꈫ'],
    u'\ua22c' : ['g u', 'ꈬ'],
    u'\ua22d' : ['g u', 'ꈭ'],
    u'\ua22e' : ['g u', 'ꈮ'],
    u'\ua22f' : ['g u', 'ꈯ'],
    u'\ua230' : ['N g e', 'ꈰ'],
    u'\ua231' : ['N g e', 'ꈱ'],
    u'\ua232' : ['N g A', 'ꈲ'],
    u'\ua233' : ['N g A', 'ꈳ'],
    u'\ua234' : ['N g A', 'ꈴ'],
    u'\ua235' : ['N g A', 'ꈵ'],
    u'\ua236' : ['N g _>', 'ꈶ'],
    u'\ua237' : ['N g _>', 'ꈷ'],
    u'\ua238' : ['N g _>', 'ꈸ'],
    u'\ua239' : ['N g o', 'ꈹ'],
    u'\ua23a' : ['N g o', 'ꈺ'],
    u'\ua23b' : ['N g o', 'ꈻ'],
    u'\ua23c' : ['N g o', 'ꈼ'],
    u'\ua23d' : ['N g @', 'ꈽ'],
    u'\ua23e' : ['N g @', 'ꈾ'],
    u'\ua23f' : ['N g @', 'ꈿ'],
    u'\ua240' : ['N g u', 'ꉀ'],
    u'\ua241' : ['N g u', 'ꉁ'],
    u'\ua242' : ['N g u', 'ꉂ'],
    u'\ua243' : ['N g u', 'ꉃ'],
    u'\ua244' : ['N g u', 'ꉄ'],
    u'\ua245' : ['N g u', 'ꉅ'],
    u'\ua246' : ['_h i', 'ꉆ'],
    u'\ua247' : ['_h i', 'ꉇ'],
    u'\ua248' : ['_h i', 'ꉈ'],
    u'\ua249' : ['_h i', 'ꉉ'],
    u'\ua24a' : ['_h e', 'ꉊ'],
    u'\ua24b' : ['_h e', 'ꉋ'],
    u'\ua24c' : ['_h e', 'ꉌ'],
    u'\ua24d' : ['_h e', 'ꉍ'],
    u'\ua24e' : ['_h A', 'ꉎ'],
    u'\ua24f' : ['_h A', 'ꉏ'],
    u'\ua250' : ['_h A', 'ꉐ'],
    u'\ua251' : ['_h A', 'ꉑ'],
    u'\ua252' : ['_h _>', 'ꉒ'],
    u'\ua253' : ['_h _>', 'ꉓ'],
    u'\ua254' : ['_h _>', 'ꉔ'],
    u'\ua255' : ['_h _>', 'ꉕ'],
    u'\ua256' : ['_h o', 'ꉖ'],
    u'\ua257' : ['_h o', 'ꉗ'],
    u'\ua258' : ['_h o', 'ꉘ'],
    u'\ua259' : ['_h o', 'ꉙ'],
    u'\ua25a' : ['_h @', 'ꉚ'],
    u'\ua25b' : ['_h @', 'ꉛ'],
    u'\ua25c' : ['_h @', 'ꉜ'],
    u'\ua25d' : ['N e', 'ꉝ'],
    u'\ua25e' : ['N e', 'ꉞ'],
    u'\ua25f' : ['N e', 'ꉟ'],
    u'\ua260' : ['N A', 'ꉠ'],
    u'\ua261' : ['N A', 'ꉡ'],
    u'\ua262' : ['N A', 'ꉢ'],
    u'\ua263' : ['N A', 'ꉣ'],
    u'\ua264' : ['N _>', 'ꉤ'],
    u'\ua265' : ['N _>', 'ꉥ'],
    u'\ua266' : ['N _>', 'ꉦ'],
    u'\ua267' : ['N o', 'ꉧ'],
    u'\ua268' : ['N o', 'ꉨ'],
    u'\ua269' : ['N o', 'ꉩ'],
    u'\ua26a' : ['N o', 'ꉪ'],
    u'\ua26b' : ['N @', 'ꉫ'],
    u'\ua26c' : ['N @', 'ꉬ'],
    u'\ua26d' : ['N @', 'ꉭ'],
    u'\ua26e' : ['_x i', 'ꉮ'],
    u'\ua26f' : ['_" e', 'ꉯ'],
    u'\ua270' : ['_x e', 'ꉰ'],
    u'\ua271' : ['_x A', 'ꉱ'],
    u'\ua272' : ['_x A', 'ꉲ'],
    u'\ua273' : ['_x A', 'ꉳ'],
    u'\ua274' : ['_x A', 'ꉴ'],
    u'\ua275' : ['_x _>', 'ꉵ'],
    u'\ua276' : ['_x _>', 'ꉶ'],
    u'\ua277' : ['_x _>', 'ꉷ'],
    u'\ua278' : ['_x _>', 'ꉸ'],
    u'\ua279' : ['_x o', 'ꉹ'],
    u'\ua27a' : ['_x o', 'ꉺ'],
    u'\ua27b' : ['_x o', 'ꉻ'],
    u'\ua27c' : ['_x o', 'ꉼ'],
    u'\ua27d' : ['_x @', 'ꉽ'],
    u'\ua27e' : ['_x @', 'ꉾ'],
    u'\ua27f' : ['_x @', 'ꉿ'],
    u'\ua280' : ['G A', 'ꊀ'],
    u'\ua281' : ['G A', 'ꊁ'],
    u'\ua282' : ['G A', 'ꊂ'],
    u'\ua283' : ['G A', 'ꊃ'],
    u'\ua284' : ['G _>', 'ꊄ'],
    u'\ua285' : ['G _>', 'ꊅ'],
    u'\ua286' : ['G _>', 'ꊆ'],
    u'\ua287' : ['G o', 'ꊇ'],
    u'\ua288' : ['G o', 'ꊈ'],
    u'\ua289' : ['G o', 'ꊉ'],
    u'\ua28a' : ['G @', 'ꊊ'],
    u'\ua28b' : ['G @', 'ꊋ'],
    u'\ua28c' : ['G @', 'ꊌ'],
    u'\ua28d' : ['ts i', 'ꊍ'],
    u'\ua28e' : ['ts i', 'ꊎ'],
    u'\ua28f' : ['ts i', 'ꊏ'],
    u'\ua290' : ['ts i', 'ꊐ'],
    u'\ua291' : ['ts e', 'ꊑ'],
    u'\ua292' : ['ts e', 'ꊒ'],
    u'\ua293' : ['ts e', 'ꊓ'],
    u'\ua294' : ['ts A', 'ꊔ'],
    u'\ua295' : ['ts A', 'ꊕ'],
    u'\ua296' : ['ts A', 'ꊖ'],
    u'\ua297' : ['ts A', 'ꊗ'],
    u'\ua298' : ['ts _>', 'ꊘ'],
    u'\ua299' : ['ts _>', 'ꊙ'],
    u'\ua29a' : ['ts _>', 'ꊚ'],
    u'\ua29b' : ['ts o', 'ꊛ'],
    u'\ua29c' : ['ts o', 'ꊜ'],
    u'\ua29d' : ['ts o', 'ꊝ'],
    u'\ua29e' : ['ts o', 'ꊞ'],
    u'\ua29f' : ['ts @', 'ꊟ'],
    u'\ua2a0' : ['ts @', 'ꊠ'],
    u'\ua2a1' : ['ts @', 'ꊡ'],
    u'\ua2a2' : ['ts u', 'ꊢ'],
    u'\ua2a3' : ['ts u', 'ꊣ'],
    u'\ua2a4' : ['ts u', 'ꊤ'],
    u'\ua2a5' : ['ts u', 'ꊥ'],
    u'\ua2a6' : ['ts u', 'ꊦ'],
    u'\ua2a7' : ['ts u', 'ꊧ'],
    u'\ua2a8' : ['ts _l', 'ꊨ'],
    u'\ua2a9' : ['ts _l', 'ꊩ'],
    u'\ua2aa' : ['ts _l', 'ꊪ'],
    u'\ua2ab' : ['ts _l', 'ꊫ'],
    u'\ua2ac' : ['ts i', 'ꊬ'],
    u'\ua2ad' : ['ts i', 'ꊭ'],
    u'\ua2ae' : ['tsh i', 'ꊮ'],
    u'\ua2af' : ['tsh i', 'ꊯ'],
    u'\ua2b0' : ['tsh i', 'ꊰ'],
    u'\ua2b1' : ['tsh i', 'ꊱ'],
    u'\ua2b2' : ['tsh e', 'ꊲ'],
    u'\ua2b3' : ['tsh e', 'ꊳ'],
    u'\ua2b4' : ['tsh e', 'ꊴ'],
    u'\ua2b5' : ['tsh e', 'ꊵ'],
    u'\ua2b6' : ['tsh A', 'ꊶ'],
    u'\ua2b7' : ['tsh A', 'ꊷ'],
    u'\ua2b8' : ['tsh A', 'ꊸ'],
    u'\ua2b9' : ['tsh A', 'ꊹ'],
    u'\ua2ba' : ['tsh _>', 'ꊺ'],
    u'\ua2bb' : ['tsh _>', 'ꊻ'],
    u'\ua2bc' : ['tsh _>', 'ꊼ'],
    u'\ua2bd' : ['tsh o', 'ꊽ'],
    u'\ua2be' : ['tsh o', 'ꊾ'],
    u'\ua2bf' : ['tsh o', 'ꊿ'],
    u'\ua2c0' : ['tsh o', 'ꋀ'],
    u'\ua2c1' : ['tsh @', 'ꋁ'],
    u'\ua2c2' : ['tsh @', 'ꋂ'],
    u'\ua2c3' : ['tsh @', 'ꋃ'],
    u'\ua2c4' : ['tsh u', 'ꋄ'],
    u'\ua2c5' : ['tsh u', 'ꋅ'],
    u'\ua2c6' : ['tsh u', 'ꋆ'],
    u'\ua2c7' : ['tsh u', 'ꋇ'],
    u'\ua2c8' : ['tsh u', 'ꋈ'],
    u'\ua2c9' : ['tsh u', 'ꋉ'],
    u'\ua2ca' : ['tsh _l', 'ꋊ'],
    u'\ua2cb' : ['tsh _l', 'ꋋ'],
    u'\ua2cc' : ['tsh _l', 'ꋌ'],
    u'\ua2cd' : ['tsh _l', 'ꋍ'],
    u'\ua2ce' : ['tsh i', 'ꋎ'],
    u'\ua2cf' : ['tsh i', 'ꋏ'],
    u'\ua2d0' : ['dz i', 'ꋐ'],
    u'\ua2d1' : ['dz i', 'ꋑ'],
    u'\ua2d2' : ['dz i', 'ꋒ'],
    u'\ua2d3' : ['dz i', 'ꋓ'],
    u'\ua2d4' : ['dz e', 'ꋔ'],
    u'\ua2d5' : ['dz e', 'ꋕ'],
    u'\ua2d6' : ['dz e', 'ꋖ'],
    u'\ua2d7' : ['dz e', 'ꋗ'],
    u'\ua2d8' : ['dz A', 'ꋘ'],
    u'\ua2d9' : ['dz A', 'ꋙ'],
    u'\ua2da' : ['dz A', 'ꋚ'],
    u'\ua2db' : ['dz A', 'ꋛ'],
    u'\ua2dc' : ['dz o', 'ꋜ'],
    u'\ua2dd' : ['dz o', 'ꋝ'],
    u'\ua2de' : ['dz o', 'ꋞ'],
    u'\ua2df' : ['dz @', 'ꋟ'],
    u'\ua2e0' : ['dz @', 'ꋠ'],
    u'\ua2e1' : ['dz @', 'ꋡ'],
    u'\ua2e2' : ['dz u', 'ꋢ'],
    u'\ua2e3' : ['dz u', 'ꋣ'],
    u'\ua2e4' : ['dz u', 'ꋤ'],
    u'\ua2e5' : ['dz u', 'ꋥ'],
    u'\ua2e6' : ['dz u', 'ꋦ'],
    u'\ua2e7' : ['dz _l', 'ꋧ'],
    u'\ua2e8' : ['dz _l', 'ꋨ'],
    u'\ua2e9' : ['dz _l', 'ꋩ'],
    u'\ua2ea' : ['dz _l', 'ꋪ'],
    u'\ua2eb' : ['dz i', 'ꋫ'],
    u'\ua2ec' : ['dz i', 'ꋬ'],
    u'\ua2ed' : ['_n dz i', 'ꋭ'],
    u'\ua2ee' : ['_n dz i', 'ꋮ'],
    u'\ua2ef' : ['_n dz i', 'ꋯ'],
    u'\ua2f0' : ['_n dz i', 'ꋰ'],
    u'\ua2f1' : ['_n dz e', 'ꋱ'],
    u'\ua2f2' : ['_n dz e', 'ꋲ'],
    u'\ua2f3' : ['_n dz e', 'ꋳ'],
    u'\ua2f4' : ['_n dz A', 'ꋴ'],
    u'\ua2f5' : ['_n dz A', 'ꋵ'],
    u'\ua2f6' : ['_n dz A', 'ꋶ'],
    u'\ua2f7' : ['_n dz A', 'ꋷ'],
    u'\ua2f8' : ['_n dz _>', 'ꋸ'],
    u'\ua2f9' : ['_n dz _>', 'ꋹ'],
    u'\ua2fa' : ['_n dz o', 'ꋺ'],
    u'\ua2fb' : ['_n dz o', 'ꋻ'],
    u'\ua2fc' : ['_n dz @', 'ꋼ'],
    u'\ua2fd' : ['_n dz @', 'ꋽ'],
    u'\ua2fe' : ['_n dz u', 'ꋾ'],
    u'\ua2ff' : ['_n dz u', 'ꋿ'],
    u'\ua300' : ['_n dz u', 'ꌀ'],
    u'\ua301' : ['_n dz u', 'ꌁ'],
    u'\ua302' : ['_n dz u', 'ꌂ'],
    u'\ua303' : ['_n dz _l', 'ꌃ'],
    u'\ua304' : ['_n dz _l', 'ꌄ'],
    u'\ua305' : ['_n dz _l', 'ꌅ'],
    u'\ua306' : ['_n dz _l', 'ꌆ'],
    u'\ua307' : ['_n dz i', 'ꌇ'],
    u'\ua308' : ['_n dz i', 'ꌈ'],
    u'\ua309' : ['s i', 'ꌉ'],
    u'\ua30a' : ['s i', 'ꌊ'],
    u'\ua30b' : ['s i', 'ꌋ'],
    u'\ua30c' : ['s i', 'ꌌ'],
    u'\ua30d' : ['s e', 'ꌍ'],
    u'\ua30e' : ['s e', 'ꌎ'],
    u'\ua30f' : ['s e', 'ꌏ'],
    u'\ua310' : ['s A', 'ꌐ'],
    u'\ua311' : ['s A', 'ꌑ'],
    u'\ua312' : ['s A', 'ꌒ'],
    u'\ua313' : ['s A', 'ꌓ'],
    u'\ua314' : ['s _>', 'ꌔ'],
    u'\ua315' : ['s _>', 'ꌕ'],
    u'\ua316' : ['s _>', 'ꌖ'],
    u'\ua317' : ['s o', 'ꌗ'],
    u'\ua318' : ['s o', 'ꌘ'],
    u'\ua319' : ['s o', 'ꌙ'],
    u'\ua31a' : ['s o', 'ꌚ'],
    u'\ua31b' : ['s @', 'ꌛ'],
    u'\ua31c' : ['s @', 'ꌜ'],
    u'\ua31d' : ['s @', 'ꌝ'],
    u'\ua31e' : ['s u', 'ꌞ'],
    u'\ua31f' : ['s u', 'ꌟ'],
    u'\ua320' : ['s u', 'ꌠ'],
    u'\ua321' : ['s u', 'ꌡ'],
    u'\ua322' : ['s u', 'ꌢ'],
    u'\ua323' : ['s u', 'ꌣ'],
    u'\ua324' : ['s _l', 'ꌤ'],
    u'\ua325' : ['s _l', 'ꌥ'],
    u'\ua326' : ['s _l', 'ꌦ'],
    u'\ua327' : ['s _l', 'ꌧ'],
    u'\ua328' : ['s i', 'ꌨ'],
    u'\ua329' : ['s i', 'ꌩ'],
    u'\ua32a ' : ['z i', 'ꌪ '],
    u'\ua32b' : ['z i', 'ꌫ'],
    u'\ua32c' : ['z i', 'ꌬ'],
    u'\ua32d' : ['z i', 'ꌭ'],
    u'\ua32e' : ['z e', 'ꌮ'],
    u'\ua32f' : ['z e', 'ꌯ'],
    u'\ua330' : ['z e', 'ꌰ'],
    u'\ua331' : ['z A', 'ꌱ'],
    u'\ua332' : ['z A', 'ꌲ'],
    u'\ua333' : ['z A', 'ꌳ'],
    u'\ua334' : ['z A', 'ꌴ'],
    u'\ua335' : ['z o', 'ꌵ'],
    u'\ua336' : ['z o', 'ꌶ'],
    u'\ua337' : ['z o', 'ꌷ'],
    u'\ua338' : ['z o', 'ꌸ'],
    u'\ua339' : ['z @', 'ꌹ'],
    u'\ua33a' : ['z @', 'ꌺ'],
    u'\ua33b' : ['z @', 'ꌻ'],
    u'\ua33c' : ['z u', 'ꌼ'],
    u'\ua33d' : ['z u', 'ꌽ'],
    u'\ua33e' : ['z u', 'ꌾ'],
    u'\ua33f' : ['z u', 'ꌿ'],
    u'\ua340' : ['z _l', 'ꍀ'],
    u'\ua341' : ['z _l', 'ꍁ'],
    u'\ua342' : ['z _l', 'ꍂ'],
    u'\ua343' : ['z _l', 'ꍃ'],
    u'\ua344' : ['z i', 'ꍄ'],
    u'\ua345' : ['z i', 'ꍅ'],
    u'\ua346' : ['ts` A', 'ꍆ'],
    u'\ua347' : ['ts` A', 'ꍇ'],
    u'\ua348' : ['ts` A', 'ꍈ'],
    u'\ua349' : ['ts` A', 'ꍉ'],
    u'\ua34a' : ['ts` _>', 'ꍊ'],
    u'\ua34b' : ['ts` _>', 'ꍋ'],
    u'\ua34c' : ['ts` _>', 'ꍌ'],
    u'\ua34d' : ['ts` o', 'ꍍ'],
    u'\ua34e' : ['ts` o', 'ꍎ'],
    u'\ua34f' : ['ts` o', 'ꍏ'],
    u'\ua350' : ['ts` o', 'ꍐ'],
    u'\ua351' : ['ts` @', 'ꍑ'],
    u'\ua352' : ['ts` @', 'ꍒ'],
    u'\ua353' : ['ts` @', 'ꍓ'],
    u'\ua354' : ['ts` @', 'ꍔ'],
    u'\ua355' : ['ts` u', 'ꍕ'],
    u'\ua356' : ['ts` u', 'ꍖ'],
    u'\ua357' : ['ts` u', 'ꍗ'],
    u'\ua358' : ['ts` u', 'ꍘ'],
    u'\ua359' : ['ts` u', 'ꍙ'],
    u'\ua35a' : ['ts` u', 'ꍚ'],
    u'\ua35b' : ['ts` _l', 'ꍛ'],
    u'\ua35c' : ['ts` _l', 'ꍜ'],
    u'\ua35d' : ['ts` _l', 'ꍝ'],
    u'\ua35e' : ['ts` _l', 'ꍞ'],
    u'\ua35f' : ['ts` i', 'ꍟ'],
    u'\ua360' : ['ts` i', 'ꍠ'],
    u'\ua361' : ['ts_rh A', 'ꍡ'],
    u'\ua362' : ['ts_rh A', 'ꍢ'],
    u'\ua363' : ['ts_rh A', 'ꍣ'],
    u'\ua364' : ['ts_rh A', 'ꍤ'],
    u'\ua365' : ['ts_rh _>', 'ꍥ'],
    u'\ua366' : ['ts_rh _>', 'ꍦ'],
    u'\ua367' : ['ts_rh _>', 'ꍧ'],
    u'\ua368' : ['ts_rh _>', 'ꍨ'],
    u'\ua369' : ['ts_rh o', 'ꍩ'],
    u'\ua36a' : ['ts_rh o', 'ꍪ'],
    u'\ua36b' : ['ts_rh o', 'ꍫ'],
    u'\ua36c' : ['ts_rh o', 'ꍬ'],
    u'\ua36d' : ['ts_rh @', 'ꍭ'],
    u'\ua36e' : ['ts_rh @', 'ꍮ'],
    u'\ua36f' : ['ts_rh @', 'ꍯ'],
    u'\ua370' : ['ts_rh @', 'ꍰ'],
    u'\ua371' : ['ts_rh u', 'ꍱ'],
    u'\ua372' : ['ts_rh u', 'ꍲ'],
    u'\ua373' : ['ts_rh u', 'ꍳ'],
    u'\ua374' : ['ts_rh u', 'ꍴ'],
    u'\ua375' : ['ts_rh u', 'ꍵ'],
    u'\ua376' : ['ts_rh _l', 'ꍶ'],
    u'\ua377' : ['ts_rh _l', 'ꍷ'],
    u'\ua378' : ['ts_rh _l', 'ꍸ'],
    u'\ua379' : ['ts_rh _l', 'ꍹ'],
    u'\ua37a' : ['ts_rh i', 'ꍺ'],
    u'\ua37b' : ['ts_rh i', 'ꍻ'],
    u'\ua37c' : ['dzr A', 'ꍼ'],
    u'\ua37d' : ['dzr A', 'ꍽ'],
    u'\ua37e' : ['dzr _>', 'ꍾ'],
    u'\ua37f' : ['dzr _>', 'ꍿ'],
    u'\ua380' : ['dzr o', 'ꎀ'],
    u'\ua381' : ['dzr o', 'ꎁ'],
    u'\ua382' : ['dzr o', 'ꎂ'],
    u'\ua383' : ['dzr o', 'ꎃ'],
    u'\ua384' : ['dzr @', 'ꎄ'],
    u'\ua385' : ['dzr @', 'ꎅ'],
    u'\ua386' : ['dzr @', 'ꎆ'],
    u'\ua387' : ['dzr @', 'ꎇ'],
    u'\ua388' : ['dzr u', 'ꎈ'],
    u'\ua389' : ['dzr u', 'ꎉ'],
    u'\ua38a' : ['dzr u', 'ꎊ'],
    u'\ua38b' : ['dzr u', 'ꎋ'],
    u'\ua38c' : ['dzr u', 'ꎌ'],
    u'\ua38d' : ['dzr u', 'ꎍ'],
    u'\ua38e' : ['dzr _l', 'ꎎ'],
    u'\ua38f' : ['dzr _l', 'ꎏ'],
    u'\ua390' : ['dzr _l', 'ꎐ'],
    u'\ua391' : ['dzr _l', 'ꎑ'],
    u'\ua392' : ['dzr i', 'ꎒ'],
    u'\ua393' : ['dzr i', 'ꎓ'],
    u'\ua394' : ['_n dzr A', 'ꎔ'],
    u'\ua395' : ['_n dzr A', 'ꎕ'],
    u'\ua396' : ['_n dzr A', 'ꎖ'],
    u'\ua397' : ['_n dzr A', 'ꎗ'],
    u'\ua398' : ['_n dzr o', 'ꎘ'],
    u'\ua399' : ['_n dzr o', 'ꎙ'],
    u'\ua39a' : ['_n dzr o', 'ꎚ'],
    u'\ua39b' : ['_n dzr @', 'ꎛ'],
    u'\ua39c' : ['_n dzr @', 'ꎜ'],
    u'\ua39d' : ['_n dzr @', 'ꎝ'],
    u'\ua39e' : ['_n dzr @', 'ꎞ'],
    u'\ua39f' : ['_n dzr u', 'ꎟ'],
    u'\ua3a0' : ['_n dzr u', 'ꎠ'],
    u'\ua3a1' : ['_n dzr u', 'ꎡ'],
    u'\ua3a2' : ['_n dzr u', 'ꎢ'],
    u'\ua3a3' : ['_n dzr u', 'ꎣ'],
    u'\ua3a4' : ['_n dzr u', 'ꎤ'],
    u'\ua3a5' : ['_n dzr _l', 'ꎥ'],
    u'\ua3a6' : ['_n dzr _l', 'ꎦ'],
    u'\ua3a7' : ['_n dzr _l', 'ꎧ'],
    u'\ua3a8' : ['_n dzr _l', 'ꎨ'],
    u'\ua3a9' : ['_n dzr i', 'ꎩ'],
    u'\ua3aa' : ['_n dzr i', 'ꎪ'],
    u'\ua3ab' : ['s` A', 'ꎫ'],
    u'\ua3ac' : ['s` A', 'ꎬ'],
    u'\ua3ad' : ['s` A', 'ꎭ'],
    u'\ua3ae' : ['s` A', 'ꎮ'],
    u'\ua3af' : ['s` _>', 'ꎯ'],
    u'\ua3b0' : ['s` _>', 'ꎰ'],
    u'\ua3b1' : ['s` _>', 'ꎱ'],
    u'\ua3b2' : ['s` o', 'ꎲ'],
    u'\ua3b3' : ['s` o', 'ꎳ'],
    u'\ua3b4' : ['s` o', 'ꎴ'],
    u'\ua3b5' : ['s` o', 'ꎵ'],
    u'\ua3b6' : ['s` @', 'ꎶ'],
    u'\ua3b7' : ['s` @', 'ꎷ'],
    u'\ua3b8' : ['s` @', 'ꎸ'],
    u'\ua3b9' : ['s` @', 'ꎹ'],
    u'\ua3ba' : ['s` u', 'ꎺ'],
    u'\ua3bb' : ['s` u', 'ꎻ'],
    u'\ua3bc' : ['s` u', 'ꎼ'],
    u'\ua3bd' : ['s` u', 'ꎽ'],
    u'\ua3be' : ['s` u', 'ꎾ'],
    u'\ua3bf' : ['s` u', 'ꎿ'],
    u'\ua3c0' : ['s` _l', 'ꏀ'],
    u'\ua3c1' : ['s` _l', 'ꏁ'],
    u'\ua3c2' : ['s` _l', 'ꏂ'],
    u'\ua3c3' : ['s` _l', 'ꏃ'],
    u'\ua3c4' : ['s` i', 'ꏄ'],
    u'\ua3c5' : ['s` i', 'ꏅ'],
    u'\ua3c6' : ['z` A', 'ꏆ'],
    u'\ua3c7' : ['z` A', 'ꏇ'],
    u'\ua3c8' : ['z` A', 'ꏈ'],
    u'\ua3c9' : ['z` A', 'ꏉ'],
    u'\ua3ca' : ['z` _>', 'ꏊ'],
    u'\ua3cb' : ['z` _>', 'ꏋ'],
    u'\ua3cc' : ['z` _>', 'ꏌ'],
    u'\ua3cd' : ['z` o', 'ꏍ'],
    u'\ua3ce' : ['z` o', 'ꏎ'],
    u'\ua3cf' : ['z` o', 'ꏏ'],
    u'\ua3d0' : ['z` o', 'ꏐ'],
    u'\ua3d1' : ['z` @', 'ꏑ'],
    u'\ua3d2' : ['z` @', 'ꏒ'],
    u'\ua3d3' : ['z` @', 'ꏓ'],
    u'\ua3d4' : ['z` u', 'ꏔ'],
    u'\ua3d5' : ['z` u', 'ꏕ'],
    u'\ua3d6' : ['z` u', 'ꏖ'],
    u'\ua3d7' : ['z` u', 'ꏗ'],
    u'\ua3d8' : ['z` u', 'ꏘ'],
    u'\ua3d9' : ['z` u', 'ꏙ'],
    u'\ua3da' : ['z` _l', 'ꏚ'],
    u'\ua3db' : ['z` _l', 'ꏛ'],
    u'\ua3dc' : ['z` _l', 'ꏜ'],
    u'\ua3dd' : ['z` _l', 'ꏝ'],
    u'\ua3de' : ['z` i', 'ꏞ'],
    u'\ua3df' : ['z` i', 'ꏟ'],
    u'\ua3e0' : ['t s\\ i', 'ꏠ'],
    u'\ua3e1' : ['t s\\ i', 'ꏡ'],
    u'\ua3e2' : ['t s\\ i', 'ꏢ'],
    u'\ua3e3' : ['t s\\ i', 'ꏣ'],
    u'\ua3e4' : ['t s\\ e', 'ꏤ'],
    u'\ua3e5' : ['t s\\ e', 'ꏥ'],
    u'\ua3e6' : ['t s\\ e', 'ꏦ'],
    u'\ua3e7' : ['t s\\ e', 'ꏧ'],
    u'\ua3e8' : ['t s\\ _>', 'ꏨ'],
    u'\ua3e9' : ['t s\\ _>', 'ꏩ'],
    u'\ua3ea' : ['t s\\ _>', 'ꏪ'],
    u'\ua3eb' : ['t s\\ _>', 'ꏫ'],
    u'\ua3ec' : ['t s\\ o', 'ꏬ'],
    u'\ua3ed' : ['t s\\ o', 'ꏭ'],
    u'\ua3ee' : ['t s\\ o', 'ꏮ'],
    u'\ua3ef' : ['t s\\ o', 'ꏯ'],
    u'\ua3f0' : ['t s\\ u', 'ꏰ'],
    u'\ua3f1' : ['t s\\ u', 'ꏱ'],
    u'\ua3f2' : ['t s\\ u', 'ꏲ'],
    u'\ua3f3' : ['t s\\ u', 'ꏳ'],
    u'\ua3f4' : ['t s\\ u', 'ꏴ'],
    u'\ua3f5' : ['t s\\ u', 'ꏵ'],
    u'\ua3f6' : ['t s\\ _l', 'ꏶ'],
    u'\ua3f7' : ['t s\\ _l', 'ꏷ'],
    u'\ua3f8' : ['t s\\ _l', 'ꏸ'],
    u'\ua3f9' : ['t s\\ _l', 'ꏹ'],
    u'\ua3fa' : ['t s\\ i', 'ꏺ'],
    u'\ua3fb' : ['t s\\ i', 'ꏻ'],
    u'\ua3fc' : ['t c}h i', 'ꏼ'],
    u'\ua3fd' : ['t c}h i', 'ꏽ'],
    u'\ua3fe' : ['t c}h i', 'ꏾ'],
    u'\ua3ff' : ['t c}h i', 'ꏿ'],
    u'\ua400' : ['t c}h e', 'ꐀ'],
    u'\ua401' : ['t c}h e', 'ꐁ'],
    u'\ua402' : ['t c}h e', 'ꐂ'],
    u'\ua403' : ['t c}h e', 'ꐃ'],
    u'\ua404' : ['t c}h _>', 'ꐄ'],
    u'\ua405' : ['t c}h _>', 'ꐅ'],
    u'\ua406' : ['t c}h _>', 'ꐆ'],
    u'\ua407' : ['t c}h _>', 'ꐇ'],
    u'\ua408' : ['t c}h o', 'ꐈ'],
    u'\ua409' : ['t c}h o', 'ꐉ'],
    u'\ua40a' : ['t c}h o', 'ꐊ'],
    u'\ua40b' : ['t c}h o', 'ꐋ'],
    u'\ua40c' : ['t c}h u', 'ꐌ'],
    u'\ua40d' : ['t c}h u', 'ꐍ'],
    u'\ua40e' : ['t c}h u', 'ꐎ'],
    u'\ua40f' : ['t c}h u', 'ꐏ'],
    u'\ua410' : ['t c}h u', 'ꐐ'],
    u'\ua411' : ['t c}h u', 'ꐑ'],
    u'\ua412' : ['t c}h _l', 'ꐒ'],
    u'\ua413' : ['t c}h _l', 'ꐓ'],
    u'\ua414' : ['t c}h _l', 'ꐔ'],
    u'\ua415' : ['t c}h _l', 'ꐕ'],
    u'\ua416' : ['t c}h i', 'ꐖ'],
    u'\ua417' : ['t c}h i', 'ꐗ'],
    u'\ua418' : ['d z\\ i', 'ꐘ'],
    u'\ua419' : ['d z\\ i', 'ꐙ'],
    u'\ua41a' : ['d z\\ i', 'ꐚ'],
    u'\ua41b' : ['d z\\ i', 'ꐛ'],
    u'\ua41c' : ['d z\\ e', 'ꐜ'],
    u'\ua41d' : ['d z\\ e', 'ꐝ'],
    u'\ua41e' : ['d z\\ e', 'ꐞ'],
    u'\ua41f' : ['d z\\ e', 'ꐟ'],
    u'\ua420' : ['d z\\ _>', 'ꐠ'],
    u'\ua421' : ['d z\\ _>', 'ꐡ'],
    u'\ua422' : ['d z\\ _>', 'ꐢ'],
    u'\ua423' : ['d z\\ o', 'ꐣ'],
    u'\ua424' : ['d z\\ o', 'ꐤ'],
    u'\ua425' : ['d z\\ o', 'ꐥ'],
    u'\ua426' : ['d z\\ o', 'ꐦ'],
    u'\ua427' : ['d z\\ u', 'ꐧ'],
    u'\ua428' : ['d z\\ u', 'ꐨ'],
    u'\ua429' : ['d z\\ u', 'ꐩ'],
    u'\ua42a' : ['d z\\ u', 'ꐪ'],
    u'\ua42b' : ['d z\\ u', 'ꐫ'],
    u'\ua42c' : ['d z\\ u', 'ꐬ'],
    u'\ua42d' : ['d z\\ _l', 'ꐭ'],
    u'\ua42e' : ['d z\\ _l', 'ꐮ'],
    u'\ua42f' : ['d z\\ _l', 'ꐯ'],
    u'\ua430' : ['d z\\ _l', 'ꐰ'],
    u'\ua431' : ['_n d z\\ i', 'ꐱ'],
    u'\ua432' : ['_n d z\\ i', 'ꐲ'],
    u'\ua433' : ['_n d z\\ i', 'ꐳ'],
    u'\ua434' : ['_n d z\\ i', 'ꐴ'],
    u'\ua435' : ['_n d z\\ e', 'ꐵ'],
    u'\ua436' : ['_n d z\\ e', 'ꐶ'],
    u'\ua437' : ['_n d z\\ e', 'ꐷ'],
    u'\ua438' : ['_n d z\\ e', 'ꐸ'],
    u'\ua439' : ['_n d z\\ _>', 'ꐹ'],
    u'\ua43a' : ['_n d z\\ _>', 'ꐺ'],
    u'\ua43b' : ['_n d z\\ o', 'ꐻ'],
    u'\ua43c' : ['_n d z\\ o', 'ꐼ'],
    u'\ua43d' : ['_n d z\\ o', 'ꐽ'],
    u'\ua43e' : ['_n d z\\ o', 'ꐾ'],
    u'\ua43f' : ['_n d z\\ u', 'ꐿ'],
    u'\ua440' : ['_n d z\\ u', 'ꑀ'],
    u'\ua441' : ['_n d z\\ u', 'ꑁ'],
    u'\ua442' : ['_n d z\\ u', 'ꑂ'],
    u'\ua443' : ['_n d z\\ u', 'ꑃ'],
    u'\ua444' : ['_n d z\\ _l', 'ꑄ'],
    u'\ua445' : ['_n d z\\ _l', 'ꑅ'],
    u'\ua446' : ['_n d z\\ _l', 'ꑆ'],
    u'\ua447' : ['_n d z\\ _l', 'ꑇ'],
    u'\ua448' : ['_n d z\\ i', 'ꑈ'],
    u'\ua449' : ['_n d z\\ i', 'ꑉ'],
    u'\ua44a' : ['n" i', 'ꑊ'],
    u'\ua44b' : ['n" i', 'ꑋ'],
    u'\ua44c' : ['n" i', 'ꑌ'],
    u'\ua44d' : ['n" i', 'ꑍ'],
    u'\ua44e' : ['n" e', 'ꑎ'],
    u'\ua44f' : ['n" e', 'ꑏ'],
    u'\ua450' : ['n" e', 'ꑐ'],
    u'\ua451' : ['n" e', 'ꑑ'],
    u'\ua452' : ['n" _>', 'ꑒ'],
    u'\ua453' : ['n" _>', 'ꑓ'],
    u'\ua454' : ['n" _>', 'ꑔ'],
    u'\ua455' : ['n" o', 'ꑕ'],
    u'\ua456' : ['n" o', 'ꑖ'],
    u'\ua457' : ['n" o', 'ꑗ'],
    u'\ua458' : ['n" o', 'ꑘ'],
    u'\ua459' : ['n" u', 'ꑙ'],
    u'\ua45a' : ['n" u', 'ꑚ'],
    u'\ua45b' : ['n" u', 'ꑛ'],
    u'\ua45c' : ['n" u', 'ꑜ'],
    u'\ua45d' : ['s\\ i', 'ꑝ'],
    u'\ua45e' : ['s\\ i', 'ꑞ'],
    u'\ua45f' : ['s\\ i', 'ꑟ'],
    u'\ua460' : ['s\\ i', 'ꑠ'],
    u'\ua461' : ['s\\ e', 'ꑡ'],
    u'\ua462' : ['s\\ e', 'ꑢ'],
    u'\ua463' : ['s\\ e', 'ꑣ'],
    u'\ua464' : ['s\\ e', 'ꑤ'],
    u'\ua465' : ['s\\ _>', 'ꑥ'],
    u'\ua466' : ['s\\ _>', 'ꑦ'],
    u'\ua467' : ['s\\ o', 'ꑧ'],
    u'\ua468' : ['s\\ o', 'ꑨ'],
    u'\ua469' : ['s\\ o', 'ꑩ'],
    u'\ua46a' : ['s\\ o', 'ꑪ'],
    u'\ua46b' : ['s\\ _l', 'ꑫ'],
    u'\ua46c' : ['s\\ _l', 'ꑬ'],
    u'\ua46d' : ['s\\ _l', 'ꑭ'],
    u'\ua46e' : ['s\\ _l', 'ꑮ'],
    u'\ua46f' : ['s\\ i', 'ꑯ'],
    u'\ua470' : ['s\\ i', 'ꑰ'],
    u'\ua471' : ['z\\ i', 'ꑱ'],
    u'\ua472' : ['z\\ i', 'ꑲ'],
    u'\ua473' : ['z\\ i', 'ꑳ'],
    u'\ua474' : ['z\\ i', 'ꑴ'],
    u'\ua475' : ['z\\ e', 'ꑵ'],
    u'\ua476' : ['z\\ e', 'ꑶ'],
    u'\ua477' : ['z\\ e', 'ꑷ'],
    u'\ua478' : ['z\\ e', 'ꑸ'],
    u'\ua479' : ['z\\ _>', 'ꑹ'],
    u'\ua47a' : ['z\\ _>', 'ꑺ'],
    u'\ua47b' : ['z\\ _>', 'ꑻ'],
    u'\ua47c' : ['z\\ _>', 'ꑼ'],
    u'\ua47d' : ['z\\ o', 'ꑽ'],
    u'\ua47e' : ['z\\ o', 'ꑾ'],
    u'\ua47f' : ['z\\ o', 'ꑿ'],
    u'\ua480' : ['z\\ o', 'ꒀ'],
    u'\ua481' : ['z\\ u', 'ꒁ'],
    u'\ua482' : ['z\\ u', 'ꒂ'],
    u'\ua483' : ['z\\ u', 'ꒃ'],
    u'\ua484' : ['z\\ u', 'ꒄ'],
    u'\ua485' : ['z\\ u', 'ꒅ'],
    u'\ua486' : ['z\\ u', 'ꒆ'],
    u'\ua487' : ['z\\ _l', 'ꒇ'],
    u'\ua488' : ['z\\ _l', 'ꒈ'],
    u'\ua489' : ['z\\ _l', 'ꒉ'],
    u'\ua48a' : ['z\\ _l', 'ꒊ'],
    u'\ua48b' : ['z\\ i', 'ꒋ'],
    u'\ua48c' : ['z\\ i', 'ꒌ'],
    u'\ua490' : ['t c}h o', '꒐'],
    u'\ua491' : ['_l i', '꒑'],
    u'\ua492' : ['k_h i', '꒒'],
    u'\ua493' : ['n" i', '꒓'],
    u'\ua494' : ['tsh _l', '꒔'],
    u'\ua495' : ['z i', '꒕'],
    u'\ua496' : ['g o', '꒖'],
    u'\ua497' : ['k e', '꒗'],
    u'\ua498' : ['m i', '꒘'],
    u'\ua499' : ['_h i', '꒙'],
    u'\ua49a' : ['_l _l', '꒚'],
    u'\ua49b' : ['b u', '꒛'],
    u'\ua49c' : ['m o', '꒜'],
    u'\ua49d' : ['z\\ o', '꒝'],
    u'\ua49e' : ['p_h u', '꒞'],
    u'\ua49f' : ['_h _>', '꒟'],
    u'\ua4a0' : ['t_h A', '꒠'],
    u'\ua4a1' : ['k A', '꒡'],
    u'\ua4a2' : ['ts u', '꒢'],
    u'\ua4a3' : ['tsh _l', '꒣'],
    u'\ua4a4' : ['d u', '꒤'],
    u'\ua4a5' : ['p u', '꒥'],
    u'\ua4a6' : ['g u', '꒦'],
    u'\ua4a7' : ['n" o', '꒧'],
    u'\ua4a8' : ['t_h u', '꒨'],
    u'\ua4a9' : ['o', '꒩'],
    u'\ua4aa' : ['d z\\ u', '꒪'],
    u'\ua4ab' : ['ts o', '꒫'],
    u'\ua4ac' : ['p _l', '꒬'],
    u'\ua4ad' : ['m0 o', '꒭'],
    u'\ua4ae' : ['z\\ i', '꒮'],
    u'\ua4af' : ['_v u', '꒯'],
    u'\ua4b0' : ['s` _l', '꒰'],
    u'\ua4b1' : ['_v e', '꒱'],
    u'\ua4b2' : ['ts A', '꒲'],
    u'\ua4b3' : ['t s\\ o', '꒳'],
    u'\ua4b4' : ['_n dz u', '꒴'],
    u'\ua4b5' : ['d z\\ _l', '꒵'],
    u'\ua4b6' : ['k o', '꒶'],
    u'\ua4b7' : ['d z\\ e', '꒷'],
    u'\ua4b8' : ['G o', '꒸'],
    u'\ua4b9' : ['t u', '꒹'],
    u'\ua4ba' : ['s` u', '꒺'],
    u'\ua4bb' : ['_l e', '꒻'],
    u'\ua4bc' : ['tsh _l', '꒼'],
    u'\ua4bd' : ['tsh _>', '꒽'],
    u'\ua4be' : ['tsh i', '꒾'],
    u'\ua4bf' : ['_h o', '꒿'],
    u'\ua4c0' : ['s` A', '꓀'],
    u'\ua4c1' : ['ts u', '꓁'],
    u'\ua4c2' : ['s` o', '꓂'],
    u'\ua4c3' : ['ts_rh e', '꓃'],
    u'\ua4c4' : ['dz e', '꓄'],
    u'\ua4c5' : ['m b e', '꓅'],
    u'\ua4c6' : ['k_h e', '꓆'],
    u'\ua720' : ['(##)', '꜠'],
    u'\ua721' : ['(##)', '꜡'],
    u'\ua800' : ['A', 'ꠀ'],
    u'\ua801' : ['i', 'ꠁ'],
    u'\ua802' : ['(DVIAVARA)', 'ꠂ'],
    u'\ua803' : ['u', 'ꠃ'],
    u'\ua804' : ['e', 'ꠄ'],
    u'\ua805' : ['o', 'ꠅ'],
    u'\ua806' : ['(HASANTA)', '꠆'],
    u'\ua807' : ['k A', 'ꠇ'],
    u'\ua808' : ['k_h A', 'ꠈ'],
    u'\ua809' : ['g A', 'ꠉ'],
    u'\ua80a' : ['g_h A', 'ꠊ'],
    u'\ua80b' : ['m A', 'ꠋ'],
    u'\ua80c' : ['_} A', 'ꠌ'],
    u'\ua80d' : ['c_h A', 'ꠍ'],
    u'\ua80e' : ['J\\ A', 'ꠎ'],
    u'\ua80f' : ['J\\_h A', 'ꠏ'],
    u'\ua810' : ['t` A', 'ꠐ'],
    u'\ua811' : ['t`_h A', 'ꠑ'],
    u'\ua812' : ['d` A', 'ꠒ'],
    u'\ua813' : ['d`_h A', 'ꠓ'],
    u'\ua814' : ['t_d A', 'ꠔ'],
    u'\ua815' : ['t_d_h A', 'ꠕ'],
    u'\ua816' : ['d_d A', 'ꠖ'],
    u'\ua817' : ['d_d_h A', 'ꠗ'],
    u'\ua818' : ['n_d A', 'ꠘ'],
    u'\ua819' : ['p A', 'ꠙ'],
    u'\ua81a' : ['p_h A', 'ꠚ'],
    u'\ua81b' : ['b A', 'ꠛ'],
    u'\ua81c' : ['b_h A', 'ꠜ'],
    u'\ua81d' : ['m A', 'ꠝ'],
    u'\ua81e' : ['r\\` A', 'ꠞ'],
    u'\ua81f' : ['_l A', 'ꠟ'],
    u'\ua820' : ['9r: A', 'ꠠ'],
    u'\ua821' : ['s A', 'ꠡ'],
    u'\ua822' : ['_h A', 'ꠢ'],
    u'\ua823' : ['A', 'ꠣ'],
    u'\ua824' : ['i', 'ꠤ'],
    u'\ua825' : ['u', 'ꠥ'],
    u'\ua826' : ['e', 'ꠦ'],
    u'\ua827' : ['o:', 'ꠧ'],
    u'\ua828' : ['(POETRY MARK1)', '꠨'],
    u'\ua829' : ['(POETRY MARK2)', '꠩'],
    u'\ua82a' : ['(POETRY MARK3)', '꠪'],
    u'\uac00' : ['k a', '가'],
    u'\uac01' : ['k a k', '각'],
    u'\uac02' : ['k a k_>', '갂'],
    u'\uac03' : ['k a k sh', '갃'],
    u'\uac04' : ['k a _n', '간'],
    u'\uac05' : ['k a _n tS', '갅'],
    u'\uac06' : ['k a _n _h', '갆'],
    u'\uac07' : ['k a t', '갇'],
    u'\uac08' : ['k a _l', '갈'],
    u'\uac09' : ['k a _l k', '갉'],
    u'\uac0a' : ['k a _l m', '갊'],
    u'\uac0b' : ['k a _l b', '갋'],
    u'\uac0c' : ['k a _l sh', '갌'],
    u'\uac0d' : ['k a _l t_h', '갍'],
    u'\uac0e' : ['k a _l p_h', '갎'],
    u'\uac0f' : ['k a _l _h', '갏'],
    u'\uac10' : ['k a m', '감'],
    u'\uac11' : ['k a p', '갑'],
    u'\uac12' : ['k a p sh', '값'],
    u'\uac13' : ['k a sh', '갓'],
    u'\uac14' : ['k a s', '갔'],
    u'\uac15' : ['k a N', '강'],
    u'\uac16' : ['k a tS', '갖'],
    u'\uac17' : ['k a tSh', '갗'],
    u'\uac18' : ['k a k_h', '갘'],
    u'\uac19' : ['k a t_h', '같'],
    u'\uac1a' : ['k a p_h', '갚'],
    u'\uac1b' : ['k a _h', '갛'],
    u'\uac1c' : ['k {', '개'],
    u'\uac1d' : ['k { k', '객'],
    u'\uac1e' : ['k { k_>', '갞'],
    u'\uac1f' : ['k { k sh', '갟'],
    u'\uac20' : ['k { _n', '갠'],
    u'\uac21' : ['k { _n tS', '갡'],
    u'\uac22' : ['k { _n _h', '갢'],
    u'\uac23' : ['k { t', '갣'],
    u'\uac24' : ['k { _l', '갤'],
    u'\uac25' : ['k { _l k', '갥'],
    u'\uac26' : ['k { _l m', '갦'],
    u'\uac27' : ['k { _l p', '갧'],
    u'\uac28' : ['k { _l sh', '갨'],
    u'\uac29' : ['k { _l t_h', '갩'],
    u'\uac2a' : ['k { _l p_h', '갪'],
    u'\uac2b' : ['k { _l _h', '갫'],
    u'\uac2c' : ['k { m', '갬'],
    u'\uac2d' : ['k { p', '갭'],
    u'\uac2e' : ['k { p sh', '갮'],
    u'\uac2f' : ['k { sh', '갯'],
    u'\uac30' : ['k { s', '갰'],
    u'\uac31' : ['k { N', '갱'],
    u'\uac32' : ['k { tS', '갲'],
    u'\uac33' : ['k { tSh', '갳'],
    u'\uac34' : ['k { k_h', '갴'],
    u'\uac35' : ['k { t_h', '갵'],
    u'\uac36' : ['k { p_h', '갶'],
    u'\uac37' : ['k { _h', '갷'],
    u'\uac38' : ['k _j a', '갸'],
    u'\uac39' : ['k _j a k', '갹'],
    u'\uac3a' : ['k _j a k_>', '갺'],
    u'\uac3b' : ['k _j a k sh', '갻'],
    u'\uac3c' : ['k _j a _n', '갼'],
    u'\uac3d' : ['k _j a _n tS', '갽'],
    u'\uac3e' : ['k _j a _n _h', '갾'],
    u'\uac3f' : ['k _j a t', '갿'],
    u'\uac40' : ['k _j a _l', '걀'],
    u'\uac41' : ['k _j a _l k', '걁'],
    u'\uac42' : ['k _j a L F', '걂'],
    u'\uac43' : ['k _j a _l p', '걃'],
    u'\uac44' : ['k _j a _l sh', '걄'],
    u'\uac45' : ['k _j a _l t_h', '걅'],
    u'\uac46' : ['k _j a _l p_h', '걆'],
    u'\uac47' : ['k _j a _l _h', '걇'],
    u'\uac48' : ['k _j a m', '걈'],
    u'\uac49' : ['k _j a p', '걉'],
    u'\uac4a' : ['k _j a p sh', '걊'],
    u'\uac4b' : ['k _j a sh', '걋'],
    u'\uac4c' : ['k _j a s', '걌'],
    u'\uac4d' : ['k _j a N', '걍'],
    u'\uac4e' : ['k _j a tS', '걎'],
    u'\uac4f' : ['k _j a tSh', '걏'],
    u'\uac50' : ['k _j a k_h', '걐'],
    u'\uac51' : ['k _j a t_h', '걑'],
    u'\uac52' : ['k _j a p_h', '걒'],
    u'\uac53' : ['k _j a _h', '걓'],
    u'\uac54' : ['k _j {', '걔'],
    u'\uac55' : ['k _j { k', '걕'],
    u'\uac56' : ['k _j { k_>', '걖'],
    u'\uac57' : ['k _j { k sh', '걗'],
    u'\uac58' : ['k _j { _n', '걘'],
    u'\uac59' : ['k _j { _n tS', '걙'],
    u'\uac5a' : ['k _j { _n _h', '걚'],
    u'\uac5b' : ['k _j { t', '걛'],
    u'\uac5c' : ['k _j { _l', '걜'],
    u'\uac5d' : ['k _j { _l k', '걝'],
    u'\uac5e' : ['k _j { _l m', '걞'],
    u'\uac5f' : ['k _j { _l p', '걟'],
    u'\uac60' : ['k _j { _l sh', '걠'],
    u'\uac61' : ['k _j { _l t_h', '걡'],
    u'\uac62' : ['k _j { _l p_h', '걢'],
    u'\uac63' : ['k _j { _l _h', '걣'],
    u'\uac64' : ['k _j { m', '걤'],
    u'\uac65' : ['k _j { p', '걥'],
    u'\uac66' : ['k _j { p sh', '걦'],
    u'\uac67' : ['k _j { sh', '걧'],
    u'\uac68' : ['k _j { s', '걨'],
    u'\uac69' : ['k _j { N', '걩'],
    u'\uac6a' : ['k _j { tS', '걪'],
    u'\uac6b' : ['k _j { tSh', '걫'],
    u'\uac6c' : ['k _j { k_h', '걬'],
    u'\uac6d' : ['k _j { t_h', '걭'],
    u'\uac6e' : ['k _j { p_h', '걮'],
    u'\uac6f' : ['k _j { _h', '걯'],
    u'\uac70' : ['k _r', '거'],
    u'\uac71' : ['k _r k', '걱'],
    u'\uac72' : ['k _r k_>', '걲'],
    u'\uac73' : ['k _r k sh', '걳'],
    u'\uac74' : ['k _r _n', '건'],
    u'\uac75' : ['k _r _n tS', '걵'],
    u'\uac76' : ['k _r _n _h', '걶'],
    u'\uac77' : ['k _r t', '걷'],
    u'\uac78' : ['k _r _l', '걸'],
    u'\uac79' : ['k _r _l k', '걹'],
    u'\uac7a' : ['k _r _l m', '걺'],
    u'\uac7b' : ['k _r _l p', '걻'],
    u'\uac7c' : ['k _r _l sh', '걼'],
    u'\uac7d' : ['k _r _l t_h', '걽'],
    u'\uac7e' : ['k _r _l p_h', '걾'],
    u'\uac7f' : ['k _r _l _h', '걿'],
    u'\uac80' : ['k _r m', '검'],
    u'\uac81' : ['k _r p', '겁'],
    u'\uac82' : ['k _r p sh', '겂'],
    u'\uac83' : ['k _r sh', '것'],
    u'\uac84' : ['k _r s', '겄'],
    u'\uac85' : ['k _r N', '겅'],
    u'\uac86' : ['k _r tS', '겆'],
    u'\uac87' : ['k _r tSh', '겇'],
    u'\uac88' : ['k _r k_h', '겈'],
    u'\uac89' : ['k _r t_h', '겉'],
    u'\uac8a' : ['k _r p_h', '겊'],
    u'\uac8b' : ['k _r _h', '겋'],
    u'\uac8c' : ['k e', '게'],
    u'\uac8d' : ['k e k', '겍'],
    u'\uac8e' : ['k e k_>', '겎'],
    u'\uac8f' : ['k e k sh', '겏'],
    u'\uac90' : ['k e _n', '겐'],
    u'\uac91' : ['k e _n tS', '겑'],
    u'\uac92' : ['k e _n _h', '겒'],
    u'\uac93' : ['k e t', '겓'],
    u'\uac94' : ['k e _l', '겔'],
    u'\uac95' : ['k e _l k', '겕'],
    u'\uac96' : ['k e _l m', '겖'],
    u'\uac97' : ['k e _l p', '겗'],
    u'\uac98' : ['k e _l sh', '겘'],
    u'\uac99' : ['k e _l t_h', '겙'],
    u'\uac9a' : ['k e _l p_h', '겚'],
    u'\uac9b' : ['k e _l _h', '겛'],
    u'\uac9c' : ['k e m', '겜'],
    u'\uac9d' : ['k e p', '겝'],
    u'\uac9e' : ['k e p sh', '겞'],
    u'\uac9f' : ['k e sh', '겟'],
    u'\uaca0' : ['k e s', '겠'],
    u'\uaca1' : ['k e N', '겡'],
    u'\uaca2' : ['k e tS', '겢'],
    u'\uaca3' : ['k e tSh', '겣'],
    u'\uaca4' : ['k e k_h', '겤'],
    u'\uaca5' : ['k e t_h', '겥'],
    u'\uaca6' : ['k e p_h', '겦'],
    u'\uaca7' : ['k e _h', '겧'],
    u'\uaca8' : ['k _j _r', '겨'],
    u'\uaca9' : ['k _j _r k', '격'],
    u'\uacaa' : ['k _j _r k_>', '겪'],
    u'\uacab' : ['k _j _r k sh', '겫'],
    u'\uacac' : ['k _j _r _n', '견'],
    u'\uacad' : ['k _j _r _n tS', '겭'],
    u'\uacae' : ['k _j _r _n _h', '겮'],
    u'\uacaf' : ['k _j _r t', '겯'],
    u'\uacb0' : ['k _j _r _l', '결'],
    u'\uacb1' : ['k _j _r _l k', '겱'],
    u'\uacb2' : ['k _j _r _l m', '겲'],
    u'\uacb3' : ['k _j _r _l p', '겳'],
    u'\uacb4' : ['k _j _r _l sh', '겴'],
    u'\uacb5' : ['k _j _r _l t_h', '겵'],
    u'\uacb6' : ['k _j _r _l p_h', '겶'],
    u'\uacb7' : ['k _j _r _l _h', '겷'],
    u'\uacb8' : ['k _j _r m', '겸'],
    u'\uacb9' : ['k _j _r p', '겹'],
    u'\uacba' : ['k _j _r p sh', '겺'],
    u'\uacbb' : ['k _j _r sh', '겻'],
    u'\uacbc' : ['k _j _r s', '겼'],
    u'\uacbd' : ['k _j _r N', '경'],
    u'\uacbe' : ['k _j _r tS', '겾'],
    u'\uacbf' : ['k _j _r tSh', '겿'],
    u'\uacc1' : ['k _j _r t_h', '곁'],
    u'\uacc2' : ['k _j _r p_h', '곂'],
    u'\uacc3' : ['k _j _r _h', '곃'],
    u'\uacc4' : ['k _j e', '계'],
    u'\uacc5' : ['k _j e k', '곅'],
    u'\uacc6' : ['k _j e k_>', '곆'],
    u'\uacc7' : ['k _j e k sh', '곇'],
    u'\uacc8' : ['k _j e _n', '곈'],
    u'\uacc9' : ['k _j e _n tS', '곉'],
    u'\uacca' : ['k _j e _n _h', '곊'],
    u'\uaccb' : ['k _j e t', '곋'],
    u'\uaccc' : ['k _j e _l', '곌'],
    u'\uaccd' : ['k _j e _l k', '곍'],
    u'\uacce' : ['k _j e _l m', '곎'],
    u'\uaccf' : ['k _j e _l p', '곏'],
    u'\uacd1' : ['k _j e _l sh', '곑'],
    u'\uacd2' : ['k _j e _l t_h', '곒'],
    u'\uacd3' : ['k _j e _l p_h', '곓'],
    u'\uacd4' : ['k _j e m', '곔'],
    u'\uacd5' : ['k _j e p', '곕'],
    u'\uacd6' : ['k _j e p sh', '곖'],
    u'\uacd7' : ['k _j e sh', '곗'],
    u'\uacd8' : ['k _j e s', '곘'],
    u'\uacd9' : ['k _j e N', '곙'],
    u'\uacda' : ['k _j e tS', '곚'],
    u'\uacdb' : ['k _j e tSh', '곛'],
    u'\uacdc' : ['k _j e k_h', '곜'],
    u'\uacdd' : ['k _j e t_h', '곝'],
    u'\uacde' : ['k _j e p_h', '곞'],
    u'\uacdf' : ['k _j e _h', '곟'],
    u'\uace0' : ['k o', '고'],
    u'\uace1' : ['k o k', '곡'],
    u'\uace2' : ['k o k_>', '곢'],
    u'\uace3' : ['k o k sh', '곣'],
    u'\uace4' : ['k o _n', '곤'],
    u'\uace5' : ['k o _n tS', '곥'],
    u'\uace6' : ['k o _n _h', '곦'],
    u'\uace7' : ['k o t', '곧'],
    u'\uace8' : ['k o _l', '골'],
    u'\uace9' : ['k o _l k', '곩'],
    u'\uacea' : ['k o _l m', '곪'],
    u'\uaceb' : ['k o _l b', '곫'],
    u'\uacec' : ['k o _l sh', '곬'],
    u'\uaced' : ['k o _l t_h', '곭'],
    u'\uacee' : ['k o _l p_h', '곮'],
    u'\uacef' : ['k o _l _h', '곯'],
    u'\uacf0' : ['k o m', '곰'],
    u'\uacf1' : ['k o p', '곱'],
    u'\uacf2' : ['k o p sh', '곲'],
    u'\uacf3' : ['k o sh', '곳'],
    u'\uacf4' : ['k o s', '곴'],
    u'\uacf5' : ['k o N', '공'],
    u'\uacf6' : ['k o tS', '곶'],
    u'\uacf7' : ['k o tSh', '곷'],
    u'\uacf8' : ['k o k_h', '곸'],
    u'\uacf9' : ['k o t_h', '곹'],
    u'\uacfa' : ['k o p_h', '곺'],
    u'\uacfb' : ['k o _h', '곻'],
    u'\uacfc' : ['k _w a', '과'],
    u'\uacfd' : ['k _w a k', '곽'],
    u'\uacfe' : ['k _w a k_>', '곾'],
    u'\uacff' : ['k _w a k sh', '곿'],
    u'\uad00' : ['k _w a _n', '관'],
    u'\uad01' : ['k _w a _n tS', '괁'],
    u'\uad02' : ['k _w a _n _h', '괂'],
    u'\uad03' : ['k _w a t', '괃'],
    u'\uad04' : ['k _w a _l', '괄'],
    u'\uad05' : ['k _w a _l k', '괅'],
    u'\uad06' : ['k _w a _l m', '괆'],
    u'\uad07' : ['k _w a _l p', '괇'],
    u'\uad08' : ['k _w a _l sh', '괈'],
    u'\uad09' : ['k _w a _l t_h', '괉'],
    u'\uad0a' : ['k _w a _l p_h', '괊'],
    u'\uad0b' : ['k _w a _l _h', '괋'],
    u'\uad0c' : ['k _w a m', '괌'],
    u'\uad0d' : ['k _w a p', '괍'],
    u'\uad0e' : ['k _w a p sh', '괎'],
    u'\uad0f' : ['k _w a sh', '괏'],
    u'\uad10' : ['k _w a s', '괐'],
    u'\uad11' : ['k _w a N', '광'],
    u'\uad12' : ['k _w a tS', '괒'],
    u'\uad13' : ['k _w a tSh', '괓'],
    u'\uad14' : ['k _w a k_h', '괔'],
    u'\uad15' : ['k _w a t_h', '괕'],
    u'\uad16' : ['k _w a p_h', '괖'],
    u'\uad17' : ['k _w a _h', '괗'],
    u'\uad18' : ['k _w {', '괘'],
    u'\uad19' : ['k _w { k', '괙'],
    u'\uad1a' : ['k _w { k_>', '괚'],
    u'\uad1b' : ['k _w { k sh', '괛'],
    u'\uad1c' : ['k _w { _n', '괜'],
    u'\uad1d' : ['k _w { _n tS', '괝'],
    u'\uad1e' : ['k _w { _n _h', '괞'],
    u'\uad1f' : ['k _w { t', '괟'],
    u'\uad20' : ['k _w { _l', '괠'],
    u'\uad21' : ['k _w { _l k', '괡'],
    u'\uad22' : ['k _w { _l m', '괢'],
    u'\uad23' : ['k _w { _l p', '괣'],
    u'\uad24' : ['k _w { _l sh', '괤'],
    u'\uad25' : ['k _w { _l t_h', '괥'],
    u'\uad26' : ['k _w { _l p_h', '괦'],
    u'\uad27' : ['k _w { _l _h', '괧'],
    u'\uad28' : ['k _w { m', '괨'],
    u'\uad29' : ['k _w { p', '괩'],
    u'\uad2a' : ['k _w { p sh', '괪'],
    u'\uad2b' : ['k _w { sh', '괫'],
    u'\uad2c' : ['k _w { s', '괬'],
    u'\uad2d' : ['k _w { N', '괭'],
    u'\uad2e' : ['k _w { tS', '괮'],
    u'\uad2f' : ['k _w { tSh', '괯'],
    u'\uad30' : ['k _w { k_h', '괰'],
    u'\uad31' : ['k _w { t_h', '괱'],
    u'\uad32' : ['k _w { p_h', '괲'],
    u'\uad33' : ['k _w { _h', '괳'],
    u'\uad34' : ['k _w e', '괴'],
    u'\uad35' : ['k _w e k', '괵'],
    u'\uad36' : ['k _w e k_>', '괶'],
    u'\uad37' : ['k _w e k sh', '괷'],
    u'\uad38' : ['k _w e _n', '괸'],
    u'\uad39' : ['k _w e _n tS', '괹'],
    u'\uad3a' : ['k _w e _n _h', '괺'],
    u'\uad3b' : ['k _w e t', '괻'],
    u'\uad3c' : ['k _w e _l', '괼'],
    u'\uad3d' : ['k _w e _l k', '괽'],
    u'\uad3e' : ['k _w e _l m', '괾'],
    u'\uad3f' : ['k _w e _l p', '괿'],
    u'\uad40' : ['k _w e _l sh', '굀'],
    u'\uad41' : ['k _w e _l t_h', '굁'],
    u'\uad42' : ['k _w e _l p_h', '굂'],
    u'\uad43' : ['k _w e _l _h', '굃'],
    u'\uad44' : ['k _w e m', '굄'],
    u'\uad45' : ['k _w e p', '굅'],
    u'\uad46' : ['k _w e p sh', '굆'],
    u'\uad47' : ['k _w e sh', '굇'],
    u'\uad48' : ['k _w e s', '굈'],
    u'\uad49' : ['k _w e N', '굉'],
    u'\uad4a' : ['k _w e tS', '굊'],
    u'\uad4b' : ['k _w e tSh', '굋'],
    u'\uad4c' : ['k _w e k_h', '굌'],
    u'\uad4d' : ['k _w e t_h', '굍'],
    u'\uad4e' : ['k _w e p_h', '굎'],
    u'\uad4f' : ['k _w e _h', '굏'],
    u'\uad50' : ['k _j o', '교'],
    u'\uad51' : ['k _j o k', '굑'],
    u'\uad52' : ['k _j o k_>', '굒'],
    u'\uad53' : ['k _j o k sh', '굓'],
    u'\uad54' : ['k _j o _n', '굔'],
    u'\uad55' : ['k _j o _n tS', '굕'],
    u'\uad56' : ['k _j o _n _h', '굖'],
    u'\uad57' : ['k _j o t', '굗'],
    u'\uad58' : ['k _j o _l', '굘'],
    u'\uad59' : ['k _j o _l k', '굙'],
    u'\uad5a' : ['k _j o _l m', '굚'],
    u'\uad5b' : ['k _j o _l p', '굛'],
    u'\uad5c' : ['k _j o _l sh', '굜'],
    u'\uad5d' : ['k _j o _l t_h', '굝'],
    u'\uad5e' : ['k _j o _l p_h', '굞'],
    u'\uad5f' : ['k _j o _l _h', '굟'],
    u'\uad60' : ['k _j o m', '굠'],
    u'\uad61' : ['k _j o p', '굡'],
    u'\uad62' : ['k _j o p sh', '굢'],
    u'\uad63' : ['k _j o sh', '굣'],
    u'\uad64' : ['k _j o s', '굤'],
    u'\uad65' : ['k _j o N', '굥'],
    u'\uad66' : ['k _j o tS', '굦'],
    u'\uad67' : ['k _j o tSh', '굧'],
    u'\uad68' : ['k _j o k_h', '굨'],
    u'\uad69' : ['k _j o t_h', '굩'],
    u'\uad6a' : ['k _j o p_h', '굪'],
    u'\uad6b' : ['k _j o _h', '굫'],
    u'\uad6c' : ['k u', '구'],
    u'\uad6d' : ['k u k', '국'],
    u'\uad6e' : ['k u k_>', '굮'],
    u'\uad6f' : ['k u k sh', '굯'],
    u'\uad70' : ['k u _n', '군'],
    u'\uad71' : ['k u _n tS', '굱'],
    u'\uad72' : ['k u _n _h', '굲'],
    u'\uad73' : ['k u t', '굳'],
    u'\uad74' : ['k u _l', '굴'],
    u'\uad75' : ['k u _l k', '굵'],
    u'\uad76' : ['k u _l m', '굶'],
    u'\uad77' : ['k u _l p', '굷'],
    u'\uad78' : ['k u _l sh', '굸'],
    u'\uad79' : ['k u _l t_h', '굹'],
    u'\uad7a' : ['k u _l p_h', '굺'],
    u'\uad7b' : ['k u _l _h', '굻'],
    u'\uad7c' : ['k u m', '굼'],
    u'\uad7d' : ['k u p', '굽'],
    u'\uad7e' : ['k u p sh', '굾'],
    u'\uad7f' : ['k u sh', '굿'],
    u'\uad80' : ['k u s', '궀'],
    u'\uad81' : ['k u N', '궁'],
    u'\uad82' : ['k u tS', '궂'],
    u'\uad83' : ['k u tSh', '궃'],
    u'\uad84' : ['k u k_h', '궄'],
    u'\uad85' : ['k u t_h', '궅'],
    u'\uad86' : ['k u p_h', '궆'],
    u'\uad87' : ['k u _h', '궇'],
    u'\uad88' : ['k _w _r', '궈'],
    u'\uad89' : ['k _w _r k', '궉'],
    u'\uad8a' : ['k _w _r k_>', '궊'],
    u'\uad8b' : ['k _w _r k sh', '궋'],
    u'\uad8c' : ['k _w _r _n', '권'],
    u'\uad8d' : ['k _w _r _n tS', '궍'],
    u'\uad8e' : ['k _w _r _n _h', '궎'],
    u'\uad8f' : ['k _w _r t', '궏'],
    u'\uad90' : ['k _w _r _l', '궐'],
    u'\uad91' : ['k _w _r _l k', '궑'],
    u'\uad92' : ['k _w _r _l m', '궒'],
    u'\uad93' : ['k _w _r _l p', '궓'],
    u'\uad94' : ['k _w _r _l sh', '궔'],
    u'\uad95' : ['k _w _r _l t_h', '궕'],
    u'\uad96' : ['k _w _r _l p_h', '궖'],
    u'\uad97' : ['k _w _r _l _h', '궗'],
    u'\uad98' : ['k _w _r m', '궘'],
    u'\uad99' : ['k _w _r p', '궙'],
    u'\uad9a' : ['k _w _r p sh', '궚'],
    u'\uad9b' : ['k _w _r sh', '궛'],
    u'\uad9c' : ['k _w _r s', '궜'],
    u'\uad9d' : ['k _w _r N', '궝'],
    u'\uad9e' : ['k _w _r tS', '궞'],
    u'\uad9f' : ['k _w _r tSh', '궟'],
    u'\uada0' : ['k _w _r k_h', '궠'],
    u'\uada1' : ['k _w _r t_h', '궡'],
    u'\uada2' : ['k _w _r p_h', '궢'],
    u'\uada3' : ['k _w _r _h', '궣'],
    u'\uada4' : ['k _w E', '궤'],
    u'\uada5' : ['k _w E k', '궥'],
    u'\uada6' : ['k _w E k_>', '궦'],
    u'\uada7' : ['k _w E k sh', '궧'],
    u'\uada8' : ['k _w E _n', '궨'],
    u'\uada9' : ['k _w E _n tS', '궩'],
    u'\uadaa' : ['k _w E _n _h', '궪'],
    u'\uadab' : ['k _w E t', '궫'],
    u'\uadac' : ['k _w E _l', '궬'],
    u'\uadad' : ['k _w E _l k', '궭'],
    u'\uadae' : ['k _w E _l m', '궮'],
    u'\uadaf' : ['k _w E _l p', '궯'],
    u'\uadb0' : ['k _w E _l sh', '궰'],
    u'\uadb1' : ['k _w E _l t_h', '궱'],
    u'\uadb2' : ['k _w E _l p_h', '궲'],
    u'\uadb3' : ['k _w E _l _h', '궳'],
    u'\uadb4' : ['k _w E m', '궴'],
    u'\uadb5' : ['k _w E p', '궵'],
    u'\uadb6' : ['k _w E p sh', '궶'],
    u'\uadb7' : ['k _w E sh', '궷'],
    u'\uadb8' : ['k _w E s', '궸'],
    u'\uadb9' : ['k _w E N', '궹'],
    u'\uadba' : ['k _w E tS', '궺'],
    u'\uadbb' : ['k _w E tSh', '궻'],
    u'\uadbc' : ['k _w E k_h', '궼'],
    u'\uadbd' : ['k _w E t_h', '궽'],
    u'\uadbe' : ['k _w E p_h', '궾'],
    u'\uadbf' : ['k _w E _h', '궿'],
    u'\uadc0' : ['k 2', '귀'],
    u'\uadc1' : ['k 2 k', '귁'],
    u'\uadc2' : ['k 2 k_>', '귂'],
    u'\uadc3' : ['k 2 k sh', '귃'],
    u'\uadc4' : ['k 2 _n', '귄'],
    u'\uadc5' : ['k 2 _n tS', '귅'],
    u'\uadc6' : ['k 2 _n _h', '귆'],
    u'\uadc7' : ['k 2 t', '귇'],
    u'\uadc8' : ['k 2 _l', '귈'],
    u'\uadc9' : ['k 2 _l k', '귉'],
    u'\uadca' : ['k 2 _l m', '귊'],
    u'\uadcb' : ['k 2 _l p', '귋'],
    u'\uadcc' : ['k 2 _l sh', '귌'],
    u'\uadcd' : ['k 2 _l t_h', '귍'],
    u'\uadce' : ['k 2 _l p_h', '귎'],
    u'\uadcf' : ['k 2 _l _h', '귏'],
    u'\uadd0' : ['k 2 m', '귐'],
    u'\uadd1' : ['k 2 p', '귑'],
    u'\uadd2' : ['k 2 p sh', '귒'],
    u'\uadd3' : ['k 2 sh', '귓'],
    u'\uadd4' : ['k 2 s', '귔'],
    u'\uadd5' : ['k 2 N', '귕'],
    u'\uadd6' : ['k 2 tS', '귖'],
    u'\uadd7' : ['k 2 tSh', '귗'],
    u'\uadd8' : ['k 2 k_h', '귘'],
    u'\uadd9' : ['k 2 t_h', '귙'],
    u'\uadda' : ['k 2 p_h', '귚'],
    u'\uaddb' : ['k 2 _h', '귛'],
    u'\uaddc' : ['k _j u', '규'],
    u'\uaddd' : ['k _j u k', '귝'],
    u'\uadde' : ['k _j u k_>', '귞'],
    u'\uaddf' : ['k _j u k sh', '귟'],
    u'\uade0' : ['k _j u _n', '균'],
    u'\uade1' : ['k _j u _n tS', '귡'],
    u'\uade2' : ['k _j u _n _h', '귢'],
    u'\uade3' : ['k _j u t', '귣'],
    u'\uade4' : ['k _j u _l', '귤'],
    u'\uade5' : ['k _j u _l k', '귥'],
    u'\uade6' : ['k _j u _l m', '귦'],
    u'\uade7' : ['k _j u _l p', '귧'],
    u'\uade8' : ['k _j u _l sh', '귨'],
    u'\uade9' : ['k _j u _l t_h', '귩'],
    u'\uadea' : ['k _j u _l p_h', '귪'],
    u'\uadeb' : ['k _j u _l _h', '귫'],
    u'\uadec' : ['k _j u m', '귬'],
    u'\uaded' : ['k _j u p', '귭'],
    u'\uadee' : ['k _j u p sh', '귮'],
    u'\uadef' : ['k _j u sh', '귯'],
    u'\uadf0' : ['k _j u s', '귰'],
    u'\uadf1' : ['k _j u N', '귱'],
    u'\uadf2' : ['k _j u tS', '귲'],
    u'\uadf3' : ['k _j u tSh', '귳'],
    u'\uadf4' : ['k _j u k_h', '귴'],
    u'\uadf5' : ['k _j u t_h', '귵'],
    u'\uadf6' : ['k _j u p_h', '귶'],
    u'\uadf7' : ['k _j u _h', '귷'],
    u'\uadf8' : ['k M', '그'],
    u'\uadf9' : ['k M k', '극'],
    u'\uadfa' : ['k M k_>', '귺'],
    u'\uadfb' : ['k M k sh', '귻'],
    u'\uadfc' : ['k M _n', '근'],
    u'\uadfd' : ['k M _n tS', '귽'],
    u'\uadfe' : ['k M _n _h', '귾'],
    u'\uadff' : ['k M t', '귿'],
    u'\uae00' : ['k M _l', '글'],
    u'\uae01' : ['k M _l k', '긁'],
    u'\uae02' : ['k M _l m', '긂'],
    u'\uae03' : ['k M _l p', '긃'],
    u'\uae04' : ['k M _l sh', '긄'],
    u'\uae05' : ['k M _l t_h', '긅'],
    u'\uae06' : ['k M _l p_h', '긆'],
    u'\uae07' : ['k M _l _h', '긇'],
    u'\uae08' : ['k M m', '금'],
    u'\uae09' : ['k M p', '급'],
    u'\uae0a' : ['k M p sh', '긊'],
    u'\uae0b' : ['k M sh', '긋'],
    u'\uae0c' : ['k M s', '긌'],
    u'\uae0d' : ['k M N', '긍'],
    u'\uae0e' : ['k M tS', '긎'],
    u'\uae0f' : ['k M tSh', '긏'],
    u'\uae10' : ['k M k_h', '긐'],
    u'\uae11' : ['k M t_h', '긑'],
    u'\uae12' : ['k M p_h', '긒'],
    u'\uae13' : ['k M _h', '긓'],
    u'\uae14' : ['k M _j', '긔'],
    u'\uae15' : ['k M _j k', '긕'],
    u'\uae16' : ['k M _j k_>', '긖'],
    u'\uae17' : ['k M _j k sh', '긗'],
    u'\uae18' : ['k M _j _n', '긘'],
    u'\uae19' : ['k M _j _n tS', '긙'],
    u'\uae1a' : ['k M _j _n _h', '긚'],
    u'\uae1b' : ['k M _j t', '긛'],
    u'\uae1c' : ['k M _j _l', '긜'],
    u'\uae1d' : ['k M _j _l k', '긝'],
    u'\uae1e' : ['k M _j _l m', '긞'],
    u'\uae1f' : ['k M _j _l p', '긟'],
    u'\uae20' : ['k M _j _l sh', '긠'],
    u'\uae21' : ['k M _j _l t_h', '긡'],
    u'\uae22' : ['k M _j _l p_h', '긢'],
    u'\uae23' : ['k M _j _l _h', '긣'],
    u'\uae24' : ['k M _j m', '긤'],
    u'\uae25' : ['k M _j p', '긥'],
    u'\uae26' : ['k M _j p sh', '긦'],
    u'\uae27' : ['k M _j sh', '긧'],
    u'\uae28' : ['k M _j s', '긨'],
    u'\uae29' : ['k M _j N', '긩'],
    u'\uae2a' : ['k M _j tS', '긪'],
    u'\uae2b' : ['k M _j tSh', '긫'],
    u'\uae2c' : ['k M _j k_h', '긬'],
    u'\uae2d' : ['k M _j t_h', '긭'],
    u'\uae2e' : ['k M _j p_h', '긮'],
    u'\uae2f' : ['k M _j _h', '긯'],
    u'\uae30' : ['k i', '기'],
    u'\uae31' : ['k i k', '긱'],
    u'\uae32' : ['k i k_>', '긲'],
    u'\uae33' : ['k i k sh', '긳'],
    u'\uae34' : ['k i _n', '긴'],
    u'\uae35' : ['k i _n tS', '긵'],
    u'\uae36' : ['k i _n _h', '긶'],
    u'\uae37' : ['k i t', '긷'],
    u'\uae38' : ['k i _l', '길'],
    u'\uae39' : ['k i _l k', '긹'],
    u'\uae3a' : ['k i _l m', '긺'],
    u'\uae3b' : ['k i _l p', '긻'],
    u'\uae3c' : ['k i _l sh', '긼'],
    u'\uae3d' : ['k i _l t_h', '긽'],
    u'\uae3e' : ['k i _l p_h', '긾'],
    u'\uae3f' : ['k i _l _h', '긿'],
    u'\uae40' : ['k i m', '김'],
    u'\uae41' : ['k i p', '깁'],
    u'\uae42' : ['k i p sh', '깂'],
    u'\uae43' : ['k i sh', '깃'],
    u'\uae44' : ['k i s', '깄'],
    u'\uae45' : ['k i N', '깅'],
    u'\uae46' : ['k i tS', '깆'],
    u'\uae47' : ['k i tSh', '깇'],
    u'\uae48' : ['k i k_h', '깈'],
    u'\uae49' : ['k i t_h', '깉'],
    u'\uae4a' : ['k i p_h', '깊'],
    u'\uae4b' : ['k i _h', '깋'],
    u'\uae4c' : ['k_> a', '까'],
    u'\uae4d' : ['k_> a k', '깍'],
    u'\uae4e' : ['k_> a k_>', '깎'],
    u'\uae4f' : ['k_> a k ah', '깏'],
    u'\uae50' : ['k_> a _n', '깐'],
    u'\uae51' : ['k_> a _n tS', '깑'],
    u'\uae52' : ['k_> a _n _h', '깒'],
    u'\uae53' : ['k_> a t', '깓'],
    u'\uae54' : ['k_> a _l', '깔'],
    u'\uae55' : ['k_> a _l k', '깕'],
    u'\uae56' : ['k_> a _l m', '깖'],
    u'\uae57' : ['k_> a _l p', '깗'],
    u'\uae58' : ['k_> a _l sh', '깘'],
    u'\uae59' : ['k_> a _l t_h', '깙'],
    u'\uae5a' : ['k_> a _l p_h', '깚'],
    u'\uae5b' : ['k_> a _l _h', '깛'],
    u'\uae5c' : ['k_> a m', '깜'],
    u'\uae5d' : ['k_> a p', '깝'],
    u'\uae5e' : ['k_> a p sh', '깞'],
    u'\uae5f' : ['k_> a sh', '깟'],
    u'\uae60' : ['k_> a s', '깠'],
    u'\uae61' : ['k_> a N', '깡'],
    u'\uae62' : ['k_> a tS', '깢'],
    u'\uae63' : ['k_> a tSh', '깣'],
    u'\uae64' : ['k_> a k_h', '깤'],
    u'\uae65' : ['k_> a t_h', '깥'],
    u'\uae66' : ['k_> a p_h', '깦'],
    u'\uae67' : ['k_> a _h', '깧'],
    u'\uae68' : ['k_> {', '깨'],
    u'\uae69' : ['k_> { k', '깩'],
    u'\uae6a' : ['k_> { k_>', '깪'],
    u'\uae6b' : ['k_> { k sh', '깫'],
    u'\uae6c' : ['k_> { _n', '깬'],
    u'\uae6d' : ['k_> { _n tS', '깭'],
    u'\uae6e' : ['k_> { _n _h', '깮'],
    u'\uae6f' : ['k_> { t', '깯'],
    u'\uae70' : ['k_> { _l', '깰'],
    u'\uae71' : ['k_> { _l k', '깱'],
    u'\uae72' : ['k_> { _l m', '깲'],
    u'\uae73' : ['k_> { _l p', '깳'],
    u'\uae74' : ['k_> { _l sh', '깴'],
    u'\uae75' : ['k_> { _l t_h', '깵'],
    u'\uae76' : ['k_> { _l p_h', '깶'],
    u'\uae77' : ['k_> { _l _h', '깷'],
    u'\uae78' : ['k_> { m', '깸'],
    u'\uae79' : ['k_> { p', '깹'],
    u'\uae7a' : ['k_> { p sh', '깺'],
    u'\uae7b' : ['k_> { sh', '깻'],
    u'\uae7c' : ['k_> { s', '깼'],
    u'\uae7d' : ['k_> { N', '깽'],
    u'\uae7e' : ['k_> { tS', '깾'],
    u'\uae7f' : ['k_> { tSh', '깿'],
    u'\uae80' : ['k_> { k_h', '꺀'],
    u'\uae81' : ['k_> { t_h', '꺁'],
    u'\uae82' : ['k_> { p_h', '꺂'],
    u'\uae83' : ['k_> { _h', '꺃'],
    u'\uae84' : ['k_> _j a', '꺄'],
    u'\uae85' : ['k_> _j a k', '꺅'],
    u'\uae86' : ['k_> _j a k_>', '꺆'],
    u'\uae87' : ['k_> _j a k sh', '꺇'],
    u'\uae88' : ['k_> _j a _n', '꺈'],
    u'\uae89' : ['k_> _j a _n tS', '꺉'],
    u'\uae8a' : ['k_> _j a _n _h', '꺊'],
    u'\uae8b' : ['k_> _j a t', '꺋'],
    u'\uae8c' : ['k_> _j a _l', '꺌'],
    u'\uae8d' : ['k_> _j a _l k', '꺍'],
    u'\uae8e' : ['k_> _j a _l m', '꺎'],
    u'\uae8f' : ['k_> _j a _l p', '꺏'],
    u'\uae90' : ['k_> _j a _l sh', '꺐'],
    u'\uae91' : ['k_> _j a _l t_h', '꺑'],
    u'\uae92' : ['k_> _j a _l p_h', '꺒'],
    u'\uae93' : ['k_> _j a _l _h', '꺓'],
    u'\uae94' : ['k_> _j a m', '꺔'],
    u'\uae95' : ['k_> _j a p', '꺕'],
    u'\uae96' : ['k_> _j a p sh', '꺖'],
    u'\uae97' : ['k_> _j a sh', '꺗'],
    u'\uae98' : ['k_> _j a s', '꺘'],
    u'\uae99' : ['k_> _j a N', '꺙'],
    u'\uae9a' : ['k_> _j a tS', '꺚'],
    u'\uae9b' : ['k_> _j a tSh', '꺛'],
    u'\uae9c' : ['k_> _j a k_h', '꺜'],
    u'\uae9d' : ['k_> _j a t_h', '꺝'],
    u'\uae9e' : ['k_> _j a p_h', '꺞'],
    u'\uae9f' : ['k_> _j a _h', '꺟'],
    u'\uaea0' : ['k_> _j {', '꺠'],
    u'\uaea1' : ['k_> _j { k', '꺡'],
    u'\uaea2' : ['k_> _j { k_>', '꺢'],
    u'\uaea3' : ['k_> _j { k sh', '꺣'],
    u'\uaea4' : ['k_> _j { _n', '꺤'],
    u'\uaea5' : ['k_> _j { _n tS', '꺥'],
    u'\uaea6' : ['k_> _j { _n _h', '꺦'],
    u'\uaea7' : ['k_> _j { t', '꺧'],
    u'\uaea8' : ['k_> _j { _l', '꺨'],
    u'\uaea9' : ['k_> _j { _l k', '꺩'],
    u'\uaeaa' : ['k_> _j { _l m', '꺪'],
    u'\uaeab' : ['k_> _j { _l p', '꺫'],
    u'\uaeac' : ['k_> _j { _l sh', '꺬'],
    u'\uaead' : ['k_> _j { _l t_h', '꺭'],
    u'\uaeae' : ['k_> _j { _l p_h', '꺮'],
    u'\uaeaf' : ['k_> _j { _l _h', '꺯'],
    u'\uaeb0' : ['k_> _j { m', '꺰'],
    u'\uaeb1' : ['k_> _j { p', '꺱'],
    u'\uaeb2' : ['k_> _j { p sh', '꺲'],
    u'\uaeb3' : ['k_> _j { sh', '꺳'],
    u'\uaeb4' : ['k_> _j { s', '꺴'],
    u'\uaeb5' : ['k_> _j { N', '꺵'],
    u'\uaeb6' : ['k_> _j { tS', '꺶'],
    u'\uaeb7' : ['k_> _j { tSh', '꺷'],
    u'\uaeb8' : ['k_> _j { k_h', '꺸'],
    u'\uaeb9' : ['k_> _j { t_h', '꺹'],
    u'\uaeba' : ['k_> _j { p_h', '꺺'],
    u'\uaebb' : ['k_> _j { _h', '꺻'],
    u'\uaebc' : ['k_> _r', '꺼'],
    u'\uaebd' : ['k_> _r k', '꺽'],
    u'\uaebe' : ['k_> _r k_>', '꺾'],
    u'\uaebf' : ['k_> _r k sh', '꺿'],
    u'\uaec0' : ['k_> _r _n', '껀'],
    u'\uaec1' : ['k_> _r _n tS', '껁'],
    u'\uaec2' : ['k_> _r _n _h', '껂'],
    u'\uaec3' : ['k_> _r t', '껃'],
    u'\uaec4' : ['k_> _r _l', '껄'],
    u'\uaec5' : ['k_> _r _l k', '껅'],
    u'\uaec6' : ['k_> _r _l m', '껆'],
    u'\uaec7' : ['k_> _r _l p', '껇'],
    u'\uaec8' : ['k_> _r _l sh', '껈'],
    u'\uaec9' : ['k_> _r _l t_h', '껉'],
    u'\uaeca' : ['k_> _r _l p_h', '껊'],
    u'\uaecb' : ['k_> _r _l _h', '껋'],
    u'\uaecc' : ['k_> _r m', '껌'],
    u'\uaecd' : ['k_> _r p', '껍'],
    u'\uaece' : ['k_> _r p sh', '껎'],
    u'\uaecf' : ['k_> _r sh', '껏'],
    u'\uaed0' : ['k_> _r s', '껐'],
    u'\uaed1' : ['k_> _r N', '껑'],
    u'\uaed2' : ['k_> _r tS', '껒'],
    u'\uaed3' : ['k_> _r tSh', '껓'],
    u'\uaed4' : ['k_> _r k_h', '껔'],
    u'\uaed5' : ['k_> _r t_h', '껕'],
    u'\uaed6' : ['k_> _r p_h', '껖'],
    u'\uaed7' : ['k_> _r _h', '껗'],
    u'\uaed8' : ['k_> e', '께'],
    u'\uaed9' : ['k_> e k', '껙'],
    u'\uaeda' : ['k_> e k_>', '껚'],
    u'\uaedb' : ['k_> e k sh', '껛'],
    u'\uaedc' : ['k_> e _n', '껜'],
    u'\uaedd' : ['k_> e _n tS', '껝'],
    u'\uaede' : ['k_> e _n _h', '껞'],
    u'\uaedf' : ['k_> e t', '껟'],
    u'\uaee0' : ['k_> e _l', '껠'],
    u'\uaee1' : ['k_> e _l k', '껡'],
    u'\uaee2' : ['k_> e _l m', '껢'],
    u'\uaee3' : ['k_> e _l p', '껣'],
    u'\uaee4' : ['k_> e _l sh', '껤'],
    u'\uaee5' : ['k_> e _l t_h', '껥'],
    u'\uaee6' : ['k_> e _l p_h', '껦'],
    u'\uaee7' : ['k_> e _l _h', '껧'],
    u'\uaee8' : ['k_> e m', '껨'],
    u'\uaee9' : ['k_> e p', '껩'],
    u'\uaeea' : ['k_> e p sh', '껪'],
    u'\uaeeb' : ['k_> e sh', '껫'],
    u'\uaeec' : ['k_> e s', '껬'],
    u'\uaeed' : ['k_> e N', '껭'],
    u'\uaeee' : ['k_> e tS', '껮'],
    u'\uaeef' : ['k_> e tSh', '껯'],
    u'\uaef0' : ['k_> e k_h', '껰'],
    u'\uaef1' : ['k_> e t_h', '껱'],
    u'\uaef2' : ['k_> e p_h', '껲'],
    u'\uaef3' : ['k_> e _h', '껳'],
    u'\uaef4' : ['k_> _j _r', '껴'],
    u'\uaef5' : ['k_> _j _r k', '껵'],
    u'\uaef6' : ['k_> _j _r k_>', '껶'],
    u'\uaef7' : ['k_> _j _r k sh', '껷'],
    u'\uaef8' : ['k_> _j _r _n', '껸'],
    u'\uaef9' : ['k_> _j _r _n tS', '껹'],
    u'\uaefa' : ['k_> _j _r _n _h', '껺'],
    u'\uaefb' : ['k_> _j _r t', '껻'],
    u'\uaefc' : ['k_> _j _r _l', '껼'],
    u'\uaefd' : ['k_> _j _r _l k', '껽'],
    u'\uaefe' : ['k_> _j _r _l m', '껾'],
    u'\uaeff' : ['k_> _j _r _l p', '껿'],
    u'\uaf00' : ['k_> _j _r _l sh', '꼀'],
    u'\uaf01' : ['k_> _j _r _l t_h', '꼁'],
    u'\uaf02' : ['k_> _j _r _l p_h', '꼂'],
    u'\uaf03' : ['k_> _j _r _l _h', '꼃'],
    u'\uaf04' : ['k_> _j _r m', '꼄'],
    u'\uaf05' : ['k_> _j _r p', '꼅'],
    u'\uaf06' : ['k_> _j _r p sh', '꼆'],
    u'\uaf07' : ['k_> _j _r sh', '꼇'],
    u'\uaf08' : ['k_> _j _r s', '꼈'],
    u'\uaf09' : ['k_> _j _r N', '꼉'],
    u'\uaf0a' : ['k_> _j _r tS', '꼊'],
    u'\uaf0b' : ['k_> _j _r tSh', '꼋'],
    u'\uaf0c' : ['k_> _j _r k_h', '꼌'],
    u'\uaf0d' : ['k_> _j _r t_h', '꼍'],
    u'\uaf0e' : ['k_> _j _r p_h', '꼎'],
    u'\uaf0f' : ['k_> _j _r _h', '꼏'],
    u'\uaf10' : ['k_> _j e', '꼐'],
    u'\uaf11' : ['k_> _j e k', '꼑'],
    u'\uaf12' : ['k_> _j e k_>', '꼒'],
    u'\uaf13' : ['k_> _j e k sh', '꼓'],
    u'\uaf14' : ['k_> _j e _n', '꼔'],
    u'\uaf15' : ['k_> _j e _n tS', '꼕'],
    u'\uaf16' : ['k_> _j e _n _h', '꼖'],
    u'\uaf17' : ['k_> _j e t', '꼗'],
    u'\uaf18' : ['k_> _j e _l', '꼘'],
    u'\uaf19' : ['k_> _j e _l k', '꼙'],
    u'\uaf1a' : ['k_> _j e _l m', '꼚'],
    u'\uaf1b' : ['k_> _j e _l p', '꼛'],
    u'\uaf1c' : ['k_> _j e _l sh', '꼜'],
    u'\uaf1d' : ['k_> _j e _l t_h', '꼝'],
    u'\uaf1e' : ['k_> _j e _l p_h', '꼞'],
    u'\uaf1f' : ['k_> _j e _l _h', '꼟'],
    u'\uaf20' : ['k_> _j e m', '꼠'],
    u'\uaf21' : ['k_> _j e p', '꼡'],
    u'\uaf22' : ['k_> _j e p sh', '꼢'],
    u'\uaf23' : ['k_> _j e sh', '꼣'],
    u'\uaf24' : ['k_> _j e s', '꼤'],
    u'\uaf25' : ['k_> _j e N', '꼥'],
    u'\uaf26' : ['k_> _j e tS', '꼦'],
    u'\uaf27' : ['k_> _j e tSh', '꼧'],
    u'\uaf28' : ['k_> _j e k_h', '꼨'],
    u'\uaf29' : ['k_> _j e t_h', '꼩'],
    u'\uaf2a' : ['k_> _j e p_h', '꼪'],
    u'\uaf2b' : ['k_> _j e _h', '꼫'],
    u'\uaf2c' : ['k_> o', '꼬'],
    u'\uaf2d' : ['k_> o k', '꼭'],
    u'\uaf2e' : ['k_> o k_>', '꼮'],
    u'\uaf2f' : ['k_> o k sh', '꼯'],
    u'\uaf30' : ['k_> o _n', '꼰'],
    u'\uaf31' : ['k_> o _n tS', '꼱'],
    u'\uaf32' : ['k_> o _n _h', '꼲'],
    u'\uaf33' : ['k_> o t', '꼳'],
    u'\uaf34' : ['k_> o _l', '꼴'],
    u'\uaf35' : ['k_> o _l k', '꼵'],
    u'\uaf36' : ['k_> o _l m', '꼶'],
    u'\uaf37' : ['k_> o _l p', '꼷'],
    u'\uaf38' : ['k_> o _l sh', '꼸'],
    u'\uaf39' : ['k_> o _l t_h', '꼹'],
    u'\uaf3a' : ['k_> o _l p_h', '꼺'],
    u'\uaf3b' : ['k_> o _l _h', '꼻'],
    u'\uaf3c' : ['k_> o m', '꼼'],
    u'\uaf3d' : ['k_> o p', '꼽'],
    u'\uaf3e' : ['k_> o p sh', '꼾'],
    u'\uaf3f' : ['k_> o sh', '꼿'],
    u'\uaf40' : ['k_> o s', '꽀'],
    u'\uaf41' : ['k_> o N', '꽁'],
    u'\uaf42' : ['k_> o tS', '꽂'],
    u'\uaf43' : ['k_> o tSh', '꽃'],
    u'\uaf44' : ['k_> o k_h', '꽄'],
    u'\uaf45' : ['k_> o t_h', '꽅'],
    u'\uaf46' : ['k_> o p_h', '꽆'],
    u'\uaf47' : ['k_> o _h', '꽇'],
    u'\uaf48' : ['k_> _w a', '꽈'],
    u'\uaf49' : ['k_> _w a k', '꽉'],
    u'\uaf4a' : ['k_> _w a k_>', '꽊'],
    u'\uaf4b' : ['k_> _w a k sh', '꽋'],
    u'\uaf4c' : ['k_> _w a _n', '꽌'],
    u'\uaf4d' : ['k_> _w a _n tS', '꽍'],
    u'\uaf4e' : ['k_> _w a _n _h', '꽎'],
    u'\uaf4f' : ['k_> _w a t', '꽏'],
    u'\uaf50' : ['k_> _w a _l', '꽐'],
    u'\uaf51' : ['k_> _w a _l k', '꽑'],
    u'\uaf52' : ['k_> _w a _l m', '꽒'],
    u'\uaf53' : ['k_> _w a _l p', '꽓'],
    u'\uaf54' : ['k_> _w a _l sh', '꽔'],
    u'\uaf55' : ['k_> _w a _l t_h', '꽕'],
    u'\uaf56' : ['k_> _w a _l p_h', '꽖'],
    u'\uaf57' : ['k_> _w a _l _h', '꽗'],
    u'\uaf58' : ['k_> _w a m', '꽘'],
    u'\uaf59' : ['k_> _w a p', '꽙'],
    u'\uaf5a' : ['k_> _w a p sh', '꽚'],
    u'\uaf5b' : ['k_> _w a sh', '꽛'],
    u'\uaf5c' : ['k_> _w a s', '꽜'],
    u'\uaf5d' : ['k_> _w a N', '꽝'],
    u'\uaf5e' : ['k_> _w a tS', '꽞'],
    u'\uaf5f' : ['k_> _w a tSh', '꽟'],
    u'\uaf60' : ['k_> _w a k_h', '꽠'],
    u'\uaf61' : ['k_> _w a t_h', '꽡'],
    u'\uaf62' : ['k_> _w a p_h', '꽢'],
    u'\uaf63' : ['k_> _w a _h', '꽣'],
    u'\uaf64' : ['k_> _w {', '꽤'],
    u'\uaf65' : ['k_> _w { k', '꽥'],
    u'\uaf66' : ['k_> _w { k_>', '꽦'],
    u'\uaf67' : ['k_> _w { k sh', '꽧'],
    u'\uaf68' : ['k_> _w { _n', '꽨'],
    u'\uaf69' : ['k_> _w { _n tS', '꽩'],
    u'\uaf6a' : ['k_> _w { _n _h', '꽪'],
    u'\uaf6b' : ['k_> _w { t', '꽫'],
    u'\uaf6c' : ['k_> _w { _l', '꽬'],
    u'\uaf6d' : ['k_> _w { _l k', '꽭'],
    u'\uaf6e' : ['k_> _w { _l m', '꽮'],
    u'\uaf6f' : ['k_> _w { _l p', '꽯'],
    u'\uaf70' : ['k_> _w { _l sh', '꽰'],
    u'\uaf71' : ['k_> _w { _l t_h', '꽱'],
    u'\uaf72' : ['k_> _w { _l p_h', '꽲'],
    u'\uaf73' : ['k_> _w { _l _h', '꽳'],
    u'\uaf74' : ['k_> _w { m', '꽴'],
    u'\uaf75' : ['k_> _w { p', '꽵'],
    u'\uaf76' : ['k_> _w { p sh', '꽶'],
    u'\uaf77' : ['k_> _w { sh', '꽷'],
    u'\uaf78' : ['k_> _w { s', '꽸'],
    u'\uaf79' : ['k_> _w { N', '꽹'],
    u'\uaf7a' : ['k_> _w { tS', '꽺'],
    u'\uaf7b' : ['k_> _w { tSh', '꽻'],
    u'\uaf7c' : ['k_> _w { k_h', '꽼'],
    u'\uaf7d' : ['k_> _w { t_h', '꽽'],
    u'\uaf7e' : ['k_> _w { p_h', '꽾'],
    u'\uaf7f' : ['k_> _w { _h', '꽿'],
    u'\uaf80' : ['k_> _w e', '꾀'],
    u'\uaf81' : ['k_> _w e k', '꾁'],
    u'\uaf82' : ['k_> _w e k_>', '꾂'],
    u'\uaf83' : ['k_> _w e k sh', '꾃'],
    u'\uaf84' : ['k_> _w e _n', '꾄'],
    u'\uaf85' : ['k_> _w e _n tS', '꾅'],
    u'\uaf86' : ['k_> _w e _n _h', '꾆'],
    u'\uaf87' : ['k_> _w e t', '꾇'],
    u'\uaf88' : ['k_> _w e _l', '꾈'],
    u'\uaf89' : ['k_> _w e _l k', '꾉'],
    u'\uaf8a' : ['k_> _w e _l m', '꾊'],
    u'\uaf8b' : ['k_> _w e _l p', '꾋'],
    u'\uaf8c' : ['k_> _w e _l sh', '꾌'],
    u'\uaf8d' : ['k_> _w e _l t_h', '꾍'],
    u'\uaf8e' : ['k_> _w e _l p_h', '꾎'],
    u'\uaf8f' : ['k_> _w e _l _h', '꾏'],
    u'\uaf90' : ['k_> _w e m', '꾐'],
    u'\uaf91' : ['k_> _w e p', '꾑'],
    u'\uaf92' : ['k_> _w e p sh', '꾒'],
    u'\uaf93' : ['k_> _w e sh', '꾓'],
    u'\uaf94' : ['k_> _w e s', '꾔'],
    u'\uaf95' : ['k_> _w e N', '꾕'],
    u'\uaf96' : ['k_> _w e tS', '꾖'],
    u'\uaf97' : ['k_> _w e tSh', '꾗'],
    u'\uaf98' : ['k_> _w e k_h', '꾘'],
    u'\uaf99' : ['k_> _w e t_h', '꾙'],
    u'\uaf9a' : ['k_> _w e p_h', '꾚'],
    u'\uaf9b' : ['k_> _w e _h', '꾛'],
    u'\uaf9c' : ['k_> _j o', '꾜'],
    u'\uaf9d' : ['k_> _j o k', '꾝'],
    u'\uaf9e' : ['k_> _j o k_>', '꾞'],
    u'\uaf9f' : ['k_> _j o k sh', '꾟'],
    u'\uafa0' : ['k_> _j o _n', '꾠'],
    u'\uafa1' : ['k_> _j o _n tS', '꾡'],
    u'\uafa2' : ['k_> _j o _n _h', '꾢'],
    u'\uafa3' : ['k_> _j o t', '꾣'],
    u'\uafa4' : ['k_> _j o _l', '꾤'],
    u'\uafa5' : ['k_> _j o _l k', '꾥'],
    u'\uafa6' : ['k_> _j o _l m', '꾦'],
    u'\uafa7' : ['k_> _j o _l p', '꾧'],
    u'\uafa8' : ['k_> _j o _l sh', '꾨'],
    u'\uafa9' : ['k_> _j o _l t_h', '꾩'],
    u'\uafaa' : ['k_> _j o _l p_h', '꾪'],
    u'\uafab' : ['k_> _j o _l _h', '꾫'],
    u'\uafac' : ['k_> _j o m', '꾬'],
    u'\uafad' : ['k_> _j o p', '꾭'],
    u'\uafae' : ['k_> _j o p sh', '꾮'],
    u'\uafaf' : ['k_> _j o sh', '꾯'],
    u'\uafb0' : ['k_> _j o s', '꾰'],
    u'\uafb1' : ['k_> _j o N', '꾱'],
    u'\uafb2' : ['k_> _j o tS', '꾲'],
    u'\uafb3' : ['k_> _j o tSh', '꾳'],
    u'\uafb4' : ['k_> _j o k_h', '꾴'],
    u'\uafb5' : ['k_> _j o t_h', '꾵'],
    u'\uafb6' : ['k_> _j o p_h', '꾶'],
    u'\uafb7' : ['k_> _j o _h', '꾷'],
    u'\uafb8' : ['k_> u', '꾸'],
    u'\uafb9' : ['k_> u k', '꾹'],
    u'\uafba' : ['k_> u k_>', '꾺'],
    u'\uafbb' : ['k_> u k sh', '꾻'],
    u'\uafbc' : ['k_> u _n', '꾼'],
    u'\uafbd' : ['k_> u _n tS', '꾽'],
    u'\uafbe' : ['k_> u _n _h', '꾾'],
    u'\uafbf' : ['k_> u t', '꾿'],
    u'\uafc0' : ['k_> u _l', '꿀'],
    u'\uafc1' : ['k_> u _l k', '꿁'],
    u'\uafc2' : ['k_> u _l m', '꿂'],
    u'\uafc3' : ['k_> u _l p', '꿃'],
    u'\uafc4' : ['k_> u _l sh', '꿄'],
    u'\uafc5' : ['k_> u _l t_h', '꿅'],
    u'\uafc6' : ['k_> u _l p_h', '꿆'],
    u'\uafc7' : ['k_> u _l _h', '꿇'],
    u'\uafc8' : ['k_> u m', '꿈'],
    u'\uafc9' : ['k_> u p', '꿉'],
    u'\uafca' : ['k_> u p sh', '꿊'],
    u'\uafcb' : ['k_> u sh', '꿋'],
    u'\uafcc' : ['k_> u s', '꿌'],
    u'\uafcd' : ['k_> u d', '꿍'],
    u'\uafce' : ['k_> u tS', '꿎'],
    u'\uafcf' : ['k_> u tSh', '꿏'],
    u'\uafd0' : ['k_> u k_h', '꿐'],
    u'\uafd1' : ['k_> u t_h', '꿑'],
    u'\uafd2' : ['k_> u p_h', '꿒'],
    u'\uafd3' : ['k_> u _h', '꿓'],
    u'\uafd4' : ['k_> _w _r', '꿔'],
    u'\uafd5' : ['k_> _w _r k', '꿕'],
    u'\uafd6' : ['k_> _w _r k_>', '꿖'],
    u'\uafd7' : ['k_> _w _r k sh', '꿗'],
    u'\uafd8' : ['k_> _w _r _n', '꿘'],
    u'\uafd9' : ['k_> _w _r _n tS', '꿙'],
    u'\uafda' : ['k_> _w _r _n _h', '꿚'],
    u'\uafdb' : ['k_> _w _r t', '꿛'],
    u'\uafdc' : ['k_> _w _r _l', '꿜'],
    u'\uafdd' : ['k_> _w _r _l k', '꿝'],
    u'\uafde' : ['k_> _w _r _l m', '꿞'],
    u'\uafdf' : ['k_> _w _r _l p', '꿟'],
    u'\uafe0' : ['k_> _w _r _l sh', '꿠'],
    u'\uafe1' : ['k_> _w _r _l t_h', '꿡'],
    u'\uafe2' : ['k_> _w _r _l p_h', '꿢'],
    u'\uafe3' : ['k_> _w _r _l _h', '꿣'],
    u'\uafe4' : ['k_> _w _r m', '꿤'],
    u'\uafe5' : ['k_> _w _r p', '꿥'],
    u'\uafe6' : ['k_> _w _r p sh', '꿦'],
    u'\uafe7' : ['k_> _w _r sh', '꿧'],
    u'\uafe8' : ['k_> _w _r s', '꿨'],
    u'\uafe9' : ['k_> _w _r N', '꿩'],
    u'\uafea' : ['k_> _w _r tS', '꿪'],
    u'\uafeb' : ['k_> _w _r tSh', '꿫'],
    u'\uafec' : ['k_> _w _r k_h', '꿬'],
    u'\uafed' : ['k_> _w _r t_h', '꿭'],
    u'\uafee' : ['k_> _w _r p_h', '꿮'],
    u'\uafef' : ['k_> _w _r _h', '꿯'],
    u'\uaff0' : ['k_> _w E', '꿰'],
    u'\uaff1' : ['k_> _w E k', '꿱'],
    u'\uaff2' : ['k_> _w E k_>', '꿲'],
    u'\uaff3' : ['k_> _w E k sh', '꿳'],
    u'\uaff4' : ['k_> _w E _n', '꿴'],
    u'\uaff5' : ['k_> _w E _n tS', '꿵'],
    u'\uaff6' : ['k_> _w E _n _h', '꿶'],
    u'\uaff7' : ['k_> _w E t', '꿷'],
    u'\uaff8' : ['k_> _w E _l', '꿸'],
    u'\uaff9' : ['k_> _w E _l k', '꿹'],
    u'\uaffa' : ['k_> _w E _l m', '꿺'],
    u'\uaffb' : ['k_> _w E _l p', '꿻'],
    u'\uaffc' : ['k_> _w E _l sh', '꿼'],
    u'\uaffd' : ['k_> _w E _l t_h', '꿽'],
    u'\uaffe' : ['k_> _w E _l p_h', '꿾'],
    u'\uafff' : ['k_> _w E _l _h', '꿿'],
    u'\ub000' : ['k_> _w E m', '뀀'],
    u'\ub001' : ['k_> _w E p', '뀁'],
    u'\ub002' : ['k_> _w E p sh', '뀂'],
    u'\ub003' : ['k_> _w E sh', '뀃'],
    u'\ub004' : ['k_> _w E s', '뀄'],
    u'\ub005' : ['k_> _w E N', '뀅'],
    u'\ub006' : ['k_> _w E tS', '뀆'],
    u'\ub007' : ['k_> _w E tSh', '뀇'],
    u'\ub008' : ['k_> _w E k_h', '뀈'],
    u'\ub009' : ['k_> _w E t_h', '뀉'],
    u'\ub00a' : ['k_> _w E p_h', '뀊'],
    u'\ub00b' : ['k_> _w E _h', '뀋'],
    u'\ub00c' : ['k_> 2', '뀌'],
    u'\ub00d' : ['k_> 2 k', '뀍'],
    u'\ub00e' : ['k_> 2 k_>', '뀎'],
    u'\ub00f' : ['k_> 2 k sh', '뀏'],
    u'\ub010' : ['k_> 2 _n', '뀐'],
    u'\ub011' : ['k_> 2 _n tS', '뀑'],
    u'\ub012' : ['k_> 2 _n _h', '뀒'],
    u'\ub013' : ['k_> 2 t', '뀓'],
    u'\ub014' : ['k_> 2 _l', '뀔'],
    u'\ub015' : ['k_> 2 _l k', '뀕'],
    u'\ub016' : ['k_> 2 _l m', '뀖'],
    u'\ub017' : ['k_> 2 _l p', '뀗'],
    u'\ub018' : ['k_> 2 _l sh', '뀘'],
    u'\ub019' : ['k_> 2 _l t_h', '뀙'],
    u'\ub01a' : ['k_> 2 _l p_h', '뀚'],
    u'\ub01b' : ['k_> 2 _l _h', '뀛'],
    u'\ub01c' : ['k_> 2 m', '뀜'],
    u'\ub01d' : ['k_> 2 p', '뀝'],
    u'\ub01e' : ['k_> 2 p sh', '뀞'],
    u'\ub01f' : ['k_> 2 sh', '뀟'],
    u'\ub020' : ['k_> 2 s', '뀠'],
    u'\ub021' : ['k_> 2 N', '뀡'],
    u'\ub022' : ['k_> 2 tS', '뀢'],
    u'\ub023' : ['k_> 2 tSh', '뀣'],
    u'\ub024' : ['k_> 2 k_h', '뀤'],
    u'\ub025' : ['k_> 2 t_h', '뀥'],
    u'\ub026' : ['k_> 2 p_h', '뀦'],
    u'\ub027' : ['k_> 2 _h', '뀧'],
    u'\ub028' : ['k_> _j u', '뀨'],
    u'\ub029' : ['k_> _j u k', '뀩'],
    u'\ub02a' : ['k_> _j u k_>', '뀪'],
    u'\ub02b' : ['k_> _j u k sh', '뀫'],
    u'\ub02c' : ['k_> _j u _n', '뀬'],
    u'\ub02d' : ['k_> _j u _n tS', '뀭'],
    u'\ub02e' : ['k_> _j u _n _h', '뀮'],
    u'\ub02f' : ['k_> _j u t', '뀯'],
    u'\ub030' : ['k_> _j u _l', '뀰'],
    u'\ub031' : ['k_> _j u _l k', '뀱'],
    u'\ub032' : ['k_> _j u _l m', '뀲'],
    u'\ub033' : ['k_> _j u _l p', '뀳'],
    u'\ub034' : ['k_> _j u _l sh', '뀴'],
    u'\ub035' : ['k_> _j u _l t_h', '뀵'],
    u'\ub036' : ['k_> _j u _l p_h', '뀶'],
    u'\ub037' : ['k_> _j u _l _h', '뀷'],
    u'\ub038' : ['k_> _j u m', '뀸'],
    u'\ub039' : ['k_> _j u p', '뀹'],
    u'\ub03a' : ['k_> _j u p sh', '뀺'],
    u'\ub03b' : ['k_> _j u sh', '뀻'],
    u'\ub03c' : ['k_> _j u s', '뀼'],
    u'\ub03d' : ['k_> _j u N', '뀽'],
    u'\ub03e' : ['k_> _j u tS', '뀾'],
    u'\ub03f' : ['k_> _j u tSh', '뀿'],
    u'\ub040' : ['k_> _j u k_h', '끀'],
    u'\ub041' : ['k_> _j u t_h', '끁'],
    u'\ub042' : ['k_> _j u p_h', '끂'],
    u'\ub043' : ['k_> _j u _h', '끃'],
    u'\ub044' : ['k_> M', '끄'],
    u'\ub045' : ['k_> M k', '끅'],
    u'\ub046' : ['k_> M k_>', '끆'],
    u'\ub047' : ['k_> M k sh', '끇'],
    u'\ub048' : ['k_> M _n', '끈'],
    u'\ub049' : ['k_> M _n tS', '끉'],
    u'\ub04a' : ['k_> M _n _h', '끊'],
    u'\ub04b' : ['k_> M t', '끋'],
    u'\ub04c' : ['k_> M _l', '끌'],
    u'\ub04d' : ['k_> M _l k', '끍'],
    u'\ub04e' : ['k_> M _l m', '끎'],
    u'\ub04f' : ['k_> M _l p', '끏'],
    u'\ub050' : ['k_> M _l sh', '끐'],
    u'\ub051' : ['k_> M _l t_h', '끑'],
    u'\ub052' : ['k_> M _l p_h', '끒'],
    u'\ub053' : ['k_> M _l _h', '끓'],
    u'\ub054' : ['k_> M m', '끔'],
    u'\ub055' : ['k_> M p', '끕'],
    u'\ub056' : ['k_> M p sh', '끖'],
    u'\ub057' : ['k_> M sh', '끗'],
    u'\ub058' : ['k_> M s', '끘'],
    u'\ub059' : ['k_> M N', '끙'],
    u'\ub05a' : ['k_> M tS', '끚'],
    u'\ub05b' : ['k_> M tSh', '끛'],
    u'\ub05c' : ['k_> M k_h', '끜'],
    u'\ub05d' : ['k_> M t_h', '끝'],
    u'\ub05e' : ['k_> M p_h', '끞'],
    u'\ub05f' : ['k_> M _h', '끟'],
    u'\ub060' : ['k_> M _j', '끠'],
    u'\ub061' : ['k_> M _j k', '끡'],
    u'\ub062' : ['k_> M _j k_>', '끢'],
    u'\ub063' : ['k_> M _j k sh', '끣'],
    u'\ub064' : ['k_> M _j _n', '끤'],
    u'\ub065' : ['k_> M _j _n tS', '끥'],
    u'\ub066' : ['k_> M _j _n _h', '끦'],
    u'\ub067' : ['k_> M _j t', '끧'],
    u'\ub068' : ['k_> M _j _l', '끨'],
    u'\ub069' : ['k_> M _j _l k', '끩'],
    u'\ub06a' : ['k_> M _j _l m', '끪'],
    u'\ub06b' : ['k_> M _j _l p', '끫'],
    u'\ub06c' : ['k_> M _j _l sh', '끬'],
    u'\ub06d' : ['k_> M _j _l t_h', '끭'],
    u'\ub06e' : ['k_> M _j _l p_h', '끮'],
    u'\ub06f' : ['k_> M _j _l _h', '끯'],
    u'\ub070' : ['k_> M _j m', '끰'],
    u'\ub071' : ['k_> M _j p', '끱'],
    u'\ub072' : ['k_> M _j p sh', '끲'],
    u'\ub073' : ['k_> M _j sh', '끳'],
    u'\ub074' : ['k_> M _j s', '끴'],
    u'\ub075' : ['k_> M _j N', '끵'],
    u'\ub076' : ['k_> M _j tS', '끶'],
    u'\ub077' : ['k_> M _j tSh', '끷'],
    u'\ub078' : ['k_> M _j k_h', '끸'],
    u'\ub079' : ['k_> M _j t_h', '끹'],
    u'\ub07a' : ['k_> M _j p_h', '끺'],
    u'\ub07b' : ['k_> M _j _h', '끻'],
    u'\ub07c' : ['k_> i', '끼'],
    u'\ub07d' : ['k_> i k', '끽'],
    u'\ub07e' : ['k_> i k_>', '끾'],
    u'\ub07f' : ['k_> i k sh', '끿'],
    u'\ub080' : ['k_> i _n', '낀'],
    u'\ub081' : ['k_> i _n tS', '낁'],
    u'\ub082' : ['k_> i _n _h', '낂'],
    u'\ub083' : ['k_> i t', '낃'],
    u'\ub084' : ['k_> i _l', '낄'],
    u'\ub085' : ['k_> i _l k', '낅'],
    u'\ub086' : ['k_> i _l m', '낆'],
    u'\ub087' : ['k_> i _l p', '낇'],
    u'\ub088' : ['k_> i _l sh', '낈'],
    u'\ub089' : ['k_> i _l t_h', '낉'],
    u'\ub08a' : ['k_> i _l p_h', '낊'],
    u'\ub08b' : ['k_> i _l _h', '낋'],
    u'\ub08c' : ['k_> i m', '낌'],
    u'\ub08d' : ['k_> i p', '낍'],
    u'\ub08e' : ['k_> i p sh', '낎'],
    u'\ub08f ' : ['k_> i sh', '낏 '],
    u'\ub090' : ['k_> i s', '낐'],
    u'\ub091' : ['k_> i N', '낑'],
    u'\ub092' : ['k_> i tS', '낒'],
    u'\ub093' : ['k_> i tSh', '낓'],
    u'\ub094' : ['k_> i k_h', '낔'],
    u'\ub095' : ['k_> i t_h', '낕'],
    u'\ub096' : ['k_> i p_h', '낖'],
    u'\ub097' : ['k_> i _h', '낗'],
    u'\ub098' : ['_n a', '나'],
    u'\ub099' : ['_n a k', '낙'],
    u'\ub09a' : ['_n a k_>', '낚'],
    u'\ub09b' : ['_n a k sh', '낛'],
    u'\ub09c' : ['_n a _n', '난'],
    u'\ub09d' : ['_n a _n tS', '낝'],
    u'\ub09e' : ['_n a _n _h', '낞'],
    u'\ub09f' : ['_n a t', '낟'],
    u'\ub0a0' : ['_n a _l', '날'],
    u'\ub0a1' : ['_n a _l k', '낡'],
    u'\ub0a2' : ['_n a _l m', '낢'],
    u'\ub0a3' : ['_n a _l p', '낣'],
    u'\ub0a4' : ['_n a _l sh', '낤'],
    u'\ub0a5' : ['_n a _l t_h', '낥'],
    u'\ub0a6' : ['_n a _l p_h', '낦'],
    u'\ub0a7' : ['_n a _l _h', '낧'],
    u'\ub0a8' : ['_n a m', '남'],
    u'\ub0a9' : ['_n a p', '납'],
    u'\ub0aa' : ['_n a p sh', '낪'],
    u'\ub0ab' : ['_n a sh', '낫'],
    u'\ub0ac' : ['_n a s', '났'],
    u'\ub0ad' : ['_n a N', '낭'],
    u'\ub0ae' : ['_n a tS', '낮'],
    u'\ub0af' : ['_n a tSh', '낯'],
    u'\ub0b0' : ['_n a k_h', '낰'],
    u'\ub0b1' : ['_n a t_h', '낱'],
    u'\ub0b2' : ['_n a p_h', '낲'],
    u'\ub0b3' : ['_n a _h', '낳'],
    u'\ub0b4' : ['_n {', '내'],
    u'\ub0b5' : ['_n { k', '낵'],
    u'\ub0b6' : ['_n { k_>', '낶'],
    u'\ub0b7' : ['_n { k sh', '낷'],
    u'\ub0b8' : ['_n { _n', '낸'],
    u'\ub0b9' : ['_n { _n tS', '낹'],
    u'\ub0ba' : ['_n { _n _h', '낺'],
    u'\ub0bb' : ['_n { t', '낻'],
    u'\ub0bc' : ['_n { _l', '낼'],
    u'\ub0bd' : ['_n { _l k', '낽'],
    u'\ub0be' : ['_n { _l m', '낾'],
    u'\ub0bf' : ['_n { _l p', '낿'],
    u'\ub0c0' : ['_n { _l sh', '냀'],
    u'\ub0c1' : ['_n { _l t_h', '냁'],
    u'\ub0c2' : ['_n { _l p_h', '냂'],
    u'\ub0c3' : ['_n { _l _h', '냃'],
    u'\ub0c4' : ['_n { m', '냄'],
    u'\ub0c5' : ['_n { p', '냅'],
    u'\ub0c6' : ['_n { p sh', '냆'],
    u'\ub0c7' : ['_n { sh', '냇'],
    u'\ub0c8' : ['_n { s', '냈'],
    u'\ub0c9' : ['_n { N', '냉'],
    u'\ub0ca' : ['_n { tS', '냊'],
    u'\ub0cb' : ['_n { tSh', '냋'],
    u'\ub0cc' : ['_n { k_h', '냌'],
    u'\ub0cd' : ['_n { t_h', '냍'],
    u'\ub0ce' : ['_n { p_h', '냎'],
    u'\ub0cf' : ['_n { _h', '냏'],
    u'\ub0d0' : ['_n _j a', '냐'],
    u'\ub0d1' : ['_n _j a k', '냑'],
    u'\ub0d2' : ['_n _j a k_>', '냒'],
    u'\ub0d3' : ['_n _j a k sh', '냓'],
    u'\ub0d4' : ['_n _j a _n', '냔'],
    u'\ub0d5' : ['_n _j a _n tS', '냕'],
    u'\ub0d6' : ['_n _j a _n _h', '냖'],
    u'\ub0d7' : ['_n _j a t', '냗'],
    u'\ub0d8' : ['_n _j a _l', '냘'],
    u'\ub0d9' : ['_n _j a _l k', '냙'],
    u'\ub0da' : ['_n _j a _l m', '냚'],
    u'\ub0db' : ['_n _j a _l p', '냛'],
    u'\ub0dc' : ['_n _j a _l sh', '냜'],
    u'\ub0dd' : ['_n _j a _l t_h', '냝'],
    u'\ub0de' : ['_n _j a _l p_h', '냞'],
    u'\ub0df' : ['_n _j a _l _h', '냟'],
    u'\ub0e0' : ['_n _j a m', '냠'],
    u'\ub0e1' : ['_n _j a p', '냡'],
    u'\ub0e2' : ['_n _j a p sh', '냢'],
    u'\ub0e3' : ['_n _j a sh', '냣'],
    u'\ub0e4' : ['_n _j a s', '냤'],
    u'\ub0e5' : ['_n _j a N', '냥'],
    u'\ub0e6' : ['_n _j a tS', '냦'],
    u'\ub0e7' : ['_n _j a tSh', '냧'],
    u'\ub0e8' : ['_n _j a k_h', '냨'],
    u'\ub0e9' : ['_n _j a t_h', '냩'],
    u'\ub0ea' : ['_n _j a p_h', '냪'],
    u'\ub0eb' : ['_n _j a _h', '냫'],
    u'\ub0ec' : ['_n _j {', '냬'],
    u'\ub0ed' : ['_n _j { k', '냭'],
    u'\ub0ee' : ['_n _j { k_>', '냮'],
    u'\ub0ef' : ['_n _j { k sh', '냯'],
    u'\ub0f0' : ['_n _j { _n', '냰'],
    u'\ub0f1' : ['_n _j { _n tS', '냱'],
    u'\ub0f2' : ['_n _j { _n _h', '냲'],
    u'\ub0f3' : ['_n _j { t', '냳'],
    u'\ub0f4' : ['_n _j { _l', '냴'],
    u'\ub0f5' : ['_n _j { _l k', '냵'],
    u'\ub0f6' : ['_n _j { _l m', '냶'],
    u'\ub0f7' : ['_n _j { _l p', '냷'],
    u'\ub0f8' : ['_n _j { _l sh', '냸'],
    u'\ub0f9' : ['_n _j { _l t_h', '냹'],
    u'\ub0fa' : ['_n _j { _l p_h', '냺'],
    u'\ub0fb' : ['_n _j { _l _h', '냻'],
    u'\ub0fc' : ['_n _j { m', '냼'],
    u'\ub0fd' : ['_n _j { p', '냽'],
    u'\ub0fe' : ['_n _j { p sh', '냾'],
    u'\ub0ff' : ['_n _j { sh', '냿'],
    u'\ub100' : ['_n _j { s', '넀'],
    u'\ub101' : ['_n _j { N', '넁'],
    u'\ub102' : ['_n _j { tS', '넂'],
    u'\ub103' : ['_n _j { tSh', '넃'],
    u'\ub104' : ['_n _j { k_h', '넄'],
    u'\ub105' : ['_n _j { t_h', '넅'],
    u'\ub106' : ['_n _j { p_h', '넆'],
    u'\ub107' : ['_n _j { _h', '넇'],
    u'\ub108' : ['_n _r', '너'],
    u'\ub109' : ['_n _r k', '넉'],
    u'\ub10a' : ['_n _r k_>', '넊'],
    u'\ub10b' : ['_n _r k sh', '넋'],
    u'\ub10c' : ['_n _r _n', '넌'],
    u'\ub10d' : ['_n _r _n tS', '넍'],
    u'\ub10e' : ['_n _r _n _h', '넎'],
    u'\ub10f' : ['_n _r t', '넏'],
    u'\ub110' : ['_n _r _l', '널'],
    u'\ub111' : ['_n _r _l k', '넑'],
    u'\ub112' : ['_n _r _l m', '넒'],
    u'\ub113' : ['_n _r _l p', '넓'],
    u'\ub114' : ['_n _r _l sh', '넔'],
    u'\ub115' : ['_n _r _l t_h', '넕'],
    u'\ub116' : ['_n _r _l p_h', '넖'],
    u'\ub117' : ['_n _r _l _h', '넗'],
    u'\ub118' : ['_n _r m', '넘'],
    u'\ub119' : ['_n _r p', '넙'],
    u'\ub11a' : ['_n _r p sh', '넚'],
    u'\ub11b' : ['_n _r sh', '넛'],
    u'\ub11c' : ['_n _r s', '넜'],
    u'\ub11d' : ['_n _r N', '넝'],
    u'\ub11e' : ['_n _r tS', '넞'],
    u'\ub11f' : ['_n _r tSh', '넟'],
    u'\ub120' : ['_n _r k_h', '넠'],
    u'\ub121' : ['_n _r t_h', '넡'],
    u'\ub122' : ['_n _r p_h', '넢'],
    u'\ub123' : ['_n _r _h', '넣'],
    u'\ub124' : ['_n e', '네'],
    u'\ub125' : ['_n e k', '넥'],
    u'\ub126' : ['_n e k_>', '넦'],
    u'\ub127' : ['_n e k sh', '넧'],
    u'\ub128' : ['_n e _n', '넨'],
    u'\ub129' : ['_n e _n tS', '넩'],
    u'\ub12a' : ['_n e _n _h', '넪'],
    u'\ub12b' : ['_n e t', '넫'],
    u'\ub12c' : ['_n e _l', '넬'],
    u'\ub12d' : ['_n e _l k', '넭'],
    u'\ub12e' : ['_n e _l m', '넮'],
    u'\ub12f' : ['_n e _l p', '넯'],
    u'\ub130' : ['_n e _l sh', '넰'],
    u'\ub131' : ['_n e _l t_h', '넱'],
    u'\ub132' : ['_n e _l p_h', '넲'],
    u'\ub133' : ['_n e _l _h', '넳'],
    u'\ub134' : ['_n e m', '넴'],
    u'\ub135' : ['_n e p', '넵'],
    u'\ub136' : ['_n e p sh', '넶'],
    u'\ub137' : ['_n e sh', '넷'],
    u'\ub138' : ['_n e s', '넸'],
    u'\ub139' : ['_n e N', '넹'],
    u'\ub13a' : ['_n e tS', '넺'],
    u'\ub13b' : ['_n e tSh', '넻'],
    u'\ub13c' : ['_n e k_h', '넼'],
    u'\ub13d' : ['_n e t_h', '넽'],
    u'\ub13e' : ['_n e p_h', '넾'],
    u'\ub13f' : ['_n e _h', '넿'],
    u'\ub140' : ['_n _j _r', '녀'],
    u'\ub141' : ['_n _j _r k', '녁'],
    u'\ub142' : ['_n _j _r k_>', '녂'],
    u'\ub143' : ['_n _j _r k sh', '녃'],
    u'\ub144' : ['_n _j _r _n', '년'],
    u'\ub145' : ['_n _j _r _n tS', '녅'],
    u'\ub146' : ['_n _j _r _n _h', '녆'],
    u'\ub147' : ['_n _j _r t', '녇'],
    u'\ub148' : ['_n _j _r _l', '녈'],
    u'\ub149' : ['_n _j _r _l k', '녉'],
    u'\ub14a' : ['_n _j _r _l m', '녊'],
    u'\ub14b' : ['_n _j _r _l p', '녋'],
    u'\ub14c' : ['_n _j _r _l sh', '녌'],
    u'\ub14d' : ['_n _j _r _l t_h', '녍'],
    u'\ub14e' : ['_n _j _r _l p_h', '녎'],
    u'\ub14f' : ['_n _j _r _l _h', '녏'],
    u'\ub150' : ['_n _j _r m', '념'],
    u'\ub151' : ['_n _j _r p', '녑'],
    u'\ub152' : ['_n _j _r p sh', '녒'],
    u'\ub153' : ['_n _j _r sh', '녓'],
    u'\ub154' : ['_n _j _r s', '녔'],
    u'\ub155' : ['_n _j _r N', '녕'],
    u'\ub156' : ['_n _j _r tS', '녖'],
    u'\ub157' : ['_n _j _r tSh', '녗'],
    u'\ub158' : ['_n _j _r k_h', '녘'],
    u'\ub159' : ['_n _j _r t_h', '녙'],
    u'\ub15a' : ['_n _j _r p_h', '녚'],
    u'\ub15b' : ['_n _j _r _h', '녛'],
    u'\ub15c' : ['_n _j e', '녜'],
    u'\ub15d' : ['_n _j e k', '녝'],
    u'\ub15e' : ['_n _j e k_>', '녞'],
    u'\ub15f' : ['_n _j e k sh', '녟'],
    u'\ub160' : ['_n _j e _n', '녠'],
    u'\ub161' : ['_n _j e _n tS', '녡'],
    u'\ub162' : ['_n _j e _n _h', '녢'],
    u'\ub163' : ['_n _j e t', '녣'],
    u'\ub164' : ['_n _j e _l', '녤'],
    u'\ub165' : ['_n _j e _l k', '녥'],
    u'\ub166' : ['_n _j e _l m', '녦'],
    u'\ub167' : ['_n _j e _l p', '녧'],
    u'\ub168' : ['_n _j e _l sh', '녨'],
    u'\ub169' : ['_n _j e _l t_h', '녩'],
    u'\ub16a' : ['_n _j e _l p_h', '녪'],
    u'\ub16b' : ['_n _j e _l _h', '녫'],
    u'\ub16c' : ['_n _j e m', '녬'],
    u'\ub16d' : ['_n _j e p', '녭'],
    u'\ub16e' : ['_n _j e p sh', '녮'],
    u'\ub16f' : ['_n _j e sh', '녯'],
    u'\ub170' : ['_n _j e s', '녰'],
    u'\ub171' : ['_n _j e N', '녱'],
    u'\ub172' : ['_n _j e tS', '녲'],
    u'\ub173' : ['_n _j e tSh', '녳'],
    u'\ub174' : ['_n _j e k_h', '녴'],
    u'\ub175' : ['_n _j e t_h', '녵'],
    u'\ub176' : ['_n _j e p_h', '녶'],
    u'\ub177' : ['_n _j e _h', '녷'],
    u'\ub178' : ['_n o', '노'],
    u'\ub179' : ['_n o k', '녹'],
    u'\ub17a' : ['_n o k_>', '녺'],
    u'\ub17b' : ['_n o k sh', '녻'],
    u'\ub17c' : ['_n o _n', '논'],
    u'\ub17d' : ['_n o _n tS', '녽'],
    u'\ub17e' : ['_n o _n _h', '녾'],
    u'\ub17f' : ['_n o t', '녿'],
    u'\ub180' : ['_n o _l', '놀'],
    u'\ub181' : ['_n o _l k', '놁'],
    u'\ub182' : ['_n o _l m', '놂'],
    u'\ub183' : ['_n o _l p', '놃'],
    u'\ub184' : ['_n o _l sh', '놄'],
    u'\ub185' : ['_n o _l t_h', '놅'],
    u'\ub186' : ['_n o _l p_h', '놆'],
    u'\ub187' : ['_n o _l _h', '놇'],
    u'\ub188' : ['_n o m', '놈'],
    u'\ub189' : ['_n o p', '놉'],
    u'\ub18a' : ['_n o p sh', '놊'],
    u'\ub18b' : ['_n o sh', '놋'],
    u'\ub18c' : ['_n o s', '놌'],
    u'\ub18d' : ['_n o N', '농'],
    u'\ub18e' : ['_n o tS', '놎'],
    u'\ub18f' : ['_n o tSh', '놏'],
    u'\ub190' : ['_n o k_h', '놐'],
    u'\ub191' : ['_n o t_h', '놑'],
    u'\ub192' : ['_n o p_h', '높'],
    u'\ub193' : ['_n o _h', '놓'],
    u'\ub194' : ['_n _w a', '놔'],
    u'\ub195' : ['_n _w a k', '놕'],
    u'\ub196' : ['_n _w a k_>', '놖'],
    u'\ub197' : ['_n _w a k sh', '놗'],
    u'\ub198' : ['_n _w a _n', '놘'],
    u'\ub199' : ['_n _w a _n tS', '놙'],
    u'\ub19a' : ['_n _w a _n _h', '놚'],
    u'\ub19b' : ['_n _w a t', '놛'],
    u'\ub19c' : ['_n _w a _l', '놜'],
    u'\ub19d' : ['_n _w a _l k', '놝'],
    u'\ub19e' : ['_n _w a _l m', '놞'],
    u'\ub19f' : ['_n _w a _l p', '놟'],
    u'\ub1a0' : ['_n _w a _l sh', '놠'],
    u'\ub1a1' : ['_n _w a _l t_h', '놡'],
    u'\ub1a2' : ['_n _w a _l p_h', '놢'],
    u'\ub1a3' : ['_n _w a _l _h', '놣'],
    u'\ub1a4' : ['_n _w a m', '놤'],
    u'\ub1a5' : ['_n _w a p', '놥'],
    u'\ub1a6' : ['_n _w a p sh', '놦'],
    u'\ub1a7' : ['_n _w a sh', '놧'],
    u'\ub1a8' : ['_n _w a s', '놨'],
    u'\ub1a9' : ['_n _w a N', '놩'],
    u'\ub1aa' : ['_n _w a tS', '놪'],
    u'\ub1ab' : ['_n _w a tSh', '놫'],
    u'\ub1ac' : ['_n _w a k_h', '놬'],
    u'\ub1ad' : ['_n _w a t_h', '놭'],
    u'\ub1ae' : ['_n _w a p_h', '놮'],
    u'\ub1af' : ['_n _w a _h', '놯'],
    u'\ub1b0' : ['_n _w {', '놰'],
    u'\ub1b1' : ['_n _w { k', '놱'],
    u'\ub1b2' : ['_n _w { k_>', '놲'],
    u'\ub1b3' : ['_n _w { k sh', '놳'],
    u'\ub1b4' : ['_n _w { _n', '놴'],
    u'\ub1b5' : ['_n _w { _n tS', '놵'],
    u'\ub1b6' : ['_n _w { _n _h', '놶'],
    u'\ub1b7' : ['_n _w { t', '놷'],
    u'\ub1b8' : ['_n _w { _l', '놸'],
    u'\ub1b9' : ['_n _w { _l k', '놹'],
    u'\ub1ba' : ['_n _w { _l m', '놺'],
    u'\ub1bb' : ['_n _w { _l p', '놻'],
    u'\ub1bc' : ['_n _w { _l sh', '놼'],
    u'\ub1bd' : ['_n _w { _l t_h', '놽'],
    u'\ub1be' : ['_n _w { _l p_h', '놾'],
    u'\ub1bf' : ['_n _w { _l _h', '놿'],
    u'\ub1c0' : ['_n _w { m', '뇀'],
    u'\ub1c1' : ['_n _w { p', '뇁'],
    u'\ub1c2' : ['_n _w { p sh', '뇂'],
    u'\ub1c3' : ['_n _w { sh', '뇃'],
    u'\ub1c4' : ['_n _w { s', '뇄'],
    u'\ub1c5' : ['_n _w { N', '뇅'],
    u'\ub1c6' : ['_n _w { tS', '뇆'],
    u'\ub1c7' : ['_n _w { tSh', '뇇'],
    u'\ub1c8' : ['_n _w { k_h', '뇈'],
    u'\ub1c9' : ['_n _w { t_h', '뇉'],
    u'\ub1ca' : ['_n _w { p_h', '뇊'],
    u'\ub1cb' : ['_n _w { _h', '뇋'],
    u'\ub1cc' : ['_n _w e', '뇌'],
    u'\ub1cd' : ['_n _w e k', '뇍'],
    u'\ub1ce' : ['_n _w e k_>', '뇎'],
    u'\ub1cf' : ['_n _w e k sh', '뇏'],
    u'\ub1d0 ' : ['_n _w e _n', '뇐 '],
    u'\ub1d1' : ['_n _w e _n tS', '뇑'],
    u'\ub1d2' : ['_n _w e _n _h', '뇒'],
    u'\ub1d3' : ['_n _w e t', '뇓'],
    u'\ub1d4' : ['_n _w e _l', '뇔'],
    u'\ub1d5' : ['_n _w e _l k', '뇕'],
    u'\ub1d6' : ['_n _w e _l m', '뇖'],
    u'\ub1d7' : ['_n _w e _l p', '뇗'],
    u'\ub1d8' : ['_n _w e _l sh', '뇘'],
    u'\ub1d9' : ['_n _w e _l t_h', '뇙'],
    u'\ub1da' : ['_n _w e _l p_h', '뇚'],
    u'\ub1db' : ['_n _w e _l _h', '뇛'],
    u'\ub1dc' : ['_n _w e m', '뇜'],
    u'\ub1dd' : ['_n _w e p', '뇝'],
    u'\ub1de' : ['_n _w e p sh', '뇞'],
    u'\ub1df' : ['_n _w e sh', '뇟'],
    u'\ub1e0' : ['_n _w e s', '뇠'],
    u'\ub1e1' : ['_n _w e N', '뇡'],
    u'\ub1e2' : ['_n _w e tS', '뇢'],
    u'\ub1e3' : ['_n _w e tSh', '뇣'],
    u'\ub1e4' : ['_n _w e k_h', '뇤'],
    u'\ub1e5' : ['_n _w e t_h', '뇥'],
    u'\ub1e6' : ['_n _w e p_h', '뇦'],
    u'\ub1e7' : ['_n _w e _h', '뇧'],
    u'\ub1e8' : ['_n _j o', '뇨'],
    u'\ub1e9' : ['_n _j o k', '뇩'],
    u'\ub1ea' : ['_n _j o k_>', '뇪'],
    u'\ub1eb' : ['_n _j o k sh', '뇫'],
    u'\ub1ec' : ['_n _j o _n', '뇬'],
    u'\ub1ed' : ['_n _j o _n tS', '뇭'],
    u'\ub1ee' : ['_n _j o _n _h', '뇮'],
    u'\ub1ef' : ['_n _j o t', '뇯'],
    u'\ub1f0' : ['_n _j o _l', '뇰'],
    u'\ub1f1' : ['_n _j o _l k', '뇱'],
    u'\ub1f2' : ['_n _j o _l m', '뇲'],
    u'\ub1f3' : ['_n _j o _l p', '뇳'],
    u'\ub1f4' : ['_n _j o _l sh', '뇴'],
    u'\ub1f5' : ['_n _j o _l t_h', '뇵'],
    u'\ub1f6' : ['_n _j o _l p_h', '뇶'],
    u'\ub1f7' : ['_n _j o _l _h', '뇷'],
    u'\ub1f8' : ['_n _j o m', '뇸'],
    u'\ub1f9' : ['_n _j o p', '뇹'],
    u'\ub1fa' : ['_n _j o p sh', '뇺'],
    u'\ub1fb' : ['_n _j o sh', '뇻'],
    u'\ub1fc' : ['_n _j o s', '뇼'],
    u'\ub1fd' : ['_n _j o N', '뇽'],
    u'\ub1fe' : ['_n _j o tS', '뇾'],
    u'\ub1ff' : ['_n _j o tSh', '뇿'],
    u'\ub200' : ['_n _j o k_h', '눀'],
    u'\ub201' : ['_n _j o t_h', '눁'],
    u'\ub202' : ['_n _j o p_h', '눂'],
    u'\ub203' : ['_n _j o _h', '눃'],
    u'\ub204' : ['_n u', '누'],
    u'\ub205' : ['_n u k', '눅'],
    u'\ub206' : ['_n u k_>', '눆'],
    u'\ub207' : ['_n u k sh', '눇'],
    u'\ub208' : ['_n u _n', '눈'],
    u'\ub209' : ['_n u _n tS', '눉'],
    u'\ub20a' : ['_n u _n _h', '눊'],
    u'\ub20b' : ['_n u t', '눋'],
    u'\ub20c' : ['_n u _l', '눌'],
    u'\ub20d' : ['_n u _l k', '눍'],
    u'\ub20e' : ['_n u _l m', '눎'],
    u'\ub20f' : ['_n u _l p', '눏'],
    u'\ub210' : ['_n u _l sh', '눐'],
    u'\ub211' : ['_n u _l t_h', '눑'],
    u'\ub212' : ['_n u _l p_h', '눒'],
    u'\ub213' : ['_n u _l _h', '눓'],
    u'\ub214' : ['_n u m', '눔'],
    u'\ub215' : ['_n u p', '눕'],
    u'\ub216' : ['_n u p sh', '눖'],
    u'\ub217' : ['_n u sh', '눗'],
    u'\ub218' : ['_n u s', '눘'],
    u'\ub219' : ['_n u N', '눙'],
    u'\ub21a' : ['_n u tS', '눚'],
    u'\ub21b' : ['_n u tSh', '눛'],
    u'\ub21c' : ['_n u k_h', '눜'],
    u'\ub21d' : ['_n u t_h', '눝'],
    u'\ub21e' : ['_n u p_h', '눞'],
    u'\ub21f' : ['_n u _h', '눟'],
    u'\ub220' : ['_n _w _r', '눠'],
    u'\ub221' : ['_n _w _r k', '눡'],
    u'\ub222' : ['_n _w _r k_>', '눢'],
    u'\ub223' : ['_n _w _r k sh', '눣'],
    u'\ub224' : ['_n _w _r _n', '눤'],
    u'\ub225' : ['_n _w _r _n tS', '눥'],
    u'\ub226' : ['_n _w _r _n _h', '눦'],
    u'\ub227' : ['_n _w _r t', '눧'],
    u'\ub228' : ['_n _w _r _l', '눨'],
    u'\ub229' : ['_n _w _r _l k', '눩'],
    u'\ub22a' : ['_n _w _r _l m', '눪'],
    u'\ub22b' : ['_n _w _r _l p', '눫'],
    u'\ub22c' : ['_n _w _r _l sh', '눬'],
    u'\ub22d' : ['_n _w _r _l t_h', '눭'],
    u'\ub22e' : ['_n _w _r _l p_h', '눮'],
    u'\ub22f' : ['_n _w _r _l _h', '눯'],
    u'\ub230' : ['_n _w _r m', '눰'],
    u'\ub231' : ['_n _w _r p', '눱'],
    u'\ub232' : ['_n _w _r p sh', '눲'],
    u'\ub233' : ['_n _w _r sh', '눳'],
    u'\ub234' : ['_n _w _r s', '눴'],
    u'\ub235' : ['_n _w _r N', '눵'],
    u'\ub236' : ['_n _w _r tS', '눶'],
    u'\ub237' : ['_n _w _r tSh', '눷'],
    u'\ub238' : ['_n _w _r k_h', '눸'],
    u'\ub239' : ['_n _w _r t_h', '눹'],
    u'\ub23a' : ['_n _w _r p_h', '눺'],
    u'\ub23b' : ['_n _w _r _h', '눻'],
    u'\ub23c' : ['_n _w E', '눼'],
    u'\ub23d' : ['_n _w E k', '눽'],
    u'\ub23e' : ['_n _w E k_>', '눾'],
    u'\ub23f' : ['_n _w E k sh', '눿'],
    u'\ub240' : ['_n _w E _n', '뉀'],
    u'\ub241' : ['_n _w E _n tS', '뉁'],
    u'\ub242' : ['_n _w E _n _h', '뉂'],
    u'\ub243' : ['_n _w E t', '뉃'],
    u'\ub244' : ['_n _w E _l', '뉄'],
    u'\ub245' : ['_n _w E _l k', '뉅'],
    u'\ub246' : ['_n _w E _l m', '뉆'],
    u'\ub247' : ['_n _w E _l p', '뉇'],
    u'\ub248' : ['_n _w E _l sh', '뉈'],
    u'\ub249' : ['_n _w E _l t_h', '뉉'],
    u'\ub24a' : ['_n _w E _l p_h', '뉊'],
    u'\ub24b' : ['_n _w E _l _h', '뉋'],
    u'\ub24c' : ['_n _w E m', '뉌'],
    u'\ub24d' : ['_n _w E p', '뉍'],
    u'\ub24e' : ['_n _w E p sh', '뉎'],
    u'\ub24f' : ['_n _w E sh', '뉏'],
    u'\ub250' : ['_n _w E s', '뉐'],
    u'\ub251' : ['_n _w E N', '뉑'],
    u'\ub252' : ['_n _w E tS', '뉒'],
    u'\ub253' : ['_n _w E tSh', '뉓'],
    u'\ub254' : ['_n _w E k_h', '뉔'],
    u'\ub255' : ['_n _w E t_h', '뉕'],
    u'\ub256' : ['_n _w E p_h', '뉖'],
    u'\ub257' : ['_n _w E _h', '뉗'],
    u'\ub258' : ['_n 2', '뉘'],
    u'\ub259' : ['_n 2 k', '뉙'],
    u'\ub25a' : ['_n 2 k_>', '뉚'],
    u'\ub25b' : ['_n 2 k sh', '뉛'],
    u'\ub25c' : ['_n 2 _n', '뉜'],
    u'\ub25d' : ['_n 2 _n tS', '뉝'],
    u'\ub25e' : ['_n 2 _n _h', '뉞'],
    u'\ub25f' : ['_n 2 t', '뉟'],
    u'\ub260' : ['_n 2 _l', '뉠'],
    u'\ub261' : ['_n 2 _l k', '뉡'],
    u'\ub262' : ['_n 2 _l m', '뉢'],
    u'\ub263' : ['_n 2 _l p', '뉣'],
    u'\ub264' : ['_n 2 _l sh', '뉤'],
    u'\ub265' : ['_n 2 _l t_h', '뉥'],
    u'\ub266' : ['_n 2 _l p_h', '뉦'],
    u'\ub267' : ['_n 2 _l _h', '뉧'],
    u'\ub268' : ['_n 2 m', '뉨'],
    u'\ub269' : ['_n 2 p', '뉩'],
    u'\ub26a' : ['_n 2 p sh', '뉪'],
    u'\ub26b' : ['_n 2 sh', '뉫'],
    u'\ub26c' : ['_n 2 s', '뉬'],
    u'\ub26d' : ['_n 2 N', '뉭'],
    u'\ub26e' : ['_n 2 tS', '뉮'],
    u'\ub26f' : ['_n 2 tSh', '뉯'],
    u'\ub270' : ['_n 2 k_h', '뉰'],
    u'\ub271' : ['_n 2 t_h', '뉱'],
    u'\ub272' : ['_n 2 p_h', '뉲'],
    u'\ub273' : ['_n 2 _h', '뉳'],
    u'\ub274' : ['_n _j u', '뉴'],
    u'\ub275' : ['_n _j u k', '뉵'],
    u'\ub276' : ['_n _j u k_>', '뉶'],
    u'\ub277' : ['_n _j u k sh', '뉷'],
    u'\ub278' : ['_n _j u _n', '뉸'],
    u'\ub279' : ['_n _j u _n tS', '뉹'],
    u'\ub27a' : ['_n _j u _n _h', '뉺'],
    u'\ub27b' : ['_n _j u t', '뉻'],
    u'\ub27c' : ['_n _j u _l', '뉼'],
    u'\ub27d' : ['_n _j u _l k', '뉽'],
    u'\ub27e' : ['_n _j u _l m', '뉾'],
    u'\ub27f' : ['_n _j u _l p', '뉿'],
    u'\ub280' : ['_n _j u _l sh', '늀'],
    u'\ub281' : ['_n _j u _l t_h', '늁'],
    u'\ub282' : ['_n _j u _l p_h', '늂'],
    u'\ub283' : ['_n _j u _l _h', '늃'],
    u'\ub284' : ['_n _j u m', '늄'],
    u'\ub285' : ['_n _j u p', '늅'],
    u'\ub286' : ['_n _j u p sh', '늆'],
    u'\ub287' : ['_n _j u sh', '늇'],
    u'\ub288' : ['_n _j u s', '늈'],
    u'\ub289' : ['_n _j u N', '늉'],
    u'\ub28a' : ['_n _j u tS', '늊'],
    u'\ub28b' : ['_n _j u tSh', '늋'],
    u'\ub28c' : ['_n _j u k_h', '늌'],
    u'\ub28d' : ['_n _j u t_h', '늍'],
    u'\ub28e' : ['_n _j u p_h', '늎'],
    u'\ub28f' : ['_n _j u _h', '늏'],
    u'\ub290' : ['_n M', '느'],
    u'\ub291' : ['_n M k', '늑'],
    u'\ub292' : ['_n M k_>', '늒'],
    u'\ub293' : ['_n M k sh', '늓'],
    u'\ub294' : ['_n M _n', '는'],
    u'\ub295' : ['_n M _n tS', '늕'],
    u'\ub296' : ['_n M nh', '늖'],
    u'\ub297' : ['_n M t', '늗'],
    u'\ub298' : ['_n M _l', '늘'],
    u'\ub299' : ['_n M _l k', '늙'],
    u'\ub29a' : ['_n M _l m', '늚'],
    u'\ub29b' : ['_n M _l p', '늛'],
    u'\ub29c' : ['_n M _l sh', '늜'],
    u'\ub29d' : ['_n M _l t_h', '늝'],
    u'\ub29e' : ['_n M _l p_h', '늞'],
    u'\ub29f' : ['_n M _l _h', '늟'],
    u'\ub2a0' : ['_n M m', '늠'],
    u'\ub2a1' : ['_n M p', '늡'],
    u'\ub2a2' : ['_n M p sh', '늢'],
    u'\ub2a3' : ['_n M sh', '늣'],
    u'\ub2a4' : ['_n M s', '늤'],
    u'\ub2a5' : ['_n M N', '능'],
    u'\ub2a6' : ['_n M tS', '늦'],
    u'\ub2a7' : ['_n M tSh', '늧'],
    u'\ub2a8' : ['_n M k_h', '늨'],
    u'\ub2a9' : ['_n M t_h', '늩'],
    u'\ub2aa' : ['_n M p_h', '늪'],
    u'\ub2ab' : ['_n M _h', '늫'],
    u'\ub2ac' : ['_n M _j', '늬'],
    u'\ub2ad' : ['_n M _j k', '늭'],
    u'\ub2ae' : ['_n M _j k_>', '늮'],
    u'\ub2af' : ['_n M _j k sh', '늯'],
    u'\ub2b0' : ['_n M _j _n', '늰'],
    u'\ub2b1' : ['_n M _j _n tS', '늱'],
    u'\ub2b2' : ['_n M _j _n _h', '늲'],
    u'\ub2b3' : ['_n M _j t', '늳'],
    u'\ub2b4' : ['_n M _j _l', '늴'],
    u'\ub2b5' : ['_n M _j _l k', '늵'],
    u'\ub2b6' : ['_n M _j _l m', '늶'],
    u'\ub2b7' : ['_n M _j _l p', '늷'],
    u'\ub2b8' : ['_n M _j _l sh', '늸'],
    u'\ub2b9' : ['_n M _j _l t_h', '늹'],
    u'\ub2ba' : ['_n M _j _l p_h', '늺'],
    u'\ub2bb' : ['_n M _j _l _h', '늻'],
    u'\ub2bc' : ['_n M _j m', '늼'],
    u'\ub2bd' : ['_n M _j p', '늽'],
    u'\ub2be' : ['_n M _j p sh', '늾'],
    u'\ub2bf' : ['_n M _j sh', '늿'],
    u'\ub2c0' : ['_n M _j s', '닀'],
    u'\ub2c1' : ['_n M _j N', '닁'],
    u'\ub2c2' : ['_n M _j tS', '닂'],
    u'\ub2c3' : ['_n M _j tSh', '닃'],
    u'\ub2c4' : ['_n M _j k_h', '닄'],
    u'\ub2c5' : ['_n M _j t_h', '닅'],
    u'\ub2c6' : ['_n M _j p_h', '닆'],
    u'\ub2c7' : ['_n M _j _h', '닇'],
    u'\ub2c8' : ['_n i', '니'],
    u'\ub2c9' : ['_n i k', '닉'],
    u'\ub2ca' : ['_n i k_>', '닊'],
    u'\ub2cb' : ['_n i k sh', '닋'],
    u'\ub2cc' : ['_n i _n', '닌'],
    u'\ub2cd' : ['_n i _n tS', '닍'],
    u'\ub2ce' : ['_n i _n _h', '닎'],
    u'\ub2cf' : ['_n i t', '닏'],
    u'\ub2d0' : ['_n i _l', '닐'],
    u'\ub2d1' : ['_n i _l k', '닑'],
    u'\ub2d2' : ['_n i _l m', '닒'],
    u'\ub2d3' : ['_n i _l p', '닓'],
    u'\ub2d4' : ['_n i _l sh', '닔'],
    u'\ub2d5' : ['_n i _l t_h', '닕'],
    u'\ub2d6' : ['_n i _l p_h', '닖'],
    u'\ub2d7' : ['_n i _l _h', '닗'],
    u'\ub2d8' : ['_n i m', '님'],
    u'\ub2d9' : ['_n i p', '닙'],
    u'\ub2da' : ['_n i p sh', '닚'],
    u'\ub2db' : ['_n i sh', '닛'],
    u'\ub2dc' : ['_n i s', '닜'],
    u'\ub2dd' : ['_n i N', '닝'],
    u'\ub2de' : ['_n i tS', '닞'],
    u'\ub2df' : ['_n i tSh', '닟'],
    u'\ub2e0' : ['_n i k_h', '닠'],
    u'\ub2e1' : ['_n i t_h', '닡'],
    u'\ub2e2' : ['_n i p_h', '닢'],
    u'\ub2e3' : ['_n i _h', '닣'],
    u'\ub2e4' : ['t a', '다'],
    u'\ub2e5' : ['t a k', '닥'],
    u'\ub2e6' : ['t a k_>', '닦'],
    u'\ub2e7' : ['t a k sh', '닧'],
    u'\ub2e8' : ['t a _n', '단'],
    u'\ub2e9' : ['t a _n tS', '닩'],
    u'\ub2ea' : ['t a _n _h', '닪'],
    u'\ub2eb' : ['t a t', '닫'],
    u'\ub2ec' : ['t a _l', '달'],
    u'\ub2ed' : ['t a _l k', '닭'],
    u'\ub2ee' : ['t a _l m', '닮'],
    u'\ub2ef' : ['t a _l b', '닯'],
    u'\ub2f0' : ['t a _l sh', '닰'],
    u'\ub2f1' : ['t a _l t_h', '닱'],
    u'\ub2f2' : ['t a _l p_h', '닲'],
    u'\ub2f3' : ['t a _l _h', '닳'],
    u'\ub2f4' : ['t a m', '담'],
    u'\ub2f5' : ['t a p', '답'],
    u'\ub2f6' : ['t a p sh', '닶'],
    u'\ub2f7' : ['t a sh', '닷'],
    u'\ub2f8' : ['t a s', '닸'],
    u'\ub2f9' : ['t a N', '당'],
    u'\ub2fa' : ['t a tS', '닺'],
    u'\ub2fb' : ['t a tSh', '닻'],
    u'\ub2fc' : ['t a k_h', '닼'],
    u'\ub2fd' : ['t a t_h', '닽'],
    u'\ub2fe' : ['t a p_h', '닾'],
    u'\ub2ff' : ['t a _h', '닿'],
    u'\ub300' : ['t {', '대'],
    u'\ub301' : ['t { k', '댁'],
    u'\ub302' : ['t { k_>', '댂'],
    u'\ub303' : ['t { k sh', '댃'],
    u'\ub304' : ['t { _n', '댄'],
    u'\ub305' : ['t { _n tS', '댅'],
    u'\ub306' : ['t { _n _h', '댆'],
    u'\ub307' : ['t { t', '댇'],
    u'\ub308' : ['t { _l', '댈'],
    u'\ub309' : ['t { _l k', '댉'],
    u'\ub30a' : ['t { _l m', '댊'],
    u'\ub30b' : ['t { _l p', '댋'],
    u'\ub30c' : ['t { _l sh', '댌'],
    u'\ub30d' : ['t { _l t_h', '댍'],
    u'\ub30e' : ['t { _l p_h', '댎'],
    u'\ub30f' : ['t { _l _h', '댏'],
    u'\ub310' : ['t { m', '댐'],
    u'\ub311' : ['t { p', '댑'],
    u'\ub312' : ['t { p sh', '댒'],
    u'\ub313' : ['t { sh', '댓'],
    u'\ub314' : ['t { s', '댔'],
    u'\ub315' : ['t { N', '댕'],
    u'\ub316' : ['t { tS', '댖'],
    u'\ub317' : ['t { tSh', '댗'],
    u'\ub318' : ['t { k_h', '댘'],
    u'\ub319' : ['t { t_h', '댙'],
    u'\ub31a' : ['t { p_h', '댚'],
    u'\ub31b' : ['t { _h', '댛'],
    u'\ub31c' : ['t _j a', '댜'],
    u'\ub31d' : ['t _j a k', '댝'],
    u'\ub31e' : ['t _j a k_>', '댞'],
    u'\ub31f' : ['t _j a k sh', '댟'],
    u'\ub320' : ['t _j a _n', '댠'],
    u'\ub321' : ['t _j a _n tS', '댡'],
    u'\ub322' : ['t _j a _n _h', '댢'],
    u'\ub323' : ['t _j a t', '댣'],
    u'\ub324' : ['t _j a _l', '댤'],
    u'\ub325' : ['t _j a _l k', '댥'],
    u'\ub326' : ['t _j a _l m', '댦'],
    u'\ub327' : ['t _j a _l p', '댧'],
    u'\ub328' : ['t _j a _l sh', '댨'],
    u'\ub329' : ['t _j a _l t_h', '댩'],
    u'\ub32a' : ['t _j a _l p_h', '댪'],
    u'\ub32b' : ['t _j a _l _h', '댫'],
    u'\ub32c' : ['t _j a m', '댬'],
    u'\ub32d' : ['t _j a p', '댭'],
    u'\ub32e' : ['t _j a p sh', '댮'],
    u'\ub32f' : ['t _j a sh', '댯'],
    u'\ub330' : ['t _j a s', '댰'],
    u'\ub331' : ['t _j a N', '댱'],
    u'\ub332' : ['t _j a tS', '댲'],
    u'\ub333' : ['t _j a tSh', '댳'],
    u'\ub334' : ['t _j a k_h', '댴'],
    u'\ub335' : ['t _j a t_h', '댵'],
    u'\ub336' : ['t _j a p_h', '댶'],
    u'\ub337' : ['t _j a _h', '댷'],
    u'\ub338' : ['t _j {', '댸'],
    u'\ub339' : ['t _j { k', '댹'],
    u'\ub33a' : ['t _j { k_>', '댺'],
    u'\ub33b' : ['t _j { k sh', '댻'],
    u'\ub33c' : ['t _j { _n', '댼'],
    u'\ub33d' : ['t _j { _n tS', '댽'],
    u'\ub33e' : ['t _j { _n _h', '댾'],
    u'\ub33f' : ['t _j { t', '댿'],
    u'\ub340' : ['t _j { _l', '덀'],
    u'\ub341' : ['t _j { _l k', '덁'],
    u'\ub342' : ['t _j { _l m', '덂'],
    u'\ub343' : ['t _j { _l p', '덃'],
    u'\ub344' : ['t _j { _l sh', '덄'],
    u'\ub345' : ['t _j { _l t_h', '덅'],
    u'\ub346' : ['t _j { _l p_h', '덆'],
    u'\ub347' : ['t _j { _l _h', '덇'],
    u'\ub348' : ['t _j { m', '덈'],
    u'\ub349' : ['t _j { p', '덉'],
    u'\ub34a' : ['t _j { p sh', '덊'],
    u'\ub34b' : ['t _j { sh', '덋'],
    u'\ub34c' : ['t _j { s', '덌'],
    u'\ub34d' : ['t _j { N', '덍'],
    u'\ub34e' : ['t _j { tS', '덎'],
    u'\ub34f' : ['t _j { tSh', '덏'],
    u'\ub350' : ['t _j { k_h', '덐'],
    u'\ub351' : ['t _j { t_h', '덑'],
    u'\ub352' : ['t _j { p_h', '덒'],
    u'\ub353' : ['t _j { _h', '덓'],
    u'\ub354' : ['t _r', '더'],
    u'\ub355' : ['t _r k', '덕'],
    u'\ub356' : ['t _r k_>', '덖'],
    u'\ub357' : ['t _r k sh', '덗'],
    u'\ub358' : ['t _r _n', '던'],
    u'\ub359' : ['t _r _n tS', '덙'],
    u'\ub35a' : ['t _r _n _h', '덚'],
    u'\ub35b' : ['t _r t', '덛'],
    u'\ub35c' : ['t _r _l', '덜'],
    u'\ub35d' : ['t _r _l k', '덝'],
    u'\ub35e' : ['t _r _l m', '덞'],
    u'\ub35f' : ['t _r _l p', '덟'],
    u'\ub360' : ['t _r _l sh', '덠'],
    u'\ub361' : ['t _r _l t_h', '덡'],
    u'\ub362' : ['t _r _l p_h', '덢'],
    u'\ub363' : ['t _r _l _h', '덣'],
    u'\ub364' : ['t _r m', '덤'],
    u'\ub365' : ['t _r p', '덥'],
    u'\ub366' : ['t _r p sh', '덦'],
    u'\ub367' : ['t _r sh', '덧'],
    u'\ub368' : ['t _r s', '덨'],
    u'\ub369' : ['t _r N', '덩'],
    u'\ub36a' : ['t _r tS', '덪'],
    u'\ub36b' : ['t _r tSh', '덫'],
    u'\ub36c' : ['t _r k_h', '덬'],
    u'\ub36d' : ['t _r t_h', '덭'],
    u'\ub36e' : ['t _r p_h', '덮'],
    u'\ub36f' : ['t _r _h', '덯'],
    u'\ub370' : ['t e', '데'],
    u'\ub371' : ['t e k', '덱'],
    u'\ub372' : ['t e k_>', '덲'],
    u'\ub373' : ['t e k sh', '덳'],
    u'\ub374' : ['t e _n', '덴'],
    u'\ub375' : ['t e _n tS', '덵'],
    u'\ub376' : ['t e _n _h', '덶'],
    u'\ub377' : ['t e t', '덷'],
    u'\ub378' : ['t e _l', '델'],
    u'\ub379' : ['t e _l k', '덹'],
    u'\ub37a' : ['t e _l m', '덺'],
    u'\ub37b' : ['t e _l p', '덻'],
    u'\ub37c' : ['t e _l sh', '덼'],
    u'\ub37d' : ['t e _l t_h', '덽'],
    u'\ub37e' : ['t e _l p_h', '덾'],
    u'\ub37f' : ['t e _l _h', '덿'],
    u'\ub380' : ['t e m', '뎀'],
    u'\ub381' : ['t e p', '뎁'],
    u'\ub382' : ['t e p sh', '뎂'],
    u'\ub383' : ['t e sh', '뎃'],
    u'\ub384' : ['t e s', '뎄'],
    u'\ub385' : ['t e N', '뎅'],
    u'\ub386' : ['t e tS', '뎆'],
    u'\ub387' : ['t e tSh', '뎇'],
    u'\ub388' : ['t e k_h', '뎈'],
    u'\ub389' : ['t e t_h', '뎉'],
    u'\ub38a' : ['t e p_h', '뎊'],
    u'\ub38b' : ['t e _h', '뎋'],
    u'\ub38c' : ['t _j _r', '뎌'],
    u'\ub38d' : ['t _j _r k', '뎍'],
    u'\ub38e' : ['t _j _r k_>', '뎎'],
    u'\ub38f' : ['t _j _r k sh', '뎏'],
    u'\ub390' : ['t _j _r _n', '뎐'],
    u'\ub391' : ['t _j _r _n tS', '뎑'],
    u'\ub392' : ['t _j _r _n _h', '뎒'],
    u'\ub393' : ['t _j _r t', '뎓'],
    u'\ub394' : ['t _j _r _l', '뎔'],
    u'\ub395' : ['t _j _r _l k', '뎕'],
    u'\ub396' : ['t _j _r _l m', '뎖'],
    u'\ub397' : ['t _j _r _l p', '뎗'],
    u'\ub398' : ['t _j _r _l sh', '뎘'],
    u'\ub399' : ['t _j _r _l t_h', '뎙'],
    u'\ub39a' : ['t _j _r _l p_h', '뎚'],
    u'\ub39b' : ['t _j _r _l _h', '뎛'],
    u'\ub39c' : ['t _j _r m', '뎜'],
    u'\ub39d' : ['t _j _r p', '뎝'],
    u'\ub39e' : ['t _j _r p sh', '뎞'],
    u'\ub39f' : ['t _j _r sh', '뎟'],
    u'\ub3a0' : ['t _j _r s', '뎠'],
    u'\ub3a1' : ['t _j _r N', '뎡'],
    u'\ub3a2' : ['t _j _r tS', '뎢'],
    u'\ub3a3' : ['t _j _r tSh', '뎣'],
    u'\ub3a4' : ['t _j _r k_h', '뎤'],
    u'\ub3a5' : ['t _j _r t_h', '뎥'],
    u'\ub3a6' : ['t _j _r p_h', '뎦'],
    u'\ub3a7' : ['t _j _r _h', '뎧'],
    u'\ub3a8 ' : ['t _j e', '뎨 '],
    u'\ub3a9' : ['t _j e k', '뎩'],
    u'\ub3aa' : ['t _j e k_>', '뎪'],
    u'\ub3ab' : ['t _j e k sh', '뎫'],
    u'\ub3ac' : ['t _j e _n', '뎬'],
    u'\ub3ad' : ['t _j e _n tS', '뎭'],
    u'\ub3ae' : ['t _j e _n _h', '뎮'],
    u'\ub3af' : ['t _j e t', '뎯'],
    u'\ub3b0' : ['t _j e _l', '뎰'],
    u'\ub3b1' : ['t _j e _l k', '뎱'],
    u'\ub3b2' : ['t _j e _l m', '뎲'],
    u'\ub3b3' : ['t _j e _l p', '뎳'],
    u'\ub3b4' : ['t _j e _l sh', '뎴'],
    u'\ub3b5' : ['t _j e _l t_h', '뎵'],
    u'\ub3b6' : ['t _j e _l p_h', '뎶'],
    u'\ub3b7' : ['t _j e _l _h', '뎷'],
    u'\ub3b8' : ['t _j e m', '뎸'],
    u'\ub3b9' : ['t _j e p', '뎹'],
    u'\ub3ba' : ['t _j e p sh', '뎺'],
    u'\ub3bb' : ['t _j e sh', '뎻'],
    u'\ub3bc' : ['t _j e s', '뎼'],
    u'\ub3bd' : ['t _j e N', '뎽'],
    u'\ub3be' : ['t _j e tS', '뎾'],
    u'\ub3bf' : ['t _j e tSh', '뎿'],
    u'\ub3c0' : ['t _j e k_h', '돀'],
    u'\ub3c1' : ['t _j e t_h', '돁'],
    u'\ub3c2' : ['t _j e p_h', '돂'],
    u'\ub3c3' : ['t _j e _h', '돃'],
    u'\ub3c4' : ['t o', '도'],
    u'\ub3c5' : ['t o k', '독'],
    u'\ub3c6' : ['t o k_>', '돆'],
    u'\ub3c7' : ['t o k sh', '돇'],
    u'\ub3c8' : ['t o _n', '돈'],
    u'\ub3c9' : ['t o _n tS', '돉'],
    u'\ub3ca' : ['t o _n _h', '돊'],
    u'\ub3cb' : ['t o t', '돋'],
    u'\ub3cc' : ['t o _l', '돌'],
    u'\ub3cd' : ['t o _l k', '돍'],
    u'\ub3ce' : ['t o _l m', '돎'],
    u'\ub3cf' : ['t o _l p', '돏'],
    u'\ub3d0' : ['t o _l sh', '돐'],
    u'\ub3d1' : ['t o _l t_h', '돑'],
    u'\ub3d2' : ['t o _l p_h', '돒'],
    u'\ub3d3' : ['t o _l _h', '돓'],
    u'\ub3d4' : ['t o m', '돔'],
    u'\ub3d5' : ['t o p', '돕'],
    u'\ub3d6' : ['t o p sh', '돖'],
    u'\ub3d7' : ['t o sh', '돗'],
    u'\ub3d8' : ['t o s', '돘'],
    u'\ub3d9' : ['t o N', '동'],
    u'\ub3da' : ['t o tS', '돚'],
    u'\ub3db' : ['t o tSh', '돛'],
    u'\ub3dc' : ['t o k_h', '돜'],
    u'\ub3dd' : ['t o t_h', '돝'],
    u'\ub3de' : ['t o p_h', '돞'],
    u'\ub3df' : ['t o _h', '돟'],
    u'\ub3e0' : ['t _w a', '돠'],
    u'\ub3e1' : ['t _w a k', '돡'],
    u'\ub3e2' : ['t _w a k_>', '돢'],
    u'\ub3e3' : ['t _w a k sh', '돣'],
    u'\ub3e4' : ['t _w a _n', '돤'],
    u'\ub3e5' : ['t _w a _n tS', '돥'],
    u'\ub3e6' : ['t _w a _n _h', '돦'],
    u'\ub3e7' : ['t _w a t', '돧'],
    u'\ub3e8' : ['t _w a _l', '돨'],
    u'\ub3e9' : ['t _w a _l k', '돩'],
    u'\ub3ea' : ['t _w a _l m', '돪'],
    u'\ub3eb' : ['t _w a _l p', '돫'],
    u'\ub3ec' : ['t _w a _l sh', '돬'],
    u'\ub3ed' : ['t _w a _l t_h', '돭'],
    u'\ub3ee' : ['t _w a _l p_h', '돮'],
    u'\ub3ef' : ['t _w a _l _h', '돯'],
    u'\ub3f0' : ['t _w a m', '돰'],
    u'\ub3f1' : ['t _w a p', '돱'],
    u'\ub3f2' : ['t _w a p sh', '돲'],
    u'\ub3f3' : ['t _w a sh', '돳'],
    u'\ub3f4' : ['t _w a s', '돴'],
    u'\ub3f5' : ['t _w a N', '돵'],
    u'\ub3f6' : ['t _w a tS', '돶'],
    u'\ub3f7' : ['t _w a tSh', '돷'],
    u'\ub3f8' : ['t _w a k_h', '돸'],
    u'\ub3f9' : ['t _w a t_h', '돹'],
    u'\ub3fa' : ['t _w a p_h', '돺'],
    u'\ub3fb' : ['t _w a _h', '돻'],
    u'\ub3fc' : ['t _w {', '돼'],
    u'\ub3fd' : ['t _w { k', '돽'],
    u'\ub3fe' : ['t _w { k_>', '돾'],
    u'\ub3ff' : ['t _w { k sh', '돿'],
    u'\ub400' : ['t _w { _n', '됀'],
    u'\ub401' : ['t _w { _n tS', '됁'],
    u'\ub402' : ['t _w { _n _h', '됂'],
    u'\ub403' : ['t _w { t', '됃'],
    u'\ub404' : ['t _w { _l', '됄'],
    u'\ub405' : ['t _w { _l k', '됅'],
    u'\ub406' : ['t _w { _l m', '됆'],
    u'\ub407' : ['t _w { _l p', '됇'],
    u'\ub408' : ['t _w { _l sh', '됈'],
    u'\ub409' : ['t _w { _l t_h', '됉'],
    u'\ub40a' : ['t _w { _l p_h', '됊'],
    u'\ub40b' : ['t _w { _l _h', '됋'],
    u'\ub40c' : ['t _w { m', '됌'],
    u'\ub40d' : ['t _w { p', '됍'],
    u'\ub40e' : ['t _w { p sh', '됎'],
    u'\ub40f' : ['t _w { sh', '됏'],
    u'\ub410' : ['t _w { s', '됐'],
    u'\ub411' : ['t _w { N', '됑'],
    u'\ub412' : ['t _w { tS', '됒'],
    u'\ub413' : ['t _w { tSh', '됓'],
    u'\ub414' : ['t _w { k_h', '됔'],
    u'\ub415' : ['t _w { t_h', '됕'],
    u'\ub416' : ['t _w { p_h', '됖'],
    u'\ub417' : ['t _w { _h', '됗'],
    u'\ub418' : ['t _w e', '되'],
    u'\ub419' : ['t _w e k', '됙'],
    u'\ub41a' : ['t _w e k_>', '됚'],
    u'\ub41b' : ['t _w e k sh', '됛'],
    u'\ub41c' : ['t _w e _n', '된'],
    u'\ub41d' : ['t _w e _n tS', '됝'],
    u'\ub41e' : ['t _w e _n _h', '됞'],
    u'\ub41f' : ['t _w e t', '됟'],
    u'\ub420' : ['t _w e _l', '될'],
    u'\ub421' : ['t _w e _l k', '됡'],
    u'\ub422' : ['t _w e _l m', '됢'],
    u'\ub423' : ['t _w e _l p', '됣'],
    u'\ub424' : ['t _w e _l sh', '됤'],
    u'\ub425' : ['t _w e _l t_h', '됥'],
    u'\ub426' : ['t _w e _l p_h', '됦'],
    u'\ub427' : ['t _w e _l _h', '됧'],
    u'\ub428' : ['t _w e m', '됨'],
    u'\ub429' : ['t _w e p', '됩'],
    u'\ub42a' : ['t _w e p sh', '됪'],
    u'\ub42b' : ['t _w e sh', '됫'],
    u'\ub42c' : ['t _w e s', '됬'],
    u'\ub42d' : ['t _w e N', '됭'],
    u'\ub42e' : ['t _w e tS', '됮'],
    u'\ub42f' : ['t _w e tSh', '됯'],
    u'\ub430' : ['t _w e k_h', '됰'],
    u'\ub431' : ['t _w e t_h', '됱'],
    u'\ub432' : ['t _w e p_h', '됲'],
    u'\ub433' : ['t _w e _h', '됳'],
    u'\ub434' : ['t _j o', '됴'],
    u'\ub435' : ['t _j o k', '됵'],
    u'\ub436' : ['t _j o k_>', '됶'],
    u'\ub437' : ['t _j o k sh', '됷'],
    u'\ub438' : ['t _j o _n', '됸'],
    u'\ub439' : ['t _j o _n tS', '됹'],
    u'\ub43a' : ['t _j o _n _h', '됺'],
    u'\ub43b' : ['t _j o t', '됻'],
    u'\ub43c' : ['t _j o _l', '됼'],
    u'\ub43d' : ['t _j o _l k', '됽'],
    u'\ub43e' : ['t _j o _l m', '됾'],
    u'\ub43f' : ['t _j o _l p', '됿'],
    u'\ub440' : ['t _j o _l sh', '둀'],
    u'\ub441' : ['t _j o _l t_h', '둁'],
    u'\ub442' : ['t _j o _l p_h', '둂'],
    u'\ub443' : ['t _j o _l _h', '둃'],
    u'\ub444' : ['t _j o m', '둄'],
    u'\ub445' : ['t _j o p', '둅'],
    u'\ub446' : ['t _j o p sh', '둆'],
    u'\ub447' : ['t _j o sh', '둇'],
    u'\ub448' : ['t _j o s', '둈'],
    u'\ub449' : ['t _j o N', '둉'],
    u'\ub44a' : ['t _j o tS', '둊'],
    u'\ub44b' : ['t _j o tSh', '둋'],
    u'\ub44c' : ['t _j o k_h', '둌'],
    u'\ub44d' : ['t _j o t_h', '둍'],
    u'\ub44e' : ['t _j o p_h', '둎'],
    u'\ub44f' : ['t _j o _h', '둏'],
    u'\ub450' : ['t u', '두'],
    u'\ub451' : ['t u k', '둑'],
    u'\ub452' : ['t u k_>', '둒'],
    u'\ub453' : ['t u k sh', '둓'],
    u'\ub454' : ['t u _n', '둔'],
    u'\ub455' : ['t u _n tS', '둕'],
    u'\ub456' : ['t u _n _h', '둖'],
    u'\ub457' : ['t u t', '둗'],
    u'\ub458' : ['t u _l', '둘'],
    u'\ub459' : ['t u _l k', '둙'],
    u'\ub45a' : ['t u _l m', '둚'],
    u'\ub45b' : ['t u _l p', '둛'],
    u'\ub45c' : ['t u _l sh', '둜'],
    u'\ub45d' : ['t u _l t_h', '둝'],
    u'\ub45e' : ['t u _l p_h', '둞'],
    u'\ub45f' : ['t u _l _h', '둟'],
    u'\ub460' : ['t u m', '둠'],
    u'\ub461' : ['t u p', '둡'],
    u'\ub462' : ['t u p sh', '둢'],
    u'\ub463' : ['t u sh', '둣'],
    u'\ub464' : ['t u s', '둤'],
    u'\ub465' : ['t u N', '둥'],
    u'\ub466' : ['t u tS', '둦'],
    u'\ub467' : ['t u tSh', '둧'],
    u'\ub468' : ['t u k_h', '둨'],
    u'\ub469' : ['t u t_h', '둩'],
    u'\ub46a' : ['t u p_h', '둪'],
    u'\ub46b' : ['t u _h', '둫'],
    u'\ub46c' : ['t _w _r', '둬'],
    u'\ub46d' : ['t _w _r k', '둭'],
    u'\ub46e' : ['t _w _r k_>', '둮'],
    u'\ub46f' : ['t _w _r k sh', '둯'],
    u'\ub470' : ['t _w _r _n', '둰'],
    u'\ub471' : ['t _w _r _n tS', '둱'],
    u'\ub472' : ['t _w _r _n _h', '둲'],
    u'\ub473' : ['t _w _r t', '둳'],
    u'\ub474' : ['t _w _r _l', '둴'],
    u'\ub475' : ['t _w _r _l k', '둵'],
    u'\ub476' : ['t _w _r _l m', '둶'],
    u'\ub477' : ['t _w _r _l p', '둷'],
    u'\ub478' : ['t _w _r _l sh', '둸'],
    u'\ub479' : ['t _w _r _l t_h', '둹'],
    u'\ub47a' : ['t _w _r _l p_h', '둺'],
    u'\ub47b' : ['t _w _r _l _h', '둻'],
    u'\ub47c' : ['t _w _r m', '둼'],
    u'\ub47d' : ['t _w _r p', '둽'],
    u'\ub47e' : ['t _w _r p sh', '둾'],
    u'\ub47f' : ['t _w _r sh', '둿'],
    u'\ub480' : ['t _w _r s', '뒀'],
    u'\ub481' : ['t _w _r N', '뒁'],
    u'\ub482' : ['t _w _r tS', '뒂'],
    u'\ub483' : ['t _w _r tSh', '뒃'],
    u'\ub484' : ['t _w _r k_h', '뒄'],
    u'\ub485' : ['t _w _r t_h', '뒅'],
    u'\ub486' : ['t _w _r p_h', '뒆'],
    u'\ub487' : ['t _w _r _h', '뒇'],
    u'\ub488' : ['t _w E', '뒈'],
    u'\ub489' : ['t _w E k', '뒉'],
    u'\ub48a' : ['t _w E k_>', '뒊'],
    u'\ub48b' : ['t _w E k sh', '뒋'],
    u'\ub48c' : ['t _w E _n', '뒌'],
    u'\ub48d' : ['t _w E _n tS', '뒍'],
    u'\ub48e' : ['t _w E _n _h', '뒎'],
    u'\ub48f' : ['t _w E t', '뒏'],
    u'\ub490' : ['t _w E _l', '뒐'],
    u'\ub491' : ['t _w E _l k', '뒑'],
    u'\ub492' : ['t _w E _l m', '뒒'],
    u'\ub493' : ['t _w E _l p', '뒓'],
    u'\ub494' : ['t _w E _l sh', '뒔'],
    u'\ub495' : ['t _w E _l t_h', '뒕'],
    u'\ub496' : ['t _w E _l p_h', '뒖'],
    u'\ub497' : ['t _w E _l _h', '뒗'],
    u'\ub498' : ['t _w E m', '뒘'],
    u'\ub499' : ['t _w E p', '뒙'],
    u'\ub49a' : ['t _w E p sh', '뒚'],
    u'\ub49b' : ['t _w E sh', '뒛'],
    u'\ub49c' : ['t _w E s', '뒜'],
    u'\ub49d' : ['t _w E N', '뒝'],
    u'\ub49e' : ['t _w E tS', '뒞'],
    u'\ub49f' : ['t _w E tSh', '뒟'],
    u'\ub4a0' : ['t _w E k_h', '뒠'],
    u'\ub4a1' : ['t _w E t_h', '뒡'],
    u'\ub4a2' : ['t _w E p_h', '뒢'],
    u'\ub4a3' : ['t _w E _h', '뒣'],
    u'\ub4a4' : ['t 2', '뒤'],
    u'\ub4a5' : ['t 2 k', '뒥'],
    u'\ub4a6' : ['t 2 k_>', '뒦'],
    u'\ub4a7' : ['t 2 k sh', '뒧'],
    u'\ub4a8' : ['t 2 _n', '뒨'],
    u'\ub4a9' : ['t 2 _n tS', '뒩'],
    u'\ub4aa' : ['t 2 _n _h', '뒪'],
    u'\ub4ab' : ['t 2 t', '뒫'],
    u'\ub4ac' : ['t 2 _l', '뒬'],
    u'\ub4ad' : ['t 2 _l k', '뒭'],
    u'\ub4ae' : ['t 2 _l m', '뒮'],
    u'\ub4af' : ['t 2 _l p', '뒯'],
    u'\ub4b0' : ['t 2 _l sh', '뒰'],
    u'\ub4b1' : ['t 2 _l t_h', '뒱'],
    u'\ub4b2' : ['t 2 _l p_h', '뒲'],
    u'\ub4b3' : ['t 2 _l _h', '뒳'],
    u'\ub4b4' : ['t 2 m', '뒴'],
    u'\ub4b5' : ['t 2 p', '뒵'],
    u'\ub4b6' : ['t 2 p sh', '뒶'],
    u'\ub4b7' : ['t 2 sh', '뒷'],
    u'\ub4b8' : ['t 2 s', '뒸'],
    u'\ub4b9' : ['t 2 N', '뒹'],
    u'\ub4ba' : ['t 2 tS', '뒺'],
    u'\ub4bb' : ['t 2 tSh', '뒻'],
    u'\ub4bc' : ['t 2 k_h', '뒼'],
    u'\ub4bd' : ['t 2 t_h', '뒽'],
    u'\ub4be' : ['t 2 p_h', '뒾'],
    u'\ub4bf' : ['t 2 _h', '뒿'],
    u'\ub4c0' : ['t _j u', '듀'],
    u'\ub4c1' : ['t _j u k', '듁'],
    u'\ub4c2' : ['t _j u k_>', '듂'],
    u'\ub4c3' : ['t _j u k sh', '듃'],
    u'\ub4c4' : ['t _j u _n', '듄'],
    u'\ub4c5' : ['t _j u _n tS', '듅'],
    u'\ub4c6' : ['t _j u _n _h', '듆'],
    u'\ub4c7' : ['t _j u t', '듇'],
    u'\ub4c8' : ['t _j u _l', '듈'],
    u'\ub4c9' : ['t _j u _l k', '듉'],
    u'\ub4ca' : ['t _j u _l m', '듊'],
    u'\ub4cb' : ['t _j u _l p', '듋'],
    u'\ub4cc' : ['t _j u _l sh', '듌'],
    u'\ub4cd' : ['t _j u _l t_h', '듍'],
    u'\ub4ce' : ['t _j u _l p_h', '듎'],
    u'\ub4cf' : ['t _j u _l _h', '듏'],
    u'\ub4d0' : ['t _j u m', '듐'],
    u'\ub4d1' : ['t _j u p', '듑'],
    u'\ub4d2' : ['t _j u p sh', '듒'],
    u'\ub4d3' : ['t _j u sh', '듓'],
    u'\ub4d4' : ['t _j u s', '듔'],
    u'\ub4d5' : ['t _j u N', '듕'],
    u'\ub4d6' : ['t _j u tS', '듖'],
    u'\ub4d7' : ['t _j u tSh', '듗'],
    u'\ub4d8' : ['t _j u k_h', '듘'],
    u'\ub4d9' : ['t _j u t_h', '듙'],
    u'\ub4da' : ['t _j u p_h', '듚'],
    u'\ub4db' : ['t _j u _h', '듛'],
    u'\ub4dc' : ['t M', '드'],
    u'\ub4dd' : ['t M k', '득'],
    u'\ub4de' : ['t M k_>', '듞'],
    u'\ub4df' : ['t M k sh', '듟'],
    u'\ub4e0' : ['t M _n', '든'],
    u'\ub4e1' : ['t M _n tS', '듡'],
    u'\ub4e2' : ['t M _n _h', '듢'],
    u'\ub4e3' : ['t M t', '듣'],
    u'\ub4e4' : ['t M _l', '들'],
    u'\ub4e5' : ['t M _l k', '듥'],
    u'\ub4e6' : ['t M _l m', '듦'],
    u'\ub4e7' : ['t M _l p', '듧'],
    u'\ub4e8' : ['t M _l sh', '듨'],
    u'\ub4e9' : ['t M _l t_h', '듩'],
    u'\ub4ea' : ['t M _l p_h', '듪'],
    u'\ub4eb' : ['t M _l _h', '듫'],
    u'\ub4ec' : ['t M m', '듬'],
    u'\ub4ed' : ['t M p', '듭'],
    u'\ub4ee' : ['t M p sh', '듮'],
    u'\ub4ef' : ['t M sh', '듯'],
    u'\ub4f0' : ['t M s', '듰'],
    u'\ub4f1' : ['t M N', '등'],
    u'\ub4f2' : ['t M tS', '듲'],
    u'\ub4f3' : ['t M tSh', '듳'],
    u'\ub4f4' : ['t M k_h', '듴'],
    u'\ub4f5' : ['t M t_h', '듵'],
    u'\ub4f6' : ['t M p_h', '듶'],
    u'\ub4f7' : ['t M _h', '듷'],
    u'\ub4f8' : ['t M _j', '듸'],
    u'\ub4f9' : ['t M _j k', '듹'],
    u'\ub4fa' : ['t M _j k_>', '듺'],
    u'\ub4fb' : ['t M _j k sh', '듻'],
    u'\ub4fc' : ['t M _j _n', '듼'],
    u'\ub4fd' : ['t M _j _n tS', '듽'],
    u'\ub4fe' : ['t M _j _n _h', '듾'],
    u'\ub4ff' : ['t M _j t', '듿'],
    u'\ub500' : ['t M _j _l', '딀'],
    u'\ub501' : ['t M _j _l k', '딁'],
    u'\ub502' : ['t M _j _l m', '딂'],
    u'\ub503' : ['t M _j _l p', '딃'],
    u'\ub504' : ['t M _j _l sh', '딄'],
    u'\ub505' : ['t M _j _l t_h', '딅'],
    u'\ub506' : ['t M _j _l p_h', '딆'],
    u'\ub507' : ['t M _j _l _h', '딇'],
    u'\ub508' : ['t M _j m', '딈'],
    u'\ub509' : ['t M _j p', '딉'],
    u'\ub50a' : ['t M _j p sh', '딊'],
    u'\ub50b' : ['t M _j sh', '딋'],
    u'\ub50c' : ['t M _j s', '딌'],
    u'\ub50d' : ['t M _j N', '딍'],
    u'\ub50e' : ['t M _j tS', '딎'],
    u'\ub50f' : ['t M _j tSh', '딏'],
    u'\ub510' : ['t M _j k_h', '딐'],
    u'\ub511' : ['t M _j t_h', '딑'],
    u'\ub512' : ['t M _j p_h', '딒'],
    u'\ub513' : ['t M _j _h', '딓'],
    u'\ub514' : ['t i', '디'],
    u'\ub515' : ['t i k', '딕'],
    u'\ub516' : ['t i k_>', '딖'],
    u'\ub517' : ['t i k sh', '딗'],
    u'\ub518' : ['t i _n', '딘'],
    u'\ub519' : ['t i _n tS', '딙'],
    u'\ub51a' : ['t i _n _h', '딚'],
    u'\ub51b' : ['t i t', '딛'],
    u'\ub51c' : ['t i _l', '딜'],
    u'\ub51d' : ['t i _l k', '딝'],
    u'\ub51e' : ['t i _l m', '딞'],
    u'\ub51f' : ['t i _l p', '딟'],
    u'\ub520' : ['t i _l sh', '딠'],
    u'\ub521' : ['t i _l t_h', '딡'],
    u'\ub522' : ['t i _l p_h', '딢'],
    u'\ub523' : ['t i _l _h', '딣'],
    u'\ub524' : ['t i m', '딤'],
    u'\ub525' : ['t i p', '딥'],
    u'\ub526' : ['t i p sh', '딦'],
    u'\ub527' : ['t i sh', '딧'],
    u'\ub528' : ['t i s', '딨'],
    u'\ub529' : ['t i N', '딩'],
    u'\ub52a' : ['t i tS', '딪'],
    u'\ub52b' : ['t i tSh', '딫'],
    u'\ub52c' : ['t i k_h', '딬'],
    u'\ub52d' : ['t i t_h', '딭'],
    u'\ub52e' : ['t i p_h', '딮'],
    u'\ub52f' : ['t i _h', '딯'],
    u'\ub530' : ['t_> a', '따'],
    u'\ub531' : ['t_> a k', '딱'],
    u'\ub532' : ['t_> a k_>', '딲'],
    u'\ub533' : ['t_> a k sh', '딳'],
    u'\ub534' : ['t_> a _n', '딴'],
    u'\ub535' : ['t_> a _n tS', '딵'],
    u'\ub536' : ['t_> a _n _h', '딶'],
    u'\ub537' : ['t_> a t', '딷'],
    u'\ub538' : ['t_> a _l', '딸'],
    u'\ub539' : ['t_> a _l k', '딹'],
    u'\ub53a' : ['t_> a _l m', '딺'],
    u'\ub53b' : ['t_> a _l p', '딻'],
    u'\ub53c' : ['t_> a _l sh', '딼'],
    u'\ub53d' : ['t_> a _l t_h', '딽'],
    u'\ub53e' : ['t_> a _l p_h', '딾'],
    u'\ub53f' : ['t_> a _l _h', '딿'],
    u'\ub540' : ['t_> a m', '땀'],
    u'\ub541' : ['t_> a p', '땁'],
    u'\ub542' : ['t_> a p sh', '땂'],
    u'\ub543' : ['t_> a sh', '땃'],
    u'\ub544' : ['t_> a s', '땄'],
    u'\ub545' : ['t_> a N', '땅'],
    u'\ub546' : ['t_> a tS', '땆'],
    u'\ub547' : ['t_> a tSh', '땇'],
    u'\ub548' : ['t_> a k_h', '땈'],
    u'\ub549' : ['t_> a t_h', '땉'],
    u'\ub54a' : ['t_> a p_h', '땊'],
    u'\ub54b' : ['t_> a _h', '땋'],
    u'\ub54c' : ['t_> {', '때'],
    u'\ub54d' : ['t_> { k', '땍'],
    u'\ub54e' : ['t_> { k_>', '땎'],
    u'\ub54f' : ['t_> { k sh', '땏'],
    u'\ub550' : ['t_> { _n', '땐'],
    u'\ub551' : ['t_> { _n tS', '땑'],
    u'\ub552' : ['t_> { _n _h', '땒'],
    u'\ub553' : ['t_> { t', '땓'],
    u'\ub554' : ['t_> { _l', '땔'],
    u'\ub555' : ['t_> { _l k', '땕'],
    u'\ub556' : ['t_> { _l m', '땖'],
    u'\ub557' : ['t_> { _l p', '땗'],
    u'\ub558' : ['t_> { _l sh', '땘'],
    u'\ub559' : ['t_> { _l t_h', '땙'],
    u'\ub55a' : ['t_> { _l p_h', '땚'],
    u'\ub55b' : ['t_> { _l _h', '땛'],
    u'\ub55c' : ['t_> { m', '땜'],
    u'\ub55d' : ['t_> { p', '땝'],
    u'\ub55e' : ['t_> { p sh', '땞'],
    u'\ub55f ' : ['t_> { sh', '땟 '],
    u'\ub560' : ['t_> { s', '땠'],
    u'\ub561' : ['t_> { N', '땡'],
    u'\ub562' : ['t_> { tS', '땢'],
    u'\ub563' : ['t_> { tSh', '땣'],
    u'\ub564' : ['t_> { k_h', '땤'],
    u'\ub565' : ['t_> { t_h', '땥'],
    u'\ub566' : ['t_> { p_h', '땦'],
    u'\ub567' : ['t_> { _h', '땧'],
    u'\ub568' : ['t_> _j a', '땨'],
    u'\ub569' : ['t_> _j a k', '땩'],
    u'\ub56a' : ['t_> _j a k_>', '땪'],
    u'\ub56b' : ['t_> _j a k sh', '땫'],
    u'\ub56c' : ['t_> _j a _n', '땬'],
    u'\ub56d' : ['t_> _j a _n tS', '땭'],
    u'\ub56e' : ['t_> _j a _n _h', '땮'],
    u'\ub56f' : ['t_> _j a t', '땯'],
    u'\ub570' : ['t_> _j a _l', '땰'],
    u'\ub571' : ['t_> _j a _l k', '땱'],
    u'\ub572' : ['t_> _j a _l m', '땲'],
    u'\ub573' : ['t_> _j a _l p', '땳'],
    u'\ub574' : ['t_> _j a _l sh', '땴'],
    u'\ub575' : ['t_> _j a _l t_h', '땵'],
    u'\ub576' : ['t_> _j a _l p_h', '땶'],
    u'\ub577' : ['t_> _j a _l _h', '땷'],
    u'\ub578' : ['t_> _j a m', '땸'],
    u'\ub579' : ['t_> _j a p', '땹'],
    u'\ub57a' : ['t_> _j a p sh', '땺'],
    u'\ub57b' : ['t_> _j a sh', '땻'],
    u'\ub57c' : ['t_> _j a s', '땼'],
    u'\ub57d' : ['t_> _j a N', '땽'],
    u'\ub57e' : ['t_> _j a tS', '땾'],
    u'\ub57f' : ['t_> _j a tSh', '땿'],
    u'\ub580' : ['t_> _j a k_h', '떀'],
    u'\ub581' : ['t_> _j a t_h', '떁'],
    u'\ub582' : ['t_> _j a p_h', '떂'],
    u'\ub583' : ['t_> _j a _h', '떃'],
    u'\ub584' : ['t_> _j {', '떄'],
    u'\ub585' : ['t_> _j { k', '떅'],
    u'\ub586' : ['t_> _j { k_>', '떆'],
    u'\ub587' : ['t_> _j { k sh', '떇'],
    u'\ub588' : ['t_> _j { _n', '떈'],
    u'\ub589' : ['t_> _j { _n tS', '떉'],
    u'\ub58a' : ['t_> _j { _n _h', '떊'],
    u'\ub58b' : ['t_> _j { t', '떋'],
    u'\ub58c' : ['t_> _j { _l', '떌'],
    u'\ub58d' : ['t_> _j { _l k', '떍'],
    u'\ub58e' : ['t_> _j { _l m', '떎'],
    u'\ub58f' : ['t_> _j { _l p', '떏'],
    u'\ub590' : ['t_> _j { _l sh', '떐'],
    u'\ub591' : ['t_> _j { _l t_h', '떑'],
    u'\ub592' : ['t_> _j { _l p_h', '떒'],
    u'\ub593' : ['t_> _j { _l _h', '떓'],
    u'\ub594' : ['t_> _j { m', '떔'],
    u'\ub595' : ['t_> _j { p', '떕'],
    u'\ub596' : ['t_> _j { p sh', '떖'],
    u'\ub597' : ['t_> _j { sh', '떗'],
    u'\ub598' : ['t_> _j { s', '떘'],
    u'\ub599' : ['t_> _j { N', '떙'],
    u'\ub59a' : ['t_> _j { tS', '떚'],
    u'\ub59b' : ['t_> _j { tSh', '떛'],
    u'\ub59c' : ['t_> _j { k_h', '떜'],
    u'\ub59d' : ['t_> _j { t_h', '떝'],
    u'\ub59e' : ['t_> _j { p_h', '떞'],
    u'\ub59f' : ['t_> _j { _h', '떟'],
    u'\ub5a0' : ['t_> _r', '떠'],
    u'\ub5a1' : ['t_> _r k', '떡'],
    u'\ub5a2' : ['t_> _r k_>', '떢'],
    u'\ub5a3' : ['t_> _r k sh', '떣'],
    u'\ub5a4' : ['t_> _r _n', '떤'],
    u'\ub5a5' : ['t_> _r _n tS', '떥'],
    u'\ub5a6' : ['t_> _r _n _h', '떦'],
    u'\ub5a7' : ['t_> _r t', '떧'],
    u'\ub5a8' : ['t_> _r _l', '떨'],
    u'\ub5a9' : ['t_> _r _l k', '떩'],
    u'\ub5aa' : ['t_> _r _l m', '떪'],
    u'\ub5ab' : ['t_> _r _l p', '떫'],
    u'\ub5ac' : ['t_> _r _l sh', '떬'],
    u'\ub5ad' : ['t_> _r _l t_h', '떭'],
    u'\ub5ae' : ['t_> _r _l p_h', '떮'],
    u'\ub5af' : ['t_> _r _l _h', '떯'],
    u'\ub5b0' : ['t_> _r m', '떰'],
    u'\ub5b1' : ['t_> _r p', '떱'],
    u'\ub5b2' : ['t_> _r p sh', '떲'],
    u'\ub5b3' : ['t_> _r sh', '떳'],
    u'\ub5b4' : ['t_> _r s', '떴'],
    u'\ub5b5' : ['t_> _r N', '떵'],
    u'\ub5b6' : ['t_> _r tS', '떶'],
    u'\ub5b7' : ['t_> _r tSh', '떷'],
    u'\ub5b8' : ['t_> _r k_h', '떸'],
    u'\ub5b9' : ['t_> _r t_h', '떹'],
    u'\ub5ba' : ['t_> _r p_h', '떺'],
    u'\ub5bb' : ['t_> _r _h', '떻'],
    u'\ub5bc' : ['t_> e', '떼'],
    u'\ub5bd' : ['t_> e k', '떽'],
    u'\ub5be' : ['t_> e k_>', '떾'],
    u'\ub5bf' : ['t_> e k sh', '떿'],
    u'\ub5c0' : ['t_> e _n', '뗀'],
    u'\ub5c1' : ['t_> e _n tS', '뗁'],
    u'\ub5c2' : ['t_> e _n _h', '뗂'],
    u'\ub5c3' : ['t_> e t', '뗃'],
    u'\ub5c4' : ['t_> e _l', '뗄'],
    u'\ub5c5' : ['t_> e _l k', '뗅'],
    u'\ub5c6' : ['t_> e _l m', '뗆'],
    u'\ub5c7' : ['t_> e _l p', '뗇'],
    u'\ub5c8' : ['t_> e _l sh', '뗈'],
    u'\ub5c9' : ['t_> e _l t_h', '뗉'],
    u'\ub5ca' : ['t_> e _l p_h', '뗊'],
    u'\ub5cb' : ['t_> e _l _h', '뗋'],
    u'\ub5cc' : ['t_> e m', '뗌'],
    u'\ub5cd' : ['t_> e p', '뗍'],
    u'\ub5ce' : ['t_> e p sh', '뗎'],
    u'\ub5cf' : ['t_> e sh', '뗏'],
    u'\ub5d0' : ['t_> e s', '뗐'],
    u'\ub5d1' : ['t_> e N', '뗑'],
    u'\ub5d2' : ['t_> e tS', '뗒'],
    u'\ub5d3' : ['t_> e tSh', '뗓'],
    u'\ub5d4' : ['t_> e k_h', '뗔'],
    u'\ub5d5' : ['t_> e t_h', '뗕'],
    u'\ub5d6' : ['t_> e p_h', '뗖'],
    u'\ub5d7' : ['t_> e _h', '뗗'],
    u'\ub5d8' : ['t_> _j _r', '뗘'],
    u'\ub5d9' : ['t_> _j _r k', '뗙'],
    u'\ub5da' : ['t_> _j _r k_>', '뗚'],
    u'\ub5db' : ['t_> _j _r k sh', '뗛'],
    u'\ub5dc' : ['t_> _j _r _n', '뗜'],
    u'\ub5dd' : ['t_> _j _r _n tS', '뗝'],
    u'\ub5de' : ['t_> _j _r _n _h', '뗞'],
    u'\ub5df' : ['t_> _j _r t', '뗟'],
    u'\ub5e0' : ['t_> _j _r _l', '뗠'],
    u'\ub5e1' : ['t_> _j _r _l k', '뗡'],
    u'\ub5e2' : ['t_> _j _r _l m', '뗢'],
    u'\ub5e3' : ['t_> _j _r _l p', '뗣'],
    u'\ub5e4' : ['t_> _j _r _l sh', '뗤'],
    u'\ub5e5' : ['t_> _j _r _l t_h', '뗥'],
    u'\ub5e6' : ['t_> _j _r _l p_h', '뗦'],
    u'\ub5e7' : ['t_> _j _r _l _h', '뗧'],
    u'\ub5e8' : ['t_> _j _r m', '뗨'],
    u'\ub5e9' : ['t_> _j _r p', '뗩'],
    u'\ub5ea' : ['t_> _j _r p sh', '뗪'],
    u'\ub5eb' : ['t_> _j _r sh', '뗫'],
    u'\ub5ec' : ['t_> _j _r s', '뗬'],
    u'\ub5ed' : ['t_> _j _r N', '뗭'],
    u'\ub5ee' : ['t_> _j _r tS', '뗮'],
    u'\ub5ef' : ['t_> _j _r tSh', '뗯'],
    u'\ub5f0' : ['t_> _j _r k_h', '뗰'],
    u'\ub5f1' : ['t_> _j _r t_h', '뗱'],
    u'\ub5f2' : ['t_> _j _r p_h', '뗲'],
    u'\ub5f3' : ['t_> _j _r _h', '뗳'],
    u'\ub5f4' : ['t_> _j e', '뗴'],
    u'\ub5f5' : ['t_> _j e k', '뗵'],
    u'\ub5f6' : ['t_> _j e k_>', '뗶'],
    u'\ub5f7' : ['t_> _j e k sh', '뗷'],
    u'\ub5f8' : ['t_> _j e _n', '뗸'],
    u'\ub5f9' : ['t_> _j e _n tS', '뗹'],
    u'\ub5fa' : ['t_> _j e _n _h', '뗺'],
    u'\ub5fb' : ['t_> _j e t', '뗻'],
    u'\ub5fc' : ['t_> _j e _l', '뗼'],
    u'\ub5fd' : ['t_> _j e _l k', '뗽'],
    u'\ub5fe' : ['t_> _j e _l m', '뗾'],
    u'\ub5ff' : ['t_> _j e _l p', '뗿'],
    u'\ub600' : ['t_> _j e _l sh', '똀'],
    u'\ub601' : ['t_> _j e _l t_h', '똁'],
    u'\ub602' : ['t_> _j e _l p_h', '똂'],
    u'\ub603' : ['t_> _j e lh', '똃'],
    u'\ub604' : ['t_> _j e m', '똄'],
    u'\ub605' : ['t_> _j e p', '똅'],
    u'\ub606' : ['t_> _j e p sh', '똆'],
    u'\ub607' : ['t_> _j e sh', '똇'],
    u'\ub608' : ['t_> _j e s', '똈'],
    u'\ub609' : ['t_> _j e N', '똉'],
    u'\ub60a' : ['t_> _j e tS', '똊'],
    u'\ub60b' : ['t_> _j e tSh', '똋'],
    u'\ub60c' : ['t_> _j e k_h', '똌'],
    u'\ub60d' : ['t_> _j e t_h', '똍'],
    u'\ub60e' : ['t_> _j e p_h', '똎'],
    u'\ub60f' : ['t_> _j e _h', '똏'],
    u'\ub610' : ['t_> o', '또'],
    u'\ub611' : ['t_> o k', '똑'],
    u'\ub612' : ['t_> o k_>', '똒'],
    u'\ub613' : ['t_> o k sh', '똓'],
    u'\ub614' : ['t_> o _n', '똔'],
    u'\ub615' : ['t_> o _n tS', '똕'],
    u'\ub616' : ['t_> o _n _h', '똖'],
    u'\ub617' : ['t_> o t', '똗'],
    u'\ub618' : ['t_> o _l', '똘'],
    u'\ub619' : ['t_> o _l k', '똙'],
    u'\ub61a' : ['t_> o _l m', '똚'],
    u'\ub61b' : ['t_> o _l p', '똛'],
    u'\ub61c' : ['t_> o _l sh', '똜'],
    u'\ub61d' : ['t_> o _l t_h', '똝'],
    u'\ub61e' : ['t_> o _l p_h', '똞'],
    u'\ub61f' : ['t_> o _l _h', '똟'],
    u'\ub620' : ['t_> o m', '똠'],
    u'\ub621' : ['t_> o p', '똡'],
    u'\ub622' : ['t_> o p sh', '똢'],
    u'\ub623' : ['t_> o sh', '똣'],
    u'\ub624' : ['t_> o s', '똤'],
    u'\ub625' : ['t_> o N', '똥'],
    u'\ub626' : ['t_> o tS', '똦'],
    u'\ub627' : ['t_> o tSh', '똧'],
    u'\ub628' : ['t_> o k_h', '똨'],
    u'\ub629' : ['t_> o t_h', '똩'],
    u'\ub62a' : ['t_> o p_h', '똪'],
    u'\ub62b' : ['t_> o _h', '똫'],
    u'\ub62c' : ['t_> _w a', '똬'],
    u'\ub62d' : ['t_> _w a k', '똭'],
    u'\ub62e' : ['t_> _w a k_>', '똮'],
    u'\ub62f' : ['t_> _w a k sh', '똯'],
    u'\ub630' : ['t_> _w a _n', '똰'],
    u'\ub631' : ['t_> _w a _n tS', '똱'],
    u'\ub632' : ['t_> _w a nh', '똲'],
    u'\ub633' : ['t_> _w a t', '똳'],
    u'\ub634' : ['t_> _w a _l', '똴'],
    u'\ub635' : ['t_> _w a _l k', '똵'],
    u'\ub636' : ['t_> _w a _l m', '똶'],
    u'\ub637' : ['t_> _w a _l p', '똷'],
    u'\ub638' : ['t_> _w a _l sh', '똸'],
    u'\ub639' : ['t_> _w a _l t_h', '똹'],
    u'\ub63a' : ['t_> _w a _l p_h', '똺'],
    u'\ub63b' : ['t_> _w a _l _h', '똻'],
    u'\ub63c' : ['t_> _w a m', '똼'],
    u'\ub63d' : ['t_> _w a p', '똽'],
    u'\ub63e' : ['t_> _w a p sh', '똾'],
    u'\ub63f' : ['t_> _w a sh', '똿'],
    u'\ub640' : ['t_> _w a s', '뙀'],
    u'\ub641' : ['t_> _w a N', '뙁'],
    u'\ub642' : ['t_> _w a tS', '뙂'],
    u'\ub643' : ['t_> _w a tSh', '뙃'],
    u'\ub644' : ['t_> _w a k_h', '뙄'],
    u'\ub645' : ['t_> _w a t_h', '뙅'],
    u'\ub646' : ['t_> _w a p_h', '뙆'],
    u'\ub647' : ['t_> _w a _h', '뙇'],
    u'\ub648' : ['t_> _w {', '뙈'],
    u'\ub649' : ['t_> _w { k', '뙉'],
    u'\ub64a' : ['t_> _w { k_>', '뙊'],
    u'\ub64b' : ['t_> _w { k sh', '뙋'],
    u'\ub64c' : ['t_> _w { _n', '뙌'],
    u'\ub64d' : ['t_> _w { _n tS', '뙍'],
    u'\ub64e' : ['t_> _w { _n _h', '뙎'],
    u'\ub64f' : ['t_> _w { t', '뙏'],
    u'\ub650' : ['t_> _w { _l', '뙐'],
    u'\ub651' : ['t_> _w { _l k', '뙑'],
    u'\ub652' : ['t_> _w { _l m', '뙒'],
    u'\ub653' : ['t_> _w { _l p', '뙓'],
    u'\ub654' : ['t_> _w { _l sh', '뙔'],
    u'\ub655' : ['t_> _w { _l t_h', '뙕'],
    u'\ub656' : ['t_> _w { _l p_h', '뙖'],
    u'\ub657' : ['t_> _w { _l _h', '뙗'],
    u'\ub658' : ['t_> _w { m', '뙘'],
    u'\ub659' : ['t_> _w { p', '뙙'],
    u'\ub65a' : ['t_> _w { p sh', '뙚'],
    u'\ub65b' : ['t_> _w { sh', '뙛'],
    u'\ub65c' : ['t_> _w { s', '뙜'],
    u'\ub65d' : ['t_> _w { N', '뙝'],
    u'\ub65e' : ['t_> _w { tS', '뙞'],
    u'\ub65f' : ['t_> _w { tSh', '뙟'],
    u'\ub660' : ['t_> _w { k_h', '뙠'],
    u'\ub661' : ['t_> _w { t_h', '뙡'],
    u'\ub662' : ['t_> _w { p_h', '뙢'],
    u'\ub663' : ['t_> _w { _h', '뙣'],
    u'\ub664' : ['t_> _w e', '뙤'],
    u'\ub665' : ['t_> _w e k', '뙥'],
    u'\ub666' : ['t_> _w e k_>', '뙦'],
    u'\ub667' : ['t_> _w e k sh', '뙧'],
    u'\ub668' : ['t_> _w e _n', '뙨'],
    u'\ub669' : ['t_> _w e _n tS', '뙩'],
    u'\ub66a' : ['t_> _w e _n _h', '뙪'],
    u'\ub66b' : ['t_> _w e t', '뙫'],
    u'\ub66c' : ['t_> _w e _l', '뙬'],
    u'\ub66d' : ['t_> _w e _l k', '뙭'],
    u'\ub66e' : ['t_> _w e _l m', '뙮'],
    u'\ub66f' : ['t_> _w e _l p', '뙯'],
    u'\ub670' : ['t_> _w e _l sh', '뙰'],
    u'\ub671' : ['t_> _w e _l t_h', '뙱'],
    u'\ub672' : ['t_> _w e _l p_h', '뙲'],
    u'\ub673' : ['t_> _w e _l _h', '뙳'],
    u'\ub674' : ['t_> _w e m', '뙴'],
    u'\ub675' : ['t_> _w e p', '뙵'],
    u'\ub676' : ['t_> _w e p sh', '뙶'],
    u'\ub677' : ['t_> _w e sh', '뙷'],
    u'\ub678' : ['t_> _w e s', '뙸'],
    u'\ub679' : ['t_> _w e N', '뙹'],
    u'\ub67a' : ['t_> _w e tS', '뙺'],
    u'\ub67b' : ['t_> _w e tSh', '뙻'],
    u'\ub67c' : ['t_> _w e k_h', '뙼'],
    u'\ub67d' : ['t_> _w e t_h', '뙽'],
    u'\ub67e' : ['t_> _w e p_h', '뙾'],
    u'\ub67f' : ['t_> _w e _h', '뙿'],
    u'\ub680' : ['t_> _j o', '뚀'],
    u'\ub681' : ['t_> _j o k', '뚁'],
    u'\ub682' : ['t_> _j o k_>', '뚂'],
    u'\ub683' : ['t_> _j o k sh', '뚃'],
    u'\ub684' : ['t_> _j o _n', '뚄'],
    u'\ub685' : ['t_> _j o _n tS', '뚅'],
    u'\ub686' : ['t_> _j o _n _h', '뚆'],
    u'\ub687' : ['t_> _j o t', '뚇'],
    u'\ub688' : ['t_> _j o _l', '뚈'],
    u'\ub689' : ['t_> _j o _l k', '뚉'],
    u'\ub68a' : ['t_> _j o _l m', '뚊'],
    u'\ub68b' : ['t_> _j o _l p', '뚋'],
    u'\ub68c' : ['t_> _j o _l sh', '뚌'],
    u'\ub68d' : ['t_> _j o _l t_h', '뚍'],
    u'\ub68e' : ['t_> _j o _l p_h', '뚎'],
    u'\ub68f' : ['t_> _j o _l _h', '뚏'],
    u'\ub690' : ['t_> _j o m', '뚐'],
    u'\ub691' : ['t_> _j o p', '뚑'],
    u'\ub692' : ['t_> _j o p sh', '뚒'],
    u'\ub693' : ['t_> _j o sh', '뚓'],
    u'\ub694' : ['t_> _j o s', '뚔'],
    u'\ub695' : ['t_> _j o N', '뚕'],
    u'\ub696' : ['t_> _j o tS', '뚖'],
    u'\ub697' : ['t_> _j o tSh', '뚗'],
    u'\ub698' : ['t_> _j o k_h', '뚘'],
    u'\ub699' : ['t_> _j o t_h', '뚙'],
    u'\ub69a' : ['t_> _j o p_h', '뚚'],
    u'\ub69b' : ['t_> _j o _h', '뚛'],
    u'\ub69c' : ['t_> u', '뚜'],
    u'\ub69d' : ['t_> u k', '뚝'],
    u'\ub69e' : ['t_> u k_>', '뚞'],
    u'\ub69f' : ['t_> u k sh', '뚟'],
    u'\ub6a0' : ['t_> u _n', '뚠'],
    u'\ub6a1' : ['t_> u _n tS', '뚡'],
    u'\ub6a2' : ['t_> u _n _h', '뚢'],
    u'\ub6a3' : ['t_> u t', '뚣'],
    u'\ub6a4' : ['t_> u _l', '뚤'],
    u'\ub6a5' : ['t_> u _l k', '뚥'],
    u'\ub6a6' : ['t_> u _l m', '뚦'],
    u'\ub6a7' : ['t_> u _l p', '뚧'],
    u'\ub6a8' : ['t_> u _l sh', '뚨'],
    u'\ub6a9' : ['t_> u _l t_h', '뚩'],
    u'\ub6aa' : ['t_> u _l p_h', '뚪'],
    u'\ub6ab' : ['t_> u _l _h', '뚫'],
    u'\ub6ac' : ['t_> u m', '뚬'],
    u'\ub6ad' : ['t_> u p', '뚭'],
    u'\ub6ae' : ['t_> u p sh', '뚮'],
    u'\ub6af' : ['t_> u sh', '뚯'],
    u'\ub6b0' : ['t_> u s', '뚰'],
    u'\ub6b1' : ['t_> u N', '뚱'],
    u'\ub6b2' : ['t_> u tS', '뚲'],
    u'\ub6b3' : ['t_> u tSh', '뚳'],
    u'\ub6b4' : ['t_> u k_h', '뚴'],
    u'\ub6b5' : ['t_> u t_h', '뚵'],
    u'\ub6b6' : ['t_> u p_h', '뚶'],
    u'\ub6b7' : ['t_> u _h', '뚷'],
    u'\ub6b8' : ['t_> _w _r', '뚸'],
    u'\ub6b9' : ['t_> _w _r k', '뚹'],
    u'\ub6ba' : ['t_> _w _r k_>', '뚺'],
    u'\ub6bb' : ['t_> _w _r k sh', '뚻'],
    u'\ub6bc' : ['t_> _w _r _n', '뚼'],
    u'\ub6bd' : ['t_> _w _r _n tS', '뚽'],
    u'\ub6be' : ['t_> _w _r _n _h', '뚾'],
    u'\ub6bf' : ['t_> _w _r t', '뚿'],
    u'\ub6c0' : ['t_> _w _r _l', '뛀'],
    u'\ub6c1' : ['t_> _w _r _l k', '뛁'],
    u'\ub6c2' : ['t_> _w _r _l m', '뛂'],
    u'\ub6c3' : ['t_> _w _r _l p', '뛃'],
    u'\ub6c4' : ['t_> _w _r _l sh', '뛄'],
    u'\ub6c5' : ['t_> _w _r _l t_h', '뛅'],
    u'\ub6c6' : ['t_> _w _r _l p_h', '뛆'],
    u'\ub6c7' : ['t_> _w _r _l _h', '뛇'],
    u'\ub6c8' : ['t_> _w _r m', '뛈'],
    u'\ub6c9' : ['t_> _w _r p', '뛉'],
    u'\ub6ca' : ['t_> _w _r p sh', '뛊'],
    u'\ub6cb' : ['t_> _w _r sh', '뛋'],
    u'\ub6cc' : ['t_> _w _r s', '뛌'],
    u'\ub6cd ' : ['t_> _w _r N', '뛍 '],
    u'\ub6ce' : ['t_> _w _r tS', '뛎'],
    u'\ub6cf' : ['t_> _w _r tSh', '뛏'],
    u'\ub6d0' : ['t_> _w _r k_h', '뛐'],
    u'\ub6d1' : ['t_> _w _r t_h', '뛑'],
    u'\ub6d2' : ['t_> _w _r p_h', '뛒'],
    u'\ub6d3' : ['t_> _w _r _h', '뛓'],
    u'\ub6d4' : ['t_> _w E', '뛔'],
    u'\ub6d5' : ['t_> _w E k', '뛕'],
    u'\ub6d6' : ['t_> _w E k_>', '뛖'],
    u'\ub6d7' : ['t_> _w E k sh', '뛗'],
    u'\ub6d8' : ['t_> _w E _n', '뛘'],
    u'\ub6d9' : ['t_> _w E _n tS', '뛙'],
    u'\ub6da' : ['t_> _w E _n _h', '뛚'],
    u'\ub6db' : ['t_> _w E t', '뛛'],
    u'\ub6dc' : ['t_> _w E _l', '뛜'],
    u'\ub6dd' : ['t_> _w E _l k', '뛝'],
    u'\ub6de' : ['t_> _w E _l m', '뛞'],
    u'\ub6df' : ['t_> _w E _l p', '뛟'],
    u'\ub6e0' : ['t_> _w E _l sh', '뛠'],
    u'\ub6e1' : ['t_> _w E _l t_h', '뛡'],
    u'\ub6e2' : ['t_> _w E _l p_h', '뛢'],
    u'\ub6e3' : ['t_> _w E _l _h', '뛣'],
    u'\ub6e4' : ['t_> _w E m', '뛤'],
    u'\ub6e5' : ['t_> _w E p', '뛥'],
    u'\ub6e6' : ['t_> _w E p sh', '뛦'],
    u'\ub6e7' : ['t_> _w E sh', '뛧'],
    u'\ub6e8' : ['t_> _w E s', '뛨'],
    u'\ub6e9' : ['t_> _w E N', '뛩'],
    u'\ub6ea' : ['t_> _w E tS', '뛪'],
    u'\ub6eb' : ['t_> _w E tSh', '뛫'],
    u'\ub6ec' : ['t_> _w E k_h', '뛬'],
    u'\ub6ed' : ['t_> _w E t_h', '뛭'],
    u'\ub6ee' : ['t_> _w E p_h', '뛮'],
    u'\ub6ef' : ['t_> _w E _h', '뛯'],
    u'\ub6f0' : ['t_> 2', '뛰'],
    u'\ub6f1' : ['t_> 2 k', '뛱'],
    u'\ub6f2' : ['t_> 2 k_>', '뛲'],
    u'\ub6f3' : ['t_> 2 k sh', '뛳'],
    u'\ub6f4' : ['t_> 2 _n', '뛴'],
    u'\ub6f5' : ['t_> 2 _n tS', '뛵'],
    u'\ub6f6' : ['t_> 2 _n _h', '뛶'],
    u'\ub6f7' : ['t_> 2 t', '뛷'],
    u'\ub6f8' : ['t_> 2 _l', '뛸'],
    u'\ub6f9' : ['t_> 2 _l k', '뛹'],
    u'\ub6fa' : ['t_> 2 _l m', '뛺'],
    u'\ub6fb' : ['t_> 2 _l p', '뛻'],
    u'\ub6fc' : ['t_> 2 _l sh', '뛼'],
    u'\ub6fd' : ['t_> 2 _l t_h', '뛽'],
    u'\ub6fe' : ['t_> 2 _l p_h', '뛾'],
    u'\ub6ff' : ['t_> 2 lh', '뛿'],
    u'\ub700' : ['t_> 2 m', '뜀'],
    u'\ub701' : ['t_> 2 p', '뜁'],
    u'\ub702' : ['t_> 2 p sh', '뜂'],
    u'\ub703' : ['t_> 2 sh', '뜃'],
    u'\ub704' : ['t_> 2 s', '뜄'],
    u'\ub705' : ['t_> 2 N', '뜅'],
    u'\ub706' : ['t_> 2 tS', '뜆'],
    u'\ub707' : ['t_> 2 tSh', '뜇'],
    u'\ub708' : ['t_> 2 k_h', '뜈'],
    u'\ub709' : ['t_> 2 t_h', '뜉'],
    u'\ub70a' : ['t_> 2 p_h', '뜊'],
    u'\ub70b' : ['t_> 2 _h', '뜋'],
    u'\ub70c' : ['t_> _j u', '뜌'],
    u'\ub70d' : ['t_> _j u k', '뜍'],
    u'\ub70e' : ['t_> _j u k_>', '뜎'],
    u'\ub70f' : ['t_> _j u k sh', '뜏'],
    u'\ub710' : ['t_> _j u _n', '뜐'],
    u'\ub711' : ['t_> _j u _n tS', '뜑'],
    u'\ub712' : ['t_> _j u _n _h', '뜒'],
    u'\ub713' : ['t_> _j u t', '뜓'],
    u'\ub714' : ['t_> _j u _l', '뜔'],
    u'\ub715' : ['t_> _j u _l k', '뜕'],
    u'\ub716' : ['t_> _j u _l m', '뜖'],
    u'\ub717' : ['t_> _j u _l p', '뜗'],
    u'\ub718' : ['t_> _j u _l sh', '뜘'],
    u'\ub719' : ['t_> _j u _l t_h', '뜙'],
    u'\ub71a' : ['t_> _j u _l p_h', '뜚'],
    u'\ub71b' : ['t_> _j u _l _h', '뜛'],
    u'\ub71c' : ['t_> _j u m', '뜜'],
    u'\ub71d' : ['t_> _j u p', '뜝'],
    u'\ub71e' : ['t_> _j u p sh', '뜞'],
    u'\ub71f' : ['t_> _j u sh', '뜟'],
    u'\ub720' : ['t_> _j u s', '뜠'],
    u'\ub721' : ['t_> _j u N', '뜡'],
    u'\ub722' : ['t_> _j u tS', '뜢'],
    u'\ub723' : ['t_> _j u tSh', '뜣'],
    u'\ub724' : ['t_> _j u k_h', '뜤'],
    u'\ub725' : ['t_> _j u t_h', '뜥'],
    u'\ub726' : ['t_> _j u p_h', '뜦'],
    u'\ub727' : ['t_> _j u _h', '뜧'],
    u'\ub728' : ['t_> M', '뜨'],
    u'\ub729' : ['t_> M k', '뜩'],
    u'\ub72a' : ['t_> M k_>', '뜪'],
    u'\ub72b' : ['t_> M k sh', '뜫'],
    u'\ub72c' : ['t_> M _n', '뜬'],
    u'\ub72d' : ['t_> M _n tS', '뜭'],
    u'\ub72e' : ['t_> M _n _h', '뜮'],
    u'\ub72f' : ['t_> M t', '뜯'],
    u'\ub730' : ['t_> M _l', '뜰'],
    u'\ub731' : ['t_> M _l k', '뜱'],
    u'\ub732' : ['t_> M _l m', '뜲'],
    u'\ub733' : ['t_> M _l p', '뜳'],
    u'\ub734' : ['t_> M _l sh', '뜴'],
    u'\ub735' : ['t_> M _l t_h', '뜵'],
    u'\ub736' : ['t_> M _l p_h', '뜶'],
    u'\ub737' : ['t_> M _l _h', '뜷'],
    u'\ub738' : ['t_> M m', '뜸'],
    u'\ub739' : ['t_> M p', '뜹'],
    u'\ub73a' : ['t_> M p sh', '뜺'],
    u'\ub73b' : ['t_> M sh', '뜻'],
    u'\ub73c' : ['t_> M s', '뜼'],
    u'\ub73d' : ['t_> M N', '뜽'],
    u'\ub73e' : ['t_> M tS', '뜾'],
    u'\ub73f' : ['t_> M tSh', '뜿'],
    u'\ub740' : ['t_> M k_h', '띀'],
    u'\ub741' : ['t_> M t_h', '띁'],
    u'\ub742' : ['t_> M p_h', '띂'],
    u'\ub743' : ['t_> M _h', '띃'],
    u'\ub744' : ['t_> M _j', '띄'],
    u'\ub745' : ['t_> M _j k', '띅'],
    u'\ub746' : ['t_> M _j k_>', '띆'],
    u'\ub747' : ['t_> M _j k sh', '띇'],
    u'\ub748' : ['t_> M _j _n', '띈'],
    u'\ub749' : ['t_> M _j _n tS', '띉'],
    u'\ub74a' : ['t_> M _j _n _h', '띊'],
    u'\ub74b' : ['t_> M _j t', '띋'],
    u'\ub74c' : ['t_> M _j _l', '띌'],
    u'\ub74d' : ['t_> M _j _l k', '띍'],
    u'\ub74e' : ['t_> M _j _l m', '띎'],
    u'\ub74f' : ['t_> M _j _l p', '띏'],
    u'\ub750' : ['t_> M _j _l sh', '띐'],
    u'\ub751' : ['t_> M _j _l t_h', '띑'],
    u'\ub752' : ['t_> M _j _l p_h', '띒'],
    u'\ub753' : ['t_> M _j _l _h', '띓'],
    u'\ub754' : ['t_> M _j m', '띔'],
    u'\ub755' : ['t_> M _j p', '띕'],
    u'\ub756' : ['t_> M _j p sh', '띖'],
    u'\ub757' : ['t_> M _j sh', '띗'],
    u'\ub758' : ['t_> M _j s', '띘'],
    u'\ub759' : ['t_> M _j N', '띙'],
    u'\ub75a' : ['t_> M _j tS', '띚'],
    u'\ub75b' : ['t_> M _j tSh', '띛'],
    u'\ub75c' : ['t_> M _j k_h', '띜'],
    u'\ub75d' : ['t_> M _j t_h', '띝'],
    u'\ub75e' : ['t_> M _j p_h', '띞'],
    u'\ub75f' : ['t_> M _j _h', '띟'],
    u'\ub760' : ['t_> i', '띠'],
    u'\ub761' : ['t_> i k', '띡'],
    u'\ub762' : ['t_> i k_>', '띢'],
    u'\ub763' : ['t_> i k sh', '띣'],
    u'\ub764' : ['t_> i _n', '띤'],
    u'\ub765' : ['t_> i _n tS', '띥'],
    u'\ub766' : ['t_> i _n _h', '띦'],
    u'\ub767' : ['t_> i t', '띧'],
    u'\ub768' : ['t_> i _l', '띨'],
    u'\ub769' : ['t_> i _l k', '띩'],
    u'\ub76a' : ['t_> i _l m', '띪'],
    u'\ub76b' : ['t_> i _l p', '띫'],
    u'\ub76c' : ['t_> i _l sh', '띬'],
    u'\ub76d' : ['t_> i _l t_h', '띭'],
    u'\ub76e' : ['t_> i _l p_h', '띮'],
    u'\ub76f' : ['t_> i _l _h', '띯'],
    u'\ub770' : ['t_> i m', '띰'],
    u'\ub771' : ['t_> i p', '띱'],
    u'\ub772' : ['t_> i p sh', '띲'],
    u'\ub773' : ['t_> i sh', '띳'],
    u'\ub774' : ['t_> i s', '띴'],
    u'\ub775' : ['t_> i N', '띵'],
    u'\ub776' : ['t_> i tS', '띶'],
    u'\ub777' : ['t_> i tSh', '띷'],
    u'\ub778' : ['t_> i k_h', '띸'],
    u'\ub779' : ['t_> i t_h', '띹'],
    u'\ub77a' : ['t_> i p_h', '띺'],
    u'\ub77b' : ['t_> i _h', '띻'],
    u'\ub77c' : ['` a', '라'],
    u'\ub77d' : ['` a k', '락'],
    u'\ub77e' : ['` a k_>', '띾'],
    u'\ub77f' : ['` a k sh', '띿'],
    u'\ub780' : ['` a _n', '란'],
    u'\ub781' : ['` a _n tS', '랁'],
    u'\ub782' : ['` a _n _h', '랂'],
    u'\ub783' : ['` a t', '랃'],
    u'\ub784' : ['` a _l', '랄'],
    u'\ub785' : ['` a _l k', '랅'],
    u'\ub786' : ['` a _l m', '랆'],
    u'\ub787' : ['` a _l p', '랇'],
    u'\ub788' : ['` a _l sh', '랈'],
    u'\ub789' : ['` a _l t_h', '랉'],
    u'\ub78a' : ['` a _l p_h', '랊'],
    u'\ub78b' : ['` a _l _h', '랋'],
    u'\ub78c' : ['` a m', '람'],
    u'\ub78d' : ['` a p', '랍'],
    u'\ub78e' : ['` a p sh', '랎'],
    u'\ub78f' : ['` a sh', '랏'],
    u'\ub790' : ['` a s', '랐'],
    u'\ub791' : ['` a N', '랑'],
    u'\ub792' : ['` a tS', '랒'],
    u'\ub793' : ['` a tSh', '랓'],
    u'\ub794' : ['` a k_h', '랔'],
    u'\ub795' : ['` a t_h', '랕'],
    u'\ub796' : ['` a p_h', '랖'],
    u'\ub797' : ['` a _h', '랗'],
    u'\ub798' : ['` {', '래'],
    u'\ub799' : ['` { k', '랙'],
    u'\ub79a' : ['` { k_>', '랚'],
    u'\ub79b' : ['` { k sh', '랛'],
    u'\ub79c' : ['` { _n', '랜'],
    u'\ub79d' : ['` { _n tS', '랝'],
    u'\ub79e' : ['` { _n _h', '랞'],
    u'\ub79f' : ['` { t', '랟'],
    u'\ub7a0' : ['` { _l', '랠'],
    u'\ub7a1' : ['` { _l k', '랡'],
    u'\ub7a2' : ['` { _l m', '랢'],
    u'\ub7a3' : ['` { _l p', '랣'],
    u'\ub7a4' : ['` { _l sh', '랤'],
    u'\ub7a5' : ['` { _l t_h', '랥'],
    u'\ub7a6' : ['` { _l p_h', '랦'],
    u'\ub7a7' : ['` { lh', '랧'],
    u'\ub7a8' : ['` { m', '램'],
    u'\ub7a9' : ['` { p', '랩'],
    u'\ub7aa' : ['` { p sh', '랪'],
    u'\ub7ab' : ['` { sh', '랫'],
    u'\ub7ac' : ['` { s', '랬'],
    u'\ub7ad' : ['` { N', '랭'],
    u'\ub7ae' : ['` { tS', '랮'],
    u'\ub7af' : ['` { tSh', '랯'],
    u'\ub7b0' : ['` { k_h', '랰'],
    u'\ub7b1' : ['` { t_h', '랱'],
    u'\ub7b2' : ['` { p_h', '랲'],
    u'\ub7b3' : ['` { _h', '랳'],
    u'\ub7b4' : ['` _j a', '랴'],
    u'\ub7b5' : ['` _j a k', '략'],
    u'\ub7b6' : ['` _j a k_>', '랶'],
    u'\ub7b7' : ['` _j a k sh', '랷'],
    u'\ub7b8' : ['` _j a _n', '랸'],
    u'\ub7b9' : ['` _j a _n tS', '랹'],
    u'\ub7ba' : ['` _j a _n _h', '랺'],
    u'\ub7bb' : ['` _j a t', '랻'],
    u'\ub7bc' : ['` _j a _l', '랼'],
    u'\ub7bd' : ['` _j a _l k', '랽'],
    u'\ub7be' : ['` _j a _l m', '랾'],
    u'\ub7bf' : ['` _j a _l p', '랿'],
    u'\ub7c0' : ['` _j a _l sh', '럀'],
    u'\ub7c1' : ['` _j a _l t_h', '럁'],
    u'\ub7c2' : ['` _j a _l p_h', '럂'],
    u'\ub7c3' : ['` _j a _l _h', '럃'],
    u'\ub7c4' : ['` _j a m', '럄'],
    u'\ub7c5' : ['` _j a p', '럅'],
    u'\ub7c6' : ['` _j a p sh', '럆'],
    u'\ub7c7' : ['` _j a sh', '럇'],
    u'\ub7c8' : ['` _j a s', '럈'],
    u'\ub7c9' : ['` _j a N', '량'],
    u'\ub7ca' : ['` _j a tS', '럊'],
    u'\ub7cb' : ['` _j a tSh', '럋'],
    u'\ub7cc' : ['` _j a k_h', '럌'],
    u'\ub7cd' : ['` _j a t_h', '럍'],
    u'\ub7ce' : ['` _j a p_h', '럎'],
    u'\ub7cf' : ['` _j a _h', '럏'],
    u'\ub7d0' : ['` _j {', '럐'],
    u'\ub7d1' : ['` _j { k', '럑'],
    u'\ub7d2' : ['` _j { k_>', '럒'],
    u'\ub7d3' : ['` _j { k sh', '럓'],
    u'\ub7d4' : ['` _j { _n', '럔'],
    u'\ub7d5' : ['` _j { _n tS', '럕'],
    u'\ub7d6' : ['` _j { _n _h', '럖'],
    u'\ub7d7' : ['` _j { t', '럗'],
    u'\ub7d8' : ['` _j { _l', '럘'],
    u'\ub7d9' : ['` _j { _l', '럙'],
    u'\ub7da' : ['` _j { _l m', '럚'],
    u'\ub7db' : ['` _j { _l p', '럛'],
    u'\ub7dc' : ['` _j { _l sh', '럜'],
    u'\ub7dd' : ['` _j { _l t_h', '럝'],
    u'\ub7de' : ['` _j { _l p_h', '럞'],
    u'\ub7df' : ['` _j { _l _h', '럟'],
    u'\ub7e0' : ['` _j { m', '럠'],
    u'\ub7e1' : ['` _j { p', '럡'],
    u'\ub7e2' : ['` _j { p sh', '럢'],
    u'\ub7e3' : ['` _j { sh', '럣'],
    u'\ub7e4' : ['` _j { s', '럤'],
    u'\ub7e5' : ['` _j { N', '럥'],
    u'\ub7e6' : ['` _j { tS', '럦'],
    u'\ub7e7' : ['` _j { tSh', '럧'],
    u'\ub7e8' : ['` _j { k_h', '럨'],
    u'\ub7e9' : ['` _j { t_h', '럩'],
    u'\ub7ea' : ['` _j { p_h', '럪'],
    u'\ub7eb' : ['` _j { _h', '럫'],
    u'\ub7ec' : ['` _r', '러'],
    u'\ub7ed' : ['` _r k', '럭'],
    u'\ub7ee' : ['` _r k_>', '럮'],
    u'\ub7ef' : ['` _r k sh', '럯'],
    u'\ub7f0' : ['` _r _n', '런'],
    u'\ub7f1' : ['` _r _n tS', '럱'],
    u'\ub7f2' : ['` _r _n _h', '럲'],
    u'\ub7f3' : ['` _r t', '럳'],
    u'\ub7f4' : ['` _r _l', '럴'],
    u'\ub7f5' : ['` _r _l k', '럵'],
    u'\ub7f6' : ['` _r _l m', '럶'],
    u'\ub7f7' : ['` _r _l p', '럷'],
    u'\ub7f8' : ['` _r _l sh', '럸'],
    u'\ub7f9' : ['` _r _l t_h', '럹'],
    u'\ub7fa' : ['` _r _l p_h', '럺'],
    u'\ub7fb' : ['` _r _l _h', '럻'],
    u'\ub7fc' : ['` _r m', '럼'],
    u'\ub7fd' : ['` _r p', '럽'],
    u'\ub7fe' : ['` _r p sh', '럾'],
    u'\ub7ff' : ['` _r sh', '럿'],
    u'\ub800' : ['` _r s', '렀'],
    u'\ub801' : ['` _r N', '렁'],
    u'\ub802' : ['` _r tS', '렂'],
    u'\ub803' : ['` _r tSh', '렃'],
    u'\ub804' : ['` _r k_h', '렄'],
    u'\ub805' : ['` _r t_h', '렅'],
    u'\ub806' : ['` _r p_h', '렆'],
    u'\ub807' : ['` _r _h', '렇'],
    u'\ub808' : ['` e', '레'],
    u'\ub809' : ['` e k', '렉'],
    u'\ub80a' : ['` e k_>', '렊'],
    u'\ub80b' : ['` e k sh', '렋'],
    u'\ub80c' : ['` e _n', '렌'],
    u'\ub80d' : ['` e _n tS', '렍'],
    u'\ub80e' : ['` e _n _h', '렎'],
    u'\ub80f' : ['` e t', '렏'],
    u'\ub810' : ['` e _l', '렐'],
    u'\ub811' : ['` e _l k', '렑'],
    u'\ub812' : ['` e _l m', '렒'],
    u'\ub813' : ['` e _l p', '렓'],
    u'\ub814' : ['` e _l sh', '렔'],
    u'\ub815' : ['` e _l t_h', '렕'],
    u'\ub816' : ['` e _l p_h', '렖'],
    u'\ub817' : ['` e _l _h', '렗'],
    u'\ub818' : ['` e m', '렘'],
    u'\ub819' : ['` e p', '렙'],
    u'\ub81a' : ['` e p sh', '렚'],
    u'\ub81b' : ['` e sh', '렛'],
    u'\ub81c' : ['` e s', '렜'],
    u'\ub81d' : ['` e N', '렝'],
    u'\ub81e' : ['` e tS', '렞'],
    u'\ub81f' : ['` e tSh', '렟'],
    u'\ub820' : ['` e k_h', '렠'],
    u'\ub821' : ['` e t_h', '렡'],
    u'\ub822' : ['` e p_h', '렢'],
    u'\ub823' : ['` e _h', '렣'],
    u'\ub824' : ['` _j _r', '려'],
    u'\ub825' : ['` _j _r k', '력'],
    u'\ub826' : ['` _j _r k_>', '렦'],
    u'\ub827' : ['` _j _r k sh', '렧'],
    u'\ub828' : ['` _j _r _n', '련'],
    u'\ub829' : ['` _j _r _n tS', '렩'],
    u'\ub82a' : ['` _j _r _n _h', '렪'],
    u'\ub82b' : ['` _j _r t', '렫'],
    u'\ub82c' : ['` _j _r _l', '렬'],
    u'\ub82d' : ['` _j _r _l k', '렭'],
    u'\ub82e' : ['` _j _r _l m', '렮'],
    u'\ub82f' : ['` _j _r _l p', '렯'],
    u'\ub830' : ['` _j _r _l sh', '렰'],
    u'\ub831' : ['` _j _r _l t_h', '렱'],
    u'\ub832' : ['` _j _r _l p_h', '렲'],
    u'\ub833' : ['` _j _r _l _h', '렳'],
    u'\ub834' : ['` _j _r m', '렴'],
    u'\ub835' : ['` _j _r p', '렵'],
    u'\ub836' : ['` _j _r p sh', '렶'],
    u'\ub837' : ['` _j _r sh', '렷'],
    u'\ub838' : ['` _j _r s', '렸'],
    u'\ub839' : ['` _j _r N', '령'],
    u'\ub83a' : ['` _j _r tS', '렺'],
    u'\ub83b' : ['` _j _r tSh', '렻'],
    u'\ub83c' : ['` _j _r k_h', '렼'],
    u'\ub83d' : ['` _j _r t_h', '렽'],
    u'\ub83e' : ['` _j _r p_h', '렾'],
    u'\ub83f' : ['` _j _r _h', '렿'],
    u'\ub840' : ['` _j e', '례'],
    u'\ub841' : ['` _j e k', '롁'],
    u'\ub842' : ['` _j e k_>', '롂'],
    u'\ub843' : ['` _j e k sh', '롃'],
    u'\ub844' : ['` _j e _n', '롄'],
    u'\ub845' : ['` _j e _n tS', '롅'],
    u'\ub846' : ['` _j e _n _h', '롆'],
    u'\ub847' : ['` _j e t', '롇'],
    u'\ub848' : ['` _j e _l', '롈'],
    u'\ub849' : ['` _j e _l k', '롉'],
    u'\ub84a' : ['` _j e _l m', '롊'],
    u'\ub84b' : ['` _j e _l p', '롋'],
    u'\ub84c' : ['` _j e _l sh', '롌'],
    u'\ub84d' : ['` _j e _l t_h', '롍'],
    u'\ub84e' : ['` _j e _l p_h', '롎'],
    u'\ub84f' : ['` _j e _l _h', '롏'],
    u'\ub850' : ['` _j e m', '롐'],
    u'\ub851' : ['` _j e p', '롑'],
    u'\ub852' : ['` _j e p sh', '롒'],
    u'\ub853' : ['` _j e sh', '롓'],
    u'\ub854' : ['` _j e s', '롔'],
    u'\ub855' : ['` _j e N', '롕'],
    u'\ub856' : ['` _j e tS', '롖'],
    u'\ub857' : ['` _j e tSh', '롗'],
    u'\ub858' : ['` _j e k_h', '롘'],
    u'\ub859' : ['` _j e t_h', '롙'],
    u'\ub85a' : ['` _j e p_h', '롚'],
    u'\ub85b' : ['` _j e _h', '롛'],
    u'\ub85c' : ['` o', '로'],
    u'\ub85d' : ['` o k', '록'],
    u'\ub85e' : ['` o k_>', '롞'],
    u'\ub85f' : ['` o k sh', '롟'],
    u'\ub860' : ['` o _n', '론'],
    u'\ub861' : ['` o _n tS', '롡'],
    u'\ub862' : ['` o _n _h', '롢'],
    u'\ub863' : ['` o t', '롣'],
    u'\ub864' : ['` o _l', '롤'],
    u'\ub865' : ['` o _l k', '롥'],
    u'\ub866' : ['` o _l m', '롦'],
    u'\ub867' : ['` o _l p', '롧'],
    u'\ub868' : ['` o _l sh', '롨'],
    u'\ub869' : ['` o _l t_h', '롩'],
    u'\ub86a' : ['` o _l p_h', '롪'],
    u'\ub86b' : ['` o _l _h', '롫'],
    u'\ub86c' : ['` o m', '롬'],
    u'\ub86d' : ['` o p', '롭'],
    u'\ub86e' : ['` o p sh', '롮'],
    u'\ub86f' : ['` o sh', '롯'],
    u'\ub870' : ['` o s', '롰'],
    u'\ub871' : ['` o N', '롱'],
    u'\ub872' : ['` o tS', '롲'],
    u'\ub873' : ['` o tSh', '롳'],
    u'\ub874' : ['` o k_h', '롴'],
    u'\ub875' : ['` o t_h', '롵'],
    u'\ub876' : ['` o p_h', '롶'],
    u'\ub877' : ['` o _h', '롷'],
    u'\ub878' : ['` _w a', '롸'],
    u'\ub879' : ['` _w a k', '롹'],
    u'\ub87a' : ['` _w a k_>', '롺'],
    u'\ub87b' : ['` _w a k sh', '롻'],
    u'\ub87c' : ['` _w a _n', '롼'],
    u'\ub87d' : ['` _w a _n tS', '롽'],
    u'\ub87e' : ['` _w a _n _h', '롾'],
    u'\ub87f' : ['` _w a t', '롿'],
    u'\ub880' : ['` _w a _l', '뢀'],
    u'\ub881' : ['` _w a _l k', '뢁'],
    u'\ub882' : ['` _w a _l m', '뢂'],
    u'\ub883' : ['` _w a _l p', '뢃'],
    u'\ub884' : ['` _w a _l sh', '뢄'],
    u'\ub885' : ['` _w a _l t_h', '뢅'],
    u'\ub886' : ['` _w a _l p_h', '뢆'],
    u'\ub887' : ['` _w a _l _h', '뢇'],
    u'\ub888' : ['` _w a m', '뢈'],
    u'\ub889' : ['` _w a p', '뢉'],
    u'\ub88a' : ['` _w a p sh', '뢊'],
    u'\ub88b' : ['` _w a sh', '뢋'],
    u'\ub88c' : ['` _w a s', '뢌'],
    u'\ub88d' : ['` _w a N', '뢍'],
    u'\ub88e' : ['` _w a tS', '뢎'],
    u'\ub88f' : ['` _w a tSh', '뢏'],
    u'\ub890' : ['` _w a k_h', '뢐'],
    u'\ub891' : ['` _w a t_h', '뢑'],
    u'\ub892' : ['` _w a p_h', '뢒'],
    u'\ub893' : ['` _w a _h', '뢓'],
    u'\ub894' : ['` _w {', '뢔'],
    u'\ub895' : ['` _w { k', '뢕'],
    u'\ub896' : ['` _w { k_>', '뢖'],
    u'\ub897' : ['` _w { k sh', '뢗'],
    u'\ub898' : ['` _w { _n', '뢘'],
    u'\ub899' : ['` _w { _n tS', '뢙'],
    u'\ub89a' : ['` _w { _n _h', '뢚'],
    u'\ub89b' : ['` _w { t', '뢛'],
    u'\ub89c' : ['` _w { _l', '뢜'],
    u'\ub89d' : ['` _w { _l k', '뢝'],
    u'\ub89e' : ['` _w { _l m', '뢞'],
    u'\ub89f' : ['` _w { _l p', '뢟'],
    u'\ub8a0' : ['` _w { _l sh', '뢠'],
    u'\ub8a1' : ['` _w { _l t_h', '뢡'],
    u'\ub8a2' : ['` _w { _l p_h', '뢢'],
    u'\ub8a3' : ['` _w { _l _h', '뢣'],
    u'\ub8a4' : ['` _w { m', '뢤'],
    u'\ub8a5' : ['` _w { p', '뢥'],
    u'\ub8a6' : ['` _w { p sh', '뢦'],
    u'\ub8a7' : ['` _w { sh', '뢧'],
    u'\ub8a8' : ['` _w { s', '뢨'],
    u'\ub8a9' : ['` _w { N', '뢩'],
    u'\ub8aa' : ['` _w { tS', '뢪'],
    u'\ub8ab' : ['` _w { tSh', '뢫'],
    u'\ub8ac' : ['` _w { k_h', '뢬'],
    u'\ub8ad' : ['` _w { t_h', '뢭'],
    u'\ub8ae' : ['` _w { p_h', '뢮'],
    u'\ub8af' : ['` _w { _h', '뢯'],
    u'\ub8b0' : ['` _w e', '뢰'],
    u'\ub8b1' : ['` _w e k', '뢱'],
    u'\ub8b2' : ['` _w e k_>', '뢲'],
    u'\ub8b3' : ['` _w e k sh', '뢳'],
    u'\ub8b4' : ['` _w e _n', '뢴'],
    u'\ub8b5' : ['` _w e _n tS', '뢵'],
    u'\ub8b6' : ['` _w e _n _h', '뢶'],
    u'\ub8b7' : ['` _w e t', '뢷'],
    u'\ub8b8' : ['` _w e _l', '뢸'],
    u'\ub8b9' : ['` _w e _l k', '뢹'],
    u'\ub8ba' : ['` _w e _l m', '뢺'],
    u'\ub8bb' : ['` _w e _l p', '뢻'],
    u'\ub8bc' : ['` _w e _l sh', '뢼'],
    u'\ub8bd' : ['` _w e _l t_h', '뢽'],
    u'\ub8be' : ['` _w e _l p_h', '뢾'],
    u'\ub8bf' : ['` _w e _l _h', '뢿'],
    u'\ub8c0' : ['` _w e m', '룀'],
    u'\ub8c1' : ['` _w e p', '룁'],
    u'\ub8c2' : ['` _w e p sh', '룂'],
    u'\ub8c3' : ['` _w e sh', '룃'],
    u'\ub8c4' : ['` _w e s', '룄'],
    u'\ub8c5' : ['` _w e N', '룅'],
    u'\ub8c6' : ['` _w e tS', '룆'],
    u'\ub8c7' : ['` _w e tSh', '룇'],
    u'\ub8c8' : ['` _w e k_h', '룈'],
    u'\ub8c9' : ['` _w e t_h', '룉'],
    u'\ub8ca' : ['` _w e p_h', '룊'],
    u'\ub8cb' : ['` _w e _h', '룋'],
    u'\ub8cc' : ['` _j o', '료'],
    u'\ub8cd' : ['` _j o k', '룍'],
    u'\ub8ce' : ['` _j o k_>', '룎'],
    u'\ub8cf' : ['` _j o k sh', '룏'],
    u'\ub8d0' : ['` _j o _n', '룐'],
    u'\ub8d1' : ['` _j o _n tS', '룑'],
    u'\ub8d2' : ['` _j o _n _h', '룒'],
    u'\ub8d3' : ['` _j o t', '룓'],
    u'\ub8d4' : ['` _j o _l', '룔'],
    u'\ub8d5' : ['` _j o _l k', '룕'],
    u'\ub8d6' : ['` _j o _l m', '룖'],
    u'\ub8d7' : ['` _j o _l p', '룗'],
    u'\ub8d8' : ['` _j o _l sh', '룘'],
    u'\ub8d9' : ['` _j o _l t_h', '룙'],
    u'\ub8da' : ['` _j o _l p_h', '룚'],
    u'\ub8db' : ['` _j o _l _h', '룛'],
    u'\ub8dc' : ['` _j o m', '룜'],
    u'\ub8dd' : ['` _j o p', '룝'],
    u'\ub8de' : ['` _j o p sh', '룞'],
    u'\ub8df' : ['` _j o sh', '룟'],
    u'\ub8e0' : ['` _j o s', '룠'],
    u'\ub8e1' : ['` _j o N', '룡'],
    u'\ub8e2' : ['` _j o tS', '룢'],
    u'\ub8e3' : ['` _j o tSh', '룣'],
    u'\ub8e4' : ['` _j o k_h', '룤'],
    u'\ub8e5' : ['` _j o t_h', '룥'],
    u'\ub8e6' : ['` _j o p_h', '룦'],
    u'\ub8e7' : ['` _j o _h', '룧'],
    u'\ub8e8' : ['` u', '루'],
    u'\ub8e9' : ['` u k', '룩'],
    u'\ub8ea' : ['` u k_>', '룪'],
    u'\ub8eb' : ['` u k sh', '룫'],
    u'\ub8ec' : ['` u _n', '룬'],
    u'\ub8ed' : ['` u _n tS', '룭'],
    u'\ub8ee' : ['` u _n _h', '룮'],
    u'\ub8ef' : ['` u t', '룯'],
    u'\ub8f0' : ['` u _l', '룰'],
    u'\ub8f1' : ['` u _l k', '룱'],
    u'\ub8f2' : ['` u _l m', '룲'],
    u'\ub8f3' : ['` u _l p', '룳'],
    u'\ub8f4' : ['` u _l sh', '룴'],
    u'\ub8f5' : ['` u _l t_h', '룵'],
    u'\ub8f6' : ['` u _l p_h', '룶'],
    u'\ub8f7' : ['` u _l _h', '룷'],
    u'\ub8f8' : ['` u m', '룸'],
    u'\ub8f9' : ['` u p', '룹'],
    u'\ub8fa' : ['` u p sh', '룺'],
    u'\ub8fb' : ['` u sh', '룻'],
    u'\ub8fc' : ['` u s', '룼'],
    u'\ub8fd' : ['` u N', '룽'],
    u'\ub8fe' : ['` u tS', '룾'],
    u'\ub8ff' : ['` u tSh', '룿'],
    u'\ub900' : ['` u k_h', '뤀'],
    u'\ub901' : ['` u t_h', '뤁'],
    u'\ub902' : ['` u p_h', '뤂'],
    u'\ub903' : ['` u _h', '뤃'],
    u'\ub904' : ['` _w _r', '뤄'],
    u'\ub905' : ['` _w _r k', '뤅'],
    u'\ub906' : ['` _w _r k_>', '뤆'],
    u'\ub907' : ['` _w _r k sh', '뤇'],
    u'\ub908' : ['` _w _r _n', '뤈'],
    u'\ub909' : ['` _w _r _n tS', '뤉'],
    u'\ub90a' : ['` _w _r _n _h', '뤊'],
    u'\ub90b' : ['` _w _r t', '뤋'],
    u'\ub90c' : ['` _w _r _l', '뤌'],
    u'\ub90d' : ['` _w _r _l k', '뤍'],
    u'\ub90e' : ['` _w _r _l m', '뤎'],
    u'\ub90f' : ['` _w _r _l p', '뤏'],
    u'\ub910' : ['` _w _r _l sh', '뤐'],
    u'\ub911' : ['` _w _r _l t_h', '뤑'],
    u'\ub912' : ['` _w _r _l p_h', '뤒'],
    u'\ub913' : ['` _w _r _l _h', '뤓'],
    u'\ub914' : ['` _w _r m', '뤔'],
    u'\ub915' : ['` _w _r p', '뤕'],
    u'\ub916' : ['` _w _r p sh', '뤖'],
    u'\ub917' : ['` _w _r sh', '뤗'],
    u'\ub918' : ['` _w _r s', '뤘'],
    u'\ub919' : ['` _w _r N', '뤙'],
    u'\ub91a' : ['` _w _r tS', '뤚'],
    u'\ub91b' : ['` _w _r tSh', '뤛'],
    u'\ub91c' : ['` _w _r k_h', '뤜'],
    u'\ub91d' : ['` _w _r t_h', '뤝'],
    u'\ub91e' : ['` _w _r p_h', '뤞'],
    u'\ub91f' : ['` _w _r _h', '뤟'],
    u'\ub920' : ['` _w E', '뤠'],
    u'\ub921' : ['` _w E k', '뤡'],
    u'\ub922' : ['` _w E k_>', '뤢'],
    u'\ub923' : ['` _w E k sh', '뤣'],
    u'\ub924' : ['` _w E _n', '뤤'],
    u'\ub925' : ['` _w E _n tS', '뤥'],
    u'\ub926' : ['` _w E _n _h', '뤦'],
    u'\ub927' : ['` _w E t', '뤧'],
    u'\ub928' : ['` _w E _l', '뤨'],
    u'\ub929' : ['` _w E _l k', '뤩'],
    u'\ub92a' : ['` _w E _l m', '뤪'],
    u'\ub92b' : ['` _w E _l p', '뤫'],
    u'\ub92c' : ['` _w E _l sh', '뤬'],
    u'\ub92d' : ['` _w E _l t_h', '뤭'],
    u'\ub92e' : ['` _w E _l p_h', '뤮'],
    u'\ub92f' : ['` _w E _l _h', '뤯'],
    u'\ub930' : ['` _w E m', '뤰'],
    u'\ub931' : ['` _w E p', '뤱'],
    u'\ub932' : ['` _w E p sh', '뤲'],
    u'\ub933' : ['` _w E sh', '뤳'],
    u'\ub934' : ['` _w E s', '뤴'],
    u'\ub935' : ['` _w E N', '뤵'],
    u'\ub936' : ['` _w E tS', '뤶'],
    u'\ub937' : ['` _w E tSh', '뤷'],
    u'\ub938' : ['` _w E k_h', '뤸'],
    u'\ub939' : ['` _w E t_h', '뤹'],
    u'\ub93a' : ['` _w E p_h', '뤺'],
    u'\ub93b' : ['` _w E _h', '뤻'],
    u'\ub93c' : ['` 2', '뤼'],
    u'\ub93d' : ['` 2 k', '뤽'],
    u'\ub93e' : ['` 2 k_>', '뤾'],
    u'\ub93f' : ['` 2 k sh', '뤿'],
    u'\ub940' : ['` 2 _n', '륀'],
    u'\ub941' : ['` 2 _n tS', '륁'],
    u'\ub942' : ['` 2 _n _h', '륂'],
    u'\ub943' : ['` 2 t', '륃'],
    u'\ub944' : ['` 2 _l', '륄'],
    u'\ub945' : ['` 2 _l k', '륅'],
    u'\ub946' : ['` 2 _l m', '륆'],
    u'\ub947' : ['` 2 _l p', '륇'],
    u'\ub948' : ['` 2 _l sh', '륈'],
    u'\ub949' : ['` 2 _l t_h', '륉'],
    u'\ub94a' : ['` 2 _l p_h', '륊'],
    u'\ub94b' : ['` 2 _l _h', '륋'],
    u'\ub94c' : ['` 2 m', '륌'],
    u'\ub94d' : ['` 2 p', '륍'],
    u'\ub94e' : ['` 2 p sh', '륎'],
    u'\ub94f' : ['` 2 sh', '륏'],
    u'\ub950' : ['` 2 s', '륐'],
    u'\ub951' : ['` 2 N', '륑'],
    u'\ub952' : ['` 2 tS', '륒'],
    u'\ub953' : ['` 2 tSh', '륓'],
    u'\ub954' : ['` 2 k_h', '륔'],
    u'\ub955' : ['` 2 t_h', '륕'],
    u'\ub956' : ['` 2 p_h', '륖'],
    u'\ub957' : ['` 2 _h', '륗'],
    u'\ub958' : ['` _j u', '류'],
    u'\ub959' : ['` _j u k', '륙'],
    u'\ub95a' : ['` _j u k_>', '륚'],
    u'\ub95b' : ['` _j u k sh', '륛'],
    u'\ub95c' : ['` _j u k _n', '륜'],
    u'\ub95d' : ['` _j u _n tS', '륝'],
    u'\ub95e' : ['` _j u _n _h', '륞'],
    u'\ub95f' : ['` _j u t', '륟'],
    u'\ub960' : ['` _j u _l', '률'],
    u'\ub961' : ['` _j u _l k', '륡'],
    u'\ub962' : ['` _j u _l m', '륢'],
    u'\ub963' : ['` _j u _l p', '륣'],
    u'\ub964' : ['` _j u _l sh', '륤'],
    u'\ub965' : ['` _j u _l t_h', '륥'],
    u'\ub966' : ['` _j u _l p_h', '륦'],
    u'\ub967' : ['` _j u _l _h', '륧'],
    u'\ub968' : ['` _j u m', '륨'],
    u'\ub969' : ['` _j u p', '륩'],
    u'\ub96a' : ['` _j u p sh', '륪'],
    u'\ub96b' : ['` _j u sh', '륫'],
    u'\ub96c' : ['` _j u s', '륬'],
    u'\ub96d' : ['` _j u N', '륭'],
    u'\ub96e' : ['` _j u tS', '륮'],
    u'\ub96f' : ['` _j u tSh', '륯'],
    u'\ub970' : ['` _j u k_h', '륰'],
    u'\ub971' : ['` _j u t_h', '륱'],
    u'\ub972' : ['` _j u p_h', '륲'],
    u'\ub973' : ['` _j u _h', '륳'],
    u'\ub974' : ['` M', '르'],
    u'\ub975' : ['` M k', '륵'],
    u'\ub976' : ['` M k_>', '륶'],
    u'\ub977' : ['` M k sh', '륷'],
    u'\ub978' : ['` M _n', '른'],
    u'\ub979' : ['` M _n tS', '륹'],
    u'\ub97a' : ['` M _n _h', '륺'],
    u'\ub97b' : ['` M t', '륻'],
    u'\ub97c' : ['` M _l', '를'],
    u'\ub97d' : ['` M _l k', '륽'],
    u'\ub97e' : ['` M _l m', '륾'],
    u'\ub97f' : ['` M _l p', '륿'],
    u'\ub980' : ['` M _l sh', '릀'],
    u'\ub981' : ['` M _l t_h', '릁'],
    u'\ub982' : ['` M _l p_h', '릂'],
    u'\ub983' : ['` M _l _h', '릃'],
    u'\ub984' : ['` M m', '름'],
    u'\ub985' : ['` M p', '릅'],
    u'\ub986' : ['` M p sh', '릆'],
    u'\ub987' : ['` M sh', '릇'],
    u'\ub988' : ['` M s', '릈'],
    u'\ub989' : ['` M N', '릉'],
    u'\ub98a' : ['` M tS', '릊'],
    u'\ub98b' : ['` M tSh', '릋'],
    u'\ub98c' : ['` M k_h', '릌'],
    u'\ub98d' : ['` M t_h', '릍'],
    u'\ub98e' : ['` M p_h', '릎'],
    u'\ub98f' : ['` M _h', '릏'],
    u'\ub990' : ['` M _j', '릐'],
    u'\ub991' : ['` M _j k', '릑'],
    u'\ub992' : ['` M _j k_>', '릒'],
    u'\ub993' : ['` M _j k sh', '릓'],
    u'\ub994' : ['` M _j _n', '릔'],
    u'\ub995' : ['` M _j _n tS', '릕'],
    u'\ub996' : ['` M _j _n _h', '릖'],
    u'\ub997' : ['` M _j t', '릗'],
    u'\ub998' : ['` M _j _l', '릘'],
    u'\ub999' : ['` M _j _l k', '릙'],
    u'\ub99a' : ['` M _j _l m', '릚'],
    u'\ub99b' : ['` M _j _l p', '릛'],
    u'\ub99c' : ['` M _j _l sh', '릜'],
    u'\ub99d' : ['` M _j _l t_h', '릝'],
    u'\ub99e' : ['` M _j _l p_h', '릞'],
    u'\ub99f' : ['` M _j _l _h', '릟'],
    u'\ub9a0' : ['` M _j m', '릠'],
    u'\ub9a1' : ['` M _j p', '릡'],
    u'\ub9a2' : ['` M _j p sh', '릢'],
    u'\ub9a3' : ['` M _j sh', '릣'],
    u'\ub9a4' : ['` M _j s', '릤'],
    u'\ub9a5' : ['` M _j N', '릥'],
    u'\ub9a6' : ['` M _j tS', '릦'],
    u'\ub9a7' : ['` M _j tSh', '릧'],
    u'\ub9a8' : ['` M _j k_h', '릨'],
    u'\ub9a9' : ['` M _j t_h', '릩'],
    u'\ub9aa' : ['` M _j p_h', '릪'],
    u'\ub9ab' : ['` M _j _h', '릫'],
    u'\ub9ac' : ['` i', '리'],
    u'\ub9ad' : ['` i k', '릭'],
    u'\ub9ae' : ['` i k_>', '릮'],
    u'\ub9af' : ['` i k sh', '릯'],
    u'\ub9b0' : ['` i _n', '린'],
    u'\ub9b1' : ['` i _n tS', '릱'],
    u'\ub9b2' : ['` i _n _h', '릲'],
    u'\ub9b3' : ['` i t', '릳'],
    u'\ub9b4' : ['` i _l', '릴'],
    u'\ub9b5' : ['` i _l k', '릵'],
    u'\ub9b6' : ['` i _l m', '릶'],
    u'\ub9b7' : ['` i _l p', '릷'],
    u'\ub9b8' : ['` i _l sh', '릸'],
    u'\ub9b9' : ['` i _l t_h', '릹'],
    u'\ub9ba' : ['` i _l p_h', '릺'],
    u'\ub9bb' : ['` i _l _h', '릻'],
    u'\ub9bc' : ['` i m', '림'],
    u'\ub9bd' : ['` i p', '립'],
    u'\ub9be' : ['` i p sh', '릾'],
    u'\ub9bf' : ['` i sh', '릿'],
    u'\ub9c0' : ['` i s', '맀'],
    u'\ub9c1' : ['` i N', '링'],
    u'\ub9c2' : ['` i tS', '맂'],
    u'\ub9c3' : ['` i tSh', '맃'],
    u'\ub9c4' : ['` i k_h', '맄'],
    u'\ub9c5' : ['` i t_h', '맅'],
    u'\ub9c6' : ['` i p_h', '맆'],
    u'\ub9c7' : ['` i _h', '맇'],
    u'\ub9c8' : ['m a', '마'],
    u'\ub9c9' : ['m a k', '막'],
    u'\ub9ca' : ['m a k_>', '맊'],
    u'\ub9cb' : ['m a k sh', '맋'],
    u'\ub9cc' : ['m a _n', '만'],
    u'\ub9cd' : ['m a _n tS', '맍'],
    u'\ub9ce' : ['m a _n _h', '많'],
    u'\ub9cf' : ['m a t', '맏'],
    u'\ub9d0' : ['m a _l', '말'],
    u'\ub9d1' : ['m a _l k', '맑'],
    u'\ub9d2' : ['m a _l m', '맒'],
    u'\ub9d3' : ['m a _l p', '맓'],
    u'\ub9d4' : ['m a _l sh', '맔'],
    u'\ub9d5' : ['m a _l t_h', '맕'],
    u'\ub9d6' : ['m a _l p_h', '맖'],
    u'\ub9d7' : ['m a _l _h', '맗'],
    u'\ub9d8' : ['m a m', '맘'],
    u'\ub9d9' : ['m a p', '맙'],
    u'\ub9da' : ['m a p sh', '맚'],
    u'\ub9db' : ['m a sh', '맛'],
    u'\ub9dc' : ['m a s', '맜'],
    u'\ub9dd' : ['m a N', '망'],
    u'\ub9de' : ['m a tS', '맞'],
    u'\ub9df' : ['m a tSh', '맟'],
    u'\ub9e0' : ['m a k_h', '맠'],
    u'\ub9e1' : ['m a t_h', '맡'],
    u'\ub9e2' : ['m a p_h', '맢'],
    u'\ub9e3' : ['m a _h', '맣'],
    u'\ub9e4' : ['m {', '매'],
    u'\ub9e5' : ['m { k', '맥'],
    u'\ub9e6' : ['m { k_>', '맦'],
    u'\ub9e7' : ['m { k sh', '맧'],
    u'\ub9e8' : ['m { _n', '맨'],
    u'\ub9e9' : ['m { _n tS', '맩'],
    u'\ub9ea' : ['m { _n _h', '맪'],
    u'\ub9eb' : ['m { t', '맫'],
    u'\ub9ec' : ['m { _l', '맬'],
    u'\ub9ed' : ['m { _l k', '맭'],
    u'\ub9ee' : ['m { _l m', '맮'],
    u'\ub9ef' : ['m { _l p', '맯'],
    u'\ub9f0' : ['m { _l sh', '맰'],
    u'\ub9f1' : ['m { _l t_h', '맱'],
    u'\ub9f2' : ['m { _l p_h', '맲'],
    u'\ub9f3' : ['m { _l _h', '맳'],
    u'\ub9f4' : ['m { m', '맴'],
    u'\ub9f5' : ['m { p', '맵'],
    u'\ub9f6' : ['m { p sh', '맶'],
    u'\ub9f7' : ['m { sh', '맷'],
    u'\ub9f8' : ['m { s', '맸'],
    u'\ub9f9' : ['m { N', '맹'],
    u'\ub9fa' : ['m { tS', '맺'],
    u'\ub9fb' : ['m { tSh', '맻'],
    u'\ub9fc' : ['m { k_h', '맼'],
    u'\ub9fd' : ['m { t_h', '맽'],
    u'\ub9fe' : ['m { p_h', '맾'],
    u'\ub9ff' : ['m { _h', '맿'],
    u'\uba00' : ['m _j a', '먀'],
    u'\uba01' : ['m _j a k', '먁'],
    u'\uba02' : ['m _j a k_>', '먂'],
    u'\uba03' : ['m _j a k sh', '먃'],
    u'\uba04' : ['m _j a _n', '먄'],
    u'\uba05' : ['m _j a _n tS', '먅'],
    u'\uba06' : ['m _j a _n _h', '먆'],
    u'\uba07' : ['m _j a t', '먇'],
    u'\uba08' : ['m _j a _l', '먈'],
    u'\uba09' : ['m _j a _l k', '먉'],
    u'\uba0a' : ['m _j a _l m', '먊'],
    u'\uba0b' : ['m _j a _l p', '먋'],
    u'\uba0c' : ['m _j a _l sh', '먌'],
    u'\uba0d' : ['m _j a _l t_h', '먍'],
    u'\uba0e' : ['m _j a _l p_h', '먎'],
    u'\uba0f' : ['m _j a _l _h', '먏'],
    u'\uba10' : ['m _j a m', '먐'],
    u'\uba11' : ['m _j a p', '먑'],
    u'\uba12' : ['m _j a p sh', '먒'],
    u'\uba13' : ['m _j a sh', '먓'],
    u'\uba14' : ['m _j a s', '먔'],
    u'\uba15' : ['m _j a N', '먕'],
    u'\uba16' : ['m _j a tS', '먖'],
    u'\uba17' : ['m _j a tSh', '먗'],
    u'\uba18' : ['m _j a k_h', '먘'],
    u'\uba19' : ['m _j a t_h', '먙'],
    u'\uba1a' : ['m _j a p_h', '먚'],
    u'\uba1b' : ['m _j a _h', '먛'],
    u'\uba1c' : ['m _j {', '먜'],
    u'\uba1d' : ['m _j { k', '먝'],
    u'\uba1e' : ['m _j { k_>', '먞'],
    u'\uba1f' : ['m _j { k sh', '먟'],
    u'\uba20' : ['m _j { _n', '먠'],
    u'\uba21' : ['m _j { _n tS', '먡'],
    u'\uba22' : ['m _j { _n _h', '먢'],
    u'\uba23' : ['m _j { t', '먣'],
    u'\uba24' : ['m _j { _l', '먤'],
    u'\uba25' : ['m _j { _l k', '먥'],
    u'\uba26' : ['m _j { _l m', '먦'],
    u'\uba27' : ['m _j { _l p', '먧'],
    u'\uba28' : ['m _j { _l sh', '먨'],
    u'\uba29' : ['m _j { _l t_h', '먩'],
    u'\uba2a' : ['m _j { _l p_h', '먪'],
    u'\uba2b' : ['m _j { _l _h', '먫'],
    u'\uba2c' : ['m _j { m', '먬'],
    u'\uba2d' : ['m _j { p', '먭'],
    u'\uba2e' : ['m _j { p sh', '먮'],
    u'\uba2f' : ['m _j { sh', '먯'],
    u'\uba30' : ['m _j { s', '먰'],
    u'\uba31' : ['m _j { N', '먱'],
    u'\uba32' : ['m _j { tS', '먲'],
    u'\uba33' : ['m _j { tSh', '먳'],
    u'\uba34' : ['m _j { k_h', '먴'],
    u'\uba35' : ['m _j { t_h', '먵'],
    u'\uba36' : ['m _j { p_h', '먶'],
    u'\uba37' : ['m _j { _h', '먷'],
    u'\uba38' : ['m _r', '머'],
    u'\uba39' : ['m _r k', '먹'],
    u'\uba3a' : ['m _r k_>', '먺'],
    u'\uba3b' : ['m _r k sh', '먻'],
    u'\uba3c' : ['m _r _n', '먼'],
    u'\uba3d' : ['m _r _n tS', '먽'],
    u'\uba3e' : ['m _r _n _h', '먾'],
    u'\uba3f' : ['m _r t', '먿'],
    u'\uba40' : ['m _r _l', '멀'],
    u'\uba41' : ['m _r _l k', '멁'],
    u'\uba42' : ['m _r _l m', '멂'],
    u'\uba43' : ['m _r _l p', '멃'],
    u'\uba44' : ['m _r _l sh', '멄'],
    u'\uba45' : ['m _r _l t_h', '멅'],
    u'\uba46' : ['m _r _l p_h', '멆'],
    u'\uba47' : ['m _r _l _h', '멇'],
    u'\uba48' : ['m _r m', '멈'],
    u'\uba49' : ['m _r p', '멉'],
    u'\uba4a' : ['m _r p sh', '멊'],
    u'\uba4b' : ['m _r sh', '멋'],
    u'\uba4c' : ['m _r s', '멌'],
    u'\uba4d' : ['m _r N', '멍'],
    u'\uba4e' : ['m _r tS', '멎'],
    u'\uba4f' : ['m _r tSh', '멏'],
    u'\uba50' : ['m _r k_h', '멐'],
    u'\uba51' : ['m _r t_h', '멑'],
    u'\uba52' : ['m _r p_h', '멒'],
    u'\uba53' : ['m _r _h', '멓'],
    u'\uba54' : ['m e', '메'],
    u'\uba55' : ['m e k', '멕'],
    u'\uba56' : ['m e k_>', '멖'],
    u'\uba57' : ['m e k sh', '멗'],
    u'\uba58' : ['m e _n', '멘'],
    u'\uba59' : ['m e _n tS', '멙'],
    u'\uba5a' : ['m e _n _h', '멚'],
    u'\uba5b' : ['m e t', '멛'],
    u'\uba5c' : ['m e _l', '멜'],
    u'\uba5d' : ['m e _l k', '멝'],
    u'\uba5e' : ['m e _l m', '멞'],
    u'\uba5f' : ['m e _l p', '멟'],
    u'\uba60' : ['m e _l sh', '멠'],
    u'\uba61' : ['m e _l t_h', '멡'],
    u'\uba62' : ['m e _l p_h', '멢'],
    u'\uba63' : ['m e _l _h', '멣'],
    u'\uba64' : ['m e m', '멤'],
    u'\uba65' : ['m e p', '멥'],
    u'\uba66' : ['m e p sh', '멦'],
    u'\uba67' : ['m e sh', '멧'],
    u'\uba68' : ['m e s', '멨'],
    u'\uba69' : ['m e N', '멩'],
    u'\uba6a' : ['m e tS', '멪'],
    u'\uba6b' : ['m e tSh', '멫'],
    u'\uba6c' : ['m e k_h', '멬'],
    u'\uba6d' : ['m e t_h', '멭'],
    u'\uba6e' : ['m e p_h', '멮'],
    u'\uba6f' : ['m e _h', '멯'],
    u'\uba70' : ['m _j _r', '며'],
    u'\uba71' : ['m _j _r k', '멱'],
    u'\uba72' : ['m _j _r k_>', '멲'],
    u'\uba73' : ['m _j _r k sh', '멳'],
    u'\uba74' : ['m _j _r _n', '면'],
    u'\uba75' : ['m _j _r _n tS', '멵'],
    u'\uba76' : ['m _j _r _n _h', '멶'],
    u'\uba77' : ['m _j _r t', '멷'],
    u'\uba78' : ['m _j _r _l', '멸'],
    u'\uba79' : ['m _j _r _l k', '멹'],
    u'\uba7a' : ['m _j _r _l m', '멺'],
    u'\uba7b' : ['m _j _r _l p', '멻'],
    u'\uba7c' : ['m _j _r _l sh', '멼'],
    u'\uba7d' : ['m _j _r _l t_h', '멽'],
    u'\uba7e' : ['m _j _r _l p_h', '멾'],
    u'\uba7f' : ['m _j _r _l _h', '멿'],
    u'\uba80' : ['m _j _r m', '몀'],
    u'\uba81' : ['m _j _r p', '몁'],
    u'\uba82' : ['m _j _r p sh', '몂'],
    u'\uba83' : ['m _j _r sh', '몃'],
    u'\uba84' : ['m _j _r s', '몄'],
    u'\uba85' : ['m _j _r N', '명'],
    u'\uba86' : ['m _j _r tS', '몆'],
    u'\uba87' : ['m _j _r tSh', '몇'],
    u'\uba88' : ['m _j _r k_h', '몈'],
    u'\uba89' : ['m _j _r t_h', '몉'],
    u'\uba8a' : ['m _j _r p_h', '몊'],
    u'\uba8b' : ['m _j _r _h', '몋'],
    u'\uba8c' : ['m _j e', '몌'],
    u'\uba8d' : ['m _j e k', '몍'],
    u'\uba8e' : ['m _j e k_>', '몎'],
    u'\uba8f' : ['m _j e k sh', '몏'],
    u'\uba90' : ['m _j e _n', '몐'],
    u'\uba91' : ['m _j e _n tS', '몑'],
    u'\uba92' : ['m _j e _n _h', '몒'],
    u'\uba93' : ['m _j e t', '몓'],
    u'\uba94' : ['m _j e _l', '몔'],
    u'\uba95' : ['m _j e _l k', '몕'],
    u'\uba96' : ['m _j e _l m', '몖'],
    u'\uba97' : ['m _j e _l p', '몗'],
    u'\uba98' : ['m _j e _l sh', '몘'],
    u'\uba99' : ['m _j e _l t_h', '몙'],
    u'\uba9a' : ['m _j e _l p_h', '몚'],
    u'\uba9b' : ['m _j e _l _h', '몛'],
    u'\uba9c' : ['m _j e m', '몜'],
    u'\uba9d' : ['m _j e p', '몝'],
    u'\uba9e' : ['m _j e p sh', '몞'],
    u'\uba9f' : ['m _j e sh', '몟'],
    u'\ubaa0' : ['m _j e s', '몠'],
    u'\ubaa1' : ['m _j e N', '몡'],
    u'\ubaa2' : ['m _j e tS', '몢'],
    u'\ubaa3' : ['m _j e tSh', '몣'],
    u'\ubaa4' : ['m _j e k_h', '몤'],
    u'\ubaa5' : ['m _j e t_h', '몥'],
    u'\ubaa6' : ['m _j e p_h', '몦'],
    u'\ubaa7' : ['m _j e _h', '몧'],
    u'\ubaa8' : ['m o', '모'],
    u'\ubaa9' : ['m o k', '목'],
    u'\ubaaa' : ['m o k_>', '몪'],
    u'\ubaab' : ['m o k sh', '몫'],
    u'\ubaac' : ['m o _n', '몬'],
    u'\ubaad' : ['m o _n tS', '몭'],
    u'\ubaae' : ['m o _n _h', '몮'],
    u'\ubaaf' : ['m o t', '몯'],
    u'\ubab0' : ['m o _l', '몰'],
    u'\ubab1' : ['m o _l k', '몱'],
    u'\ubab2' : ['m o _l m', '몲'],
    u'\ubab3' : ['m o _l p', '몳'],
    u'\ubab4' : ['m o _l sh', '몴'],
    u'\ubab5' : ['m o _l t_h', '몵'],
    u'\ubab6' : ['m o _l p_h', '몶'],
    u'\ubab7' : ['m o _l _h', '몷'],
    u'\ubab8' : ['m o m', '몸'],
    u'\ubab9' : ['m o p', '몹'],
    u'\ubaba' : ['m o p sh', '몺'],
    u'\ubabb' : ['m o sh', '못'],
    u'\ubabc' : ['m o s', '몼'],
    u'\ubabd' : ['m o N', '몽'],
    u'\ubabe' : ['m o tS', '몾'],
    u'\ubabf' : ['m o tSh', '몿'],
    u'\ubac0' : ['m o k_h', '뫀'],
    u'\ubac1' : ['m o t_h', '뫁'],
    u'\ubac2' : ['m o p_h', '뫂'],
    u'\ubac3' : ['m o _h', '뫃'],
    u'\ubac4' : ['m _w a', '뫄'],
    u'\ubac5' : ['m _w a k', '뫅'],
    u'\ubac6' : ['m _w a k_>', '뫆'],
    u'\ubac7' : ['m _w a k sh', '뫇'],
    u'\ubac8' : ['m _w a _n', '뫈'],
    u'\ubac9' : ['m _w a _n tS', '뫉'],
    u'\ubaca' : ['m _w a _n _h', '뫊'],
    u'\ubacb' : ['m _w a t', '뫋'],
    u'\ubacc' : ['m _w a _l', '뫌'],
    u'\ubacd' : ['m _w a _l k', '뫍'],
    u'\ubace' : ['m _w a _l m', '뫎'],
    u'\ubacf' : ['m _w a _l p', '뫏'],
    u'\ubad0' : ['m _w a _l sh', '뫐'],
    u'\ubad1' : ['m _w a _l t_h', '뫑'],
    u'\ubad2' : ['m _w a _l p_h', '뫒'],
    u'\ubad3' : ['m _w a _l _h', '뫓'],
    u'\ubad4' : ['m _w a m', '뫔'],
    u'\ubad5' : ['m _w a p', '뫕'],
    u'\ubad6' : ['m _w a p sh', '뫖'],
    u'\ubad7' : ['m _w a sh', '뫗'],
    u'\ubad8' : ['m _w a s', '뫘'],
    u'\ubad9' : ['m _w a N', '뫙'],
    u'\ubada' : ['m _w a tS', '뫚'],
    u'\ubadb' : ['m _w a tSh', '뫛'],
    u'\ubadc' : ['m _w a k_h', '뫜'],
    u'\ubadd' : ['m _w a t_h', '뫝'],
    u'\ubade' : ['m _w a p_h', '뫞'],
    u'\ubadf' : ['m _w a _h', '뫟'],
    u'\ubae0' : ['m _w {', '뫠'],
    u'\ubae1' : ['m _w { k', '뫡'],
    u'\ubae2' : ['m _w { k_>', '뫢'],
    u'\ubae3' : ['m _w { k sh', '뫣'],
    u'\ubae4' : ['m _w { _n', '뫤'],
    u'\ubae5' : ['m _w { _n tS', '뫥'],
    u'\ubae6' : ['m _w { _n _h', '뫦'],
    u'\ubae7' : ['m _w { t', '뫧'],
    u'\ubae8' : ['m _w { _l', '뫨'],
    u'\ubae9' : ['m _w { _l k', '뫩'],
    u'\ubaea' : ['m _w { _l m', '뫪'],
    u'\ubaeb' : ['m _w { _l p', '뫫'],
    u'\ubaec' : ['m _w { _l sh', '뫬'],
    u'\ubaed' : ['m _w { _l t_h', '뫭'],
    u'\ubaee' : ['m _w { _l p_h', '뫮'],
    u'\ubaef' : ['m _w { _l _h', '뫯'],
    u'\ubaf0' : ['m _w { m', '뫰'],
    u'\ubaf1' : ['m _w { p', '뫱'],
    u'\ubaf2' : ['m _w { p sh', '뫲'],
    u'\ubaf3' : ['m _w { sh', '뫳'],
    u'\ubaf4' : ['m _w { s', '뫴'],
    u'\ubaf5' : ['m _w { N', '뫵'],
    u'\ubaf6' : ['m _w { tS', '뫶'],
    u'\ubaf7' : ['m _w { tSh', '뫷'],
    u'\ubaf8' : ['m _w { k_h', '뫸'],
    u'\ubaf9' : ['m _w { t_h', '뫹'],
    u'\ubafa' : ['m _w { p_h', '뫺'],
    u'\ubafb' : ['m _w { _h', '뫻'],
    u'\ubafc' : ['m _w e', '뫼'],
    u'\ubafd' : ['m _w e k', '뫽'],
    u'\ubafe' : ['m _w e k_>', '뫾'],
    u'\ubaff' : ['m _w e k sh', '뫿'],
    u'\ubb00' : ['m _w e _n', '묀'],
    u'\ubb01' : ['m _w e _n tS', '묁'],
    u'\ubb02' : ['m _w e _n _h', '묂'],
    u'\ubb03' : ['m _w e t', '묃'],
    u'\ubb04' : ['m _w e _l', '묄'],
    u'\ubb05' : ['m _w e _l k', '묅'],
    u'\ubb06' : ['m _w e _l m', '묆'],
    u'\ubb07' : ['m _w e _l p', '묇'],
    u'\ubb08' : ['m _w e _l sh', '묈'],
    u'\ubb09' : ['m _w e _l t_h', '묉'],
    u'\ubb0a' : ['m _w e _l p_h', '묊'],
    u'\ubb0b' : ['m _w e _l _h', '묋'],
    u'\ubb0c' : ['m _w e m', '묌'],
    u'\ubb0d' : ['m _w e p', '묍'],
    u'\ubb0e' : ['m _w e p sh', '묎'],
    u'\ubb0f' : ['m _w e sh', '묏'],
    u'\ubb10' : ['m _w e s', '묐'],
    u'\ubb11' : ['m _w e N', '묑'],
    u'\ubb12' : ['m _w e tS', '묒'],
    u'\ubb13' : ['m _w e tSh', '묓'],
    u'\ubb14' : ['m _w e k_h', '묔'],
    u'\ubb15' : ['m _w e t_h', '묕'],
    u'\ubb16' : ['m _w e p_h', '묖'],
    u'\ubb17' : ['m _w e _h', '묗'],
    u'\ubb18' : ['m _j o', '묘'],
    u'\ubb19' : ['m _j o k', '묙'],
    u'\ubb1a' : ['m _j o k_>', '묚'],
    u'\ubb1b' : ['m _j o k sh', '묛'],
    u'\ubb1c' : ['m _j o _n', '묜'],
    u'\ubb1d' : ['m _j o _n tS', '묝'],
    u'\ubb1e' : ['m _j o _n _h', '묞'],
    u'\ubb1f' : ['m _j o t', '묟'],
    u'\ubb20' : ['m _j o _l', '묠'],
    u'\ubb21' : ['m _j o _l k', '묡'],
    u'\ubb22' : ['m _j o _l m', '묢'],
    u'\ubb23' : ['m _j o _l p', '묣'],
    u'\ubb24' : ['m _j o _l sh', '묤'],
    u'\ubb25' : ['m _j o _l t_h', '묥'],
    u'\ubb26' : ['m _j o _l p_h', '묦'],
    u'\ubb27' : ['m _j o _l _h', '묧'],
    u'\ubb28' : ['m _j o m', '묨'],
    u'\ubb29' : ['m _j o p', '묩'],
    u'\ubb2a' : ['m _j o p sh', '묪'],
    u'\ubb2b' : ['m _j o sh', '묫'],
    u'\ubb2c' : ['m _j o s', '묬'],
    u'\ubb2d' : ['m _j o N', '묭'],
    u'\ubb2e' : ['m _j o tS', '묮'],
    u'\ubb2f' : ['m _j o tSh', '묯'],
    u'\ubb30' : ['m _j o k_h', '묰'],
    u'\ubb31' : ['m _j o t_h', '묱'],
    u'\ubb32' : ['m _j o p_h', '묲'],
    u'\ubb33' : ['m _j o _h', '묳'],
    u'\ubb34' : ['m u', '무'],
    u'\ubb35' : ['m u k', '묵'],
    u'\ubb36' : ['m u k_>', '묶'],
    u'\ubb37' : ['m u k sh', '묷'],
    u'\ubb38' : ['m u _n', '문'],
    u'\ubb39' : ['m u _n tS', '묹'],
    u'\ubb3a' : ['m u _n _h', '묺'],
    u'\ubb3b' : ['m u t', '묻'],
    u'\ubb3c' : ['m u _l', '물'],
    u'\ubb3d' : ['m u _l k', '묽'],
    u'\ubb3e' : ['m u _l m', '묾'],
    u'\ubb3f' : ['m u _l p', '묿'],
    u'\ubb40' : ['m u _l sh', '뭀'],
    u'\ubb41' : ['m u _l t_h', '뭁'],
    u'\ubb42' : ['m u _l p_h', '뭂'],
    u'\ubb43' : ['m u _l _h', '뭃'],
    u'\ubb44' : ['m u m', '뭄'],
    u'\ubb45' : ['m u p', '뭅'],
    u'\ubb46' : ['m u p sh', '뭆'],
    u'\ubb47' : ['m u sh', '뭇'],
    u'\ubb48' : ['m u s', '뭈'],
    u'\ubb49' : ['m u N', '뭉'],
    u'\ubb4a' : ['m u tS', '뭊'],
    u'\ubb4b' : ['m u tSh', '뭋'],
    u'\ubb4c' : ['m u k_h', '뭌'],
    u'\ubb4d' : ['m u t_h', '뭍'],
    u'\ubb4e' : ['m u p_h', '뭎'],
    u'\ubb4f' : ['m u _h', '뭏'],
    u'\ubb50' : ['m _w _r', '뭐'],
    u'\ubb51' : ['m _w _r k', '뭑'],
    u'\ubb52' : ['m _w _r k_>', '뭒'],
    u'\ubb53' : ['m _w _r k sh', '뭓'],
    u'\ubb54' : ['m _w _r _n', '뭔'],
    u'\ubb55' : ['m _w _r _n tS', '뭕'],
    u'\ubb56' : ['m _w _r _n _h', '뭖'],
    u'\ubb57' : ['m _w _r t', '뭗'],
    u'\ubb58' : ['m _w _r _l', '뭘'],
    u'\ubb59' : ['m _w _r _l k', '뭙'],
    u'\ubb5a' : ['m _w _r _l m', '뭚'],
    u'\ubb5b' : ['m _w _r _l p', '뭛'],
    u'\ubb5c' : ['m _w _r _l sh', '뭜'],
    u'\ubb5d' : ['m _w _r _l t_h', '뭝'],
    u'\ubb5e' : ['m _w _r _l p_h', '뭞'],
    u'\ubb5f' : ['m _w _r _l _h', '뭟'],
    u'\ubb60' : ['m _w _r m', '뭠'],
    u'\ubb61' : ['m _w _r p', '뭡'],
    u'\ubb62' : ['m _w _r p sh', '뭢'],
    u'\ubb63' : ['m _w _r sh', '뭣'],
    u'\ubb64' : ['m _w _r s', '뭤'],
    u'\ubb65' : ['m _w _r N', '뭥'],
    u'\ubb66' : ['m _w _r tS', '뭦'],
    u'\ubb67' : ['m _w _r tSh', '뭧'],
    u'\ubb68' : ['m _w _r k_h', '뭨'],
    u'\ubb69' : ['m _w _r t_h', '뭩'],
    u'\ubb6a' : ['m _w _r p_h', '뭪'],
    u'\ubb6b' : ['m _w _r _h', '뭫'],
    u'\ubb6c' : ['m _w E', '뭬'],
    u'\ubb6d' : ['m _w E k', '뭭'],
    u'\ubb6e' : ['m _w E k_>', '뭮'],
    u'\ubb6f' : ['m _w E k sh', '뭯'],
    u'\ubb70' : ['m _w E _n', '뭰'],
    u'\ubb71' : ['m _w E _n tS', '뭱'],
    u'\ubb72' : ['m _w E _n _h', '뭲'],
    u'\ubb73' : ['m _w E t', '뭳'],
    u'\ubb74' : ['m _w E _l', '뭴'],
    u'\ubb75' : ['m _w E _l k', '뭵'],
    u'\ubb76' : ['m _w E _l m', '뭶'],
    u'\ubb77' : ['m _w E _l p', '뭷'],
    u'\ubb78' : ['m _w E _l sh', '뭸'],
    u'\ubb79' : ['m _w E _l t_h', '뭹'],
    u'\ubb7a' : ['m _w E _l p_h', '뭺'],
    u'\ubb7b' : ['m _w E _l _h', '뭻'],
    u'\ubb7c' : ['m _w E m', '뭼'],
    u'\ubb7d' : ['m _w E p', '뭽'],
    u'\ubb7e' : ['m _w E p sh', '뭾'],
    u'\ubb7f' : ['m _w E sh', '뭿'],
    u'\ubb80' : ['m _w E s', '뮀'],
    u'\ubb81' : ['m _w E N', '뮁'],
    u'\ubb82' : ['m _w E tS', '뮂'],
    u'\ubb83' : ['m _w E tSh', '뮃'],
    u'\ubb84' : ['m _w E k_h', '뮄'],
    u'\ubb85' : ['m _w E t_h', '뮅'],
    u'\ubb86' : ['m _w E p_h', '뮆'],
    u'\ubb87' : ['m _w E _h', '뮇'],
    u'\ubb88' : ['m 2', '뮈'],
    u'\ubb89' : ['m 2 k', '뮉'],
    u'\ubb8a' : ['m 2 k_>', '뮊'],
    u'\ubb8b' : ['m 2 k sh', '뮋'],
    u'\ubb8c' : ['m 2 _n', '뮌'],
    u'\ubb8d' : ['m 2 _n tS', '뮍'],
    u'\ubb8e' : ['m 2 _n _h', '뮎'],
    u'\ubb8f' : ['m 2 t', '뮏'],
    u'\ubb90' : ['m 2 _l', '뮐'],
    u'\ubb91' : ['m 2 _l k', '뮑'],
    u'\ubb92' : ['m 2 _l m', '뮒'],
    u'\ubb93' : ['m 2 _l p', '뮓'],
    u'\ubb94' : ['m 2 _l sh', '뮔'],
    u'\ubb95' : ['m 2 _l t_h', '뮕'],
    u'\ubb96' : ['m 2 _l p_h', '뮖'],
    u'\ubb97' : ['m 2 _l _h', '뮗'],
    u'\ubb98' : ['m 2 m', '뮘'],
    u'\ubb99' : ['m 2 p', '뮙'],
    u'\ubb9a' : ['m 2 p sh', '뮚'],
    u'\ubb9b' : ['m 2 sh', '뮛'],
    u'\ubb9c' : ['m 2 s', '뮜'],
    u'\ubb9d' : ['m 2 N', '뮝'],
    u'\ubb9e' : ['m 2 tS', '뮞'],
    u'\ubb9f' : ['m 2 tSh', '뮟'],
    u'\ubba0' : ['m 2 k_h', '뮠'],
    u'\ubba1' : ['m 2 t_h', '뮡'],
    u'\ubba2' : ['m 2 p_h', '뮢'],
    u'\ubba3' : ['m 2 _h', '뮣'],
    u'\ubba4' : ['m _j u', '뮤'],
    u'\ubba5' : ['m _j u k', '뮥'],
    u'\ubba6' : ['m _j u k_>', '뮦'],
    u'\ubba7' : ['m _j u k sh', '뮧'],
    u'\ubba8' : ['m _j u _n', '뮨'],
    u'\ubba9' : ['m _j u _n tS', '뮩'],
    u'\ubbaa' : ['m _j u _n _h', '뮪'],
    u'\ubbab' : ['m _j u t', '뮫'],
    u'\ubbac' : ['m _j u _l', '뮬'],
    u'\ubbad' : ['m _j u _l k', '뮭'],
    u'\ubbae' : ['m _j u _l m', '뮮'],
    u'\ubbaf' : ['m _j u _l p', '뮯'],
    u'\ubbb0' : ['m _j u _l sh', '뮰'],
    u'\ubbb1' : ['m _j u _l t_h', '뮱'],
    u'\ubbb2' : ['m _j u _l p_h', '뮲'],
    u'\ubbb3' : ['m _j u _l _h', '뮳'],
    u'\ubbb4' : ['m _j u m', '뮴'],
    u'\ubbb5' : ['m _j u p', '뮵'],
    u'\ubbb6' : ['m _j u p sh', '뮶'],
    u'\ubbb7' : ['m _j u sh', '뮷'],
    u'\ubbb8' : ['m _j u s', '뮸'],
    u'\ubbb9' : ['m _j u N', '뮹'],
    u'\ubbba' : ['m _j u tS', '뮺'],
    u'\ubbbb' : ['m _j u tSh', '뮻'],
    u'\ubbbc' : ['m _j u k_h', '뮼'],
    u'\ubbbd' : ['m _j u t_h', '뮽'],
    u'\ubbbe' : ['m _j u p_h', '뮾'],
    u'\ubbbf' : ['m _j u _h', '뮿'],
    u'\ubbc0' : ['m M', '므'],
    u'\ubbc1' : ['m M k', '믁'],
    u'\ubbc2' : ['m M k_>', '믂'],
    u'\ubbc3' : ['m M k sh', '믃'],
    u'\ubbc4' : ['m M _n', '믄'],
    u'\ubbc5' : ['m M _n tS', '믅'],
    u'\ubbc6' : ['m M _n _h', '믆'],
    u'\ubbc7' : ['m M t', '믇'],
    u'\ubbc8' : ['m M _l', '믈'],
    u'\ubbc9' : ['m M _l k', '믉'],
    u'\ubbca' : ['m M _l m', '믊'],
    u'\ubbcb' : ['m M _l p', '믋'],
    u'\ubbcc' : ['m M _l sh', '믌'],
    u'\ubbcd' : ['m M _l t_h', '믍'],
    u'\ubbce' : ['m M _l p_h', '믎'],
    u'\ubbcf' : ['m M _l _h', '믏'],
    u'\ubbd0' : ['m M m', '믐'],
    u'\ubbd1' : ['m M p', '믑'],
    u'\ubbd2' : ['m M p sh', '믒'],
    u'\ubbd3' : ['m M sh', '믓'],
    u'\ubbd4' : ['m M s', '믔'],
    u'\ubbd5' : ['m M N', '믕'],
    u'\ubbd6' : ['m M tS', '믖'],
    u'\ubbd7' : ['m M tSh', '믗'],
    u'\ubbd8' : ['m M k_h', '믘'],
    u'\ubbd9' : ['m M t_h', '믙'],
    u'\ubbda' : ['m M p_h', '믚'],
    u'\ubbdb' : ['m M _h', '믛'],
    u'\ubbdc' : ['m M _j', '믜'],
    u'\ubbdd' : ['m M _j k', '믝'],
    u'\ubbde' : ['m M _j k_>', '믞'],
    u'\ubbdf' : ['m M _j k sh', '믟'],
    u'\ubbe0' : ['m M _j _n', '믠'],
    u'\ubbe1' : ['m M _j _n tS', '믡'],
    u'\ubbe2' : ['m M _j _n _h', '믢'],
    u'\ubbe3' : ['m M _j t', '믣'],
    u'\ubbe4' : ['m M _j _l', '믤'],
    u'\ubbe5' : ['m M _j _l k', '믥'],
    u'\ubbe6' : ['m M _j _l m', '믦'],
    u'\ubbe7' : ['m M _j _l p', '믧'],
    u'\ubbe8' : ['m M _j _l sh', '믨'],
    u'\ubbe9' : ['m M _j _l t_h', '믩'],
    u'\ubbea' : ['m M _j _l p_h', '믪'],
    u'\ubbeb' : ['m M _j _l _h', '믫'],
    u'\ubbec' : ['m M _j m', '믬'],
    u'\ubbed' : ['m M _j p', '믭'],
    u'\ubbee' : ['m M _j p sh', '믮'],
    u'\ubbef' : ['m M _j sh', '믯'],
    u'\ubbf0' : ['m M _j s', '믰'],
    u'\ubbf1' : ['m M _j N', '믱'],
    u'\ubbf2' : ['m M _j tS', '믲'],
    u'\ubbf3' : ['m M _j tSh', '믳'],
    u'\ubbf4' : ['m M _j k_h', '믴'],
    u'\ubbf5' : ['m M _j t_h', '믵'],
    u'\ubbf6' : ['m M _j p_h', '믶'],
    u'\ubbf7' : ['m M _j _h', '믷'],
    u'\ubbf8' : ['m i', '미'],
    u'\ubbf9' : ['m i k', '믹'],
    u'\ubbfa' : ['m i k_>', '믺'],
    u'\ubbfb' : ['m i k sh', '믻'],
    u'\ubbfc' : ['m i _n', '민'],
    u'\ubbfd' : ['m i _n tS', '믽'],
    u'\ubbfe' : ['m i _n _h', '믾'],
    u'\ubbff' : ['m i t', '믿'],
    u'\ubc00' : ['m i _l', '밀'],
    u'\ubc01' : ['m i _l k', '밁'],
    u'\ubc02' : ['m i _l m', '밂'],
    u'\ubc03' : ['m i _l p', '밃'],
    u'\ubc04' : ['m i _l sh', '밄'],
    u'\ubc05' : ['m i _l t_h', '밅'],
    u'\ubc06' : ['m i _l p_h', '밆'],
    u'\ubc07' : ['m i _l _h', '밇'],
    u'\ubc08' : ['m i m', '밈'],
    u'\ubc09' : ['m i p', '밉'],
    u'\ubc0a' : ['m i p sh', '밊'],
    u'\ubc0b' : ['m i sh', '밋'],
    u'\ubc0c' : ['m i s', '밌'],
    u'\ubc0d' : ['m i N', '밍'],
    u'\ubc0e' : ['m i tS', '밎'],
    u'\ubc0f' : ['m i tSh', '및'],
    u'\ubc10' : ['m i k_h', '밐'],
    u'\ubc11' : ['m i t_h', '밑'],
    u'\ubc12' : ['m i p_h', '밒'],
    u'\ubc13' : ['m i _h', '밓'],
    u'\ubc14' : ['p a', '바'],
    u'\ubc15' : ['p a k', '박'],
    u'\ubc16' : ['p a k_>', '밖'],
    u'\ubc17' : ['p a k sh', '밗'],
    u'\ubc18' : ['p a _n', '반'],
    u'\ubc19' : ['p a _n tS', '밙'],
    u'\ubc1a' : ['p a _n _h', '밚'],
    u'\ubc1b' : ['p a t', '받'],
    u'\ubc1c' : ['p a _l', '발'],
    u'\ubc1d' : ['p a _l k', '밝'],
    u'\ubc1e' : ['p a _l m', '밞'],
    u'\ubc1f' : ['p a _l p', '밟'],
    u'\ubc20' : ['p a _l sh', '밠'],
    u'\ubc21' : ['p a _l t_h', '밡'],
    u'\ubc22' : ['p a _l p_h', '밢'],
    u'\ubc23' : ['p a _l _h', '밣'],
    u'\ubc24' : ['p a m', '밤'],
    u'\ubc25' : ['p a p', '밥'],
    u'\ubc26' : ['p a p sh', '밦'],
    u'\ubc27' : ['p a sh', '밧'],
    u'\ubc28' : ['p a s', '밨'],
    u'\ubc29' : ['p a N', '방'],
    u'\ubc2a' : ['p a tS', '밪'],
    u'\ubc2b' : ['p a tSh', '밫'],
    u'\ubc2c' : ['p a k_h', '밬'],
    u'\ubc2d' : ['p a t_h', '밭'],
    u'\ubc2e' : ['p a p_h', '밮'],
    u'\ubc2f' : ['p a _h', '밯'],
    u'\ubc30' : ['p {', '배'],
    u'\ubc31' : ['p { k', '백'],
    u'\ubc32' : ['p { k_>', '밲'],
    u'\ubc33' : ['p { k sh', '밳'],
    u'\ubc34' : ['p { _n', '밴'],
    u'\ubc35' : ['p { _n tS', '밵'],
    u'\ubc36' : ['p { _n _h', '밶'],
    u'\ubc37' : ['p { t', '밷'],
    u'\ubc38' : ['p { _l', '밸'],
    u'\ubc39' : ['p { _l k', '밹'],
    u'\ubc3a' : ['p { _l m', '밺'],
    u'\ubc3b' : ['p { _l p', '밻'],
    u'\ubc3c' : ['p { _l sh', '밼'],
    u'\ubc3d' : ['p { _l t_h', '밽'],
    u'\ubc3e' : ['p { _l p_h', '밾'],
    u'\ubc3f' : ['p { _l _h', '밿'],
    u'\ubc40' : ['p { m', '뱀'],
    u'\ubc41' : ['p { p', '뱁'],
    u'\ubc42' : ['p { p sh', '뱂'],
    u'\ubc43' : ['p { sh', '뱃'],
    u'\ubc44' : ['p { s', '뱄'],
    u'\ubc45' : ['p { N', '뱅'],
    u'\ubc46' : ['p { tS', '뱆'],
    u'\ubc47' : ['p { tSh', '뱇'],
    u'\ubc48' : ['p { k_h', '뱈'],
    u'\ubc49' : ['p { t_h', '뱉'],
    u'\ubc4a' : ['p { p_h', '뱊'],
    u'\ubc4b' : ['p { _h', '뱋'],
    u'\ubc4c' : ['p _j a', '뱌'],
    u'\ubc4d' : ['p _j a k', '뱍'],
    u'\ubc4e' : ['p _j a k_>', '뱎'],
    u'\ubc4f' : ['p _j a k sh', '뱏'],
    u'\ubc50' : ['p _j a _n', '뱐'],
    u'\ubc51' : ['p _j a _n tS', '뱑'],
    u'\ubc52' : ['p _j a _n _h', '뱒'],
    u'\ubc53' : ['p _j a t', '뱓'],
    u'\ubc54' : ['p _j a _l', '뱔'],
    u'\ubc55' : ['p _j a _l k', '뱕'],
    u'\ubc56' : ['p _j a _l m', '뱖'],
    u'\ubc57' : ['p _j a _l p', '뱗'],
    u'\ubc58' : ['p _j a _l sh', '뱘'],
    u'\ubc59' : ['p _j a _l t_h', '뱙'],
    u'\ubc5a' : ['p _j a _l p_h', '뱚'],
    u'\ubc5b' : ['p _j a lh', '뱛'],
    u'\ubc5c' : ['p _j a m', '뱜'],
    u'\ubc5d' : ['p _j a p', '뱝'],
    u'\ubc5e' : ['p _j a p sh', '뱞'],
    u'\ubc5f' : ['p _j a sh', '뱟'],
    u'\ubc60' : ['p _j a s', '뱠'],
    u'\ubc61' : ['p _j a N', '뱡'],
    u'\ubc62' : ['p _j a tS', '뱢'],
    u'\ubc63' : ['p _j a tSh', '뱣'],
    u'\ubc64' : ['p _j a k_h', '뱤'],
    u'\ubc65' : ['p _j a t_h', '뱥'],
    u'\ubc66' : ['p _j a p_h', '뱦'],
    u'\ubc67' : ['p _j a _h', '뱧'],
    u'\ubc68' : ['p _j {', '뱨'],
    u'\ubc69' : ['p _j { k', '뱩'],
    u'\ubc6a' : ['p _j { k_>', '뱪'],
    u'\ubc6b' : ['p _j { k sh', '뱫'],
    u'\ubc6c' : ['p _j { _n', '뱬'],
    u'\ubc6d' : ['p _j { _n tS', '뱭'],
    u'\ubc6e' : ['p _j { _n _h', '뱮'],
    u'\ubc6f' : ['p _j { t', '뱯'],
    u'\ubc70' : ['p _j { _l', '뱰'],
    u'\ubc71' : ['p _j { _l k', '뱱'],
    u'\ubc72' : ['p _j { _l m', '뱲'],
    u'\ubc73' : ['p _j { _l p', '뱳'],
    u'\ubc74' : ['p _j { _l sh', '뱴'],
    u'\ubc75' : ['p _j { _l t_h', '뱵'],
    u'\ubc76' : ['p _j { _l p_h', '뱶'],
    u'\ubc77' : ['p _j { _l _h', '뱷'],
    u'\ubc78' : ['p _j { m', '뱸'],
    u'\ubc79' : ['p _j { p', '뱹'],
    u'\ubc7a' : ['p _j { p sh', '뱺'],
    u'\ubc7b' : ['p _j { sh', '뱻'],
    u'\ubc7c' : ['p _j { s', '뱼'],
    u'\ubc7d' : ['p _j { N', '뱽'],
    u'\ubc7e' : ['p _j { tS', '뱾'],
    u'\ubc7f' : ['p _j { tSh', '뱿'],
    u'\ubc80' : ['p _j { k_h', '벀'],
    u'\ubc81' : ['p _j { t_h', '벁'],
    u'\ubc82' : ['p _j { p_h', '벂'],
    u'\ubc83' : ['p _j { _h', '벃'],
    u'\ubc84' : ['p _r', '버'],
    u'\ubc85' : ['p _r k', '벅'],
    u'\ubc86' : ['p _r k_>', '벆'],
    u'\ubc87' : ['p _r k sh', '벇'],
    u'\ubc88' : ['p _r _n', '번'],
    u'\ubc89' : ['p _r _n tS', '벉'],
    u'\ubc8a' : ['p _r _n _h', '벊'],
    u'\ubc8b' : ['p _r t', '벋'],
    u'\ubc8c' : ['p _r _l', '벌'],
    u'\ubc8d' : ['p _r _l k', '벍'],
    u'\ubc8e' : ['p _r _l m', '벎'],
    u'\ubc8f' : ['p _r _l p', '벏'],
    u'\ubc90' : ['p _r _l sh', '벐'],
    u'\ubc91' : ['p _r _l t_h', '벑'],
    u'\ubc92' : ['p _r _l p_h', '벒'],
    u'\ubc93' : ['p _r _l _h', '벓'],
    u'\ubc94' : ['p _r m', '범'],
    u'\ubc95' : ['p _r p', '법'],
    u'\ubc96' : ['p _r p sh', '벖'],
    u'\ubc97' : ['p _r sh', '벗'],
    u'\ubc98' : ['p _r s', '벘'],
    u'\ubc99' : ['p _r N', '벙'],
    u'\ubc9a' : ['p _r tS', '벚'],
    u'\ubc9b' : ['p _r tSh', '벛'],
    u'\ubc9c' : ['p _r k_h', '벜'],
    u'\ubc9d' : ['p _r t_h', '벝'],
    u'\ubc9e' : ['p _r p_h', '벞'],
    u'\ubc9f' : ['p _r _h', '벟'],
    u'\ubca0' : ['p e', '베'],
    u'\ubca1' : ['p e k', '벡'],
    u'\ubca2' : ['p e k_>', '벢'],
    u'\ubca3' : ['p e k sh', '벣'],
    u'\ubca4' : ['p e _n', '벤'],
    u'\ubca5' : ['p e _n tS', '벥'],
    u'\ubca6' : ['p e _n _h', '벦'],
    u'\ubca7' : ['p e t', '벧'],
    u'\ubca8' : ['p e _l', '벨'],
    u'\ubca9' : ['p e _l k', '벩'],
    u'\ubcaa' : ['p e _l m', '벪'],
    u'\ubcab' : ['p e _l p', '벫'],
    u'\ubcac' : ['p e _l sh', '벬'],
    u'\ubcad' : ['p e _l t_h', '벭'],
    u'\ubcae' : ['p e _l p_h', '벮'],
    u'\ubcaf' : ['p e _l _h', '벯'],
    u'\ubcb0' : ['p e m', '벰'],
    u'\ubcb1' : ['p e p', '벱'],
    u'\ubcb2' : ['p e p sh', '벲'],
    u'\ubcb3' : ['p e sh', '벳'],
    u'\ubcb4' : ['p e s', '벴'],
    u'\ubcb5' : ['p e N', '벵'],
    u'\ubcb6' : ['p e tS', '벶'],
    u'\ubcb7' : ['p e tSh', '벷'],
    u'\ubcb8' : ['p e k_h', '벸'],
    u'\ubcb9' : ['p e t_h', '벹'],
    u'\ubcba' : ['p e p_h', '벺'],
    u'\ubcbb' : ['p e _h', '벻'],
    u'\ubcbc' : ['p _j _r', '벼'],
    u'\ubcbd' : ['p _j _r k', '벽'],
    u'\ubcbe' : ['p _j _r k_>', '벾'],
    u'\ubcbf' : ['p _j _r k sh', '벿'],
    u'\ubcc0' : ['p _j _r _n', '변'],
    u'\ubcc1' : ['p _j _r _n tS', '볁'],
    u'\ubcc2' : ['p _j _r _n _h', '볂'],
    u'\ubcc3' : ['p _j _r t', '볃'],
    u'\ubcc4' : ['p _j _r _l', '별'],
    u'\ubcc5' : ['p _j _r _l k', '볅'],
    u'\ubcc6' : ['p _j _r _l m', '볆'],
    u'\ubcc7' : ['p _j _r _l p', '볇'],
    u'\ubcc8' : ['p _j _r _l sh', '볈'],
    u'\ubcc9' : ['p _j _r _l t_h', '볉'],
    u'\ubcca' : ['p _j _r _l p_h', '볊'],
    u'\ubccb' : ['p _j _r _l _h', '볋'],
    u'\ubccc' : ['p _j _r m', '볌'],
    u'\ubccd' : ['p _j _r p', '볍'],
    u'\ubcce' : ['p _j _r p sh', '볎'],
    u'\ubccf' : ['p _j _r sh', '볏'],
    u'\ubcd0' : ['p _j _r s', '볐'],
    u'\ubcd1' : ['p _j _r N', '병'],
    u'\ubcd2' : ['p _j _r tS', '볒'],
    u'\ubcd3' : ['p _j _r tSh', '볓'],
    u'\ubcd4' : ['p _j _r k_h', '볔'],
    u'\ubcd5' : ['p _j _r t_h', '볕'],
    u'\ubcd6' : ['p _j _r p_h', '볖'],
    u'\ubcd7' : ['p _j _r _h', '볗'],
    u'\ubcd8' : ['p _j e', '볘'],
    u'\ubcd9' : ['p _j e k', '볙'],
    u'\ubcda' : ['p _j e k_>', '볚'],
    u'\ubcdb' : ['p _j e k sh', '볛'],
    u'\ubcdc' : ['p _j e _n', '볜'],
    u'\ubcdd' : ['p _j e _n tS', '볝'],
    u'\ubcde' : ['p _j e _n _h', '볞'],
    u'\ubcdf' : ['p _j e t', '볟'],
    u'\ubce0' : ['p _j e _l', '볠'],
    u'\ubce1' : ['p _j e _l k', '볡'],
    u'\ubce2' : ['p _j e _l m', '볢'],
    u'\ubce3' : ['p _j e _l p', '볣'],
    u'\ubce4' : ['p _j e _l sh', '볤'],
    u'\ubce5' : ['p _j e _l t_h', '볥'],
    u'\ubce6' : ['p _j e _l p_h', '볦'],
    u'\ubce7' : ['p _j e _l _h', '볧'],
    u'\ubce8' : ['p _j e m', '볨'],
    u'\ubce9' : ['p _j e p', '볩'],
    u'\ubcea' : ['p _j e p sh', '볪'],
    u'\ubceb' : ['p _j e sh', '볫'],
    u'\ubcec' : ['p _j e s', '볬'],
    u'\ubced' : ['p _j e N', '볭'],
    u'\ubcee' : ['p _j e tS', '볮'],
    u'\ubcef' : ['p _j e tSh', '볯'],
    u'\ubcf0' : ['p _j e k_h', '볰'],
    u'\ubcf1' : ['p _j e t_h', '볱'],
    u'\ubcf2' : ['p _j e p_h', '볲'],
    u'\ubcf3' : ['p _j e _h', '볳'],
    u'\ubcf4' : ['p o', '보'],
    u'\ubcf5' : ['p o k', '복'],
    u'\ubcf6' : ['p o k_>', '볶'],
    u'\ubcf7' : ['p o k sh', '볷'],
    u'\ubcf8' : ['p o _n', '본'],
    u'\ubcf9' : ['p o _n tS', '볹'],
    u'\ubcfa' : ['p o _n _h', '볺'],
    u'\ubcfb' : ['p o t', '볻'],
    u'\ubcfc' : ['p o _l', '볼'],
    u'\ubcfd' : ['p o _l k', '볽'],
    u'\ubcfe' : ['p o _l m', '볾'],
    u'\ubcff' : ['p o _l p', '볿'],
    u'\ubd00' : ['p o _l sh', '봀'],
    u'\ubd01' : ['p o _l t_h', '봁'],
    u'\ubd02' : ['p o _l p_h', '봂'],
    u'\ubd03' : ['p o _l _h', '봃'],
    u'\ubd04' : ['p o m', '봄'],
    u'\ubd05' : ['p o p', '봅'],
    u'\ubd06' : ['p o p sh', '봆'],
    u'\ubd07' : ['p o sh', '봇'],
    u'\ubd08' : ['p o s', '봈'],
    u'\ubd09' : ['p o N', '봉'],
    u'\ubd0a' : ['p o tS', '봊'],
    u'\ubd0b' : ['p o tSh', '봋'],
    u'\ubd0c' : ['p o k_h', '봌'],
    u'\ubd0d' : ['p o t_h', '봍'],
    u'\ubd0e' : ['p o p_h', '봎'],
    u'\ubd0f' : ['p o _h', '봏'],
    u'\ubd10' : ['p _w a', '봐'],
    u'\ubd11' : ['p _w a k', '봑'],
    u'\ubd12' : ['p _w a k_>', '봒'],
    u'\ubd13' : ['p _w a k sh', '봓'],
    u'\ubd14' : ['p _w a _n', '봔'],
    u'\ubd15' : ['p _w a _n tS', '봕'],
    u'\ubd16' : ['p _w a nh', '봖'],
    u'\ubd17' : ['p _w a t', '봗'],
    u'\ubd18' : ['p _w a _l', '봘'],
    u'\ubd19' : ['p _w a _l k', '봙'],
    u'\ubd1a' : ['p _w a _l m', '봚'],
    u'\ubd1b' : ['p _w a _l p', '봛'],
    u'\ubd1c' : ['p _w a _l sh', '봜'],
    u'\ubd1d' : ['p _w a _l t_h', '봝'],
    u'\ubd1e' : ['p _w a _l p_h', '봞'],
    u'\ubd1f' : ['p _w a _l _h', '봟'],
    u'\ubd20' : ['p _w a m', '봠'],
    u'\ubd21' : ['p _w a p', '봡'],
    u'\ubd22' : ['p _w a p sh', '봢'],
    u'\ubd23' : ['p _w a sh', '봣'],
    u'\ubd24' : ['p _w a s', '봤'],
    u'\ubd25 ' : ['p _w a N', '봥 '],
    u'\ubd26' : ['p _w a tS', '봦'],
    u'\ubd27' : ['p _w a tSh', '봧'],
    u'\ubd28' : ['p _w a k_h', '봨'],
    u'\ubd29' : ['p _w a t_h', '봩'],
    u'\ubd2a' : ['p _w a p_h', '봪'],
    u'\ubd2b' : ['p _w a _h', '봫'],
    u'\ubd2c' : ['p _w {', '봬'],
    u'\ubd2d' : ['p _w { k', '봭'],
    u'\ubd2e' : ['p _w { k_>', '봮'],
    u'\ubd2f' : ['p _w { k sh', '봯'],
    u'\ubd30' : ['p _w { _n', '봰'],
    u'\ubd31' : ['p _w { _n tS', '봱'],
    u'\ubd32' : ['p _w { _n _h', '봲'],
    u'\ubd33' : ['p _w { t', '봳'],
    u'\ubd34' : ['p _w { _l', '봴'],
    u'\ubd35' : ['p _w { _l k', '봵'],
    u'\ubd36' : ['p _w { _l m', '봶'],
    u'\ubd37' : ['p _w { _l p', '봷'],
    u'\ubd38' : ['p _w { _l sh', '봸'],
    u'\ubd39' : ['p _w { _l t_h', '봹'],
    u'\ubd3a' : ['p _w { _l p_h', '봺'],
    u'\ubd3b' : ['p _w { _l _h', '봻'],
    u'\ubd3c' : ['p _w { m', '봼'],
    u'\ubd3d' : ['p _w { p', '봽'],
    u'\ubd3e' : ['p _w { p sh', '봾'],
    u'\ubd3f' : ['p _w { sh', '봿'],
    u'\ubd40' : ['p _w { s', '뵀'],
    u'\ubd41' : ['p _w { N', '뵁'],
    u'\ubd42' : ['p _w { tS', '뵂'],
    u'\ubd43' : ['p _w { tSh', '뵃'],
    u'\ubd44' : ['p _w { k_h', '뵄'],
    u'\ubd45' : ['p _w { t_h', '뵅'],
    u'\ubd46' : ['p _w { p_h', '뵆'],
    u'\ubd47' : ['p _w { _h', '뵇'],
    u'\ubd48' : ['p _w e', '뵈'],
    u'\ubd49' : ['p _w e k', '뵉'],
    u'\ubd4a' : ['p _w e k_>', '뵊'],
    u'\ubd4b' : ['p _w e k sh', '뵋'],
    u'\ubd4c' : ['p _w e _n', '뵌'],
    u'\ubd4d' : ['p _w e _n tS', '뵍'],
    u'\ubd4e' : ['p _w e _n _h', '뵎'],
    u'\ubd4f' : ['p _w e t', '뵏'],
    u'\ubd50' : ['p _w e _l', '뵐'],
    u'\ubd51' : ['p _w e _l k', '뵑'],
    u'\ubd52' : ['p _w e _l m', '뵒'],
    u'\ubd53' : ['p _w e _l p', '뵓'],
    u'\ubd54' : ['p _w e _l sh', '뵔'],
    u'\ubd55' : ['p _w e _l t_h', '뵕'],
    u'\ubd56' : ['p _w e _l p_h', '뵖'],
    u'\ubd57' : ['p _w e _l _h', '뵗'],
    u'\ubd58' : ['p _w e m', '뵘'],
    u'\ubd59' : ['p _w e p', '뵙'],
    u'\ubd5a' : ['p _w e p sh', '뵚'],
    u'\ubd5b' : ['p _w e sh', '뵛'],
    u'\ubd5c' : ['p _w e s', '뵜'],
    u'\ubd5d' : ['p _w e N', '뵝'],
    u'\ubd5e' : ['p _w e tS', '뵞'],
    u'\ubd5f' : ['p _w e tSh', '뵟'],
    u'\ubd60' : ['p _w e k_h', '뵠'],
    u'\ubd61' : ['p _w e t_h', '뵡'],
    u'\ubd62' : ['p _w e p_h', '뵢'],
    u'\ubd63' : ['p _w e _h', '뵣'],
    u'\ubd64' : ['p _j o', '뵤'],
    u'\ubd65' : ['p _j o k', '뵥'],
    u'\ubd66' : ['p _j o k_>', '뵦'],
    u'\ubd67' : ['p _j o k sh', '뵧'],
    u'\ubd68' : ['p _j o _n', '뵨'],
    u'\ubd69' : ['p _j o _n tS', '뵩'],
    u'\ubd6a' : ['p _j o _n _h', '뵪'],
    u'\ubd6b' : ['p _j o t', '뵫'],
    u'\ubd6c' : ['p _j o _l', '뵬'],
    u'\ubd6d' : ['p _j o _l k', '뵭'],
    u'\ubd6e' : ['p _j o _l m', '뵮'],
    u'\ubd6f' : ['p _j o _l p', '뵯'],
    u'\ubd70' : ['p _j o _l sh', '뵰'],
    u'\ubd71' : ['p _j o _l t_h', '뵱'],
    u'\ubd72' : ['p _j o _l p_h', '뵲'],
    u'\ubd73' : ['p _j o _l _h', '뵳'],
    u'\ubd74' : ['p _j o m', '뵴'],
    u'\ubd75' : ['p _j o p', '뵵'],
    u'\ubd76' : ['p _j o p sh', '뵶'],
    u'\ubd77' : ['p _j o sh', '뵷'],
    u'\ubd78' : ['p _j o s', '뵸'],
    u'\ubd79' : ['p _j o N', '뵹'],
    u'\ubd7a' : ['p _j o tS', '뵺'],
    u'\ubd7b' : ['p _j o tSh', '뵻'],
    u'\ubd7c' : ['p _j o k_h', '뵼'],
    u'\ubd7d' : ['p _j o t_h', '뵽'],
    u'\ubd7e' : ['p _j o p_h', '뵾'],
    u'\ubd7f' : ['p _j o _h', '뵿'],
    u'\ubd80' : ['p u', '부'],
    u'\ubd81' : ['p u k', '북'],
    u'\ubd82' : ['p u k_>', '붂'],
    u'\ubd83' : ['p u k sh', '붃'],
    u'\ubd84' : ['p u _n', '분'],
    u'\ubd85' : ['p u _n tS', '붅'],
    u'\ubd86' : ['p u _n _h', '붆'],
    u'\ubd87' : ['p u t', '붇'],
    u'\ubd88' : ['p u _l', '불'],
    u'\ubd89' : ['p u _l k', '붉'],
    u'\ubd8a' : ['p u _l m', '붊'],
    u'\ubd8b' : ['p u _l p', '붋'],
    u'\ubd8c' : ['p u _l sh', '붌'],
    u'\ubd8d' : ['p u _l t_h', '붍'],
    u'\ubd8e' : ['p u _l p_h', '붎'],
    u'\ubd8f' : ['p u _l _h', '붏'],
    u'\ubd90' : ['p u m', '붐'],
    u'\ubd91' : ['p u p', '붑'],
    u'\ubd92' : ['p u p sh', '붒'],
    u'\ubd93' : ['p u sh', '붓'],
    u'\ubd94' : ['p u s', '붔'],
    u'\ubd95' : ['p u N', '붕'],
    u'\ubd96' : ['p u tS', '붖'],
    u'\ubd97' : ['p u tSh', '붗'],
    u'\ubd98' : ['p u k_h', '붘'],
    u'\ubd99' : ['p u t_h', '붙'],
    u'\ubd9a' : ['p u p_h', '붚'],
    u'\ubd9b' : ['p u _h', '붛'],
    u'\ubd9c' : ['p _w _r', '붜'],
    u'\ubd9d' : ['p _w _r k', '붝'],
    u'\ubd9e' : ['p _w _r k_>', '붞'],
    u'\ubd9f' : ['p _w _r k sh', '붟'],
    u'\ubda0' : ['p _w _r _n', '붠'],
    u'\ubda1' : ['p _w _r _n tS', '붡'],
    u'\ubda2' : ['p _w _r _n _h', '붢'],
    u'\ubda3' : ['p _w _r t', '붣'],
    u'\ubda4' : ['p _w _r _l', '붤'],
    u'\ubda5' : ['p _w _r _l k', '붥'],
    u'\ubda6' : ['p _w _r _l m', '붦'],
    u'\ubda7' : ['p _w _r _l p', '붧'],
    u'\ubda8' : ['p _w _r _l sh', '붨'],
    u'\ubda9' : ['p _w _r _l t_h', '붩'],
    u'\ubdaa' : ['p _w _r _l p_h', '붪'],
    u'\ubdab' : ['p _w _r _l _h', '붫'],
    u'\ubdac' : ['p _w _r m', '붬'],
    u'\ubdad' : ['p _w _r p', '붭'],
    u'\ubdae' : ['p _w _r p sh', '붮'],
    u'\ubdaf' : ['p _w _r sh', '붯'],
    u'\ubdb0' : ['p _w _r s', '붰'],
    u'\ubdb1' : ['p _w _r N', '붱'],
    u'\ubdb2' : ['p _w _r tS', '붲'],
    u'\ubdb3' : ['p _w _r tSh', '붳'],
    u'\ubdb4' : ['p _w _r k_h', '붴'],
    u'\ubdb5' : ['p _w _r t_h', '붵'],
    u'\ubdb6' : ['p _w _r p_h', '붶'],
    u'\ubdb7' : ['p _w _r _h', '붷'],
    u'\ubdb8' : ['p _w E', '붸'],
    u'\ubdb9' : ['p _w E k', '붹'],
    u'\ubdba' : ['p _w E k_>', '붺'],
    u'\ubdbb' : ['p _w E k sh', '붻'],
    u'\ubdbc' : ['p _w E _n', '붼'],
    u'\ubdbd' : ['p _w E _n tS', '붽'],
    u'\ubdbe' : ['p _w E _n _h', '붾'],
    u'\ubdbf' : ['p _w E t', '붿'],
    u'\ubdc0' : ['p _w E _l', '뷀'],
    u'\ubdc1' : ['p _w E _l k', '뷁'],
    u'\ubdc2' : ['p _w E _l m', '뷂'],
    u'\ubdc3' : ['p _w E _l p', '뷃'],
    u'\ubdc4' : ['p _w E _l sh', '뷄'],
    u'\ubdc5' : ['p _w E _l t_h', '뷅'],
    u'\ubdc6' : ['p _w E _l p_h', '뷆'],
    u'\ubdc7' : ['p _w E _l _h', '뷇'],
    u'\ubdc8' : ['p _w E m', '뷈'],
    u'\ubdc9' : ['p _w E p', '뷉'],
    u'\ubdca' : ['p _w E p sh', '뷊'],
    u'\ubdcb' : ['p _w E sh', '뷋'],
    u'\ubdcc' : ['p _w E s', '뷌'],
    u'\ubdcd' : ['p _w E N', '뷍'],
    u'\ubdce' : ['p _w E tS', '뷎'],
    u'\ubdcf' : ['p _w E tSh', '뷏'],
    u'\ubdd0' : ['p _w E k_h', '뷐'],
    u'\ubdd1' : ['p _w E t_h', '뷑'],
    u'\ubdd2' : ['p _w E p_h', '뷒'],
    u'\ubdd3' : ['p _w E _h', '뷓'],
    u'\ubdd4' : ['p 2', '뷔'],
    u'\ubdd5' : ['p 2 k', '뷕'],
    u'\ubdd6' : ['p 2 k_>', '뷖'],
    u'\ubdd7' : ['p 2 k sh', '뷗'],
    u'\ubdd8' : ['p 2 _n', '뷘'],
    u'\ubdd9' : ['p 2 _n tS', '뷙'],
    u'\ubdda' : ['p 2 _n _h', '뷚'],
    u'\ubddb' : ['p 2 t', '뷛'],
    u'\ubddc' : ['p 2 _l', '뷜'],
    u'\ubddd' : ['p 2 _l k', '뷝'],
    u'\ubdde' : ['p 2 _l m', '뷞'],
    u'\ubddf' : ['p 2 _l p', '뷟'],
    u'\ubde0' : ['p 2 _l sh', '뷠'],
    u'\ubde1' : ['p 2 _l t_h', '뷡'],
    u'\ubde2' : ['p 2 _l p_h', '뷢'],
    u'\ubde3' : ['p 2 _l s', '뷣'],
    u'\ubde4' : ['p 2 m', '뷤'],
    u'\ubde5' : ['p 2 p', '뷥'],
    u'\ubde6' : ['p 2 p sh', '뷦'],
    u'\ubde7' : ['p 2 sh', '뷧'],
    u'\ubde8' : ['p 2 s', '뷨'],
    u'\ubde9' : ['p 2 N', '뷩'],
    u'\ubdea' : ['p 2 tS', '뷪'],
    u'\ubdeb' : ['p 2 tSh', '뷫'],
    u'\ubdec' : ['p 2 k_h', '뷬'],
    u'\ubded' : ['p 2 t_h', '뷭'],
    u'\ubdee' : ['p 2 p_h', '뷮'],
    u'\ubdef' : ['p 2 _h', '뷯'],
    u'\ubdf0' : ['p _j u', '뷰'],
    u'\ubdf1' : ['p _j u k', '뷱'],
    u'\ubdf2' : ['p _j u k_>', '뷲'],
    u'\ubdf3' : ['p _j u k sh', '뷳'],
    u'\ubdf4' : ['p _j u _n', '뷴'],
    u'\ubdf5' : ['p _j u _n tS', '뷵'],
    u'\ubdf6' : ['p _j u _n _h', '뷶'],
    u'\ubdf7' : ['p _j u t', '뷷'],
    u'\ubdf8' : ['p _j u _l', '뷸'],
    u'\ubdf9' : ['p _j u _l k', '뷹'],
    u'\ubdfa' : ['p _j u _l m', '뷺'],
    u'\ubdfb' : ['p _j u _l p', '뷻'],
    u'\ubdfc' : ['p _j u _l sh', '뷼'],
    u'\ubdfd' : ['p _j u _l t_h', '뷽'],
    u'\ubdfe' : ['p _j u _l p_h', '뷾'],
    u'\ubdff' : ['p _j u _l _h', '뷿'],
    u'\ube00' : ['p _j u m', '븀'],
    u'\ube01' : ['p _j u p', '븁'],
    u'\ube02' : ['p _j u p sh', '븂'],
    u'\ube03' : ['p _j u sh', '븃'],
    u'\ube04' : ['p _j u s', '븄'],
    u'\ube05' : ['p _j u N', '븅'],
    u'\ube06' : ['p _j u tS', '븆'],
    u'\ube07' : ['p _j u tSh', '븇'],
    u'\ube08' : ['p _j u k_h', '븈'],
    u'\ube09' : ['p _j u t_h', '븉'],
    u'\ube0a' : ['p _j u p_h', '븊'],
    u'\ube0b' : ['p _j u _h', '븋'],
    u'\ube0c' : ['p M', '브'],
    u'\ube0d' : ['p M k', '븍'],
    u'\ube0e' : ['p M k_>', '븎'],
    u'\ube0f' : ['p M k sh', '븏'],
    u'\ube10' : ['p M _n', '븐'],
    u'\ube11' : ['p M _n tS', '븑'],
    u'\ube12' : ['p M _n _h', '븒'],
    u'\ube13' : ['p M t', '븓'],
    u'\ube14' : ['p M _l', '블'],
    u'\ube15' : ['p M _l k', '븕'],
    u'\ube16' : ['p M _l m', '븖'],
    u'\ube17' : ['p M _l p', '븗'],
    u'\ube18' : ['p M _l sh', '븘'],
    u'\ube19' : ['p M _l t_h', '븙'],
    u'\ube1a' : ['p M _l p_h', '븚'],
    u'\ube1b' : ['p M _l _h', '븛'],
    u'\ube1c' : ['p M m', '븜'],
    u'\ube1d' : ['p M p', '븝'],
    u'\ube1e' : ['p M p sh', '븞'],
    u'\ube1f' : ['p M sh', '븟'],
    u'\ube20' : ['p M s', '븠'],
    u'\ube21' : ['p M N', '븡'],
    u'\ube22' : ['p M tS', '븢'],
    u'\ube23' : ['p M tSh', '븣'],
    u'\ube24' : ['p M k_h', '븤'],
    u'\ube25' : ['p M t_h', '븥'],
    u'\ube26' : ['p M p_h', '븦'],
    u'\ube27' : ['p M _h', '븧'],
    u'\ube28' : ['p M _j', '븨'],
    u'\ube29' : ['p M _j k', '븩'],
    u'\ube2a' : ['p M _j k_>', '븪'],
    u'\ube2b' : ['p M _j k sh', '븫'],
    u'\ube2c' : ['p M _j _n', '븬'],
    u'\ube2d' : ['p M _j _n tS', '븭'],
    u'\ube2e' : ['p M _j _n _h', '븮'],
    u'\ube2f' : ['p M _j t', '븯'],
    u'\ube30' : ['p M _j _l', '븰'],
    u'\ube31' : ['p M _j _l k', '븱'],
    u'\ube32' : ['p M _j _l m', '븲'],
    u'\ube33' : ['p M _j _l p', '븳'],
    u'\ube34' : ['p M _j _l sh', '븴'],
    u'\ube35' : ['p M _j _l t_h', '븵'],
    u'\ube36' : ['p M _j _l p_h', '븶'],
    u'\ube37' : ['p M _j _l _h', '븷'],
    u'\ube38' : ['p M _j m', '븸'],
    u'\ube39' : ['p M _j p', '븹'],
    u'\ube3a' : ['p M _j p sh', '븺'],
    u'\ube3b' : ['p M _j sh', '븻'],
    u'\ube3c' : ['p M _j s', '븼'],
    u'\ube3d' : ['p M _j N', '븽'],
    u'\ube3e' : ['p M _j tS', '븾'],
    u'\ube3f' : ['p M _j tSh', '븿'],
    u'\ube40' : ['p M _j k_h', '빀'],
    u'\ube41' : ['p M _j t_h', '빁'],
    u'\ube42' : ['p M _j p_h', '빂'],
    u'\ube43' : ['p M _j _h', '빃'],
    u'\ube44' : ['p i', '비'],
    u'\ube45' : ['p i k', '빅'],
    u'\ube46' : ['p i k_>', '빆'],
    u'\ube47' : ['p i k sh', '빇'],
    u'\ube48' : ['p i _n', '빈'],
    u'\ube49' : ['p i _n tS', '빉'],
    u'\ube4a' : ['p i _n _h', '빊'],
    u'\ube4b' : ['p i t', '빋'],
    u'\ube4c' : ['p i _l', '빌'],
    u'\ube4d' : ['p i _l k', '빍'],
    u'\ube4e' : ['p i _l m', '빎'],
    u'\ube4f' : ['p i _l p', '빏'],
    u'\ube50' : ['p i _l sh', '빐'],
    u'\ube51' : ['p i _l t_h', '빑'],
    u'\ube52' : ['p i _l p_h', '빒'],
    u'\ube53' : ['p i _l _h', '빓'],
    u'\ube54' : ['p i m', '빔'],
    u'\ube55' : ['p i p', '빕'],
    u'\ube56' : ['p i p sh', '빖'],
    u'\ube57' : ['p i sh', '빗'],
    u'\ube58' : ['p i s', '빘'],
    u'\ube59' : ['p i N', '빙'],
    u'\ube5a' : ['p i tS', '빚'],
    u'\ube5b' : ['p i tSh', '빛'],
    u'\ube5c' : ['p i k_h', '빜'],
    u'\ube5d' : ['p i t_h', '빝'],
    u'\ube5e' : ['p i p_h', '빞'],
    u'\ube5f' : ['p i _h', '빟'],
    u'\ube60' : ['p_> a', '빠'],
    u'\ube61' : ['p_> a k', '빡'],
    u'\ube62' : ['p_> a k_>', '빢'],
    u'\ube63' : ['p_> a k sh', '빣'],
    u'\ube64' : ['p_> a _n', '빤'],
    u'\ube65' : ['p_> a _n tS', '빥'],
    u'\ube66' : ['p_> a _n _h', '빦'],
    u'\ube67' : ['p_> a t', '빧'],
    u'\ube68' : ['p_> a _l', '빨'],
    u'\ube69' : ['p_> a _l k', '빩'],
    u'\ube6a' : ['p_> a _l m', '빪'],
    u'\ube6b' : ['p_> a _l p', '빫'],
    u'\ube6c' : ['p_> a _l sh', '빬'],
    u'\ube6d' : ['p_> a _l t_h', '빭'],
    u'\ube6e' : ['p_> a _l p_h', '빮'],
    u'\ube6f' : ['p_> a _l _h', '빯'],
    u'\ube70' : ['p_> a m', '빰'],
    u'\ube71' : ['p_> a p', '빱'],
    u'\ube72' : ['p_> a p sh', '빲'],
    u'\ube73' : ['p_> a sh', '빳'],
    u'\ube74' : ['p_> a s', '빴'],
    u'\ube75' : ['p_> a N', '빵'],
    u'\ube76' : ['p_> a tS', '빶'],
    u'\ube77' : ['p_> a tSh', '빷'],
    u'\ube78' : ['p_> a k_h', '빸'],
    u'\ube79' : ['p_> a t_h', '빹'],
    u'\ube7a' : ['p_> a p_h', '빺'],
    u'\ube7b' : ['p_> a _h', '빻'],
    u'\ube7c' : ['p_> {', '빼'],
    u'\ube7d' : ['p_> { k', '빽'],
    u'\ube7e' : ['p_> { k_>', '빾'],
    u'\ube7f' : ['p_> { k sh', '빿'],
    u'\ube80' : ['p_> { _n', '뺀'],
    u'\ube81' : ['p_> { _n tS', '뺁'],
    u'\ube82' : ['p_> { _n _h', '뺂'],
    u'\ube83' : ['p_> { t', '뺃'],
    u'\ube84' : ['p_> { _l', '뺄'],
    u'\ube85' : ['p_> { _l k', '뺅'],
    u'\ube86' : ['p_> { _l m', '뺆'],
    u'\ube87' : ['p_> { _l p', '뺇'],
    u'\ube88' : ['p_> { _l sh', '뺈'],
    u'\ube89' : ['p_> { _l t_h', '뺉'],
    u'\ube8a' : ['p_> { _l p_h', '뺊'],
    u'\ube8b' : ['p_> { _l _h', '뺋'],
    u'\ube8c' : ['p_> { m', '뺌'],
    u'\ube8d' : ['p_> { p', '뺍'],
    u'\ube8e' : ['p_> { p sh', '뺎'],
    u'\ube8f' : ['p_> { sh', '뺏'],
    u'\ube90' : ['p_> { s', '뺐'],
    u'\ube91' : ['p_> { N', '뺑'],
    u'\ube92' : ['p_> { tS', '뺒'],
    u'\ube93' : ['p_> { tSh', '뺓'],
    u'\ube94' : ['p_> { k_h', '뺔'],
    u'\ube95' : ['p_> { t_h', '뺕'],
    u'\ube96' : ['p_> { p_h', '뺖'],
    u'\ube97' : ['p_> { _h', '뺗'],
    u'\ube98' : ['p_> _j a', '뺘'],
    u'\ube99' : ['p_> _j a k', '뺙'],
    u'\ube9a' : ['p_> _j a k_>', '뺚'],
    u'\ube9b' : ['p_> _j a k sh', '뺛'],
    u'\ube9c' : ['p_> _j a _n', '뺜'],
    u'\ube9d' : ['p_> _j a _n tS', '뺝'],
    u'\ube9e' : ['p_> _j a _n _h', '뺞'],
    u'\ube9f' : ['p_> _j a t', '뺟'],
    u'\ubea0' : ['p_> _j a _l', '뺠'],
    u'\ubea1' : ['p_> _j a _l k', '뺡'],
    u'\ubea2' : ['p_> _j a _l m', '뺢'],
    u'\ubea3' : ['p_> _j a _l p', '뺣'],
    u'\ubea4' : ['p_> _j a _l sh', '뺤'],
    u'\ubea5' : ['p_> _j a _l t_h', '뺥'],
    u'\ubea6' : ['p_> _j a _l p_h', '뺦'],
    u'\ubea7' : ['p_> _j a _l _h', '뺧'],
    u'\ubea8' : ['p_> _j a m', '뺨'],
    u'\ubea9' : ['p_> _j a p', '뺩'],
    u'\ubeaa' : ['p_> _j a p sh', '뺪'],
    u'\ubeab' : ['p_> _j a sh', '뺫'],
    u'\ubeac' : ['p_> _j a s', '뺬'],
    u'\ubead' : ['p_> _j a N', '뺭'],
    u'\ubeae' : ['p_> _j a tS', '뺮'],
    u'\ubeaf' : ['p_> _j a tSh', '뺯'],
    u'\ubeb0' : ['p_> _j a k_h', '뺰'],
    u'\ubeb1' : ['p_> _j a t_h', '뺱'],
    u'\ubeb2' : ['p_> _j a p_h', '뺲'],
    u'\ubeb3' : ['p_> _j a _h', '뺳'],
    u'\ubeb4' : ['p_> _j {', '뺴'],
    u'\ubeb5' : ['p_> _j { k', '뺵'],
    u'\ubeb6' : ['p_> _j { k_>', '뺶'],
    u'\ubeb7' : ['p_> _j { k sh', '뺷'],
    u'\ubeb8' : ['p_> _j { _n', '뺸'],
    u'\ubeb9' : ['p_> _j { _n tS', '뺹'],
    u'\ubeba' : ['p_> _j { _n _h', '뺺'],
    u'\ubebb' : ['p_> _j { t', '뺻'],
    u'\ubebc' : ['p_> _j { _l', '뺼'],
    u'\ubebd' : ['p_> _j { _l k', '뺽'],
    u'\ubebe' : ['p_> _j { _l m', '뺾'],
    u'\ubebf' : ['p_> _j { _l p', '뺿'],
    u'\ubec0' : ['p_> _j { _l sh', '뻀'],
    u'\ubec1' : ['p_> _j { _l t_h', '뻁'],
    u'\ubec2' : ['p_> _j { _l p_h', '뻂'],
    u'\ubec3' : ['p_> _j { _l _h', '뻃'],
    u'\ubec4' : ['p_> _j { m', '뻄'],
    u'\ubec5' : ['p_> _j { p', '뻅'],
    u'\ubec6' : ['p_> _j { p sh', '뻆'],
    u'\ubec7' : ['p_> _j { sh', '뻇'],
    u'\ubec8' : ['p_> _j { s', '뻈'],
    u'\ubec9' : ['p_> _j { N', '뻉'],
    u'\ubeca' : ['p_> _j { tS', '뻊'],
    u'\ubecb' : ['p_> _j { tSh', '뻋'],
    u'\ubecc' : ['p_> _j { k_h', '뻌'],
    u'\ubecd' : ['p_> _j { t_h', '뻍'],
    u'\ubece' : ['p_> _j { p_h', '뻎'],
    u'\ubecf' : ['p_> _j { _h', '뻏'],
    u'\ubed0' : ['p_> _r', '뻐'],
    u'\ubed1' : ['p_> _r k', '뻑'],
    u'\ubed2' : ['p_> _r k_>', '뻒'],
    u'\ubed3' : ['p_> _r k sh', '뻓'],
    u'\ubed4' : ['p_> _r _n', '뻔'],
    u'\ubed5' : ['p_> _r _n tS', '뻕'],
    u'\ubed6' : ['p_> _r _n _h', '뻖'],
    u'\ubed7' : ['p_> _r t', '뻗'],
    u'\ubed8' : ['p_> _r _l', '뻘'],
    u'\ubed9' : ['p_> _r _l k', '뻙'],
    u'\ubeda' : ['p_> _r _l m', '뻚'],
    u'\ubedb' : ['p_> _r _l p', '뻛'],
    u'\ubedc' : ['p_> _r _l sh', '뻜'],
    u'\ubedd' : ['p_> _r _l t_h', '뻝'],
    u'\ubede' : ['p_> _r _l p_h', '뻞'],
    u'\ubedf' : ['p_> _r _l _h', '뻟'],
    u'\ubee0' : ['p_> _r m', '뻠'],
    u'\ubee1' : ['p_> _r p', '뻡'],
    u'\ubee2' : ['p_> _r p sh', '뻢'],
    u'\ubee3' : ['p_> _r sh', '뻣'],
    u'\ubee4' : ['p_> _r s', '뻤'],
    u'\ubee5' : ['p_> _r N', '뻥'],
    u'\ubee6' : ['p_> _r tS', '뻦'],
    u'\ubee7' : ['p_> _r tSh', '뻧'],
    u'\ubee8' : ['p_> _r k_h', '뻨'],
    u'\ubee9' : ['p_> _r t_h', '뻩'],
    u'\ubeea' : ['p_> _r p_h', '뻪'],
    u'\ubeeb' : ['p_> _r _h', '뻫'],
    u'\ubeec' : ['p_> e', '뻬'],
    u'\ubeed' : ['p_> e k', '뻭'],
    u'\ubeee' : ['p_> e k_>', '뻮'],
    u'\ubeef' : ['p_> e k sh', '뻯'],
    u'\ubef0' : ['p_> e _n', '뻰'],
    u'\ubef1' : ['p_> e _n tS', '뻱'],
    u'\ubef2' : ['p_> e _n _h', '뻲'],
    u'\ubef3' : ['p_> e t', '뻳'],
    u'\ubef4' : ['p_> e _l', '뻴'],
    u'\ubef5' : ['p_> e _l k', '뻵'],
    u'\ubef6' : ['p_> e _l m', '뻶'],
    u'\ubef7' : ['p_> e _l p', '뻷'],
    u'\ubef8' : ['p_> e _l sh', '뻸'],
    u'\ubef9' : ['p_> e _l t_h', '뻹'],
    u'\ubefa' : ['p_> e _l p_h', '뻺'],
    u'\ubefb' : ['p_> e _l _h', '뻻'],
    u'\ubefc' : ['p_> e m', '뻼'],
    u'\ubefd' : ['p_> e p', '뻽'],
    u'\ubefe' : ['p_> e p sh', '뻾'],
    u'\ubeff' : ['p_> e sh', '뻿'],
    u'\ubf00' : ['p_> e s', '뼀'],
    u'\ubf01' : ['p_> e N', '뼁'],
    u'\ubf02' : ['p_> e tS', '뼂'],
    u'\ubf03' : ['p_> e tSh', '뼃'],
    u'\ubf04' : ['p_> e k_h', '뼄'],
    u'\ubf05' : ['p_> e t_h', '뼅'],
    u'\ubf06' : ['p_> e p_h', '뼆'],
    u'\ubf07' : ['p_> e _h', '뼇'],
    u'\ubf08' : ['p_> _j _r', '뼈'],
    u'\ubf09' : ['p_> _j _r k', '뼉'],
    u'\ubf0a' : ['p_> _j _r k_>', '뼊'],
    u'\ubf0b' : ['p_> _j _r k sh', '뼋'],
    u'\ubf0c' : ['p_> _j _r _n', '뼌'],
    u'\ubf0d' : ['p_> _j _r _n tS', '뼍'],
    u'\ubf0e' : ['p_> _j _r _n _h', '뼎'],
    u'\ubf0f' : ['p_> _j _r t', '뼏'],
    u'\ubf10' : ['p_> _j _r _l', '뼐'],
    u'\ubf11' : ['p_> _j _r _l k', '뼑'],
    u'\ubf12' : ['p_> _j _r _l m', '뼒'],
    u'\ubf13' : ['p_> _j _r _l p', '뼓'],
    u'\ubf14' : ['p_> _j _r _l sh', '뼔'],
    u'\ubf15' : ['p_> _j _r _l t_h', '뼕'],
    u'\ubf16' : ['p_> _j _r _l p_h', '뼖'],
    u'\ubf17' : ['p_> _j _r _l _h', '뼗'],
    u'\ubf18' : ['p_> _j _r m', '뼘'],
    u'\ubf19' : ['p_> _j _r p', '뼙'],
    u'\ubf1a' : ['p_> _j _r p sh', '뼚'],
    u'\ubf1b' : ['p_> _j _r sh', '뼛'],
    u'\ubf1c' : ['p_> _j _r s', '뼜'],
    u'\ubf1d' : ['p_> _j _r N', '뼝'],
    u'\ubf1e' : ['p_> _j _r tS', '뼞'],
    u'\ubf1f' : ['p_> _j _r tSh', '뼟'],
    u'\ubf20' : ['p_> _j _r k_h', '뼠'],
    u'\ubf21' : ['p_> _j _r t_h', '뼡'],
    u'\ubf22' : ['p_> _j _r p_h', '뼢'],
    u'\ubf23' : ['p_> _j _r _h', '뼣'],
    u'\ubf24' : ['p_> _j e', '뼤'],
    u'\ubf25' : ['p_> _j e k', '뼥'],
    u'\ubf26' : ['p_> _j e k_>', '뼦'],
    u'\ubf27' : ['p_> _j e k sh', '뼧'],
    u'\ubf28' : ['p_> _j e _n', '뼨'],
    u'\ubf29' : ['p_> _j e _n tS', '뼩'],
    u'\ubf2a' : ['p_> _j e _n _h', '뼪'],
    u'\ubf2b' : ['p_> _j e t', '뼫'],
    u'\ubf2c' : ['p_> _j e _l', '뼬'],
    u'\ubf2d' : ['p_> _j e _l k', '뼭'],
    u'\ubf2e' : ['p_> _j e _l m', '뼮'],
    u'\ubf2f' : ['p_> _j e _l p', '뼯'],
    u'\ubf30' : ['p_> _j e _l sh', '뼰'],
    u'\ubf31' : ['p_> _j e _l t_h', '뼱'],
    u'\ubf32' : ['p_> _j e _l p_h', '뼲'],
    u'\ubf33' : ['p_> _j e _l _h', '뼳'],
    u'\ubf34' : ['p_> _j e m', '뼴'],
    u'\ubf35' : ['p_> _j e p', '뼵'],
    u'\ubf36' : ['p_> _j e p sh', '뼶'],
    u'\ubf37' : ['p_> _j e sh', '뼷'],
    u'\ubf38' : ['p_> _j e s', '뼸'],
    u'\ubf39' : ['p_> _j e N', '뼹'],
    u'\ubf3a' : ['p_> _j e tS', '뼺'],
    u'\ubf3b' : ['p_> _j e tSh', '뼻'],
    u'\ubf3c' : ['p_> _j e k_h', '뼼'],
    u'\ubf3d' : ['p_> _j e t_h', '뼽'],
    u'\ubf3e' : ['p_> _j e p_h', '뼾'],
    u'\ubf3f' : ['p_> _j e _h', '뼿'],
    u'\ubf40' : ['p_> o', '뽀'],
    u'\ubf41' : ['p_> o k', '뽁'],
    u'\ubf42' : ['p_> o k_>', '뽂'],
    u'\ubf43' : ['p_> o k sh', '뽃'],
    u'\ubf44' : ['p_> o _n', '뽄'],
    u'\ubf45' : ['p_> o _n tS', '뽅'],
    u'\ubf46' : ['p_> o _n _h', '뽆'],
    u'\ubf47' : ['p_> o t', '뽇'],
    u'\ubf48' : ['p_> o _l', '뽈'],
    u'\ubf49' : ['p_> o _l k', '뽉'],
    u'\ubf4a' : ['p_> o _l m', '뽊'],
    u'\ubf4b' : ['p_> o _l p', '뽋'],
    u'\ubf4c' : ['p_> o _l sh', '뽌'],
    u'\ubf4d' : ['p_> o _l t_h', '뽍'],
    u'\ubf4e' : ['p_> o _l p_h', '뽎'],
    u'\ubf4f' : ['p_> o _l _h', '뽏'],
    u'\ubf50' : ['p_> o m', '뽐'],
    u'\ubf51' : ['p_> o p', '뽑'],
    u'\ubf52' : ['p_> o p sh', '뽒'],
    u'\ubf53' : ['p_> o sh', '뽓'],
    u'\ubf54' : ['p_> o s', '뽔'],
    u'\ubf55' : ['p_> o N', '뽕'],
    u'\ubf56' : ['p_> o tS', '뽖'],
    u'\ubf57' : ['p_> o tSh', '뽗'],
    u'\ubf58' : ['p_> o k_h', '뽘'],
    u'\ubf59' : ['p_> o t_h', '뽙'],
    u'\ubf5a' : ['p_> o p_h', '뽚'],
    u'\ubf5b' : ['p_> o _h', '뽛'],
    u'\ubf5c' : ['p_> _w a', '뽜'],
    u'\ubf5d' : ['p_> _w a k', '뽝'],
    u'\ubf5e' : ['p_> _w a k_>', '뽞'],
    u'\ubf5f' : ['p_> _w a k sh', '뽟'],
    u'\ubf60' : ['p_> _w a _n', '뽠'],
    u'\ubf61' : ['p_> _w a _n tS', '뽡'],
    u'\ubf62' : ['p_> _w a _n _h', '뽢'],
    u'\ubf63' : ['p_> _w a t', '뽣'],
    u'\ubf64' : ['p_> _w a _l', '뽤'],
    u'\ubf65' : ['p_> _w a _l k', '뽥'],
    u'\ubf66' : ['p_> _w a _l m', '뽦'],
    u'\ubf67' : ['p_> _w a _l p', '뽧'],
    u'\ubf68' : ['p_> _w a _l sh', '뽨'],
    u'\ubf69' : ['p_> _w a _l t_h', '뽩'],
    u'\ubf6a' : ['p_> _w a _l p_h', '뽪'],
    u'\ubf6b' : ['p_> _w a _l _h', '뽫'],
    u'\ubf6c' : ['p_> _w a m', '뽬'],
    u'\ubf6d' : ['p_> _w a p', '뽭'],
    u'\ubf6e' : ['p_> _w a p sh', '뽮'],
    u'\ubf6f' : ['p_> _w a sh', '뽯'],
    u'\ubf70 ' : ['p_> _w a s', '뽰 '],
    u'\ubf71' : ['p_> _w a N', '뽱'],
    u'\ubf72' : ['p_> _w a tS', '뽲'],
    u'\ubf73' : ['p_> _w a tSh', '뽳'],
    u'\ubf74' : ['p_> _w a k_h', '뽴'],
    u'\ubf75' : ['p_> _w a t_h', '뽵'],
    u'\ubf76' : ['p_> _w a p_h', '뽶'],
    u'\ubf77' : ['p_> _w a _h', '뽷'],
    u'\ubf78' : ['p_> _w {', '뽸'],
    u'\ubf79' : ['p_> _w { k', '뽹'],
    u'\ubf7a' : ['p_> _w { k_>', '뽺'],
    u'\ubf7b' : ['p_> _w { k sh', '뽻'],
    u'\ubf7c' : ['p_> _w { _n', '뽼'],
    u'\ubf7d' : ['p_> _w { _n tS', '뽽'],
    u'\ubf7e' : ['p_> _w { _n _h', '뽾'],
    u'\ubf7f' : ['p_> _w { t', '뽿'],
    u'\ubf80' : ['p_> _w { _l', '뾀'],
    u'\ubf81' : ['p_> _w { _l k', '뾁'],
    u'\ubf82' : ['p_> _w { _l m', '뾂'],
    u'\ubf83' : ['p_> _w { _l p', '뾃'],
    u'\ubf84' : ['p_> _w { _l sh', '뾄'],
    u'\ubf85' : ['p_> _w { _l t_h', '뾅'],
    u'\ubf86' : ['p_> _w { _l p_h', '뾆'],
    u'\ubf87' : ['p_> _w { _l _h', '뾇'],
    u'\ubf88' : ['p_> _w { m', '뾈'],
    u'\ubf89' : ['p_> _w { p', '뾉'],
    u'\ubf8a' : ['p_> _w { p sh', '뾊'],
    u'\ubf8b' : ['p_> _w { sh', '뾋'],
    u'\ubf8c' : ['p_> _w { s', '뾌'],
    u'\ubf8d' : ['p_> _w { N', '뾍'],
    u'\ubf8e' : ['p_> _w { tS', '뾎'],
    u'\ubf8f' : ['p_> _w { tSh', '뾏'],
    u'\ubf90' : ['p_> _w { k_h', '뾐'],
    u'\ubf91' : ['p_> _w { t_h', '뾑'],
    u'\ubf92' : ['p_> _w { p_h', '뾒'],
    u'\ubf93' : ['p_> _w { _h', '뾓'],
    u'\ubf94' : ['p_> _w e', '뾔'],
    u'\ubf95' : ['p_> _w e k', '뾕'],
    u'\ubf96' : ['p_> _w e k_>', '뾖'],
    u'\ubf97' : ['p_> _w e k sh', '뾗'],
    u'\ubf98' : ['p_> _w e _n', '뾘'],
    u'\ubf99' : ['p_> _w e _n tS', '뾙'],
    u'\ubf9a' : ['p_> _w e _n _h', '뾚'],
    u'\ubf9b' : ['p_> _w e t', '뾛'],
    u'\ubf9c' : ['p_> _w e _l', '뾜'],
    u'\ubf9d' : ['p_> _w e _l k', '뾝'],
    u'\ubf9e' : ['p_> _w e _l m', '뾞'],
    u'\ubf9f' : ['p_> _w e _l p', '뾟'],
    u'\ubfa0' : ['p_> _w e _l sh', '뾠'],
    u'\ubfa1' : ['p_> _w e _l t_h', '뾡'],
    u'\ubfa2' : ['p_> _w e _l p_h', '뾢'],
    u'\ubfa3' : ['p_> _w e _l _h', '뾣'],
    u'\ubfa4' : ['p_> _w e m', '뾤'],
    u'\ubfa5' : ['p_> _w e p', '뾥'],
    u'\ubfa6' : ['p_> _w e p sh', '뾦'],
    u'\ubfa7' : ['p_> _w e sh', '뾧'],
    u'\ubfa8' : ['p_> _w e s', '뾨'],
    u'\ubfa9' : ['p_> _w e N', '뾩'],
    u'\ubfaa' : ['p_> _w e tS', '뾪'],
    u'\ubfab' : ['p_> _w e tSh', '뾫'],
    u'\ubfac' : ['p_> _w e k_h', '뾬'],
    u'\ubfad' : ['p_> _w e t_h', '뾭'],
    u'\ubfae' : ['p_> _w e p_h', '뾮'],
    u'\ubfaf' : ['p_> _w e _h', '뾯'],
    u'\ubfb0' : ['p_> _j o', '뾰'],
    u'\ubfb1' : ['p_> _j o k', '뾱'],
    u'\ubfb2' : ['p_> _j o k_>', '뾲'],
    u'\ubfb3' : ['p_> _j o k sh', '뾳'],
    u'\ubfb4' : ['p_> _j o _n', '뾴'],
    u'\ubfb5' : ['p_> _j o _n tS', '뾵'],
    u'\ubfb6' : ['p_> _j o _n _h', '뾶'],
    u'\ubfb7' : ['p_> _j o t', '뾷'],
    u'\ubfb8' : ['p_> _j o _l', '뾸'],
    u'\ubfb9' : ['p_> _j o _l k', '뾹'],
    u'\ubfba' : ['p_> _j o _l m', '뾺'],
    u'\ubfbb' : ['p_> _j o _l p', '뾻'],
    u'\ubfbc' : ['p_> _j o _l sh', '뾼'],
    u'\ubfbd' : ['p_> _j o _l t_h', '뾽'],
    u'\ubfbe' : ['p_> _j o _l p_h', '뾾'],
    u'\ubfbf' : ['p_> _j o _l _h', '뾿'],
    u'\ubfc0' : ['p_> _j o m', '뿀'],
    u'\ubfc1' : ['p_> _j o p', '뿁'],
    u'\ubfc2' : ['p_> _j o p sh', '뿂'],
    u'\ubfc3' : ['p_> _j o sh', '뿃'],
    u'\ubfc4' : ['p_> _j o s', '뿄'],
    u'\ubfc5' : ['p_> _j o N', '뿅'],
    u'\ubfc6' : ['p_> _j o tS', '뿆'],
    u'\ubfc7' : ['p_> _j o tSh', '뿇'],
    u'\ubfc8' : ['p_> _j o k_h', '뿈'],
    u'\ubfc9' : ['p_> _j o t_h', '뿉'],
    u'\ubfca' : ['p_> _j o p_h', '뿊'],
    u'\ubfcb' : ['p_> _j o _h', '뿋'],
    u'\ubfcc' : ['p_> u', '뿌'],
    u'\ubfcd' : ['p_> u k', '뿍'],
    u'\ubfce' : ['p_> u k_>', '뿎'],
    u'\ubfcf' : ['p_> u k sh', '뿏'],
    u'\ubfd0' : ['p_> u _n', '뿐'],
    u'\ubfd1' : ['p_> u _n tS', '뿑'],
    u'\ubfd2' : ['p_> u _n _h', '뿒'],
    u'\ubfd3' : ['p_> u t', '뿓'],
    u'\ubfd4' : ['p_> u _l', '뿔'],
    u'\ubfd5' : ['p_> u _l k', '뿕'],
    u'\ubfd6' : ['p_> u _l m', '뿖'],
    u'\ubfd7' : ['p_> u _l p', '뿗'],
    u'\ubfd8' : ['p_> u _l sh', '뿘'],
    u'\ubfd9' : ['p_> u _l t_h', '뿙'],
    u'\ubfda' : ['p_> u _l p_h', '뿚'],
    u'\ubfdb' : ['p_> u _l _h', '뿛'],
    u'\ubfdc' : ['p_> u m', '뿜'],
    u'\ubfdd' : ['p_> u p', '뿝'],
    u'\ubfde' : ['p_> u p sh', '뿞'],
    u'\ubfdf' : ['p_> u sh', '뿟'],
    u'\ubfe0' : ['p_> u s', '뿠'],
    u'\ubfe1' : ['p_> u N', '뿡'],
    u'\ubfe2' : ['p_> u tS', '뿢'],
    u'\ubfe3' : ['p_> u tSh', '뿣'],
    u'\ubfe4' : ['p_> u k_h', '뿤'],
    u'\ubfe5' : ['p_> u t_h', '뿥'],
    u'\ubfe6' : ['p_> u p_h', '뿦'],
    u'\ubfe7' : ['p_> u _h', '뿧'],
    u'\ubfe8' : ['p_> _w _r', '뿨'],
    u'\ubfe9' : ['p_> _w _r k', '뿩'],
    u'\ubfea' : ['p_> _w _r k_>', '뿪'],
    u'\ubfeb' : ['p_> _w _r k sh', '뿫'],
    u'\ubfec' : ['p_> _w _r _n', '뿬'],
    u'\ubfed' : ['p_> _w _r _n tS', '뿭'],
    u'\ubfee' : ['p_> _w _r _n _h', '뿮'],
    u'\ubfef' : ['p_> _w _r t', '뿯'],
    u'\ubff0' : ['p_> _w _r _l', '뿰'],
    u'\ubff1' : ['p_> _w _r _l k', '뿱'],
    u'\ubff2' : ['p_> _w _r _l m', '뿲'],
    u'\ubff3' : ['p_> _w _r _l p', '뿳'],
    u'\ubff4' : ['p_> _w _r _l sh', '뿴'],
    u'\ubff5' : ['p_> _w _r _l t_h', '뿵'],
    u'\ubff6' : ['p_> _w _r _l p_h', '뿶'],
    u'\ubff7' : ['p_> _w _r _l _h', '뿷'],
    u'\ubff8' : ['p_> _w _r m', '뿸'],
    u'\ubff9' : ['p_> _w _r p', '뿹'],
    u'\ubffa' : ['p_> _w _r p sh', '뿺'],
    u'\ubffb' : ['p_> _w _r sh', '뿻'],
    u'\ubffc' : ['p_> _w _r s', '뿼'],
    u'\ubffd' : ['p_> _w _r N', '뿽'],
    u'\ubffe' : ['p_> _w _r tS', '뿾'],
    u'\ubfff' : ['p_> _w _r tSh', '뿿'],
    u'\uc000' : ['p_> _w _r k_h', '쀀'],
    u'\uc001' : ['p_> _w _r t_h', '쀁'],
    u'\uc002' : ['p_> _w _r p_h', '쀂'],
    u'\uc003' : ['p_> _w _r _h', '쀃'],
    u'\uc004' : ['p_> _w E', '쀄'],
    u'\uc005' : ['p_> _w E k', '쀅'],
    u'\uc006' : ['p_> _w E k_>', '쀆'],
    u'\uc007' : ['p_> _w E k sh', '쀇'],
    u'\uc008' : ['p_> _w E _n', '쀈'],
    u'\uc009' : ['p_> _w E _n tS', '쀉'],
    u'\uc00a' : ['p_> _w E _n _h', '쀊'],
    u'\uc00b' : ['p_> _w E t', '쀋'],
    u'\uc00c' : ['p_> _w E _l', '쀌'],
    u'\uc00d' : ['p_> _w E _l k', '쀍'],
    u'\uc00e' : ['p_> _w E _l m', '쀎'],
    u'\uc00f' : ['p_> _w E _l p', '쀏'],
    u'\uc010' : ['p_> _w E _l sh', '쀐'],
    u'\uc011' : ['p_> _w E _l t_h', '쀑'],
    u'\uc012' : ['p_> _w E _l p_h', '쀒'],
    u'\uc013' : ['p_> _w E _l _h', '쀓'],
    u'\uc014' : ['p_> _w E m', '쀔'],
    u'\uc015' : ['p_> _w E p', '쀕'],
    u'\uc016' : ['p_> _w E p sh', '쀖'],
    u'\uc017' : ['p_> _w E sh', '쀗'],
    u'\uc018' : ['p_> _w E s', '쀘'],
    u'\uc019' : ['p_> _w E N', '쀙'],
    u'\uc01a' : ['p_> _w E tS', '쀚'],
    u'\uc01b' : ['p_> _w E tSh', '쀛'],
    u'\uc01c' : ['p_> _w E k_h', '쀜'],
    u'\uc01d' : ['p_> _w E t_h', '쀝'],
    u'\uc01e' : ['p_> _w E p_h', '쀞'],
    u'\uc01f' : ['p_> _w E _h', '쀟'],
    u'\uc020' : ['p_> 2', '쀠'],
    u'\uc021' : ['p_> 2 k', '쀡'],
    u'\uc022' : ['p_> 2 k_>', '쀢'],
    u'\uc023' : ['p_> 2 k sh', '쀣'],
    u'\uc024' : ['p_> 2 _n', '쀤'],
    u'\uc025' : ['p_> 2 _n tS', '쀥'],
    u'\uc026' : ['p_> 2 _n _h', '쀦'],
    u'\uc027' : ['p_> 2 t', '쀧'],
    u'\uc028' : ['p_> 2 _l', '쀨'],
    u'\uc029' : ['p_> 2 _l k', '쀩'],
    u'\uc02a' : ['p_> 2 _l m', '쀪'],
    u'\uc02b' : ['p_> 2 _l p', '쀫'],
    u'\uc02c' : ['p_> 2 _l sh', '쀬'],
    u'\uc02d' : ['p_> 2 _l t_h', '쀭'],
    u'\uc02e' : ['p_> 2 _l p_h', '쀮'],
    u'\uc02f' : ['p_> 2 _l _h', '쀯'],
    u'\uc030' : ['p_> 2 m', '쀰'],
    u'\uc031' : ['p_> 2 p', '쀱'],
    u'\uc032' : ['p_> 2 p sh', '쀲'],
    u'\uc033' : ['p_> 2 sh', '쀳'],
    u'\uc034' : ['p_> 2 s', '쀴'],
    u'\uc035' : ['p_> 2 N', '쀵'],
    u'\uc036' : ['p_> 2 tS', '쀶'],
    u'\uc037' : ['p_> 2 tSh', '쀷'],
    u'\uc038' : ['p_> 2 k_h', '쀸'],
    u'\uc039' : ['p_> 2 t_h', '쀹'],
    u'\uc03a' : ['p_> 2 p_h', '쀺'],
    u'\uc03b' : ['p_> 2 _h', '쀻'],
    u'\uc03c' : ['p_> _j u', '쀼'],
    u'\uc03d' : ['p_> _j u k', '쀽'],
    u'\uc03e' : ['p_> _j u k_>', '쀾'],
    u'\uc03f' : ['p_> _j u k sh', '쀿'],
    u'\uc040' : ['p_> _j u _n', '쁀'],
    u'\uc041' : ['p_> _j u _n tS', '쁁'],
    u'\uc042' : ['p_> _j u _n _h', '쁂'],
    u'\uc043' : ['p_> _j u t', '쁃'],
    u'\uc044' : ['p_> _j u _l', '쁄'],
    u'\uc045' : ['p_> _j u _l k', '쁅'],
    u'\uc046' : ['p_> _j u _l m', '쁆'],
    u'\uc047' : ['p_> _j u _l p', '쁇'],
    u'\uc048' : ['p_> _j u _l sh', '쁈'],
    u'\uc049' : ['p_> _j u _l t_h', '쁉'],
    u'\uc04a' : ['p_> _j u _l p_h', '쁊'],
    u'\uc04b' : ['p_> _j u _l _h', '쁋'],
    u'\uc04c' : ['p_> _j u m', '쁌'],
    u'\uc04d' : ['p_> _j u p', '쁍'],
    u'\uc04e' : ['p_> _j u p sh', '쁎'],
    u'\uc04f' : ['p_> _j u sh', '쁏'],
    u'\uc050' : ['p_> _j u s', '쁐'],
    u'\uc051' : ['p_> _j u N', '쁑'],
    u'\uc052' : ['p_> _j u tS', '쁒'],
    u'\uc053' : ['p_> _j u tSh', '쁓'],
    u'\uc054' : ['p_> _j u k_h', '쁔'],
    u'\uc055' : ['p_> _j u t_h', '쁕'],
    u'\uc056' : ['p_> _j u p_h', '쁖'],
    u'\uc057' : ['p_> _j u _h', '쁗'],
    u'\uc058' : ['p_> M', '쁘'],
    u'\uc059' : ['p_> M k', '쁙'],
    u'\uc05a' : ['p_> M k_>', '쁚'],
    u'\uc05b' : ['p_> M k sh', '쁛'],
    u'\uc05c' : ['p_> M _n', '쁜'],
    u'\uc05d' : ['p_> M _n tS', '쁝'],
    u'\uc05e' : ['p_> M _n _h', '쁞'],
    u'\uc05f' : ['p_> M t', '쁟'],
    u'\uc060' : ['p_> M _l', '쁠'],
    u'\uc061' : ['p_> M _l k', '쁡'],
    u'\uc062' : ['p_> M _l m', '쁢'],
    u'\uc063' : ['p_> M _l p', '쁣'],
    u'\uc064' : ['p_> M _l sh', '쁤'],
    u'\uc065' : ['p_> M _l t_h', '쁥'],
    u'\uc066' : ['p_> M _l p_h', '쁦'],
    u'\uc067' : ['p_> M _l _h', '쁧'],
    u'\uc068' : ['p_> M m', '쁨'],
    u'\uc069' : ['p_> M p', '쁩'],
    u'\uc06a' : ['p_> M p sh', '쁪'],
    u'\uc06b' : ['p_> M sh', '쁫'],
    u'\uc06c' : ['p_> M s', '쁬'],
    u'\uc06d' : ['p_> M N', '쁭'],
    u'\uc06e' : ['p_> M tS', '쁮'],
    u'\uc06f' : ['p_> M tSh', '쁯'],
    u'\uc070' : ['p_> M k_h', '쁰'],
    u'\uc071' : ['p_> M t_h', '쁱'],
    u'\uc072' : ['p_> M p_h', '쁲'],
    u'\uc073' : ['p_> M _h', '쁳'],
    u'\uc074' : ['p_> M _j', '쁴'],
    u'\uc075' : ['p_> M _j k', '쁵'],
    u'\uc076' : ['p_> M _j k_>', '쁶'],
    u'\uc077' : ['p_> M _j k sh', '쁷'],
    u'\uc078' : ['p_> M _j _n', '쁸'],
    u'\uc079' : ['p_> M _j _n tS', '쁹'],
    u'\uc07a' : ['p_> M _j _n _h', '쁺'],
    u'\uc07b' : ['p_> M _j t', '쁻'],
    u'\uc07c' : ['p_> M _j _l', '쁼'],
    u'\uc07d' : ['p_> M _j _l k', '쁽'],
    u'\uc07e' : ['p_> M _j _l m', '쁾'],
    u'\uc07f' : ['p_> M _j _l p', '쁿'],
    u'\uc080' : ['p_> M _j _l sh', '삀'],
    u'\uc081' : ['p_> M _j _l t_h', '삁'],
    u'\uc082' : ['p_> M _j _l p_h', '삂'],
    u'\uc083' : ['p_> M _j _l _h', '삃'],
    u'\uc084' : ['p_> M _j m', '삄'],
    u'\uc085' : ['p_> M _j p', '삅'],
    u'\uc086' : ['p_> M _j p sh', '삆'],
    u'\uc087' : ['p_> M _j sh', '삇'],
    u'\uc088' : ['p_> M _j s', '삈'],
    u'\uc089' : ['p_> M _j N', '삉'],
    u'\uc08a' : ['p_> M _j tS', '삊'],
    u'\uc08b' : ['p_> M _j tSh', '삋'],
    u'\uc08c' : ['p_> M _j k_h', '삌'],
    u'\uc08d' : ['p_> M _j t_h', '삍'],
    u'\uc08e' : ['p_> M _j p_h', '삎'],
    u'\uc08f' : ['p_> M _j _h', '삏'],
    u'\uc090' : ['p_> i', '삐'],
    u'\uc091' : ['p_> i k', '삑'],
    u'\uc092' : ['p_> i k_>', '삒'],
    u'\uc093' : ['p_> i k sh', '삓'],
    u'\uc094' : ['p_> i _n', '삔'],
    u'\uc095' : ['p_> i _n tS', '삕'],
    u'\uc096' : ['p_> i _n _h', '삖'],
    u'\uc097' : ['p_> i t', '삗'],
    u'\uc098' : ['p_> i _l', '삘'],
    u'\uc099' : ['p_> i _l k', '삙'],
    u'\uc09a' : ['p_> i _l m', '삚'],
    u'\uc09b' : ['p_> i _l p', '삛'],
    u'\uc09c' : ['p_> i _l sh', '삜'],
    u'\uc09d' : ['p_> i _l t_h', '삝'],
    u'\uc09e' : ['p_> i _l p_h', '삞'],
    u'\uc09f' : ['p_> i _l _h', '삟'],
    u'\uc0a0' : ['p_> i m', '삠'],
    u'\uc0a1' : ['p_> i p', '삡'],
    u'\uc0a2' : ['p_> i p sh', '삢'],
    u'\uc0a3' : ['p_> i sh', '삣'],
    u'\uc0a4' : ['p_> i s', '삤'],
    u'\uc0a5' : ['p_> i N', '삥'],
    u'\uc0a6' : ['p_> i tS', '삦'],
    u'\uc0a7' : ['p_> i tSh', '삧'],
    u'\uc0a8' : ['p_> i k_h', '삨'],
    u'\uc0a9' : ['p_> i t_h', '삩'],
    u'\uc0aa' : ['p_> i p_h', '삪'],
    u'\uc0ab' : ['p_> i _h', '삫'],
    u'\uc0ac' : ['sh a', '사'],
    u'\uc0ad' : ['sh a k', '삭'],
    u'\uc0ae' : ['sh a k_>', '삮'],
    u'\uc0af' : ['sh a k sh', '삯'],
    u'\uc0b0' : ['sh a _n', '산'],
    u'\uc0b1' : ['sh a _n tS', '삱'],
    u'\uc0b2' : ['sh a _n _h', '삲'],
    u'\uc0b3' : ['sh a t', '삳'],
    u'\uc0b4' : ['sh a _l', '살'],
    u'\uc0b5' : ['sh a _l k', '삵'],
    u'\uc0b6' : ['sh a _l m', '삶'],
    u'\uc0b7' : ['sh a _l p', '삷'],
    u'\uc0b8' : ['sh a _l sh', '삸'],
    u'\uc0b9' : ['sh a _l t_h', '삹'],
    u'\uc0ba' : ['sh a _l p_h', '삺'],
    u'\uc0bb' : ['sh a _l _h', '삻'],
    u'\uc0bc' : ['sh a m', '삼'],
    u'\uc0bd' : ['sh a p', '삽'],
    u'\uc0be' : ['sh a p sh', '삾'],
    u'\uc0bf' : ['sh a sh', '삿'],
    u'\uc0c0' : ['sh a s', '샀'],
    u'\uc0c1' : ['sh a N', '상'],
    u'\uc0c2' : ['sh a tS', '샂'],
    u'\uc0c3' : ['sh a tSh', '샃'],
    u'\uc0c4' : ['sh a k_h', '샄'],
    u'\uc0c5' : ['sh a t_h', '샅'],
    u'\uc0c6' : ['sh a p_h', '샆'],
    u'\uc0c7' : ['sh a _h', '샇'],
    u'\uc0c8' : ['sh {', '새'],
    u'\uc0c9' : ['sh { k', '색'],
    u'\uc0ca' : ['sh { k_>', '샊'],
    u'\uc0cb' : ['sh { k sh', '샋'],
    u'\uc0cc' : ['sh { _n', '샌'],
    u'\uc0cd' : ['sh { _n tS', '샍'],
    u'\uc0ce' : ['sh { _n _h', '샎'],
    u'\uc0cf' : ['sh { t', '샏'],
    u'\uc0d0' : ['sh { _l', '샐'],
    u'\uc0d1' : ['sh { _l k', '샑'],
    u'\uc0d2' : ['sh { _l m', '샒'],
    u'\uc0d3' : ['sh { _l p', '샓'],
    u'\uc0d4' : ['sh { _l sh', '샔'],
    u'\uc0d5' : ['sh { _l t_h', '샕'],
    u'\uc0d6' : ['sh { _l p_h', '샖'],
    u'\uc0d7' : ['sh { _l _h', '샗'],
    u'\uc0d8' : ['sh { m', '샘'],
    u'\uc0d9' : ['sh { p', '샙'],
    u'\uc0da' : ['sh { p sh', '샚'],
    u'\uc0db' : ['sh { sh', '샛'],
    u'\uc0dc' : ['sh { s', '샜'],
    u'\uc0dd' : ['sh { N', '생'],
    u'\uc0de' : ['sh { tS', '샞'],
    u'\uc0df' : ['sh { tSh', '샟'],
    u'\uc0e0' : ['sh { k_h', '샠'],
    u'\uc0e1' : ['sh { t_h', '샡'],
    u'\uc0e2' : ['sh { p_h', '샢'],
    u'\uc0e3' : ['sh { _h', '샣'],
    u'\uc0e4' : ['sh _j a', '샤'],
    u'\uc0e5' : ['sh _j a k', '샥'],
    u'\uc0e6' : ['sh _j a k_>', '샦'],
    u'\uc0e7' : ['sh _j a k sh', '샧'],
    u'\uc0e8' : ['sh _j a _n', '샨'],
    u'\uc0e9' : ['sh _j a _n tS', '샩'],
    u'\uc0ea' : ['sh _j a _n _h', '샪'],
    u'\uc0eb' : ['sh _j a t', '샫'],
    u'\uc0ec' : ['sh _j a _l', '샬'],
    u'\uc0ed' : ['sh _j a _l k', '샭'],
    u'\uc0ee' : ['sh _j a _l m', '샮'],
    u'\uc0ef' : ['sh _j a _l p', '샯'],
    u'\uc0f0' : ['sh _j a _l sh', '샰'],
    u'\uc0f1' : ['sh _j a _l t_h', '샱'],
    u'\uc0f2' : ['sh _j a _l p_h', '샲'],
    u'\uc0f3' : ['sh _j a _l _h', '샳'],
    u'\uc0f4' : ['sh _j a m', '샴'],
    u'\uc0f5' : ['sh _j a p', '샵'],
    u'\uc0f6' : ['sh _j a p sh', '샶'],
    u'\uc0f7' : ['sh _j a sh', '샷'],
    u'\uc0f8' : ['sh _j a s', '샸'],
    u'\uc0f9' : ['sh _j a N', '샹'],
    u'\uc0fa' : ['sh _j a tS', '샺'],
    u'\uc0fb' : ['sh _j a tSh', '샻'],
    u'\uc0fc' : ['sh _j a k_h', '샼'],
    u'\uc0fd' : ['sh _j a t_h', '샽'],
    u'\uc0fe' : ['sh _j a p_h', '샾'],
    u'\uc0ff' : ['sh _j a _h', '샿'],
    u'\uc100' : ['sh _j {', '섀'],
    u'\uc101' : ['sh _j { k', '섁'],
    u'\uc102' : ['sh _j { k_>', '섂'],
    u'\uc103' : ['sh _j { k sh', '섃'],
    u'\uc104' : ['sh _j { _n', '섄'],
    u'\uc105' : ['sh _j { _n tS', '섅'],
    u'\uc106' : ['sh _j { _n _h', '섆'],
    u'\uc107' : ['sh _j { t', '섇'],
    u'\uc108' : ['sh _j { _l', '섈'],
    u'\uc109' : ['sh _j { _l k', '섉'],
    u'\uc10a' : ['sh _j { _l m', '섊'],
    u'\uc10b' : ['sh _j { _l p', '섋'],
    u'\uc10c' : ['sh _j { _l sh', '섌'],
    u'\uc10d' : ['sh _j { _l t_h', '섍'],
    u'\uc10e' : ['sh _j { _l p_h', '섎'],
    u'\uc10f' : ['sh _j { _l _h', '섏'],
    u'\uc110' : ['sh _j { m', '섐'],
    u'\uc111' : ['sh _j { p', '섑'],
    u'\uc112' : ['sh _j { p sh', '섒'],
    u'\uc113' : ['sh _j { sh', '섓'],
    u'\uc114' : ['sh _j { s', '섔'],
    u'\uc115' : ['sh _j { N', '섕'],
    u'\uc116' : ['sh _j { tS', '섖'],
    u'\uc117' : ['sh _j { tSh', '섗'],
    u'\uc118' : ['sh _j { k_h', '섘'],
    u'\uc119' : ['sh _j { t_h', '섙'],
    u'\uc11a' : ['sh _j { p_h', '섚'],
    u'\uc11b' : ['sh _j { _h', '섛'],
    u'\uc11c' : ['sh _r', '서'],
    u'\uc11d' : ['sh _r k', '석'],
    u'\uc11e' : ['sh _r k_>', '섞'],
    u'\uc11f' : ['sh _r k sh', '섟'],
    u'\uc120' : ['sh _r _n', '선'],
    u'\uc121' : ['sh _r _n tS', '섡'],
    u'\uc122' : ['sh _r _n _h', '섢'],
    u'\uc123' : ['sh _r t', '섣'],
    u'\uc124' : ['sh _r _l', '설'],
    u'\uc125' : ['sh _r _l k', '섥'],
    u'\uc126' : ['sh _r _l m', '섦'],
    u'\uc127' : ['sh _r _l p', '섧'],
    u'\uc128' : ['sh _r _l sh', '섨'],
    u'\uc129' : ['sh _r _l t_h', '섩'],
    u'\uc12a' : ['sh _r _l p_h', '섪'],
    u'\uc12b' : ['sh _r _l _h', '섫'],
    u'\uc12c' : ['sh _r m', '섬'],
    u'\uc12d' : ['sh _r p', '섭'],
    u'\uc12e' : ['sh _r p sh', '섮'],
    u'\uc12f' : ['sh _r sh', '섯'],
    u'\uc130' : ['sh _r s', '섰'],
    u'\uc131' : ['sh _r N', '성'],
    u'\uc132' : ['sh _r tS', '섲'],
    u'\uc133' : ['sh _r tSh', '섳'],
    u'\uc134' : ['sh _r k_h', '섴'],
    u'\uc135' : ['sh _r t_h', '섵'],
    u'\uc136' : ['sh _r p_h', '섶'],
    u'\uc137' : ['sh _r _h', '섷'],
    u'\uc138' : ['sh e', '세'],
    u'\uc139' : ['sh e k', '섹'],
    u'\uc13a' : ['sh e k_>', '섺'],
    u'\uc13b' : ['sh e k sh', '섻'],
    u'\uc13c' : ['sh e _n', '센'],
    u'\uc13d' : ['sh e _n tS', '섽'],
    u'\uc13e' : ['sh e _n _h', '섾'],
    u'\uc13f' : ['sh e t', '섿'],
    u'\uc140' : ['sh e _l', '셀'],
    u'\uc141' : ['sh e _l k', '셁'],
    u'\uc142' : ['sh e _l m', '셂'],
    u'\uc143' : ['sh e _l p', '셃'],
    u'\uc144' : ['sh e _l sh', '셄'],
    u'\uc145' : ['sh e _l t_h', '셅'],
    u'\uc146' : ['sh e _l p_h', '셆'],
    u'\uc147' : ['sh e _l _h', '셇'],
    u'\uc148' : ['sh e m', '셈'],
    u'\uc149' : ['sh e p', '셉'],
    u'\uc14a' : ['sh e p sh', '셊'],
    u'\uc14b' : ['sh e sh', '셋'],
    u'\uc14c' : ['sh e s', '셌'],
    u'\uc14d' : ['sh e N', '셍'],
    u'\uc14e' : ['sh e tS', '셎'],
    u'\uc14f' : ['sh e tSh', '셏'],
    u'\uc150' : ['sh e k_h', '셐'],
    u'\uc151' : ['sh e t_h', '셑'],
    u'\uc152' : ['sh e p_h', '셒'],
    u'\uc153' : ['sh e _h', '셓'],
    u'\uc154' : ['sh _j _r', '셔'],
    u'\uc155' : ['sh _j _r k', '셕'],
    u'\uc156' : ['sh _j _r k_>', '셖'],
    u'\uc157' : ['sh _j _r k sh', '셗'],
    u'\uc158' : ['sh _j _r _n', '션'],
    u'\uc159' : ['sh _j _r _n tS', '셙'],
    u'\uc15a' : ['sh _j _r _n _h', '셚'],
    u'\uc15b' : ['sh _j _r t', '셛'],
    u'\uc15c' : ['sh _j _r _l', '셜'],
    u'\uc15d' : ['sh _j _r _l k', '셝'],
    u'\uc15e' : ['sh _j _r _l m', '셞'],
    u'\uc15f' : ['sh _j _r _l p', '셟'],
    u'\uc160' : ['sh _j _r _l sh', '셠'],
    u'\uc161' : ['sh _j _r _l t_h', '셡'],
    u'\uc162' : ['sh _j _r _l p_h', '셢'],
    u'\uc163' : ['sh _j _r _l _h', '셣'],
    u'\uc164' : ['sh _j _r m', '셤'],
    u'\uc165' : ['sh _j _r p', '셥'],
    u'\uc166' : ['sh _j _r p sh', '셦'],
    u'\uc167' : ['sh _j _r sh', '셧'],
    u'\uc168' : ['sh _j _r s', '셨'],
    u'\uc169' : ['sh _j _r N', '셩'],
    u'\uc16a' : ['sh _j _r tS', '셪'],
    u'\uc16b' : ['sh _j _r tSh', '셫'],
    u'\uc16c' : ['sh _j _r k_h', '셬'],
    u'\uc16d' : ['sh _j _r t_h', '셭'],
    u'\uc16e' : ['sh _j _r p_h', '셮'],
    u'\uc16f' : ['sh _j _r _h', '셯'],
    u'\uc170' : ['sh _j e', '셰'],
    u'\uc171' : ['sh _j e k', '셱'],
    u'\uc172' : ['sh _j e k_>', '셲'],
    u'\uc173' : ['sh _j e k sh', '셳'],
    u'\uc174' : ['sh _j e _n', '셴'],
    u'\uc175' : ['sh _j e _n tS', '셵'],
    u'\uc176' : ['sh _j e _n _h', '셶'],
    u'\uc177' : ['sh _j e t', '셷'],
    u'\uc178' : ['sh _j e _l', '셸'],
    u'\uc179' : ['sh _j e _l k', '셹'],
    u'\uc17a' : ['sh _j e _l m', '셺'],
    u'\uc17b' : ['sh _j e _l p', '셻'],
    u'\uc17c' : ['sh _j e _l sh', '셼'],
    u'\uc17d' : ['sh _j e _l t_h', '셽'],
    u'\uc17e' : ['sh _j e _l p_h', '셾'],
    u'\uc17f' : ['sh _j e _l _h', '셿'],
    u'\uc180' : ['sh _j e m', '솀'],
    u'\uc181' : ['sh _j e p', '솁'],
    u'\uc182' : ['sh _j e p sh', '솂'],
    u'\uc183' : ['sh _j e sh', '솃'],
    u'\uc184' : ['sh _j e s', '솄'],
    u'\uc185' : ['sh _j e N', '솅'],
    u'\uc186' : ['sh _j e tS', '솆'],
    u'\uc187' : ['sh _j e tSh', '솇'],
    u'\uc188' : ['sh _j e k_h', '솈'],
    u'\uc189' : ['sh _j e t_h', '솉'],
    u'\uc18a' : ['sh _j e p_h', '솊'],
    u'\uc18b' : ['sh _j e _h', '솋'],
    u'\uc18c' : ['sh o', '소'],
    u'\uc18d' : ['sh o k', '속'],
    u'\uc18e' : ['sh o k_>', '솎'],
    u'\uc18f' : ['sh o k sh', '솏'],
    u'\uc190' : ['sh o _n', '손'],
    u'\uc191' : ['sh o _n tS', '솑'],
    u'\uc192' : ['sh o _n _h', '솒'],
    u'\uc193' : ['sh o t', '솓'],
    u'\uc194' : ['sh o _l', '솔'],
    u'\uc195' : ['sh o _l k', '솕'],
    u'\uc196' : ['sh o _l m', '솖'],
    u'\uc197' : ['sh o _l p', '솗'],
    u'\uc198' : ['sh o _l sh', '솘'],
    u'\uc199' : ['sh o _l t_h', '솙'],
    u'\uc19a' : ['sh o _l p_h', '솚'],
    u'\uc19b' : ['sh o _l _h', '솛'],
    u'\uc19c' : ['sh o m', '솜'],
    u'\uc19d' : ['sh o p', '솝'],
    u'\uc19e' : ['sh o p sh', '솞'],
    u'\uc19f' : ['sh o sh', '솟'],
    u'\uc1a0' : ['sh o s', '솠'],
    u'\uc1a1' : ['sh o N', '송'],
    u'\uc1a2' : ['sh o tS', '솢'],
    u'\uc1a3' : ['sh o tSh', '솣'],
    u'\uc1a4' : ['sh o k_h', '솤'],
    u'\uc1a5' : ['sh o t_h', '솥'],
    u'\uc1a6' : ['sh o p_h', '솦'],
    u'\uc1a7' : ['sh o _h', '솧'],
    u'\uc1a8' : ['sh _w a', '솨'],
    u'\uc1a9' : ['sh _w a k', '솩'],
    u'\uc1aa' : ['sh _w a k_>', '솪'],
    u'\uc1ab' : ['sh _w a k sh', '솫'],
    u'\uc1ac' : ['sh _w a _n', '솬'],
    u'\uc1ad' : ['sh _w a _n tS', '솭'],
    u'\uc1ae' : ['sh _w a _n _h', '솮'],
    u'\uc1af' : ['sh _w a t', '솯'],
    u'\uc1b0' : ['sh _w a _l', '솰'],
    u'\uc1b1' : ['sh _w a _l k', '솱'],
    u'\uc1b2' : ['sh _w a _l m', '솲'],
    u'\uc1b3' : ['sh _w a _l p', '솳'],
    u'\uc1b4' : ['sh _w a _l sh', '솴'],
    u'\uc1b5' : ['sh _w a _l t_h', '솵'],
    u'\uc1b6' : ['sh _w a _l p_h', '솶'],
    u'\uc1b7' : ['sh _w a _l _h', '솷'],
    u'\uc1b8' : ['sh _w a m', '솸'],
    u'\uc1b9' : ['sh _w a p', '솹'],
    u'\uc1ba' : ['sh _w a p sh', '솺'],
    u'\uc1bb' : ['sh _w a sh', '솻'],
    u'\uc1bc' : ['sh _w a s', '솼'],
    u'\uc1bd' : ['sh _w a N', '솽'],
    u'\uc1be' : ['sh _w a tS', '솾'],
    u'\uc1bf' : ['sh _w a tSh', '솿'],
    u'\uc1c0' : ['sh _w a k_h', '쇀'],
    u'\uc1c1' : ['sh _w a t_h', '쇁'],
    u'\uc1c2' : ['sh _w a p_h', '쇂'],
    u'\uc1c3' : ['sh _w a _h', '쇃'],
    u'\uc1c4' : ['sh _w {', '쇄'],
    u'\uc1c5' : ['sh _w { k', '쇅'],
    u'\uc1c6' : ['sh _w { k_>', '쇆'],
    u'\uc1c7' : ['sh _w { k sh', '쇇'],
    u'\uc1c8' : ['sh _w { _n', '쇈'],
    u'\uc1c9' : ['sh _w { _n tS', '쇉'],
    u'\uc1ca' : ['sh _w { _n _h', '쇊'],
    u'\uc1cb' : ['sh _w { t', '쇋'],
    u'\uc1cc' : ['sh _w { _l', '쇌'],
    u'\uc1cd' : ['sh _w { _l k', '쇍'],
    u'\uc1ce' : ['sh _w { _l m', '쇎'],
    u'\uc1cf' : ['sh _w { _l p', '쇏'],
    u'\uc1d0' : ['sh _w { _l sh', '쇐'],
    u'\uc1d1' : ['sh _w { _l t_h', '쇑'],
    u'\uc1d2' : ['sh _w { _l p_h', '쇒'],
    u'\uc1d3' : ['sh _w { _l _h', '쇓'],
    u'\uc1d4' : ['sh _w { m', '쇔'],
    u'\uc1d5' : ['sh _w { p', '쇕'],
    u'\uc1d6' : ['sh _w { p sh', '쇖'],
    u'\uc1d7' : ['sh _w { sh', '쇗'],
    u'\uc1d8' : ['sh _w { s', '쇘'],
    u'\uc1d9' : ['sh _w { N', '쇙'],
    u'\uc1da' : ['sh _w { tS', '쇚'],
    u'\uc1db' : ['sh _w { tSh', '쇛'],
    u'\uc1dc' : ['sh _w { k_h', '쇜'],
    u'\uc1dd' : ['sh _w { t_h', '쇝'],
    u'\uc1de' : ['sh _w { p_h', '쇞'],
    u'\uc1df' : ['sh _w { _h', '쇟'],
    u'\uc1e0' : ['sh _w e', '쇠'],
    u'\uc1e1' : ['sh _w e k', '쇡'],
    u'\uc1e2' : ['sh _w e k_>', '쇢'],
    u'\uc1e3' : ['sh _w e k sh', '쇣'],
    u'\uc1e4' : ['sh _w e _n', '쇤'],
    u'\uc1e5' : ['sh _w e _n tS', '쇥'],
    u'\uc1e6' : ['sh _w e _n _h', '쇦'],
    u'\uc1e7' : ['sh _w e t', '쇧'],
    u'\uc1e8' : ['sh _w e _l', '쇨'],
    u'\uc1e9' : ['sh _w e _l k', '쇩'],
    u'\uc1ea' : ['sh _w e _l m', '쇪'],
    u'\uc1eb' : ['sh _w e _l p', '쇫'],
    u'\uc1ec' : ['sh _w e _l sh', '쇬'],
    u'\uc1ed' : ['sh _w e _l t_h', '쇭'],
    u'\uc1ee' : ['sh _w e _l p_h', '쇮'],
    u'\uc1ef' : ['sh _w e _l _h', '쇯'],
    u'\uc1f0' : ['sh _w e m', '쇰'],
    u'\uc1f1' : ['sh _w e p', '쇱'],
    u'\uc1f2' : ['sh _w e p sh', '쇲'],
    u'\uc1f3' : ['sh _w e sh', '쇳'],
    u'\uc1f4' : ['sh _w e s', '쇴'],
    u'\uc1f5' : ['sh _w e N', '쇵'],
    u'\uc1f6' : ['sh _w e tS', '쇶'],
    u'\uc1f7' : ['sh _w e tSh', '쇷'],
    u'\uc1f8' : ['sh _w e k_h', '쇸'],
    u'\uc1f9' : ['sh _w e t_h', '쇹'],
    u'\uc1fa' : ['sh _w e p_h', '쇺'],
    u'\uc1fb' : ['sh _w e _h', '쇻'],
    u'\uc1fc' : ['sh _j o', '쇼'],
    u'\uc1fd' : ['sh _j o k', '쇽'],
    u'\uc1fe' : ['sh _j o k_>', '쇾'],
    u'\uc1ff' : ['sh _j o k sh', '쇿'],
    u'\uc200' : ['sh _j o _n', '숀'],
    u'\uc201' : ['sh _j o _n tS', '숁'],
    u'\uc202' : ['sh _j o _n _h', '숂'],
    u'\uc203' : ['sh _j o t', '숃'],
    u'\uc204' : ['sh _j o _l', '숄'],
    u'\uc205' : ['sh _j o _l k', '숅'],
    u'\uc206' : ['sh _j o _l m', '숆'],
    u'\uc207' : ['sh _j o _l p', '숇'],
    u'\uc208' : ['sh _j o _l sh', '숈'],
    u'\uc209' : ['sh _j o _l t_h', '숉'],
    u'\uc20a' : ['sh _j o _l p_h', '숊'],
    u'\uc20b' : ['sh _j o _l _h', '숋'],
    u'\uc20c' : ['sh _j o m', '숌'],
    u'\uc20d' : ['sh _j o p', '숍'],
    u'\uc20e' : ['sh _j o p sh', '숎'],
    u'\uc20f' : ['sh _j o sh', '숏'],
    u'\uc210' : ['sh _j o s', '숐'],
    u'\uc211' : ['sh _j o N', '숑'],
    u'\uc212' : ['sh _j o tS', '숒'],
    u'\uc213' : ['sh _j o tSh', '숓'],
    u'\uc214' : ['sh _j o k_h', '숔'],
    u'\uc215' : ['sh _j o t_h', '숕'],
    u'\uc216' : ['sh _j o p_h', '숖'],
    u'\uc217' : ['sh _j o _h', '숗'],
    u'\uc218' : ['sh u', '수'],
    u'\uc219' : ['sh u k', '숙'],
    u'\uc21a' : ['sh u k_>', '숚'],
    u'\uc21b' : ['sh u k sh', '숛'],
    u'\uc21c' : ['sh u _n', '순'],
    u'\uc21d' : ['sh u _n tS', '숝'],
    u'\uc21e' : ['sh u _n _h', '숞'],
    u'\uc21f' : ['sh u t', '숟'],
    u'\uc220' : ['sh u _l', '술'],
    u'\uc221' : ['sh u _l k', '숡'],
    u'\uc222' : ['sh u _l m', '숢'],
    u'\uc223' : ['sh u _l p', '숣'],
    u'\uc224' : ['sh u _l sh', '숤'],
    u'\uc225' : ['sh u _l t_h', '숥'],
    u'\uc226' : ['sh u _l p_h', '숦'],
    u'\uc227' : ['sh u _l _h', '숧'],
    u'\uc228' : ['sh u m', '숨'],
    u'\uc229' : ['sh u p', '숩'],
    u'\uc22a' : ['sh u p sh', '숪'],
    u'\uc22b' : ['sh u sh', '숫'],
    u'\uc22c' : ['sh u s', '숬'],
    u'\uc22d' : ['sh u N', '숭'],
    u'\uc22e' : ['sh u tS', '숮'],
    u'\uc22f' : ['sh u tSh', '숯'],
    u'\uc230' : ['sh u k_h', '숰'],
    u'\uc231' : ['sh u t_h', '숱'],
    u'\uc232' : ['sh u p_h', '숲'],
    u'\uc233' : ['sh u _h', '숳'],
    u'\uc234' : ['sh _w _r', '숴'],
    u'\uc235' : ['sh _w _r k', '숵'],
    u'\uc236' : ['sh _w _r k_>', '숶'],
    u'\uc237' : ['sh _w _r k sh', '숷'],
    u'\uc238' : ['sh _w _r _n', '숸'],
    u'\uc239' : ['sh _w _r _n tS', '숹'],
    u'\uc23a' : ['sh _w _r _n _h', '숺'],
    u'\uc23b' : ['sh _w _r t', '숻'],
    u'\uc23c' : ['sh _w _r _l', '숼'],
    u'\uc23d' : ['sh _w _r _l k', '숽'],
    u'\uc23e' : ['sh _w _r _l m', '숾'],
    u'\uc23f' : ['sh _w _r _l p', '숿'],
    u'\uc240' : ['sh _w _r _l sh', '쉀'],
    u'\uc241' : ['sh _w _r _l t_h', '쉁'],
    u'\uc242' : ['sh _w _r _l p_h', '쉂'],
    u'\uc243' : ['sh _w _r _l _h', '쉃'],
    u'\uc244' : ['sh _w _r m', '쉄'],
    u'\uc245' : ['sh _w _r p', '쉅'],
    u'\uc246' : ['sh _w _r p sh', '쉆'],
    u'\uc247' : ['sh _w _r sh', '쉇'],
    u'\uc248' : ['sh _w _r s', '쉈'],
    u'\uc249' : ['sh _w _r N', '쉉'],
    u'\uc24a' : ['sh _w _r tS', '쉊'],
    u'\uc24b' : ['sh _w _r tSh', '쉋'],
    u'\uc24c' : ['sh _w _r k_h', '쉌'],
    u'\uc24d' : ['sh _w _r t_h', '쉍'],
    u'\uc24e' : ['sh _w _r p_h', '쉎'],
    u'\uc24f' : ['sh _w _r _h', '쉏'],
    u'\uc250' : ['sh _w E', '쉐'],
    u'\uc251' : ['sh _w E k', '쉑'],
    u'\uc252' : ['sh _w E k_>', '쉒'],
    u'\uc253' : ['sh _w E k sh', '쉓'],
    u'\uc254' : ['sh _w E _n', '쉔'],
    u'\uc255' : ['sh _w E _n tS', '쉕'],
    u'\uc256' : ['sh _w E _n _h', '쉖'],
    u'\uc257' : ['sh _w E t', '쉗'],
    u'\uc258' : ['sh _w E _l', '쉘'],
    u'\uc259' : ['sh _w E _l k', '쉙'],
    u'\uc25a' : ['sh _w E _l m', '쉚'],
    u'\uc25b' : ['sh _w E _l p', '쉛'],
    u'\uc25c' : ['sh _w E _l sh', '쉜'],
    u'\uc25d' : ['sh _w E _l t_h', '쉝'],
    u'\uc25e' : ['sh _w E _l p_h', '쉞'],
    u'\uc25f' : ['sh _w E _l _h', '쉟'],
    u'\uc260' : ['sh _w E m', '쉠'],
    u'\uc261' : ['sh _w E p', '쉡'],
    u'\uc262' : ['sh _w E p sh', '쉢'],
    u'\uc263' : ['sh _w E sh', '쉣'],
    u'\uc264' : ['sh _w E s', '쉤'],
    u'\uc265' : ['sh _w E N', '쉥'],
    u'\uc266' : ['sh _w E tS', '쉦'],
    u'\uc267' : ['sh _w E tSh', '쉧'],
    u'\uc268' : ['sh _w E k_h', '쉨'],
    u'\uc269' : ['sh _w E t_h', '쉩'],
    u'\uc26a' : ['sh _w E p_h', '쉪'],
    u'\uc26b' : ['sh _w E _h', '쉫'],
    u'\uc26c' : ['sh 2', '쉬'],
    u'\uc26d' : ['sh 2 k', '쉭'],
    u'\uc26e' : ['sh 2 k_>', '쉮'],
    u'\uc26f' : ['sh 2 k sh', '쉯'],
    u'\uc270' : ['sh 2 _n', '쉰'],
    u'\uc271' : ['sh 2 _n tS', '쉱'],
    u'\uc272' : ['sh 2 _n _h', '쉲'],
    u'\uc273' : ['sh 2 t', '쉳'],
    u'\uc274' : ['sh 2 _l', '쉴'],
    u'\uc275' : ['sh 2 _l k', '쉵'],
    u'\uc276' : ['sh 2 _l m', '쉶'],
    u'\uc277' : ['sh 2 _l p', '쉷'],
    u'\uc278' : ['sh 2 _l sh', '쉸'],
    u'\uc279' : ['sh 2 _l t_h', '쉹'],
    u'\uc27a' : ['sh 2 _l p_h', '쉺'],
    u'\uc27b' : ['sh 2 _l _h', '쉻'],
    u'\uc27c' : ['sh 2 m', '쉼'],
    u'\uc27d' : ['sh 2 p', '쉽'],
    u'\uc27e' : ['sh 2 p sh', '쉾'],
    u'\uc27f' : ['sh 2 sh', '쉿'],
    u'\uc280' : ['sh 2 s', '슀'],
    u'\uc281' : ['sh 2 N', '슁'],
    u'\uc282' : ['sh 2 tS', '슂'],
    u'\uc283' : ['sh 2 tSh', '슃'],
    u'\uc284' : ['sh 2 k_h', '슄'],
    u'\uc285' : ['sh 2 t_h', '슅'],
    u'\uc286' : ['sh 2 p_h', '슆'],
    u'\uc287' : ['sh 2 _h', '슇'],
    u'\uc288' : ['sh _j u', '슈'],
    u'\uc289' : ['sh _j u k', '슉'],
    u'\uc28a' : ['sh _j u k_>', '슊'],
    u'\uc28b' : ['sh _j u k sh', '슋'],
    u'\uc28c' : ['sh _j u _n', '슌'],
    u'\uc28d' : ['sh _j u _n tS', '슍'],
    u'\uc28e' : ['sh _j u _n _h', '슎'],
    u'\uc28f' : ['sh _j u t', '슏'],
    u'\uc290' : ['sh _j u _l', '슐'],
    u'\uc291' : ['sh _j u _l k', '슑'],
    u'\uc292' : ['sh _j u _l m', '슒'],
    u'\uc293' : ['sh _j u _l p', '슓'],
    u'\uc294' : ['sh _j u _l sh', '슔'],
    u'\uc295' : ['sh _j u _l t_h', '슕'],
    u'\uc296' : ['sh _j u _l p_h', '슖'],
    u'\uc297' : ['sh _j u _l _h', '슗'],
    u'\uc298' : ['sh _j u m', '슘'],
    u'\uc299' : ['sh _j u p', '슙'],
    u'\uc29a' : ['sh _j u p sh', '슚'],
    u'\uc29b' : ['sh _j u sh', '슛'],
    u'\uc29c' : ['sh _j u s', '슜'],
    u'\uc29d' : ['sh _j u N', '슝'],
    u'\uc29e' : ['sh _j u tS', '슞'],
    u'\uc29f' : ['sh _j u tSh', '슟'],
    u'\uc2a0' : ['sh _j u k_h', '슠'],
    u'\uc2a1' : ['sh _j u t_h', '슡'],
    u'\uc2a2' : ['sh _j u p_h', '슢'],
    u'\uc2a3' : ['sh _j u _h', '슣'],
    u'\uc2a4' : ['sh M', '스'],
    u'\uc2a5' : ['sh M k', '슥'],
    u'\uc2a6' : ['sh M k_>', '슦'],
    u'\uc2a7' : ['sh M k sh', '슧'],
    u'\uc2a8' : ['sh M _n', '슨'],
    u'\uc2a9' : ['sh M _n tS', '슩'],
    u'\uc2aa' : ['sh M _n _h', '슪'],
    u'\uc2ab' : ['sh M t', '슫'],
    u'\uc2ac' : ['sh M _l', '슬'],
    u'\uc2ad' : ['sh M _l k', '슭'],
    u'\uc2ae' : ['sh M _l m', '슮'],
    u'\uc2af' : ['sh M _l p', '슯'],
    u'\uc2b0' : ['sh M _l sh', '슰'],
    u'\uc2b1' : ['sh M _l t_h', '슱'],
    u'\uc2b2' : ['sh M _l p_h', '슲'],
    u'\uc2b3' : ['sh M _l _h', '슳'],
    u'\uc2b4' : ['sh M m', '슴'],
    u'\uc2b5' : ['sh M p', '습'],
    u'\uc2b6' : ['sh M p sh', '슶'],
    u'\uc2b7' : ['sh M sh', '슷'],
    u'\uc2b8' : ['sh M s', '슸'],
    u'\uc2b9' : ['sh M N', '승'],
    u'\uc2ba' : ['sh M tS', '슺'],
    u'\uc2bb' : ['sh M tSh', '슻'],
    u'\uc2bc' : ['sh M k_h', '슼'],
    u'\uc2bd' : ['sh M t_h', '슽'],
    u'\uc2be' : ['sh M p_h', '슾'],
    u'\uc2bf' : ['sh M _h', '슿'],
    u'\uc2c0' : ['sh M _j', '싀'],
    u'\uc2c1' : ['sh M _j k', '싁'],
    u'\uc2c2' : ['sh M _j k_>', '싂'],
    u'\uc2c3' : ['sh M _j k sh', '싃'],
    u'\uc2c4' : ['sh M _j _n', '싄'],
    u'\uc2c5' : ['sh M _j _n tS', '싅'],
    u'\uc2c6' : ['sh M _j _n _h', '싆'],
    u'\uc2c7' : ['sh M _j t', '싇'],
    u'\uc2c8' : ['sh M _j _l', '싈'],
    u'\uc2c9' : ['sh M _j _l k', '싉'],
    u'\uc2ca' : ['sh M _j _l m', '싊'],
    u'\uc2cb' : ['sh M _j _l p', '싋'],
    u'\uc2cc' : ['sh M _j _l sh', '싌'],
    u'\uc2cd' : ['sh M _j _l t_h', '싍'],
    u'\uc2ce' : ['sh M _j _l p_h', '싎'],
    u'\uc2cf' : ['sh M _j _l _h', '싏'],
    u'\uc2d0' : ['sh M _j m', '싐'],
    u'\uc2d1' : ['sh M _j p', '싑'],
    u'\uc2d2' : ['sh M _j p sh', '싒'],
    u'\uc2d3' : ['sh M _j sh', '싓'],
    u'\uc2d4' : ['sh M _j s', '싔'],
    u'\uc2d5' : ['sh M _j N', '싕'],
    u'\uc2d6' : ['sh M _j tS', '싖'],
    u'\uc2d7' : ['sh M _j tSh', '싗'],
    u'\uc2d8' : ['sh M _j k_h', '싘'],
    u'\uc2d9' : ['sh M _j t_h', '싙'],
    u'\uc2da' : ['sh M _j p_h', '싚'],
    u'\uc2db' : ['sh M _j _h', '싛'],
    u'\uc2dc' : ['sh i', '시'],
    u'\uc2dd' : ['sh i k', '식'],
    u'\uc2de' : ['sh i k_>', '싞'],
    u'\uc2df' : ['sh i k sh', '싟'],
    u'\uc2e0' : ['sh i _n', '신'],
    u'\uc2e1' : ['sh i _n tS', '싡'],
    u'\uc2e2' : ['sh i _n _h', '싢'],
    u'\uc2e3' : ['sh i t', '싣'],
    u'\uc2e4' : ['sh i _l', '실'],
    u'\uc2e5' : ['sh i _l k', '싥'],
    u'\uc2e6' : ['sh i _l m', '싦'],
    u'\uc2e7' : ['sh i _l p', '싧'],
    u'\uc2e8' : ['sh i _l sh', '싨'],
    u'\uc2e9' : ['sh i _l t_h', '싩'],
    u'\uc2ea' : ['sh i _l p_h', '싪'],
    u'\uc2eb' : ['sh i _l _h', '싫'],
    u'\uc2ec' : ['sh i m', '심'],
    u'\uc2ed' : ['sh i p', '십'],
    u'\uc2ee' : ['sh i p sh', '싮'],
    u'\uc2ef' : ['sh i sh', '싯'],
    u'\uc2f0' : ['sh i s', '싰'],
    u'\uc2f1' : ['sh i N', '싱'],
    u'\uc2f2' : ['sh i tS', '싲'],
    u'\uc2f3' : ['sh i tSh', '싳'],
    u'\uc2f4' : ['sh i k_h', '싴'],
    u'\uc2f5' : ['sh i t_h', '싵'],
    u'\uc2f6' : ['sh i p_h', '싶'],
    u'\uc2f7' : ['sh i _h', '싷'],
    u'\uc2f8' : ['s a', '싸'],
    u'\uc2f9' : ['s a k', '싹'],
    u'\uc2fa' : ['s a k_>', '싺'],
    u'\uc2fb' : ['s a k sh', '싻'],
    u'\uc2fc' : ['s a _n', '싼'],
    u'\uc2fd' : ['s a _n tS', '싽'],
    u'\uc2fe' : ['s a _n _h', '싾'],
    u'\uc2ff' : ['s a t', '싿'],
    u'\uc300' : ['s a _l', '쌀'],
    u'\uc301' : ['s a _l k', '쌁'],
    u'\uc302' : ['s a _l m', '쌂'],
    u'\uc303' : ['s a _l p', '쌃'],
    u'\uc304' : ['s a _l sh', '쌄'],
    u'\uc305' : ['s a _l t_h', '쌅'],
    u'\uc306' : ['s a _l p_h', '쌆'],
    u'\uc307' : ['s a _l _h', '쌇'],
    u'\uc308' : ['s a m', '쌈'],
    u'\uc309' : ['s a p', '쌉'],
    u'\uc30a' : ['s a p sh', '쌊'],
    u'\uc30b' : ['s a sh', '쌋'],
    u'\uc30c' : ['s a s', '쌌'],
    u'\uc30d' : ['s a N', '쌍'],
    u'\uc30e' : ['s a tS', '쌎'],
    u'\uc30f' : ['s a tSh', '쌏'],
    u'\uc310' : ['s a k_h', '쌐'],
    u'\uc311' : ['s a t_h', '쌑'],
    u'\uc312' : ['s a p_h', '쌒'],
    u'\uc313' : ['s a _h', '쌓'],
    u'\uc314' : ['s {', '쌔'],
    u'\uc315' : ['s { k', '쌕'],
    u'\uc316' : ['s { k_>', '쌖'],
    u'\uc317' : ['s { k sh', '쌗'],
    u'\uc318' : ['s { _n', '쌘'],
    u'\uc319' : ['s { _n tS', '쌙'],
    u'\uc31a' : ['s { _n _h', '쌚'],
    u'\uc31b' : ['s { t', '쌛'],
    u'\uc31c' : ['s { _l', '쌜'],
    u'\uc31d' : ['s { _l k', '쌝'],
    u'\uc31e' : ['s { _l m', '쌞'],
    u'\uc31f' : ['s { _l p', '쌟'],
    u'\uc320' : ['s { _l sh', '쌠'],
    u'\uc321' : ['s { _l t_h', '쌡'],
    u'\uc322' : ['s { _l p_h', '쌢'],
    u'\uc323' : ['s { _l _h', '쌣'],
    u'\uc324' : ['s { m', '쌤'],
    u'\uc325' : ['s { p', '쌥'],
    u'\uc326' : ['s { p sh', '쌦'],
    u'\uc327' : ['s { sh', '쌧'],
    u'\uc328' : ['s { s', '쌨'],
    u'\uc329' : ['s { N', '쌩'],
    u'\uc32a' : ['s { tS', '쌪'],
    u'\uc32b' : ['s { tSh', '쌫'],
    u'\uc32c' : ['s { k_h', '쌬'],
    u'\uc32d' : ['s { t_h', '쌭'],
    u'\uc32e' : ['s { p_h', '쌮'],
    u'\uc32f' : ['s { _h', '쌯'],
    u'\uc330' : ['s _j a', '쌰'],
    u'\uc331' : ['s _j a k', '쌱'],
    u'\uc332' : ['s _j a k_>', '쌲'],
    u'\uc333' : ['s _j a k sh', '쌳'],
    u'\uc334' : ['s _j a _n', '쌴'],
    u'\uc335' : ['s _j a _n tS', '쌵'],
    u'\uc336' : ['s _j a _n _h', '쌶'],
    u'\uc337' : ['s _j a t', '쌷'],
    u'\uc338' : ['s _j a _l', '쌸'],
    u'\uc339' : ['s _j a _l k', '쌹'],
    u'\uc33a' : ['s _j a _l m', '쌺'],
    u'\uc33b' : ['s _j a _l p', '쌻'],
    u'\uc33c' : ['s _j a _l sh', '쌼'],
    u'\uc33d' : ['s _j a _l t_h', '쌽'],
    u'\uc33e' : ['s _j a _l p_h', '쌾'],
    u'\uc33f' : ['s _j a _l s', '쌿'],
    u'\uc340' : ['s _j a m', '썀'],
    u'\uc341' : ['s _j a p', '썁'],
    u'\uc342' : ['s _j a p sh', '썂'],
    u'\uc343' : ['s _j a sh', '썃'],
    u'\uc344' : ['s _j a s', '썄'],
    u'\uc345' : ['s _j a N', '썅'],
    u'\uc346' : ['s _j a tS', '썆'],
    u'\uc347' : ['s _j a tSh', '썇'],
    u'\uc348' : ['s _j a k_h', '썈'],
    u'\uc349' : ['s _j a t_h', '썉'],
    u'\uc34a' : ['s _j a p_h', '썊'],
    u'\uc34b' : ['s _j a _h', '썋'],
    u'\uc34c' : ['s _j {', '썌'],
    u'\uc34d' : ['s _j { k', '썍'],
    u'\uc34e' : ['s _j { k_>', '썎'],
    u'\uc34f' : ['s _j { k sh', '썏'],
    u'\uc350' : ['s _j { _n', '썐'],
    u'\uc351' : ['s _j { _n tS', '썑'],
    u'\uc352' : ['s _j { _n _h', '썒'],
    u'\uc353' : ['s _j { t', '썓'],
    u'\uc354' : ['s _j { _l', '썔'],
    u'\uc355' : ['s _j { _l k', '썕'],
    u'\uc356' : ['s _j { _l m', '썖'],
    u'\uc357' : ['s _j { _l p', '썗'],
    u'\uc358' : ['s _j { _l sh', '썘'],
    u'\uc359' : ['s _j { _l t_h', '썙'],
    u'\uc35a' : ['s _j { _l p_h', '썚'],
    u'\uc35b' : ['s _j { _l _h', '썛'],
    u'\uc35c' : ['s _j { m', '썜'],
    u'\uc35d' : ['s _j { p', '썝'],
    u'\uc35e' : ['s _j { p sh', '썞'],
    u'\uc35f' : ['s _j { sh', '썟'],
    u'\uc360' : ['s _j { s', '썠'],
    u'\uc361' : ['s _j { N', '썡'],
    u'\uc362' : ['s _j { tS', '썢'],
    u'\uc363' : ['s _j { tSh', '썣'],
    u'\uc364' : ['s _j { k_h', '썤'],
    u'\uc365' : ['s _j { t_h', '썥'],
    u'\uc366' : ['s _j { p_h', '썦'],
    u'\uc367' : ['s _j { _h', '썧'],
    u'\uc368' : ['s _r', '써'],
    u'\uc369' : ['s _r k', '썩'],
    u'\uc36a' : ['s _r k_>', '썪'],
    u'\uc36b' : ['s _r k sh', '썫'],
    u'\uc36c' : ['s _r _n', '썬'],
    u'\uc36d' : ['s _r _n tS', '썭'],
    u'\uc36e' : ['s _r _n _h', '썮'],
    u'\uc36f' : ['s _r t', '썯'],
    u'\uc370' : ['s _r _l', '썰'],
    u'\uc371' : ['s _r _l k', '썱'],
    u'\uc372' : ['s _r _l m', '썲'],
    u'\uc373' : ['s _r _l p', '썳'],
    u'\uc374' : ['s _r _l sh', '썴'],
    u'\uc375' : ['s _r _l t_h', '썵'],
    u'\uc376' : ['s _r _l p_h', '썶'],
    u'\uc377' : ['s _r _l _h', '썷'],
    u'\uc378' : ['s _r m', '썸'],
    u'\uc379' : ['s _r p', '썹'],
    u'\uc37a' : ['s _r p sh', '썺'],
    u'\uc37b' : ['s _r sh', '썻'],
    u'\uc37c' : ['s _r s', '썼'],
    u'\uc37d' : ['s _r N', '썽'],
    u'\uc37e' : ['s _r tS', '썾'],
    u'\uc37f' : ['s _r tSh', '썿'],
    u'\uc380' : ['s _r k_h', '쎀'],
    u'\uc381' : ['s _r t_h', '쎁'],
    u'\uc382' : ['s _r p_h', '쎂'],
    u'\uc383' : ['s _r _h', '쎃'],
    u'\uc384' : ['s e', '쎄'],
    u'\uc385' : ['s e k', '쎅'],
    u'\uc386' : ['s e k_>', '쎆'],
    u'\uc387' : ['s e k sh', '쎇'],
    u'\uc388' : ['s e _n', '쎈'],
    u'\uc389' : ['s e _n tS', '쎉'],
    u'\uc38a' : ['s e _n _h', '쎊'],
    u'\uc38b' : ['s e t', '쎋'],
    u'\uc38c' : ['s e _l', '쎌'],
    u'\uc38d' : ['s e _l k', '쎍'],
    u'\uc38e' : ['s e _l m', '쎎'],
    u'\uc38f' : ['s e _l p', '쎏'],
    u'\uc390' : ['s e _l sh', '쎐'],
    u'\uc391' : ['s e _l t_h', '쎑'],
    u'\uc392' : ['s e _l p_h', '쎒'],
    u'\uc393' : ['s e _l _h', '쎓'],
    u'\uc394' : ['s e m', '쎔'],
    u'\uc395' : ['s e p', '쎕'],
    u'\uc396' : ['s e p sh', '쎖'],
    u'\uc397' : ['s e sh', '쎗'],
    u'\uc398' : ['s e s', '쎘'],
    u'\uc399' : ['s e N', '쎙'],
    u'\uc39a' : ['s e tS', '쎚'],
    u'\uc39b' : ['s e tSh', '쎛'],
    u'\uc39c' : ['s e k_h', '쎜'],
    u'\uc39d' : ['s e t_h', '쎝'],
    u'\uc39e' : ['s e p_h', '쎞'],
    u'\uc39f' : ['s e _h', '쎟'],
    u'\uc3a0' : ['s _j _r', '쎠'],
    u'\uc3a1' : ['s _j _r k', '쎡'],
    u'\uc3a2' : ['s _j _r k_>', '쎢'],
    u'\uc3a3' : ['s _j _r k sh', '쎣'],
    u'\uc3a4' : ['s _j _r _n', '쎤'],
    u'\uc3a5' : ['s _j _r _n tS', '쎥'],
    u'\uc3a6' : ['s _j _r nh', '쎦'],
    u'\uc3a7' : ['s _j _r t', '쎧'],
    u'\uc3a8' : ['s _j _r _l', '쎨'],
    u'\uc3a9' : ['s _j _r _l k', '쎩'],
    u'\uc3aa' : ['s _j _r _l m', '쎪'],
    u'\uc3ab' : ['s _j _r _l p', '쎫'],
    u'\uc3ac' : ['s _j _r _l sh', '쎬'],
    u'\uc3ad' : ['s _j _r _l t_h', '쎭'],
    u'\uc3ae' : ['s _j _r _l p_h', '쎮'],
    u'\uc3af' : ['s _j _r _l _h', '쎯'],
    u'\uc3b0' : ['s _j _r m', '쎰'],
    u'\uc3b1' : ['s _j _r p', '쎱'],
    u'\uc3b2' : ['s _j _r p sh', '쎲'],
    u'\uc3b3' : ['s _j _r sh', '쎳'],
    u'\uc3b4' : ['s _j _r s', '쎴'],
    u'\uc3b5' : ['s _j _r N', '쎵'],
    u'\uc3b6' : ['s _j _r tS', '쎶'],
    u'\uc3b7' : ['s _j _r tSh', '쎷'],
    u'\uc3b8' : ['s _j _r k_h', '쎸'],
    u'\uc3b9' : ['s _j _r t_h', '쎹'],
    u'\uc3ba' : ['s _j _r p_h', '쎺'],
    u'\uc3bb' : ['s _j _r _h', '쎻'],
    u'\uc3bc' : ['s _j e', '쎼'],
    u'\uc3bd' : ['s _j e k', '쎽'],
    u'\uc3be' : ['s _j e k_>', '쎾'],
    u'\uc3bf' : ['s _j e k sh', '쎿'],
    u'\uc3c0' : ['s _j e _n', '쏀'],
    u'\uc3c1' : ['s _j e _n tS', '쏁'],
    u'\uc3c2' : ['s _j e _n _h', '쏂'],
    u'\uc3c3' : ['s _j e t', '쏃'],
    u'\uc3c4' : ['s _j e _l', '쏄'],
    u'\uc3c5' : ['s _j e _l k', '쏅'],
    u'\uc3c6' : ['s _j e _l m', '쏆'],
    u'\uc3c7' : ['s _j e _l p', '쏇'],
    u'\uc3c8' : ['s _j e _l sh', '쏈'],
    u'\uc3c9' : ['s _j e _l t_h', '쏉'],
    u'\uc3ca' : ['s _j e _l p_h', '쏊'],
    u'\uc3cb' : ['s _j e _l _h', '쏋'],
    u'\uc3cc' : ['s _j e m', '쏌'],
    u'\uc3cd' : ['s _j e p', '쏍'],
    u'\uc3ce' : ['s _j e p sh', '쏎'],
    u'\uc3cf' : ['s _j e sh', '쏏'],
    u'\uc3d0' : ['s _j e s', '쏐'],
    u'\uc3d1' : ['s _j e N', '쏑'],
    u'\uc3d2' : ['s _j e tS', '쏒'],
    u'\uc3d3' : ['s _j e tSh', '쏓'],
    u'\uc3d4' : ['s _j e k_h', '쏔'],
    u'\uc3d5' : ['s _j e t_h', '쏕'],
    u'\uc3d6' : ['s _j e p_h', '쏖'],
    u'\uc3d7' : ['s _j e _h', '쏗'],
    u'\uc3d8' : ['s o', '쏘'],
    u'\uc3d9' : ['s o k', '쏙'],
    u'\uc3da' : ['s o k_>', '쏚'],
    u'\uc3db' : ['s o k sh', '쏛'],
    u'\uc3dc' : ['s o _n', '쏜'],
    u'\uc3dd' : ['s o _n tS', '쏝'],
    u'\uc3de' : ['s o _n _h', '쏞'],
    u'\uc3df' : ['s o t', '쏟'],
    u'\uc3e0' : ['s o _l', '쏠'],
    u'\uc3e1' : ['s o _l k', '쏡'],
    u'\uc3e2' : ['s o _l m', '쏢'],
    u'\uc3e3' : ['s o _l p', '쏣'],
    u'\uc3e4' : ['s o _l sh', '쏤'],
    u'\uc3e5' : ['s o _l t_h', '쏥'],
    u'\uc3e6' : ['s o _l p_h', '쏦'],
    u'\uc3e7' : ['s o _l _h', '쏧'],
    u'\uc3e8' : ['s o m', '쏨'],
    u'\uc3e9' : ['s o p', '쏩'],
    u'\uc3ea' : ['s o p sh', '쏪'],
    u'\uc3eb' : ['s o sh', '쏫'],
    u'\uc3ec' : ['s o s', '쏬'],
    u'\uc3ed' : ['s o N', '쏭'],
    u'\uc3ee' : ['s o tS', '쏮'],
    u'\uc3ef' : ['s o tSh', '쏯'],
    u'\uc3f0' : ['s o k_h', '쏰'],
    u'\uc3f1' : ['s o t_h', '쏱'],
    u'\uc3f2' : ['s o p_h', '쏲'],
    u'\uc3f3' : ['s o _h', '쏳'],
    u'\uc3f4' : ['s _w a', '쏴'],
    u'\uc3f5' : ['s _w a k', '쏵'],
    u'\uc3f6' : ['s _w a k_>', '쏶'],
    u'\uc3f7' : ['s _w a k sh', '쏷'],
    u'\uc3f8' : ['s _w a _n', '쏸'],
    u'\uc3f9' : ['s _w a _n tS', '쏹'],
    u'\uc3fa' : ['s _w a _n _h', '쏺'],
    u'\uc3fb' : ['s _w a t', '쏻'],
    u'\uc3fc' : ['s _w a _l', '쏼'],
    u'\uc3fd' : ['s _w a _l k', '쏽'],
    u'\uc3fe' : ['s _w a _l m', '쏾'],
    u'\uc3ff' : ['s _w a _l p', '쏿'],
    u'\uc400' : ['s _w a _l sh', '쐀'],
    u'\uc401' : ['s _w a _l t_h', '쐁'],
    u'\uc402' : ['s _w a _l p_h', '쐂'],
    u'\uc403' : ['s _w a _l _h', '쐃'],
    u'\uc404' : ['s _w a m', '쐄'],
    u'\uc405' : ['s _w a p', '쐅'],
    u'\uc406' : ['s _w a p sh', '쐆'],
    u'\uc407' : ['s _w a sh', '쐇'],
    u'\uc408' : ['s _w a s', '쐈'],
    u'\uc409' : ['s _w a N', '쐉'],
    u'\uc40a' : ['s _w a tS', '쐊'],
    u'\uc40b' : ['s _w a tSh', '쐋'],
    u'\uc40c' : ['s _w a k_h', '쐌'],
    u'\uc40d' : ['s _w a t_h', '쐍'],
    u'\uc40e' : ['s _w a p_h', '쐎'],
    u'\uc40f' : ['s _w a _h', '쐏'],
    u'\uc410' : ['s _w {', '쐐'],
    u'\uc411' : ['s _w { k', '쐑'],
    u'\uc412' : ['s _w { k_>', '쐒'],
    u'\uc413' : ['s _w { k sh', '쐓'],
    u'\uc414' : ['s _w { _n', '쐔'],
    u'\uc415' : ['s _w { _n tS', '쐕'],
    u'\uc416' : ['s _w { _n _h', '쐖'],
    u'\uc417' : ['s _w { t', '쐗'],
    u'\uc418' : ['s _w { _l', '쐘'],
    u'\uc419' : ['s _w { _l k', '쐙'],
    u'\uc41a' : ['s _w { _l m', '쐚'],
    u'\uc41b' : ['s _w { _l p', '쐛'],
    u'\uc41c' : ['s _w { _l sh', '쐜'],
    u'\uc41d' : ['s _w { _l t_h', '쐝'],
    u'\uc41e' : ['s _w { _l p_h', '쐞'],
    u'\uc41f' : ['s _w { _l _h', '쐟'],
    u'\uc420' : ['s _w { m', '쐠'],
    u'\uc421' : ['s _w { p', '쐡'],
    u'\uc422' : ['s _w { p sh', '쐢'],
    u'\uc423' : ['s _w { sh', '쐣'],
    u'\uc424' : ['s _w { s', '쐤'],
    u'\uc425' : ['s _w { N', '쐥'],
    u'\uc426' : ['s _w { tS', '쐦'],
    u'\uc427' : ['s _w { tSh', '쐧'],
    u'\uc428' : ['s _w { k_h', '쐨'],
    u'\uc429' : ['s _w { t_h', '쐩'],
    u'\uc42a' : ['s _w { p_h', '쐪'],
    u'\uc42b' : ['s _w { _h', '쐫'],
    u'\uc42c' : ['s _w e', '쐬'],
    u'\uc42d' : ['s _w e k', '쐭'],
    u'\uc42e' : ['s _w e k_>', '쐮'],
    u'\uc42f' : ['s _w e k sh', '쐯'],
    u'\uc430' : ['s _w e _n', '쐰'],
    u'\uc431' : ['s _w e _n tS', '쐱'],
    u'\uc432' : ['s _w e nh', '쐲'],
    u'\uc433' : ['s _w e t', '쐳'],
    u'\uc434' : ['s _w e _l', '쐴'],
    u'\uc435' : ['s _w e _l k', '쐵'],
    u'\uc436' : ['s _w e _l m', '쐶'],
    u'\uc437' : ['s _w e _l p', '쐷'],
    u'\uc438' : ['s _w e _l sh', '쐸'],
    u'\uc439' : ['s _w e _l t_h', '쐹'],
    u'\uc43a' : ['s _w e _l p_h', '쐺'],
    u'\uc43b' : ['s _w e _l _h', '쐻'],
    u'\uc43c' : ['s _w e m', '쐼'],
    u'\uc43d' : ['s _w e p', '쐽'],
    u'\uc43e' : ['s _w e p sh', '쐾'],
    u'\uc43f' : ['s _w e sh', '쐿'],
    u'\uc440' : ['s _w e s', '쑀'],
    u'\uc441' : ['s _w e N', '쑁'],
    u'\uc442' : ['s _w e tS', '쑂'],
    u'\uc443' : ['s _w e tSh', '쑃'],
    u'\uc444' : ['s _w e k_h', '쑄'],
    u'\uc445' : ['s _w e t_h', '쑅'],
    u'\uc446' : ['s _w e p_h', '쑆'],
    u'\uc447' : ['s _w e _h', '쑇'],
    u'\uc448' : ['s _j o', '쑈'],
    u'\uc449' : ['s _j o k', '쑉'],
    u'\uc44a' : ['s _j o k_>', '쑊'],
    u'\uc44b' : ['s _j o k sh', '쑋'],
    u'\uc44c' : ['s _j o _n', '쑌'],
    u'\uc44d' : ['s _j o _n tS', '쑍'],
    u'\uc44e' : ['s _j o _n _h', '쑎'],
    u'\uc44f' : ['s _j o t', '쑏'],
    u'\uc450' : ['s _j o _l', '쑐'],
    u'\uc451' : ['s _j o _l k', '쑑'],
    u'\uc452' : ['s _j o _l m', '쑒'],
    u'\uc453' : ['s _j o _l p', '쑓'],
    u'\uc454' : ['s _j o _l sh', '쑔'],
    u'\uc455' : ['s _j o _l t_h', '쑕'],
    u'\uc456' : ['s _j o _l p_h', '쑖'],
    u'\uc457' : ['s _j o _l _h', '쑗'],
    u'\uc458' : ['s _j o m', '쑘'],
    u'\uc459' : ['s _j o p', '쑙'],
    u'\uc45a' : ['s _j o p sh', '쑚'],
    u'\uc45b' : ['s _j o sh', '쑛'],
    u'\uc45c' : ['s _j o s', '쑜'],
    u'\uc45d' : ['s _j o N', '쑝'],
    u'\uc45e' : ['s _j o tS', '쑞'],
    u'\uc45f' : ['s _j o tSh', '쑟'],
    u'\uc460' : ['s _j o k_h', '쑠'],
    u'\uc461' : ['s _j o t_h', '쑡'],
    u'\uc462' : ['s _j o p_h', '쑢'],
    u'\uc463' : ['s _j o _h', '쑣'],
    u'\uc464' : ['s u', '쑤'],
    u'\uc465' : ['s u k', '쑥'],
    u'\uc466' : ['s u k_>', '쑦'],
    u'\uc467' : ['s u k sh', '쑧'],
    u'\uc468' : ['s u _n', '쑨'],
    u'\uc469' : ['s u _n tS', '쑩'],
    u'\uc46a' : ['s u _n _h', '쑪'],
    u'\uc46b' : ['s u t', '쑫'],
    u'\uc46c' : ['s u _l', '쑬'],
    u'\uc46d' : ['s u _l k', '쑭'],
    u'\uc46e' : ['s u _l m', '쑮'],
    u'\uc46f' : ['s u _l p', '쑯'],
    u'\uc470' : ['s u _l sh', '쑰'],
    u'\uc471' : ['s u _l t_h', '쑱'],
    u'\uc472' : ['s u _l p_h', '쑲'],
    u'\uc473' : ['s u _l _h', '쑳'],
    u'\uc474' : ['s u m', '쑴'],
    u'\uc475' : ['s u p', '쑵'],
    u'\uc476' : ['s u p sh', '쑶'],
    u'\uc477' : ['s u sh', '쑷'],
    u'\uc478' : ['s u s', '쑸'],
    u'\uc479' : ['s u N', '쑹'],
    u'\uc47a' : ['s u tS', '쑺'],
    u'\uc47b' : ['s u tSh', '쑻'],
    u'\uc47c' : ['s u k_h', '쑼'],
    u'\uc47d' : ['s u t_h', '쑽'],
    u'\uc47e' : ['s u p_h', '쑾'],
    u'\uc47f' : ['s u _h', '쑿'],
    u'\uc480' : ['s _w _r', '쒀'],
    u'\uc481' : ['s _w _r k', '쒁'],
    u'\uc482' : ['s _w _r k_>', '쒂'],
    u'\uc483' : ['s _w _r k sh', '쒃'],
    u'\uc484' : ['s _w _r _n', '쒄'],
    u'\uc485' : ['s _w _r _n tS', '쒅'],
    u'\uc486' : ['s _w _r _n _h', '쒆'],
    u'\uc487' : ['s _w _r t', '쒇'],
    u'\uc488' : ['s _w _r _l', '쒈'],
    u'\uc489' : ['s _w _r _l k', '쒉'],
    u'\uc48a' : ['s _w _r _l m', '쒊'],
    u'\uc48b' : ['s _w _r _l p', '쒋'],
    u'\uc48c' : ['s _w _r _l sh', '쒌'],
    u'\uc48d' : ['s _w _r _l t_h', '쒍'],
    u'\uc48e' : ['s _w _r _l p_h', '쒎'],
    u'\uc48f' : ['s _w _r _l _h', '쒏'],
    u'\uc490' : ['s _w _r m', '쒐'],
    u'\uc491' : ['s _w _r p', '쒑'],
    u'\uc492' : ['s _w _r p sh', '쒒'],
    u'\uc493' : ['s _w _r sh', '쒓'],
    u'\uc494' : ['s _w _r s', '쒔'],
    u'\uc495' : ['s _w _r N', '쒕'],
    u'\uc496' : ['s _w _r tS', '쒖'],
    u'\uc497' : ['s _w _r tSh', '쒗'],
    u'\uc498' : ['s _w _r k_h', '쒘'],
    u'\uc499' : ['s _w _r t_h', '쒙'],
    u'\uc49a' : ['s _w _r p_h', '쒚'],
    u'\uc49b' : ['s _w _r _h', '쒛'],
    u'\uc49c' : ['s _w E', '쒜'],
    u'\uc49d' : ['s _w E k', '쒝'],
    u'\uc49e' : ['s _w E k_>', '쒞'],
    u'\uc49f' : ['s _w E k sh', '쒟'],
    u'\uc4a0' : ['s _w E _n', '쒠'],
    u'\uc4a1' : ['s _w E _n tS', '쒡'],
    u'\uc4a2' : ['s _w E _n _h', '쒢'],
    u'\uc4a3' : ['s _w E t', '쒣'],
    u'\uc4a4' : ['s _w E _l', '쒤'],
    u'\uc4a5' : ['s _w E _l k', '쒥'],
    u'\uc4a6' : ['s _w E _l m', '쒦'],
    u'\uc4a7' : ['s _w E _l p', '쒧'],
    u'\uc4a8' : ['s _w E _l sh', '쒨'],
    u'\uc4a9' : ['s _w E _l t_h', '쒩'],
    u'\uc4aa' : ['s _w E _l p_h', '쒪'],
    u'\uc4ab' : ['s _w E _l _h', '쒫'],
    u'\uc4ac' : ['s _w E m', '쒬'],
    u'\uc4ad' : ['s _w E p', '쒭'],
    u'\uc4ae' : ['s _w E p sh', '쒮'],
    u'\uc4af' : ['s _w E sh', '쒯'],
    u'\uc4b0' : ['s _w E s', '쒰'],
    u'\uc4b1' : ['s _w E N', '쒱'],
    u'\uc4b2' : ['s _w E tS', '쒲'],
    u'\uc4b3' : ['s _w E tSh', '쒳'],
    u'\uc4b4' : ['s _w E k_h', '쒴'],
    u'\uc4b5' : ['s _w E t_h', '쒵'],
    u'\uc4b6' : ['s _w E p_h', '쒶'],
    u'\uc4b7' : ['s _w E _h', '쒷'],
    u'\uc4b8' : ['s 2', '쒸'],
    u'\uc4b9' : ['s 2 k', '쒹'],
    u'\uc4ba' : ['s 2 k_>', '쒺'],
    u'\uc4bb' : ['s 2 k sh', '쒻'],
    u'\uc4bc' : ['s 2 _n', '쒼'],
    u'\uc4bd' : ['s 2 _n tS', '쒽'],
    u'\uc4be' : ['s 2 _n _h', '쒾'],
    u'\uc4bf' : ['s 2 t', '쒿'],
    u'\uc4c0' : ['s 2 _l', '쓀'],
    u'\uc4c1' : ['s 2 _l k', '쓁'],
    u'\uc4c2' : ['s 2 _l m', '쓂'],
    u'\uc4c3' : ['s 2 _l p', '쓃'],
    u'\uc4c4' : ['s 2 _l sh', '쓄'],
    u'\uc4c5' : ['s 2 _l t_h', '쓅'],
    u'\uc4c6' : ['s 2 _l p_h', '쓆'],
    u'\uc4c7' : ['s 2 _l _h', '쓇'],
    u'\uc4c8' : ['s 2 m', '쓈'],
    u'\uc4c9' : ['s 2 p', '쓉'],
    u'\uc4ca' : ['s 2 p sh', '쓊'],
    u'\uc4cb' : ['s 2 sh', '쓋'],
    u'\uc4cc' : ['s 2 s', '쓌'],
    u'\uc4cd' : ['s 2 N', '쓍'],
    u'\uc4ce' : ['s 2 tS', '쓎'],
    u'\uc4cf' : ['s 2 tSh', '쓏'],
    u'\uc4d0' : ['s 2 k_h', '쓐'],
    u'\uc4d1' : ['s 2 t_h', '쓑'],
    u'\uc4d2' : ['s 2 p_h', '쓒'],
    u'\uc4d3' : ['s 2 _h', '쓓'],
    u'\uc4d4' : ['s _j u', '쓔'],
    u'\uc4d5' : ['s _j u k', '쓕'],
    u'\uc4d6' : ['s _j u k_>', '쓖'],
    u'\uc4d7' : ['s _j u k sh', '쓗'],
    u'\uc4d8' : ['s _j u _n', '쓘'],
    u'\uc4d9' : ['s _j u _n tS', '쓙'],
    u'\uc4da' : ['s _j u _n _h', '쓚'],
    u'\uc4db' : ['s _j u t', '쓛'],
    u'\uc4dc' : ['s _j u _l', '쓜'],
    u'\uc4dd' : ['s _j u _l', '쓝'],
    u'\uc4de' : ['s _j u _l m', '쓞'],
    u'\uc4df' : ['s _j u _l p', '쓟'],
    u'\uc4e0' : ['s _j u _l sh', '쓠'],
    u'\uc4e1' : ['s _j u _l t_h', '쓡'],
    u'\uc4e2' : ['s _j u _l p_h', '쓢'],
    u'\uc4e3' : ['s _j u _l _h', '쓣'],
    u'\uc4e4' : ['s _j u m', '쓤'],
    u'\uc4e5' : ['s _j u p', '쓥'],
    u'\uc4e7' : ['s _j u sh', '쓧'],
    u'\uc4e8' : ['s _j u s', '쓨'],
    u'\uc4e9' : ['s _j u N', '쓩'],
    u'\uc4ea' : ['s _j u tS', '쓪'],
    u'\uc4eb' : ['s _j u tSh', '쓫'],
    u'\uc4ec' : ['s _j u k_h', '쓬'],
    u'\uc4ed' : ['s _j u t_h', '쓭'],
    u'\uc4ee' : ['s _j u p_h', '쓮'],
    u'\uc4ef' : ['s _j u _h', '쓯'],
    u'\uc4f0' : ['s M', '쓰'],
    u'\uc4f1' : ['s M k', '쓱'],
    u'\uc4f2' : ['s M k_>', '쓲'],
    u'\uc4f3' : ['s M k sh', '쓳'],
    u'\uc4f4' : ['s M _n', '쓴'],
    u'\uc4f5' : ['s M _n tS', '쓵'],
    u'\uc4f6' : ['s M _n _h', '쓶'],
    u'\uc4f7' : ['s M t', '쓷'],
    u'\uc4f8' : ['s M _l', '쓸'],
    u'\uc4f9' : ['s M _l k', '쓹'],
    u'\uc4fa' : ['s M _l m', '쓺'],
    u'\uc4fb' : ['s M _l p', '쓻'],
    u'\uc4fc' : ['s M _l sh', '쓼'],
    u'\uc4fd' : ['s M _l t_h', '쓽'],
    u'\uc4fe' : ['s M _l p_h', '쓾'],
    u'\uc4ff' : ['s M _l _h', '쓿'],
    u'\uc500' : ['s M m', '씀'],
    u'\uc501' : ['s M p', '씁'],
    u'\uc502' : ['s M p sh', '씂'],
    u'\uc503' : ['s M sh', '씃'],
    u'\uc504' : ['s M s', '씄'],
    u'\uc505' : ['s M N', '씅'],
    u'\uc506' : ['s M tS', '씆'],
    u'\uc507' : ['s M tSh', '씇'],
    u'\uc508' : ['s M k_h', '씈'],
    u'\uc509' : ['s M t_h', '씉'],
    u'\uc50a' : ['s M p_h', '씊'],
    u'\uc50b' : ['s M _h', '씋'],
    u'\uc50c' : ['s M _j', '씌'],
    u'\uc50d' : ['s M _j k', '씍'],
    u'\uc50e' : ['s M _j k_>', '씎'],
    u'\uc50f' : ['s M _j k sh', '씏'],
    u'\uc510' : ['s M _j _n', '씐'],
    u'\uc511' : ['s M _j _n tS', '씑'],
    u'\uc512' : ['s M _j _n _h', '씒'],
    u'\uc513' : ['s M _j t', '씓'],
    u'\uc514' : ['s M _j _l', '씔'],
    u'\uc515' : ['s M _j _l k', '씕'],
    u'\uc516' : ['s M _j _l m', '씖'],
    u'\uc517' : ['s M _j _l p', '씗'],
    u'\uc518' : ['s M _j _l sh', '씘'],
    u'\uc519' : ['s M _j _l t_h', '씙'],
    u'\uc51a' : ['s M _j _l p_h', '씚'],
    u'\uc51b' : ['s M _j _l _h', '씛'],
    u'\uc51c' : ['s M _j m', '씜'],
    u'\uc51d' : ['s M _j p', '씝'],
    u'\uc51e' : ['s M _j p sh', '씞'],
    u'\uc51f' : ['s M _j sh', '씟'],
    u'\uc520' : ['s M _j s', '씠'],
    u'\uc521' : ['s M _j N', '씡'],
    u'\uc522' : ['s M _j tS', '씢'],
    u'\uc523' : ['s M _j tSh', '씣'],
    u'\uc524' : ['s M _j k_h', '씤'],
    u'\uc525' : ['s M _j t_h', '씥'],
    u'\uc526' : ['s M _j p_h', '씦'],
    u'\uc527' : ['s M _j _h', '씧'],
    u'\uc528' : ['s i', '씨'],
    u'\uc529' : ['s i k', '씩'],
    u'\uc52a' : ['s i k_>', '씪'],
    u'\uc52b' : ['s i k sh', '씫'],
    u'\uc52c' : ['s i _n', '씬'],
    u'\uc52d' : ['s i _n tS', '씭'],
    u'\uc52e' : ['s i _n _h', '씮'],
    u'\uc52f' : ['s i t', '씯'],
    u'\uc530' : ['s i _l', '씰'],
    u'\uc531' : ['s i _l k', '씱'],
    u'\uc532' : ['s i _l m', '씲'],
    u'\uc533' : ['s i _l p', '씳'],
    u'\uc534' : ['s i _l sh', '씴'],
    u'\uc535' : ['s i _l t_h', '씵'],
    u'\uc536' : ['s i _l p_h', '씶'],
    u'\uc537' : ['s i _l _h', '씷'],
    u'\uc538' : ['s i m', '씸'],
    u'\uc539' : ['s i p', '씹'],
    u'\uc53a' : ['s i p sh', '씺'],
    u'\uc53b' : ['s i sh', '씻'],
    u'\uc53c' : ['s i s', '씼'],
    u'\uc53d' : ['s i N', '씽'],
    u'\uc53e' : ['s i tS', '씾'],
    u'\uc53f' : ['s i tSh', '씿'],
    u'\uc540' : ['s i k_h', '앀'],
    u'\uc541' : ['s i t_h', '앁'],
    u'\uc542' : ['s i p_h', '앂'],
    u'\uc543' : ['s i _h', '앃'],
    u'\uc544' : ['a', '아'],
    u'\uc545' : ['a k', '악'],
    u'\uc546' : ['a k_>', '앆'],
    u'\uc547' : ['a k sh', '앇'],
    u'\uc548' : ['a _n', '안'],
    u'\uc549' : ['a _n tS', '앉'],
    u'\uc54a' : ['a _n _h', '않'],
    u'\uc54b' : ['a t', '앋'],
    u'\uc54c' : ['a _l', '알'],
    u'\uc54d' : ['a _l k', '앍'],
    u'\uc54e' : ['a _l m', '앎'],
    u'\uc54f' : ['a _l p', '앏'],
    u'\uc550' : ['a _l sh', '앐'],
    u'\uc551' : ['a _l t_h', '앑'],
    u'\uc552' : ['a _l p_h', '앒'],
    u'\uc553' : ['a _l _h', '앓'],
    u'\uc554' : ['a m', '암'],
    u'\uc555' : ['a p', '압'],
    u'\uc556' : ['a p sh', '앖'],
    u'\uc557' : ['a sh', '앗'],
    u'\uc558' : ['a s', '았'],
    u'\uc559' : ['a N', '앙'],
    u'\uc55a' : ['a tS', '앚'],
    u'\uc55b' : ['a tSh', '앛'],
    u'\uc55c' : ['a k_h', '앜'],
    u'\uc55d' : ['a t_h', '앝'],
    u'\uc55e' : ['a p_h', '앞'],
    u'\uc55f' : ['a _h', '앟'],
    u'\uc560' : ['{', '애'],
    u'\uc561' : ['{ k', '액'],
    u'\uc562' : ['{ k_>', '앢'],
    u'\uc563' : ['{ k sh', '앣'],
    u'\uc564' : ['{ _n', '앤'],
    u'\uc565' : ['{ _n tS', '앥'],
    u'\uc566' : ['{ _n _h', '앦'],
    u'\uc567' : ['{ t', '앧'],
    u'\uc568' : ['{ _l', '앨'],
    u'\uc569' : ['{ _l k', '앩'],
    u'\uc56a' : ['{ _l m', '앪'],
    u'\uc56b' : ['{ _l p', '앫'],
    u'\uc56c' : ['{ _l sh', '앬'],
    u'\uc56d' : ['{ _l t_h', '앭'],
    u'\uc56e' : ['{ _l p_h', '앮'],
    u'\uc56f' : ['{ _l _h', '앯'],
    u'\uc570' : ['{ m', '앰'],
    u'\uc571' : ['{ p', '앱'],
    u'\uc572' : ['{ p sh', '앲'],
    u'\uc573' : ['{ sh', '앳'],
    u'\uc574' : ['{ s', '앴'],
    u'\uc575' : ['{ N', '앵'],
    u'\uc576' : ['{ tS', '앶'],
    u'\uc577' : ['{ tSh', '앷'],
    u'\uc578' : ['{ k_h', '앸'],
    u'\uc579' : ['{ t_h', '앹'],
    u'\uc57a' : ['{ p_h', '앺'],
    u'\uc57b' : ['{ _h', '앻'],
    u'\uc57c' : ['_j a', '야'],
    u'\uc57d' : ['_j a k', '약'],
    u'\uc57e' : ['_j a k_>', '앾'],
    u'\uc57f' : ['_j a k sh', '앿'],
    u'\uc580' : ['_j a _n', '얀'],
    u'\uc581' : ['_j a _n tS', '얁'],
    u'\uc582' : ['_j a _n _h', '얂'],
    u'\uc583' : ['_j a t', '얃'],
    u'\uc584' : ['_j a _l', '얄'],
    u'\uc585' : ['_j a _l k', '얅'],
    u'\uc586' : ['_j a _l m', '얆'],
    u'\uc587' : ['_j a _l p', '얇'],
    u'\uc588' : ['_j a _l sh', '얈'],
    u'\uc589' : ['_j a _l t_h', '얉'],
    u'\uc58a' : ['_j a _l p_h', '얊'],
    u'\uc58b' : ['_j a _l _h', '얋'],
    u'\uc58c' : ['_j a m', '얌'],
    u'\uc58d' : ['_j a p', '얍'],
    u'\uc58e' : ['_j a p sh', '얎'],
    u'\uc58f' : ['_j a sh', '얏'],
    u'\uc590' : ['_j a s', '얐'],
    u'\uc591' : ['_j a N', '양'],
    u'\uc592' : ['_j a tS', '얒'],
    u'\uc593' : ['_j a tSh', '얓'],
    u'\uc594' : ['_j a k_h', '얔'],
    u'\uc595' : ['_j a t_h', '얕'],
    u'\uc596' : ['_j a p_h', '얖'],
    u'\uc597' : ['_j a _h', '얗'],
    u'\uc598' : ['_j {', '얘'],
    u'\uc599' : ['_j { k', '얙'],
    u'\uc59a' : ['_j { k_>', '얚'],
    u'\uc59b' : ['_j { k sh', '얛'],
    u'\uc59c' : ['_j { _n', '얜'],
    u'\uc59d' : ['_j { _n tS', '얝'],
    u'\uc59e' : ['_j { _n _h', '얞'],
    u'\uc59f' : ['_j { t', '얟'],
    u'\uc5a0' : ['_j { _l', '얠'],
    u'\uc5a1' : ['_j { _l k', '얡'],
    u'\uc5a2' : ['_j { _l m', '얢'],
    u'\uc5a3' : ['_j { _l p', '얣'],
    u'\uc5a4' : ['_j { _l sh', '얤'],
    u'\uc5a5' : ['_j { _l t_h', '얥'],
    u'\uc5a6' : ['_j { _l p_h', '얦'],
    u'\uc5a7' : ['_j { _l _h', '얧'],
    u'\uc5a8' : ['_j { m', '얨'],
    u'\uc5a9' : ['_j { p', '얩'],
    u'\uc5aa' : ['_j { p sh', '얪'],
    u'\uc5ab' : ['_j { sh', '얫'],
    u'\uc5ac' : ['_j { s', '얬'],
    u'\uc5ad' : ['_j { N', '얭'],
    u'\uc5ae' : ['_j { tS', '얮'],
    u'\uc5af' : ['_j { tSh', '얯'],
    u'\uc5b0' : ['_j { k_h', '얰'],
    u'\uc5b1' : ['_j { t_h', '얱'],
    u'\uc5b2' : ['_j { p_h', '얲'],
    u'\uc5b3' : ['_j { _h', '얳'],
    u'\uc5b4' : ['_r', '어'],
    u'\uc5b5' : ['_r k', '억'],
    u'\uc5b6' : ['_r k_>', '얶'],
    u'\uc5b7' : ['_r k sh', '얷'],
    u'\uc5b8' : ['_r _n', '언'],
    u'\uc5b9' : ['_r _n tS', '얹'],
    u'\uc5ba' : ['_r _n _h', '얺'],
    u'\uc5bb' : ['_r t', '얻'],
    u'\uc5bc' : ['_r _l', '얼'],
    u'\uc5bd' : ['_r _l k', '얽'],
    u'\uc5be' : ['_r _l m', '얾'],
    u'\uc5bf' : ['_r _l p', '얿'],
    u'\uc5c0' : ['_r _l sh', '엀'],
    u'\uc5c1' : ['_r _l t_h', '엁'],
    u'\uc5c2' : ['_r _l p_h', '엂'],
    u'\uc5c3' : ['_r _l _h', '엃'],
    u'\uc5c4' : ['_r m', '엄'],
    u'\uc5c5' : ['_r p', '업'],
    u'\uc5c6' : ['_r p sh', '없'],
    u'\uc5c7' : ['_r sh', '엇'],
    u'\uc5c8' : ['_r s', '었'],
    u'\uc5c9' : ['_r N', '엉'],
    u'\uc5ca' : ['_r tS', '엊'],
    u'\uc5cb' : ['_r tSh', '엋'],
    u'\uc5cc' : ['_r k_h', '엌'],
    u'\uc5cd' : ['_r t_h', '엍'],
    u'\uc5ce' : ['_r p_h', '엎'],
    u'\uc5cf' : ['_r _h', '엏'],
    u'\uc5d0' : ['e', '에'],
    u'\uc5d1' : ['e k', '엑'],
    u'\uc5d2' : ['e k_>', '엒'],
    u'\uc5d3' : ['e k sh', '엓'],
    u'\uc5d4' : ['e _n', '엔'],
    u'\uc5d5' : ['e _n tS', '엕'],
    u'\uc5d6' : ['e _n _h', '엖'],
    u'\uc5d7' : ['e t', '엗'],
    u'\uc5d8' : ['e _l', '엘'],
    u'\uc5d9' : ['e _l k', '엙'],
    u'\uc5da' : ['e _l m', '엚'],
    u'\uc5db' : ['e _l p', '엛'],
    u'\uc5dc' : ['e _l sh', '엜'],
    u'\uc5dd' : ['e _l t_h', '엝'],
    u'\uc5de' : ['e _l p_h', '엞'],
    u'\uc5df' : ['e _l _h', '엟'],
    u'\uc5e0' : ['e m', '엠'],
    u'\uc5e1' : ['e p', '엡'],
    u'\uc5e2' : ['e p sh', '엢'],
    u'\uc5e3' : ['e sh', '엣'],
    u'\uc5e4' : ['e s', '엤'],
    u'\uc5e5' : ['e N', '엥'],
    u'\uc5e6' : ['e tS', '엦'],
    u'\uc5e7' : ['e tSh', '엧'],
    u'\uc5e8' : ['e k_h', '엨'],
    u'\uc5e9' : ['e t_h', '엩'],
    u'\uc5ea' : ['e p_h', '엪'],
    u'\uc5eb' : ['e _h', '엫'],
    u'\uc5ec' : ['_j _r', '여'],
    u'\uc5ed' : ['_j _r k', '역'],
    u'\uc5ee' : ['_j _r k_>', '엮'],
    u'\uc5ef' : ['_j _r k sh', '엯'],
    u'\uc5f0' : ['_j _r _n', '연'],
    u'\uc5f1' : ['_j _r _n tS', '엱'],
    u'\uc5f2' : ['_j _r _n _h', '엲'],
    u'\uc5f3' : ['_j _r t', '엳'],
    u'\uc5f4' : ['_j _r _l', '열'],
    u'\uc5f5' : ['_j _r _l k', '엵'],
    u'\uc5f6' : ['_j _r _l m', '엶'],
    u'\uc5f7' : ['_j _r _l p', '엷'],
    u'\uc5f8' : ['_j _r _l sh', '엸'],
    u'\uc5f9' : ['_j _r _l t_h', '엹'],
    u'\uc5fa' : ['_j _r _l p_h', '엺'],
    u'\uc5fb' : ['_j _r _l _h', '엻'],
    u'\uc5fc' : ['_j _r m', '염'],
    u'\uc5fd' : ['_j _r p', '엽'],
    u'\uc5fe' : ['_j _r p sh', '엾'],
    u'\uc5ff' : ['_j _r sh', '엿'],
    u'\uc600' : ['_j _r s', '였'],
    u'\uc601' : ['_j _r N', '영'],
    u'\uc602' : ['_j _r tS', '옂'],
    u'\uc603' : ['_j _r tSh', '옃'],
    u'\uc604' : ['_j _r k_h', '옄'],
    u'\uc605' : ['_j _r t_h', '옅'],
    u'\uc606' : ['_j _r p_h', '옆'],
    u'\uc607' : ['_j _r _h', '옇'],
    u'\uc608' : ['_j e', '예'],
    u'\uc609' : ['_j e k', '옉'],
    u'\uc60a' : ['_j e k_>', '옊'],
    u'\uc60b' : ['_j e k sh', '옋'],
    u'\uc60c' : ['_j e _n', '옌'],
    u'\uc60d' : ['_j e _n tS', '옍'],
    u'\uc60e' : ['_j e _n _h', '옎'],
    u'\uc60f' : ['_j e t', '옏'],
    u'\uc610' : ['_j e _l', '옐'],
    u'\uc611' : ['_j e _l k', '옑'],
    u'\uc612' : ['_j e _l m', '옒'],
    u'\uc613' : ['_j e _l p', '옓'],
    u'\uc614' : ['_j e _l sh', '옔'],
    u'\uc615' : ['_j e _l t_h', '옕'],
    u'\uc616' : ['_j e _l p_h', '옖'],
    u'\uc617' : ['_j e _l _h', '옗'],
    u'\uc618' : ['_j e m', '옘'],
    u'\uc619' : ['_j e p', '옙'],
    u'\uc61a' : ['_j e p sh', '옚'],
    u'\uc61b' : ['_j e sh', '옛'],
    u'\uc61c' : ['_j e s', '옜'],
    u'\uc61d' : ['_j e N', '옝'],
    u'\uc61e' : ['_j e tS', '옞'],
    u'\uc61f' : ['_j e tSh', '옟'],
    u'\uc620' : ['_j e k_h', '옠'],
    u'\uc621' : ['_j e t_h', '옡'],
    u'\uc622' : ['_j e p_h', '옢'],
    u'\uc623' : ['_j e _h', '옣'],
    u'\uc624' : ['o', '오'],
    u'\uc625' : ['o k', '옥'],
    u'\uc626' : ['o k_>', '옦'],
    u'\uc627' : ['o k sh', '옧'],
    u'\uc628' : ['o _n', '온'],
    u'\uc629' : ['o _n tS', '옩'],
    u'\uc62a' : ['o _n _h', '옪'],
    u'\uc62b' : ['o t', '옫'],
    u'\uc62c' : ['o _l', '올'],
    u'\uc62d' : ['o _l k', '옭'],
    u'\uc62e' : ['o _l m', '옮'],
    u'\uc62f' : ['o _l p', '옯'],
    u'\uc630' : ['o _l sh', '옰'],
    u'\uc631' : ['o _l t_h', '옱'],
    u'\uc632' : ['o _l p_h', '옲'],
    u'\uc633' : ['o _l _h', '옳'],
    u'\uc634' : ['o m', '옴'],
    u'\uc635' : ['o p', '옵'],
    u'\uc636' : ['o p sh', '옶'],
    u'\uc637' : ['o sh', '옷'],
    u'\uc638' : ['o s', '옸'],
    u'\uc639' : ['o N', '옹'],
    u'\uc63a' : ['o tS', '옺'],
    u'\uc63b' : ['o tSh', '옻'],
    u'\uc63c' : ['o k_h', '옼'],
    u'\uc63d' : ['o t_h', '옽'],
    u'\uc63e' : ['o p_h', '옾'],
    u'\uc63f' : ['o _h', '옿'],
    u'\uc640' : ['_w a', '와'],
    u'\uc641' : ['_w a k', '왁'],
    u'\uc642' : ['_w a k_>', '왂'],
    u'\uc643' : ['_w a k sh', '왃'],
    u'\uc644' : ['_w a _n', '완'],
    u'\uc645' : ['_w a _n tS', '왅'],
    u'\uc646' : ['_w a _n _h', '왆'],
    u'\uc647' : ['_w a t', '왇'],
    u'\uc648' : ['_w a _l', '왈'],
    u'\uc649' : ['_w a _l k', '왉'],
    u'\uc64a' : ['_w a _l m', '왊'],
    u'\uc64b' : ['_w a _l p', '왋'],
    u'\uc64c' : ['_w a _l sh', '왌'],
    u'\uc64d' : ['_w a _l t_h', '왍'],
    u'\uc64e' : ['_w a _l p_h', '왎'],
    u'\uc64f' : ['_w a _l _h', '왏'],
    u'\uc650' : ['_w a m', '왐'],
    u'\uc651' : ['_w a p', '왑'],
    u'\uc652' : ['_w a p sh', '왒'],
    u'\uc653' : ['_w a sh', '왓'],
    u'\uc654' : ['_w a s', '왔'],
    u'\uc655' : ['_w a N', '왕'],
    u'\uc656' : ['_w a tS', '왖'],
    u'\uc657' : ['_w a tSh', '왗'],
    u'\uc658' : ['_w a k_h', '왘'],
    u'\uc659' : ['_w a t_h', '왙'],
    u'\uc65a' : ['_w a p_h', '왚'],
    u'\uc65b' : ['_w a _h', '왛'],
    u'\uc65c' : ['_w {', '왜'],
    u'\uc65d' : ['_w { k', '왝'],
    u'\uc65e' : ['_w { k_>', '왞'],
    u'\uc65f' : ['_w { k sh', '왟'],
    u'\uc660' : ['_w { _n', '왠'],
    u'\uc661' : ['_w { _n tS', '왡'],
    u'\uc662' : ['_w { _n _h', '왢'],
    u'\uc663' : ['_w { t', '왣'],
    u'\uc664' : ['_w { _l', '왤'],
    u'\uc665' : ['_w { _l k', '왥'],
    u'\uc666' : ['_w { _l m', '왦'],
    u'\uc667' : ['_w { _l p', '왧'],
    u'\uc668' : ['_w { _l sh', '왨'],
    u'\uc669' : ['_w { _l t_h', '왩'],
    u'\uc66a' : ['_w { _l p_h', '왪'],
    u'\uc66b' : ['_w { _l _h', '왫'],
    u'\uc66c' : ['_w { m', '왬'],
    u'\uc66d' : ['_w { p', '왭'],
    u'\uc66e' : ['_w { p sh', '왮'],
    u'\uc66f' : ['_w { sh', '왯'],
    u'\uc670' : ['_w { s', '왰'],
    u'\uc671' : ['_w { N', '왱'],
    u'\uc672' : ['_w { tS', '왲'],
    u'\uc673' : ['_w { tSh', '왳'],
    u'\uc674' : ['_w { k_h', '왴'],
    u'\uc675' : ['_w { t_h', '왵'],
    u'\uc676' : ['_w { p_h', '왶'],
    u'\uc677' : ['_w { _h', '왷'],
    u'\uc678' : ['_w e', '외'],
    u'\uc679' : ['_w e k', '왹'],
    u'\uc67a' : ['_w e k_>', '왺'],
    u'\uc67b' : ['_w e k sh', '왻'],
    u'\uc67c' : ['_w e _n', '왼'],
    u'\uc67d' : ['_w e _n tS', '왽'],
    u'\uc67e' : ['_w e _n _h', '왾'],
    u'\uc67f' : ['_w e t', '왿'],
    u'\uc680' : ['_w e _l', '욀'],
    u'\uc681' : ['_w e _l k', '욁'],
    u'\uc682' : ['_w e _l m', '욂'],
    u'\uc683' : ['_w e _l p', '욃'],
    u'\uc684' : ['_w e _l sh', '욄'],
    u'\uc685' : ['_w e _l t_h', '욅'],
    u'\uc686' : ['_w e _l p_h', '욆'],
    u'\uc687' : ['_w e _l _h', '욇'],
    u'\uc688' : ['_w e m', '욈'],
    u'\uc689' : ['_w e p', '욉'],
    u'\uc68a' : ['_w e p sh', '욊'],
    u'\uc68b' : ['_w e sh', '욋'],
    u'\uc68c' : ['_w e s', '욌'],
    u'\uc68d' : ['_w e N', '욍'],
    u'\uc68e' : ['_w e tS', '욎'],
    u'\uc68f' : ['_w e tSh', '욏'],
    u'\uc690' : ['_w e k_h', '욐'],
    u'\uc691' : ['_w e t_h', '욑'],
    u'\uc692' : ['_w e p_h', '욒'],
    u'\uc693' : ['_w e _h', '욓'],
    u'\uc694' : ['_j o', '요'],
    u'\uc695' : ['_j o k', '욕'],
    u'\uc696' : ['_j o k_>', '욖'],
    u'\uc697' : ['_j o k sh', '욗'],
    u'\uc698' : ['_j o _n', '욘'],
    u'\uc699' : ['_j o _n tS', '욙'],
    u'\uc69a' : ['_j o _n _h', '욚'],
    u'\uc69b' : ['_j o t', '욛'],
    u'\uc69c' : ['_j o _l', '욜'],
    u'\uc69d' : ['_j o _l k', '욝'],
    u'\uc69e' : ['_j o _l m', '욞'],
    u'\uc69f' : ['_j o _l p', '욟'],
    u'\uc6a0' : ['_j o _l sh', '욠'],
    u'\uc6a1' : ['_j o _l t_h', '욡'],
    u'\uc6a2' : ['_j o _l p_h', '욢'],
    u'\uc6a3' : ['_j o _l _h', '욣'],
    u'\uc6a4' : ['_j o m', '욤'],
    u'\uc6a5' : ['_j o p', '욥'],
    u'\uc6a6' : ['_j o p sh', '욦'],
    u'\uc6a7' : ['_j o sh', '욧'],
    u'\uc6a8' : ['_j o s', '욨'],
    u'\uc6a9' : ['_j o N', '용'],
    u'\uc6aa' : ['_j o tS', '욪'],
    u'\uc6ab' : ['_j o tSh', '욫'],
    u'\uc6ac' : ['_j o k_h', '욬'],
    u'\uc6ad' : ['_j o t_h', '욭'],
    u'\uc6ae' : ['_j o p_h', '욮'],
    u'\uc6af' : ['_j o _h', '욯'],
    u'\uc6b0' : ['u', '우'],
    u'\uc6b1' : ['u k', '욱'],
    u'\uc6b2' : ['u k_>', '욲'],
    u'\uc6b3' : ['u k sh', '욳'],
    u'\uc6b4' : ['u _n', '운'],
    u'\uc6b5' : ['u _n tS', '욵'],
    u'\uc6b6' : ['u _n _h', '욶'],
    u'\uc6b7' : ['u t', '욷'],
    u'\uc6b8' : ['u _l', '울'],
    u'\uc6b9' : ['u _l k', '욹'],
    u'\uc6ba' : ['u _l m', '욺'],
    u'\uc6bb' : ['u _l p', '욻'],
    u'\uc6bc' : ['u _l sh', '욼'],
    u'\uc6bd' : ['u _l t_h', '욽'],
    u'\uc6be' : ['u _l p_h', '욾'],
    u'\uc6bf' : ['u _l _h', '욿'],
    u'\uc6c0' : ['u m', '움'],
    u'\uc6c1' : ['u p', '웁'],
    u'\uc6c2' : ['u p sh', '웂'],
    u'\uc6c3' : ['u sh', '웃'],
    u'\uc6c4' : ['u s', '웄'],
    u'\uc6c5' : ['u N', '웅'],
    u'\uc6c6' : ['u tS', '웆'],
    u'\uc6c7' : ['u tSh', '웇'],
    u'\uc6c8' : ['u k_h', '웈'],
    u'\uc6c9' : ['u t_h', '웉'],
    u'\uc6ca' : ['u p_h', '웊'],
    u'\uc6cb' : ['u _h', '웋'],
    u'\uc6cc' : ['_w _r', '워'],
    u'\uc6cd' : ['_w _r k', '웍'],
    u'\uc6ce' : ['_w _r k_>', '웎'],
    u'\uc6cf' : ['_w _r k sh', '웏'],
    u'\uc6d0' : ['_w _r _n', '원'],
    u'\uc6d1' : ['_w _r _n tS', '웑'],
    u'\uc6d2' : ['_w _r _n _h', '웒'],
    u'\uc6d3' : ['_w _r t', '웓'],
    u'\uc6d4' : ['_w _r _l', '월'],
    u'\uc6d5' : ['_w _r _l k', '웕'],
    u'\uc6d6' : ['_w _r _l m', '웖'],
    u'\uc6d7' : ['_w _r _l p', '웗'],
    u'\uc6d8' : ['_w _r _l sh', '웘'],
    u'\uc6d9' : ['_w _r _l t_h', '웙'],
    u'\uc6da' : ['_w _r _l p_h', '웚'],
    u'\uc6db' : ['_w _r _l _h', '웛'],
    u'\uc6dc' : ['_w _r m', '웜'],
    u'\uc6dd' : ['_w _r p', '웝'],
    u'\uc6de' : ['_w _r p sh', '웞'],
    u'\uc6df' : ['_w _r sh', '웟'],
    u'\uc6e0' : ['_w _r s', '웠'],
    u'\uc6e1' : ['_w _r N', '웡'],
    u'\uc6e2' : ['_w _r tS', '웢'],
    u'\uc6e3' : ['_w _r tSh', '웣'],
    u'\uc6e4' : ['_w _r k_h', '웤'],
    u'\uc6e5' : ['_w _r t_h', '웥'],
    u'\uc6e6' : ['_w _r p_h', '웦'],
    u'\uc6e7' : ['_w _r _h', '웧'],
    u'\uc6e8' : ['_w E', '웨'],
    u'\uc6e9' : ['_w E k', '웩'],
    u'\uc6ea' : ['_w E k_>', '웪'],
    u'\uc6eb' : ['_w E k sh', '웫'],
    u'\uc6ec' : ['_w E _n', '웬'],
    u'\uc6ed' : ['_w E _n tS', '웭'],
    u'\uc6ee' : ['_w E _n _h', '웮'],
    u'\uc6ef' : ['_w E t', '웯'],
    u'\uc6f0' : ['_w E _l', '웰'],
    u'\uc6f1' : ['_w E _l k', '웱'],
    u'\uc6f2' : ['_w E _l m', '웲'],
    u'\uc6f3' : ['_w E _l p', '웳'],
    u'\uc6f4' : ['_w E _l sh', '웴'],
    u'\uc6f5' : ['_w E _l t_h', '웵'],
    u'\uc6f6' : ['_w E _l p_h', '웶'],
    u'\uc6f7' : ['_w E _l _h', '웷'],
    u'\uc6f8' : ['_w E m', '웸'],
    u'\uc6f9' : ['_w E p', '웹'],
    u'\uc6fa' : ['_w E p sh', '웺'],
    u'\uc6fb' : ['_w E sh', '웻'],
    u'\uc6fc' : ['_w E s', '웼'],
    u'\uc6fd' : ['_w E N', '웽'],
    u'\uc6fe' : ['_w E tS', '웾'],
    u'\uc6ff' : ['_w E tSh', '웿'],
    u'\uc700' : ['_w E k_h', '윀'],
    u'\uc701' : ['_w E t_h', '윁'],
    u'\uc702' : ['_w E p_h', '윂'],
    u'\uc703' : ['_w E _h', '윃'],
    u'\uc704' : ['2', '위'],
    u'\uc705' : ['2 k', '윅'],
    u'\uc706' : ['2 k_>', '윆'],
    u'\uc707' : ['2 k sh', '윇'],
    u'\uc708' : ['2 _n', '윈'],
    u'\uc709' : ['2 _n tS', '윉'],
    u'\uc70a' : ['2 _n _h', '윊'],
    u'\uc70b' : ['2 t', '윋'],
    u'\uc70c' : ['2 _l', '윌'],
    u'\uc70d' : ['2 _l k', '윍'],
    u'\uc70e' : ['2 _l m', '윎'],
    u'\uc70f' : ['2 _l p', '윏'],
    u'\uc710' : ['2 _l sh', '윐'],
    u'\uc711' : ['2 _l t_h', '윑'],
    u'\uc712' : ['2 _l p_h', '윒'],
    u'\uc713' : ['2 _l _h', '윓'],
    u'\uc714' : ['2 m', '윔'],
    u'\uc715' : ['2 p', '윕'],
    u'\uc716' : ['2 p sh', '윖'],
    u'\uc717' : ['2 sh', '윗'],
    u'\uc718' : ['2 s', '윘'],
    u'\uc719' : ['2 N', '윙'],
    u'\uc71a' : ['2 tS', '윚'],
    u'\uc71b' : ['2 tSh', '윛'],
    u'\uc71c' : ['2 k_h', '윜'],
    u'\uc71d' : ['2 t_h', '윝'],
    u'\uc71e' : ['2 p_h', '윞'],
    u'\uc71f' : ['2 _h', '윟'],
    u'\uc720' : ['_j u', '유'],
    u'\uc721' : ['_j u k', '육'],
    u'\uc722' : ['_j u k_>', '윢'],
    u'\uc723' : ['_j u k sh', '윣'],
    u'\uc724' : ['_j u _n', '윤'],
    u'\uc725' : ['_j u _n tS', '윥'],
    u'\uc726' : ['_j u _n _h', '윦'],
    u'\uc727' : ['_j u t', '윧'],
    u'\uc728' : ['_j u _l', '율'],
    u'\uc729' : ['_j u _l k', '윩'],
    u'\uc72a' : ['_j u _l m', '윪'],
    u'\uc72b' : ['_j u _l p', '윫'],
    u'\uc72c' : ['_j u _l sh', '윬'],
    u'\uc72d' : ['_j u _l t_h', '윭'],
    u'\uc72e' : ['_j u _l p_h', '윮'],
    u'\uc72f' : ['_j u _l _h', '윯'],
    u'\uc730' : ['_j u m', '윰'],
    u'\uc731' : ['_j u p', '윱'],
    u'\uc732' : ['_j u p sh', '윲'],
    u'\uc733' : ['_j u sh', '윳'],
    u'\uc734' : ['_j u s', '윴'],
    u'\uc735' : ['_j u N', '융'],
    u'\uc736' : ['_j u tS', '윶'],
    u'\uc737' : ['_j u tSh', '윷'],
    u'\uc738' : ['_j u k_h', '윸'],
    u'\uc739' : ['_j u t_h', '윹'],
    u'\uc73a' : ['_j u p_h', '윺'],
    u'\uc73b' : ['_j u _h', '윻'],
    u'\uc73c' : ['M', '으'],
    u'\uc73d' : ['M k', '윽'],
    u'\uc73e' : ['M k_>', '윾'],
    u'\uc73f' : ['M k sh', '윿'],
    u'\uc740' : ['M _n', '은'],
    u'\uc741' : ['M _n tS', '읁'],
    u'\uc742' : ['M nh', '읂'],
    u'\uc743' : ['M t', '읃'],
    u'\uc744' : ['M _l', '을'],
    u'\uc745' : ['M _l k', '읅'],
    u'\uc746' : ['M _l m', '읆'],
    u'\uc747' : ['M _l p', '읇'],
    u'\uc748' : ['M _l sh', '읈'],
    u'\uc749' : ['M _l t_h', '읉'],
    u'\uc74a' : ['M _l p_h', '읊'],
    u'\uc74b' : ['M _l _h', '읋'],
    u'\uc74c' : ['M m', '음'],
    u'\uc74d' : ['M p', '읍'],
    u'\uc74e' : ['M p sh', '읎'],
    u'\uc74f' : ['M sh', '읏'],
    u'\uc750' : ['M s', '읐'],
    u'\uc751' : ['M N', '응'],
    u'\uc752' : ['M tS', '읒'],
    u'\uc753' : ['M tSh', '읓'],
    u'\uc754' : ['M k_h', '읔'],
    u'\uc755' : ['M t_h', '읕'],
    u'\uc756' : ['M p_h', '읖'],
    u'\uc757' : ['M _h', '읗'],
    u'\uc758' : ['M _j', '의'],
    u'\uc759' : ['M _j k', '읙'],
    u'\uc75a' : ['M _j k_>', '읚'],
    u'\uc75b' : ['M _j k sh', '읛'],
    u'\uc75c' : ['M _j _n', '읜'],
    u'\uc75d' : ['M _j _n tS', '읝'],
    u'\uc75e' : ['M _j _n _h', '읞'],
    u'\uc75f' : ['M _j t', '읟'],
    u'\uc760' : ['M _j _l', '읠'],
    u'\uc761' : ['M _j _l k', '읡'],
    u'\uc762' : ['M _j _l m', '읢'],
    u'\uc763' : ['M _j _l p', '읣'],
    u'\uc764' : ['M _j _l sh', '읤'],
    u'\uc765' : ['M _j _l t_h', '읥'],
    u'\uc766' : ['M _j _l p_h', '읦'],
    u'\uc767' : ['M _j _l _h', '읧'],
    u'\uc768' : ['M _j m', '읨'],
    u'\uc769' : ['M _j p', '읩'],
    u'\uc76a' : ['M _j p sh', '읪'],
    u'\uc76b' : ['M _j sh', '읫'],
    u'\uc76c' : ['M _j s', '읬'],
    u'\uc76d' : ['M _j N', '읭'],
    u'\uc76e' : ['M _j tS', '읮'],
    u'\uc76f' : ['M _j tSh', '읯'],
    u'\uc770' : ['M _j k_h', '읰'],
    u'\uc771' : ['M _j t_h', '읱'],
    u'\uc772' : ['M _j p_h', '읲'],
    u'\uc773' : ['M _j _h', '읳'],
    u'\uc774' : ['i', '이'],
    u'\uc775' : ['i k', '익'],
    u'\uc776' : ['i k_>', '읶'],
    u'\uc777' : ['i k sh', '읷'],
    u'\uc778' : ['i _n', '인'],
    u'\uc779' : ['i _n tS', '읹'],
    u'\uc77a' : ['i _n _h', '읺'],
    u'\uc77b' : ['i t', '읻'],
    u'\uc77c' : ['i _l', '일'],
    u'\uc77d' : ['i _l k', '읽'],
    u'\uc77e' : ['i _l m', '읾'],
    u'\uc77f' : ['i _l p', '읿'],
    u'\uc780' : ['i _l sh', '잀'],
    u'\uc781' : ['i _l t_h', '잁'],
    u'\uc782' : ['i _l p_h', '잂'],
    u'\uc783' : ['i _l _h', '잃'],
    u'\uc784' : ['i m', '임'],
    u'\uc785' : ['i p', '입'],
    u'\uc786' : ['i p sh', '잆'],
    u'\uc787' : ['i sh', '잇'],
    u'\uc788' : ['i s', '있'],
    u'\uc789' : ['i N', '잉'],
    u'\uc78a' : ['i tS', '잊'],
    u'\uc78b' : ['i tSh', '잋'],
    u'\uc78c' : ['i k_h', '잌'],
    u'\uc78d' : ['i t_h', '잍'],
    u'\uc78e' : ['i p_h', '잎'],
    u'\uc78f' : ['i _h', '잏'],
    u'\uc790' : ['tS a', '자'],
    u'\uc791' : ['tS a k', '작'],
    u'\uc792' : ['tS a k_>', '잒'],
    u'\uc793' : ['tS a k sh', '잓'],
    u'\uc794' : ['tS a _n', '잔'],
    u'\uc795' : ['tS a _n tS', '잕'],
    u'\uc796' : ['tS a _n _h', '잖'],
    u'\uc797' : ['tS a t', '잗'],
    u'\uc798' : ['tS a _l', '잘'],
    u'\uc799' : ['tS a _l k', '잙'],
    u'\uc79a' : ['tS a _l m', '잚'],
    u'\uc79b' : ['tS a _l p', '잛'],
    u'\uc79c' : ['tS a _l sh', '잜'],
    u'\uc79d' : ['tS a _l t_h', '잝'],
    u'\uc79e' : ['tS a _l p_h', '잞'],
    u'\uc79f' : ['tS a _l _h', '잟'],
    u'\uc7a0' : ['tS a m', '잠'],
    u'\uc7a1' : ['tS a p', '잡'],
    u'\uc7a2' : ['tS a p sh', '잢'],
    u'\uc7a3' : ['tS a sh', '잣'],
    u'\uc7a4' : ['tS a s', '잤'],
    u'\uc7a5' : ['tS a N', '장'],
    u'\uc7a6' : ['tS a tS', '잦'],
    u'\uc7a7' : ['tS a tSh', '잧'],
    u'\uc7a8' : ['tS a k_h', '잨'],
    u'\uc7a9' : ['tS a t_h', '잩'],
    u'\uc7aa' : ['tS a p_h', '잪'],
    u'\uc7ab' : ['tS a _h', '잫'],
    u'\uc7ac' : ['tS {', '재'],
    u'\uc7ad' : ['tS { k', '잭'],
    u'\uc7ae' : ['tS { k_>', '잮'],
    u'\uc7af' : ['tS { k sh', '잯'],
    u'\uc7b0' : ['tS { _n', '잰'],
    u'\uc7b1' : ['tS { _n tS', '잱'],
    u'\uc7b2' : ['tS { _n _h', '잲'],
    u'\uc7b3' : ['tS { t', '잳'],
    u'\uc7b4' : ['tS { _l', '잴'],
    u'\uc7b5' : ['tS { _l k', '잵'],
    u'\uc7b6' : ['tS { _l m', '잶'],
    u'\uc7b7' : ['tS { _l p', '잷'],
    u'\uc7b8' : ['tS { _l sh', '잸'],
    u'\uc7b9' : ['tS { _l t_h', '잹'],
    u'\uc7ba' : ['tS { _l p_h', '잺'],
    u'\uc7bb' : ['tS { _l _h', '잻'],
    u'\uc7bc' : ['tS { m', '잼'],
    u'\uc7bd' : ['tS { p', '잽'],
    u'\uc7be' : ['tS { p sh', '잾'],
    u'\uc7bf' : ['tS { sh', '잿'],
    u'\uc7c0' : ['tS { s', '쟀'],
    u'\uc7c1' : ['tS { N', '쟁'],
    u'\uc7c2' : ['tS { tS', '쟂'],
    u'\uc7c3' : ['tS { tSh', '쟃'],
    u'\uc7c4' : ['tS { k_h', '쟄'],
    u'\uc7c5' : ['tS { t_h', '쟅'],
    u'\uc7c6' : ['tS { p_h', '쟆'],
    u'\uc7c7' : ['tS { _h', '쟇'],
    u'\uc7c8' : ['tS _j a', '쟈'],
    u'\uc7c9' : ['tS _j a k', '쟉'],
    u'\uc7ca' : ['tS _j a k_>', '쟊'],
    u'\uc7cb' : ['tS _j a k sh', '쟋'],
    u'\uc7cc' : ['tS _j a _n', '쟌'],
    u'\uc7cd' : ['tS _j a _n tS', '쟍'],
    u'\uc7ce' : ['tS _j a _n _h', '쟎'],
    u'\uc7cf' : ['tS _j a t', '쟏'],
    u'\uc7d0' : ['tS _j a _l', '쟐'],
    u'\uc7d1' : ['tS _j a _l k', '쟑'],
    u'\uc7d2' : ['tS _j a _l m', '쟒'],
    u'\uc7d3' : ['tS _j a _l p', '쟓'],
    u'\uc7d4' : ['tS _j a _l sh', '쟔'],
    u'\uc7d5' : ['tS _j a _l t_h', '쟕'],
    u'\uc7d6' : ['tS _j a _l p_h', '쟖'],
    u'\uc7d7' : ['tS _j a _l _h', '쟗'],
    u'\uc7d8' : ['tS _j a m', '쟘'],
    u'\uc7d9' : ['tS _j a p', '쟙'],
    u'\uc7da' : ['tS _j a p sh', '쟚'],
    u'\uc7db' : ['tS _j a sh', '쟛'],
    u'\uc7dc' : ['tS _j a s', '쟜'],
    u'\uc7dd' : ['tS _j a N', '쟝'],
    u'\uc7de' : ['tS _j a tS', '쟞'],
    u'\uc7df' : ['tS _j a tSh', '쟟'],
    u'\uc7e0' : ['tS _j a k_h', '쟠'],
    u'\uc7e1' : ['tS _j a t_h', '쟡'],
    u'\uc7e2' : ['tS _j a p_h', '쟢'],
    u'\uc7e3' : ['tS _j a _h', '쟣'],
    u'\uc7e4' : ['tS _j {', '쟤'],
    u'\uc7e5' : ['tS _j { k', '쟥'],
    u'\uc7e6' : ['tS _j { k_>', '쟦'],
    u'\uc7e7' : ['tS _j { k sh', '쟧'],
    u'\uc7e8' : ['tS _j { _n', '쟨'],
    u'\uc7e9' : ['tS _j { _n tS', '쟩'],
    u'\uc7ea' : ['tS _j { _n _h', '쟪'],
    u'\uc7eb' : ['tS _j { t', '쟫'],
    u'\uc7ec' : ['tS _j { _l', '쟬'],
    u'\uc7ed' : ['tS _j { _l k', '쟭'],
    u'\uc7ee' : ['tS _j { _l m', '쟮'],
    u'\uc7ef' : ['tS _j { _l p', '쟯'],
    u'\uc7f0' : ['tS _j { _l sh', '쟰'],
    u'\uc7f1' : ['tS _j { _l t_h', '쟱'],
    u'\uc7f2' : ['tS _j { _l p_h', '쟲'],
    u'\uc7f3' : ['tS _j { _l _h', '쟳'],
    u'\uc7f4' : ['tS _j { m', '쟴'],
    u'\uc7f5' : ['tS _j { p', '쟵'],
    u'\uc7f6' : ['tS _j { p sh', '쟶'],
    u'\uc7f7' : ['tS _j { sh', '쟷'],
    u'\uc7f8' : ['tS _j { s', '쟸'],
    u'\uc7f9' : ['tS _j { N', '쟹'],
    u'\uc7fa' : ['tS _j { tS', '쟺'],
    u'\uc7fb' : ['tS _j { tSh', '쟻'],
    u'\uc7fc' : ['tS _j { k_h', '쟼'],
    u'\uc7fd' : ['tS _j { t_h', '쟽'],
    u'\uc7fe' : ['tS _j { p_h', '쟾'],
    u'\uc7ff' : ['tS _j { _h', '쟿'],
    u'\uc800' : ['tS _r', '저'],
    u'\uc801' : ['tS _r k', '적'],
    u'\uc802' : ['tS _r k_>', '젂'],
    u'\uc803' : ['tS _r k sh', '젃'],
    u'\uc804' : ['tS _r _n', '전'],
    u'\uc805' : ['tS _r _n tS', '젅'],
    u'\uc806' : ['tS _r _n _h', '젆'],
    u'\uc807' : ['tS _r t', '젇'],
    u'\uc808' : ['tS _r _l', '절'],
    u'\uc809' : ['tS _r _l k', '젉'],
    u'\uc80a' : ['tS _r _l m', '젊'],
    u'\uc80b' : ['tS _r _l p', '젋'],
    u'\uc80c' : ['tS _r _l sh', '젌'],
    u'\uc80d' : ['tS _r _l t_h', '젍'],
    u'\uc80e' : ['tS _r _l p_h', '젎'],
    u'\uc80f' : ['tS _r _l _h', '젏'],
    u'\uc810' : ['tS _r m', '점'],
    u'\uc811' : ['tS _r p', '접'],
    u'\uc812' : ['tS _r p sh', '젒'],
    u'\uc813' : ['tS _r sh', '젓'],
    u'\uc814' : ['tS _r s', '젔'],
    u'\uc815' : ['tS _r N', '정'],
    u'\uc816' : ['tS _r tS', '젖'],
    u'\uc817' : ['tS _r tSh', '젗'],
    u'\uc818' : ['tS _r k_h', '젘'],
    u'\uc819' : ['tS _r t_h', '젙'],
    u'\uc81a' : ['tS _r p_h', '젚'],
    u'\uc81b' : ['tS _r _h', '젛'],
    u'\uc81c' : ['tS e', '제'],
    u'\uc81d' : ['tS e k', '젝'],
    u'\uc81e' : ['tS e k_>', '젞'],
    u'\uc81f' : ['tS e k sh', '젟'],
    u'\uc820' : ['tS e _n', '젠'],
    u'\uc821' : ['tS e _n tS', '젡'],
    u'\uc822' : ['tS e _n _h', '젢'],
    u'\uc823' : ['tS e t', '젣'],
    u'\uc824' : ['tS e _l', '젤'],
    u'\uc825' : ['tS e _l k', '젥'],
    u'\uc826' : ['tS e _l m', '젦'],
    u'\uc827' : ['tS e _l p', '젧'],
    u'\uc828' : ['tS e _l sh', '젨'],
    u'\uc829' : ['tS e _l t_h', '젩'],
    u'\uc82a' : ['tS e _l p_h', '젪'],
    u'\uc82b' : ['tS e _l _h', '젫'],
    u'\uc82c' : ['tS e m', '젬'],
    u'\uc82d' : ['tS e p', '젭'],
    u'\uc82e' : ['tS e p sh', '젮'],
    u'\uc82f' : ['tS e sh', '젯'],
    u'\uc830' : ['tS e s', '젰'],
    u'\uc831' : ['tS e N', '젱'],
    u'\uc832' : ['tS e tS', '젲'],
    u'\uc833' : ['tS e tSh', '젳'],
    u'\uc834' : ['tS e k_h', '젴'],
    u'\uc835' : ['tS e t_h', '젵'],
    u'\uc836' : ['tS e p_h', '젶'],
    u'\uc837' : ['tS e _h', '젷'],
    u'\uc838' : ['tS _j _r', '져'],
    u'\uc839' : ['tS _j _r k', '젹'],
    u'\uc83a' : ['tS _j _r k_>', '젺'],
    u'\uc83b' : ['tS _j _r k sh', '젻'],
    u'\uc83c' : ['tS _j _r _n', '젼'],
    u'\uc83d' : ['tS _j _r _n tS', '젽'],
    u'\uc83e' : ['tS _j _r _n _h', '젾'],
    u'\uc83f' : ['tS _j _r t', '젿'],
    u'\uc840' : ['tS _j _r _l', '졀'],
    u'\uc841' : ['tS _j _r _l k', '졁'],
    u'\uc842' : ['tS _j _r _l m', '졂'],
    u'\uc843' : ['tS _j _r _l p', '졃'],
    u'\uc844' : ['tS _j _r _l sh', '졄'],
    u'\uc845' : ['tS _j _r _l t_h', '졅'],
    u'\uc846' : ['tS _j _r _l p_h', '졆'],
    u'\uc847' : ['tS _j _r _l _h', '졇'],
    u'\uc848' : ['tS _j _r m', '졈'],
    u'\uc849' : ['tS _j _r p', '졉'],
    u'\uc84a' : ['tS _j _r p sh', '졊'],
    u'\uc84b' : ['tS _j _r sh', '졋'],
    u'\uc84c' : ['tS _j _r s', '졌'],
    u'\uc84d' : ['tS _j _r N', '졍'],
    u'\uc84e' : ['tS _j _r tS', '졎'],
    u'\uc84f' : ['tS _j _r tSh', '졏'],
    u'\uc850' : ['tS _j _r k_h', '졐'],
    u'\uc851' : ['tS _j _r t_h', '졑'],
    u'\uc852' : ['tS _j _r p_h', '졒'],
    u'\uc853' : ['tS _j _r _h', '졓'],
    u'\uc854' : ['tS _j e', '졔'],
    u'\uc855' : ['tS _j e k', '졕'],
    u'\uc856' : ['tS _j e k_>', '졖'],
    u'\uc857' : ['tS _j e k sh', '졗'],
    u'\uc858' : ['tS _j e _n', '졘'],
    u'\uc859' : ['tS _j e _n tS', '졙'],
    u'\uc85a' : ['tS _j e _n _h', '졚'],
    u'\uc85b' : ['tS _j e t', '졛'],
    u'\uc85c' : ['tS _j e _l', '졜'],
    u'\uc85d' : ['tS _j e _l k', '졝'],
    u'\uc85e' : ['tS _j e _l m', '졞'],
    u'\uc85f' : ['tS _j e _l p', '졟'],
    u'\uc860' : ['tS _j e _l sh', '졠'],
    u'\uc861' : ['tS _j e _l t_h', '졡'],
    u'\uc862' : ['tS _j e _l p_h', '졢'],
    u'\uc863' : ['tS _j e _l _h', '졣'],
    u'\uc864' : ['tS _j e m', '졤'],
    u'\uc865' : ['tS _j e p', '졥'],
    u'\uc866' : ['tS _j e p sh', '졦'],
    u'\uc867' : ['tS _j e sh', '졧'],
    u'\uc868' : ['tS _j e s', '졨'],
    u'\uc869' : ['tS _j e N', '졩'],
    u'\uc86a' : ['tS _j e tS', '졪'],
    u'\uc86b' : ['tS _j e tSh', '졫'],
    u'\uc86c' : ['tS _j e k_h', '졬'],
    u'\uc86d' : ['tS _j e t_h', '졭'],
    u'\uc86e' : ['tS _j e p_h', '졮'],
    u'\uc86f' : ['tS _j e _h', '졯'],
    u'\uc870' : ['tS o', '조'],
    u'\uc871' : ['tS o k', '족'],
    u'\uc872' : ['tS o k_>', '졲'],
    u'\uc873' : ['tS o k sh', '졳'],
    u'\uc874' : ['tS o _n', '존'],
    u'\uc875' : ['tS o _n tS', '졵'],
    u'\uc876' : ['tS o _n _h', '졶'],
    u'\uc877' : ['tS o t', '졷'],
    u'\uc878' : ['tS o _l', '졸'],
    u'\uc879' : ['tS o _l k', '졹'],
    u'\uc87a' : ['tS o _l m', '졺'],
    u'\uc87b' : ['tS o _l p', '졻'],
    u'\uc87c' : ['tS o _l sh', '졼'],
    u'\uc87d' : ['tS o _l t_h', '졽'],
    u'\uc87e' : ['tS o _l p_h', '졾'],
    u'\uc87f' : ['tS o _l _h', '졿'],
    u'\uc880' : ['tS o m', '좀'],
    u'\uc881' : ['tS o p', '좁'],
    u'\uc882' : ['tS o p sh', '좂'],
    u'\uc883' : ['tS o sh', '좃'],
    u'\uc884' : ['tS o s', '좄'],
    u'\uc885' : ['tS o N', '종'],
    u'\uc886' : ['tS o tS', '좆'],
    u'\uc887' : ['tS o tSh', '좇'],
    u'\uc888' : ['tS o k_h', '좈'],
    u'\uc889' : ['tS o t_h', '좉'],
    u'\uc88a' : ['tS o p_h', '좊'],
    u'\uc88b' : ['tS o _h', '좋'],
    u'\uc88c' : ['tS _w a', '좌'],
    u'\uc88d' : ['tS _w a k', '좍'],
    u'\uc88e' : ['tS _w a k_>', '좎'],
    u'\uc88f' : ['tS _w a k sh', '좏'],
    u'\uc890' : ['tS _w a _n', '좐'],
    u'\uc891' : ['tS _w a _n tS', '좑'],
    u'\uc892' : ['tS _w a _n _h', '좒'],
    u'\uc893' : ['tS _w a t', '좓'],
    u'\uc894' : ['tS _w a _l', '좔'],
    u'\uc895' : ['tS _w a _l k', '좕'],
    u'\uc896' : ['tS _w a _l m', '좖'],
    u'\uc897' : ['tS _w a _l p', '좗'],
    u'\uc898' : ['tS _w a _l sh', '좘'],
    u'\uc899' : ['tS _w a _l t_h', '좙'],
    u'\uc89a' : ['tS _w a _l p_h', '좚'],
    u'\uc89b' : ['tS _w a _l _h', '좛'],
    u'\uc89c' : ['tS _w a m', '좜'],
    u'\uc89d' : ['tS _w a p', '좝'],
    u'\uc89e' : ['tS _w a p sh', '좞'],
    u'\uc89f' : ['tS _w a sh', '좟'],
    u'\uc8a0' : ['tS _w a s', '좠'],
    u'\uc8a1' : ['tS _w a N', '좡'],
    u'\uc8a2' : ['tS _w a tS', '좢'],
    u'\uc8a3' : ['tS _w a tSh', '좣'],
    u'\uc8a4' : ['tS _w a k_h', '좤'],
    u'\uc8a5' : ['tS _w a t_h', '좥'],
    u'\uc8a6' : ['tS _w a p_h', '좦'],
    u'\uc8a7' : ['tS _w a _h', '좧'],
    u'\uc8a8' : ['tS _w {', '좨'],
    u'\uc8a9' : ['tS _w { k', '좩'],
    u'\uc8aa' : ['tS _w { k_>', '좪'],
    u'\uc8ab' : ['tS _w { k sh', '좫'],
    u'\uc8ac' : ['tS _w { _n', '좬'],
    u'\uc8ad' : ['tS _w { _n tS', '좭'],
    u'\uc8ae' : ['tS _w { _n _h', '좮'],
    u'\uc8af' : ['tS _w { t', '좯'],
    u'\uc8b0' : ['tS _w { _l', '좰'],
    u'\uc8b1' : ['tS _w { _l k', '좱'],
    u'\uc8b2' : ['tS _w { _l m', '좲'],
    u'\uc8b3' : ['tS _w { _l p', '좳'],
    u'\uc8b4' : ['tS _w { _l sh', '좴'],
    u'\uc8b5' : ['tS _w { _l t_h', '좵'],
    u'\uc8b6' : ['tS _w { _l p_h', '좶'],
    u'\uc8b7' : ['tS _w { _l _h', '좷'],
    u'\uc8b8' : ['tS _w { m', '좸'],
    u'\uc8b9' : ['tS _w { p', '좹'],
    u'\uc8ba' : ['tS _w { p sh', '좺'],
    u'\uc8bb' : ['tS _w { sh', '좻'],
    u'\uc8bc' : ['tS _w { s', '좼'],
    u'\uc8bd' : ['tS _w { N', '좽'],
    u'\uc8be' : ['tS _w { tS', '좾'],
    u'\uc8bf' : ['tS _w { tSh', '좿'],
    u'\uc8c0' : ['tS _w { k_h', '죀'],
    u'\uc8c1' : ['tS _w { t_h', '죁'],
    u'\uc8c2' : ['tS _w { p_h', '죂'],
    u'\uc8c3' : ['tS _w { _h', '죃'],
    u'\uc8c4' : ['tS _w e', '죄'],
    u'\uc8c5' : ['tS _w e k', '죅'],
    u'\uc8c6' : ['tS _w e k_>', '죆'],
    u'\uc8c7' : ['tS _w e k sh', '죇'],
    u'\uc8c8' : ['tS _w e _n', '죈'],
    u'\uc8c9' : ['tS _w e _n tS', '죉'],
    u'\uc8ca' : ['tS _w e _n _h', '죊'],
    u'\uc8cb' : ['tS _w e t', '죋'],
    u'\uc8cc' : ['tS _w e _l', '죌'],
    u'\uc8cd' : ['tS _w e _l k', '죍'],
    u'\uc8ce' : ['tS _w e _l m', '죎'],
    u'\uc8cf' : ['tS _w e _l p', '죏'],
    u'\uc8d0' : ['tS _w e _l sh', '죐'],
    u'\uc8d1' : ['tS _w e _l t_h', '죑'],
    u'\uc8d2' : ['tS _w e _l p_h', '죒'],
    u'\uc8d3' : ['tS _w e _l _h', '죓'],
    u'\uc8d4' : ['tS _w e m', '죔'],
    u'\uc8d5' : ['tS _w e p', '죕'],
    u'\uc8d6' : ['tS _w e p sh', '죖'],
    u'\uc8d7' : ['tS _w e sh', '죗'],
    u'\uc8d8' : ['tS _w e s', '죘'],
    u'\uc8d9' : ['tS _w e N', '죙'],
    u'\uc8da' : ['tS _w e tS', '죚'],
    u'\uc8db' : ['tS _w e tSh', '죛'],
    u'\uc8dc' : ['tS _w e k_h', '죜'],
    u'\uc8dd' : ['tS _w e t_h', '죝'],
    u'\uc8de' : ['tS _w e p_h', '죞'],
    u'\uc8df' : ['tS _w e _h', '죟'],
    u'\uc8e0' : ['tS _j o', '죠'],
    u'\uc8e1' : ['tS _j o k', '죡'],
    u'\uc8e2' : ['tS _j o k_>', '죢'],
    u'\uc8e3' : ['tS _j o k sh', '죣'],
    u'\uc8e4' : ['tS _j o _n', '죤'],
    u'\uc8e5' : ['tS _j o _n tS', '죥'],
    u'\uc8e6' : ['tS _j o _n _h', '죦'],
    u'\uc8e7' : ['tS _j o t', '죧'],
    u'\uc8e8' : ['tS _j o _l', '죨'],
    u'\uc8e9' : ['tS _j o _l k', '죩'],
    u'\uc8ea' : ['tS _j o _l m', '죪'],
    u'\uc8eb' : ['tS _j o _l p', '죫'],
    u'\uc8ec' : ['tS _j o _l sh', '죬'],
    u'\uc8ed' : ['tS _j o _l t_h', '죭'],
    u'\uc8ee' : ['tS _j o _l p_h', '죮'],
    u'\uc8ef' : ['tS _j o _l _h', '죯'],
    u'\uc8f0' : ['tS _j o m', '죰'],
    u'\uc8f1' : ['tS _j o p', '죱'],
    u'\uc8f2' : ['tS _j o p sh', '죲'],
    u'\uc8f3' : ['tS _j o sh', '죳'],
    u'\uc8f4' : ['tS _j o s', '죴'],
    u'\uc8f5' : ['tS _j o N', '죵'],
    u'\uc8f6' : ['tS _j o tS', '죶'],
    u'\uc8f7' : ['tS _j o tSh', '죷'],
    u'\uc8f8' : ['tS _j o k_h', '죸'],
    u'\uc8f9' : ['tS _j o t_h', '죹'],
    u'\uc8fa' : ['tS _j o p_h', '죺'],
    u'\uc8fb' : ['tS _j o _h', '죻'],
    u'\uc8fc' : ['tS u', '주'],
    u'\uc8fd' : ['tS u k', '죽'],
    u'\uc8fe' : ['tS u k_>', '죾'],
    u'\uc8ff' : ['tS u k sh', '죿'],
    u'\uc900' : ['tS u _n', '준'],
    u'\uc901' : ['tS u _n tS', '줁'],
    u'\uc902' : ['tS u _n _h', '줂'],
    u'\uc903' : ['tS u t', '줃'],
    u'\uc904' : ['tS u _l', '줄'],
    u'\uc905' : ['tS u _l k', '줅'],
    u'\uc906' : ['tS u _l m', '줆'],
    u'\uc907' : ['tS u _l p', '줇'],
    u'\uc908' : ['tS u _l sh', '줈'],
    u'\uc909' : ['tS u _l t_h', '줉'],
    u'\uc90a' : ['tS u _l p_h', '줊'],
    u'\uc90b' : ['tS u _l _h', '줋'],
    u'\uc90c' : ['tS u m', '줌'],
    u'\uc90d' : ['tS u p', '줍'],
    u'\uc90e' : ['tS u p sh', '줎'],
    u'\uc90f' : ['tS u sh', '줏'],
    u'\uc910' : ['tS u s', '줐'],
    u'\uc911' : ['tS u N', '중'],
    u'\uc912' : ['tS u tS', '줒'],
    u'\uc913' : ['tS u tSh', '줓'],
    u'\uc914' : ['tS u k_h', '줔'],
    u'\uc915' : ['tS u t_h', '줕'],
    u'\uc916' : ['tS u p_h', '줖'],
    u'\uc917' : ['tS u _h', '줗'],
    u'\uc918' : ['tS _w _r', '줘'],
    u'\uc919' : ['tS _w _r k', '줙'],
    u'\uc91a' : ['tS _w _r k_>', '줚'],
    u'\uc91b' : ['tS _w _r k sh', '줛'],
    u'\uc91c' : ['tS _w _r _n', '줜'],
    u'\uc91d' : ['tS _w _r _n tS', '줝'],
    u'\uc91e' : ['tS _w _r _n _h', '줞'],
    u'\uc91f' : ['tS _w _r t', '줟'],
    u'\uc920' : ['tS _w _r _l', '줠'],
    u'\uc921' : ['tS _w _r _l k', '줡'],
    u'\uc922' : ['tS _w _r _l m', '줢'],
    u'\uc923' : ['tS _w _r _l p', '줣'],
    u'\uc924' : ['tS _w _r _l sh', '줤'],
    u'\uc925' : ['tS _w _r _l t_h', '줥'],
    u'\uc926' : ['tS _w _r _l p_h', '줦'],
    u'\uc927' : ['tS _w _r _l _h', '줧'],
    u'\uc928' : ['tS _w _r m', '줨'],
    u'\uc929' : ['tS _w _r p', '줩'],
    u'\uc92a' : ['tS _w _r p sh', '줪'],
    u'\uc92b' : ['tS _w _r sh', '줫'],
    u'\uc92c' : ['tS _w _r s', '줬'],
    u'\uc92d' : ['tS _w _r N', '줭'],
    u'\uc92e' : ['tS _w _r tS', '줮'],
    u'\uc92f' : ['tS _w _r tSh', '줯'],
    u'\uc930' : ['tS _w _r k_h', '줰'],
    u'\uc931' : ['tS _w _r t_h', '줱'],
    u'\uc932' : ['tS _w _r p_h', '줲'],
    u'\uc933' : ['tS _w _r _h', '줳'],
    u'\uc934' : ['tS _w E', '줴'],
    u'\uc935' : ['tS _w E k', '줵'],
    u'\uc936' : ['tS _w E k_>', '줶'],
    u'\uc937' : ['tS _w E k sh', '줷'],
    u'\uc938' : ['tS _w E _n', '줸'],
    u'\uc939' : ['tS _w E _n tS', '줹'],
    u'\uc93a' : ['tS _w E _n _h', '줺'],
    u'\uc93b' : ['tS _w E t', '줻'],
    u'\uc93c' : ['tS _w E _l', '줼'],
    u'\uc93d' : ['tS _w E _l k', '줽'],
    u'\uc93e' : ['tS _w E _l m', '줾'],
    u'\uc93f' : ['tS _w E _l p', '줿'],
    u'\uc940' : ['tS _w E _l sh', '쥀'],
    u'\uc941' : ['tS _w E _l t_h', '쥁'],
    u'\uc942' : ['tS _w E _l p_h', '쥂'],
    u'\uc943' : ['tS _w E _l _h', '쥃'],
    u'\uc944' : ['tS _w E m', '쥄'],
    u'\uc945' : ['tS _w E p', '쥅'],
    u'\uc946' : ['tS _w E p sh', '쥆'],
    u'\uc947' : ['tS _w E sh', '쥇'],
    u'\uc948' : ['tS _w E s', '쥈'],
    u'\uc949' : ['tS _w E N', '쥉'],
    u'\uc94a' : ['tS _w E tS', '쥊'],
    u'\uc94b' : ['tS _w E tSh', '쥋'],
    u'\uc94c' : ['tS _w E k_h', '쥌'],
    u'\uc94d' : ['tS _w E t_h', '쥍'],
    u'\uc94e' : ['tS _w E p_h', '쥎'],
    u'\uc94f' : ['tS _w E _h', '쥏'],
    u'\uc950' : ['tS 2', '쥐'],
    u'\uc951' : ['tS 2 k', '쥑'],
    u'\uc952' : ['tS 2 k_>', '쥒'],
    u'\uc953' : ['tS 2 k sh', '쥓'],
    u'\uc954' : ['tS 2 _n', '쥔'],
    u'\uc955' : ['tS 2 _n tS', '쥕'],
    u'\uc956' : ['tS 2 _n _h', '쥖'],
    u'\uc957' : ['tS 2 t', '쥗'],
    u'\uc958' : ['tS 2 _l', '쥘'],
    u'\uc959' : ['tS 2 _l k', '쥙'],
    u'\uc95a' : ['tS 2 _l m', '쥚'],
    u'\uc95b' : ['tS 2 _l p', '쥛'],
    u'\uc95c' : ['tS 2 _l sh', '쥜'],
    u'\uc95d' : ['tS 2 _l t_h', '쥝'],
    u'\uc95e' : ['tS 2 _l p_h', '쥞'],
    u'\uc95f' : ['tS 2 _l _h', '쥟'],
    u'\uc960' : ['tS 2 m', '쥠'],
    u'\uc961' : ['tS 2 p', '쥡'],
    u'\uc962' : ['tS 2 p sh', '쥢'],
    u'\uc963' : ['tS 2 sh', '쥣'],
    u'\uc964' : ['tS 2 s', '쥤'],
    u'\uc965' : ['tS 2 N', '쥥'],
    u'\uc966' : ['tS 2 tS', '쥦'],
    u'\uc967' : ['tS 2 tSh', '쥧'],
    u'\uc968' : ['tS 2 k_h', '쥨'],
    u'\uc969' : ['tS 2 t_h', '쥩'],
    u'\uc96a' : ['tS 2 p_h', '쥪'],
    u'\uc96b' : ['tS 2 _h', '쥫'],
    u'\uc96c' : ['tS _j u', '쥬'],
    u'\uc96d' : ['tS _j u k', '쥭'],
    u'\uc96e' : ['tS _j u k_>', '쥮'],
    u'\uc96f' : ['tS _j u k sh', '쥯'],
    u'\uc970' : ['tS _j u _n', '쥰'],
    u'\uc971' : ['tS _j u _n tS', '쥱'],
    u'\uc972' : ['tS _j u _n _h', '쥲'],
    u'\uc973' : ['tS _j u t', '쥳'],
    u'\uc974' : ['tS _j u _l', '쥴'],
    u'\uc975' : ['tS _j u _l k', '쥵'],
    u'\uc976' : ['tS _j u _l m', '쥶'],
    u'\uc977' : ['tS _j u _l p', '쥷'],
    u'\uc978' : ['tS _j u _l sh', '쥸'],
    u'\uc979' : ['tS _j u _l t_h', '쥹'],
    u'\uc97a' : ['tS _j u _l p_h', '쥺'],
    u'\uc97b' : ['tS _j u _l _h', '쥻'],
    u'\uc97c' : ['tS _j u m', '쥼'],
    u'\uc97d' : ['tS _j u p', '쥽'],
    u'\uc97e' : ['tS _j u p sh', '쥾'],
    u'\uc97f' : ['tS _j u sh', '쥿'],
    u'\uc980' : ['tS _j u s', '즀'],
    u'\uc981' : ['tS _j u N', '즁'],
    u'\uc982' : ['tS _j u tS', '즂'],
    u'\uc983' : ['tS _j u tSh', '즃'],
    u'\uc984' : ['tS _j u k_h', '즄'],
    u'\uc985' : ['tS _j u t_h', '즅'],
    u'\uc986' : ['tS _j u p_h', '즆'],
    u'\uc987' : ['tS _j u _h', '즇'],
    u'\uc988' : ['tS M', '즈'],
    u'\uc989' : ['tS M k', '즉'],
    u'\uc98a' : ['tS M k_>', '즊'],
    u'\uc98b' : ['tS M k sh', '즋'],
    u'\uc98c' : ['tS M _n', '즌'],
    u'\uc98d' : ['tS M _n tS', '즍'],
    u'\uc98e' : ['tS M _n _h', '즎'],
    u'\uc98f' : ['tS M t', '즏'],
    u'\uc990' : ['tS M _l', '즐'],
    u'\uc991' : ['tS M _l k', '즑'],
    u'\uc992' : ['tS M _l m', '즒'],
    u'\uc993' : ['tS M _l p', '즓'],
    u'\uc994' : ['tS M _l sh', '즔'],
    u'\uc995' : ['tS M _l t_h', '즕'],
    u'\uc996' : ['tS M _l p_h', '즖'],
    u'\uc997' : ['tS M _l _h', '즗'],
    u'\uc998' : ['tS M m', '즘'],
    u'\uc999' : ['tS M p', '즙'],
    u'\uc99a' : ['tS M p sh', '즚'],
    u'\uc99b' : ['tS M sh', '즛'],
    u'\uc99c' : ['tS M s', '즜'],
    u'\uc99d' : ['tS M N', '증'],
    u'\uc99e' : ['tS M tS', '즞'],
    u'\uc99f' : ['tS M tSh', '즟'],
    u'\uc9a0' : ['tS M k_h', '즠'],
    u'\uc9a1' : ['tS M t_h', '즡'],
    u'\uc9a2' : ['tS M p_h', '즢'],
    u'\uc9a3' : ['tS M _h', '즣'],
    u'\uc9a4' : ['tS M _j', '즤'],
    u'\uc9a5' : ['tS M _j k', '즥'],
    u'\uc9a6' : ['tS M _j k_>', '즦'],
    u'\uc9a7' : ['tS M _j k sh', '즧'],
    u'\uc9a8' : ['tS M _j _n', '즨'],
    u'\uc9a9' : ['tS M _j _n tS', '즩'],
    u'\uc9aa' : ['tS M _j _n _h', '즪'],
    u'\uc9ab' : ['tS M _j t', '즫'],
    u'\uc9ac' : ['tS M _j _l', '즬'],
    u'\uc9ad' : ['tS M _j _l k', '즭'],
    u'\uc9ae' : ['tS M _j _l m', '즮'],
    u'\uc9af' : ['tS M _j _l p', '즯'],
    u'\uc9b0' : ['tS M _j _l sh', '즰'],
    u'\uc9b1' : ['tS M _j _l t_h', '즱'],
    u'\uc9b2' : ['tS M _j _l p_h', '즲'],
    u'\uc9b3' : ['tS M _j _l _h', '즳'],
    u'\uc9b4' : ['tS M _j m', '즴'],
    u'\uc9b5' : ['tS M _j p', '즵'],
    u'\uc9b6' : ['tS M _j p sh', '즶'],
    u'\uc9b7' : ['tS M _j sh', '즷'],
    u'\uc9b8' : ['tS M _j s', '즸'],
    u'\uc9b9' : ['tS M _j N', '즹'],
    u'\uc9ba' : ['tS M _j tS', '즺'],
    u'\uc9bb' : ['tS M _j tSh', '즻'],
    u'\uc9bc' : ['tS M _j k_h', '즼'],
    u'\uc9bd' : ['tS M _j t_h', '즽'],
    u'\uc9be' : ['tS M _j p_h', '즾'],
    u'\uc9bf' : ['tS M _j _h', '즿'],
    u'\uc9c0' : ['tS i', '지'],
    u'\uc9c1' : ['tS i k', '직'],
    u'\uc9c2' : ['tS i k_>', '짂'],
    u'\uc9c3' : ['tS i k sh', '짃'],
    u'\uc9c4' : ['tS i _n', '진'],
    u'\uc9c5' : ['tS i _n tS', '짅'],
    u'\uc9c6' : ['tS i _n _h', '짆'],
    u'\uc9c7' : ['tS i t', '짇'],
    u'\uc9c8' : ['tS i _l', '질'],
    u'\uc9c9' : ['tS i _l k', '짉'],
    u'\uc9ca' : ['tS i _l m', '짊'],
    u'\uc9cb' : ['tS i _l p', '짋'],
    u'\uc9cc' : ['tS i _l sh', '짌'],
    u'\uc9cd' : ['tS i _l t_h', '짍'],
    u'\uc9ce' : ['tS i _l p_h', '짎'],
    u'\uc9cf' : ['tS i _l _h', '짏'],
    u'\uc9d0' : ['tS i m', '짐'],
    u'\uc9d1' : ['tS i p', '집'],
    u'\uc9d2' : ['tS i p sh', '짒'],
    u'\uc9d3' : ['tS i sh', '짓'],
    u'\uc9d4' : ['tS i s', '짔'],
    u'\uc9d5' : ['tS i N', '징'],
    u'\uc9d6' : ['tS i tS', '짖'],
    u'\uc9d7' : ['tS i tSh', '짗'],
    u'\uc9d8' : ['tS i k_h', '짘'],
    u'\uc9d9' : ['tS i t_h', '짙'],
    u'\uc9da' : ['tS i p_h', '짚'],
    u'\uc9db' : ['tS i _h', '짛'],
    u'\uc9dc' : ['tS> a', '짜'],
    u'\uc9dd' : ['tS> a k', '짝'],
    u'\uc9de' : ['tS> a k_>', '짞'],
    u'\uc9df' : ['tS> a k sh', '짟'],
    u'\uc9e0' : ['tS> a _n', '짠'],
    u'\uc9e1' : ['tS> a _n tS', '짡'],
    u'\uc9e2' : ['tS> a _n _h', '짢'],
    u'\uc9e3' : ['tS> a t', '짣'],
    u'\uc9e4' : ['tS> a _l', '짤'],
    u'\uc9e5' : ['tS> a _l k', '짥'],
    u'\uc9e6' : ['tS> a _l m', '짦'],
    u'\uc9e7' : ['tS> a _l p', '짧'],
    u'\uc9e8' : ['tS> a _l sh', '짨'],
    u'\uc9e9' : ['tS> a _l t_h', '짩'],
    u'\uc9ea' : ['tS> a _l p_h', '짪'],
    u'\uc9eb' : ['tS> a _l _h', '짫'],
    u'\uc9ec' : ['tS> a m', '짬'],
    u'\uc9ed' : ['tS> a p', '짭'],
    u'\uc9ee' : ['tS> a p sh', '짮'],
    u'\uc9ef' : ['tS> a sh', '짯'],
    u'\uc9f0' : ['tS> a s', '짰'],
    u'\uc9f1' : ['tS> a N', '짱'],
    u'\uc9f2' : ['tS> a tS', '짲'],
    u'\uc9f3' : ['tS> a tSh', '짳'],
    u'\uc9f4' : ['tS> a k_h', '짴'],
    u'\uc9f5' : ['tS> a t_h', '짵'],
    u'\uc9f6' : ['tS> a p_h', '짶'],
    u'\uc9f7' : ['tS> a _h', '짷'],
    u'\uc9f8' : ['tS> {', '째'],
    u'\uc9f9' : ['tS> { k', '짹'],
    u'\uc9fa' : ['tS> { k_>', '짺'],
    u'\uc9fb' : ['tS> { k sh', '짻'],
    u'\uc9fc' : ['tS> { _n', '짼'],
    u'\uc9fd' : ['tS> { _n tS', '짽'],
    u'\uc9fe' : ['tS> { _n _h', '짾'],
    u'\uc9ff' : ['tS> { t', '짿'],
    u'\uca00' : ['tS> { _l', '쨀'],
    u'\uca01' : ['tS> { _l k', '쨁'],
    u'\uca02' : ['tS> { _l m', '쨂'],
    u'\uca03' : ['tS> { _l p', '쨃'],
    u'\uca04' : ['tS> { _l sh', '쨄'],
    u'\uca05' : ['tS> { _l t_h', '쨅'],
    u'\uca06' : ['tS> { _l p_h', '쨆'],
    u'\uca07' : ['tS> { _l _h', '쨇'],
    u'\uca08' : ['tS> { m', '쨈'],
    u'\uca09' : ['tS> { p', '쨉'],
    u'\uca0a' : ['tS> { p sh', '쨊'],
    u'\uca0b' : ['tS> { sh', '쨋'],
    u'\uca0c' : ['tS> { s', '쨌'],
    u'\uca0d' : ['tS> { N', '쨍'],
    u'\uca0e' : ['tS> { tS', '쨎'],
    u'\uca0f' : ['tS> { tSh', '쨏'],
    u'\uca10' : ['tS> { k_h', '쨐'],
    u'\uca11' : ['tS> { t_h', '쨑'],
    u'\uca12' : ['tS> { p_h', '쨒'],
    u'\uca13' : ['tS> { _h', '쨓'],
    u'\uca14' : ['tS> _j a', '쨔'],
    u'\uca15' : ['tS> _j a k', '쨕'],
    u'\uca16' : ['tS> _j a k_>', '쨖'],
    u'\uca17' : ['tS> _j a k sh', '쨗'],
    u'\uca18' : ['tS> _j a _n', '쨘'],
    u'\uca19' : ['tS> _j a _n tS', '쨙'],
    u'\uca1a' : ['tS> _j a _n _h', '쨚'],
    u'\uca1b' : ['tS> _j a t', '쨛'],
    u'\uca1c' : ['tS> _j a _l', '쨜'],
    u'\uca1d' : ['tS> _j a _l k', '쨝'],
    u'\uca1e' : ['tS> _j a _l m', '쨞'],
    u'\uca1f' : ['tS> _j a _l p', '쨟'],
    u'\uca20' : ['tS> _j a _l sh', '쨠'],
    u'\uca21' : ['tS> _j a _l t_h', '쨡'],
    u'\uca22' : ['tS> _j a _l p_h', '쨢'],
    u'\uca23' : ['tS> _j a _l _h', '쨣'],
    u'\uca24' : ['tS> _j a m', '쨤'],
    u'\uca25' : ['tS> _j a p', '쨥'],
    u'\uca26' : ['tS> _j a p sh', '쨦'],
    u'\uca27' : ['tS> _j a sh', '쨧'],
    u'\uca28' : ['tS> _j a s', '쨨'],
    u'\uca29' : ['tS> _j a N', '쨩'],
    u'\uca2a' : ['tS> _j a tS', '쨪'],
    u'\uca2b' : ['tS> _j a tSh', '쨫'],
    u'\uca2c' : ['tS> _j a k_h', '쨬'],
    u'\uca2d' : ['tS> _j a t_h', '쨭'],
    u'\uca2e' : ['tS> _j a p_h', '쨮'],
    u'\uca2f' : ['tS> _j a _h', '쨯'],
    u'\uca30' : ['tS> _j {', '쨰'],
    u'\uca31' : ['tS> _j { k', '쨱'],
    u'\uca32' : ['tS> _j { k_>', '쨲'],
    u'\uca33' : ['tS> _j { k sh', '쨳'],
    u'\uca34' : ['tS> _j { _n', '쨴'],
    u'\uca35' : ['tS> _j { _n tS', '쨵'],
    u'\uca36' : ['tS> _j { _n _h', '쨶'],
    u'\uca37' : ['tS> _j { t', '쨷'],
    u'\uca38' : ['tS> _j { _l', '쨸'],
    u'\uca39' : ['tS> _j { _l k', '쨹'],
    u'\uca3a' : ['tS> _j { _l m', '쨺'],
    u'\uca3b' : ['tS> _j { _l p', '쨻'],
    u'\uca3c' : ['tS> _j { _l sh', '쨼'],
    u'\uca3d' : ['tS> _j { _l t_h', '쨽'],
    u'\uca3e' : ['tS> _j { _l p_h', '쨾'],
    u'\uca3f' : ['tS> _j { _l _h', '쨿'],
    u'\uca40' : ['tS> _j { m', '쩀'],
    u'\uca41' : ['tS> _j { p', '쩁'],
    u'\uca42' : ['tS> _j { p sh', '쩂'],
    u'\uca43' : ['tS> _j { sh', '쩃'],
    u'\uca44' : ['tS> _j { s', '쩄'],
    u'\uca45' : ['tS> _j { N', '쩅'],
    u'\uca46' : ['tS> _j { tS', '쩆'],
    u'\uca47' : ['tS> _j { tSh', '쩇'],
    u'\uca48' : ['tS> _j { k_h', '쩈'],
    u'\uca49' : ['tS> _j { t_h', '쩉'],
    u'\uca4a' : ['tS> _j { p_h', '쩊'],
    u'\uca4b' : ['tS> _j { _h', '쩋'],
    u'\uca4c' : ['tS> _r', '쩌'],
    u'\uca4d' : ['tS> _r k', '쩍'],
    u'\uca4e' : ['tS> _r k_>', '쩎'],
    u'\uca4f' : ['tS> _r k sh', '쩏'],
    u'\uca50' : ['tS> _r _n', '쩐'],
    u'\uca51' : ['tS> _r _n tS', '쩑'],
    u'\uca52' : ['tS> _r _n _h', '쩒'],
    u'\uca53' : ['tS> _r t', '쩓'],
    u'\uca54' : ['tS> _r _l', '쩔'],
    u'\uca55' : ['tS> _r _l k', '쩕'],
    u'\uca56' : ['tS> _r _l m', '쩖'],
    u'\uca57' : ['tS> _r _l p', '쩗'],
    u'\uca58' : ['tS> _r _l sh', '쩘'],
    u'\uca59' : ['tS> _r _l t_h', '쩙'],
    u'\uca5a' : ['tS> _r _l p_h', '쩚'],
    u'\uca5b' : ['tS> _r _l _h', '쩛'],
    u'\uca5c' : ['tS> _r m', '쩜'],
    u'\uca5d' : ['tS> _r p', '쩝'],
    u'\uca5e' : ['tS> _r p sh', '쩞'],
    u'\uca5f' : ['tS> _r sh', '쩟'],
    u'\uca60' : ['tS> _r s', '쩠'],
    u'\uca61' : ['tS> _r N', '쩡'],
    u'\uca62' : ['tS> _r tS', '쩢'],
    u'\uca63' : ['tS> _r tSh', '쩣'],
    u'\uca64' : ['tS> _r k_h', '쩤'],
    u'\uca65' : ['tS> _r t_h', '쩥'],
    u'\uca66' : ['tS> _r p_h', '쩦'],
    u'\uca67' : ['tS> _r _h', '쩧'],
    u'\uca68' : ['tS> e', '쩨'],
    u'\uca69' : ['tS> e k', '쩩'],
    u'\uca6a' : ['tS> e k_>', '쩪'],
    u'\uca6b' : ['tS> e k sh', '쩫'],
    u'\uca6c' : ['tS> e _n', '쩬'],
    u'\uca6d' : ['tS> e _n tS', '쩭'],
    u'\uca6e' : ['tS> e _n _h', '쩮'],
    u'\uca6f' : ['tS> e t', '쩯'],
    u'\uca70' : ['tS> e _l', '쩰'],
    u'\uca71' : ['tS> e _l k', '쩱'],
    u'\uca72' : ['tS> e _l m', '쩲'],
    u'\uca73' : ['tS> e _l p', '쩳'],
    u'\uca74' : ['tS> e _l sh', '쩴'],
    u'\uca75' : ['tS> e _l t_h', '쩵'],
    u'\uca76' : ['tS> e _l p_h', '쩶'],
    u'\uca77' : ['tS> e _l _h', '쩷'],
    u'\uca78' : ['tS> e m', '쩸'],
    u'\uca79' : ['tS> e p', '쩹'],
    u'\uca7a' : ['tS> e p sh', '쩺'],
    u'\uca7b' : ['tS> e sh', '쩻'],
    u'\uca7c' : ['tS> e s', '쩼'],
    u'\uca7d' : ['tS> e N', '쩽'],
    u'\uca7e' : ['tS> e tS', '쩾'],
    u'\uca7f' : ['tS> e tSh', '쩿'],
    u'\uca80' : ['tS> e k_h', '쪀'],
    u'\uca81' : ['tS> e t_h', '쪁'],
    u'\uca82' : ['tS> e p_h', '쪂'],
    u'\uca83' : ['tS> e _h', '쪃'],
    u'\uca84' : ['tS> _j _r', '쪄'],
    u'\uca85' : ['tS> _j _r k', '쪅'],
    u'\uca86' : ['tS> _j _r k_>', '쪆'],
    u'\uca87' : ['tS> _j _r k sh', '쪇'],
    u'\uca88' : ['tS> _j _r _n', '쪈'],
    u'\uca89' : ['tS> _j _r _n tS', '쪉'],
    u'\uca8a' : ['tS> _j _r _n _h', '쪊'],
    u'\uca8b' : ['tS> _j _r t', '쪋'],
    u'\uca8c' : ['tS> _j _r _l', '쪌'],
    u'\uca8d' : ['tS> _j _r _l k', '쪍'],
    u'\uca8e' : ['tS> _j _r _l m', '쪎'],
    u'\uca8f' : ['tS> _j _r _l p', '쪏'],
    u'\uca90' : ['tS> _j _r _l sh', '쪐'],
    u'\uca91' : ['tS> _j _r _l t_h', '쪑'],
    u'\uca92' : ['tS> _j _r _l p_h', '쪒'],
    u'\uca93' : ['tS> _j _r _l _h', '쪓'],
    u'\uca94' : ['tS> _j _r m', '쪔'],
    u'\uca95' : ['tS> _j _r p', '쪕'],
    u'\uca96' : ['tS> _j _r p sh', '쪖'],
    u'\uca97' : ['tS> _j _r sh', '쪗'],
    u'\uca98' : ['tS> _j _r s', '쪘'],
    u'\uca99' : ['tS> _j _r N', '쪙'],
    u'\uca9a' : ['tS> _j _r tS', '쪚'],
    u'\uca9b' : ['tS> _j _r tSh', '쪛'],
    u'\uca9c' : ['tS> _j _r k_h', '쪜'],
    u'\uca9d' : ['tS> _j _r t_h', '쪝'],
    u'\uca9e' : ['tS> _j _r p_h', '쪞'],
    u'\uca9f' : ['tS> _j _r _h', '쪟'],
    u'\ucaa0' : ['tS> _j e', '쪠'],
    u'\ucaa1' : ['tS> _j e k', '쪡'],
    u'\ucaa2' : ['tS> _j e k_>', '쪢'],
    u'\ucaa3' : ['tS> _j e k sh', '쪣'],
    u'\ucaa4' : ['tS> _j e _n', '쪤'],
    u'\ucaa5' : ['tS> _j e _n tS', '쪥'],
    u'\ucaa6' : ['tS> _j e _n _h', '쪦'],
    u'\ucaa7' : ['tS> _j e t', '쪧'],
    u'\ucaa8' : ['tS> _j e _l', '쪨'],
    u'\ucaa9' : ['tS> _j e _l k', '쪩'],
    u'\ucaaa' : ['tS> _j e _l m', '쪪'],
    u'\ucaab' : ['tS> _j e _l p', '쪫'],
    u'\ucaac' : ['tS> _j e _l sh', '쪬'],
    u'\ucaad' : ['tS> _j e _l t_h', '쪭'],
    u'\ucaae' : ['tS> _j e _l p_h', '쪮'],
    u'\ucaaf' : ['tS> _j e _l _h', '쪯'],
    u'\ucab0' : ['tS> _j e m', '쪰'],
    u'\ucab1' : ['tS> _j e p', '쪱'],
    u'\ucab2' : ['tS> _j e p sh', '쪲'],
    u'\ucab3' : ['tS> _j e sh', '쪳'],
    u'\ucab4' : ['tS> _j e s', '쪴'],
    u'\ucab5' : ['tS> _j e N', '쪵'],
    u'\ucab6' : ['tS> _j e tS', '쪶'],
    u'\ucab7' : ['tS> _j e tSh', '쪷'],
    u'\ucab8' : ['tS> _j e k_h', '쪸'],
    u'\ucab9' : ['tS> _j e t_h', '쪹'],
    u'\ucaba' : ['tS> _j e p_h', '쪺'],
    u'\ucabb' : ['tS> _j e _h', '쪻'],
    u'\ucabc' : ['tS> o', '쪼'],
    u'\ucabd' : ['tS> o k', '쪽'],
    u'\ucabe' : ['tS> o k_>', '쪾'],
    u'\ucabf' : ['tS> o k sh', '쪿'],
    u'\ucac0' : ['tS> o _n', '쫀'],
    u'\ucac1' : ['tS> o _n tS', '쫁'],
    u'\ucac2' : ['tS> o _n _h', '쫂'],
    u'\ucac3' : ['tS> o t', '쫃'],
    u'\ucac4' : ['tS> o _l', '쫄'],
    u'\ucac5' : ['tS> o _l k', '쫅'],
    u'\ucac6' : ['tS> o _l m', '쫆'],
    u'\ucac7' : ['tS> o _l p', '쫇'],
    u'\ucac8' : ['tS> o _l sh', '쫈'],
    u'\ucac9' : ['tS> o _l t_h', '쫉'],
    u'\ucaca' : ['tS> o _l p_h', '쫊'],
    u'\ucacb' : ['tS> o _l _h', '쫋'],
    u'\ucacc' : ['tS> o m', '쫌'],
    u'\ucacd' : ['tS> o p', '쫍'],
    u'\ucace' : ['tS> o p sh', '쫎'],
    u'\ucacf' : ['tS> o sh', '쫏'],
    u'\ucad0' : ['tS> o s', '쫐'],
    u'\ucad1' : ['tS> o N', '쫑'],
    u'\ucad2' : ['tS> o tS', '쫒'],
    u'\ucad3' : ['tS> o tSh', '쫓'],
    u'\ucad4' : ['tS> o k_h', '쫔'],
    u'\ucad5' : ['tS> o t_h', '쫕'],
    u'\ucad6' : ['tS> o p_h', '쫖'],
    u'\ucad7' : ['tS> o _h', '쫗'],
    u'\ucad8' : ['tS> _w a', '쫘'],
    u'\ucad9' : ['tS> _w a k', '쫙'],
    u'\ucada' : ['tS> _w a k_>', '쫚'],
    u'\ucadb' : ['tS> _w a k sh', '쫛'],
    u'\ucadc' : ['tS> _w a _n', '쫜'],
    u'\ucadd' : ['tS> _w a _n tS', '쫝'],
    u'\ucade' : ['tS> _w a _n _h', '쫞'],
    u'\ucadf' : ['tS> _w a t', '쫟'],
    u'\ucae0' : ['tS> _w a _l', '쫠'],
    u'\ucae1' : ['tS> _w a _l k', '쫡'],
    u'\ucae2' : ['tS> _w a _l m', '쫢'],
    u'\ucae3' : ['tS> _w a _l p', '쫣'],
    u'\ucae4' : ['tS> _w a _l sh', '쫤'],
    u'\ucae5' : ['tS> _w a _l t_h', '쫥'],
    u'\ucae6' : ['tS> _w a _l p_h', '쫦'],
    u'\ucae7' : ['tS> _w a _l _h', '쫧'],
    u'\ucae8' : ['tS> _w a m', '쫨'],
    u'\ucae9' : ['tS> _w a p', '쫩'],
    u'\ucaea' : ['tS> _w a p sh', '쫪'],
    u'\ucaeb' : ['tS> _w a sh', '쫫'],
    u'\ucaec' : ['tS> _w a s', '쫬'],
    u'\ucaed' : ['tS> _w a N', '쫭'],
    u'\ucaee' : ['tS> _w a tS', '쫮'],
    u'\ucaef' : ['tS> _w a tSh', '쫯'],
    u'\ucaf0' : ['tS> _w a k_h', '쫰'],
    u'\ucaf1' : ['tS> _w a t_h', '쫱'],
    u'\ucaf2' : ['tS> _w a p_h', '쫲'],
    u'\ucaf3' : ['tS> _w a _h', '쫳'],
    u'\ucaf4' : ['tS> _w {', '쫴'],
    u'\ucaf5' : ['tS> _w { k', '쫵'],
    u'\ucaf6' : ['tS> _w { k_>', '쫶'],
    u'\ucaf7' : ['tS> _w { k sh', '쫷'],
    u'\ucaf8' : ['tS> _w { _n', '쫸'],
    u'\ucaf9' : ['tS> _w { _n tS', '쫹'],
    u'\ucafa' : ['tS> _w { _n _h', '쫺'],
    u'\ucafb' : ['tS> _w { t', '쫻'],
    u'\ucafc' : ['tS> _w { _l', '쫼'],
    u'\ucafd' : ['tS> _w { _l k', '쫽'],
    u'\ucafe' : ['tS> _w { _l m', '쫾'],
    u'\ucaff' : ['tS> _w { _l p', '쫿'],
    u'\ucb00' : ['tS> _w { _l sh', '쬀'],
    u'\ucb01' : ['tS> _w { _l t_h', '쬁'],
    u'\ucb02' : ['tS> _w { _l p_h', '쬂'],
    u'\ucb03' : ['tS> _w { _l _h', '쬃'],
    u'\ucb04' : ['tS> _w { m', '쬄'],
    u'\ucb05' : ['tS> _w { p', '쬅'],
    u'\ucb06' : ['tS> _w { p sh', '쬆'],
    u'\ucb07' : ['tS> _w { sh', '쬇'],
    u'\ucb08' : ['tS> _w { s', '쬈'],
    u'\ucb09' : ['tS> _w { N', '쬉'],
    u'\ucb0a' : ['tS> _w { tS', '쬊'],
    u'\ucb0b' : ['tS> _w { tSh', '쬋'],
    u'\ucb0c' : ['tS> _w { k_h', '쬌'],
    u'\ucb0d' : ['tS> _w { t_h', '쬍'],
    u'\ucb0e' : ['tS> _w { p_h', '쬎'],
    u'\ucb0f' : ['tS> _w { _h', '쬏'],
    u'\ucb10' : ['tS> _w e', '쬐'],
    u'\ucb11' : ['tS> _w e k', '쬑'],
    u'\ucb12' : ['tS> _w e k_>', '쬒'],
    u'\ucb13' : ['tS> _w e k sh', '쬓'],
    u'\ucb14' : ['tS> _w e _n', '쬔'],
    u'\ucb15' : ['tS> _w e _n tS', '쬕'],
    u'\ucb16' : ['tS> _w e _n _h', '쬖'],
    u'\ucb17' : ['tS> _w e t', '쬗'],
    u'\ucb18' : ['tS> _w e _l', '쬘'],
    u'\ucb19' : ['tS> _w e _l k', '쬙'],
    u'\ucb1a' : ['tS> _w e _l m', '쬚'],
    u'\ucb1b' : ['tS> _w e _l p', '쬛'],
    u'\ucb1c' : ['tS> _w e _l sh', '쬜'],
    u'\ucb1d' : ['tS> _w e _l t_h', '쬝'],
    u'\ucb1e' : ['tS> _w e _l p_h', '쬞'],
    u'\ucb1f' : ['tS> _w e _l _h', '쬟'],
    u'\ucb20' : ['tS> _w e m', '쬠'],
    u'\ucb21' : ['tS> _w e p', '쬡'],
    u'\ucb22' : ['tS> _w e p sh', '쬢'],
    u'\ucb23' : ['tS> _w e sh', '쬣'],
    u'\ucb24' : ['tS> _w e s', '쬤'],
    u'\ucb25' : ['tS> _w e N', '쬥'],
    u'\ucb26' : ['tS> _w e tS', '쬦'],
    u'\ucb27' : ['tS> _w e tSh', '쬧'],
    u'\ucb28' : ['tS> _w e k_h', '쬨'],
    u'\ucb29' : ['tS> _w e t_h', '쬩'],
    u'\ucb2a' : ['tS> _w e p_h', '쬪'],
    u'\ucb2b' : ['tS> _w e _h', '쬫'],
    u'\ucb2c' : ['tS> _j o', '쬬'],
    u'\ucb2d' : ['tS> _j o k', '쬭'],
    u'\ucb2e' : ['tS> _j o k_>', '쬮'],
    u'\ucb2f' : ['tS> _j o k sh', '쬯'],
    u'\ucb30' : ['tS> _j o _n', '쬰'],
    u'\ucb31' : ['tS> _j o _n tS', '쬱'],
    u'\ucb32' : ['tS> _j o _n _h', '쬲'],
    u'\ucb33' : ['tS> _j o t', '쬳'],
    u'\ucb34' : ['tS> _j o _l', '쬴'],
    u'\ucb35' : ['tS> _j o _l k', '쬵'],
    u'\ucb36' : ['tS> _j o _l m', '쬶'],
    u'\ucb37' : ['tS> _j o _l p', '쬷'],
    u'\ucb38' : ['tS> _j o _l sh', '쬸'],
    u'\ucb39' : ['tS> _j o _l t_h', '쬹'],
    u'\ucb3a' : ['tS> _j o _l p_h', '쬺'],
    u'\ucb3b' : ['tS> _j o _l _h', '쬻'],
    u'\ucb3c' : ['tS> _j o m', '쬼'],
    u'\ucb3d' : ['tS> _j o p', '쬽'],
    u'\ucb3e' : ['tS> _j o p sh', '쬾'],
    u'\ucb3f' : ['tS> _j o sh', '쬿'],
    u'\ucb40' : ['tS> _j o s', '쭀'],
    u'\ucb41' : ['tS> _j o N', '쭁'],
    u'\ucb42' : ['tS> _j o tS', '쭂'],
    u'\ucb43' : ['tS> _j o tSh', '쭃'],
    u'\ucb44' : ['tS> _j o k_h', '쭄'],
    u'\ucb45' : ['tS> _j o t_h', '쭅'],
    u'\ucb46' : ['tS> _j o p_h', '쭆'],
    u'\ucb47' : ['tS> _j o _h', '쭇'],
    u'\ucb48' : ['tS> u', '쭈'],
    u'\ucb49' : ['tS> u k', '쭉'],
    u'\ucb4a' : ['tS> u k_>', '쭊'],
    u'\ucb4b' : ['tS> u k sh', '쭋'],
    u'\ucb4c' : ['tS> u _n', '쭌'],
    u'\ucb4d' : ['tS> u _n tS', '쭍'],
    u'\ucb4e' : ['tS> u _n _h', '쭎'],
    u'\ucb4f' : ['tS> u t', '쭏'],
    u'\ucb50' : ['tS> u _l', '쭐'],
    u'\ucb51' : ['tS> u _l k', '쭑'],
    u'\ucb52' : ['tS> u _l m', '쭒'],
    u'\ucb53' : ['tS> u _l p', '쭓'],
    u'\ucb54' : ['tS> u _l sh', '쭔'],
    u'\ucb55' : ['tS> u _l t_h', '쭕'],
    u'\ucb56' : ['tS> u _l p_h', '쭖'],
    u'\ucb57' : ['tS> u _l _h', '쭗'],
    u'\ucb58' : ['tS> u m', '쭘'],
    u'\ucb59' : ['tS> u p', '쭙'],
    u'\ucb5a' : ['tS> u p sh', '쭚'],
    u'\ucb5b' : ['tS> u sh', '쭛'],
    u'\ucb5c' : ['tS> u s', '쭜'],
    u'\ucb5d' : ['tS> u N', '쭝'],
    u'\ucb5e' : ['tS> u tS', '쭞'],
    u'\ucb5f' : ['tS> u tSh', '쭟'],
    u'\ucb60' : ['tS> u k_h', '쭠'],
    u'\ucb61' : ['tS> u t_h', '쭡'],
    u'\ucb62' : ['tS> u p_h', '쭢'],
    u'\ucb63' : ['tS> u _h', '쭣'],
    u'\ucb64' : ['tS> _w _r', '쭤'],
    u'\ucb65' : ['tS> _w _r k', '쭥'],
    u'\ucb66' : ['tS> _w _r k_>', '쭦'],
    u'\ucb67' : ['tS> _w _r k sh', '쭧'],
    u'\ucb68' : ['tS> _w _r _n', '쭨'],
    u'\ucb69' : ['tS> _w _r _n tS', '쭩'],
    u'\ucb6a' : ['tS> _w _r _n _h', '쭪'],
    u'\ucb6b' : ['tS> _w _r t', '쭫'],
    u'\ucb6c' : ['tS> _w _r _l', '쭬'],
    u'\ucb6d' : ['tS> _w _r _l k', '쭭'],
    u'\ucb6e' : ['tS> _w _r _l m', '쭮'],
    u'\ucb6f' : ['tS> _w _r _l p', '쭯'],
    u'\ucb70' : ['tS> _w _r _l sh', '쭰'],
    u'\ucb71' : ['tS> _w _r _l t_h', '쭱'],
    u'\ucb72' : ['tS> _w _r _l p_h', '쭲'],
    u'\ucb73' : ['tS> _w _r _l _h', '쭳'],
    u'\ucb74' : ['tS> _w _r m', '쭴'],
    u'\ucb75' : ['tS> _w _r p', '쭵'],
    u'\ucb76' : ['tS> _w _r p sh', '쭶'],
    u'\ucb77' : ['tS> _w _r sh', '쭷'],
    u'\ucb78' : ['tS> _w _r s', '쭸'],
    u'\ucb79' : ['tS> _w _r N', '쭹'],
    u'\ucb7a' : ['tS> _w _r tS', '쭺'],
    u'\ucb7b' : ['tS> _w _r tSh', '쭻'],
    u'\ucb7c' : ['tS> _w _r k_h', '쭼'],
    u'\ucb7d' : ['tS> _w _r t_h', '쭽'],
    u'\ucb7e' : ['tS> _w _r p_h', '쭾'],
    u'\ucb7f' : ['tS> _w _r _h', '쭿'],
    u'\ucb80' : ['tS> _w E', '쮀'],
    u'\ucb81' : ['tS> _w E k', '쮁'],
    u'\ucb82' : ['tS> _w E k_>', '쮂'],
    u'\ucb83' : ['tS> _w E k sh', '쮃'],
    u'\ucb84' : ['tS> _w E _n', '쮄'],
    u'\ucb85' : ['tS> _w E _n tS', '쮅'],
    u'\ucb86' : ['tS> _w E _n _h', '쮆'],
    u'\ucb87' : ['tS> _w E t', '쮇'],
    u'\ucb88' : ['tS> _w E _l', '쮈'],
    u'\ucb89' : ['tS> _w E _l k', '쮉'],
    u'\ucb8a' : ['tS> _w E _l m', '쮊'],
    u'\ucb8b' : ['tS> _w E _l p', '쮋'],
    u'\ucb8c' : ['tS> _w E _l sh', '쮌'],
    u'\ucb8d' : ['tS> _w E _l t_h', '쮍'],
    u'\ucb8e' : ['tS> _w E _l p_h', '쮎'],
    u'\ucb8f' : ['tS> _w E _l _h', '쮏'],
    u'\ucb90' : ['tS> _w E m', '쮐'],
    u'\ucb91' : ['tS> _w E p', '쮑'],
    u'\ucb92' : ['tS> _w E p sh', '쮒'],
    u'\ucb93' : ['tS> _w E sh', '쮓'],
    u'\ucb94' : ['tS> _w E s', '쮔'],
    u'\ucb95' : ['tS> _w E N', '쮕'],
    u'\ucb96' : ['tS> _w E tS', '쮖'],
    u'\ucb97' : ['tS> _w E tSh', '쮗'],
    u'\ucb98' : ['tS> _w E k_h', '쮘'],
    u'\ucb99' : ['tS> _w E t_h', '쮙'],
    u'\ucb9a' : ['tS> _w E p_h', '쮚'],
    u'\ucb9b' : ['tS> _w E _h', '쮛'],
    u'\ucb9c' : ['tS> 2', '쮜'],
    u'\ucb9d' : ['tS> 2 k', '쮝'],
    u'\ucb9e' : ['tS> 2 k_>', '쮞'],
    u'\ucb9f' : ['tS> 2 k sh', '쮟'],
    u'\ucba0' : ['tS> 2 _n', '쮠'],
    u'\ucba1' : ['tS> 2 _n tS', '쮡'],
    u'\ucba2' : ['tS> 2 _n _h', '쮢'],
    u'\ucba3' : ['tS> 2 t', '쮣'],
    u'\ucba4' : ['tS> 2 _l', '쮤'],
    u'\ucba5' : ['tS> 2 _l k', '쮥'],
    u'\ucba6' : ['tS> 2 _l m', '쮦'],
    u'\ucba7' : ['tS> 2 _l p', '쮧'],
    u'\ucba8' : ['tS> 2 _l sh', '쮨'],
    u'\ucba9' : ['tS> 2 _l t_h', '쮩'],
    u'\ucbaa' : ['tS> 2 _l p_h', '쮪'],
    u'\ucbab' : ['tS> 2 _l _h', '쮫'],
    u'\ucbac' : ['tS> 2 m', '쮬'],
    u'\ucbad' : ['tS> 2 p', '쮭'],
    u'\ucbae' : ['tS> 2 p sh', '쮮'],
    u'\ucbaf' : ['tS> 2 sh', '쮯'],
    u'\ucbb0' : ['tS> 2 s', '쮰'],
    u'\ucbb1' : ['tS> 2 N', '쮱'],
    u'\ucbb2' : ['tS> 2 tS', '쮲'],
    u'\ucbb3' : ['tS> 2 tSh', '쮳'],
    u'\ucbb4' : ['tS> 2 k_h', '쮴'],
    u'\ucbb5' : ['tS> 2 t_h', '쮵'],
    u'\ucbb6' : ['tS> 2 p_h', '쮶'],
    u'\ucbb7' : ['tS> 2 _h', '쮷'],
    u'\ucbb8' : ['tS> _j u', '쮸'],
    u'\ucbb9' : ['tS> _j u k', '쮹'],
    u'\ucbba' : ['tS> _j u k_>', '쮺'],
    u'\ucbbb' : ['tS> _j u k sh', '쮻'],
    u'\ucbbc' : ['tS> _j u _n', '쮼'],
    u'\ucbbd' : ['tS> _j u _n tS', '쮽'],
    u'\ucbbe' : ['tS> _j u _n _h', '쮾'],
    u'\ucbbf' : ['tS> _j u t', '쮿'],
    u'\ucbc0' : ['tS> _j u _l', '쯀'],
    u'\ucbc1' : ['tS> _j u _l k', '쯁'],
    u'\ucbc2' : ['tS> _j u _l m', '쯂'],
    u'\ucbc3' : ['tS> _j u _l p', '쯃'],
    u'\ucbc4' : ['tS> _j u _l sh', '쯄'],
    u'\ucbc5' : ['tS> _j u _l t_h', '쯅'],
    u'\ucbc6' : ['tS> _j u _l p_h', '쯆'],
    u'\ucbc7' : ['tS> _j u _l _h', '쯇'],
    u'\ucbc8' : ['tS> _j u m', '쯈'],
    u'\ucbc9' : ['tS> _j u p', '쯉'],
    u'\ucbca' : ['tS> _j u p sh', '쯊'],
    u'\ucbcb' : ['tS> _j u sh', '쯋'],
    u'\ucbcc' : ['tS> _j u s', '쯌'],
    u'\ucbcd' : ['tS> _j u N', '쯍'],
    u'\ucbce' : ['tS> _j u tS', '쯎'],
    u'\ucbcf' : ['tS> _j u tSh', '쯏'],
    u'\ucbd0' : ['tS> _j u k_h', '쯐'],
    u'\ucbd1' : ['tS> _j u t_h', '쯑'],
    u'\ucbd2' : ['tS> _j u p_h', '쯒'],
    u'\ucbd3' : ['tS> _j u _h', '쯓'],
    u'\ucbd4' : ['tS> M', '쯔'],
    u'\ucbd5' : ['tS> M k', '쯕'],
    u'\ucbd6' : ['tS> M k_>', '쯖'],
    u'\ucbd7' : ['tS> M k s', '쯗'],
    u'\ucbd8' : ['tS> M _n', '쯘'],
    u'\ucbd9' : ['tS> M _n tS', '쯙'],
    u'\ucbda' : ['tS> M _n _h', '쯚'],
    u'\ucbdb' : ['tS> M t', '쯛'],
    u'\ucbdc' : ['tS> M _l', '쯜'],
    u'\ucbdd' : ['tS> M _l k', '쯝'],
    u'\ucbde' : ['tS> M _l m', '쯞'],
    u'\ucbdf' : ['tS> M _l p', '쯟'],
    u'\ucbe0' : ['tS> M _l sh', '쯠'],
    u'\ucbe1' : ['tS> M _l t_h', '쯡'],
    u'\ucbe2' : ['tS> M _l p_h', '쯢'],
    u'\ucbe3' : ['tS> M _l _h', '쯣'],
    u'\ucbe4' : ['tS> M m', '쯤'],
    u'\ucbe5' : ['tS> M p', '쯥'],
    u'\ucbe6' : ['tS> M p sh', '쯦'],
    u'\ucbe7' : ['tS> M sh', '쯧'],
    u'\ucbe8' : ['tS> M s', '쯨'],
    u'\ucbe9' : ['tS> M N', '쯩'],
    u'\ucbea' : ['tS> M tS', '쯪'],
    u'\ucbeb' : ['tS> M tSh', '쯫'],
    u'\ucbec' : ['tS> M k_h', '쯬'],
    u'\ucbed' : ['tS> M t_h', '쯭'],
    u'\ucbee' : ['tS> M p_h', '쯮'],
    u'\ucbef' : ['tS> M _h', '쯯'],
    u'\ucbf0' : ['tS> M _j', '쯰'],
    u'\ucbf1' : ['tS> M _j k', '쯱'],
    u'\ucbf2' : ['tS> M _j k_>', '쯲'],
    u'\ucbf3' : ['tS> M _j k sh', '쯳'],
    u'\ucbf4' : ['tS> M _j _n', '쯴'],
    u'\ucbf5' : ['tS> M _j _n tS', '쯵'],
    u'\ucbf6' : ['tS> M _j _n _h', '쯶'],
    u'\ucbf7' : ['tS> M _j t', '쯷'],
    u'\ucbf8' : ['tS> M _j _l', '쯸'],
    u'\ucbf9' : ['tS> M _j _l k', '쯹'],
    u'\ucbfa' : ['tS> M _j _l m', '쯺'],
    u'\ucbfb' : ['tS> M _j _l p', '쯻'],
    u'\ucbfc' : ['tS> M _j _l sh', '쯼'],
    u'\ucbfd' : ['tS> M _j _l t_h', '쯽'],
    u'\ucbfe' : ['tS> M _j _l p_h', '쯾'],
    u'\ucbff' : ['tS> M _j _l _h', '쯿'],
    u'\ucc00' : ['tS> M _j m', '찀'],
    u'\ucc01' : ['tS> M _j p', '찁'],
    u'\ucc02' : ['tS> M _j p sh', '찂'],
    u'\ucc03' : ['tS> M _j sh', '찃'],
    u'\ucc04' : ['tS> M _j s', '찄'],
    u'\ucc05' : ['tS> M _j N', '찅'],
    u'\ucc06' : ['tS> M _j tS', '찆'],
    u'\ucc07' : ['tS> M _j tSh', '찇'],
    u'\ucc08' : ['tS> M _j k_h', '찈'],
    u'\ucc09' : ['tS> M _j t_h', '찉'],
    u'\ucc0a' : ['tS> M _j p_h', '찊'],
    u'\ucc0b' : ['tS> M _j _h', '찋'],
    u'\ucc0c' : ['tS> i', '찌'],
    u'\ucc0d' : ['tS> i k', '찍'],
    u'\ucc0e' : ['tS> i k_>', '찎'],
    u'\ucc0f' : ['tS> i k sh', '찏'],
    u'\ucc10' : ['tS> i _n', '찐'],
    u'\ucc11' : ['tS> i _n tS', '찑'],
    u'\ucc12' : ['tS> i _n _h', '찒'],
    u'\ucc13' : ['tS> i t', '찓'],
    u'\ucc14' : ['tS> i _l', '찔'],
    u'\ucc15' : ['tS> i _l k', '찕'],
    u'\ucc16' : ['tS> i _l m', '찖'],
    u'\ucc17' : ['tS> i _l p', '찗'],
    u'\ucc18' : ['tS> i _l sh', '찘'],
    u'\ucc19' : ['tS> i _l t_h', '찙'],
    u'\ucc1a' : ['tS> i _l p_h', '찚'],
    u'\ucc1b' : ['tS> i _l _h', '찛'],
    u'\ucc1c' : ['tS> i m', '찜'],
    u'\ucc1d' : ['tS> i p', '찝'],
    u'\ucc1e' : ['tS> i p sh', '찞'],
    u'\ucc1f' : ['tS> i sh', '찟'],
    u'\ucc20' : ['tS> i s', '찠'],
    u'\ucc21' : ['tS> i N', '찡'],
    u'\ucc22' : ['tS> i tS', '찢'],
    u'\ucc23' : ['tS> i tSh', '찣'],
    u'\ucc24' : ['tS> i k_h', '찤'],
    u'\ucc25' : ['tS> i t_h', '찥'],
    u'\ucc26' : ['tS> i p_h', '찦'],
    u'\ucc27' : ['tS> i _h', '찧'],
    u'\ucc28' : ['tSh a', '차'],
    u'\ucc29' : ['tSh a k', '착'],
    u'\ucc2a' : ['tSh a k_>', '찪'],
    u'\ucc2b' : ['tSh a k sh', '찫'],
    u'\ucc2c' : ['tSh a _n', '찬'],
    u'\ucc2d' : ['tSh a _n tS', '찭'],
    u'\ucc2e' : ['tSh a _n _h', '찮'],
    u'\ucc2f' : ['tSh a t', '찯'],
    u'\ucc30' : ['tSh a _l', '찰'],
    u'\ucc31' : ['tSh a _l k', '찱'],
    u'\ucc32' : ['tSh a _l m', '찲'],
    u'\ucc33' : ['tSh a _l p', '찳'],
    u'\ucc34' : ['tSh a _l sh', '찴'],
    u'\ucc35' : ['tSh a _l t_h', '찵'],
    u'\ucc36' : ['tSh a _l p_h', '찶'],
    u'\ucc37' : ['tSh a _l _h', '찷'],
    u'\ucc38' : ['tSh a m', '참'],
    u'\ucc39' : ['tSh a p', '찹'],
    u'\ucc3a' : ['tSh a p sh', '찺'],
    u'\ucc3b' : ['tSh a sh', '찻'],
    u'\ucc3c' : ['tSh a s', '찼'],
    u'\ucc3d' : ['tSh a N', '창'],
    u'\ucc3e' : ['tSh a tS', '찾'],
    u'\ucc3f' : ['tSh a tSh', '찿'],
    u'\ucc40' : ['tSh a k_h', '챀'],
    u'\ucc41' : ['tSh a t_h', '챁'],
    u'\ucc42' : ['tSh a p_h', '챂'],
    u'\ucc43' : ['tSh a _h', '챃'],
    u'\ucc44' : ['tSh {', '채'],
    u'\ucc45' : ['tSh { k', '책'],
    u'\ucc46' : ['tSh { k_>', '챆'],
    u'\ucc47' : ['tSh { k sh', '챇'],
    u'\ucc48' : ['tSh { _n', '챈'],
    u'\ucc49' : ['tSh { _n tS', '챉'],
    u'\ucc4a' : ['tSh { _n _h', '챊'],
    u'\ucc4b' : ['tSh { t', '챋'],
    u'\ucc4c' : ['tSh { _l', '챌'],
    u'\ucc4d' : ['tSh { _l k', '챍'],
    u'\ucc4e' : ['tSh { _l m', '챎'],
    u'\ucc4f' : ['tSh { _l p', '챏'],
    u'\ucc50' : ['tSh { _l sh', '챐'],
    u'\ucc51' : ['tSh { _l t_h', '챑'],
    u'\ucc52' : ['tSh { _l p_h', '챒'],
    u'\ucc53' : ['tSh { _l _h', '챓'],
    u'\ucc54' : ['tSh { m', '챔'],
    u'\ucc55' : ['tSh { p', '챕'],
    u'\ucc56' : ['tSh { p sh', '챖'],
    u'\ucc57' : ['tSh { sh', '챗'],
    u'\ucc58' : ['tSh { s', '챘'],
    u'\ucc59' : ['tSh { N', '챙'],
    u'\ucc5a' : ['tSh { tS', '챚'],
    u'\ucc5b' : ['tSh { tSh', '챛'],
    u'\ucc5c' : ['tSh { k_h', '챜'],
    u'\ucc5d' : ['tSh { t_h', '챝'],
    u'\ucc5e' : ['tSh { p_h', '챞'],
    u'\ucc5f' : ['tSh { _h', '챟'],
    u'\ucc60' : ['tSh _j a', '챠'],
    u'\ucc61' : ['tSh _j a k', '챡'],
    u'\ucc62' : ['tSh _j a k_>', '챢'],
    u'\ucc63' : ['tSh _j a k sh', '챣'],
    u'\ucc64' : ['tSh _j a _n', '챤'],
    u'\ucc65' : ['tSh _j a _n tS', '챥'],
    u'\ucc66' : ['tSh _j a _n _h', '챦'],
    u'\ucc67' : ['tSh _j a t', '챧'],
    u'\ucc68' : ['tSh _j a _l', '챨'],
    u'\ucc69' : ['tSh _j a _l k', '챩'],
    u'\ucc6a' : ['tSh _j a _l m', '챪'],
    u'\ucc6b' : ['tSh _j a _l p', '챫'],
    u'\ucc6c' : ['tSh _j a _l sh', '챬'],
    u'\ucc6d' : ['tSh _j a _l t_h', '챭'],
    u'\ucc6e' : ['tSh _j a _l p_h', '챮'],
    u'\ucc6f' : ['tSh _j a _l _h', '챯'],
    u'\ucc70' : ['tSh _j a m', '챰'],
    u'\ucc71' : ['tSh _j a p', '챱'],
    u'\ucc72' : ['tSh _j a p sh', '챲'],
    u'\ucc73' : ['tSh _j a sh', '챳'],
    u'\ucc74' : ['tSh _j a s', '챴'],
    u'\ucc75' : ['tSh _j a N', '챵'],
    u'\ucc76' : ['tSh _j a tS', '챶'],
    u'\ucc77' : ['tSh _j a tSh', '챷'],
    u'\ucc78' : ['tSh _j a k_h', '챸'],
    u'\ucc79' : ['tSh _j a t_h', '챹'],
    u'\ucc7a' : ['tSh _j a p_h', '챺'],
    u'\ucc7b' : ['tSh _j a _h', '챻'],
    u'\ucc7c' : ['tSh _j {', '챼'],
    u'\ucc7d' : ['tSh _j { k', '챽'],
    u'\ucc7e' : ['tSh _j { k_>', '챾'],
    u'\ucc7f' : ['tSh _j { k sh', '챿'],
    u'\ucc80' : ['tSh _j { _n', '첀'],
    u'\ucc81' : ['tSh _j { _n tS', '첁'],
    u'\ucc82' : ['tSh _j { _n _h', '첂'],
    u'\ucc83' : ['tSh _j { t', '첃'],
    u'\ucc84' : ['tSh _j { _l', '첄'],
    u'\ucc85' : ['tSh _j { _l k', '첅'],
    u'\ucc86' : ['tSh _j { _l m', '첆'],
    u'\ucc87' : ['tSh _j { _l p', '첇'],
    u'\ucc88' : ['tSh _j { _l sh', '첈'],
    u'\ucc89' : ['tSh _j { _l t_h', '첉'],
    u'\ucc8a' : ['tSh _j { _l p_h', '첊'],
    u'\ucc8b' : ['tSh _j { _l _h', '첋'],
    u'\ucc8c' : ['tSh _j { m', '첌'],
    u'\ucc8d' : ['tSh _j { p', '첍'],
    u'\ucc8e' : ['tSh _j { p sh', '첎'],
    u'\ucc8f' : ['tSh _j { sh', '첏'],
    u'\ucc90' : ['tSh _j { s', '첐'],
    u'\ucc91' : ['tSh _j { N', '첑'],
    u'\ucc92' : ['tSh _j { tS', '첒'],
    u'\ucc93' : ['tSh _j { tSh', '첓'],
    u'\ucc94' : ['tSh _j { k_h', '첔'],
    u'\ucc95' : ['tSh _j { t_h', '첕'],
    u'\ucc96' : ['tSh _j { p_h', '첖'],
    u'\ucc97' : ['tSh _j { _h', '첗'],
    u'\ucc98' : ['tSh _r', '처'],
    u'\ucc99' : ['tSh _r k', '척'],
    u'\ucc9a' : ['tSh _r k_>', '첚'],
    u'\ucc9b' : ['tSh _r k sh', '첛'],
    u'\ucc9c' : ['tSh _r _n', '천'],
    u'\ucc9d' : ['tSh _r _n tS', '첝'],
    u'\ucc9e' : ['tSh _r _n _h', '첞'],
    u'\ucc9f' : ['tSh _r t', '첟'],
    u'\ucca0' : ['tSh _r _l', '철'],
    u'\ucca1' : ['tSh _r _l k', '첡'],
    u'\ucca2' : ['tSh _r _l m', '첢'],
    u'\ucca3' : ['tSh _r _l p', '첣'],
    u'\ucca4' : ['tSh _r _l sh', '첤'],
    u'\ucca5' : ['tSh _r _l t_h', '첥'],
    u'\ucca6' : ['tSh _r _l p_h', '첦'],
    u'\ucca7' : ['tSh _r _l _h', '첧'],
    u'\ucca8' : ['tSh _r m', '첨'],
    u'\ucca9' : ['tSh _r p', '첩'],
    u'\uccaa' : ['tSh _r p sh', '첪'],
    u'\uccab' : ['tSh _r sh', '첫'],
    u'\uccac' : ['tSh _r s', '첬'],
    u'\uccad' : ['tSh _r N', '청'],
    u'\uccae' : ['tSh _r tS', '첮'],
    u'\uccaf' : ['tSh _r tSh', '첯'],
    u'\uccb0' : ['tSh _r k_h', '첰'],
    u'\uccb1' : ['tSh _r t_h', '첱'],
    u'\uccb2' : ['tSh _r p_h', '첲'],
    u'\uccb3' : ['tSh _r _h', '첳'],
    u'\uccb4' : ['tSh e', '체'],
    u'\uccb5' : ['tSh e k', '첵'],
    u'\uccb6' : ['tSh e k_>', '첶'],
    u'\uccb7' : ['tSh e k sh', '첷'],
    u'\uccb8' : ['tSh e _n', '첸'],
    u'\uccb9' : ['tSh e _n tS', '첹'],
    u'\uccba' : ['tSh e _n _h', '첺'],
    u'\uccbb' : ['tSh e t', '첻'],
    u'\uccbc' : ['tSh e _l', '첼'],
    u'\uccbd' : ['tSh e _l k', '첽'],
    u'\uccbe' : ['tSh e _l m', '첾'],
    u'\uccbf' : ['tSh e _l p', '첿'],
    u'\uccc0' : ['tSh e _l sh', '쳀'],
    u'\uccc1' : ['tSh e _l t_h', '쳁'],
    u'\uccc2' : ['tSh e _l p_h', '쳂'],
    u'\uccc3' : ['tSh e _l _h', '쳃'],
    u'\uccc4' : ['tSh e m', '쳄'],
    u'\uccc5' : ['tSh e p', '쳅'],
    u'\uccc6' : ['tSh e p sh', '쳆'],
    u'\uccc7' : ['tSh e sh', '쳇'],
    u'\uccc8' : ['tSh e s', '쳈'],
    u'\uccc9' : ['tSh e N', '쳉'],
    u'\uccca' : ['tSh e tS', '쳊'],
    u'\ucccb' : ['tSh e tSh', '쳋'],
    u'\ucccc' : ['tSh e k_h', '쳌'],
    u'\ucccd' : ['tSh e t_h', '쳍'],
    u'\uccce' : ['tSh e p_h', '쳎'],
    u'\ucccf' : ['tSh e _h', '쳏'],
    u'\uccd0' : ['tSh _j _r', '쳐'],
    u'\uccd1' : ['tSh _j _r k', '쳑'],
    u'\uccd2' : ['tSh _j _r k_>', '쳒'],
    u'\uccd3' : ['tSh _j _r k sh', '쳓'],
    u'\uccd4' : ['tSh _j _r _n', '쳔'],
    u'\uccd5' : ['tSh _j _r _n tS', '쳕'],
    u'\uccd6' : ['tSh _j _r _n _h', '쳖'],
    u'\uccd7' : ['tSh _j _r t', '쳗'],
    u'\uccd8' : ['tSh _j _r _l', '쳘'],
    u'\uccd9' : ['tSh _j _r _l k', '쳙'],
    u'\uccda' : ['tSh _j _r _l m', '쳚'],
    u'\uccdb' : ['tSh _j _r _l p', '쳛'],
    u'\uccdc' : ['tSh _j _r _l sh', '쳜'],
    u'\uccdd' : ['tSh _j _r _l t_h', '쳝'],
    u'\uccde' : ['tSh _j _r _l p_h', '쳞'],
    u'\uccdf' : ['tSh _j _r _l _h', '쳟'],
    u'\ucce0' : ['tSh _j _r m', '쳠'],
    u'\ucce1' : ['tSh _j _r p', '쳡'],
    u'\ucce2' : ['tSh _j _r p sh', '쳢'],
    u'\ucce3' : ['tSh _j _r sh', '쳣'],
    u'\ucce4' : ['tSh _j _r s', '쳤'],
    u'\ucce5' : ['tSh _j _r N', '쳥'],
    u'\ucce6' : ['tSh _j _r tS', '쳦'],
    u'\ucce7' : ['tSh _j _r tSh', '쳧'],
    u'\ucce8' : ['tSh _j _r k_h', '쳨'],
    u'\ucce9' : ['tSh _j _r t_h', '쳩'],
    u'\uccea' : ['tSh _j _r p_h', '쳪'],
    u'\ucceb' : ['tSh _j _r _h', '쳫'],
    u'\uccec' : ['tSh _j e', '쳬'],
    u'\ucced' : ['tSh _j e k', '쳭'],
    u'\uccee' : ['tSh _j e k_>', '쳮'],
    u'\uccef' : ['tSh _j e k sh', '쳯'],
    u'\uccf0' : ['tSh _j e _n', '쳰'],
    u'\uccf1' : ['tSh _j e _n tS', '쳱'],
    u'\uccf2' : ['tSh _j e nh', '쳲'],
    u'\uccf3' : ['tSh _j e t', '쳳'],
    u'\uccf4' : ['tSh _j e _l', '쳴'],
    u'\uccf5' : ['tSh _j e _l k', '쳵'],
    u'\uccf6' : ['tSh _j e _l m', '쳶'],
    u'\uccf7' : ['tSh _j e _l p', '쳷'],
    u'\uccf8' : ['tSh _j e _l sh', '쳸'],
    u'\uccf9' : ['tSh _j e _l t_h', '쳹'],
    u'\uccfa' : ['tSh _j e _l p_h', '쳺'],
    u'\uccfb' : ['tSh _j e _l _h', '쳻'],
    u'\uccfc' : ['tSh _j e m', '쳼'],
    u'\uccfd' : ['tSh _j e p', '쳽'],
    u'\uccfe' : ['tSh _j e p sh', '쳾'],
    u'\uccff' : ['tSh _j e sh', '쳿'],
    u'\ucd00' : ['tSh _j e s', '촀'],
    u'\ucd01' : ['tSh _j e N', '촁'],
    u'\ucd02' : ['tSh _j e tS', '촂'],
    u'\ucd03' : ['tSh _j e tSh', '촃'],
    u'\ucd04' : ['tSh _j e k_h', '촄'],
    u'\ucd05' : ['tSh _j e t_h', '촅'],
    u'\ucd06' : ['tSh _j e p_h', '촆'],
    u'\ucd07' : ['tSh _j e _h', '촇'],
    u'\ucd08' : ['tSh o', '초'],
    u'\ucd09' : ['tSh o k', '촉'],
    u'\ucd0a' : ['tSh o k_>', '촊'],
    u'\ucd0b' : ['tSh o k sh', '촋'],
    u'\ucd0c' : ['tSh o _n', '촌'],
    u'\ucd0d' : ['tSh o _n tS', '촍'],
    u'\ucd0e' : ['tSh o _n _h', '촎'],
    u'\ucd0f' : ['tSh o t', '촏'],
    u'\ucd10' : ['tSh o _l', '촐'],
    u'\ucd11' : ['tSh o _l k', '촑'],
    u'\ucd12' : ['tSh o _l m', '촒'],
    u'\ucd13' : ['tSh o _l p', '촓'],
    u'\ucd14' : ['tSh o _l sh', '촔'],
    u'\ucd15' : ['tSh o _l t_h', '촕'],
    u'\ucd16' : ['tSh o _l p_h', '촖'],
    u'\ucd17' : ['tSh o _l _h', '촗'],
    u'\ucd18' : ['tSh o m', '촘'],
    u'\ucd19' : ['tSh o p', '촙'],
    u'\ucd1a' : ['tSh o p sh', '촚'],
    u'\ucd1b' : ['tSh o sh', '촛'],
    u'\ucd1c' : ['tSh o s', '촜'],
    u'\ucd1d' : ['tSh o N', '총'],
    u'\ucd1e' : ['tSh o tS', '촞'],
    u'\ucd1f' : ['tSh o tSh', '촟'],
    u'\ucd20' : ['tSh o k_h', '촠'],
    u'\ucd21' : ['tSh o t_h', '촡'],
    u'\ucd22' : ['tSh o p_h', '촢'],
    u'\ucd23' : ['tSh o _h', '촣'],
    u'\ucd24' : ['tSh _w a', '촤'],
    u'\ucd25' : ['tSh _w a k', '촥'],
    u'\ucd26' : ['tSh _w a k_>', '촦'],
    u'\ucd27' : ['tSh _w a k sh', '촧'],
    u'\ucd28' : ['tSh _w a _n', '촨'],
    u'\ucd29' : ['tSh _w a _n tS', '촩'],
    u'\ucd2a' : ['tSh _w a _n _h', '촪'],
    u'\ucd2b' : ['tSh _w a t', '촫'],
    u'\ucd2c' : ['tSh _w a _l', '촬'],
    u'\ucd2d' : ['tSh _w a _l k', '촭'],
    u'\ucd2e' : ['tSh _w a _l m', '촮'],
    u'\ucd2f' : ['tSh _w a _l p', '촯'],
    u'\ucd30' : ['tSh _w a _l sh', '촰'],
    u'\ucd31' : ['tSh _w a _l t_h', '촱'],
    u'\ucd32' : ['tSh _w a _l p_h', '촲'],
    u'\ucd33' : ['tSh _w a _l _h', '촳'],
    u'\ucd34' : ['tSh _w a m', '촴'],
    u'\ucd35' : ['tSh _w a p', '촵'],
    u'\ucd36' : ['tSh _w a p sh', '촶'],
    u'\ucd37' : ['tSh _w a sh', '촷'],
    u'\ucd38' : ['tSh _w a s', '촸'],
    u'\ucd39' : ['tSh _w a N', '촹'],
    u'\ucd3a' : ['tSh _w a tS', '촺'],
    u'\ucd3b' : ['tSh _w a tSh', '촻'],
    u'\ucd3c' : ['tSh _w a k_h', '촼'],
    u'\ucd3d' : ['tSh _w a t_h', '촽'],
    u'\ucd3e' : ['tSh _w a p_h', '촾'],
    u'\ucd3f' : ['tSh _w a _h', '촿'],
    u'\ucd40' : ['tSh _w {', '쵀'],
    u'\ucd41' : ['tSh _w { k', '쵁'],
    u'\ucd42' : ['tSh _w { k_>', '쵂'],
    u'\ucd43' : ['tSh _w { k sh', '쵃'],
    u'\ucd44' : ['tSh _w { _n', '쵄'],
    u'\ucd45' : ['tSh _w { _n tS', '쵅'],
    u'\ucd46' : ['tSh _w { _n _h', '쵆'],
    u'\ucd47' : ['tSh _w { t', '쵇'],
    u'\ucd48' : ['tSh _w { _l', '쵈'],
    u'\ucd49' : ['tSh _w { _l k', '쵉'],
    u'\ucd4a' : ['tSh _w { _l m', '쵊'],
    u'\ucd4b' : ['tSh _w { _l p', '쵋'],
    u'\ucd4c' : ['tSh _w { _l sh', '쵌'],
    u'\ucd4d' : ['tSh _w { _l t_h', '쵍'],
    u'\ucd4e' : ['tSh _w { _l p_h', '쵎'],
    u'\ucd4f' : ['tSh _w { _l _h', '쵏'],
    u'\ucd50' : ['tSh _w { m', '쵐'],
    u'\ucd51' : ['tSh _w { p', '쵑'],
    u'\ucd52' : ['tSh _w { p sh', '쵒'],
    u'\ucd53' : ['tSh _w { sh', '쵓'],
    u'\ucd54' : ['tSh _w { s', '쵔'],
    u'\ucd55' : ['tSh _w { N', '쵕'],
    u'\ucd56' : ['tSh _w { tS', '쵖'],
    u'\ucd57' : ['tSh _w { tSh', '쵗'],
    u'\ucd58' : ['tSh _w { k_h', '쵘'],
    u'\ucd59' : ['tSh _w { t_h', '쵙'],
    u'\ucd5a' : ['tSh _w { p_h', '쵚'],
    u'\ucd5b' : ['tSh _w { _h', '쵛'],
    u'\ucd5c' : ['tSh _w e', '최'],
    u'\ucd5d' : ['tSh _w e k', '쵝'],
    u'\ucd5e' : ['tSh _w e k_>', '쵞'],
    u'\ucd5f' : ['tSh _w e k sh', '쵟'],
    u'\ucd60' : ['tSh _w e _n', '쵠'],
    u'\ucd61' : ['tSh _w e _n tS', '쵡'],
    u'\ucd62' : ['tSh _w e _n _h', '쵢'],
    u'\ucd63' : ['tSh _w e t', '쵣'],
    u'\ucd64' : ['tSh _w e _l', '쵤'],
    u'\ucd65' : ['tSh _w e _l k', '쵥'],
    u'\ucd66' : ['tSh _w e _l m', '쵦'],
    u'\ucd67' : ['tSh _w e _l p', '쵧'],
    u'\ucd68' : ['tSh _w e _l sh', '쵨'],
    u'\ucd69' : ['tSh _w e _l t_h', '쵩'],
    u'\ucd6a' : ['tSh _w e _l p_h', '쵪'],
    u'\ucd6b' : ['tSh _w e _l _h', '쵫'],
    u'\ucd6c' : ['tSh _w e m', '쵬'],
    u'\ucd6d' : ['tSh _w e p', '쵭'],
    u'\ucd6e' : ['tSh _w e p sh', '쵮'],
    u'\ucd6f' : ['tSh _w e sh', '쵯'],
    u'\ucd70' : ['tSh _w e s', '쵰'],
    u'\ucd71' : ['tSh _w e N', '쵱'],
    u'\ucd72' : ['tSh _w e tS', '쵲'],
    u'\ucd73' : ['tSh _w e tSh', '쵳'],
    u'\ucd74' : ['tSh _w e k_h', '쵴'],
    u'\ucd75' : ['tSh _w e t_h', '쵵'],
    u'\ucd76' : ['tSh _w e p_h', '쵶'],
    u'\ucd77' : ['tSh _w e _h', '쵷'],
    u'\ucd78' : ['tSh _j o', '쵸'],
    u'\ucd79' : ['tSh _j o k', '쵹'],
    u'\ucd7a' : ['tSh _j o k_>', '쵺'],
    u'\ucd7b' : ['tSh _j o k sh', '쵻'],
    u'\ucd7c' : ['tSh _j o _n', '쵼'],
    u'\ucd7d' : ['tSh _j o _n tS', '쵽'],
    u'\ucd7e' : ['tSh _j o _n _h', '쵾'],
    u'\ucd7f' : ['tSh _j o t', '쵿'],
    u'\ucd80' : ['tSh _j o _l', '춀'],
    u'\ucd81' : ['tSh _j o _l k', '춁'],
    u'\ucd82' : ['tSh _j o _l m', '춂'],
    u'\ucd83' : ['tSh _j o _l p', '춃'],
    u'\ucd84' : ['tSh _j o _l sh', '춄'],
    u'\ucd85' : ['tSh _j o _l t_h', '춅'],
    u'\ucd86' : ['tSh _j o _l p_h', '춆'],
    u'\ucd87' : ['tSh _j o _l _h', '춇'],
    u'\ucd88' : ['tSh _j o m', '춈'],
    u'\ucd89' : ['tSh _j o p', '춉'],
    u'\ucd8a' : ['tSh _j o p sh', '춊'],
    u'\ucd8b' : ['tSh _j o sh', '춋'],
    u'\ucd8c' : ['tSh _j o s', '춌'],
    u'\ucd8d' : ['tSh _j o N', '춍'],
    u'\ucd8e' : ['tSh _j o tS', '춎'],
    u'\ucd8f' : ['tSh _j o tSh', '춏'],
    u'\ucd90' : ['tSh _j o k_h', '춐'],
    u'\ucd91' : ['tSh _j o t_h', '춑'],
    u'\ucd92' : ['tSh _j o p_h', '춒'],
    u'\ucd93' : ['tSh _j o _h', '춓'],
    u'\ucd94' : ['tSh u', '추'],
    u'\ucd95' : ['tSh u k', '축'],
    u'\ucd96' : ['tSh u k_>', '춖'],
    u'\ucd97' : ['tSh u k sh', '춗'],
    u'\ucd98' : ['tSh u _n', '춘'],
    u'\ucd99' : ['tSh u _n tS', '춙'],
    u'\ucd9a' : ['tSh u _n _h', '춚'],
    u'\ucd9b' : ['tSh u t', '춛'],
    u'\ucd9c' : ['tSh u _l', '출'],
    u'\ucd9d' : ['tSh u _l k', '춝'],
    u'\ucd9e' : ['tSh u _l m', '춞'],
    u'\ucd9f' : ['tSh u _l p', '춟'],
    u'\ucda0' : ['tSh u _l sh', '춠'],
    u'\ucda1' : ['tSh u _l t_h', '춡'],
    u'\ucda2' : ['tSh u _l p_h', '춢'],
    u'\ucda3' : ['tSh u _l _h', '춣'],
    u'\ucda4' : ['tSh u m', '춤'],
    u'\ucda5' : ['tSh u p', '춥'],
    u'\ucda6' : ['tSh u p sh', '춦'],
    u'\ucda7' : ['tSh u sh', '춧'],
    u'\ucda8' : ['tSh u s', '춨'],
    u'\ucda9' : ['tSh u N', '충'],
    u'\ucdaa' : ['tSh u tS', '춪'],
    u'\ucdab' : ['tSh u tSh', '춫'],
    u'\ucdac' : ['tSh u k_h', '춬'],
    u'\ucdad' : ['tSh u t_h', '춭'],
    u'\ucdae' : ['tSh u p_h', '춮'],
    u'\ucdaf' : ['tSh u _h', '춯'],
    u'\ucdb0' : ['tSh _w _r', '춰'],
    u'\ucdb1' : ['tSh _w _r k', '춱'],
    u'\ucdb2' : ['tSh _w _r k_>', '춲'],
    u'\ucdb3' : ['tSh _w _r k sh', '춳'],
    u'\ucdb4' : ['tSh _w _r _n', '춴'],
    u'\ucdb5' : ['tSh _w _r _n tS', '춵'],
    u'\ucdb6' : ['tSh _w _r _n _h', '춶'],
    u'\ucdb7' : ['tSh _w _r t', '춷'],
    u'\ucdb8' : ['tSh _w _r _l', '춸'],
    u'\ucdb9' : ['tSh _w _r _l k', '춹'],
    u'\ucdba' : ['tSh _w _r _l m', '춺'],
    u'\ucdbb' : ['tSh _w _r _l p', '춻'],
    u'\ucdbc' : ['tSh _w _r _l sh', '춼'],
    u'\ucdbd' : ['tSh _w _r _l t_h', '춽'],
    u'\ucdbe' : ['tSh _w _r _l p_h', '춾'],
    u'\ucdbf' : ['tSh _w _r _l _h', '춿'],
    u'\ucdc0' : ['tSh _w _r m', '췀'],
    u'\ucdc1' : ['tSh _w _r p', '췁'],
    u'\ucdc2' : ['tSh _w _r p sh', '췂'],
    u'\ucdc3' : ['tSh _w _r sh', '췃'],
    u'\ucdc4' : ['tSh _w _r s', '췄'],
    u'\ucdc5' : ['tSh _w _r N', '췅'],
    u'\ucdc6' : ['tSh _w _r tS', '췆'],
    u'\ucdc7' : ['tSh _w _r tSh', '췇'],
    u'\ucdc8' : ['tSh _w _r k_h', '췈'],
    u'\ucdc9' : ['tSh _w _r t_h', '췉'],
    u'\ucdca' : ['tSh _w _r p_h', '췊'],
    u'\ucdcb' : ['tSh _w _r _h', '췋'],
    u'\ucdcc' : ['tSh _w E', '췌'],
    u'\ucdcd' : ['tSh _w E k', '췍'],
    u'\ucdce' : ['tSh _w E k_>', '췎'],
    u'\ucdcf' : ['tSh _w E k sh', '췏'],
    u'\ucdd0' : ['tSh _w E _n', '췐'],
    u'\ucdd1' : ['tSh _w E _n tS', '췑'],
    u'\ucdd2' : ['tSh _w E _n _h', '췒'],
    u'\ucdd3' : ['tSh _w E t', '췓'],
    u'\ucdd4' : ['tSh _w E _l', '췔'],
    u'\ucdd5' : ['tSh _w E _l k', '췕'],
    u'\ucdd6' : ['tSh _w E _l m', '췖'],
    u'\ucdd7' : ['tSh _w E _l p', '췗'],
    u'\ucdd8' : ['tSh _w E _l sh', '췘'],
    u'\ucdd9' : ['tSh _w E _l t_h', '췙'],
    u'\ucdda' : ['tSh _w E _l p_h', '췚'],
    u'\ucddb' : ['tSh _w E _l _h', '췛'],
    u'\ucddc' : ['tSh _w E m', '췜'],
    u'\ucddd' : ['tSh _w E p', '췝'],
    u'\ucdde' : ['tSh _w E p sh', '췞'],
    u'\ucddf' : ['tSh _w E sh', '췟'],
    u'\ucde0' : ['tSh _w E s', '췠'],
    u'\ucde1' : ['tSh _w E N', '췡'],
    u'\ucde2' : ['tSh _w E tS', '췢'],
    u'\ucde3' : ['tSh _w E tSh', '췣'],
    u'\ucde4' : ['tSh _w E k_h', '췤'],
    u'\ucde5' : ['tSh _w E t_h', '췥'],
    u'\ucde6' : ['tSh _w E p_h', '췦'],
    u'\ucde7' : ['tSh _w E _h', '췧'],
    u'\ucde8' : ['tSh 2', '취'],
    u'\ucde9' : ['tSh 2 k', '췩'],
    u'\ucdea' : ['tSh 2 k_>', '췪'],
    u'\ucdeb' : ['tSh 2 k sh', '췫'],
    u'\ucdec' : ['tSh 2 _n', '췬'],
    u'\ucded' : ['tSh 2 _n tS', '췭'],
    u'\ucdee' : ['tSh 2 _n _h', '췮'],
    u'\ucdef' : ['tSh 2 t', '췯'],
    u'\ucdf0' : ['tSh 2 _l', '췰'],
    u'\ucdf1' : ['tSh 2 _l k', '췱'],
    u'\ucdf2' : ['tSh 2 _l m', '췲'],
    u'\ucdf3' : ['tSh 2 _l p', '췳'],
    u'\ucdf4' : ['tSh 2 _l sh', '췴'],
    u'\ucdf5' : ['tSh 2 _l t_h', '췵'],
    u'\ucdf6' : ['tSh 2 _l p_h', '췶'],
    u'\ucdf7' : ['tSh 2 _l _h', '췷'],
    u'\ucdf8' : ['tSh 2 m', '췸'],
    u'\ucdf9' : ['tSh 2 p', '췹'],
    u'\ucdfa' : ['tSh 2 p sh', '췺'],
    u'\ucdfb' : ['tSh 2 sh', '췻'],
    u'\ucdfc' : ['tSh 2 s', '췼'],
    u'\ucdfd' : ['tSh 2 N', '췽'],
    u'\ucdfe' : ['tSh 2 tS', '췾'],
    u'\ucdff' : ['tSh 2 tSh', '췿'],
    u'\uce00' : ['tSh 2 k_h', '츀'],
    u'\uce01' : ['tSh 2 t_h', '츁'],
    u'\uce02' : ['tSh 2 p_h', '츂'],
    u'\uce03' : ['tSh 2 _h', '츃'],
    u'\uce04' : ['tSh _j u', '츄'],
    u'\uce05' : ['tSh _j u k', '츅'],
    u'\uce06' : ['tSh _j u k_>', '츆'],
    u'\uce07' : ['tSh _j u k sh', '츇'],
    u'\uce08' : ['tSh _j u _n', '츈'],
    u'\uce09' : ['tSh _j u _n tS', '츉'],
    u'\uce0a' : ['tSh _j u _n _h', '츊'],
    u'\uce0b' : ['tSh _j u t', '츋'],
    u'\uce0c' : ['tSh _j u _l', '츌'],
    u'\uce0d' : ['tSh _j u _l k', '츍'],
    u'\uce0e' : ['tSh _j u _l m', '츎'],
    u'\uce0f' : ['tSh _j u _l p', '츏'],
    u'\uce10' : ['tSh _j u _l sh', '츐'],
    u'\uce11' : ['tSh _j u _l t_h', '츑'],
    u'\uce12' : ['tSh _j u _l p_h', '츒'],
    u'\uce13' : ['tSh _j u _l _h', '츓'],
    u'\uce14' : ['tSh _j u m', '츔'],
    u'\uce15' : ['tSh _j u p', '츕'],
    u'\uce16' : ['tSh _j u p sh', '츖'],
    u'\uce17' : ['tSh _j u sh', '츗'],
    u'\uce18' : ['tSh _j u s', '츘'],
    u'\uce19' : ['tSh _j u N', '츙'],
    u'\uce1a' : ['tSh _j u tS', '츚'],
    u'\uce1b' : ['tSh _j u tSh', '츛'],
    u'\uce1c' : ['tSh _j u k_h', '츜'],
    u'\uce1d' : ['tSh _j u t_h', '츝'],
    u'\uce1e' : ['tSh _j u p_h', '츞'],
    u'\uce1f' : ['tSh _j u _h', '츟'],
    u'\uce20' : ['tSh M', '츠'],
    u'\uce21' : ['tSh M k', '측'],
    u'\uce22' : ['tSh M k_>', '츢'],
    u'\uce23' : ['tSh M k sh', '츣'],
    u'\uce24' : ['tSh M _n', '츤'],
    u'\uce25' : ['tSh M _n tS', '츥'],
    u'\uce26' : ['tSh M _n _h', '츦'],
    u'\uce27' : ['tSh M t', '츧'],
    u'\uce28' : ['tSh M _l', '츨'],
    u'\uce29' : ['tSh M _l k', '츩'],
    u'\uce2a' : ['tSh M _l m', '츪'],
    u'\uce2b' : ['tSh M _l p', '츫'],
    u'\uce2c' : ['tSh M _l sh', '츬'],
    u'\uce2d' : ['tSh M _l t_h', '츭'],
    u'\uce2e' : ['tSh M _l p_h', '츮'],
    u'\uce2f' : ['tSh M _l _h', '츯'],
    u'\uce30' : ['tSh M m', '츰'],
    u'\uce31' : ['tSh M p', '츱'],
    u'\uce32' : ['tSh M p sh', '츲'],
    u'\uce33' : ['tSh M sh', '츳'],
    u'\uce34' : ['tSh M s', '츴'],
    u'\uce35' : ['tSh M N', '층'],
    u'\uce36' : ['tSh M tS', '츶'],
    u'\uce37' : ['tSh M tSh', '츷'],
    u'\uce38' : ['tSh M k_h', '츸'],
    u'\uce39' : ['tSh M t_h', '츹'],
    u'\uce3a' : ['tSh M p_h', '츺'],
    u'\uce3b' : ['tSh M _h', '츻'],
    u'\uce3c' : ['tSh M _j', '츼'],
    u'\uce3d' : ['tSh M _j k', '츽'],
    u'\uce3e' : ['tSh M _j k_>', '츾'],
    u'\uce3f' : ['tSh M _j k sh', '츿'],
    u'\uce40' : ['tSh M _j _n', '칀'],
    u'\uce41' : ['tSh M _j _n tS', '칁'],
    u'\uce42' : ['tSh M _j _n _h', '칂'],
    u'\uce43' : ['tSh M _j t', '칃'],
    u'\uce44' : ['tSh M _j _l', '칄'],
    u'\uce45' : ['tSh M _j _l k', '칅'],
    u'\uce46' : ['tSh M _j _l m', '칆'],
    u'\uce47' : ['tSh M _j _l p', '칇'],
    u'\uce48' : ['tSh M _j _l sh', '칈'],
    u'\uce49' : ['tSh M _j _l t_h', '칉'],
    u'\uce4a' : ['tSh M _j _l p_h', '칊'],
    u'\uce4b' : ['tSh M _j _l _h', '칋'],
    u'\uce4c' : ['tSh M _j m', '칌'],
    u'\uce4d' : ['tSh M _j p', '칍'],
    u'\uce4e' : ['tSh M _j p sh', '칎'],
    u'\uce4f' : ['tSh M _j sh', '칏'],
    u'\uce50' : ['tSh M _j s', '칐'],
    u'\uce51' : ['tSh M _j N', '칑'],
    u'\uce52' : ['tSh M _j tS', '칒'],
    u'\uce53' : ['tSh M _j tSh', '칓'],
    u'\uce54' : ['tSh M _j k_h', '칔'],
    u'\uce55' : ['tSh M _j t_h', '칕'],
    u'\uce56' : ['tSh M _j p_h', '칖'],
    u'\uce57' : ['tSh M _j _h', '칗'],
    u'\uce58' : ['tSh i', '치'],
    u'\uce59' : ['tSh i k', '칙'],
    u'\uce5a' : ['tSh i k_>', '칚'],
    u'\uce5b' : ['tSh i k sh', '칛'],
    u'\uce5c' : ['tSh i _n', '친'],
    u'\uce5d' : ['tSh i _n tS', '칝'],
    u'\uce5e' : ['tSh i _n _h', '칞'],
    u'\uce5f' : ['tSh i t', '칟'],
    u'\uce60' : ['tSh i _l', '칠'],
    u'\uce61' : ['tSh i _l k', '칡'],
    u'\uce62' : ['tSh i _l m', '칢'],
    u'\uce63' : ['tSh i _l p', '칣'],
    u'\uce64' : ['tSh i _l sh', '칤'],
    u'\uce65' : ['tSh i _l t_h', '칥'],
    u'\uce66' : ['tSh i _l p_h', '칦'],
    u'\uce67' : ['tSh i _l _h', '칧'],
    u'\uce68' : ['tSh i m', '침'],
    u'\uce69' : ['tSh i p', '칩'],
    u'\uce6a' : ['tSh i p sh', '칪'],
    u'\uce6b' : ['tSh i sh', '칫'],
    u'\uce6c' : ['tSh i s', '칬'],
    u'\uce6d' : ['tSh i N', '칭'],
    u'\uce6e' : ['tSh i tS', '칮'],
    u'\uce6f' : ['tSh i tSh', '칯'],
    u'\uce70' : ['tSh i k_h', '칰'],
    u'\uce71' : ['tSh i t_h', '칱'],
    u'\uce72' : ['tSh i p_h', '칲'],
    u'\uce73' : ['tSh i _h', '칳'],
    u'\uce74' : ['k_h a', '카'],
    u'\uce75' : ['k_h a k', '칵'],
    u'\uce76' : ['k_h a k_>', '칶'],
    u'\uce77' : ['k_h a k sh', '칷'],
    u'\uce78' : ['k_h a _n', '칸'],
    u'\uce79' : ['k_h a _n tS', '칹'],
    u'\uce7a' : ['k_h a _n _h', '칺'],
    u'\uce7b' : ['k_h a t', '칻'],
    u'\uce7c' : ['k_h a _l', '칼'],
    u'\uce7d' : ['k_h a _l k', '칽'],
    u'\uce7e' : ['k_h a _l m', '칾'],
    u'\uce7f' : ['k_h a _l p', '칿'],
    u'\uce80' : ['k_h a _l sh', '캀'],
    u'\uce81' : ['k_h a _l t_h', '캁'],
    u'\uce82' : ['k_h a _l p_h', '캂'],
    u'\uce83' : ['k_h a _l _h', '캃'],
    u'\uce84' : ['k_h a m', '캄'],
    u'\uce85' : ['k_h a p', '캅'],
    u'\uce86' : ['k_h a p sh', '캆'],
    u'\uce87' : ['k_h a sh', '캇'],
    u'\uce88' : ['k_h a s', '캈'],
    u'\uce89' : ['k_h a N', '캉'],
    u'\uce8a' : ['k_h a tS', '캊'],
    u'\uce8b' : ['k_h a tSh', '캋'],
    u'\uce8c' : ['k_h a k_h', '캌'],
    u'\uce8d' : ['k_h a t_h', '캍'],
    u'\uce8e' : ['k_h a p_h', '캎'],
    u'\uce8f' : ['k_h a _h', '캏'],
    u'\uce90' : ['k_h {', '캐'],
    u'\uce91' : ['k_h { k', '캑'],
    u'\uce92' : ['k_h { k_>', '캒'],
    u'\uce93' : ['k_h { k sh', '캓'],
    u'\uce94' : ['k_h { _n', '캔'],
    u'\uce95' : ['k_h { _n tS', '캕'],
    u'\uce96' : ['k_h { _n _h', '캖'],
    u'\uce97' : ['k_h { t', '캗'],
    u'\uce98' : ['k_h { _l', '캘'],
    u'\uce99' : ['k_h { _l k', '캙'],
    u'\uce9a' : ['k_h { _l m', '캚'],
    u'\uce9b' : ['k_h { _l p', '캛'],
    u'\uce9c' : ['k_h { _l sh', '캜'],
    u'\uce9d' : ['k_h { _l t_h', '캝'],
    u'\uce9e' : ['k_h { _l p_h', '캞'],
    u'\uce9f' : ['k_h { _l _h', '캟'],
    u'\ucea0' : ['k_h { m', '캠'],
    u'\ucea1' : ['k_h { p', '캡'],
    u'\ucea2' : ['k_h { p sh', '캢'],
    u'\ucea3' : ['k_h { sh', '캣'],
    u'\ucea4' : ['k_h { s', '캤'],
    u'\ucea5' : ['k_h { N', '캥'],
    u'\ucea6' : ['k_h { tS', '캦'],
    u'\ucea7' : ['k_h { tSh', '캧'],
    u'\ucea8' : ['k_h { k_h', '캨'],
    u'\ucea9' : ['k_h { t_h', '캩'],
    u'\uceaa' : ['k_h { p_h', '캪'],
    u'\uceab' : ['k_h { _h', '캫'],
    u'\uceac' : ['k_h _j a', '캬'],
    u'\ucead' : ['k_h _j a k', '캭'],
    u'\uceae' : ['k_h _j a k_>', '캮'],
    u'\uceaf' : ['k_h _j a k sh', '캯'],
    u'\uceb0' : ['k_h _j a _n', '캰'],
    u'\uceb1' : ['k_h _j a _n tS', '캱'],
    u'\uceb2' : ['k_h _j a _n _h', '캲'],
    u'\uceb3' : ['k_h _j a t', '캳'],
    u'\uceb4' : ['k_h _j a _l', '캴'],
    u'\uceb5' : ['k_h _j a _l k', '캵'],
    u'\uceb6' : ['k_h _j a _l m', '캶'],
    u'\uceb7' : ['k_h _j a _l p', '캷'],
    u'\uceb8' : ['k_h _j a _l sh', '캸'],
    u'\uceb9' : ['k_h _j a _l t_h', '캹'],
    u'\uceba' : ['k_h _j a _l p_h', '캺'],
    u'\ucebb' : ['k_h _j a _l _h', '캻'],
    u'\ucebc' : ['k_h _j a m', '캼'],
    u'\ucebd' : ['k_h _j a p', '캽'],
    u'\ucebe' : ['k_h _j a p sh', '캾'],
    u'\ucebf' : ['k_h _j a sh', '캿'],
    u'\ucec0' : ['k_h _j a s', '컀'],
    u'\ucec1' : ['k_h _j a N', '컁'],
    u'\ucec2' : ['k_h _j a tS', '컂'],
    u'\ucec3' : ['k_h _j a tSh', '컃'],
    u'\ucec4' : ['k_h _j a k_h', '컄'],
    u'\ucec5' : ['k_h _j a t_h', '컅'],
    u'\ucec6' : ['k_h _j a p_h', '컆'],
    u'\ucec7' : ['k_h _j a _h', '컇'],
    u'\ucec8' : ['k_h _j {', '컈'],
    u'\ucec9' : ['k_h _j { k', '컉'],
    u'\uceca' : ['k_h _j { k_>', '컊'],
    u'\ucecb' : ['k_h _j { k sh', '컋'],
    u'\ucecc' : ['k_h _j { _n', '컌'],
    u'\ucecd' : ['k_h _j { _n tS', '컍'],
    u'\ucece' : ['k_h _j { _n _h', '컎'],
    u'\ucecf' : ['k_h _j { t', '컏'],
    u'\uced0' : ['k_h _j { _l', '컐'],
    u'\uced1' : ['k_h _j { _l k', '컑'],
    u'\uced2' : ['k_h _j { _l m', '컒'],
    u'\uced3' : ['k_h _j { _l p', '컓'],
    u'\uced4' : ['k_h _j { _l sh', '컔'],
    u'\uced5' : ['k_h _j { _l t_h', '컕'],
    u'\uced6' : ['k_h _j { _l p_h', '컖'],
    u'\uced7' : ['k_h _j { _l _h', '컗'],
    u'\uced8' : ['k_h _j { m', '컘'],
    u'\uced9' : ['k_h _j { p', '컙'],
    u'\uceda' : ['k_h _j { p sh', '컚'],
    u'\ucedb' : ['k_h _j { sh', '컛'],
    u'\ucedc' : ['k_h _j { s', '컜'],
    u'\ucedd' : ['k_h _j { N', '컝'],
    u'\ucede' : ['k_h _j { tS', '컞'],
    u'\ucedf' : ['k_h _j { tSh', '컟'],
    u'\ucee0' : ['k_h _j { k_h', '컠'],
    u'\ucee1' : ['k_h _j { t_h', '컡'],
    u'\ucee2' : ['k_h _j { p_h', '컢'],
    u'\ucee3' : ['k_h _j { _h', '컣'],
    u'\ucee4' : ['k_h _r', '커'],
    u'\ucee5' : ['k_h _r k', '컥'],
    u'\ucee6' : ['k_h _r k_>', '컦'],
    u'\ucee7' : ['k_h _r k sh', '컧'],
    u'\ucee8' : ['k_h _r _n', '컨'],
    u'\ucee9' : ['k_h _r _n tS', '컩'],
    u'\uceea' : ['k_h _r _n _h', '컪'],
    u'\uceeb' : ['k_h _r t', '컫'],
    u'\uceec' : ['k_h _r _l', '컬'],
    u'\uceed' : ['k_h _r _l k', '컭'],
    u'\uceee' : ['k_h _r _l m', '컮'],
    u'\uceef' : ['k_h _r _l p', '컯'],
    u'\ucef0' : ['k_h _r _l sh', '컰'],
    u'\ucef1' : ['k_h _r _l t_h', '컱'],
    u'\ucef2' : ['k_h _r _l p_h', '컲'],
    u'\ucef3' : ['k_h _r _l _h', '컳'],
    u'\ucef4' : ['k_h _r m', '컴'],
    u'\ucef5' : ['k_h _r p', '컵'],
    u'\ucef6' : ['k_h _r p sh', '컶'],
    u'\ucef7' : ['k_h _r sh', '컷'],
    u'\ucef8' : ['k_h _r s', '컸'],
    u'\ucef9' : ['k_h _r N', '컹'],
    u'\ucefa' : ['k_h _r tS', '컺'],
    u'\ucefb' : ['k_h _r tSh', '컻'],
    u'\ucefc' : ['k_h _r k_h', '컼'],
    u'\ucefd' : ['k_h _r t_h', '컽'],
    u'\ucefe' : ['k_h _r p_h', '컾'],
    u'\uceff' : ['k_h _r _h', '컿'],
    u'\ucf00' : ['k_h e', '케'],
    u'\ucf01' : ['k_h e k', '켁'],
    u'\ucf02' : ['k_h e k_>', '켂'],
    u'\ucf03' : ['k_h e k sh', '켃'],
    u'\ucf04' : ['k_h e _n', '켄'],
    u'\ucf05' : ['k_h e _n tS', '켅'],
    u'\ucf06' : ['k_h e _n _h', '켆'],
    u'\ucf07' : ['k_h e t', '켇'],
    u'\ucf08' : ['k_h e _l', '켈'],
    u'\ucf09' : ['k_h e _l k', '켉'],
    u'\ucf0a' : ['k_h e _l m', '켊'],
    u'\ucf0b' : ['k_h e _l p', '켋'],
    u'\ucf0c' : ['k_h e _l sh', '켌'],
    u'\ucf0d' : ['k_h e _l t_h', '켍'],
    u'\ucf0e' : ['k_h e _l p_h', '켎'],
    u'\ucf0f' : ['k_h e _l _h', '켏'],
    u'\ucf10' : ['k_h e m', '켐'],
    u'\ucf11' : ['k_h e p', '켑'],
    u'\ucf12' : ['k_h e p sh', '켒'],
    u'\ucf13' : ['k_h e sh', '켓'],
    u'\ucf14' : ['k_h e s', '켔'],
    u'\ucf15' : ['k_h e N', '켕'],
    u'\ucf16' : ['k_h e tS', '켖'],
    u'\ucf17' : ['k_h e tSh', '켗'],
    u'\ucf18' : ['k_h e k_h', '켘'],
    u'\ucf19' : ['k_h e t_h', '켙'],
    u'\ucf1a' : ['k_h e p_h', '켚'],
    u'\ucf1b' : ['k_h e _h', '켛'],
    u'\ucf1c' : ['k_h _j _r', '켜'],
    u'\ucf1d' : ['k_h _j _r k', '켝'],
    u'\ucf1e' : ['k_h _j _r k_>', '켞'],
    u'\ucf1f' : ['k_h _j _r k sh', '켟'],
    u'\ucf20' : ['k_h _j _r _n', '켠'],
    u'\ucf21' : ['k_h _j _r _n tS', '켡'],
    u'\ucf22' : ['k_h _j _r _n _h', '켢'],
    u'\ucf23' : ['k_h _j _r t', '켣'],
    u'\ucf24' : ['k_h _j _r _l', '켤'],
    u'\ucf25' : ['k_h _j _r _l k', '켥'],
    u'\ucf26' : ['k_h _j _r _l m', '켦'],
    u'\ucf27' : ['k_h _j _r _l p', '켧'],
    u'\ucf28' : ['k_h _j _r _l sh', '켨'],
    u'\ucf29' : ['k_h _j _r _l t_h', '켩'],
    u'\ucf2a' : ['k_h _j _r _l p_h', '켪'],
    u'\ucf2b' : ['k_h _j _r _l _h', '켫'],
    u'\ucf2c' : ['k_h _j _r m', '켬'],
    u'\ucf2d' : ['k_h _j _r p', '켭'],
    u'\ucf2e' : ['k_h _j _r p sh', '켮'],
    u'\ucf2f' : ['k_h _j _r sh', '켯'],
    u'\ucf30' : ['k_h _j _r s', '켰'],
    u'\ucf31' : ['k_h _j _r N', '켱'],
    u'\ucf32' : ['k_h _j _r tS', '켲'],
    u'\ucf33' : ['k_h _j _r tSh', '켳'],
    u'\ucf34' : ['k_h _j _r k_h', '켴'],
    u'\ucf35' : ['k_h _j _r t_h', '켵'],
    u'\ucf36' : ['k_h _j _r p_h', '켶'],
    u'\ucf37' : ['k_h _j _r _h', '켷'],
    u'\ucf38' : ['k_h _j e', '켸'],
    u'\ucf39' : ['k_h _j e k', '켹'],
    u'\ucf3a' : ['k_h _j e k_>', '켺'],
    u'\ucf3b' : ['k_h _j e k sh', '켻'],
    u'\ucf3c' : ['k_h _j e _n', '켼'],
    u'\ucf3d' : ['k_h _j e _n tS', '켽'],
    u'\ucf3e' : ['k_h _j e _n _h', '켾'],
    u'\ucf3f' : ['k_h _j e t', '켿'],
    u'\ucf40' : ['k_h _j e _l', '콀'],
    u'\ucf41' : ['k_h _j e _l k', '콁'],
    u'\ucf42' : ['k_h _j e _l m', '콂'],
    u'\ucf43' : ['k_h _j e _l p', '콃'],
    u'\ucf44' : ['k_h _j e _l sh', '콄'],
    u'\ucf45' : ['k_h _j e _l t_h', '콅'],
    u'\ucf46' : ['k_h _j e _l p_h', '콆'],
    u'\ucf47' : ['k_h _j e _l _h', '콇'],
    u'\ucf48' : ['k_h _j e m', '콈'],
    u'\ucf49' : ['k_h _j e p', '콉'],
    u'\ucf4a' : ['k_h _j e p sh', '콊'],
    u'\ucf4b' : ['k_h _j e sh', '콋'],
    u'\ucf4c' : ['k_h _j e s', '콌'],
    u'\ucf4d' : ['k_h _j e N', '콍'],
    u'\ucf4e' : ['k_h _j e tS', '콎'],
    u'\ucf4f' : ['k_h _j e tSh', '콏'],
    u'\ucf50' : ['k_h _j e k_h', '콐'],
    u'\ucf51' : ['k_h _j e t_h', '콑'],
    u'\ucf52' : ['k_h _j e p_h', '콒'],
    u'\ucf53' : ['k_h _j e _h', '콓'],
    u'\ucf54' : ['k_h o', '코'],
    u'\ucf55' : ['k_h o k', '콕'],
    u'\ucf56' : ['k_h o k_>', '콖'],
    u'\ucf57' : ['k_h o k sh', '콗'],
    u'\ucf58' : ['k_h o _n', '콘'],
    u'\ucf59' : ['k_h o _n tS', '콙'],
    u'\ucf5a' : ['k_h o _n _h', '콚'],
    u'\ucf5b' : ['k_h o t', '콛'],
    u'\ucf5c' : ['k_h o _l', '콜'],
    u'\ucf5d' : ['k_h o _l k', '콝'],
    u'\ucf5e' : ['k_h o _l m', '콞'],
    u'\ucf5f' : ['k_h o _l p', '콟'],
    u'\ucf60' : ['k_h o _l sh', '콠'],
    u'\ucf61' : ['k_h o _l t_h', '콡'],
    u'\ucf62' : ['k_h o _l p_h', '콢'],
    u'\ucf63' : ['k_h o _l _h', '콣'],
    u'\ucf64' : ['k_h o m', '콤'],
    u'\ucf65' : ['k_h o p', '콥'],
    u'\ucf66' : ['k_h o p sh', '콦'],
    u'\ucf67' : ['k_h o sh', '콧'],
    u'\ucf68' : ['k_h o s', '콨'],
    u'\ucf69' : ['k_h o N', '콩'],
    u'\ucf6a' : ['k_h o tS', '콪'],
    u'\ucf6b' : ['k_h o tSh', '콫'],
    u'\ucf6c' : ['k_h o k_h', '콬'],
    u'\ucf6d' : ['k_h o t_h', '콭'],
    u'\ucf6e' : ['k_h o p_h', '콮'],
    u'\ucf6f' : ['k_h o _h', '콯'],
    u'\ucf70' : ['k_h _w a', '콰'],
    u'\ucf71' : ['k_h _w a k', '콱'],
    u'\ucf72' : ['k_h _w a k_>', '콲'],
    u'\ucf73' : ['k_h _w a k sh', '콳'],
    u'\ucf74' : ['k_h _w a _n', '콴'],
    u'\ucf75' : ['k_h _w a _n tS', '콵'],
    u'\ucf76' : ['k_h _w a _n _h', '콶'],
    u'\ucf77' : ['k_h _w a t', '콷'],
    u'\ucf78' : ['k_h _w a _l', '콸'],
    u'\ucf79' : ['k_h _w a _l k', '콹'],
    u'\ucf7a' : ['k_h _w a _l m', '콺'],
    u'\ucf7b' : ['k_h _w a _l p', '콻'],
    u'\ucf7c' : ['k_h _w a _l sh', '콼'],
    u'\ucf7d' : ['k_h _w a _l t_h', '콽'],
    u'\ucf7e' : ['k_h _w a _l p_h', '콾'],
    u'\ucf7f' : ['k_h _w a _l _h', '콿'],
    u'\ucf80' : ['k_h _w a m', '쾀'],
    u'\ucf81' : ['k_h _w a p', '쾁'],
    u'\ucf82' : ['k_h _w a p sh', '쾂'],
    u'\ucf83' : ['k_h _w a sh', '쾃'],
    u'\ucf84' : ['k_h _w a s', '쾄'],
    u'\ucf85' : ['k_h _w a N', '쾅'],
    u'\ucf86' : ['k_h _w a tS', '쾆'],
    u'\ucf87' : ['k_h _w a tSh', '쾇'],
    u'\ucf88' : ['k_h _w a k_h', '쾈'],
    u'\ucf89' : ['k_h _w a t_h', '쾉'],
    u'\ucf8a' : ['k_h _w a p_h', '쾊'],
    u'\ucf8b' : ['k_h _w a _h', '쾋'],
    u'\ucf8c' : ['k_h _w {', '쾌'],
    u'\ucf8d' : ['k_h _w { k', '쾍'],
    u'\ucf8e' : ['k_h _w { k_>', '쾎'],
    u'\ucf8f' : ['k_h _w { k sh', '쾏'],
    u'\ucf90' : ['k_h _w { _n', '쾐'],
    u'\ucf91' : ['k_h _w { _n tS', '쾑'],
    u'\ucf92' : ['k_h _w { _n _h', '쾒'],
    u'\ucf93' : ['k_h _w { t', '쾓'],
    u'\ucf94' : ['k_h _w { _l', '쾔'],
    u'\ucf95' : ['k_h _w { _l k', '쾕'],
    u'\ucf96' : ['k_h _w { _l m', '쾖'],
    u'\ucf97' : ['k_h _w { _l p', '쾗'],
    u'\ucf98' : ['k_h _w { _l sh', '쾘'],
    u'\ucf99' : ['k_h _w { _l t_h', '쾙'],
    u'\ucf9a' : ['k_h _w { _l p_h', '쾚'],
    u'\ucf9b' : ['k_h _w { _l _h', '쾛'],
    u'\ucf9c' : ['k_h _w { m', '쾜'],
    u'\ucf9d' : ['k_h _w { p', '쾝'],
    u'\ucf9e' : ['k_h _w { p sh', '쾞'],
    u'\ucf9f' : ['k_h _w { sh', '쾟'],
    u'\ucfa0' : ['k_h _w { s', '쾠'],
    u'\ucfa1' : ['k_h _w { N', '쾡'],
    u'\ucfa2' : ['k_h _w { tS', '쾢'],
    u'\ucfa3' : ['k_h _w { tSh', '쾣'],
    u'\ucfa4' : ['k_h _w { k_h', '쾤'],
    u'\ucfa5' : ['k_h _w { t_h', '쾥'],
    u'\ucfa6' : ['k_h _w { p_h', '쾦'],
    u'\ucfa7' : ['k_h _w { _h', '쾧'],
    u'\ucfa8' : ['k_h _w e', '쾨'],
    u'\ucfa9' : ['k_h _w e k', '쾩'],
    u'\ucfaa' : ['k_h _w e k_>', '쾪'],
    u'\ucfab' : ['k_h _w e k sh', '쾫'],
    u'\ucfac' : ['k_h _w e _n', '쾬'],
    u'\ucfad' : ['k_h _w e _n tS', '쾭'],
    u'\ucfae' : ['k_h _w e _n _h', '쾮'],
    u'\ucfaf' : ['k_h _w e t', '쾯'],
    u'\ucfb0' : ['k_h _w e _l', '쾰'],
    u'\ucfb1' : ['k_h _w e _l k', '쾱'],
    u'\ucfb2' : ['k_h _w e _l m', '쾲'],
    u'\ucfb3' : ['k_h _w e _l p', '쾳'],
    u'\ucfb4' : ['k_h _w e _l sh', '쾴'],
    u'\ucfb5' : ['k_h _w e _l t_h', '쾵'],
    u'\ucfb6' : ['k_h _w e _l p_h', '쾶'],
    u'\ucfb7' : ['k_h _w e _l _h', '쾷'],
    u'\ucfb8' : ['k_h _w e m', '쾸'],
    u'\ucfb9' : ['k_h _w e p', '쾹'],
    u'\ucfba' : ['k_h _w e p sh', '쾺'],
    u'\ucfbb' : ['k_h _w e sh', '쾻'],
    u'\ucfbc' : ['k_h _w e s', '쾼'],
    u'\ucfbd' : ['k_h _w e N', '쾽'],
    u'\ucfbe' : ['k_h _w e tS', '쾾'],
    u'\ucfbf' : ['k_h _w e tSh', '쾿'],
    u'\ucfc0' : ['k_h _w e k_h', '쿀'],
    u'\ucfc1' : ['k_h _w e t_h', '쿁'],
    u'\ucfc2' : ['k_h _w e p_h', '쿂'],
    u'\ucfc3' : ['k_h _w e _h', '쿃'],
    u'\ucfc4' : ['k_h _j o', '쿄'],
    u'\ucfc5' : ['k_h _j o k', '쿅'],
    u'\ucfc6' : ['k_h _j o k_>', '쿆'],
    u'\ucfc7' : ['k_h _j o k sh', '쿇'],
    u'\ucfc8' : ['k_h _j o _n', '쿈'],
    u'\ucfc9' : ['k_h _j o _n tS', '쿉'],
    u'\ucfca' : ['k_h _j o _n _h', '쿊'],
    u'\ucfcb' : ['k_h _j o t', '쿋'],
    u'\ucfcc' : ['k_h _j o _l', '쿌'],
    u'\ucfcd' : ['k_h _j o _l k', '쿍'],
    u'\ucfce' : ['k_h _j o _l m', '쿎'],
    u'\ucfcf' : ['k_h _j o _l p', '쿏'],
    u'\ucfd0' : ['k_h _j o _l sh', '쿐'],
    u'\ucfd1' : ['k_h _j o _l t_h', '쿑'],
    u'\ucfd2' : ['k_h _j o _l p_h', '쿒'],
    u'\ucfd3' : ['k_h _j o _l _h', '쿓'],
    u'\ucfd4' : ['k_h _j o m', '쿔'],
    u'\ucfd5' : ['k_h _j o p', '쿕'],
    u'\ucfd6' : ['k_h _j o p sh', '쿖'],
    u'\ucfd7' : ['k_h _j o sh', '쿗'],
    u'\ucfd8' : ['k_h _j o s', '쿘'],
    u'\ucfd9' : ['k_h _j o N', '쿙'],
    u'\ucfda' : ['k_h _j o tS', '쿚'],
    u'\ucfdb' : ['k_h _j o tSh', '쿛'],
    u'\ucfdc' : ['k_h _j o k_h', '쿜'],
    u'\ucfdd' : ['k_h _j o t_h', '쿝'],
    u'\ucfde' : ['k_h _j o p_h', '쿞'],
    u'\ucfdf' : ['k_h _j o _h', '쿟'],
    u'\ucfe0' : ['k_h u', '쿠'],
    u'\ucfe1' : ['k_h u k', '쿡'],
    u'\ucfe2' : ['k_h u k_>', '쿢'],
    u'\ucfe3' : ['k_h u k sh', '쿣'],
    u'\ucfe4' : ['k_h u _n', '쿤'],
    u'\ucfe5' : ['k_h u _n tS', '쿥'],
    u'\ucfe6' : ['k_h u _n _h', '쿦'],
    u'\ucfe7' : ['k_h u t', '쿧'],
    u'\ucfe8' : ['k_h u _l', '쿨'],
    u'\ucfe9' : ['k_h u _l k', '쿩'],
    u'\ucfea' : ['k_h u _l m', '쿪'],
    u'\ucfeb' : ['k_h u _l p', '쿫'],
    u'\ucfec' : ['k_h u _l sh', '쿬'],
    u'\ucfed' : ['k_h u _l t_h', '쿭'],
    u'\ucfee' : ['k_h u _l p_h', '쿮'],
    u'\ucfef' : ['k_h u _l _h', '쿯'],
    u'\ucff0' : ['k_h u m', '쿰'],
    u'\ucff1' : ['k_h u p', '쿱'],
    u'\ucff2' : ['k_h u p sh', '쿲'],
    u'\ucff3' : ['k_h u sh', '쿳'],
    u'\ucff4' : ['k_h u s', '쿴'],
    u'\ucff5' : ['k_h u N', '쿵'],
    u'\ucff6' : ['k_h u tS', '쿶'],
    u'\ucff7' : ['k_h u tSh', '쿷'],
    u'\ucff8' : ['k_h u k_h', '쿸'],
    u'\ucff9' : ['k_h u t_h', '쿹'],
    u'\ucffa' : ['k_h u p_h', '쿺'],
    u'\ucffb' : ['k_h u _h', '쿻'],
    u'\ucffc' : ['k_h _w _r', '쿼'],
    u'\ucffd' : ['k_h _w _r k', '쿽'],
    u'\ucffe' : ['k_h _w _r k_>', '쿾'],
    u'\ucfff' : ['k_h _w _r k sh', '쿿'],
    u'\ud000' : ['k_h _w _r _n', '퀀'],
    u'\ud001' : ['k_h _w _r _n tS', '퀁'],
    u'\ud002' : ['k_h _w _r _n _h', '퀂'],
    u'\ud003' : ['k_h _w _r t', '퀃'],
    u'\ud004' : ['k_h _w _r _l', '퀄'],
    u'\ud005' : ['k_h _w _r _l k', '퀅'],
    u'\ud006' : ['k_h _w _r _l m', '퀆'],
    u'\ud007' : ['k_h _w _r _l p', '퀇'],
    u'\ud008' : ['k_h _w _r _l sh', '퀈'],
    u'\ud009' : ['k_h _w _r _l t_h', '퀉'],
    u'\ud00a' : ['k_h _w _r _l p_h', '퀊'],
    u'\ud00b' : ['k_h _w _r _l _h', '퀋'],
    u'\ud00c' : ['k_h _w _r m', '퀌'],
    u'\ud00d' : ['k_h _w _r p', '퀍'],
    u'\ud00e' : ['k_h _w _r p sh', '퀎'],
    u'\ud00f' : ['k_h _w _r sh', '퀏'],
    u'\ud010' : ['k_h _w _r s', '퀐'],
    u'\ud011' : ['k_h _w _r N', '퀑'],
    u'\ud012' : ['k_h _w _r tS', '퀒'],
    u'\ud013' : ['k_h _w _r tSh', '퀓'],
    u'\ud014' : ['k_h _w _r k_h', '퀔'],
    u'\ud015' : ['k_h _w _r t_h', '퀕'],
    u'\ud016' : ['k_h _w _r p_h', '퀖'],
    u'\ud017' : ['k_h _w _r _h', '퀗'],
    u'\ud018' : ['k_h _w E', '퀘'],
    u'\ud019' : ['k_h _w E k', '퀙'],
    u'\ud01a' : ['k_h _w E k_>', '퀚'],
    u'\ud01b' : ['k_h _w E k sh', '퀛'],
    u'\ud01c' : ['k_h _w E _n', '퀜'],
    u'\ud01d' : ['k_h _w E _n tS', '퀝'],
    u'\ud01e' : ['k_h _w E _n _h', '퀞'],
    u'\ud01f' : ['k_h _w E t', '퀟'],
    u'\ud020' : ['k_h _w E _l', '퀠'],
    u'\ud021' : ['k_h _w E _l k', '퀡'],
    u'\ud022' : ['k_h _w E _l m', '퀢'],
    u'\ud023' : ['k_h _w E _l p', '퀣'],
    u'\ud024' : ['k_h _w E _l sh', '퀤'],
    u'\ud025' : ['k_h _w E _l t_h', '퀥'],
    u'\ud026' : ['k_h _w E _l p_h', '퀦'],
    u'\ud027' : ['k_h _w E _l _h', '퀧'],
    u'\ud028' : ['k_h _w E m', '퀨'],
    u'\ud029' : ['k_h _w E p', '퀩'],
    u'\ud02a' : ['k_h _w E p sh', '퀪'],
    u'\ud02b' : ['k_h _w E sh', '퀫'],
    u'\ud02c' : ['k_h _w E s', '퀬'],
    u'\ud02d' : ['k_h _w E N', '퀭'],
    u'\ud02e' : ['k_h _w E tS', '퀮'],
    u'\ud02f' : ['k_h _w E tSh', '퀯'],
    u'\ud030' : ['k_h _w E k_h', '퀰'],
    u'\ud031' : ['k_h _w E t_h', '퀱'],
    u'\ud032' : ['k_h _w E p_h', '퀲'],
    u'\ud033' : ['k_h _w E _h', '퀳'],
    u'\ud034' : ['k_h 2', '퀴'],
    u'\ud035' : ['k_h 2 k', '퀵'],
    u'\ud036' : ['k_h 2 k_>', '퀶'],
    u'\ud037' : ['k_h 2 k sh', '퀷'],
    u'\ud038' : ['k_h 2 _n', '퀸'],
    u'\ud039' : ['k_h 2 _n tS', '퀹'],
    u'\ud03a' : ['k_h 2 _n _h', '퀺'],
    u'\ud03b' : ['k_h 2 t', '퀻'],
    u'\ud03c' : ['k_h 2 _l', '퀼'],
    u'\ud03d' : ['k_h 2 _l k', '퀽'],
    u'\ud03e' : ['k_h 2 _l m', '퀾'],
    u'\ud03f' : ['k_h 2 _l p', '퀿'],
    u'\ud040' : ['k_h 2 _l sh', '큀'],
    u'\ud041' : ['k_h 2 _l t_h', '큁'],
    u'\ud042' : ['k_h 2 _l p_h', '큂'],
    u'\ud043' : ['k_h 2 _l _h', '큃'],
    u'\ud044' : ['k_h 2 m', '큄'],
    u'\ud045' : ['k_h 2 p', '큅'],
    u'\ud046' : ['k_h 2 p sh', '큆'],
    u'\ud047' : ['k_h 2 sh', '큇'],
    u'\ud048' : ['k_h 2 s', '큈'],
    u'\ud049' : ['k_h 2 N', '큉'],
    u'\ud04a' : ['k_h 2 tS', '큊'],
    u'\ud04b' : ['k_h 2 tSh', '큋'],
    u'\ud04c' : ['k_h 2 k_h', '큌'],
    u'\ud04d' : ['k_h 2 t_h', '큍'],
    u'\ud04e' : ['k_h 2 p_h', '큎'],
    u'\ud04f' : ['k_h 2 _h', '큏'],
    u'\ud050' : ['k_h _j u', '큐'],
    u'\ud051' : ['k_h _j u k', '큑'],
    u'\ud052' : ['k_h _j u k_>', '큒'],
    u'\ud053' : ['k_h _j u k sh', '큓'],
    u'\ud054' : ['k_h _j u _n', '큔'],
    u'\ud055' : ['k_h _j u _n tS', '큕'],
    u'\ud056' : ['k_h _j u _n _h', '큖'],
    u'\ud057' : ['k_h _j u t', '큗'],
    u'\ud058' : ['k_h _j u _l', '큘'],
    u'\ud059' : ['k_h _j u _l k', '큙'],
    u'\ud05a' : ['k_h _j u _l m', '큚'],
    u'\ud05b' : ['k_h _j u _l p', '큛'],
    u'\ud05c' : ['k_h _j u _l sh', '큜'],
    u'\ud05d' : ['k_h _j u _l t_h', '큝'],
    u'\ud05e' : ['k_h _j u _l p_h', '큞'],
    u'\ud05f' : ['k_h _j u _l _h', '큟'],
    u'\ud060' : ['k_h _j u m', '큠'],
    u'\ud061' : ['k_h _j u p', '큡'],
    u'\ud062' : ['k_h _j u p sh', '큢'],
    u'\ud063' : ['k_h _j u sh', '큣'],
    u'\ud064' : ['k_h _j u s', '큤'],
    u'\ud065' : ['k_h _j u N', '큥'],
    u'\ud066' : ['k_h _j u tS', '큦'],
    u'\ud067' : ['k_h _j u tSh', '큧'],
    u'\ud068' : ['k_h _j u k_h', '큨'],
    u'\ud069' : ['k_h _j u t_h', '큩'],
    u'\ud06a' : ['k_h _j u p_h', '큪'],
    u'\ud06b' : ['k_h _j u _h', '큫'],
    u'\ud06c' : ['k_h M', '크'],
    u'\ud06d' : ['k_h M k', '큭'],
    u'\ud06e' : ['k_h M k_>', '큮'],
    u'\ud06f' : ['k_h M k sh', '큯'],
    u'\ud070' : ['k_h M _n', '큰'],
    u'\ud071' : ['k_h M _n tS', '큱'],
    u'\ud072' : ['k_h M _n _h', '큲'],
    u'\ud073' : ['k_h M t', '큳'],
    u'\ud074' : ['k_h M _l', '클'],
    u'\ud075' : ['k_h M _l k', '큵'],
    u'\ud076' : ['k_h M _l m', '큶'],
    u'\ud077' : ['k_h M _l p', '큷'],
    u'\ud078' : ['k_h M _l sh', '큸'],
    u'\ud079' : ['k_h M _l t_h', '큹'],
    u'\ud07a' : ['k_h M _l p_h', '큺'],
    u'\ud07b' : ['k_h M _l _h', '큻'],
    u'\ud07c' : ['k_h M m', '큼'],
    u'\ud07d' : ['k_h M p', '큽'],
    u'\ud07e' : ['k_h M p sh', '큾'],
    u'\ud07f' : ['k_h M sh', '큿'],
    u'\ud080' : ['k_h M s', '킀'],
    u'\ud081' : ['k_h M N', '킁'],
    u'\ud082' : ['k_h M tS', '킂'],
    u'\ud083' : ['k_h M tSh', '킃'],
    u'\ud084' : ['k_h M k_h', '킄'],
    u'\ud085' : ['k_h M t_h', '킅'],
    u'\ud086' : ['k_h M p_h', '킆'],
    u'\ud087' : ['k_h M _h', '킇'],
    u'\ud088' : ['k_h M _j', '킈'],
    u'\ud089' : ['k_h M _j k', '킉'],
    u'\ud08a' : ['k_h M _j k_>', '킊'],
    u'\ud08b' : ['k_h M _j k sh', '킋'],
    u'\ud08c' : ['k_h M _j _n', '킌'],
    u'\ud08d' : ['k_h M _j _n tS', '킍'],
    u'\ud08e' : ['k_h M _j _n _h', '킎'],
    u'\ud08f' : ['k_h M _j t', '킏'],
    u'\ud090' : ['k_h M _j _l', '킐'],
    u'\ud091' : ['k_h M _j _l k', '킑'],
    u'\ud092' : ['k_h M _j _l m', '킒'],
    u'\ud093' : ['k_h M _j _l p', '킓'],
    u'\ud094' : ['k_h M _j _l sh', '킔'],
    u'\ud095' : ['k_h M _j _l t_h', '킕'],
    u'\ud096' : ['k_h M _j _l p_h', '킖'],
    u'\ud097' : ['k_h M _j _l _h', '킗'],
    u'\ud098' : ['k_h M _j m', '킘'],
    u'\ud099' : ['k_h M _j p', '킙'],
    u'\ud09a' : ['k_h M _j p sh', '킚'],
    u'\ud09b' : ['k_h M _j sh', '킛'],
    u'\ud09c' : ['k_h M _j s', '킜'],
    u'\ud09d' : ['k_h M _j N', '킝'],
    u'\ud09e' : ['k_h M _j tS', '킞'],
    u'\ud09f' : ['k_h M _j tSh', '킟'],
    u'\ud0a0' : ['k_h M _j k_h', '킠'],
    u'\ud0a1' : ['k_h M _j t_h', '킡'],
    u'\ud0a2' : ['k_h M _j p_h', '킢'],
    u'\ud0a3' : ['k_h M _j _h', '킣'],
    u'\ud0a4' : ['k_h i', '키'],
    u'\ud0a5' : ['k_h i k', '킥'],
    u'\ud0a6' : ['k_h i k_>', '킦'],
    u'\ud0a7' : ['k_h i k sh', '킧'],
    u'\ud0a8' : ['k_h i _n', '킨'],
    u'\ud0a9' : ['k_h i _n tS', '킩'],
    u'\ud0aa' : ['k_h i _n _h', '킪'],
    u'\ud0ab' : ['k_h i t', '킫'],
    u'\ud0ac' : ['k_h i _l', '킬'],
    u'\ud0ad' : ['k_h i _l k', '킭'],
    u'\ud0ae' : ['k_h i _l m', '킮'],
    u'\ud0af' : ['k_h i _l p', '킯'],
    u'\ud0b0' : ['k_h i _l sh', '킰'],
    u'\ud0b1' : ['k_h i _l t_h', '킱'],
    u'\ud0b2' : ['k_h i _l p_h', '킲'],
    u'\ud0b3' : ['k_h i _l _h', '킳'],
    u'\ud0b4' : ['k_h i m', '킴'],
    u'\ud0b5' : ['k_h i p', '킵'],
    u'\ud0b6' : ['k_h i p sh', '킶'],
    u'\ud0b7' : ['k_h i sh', '킷'],
    u'\ud0b8' : ['k_h i s', '킸'],
    u'\ud0b9' : ['k_h i N', '킹'],
    u'\ud0ba' : ['k_h i tS', '킺'],
    u'\ud0bb' : ['k_h i tSh', '킻'],
    u'\ud0bc' : ['k_h i k_h', '킼'],
    u'\ud0bd' : ['k_h i t_h', '킽'],
    u'\ud0be' : ['k_h i p_h', '킾'],
    u'\ud0bf' : ['k_h i _h', '킿'],
    u'\ud0c0' : ['t_h a', '타'],
    u'\ud0c1' : ['t_h a k', '탁'],
    u'\ud0c2' : ['t_h a k_>', '탂'],
    u'\ud0c3' : ['t_h a k sh', '탃'],
    u'\ud0c4' : ['t_h a _n', '탄'],
    u'\ud0c5' : ['t_h a _n tS', '탅'],
    u'\ud0c6' : ['t_h a _n _h', '탆'],
    u'\ud0c7' : ['t_h a t', '탇'],
    u'\ud0c8' : ['t_h a _l', '탈'],
    u'\ud0c9' : ['t_h a _l k', '탉'],
    u'\ud0ca' : ['t_h a _l m', '탊'],
    u'\ud0cb' : ['t_h a _l p', '탋'],
    u'\ud0cc' : ['t_h a _l sh', '탌'],
    u'\ud0cd' : ['t_h a _l t_h', '탍'],
    u'\ud0ce' : ['t_h a _l p_h', '탎'],
    u'\ud0cf' : ['t_h a _l _h', '탏'],
    u'\ud0d0' : ['t_h a m', '탐'],
    u'\ud0d1' : ['t_h a p', '탑'],
    u'\ud0d2' : ['t_h a p sh', '탒'],
    u'\ud0d3' : ['t_h a sh', '탓'],
    u'\ud0d4' : ['t_h a s', '탔'],
    u'\ud0d5' : ['t_h a N', '탕'],
    u'\ud0d6' : ['t_h a tS', '탖'],
    u'\ud0d7' : ['t_h a tSh', '탗'],
    u'\ud0d8' : ['t_h a k_h', '탘'],
    u'\ud0d9' : ['t_h a t_h', '탙'],
    u'\ud0da' : ['t_h a p_h', '탚'],
    u'\ud0db' : ['t_h a _h', '탛'],
    u'\ud0dc' : ['t_h {', '태'],
    u'\ud0dd' : ['t_h { k', '택'],
    u'\ud0de' : ['t_h { k_>', '탞'],
    u'\ud0df' : ['t_h { k sh', '탟'],
    u'\ud0e0' : ['t_h { _n', '탠'],
    u'\ud0e1' : ['t_h { _n tS', '탡'],
    u'\ud0e2' : ['t_h { _n _h', '탢'],
    u'\ud0e3' : ['t_h { t', '탣'],
    u'\ud0e4' : ['t_h { _l', '탤'],
    u'\ud0e5' : ['t_h { _l k', '탥'],
    u'\ud0e6' : ['t_h { _l m', '탦'],
    u'\ud0e7' : ['t_h { _l p', '탧'],
    u'\ud0e8' : ['t_h { _l sh', '탨'],
    u'\ud0e9' : ['t_h { _l t_h', '탩'],
    u'\ud0ea' : ['t_h { _l p_h', '탪'],
    u'\ud0eb' : ['t_h { _l _h', '탫'],
    u'\ud0ec' : ['t_h { m', '탬'],
    u'\ud0ed' : ['t_h { p', '탭'],
    u'\ud0ee' : ['t_h { p sh', '탮'],
    u'\ud0ef' : ['t_h { sh', '탯'],
    u'\ud0f0' : ['t_h { s', '탰'],
    u'\ud0f1' : ['t_h { N', '탱'],
    u'\ud0f2' : ['t_h { tS', '탲'],
    u'\ud0f3' : ['t_h { tSh', '탳'],
    u'\ud0f4' : ['t_h { k_h', '탴'],
    u'\ud0f5' : ['t_h { t_h', '탵'],
    u'\ud0f6' : ['t_h { p_h', '탶'],
    u'\ud0f7' : ['t_h { _h', '탷'],
    u'\ud0f8' : ['t_h _j a', '탸'],
    u'\ud0f9' : ['t_h _j a k', '탹'],
    u'\ud0fa' : ['t_h _j a k_>', '탺'],
    u'\ud0fb' : ['t_h _j a k sh', '탻'],
    u'\ud0fc' : ['t_h _j a _n', '탼'],
    u'\ud0fd' : ['t_h _j a _n tS', '탽'],
    u'\ud0fe' : ['t_h _j a _n _h', '탾'],
    u'\ud0ff' : ['t_h _j a t', '탿'],
    u'\ud100' : ['t_h _j a _l', '턀'],
    u'\ud101' : ['t_h _j a _l k', '턁'],
    u'\ud102' : ['t_h _j a _l m', '턂'],
    u'\ud103' : ['t_h _j a _l p', '턃'],
    u'\ud104' : ['t_h _j a _l sh', '턄'],
    u'\ud105' : ['t_h _j a _l t_h', '턅'],
    u'\ud106' : ['t_h _j a _l p_h', '턆'],
    u'\ud107' : ['t_h _j a _l _h', '턇'],
    u'\ud108' : ['t_h _j a m', '턈'],
    u'\ud109' : ['t_h _j a p', '턉'],
    u'\ud10a' : ['t_h _j a p sh', '턊'],
    u'\ud10b' : ['t_h _j a sh', '턋'],
    u'\ud10c' : ['t_h _j a s', '턌'],
    u'\ud10d' : ['t_h _j a N', '턍'],
    u'\ud10e' : ['t_h _j a tS', '턎'],
    u'\ud10f' : ['t_h _j a tSh', '턏'],
    u'\ud110' : ['t_h _j a k_h', '턐'],
    u'\ud111' : ['t_h _j a t_h', '턑'],
    u'\ud112' : ['t_h _j a p_h', '턒'],
    u'\ud113' : ['t_h _j a _h', '턓'],
    u'\ud114' : ['t_h _j {', '턔'],
    u'\ud115' : ['t_h _j { k', '턕'],
    u'\ud116' : ['t_h _j { k_>', '턖'],
    u'\ud117' : ['t_h _j { k sh', '턗'],
    u'\ud118' : ['t_h _j { _n', '턘'],
    u'\ud119' : ['t_h _j { _n tS', '턙'],
    u'\ud11a' : ['t_h _j { _n _h', '턚'],
    u'\ud11b' : ['t_h _j { t', '턛'],
    u'\ud11c' : ['t_h _j { _l', '턜'],
    u'\ud11d' : ['t_h _j { _l k', '턝'],
    u'\ud11e' : ['t_h _j { _l m', '턞'],
    u'\ud11f' : ['t_h _j { _l p', '턟'],
    u'\ud120' : ['t_h _j { _l sh', '턠'],
    u'\ud121' : ['t_h _j { _l t_h', '턡'],
    u'\ud122' : ['t_h _j { _l p_h', '턢'],
    u'\ud123' : ['t_h _j { _l _h', '턣'],
    u'\ud124' : ['t_h _j { m', '턤'],
    u'\ud125' : ['t_h _j { p', '턥'],
    u'\ud126' : ['t_h _j { p sh', '턦'],
    u'\ud127' : ['t_h _j { sh', '턧'],
    u'\ud128' : ['t_h _j { s', '턨'],
    u'\ud129' : ['t_h _j { N', '턩'],
    u'\ud12a' : ['t_h _j { tS', '턪'],
    u'\ud12b' : ['t_h _j { tSh', '턫'],
    u'\ud12c' : ['t_h _j { k_h', '턬'],
    u'\ud12d' : ['t_h _j { t_h', '턭'],
    u'\ud12e' : ['t_h _j { p_h', '턮'],
    u'\ud12f' : ['t_h _j { _h', '턯'],
    u'\ud130' : ['t_h _r', '터'],
    u'\ud131' : ['t_h _r k', '턱'],
    u'\ud132' : ['t_h _r k_>', '턲'],
    u'\ud133' : ['t_h _r k sh', '턳'],
    u'\ud134' : ['t_h _r _n', '턴'],
    u'\ud135' : ['t_h _r _n tS', '턵'],
    u'\ud136' : ['t_h _r _n _h', '턶'],
    u'\ud137' : ['t_h _r t', '턷'],
    u'\ud138' : ['t_h _r _l', '털'],
    u'\ud139' : ['t_h _r _l k', '턹'],
    u'\ud13a' : ['t_h _r _l m', '턺'],
    u'\ud13b' : ['t_h _r _l p', '턻'],
    u'\ud13c' : ['t_h _r _l sh', '턼'],
    u'\ud13d' : ['t_h _r _l t_h', '턽'],
    u'\ud13e' : ['t_h _r _l p_h', '턾'],
    u'\ud13f' : ['t_h _r _l _h', '턿'],
    u'\ud140' : ['t_h _r m', '텀'],
    u'\ud141' : ['t_h _r p', '텁'],
    u'\ud142' : ['t_h _r p sh', '텂'],
    u'\ud143' : ['t_h _r sh', '텃'],
    u'\ud144' : ['t_h _r s', '텄'],
    u'\ud145' : ['t_h _r N', '텅'],
    u'\ud146' : ['t_h _r tS', '텆'],
    u'\ud147' : ['t_h _r tSh', '텇'],
    u'\ud148' : ['t_h _r k_h', '텈'],
    u'\ud149' : ['t_h _r t_h', '텉'],
    u'\ud14a' : ['t_h _r p_h', '텊'],
    u'\ud14b' : ['t_h _r _h', '텋'],
    u'\ud14c' : ['t_h e', '테'],
    u'\ud14d' : ['t_h e k', '텍'],
    u'\ud14e' : ['t_h e k_>', '텎'],
    u'\ud14f' : ['t_h e k sh', '텏'],
    u'\ud150' : ['t_h e _n', '텐'],
    u'\ud151' : ['t_h e _n tS', '텑'],
    u'\ud152' : ['t_h e _n _h', '텒'],
    u'\ud153' : ['t_h e t', '텓'],
    u'\ud154' : ['t_h e _l', '텔'],
    u'\ud155' : ['t_h e _l k', '텕'],
    u'\ud156' : ['t_h e _l m', '텖'],
    u'\ud157' : ['t_h e _l p', '텗'],
    u'\ud158' : ['t_h e _l sh', '텘'],
    u'\ud159' : ['t_h e _l t_h', '텙'],
    u'\ud15a' : ['t_h e _l p_h', '텚'],
    u'\ud15b' : ['t_h e _l _h', '텛'],
    u'\ud15c' : ['t_h e m', '템'],
    u'\ud15d' : ['t_h e p', '텝'],
    u'\ud15e' : ['t_h e p sh', '텞'],
    u'\ud15f' : ['t_h e sh', '텟'],
    u'\ud160' : ['t_h e s', '텠'],
    u'\ud161' : ['t_h e N', '텡'],
    u'\ud162' : ['t_h e tS', '텢'],
    u'\ud163' : ['t_h e tSh', '텣'],
    u'\ud164' : ['t_h e k_h', '텤'],
    u'\ud165' : ['t_h e t_h', '텥'],
    u'\ud166' : ['t_h e p_h', '텦'],
    u'\ud167' : ['t_h e _h', '텧'],
    u'\ud168' : ['t_h _j _r', '텨'],
    u'\ud169' : ['t_h _j _r k', '텩'],
    u'\ud16a' : ['t_h _j _r k_>', '텪'],
    u'\ud16b' : ['t_h _j _r k sh', '텫'],
    u'\ud16c' : ['t_h _j _r _n', '텬'],
    u'\ud16d' : ['t_h _j _r _n tS', '텭'],
    u'\ud16e' : ['t_h _j _r _n _h', '텮'],
    u'\ud16f' : ['t_h _j _r t', '텯'],
    u'\ud170' : ['t_h _j _r _l', '텰'],
    u'\ud171' : ['t_h _j _r _l k', '텱'],
    u'\ud172' : ['t_h _j _r _l m', '텲'],
    u'\ud173' : ['t_h _j _r _l p', '텳'],
    u'\ud174' : ['t_h _j _r _l sh', '텴'],
    u'\ud175' : ['t_h _j _r _l t_h', '텵'],
    u'\ud176' : ['t_h _j _r _l p_h', '텶'],
    u'\ud177' : ['t_h _j _r _l _h', '텷'],
    u'\ud178' : ['t_h _j _r m', '텸'],
    u'\ud179' : ['t_h _j _r p', '텹'],
    u'\ud17a' : ['t_h _j _r p sh', '텺'],
    u'\ud17b' : ['t_h _j _r sh', '텻'],
    u'\ud17c' : ['t_h _j _r s', '텼'],
    u'\ud17d' : ['t_h _j _r N', '텽'],
    u'\ud17e' : ['t_h _j _r tS', '텾'],
    u'\ud17f' : ['t_h _j _r tSh', '텿'],
    u'\ud180' : ['t_h _j _r k_h', '톀'],
    u'\ud181' : ['t_h _j _r t_h', '톁'],
    u'\ud182' : ['t_h _j _r p_h', '톂'],
    u'\ud183' : ['t_h _j _r _h', '톃'],
    u'\ud184' : ['t_h _j e', '톄'],
    u'\ud185' : ['t_h _j e k', '톅'],
    u'\ud186' : ['t_h _j e k_>', '톆'],
    u'\ud187' : ['t_h _j e k sh', '톇'],
    u'\ud188' : ['t_h _j e _n', '톈'],
    u'\ud189' : ['t_h _j e _n tS', '톉'],
    u'\ud18a' : ['t_h _j e _n _h', '톊'],
    u'\ud18b' : ['t_h _j e t', '톋'],
    u'\ud18c' : ['t_h _j e _l', '톌'],
    u'\ud18d' : ['t_h _j e _l k', '톍'],
    u'\ud18e' : ['t_h _j e _l m', '톎'],
    u'\ud18f' : ['t_h _j e _l p', '톏'],
    u'\ud190' : ['t_h _j e _l sh', '톐'],
    u'\ud191' : ['t_h _j e _l t_h', '톑'],
    u'\ud192' : ['t_h _j e _l p_h', '톒'],
    u'\ud193' : ['t_h _j e _l _h', '톓'],
    u'\ud194' : ['t_h _j e m', '톔'],
    u'\ud195' : ['t_h _j e p', '톕'],
    u'\ud196' : ['t_h _j e p sh', '톖'],
    u'\ud197' : ['t_h _j e sh', '톗'],
    u'\ud198' : ['t_h _j e s', '톘'],
    u'\ud199' : ['t_h _j e N', '톙'],
    u'\ud19a' : ['t_h _j e tS', '톚'],
    u'\ud19b' : ['t_h _j e tSh', '톛'],
    u'\ud19c' : ['t_h _j e k_h', '톜'],
    u'\ud19d' : ['t_h _j e t_h', '톝'],
    u'\ud19e' : ['t_h _j e p_h', '톞'],
    u'\ud19f' : ['t_h _j e _h', '톟'],
    u'\ud1a0' : ['t_h o', '토'],
    u'\ud1a1' : ['t_h o k', '톡'],
    u'\ud1a2' : ['t_h o k_>', '톢'],
    u'\ud1a3' : ['t_h o k sh', '톣'],
    u'\ud1a4' : ['t_h o _n', '톤'],
    u'\ud1a5' : ['t_h o _n tS', '톥'],
    u'\ud1a6' : ['t_h o _n _h', '톦'],
    u'\ud1a7' : ['t_h o t', '톧'],
    u'\ud1a8' : ['t_h o _l', '톨'],
    u'\ud1a9' : ['t_h o _l k', '톩'],
    u'\ud1aa' : ['t_h o _l m', '톪'],
    u'\ud1ab' : ['t_h o _l p', '톫'],
    u'\ud1ac' : ['t_h o _l sh', '톬'],
    u'\ud1ad' : ['t_h o _l t_h', '톭'],
    u'\ud1ae' : ['t_h o _l p_h', '톮'],
    u'\ud1af' : ['t_h o _l _h', '톯'],
    u'\ud1b0' : ['t_h o m', '톰'],
    u'\ud1b1' : ['t_h o p', '톱'],
    u'\ud1b2' : ['t_h o p sh', '톲'],
    u'\ud1b3' : ['t_h o sh', '톳'],
    u'\ud1b4' : ['t_h o s', '톴'],
    u'\ud1b5' : ['t_h o N', '통'],
    u'\ud1b6' : ['t_h o tS', '톶'],
    u'\ud1b7' : ['t_h o tSh', '톷'],
    u'\ud1b8' : ['t_h o k_h', '톸'],
    u'\ud1b9' : ['t_h o t_h', '톹'],
    u'\ud1ba' : ['t_h o p_h', '톺'],
    u'\ud1bb' : ['t_h o _h', '톻'],
    u'\ud1bc' : ['t_h _w a', '톼'],
    u'\ud1bd' : ['t_h _w a k', '톽'],
    u'\ud1be' : ['t_h _w a k_>', '톾'],
    u'\ud1bf' : ['t_h _w a k sh', '톿'],
    u'\ud1c0' : ['t_h _w a _n', '퇀'],
    u'\ud1c1' : ['t_h _w a _n tS', '퇁'],
    u'\ud1c2' : ['t_h _w a _n _h', '퇂'],
    u'\ud1c3' : ['t_h _w a t', '퇃'],
    u'\ud1c4' : ['t_h _w a _l', '퇄'],
    u'\ud1c5' : ['t_h _w a _l k', '퇅'],
    u'\ud1c6' : ['t_h _w a _l m', '퇆'],
    u'\ud1c7' : ['t_h _w a _l p', '퇇'],
    u'\ud1c8' : ['t_h _w a _l sh', '퇈'],
    u'\ud1c9' : ['t_h _w a _l t_h', '퇉'],
    u'\ud1ca' : ['t_h _w a _l p_h', '퇊'],
    u'\ud1cb' : ['t_h _w a _l _h', '퇋'],
    u'\ud1cc' : ['t_h _w a m', '퇌'],
    u'\ud1cd' : ['t_h _w a p', '퇍'],
    u'\ud1ce' : ['t_h _w a p sh', '퇎'],
    u'\ud1cf' : ['t_h _w a sh', '퇏'],
    u'\ud1d0' : ['t_h _w a s', '퇐'],
    u'\ud1d1' : ['t_h _w a N', '퇑'],
    u'\ud1d2' : ['t_h _w a tS', '퇒'],
    u'\ud1d3' : ['t_h _w a tSh', '퇓'],
    u'\ud1d4' : ['t_h _w a k_h', '퇔'],
    u'\ud1d5' : ['t_h _w a t_h', '퇕'],
    u'\ud1d6' : ['t_h _w a p_h', '퇖'],
    u'\ud1d7' : ['t_h _w a _h', '퇗'],
    u'\ud1d8' : ['t_h _w {', '퇘'],
    u'\ud1d9' : ['t_h _w { k', '퇙'],
    u'\ud1da' : ['t_h _w { k_>', '퇚'],
    u'\ud1db' : ['t_h _w { k sh', '퇛'],
    u'\ud1dc' : ['t_h _w { _n', '퇜'],
    u'\ud1dd' : ['t_h _w { _n tS', '퇝'],
    u'\ud1de' : ['t_h _w { _n _h', '퇞'],
    u'\ud1df' : ['t_h _w { t', '퇟'],
    u'\ud1e0' : ['t_h _w { _l', '퇠'],
    u'\ud1e1' : ['t_h _w { _l k', '퇡'],
    u'\ud1e2' : ['t_h _w { _l m', '퇢'],
    u'\ud1e3' : ['t_h _w { _l p', '퇣'],
    u'\ud1e4' : ['t_h _w { _l sh', '퇤'],
    u'\ud1e5' : ['t_h _w { _l t_h', '퇥'],
    u'\ud1e6' : ['t_h _w { _l p_h', '퇦'],
    u'\ud1e7' : ['t_h _w { _l _h', '퇧'],
    u'\ud1e8' : ['t_h _w { m', '퇨'],
    u'\ud1e9' : ['t_h _w { p', '퇩'],
    u'\ud1ea' : ['t_h _w { p sh', '퇪'],
    u'\ud1eb' : ['t_h _w { sh', '퇫'],
    u'\ud1ec' : ['t_h _w { s', '퇬'],
    u'\ud1ed' : ['t_h _w { N', '퇭'],
    u'\ud1ee' : ['t_h _w { tS', '퇮'],
    u'\ud1ef' : ['t_h _w { tSh', '퇯'],
    u'\ud1f0' : ['t_h _w { k_h', '퇰'],
    u'\ud1f1' : ['t_h _w { t_h', '퇱'],
    u'\ud1f2' : ['t_h _w { p_h', '퇲'],
    u'\ud1f3' : ['t_h _w { _h', '퇳'],
    u'\ud1f4' : ['t_h _w e', '퇴'],
    u'\ud1f5' : ['t_h _w e k', '퇵'],
    u'\ud1f6' : ['t_h _w e k_>', '퇶'],
    u'\ud1f7' : ['t_h _w e k sh', '퇷'],
    u'\ud1f8' : ['t_h _w e _n', '퇸'],
    u'\ud1f9' : ['t_h _w e _n tS', '퇹'],
    u'\ud1fa' : ['t_h _w e _n _h', '퇺'],
    u'\ud1fb' : ['t_h _w e t', '퇻'],
    u'\ud1fc' : ['t_h _w e _l', '퇼'],
    u'\ud1fd' : ['t_h _w e _l k', '퇽'],
    u'\ud1fe' : ['t_h _w e _l m', '퇾'],
    u'\ud1ff' : ['t_h _w e _l p', '퇿'],
    u'\ud200' : ['t_h _w e _l sh', '툀'],
    u'\ud201' : ['t_h _w e _l t_h', '툁'],
    u'\ud202' : ['t_h _w e _l p_h', '툂'],
    u'\ud203' : ['t_h _w e _l _h', '툃'],
    u'\ud204' : ['t_h _w e m', '툄'],
    u'\ud205' : ['t_h _w e p', '툅'],
    u'\ud206' : ['t_h _w e p sh', '툆'],
    u'\ud207' : ['t_h _w e sh', '툇'],
    u'\ud208' : ['t_h _w e s', '툈'],
    u'\ud209' : ['t_h _w e N', '툉'],
    u'\ud20a' : ['t_h _w e tS', '툊'],
    u'\ud20b' : ['t_h _w e tSh', '툋'],
    u'\ud20c' : ['t_h _w e k_h', '툌'],
    u'\ud20d' : ['t_h _w e t_h', '툍'],
    u'\ud20e' : ['t_h _w e p_h', '툎'],
    u'\ud20f' : ['t_h _w e _h', '툏'],
    u'\ud210' : ['t_h _j o', '툐'],
    u'\ud211' : ['t_h _j o k', '툑'],
    u'\ud212' : ['t_h _j o k_>', '툒'],
    u'\ud213' : ['t_h _j o k sh', '툓'],
    u'\ud214' : ['t_h _j o _n', '툔'],
    u'\ud215' : ['t_h _j o _n tS', '툕'],
    u'\ud216' : ['t_h _j o _n _h', '툖'],
    u'\ud217' : ['t_h _j o t', '툗'],
    u'\ud218' : ['t_h _j o _l', '툘'],
    u'\ud219' : ['t_h _j o _l k', '툙'],
    u'\ud21a' : ['t_h _j o _l m', '툚'],
    u'\ud21b' : ['t_h _j o _l p', '툛'],
    u'\ud21c' : ['t_h _j o _l sh', '툜'],
    u'\ud21d' : ['t_h _j o _l t_h', '툝'],
    u'\ud21e' : ['t_h _j o _l p_h', '툞'],
    u'\ud21f' : ['t_h _j o _l _h', '툟'],
    u'\ud220' : ['t_h _j o m', '툠'],
    u'\ud221' : ['t_h _j o p', '툡'],
    u'\ud222' : ['t_h _j o p sh', '툢'],
    u'\ud223' : ['t_h _j o sh', '툣'],
    u'\ud224' : ['t_h _j o s', '툤'],
    u'\ud225' : ['t_h _j o N', '툥'],
    u'\ud226' : ['t_h _j o tS', '툦'],
    u'\ud227' : ['t_h _j o tSh', '툧'],
    u'\ud228' : ['t_h _j o k_h', '툨'],
    u'\ud229' : ['t_h _j o t_h', '툩'],
    u'\ud22a' : ['t_h _j o p_h', '툪'],
    u'\ud22b' : ['t_h _j o _h', '툫'],
    u'\ud22c' : ['t_h u', '투'],
    u'\ud22d' : ['t_h u k', '툭'],
    u'\ud22e' : ['t_h u k_>', '툮'],
    u'\ud22f' : ['t_h u k sh', '툯'],
    u'\ud230' : ['t_h u _n', '툰'],
    u'\ud231' : ['t_h u _n tS', '툱'],
    u'\ud232' : ['t_h u _n _h', '툲'],
    u'\ud233' : ['t_h u t', '툳'],
    u'\ud234' : ['t_h u _l', '툴'],
    u'\ud235' : ['t_h u _l k', '툵'],
    u'\ud236' : ['t_h u _l m', '툶'],
    u'\ud237' : ['t_h u _l p', '툷'],
    u'\ud238' : ['t_h u _l sh', '툸'],
    u'\ud239' : ['t_h u _l t_h', '툹'],
    u'\ud23a' : ['t_h u _l p_h', '툺'],
    u'\ud23b' : ['t_h u _l _h', '툻'],
    u'\ud23c' : ['t_h u m', '툼'],
    u'\ud23d' : ['t_h u p', '툽'],
    u'\ud23e' : ['t_h u p sh', '툾'],
    u'\ud23f' : ['t_h u sh', '툿'],
    u'\ud240' : ['t_h u s', '퉀'],
    u'\ud241' : ['t_h u N', '퉁'],
    u'\ud242' : ['t_h u tS', '퉂'],
    u'\ud243' : ['t_h u tSh', '퉃'],
    u'\ud244' : ['t_h u k_h', '퉄'],
    u'\ud245' : ['t_h u t_h', '퉅'],
    u'\ud246' : ['t_h u p_h', '퉆'],
    u'\ud247' : ['t_h u _h', '퉇'],
    u'\ud248' : ['t_h _w _r', '퉈'],
    u'\ud249' : ['t_h _w _r k', '퉉'],
    u'\ud24a' : ['t_h _w _r k_>', '퉊'],
    u'\ud24b' : ['t_h _w _r k sh', '퉋'],
    u'\ud24c' : ['t_h _w _r _n', '퉌'],
    u'\ud24d' : ['t_h _w _r _n tS', '퉍'],
    u'\ud24e' : ['t_h _w _r _n _h', '퉎'],
    u'\ud24f' : ['t_h _w _r t', '퉏'],
    u'\ud250' : ['t_h _w _r _l', '퉐'],
    u'\ud251' : ['t_h _w _r _l k', '퉑'],
    u'\ud252' : ['t_h _w _r _l m', '퉒'],
    u'\ud253' : ['t_h _w _r _l p', '퉓'],
    u'\ud254' : ['t_h _w _r _l sh', '퉔'],
    u'\ud255' : ['t_h _w _r _l t_h', '퉕'],
    u'\ud256' : ['t_h _w _r _l p_h', '퉖'],
    u'\ud257' : ['t_h _w _r _l _h', '퉗'],
    u'\ud258' : ['t_h _w _r m', '퉘'],
    u'\ud259' : ['t_h _w _r p', '퉙'],
    u'\ud25a' : ['t_h _w _r p sh', '퉚'],
    u'\ud25b' : ['t_h _w _r sh', '퉛'],
    u'\ud25c' : ['t_h _w _r s', '퉜'],
    u'\ud25d' : ['t_h _w _r N', '퉝'],
    u'\ud25e' : ['t_h _w _r tS', '퉞'],
    u'\ud25f' : ['t_h _w _r tSh', '퉟'],
    u'\ud260' : ['t_h _w _r k_h', '퉠'],
    u'\ud261' : ['t_h _w _r t_h', '퉡'],
    u'\ud262' : ['t_h _w _r p_h', '퉢'],
    u'\ud263' : ['t_h _w _r _h', '퉣'],
    u'\ud264' : ['t_h _w E', '퉤'],
    u'\ud265' : ['t_h _w E k', '퉥'],
    u'\ud266' : ['t_h _w E k_>', '퉦'],
    u'\ud267' : ['t_h _w E k sh', '퉧'],
    u'\ud268' : ['t_h _w E _n', '퉨'],
    u'\ud269' : ['t_h _w E _n tS', '퉩'],
    u'\ud26a' : ['t_h _w E _n _h', '퉪'],
    u'\ud26b' : ['t_h _w E t', '퉫'],
    u'\ud26c' : ['t_h _w E _l', '퉬'],
    u'\ud26d' : ['t_h _w E _l k', '퉭'],
    u'\ud26e' : ['t_h _w E _l m', '퉮'],
    u'\ud26f' : ['t_h _w E _l p', '퉯'],
    u'\ud270' : ['t_h _w E _l sh', '퉰'],
    u'\ud271' : ['t_h _w E _l t_h', '퉱'],
    u'\ud272' : ['t_h _w E _l p_h', '퉲'],
    u'\ud273' : ['t_h _w E _l _h', '퉳'],
    u'\ud274' : ['t_h _w E m', '퉴'],
    u'\ud275' : ['t_h _w E p', '퉵'],
    u'\ud276' : ['t_h _w E p sh', '퉶'],
    u'\ud277' : ['t_h _w E sh', '퉷'],
    u'\ud278' : ['t_h _w E s', '퉸'],
    u'\ud279' : ['t_h _w E N', '퉹'],
    u'\ud27a' : ['t_h _w E tS', '퉺'],
    u'\ud27b' : ['t_h _w E tSh', '퉻'],
    u'\ud27c' : ['t_h _w E k_h', '퉼'],
    u'\ud27d' : ['t_h _w E t_h', '퉽'],
    u'\ud27e' : ['t_h _w E p_h', '퉾'],
    u'\ud27f' : ['t_h _w E _h', '퉿'],
    u'\ud280' : ['t_h 2', '튀'],
    u'\ud281' : ['t_h 2 k', '튁'],
    u'\ud282' : ['t_h 2 k_>', '튂'],
    u'\ud283' : ['t_h 2 k sh', '튃'],
    u'\ud284' : ['t_h 2 _n', '튄'],
    u'\ud285' : ['t_h 2 _n tS', '튅'],
    u'\ud286' : ['t_h 2 _n _h', '튆'],
    u'\ud287' : ['t_h 2 t', '튇'],
    u'\ud288' : ['t_h 2 _l', '튈'],
    u'\ud289' : ['t_h 2 _l k', '튉'],
    u'\ud28a' : ['t_h 2 _l m', '튊'],
    u'\ud28b' : ['t_h 2 _l p', '튋'],
    u'\ud28c' : ['t_h 2 _l sh', '튌'],
    u'\ud28d' : ['t_h 2 _l t_h', '튍'],
    u'\ud28e' : ['t_h 2 _l p_h', '튎'],
    u'\ud28f' : ['t_h 2 _l _h', '튏'],
    u'\ud290' : ['t_h 2 m', '튐'],
    u'\ud291' : ['t_h 2 p', '튑'],
    u'\ud292' : ['t_h 2 p sh', '튒'],
    u'\ud293' : ['t_h 2 sh', '튓'],
    u'\ud294' : ['t_h 2 s', '튔'],
    u'\ud295' : ['t_h 2 N', '튕'],
    u'\ud296' : ['t_h 2 tS', '튖'],
    u'\ud297' : ['t_h 2 tSh', '튗'],
    u'\ud298' : ['t_h 2 k_h', '튘'],
    u'\ud299' : ['t_h 2 t_h', '튙'],
    u'\ud29a' : ['t_h 2 p_h', '튚'],
    u'\ud29b' : ['t_h 2 _h', '튛'],
    u'\ud29c' : ['t_h _j u', '튜'],
    u'\ud29d' : ['t_h _j u k', '튝'],
    u'\ud29e' : ['t_h _j u k_>', '튞'],
    u'\ud29f' : ['t_h _j u k sh', '튟'],
    u'\ud2a0' : ['t_h _j u _n', '튠'],
    u'\ud2a1' : ['t_h _j u _n tS', '튡'],
    u'\ud2a2' : ['t_h _j u _n _h', '튢'],
    u'\ud2a3' : ['t_h _j u t', '튣'],
    u'\ud2a4' : ['t_h _j u _l', '튤'],
    u'\ud2a5' : ['t_h _j u _l k', '튥'],
    u'\ud2a6' : ['t_h _j u _l m', '튦'],
    u'\ud2a7' : ['t_h _j u _l p', '튧'],
    u'\ud2a8' : ['t_h _j u _l sh', '튨'],
    u'\ud2a9' : ['t_h _j u _l t_h', '튩'],
    u'\ud2aa' : ['t_h _j u _l p_h', '튪'],
    u'\ud2ab' : ['t_h _j u _l _h', '튫'],
    u'\ud2ac' : ['t_h _j u m', '튬'],
    u'\ud2ad' : ['t_h _j u p', '튭'],
    u'\ud2ae' : ['t_h _j u p sh', '튮'],
    u'\ud2af' : ['t_h _j u sh', '튯'],
    u'\ud2b0' : ['t_h _j u s', '튰'],
    u'\ud2b1' : ['t_h _j u N', '튱'],
    u'\ud2b2' : ['t_h _j u tS', '튲'],
    u'\ud2b3' : ['t_h _j u tSh', '튳'],
    u'\ud2b4' : ['t_h _j u k_h', '튴'],
    u'\ud2b5' : ['t_h _j u t_h', '튵'],
    u'\ud2b6' : ['t_h _j u p_h', '튶'],
    u'\ud2b7' : ['t_h _j u _h', '튷'],
    u'\ud2b8' : ['t_h M', '트'],
    u'\ud2b9' : ['t_h M k', '특'],
    u'\ud2ba' : ['t_h M k_>', '튺'],
    u'\ud2bb' : ['t_h M k sh', '튻'],
    u'\ud2bc' : ['t_h M _n', '튼'],
    u'\ud2bd' : ['t_h M _n tS', '튽'],
    u'\ud2be' : ['t_h M _n _h', '튾'],
    u'\ud2bf' : ['t_h M t', '튿'],
    u'\ud2c0' : ['t_h M _l', '틀'],
    u'\ud2c1' : ['t_h M _l k', '틁'],
    u'\ud2c2' : ['t_h M _l m', '틂'],
    u'\ud2c3' : ['t_h M _l p', '틃'],
    u'\ud2c4' : ['t_h M _l sh', '틄'],
    u'\ud2c5' : ['t_h M _l t_h', '틅'],
    u'\ud2c6' : ['t_h M _l p_h', '틆'],
    u'\ud2c7' : ['t_h M _l _h', '틇'],
    u'\ud2c8' : ['t_h M m', '틈'],
    u'\ud2c9' : ['t_h M p', '틉'],
    u'\ud2ca' : ['t_h M p sh', '틊'],
    u'\ud2cb' : ['t_h M sh', '틋'],
    u'\ud2cc' : ['t_h M s', '틌'],
    u'\ud2cd' : ['t_h M N', '틍'],
    u'\ud2ce' : ['t_h M tS', '틎'],
    u'\ud2cf' : ['t_h M tSh', '틏'],
    u'\ud2d0' : ['t_h M k_h', '틐'],
    u'\ud2d1' : ['t_h M t_h', '틑'],
    u'\ud2d2' : ['t_h M p_h', '틒'],
    u'\ud2d3' : ['t_h M _h', '틓'],
    u'\ud2d4' : ['t_h M _j', '틔'],
    u'\ud2d5' : ['t_h M _j k', '틕'],
    u'\ud2d6' : ['t_h M _j k_>', '틖'],
    u'\ud2d7' : ['t_h M _j k sh', '틗'],
    u'\ud2d8' : ['t_h M _j _n', '틘'],
    u'\ud2d9' : ['t_h M _j _n tS', '틙'],
    u'\ud2da' : ['t_h M _j _n _h', '틚'],
    u'\ud2db' : ['t_h M _j t', '틛'],
    u'\ud2dc' : ['t_h M _j _l', '틜'],
    u'\ud2dd' : ['t_h M _j _l k', '틝'],
    u'\ud2de' : ['t_h M _j _l m', '틞'],
    u'\ud2df' : ['t_h M _j _l p', '틟'],
    u'\ud2e0' : ['t_h M _j _l sh', '틠'],
    u'\ud2e1' : ['t_h M _j _l t_h', '틡'],
    u'\ud2e2' : ['t_h M _j _l p_h', '틢'],
    u'\ud2e3' : ['t_h M _j _l _h', '틣'],
    u'\ud2e4' : ['t_h M _j m', '틤'],
    u'\ud2e5' : ['t_h M _j p', '틥'],
    u'\ud2e6' : ['t_h M _j p sh', '틦'],
    u'\ud2e7' : ['t_h M _j sh', '틧'],
    u'\ud2e8' : ['t_h M _j s', '틨'],
    u'\ud2e9' : ['t_h M _j N', '틩'],
    u'\ud2ea' : ['t_h M _j tS', '틪'],
    u'\ud2eb' : ['t_h M _j tSh', '틫'],
    u'\ud2ec' : ['t_h M _j k_h', '틬'],
    u'\ud2ed' : ['t_h M _j t_h', '틭'],
    u'\ud2ee' : ['t_h M _j p_h', '틮'],
    u'\ud2ef' : ['t_h M _j _h', '틯'],
    u'\ud2f0' : ['t_h i', '티'],
    u'\ud2f1' : ['t_h i k', '틱'],
    u'\ud2f2' : ['t_h i k_>', '틲'],
    u'\ud2f3' : ['t_h i k sh', '틳'],
    u'\ud2f4' : ['t_h i _n', '틴'],
    u'\ud2f5' : ['t_h i _n tS', '틵'],
    u'\ud2f6' : ['t_h i _n _h', '틶'],
    u'\ud2f7' : ['t_h i t', '틷'],
    u'\ud2f8' : ['t_h i _l', '틸'],
    u'\ud2f9' : ['t_h i _l k', '틹'],
    u'\ud2fa' : ['t_h i _l m', '틺'],
    u'\ud2fb' : ['t_h i _l p', '틻'],
    u'\ud2fc' : ['t_h i _l sh', '틼'],
    u'\ud2fd' : ['t_h i _l t_h', '틽'],
    u'\ud2fe' : ['t_h i _l p_h', '틾'],
    u'\ud2ff' : ['t_h i _l _h', '틿'],
    u'\ud300' : ['t_h i m', '팀'],
    u'\ud301' : ['t_h i p', '팁'],
    u'\ud302' : ['t_h i p sh', '팂'],
    u'\ud303' : ['t_h i sh', '팃'],
    u'\ud304' : ['t_h i s', '팄'],
    u'\ud305' : ['t_h i N', '팅'],
    u'\ud306' : ['t_h i tS', '팆'],
    u'\ud307' : ['t_h i tSh', '팇'],
    u'\ud308' : ['t_h i k_h', '팈'],
    u'\ud309' : ['t_h i t_h', '팉'],
    u'\ud30a' : ['t_h i p_h', '팊'],
    u'\ud30b' : ['t_h i _h', '팋'],
    u'\ud30c' : ['p_h a', '파'],
    u'\ud30d' : ['p_h a k', '팍'],
    u'\ud30e' : ['p_h a k_>', '팎'],
    u'\ud30f' : ['p_h a k sh', '팏'],
    u'\ud310' : ['p_h a _n', '판'],
    u'\ud311' : ['p_h a _n tS', '팑'],
    u'\ud312' : ['p_h a _n _h', '팒'],
    u'\ud313' : ['p_h a t', '팓'],
    u'\ud314' : ['p_h a _l', '팔'],
    u'\ud315' : ['p_h a _l k', '팕'],
    u'\ud316' : ['p_h a _l m', '팖'],
    u'\ud317' : ['p_h a _l p', '팗'],
    u'\ud318' : ['p_h a _l sh', '팘'],
    u'\ud319' : ['p_h a _l t_h', '팙'],
    u'\ud31a' : ['p_h a _l p_h', '팚'],
    u'\ud31b' : ['p_h a _l _h', '팛'],
    u'\ud31c' : ['p_h a m', '팜'],
    u'\ud31d' : ['p_h a p', '팝'],
    u'\ud31e' : ['p_h a p sh', '팞'],
    u'\ud31f' : ['p_h a sh', '팟'],
    u'\ud320' : ['p_h a s', '팠'],
    u'\ud321' : ['p_h a N', '팡'],
    u'\ud322' : ['p_h a tS', '팢'],
    u'\ud323' : ['p_h a tSh', '팣'],
    u'\ud324' : ['p_h a k_h', '팤'],
    u'\ud325' : ['p_h a t_h', '팥'],
    u'\ud326' : ['p_h a p_h', '팦'],
    u'\ud327' : ['p_h a _h', '팧'],
    u'\ud328' : ['p_h {', '패'],
    u'\ud329' : ['p_h { k', '팩'],
    u'\ud32a' : ['p_h { k_>', '팪'],
    u'\ud32b' : ['p_h { k sh', '팫'],
    u'\ud32c' : ['p_h { _n', '팬'],
    u'\ud32d' : ['p_h { _n tS', '팭'],
    u'\ud32e' : ['p_h { _n _h', '팮'],
    u'\ud32f' : ['p_h { t', '팯'],
    u'\ud330' : ['p_h { _l', '팰'],
    u'\ud331' : ['p_h { _l k', '팱'],
    u'\ud332' : ['p_h { _l m', '팲'],
    u'\ud333' : ['p_h { _l p', '팳'],
    u'\ud334' : ['p_h { _l sh', '팴'],
    u'\ud335' : ['p_h { _l t_h', '팵'],
    u'\ud336' : ['p_h { _l p_h', '팶'],
    u'\ud337' : ['p_h { _l _h', '팷'],
    u'\ud338' : ['p_h { m', '팸'],
    u'\ud339' : ['p_h { p', '팹'],
    u'\ud33a' : ['p_h { p sh', '팺'],
    u'\ud33b' : ['p_h { sh', '팻'],
    u'\ud33c' : ['p_h { s', '팼'],
    u'\ud33d' : ['p_h { N', '팽'],
    u'\ud33e' : ['p_h { tS', '팾'],
    u'\ud33f' : ['p_h { tSh', '팿'],
    u'\ud340' : ['p_h { k_h', '퍀'],
    u'\ud341' : ['p_h { t_h', '퍁'],
    u'\ud342' : ['p_h { p_h', '퍂'],
    u'\ud343' : ['p_h { _h', '퍃'],
    u'\ud344' : ['p_h _j a', '퍄'],
    u'\ud345' : ['p_h _j a k', '퍅'],
    u'\ud346' : ['p_h _j a k_>', '퍆'],
    u'\ud347' : ['p_h _j a k sh', '퍇'],
    u'\ud348' : ['p_h _j a _n', '퍈'],
    u'\ud349' : ['p_h _j a _n tS', '퍉'],
    u'\ud34a' : ['p_h _j a _n _h', '퍊'],
    u'\ud34b' : ['p_h _j a t', '퍋'],
    u'\ud34c' : ['p_h _j a _l', '퍌'],
    u'\ud34d' : ['p_h _j a _l k', '퍍'],
    u'\ud34e' : ['p_h _j a _l m', '퍎'],
    u'\ud34f' : ['p_h _j a _l p', '퍏'],
    u'\ud350' : ['p_h _j a _l sh', '퍐'],
    u'\ud351' : ['p_h _j a _l t_h', '퍑'],
    u'\ud352' : ['p_h _j a _l p_h', '퍒'],
    u'\ud353' : ['p_h _j a _l _h', '퍓'],
    u'\ud354' : ['p_h _j a m', '퍔'],
    u'\ud355' : ['p_h _j a p', '퍕'],
    u'\ud356' : ['p_h _j a p sh', '퍖'],
    u'\ud357' : ['p_h _j a sh', '퍗'],
    u'\ud358' : ['p_h _j a s', '퍘'],
    u'\ud359' : ['p_h _j a N', '퍙'],
    u'\ud35a' : ['p_h _j a tS', '퍚'],
    u'\ud35b' : ['p_h _j a tSh', '퍛'],
    u'\ud35c' : ['p_h _j a k_h', '퍜'],
    u'\ud35d' : ['p_h _j a t_h', '퍝'],
    u'\ud35e' : ['p_h _j a p_h', '퍞'],
    u'\ud35f' : ['p_h _j a _h', '퍟'],
    u'\ud360' : ['p_h _j {', '퍠'],
    u'\ud361' : ['p_h _j { k', '퍡'],
    u'\ud362' : ['p_h _j { k_>', '퍢'],
    u'\ud363' : ['p_h _j { k sh', '퍣'],
    u'\ud364' : ['p_h _j { _n', '퍤'],
    u'\ud365' : ['p_h _j { _n tS', '퍥'],
    u'\ud366' : ['p_h _j { _n _h', '퍦'],
    u'\ud367' : ['p_h _j { t', '퍧'],
    u'\ud368' : ['p_h _j { _l', '퍨'],
    u'\ud369' : ['p_h _j { _l k', '퍩'],
    u'\ud36a' : ['p_h _j { _l m', '퍪'],
    u'\ud36b' : ['p_h _j { _l p', '퍫'],
    u'\ud36c' : ['p_h _j { _l sh', '퍬'],
    u'\ud36d' : ['p_h _j { _l t_h', '퍭'],
    u'\ud36e' : ['p_h _j { _l p_h', '퍮'],
    u'\ud36f' : ['p_h _j { _l _h', '퍯'],
    u'\ud370' : ['p_h _j { m', '퍰'],
    u'\ud371' : ['p_h _j { p', '퍱'],
    u'\ud372' : ['p_h _j { p sh', '퍲'],
    u'\ud373' : ['p_h _j { sh', '퍳'],
    u'\ud374' : ['p_h _j { s', '퍴'],
    u'\ud375' : ['p_h _j { N', '퍵'],
    u'\ud376' : ['p_h _j { tS', '퍶'],
    u'\ud377' : ['p_h _j { tSh', '퍷'],
    u'\ud378' : ['p_h _j { k_h', '퍸'],
    u'\ud379' : ['p_h _j { t_h', '퍹'],
    u'\ud37a' : ['p_h _j { p_h', '퍺'],
    u'\ud37b' : ['p_h _j { _h', '퍻'],
    u'\ud37c' : ['p_h _r', '퍼'],
    u'\ud37d' : ['p_h _r k', '퍽'],
    u'\ud37e' : ['p_h _r k_>', '퍾'],
    u'\ud37f' : ['p_h _r k sh', '퍿'],
    u'\ud380' : ['p_h _r _n', '펀'],
    u'\ud381' : ['p_h _r _n tS', '펁'],
    u'\ud382' : ['p_h _r _n _h', '펂'],
    u'\ud383' : ['p_h _r t', '펃'],
    u'\ud384' : ['p_h _r _l', '펄'],
    u'\ud385' : ['p_h _r _l k', '펅'],
    u'\ud386' : ['p_h _r _l m', '펆'],
    u'\ud387' : ['p_h _r _l p', '펇'],
    u'\ud388' : ['p_h _r _l sh', '펈'],
    u'\ud389' : ['p_h _r _l t_h', '펉'],
    u'\ud38a' : ['p_h _r _l p_h', '펊'],
    u'\ud38b' : ['p_h _r _l _h', '펋'],
    u'\ud38c' : ['p_h _r m', '펌'],
    u'\ud38d' : ['p_h _r p', '펍'],
    u'\ud38e' : ['p_h _r p sh', '펎'],
    u'\ud38f' : ['p_h _r sh', '펏'],
    u'\ud390' : ['p_h _r s', '펐'],
    u'\ud391' : ['p_h _r N', '펑'],
    u'\ud392' : ['p_h _r tS', '펒'],
    u'\ud393' : ['p_h _r tSh', '펓'],
    u'\ud394' : ['p_h _r k_h', '펔'],
    u'\ud395' : ['p_h _r t_h', '펕'],
    u'\ud396' : ['p_h _r p_h', '펖'],
    u'\ud397' : ['p_h _r _h', '펗'],
    u'\ud398' : ['p_h e', '페'],
    u'\ud399' : ['p_h e k', '펙'],
    u'\ud39a' : ['p_h e k_>', '펚'],
    u'\ud39b' : ['p_h e k sh', '펛'],
    u'\ud39c' : ['p_h e _n', '펜'],
    u'\ud39d' : ['p_h e _n tS', '펝'],
    u'\ud39e' : ['p_h e _n _h', '펞'],
    u'\ud39f' : ['p_h e t', '펟'],
    u'\ud3a0' : ['p_h e _l', '펠'],
    u'\ud3a1' : ['p_h e _l k', '펡'],
    u'\ud3a2' : ['p_h e _l m', '펢'],
    u'\ud3a3' : ['p_h e _l p', '펣'],
    u'\ud3a4' : ['p_h e _l sh', '펤'],
    u'\ud3a5' : ['p_h e _l t_h', '펥'],
    u'\ud3a6' : ['p_h e _l p_h', '펦'],
    u'\ud3a7' : ['p_h e _l _h', '펧'],
    u'\ud3a8' : ['p_h e m', '펨'],
    u'\ud3a9' : ['p_h e p', '펩'],
    u'\ud3aa' : ['p_h e p sh', '펪'],
    u'\ud3ab' : ['p_h e sh', '펫'],
    u'\ud3ac' : ['p_h e s', '펬'],
    u'\ud3ad' : ['p_h e N', '펭'],
    u'\ud3ae' : ['p_h e tS', '펮'],
    u'\ud3af' : ['p_h e tSh', '펯'],
    u'\ud3b0' : ['p_h e k_h', '펰'],
    u'\ud3b1' : ['p_h e t_h', '펱'],
    u'\ud3b2' : ['p_h e p_h', '펲'],
    u'\ud3b3' : ['p_h e _h', '펳'],
    u'\ud3b4' : ['p_h _j _r', '펴'],
    u'\ud3b5' : ['p_h _j _r k', '펵'],
    u'\ud3b6' : ['p_h _j _r k_>', '펶'],
    u'\ud3b7' : ['p_h _j _r k sh', '펷'],
    u'\ud3b8' : ['p_h _j _r _n', '편'],
    u'\ud3b9' : ['p_h _j _r _n tS', '펹'],
    u'\ud3ba' : ['p_h _j _r _n _h', '펺'],
    u'\ud3bb' : ['p_h _j _r t', '펻'],
    u'\ud3bc' : ['p_h _j _r _l', '펼'],
    u'\ud3bd' : ['p_h _j _r _l k', '펽'],
    u'\ud3be' : ['p_h _j _r _l m', '펾'],
    u'\ud3bf' : ['p_h _j _r _l p', '펿'],
    u'\ud3c0' : ['p_h _j _r _l sh', '폀'],
    u'\ud3c1' : ['p_h _j _r _l t_h', '폁'],
    u'\ud3c2' : ['p_h _j _r _l p_h', '폂'],
    u'\ud3c3' : ['p_h _j _r _l _h', '폃'],
    u'\ud3c4' : ['p_h _j _r m', '폄'],
    u'\ud3c5' : ['p_h _j _r p', '폅'],
    u'\ud3c6' : ['p_h _j _r p sh', '폆'],
    u'\ud3c7' : ['p_h _j _r sh', '폇'],
    u'\ud3c8' : ['p_h _j _r s', '폈'],
    u'\ud3c9' : ['p_h _j _r N', '평'],
    u'\ud3ca' : ['p_h _j _r tS', '폊'],
    u'\ud3cb' : ['p_h _j _r tSh', '폋'],
    u'\ud3cc' : ['p_h _j _r k_h', '폌'],
    u'\ud3cd' : ['p_h _j _r t_h', '폍'],
    u'\ud3ce' : ['p_h _j _r p_h', '폎'],
    u'\ud3cf' : ['p_h _j _r _h', '폏'],
    u'\ud3d0' : ['p_h _j e', '폐'],
    u'\ud3d1' : ['p_h _j e k', '폑'],
    u'\ud3d2' : ['p_h _j e k_>', '폒'],
    u'\ud3d3' : ['p_h _j e k sh', '폓'],
    u'\ud3d4' : ['p_h _j e _n', '폔'],
    u'\ud3d5' : ['p_h _j e _n tS', '폕'],
    u'\ud3d6' : ['p_h _j e _n _h', '폖'],
    u'\ud3d7' : ['p_h _j e t', '폗'],
    u'\ud3d8' : ['p_h _j e _l', '폘'],
    u'\ud3d9' : ['p_h _j e _l k', '폙'],
    u'\ud3da' : ['p_h _j e _l m', '폚'],
    u'\ud3db' : ['p_h _j e _l p', '폛'],
    u'\ud3dc' : ['p_h _j e _l sh', '폜'],
    u'\ud3dd' : ['p_h _j e _l t_h', '폝'],
    u'\ud3de' : ['p_h _j e _l p_h', '폞'],
    u'\ud3df' : ['p_h _j e _l _h', '폟'],
    u'\ud3e0' : ['p_h _j e m', '폠'],
    u'\ud3e1' : ['p_h _j e p', '폡'],
    u'\ud3e2' : ['p_h _j e p sh', '폢'],
    u'\ud3e3' : ['p_h _j e sh', '폣'],
    u'\ud3e4' : ['p_h _j e s', '폤'],
    u'\ud3e5' : ['p_h _j e N', '폥'],
    u'\ud3e6' : ['p_h _j e tS', '폦'],
    u'\ud3e7' : ['p_h _j e tSh', '폧'],
    u'\ud3e8' : ['p_h _j e k_h', '폨'],
    u'\ud3e9' : ['p_h _j e t_h', '폩'],
    u'\ud3ea' : ['p_h _j e p_h', '폪'],
    u'\ud3eb' : ['p_h _j e _h', '폫'],
    u'\ud3ec' : ['p_h o', '포'],
    u'\ud3ed' : ['p_h o k', '폭'],
    u'\ud3ee' : ['p_h o k_>', '폮'],
    u'\ud3ef' : ['p_h o k sh', '폯'],
    u'\ud3f0' : ['p_h o _n', '폰'],
    u'\ud3f1' : ['p_h o _n tS', '폱'],
    u'\ud3f2' : ['p_h o _n _h', '폲'],
    u'\ud3f3' : ['p_h o t', '폳'],
    u'\ud3f4' : ['p_h o _l', '폴'],
    u'\ud3f5' : ['p_h o _l k', '폵'],
    u'\ud3f6' : ['p_h o _l m', '폶'],
    u'\ud3f7' : ['p_h o _l p', '폷'],
    u'\ud3f8' : ['p_h o _l sh', '폸'],
    u'\ud3f9' : ['p_h o _l t_h', '폹'],
    u'\ud3fa' : ['p_h o _l p_h', '폺'],
    u'\ud3fb' : ['p_h o _l _h', '폻'],
    u'\ud3fc' : ['p_h o m', '폼'],
    u'\ud3fd' : ['p_h o p', '폽'],
    u'\ud3fe' : ['p_h o p sh', '폾'],
    u'\ud3ff' : ['p_h o sh', '폿'],
    u'\ud400' : ['p_h o s', '퐀'],
    u'\ud401' : ['p_h o N', '퐁'],
    u'\ud402' : ['p_h o tS', '퐂'],
    u'\ud403' : ['p_h o tSh', '퐃'],
    u'\ud404' : ['p_h o k_h', '퐄'],
    u'\ud405' : ['p_h o t_h', '퐅'],
    u'\ud406' : ['p_h o p_h', '퐆'],
    u'\ud407' : ['p_h o _h', '퐇'],
    u'\ud408' : ['p_h _w a', '퐈'],
    u'\ud409' : ['p_h _w a k', '퐉'],
    u'\ud40a' : ['p_h _w a k_>', '퐊'],
    u'\ud40b' : ['p_h _w a k sh', '퐋'],
    u'\ud40c' : ['p_h _w a _n', '퐌'],
    u'\ud40d' : ['p_h _w a _n tS', '퐍'],
    u'\ud40e' : ['p_h _w a _n _h', '퐎'],
    u'\ud40f' : ['p_h _w a t', '퐏'],
    u'\ud410' : ['p_h _w a _l', '퐐'],
    u'\ud411' : ['p_h _w a _l k', '퐑'],
    u'\ud412' : ['p_h _w a _l m', '퐒'],
    u'\ud413' : ['p_h _w a _l p', '퐓'],
    u'\ud414' : ['p_h _w a _l sh', '퐔'],
    u'\ud415' : ['p_h _w a _l t_h', '퐕'],
    u'\ud416' : ['p_h _w a _l p_h', '퐖'],
    u'\ud417' : ['p_h _w a _l _h', '퐗'],
    u'\ud418' : ['p_h _w a m', '퐘'],
    u'\ud419' : ['p_h _w a p', '퐙'],
    u'\ud41a' : ['p_h _w a p sh', '퐚'],
    u'\ud41b' : ['p_h _w a sh', '퐛'],
    u'\ud41c' : ['p_h _w a s', '퐜'],
    u'\ud41d' : ['p_h _w a N', '퐝'],
    u'\ud41e' : ['p_h _w a tS', '퐞'],
    u'\ud41f' : ['p_h _w a tSh', '퐟'],
    u'\ud420' : ['p_h _w a k_h', '퐠'],
    u'\ud421' : ['p_h _w a t_h', '퐡'],
    u'\ud422' : ['p_h _w a p_h', '퐢'],
    u'\ud423' : ['p_h _w a _h', '퐣'],
    u'\ud424' : ['p_h _w {', '퐤'],
    u'\ud425' : ['p_h _w { k', '퐥'],
    u'\ud426' : ['p_h _w { k_>', '퐦'],
    u'\ud427' : ['p_h _w { k sh', '퐧'],
    u'\ud428' : ['p_h _w { _n', '퐨'],
    u'\ud429' : ['p_h _w { _n tS', '퐩'],
    u'\ud42a' : ['p_h _w { _n _h', '퐪'],
    u'\ud42b' : ['p_h _w { t', '퐫'],
    u'\ud42c' : ['p_h _w { _l', '퐬'],
    u'\ud42d' : ['p_h _w { _l k', '퐭'],
    u'\ud42e' : ['p_h _w { _l m', '퐮'],
    u'\ud42f' : ['p_h _w { _l p', '퐯'],
    u'\ud430' : ['p_h _w { _l sh', '퐰'],
    u'\ud431' : ['p_h _w { _l t_h', '퐱'],
    u'\ud432' : ['p_h _w { _l p_h', '퐲'],
    u'\ud433' : ['p_h _w { _l _h', '퐳'],
    u'\ud434' : ['p_h _w { m', '퐴'],
    u'\ud435' : ['p_h _w { p', '퐵'],
    u'\ud436' : ['p_h _w { p sh', '퐶'],
    u'\ud437' : ['p_h _w { sh', '퐷'],
    u'\ud438' : ['p_h _w { s', '퐸'],
    u'\ud439' : ['p_h _w { N', '퐹'],
    u'\ud43a' : ['p_h _w { tS', '퐺'],
    u'\ud43b' : ['p_h _w { tSh', '퐻'],
    u'\ud43c' : ['p_h _w { k_h', '퐼'],
    u'\ud43d' : ['p_h _w { t_h', '퐽'],
    u'\ud43e' : ['p_h _w { p_h', '퐾'],
    u'\ud43f' : ['p_h _w { _h', '퐿'],
    u'\ud440' : ['p_h _w e', '푀'],
    u'\ud441' : ['p_h _w e k', '푁'],
    u'\ud442' : ['p_h _w e k_>', '푂'],
    u'\ud443' : ['p_h _w e k sh', '푃'],
    u'\ud444' : ['p_h _w e _n', '푄'],
    u'\ud445' : ['p_h _w e _n tS', '푅'],
    u'\ud446' : ['p_h _w e _n _h', '푆'],
    u'\ud447' : ['p_h _w e t', '푇'],
    u'\ud448' : ['p_h _w e _l', '푈'],
    u'\ud449' : ['p_h _w e _l k', '푉'],
    u'\ud44a' : ['p_h _w e _l m', '푊'],
    u'\ud44b' : ['p_h _w e _l p', '푋'],
    u'\ud44c' : ['p_h _w e _l sh', '푌'],
    u'\ud44d' : ['p_h _w e _l t_h', '푍'],
    u'\ud44e' : ['p_h _w e _l p_h', '푎'],
    u'\ud44f' : ['p_h _w e _l _h', '푏'],
    u'\ud450' : ['p_h _w e m', '푐'],
    u'\ud451' : ['p_h _w e p', '푑'],
    u'\ud452' : ['p_h _w e p sh', '푒'],
    u'\ud453' : ['p_h _w e sh', '푓'],
    u'\ud454' : ['p_h _w e s', '푔'],
    u'\ud455' : ['p_h _w e N', '푕'],
    u'\ud456' : ['p_h _w e tS', '푖'],
    u'\ud457' : ['p_h _w e tSh', '푗'],
    u'\ud458' : ['p_h _w e k_h', '푘'],
    u'\ud459' : ['p_h _w e t_h', '푙'],
    u'\ud45a' : ['p_h _w e p_h', '푚'],
    u'\ud45b' : ['p_h _w e _h', '푛'],
    u'\ud45c' : ['p_h _j o', '표'],
    u'\ud45d' : ['p_h _j o k', '푝'],
    u'\ud45e' : ['p_h _j o k_>', '푞'],
    u'\ud45f' : ['p_h _j o k sh', '푟'],
    u'\ud460' : ['p_h _j o _n', '푠'],
    u'\ud461' : ['p_h _j o _n tS', '푡'],
    u'\ud462' : ['p_h _j o _n _h', '푢'],
    u'\ud463' : ['p_h _j o t', '푣'],
    u'\ud464' : ['p_h _j o _l', '푤'],
    u'\ud465' : ['p_h _j o _l k', '푥'],
    u'\ud466' : ['p_h _j o _l m', '푦'],
    u'\ud467' : ['p_h _j o _l p', '푧'],
    u'\ud468' : ['p_h _j o _l sh', '푨'],
    u'\ud469' : ['p_h _j o _l t_h', '푩'],
    u'\ud46a' : ['p_h _j o _l p_h', '푪'],
    u'\ud46b' : ['p_h _j o _l _h', '푫'],
    u'\ud46c' : ['p_h _j o m', '푬'],
    u'\ud46d' : ['p_h _j o p', '푭'],
    u'\ud46e' : ['p_h _j o p sh', '푮'],
    u'\ud46f' : ['p_h _j o sh', '푯'],
    u'\ud470' : ['p_h _j o s', '푰'],
    u'\ud471' : ['p_h _j o N', '푱'],
    u'\ud472' : ['p_h _j o tS', '푲'],
    u'\ud473' : ['p_h _j o tSh', '푳'],
    u'\ud474' : ['p_h _j o k_h', '푴'],
    u'\ud475' : ['p_h _j o t_h', '푵'],
    u'\ud476' : ['p_h _j o p_h', '푶'],
    u'\ud477' : ['p_h _j o _h', '푷'],
    u'\ud478' : ['p_h u', '푸'],
    u'\ud479' : ['p_h u k', '푹'],
    u'\ud47a' : ['p_h u k_>', '푺'],
    u'\ud47b' : ['p_h u k sh', '푻'],
    u'\ud47c' : ['p_h u _n', '푼'],
    u'\ud47d' : ['p_h u _n tS', '푽'],
    u'\ud47e' : ['p_h u _n _h', '푾'],
    u'\ud47f' : ['p_h u t', '푿'],
    u'\ud480' : ['p_h u _l', '풀'],
    u'\ud481' : ['p_h u _l k', '풁'],
    u'\ud482' : ['p_h u _l m', '풂'],
    u'\ud483' : ['p_h u _l p', '풃'],
    u'\ud484' : ['p_h u _l sh', '풄'],
    u'\ud485' : ['p_h u _l t_h', '풅'],
    u'\ud486' : ['p_h u _l p_h', '풆'],
    u'\ud487' : ['p_h u _l _h', '풇'],
    u'\ud488' : ['p_h u m', '품'],
    u'\ud489' : ['p_h u p', '풉'],
    u'\ud48a' : ['p_h u p sh', '풊'],
    u'\ud48b' : ['p_h u sh', '풋'],
    u'\ud48c' : ['p_h u s', '풌'],
    u'\ud48d' : ['p_h u N', '풍'],
    u'\ud48e' : ['p_h u tS', '풎'],
    u'\ud48f' : ['p_h u tSh', '풏'],
    u'\ud490' : ['p_h u k_h', '풐'],
    u'\ud491' : ['p_h u t_h', '풑'],
    u'\ud492' : ['p_h u p_h', '풒'],
    u'\ud493' : ['p_h u _h', '풓'],
    u'\ud494' : ['p_h _w _r', '풔'],
    u'\ud495' : ['p_h _w _r k', '풕'],
    u'\ud496' : ['p_h _w _r k_>', '풖'],
    u'\ud497' : ['p_h _w _r k sh', '풗'],
    u'\ud498' : ['p_h _w _r _n', '풘'],
    u'\ud499' : ['p_h _w _r _n tS', '풙'],
    u'\ud49a' : ['p_h _w _r _n _h', '풚'],
    u'\ud49b' : ['p_h _w _r t', '풛'],
    u'\ud49c' : ['p_h _w _r _l', '풜'],
    u'\ud49d' : ['p_h _w _r _l k', '풝'],
    u'\ud49e' : ['p_h _w _r _l m', '풞'],
    u'\ud49f' : ['p_h _w _r _l p', '풟'],
    u'\ud4a0' : ['p_h _w _r _l sh', '풠'],
    u'\ud4a1' : ['p_h _w _r _l t_h', '풡'],
    u'\ud4a2' : ['p_h _w _r _l p_h', '풢'],
    u'\ud4a3' : ['p_h _w _r _l _h', '풣'],
    u'\ud4a4' : ['p_h _w _r m', '풤'],
    u'\ud4a5' : ['p_h _w _r p', '풥'],
    u'\ud4a6' : ['p_h _w _r p sh', '풦'],
    u'\ud4a7' : ['p_h _w _r sh', '풧'],
    u'\ud4a8' : ['p_h _w _r s', '풨'],
    u'\ud4a9' : ['p_h _w _r N', '풩'],
    u'\ud4aa' : ['p_h _w _r tS', '풪'],
    u'\ud4ab' : ['p_h _w _r tSh', '풫'],
    u'\ud4ac' : ['p_h _w _r k_h', '풬'],
    u'\ud4ad' : ['p_h _w _r t_h', '풭'],
    u'\ud4ae' : ['p_h _w _r p_h', '풮'],
    u'\ud4af' : ['p_h _w _r _h', '풯'],
    u'\ud4b0' : ['p_h _w E', '풰'],
    u'\ud4b1' : ['p_h _w E k', '풱'],
    u'\ud4b2' : ['p_h _w E k_>', '풲'],
    u'\ud4b3' : ['p_h _w E k sh', '풳'],
    u'\ud4b4' : ['p_h _w E _n', '풴'],
    u'\ud4b5' : ['p_h _w E _n tS', '풵'],
    u'\ud4b6' : ['p_h _w E _n _h', '풶'],
    u'\ud4b7' : ['p_h _w E t', '풷'],
    u'\ud4b8' : ['p_h _w E _l', '풸'],
    u'\ud4b9' : ['p_h _w E _l k', '풹'],
    u'\ud4ba' : ['p_h _w E _l m', '풺'],
    u'\ud4bb' : ['p_h _w E _l p', '풻'],
    u'\ud4bc' : ['p_h _w E _l sh', '풼'],
    u'\ud4bd' : ['p_h _w E _l t_h', '풽'],
    u'\ud4be' : ['p_h _w E _l p_h', '풾'],
    u'\ud4bf' : ['p_h _w E _l _h', '풿'],
    u'\ud4c0' : ['p_h _w E m', '퓀'],
    u'\ud4c1' : ['p_h _w E p', '퓁'],
    u'\ud4c2' : ['p_h _w E p sh', '퓂'],
    u'\ud4c3' : ['p_h _w E sh', '퓃'],
    u'\ud4c4' : ['p_h _w E s', '퓄'],
    u'\ud4c5' : ['p_h _w E N', '퓅'],
    u'\ud4c6' : ['p_h _w E tS', '퓆'],
    u'\ud4c7' : ['p_h _w E tSh', '퓇'],
    u'\ud4c8' : ['p_h _w E k_h', '퓈'],
    u'\ud4c9' : ['p_h _w E t_h', '퓉'],
    u'\ud4ca' : ['p_h _w E p_h', '퓊'],
    u'\ud4cb' : ['p_h _w E _h', '퓋'],
    u'\ud4cc' : ['p_h 2', '퓌'],
    u'\ud4cd' : ['p_h 2 k', '퓍'],
    u'\ud4ce' : ['p_h 2 k_>', '퓎'],
    u'\ud4cf' : ['p_h 2 k sh', '퓏'],
    u'\ud4d0' : ['p_h 2 _n', '퓐'],
    u'\ud4d1' : ['p_h 2 _n tS', '퓑'],
    u'\ud4d2' : ['p_h 2 _n _h', '퓒'],
    u'\ud4d3' : ['p_h 2 t', '퓓'],
    u'\ud4d4' : ['p_h 2 _l', '퓔'],
    u'\ud4d5' : ['p_h 2 _l k', '퓕'],
    u'\ud4d6' : ['p_h 2 _l m', '퓖'],
    u'\ud4d7' : ['p_h 2 _l p', '퓗'],
    u'\ud4d8' : ['p_h 2 _l sh', '퓘'],
    u'\ud4d9' : ['p_h 2 _l t_h', '퓙'],
    u'\ud4da' : ['p_h 2 _l p_h', '퓚'],
    u'\ud4db' : ['p_h 2 _l _h', '퓛'],
    u'\ud4dc' : ['p_h 2 m', '퓜'],
    u'\ud4dd' : ['p_h 2 p', '퓝'],
    u'\ud4de' : ['p_h 2 p sh', '퓞'],
    u'\ud4df' : ['p_h 2 sh', '퓟'],
    u'\ud4e0' : ['p_h 2 s', '퓠'],
    u'\ud4e1' : ['p_h 2 N', '퓡'],
    u'\ud4e2' : ['p_h 2 tS', '퓢'],
    u'\ud4e3' : ['p_h 2 tSh', '퓣'],
    u'\ud4e4' : ['p_h 2 k_h', '퓤'],
    u'\ud4e5' : ['p_h 2 t_h', '퓥'],
    u'\ud4e6' : ['p_h 2 p_h', '퓦'],
    u'\ud4e7' : ['p_h 2 _h', '퓧'],
    u'\ud4e8' : ['p_h _j u', '퓨'],
    u'\ud4e9' : ['p_h _j u k', '퓩'],
    u'\ud4ea' : ['p_h _j u k_>', '퓪'],
    u'\ud4eb' : ['p_h _j u k sh', '퓫'],
    u'\ud4ec' : ['p_h _j u _n', '퓬'],
    u'\ud4ed' : ['p_h _j u _n tS', '퓭'],
    u'\ud4ee' : ['p_h _j u _n _h', '퓮'],
    u'\ud4ef' : ['p_h _j u t', '퓯'],
    u'\ud4f0' : ['p_h _j u _l', '퓰'],
    u'\ud4f1' : ['p_h _j u _l k', '퓱'],
    u'\ud4f2' : ['p_h _j u _l m', '퓲'],
    u'\ud4f3' : ['p_h _j u _l p', '퓳'],
    u'\ud4f4' : ['p_h _j u _l sh', '퓴'],
    u'\ud4f5' : ['p_h _j u _l t_h', '퓵'],
    u'\ud4f6' : ['p_h _j u _l p_h', '퓶'],
    u'\ud4f7' : ['p_h _j u _l _h', '퓷'],
    u'\ud4f8' : ['p_h _j u m', '퓸'],
    u'\ud4f9' : ['p_h _j u p', '퓹'],
    u'\ud4fa' : ['p_h _j u p sh', '퓺'],
    u'\ud4fb' : ['p_h _j u sh', '퓻'],
    u'\ud4fc' : ['p_h _j u s', '퓼'],
    u'\ud4fd' : ['p_h _j u N', '퓽'],
    u'\ud4fe' : ['p_h _j u tS', '퓾'],
    u'\ud4ff' : ['p_h _j u tSh', '퓿'],
    u'\ud500' : ['p_h _j u k_h', '픀'],
    u'\ud501' : ['p_h _j u t_h', '픁'],
    u'\ud502' : ['p_h _j u p_h', '픂'],
    u'\ud503' : ['p_h _j u _h', '픃'],
    u'\ud504' : ['p_h M', '프'],
    u'\ud505' : ['p_h M k', '픅'],
    u'\ud506' : ['p_h M k_>', '픆'],
    u'\ud507' : ['p_h M k sh', '픇'],
    u'\ud508' : ['p_h M _n', '픈'],
    u'\ud509' : ['p_h M _n tS', '픉'],
    u'\ud50a' : ['p_h M _n _h', '픊'],
    u'\ud50b' : ['p_h M t', '픋'],
    u'\ud50c' : ['p_h M _l', '플'],
    u'\ud50d' : ['p_h M _l k', '픍'],
    u'\ud50e' : ['p_h M _l m', '픎'],
    u'\ud50f' : ['p_h M _l p', '픏'],
    u'\ud510' : ['p_h M _l sh', '픐'],
    u'\ud511' : ['p_h M _l t_h', '픑'],
    u'\ud512' : ['p_h M _l p_h', '픒'],
    u'\ud513' : ['p_h M _l _h', '픓'],
    u'\ud514' : ['p_h M m', '픔'],
    u'\ud515' : ['p_h M p', '픕'],
    u'\ud516' : ['p_h M p sh', '픖'],
    u'\ud517' : ['p_h M sh', '픗'],
    u'\ud518' : ['p_h M s', '픘'],
    u'\ud519' : ['p_h M N', '픙'],
    u'\ud51a' : ['p_h M tS', '픚'],
    u'\ud51b' : ['p_h M tSh', '픛'],
    u'\ud51c' : ['p_h M k_h', '픜'],
    u'\ud51d' : ['p_h M t_h', '픝'],
    u'\ud51e' : ['p_h M p_h', '픞'],
    u'\ud51f' : ['p_h M _h', '픟'],
    u'\ud520' : ['p_h M _j', '픠'],
    u'\ud521' : ['p_h M _j k', '픡'],
    u'\ud522' : ['p_h M _j k_>', '픢'],
    u'\ud523' : ['p_h M _j k sh', '픣'],
    u'\ud524' : ['p_h M _j _n', '픤'],
    u'\ud525' : ['p_h M _j _n tS', '픥'],
    u'\ud526' : ['p_h M _j _n _h', '픦'],
    u'\ud527' : ['p_h M _j t', '픧'],
    u'\ud528' : ['p_h M _j _l', '픨'],
    u'\ud529' : ['p_h M _j _l k', '픩'],
    u'\ud52a' : ['p_h M _j _l m', '픪'],
    u'\ud52b' : ['p_h M _j _l p', '픫'],
    u'\ud52c' : ['p_h M _j _l sh', '픬'],
    u'\ud52d' : ['p_h M _j _l t_h', '픭'],
    u'\ud52e' : ['p_h M _j _l p_h', '픮'],
    u'\ud52f' : ['p_h M _j _l _h', '픯'],
    u'\ud530' : ['p_h M _j m', '픰'],
    u'\ud531' : ['p_h M _j p', '픱'],
    u'\ud532' : ['p_h M _j p sh', '픲'],
    u'\ud533' : ['p_h M _j sh', '픳'],
    u'\ud534' : ['p_h M _j s', '픴'],
    u'\ud535' : ['p_h M _j N', '픵'],
    u'\ud536' : ['p_h M _j tS', '픶'],
    u'\ud537' : ['p_h M _j tSh', '픷'],
    u'\ud538' : ['p_h M _j k_h', '픸'],
    u'\ud539' : ['p_h M _j t_h', '픹'],
    u'\ud53a' : ['p_h M _j p_h', '픺'],
    u'\ud53b' : ['p_h M _j _h', '픻'],
    u'\ud53c' : ['p_h i', '피'],
    u'\ud53d' : ['p_h i k', '픽'],
    u'\ud53e' : ['p_h i k_>', '픾'],
    u'\ud53f' : ['p_h i k sh', '픿'],
    u'\ud540' : ['p_h i _n', '핀'],
    u'\ud541' : ['p_h i _n tS', '핁'],
    u'\ud542' : ['p_h i _n _h', '핂'],
    u'\ud543' : ['p_h i t', '핃'],
    u'\ud544' : ['p_h i _l', '필'],
    u'\ud545' : ['p_h i _l k', '핅'],
    u'\ud546' : ['p_h i _l m', '핆'],
    u'\ud547' : ['p_h i _l p', '핇'],
    u'\ud548' : ['p_h i _l sh', '핈'],
    u'\ud549' : ['p_h i _l t_h', '핉'],
    u'\ud54a' : ['p_h i _l p_h', '핊'],
    u'\ud54b' : ['p_h i _l _h', '핋'],
    u'\ud54c' : ['p_h i m', '핌'],
    u'\ud54d' : ['p_h i p', '핍'],
    u'\ud54e' : ['p_h i p sh', '핎'],
    u'\ud54f' : ['p_h i sh', '핏'],
    u'\ud550' : ['p_h i s', '핐'],
    u'\ud551' : ['p_h i N', '핑'],
    u'\ud552' : ['p_h i tS', '핒'],
    u'\ud553' : ['p_h i tSh', '핓'],
    u'\ud554' : ['p_h i k_h', '핔'],
    u'\ud555' : ['p_h i t_h', '핕'],
    u'\ud556' : ['p_h i p_h', '핖'],
    u'\ud557' : ['p_h i _h', '핗'],
    u'\ud558' : ['_h a', '하'],
    u'\ud559' : ['_h a k', '학'],
    u'\ud55a' : ['_h a k_>', '핚'],
    u'\ud55b' : ['_h a k sh', '핛'],
    u'\ud55c' : ['_h a _n', '한'],
    u'\ud55d' : ['_h a _n tS', '핝'],
    u'\ud55e' : ['_h a _n _h', '핞'],
    u'\ud55f' : ['_h a t', '핟'],
    u'\ud560' : ['_h a _l', '할'],
    u'\ud561' : ['_h a _l k', '핡'],
    u'\ud562' : ['_h a _l m', '핢'],
    u'\ud563' : ['_h a _l p', '핣'],
    u'\ud564' : ['_h a _l sh', '핤'],
    u'\ud565' : ['_h a _l t_h', '핥'],
    u'\ud566' : ['_h a _l p_h', '핦'],
    u'\ud567' : ['_h a _l _h', '핧'],
    u'\ud568' : ['_h a m', '함'],
    u'\ud569' : ['_h a p', '합'],
    u'\ud56a' : ['_h a p sh', '핪'],
    u'\ud56b' : ['_h a sh', '핫'],
    u'\ud56c' : ['_h a s', '핬'],
    u'\ud56d' : ['_h a N', '항'],
    u'\ud56e' : ['_h a tS', '핮'],
    u'\ud56f' : ['_h a tSh', '핯'],
    u'\ud570' : ['_h a k_h', '핰'],
    u'\ud571' : ['_h a t_h', '핱'],
    u'\ud572' : ['_h a p_h', '핲'],
    u'\ud573' : ['_h a _h', '핳'],
    u'\ud574' : ['_h {', '해'],
    u'\ud575' : ['_h { k', '핵'],
    u'\ud576' : ['_h { k_>', '핶'],
    u'\ud577' : ['_h { k sh', '핷'],
    u'\ud578' : ['_h { _n', '핸'],
    u'\ud579' : ['_h { _n tS', '핹'],
    u'\ud57a' : ['_h { _n _h', '핺'],
    u'\ud57b' : ['_h { t', '핻'],
    u'\ud57c' : ['_h { _l', '핼'],
    u'\ud57d' : ['_h { _l k', '핽'],
    u'\ud57e' : ['_h { _l m', '핾'],
    u'\ud57f' : ['_h { _l p', '핿'],
    u'\ud580' : ['_h { _l sh', '햀'],
    u'\ud581' : ['_h { _l t_h', '햁'],
    u'\ud582' : ['_h { _l p_h', '햂'],
    u'\ud583' : ['_h { _l _h', '햃'],
    u'\ud584' : ['_h { m', '햄'],
    u'\ud585' : ['_h { p', '햅'],
    u'\ud586' : ['_h { p sh', '햆'],
    u'\ud587' : ['_h { sh', '햇'],
    u'\ud588' : ['_h { s', '했'],
    u'\ud589' : ['_h { N', '행'],
    u'\ud58a' : ['_h { tS', '햊'],
    u'\ud58b' : ['_h { tSh', '햋'],
    u'\ud58c' : ['_h { k_h', '햌'],
    u'\ud58d' : ['_h { t_h', '햍'],
    u'\ud58e' : ['_h { p_h', '햎'],
    u'\ud58f' : ['_h { _h', '햏'],
    u'\ud590' : ['_h _j a', '햐'],
    u'\ud591' : ['_h _j a k', '햑'],
    u'\ud592' : ['_h _j a k_>', '햒'],
    u'\ud593' : ['_h _j a k sh', '햓'],
    u'\ud594' : ['_h _j a _n', '햔'],
    u'\ud595' : ['_h _j a _n tS', '햕'],
    u'\ud596' : ['_h _j a _n _h', '햖'],
    u'\ud597' : ['_h _j a t', '햗'],
    u'\ud598' : ['_h _j a _l', '햘'],
    u'\ud599' : ['_h _j a _l k', '햙'],
    u'\ud59a' : ['_h _j a _l m', '햚'],
    u'\ud59b' : ['_h _j a _l p', '햛'],
    u'\ud59c' : ['_h _j a _l sh', '햜'],
    u'\ud59d' : ['_h _j a _l t_h', '햝'],
    u'\ud59e' : ['_h _j a _l p_h', '햞'],
    u'\ud59f' : ['_h _j a _l _h', '햟'],
    u'\ud5a0' : ['_h _j a m', '햠'],
    u'\ud5a1' : ['_h _j a p', '햡'],
    u'\ud5a2' : ['_h _j a p sh', '햢'],
    u'\ud5a3' : ['_h _j a sh', '햣'],
    u'\ud5a4' : ['_h _j a s', '햤'],
    u'\ud5a5' : ['_h _j a N', '향'],
    u'\ud5a6' : ['_h _j a tS', '햦'],
    u'\ud5a7' : ['_h _j a tSh', '햧'],
    u'\ud5a8' : ['_h _j a k_h', '햨'],
    u'\ud5a9' : ['_h _j a t_h', '햩'],
    u'\ud5aa' : ['_h _j a p_h', '햪'],
    u'\ud5ab' : ['_h _j a _h', '햫'],
    u'\ud5ac' : ['_h _j {', '햬'],
    u'\ud5ad' : ['_h _j { k', '햭'],
    u'\ud5ae' : ['_h _j { k_>', '햮'],
    u'\ud5af' : ['_h _j { k sh', '햯'],
    u'\ud5b0' : ['_h _j { _n', '햰'],
    u'\ud5b1' : ['_h _j { _n tS', '햱'],
    u'\ud5b2' : ['_h _j { _n _h', '햲'],
    u'\ud5b3' : ['_h _j { t', '햳'],
    u'\ud5b4' : ['_h _j { _l', '햴'],
    u'\ud5b5' : ['_h _j { _l k', '햵'],
    u'\ud5b6' : ['_h _j { _l m', '햶'],
    u'\ud5b7' : ['_h _j { _l p', '햷'],
    u'\ud5b8' : ['_h _j { _l sh', '햸'],
    u'\ud5b9' : ['_h _j { _l t_h', '햹'],
    u'\ud5ba' : ['_h _j { _l p_h', '햺'],
    u'\ud5bb' : ['_h _j { _l _h', '햻'],
    u'\ud5bc' : ['_h _j { m', '햼'],
    u'\ud5bd' : ['_h _j { p', '햽'],
    u'\ud5be' : ['_h _j { p sh', '햾'],
    u'\ud5bf' : ['_h _j { sh', '햿'],
    u'\ud5c0' : ['_h _j { s', '헀'],
    u'\ud5c1' : ['_h _j { N', '헁'],
    u'\ud5c2' : ['_h _j { tS', '헂'],
    u'\ud5c3' : ['_h _j { tSh', '헃'],
    u'\ud5c4' : ['_h _j { k_h', '헄'],
    u'\ud5c5' : ['_h _j { t_h', '헅'],
    u'\ud5c6' : ['_h _j { p_h', '헆'],
    u'\ud5c7' : ['_h _j { _h', '헇'],
    u'\ud5c8' : ['_h _r', '허'],
    u'\ud5c9' : ['_h _r k', '헉'],
    u'\ud5ca' : ['_h _r k_>', '헊'],
    u'\ud5cb' : ['_h _r k sh', '헋'],
    u'\ud5cc' : ['_h _r _n', '헌'],
    u'\ud5cd' : ['_h _r _n tS', '헍'],
    u'\ud5ce' : ['_h _r _n _h', '헎'],
    u'\ud5cf' : ['_h _r t', '헏'],
    u'\ud5d0' : ['_h _r _l', '헐'],
    u'\ud5d1' : ['_h _r _l k', '헑'],
    u'\ud5d2' : ['_h _r _l m', '헒'],
    u'\ud5d3' : ['_h _r _l p', '헓'],
    u'\ud5d4' : ['_h _r _l sh', '헔'],
    u'\ud5d5' : ['_h _r _l t_h', '헕'],
    u'\ud5d6' : ['_h _r _l p_h', '헖'],
    u'\ud5d7' : ['_h _r _l _h', '헗'],
    u'\ud5d8' : ['_h _r m', '험'],
    u'\ud5d9' : ['_h _r p', '헙'],
    u'\ud5da' : ['_h _r p sh', '헚'],
    u'\ud5db' : ['_h _r sh', '헛'],
    u'\ud5dc' : ['_h _r s', '헜'],
    u'\ud5dd' : ['_h _r N', '헝'],
    u'\ud5de' : ['_h _r tS', '헞'],
    u'\ud5df' : ['_h _r tSh', '헟'],
    u'\ud5e0' : ['_h _r k_h', '헠'],
    u'\ud5e1' : ['_h _r t_h', '헡'],
    u'\ud5e2' : ['_h _r p_h', '헢'],
    u'\ud5e3' : ['_h _r _h', '헣'],
    u'\ud5e4' : ['_h e', '헤'],
    u'\ud5e5' : ['_h e k', '헥'],
    u'\ud5e6' : ['_h e k_>', '헦'],
    u'\ud5e7' : ['_h e k sh', '헧'],
    u'\ud5e8' : ['_h e _n', '헨'],
    u'\ud5e9' : ['_h e _n tS', '헩'],
    u'\ud5ea' : ['_h e _n _h', '헪'],
    u'\ud5eb' : ['_h e t', '헫'],
    u'\ud5ec' : ['_h e _l', '헬'],
    u'\ud5ed' : ['_h e _l k', '헭'],
    u'\ud5ee' : ['_h e _l m', '헮'],
    u'\ud5ef' : ['_h e _l p', '헯'],
    u'\ud5f0' : ['_h e _l sh', '헰'],
    u'\ud5f1' : ['_h e _l t_h', '헱'],
    u'\ud5f2' : ['_h e _l p_h', '헲'],
    u'\ud5f3' : ['_h e _l _h', '헳'],
    u'\ud5f4' : ['_h e m', '헴'],
    u'\ud5f5' : ['_h e p', '헵'],
    u'\ud5f6' : ['_h e p sh', '헶'],
    u'\ud5f7' : ['_h e sh', '헷'],
    u'\ud5f8' : ['_h e s', '헸'],
    u'\ud5f9' : ['_h e N', '헹'],
    u'\ud5fa' : ['_h e tS', '헺'],
    u'\ud5fb' : ['_h e tSh', '헻'],
    u'\ud5fc' : ['_h e k_h', '헼'],
    u'\ud5fd' : ['_h e t_h', '헽'],
    u'\ud5fe' : ['_h e p_h', '헾'],
    u'\ud5ff' : ['_h e _h', '헿'],
    u'\ud600' : ['_h _j _r', '혀'],
    u'\ud601' : ['_h _j _r k', '혁'],
    u'\ud602' : ['_h _j _r k_>', '혂'],
    u'\ud603' : ['_h _j _r k sh', '혃'],
    u'\ud604' : ['_h _j _r _n', '현'],
    u'\ud605' : ['_h _j _r _n tS', '혅'],
    u'\ud606' : ['_h _j _r _n _h', '혆'],
    u'\ud607' : ['_h _j _r t', '혇'],
    u'\ud608' : ['_h _j _r _l', '혈'],
    u'\ud609' : ['_h _j _r _l k', '혉'],
    u'\ud60a' : ['_h _j _r _l m', '혊'],
    u'\ud60b' : ['_h _j _r _l p', '혋'],
    u'\ud60c' : ['_h _j _r _l sh', '혌'],
    u'\ud60d' : ['_h _j _r _l t_h', '혍'],
    u'\ud60e' : ['_h _j _r _l p_h', '혎'],
    u'\ud60f' : ['_h _j _r _l _h', '혏'],
    u'\ud610' : ['_h _j _r m', '혐'],
    u'\ud611' : ['_h _j _r p', '협'],
    u'\ud612' : ['_h _j _r p sh', '혒'],
    u'\ud613' : ['_h _j _r sh', '혓'],
    u'\ud614' : ['_h _j _r s', '혔'],
    u'\ud615' : ['_h _j _r N', '형'],
    u'\ud616' : ['_h _j _r tS', '혖'],
    u'\ud617' : ['_h _j _r tSh', '혗'],
    u'\ud618' : ['_h _j _r k_h', '혘'],
    u'\ud619' : ['_h _j _r t_h', '혙'],
    u'\ud61a' : ['_h _j _r p_h', '혚'],
    u'\ud61b' : ['_h _j _r _h', '혛'],
    u'\ud61c' : ['_h _j e', '혜'],
    u'\ud61d' : ['_h _j e k', '혝'],
    u'\ud61e' : ['_h _j e k_>', '혞'],
    u'\ud61f' : ['_h _j e k sh', '혟'],
    u'\ud620' : ['_h _j e _n', '혠'],
    u'\ud621' : ['_h _j e _n tS', '혡'],
    u'\ud622' : ['_h _j e _n _h', '혢'],
    u'\ud623' : ['_h _j e t', '혣'],
    u'\ud624' : ['_h _j e _l', '혤'],
    u'\ud625' : ['_h _j e _l k', '혥'],
    u'\ud626' : ['_h _j e _l m', '혦'],
    u'\ud627' : ['_h _j e _l p', '혧'],
    u'\ud628' : ['_h _j e _l sh', '혨'],
    u'\ud629' : ['_h _j e _l t_h', '혩'],
    u'\ud62a' : ['_h _j e _l p_h', '혪'],
    u'\ud62b' : ['_h _j e _l _h', '혫'],
    u'\ud62c' : ['_h _j e m', '혬'],
    u'\ud62d' : ['_h _j e p', '혭'],
    u'\ud62e' : ['_h _j e p sh', '혮'],
    u'\ud62f' : ['_h _j e sh', '혯'],
    u'\ud630' : ['_h _j e s', '혰'],
    u'\ud631' : ['_h _j e N', '혱'],
    u'\ud632' : ['_h _j e tS', '혲'],
    u'\ud633' : ['_h _j e tSh', '혳'],
    u'\ud634' : ['_h _j e k_h', '혴'],
    u'\ud635' : ['_h _j e t_h', '혵'],
    u'\ud636' : ['_h _j e p_h', '혶'],
    u'\ud637' : ['_h _j e _h', '혷'],
    u'\ud638' : ['_h o', '호'],
    u'\ud639' : ['_h o k', '혹'],
    u'\ud63a' : ['_h o k_>', '혺'],
    u'\ud63b' : ['_h o k sh', '혻'],
    u'\ud63c' : ['_h o _n', '혼'],
    u'\ud63d' : ['_h o _n tS', '혽'],
    u'\ud63e' : ['_h o _n _h', '혾'],
    u'\ud63f' : ['_h o t', '혿'],
    u'\ud640' : ['_h o _l', '홀'],
    u'\ud641' : ['_h o _l k', '홁'],
    u'\ud642' : ['_h o _l m', '홂'],
    u'\ud643' : ['_h o _l p', '홃'],
    u'\ud644' : ['_h o _l sh', '홄'],
    u'\ud645' : ['_h o _l t_h', '홅'],
    u'\ud646' : ['_h o _l p_h', '홆'],
    u'\ud647' : ['_h o _l _h', '홇'],
    u'\ud648' : ['_h o m', '홈'],
    u'\ud649' : ['_h o p', '홉'],
    u'\ud64a' : ['_h o p sh', '홊'],
    u'\ud64b' : ['_h o sh', '홋'],
    u'\ud64c' : ['_h o s', '홌'],
    u'\ud64d' : ['_h o N', '홍'],
    u'\ud64e' : ['_h o tS', '홎'],
    u'\ud64f' : ['_h o tSh', '홏'],
    u'\ud650' : ['_h o k_h', '홐'],
    u'\ud651' : ['_h o t_h', '홑'],
    u'\ud652' : ['_h o p_h', '홒'],
    u'\ud653' : ['_h o _h', '홓'],
    u'\ud654' : ['_h _w a', '화'],
    u'\ud655' : ['_h _w a k', '확'],
    u'\ud656' : ['_h _w a k_>', '홖'],
    u'\ud657' : ['_h _w a k sh', '홗'],
    u'\ud658' : ['_h _w a _n', '환'],
    u'\ud659' : ['_h _w a _n tS', '홙'],
    u'\ud65a' : ['_h _w a _n _h', '홚'],
    u'\ud65b' : ['_h _w a t', '홛'],
    u'\ud65c' : ['_h _w a _l', '활'],
    u'\ud65d' : ['_h _w a _l k', '홝'],
    u'\ud65e' : ['_h _w a _l m', '홞'],
    u'\ud65f' : ['_h _w a _l p', '홟'],
    u'\ud660' : ['_h _w a _l sh', '홠'],
    u'\ud661' : ['_h _w a _l t_h', '홡'],
    u'\ud662' : ['_h _w a _l p_h', '홢'],
    u'\ud663' : ['_h _w a _l _h', '홣'],
    u'\ud664' : ['_h _w a m', '홤'],
    u'\ud665' : ['_h _w a p', '홥'],
    u'\ud666' : ['_h _w a p sh', '홦'],
    u'\ud667' : ['_h _w a sh', '홧'],
    u'\ud668' : ['_h _w a s', '홨'],
    u'\ud669' : ['_h _w a N', '황'],
    u'\ud66a' : ['_h _w a tS', '홪'],
    u'\ud66b' : ['_h _w a tSh', '홫'],
    u'\ud66c' : ['_h _w a k_h', '홬'],
    u'\ud66d' : ['_h _w a t_h', '홭'],
    u'\ud66e' : ['_h _w a p_h', '홮'],
    u'\ud66f' : ['_h _w a _h', '홯'],
    u'\ud670' : ['_h _w {', '홰'],
    u'\ud671' : ['_h _w { k', '홱'],
    u'\ud672' : ['_h _w { k_>', '홲'],
    u'\ud673' : ['_h _w { k sh', '홳'],
    u'\ud674' : ['_h _w { _n', '홴'],
    u'\ud675' : ['_h _w { _n tS', '홵'],
    u'\ud676' : ['_h _w { _n _h', '홶'],
    u'\ud677' : ['_h _w { t', '홷'],
    u'\ud678' : ['_h _w { _l', '홸'],
    u'\ud679' : ['_h _w { _l k', '홹'],
    u'\ud67a' : ['_h _w { _l m', '홺'],
    u'\ud67b' : ['_h _w { _l p', '홻'],
    u'\ud67c' : ['_h _w { _l sh', '홼'],
    u'\ud67d' : ['_h _w { _l t_h', '홽'],
    u'\ud67e' : ['_h _w { _l p_h', '홾'],
    u'\ud67f' : ['_h _w { _l _h', '홿'],
    u'\ud680' : ['_h _w { m', '횀'],
    u'\ud681' : ['_h _w { p', '횁'],
    u'\ud682' : ['_h _w { p sh', '횂'],
    u'\ud683' : ['_h _w { sh', '횃'],
    u'\ud684' : ['_h _w { s', '횄'],
    u'\ud685' : ['_h _w { N', '횅'],
    u'\ud686' : ['_h _w { tS', '횆'],
    u'\ud687' : ['_h _w { tSh', '횇'],
    u'\ud688' : ['_h _w { k_h', '횈'],
    u'\ud689' : ['_h _w { t_h', '횉'],
    u'\ud68a' : ['_h _w { p_h', '횊'],
    u'\ud68b' : ['_h _w { _h', '횋'],
    u'\ud68c' : ['_h _w e', '회'],
    u'\ud68d' : ['_h _w e k', '획'],
    u'\ud68e' : ['_h _w e k_>', '횎'],
    u'\ud68f' : ['_h _w e k sh', '횏'],
    u'\ud690' : ['_h _w e _n', '횐'],
    u'\ud691' : ['_h _w e _n tS', '횑'],
    u'\ud692' : ['_h _w e _n _h', '횒'],
    u'\ud693' : ['_h _w e t', '횓'],
    u'\ud694' : ['_h _w e _l', '횔'],
    u'\ud695' : ['_h _w e _l k', '횕'],
    u'\ud696' : ['_h _w e _l m', '횖'],
    u'\ud697' : ['_h _w e _l p', '횗'],
    u'\ud698' : ['_h _w e _l sh', '횘'],
    u'\ud699' : ['_h _w e _l t_h', '횙'],
    u'\ud69a' : ['_h _w e _l p_h', '횚'],
    u'\ud69b' : ['_h _w e _l _h', '횛'],
    u'\ud69c' : ['_h _w e m', '횜'],
    u'\ud69d' : ['_h _w e p', '횝'],
    u'\ud69e' : ['_h _w e p sh', '횞'],
    u'\ud69f' : ['_h _w e sh', '횟'],
    u'\ud6a0' : ['_h _w e s', '횠'],
    u'\ud6a1' : ['_h _w e N', '횡'],
    u'\ud6a2' : ['_h _w e tS', '횢'],
    u'\ud6a3' : ['_h _w e tSh', '횣'],
    u'\ud6a4' : ['_h _w e k_h', '횤'],
    u'\ud6a5' : ['_h _w e t_h', '횥'],
    u'\ud6a6' : ['_h _w e p_h', '횦'],
    u'\ud6a7' : ['_h _w e _h', '횧'],
    u'\ud6a8' : ['_h _j o', '효'],
    u'\ud6a9' : ['_h _j o k', '횩'],
    u'\ud6aa' : ['_h _j o k_>', '횪'],
    u'\ud6ab' : ['_h _j o k sh', '횫'],
    u'\ud6ac' : ['_h _j o _n', '횬'],
    u'\ud6ad' : ['_h _j o _n tS', '횭'],
    u'\ud6ae' : ['_h _j o _n _h', '횮'],
    u'\ud6af' : ['_h _j o t', '횯'],
    u'\ud6b0' : ['_h _j o _l', '횰'],
    u'\ud6b1' : ['_h _j o _l k', '횱'],
    u'\ud6b2' : ['_h _j o _l m', '횲'],
    u'\ud6b3' : ['_h _j o _l p', '횳'],
    u'\ud6b4' : ['_h _j o _l sh', '횴'],
    u'\ud6b5' : ['_h _j o _l t_h', '횵'],
    u'\ud6b6' : ['_h _j o _l p_h', '횶'],
    u'\ud6b7' : ['_h _j o _l _h', '횷'],
    u'\ud6b8' : ['_h _j o m', '횸'],
    u'\ud6b9' : ['_h _j o p', '횹'],
    u'\ud6ba' : ['_h _j o p sh', '횺'],
    u'\ud6bb' : ['_h _j o sh', '횻'],
    u'\ud6bc' : ['_h _j o s', '횼'],
    u'\ud6bd' : ['_h _j o N', '횽'],
    u'\ud6be' : ['_h _j o tS', '횾'],
    u'\ud6bf' : ['_h _j o tSh', '횿'],
    u'\ud6c0' : ['_h _j o k_h', '훀'],
    u'\ud6c1' : ['_h _j o t_h', '훁'],
    u'\ud6c2' : ['_h _j o p_h', '훂'],
    u'\ud6c3' : ['_h _j o _h', '훃'],
    u'\ud6c4' : ['_h u', '후'],
    u'\ud6c5' : ['_h u k', '훅'],
    u'\ud6c6' : ['_h u k_>', '훆'],
    u'\ud6c7' : ['_h u k sh', '훇'],
    u'\ud6c8' : ['_h u _n', '훈'],
    u'\ud6c9' : ['_h u _n tS', '훉'],
    u'\ud6ca' : ['_h u _n _h', '훊'],
    u'\ud6cb' : ['_h u t', '훋'],
    u'\ud6cc' : ['_h u _l', '훌'],
    u'\ud6cd' : ['_h u _l k', '훍'],
    u'\ud6ce' : ['_h u _l m', '훎'],
    u'\ud6cf' : ['_h u _l p', '훏'],
    u'\ud6d0' : ['_h u _l sh', '훐'],
    u'\ud6d1' : ['_h u _l t_h', '훑'],
    u'\ud6d2' : ['_h u _l p_h', '훒'],
    u'\ud6d3' : ['_h u _l _h', '훓'],
    u'\ud6d4' : ['_h u m', '훔'],
    u'\ud6d5' : ['_h u p', '훕'],
    u'\ud6d6' : ['_h u p sh', '훖'],
    u'\ud6d7' : ['_h u sh', '훗'],
    u'\ud6d8' : ['_h u s', '훘'],
    u'\ud6d9' : ['_h u N', '훙'],
    u'\ud6da' : ['_h u tS', '훚'],
    u'\ud6db' : ['_h u tSh', '훛'],
    u'\ud6dc' : ['_h u k_h', '훜'],
    u'\ud6dd' : ['_h u t_h', '훝'],
    u'\ud6de' : ['_h u p_h', '훞'],
    u'\ud6df' : ['_h u _h', '훟'],
    u'\ud6e0' : ['_h _w _r', '훠'],
    u'\ud6e1' : ['_h _w _r k', '훡'],
    u'\ud6e2' : ['_h _w _r k_>', '훢'],
    u'\ud6e3' : ['_h _w _r k sh', '훣'],
    u'\ud6e4' : ['_h _w _r _n', '훤'],
    u'\ud6e5' : ['_h _w _r _n tS', '훥'],
    u'\ud6e6' : ['_h _w _r _n _h', '훦'],
    u'\ud6e7' : ['_h _w _r t', '훧'],
    u'\ud6e8' : ['_h _w _r _l', '훨'],
    u'\ud6e9' : ['_h _w _r _l k', '훩'],
    u'\ud6ea' : ['_h _w _r _l m', '훪'],
    u'\ud6eb' : ['_h _w _r _l p', '훫'],
    u'\ud6ec' : ['_h _w _r _l sh', '훬'],
    u'\ud6ed' : ['_h _w _r _l t_h', '훭'],
    u'\ud6ee' : ['_h _w _r _l p_h', '훮'],
    u'\ud6ef' : ['_h _w _r _l _h', '훯'],
    u'\ud6f0' : ['_h _w _r m', '훰'],
    u'\ud6f1' : ['_h _w _r p', '훱'],
    u'\ud6f2' : ['_h _w _r p sh', '훲'],
    u'\ud6f3' : ['_h _w _r sh', '훳'],
    u'\ud6f4' : ['_h _w _r s', '훴'],
    u'\ud6f5' : ['_h _w _r N', '훵'],
    u'\ud6f6' : ['_h _w _r tS', '훶'],
    u'\ud6f7' : ['_h _w _r tSh', '훷'],
    u'\ud6f8' : ['_h _w _r k_h', '훸'],
    u'\ud6f9' : ['_h _w _r t_h', '훹'],
    u'\ud6fa' : ['_h _w _r p_h', '훺'],
    u'\ud6fb' : ['_h _w _r _h', '훻'],
    u'\ud6fc' : ['_h _w E', '훼'],
    u'\ud6fd' : ['_h _w E k', '훽'],
    u'\ud6fe' : ['_h _w E k_>', '훾'],
    u'\ud6ff' : ['_h _w E k sh', '훿'],
    u'\ud700' : ['_h _w E _n', '휀'],
    u'\ud701' : ['_h _w E _n tS', '휁'],
    u'\ud702' : ['_h _w E _n _h', '휂'],
    u'\ud703' : ['_h _w E t', '휃'],
    u'\ud704' : ['_h _w E _l', '휄'],
    u'\ud705' : ['_h _w E _l k', '휅'],
    u'\ud706' : ['_h _w E _l m', '휆'],
    u'\ud707' : ['_h _w E _l p', '휇'],
    u'\ud708' : ['_h _w E _l sh', '휈'],
    u'\ud709' : ['_h _w E _l t_h', '휉'],
    u'\ud70a' : ['_h _w E _l p_h', '휊'],
    u'\ud70b' : ['_h _w E _l _h', '휋'],
    u'\ud70c' : ['_h _w E m', '휌'],
    u'\ud70d' : ['_h _w E p', '휍'],
    u'\ud70e' : ['_h _w E p sh', '휎'],
    u'\ud70f' : ['_h _w E sh', '휏'],
    u'\ud710' : ['_h _w E s', '휐'],
    u'\ud711' : ['_h _w E N', '휑'],
    u'\ud712' : ['_h _w E tS', '휒'],
    u'\ud713' : ['_h _w E tSh', '휓'],
    u'\ud714' : ['_h _w E k_h', '휔'],
    u'\ud715' : ['_h _w E t_h', '휕'],
    u'\ud716' : ['_h _w E p_h', '휖'],
    u'\ud717' : ['_h _w E _h', '휗'],
    u'\ud718' : ['_h 2', '휘'],
    u'\ud719' : ['_h 2 k', '휙'],
    u'\ud71a' : ['_h 2 k_>', '휚'],
    u'\ud71b' : ['_h 2 k sh', '휛'],
    u'\ud71c' : ['_h 2 _n', '휜'],
    u'\ud71d' : ['_h 2 _n tS', '휝'],
    u'\ud71e' : ['_h 2 _n _h', '휞'],
    u'\ud71f' : ['_h 2 t', '휟'],
    u'\ud720' : ['_h 2 _l', '휠'],
    u'\ud721' : ['_h 2 _l k', '휡'],
    u'\ud722' : ['_h 2 _l m', '휢'],
    u'\ud723' : ['_h 2 _l p', '휣'],
    u'\ud724' : ['_h 2 _l sh', '휤'],
    u'\ud725' : ['_h 2 _l t_h', '휥'],
    u'\ud726' : ['_h 2 _l p_h', '휦'],
    u'\ud727' : ['_h 2 _l _h', '휧'],
    u'\ud728' : ['_h 2 m', '휨'],
    u'\ud729' : ['_h 2 p', '휩'],
    u'\ud72a' : ['_h 2 p sh', '휪'],
    u'\ud72b' : ['_h 2 sh', '휫'],
    u'\ud72c' : ['_h 2 s', '휬'],
    u'\ud72d' : ['_h 2 N', '휭'],
    u'\ud72e' : ['_h 2 tS', '휮'],
    u'\ud72f' : ['_h 2 tSh', '휯'],
    u'\ud730' : ['_h 2 k_h', '휰'],
    u'\ud731' : ['_h 2 t_h', '휱'],
    u'\ud732' : ['_h 2 p_h', '휲'],
    u'\ud733' : ['_h 2 _h', '휳'],
    u'\ud734' : ['_h _j u', '휴'],
    u'\ud735' : ['_h _j u k', '휵'],
    u'\ud736' : ['_h _j u k_>', '휶'],
    u'\ud737' : ['_h _j u k sh', '휷'],
    u'\ud738' : ['_h _j u _n', '휸'],
    u'\ud739' : ['_h _j u _n tS', '휹'],
    u'\ud73a' : ['_h _j u _n _h', '휺'],
    u'\ud73b' : ['_h _j u t', '휻'],
    u'\ud73c' : ['_h _j u _l', '휼'],
    u'\ud73d' : ['_h _j u _l k', '휽'],
    u'\ud73e' : ['_h _j u _l m', '휾'],
    u'\ud73f' : ['_h _j u _l p', '휿'],
    u'\ud740' : ['_h _j u _l sh', '흀'],
    u'\ud741' : ['_h _j u _l t_h', '흁'],
    u'\ud742' : ['_h _j u _l p_h', '흂'],
    u'\ud743' : ['_h _j u _l _h', '흃'],
    u'\ud744' : ['_h _j u m', '흄'],
    u'\ud745' : ['_h _j u p', '흅'],
    u'\ud746' : ['_h _j u p sh', '흆'],
    u'\ud747' : ['_h _j u sh', '흇'],
    u'\ud748' : ['_h _j u s', '흈'],
    u'\ud749' : ['_h _j u N', '흉'],
    u'\ud74a' : ['_h _j u tS', '흊'],
    u'\ud74b' : ['_h _j u tSh', '흋'],
    u'\ud74c' : ['_h _j u k_h', '흌'],
    u'\ud74d' : ['_h _j u t_h', '흍'],
    u'\ud74e' : ['_h _j u p_h', '흎'],
    u'\ud74f' : ['_h _j u _h', '흏'],
    u'\ud750' : ['_h M', '흐'],
    u'\ud751' : ['_h M k', '흑'],
    u'\ud752' : ['_h M k_>', '흒'],
    u'\ud753' : ['_h M k sh', '흓'],
    u'\ud754' : ['_h M _n', '흔'],
    u'\ud755' : ['_h M _n tS', '흕'],
    u'\ud756' : ['_h M _n _h', '흖'],
    u'\ud757' : ['_h M t', '흗'],
    u'\ud758' : ['_h M _l', '흘'],
    u'\ud759' : ['_h M _l k', '흙'],
    u'\ud75a' : ['_h M _l m', '흚'],
    u'\ud75b' : ['_h M _l p', '흛'],
    u'\ud75c' : ['_h M _l sh', '흜'],
    u'\ud75d' : ['_h M _l t_h', '흝'],
    u'\ud75e' : ['_h M _l p_h', '흞'],
    u'\ud75f' : ['_h M _l _h', '흟'],
    u'\ud760' : ['_h M m', '흠'],
    u'\ud761' : ['_h M p', '흡'],
    u'\ud762' : ['_h M p sh', '흢'],
    u'\ud763' : ['_h M sh', '흣'],
    u'\ud764' : ['_h M s', '흤'],
    u'\ud765' : ['_h M N', '흥'],
    u'\ud766' : ['_h M tS', '흦'],
    u'\ud767' : ['_h M tSh', '흧'],
    u'\ud768' : ['_h M k_h', '흨'],
    u'\ud769' : ['_h M t_h', '흩'],
    u'\ud76a' : ['_h M p_h', '흪'],
    u'\ud76b' : ['_h M _h', '흫'],
    u'\ud76c' : ['_h M _j', '희'],
    u'\ud76d' : ['_h M _j k', '흭'],
    u'\ud76e' : ['_h M _j k_>', '흮'],
    u'\ud76f' : ['_h M _j k sh', '흯'],
    u'\ud770' : ['_h M _j _n', '흰'],
    u'\ud771' : ['_h M _j _n tS', '흱'],
    u'\ud772' : ['_h M _j _n _h', '흲'],
    u'\ud773' : ['_h M _j t', '흳'],
    u'\ud774' : ['_h M _j _l', '흴'],
    u'\ud775' : ['_h M _j _l k', '흵'],
    u'\ud776' : ['_h M _j _l m', '흶'],
    u'\ud777' : ['_h M _j _l p', '흷'],
    u'\ud778' : ['_h M _j _l sh', '흸'],
    u'\ud779' : ['_h M _j _l t_h', '흹'],
    u'\ud77a' : ['_h M _j _l p_h', '흺'],
    u'\ud77b' : ['_h M _j _l _h', '흻'],
    u'\ud77c' : ['_h M _j m', '흼'],
    u'\ud77d' : ['_h M _j p', '흽'],
    u'\ud77e' : ['_h M _j p sh', '흾'],
    u'\ud77f' : ['_h M _j sh', '흿'],
    u'\ud780' : ['_h M _j s', '힀'],
    u'\ud781' : ['_h M _j N', '힁'],
    u'\ud782' : ['_h M _j tS', '힂'],
    u'\ud783' : ['_h M _j tSh', '힃'],
    u'\ud784' : ['_h M _j k_h', '힄'],
    u'\ud785' : ['_h M _j t_h', '힅'],
    u'\ud786' : ['_h M _j p_h', '힆'],
    u'\ud787' : ['_h M _j _h', '힇'],
    u'\ud788' : ['_h i', '히'],
    u'\ud789' : ['_h i k', '힉'],
    u'\ud78a' : ['_h i k_>', '힊'],
    u'\ud78b' : ['_h i k sh', '힋'],
    u'\ud78c' : ['_h i _n', '힌'],
    u'\ud78d' : ['_h i _n tS', '힍'],
    u'\ud78e' : ['_h i _n _h', '힎'],
    u'\ud78f' : ['_h i t', '힏'],
    u'\ud790' : ['_h i _l', '힐'],
    u'\ud791' : ['_h i _l k', '힑'],
    u'\ud792' : ['_h i _l m', '힒'],
    u'\ud793' : ['_h i _l p', '힓'],
    u'\ud794' : ['_h i _l sh', '힔'],
    u'\ud795' : ['_h i _l t_h', '힕'],
    u'\ud796' : ['_h i _l p_h', '힖'],
    u'\ud797' : ['_h i _l _h', '힗'],
    u'\ud798' : ['_h i m', '힘'],
    u'\ud799' : ['_h i p', '힙'],
    u'\ud79a' : ['_h i p sh', '힚'],
    u'\ud79b' : ['_h i sh', '힛'],
    u'\ud79c' : ['_h i s', '힜'],
    u'\ud79d' : ['_h i N', '힝'],
    u'\ud79e' : ['_h i tS', '힞'],
    u'\ud79f' : ['_h i tSh', '힟'],
    u'\ud7a0' : ['_h i k_h', '힠'],
    u'\ud7a1' : ['_h i t_h', '힡'],
    u'\ud7a2' : ['_h i p_h', '힢'],
    u'\ud7a3' : ['_h i _h', '힣'],
    u'\ufb00' : ['f f', 'ﬀ'],
    u'\ufb01' : ['f i', 'ﬁ'],
    u'\ufb02' : ['f _l', 'ﬂ'],
    u'\ufb03' : ['f f i', 'ﬃ'],
    u'\ufb04' : ['f f _l', 'ﬄ'],
    u'\ufb05' : ['s t', 'ﬅ'],
    u'\ufb06' : ['s t', 'ﬆ'],
    u'\ufb13' : ['m _n', 'ﬓ'],
    u'\ufb14' : ['m _h', 'ﬔ'],
    u'\ufb15' : ['m dZ', 'ﬕ'],
    u'\ufb16' : ['m _n', 'ﬖ'],
    u'\ufb17' : ['m _x', 'ﬗ'],
    u'\ufb1d' : ['_j', 'יִ'],
    u'\ufb1e' : ['(HEBREW POINT JUDEO-SPANISH VARIKA)', 'ﬞ'],
    u'\ufb1f' : ['>j', 'ײַ'],
    u'\ufb20' : ['?, 2', 'ﬠ'],
    u'\ufb21' : ['?, 2', 'ﬡ'],
    u'\ufb22' : ['d', 'ﬢ'],
    u'\ufb23' : ['_h', 'ﬣ'],
    u'\ufb24' : ['k, _x', 'ﬤ'],
    u'\ufb25' : ['_l', 'ﬥ'],
    u'\ufb26' : ['m', 'ﬦ'],
    u'\ufb27' : ['k', 'ﬧ'],
    u'\ufb28' : ['t', 'ﬨ'],
    u'\ufb29' : ['(ALTERNATIVE PLUS SIGN)', '﬩'],
    u'\ufb2a' : ['s, S', 'שׁ'],
    u'\ufb2b' : ['s, S', 'שׂ'],
    u'\ufb2c' : ['s, S', 'שּׁ'],
    u'\ufb2d' : ['s, S', 'שּׂ'],
    u'\ufb2e' : ['?, 2', 'אַ'],
    u'\ufb2f' : ['?, 2', 'אָ'],
    u'\ufb30' : ['?, 2', 'אּ'],
    u'\ufb31' : ['b, _v', 'בּ'],
    u'\ufb32' : ['g', 'גּ'],
    u'\ufb33' : ['d', 'דּ'],
    u'\ufb34' : ['_h', 'הּ'],
    u'\ufb35' : ['_v', 'וּ'],
    u'\ufb36' : ['z', 'זּ'],
    u'\ufb38' : ['t', 'טּ'],
    u'\ufb39' : ['_j', 'יּ'],
    u'\ufb3a' : ['k, _x', 'ךּ'],
    u'\ufb3b' : ['k, _x', 'כּ'],
    u'\ufb3c' : ['_l', 'לּ'],
    u'\ufb3e' : ['m', 'מּ'],
    u'\ufb40' : ['_n', 'נּ'],
    u'\ufb41' : ['s', 'סּ'],
    u'\ufb43' : ['p, f', 'ףּ'],
    u'\ufb44' : ['p, f', 'פּ'],
    u'\ufb46' : ['ts', 'צּ'],
    u'\ufb47' : ['k', 'קּ'],
    u'\ufb48' : ['k', 'רּ'],
    u'\ufb49' : ['s, S', 'שּ'],
    u'\ufb4a' : ['t', 'תּ'],
    u'\ufb4b' : ['_v', 'וֹ'],
    u'\ufb4c' : ['b, _v', 'בֿ'],
    u'\ufb4d' : ['k, _x', 'כֿ'],
    u'\ufb4e' : ['p, f', 'פֿ'],
    u'\ufb4f' : ['?, 2', 'ﭏ'],
    u'\ufb50' : ['(##)', 'ﭐ'],
    u'\ufb51' : ['(##)', 'ﭑ'],
    u'\ufb52' : ['(##)', 'ﭒ'],
    u'\ufb53' : ['(##)', 'ﭓ'],
    u'\ufb54' : ['(##)', 'ﭔ'],
    u'\ufb55' : ['(##)', 'ﭕ'],
    u'\ufb56' : ['(##)', 'ﭖ'],
    u'\ufb57' : ['(##)', 'ﭗ'],
    u'\ufb58' : ['(##)', 'ﭘ'],
    u'\ufb59' : ['(##)', 'ﭙ'],
    u'\ufb5a' : ['(##)', 'ﭚ'],
    u'\ufb5b' : ['(##)', 'ﭛ'],
    u'\ufb5c' : ['(##)', 'ﭜ'],
    u'\ufb5d' : ['(##)', 'ﭝ'],
    u'\ufb5e' : ['(##)', 'ﭞ'],
    u'\ufb5f' : ['(##)', 'ﭟ'],
    u'\ufb60' : ['(##)', 'ﭠ'],
    u'\ufb61' : ['(##)', 'ﭡ'],
    u'\ufb62' : ['(##)', 'ﭢ'],
    u'\ufb63' : ['(##)', 'ﭣ'],
    u'\ufb64' : ['(##)', 'ﭤ'],
    u'\ufb65' : ['(##)', 'ﭥ'],
    u'\ufb66' : ['(##)', 'ﭦ'],
    u'\ufb67' : ['(##)', 'ﭧ'],
    u'\ufb68' : ['(##)', 'ﭨ'],
    u'\ufb69' : ['(##)', 'ﭩ'],
    u'\ufb6a' : ['(##)', 'ﭪ'],
    u'\ufb6b' : ['(##)', 'ﭫ'],
    u'\ufb6c' : ['(##)', 'ﭬ'],
    u'\ufb6d' : ['(##)', 'ﭭ'],
    u'\ufb6e' : ['(##)', 'ﭮ'],
    u'\ufb6f' : ['(##)', 'ﭯ'],
    u'\ufb70' : ['(##)', 'ﭰ'],
    u'\ufb71' : ['(##)', 'ﭱ'],
    u'\ufb72' : ['(##)', 'ﭲ'],
    u'\ufb73' : ['(##)', 'ﭳ'],
    u'\ufb74' : ['(##)', 'ﭴ'],
    u'\ufb75' : ['(##)', 'ﭵ'],
    u'\ufb76' : ['(##)', 'ﭶ'],
    u'\ufb77' : ['(##)', 'ﭷ'],
    u'\ufb78' : ['(##)', 'ﭸ'],
    u'\ufb79' : ['(##)', 'ﭹ'],
    u'\ufb7a' : ['(##)', 'ﭺ'],
    u'\ufb7b' : ['(##)', 'ﭻ'],
    u'\ufb7c' : ['(##)', 'ﭼ'],
    u'\ufb7d' : ['(##)', 'ﭽ'],
    u'\ufb7e' : ['(##)', 'ﭾ'],
    u'\ufb7f' : ['(##)', 'ﭿ'],
    u'\ufb80' : ['(##)', 'ﮀ'],
    u'\ufb81' : ['(##)', 'ﮁ'],
    u'\ufb82' : ['(##)', 'ﮂ'],
    u'\ufb83' : ['(##)', 'ﮃ'],
    u'\ufb84' : ['(##)', 'ﮄ'],
    u'\ufb85' : ['(##)', 'ﮅ'],
    u'\ufb86' : ['(##)', 'ﮆ'],
    u'\ufb87' : ['(##)', 'ﮇ'],
    u'\ufb88' : ['(##)', 'ﮈ'],
    u'\ufb89' : ['(##)', 'ﮉ'],
    u'\ufb8a' : ['(##)', 'ﮊ'],
    u'\ufb8b' : ['(##)', 'ﮋ'],
    u'\ufb8c' : ['(##)', 'ﮌ'],
    u'\ufb8d' : ['(##)', 'ﮍ'],
    u'\ufb8e' : ['(##)', 'ﮎ'],
    u'\ufb8f' : ['(##)', 'ﮏ'],
    u'\ufb90' : ['(##)', 'ﮐ'],
    u'\ufb91' : ['(##)', 'ﮑ'],
    u'\ufb92' : ['(##)', 'ﮒ'],
    u'\ufb93' : ['(##)', 'ﮓ'],
    u'\ufb94' : ['(##)', 'ﮔ'],
    u'\ufb95' : ['(##)', 'ﮕ'],
    u'\ufb96' : ['(##)', 'ﮖ'],
    u'\ufb97' : ['(##)', 'ﮗ'],
    u'\ufb98' : ['(##)', 'ﮘ'],
    u'\ufb99' : ['(##)', 'ﮙ'],
    u'\ufb9a' : ['(##)', 'ﮚ'],
    u'\ufb9b' : ['(##)', 'ﮛ'],
    u'\ufb9c' : ['(##)', 'ﮜ'],
    u'\ufb9d' : ['(##)', 'ﮝ'],
    u'\ufb9e' : ['(##)', 'ﮞ'],
    u'\ufb9f' : ['(##)', 'ﮟ'],
    u'\ufba0' : ['(##)', 'ﮠ'],
    u'\ufba1' : ['(##)', 'ﮡ'],
    u'\ufba2' : ['(##)', 'ﮢ'],
    u'\ufba3' : ['(##)', 'ﮣ'],
    u'\ufba4' : ['(##)', 'ﮤ'],
    u'\ufba5' : ['(##)', 'ﮥ'],
    u'\ufba6' : ['(##)', 'ﮦ'],
    u'\ufba7' : ['(##)', 'ﮧ'],
    u'\ufba8' : ['(##)', 'ﮨ'],
    u'\ufba9' : ['(##)', 'ﮩ'],
    u'\ufbaa' : ['(##)', 'ﮪ'],
    u'\ufbab' : ['(##)', 'ﮫ'],
    u'\ufbac' : ['(##)', 'ﮬ'],
    u'\ufbad' : ['(##)', 'ﮭ'],
    u'\ufbae' : ['(##)', 'ﮮ'],
    u'\ufbaf' : ['(##)', 'ﮯ'],
    u'\ufbb0' : ['(##)', 'ﮰ'],
    u'\ufbb1' : ['(##)', 'ﮱ'],
    u'\ufbd3' : ['(##)', 'ﯓ'],
    u'\ufbd4' : ['(##)', 'ﯔ'],
    u'\ufbd5' : ['(##)', 'ﯕ'],
    u'\ufbd6' : ['(##)', 'ﯖ'],
    u'\ufbd7' : ['(##)', 'ﯗ'],
    u'\ufbd8' : ['(##)', 'ﯘ'],
    u'\ufbd9' : ['(##)', 'ﯙ'],
    u'\ufbda' : ['(##)', 'ﯚ'],
    u'\ufbdb' : ['(##)', 'ﯛ'],
    u'\ufbdc' : ['(##)', 'ﯜ'],
    u'\ufbdd' : ['(##)', 'ﯝ'],
    u'\ufbde' : ['(##)', 'ﯞ'],
    u'\ufbdf' : ['(##)', 'ﯟ'],
    u'\ufbe0' : ['(##)', 'ﯠ'],
    u'\ufbe1' : ['(##)', 'ﯡ'],
    u'\ufbe2' : ['(##)', 'ﯢ'],
    u'\ufbe3' : ['(##)', 'ﯣ'],
    u'\ufbe4' : ['(##)', 'ﯤ'],
    u'\ufbe5' : ['(##)', 'ﯥ'],
    u'\ufbe6' : ['(##)', 'ﯦ'],
    u'\ufbe7' : ['(##)', 'ﯧ'],
    u'\ufbe8' : ['(##)', 'ﯨ'],
    u'\ufbe9' : ['(##)', 'ﯩ'],
    u'\ufbea' : ['(##)', 'ﯪ'],
    u'\ufbeb' : ['(##)', 'ﯫ'],
    u'\ufbec' : ['(##)', 'ﯬ'],
    u'\ufbed' : ['(##)', 'ﯭ'],
    u'\ufbee' : ['(##)', 'ﯮ'],
    u'\ufbef' : ['(##)', 'ﯯ'],
    u'\ufbf0' : ['(##)', 'ﯰ'],
    u'\ufbf1' : ['(##)', 'ﯱ'],
    u'\ufbf2' : ['(##)', 'ﯲ'],
    u'\ufbf3' : ['(##)', 'ﯳ'],
    u'\ufbf4' : ['(##)', 'ﯴ'],
    u'\ufbf5' : ['(##)', 'ﯵ'],
    u'\ufbf6' : ['(##)', 'ﯶ'],
    u'\ufbf7' : ['(##)', 'ﯷ'],
    u'\ufbf8' : ['(##)', 'ﯸ'],
    u'\ufbf9' : ['(##)', 'ﯹ'],
    u'\ufbfa' : ['(##)', 'ﯺ'],
    u'\ufbfb' : ['(##)', 'ﯻ'],
    u'\ufbfc' : ['(##)', 'ﯼ'],
    u'\ufbfd' : ['(##)', 'ﯽ'],
    u'\ufbfe' : ['(##)', 'ﯾ'],
    u'\ufbff' : ['(##)', 'ﯿ'],
    u'\ufe50' : ['(##)', '﹐'],
    u'\ufe51' : ['(##)', '﹑'],
    u'\ufe52' : ['(##)', '﹒'],
    u'\ufe54' : ['(##)', '﹔'],
    u'\ufe55' : ['(##)', '﹕'],
    u'\ufe56' : ['(##)', '﹖'],
    u'\ufe57' : ['(##)', '﹗'],
    u'\ufe58' : ['(##)', '﹘'],
    u'\ufe59' : ['(##)', '﹙'],
    u'\ufe5a' : ['(##)', '﹚'],
    u'\ufe5b' : ['(##)', '﹛'],
    u'\ufe5c' : ['(##)', '﹜'],
    u'\ufe5d' : ['(##)', '﹝'],
    u'\ufe5e' : ['(##)', '﹞'],
    u'\ufe5f' : ['(##)', '﹟'],
    u'\ufe60' : ['(##)', '﹠'],
    u'\ufe61' : ['(##)', '﹡'],
    u'\ufe62' : ['(##)', '﹢'],
    u'\ufe63' : ['(##)', '﹣'],
    u'\ufe64' : ['(##)', '﹤'],
    u'\ufe65' : ['(##)', '﹥'],
    u'\ufe66' : ['(##)', '﹦'],
    u'\ufe68' : ['(##)', '﹨'],
    u'\ufe69' : ['(##)', '﹩'],
    u'\ufe6a' : ['(##)', '﹪'],
    u'\ufe6b' : ['(##)', '﹫'],
    u'\ufeff' : ['(BOM)', '﻿'],
}

########NEW FILE########
__FILENAME__ = unittest
# -*- coding: utf-8 -*-

## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

"""Generic unit-test functions.
"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
hollingk@cslu.ogi.edu (Kristy Hollingshead)
"""

import os
import sys

def CompareOutputFiles(gold_file, test_file):
  p = open(gold_file, 'r')
  glines = p.readlines()
  p.close()
  p = open(test_file, 'r')
  tlines = p.readlines()
  p.close()
  assert len(glines) == len(tlines), \
      'Test and gold differ, investigate with "diff %s %s"' % \
      (gold_file, test_file)
  for i in range(len(tlines)):
    assert glines[i] == tlines[i], \
        'Test and gold differ, investigate with "diff %s %s"' % \
        (gold_file, test_file)
  os.system('rm -f %s' % test_file)

def TestUnitOutputs(unitname, gold_file, test_file):
  CompareOutputFiles(gold_file, test_file)
  print '%s successful' % unitname

########NEW FILE########
__FILENAME__ = chinese
"""
Chinese (Mandarin) prons for characters 
"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
"""

DUMMY_PHONE_ = 'DUM'

MANDARIN_ = {}

def LoadMandarinWbTable(table):
  if MANDARIN_: return          ## already loaded
  p = open(table)
  lines = p.readlines()
  p.close()
  for line in lines:
    line = line.strip().split('\t')
    try: MANDARIN_[line[0]] = line[1].strip() ## just use first pron
    except IndexError: MANDARIN_[line[0]] = ''

def HanziToWorldBet(string):
  output = []
  some_success = False
  for c in unicode(string, 'utf8'):
    c = c.encode('utf-8')
    try:
      output.append(MANDARIN_[c])
      some_success = True
    except KeyError:
      output.append(DUMMY_PHONE_)
  return ' '.join(output), some_success

########NEW FILE########
__FILENAME__ = english
"""English prons for about 600K strings via Festival
"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
"""

ENGLISH_ = {}


def LoadEnglishWbTable(table):
  if ENGLISH_: return          ## already loaded
  p = open(table)
  lines = p.readlines()
  p.close()
  for line in lines:
    line = line.split()
    ENGLISH_[line[0]] = ' '.join(line[1:])


def EnglishToWorldBet(string):
  try: return ENGLISH_[string]
  except KeyError: return None



########NEW FILE########
__FILENAME__ = kunyomi
"""
Native Japanese pronunciations for characters
"""
__author__ = """
rws@uiuc.edu (Richard Sproat)
kkim36@uiuc.edu (Kyoung-young Kim)
"""

DUMMY_PHONE_ = 'DUM'

KUNYOMI_ = {}

def LoadKunyomiWbTable(table):
  if KUNYOMI_: return          ## already loaded
  p = open(table)
  lines = p.readlines()
  p.close()
  for line in lines:
    line = line.split()
    KUNYOMI_[line[0]] = ' '.join(line[1:])


def KanjiToWorldBet(string):
  output = []
  some_success = False
  for c in unicode(string, 'utf8'):
    c = c.encode('utf-8')
    try:
      output.append(KUNYOMI_[c])
      some_success = True
    except KeyError:
      output.append(DUMMY_PHONE_)
  return ' '.join(output), some_success


########NEW FILE########
__FILENAME__ = kunyomi_new
"""
Native Japanese pronunciations for characters based on Kunyomi
pronunciations from Wiktionary. These include guesses on application
of rendaku.
"""
__author__ = """
rws@uiuc.edu (Richard Sproat)
"""

KUNYOMI_ = {}
RENDAKU_ = {}


def RendakuWorldBet(worldbet):
  """If the romaji is marked with '*' the form may undergo rendaku
  """
  worldbet = worldbet.split()
  if worldbet[0] == 'h': return 'b ' + ' '.join(worldbet[1:])
  if worldbet[0] == 't': return 'd ' + ' '.join(worldbet[1:])
  if worldbet[0] == 'k': return 'g ' + ' '.join(worldbet[1:])
  if worldbet[0] == 's': return 'z ' + ' '.join(worldbet[1:])
  if worldbet[0] == 'ts': return 'z ' + ' '.join(worldbet[1:])
  if worldbet[0] == 'S': return 'j ' + ' '.join(worldbet[1:])
  return ' '.join(worldbet)


def LoadKunyomiWbTable(table):
  if KUNYOMI_: return          ## already loaded
  p = open(table)
  lines = p.readlines()
  p.close()
  for line in lines:
    line = line.strip().split('\t')
    romaji = line[2].strip()
    pron = line[3].strip()
    KUNYOMI_[line[0]] = pron
    if romaji[0] == '*': RENDAKU_[line[0]] = True


def KanjiToWorldBet(string):
  output = []
  some_success = False
  internal = False
  for c in unicode(string, 'utf8'):
    c = c.encode('utf-8')
    try:
      pron = KUNYOMI_[c]
      if internal and c in RENDAKU_:
        pron = RendakuWorldBet(pron)
      output.append(pron)
      some_success = True
    except KeyError:
      output.append(c)
    internal = True
  return ' '.join(output), some_success

########NEW FILE########
__FILENAME__ = latin
"""Simple table-lookup for pronunciation of extended latin strings.
"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
"""

## Extended Latin

LATIN_ = {}
DUMMY_PHONE_ = 'DUM'

def LoadLatinWbTable(table):
  p = open(table)
  lines = p.readlines()
  p.close()
  for line in lines:
    line = line.split()
    LATIN_[line[0]] = ' '.join(line[1:])

def LatinToWorldBet(string):
  output = []
  some_success = False
  for c in unicode(string, 'utf8'):
    c = c.encode('utf-8')
    try:
      output.append(LATIN_[c])
      some_success = True
    except KeyError:
      if c.isdigit():
        output.append(DUMMY_PHONE_)
      else:
        output.append(c)
  return ' '.join(output), some_success



########NEW FILE########
__FILENAME__ = script
"""Script classifier, based on
/usr/share/perl/5.8.8/unicore/Blocks.txt

and generally useful script utilities.
"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
"""

import unicodedata

UNKNOWN_SCRIPT_ = 'UNKNOWN_SCRIPT_'

SCRIPTS_WITH_CAPITALIZATION_ = [ 'Latin', 'Greek', 'Cyrillic',
                                 'Armenian' ]

def CharacterToScript(u):
  """ Return the script of a unicode codepoint, only considering those
  codepoints that correspond to characters of a script.
  """
  if u >= u'\u0000' and u <= u'\u007F': return 'Latin'
  elif u >= u'\u0080' and u <= u'\u00FF': return 'Latin'
  elif u >= u'\u0100' and u <= u'\u017F': return 'Latin'
  elif u >= u'\u0180' and u <= u'\u024F': return 'Latin'
  elif u >= u'\u0370' and u <= u'\u03FF': return 'Greek'
  elif u >= u'\u0400' and u <= u'\u04FF': return 'Cyrillic'
  elif u >= u'\u0500' and u <= u'\u052F': return 'Cyrillic'
  elif u >= u'\u0530' and u <= u'\u058F': return 'Armenian'
  elif u >= u'\u0590' and u <= u'\u05FF': return 'Hebrew'
  elif u >= u'\u0600' and u <= u'\u06FF': return 'Arabic'
  elif u >= u'\u0700' and u <= u'\u074F': return 'Syriac'
  elif u >= u'\u0750' and u <= u'\u077F': return 'Arabic'
  elif u >= u'\u0780' and u <= u'\u07BF': return 'Thaana'
  elif u >= u'\u0900' and u <= u'\u097F': return 'Devanagari'
  elif u >= u'\u0980' and u <= u'\u09FF': return 'Bengali'
  elif u >= u'\u0A00' and u <= u'\u0A7F': return 'Gurmukhi'
  elif u >= u'\u0A80' and u <= u'\u0AFF': return 'Gujarati'
  elif u >= u'\u0B00' and u <= u'\u0B7F': return 'Oriya'
  elif u >= u'\u0B80' and u <= u'\u0BFF': return 'Tamil'
  elif u >= u'\u0C00' and u <= u'\u0C7F': return 'Telugu'
  elif u >= u'\u0C80' and u <= u'\u0CFF': return 'Kannada'
  elif u >= u'\u0D00' and u <= u'\u0D7F': return 'Malayalam'
  elif u >= u'\u0D80' and u <= u'\u0DFF': return 'Sinhala'
  elif u >= u'\u0E00' and u <= u'\u0E7F': return 'Thai'
  elif u >= u'\u0E80' and u <= u'\u0EFF': return 'Lao'
  elif u >= u'\u0F00' and u <= u'\u0FFF': return 'Tibetan'
  elif u >= u'\u1000' and u <= u'\u109F': return 'Burmese'
  elif u >= u'\u10A0' and u <= u'\u10FF': return 'Georgian'
  elif u >= u'\u1100' and u <= u'\u11FF': return 'Hangul'
  elif u >= u'\u1200' and u <= u'\u137F': return 'Ethiopic'
  elif u >= u'\u1380' and u <= u'\u139F': return 'Ethiopic'
  elif u >= u'\u13A0' and u <= u'\u13FF': return 'Cherokee'
  elif u >= u'\u1400' and u <= u'\u167F': return 'UCS'
  elif u >= u'\u1680' and u <= u'\u169F': return 'Ogham'
  elif u >= u'\u16A0' and u <= u'\u16FF': return 'Runic'
  elif u >= u'\u1700' and u <= u'\u171F': return 'Tagalog'
  elif u >= u'\u1720' and u <= u'\u173F': return 'Hanunoo'
  elif u >= u'\u1740' and u <= u'\u175F': return 'Buhid'
  elif u >= u'\u1760' and u <= u'\u177F': return 'Tagbanwa'
  elif u >= u'\u1780' and u <= u'\u17FF': return 'Khmer'
  elif u >= u'\u1800' and u <= u'\u18AF': return 'Mongolian'
  elif u >= u'\u1900' and u <= u'\u194F': return 'Limbu'
  elif u >= u'\u1950' and u <= u'\u197F': return 'Tai Le'
  elif u >= u'\u1980' and u <= u'\u19DF': return 'New Tai Lue'
  elif u >= u'\u19E0' and u <= u'\u19FF': return 'Khmer'
  elif u >= u'\u1A00' and u <= u'\u1A1F': return 'Buginese'
  elif u >= u'\u1E00' and u <= u'\u1EFF': return 'Latin'
  elif u >= u'\u1F00' and u <= u'\u1FFF': return 'Greek'
  elif u >= u'\u2C00' and u <= u'\u2C5F': return 'Glagolitic'
  elif u >= u'\u2C80' and u <= u'\u2CFF': return 'Coptic'
  elif u >= u'\u2D00' and u <= u'\u2D2F': return 'Georgian'
  elif u >= u'\u2D30' and u <= u'\u2D7F': return 'Tifinagh'
  elif u >= u'\u2D80' and u <= u'\u2DDF': return 'Ethiopic'
  elif u >= u'\u2E80' and u <= u'\u2EFF': return 'CJK'
  elif u >= u'\u2F00' and u <= u'\u2FDF': return 'Kangxi Radicals'
  elif u >= u'\u3040' and u <= u'\u309F': return 'Hiragana'
  elif u >= u'\u30A0' and u <= u'\u30FF': return 'Katakana'
  elif u >= u'\u3100' and u <= u'\u312F': return 'Bopomofo'
  elif u >= u'\u3130' and u <= u'\u318F': return 'Hangul'
  elif u >= u'\u3190' and u <= u'\u319F': return 'Kanbun'
  elif u >= u'\u31A0' and u <= u'\u31BF': return 'Bopomofo'
  elif u >= u'\u31F0' and u <= u'\u31FF': return 'Katakana'
  elif u >= u'\u3300' and u <= u'\u33FF': return 'CJK'
  elif u >= u'\u3400' and u <= u'\u4DBF': return 'CJK'
  elif u >= u'\u4E00' and u <= u'\u9FFF': return 'CJK'
  elif u >= u'\uA000' and u <= u'\uA48F': return 'Yi'
  elif u >= u'\uA490' and u <= u'\uA4CF': return 'Yi'
  elif u >= u'\uA800' and u <= u'\uA82F': return 'Syloti Nagri'
  elif u >= u'\uAC00' and u <= u'\uD7AF': return 'Hangul'
  elif u >= u'\uF900' and u <= u'\uFAFF': return 'CJK'
  elif u >= u'\uFE30' and u <= u'\uFE4F': return 'CJK'
  elif u >= u'\uFE70' and u <= u'\uFEFF': return 'Arabic'
  elif u >= u'\u10000' and u <= u'\u1007F': return 'Linear B'
  elif u >= u'\u10080' and u <= u'\u100FF': return 'Linear B'
  elif u >= u'\u10300' and u <= u'\u1032F': return 'Old Italic'
  elif u >= u'\u10330' and u <= u'\u1034F': return 'Gothic'
  elif u >= u'\u10380' and u <= u'\u1039F': return 'Ugaritic'
  elif u >= u'\u103A0' and u <= u'\u103DF': return 'Old Persian'
  elif u >= u'\u10400' and u <= u'\u1044F': return 'Deseret'
  elif u >= u'\u10450' and u <= u'\u1047F': return 'Shavian'
  elif u >= u'\u10480' and u <= u'\u104AF': return 'Osmanya'
  elif u >= u'\u10800' and u <= u'\u1083F': return 'Cypriot Syllabary'
  elif u >= u'\u10A00' and u <= u'\u10A5F': return 'Kharoshthi'
  elif u >= u'\u20000' and u <= u'\u2A6DF': return 'CJK'
  elif u >= u'\u2F800' and u <= u'\u2FA1F': return 'CJK'
  else: return UNKNOWN_SCRIPT_


def StringToScript(string, encoding='utf8'):
  stats = {}
  try: ustring = unicode(string, encoding)
  except TypeError: ustring = string
  for u in ustring:
    if u.isspace(): continue
    try: stats[CharacterToScript(u)] += 1
    except KeyError: stats[CharacterToScript(u)] = 1
  max_ = 0
  script = UNKNOWN_SCRIPT_
  for s in stats:
    if stats[s] > max_:
      max_ = stats[s]
      script = s
  return script


def Lower(string, encoding='utf8'):
  try:
    return unicode(string, encoding).lower().encode(encoding)
  except TypeError:
    return string.lower().encode(encoding)


def Upper(string, encoding='utf8'):
  return unicode(string, encoding).upper().encode(encoding)


def SupportsCapitalization(string, encoding='utf8'):
  return StringToScript(string) in SCRIPTS_WITH_CAPITALIZATION_


def IsCapitalized(string, encoding='utf8'):
  try: ustring = unicode(string, encoding)
  except TypeError: ustring = string
  if ustring.lower()[0] == ustring[0]:
    return False
  return True


def IsPunctuation(character, encoding='utf-8'):
  try: uchar = unicode(character, encoding)
  except TypeError: uchar = character
  return unicodedata.category(uchar)[:1] == 'P'


def IsUnicodePunctuation(ucharacter):
  return unicodedata.category(ucharacter)[:1] == 'P'


def HasPunctuation(word, encoding='utf-8'):
  haspunctuation = False
  try: uword = unicode(word, encoding)
  except TypeError: uword = word
  for uchar in uword:
    if unicodedata.category(uchar)[:1] == 'P':
      haspunctuation = True
      break
  return haspunctuation


def HasDigit(word, encoding='utf-8'):
  hasdigit = False
  try: uword = unicode(word, encoding)
  except TypeError: uword = word
  for uchar in uword:
    if unicodedata.category(uchar) == 'Nd':
      hasdigit = True
      break
  return hasdigit


def IsUnicodeDigit(ucharacter):
  return unicodedata.category(ucharacter) == 'Nd'

########NEW FILE########
__FILENAME__ = xmlhandler
# -*- coding: utf-8 -*-

## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

"""Handler for XML format for extracted data. Input-text looks as follows:

<?xml version="1.0" encoding="UTF-8"?>
<doclist>
  <doc>
    <lang id="eng">
      <token count="3" morphs="Clinton" prons="k l I n t &amp; n">Clinton</token>
      <token count="3" morphs="Bush 's" prons="b U S">Bush</token>
    </lang>
    <lang id="zho">
      <token count="3" morphs="克林頓" prons="kh &amp; l i n t u n">克林頓</token>
      <token count="1" morphs="" prons="k a u t a u u ; t A k A s i m A j a">高島屋</token>
    </lang>
  </doc>
  <doc>
    <lang id="eng">
      <token count="2" morphs="Clinton" prons="k l I n t &amp; n">Clinton</token>
      <token count="3" morphs="Bush 's" prons="b U S">Bush</token>
    </lang>
    <lang id="ara">
      <token count="3" morphs="كلينتون" prons="k l j n t w n">كلينتون</token>
    </lang>
  </doc>
</doclist>

Also provides functions for converting from raw text to XML format.

"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
hollingk@cslu.ogi.edu (Kristy Hollingshead)
"""

import xml.sax.handler
import documents
import tokens
from __init__ import BASE_

class XmlHandler(xml.sax.handler.ContentHandler):
  def __init__(self):
    self.parser_ = xml.sax.make_parser()
    self.parser_.setContentHandler(self)
    self.in_token_ = False
    self.lang_ = None
    self.doc_ = None
    self.doclist_ = None

  def startElement(self, name, attributes):
    if name == 'doclist':
      self.in_token_ = False
      self.doclist_ = documents.Doclist()
      pass
    elif name == 'doc':
      self.in_token_ = False
      self.doc_ = documents.Doc()
      pass
    elif name == 'lang':
      self.in_token_ = False
      self.lang_ = tokens.Lang()
      try: self.lang_.SetId(attributes['id'])
      except KeyError: pass
    elif name == 'token':
      self.token_string_ = ''
      self.in_token_ = True
      try: self.count_ = int(attributes['count'])
      except KeyError: self.count_ = 1
      try: self.morphs_ = attributes['morphs']
      except KeyError: self.morphs_ = ''
      try: self.prons_ = attributes['prons']
      except KeyError: self.prons_ = ''

  def characters(self, data):
    if self.in_token_:
      self.token_string_ += data
 
  def endElement(self, name):
    if name == 'doclist':
      pass
    elif name == 'doc':
      self.doclist_.AddDoc(self.doc_)
      self.doc_ = None
    elif name == 'lang':
      self.doc_.AddLang(self.lang_)
      self.lang_ = None
    elif name == 'token':
      token_ = tokens.Token(self.token_string_)
      token_.SetCount(self.count_)
      token_.SetMorphs(self.morphs_.split())
      prons = self.prons_.split(';')
      for pron in prons:
        token_.AddPronunciation(pron.strip())
      self.lang_.AddToken(token_)

  def Decode(self, filename):
    self.parser_.parse(filename)
    return self.doclist_

  def DocList(self):
    return self.doclist_

########NEW FILE########
__FILENAME__ = xmlhandler_unittest
# -*- coding: utf-8 -*-

## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

"""Unit test for dochandler and token functions.
Also exercises token_comp a bit.
"""

__author__ = """
rws@uiuc.edu (Richard Sproat)
hollingk@cslu.ogi.edu (Kristy Hollingshead)
"""

import os
import sys
import xml.sax
import unittest
import xmlhandler
import tokens
import documents
from __init__ import BASE_


GOLDEN_FILE_ =  '%s/testdata/doctest.xml' % BASE_ 
TEST_FILE_ = '/tmp/doctest.xml'

def CreateDoclist():
  doclist = documents.Doclist()
  doc = documents.Doc()
  lang = tokens.Lang()
  lang.SetId('eng')
  token_ = tokens.Token('Bush')
  token_.SetCount(1)
  token_.AddPronunciation('b U S')
  token_.SetMorphs(['Bush', "'s"])
  lang.AddToken(token_)
  token_ = tokens.Token('Clinton')
  token_.SetCount(3)
  token_.AddPronunciation('k l I n t & n')
  token_.AddPronunciation('k l I n t > n')
  token_.SetMorphs(['Clinton'])
  lang.AddToken(token_)
  token_ = tokens.Token('Bush')
  token_.SetCount(3)
  token_.AddPronunciation('b U S')
  token_.SetMorphs(['Bush', "'s",])
  lang.AddToken(token_)
  lang.CompactTokens()
  doc.AddLang(lang)
  lang = tokens.Lang()
  lang.SetId('zho')
  token_ = tokens.Token('克林頓')
  token_.SetCount(3)
  token_.AddPronunciation('kh & l i n t u n')
  token_.SetMorphs(['克林頓'])
  lang.AddToken(token_)
  token_ = tokens.Token('高島屋')
  token_.SetCount(1)
  token_.AddPronunciation('k a u t a u u')
  token_.AddPronunciation('t A k A s i m A j a')
  lang.AddToken(token_)
  doc.AddLang(lang)
  doclist.AddDoc(doc)
  doc = documents.Doc()
  lang = tokens.Lang()
  lang.SetId('eng')
  token_ = tokens.Token('Clinton')
  token_.SetCount(2)
  token_.AddPronunciation('k l I n t & n')
  token_.SetMorphs(['Clinton'])
  lang.AddToken(token_)
  token_ = tokens.Token('Bush')
  token_.SetCount(3)
  token_.AddPronunciation('b U S')
  token_.SetMorphs(['Bush', "'s"])
  lang.AddToken(token_)
  doc.AddLang(lang)
  lang = tokens.Lang()
  lang.SetId('ara')
  token_ = tokens.Token('كلينتون')
  token_.SetCount(3)
  token_.AddPronunciation('k l j n t w n')
  token_.SetMorphs(['كلينتون'])
  lang.AddToken(token_)
  doc.AddLang(lang)
  doclist.AddDoc(doc)
  return doclist


def main(output = False):
  if output:
    doclist = CreateDoclist()
    doclist.XmlDump(GOLDEN_FILE_, utf8 = True)
  else:
    handler = xmlhandler.XmlHandler()
    doclist = handler.Decode(GOLDEN_FILE_)
    doclist.XmlDump(TEST_FILE_, utf8 = True)
    unittest.TestUnitOutputs(sys.argv[0], \
                             GOLDEN_FILE_, TEST_FILE_)


if __name__ == '__main__':
  if len(sys.argv) > 1 and sys.argv[1] == 'generate':
    main(True)
  else:
    main()

########NEW FILE########
__FILENAME__ = seqclass
from nltk.classify import iis
import yaml
import os

class SequentialClassifier(object):
    def __init__(self, left=2, right=0):
    #left = look back
    #right = look forward
        self._model = []
        self._left = left
        self._right = right
        self._leftcontext = [None] * (left)
        self._history = self._leftcontext
        self._rightcontext = [None] * (right)

    def size(self):
        return len(self._model)
    

    def classify(self, featuresets):
        if self.size() == 0:
            raise ValueError, 'Tagger is not trained'
        
        for i, featureset in enumerate(featuresets):

            #if i >= self._left:
                #self._leftcontext = sequence[i-self._left : i]
            #else:
                #self._leftcontext = sequence[:i]

                
            self._rightcontext = sequence[i+1 : i+1+self._right]
            
            label = self.classify_one(featureset)
            featureset['label'] = label
            
            del self._leftcontext[0]
            self._leftcontext.append(featureset)

            yield label

                
    def classify_one(self, featureset):
        """
        Classify a single featureset.
        """
        return self._model([featureset][0])

    def contexts(self, sequence):
        """
        Build a generator of triples (left context, item, right context).
    
        @param sequence: Input sequence
        @type sequence: C{list}
        @rtype: C{generator} of triples (left_context, token, right_contex)
                """
            
        for i in range(len(sequence)):
                if i >= self._left:
                        left_context = sequence[i - self._left:i]
                else:
                        left_context = sequence[:i]
        
                right_context = sequence[i+1 : i+1+self._right]
    
                yield (left_context, sequence[i], right_context)  
                    
                    
    def detect_features(self, context):
        from string import join

        left_context, item, right_context = context
        features = {}
        token = item['token']
        
        features['cur_token(%s)' % token] = True
        features['is_title'] = token.istitle()
        features['is_digit'] = token.isdigit()
        features['is_upper'] = token.isupper()
        features['POS(%s)' % item['POS']] = True
        
        if left_context == []:
            features['initword'] = True
        else:
            left_labels = join([item['label'] for item in left_context], '_')
            features['left_labels(%s)' % left_labels] = True
            
        return features
    
    def save_features(self, training_data, filename):
         
        stream = open(filename,'w')
        yaml.dump_all(training_data, stream)
        print "Saving features to %s" % os.path.abspath(filename)        
        stream.close()

    
    def corpus2training_data(self, training_corpus, model_name='default', save=False):
        
        dict_corpus = tabular2dict(training_corpus, KEYS)
        contexts = self.contexts(dict_corpus)
        
        print "Detecting features"
        training_data = [(self.detect_features(c), c[1]['label']) for c in contexts]
        
        if save:
            feature_file = model_name + '.yaml'
            self.save_features(training_data, feature_file)
            
        else:
            return training_data
        
        
        
        
    def train(self, training_corpus, classifier=iis):
        """
        Train a classifier.
        """
        if self.size() != 0:
            raise ValueError, 'Classifier is already trained'
        
        training_data = self.corpus2training_data(training_corpus)
        
        print "Training classifier"
        self._model = iis(training_data)
            
            



def tabular2dict(tabular, keys):
    """
    Utility function to turn tabular format CONLL data into a
    sequence of dictionaries.
    @param tabular: tabular input
    @param keys: a dictionary that maps field positions into feature names
    @rtype: C{list} of featuresets
    """
    tokendicts = []
    lines = tabular.splitlines()
    for line in lines:
        line = line.strip()
        line = line.split()
        if line:
            tokendict = {}
            for i in range(len(line)):
                key = keys[i]
                tokendict [key] = line[i]
            tokendicts.append(tokendict )
    return tokendicts

KEYS = {0: 'token', 1: 'POS', 2: 'label'}



def demo():
    
    tabtrain = \
        """Het Art O
        Hof N B-ORG
        van Prep I-ORG
        Cassatie N I-ORG
        verbrak V O
        het Art O
        arrest N O
        """
    
    tabtest = \
        """Het Art 
        Hof N 
        van Prep 
        Cassatie N 
        verbrak V
        het Art
        arrest N
        """
    
    test = tabular2dict(tabtest, KEYS)
    train = tabular2dict(tabtrain, KEYS)

    sc = SequentialClassifier(2, 0)
    
    sc.train(tabtrain)
    sc.classify(tabtest)




demo()

########NEW FILE########
__FILENAME__ = stringcomp
# Natural Language Toolkit
# String Comparison Module
# Author: Tiago Tresoldi <tresoldi@users.sf.net>

"""
String Comparison Module.

Author: Tiago Tresoldi <tresoldi@users.sf.net>
Based on previous work by Qi Xiao Yang, Sung Sam Yuan, Li Zhao, Lu Chun,
and Sung Peng.
"""

def stringcomp (fx, fy):
    """
    Return a number within C{0.0} and C{1.0} indicating the similarity between
    two strings. A perfect match is C{1.0}, not match at all is C{0.0}.

    This is an implementation of the string comparison algorithm (also known
    as "string similarity") published by Qi Xiao Yang, Sung Sam Yuan, Li Zhao,
    Lu Chun and Sun Peng in a paper called "Faster Algorithm of String
    Comparison" ( http://front.math.ucdavis.edu/0112.6022 ). Please note that,
    however, this implementation presents some relevant differences that
    will lead to different numerical results (read the comments for more
    details).
  
    @param fx: A C{string}.
    @param fy: A C{string}.
  
    @return: A float with the value of the comparision between C{fx} and C{fy}.
             C{1.0} indicates a perfect match, C{0.0} no match at all.
    @rtype: C{float}
    """

    # get the smaller of 'n' and 'm', and of 'fx' and 'fy'
    n, m = len(fx), len(fy)
    if m < n:
        (n, m) = (m, n)
        (fx, fy) = (fy, fx)

    # Sum of the Square of the Number of the same Characters
    ssnc = 0.

    # My implementation presents some relevant differences to the pseudo-code
    # presented in the paper by Yang et al., which in a number of cases will
    # lead to different numerical results (and, while no empirical tests have
    # been perfomed, I expect this to be slower than the original).
    # The differences are due to two specific characteristcs of the original
    # algorithm that I consider undesiderable for my purposes:
    #
    # 1. It does not takes into account the probable repetition of the same
    #    substring inside the strings to be compared (such as "you" in "where
    #    do you think that you are going?") because, as far as I was able to
    #    understand, it count only the first occurence of each substring
    #    found.
    # 2. It does not seem to consider the high probability of having more 
    #    than one pattern of the same length (for example, comparing between
    #    "abc1def" and "abc2def" seems to consider only one three-character
    #    pattern, "abc").
    #
    # Demonstrating the differences between the two algorithms (or, at least,
    # between my implementation of the original and the revised one):
    #
    # "abc1def" and "abc2def"
    #    Original: 0.534
    #    Current:  0.606
    for length in range(n, 0, -1):
        while True:
            length_prev_ssnc = ssnc
            for i in range(len(fx)-length+1):
                pattern = fx[i:i+length]
                pattern_prev_ssnc = ssnc
                fx_removed = False
                while True:
                    index = fy.find(pattern)
                    if index != -1:
                        ssnc += (2.*length)**2
                        if fx_removed == False:
                            fx = fx[:i] + fx[i+length:]
                            fx_removed = True
                        fy = fy[:index] + fy[index+length:]
                    else:
                        break
                if ssnc != pattern_prev_ssnc:
                    break
            if ssnc == length_prev_ssnc:
                break

    return (ssnc/((n+m)**2.))**0.5


def demo ():
    print "Comparison between 'python' and 'python': %.2f" % stringcomp("python", "python")
    print "Comparison between 'python' and 'Python': %.2f" % stringcomp("python", "Python")
    print "Comparison between 'NLTK' and 'NTLK': %.2f" % stringcomp("NLTK", "NTLK")
    print "Comparison between 'abc' and 'def': %.2f" % stringcomp("abc", "def")
  
    print "Word most similar to 'australia' in list ['canada', 'brazil', 'egypt', 'thailand', 'austria']:"
    max_score = 0.0 ; best_match = None
    for country in ["canada", "brazil", "egypt", "thailand", "austria"]:
        score = stringcomp("australia", country)
        if score > max_score:
            best_match = country
            max_score = score
        print "(comparison between 'australia' and '%s': %.2f)" % (country, score)
    print "Word most similar to 'australia' is '%s' (score: %.2f)" % (best_match, max_score)
  
if __name__ == "__main__":
    demo()

########NEW FILE########
__FILENAME__ = tnt
# Natural Language Toolkit: Interface to TnT 
#
# Author: Dan Garrette <dhgarrette@gmail.com>
#
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

import os
import tempfile
from nltk import tokenize
from nltk.internals import find_binary

_tnt_bin = None

def config_tnt(bin=None, verbose=False):
    """
    Configure the location of TnT Executable
    
    @param path: Path to the TnT executable
    @type path: C{str}
    """
    
    try:
        if _tnt_bin:
            return _tnt_bin
    except UnboundLocalError:
        pass
    
    # Find the tnt binary.
    tnt_bin = find_binary('tnt', bin,
        searchpath=tnt_search, env_vars=['TNTHOME'],
        url='http://www.coli.uni-saarland.de/~thorsten/tnt/',
        verbose=verbose)
    
    _tnt_bin = tnt_bin
    return _tnt_bin

tnt_search = ['.',
              '/usr/lib/tnt',
              '/usr/local/bin',
              '/usr/local/bin/tnt']

def pos_tag(sentence, model_path=None, verbose=False):
    """
    Use TnT to parse a sentence
    
    @param sentence: Input sentence to parse
    @type sentence: L{str}
    @return: C{DepGraph} the dependency graph representation of the sentence
    """
    
    tnt_bin = config_tnt(verbose=verbose)
    
    if not model_path:
        model_path = '%s/models/wsj' % tnt_bin[:-4]
        
    input_file =  '%s/tnt_in.txt' % tnt_bin[:-4]
    output_file = '%s/tnt_out.txt' % tempfile.gettempdir()
    
    execute_string = '%s %s %s > %s'
    if not verbose:
        execute_string += ' 2> %s/tnt.out' % tempfile.gettempdir()
    
    tagged_words = []
    
    f = None
    try:
        if verbose: 
            print 'Begin input file creation' 
            print 'input_file=%s' % input_file

        f = open(input_file, 'w')
        words = tokenize.WhitespaceTokenizer().tokenize(sentence)
        for word in words:
            f.write('%s\n' % word)
        f.write('\n')
        f.close()
        if verbose: print 'End input file creation'
    
        if verbose:
            print 'tnt_bin=%s' % tnt_bin 
            print 'model_path=%s' % model_path
            print 'output_file=%s' % output_file
    
        execute_string = execute_string % (tnt_bin, model_path, input_file, output_file)
        
        if verbose: 
            print 'execute_string=%s' % execute_string
        
        if verbose: print 'Begin tagging'
        tnt_exit = os.system(execute_string)
        if verbose: print 'End tagging (exit code=%s)' % tnt_exit
        
        f = open(output_file, 'r')
        lines = f.readlines()
        f.close()

        tagged_words = []
        tokenizer = tokenize.WhitespaceTokenizer()
        for line in lines:
            if not line.startswith('%%'):
                tokens = tokenizer.tokenize(line.strip())
                if len(tokens) == 2:
                    tagged_words.append((tokens[0], tokens[1]))
                
        if verbose:
            for tag in tagged_words:
                print tag

    finally:
        if f: f.close()

    return tagged_words


if __name__ == '__main__':
#    train(True)

    pos_tag('John sees Mary', verbose=True)
    

########NEW FILE########
__FILENAME__ = textgrid
# Natural Language Toolkit: TextGrid analysis
#
# Copyright (C) 2001-2011 NLTK Project
# Author: Margaret Mitchell <itallow@gmail.com>
#         Steven Bird <sb@csse.unimelb.edu.au> (revisions)
# URL: <http://www.nltk.org>
# For license information, see LICENSE.TXT
#

"""
Tools for reading TextGrid files, the format used by Praat.

Module contents
===============

The textgrid corpus reader provides 4 data items and 1 function
for each textgrid file.  For each tier in the file, the reader
provides 10 data items and 2 functions.
 
For the full textgrid file: 

  - size
    The number of tiers in the file.

  - xmin
    First marked time of the file.

  - xmax
    Last marked time of the file.

  - t_time
    xmax - xmin.

  - text_type
    The style of TextGrid format:
        - ooTextFile:  Organized by tier.
        - ChronTextFile:  Organized by time.
        - OldooTextFile:  Similar to ooTextFile.

  - to_chron()
    Convert given file to a ChronTextFile format.

  - to_oo()
    Convert given file to an ooTextFile format.

For each tier:

  - text_type
    The style of TextGrid format, as above.

  - classid
    The style of transcription on this tier:
        - IntervalTier:  Transcription is marked as intervals.
        - TextTier:  Transcription is marked as single points.

  - nameid
    The name of the tier.

  - xmin
    First marked time of the tier.

  - xmax
    Last marked time of the tier.

  - size
    Number of entries in the tier.

  - transcript
    The raw transcript for the tier.

  - simple_transcript
    The transcript formatted as a list of tuples: (time1, time2, utterance).

  - tier_info
    List of (classid, nameid, xmin, xmax, size, transcript).

  - min_max()
    A tuple of (xmin, xmax).  

  - time(non_speech_marker)
    Returns the utterance time of a given tier.
    Excludes entries that begin with a non-speech marker.

"""

# needs more cleanup, subclassing, epydoc docstrings

import sys
import re

TEXTTIER = "TextTier"
INTERVALTIER = "IntervalTier"

OOTEXTFILE = re.compile(r"""(?x)
            xmin\ =\ (.*)[\r\n]+
            xmax\ =\ (.*)[\r\n]+
            [\s\S]+?size\ =\ (.*)[\r\n]+ 
""")

CHRONTEXTFILE = re.compile(r"""(?x)
            [\r\n]+(\S+)\ 
            (\S+)\ +!\ Time\ domain.\ *[\r\n]+
            (\S+)\ +!\ Number\ of\ tiers.\ *[\r\n]+"
""")

OLDOOTEXTFILE = re.compile(r"""(?x)
            [\r\n]+(\S+)
            [\r\n]+(\S+)
            [\r\n]+.+[\r\n]+(\S+)
""")



#################################################################
# TextGrid Class
#################################################################

class TextGrid(object):
    """
    Class to manipulate the TextGrid format used by Praat.
    Separates each tier within this file into its own Tier
    object.  Each TextGrid object has
    a number of tiers (size), xmin, xmax, a text type to help
    with the different styles of TextGrid format, and tiers with their
    own attributes.
    """

    def __init__(self, read_file):
        """
        Takes open read file as input, initializes attributes 
        of the TextGrid file.
        @type read_file: An open TextGrid file, mode "r".
        @param size:  Number of tiers.
        @param xmin: xmin.
        @param xmax: xmax.
        @param t_time:  Total time of TextGrid file.
        @param text_type:  TextGrid format.
        @type tiers:  A list of tier objects.
        """

        self.read_file = read_file
        self.size = 0
        self.xmin = 0
        self.xmax = 0
        self.t_time = 0
        self.text_type = self._check_type()
        self.tiers = self._find_tiers()

    def __iter__(self):
        for tier in self.tiers:
            yield tier

    def next(self):
        if self.idx == (self.size - 1):
            raise StopIteration
        self.idx += 1
        return self.tiers[self.idx]

    @staticmethod
    def load(file):
        """
        @param file: a file in TextGrid format
        """

        return TextGrid(open(file).read())

    def _load_tiers(self, header):
        """
        Iterates over each tier and grabs tier information.
        """ 

        tiers = []
        if self.text_type == "ChronTextFile":
            m = re.compile(header)
            tier_headers = m.findall(self.read_file)
            tier_re = " \d+.?\d* \d+.?\d*[\r\n]+\"[^\"]*\""
            for i in range(0, self.size):
                tier_info = [tier_headers[i]] + \
                re.findall(str(i + 1) + tier_re, self.read_file)
                tier_info = "\n".join(tier_info)
                tiers.append(Tier(tier_info, self.text_type, self.t_time))
            return tiers

        tier_re = header + "[\s\S]+?(?=" + header + "|$$)"
        m = re.compile(tier_re)
        tier_iter = m.finditer(self.read_file)
        for iterator in tier_iter:
            (begin, end) = iterator.span()
            tier_info = self.read_file[begin:end]
            tiers.append(Tier(tier_info, self.text_type, self.t_time))
        return tiers
    
    def _check_type(self):
        """
        Figures out the TextGrid format.
        """

        m = re.match("(.*)[\r\n](.*)[\r\n](.*)[\r\n](.*)", self.read_file)
        try:
            type_id = m.group(1).strip()
        except AttributeError:
            raise TypeError("Cannot read file -- try TextGrid.load()")
        xmin = m.group(4)
        if type_id == "File type = \"ooTextFile\"":
            if "xmin" not in xmin:
                text_type = "OldooTextFile"
            else:
                text_type = "ooTextFile"
        elif type_id == "\"Praat chronological TextGrid text file\"":
            text_type = "ChronTextFile"
        else: 
            raise TypeError("Unknown format '(%s)'", (type_id))
        return text_type
        
    def _find_tiers(self):
        """
        Splits the textgrid file into substrings corresponding to tiers. 
        """

        if self.text_type == "ooTextFile":
            m = OOTEXTFILE
            header = " +item \["
        elif self.text_type == "ChronTextFile":
            m = CHRONTEXTFILE
            header = "\"\S+\" \".*\" \d+\.?\d* \d+\.?\d*"
        elif self.text_type == "OldooTextFile":
            m = OLDOOTEXTFILE
            header = "\".*\"[\r\n]+\".*\""

        file_info = m.findall(self.read_file)[0]
        self.xmin = float(file_info[0])
        self.xmax = float(file_info[1])
        self.t_time = self.xmax - self.xmin
        self.size = int(file_info[2])
        tiers = self._load_tiers(header)
        return tiers

    def to_chron(self):
        """ 
        @return:  String in Chronological TextGrid file format.
        """

        chron_file = ""
        chron_file += "\"Praat chronological TextGrid text file\"\n"
        chron_file += str(self.xmin) + " " + str(self.xmax)
        chron_file += "   ! Time domain.\n"
        chron_file += str(self.size) + "   ! Number of tiers.\n"
        for tier in self.tiers:
            idx = (self.tiers.index(tier)) + 1
            tier_header = "\"" + tier.classid + "\" \"" \
                          + tier.nameid + "\" " + str(tier.xmin) \
                          + " " + str(tier.xmax)
            chron_file += tier_header + "\n"
            transcript = tier.simple_transcript
            for (xmin, xmax, utt) in transcript:
                chron_file += str(idx) + " " + str(xmin) 
                chron_file += " " + str(xmax) +"\n"
                chron_file += "\"" + utt + "\"\n"
        return chron_file

    def to_oo(self):
        """ 
        @return:  A string in OoTextGrid file format.
        """
   
        oo_file = ""
        oo_file += "File type = \"ooTextFile\"\n"
        oo_file += "Object class = \"TextGrid\"\n\n"
        oo_file += "xmin = ", self.xmin, "\n"
        oo_file += "xmax = ", self.xmax, "\n"
        oo_file += "tiers? <exists>\n"
        oo_file += "size = ", self.size, "\n"
        oo_file += "item []:\n"
        for i in range(len(self.tiers)):
            oo_file += "%4s%s [%s]" % ("", "item", i + 1)
            _curr_tier = self.tiers[i]
            for (x, y) in _curr_tier.header:
                oo_file += "%8s%s = \"%s\"" % ("", x, y)
            if _curr_tier.classid != TEXTTIER:
                for (xmin, xmax, text) in _curr_tier.simple_transcript:
                    oo_file += "%12s%s = %s" % ("", "xmin", xmin)
                    oo_file += "%12s%s = %s" % ("", "xmax", xmax)
                    oo_file += "%12s%s = \"%s\"" % ("", "text", text)
            else:
                for (time, mark) in _curr_tier.simple_transcript:
                    oo_file += "%12s%s = %s" % ("", "time", time)
                    oo_file += "%12s%s = %s" % ("", "mark", mark)
        return oo_file


#################################################################
# Tier Class
#################################################################

class Tier(object):
    """ 
    A container for each tier.
    """

    def __init__(self, tier, text_type, t_time):
        """
        Initializes attributes of the tier: class, name, xmin, xmax
        size, transcript, total time.  
        Utilizes text_type to guide how to parse the file.
        @type tier: a tier object; single item in the TextGrid list.
        @param text_type:  TextGrid format
        @param t_time:  Total time of TextGrid file.
        @param classid:  Type of tier (point or interval).
        @param nameid:  Name of tier.
        @param xmin:  xmin of the tier.
        @param xmax:  xmax of the tier.
        @param size:  Number of entries in the tier
        @param transcript:  The raw transcript for the tier.
        """

        self.tier = tier
        self.text_type = text_type
        self.t_time = t_time
        self.classid = ""
        self.nameid = ""
        self.xmin = 0
        self.xmax = 0
        self.size = 0
        self.transcript = ""
        self.tier_info = ""
        self._make_info()
        self.simple_transcript = self.make_simple_transcript()
        if self.classid != TEXTTIER:
            self.mark_type = "intervals"
        else:
            self.mark_type = "points"
            self.header = [("class", self.classid), ("name", self.nameid), \
            ("xmin", self.xmin), ("xmax", self.xmax), ("size", self.size)]

    def __iter__(self):
        return self
  
    def _make_info(self):
        """
        Figures out most attributes of the tier object:
        class, name, xmin, xmax, transcript.
        """

        trans = "([\S\s]*)"
        if self.text_type == "ChronTextFile":
            classid = "\"(.*)\" +"
            nameid = "\"(.*)\" +"
            xmin = "(\d+\.?\d*) +"
            xmax = "(\d+\.?\d*) *[\r\n]+"
            # No size values are given in the Chronological Text File format.
            self.size = None
            size = ""
        elif self.text_type == "ooTextFile":
            classid = " +class = \"(.*)\" *[\r\n]+"
            nameid = " +name = \"(.*)\" *[\r\n]+"
            xmin = " +xmin = (\d+\.?\d*) *[\r\n]+"
            xmax = " +xmax = (\d+\.?\d*) *[\r\n]+"
            size = " +\S+: size = (\d+) *[\r\n]+"
        elif self.text_type == "OldooTextFile":
            classid = "\"(.*)\" *[\r\n]+"
            nameid = "\"(.*)\" *[\r\n]+"
            xmin = "(\d+\.?\d*) *[\r\n]+"
            xmax = "(\d+\.?\d*) *[\r\n]+"
            size = "(\d+) *[\r\n]+"
        m = re.compile(classid + nameid + xmin + xmax + size + trans)
        self.tier_info = m.findall(self.tier)[0]
        self.classid = self.tier_info[0]
        self.nameid = self.tier_info[1]
        self.xmin = float(self.tier_info[2])
        self.xmax = float(self.tier_info[3])
        if self.size != None:
            self.size = int(self.tier_info[4])
        self.transcript = self.tier_info[-1]
            
    def make_simple_transcript(self):
        """ 
        @return:  Transcript of the tier, in form [(start_time end_time label)]
        """

        if self.text_type == "ChronTextFile":
            trans_head = ""
            trans_xmin = " (\S+)"
            trans_xmax = " (\S+)[\r\n]+"
            trans_text = "\"([\S\s]*?)\""
        elif self.text_type == "ooTextFile":
            trans_head = " +\S+ \[\d+\]: *[\r\n]+"
            trans_xmin = " +\S+ = (\S+) *[\r\n]+"
            trans_xmax = " +\S+ = (\S+) *[\r\n]+"
            trans_text = " +\S+ = \"([^\"]*?)\""    
        elif self.text_type == "OldooTextFile":
            trans_head = ""
            trans_xmin = "(.*)[\r\n]+"
            trans_xmax = "(.*)[\r\n]+"
            trans_text = "\"([\S\s]*?)\""
        if self.classid == TEXTTIER:
            trans_xmin = ""
        trans_m = re.compile(trans_head + trans_xmin + trans_xmax + trans_text)
        self.simple_transcript = trans_m.findall(self.transcript)
        return self.simple_transcript

    def transcript(self):
        """
        @return:  Transcript of the tier, as it appears in the file.
        """
       
        return self.transcript

    def time(self, non_speech_char="."):
        """
        @return: Utterance time of a given tier.
        Screens out entries that begin with a non-speech marker.        
        """

        total = 0.0
        if self.classid != TEXTTIER:
            for (time1, time2, utt) in self.simple_transcript:
                utt = utt.strip()
                if utt and not utt[0] == ".":
                    total += (float(time2) - float(time1))
        return total
                    
    def tier_name(self):
        """
        @return:  Tier name of a given tier.
        """

        return self.nameid

    def classid(self):
        """
        @return:  Type of transcription on tier.
        """

        return self.classid

    def min_max(self):
        """
        @return:  (xmin, xmax) tuple for a given tier.
        """

        return (self.xmin, self.xmax)

    def __repr__(self):
        return "<%s \"%s\" (%.2f, %.2f) %.2f%%>" % (self.classid, self.nameid, self.xmin, self.xmax, 100*self.time()/self.t_time)

    def __str__(self):
        return self.__repr__() + "\n  " + "\n  ".join(" ".join(row) for row in self.simple_transcript)

def demo_TextGrid(demo_data):
    print "** Demo of the TextGrid class. **"

    fid = TextGrid(demo_data)
    print "Tiers:", fid.size

    for i, tier in enumerate(fid):
        print "\n***"
        print "Tier:", i + 1
        print tier

def demo():
    # Each demo demonstrates different TextGrid formats.
    print "Format 1"
    demo_TextGrid(demo_data1)
    print "\nFormat 2"
    demo_TextGrid(demo_data2)
    print "\nFormat 3"
    demo_TextGrid(demo_data3)


demo_data1 = """File type = "ooTextFile"
Object class = "TextGrid"

xmin = 0 
xmax = 2045.144149659864
tiers? <exists> 
size = 3 
item []: 
    item [1]:
        class = "IntervalTier" 
        name = "utterances" 
        xmin = 0 
        xmax = 2045.144149659864 
        intervals: size = 5 
        intervals [1]:
            xmin = 0 
            xmax = 2041.4217474125382 
            text = "" 
        intervals [2]:
            xmin = 2041.4217474125382 
            xmax = 2041.968276643991 
            text = "this" 
        intervals [3]:
            xmin = 2041.968276643991 
            xmax = 2042.5281632653062 
            text = "is" 
        intervals [4]:
            xmin = 2042.5281632653062 
            xmax = 2044.0487352585324 
            text = "a" 
        intervals [5]:
            xmin = 2044.0487352585324 
            xmax = 2045.144149659864 
            text = "demo" 
    item [2]:
        class = "TextTier" 
        name = "notes" 
        xmin = 0 
        xmax = 2045.144149659864 
        points: size = 3 
        points [1]:
            time = 2041.4217474125382 
            mark = ".begin_demo"
        points [2]:
            time = 2043.8338291031832
            mark = "voice gets quiet here" 
        points [3]:
            time = 2045.144149659864
            mark = ".end_demo" 
    item [3]:
        class = "IntervalTier" 
        name = "phones" 
        xmin = 0 
        xmax = 2045.144149659864
        intervals: size = 12
        intervals [1]:
            xmin = 0 
            xmax = 2041.4217474125382 
            text = "" 
        intervals [2]:
            xmin = 2041.4217474125382 
            xmax = 2041.5438290324326 
            text = "D"
        intervals [3]:
            xmin = 2041.5438290324326
            xmax = 2041.7321032910372
            text = "I"
        intervals [4]:
            xmin = 2041.7321032910372            
            xmax = 2041.968276643991 
            text = "s" 
        intervals [5]:
            xmin = 2041.968276643991 
            xmax = 2042.232189031843
            text = "I"
        intervals [6]:
            xmin = 2042.232189031843
            xmax = 2042.5281632653062 
            text = "z" 
        intervals [7]:
            xmin = 2042.5281632653062 
            xmax = 2044.0487352585324 
            text = "eI" 
        intervals [8]:
            xmin = 2044.0487352585324 
            xmax = 2044.2487352585324
            text = "dc"
        intervals [9]:
            xmin = 2044.2487352585324
            xmax = 2044.3102321849011
            text = "d"
        intervals [10]:
            xmin = 2044.3102321849011
            xmax = 2044.5748932104329
            text = "E"
        intervals [11]:
            xmin = 2044.5748932104329
            xmax = 2044.8329108578437
            text = "m"
        intervals [12]:
            xmin = 2044.8329108578437
            xmax = 2045.144149659864 
            text = "oU" 
"""

demo_data2 = """File type = "ooTextFile"
Object class = "TextGrid"

0
2.8
<exists>
2
"IntervalTier"
"utterances"
0
2.8
3
0
1.6229213249309031
""
1.6229213249309031
2.341428074708195
"demo"
2.341428074708195
2.8
""
"IntervalTier"
"phones"
0
2.8
6
0
1.6229213249309031
""
1.6229213249309031
1.6428291382019483
"dc"
1.6428291382019483
1.65372183721983721
"d"
1.65372183721983721
1.94372874328943728
"E"
1.94372874328943728
2.13821938291038210
"m"
2.13821938291038210
2.341428074708195
"oU"
2.341428074708195
2.8
""
"""

demo_data3 = """"Praat chronological TextGrid text file"
0 2.8   ! Time domain.
2   ! Number of tiers.
"IntervalTier" "utterances" 0 2.8
"IntervalTier" "utterances" 0 2.8
1 0 1.6229213249309031
""
2 0 1.6229213249309031
""
2 1.6229213249309031 1.6428291382019483
"dc"
2 1.6428291382019483 1.65372183721983721
"d"
2 1.65372183721983721 1.94372874328943728
"E"
2 1.94372874328943728 2.13821938291038210
"m"
2 2.13821938291038210 2.341428074708195
"oU"
1 1.6229213249309031 2.341428074708195
"demo"
1 2.341428074708195 2.8
""
2 2.341428074708195 2.8
""
"""

if __name__ == "__main__":
    demo()


########NEW FILE########
__FILENAME__ = corpus
# -*- coding: utf-8 -*-
# Copyright © 2007-2008 Stockholm TreeAligner Project
# Author: Torsten Marek <shlomme@gmx.net>
# Licensed under the GNU GPLv2
"""This module provides a single point of access to all functionality of a TigerSearch corpus
once its index has been created.
"""
from nltk_contrib.tiger.indexer import graph_serializer
from nltk_contrib.tiger.graph import NodeType
from nltk_contrib.tiger.index import IndexNodeId
from nltk_contrib.tiger.query import TsqlQueryEvaluator

__all__ = ("Corpus", )

# TODO: provide methods for accessing edge labels and feature values.


class CorpusInfo(object):
    def __init__(self, db):
        self._cursor = db.cursor()
        self.corpus_size = self._cursor.execute("SELECT COUNT(*) FROM graphs").fetchone()[0]
        
    def get_feature_names(self, node_type):
        if node_type is NodeType.UNKNOWN:
            return frozenset(row[0] for row in self._cursor.execute(
                "SELECT name FROM features"))
        else:
            return frozenset(row[0] for row in self._cursor.execute(
                "SELECT name FROM features WHERE domain = ?", (node_type.key, )))

    @property
    def feature_types(self):
        if not hasattr(self, "_feature_types"):
            self._feature_types = dict(
                (feat, node_type) 
                for node_type in [NodeType.TERMINAL, NodeType.NONTERMINAL]
                for feat in self.get_feature_names(node_type))

        return self._feature_types


class Corpus(object):
    """A class that provides access to a TIGER-XML corpus.
    
    
    Lifecycle
    =========
    Corpora should always be instantiated using the factory methods in `nltk_contrib.tiger`. When a corpus
    object is not used anymore, it should be `close()`ed.
    
    Graphs
    ======
    Graphs can be loaded by their ordinal, using the function `get_graph(o)`. Graph ordinals are in 
    the range ``[0..len(corpus)[``, and assigned by indexer that created the corpus. The Tiger
    corpus indexer assigns graph ids based on the order in the input file.
    
    XML ids and index ids
    =====================
    The `Corpus` class provides several methods to convert the internal index ids to the XML ids
    found in the TIGER-XML corpus, and vice versa:
    
     * `get_xml_node_id`
     * `get_xml_graph_id`
     * `get_index_node_id`
    
    Query Evaluation
    ================
    The method `get_query_evaluator` returns the TigerSearch query evaluator for this corpus.
    """
    DEFAULT_DESERIALIZER = graph_serializer.GraphDeserializer

    def __init__(self, treebank_id, db, db_provider):
        self.treebank_id = treebank_id
        
        self._db_provider = db_provider
        self._db = db
        self._info = CorpusInfo(db)
        
        self._cursor = self._db.cursor()
        
        self._deserializer = self.DEFAULT_DESERIALIZER(
            self._get_edge_label_rmap(),
            self._get_secedge_label_rmap(),
            self._get_feature_revmap(NodeType.TERMINAL),
            self._get_feature_revmap(NodeType.NONTERMINAL))
        
        self._evaluator = None

    def _get_edge_label_rmap(self):
        return [unicode(r[0]) 
                for r in self._cursor.execute("SELECT label FROM edge_labels ORDER BY id")]
    
    def _get_secedge_label_rmap(self):
        return [unicode(r[0]) 
                for r in self._cursor.execute("SELECT label FROM secedge_labels ORDER BY id")]
        
    def _get_domain_features(self, domain):
        return self._cursor.execute(
            "SELECT id, name FROM features WHERE domain = ? ORDER BY order_id",
            (domain.key,)).fetchall()
    
    def _get_feature_revmap(self, domain):
        l = []
        for row in self._get_domain_features(domain):
            values = []
            l.append((row[1], values))
            for r in self._cursor.execute(
                "SELECT value FROM feature_values WHERE feature_id = ? ORDER BY value_id", 
                (row[0],)):
                values.append(unicode(r[0]))
        return l
    
    def __iter__(self):
        """Returns an iterator that produces all graphs in this corpus, ordered by id."""
        c = self._db.cursor()
        for graph_id, graph_data in c.execute("SELECT id, data FROM graphs ORDER BY id"):
            yield self._deserializer.deserialize_graph(graph_id, graph_data)
    
    def __len__(self):
        """Returns the number of graphs in the corpus."""
        return self._info.corpus_size
    
    def get_index_node_id(self, xml_node_id):
        """Returns the index node id associated with the `xml_node_id` from the TIGER-XML file."""
        result = self._cursor.execute("SELECT id FROM node_data WHERE xml_node_id = ?", 
                                      (xml_node_id,)).fetchone()
        return IndexNodeId.from_int(result[0])

    def get_index_graph_id(self, xml_graph_id):
        """Returns the index id of the graph with the `xml_graph_id` from the TIGER-XML file."""
        result = self._cursor.execute("SELECT id FROM graphs WHERE xml_graph_id = ?", 
                                      (xml_graph_id,)).fetchone()
        return result[0]
        
    def get_xml_node_id(self, node_id):
        """Returns the XML id associated with `node_id` from the index."""
        result = self._cursor.execute("SELECT xml_node_id FROM node_data WHERE id = ?", 
                                      (node_id.to_int(),)).fetchone()
        return result[0]

    def get_xml_graph_id(self, graph_ordinal):
        """Returns the XML id of the graph with number `graph_ordinal`."""
        result = self._cursor.execute("SELECT xml_graph_id FROM graphs WHERE id = ?", 
                                      (graph_ordinal,)).fetchone()
        return result[0]
    
    def get_root_id(self, ordinal):
        assert 0 <= ordinal < len(self)
        result = self._cursor.execute("SELECT data FROM graphs WHERE id = ?", 
                                      (ordinal,)).fetchone()
        return self._deserializer.get_root_id(result[0])
        
    def get_graph(self, ordinal):
        """Returns the Tiger graph datastructure for the graph with number `ordinal`."""
        assert 0 <= ordinal < len(self)
        result = self._cursor.execute("SELECT id, data FROM graphs WHERE id = ?", 
                                      (ordinal,)).fetchone()
        return self._deserializer.deserialize_graph(result[0], result[1])
    
    def get_query_evaluator(self):
        """Returns the TigerSearch query evaluator for this corpus."""
        if self._evaluator is None:
            self._evaluator = TsqlQueryEvaluator(self._db, self._db_provider, self._info)
        return self._evaluator
        
    def close(self):
        """Closes the corpus.
        
        After a call to close, the corpus cannot be used any more.
        """
        self._db.close()
        self._db = None
        self._cursor = None
        self._evaluator = None
    
    def reopen(self):
        """Reopens the corpus.
        
        If the database connection was open before, it will be closed first.
        """
        if self._db:
            self.close()
        self._db = self._db_provider.connect()
        self._cursor = self._db.cursor()
        self._info = CorpusInfo(self._db)
    

########NEW FILE########
__FILENAME__ = demo
# -*- coding: utf-8 -*-
# Copyright © 2007-2008 Stockholm TreeAligner Project
# Author: Torsten Marek <shlomme@gmx.net>
# Licensed under the GNU GPLv2
from __future__ import with_statement

# TODO: convert traces
import sys
import os
import tempfile
from optparse import OptionParser
from itertools import count

from nltk_contrib.tiger.utils.etree_xml import ET
from nltk_contrib.tiger import open_corpus_volatile, open_corpus

import functools

class _E(object):

    def __call__(self, tag, *children, **attrib):
        elem = ET.Element(tag, attrib)
        for item in children:
            if isinstance(item, dict):
                elem.attrib.update(item)
            elif isinstance(item, basestring):
                if len(elem):
                    elem[-1].tail = (elem[-1].tail or "") + item
                else:
                    elem.text = (elem.text or "") + item
            elif ET.iselement(item):
                elem.append(item)
            else:
                raise TypeError("bad argument: %r" % item)
        return elem

    def __getattr__(self, tag):
        return functools.partial(self, tag)

# create factory object
E = _E()

class Graph(object):
    DEFAULT_LABEL = "--"
    def __init__(self, graph_id):
        self._id = graph_id
        self._get_t_id = count(1).next
        self._get_nt_id = count(500).next

        self._terminals = ET.Element("terminals")
        self._nonterminals = ET.Element("nonterminals")
    
    def add_terminal(self, word, pos):
        node_id = "s%i_%i" % (self._id, self._get_t_id())

        ET.SubElement(self._terminals, "t", id=node_id, word=word, pos=pos)
        return (self.DEFAULT_LABEL, node_id), pos

    def add_nonterminal(self, cat, children):
        node_id = "s%i_%i" % (self._id, self._get_nt_id())

        parts = cat.split("-")
        real_cat = parts[0]
        if len(parts) > 1 and not parts[1].isdigit():
            lbl = parts[1].split("=")[0]
        else:
            lbl = self.DEFAULT_LABEL
        nt = ET.SubElement(self._nonterminals, "nt", id=node_id, cat=real_cat)
        for child_lbl, child_id in children:
            ET.SubElement(nt, "edge", idref=child_id, label=child_lbl)
        return (lbl, node_id), real_cat

    def get_xml(self, root_id):
        return E.s(
            E.graph(
                self._terminals,
                self._nonterminals,
                root=root_id),
            id="s%i" % (self._id))

class TreebankConverter(object):
    def __init__(self):
        self._cats = set()
        self._pos_tags = set()
        self._edge_labels = set()
        self._secedge_labels = set()

        self._graphs = []
        self._next_s_id = count(1).next
        

    def _get_children(self, penn_tree, graph):
        children = []
        for child in penn_tree:
            if len(child) == 1 and isinstance(child[0], basestring):
                edge, tag = graph.add_terminal(child[0], child.node)
                self._pos_tags.add(tag)
            else:
                edge, cat = graph.add_nonterminal(child.node, self._get_children(child, graph))
                self._cats.add(cat)
            self._edge_labels.add(edge[0])
            children.append(edge)

        return children

    def add_sentence(self, sentence):
        graph = Graph(self._next_s_id())
        lbl, root_id = self._get_children([sentence], graph)[0]
        self._graphs.append(graph.get_xml(root_id))
    
    def _edgelabels(self):
        e = E.edgelabel()
        for lbl in self._edge_labels:
            e.append(E.value("", name=lbl))
        return e

    def _list_feature(self, name, domain, feature_set):
        f = E.feature(name=name, domain=domain)
        for feature in feature_set:
            f.append(E.value("", name=feature))
        return f

    def _get_annotations(self):
        return E.annotation(
            E.feature(name="word", domain="T"),
            self._list_feature("pos", "T", self._pos_tags),
            self._list_feature("cat", "NT", self._cats),
            self._edgelabels(),
            E.secedgelabel())

    def _get_header(self):
        return E.head(
            E.meta(
                E.name("Penn Treebank Sampler"),
                E.author("Autogenerated"),
                E.date("today"),
                E.description("Penn Treebank Sampler distributed with NLTK converted to TIGER-XML"),
                E.format("Penn-Treebank Format"),
                E.history("Autogenerated")),
            self._get_annotations())

    def _get_body(self):
        body = E.body()
        body[:] = self._graphs
        return body

    def write(self, filename):
        corpus = E.corpus(
            self._get_header(), 
            self._get_body(),
            id="penn-sampler")
        ET.ElementTree(corpus).write(filename, encoding="UTF-8")

def convert_wsj(file_obj):
    from nltk.corpus import treebank
    sys.stderr.write("Converting Penn Treebank sampler...\n")
    tb = TreebankConverter()
    for sentence in treebank.parsed_sents():
        tb.add_sentence(sentence)
    tb.write(file_obj)
    

def demo():
    op = OptionParser()
    op.add_option("-c", "--corpus-file", help="If specified, the Penn sample will be stored/loaded in the given path.", metavar="FILE", action="store")
    op.add_option("-q", "--query", help="The query to be evaluated", default='[cat="NP"] > [cat="PP"]')
    
    options, args = op.parse_args()

    if options.corpus_file:
        corpus_file = options.corpus_file
        if not os.path.exists(corpus_file):
            convert_wsj(corpus_file)
        
        corpus = open_corpus("penn", corpus_file, veeroot=False)
    else:
        sys.stderr.write("Info: Use '-c' to keep converted corpus and skip conversion/indexing.\n")
        corpus_file = tempfile.TemporaryFile()
        convert_wsj(corpus_file)
        corpus_file.seek(0)
        corpus = open_corpus_volatile("penn", corpus_file, veeroot=False)
        corpus_file.close()
        
    print "Corpus size: %i graphs. " % (len(corpus, ))
    evaluator = corpus.get_query_evaluator()
    # Corpus is too small for parallel evaluation to speed up things
    evaluator.set_allow_parallel(False)
    print "Evaluating: %s" % (options.query, )
    query = evaluator.prepare_query(options.query)
    for result in query.evaluate():
        print "Graph: %s, matches: %i" % (corpus.get_xml_graph_id(result[0]), len(result[1]))
    # for access to the graph objects, see nltk_contrib.tiger.corpus and nltk_contrib.tiger.graph


if __name__ == "__main__":
    demo()

########NEW FILE########
__FILENAME__ = graph
# -*- coding: utf-8 -*-
# Copyright © 2007-2008 Stockholm TreeAligner Project
# Author: Torsten Marek <shlomme@gmx.net>
# Licensed under the GNU GPLv2
from collections import deque
from copy import deepcopy

from nltk_contrib.tiger.utils.enum import Enum, enum_member

__all__ = ("NodeType", "TerminalNode", "NonterminalNode", "TigerGraph", "veeroot_graph")

DEFAULT_VROOT_EDGE_LABEL = "--"


class NodeType(Enum):
    __fields__  = ("key",)

    TERMINAL = enum_member("T")
    NONTERMINAL = enum_member("N")
    UNKNOWN = enum_member("FREC")
    
    @staticmethod
    def fromkey(type_key):
        if type_key == "T":
            return NodeType.TERMINAL
        elif type_key == "N":
            return NodeType.NONTERMINAL
        else:
            raise ValueError, "Unknown domain key '%s'." % (type_key,)
    
    def __invert__(self):
        if self is self.__class__.TERMINAL:
            return self.__class__.NONTERMINAL
        elif self is self.__class__.NONTERMINAL:
            return self.__class__.TERMINAL
        else:
            raise ValueError, "Cannot invert '%s'." % (self,)


class _TigerNode(object):
    __slots__ = ("id", "features", "secedges", "TYPE")

    def __init__(self, id_):
        self.id = id_
        self.features = {}
        self.secedges = None

    def __eq__(self, other):
        return self.TYPE is other.TYPE and self.id == other.id \
               and self.features == other.features and self.secedges == other.secedges

    def __ne__(self, other):
        return not (self == other)


class TerminalNode(_TigerNode):
    __slots__ = ("order", )
    TYPE = NodeType.TERMINAL
    
    def __init__(self, id_):
        _TigerNode.__init__(self, id_)
        self.order = None
    
    def __eq__(self, other):
        return self.order == other.order and super(TerminalNode, self).__eq__(other)
    
    def __repr__(self): # pragma: nocover
        return "T(%s, %s, %s, %s)" % (self.id, self.features, self.order,
                                      [] if self.secedges is None else self.secedges)
    
    def __deepcopy__(self, memo):
        n = TerminalNode(self.id)
        n.order = self.order
        n.features = deepcopy(self.features, memo)
        n.secedges = deepcopy(self.secedges, memo)
        return n


class NonterminalNode(_TigerNode):
    __slots__ = ("edges",)
    TYPE = NodeType.NONTERMINAL
    
    def __init__(self, id_):
        super(NonterminalNode, self).__init__(id_)
        self.edges = None
        
    def __eq__(self, other):
        return self.edges == other.edges and super(NonterminalNode, self).__eq__(other)

    def __repr__(self): # pragma: nocover
        return "NT(%s, %s, %s, %s)" % (self.id, self.features, self.edges, 
                                       [] if self.secedges is None else self.secedges)

    def __iter__(self):
        return iter(self.edges)
    
    def __deepcopy__(self, memo):
        n = NonterminalNode(self.id)
        n.features = deepcopy(self.features, memo)
        n.secedges = deepcopy(self.secedges, memo)
        n.edges = deepcopy(self.edges, memo)
        return n
        
    
def veeroot_graph(graph, roots, edge_label = DEFAULT_VROOT_EDGE_LABEL):
    vroot_id = "%s_VROOT" % (graph.id,)
    
    assert vroot_id not in graph.nodes, \
           "Graph %s already has a node named '%s_VROOT'" % (graph.id, vroot_id)
    
    vroot_node = NonterminalNode(vroot_id)
    vroot_node.edges = [(edge_label, child_id) for child_id in roots]
    graph.root_id = vroot_id
    graph.nodes[vroot_id] = vroot_node

    
class NodeIndexData(object):
    __slots__ = ("id", "edge_label", "gorn_address")

    def __init__(self, id_, **kwargs):
        self.id = id_
        self.edge_label = None
        self.gorn_address = None
        for name, value in kwargs.iteritems():
            setattr(self, name, value)

            
class TerminalIndexData(NodeIndexData):
    __slots__ = ("order", )

    def __init__(self, id_, **kwargs):
        super(TerminalIndexData, self).__init__(id_, **kwargs)
        
            
class NonterminalIndexData(NodeIndexData):
    __slots__ = ("is_continuous", "arity", "token_arity", "left_corner", "right_corner", "children_type")

    def __init__(self, id_, **kwargs):
        self.is_continuous = True        
        super(NonterminalIndexData, self).__init__(id_, **kwargs)


class TigerGraph(object):
    __slots__ = ("id", "root_id", "nodes")
    def __init__(self, id_):
        self.id = id_
        self.nodes = {}
        self.root_id = None

    def __eq__(self, other):
        return self.id == other.id and self.nodes == other.nodes  and self.root_id == other.root_id
    
    def __ne__(self, other):
        return not (self == other)
    
    def __iter__(self):
        return self.nodes.itervalues()
    
    def terminals(self):
        return (n for n in self.nodes.itervalues() if n.TYPE is NodeType.TERMINAL)

    def nonterminals(self):
        return (n for n in self.nodes.itervalues() if n.TYPE is NodeType.NONTERMINAL)

    def copy(self):
        g = TigerGraph(self.id)
        g.root_id = self.root_id
        g.nodes = self.nodes.copy()
        return g
    
    def get_roots(self):
        unconnected = set(self.nodes)
        
        if self.nodes[self.root_id].TYPE is NodeType.NONTERMINAL:
            worklist = deque([self.root_id])
            while len(worklist) > 0:
                node_id = worklist.popleft()
                for label, child_id in self.nodes[node_id]:
                    unconnected.remove(child_id)
                    if self.nodes[child_id].TYPE is NodeType.NONTERMINAL:
                        worklist.append(child_id)
        
        return unconnected
    
    def compute_node_information(self):
        terminals = {}
        nonterminals = {}
        
        self._compute_dominance(terminals, nonterminals)
        self._compute_corners(nonterminals)

        return nonterminals.values(), terminals.values()

    def _compute_corners(self, nonterminals):
        def traverse(node_id, node_data):
            corners = []
            for label, child_id in self.nodes[node_id].edges:
                if self.nodes[child_id].TYPE is NodeType.TERMINAL:
                    corners.append(child_id)
                else:
                    corners += traverse(child_id, nonterminals[child_id])
            
            lco = node_data.left_corner = min(corners, key = lambda x: self.nodes[x].order)
            rco = node_data.right_corner = max(corners, key = lambda x: self.nodes[x].order)
            
            node_data.is_continuous = (
                node_data.token_arity == self.nodes[rco].order - self.nodes[lco].order + 1)
            return lco, rco
        
        if self.nodes[self.root_id].TYPE is NodeType.NONTERMINAL:
            traverse(self.root_id, nonterminals[self.root_id])

    def _get_children_type(self, node):
        x = 0
        for child in node:
            if self.nodes[child[1]].TYPE is NodeType.TERMINAL:
                x |= 1
            else:
                x |= 2
        return x
        
    def _compute_dominance(self, terminals, nonterminals):
        def traverse(node, gorn_address):
            token_arity = 0
            
            for idx, (label, child_id) in enumerate(node.edges):
                child_address = gorn_address + (idx, )
                child_node = self.nodes[child_id]
                if child_node.TYPE is NodeType.TERMINAL:
                    terminals[child_id] = TerminalIndexData(child_id, edge_label = label,
                                                            gorn_address = child_address, 
                                                            order = child_node.order)
                    token_arity += 1
                else:
                    d = nonterminals[child_id] = NonterminalIndexData(
                        child_id, edge_label = label, gorn_address = child_address,
                        arity = len(child_node.edges), 
                        children_type = self._get_children_type(child_node))
                    d.token_arity = traverse(child_node, child_address)
                    token_arity += d.token_arity
                    
            return token_arity
        
        
        root_node = self.nodes[self.root_id]
        if root_node.TYPE is NodeType.TERMINAL:
            terminals[self.root_id] = TerminalIndexData(self.root_id, gorn_address = (),
                                                        order = root_node.order)
        else:
            d = nonterminals[self.root_id] = NonterminalIndexData(
                self.root_id, gorn_address = (), arity = len(root_node.edges), 
                children_type = self._get_children_type(root_node))
        
            d.token_arity = traverse(root_node, ())

########NEW FILE########
__FILENAME__ = index
# -*- coding: utf-8 -*-
# Copyright © 2007-2008 Stockholm TreeAligner Project
# Author: Torsten Marek <shlomme@gmx.net>
# Licensed under the GNU GPLv2
"""Classes and methods related to the TIGER corpus on-disk index."""
import operator
import array

__all__ = ("IndexNodeId", "CONTINUOUS", "DISCONTINUOUS")

ID = 0
EDGE_LABEL = 1
CONTINUITY = 2
LEFT_CORNER = 3
RIGHT_CORNER = 4
TOKEN_ORDER = 5
GORN_ADDRESS = 6


CONTINUOUS = 1
DISCONTINUOUS = 3

class IndexNodeId(tuple):
    __slots__ = ()
    NODE_BIT_WIDTH = 12
    NODE_BITMASK = (1 << NODE_BIT_WIDTH) - 1

    def __new__(cls, graph_id, node_id):
        return tuple.__new__(cls, (graph_id, node_id))
    
    def __repr__(self):
        return "IndexNodeId(%i, %i)" % self
    
    def __reduce__(self):
        return (IndexNodeId, (self[0], self[1]))
    
    @classmethod
    def from_int(cls, i):
        return tuple.__new__(cls, (i >> 12, i & IndexNodeId.NODE_BITMASK))
    
    def to_int(self):
        return self[0] << self.NODE_BIT_WIDTH | self[1]
    
    def check(self):
        return self.graph_id < (1 << 20) and self.node_id < (1 << self.NODE_BIT_WIDTH)
    
    graph_id = property(operator.itemgetter(0))
    node_id = property(operator.itemgetter(1))

def gorn2db(address_tuple):
    return buffer(array.array("b", address_tuple).tostring())

########NEW FILE########
__FILENAME__ = graph_serializer
# -*- coding: utf-8 -*-
# Copyright © 2007-2008 Stockholm TreeAligner Project
# Author: Torsten Marek <shlomme@gmx.net>
# Licensed under the GNU GPLv2
import cPickle
import cStringIO
import zlib

from nltk_contrib.tiger.graph import NodeType, TigerGraph, NonterminalNode, TerminalNode

__all__ = ("GraphSerializer", "GraphDeserializer")

class GraphSerializer(object):
    def __init__(self):
        self._feature_value_map = {}
        self._feature_count = {
            NodeType.TERMINAL: 0, 
            NodeType.NONTERMINAL:  0
        }
        self.graphs = 0
        self.total_size = 0
        
    def set_edge_label_map(self, edge_label_map):
        self._edge_label_map = edge_label_map
    
    def set_secedge_label_map(self, secedge_label_map):
        self._secedge_label_map = secedge_label_map
    
    def add_feature_value_map(self, feature_name, domain, position, value_map):
        self._feature_count[domain] += 1
        self._feature_value_map[feature_name] = (position, value_map, domain)
        
    def _convert_edges(self, edge_list, label_map):
        l = []
        for label, target_id in edge_list:
            l.append(label_map[label])
            l.append(target_id)
        return tuple(l)

    def serialize_graph(self, graph):
        self.graphs += 1
        nts = []
        se_nts = []
        ts = []
        se_ts = []
        
        for node in graph.terminals():
            feature_list = [None] * self._feature_count[node.TYPE]
            for key, value in node.features.iteritems():
                descriptor = self._feature_value_map[key]
                assert descriptor[2] is node.TYPE
                feature_list[descriptor[0]] = descriptor[1][value]
            
            if node.secedges is None:
                ts.extend((node.id, tuple(feature_list), node.order))
            else:
                se_ts.extend((node.id, tuple(feature_list), node.order, 
                              self._convert_edges(node.secedges, self._secedge_label_map)))
            
        
        for node in graph.nonterminals():
            feature_list = [None] * self._feature_count[node.TYPE]
            for key, value in node.features.iteritems():
                descriptor = self._feature_value_map[key]
                assert descriptor[2] is node.TYPE
                feature_list[descriptor[0]] = descriptor[1][value]
            
            edges = self._convert_edges(node.edges, self._edge_label_map)
            
            if node.secedges is None:
                nts.extend((node.id, tuple(feature_list), edges))
            else:
                se_nts.extend((node.id, tuple(feature_list), edges, 
                               self._convert_edges(node.secedges, self._secedge_label_map)))
        
        d = cStringIO.StringIO()
        p = cPickle.Pickler(d, protocol = 2)
        p.dump(graph.root_id)
        for l in (nts, ts, se_nts, se_ts):
            p.dump(tuple(l))
        
        s = zlib.compress(d.getvalue())
        self.total_size += len(s)
        return s
    

class GraphDeserializer(object):
    def __init__(self, edge_label_rmap, secedge_label_rmap, t_feature_rmap, nt_feature_rmap):
        self._edge_label_rmap = edge_label_rmap
        self._secedge_label_rmap = secedge_label_rmap
        self._t_features = t_feature_rmap
        self._nt_features = nt_feature_rmap

    def _get_edge_list(self, edge_data, label_rmap):
        return [(label_rmap[edge_data[i]], edge_data[i+1])
                 for i in xrange(0, len(edge_data), 2)]
        
    def _get_features(self, fmap, stored):
        return dict(
            (fmap[idx][0], fmap[idx][1][value])
            for idx, value in enumerate(stored) if value is not None)
    
    def get_root_id(self, graph_data_buffer):
        up = cPickle.Unpickler(cStringIO.StringIO(zlib.decompress(graph_data_buffer)))
        return up.load()
    
    def deserialize_graph(self, graph_id, graph_data_buffer):
        graph = TigerGraph(graph_id)
        up = cPickle.Unpickler(cStringIO.StringIO(zlib.decompress(graph_data_buffer)))

        graph.root_id = up.load()
        data = up.load()
        for i in xrange(0, len(data), 3):
            node = NonterminalNode(data[i])
            node.features  = self._get_features(self._nt_features, data[i+1])

            node.edges = self._get_edge_list(data[i+2], self._edge_label_rmap)
            
            graph.nodes[node.id] = node
        
        data = up.load()
        for i in xrange(0, len(data), 3):
            node = TerminalNode(data[i])
            node.features  = self._get_features(self._t_features, data[i+1])
            node.order = data[i+2]
            graph.nodes[node.id] = node
        
        data = up.load()
        for i in xrange(0, len(data), 4):
            node = NonterminalNode(data[i])
            node.features  = self._get_features(self._nt_features, data[i+1])
            node.edges = self._get_edge_list(data[i+2], self._edge_label_rmap)
            node.secedges = self._get_edge_list(data[i+3], self._secedge_label_rmap)
            
            graph.nodes[node.id] = node
            
        data = up.load()
        for i in xrange(0, len(data), 4):
            node = TerminalNode(data[i])
            node.features  = self._get_features(self._t_features, data[i+1])
            node.order = data[i+2]
            node.secedges = self._get_edge_list(data[i+3], self._secedge_label_rmap)
            
            graph.nodes[node.id] = node
        
        return graph

########NEW FILE########
__FILENAME__ = tiger_corpus_indexer
# -*- coding: utf-8 -*-
# Copyright © 2007-2008 Stockholm TreeAligner Project
# Author: Torsten Marek <shlomme@gmx.net>
# Licensed under the GNU GPLv2

from collections import defaultdict
from itertools import count
import logging

def get_version_string():
    import nltk
    return "nltk %s" % (nltk.__version__, )

from nltk_contrib.tiger.index import IndexNodeId, CONTINUOUS, DISCONTINUOUS, gorn2db
from nltk_contrib.tiger.graph import NodeType, veeroot_graph, DEFAULT_VROOT_EDGE_LABEL

__all__ = ("TigerCorpusIndexer",)

# think about splitting feature_values table into several small tables.
# add NOT NULLS everywhere

# TODO: create proper progress reporter interface, hand in

INDEX_VERSION = 3

class _Tables(object):
    FEATURES = """CREATE TABLE features
    (id INTEGER PRIMARY KEY, order_id INTEGER, name TEXT, domain TEXT)"""
    
    FEATURE_VALUES = """CREATE TABLE feature_values
    (feature_id INTEGER, value_id INTEGER, value TEXT, description TEXT, 
     UNIQUE(feature_id, value_id))"""
    
    EDGE_LABELS = """CREATE TABLE edge_labels
    (id INTEGER PRIMARY KEY, label TEXT UNIQUE, description TEXT)"""
    
    SECEDGE_LABELS = """CREATE TABLE secedge_labels
    (id INTEGER PRIMARY KEY, label TEXT UNIQUE, description TEXT)"""
    
    GRAPHS = """CREATE TABLE graphs
    (id INTEGER PRIMARY KEY, xml_graph_id TEXT, data BLOB)"""
    
    METADATA = """CREATE TABLE corpus_metadata 
    (key TEXT UNIQUE, value TEXT)"""

    INDEX_METADATA = """CREATE TABLE index_metadata 
    (key TEXT UNIQUE, value NONE)"""
    
    NODE_DATA = """CREATE TABLE node_data 
    (id INTEGER PRIMARY KEY, xml_node_id TEXT, edge_label INTEGER, 
    gorn_address BLOB, continuity INTEGER, arity INTEGER,
    tokenarity INTEGER, left_corner INTEGER, right_corner INTEGER, token_order INTEGER)"""

    FEATURE_IIDX_TEMPLATE = """CREATE TABLE feature_iidx_%s 
    (node_id INTEGER PRIMARY KEY NOT NULL, value_id INTEGER NOT NULL)"""
    
    SECEDGES = """CREATE TABLE secedges
    (origin_id INT, target_id INT, label_id INT)"""

    
class TigerCorpusIndexer(object):
    def __init__(self, db, graph_serializer, progress = False, always_veeroot = True):
        self._db = db
        self._cursor = db.cursor()
        self._progress = progress
        self._always_veeroot = always_veeroot
        
        self._graphs = 0
        
        self._cursor.execute(_Tables.FEATURES)
        self._cursor.execute(_Tables.FEATURE_VALUES)
        self._cursor.execute(_Tables.EDGE_LABELS)
        self._cursor.execute(_Tables.SECEDGE_LABELS)
        self._cursor.execute(_Tables.GRAPHS)
        self._cursor.execute(_Tables.METADATA)
        self._cursor.execute(_Tables.INDEX_METADATA)
        self._cursor.execute(_Tables.NODE_DATA)
        self._cursor.execute(_Tables.SECEDGES)
        
        self._serializer = graph_serializer
        self._open_list_features = []
        self._feature_count = {
            NodeType.TERMINAL: 0,
            NodeType.NONTERMINAL: 0}
        
        self._feature_iidx_stmts = {}
        self._feature_value_maps = {}

        self._insert_lists = defaultdict(list)
        self._store_creator_metadata()
        
    def _store_creator_metadata(self):
        self._add_index_metadata(creator=get_version_string(), index_version=INDEX_VERSION)
    
    def _add_index_metadata(self, **kwargs):
        self._cursor.executemany("INSERT INTO index_metadata (key, value) VALUES (?, ?)", 
                                 kwargs.iteritems())
        
    def set_metadata(self, metadata):
        self._cursor.executemany("INSERT INTO corpus_metadata (key, value) VALUES (?, ?)", 
                                 metadata.iteritems())
                             
    
    def add_feature(self, feature_name, domain, feature_values):
        order_id = self._feature_count[domain]
        self._feature_count[domain] += 1
        self._cursor.execute("INSERT INTO features (order_id, name, domain) VALUES (?, ?, ?)", 
                             (order_id, feature_name, domain.key))
        feature_id = self._cursor.lastrowid
        
        if len(feature_values) > 0:
            value_map = dict((feature_value, idx) for idx, feature_value in enumerate(feature_values))
            
            self._cursor.executemany("INSERT INTO feature_values (feature_id, value_id, value, description) VALUES (?, ?, ?, ?)",
                                     ((feature_id, value_map[value], value, description) 
                                      for value, description in feature_values.iteritems()))
        else:
            value_map = defaultdict(count().next)
            self._open_list_features.append((feature_id, value_map))
        
        self._feature_value_maps[feature_name] = (value_map, domain)
        self._serializer.add_feature_value_map(feature_name, domain, order_id, value_map)
        self._create_feature_value_index(feature_name)
        return feature_id

    
    def set_edge_labels(self, edge_labels):
        if self._always_veeroot:
            assert DEFAULT_VROOT_EDGE_LABEL in edge_labels, "no neutral edge label"
            
        self._cursor.executemany("INSERT INTO edge_labels (id, label, description) VALUES (?, ?, ?)", 
                                 ((idx, e[0], e[1]) for idx, e in enumerate(edge_labels.iteritems())))
        self._edge_label_map = dict(self._cursor.execute("SELECT label, id FROM edge_labels"))
        self._serializer.set_edge_label_map(self._edge_label_map)
    
    def set_secedge_labels(self, secedge_labels):
        self._cursor.executemany("INSERT INTO secedge_labels (id, label, description) VALUES (?, ?, ?)", 
                                 ((idx, e[0], e[1]) for idx, e in enumerate(secedge_labels.iteritems())))
        self._secedge_label_map = dict(self._cursor.execute("SELECT label, id FROM secedge_labels"))
        self._serializer.set_secedge_label_map(self._secedge_label_map)
    
    def _create_feature_value_index(self, feature_name):
        feature_name = str(feature_name)
        assert feature_name.isalpha()
        
        self._cursor.execute(_Tables.FEATURE_IIDX_TEMPLATE % (feature_name,))
        
        
        self._feature_iidx_stmts[feature_name] = "INSERT INTO feature_iidx_%s (node_id, value_id) VALUES (?, ?)" % (feature_name,)
    
    def get_terminal_index_data(self, node, node_ids):
        return (node_ids[node.id].to_int(), node.id, self._edge_label_map.get(node.edge_label, None),
                gorn2db(node.gorn_address), node.order)
    
    def get_nonterminal_index_data(self, node, node_ids):
        return (node_ids[node.id].to_int(), node.id, self._edge_label_map.get(node.edge_label, None),
                gorn2db(node.gorn_address),
                (CONTINUOUS if node.is_continuous else DISCONTINUOUS), 
                node.arity, node.token_arity, 
                node_ids[node.left_corner].to_int(), node_ids[node.right_corner].to_int(), 
                node.children_type)

    def _store_node_data(self, graph, node_ids):
        nonterminals, terminals = graph.compute_node_information()
        
        self._cursor.executemany("""INSERT INTO node_data 
        (id, xml_node_id, edge_label, gorn_address, continuity, arity, tokenarity, left_corner, right_corner, token_order) 
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)""",
                             (self.get_nonterminal_index_data(nt, node_ids) for nt in nonterminals))
        
        self._cursor.executemany("INSERT INTO node_data (id, xml_node_id, edge_label, gorn_address, token_order, continuity) VALUES (?, ?, ?, ?, ?, 0)",
                                 (self.get_terminal_index_data(t, node_ids) for t in terminals))
        
    
    def _index_feature_values(self, graph, node_ids):
        for node in graph:
            for feature_name, feature_value in node.features.iteritems():
                value_map, domain = self._feature_value_maps[feature_name]
                assert node.TYPE is domain
                self._insert_lists[feature_name].append((node_ids[node.id].to_int(), value_map[feature_value]))
        
        if self._graphs % 1000 == 0:
            self._flush_node_feature_values()
            
    def _index_secedges(self, graph, node_ids):
        for node in graph:
            if node.secedges is not None:
                self._cursor.executemany(
                    "INSERT INTO secedges (origin_id, target_id, label_id) VALUES (?, ?, ?)",
                    ((node_ids[node.id].to_int(), node_ids[graph.nodes[target_node].id].to_int(), 
                      self._secedge_label_map[label])
                     for label, target_node in node.secedges))
                
    def _flush_node_feature_values(self):
        for feature_name, values in self._insert_lists.iteritems():
            self._cursor.executemany(self._feature_iidx_stmts[feature_name], values)
        self._insert_lists = defaultdict(list)

    def _convert_ids(self, graph, node_ids): # split out into separate method
        def _convert_edgelist(l):
            return [(label, node_ids[target_xml_id]) 
                    for label, target_xml_id in l]
      
        graph.id = self._graphs
        graph.root_id = node_ids[graph.root_id]
        for xml_node_id in graph.nodes.keys():
            node = graph.nodes.pop(xml_node_id)
            node.id = node_ids[node.id]
            graph.nodes[node_ids[xml_node_id]] = node
            if node.secedges:
                node.secedges = _convert_edgelist(node.secedges)
            if node.TYPE is NodeType.NONTERMINAL:
                node.edges = _convert_edgelist(node.edges)

    def add_graph(self, graph):
        try:
            roots = graph.get_roots()
        except KeyError, e:
            logging.error("Graph %s is faulty: node %s referenced more than once.",
                          graph.id, e.args[0])
            return

        if self._always_veeroot:
            veeroot_graph(graph, roots)
        else:
            assert len(roots) == 1, "No auto-veerooting, but several unconnected subgraphs %s in %s." % (roots, graph.id)
                
        node_ids = dict((xml_node_id, IndexNodeId(self._graphs, idx))
                        for idx, xml_node_id in enumerate(graph.nodes))
        
        xml_id = graph.id
        self._store_node_data(graph, node_ids)
        self._index_feature_values(graph, node_ids)
        self._index_secedges(graph, node_ids)
        
        self._convert_ids(graph, node_ids)
        
        self._cursor.execute("INSERT INTO graphs (id, xml_graph_id, data) VALUES (?, ?, ?)",
                             (self._graphs, xml_id, buffer(self._serializer.serialize_graph(graph))))
        self._graphs += 1
        if self._progress and self._graphs % 100 == 0:
            print self._graphs
    
    def finalize(self, optimize = True):
        if self._progress:
            print "finalize"
        self._flush_node_feature_values()
        
        if self._progress:
            print "inserting feature values"
        for feature_id, feature_value_map in self._open_list_features:
            self._cursor.executemany("INSERT INTO feature_values (feature_id, value_id, value) VALUES (?, ?, ?)",
                                     ((feature_id, value_id, value) 
                                      for value, value_id in feature_value_map.iteritems()))
        del self._open_list_features

        if self._progress:
            print "Committing database"
        self._db.commit()

        self._cursor.execute("CREATE INDEX feature_id_idx ON feature_values (feature_id)")
        
        for feature_name in self._feature_value_maps:
            if self._progress:
                print "creating index for feature '%s'" % (feature_name,)
            self._cursor.execute("CREATE INDEX %s_iidx_idx ON feature_iidx_%s (value_id)" % (feature_name, feature_name))
        
        if self._progress:
            print "creating index for xml node ids"
        self._cursor.execute("CREATE UNIQUE INDEX xml_node_id_idx ON node_data (xml_node_id)")
        
        if self._progress:
            print "creating index for xml graph ids"
        self._cursor.execute("CREATE UNIQUE INDEX xml_graph_id_idx ON graphs (xml_graph_id)")
        
        if self._progress:
            print "creating secedge indices"
        self._cursor.execute("CREATE INDEX se_origin_idx ON secedges (origin_id)")
        self._cursor.execute("CREATE INDEX se_target_idx ON secedges (target_id)")

        self._db.commit()
        
        if optimize:
            if self._progress:
                print "Optimizing database"
            self._db.execute("VACUUM")
        
        self._add_index_metadata(finished = True)
        self._db.commit()
        
        self._db = None
        self._cursor = None

########NEW FILE########
__FILENAME__ = ast
# -*- coding: utf-8 -*-
# Copyright © 2007-2008 Stockholm TreeAligner Project
# Author: Torsten Marek <shlomme@gmx.net>
# Licensed under the GNU GPLv2
"""This module defines all nodes in the abstract syntax trees (ASTs) of TIGERSearch queries.

AST node types
==============

AST node types can be checked using the attribute `TYPE`, i. e.

>>> n.TYPE is ast.Conjunction

In the current implementation, `n.TYPE` is a shorthand for `n.__class__`. However,
clients must not rely on the `TYPE` attribute being any particular object. The only
operations that can be done with this property is equality and identity checking.


Attributes
==========

The attributes of AST nodes are defined in the `__slot__` variable of the class
and set via introspection by the base class constructor. Please see the documentation
of the individual nodes for the semantics of the attributes.


Equality
========

Since all attributes are known, AST trees support equality checks, i.e.

>>> n == Conjunction([StringLiteral("a"), StringLiteral("b")])

works intuitively. Note however, that partial matching is not supported, i.e. both
trees need to be completely specified.


`__repr__`
=========
`__repr__` is implemented in such a way that the whole tree will be printed out
(not pretty-printed!). The usual contract

>>> query_ast == eval(repr(query_ast))

holds.


Iteration
=========

If a node is not a leaf node (i.e. when the method `is_leaf` returns `True`,
it supports two forms of iteration:

 * Standard iteration via `__iter__`
    All attributes of the node that are also nodes will be produced
 * Iteration with `named_iter`
    Along with the child nodes, the names of the child nodes will be returned.
    No assumptions may be made on the types of the names, they can only be used
    to replace an existing child node via `set_child`.
"""
from operator import attrgetter
from itertools import izip
from nltk_contrib.tiger.utils.enum import Enum, enum_member

# TODO: figure out how to support slots in class hierarchies. Currently, 
# the slot "children" from _Composite needs to be duplicated in Predicate.

class VariableTypes(Enum):
    """This enumeration defines all types of variables that can be defined
    in TIGERSearch queries.

    See the documentation of `nltk_contrib.tiger.query.tsqlparser` which parts of the
    grammar use which variable types.
    """
    FeatureValue = enum_member()
    FeatureDescription = enum_member()
    NodeIdentifier = enum_member()

    
class ContainerTypes(Enum):
    Single = enum_member()
    Set = enum_member()
    

# abstract node base classes
class _Node(object):
    """The abstract base class of all AST nodes.

    All *magic* features, like equality checks, `__repr__`, automatic
    setting of attributes, iteration and child replacemnt are implemented
    in this class.

    This class cannot be instantiated.

    *Attributes*: none
    """
    __slots__ = ()

    TYPE = property(lambda self: self.__class__)

    def __new__(cls, *args, **kwargs):
        if cls._is_abstract(cls.__name__):
            raise TypeError, "cannot instantiate abstract class '%s'." % (cls, )
        else:
            return object.__new__(cls)

    def __init__(self, *args):
        assert len(args) == len(self.__slots__), \
               (self.__class__.__name__, args, self.__slots__)
        for name, value in izip(self.__slots__, args):
            setattr(self, name, value)

    @staticmethod
    def _is_abstract(classname):
        """Returns `True` if a class is understood to be an abstract class.

        The names of abstract classes start with an underscore (``_``).
        """
        return classname.startswith("_")

    def __ne__(self, other):
        return not (self == other)

    def __eq__(self, other):
        if isinstance(other, _Node) and self.TYPE == other.TYPE:
            for name in self.__slots__:
                if getattr(self, name) != getattr(other, name):
                    return False
            return True
        else:
            return False

    def __repr__(self):
        return u"%s(%s)" % (self.__class__.__name__,
                            ",".join(repr(getattr(self, v)) for v in self.__slots__))

    def __iter__(self):
        for v in self.__slots__:
            obj = getattr(self, v)
            if isinstance(obj, _Node):
                yield obj

    def named_iter(self):
        """Returns a tuple `(name_tag, child_node)` for all child nodes."""
        for v in self.__slots__:
            obj = getattr(self, v)
            if isinstance(obj, _Node):
                yield v, obj

    @staticmethod
    def is_leaf():
        """Returns `True` if the node is a leaf node, `False` otherwise."""
        return False

    def set_child(self, name_tag, new_child):
        """Replaces a child node with another AST node `new_child`.

        Children are specified using `name_tag` returned by `named_iter`.

        `new_child` must be a subclass of `_Node`.
        """
        if self.is_leaf():
            raise TypeError, "cannot set children on leaf nodes"
        else:
            assert isinstance(getattr(self, name_tag), _Node)
            assert isinstance(new_child, _Node)
            setattr(self, name_tag, new_child)


class _LeafNode(_Node):
    """The base class for all leaf nodes."""

    @staticmethod
    def is_leaf():
        """Returns `True` if the node is a leaf node, `False` otherwise."""
        return True


class _CompositeNode(_Node):
    """The base class of all composite nodes, i.e. nodes with a possibly
    unbounded number of other children, using for grouping (logic operations etc).

    *Attributes*:
      - `children`: the child nodes
    """
    __slots__ = ("children", )

    def __init__(self, *args):
        assert len(args[-1]) > -1
        _Node.__init__(self, *args)

    def __iter__(self):
        return iter(self.children)

    def named_iter(self):
        """Returns a tuple `(name_tag, child_node)` for all child nodes."""
        return enumerate(self.children)

    def set_child(self, name, new_child):
        """Replaces a child node with another AST node `new_child`.

        Children are specified using `name_tag` returned by `named_iter`.

        `new_child` must be a subclass of `_Node`.
        """
        assert isinstance(new_child, _Node)
        self.children[name] = new_child

    def apply_associativity(self):
        """Applies the law of associativity to the composite node.

        After this operation, all the children of child nodes with the same
        type as this node will be inlined.

        Example:

        >>> n = Conjunction([StringLiteral("a"),
        ...                  Conjunction([StringLiteral("b"), StringLiteral("c")])])
        >>> n.apply_associativity()
        >>> n == Conjunction([StringLiteral("a"), StringLiteral("b"), StringLiteral("c")
        """
        nc = []
        for c in self.children:
            if self.TYPE is c.TYPE:
                nc.extend(c.children)
            else:
                nc.append(c)
        self.children = nc


class _NodeRelationOperator(_Node):
    """The base class for all node relation operators.

    *Attributes*:
      - `left_operand`: the node (in the TIGERSearch meaning) operand on the left side
      - `right_operand`: the node on the right right of the operator
      - `modifiers`: the operator modifiers, as specified by the grammar and the classes.
      
    *Special class attributes*:
      - `modifiers`: a dictionary of all modifiers with their default values
      - `converters`: a dictionary that contains converter functions for all modifiers
                      these converter functions are used in the `create` method. For the 
                      modifier `negated`, no converter has to be defined.
    """
    __slots__ = ("left_operand", "right_operand", "modifiers")
    __modifiers__ = {}

    def __init__(self, left_operand, right_operand, **kwargs):
        modifiers = self.__modifiers__.copy()
        modifiers.update(kwargs)

        _Node.__init__(self, left_operand, right_operand, modifiers)

    def __repr__(self):
        return u"%s(%s, %s, **%s)" % (self.__class__.__name__,
                                      self.left_operand, self.right_operand, self.modifiers)
    
    @classmethod
    def create(cls, parse_result):
        """Creates an operator AST node from a parse result."""
        cls.__converters__["negated"] = lambda o: o.negated != ""
        modifiers = dict(
            (name, cls.__converters__[name](parse_result.operator))
            for name in cls.__modifiers__)
            
        return cls(parse_result.leftOperand, parse_result.rightOperand, **modifiers)
        
    @staticmethod
    def _get_label(operator):
        """Returns the label modifier from an operator parse result, or `None` if not defined."""
        return None if len(operator.label) == 0 else operator.label

    @staticmethod
    def _get_range(operator):
        """Returns the range modifier from an operator parse result."""
        if operator.indirect == "*":
            return (1, )
        elif operator.mindist != "":
            return (int(operator.mindist), int(operator.maxdist or operator.mindist))
        else: 
            return (1, 1)


## Actual node classes

class Conjunction(_CompositeNode):
    """A class that represents a conjunction of terms.

    *Attributes*: see `_CompositeNode`
    """
    def __invert__(self):
        return Disjunction([Negation(c) for c in self])


class Disjunction(_CompositeNode):
    """A class that represents a disjunction of terms.

    *Attributes*: see `_CompositeNode`
    """
    def __invert__(self):
        return Conjunction([Negation(c) for c in self])


class PrecedenceOperator(_NodeRelationOperator):
    """A class that represents the precedence operator ``.``.

    *Attributes*: see `_NodeRelationOperator`
    :Modifiers:
     - `range`: a tuple, `(minimum, maximum)` reach of the precedence constraint
     - `negated`: if `True`, the constraint is negated
    """

    __modifiers__ = {
        "range" : (1, 1),
        "negated" : False }
    
    __converters__ = {
        "range": _NodeRelationOperator._get_range,
    }
    

class DominanceOperator(_NodeRelationOperator):
    """A class that represents the dominance operator ``>``.

    Note that corner dominance is handled by a separate operator.

    *Attributes*: see `_NodeRelationOperator`
    :Modifiers:
     - `range`: a tuple, `(minimum, maximum)` reach of the dominance constraint.
     - `label`: the label string for labeled dominance, or `None`
     - `negated`: if `True`, the constraint is negated
    """
    __modifiers__ = {
        "range" : (1, 1),
        "label" : None,
        "negated" : False }
    __converters__ = {
        "range": _NodeRelationOperator._get_range,
        "label": _NodeRelationOperator._get_label
    }
    

class CornerOperator(_NodeRelationOperator):
    """A class that represents the corner dominance operator ``>@``.

    *Attributes*: see `_NodeRelationOperator`
    :Modifiers:
     - `corner`: ``"r"`` or ``"l"``, depending on the specified corner
     - `negated`: if `True`, the constraint is negated
    """
    __modifiers__ = {
        "corner" : "",
        "negated": False }
    
    __converters__ = {
        "corner": attrgetter("corner")
    }
    

class SecEdgeOperator(_NodeRelationOperator):
    """A class that represents the secondary edge dominance operator ``>~``.

    *Attributes*: see `_NodeRelationOperator`
    :Modifiers:
     - `label`: the label string for labeled secondary edge dominance, or `None`
     - `negated`: if `True`, the constraint is negated
    """
    __modifiers__ = {
        "label" : None,
        "negated" : False }
    
    __converters__ = {
        "label": _NodeRelationOperator._get_label
    }


class SiblingOperator(_NodeRelationOperator):
    """A class that represents the sibling operator operator ``$``.

    *Attributes*: see `_NodeRelationOperator`
    :Modifiers:
     - `ordered`: if `True`, the left operand should precede the right one
     - `negated`: if `True`, the constraint is negated
    """
    __modifiers__ = {
        "negated" : False,
        "ordered" : False }

    __converters__ = {
        "ordered": lambda o: o.ordered != "",
    }


class Negation(_Node):
    """A class that represents the negation of a term.

    *Attributes*:
     - 'expression': the negated expression
    """
    __slots__ = ("expression", )


    def __invert__(self):
        return self.expression


class NodeDescription(_Node):
    """A class that represents a node description.

    Please see the grammar for the child nodes that can be contained
    in a node description.

    *Attributes*:
     - `expression`: the node description expression, a boolean expression
                     of feature constraints
    """
    __slots__ = ("expression", )


class VariableDefinition(_Node):
    """A class that represents a variable definition.

    *Attributes*:
     - `variable`: the variable, a `Variable` node
     - `expression`: the referent of the variable, a node description, feature value or constraint
    """
    __slots__ = ("variable", "expression")

    
class TsqlExpression(_Node):
    """The toplevel root node for TIGERSearch queries.

    *Attributes*:
     - `expression`: the query, either a single term or a conjunction of terms
    """
    __slots__ = ("expression", )

    
class Predicate(_CompositeNode):
    """A class that represents a node predicate.

    Currently, the only argument types allowed by the grammer are nodes or
    integer literals. This may change in the future.

    The function `apply_associativity` doesn't do anything useful for this node,
    because predicates cannot be nested.
    
    *Attributes*:
     - `name`: the name of the predicate
     - `children`: the list of arguments, a list of AST nodes
    """
    __slots__ = ("name", "children")


class FeatureConstraint(_Node):
    """A class that represents a featur constraint.

    *Attributes*:
     - `feature`: the feature name
     - `expression`: a boolean expression for the feature value
    """
    __slots__ = ("feature", "expression")


class StringLiteral(_LeafNode):
    """A class that represents a string literal.

    *Attributes*:
     - `string`: the string literal
    """
    __slots__ = ("string", )


class RegexLiteral(_LeafNode):
    """A class that represents a regex literal.

    *Attributes*:
     - `regex`: the regular expression
    """
    __slots__ = ("regex", )


class IntegerLiteral(_LeafNode):
    """A class that represents an integer literal.

    *Attributes*:
     - `value`: the integer value
    """
    __slots__ = ("value", )


class FeatureRecord(_LeafNode):
    """A class that represents a feature record.

    *Attributes*:
     - `type`: a member of `nltk_contrib.tiger.graph.NodeType`
    """
    __slots__ = ("type",)

    def __invert__(self):
        return FeatureRecord(~self.type)


class Variable(_LeafNode):
    """A class that represents a variable.

    *Attributes*:
     - `name`: the variable name
     - `type`: the variable type, an enum value of `VariableTypes`
     - `container`: the container type, an enum value of `ContainerTypes`
    """
    __slots__ = ("name", "type", "container")


class VariableReference(_LeafNode):
    """A class that represents a variable reference.

    *Attributes*:
     - `variable`: the actual variable
    """
    __slots__ = ("variable",)


class Nop(_LeafNode):
    """A node that represents an empty tree."""

########NEW FILE########
__FILENAME__ = ast_utils
# -*- coding: utf-8 -*-
# Copyright © 2007-2008 Stockholm TreeAligner Project
# Author: Torsten Marek <shlomme@gmx.net>
# Licensed under the GNU GPLv2
"""Utility functions for the abstract syntax trees defined in `nltk_contrib.tiger.query.ast`.
"""
from functools import partial

from nltk_contrib.tiger.query import ast
from nltk_contrib.tiger.query.ast_visitor import AstVisitor, node_handler
from nltk_contrib.tiger.query.exceptions import UndefinedNameError

__all__ = ["create_varref", "create_vardef", "split_children", "to_dnf"]

def create_varref(name, var_type=ast.VariableTypes.NodeIdentifier, 
                  container_type=ast.ContainerTypes.Single):
    """Creates a new `VariableReference` to a variable `name`.
    
    *Parameters*:
     * `name`: the name of the variable.
     * `var_type`: the type of the variable, a member of `nltk_contrib.tiger.query.ast.VariableTypes`.
     * `container_type`: the container type of the variable, a member of 
         `nltk_contrib.tiger.query.ast.ContainerTypes`.
    """
    return ast.VariableReference(ast.Variable(name, var_type, container_type))



def create_vardef(name, expression, var_type=ast.VariableTypes.NodeIdentifier, 
                  container_type=ast.ContainerTypes.Single):
    """Creates a new `VariableDefinition` of a variable `name` and a RHS `expression`.
    
    *Parameters*:
     * `name`: the name of the variable
     * `expression`: the expression which is assigned to the variable
     * `var_type`: the type of the variable, a member of `nltk_contrib.tiger.query.ast.VariableTypes`
     * `container_type`: the container type, a member of `nltk_contrib.tiger.query.ast.ContainerTypes`
    """
    return ast.VariableDefinition(ast.Variable(name, var_type, container_type), expression)


def split_children(ast_node, ast_type):
    """Sorts the children of `ast_node` into two lists, based on their type.
    
    *Parameters*:
     * `ast_node`: the AST node whose children should be sorted
     * `ast_type`: the type all children in the first list should have.
     
    *Return Value*:
    A tuple `(a, b)` of lists, `a` contains all child nodes with type `ast_type`, `b` the rest.
    """
    match, nomatch = [], []
    for node in ast_node:
        if node.TYPE is ast_type:
            match.append(node)
        else:
            nomatch.append(node)
    return match, nomatch


class NegNormalizer(AstVisitor):
    """An AST visitor that normalizes all negations in a boolean expression.
    
    The visitor applies the following transformations:
     - elimination of double negation: ``!!a => a``
     - De Morgan's laws:
       * ``!(a | b) => !a & !b``
       * ``!(a & b) => !a | !b``
       
    After the transformation, `Negation` nodes will only contain atoms, not other terms.
    
    *Parameters*:
     * `feature_types`: a dictionary that maps feature names to `NodeType`s, needed for negation
       `FeatureRecords`.
    """
    def __init__(self, feature_types):
        super(self.__class__, self).__init__()
        self._feature_types = feature_types
    
    @node_handler(ast.Negation)
    def normalize_neg(self, child_node):
        """Normalizes a negation if is reigns over something other than a term."""
        if child_node.expression.TYPE is ast.FeatureConstraint:
            # cf. TIGERSearch Query Language, section 8.4 (http://tinyurl.com/2jm24u)
            # !(pos="ART") === !(T & pos="ART") === !(T) | (pos != "ART")
            try:
                orig_type = self._feature_types[child_node.expression.feature]
            except KeyError, e:
                raise UndefinedNameError, (UndefinedNameError.FEATURE, e.args[0])
            return self.REPLACE(
                ast.Disjunction([
                    ast.FeatureRecord(~orig_type),
                    ast.FeatureConstraint(
                        child_node.expression.feature,
                        ast.Negation(child_node.expression.expression))]))
        else:
            try:
                return self.REPLACE(~child_node.expression)
            except TypeError:
                return self.CONTINUE(child_node)


def outer_product(l):
    """Produces the outer product of a list of lists.
    
    Example:
    >>> l = [[1, 2], [3, 4]]
    >>> outer_product(l) == [[1, 3], [1, 4], [2, 3], [2, 4]]
    
    If the number of lists is fixed, it is better to use a list comprehension
    >>> [(e1, e2) for e1 in l1 for e2 in l2]
    """
    def _expand(l, pos, result):
        """Recurses on `l` and produces all combinations of elements from the lists."""
        for e in l[pos]:
            result.append(e)
            if pos == len(l) - 1:
                yield result[:]
            else:
                for r in _expand(l, pos + 1, result):
                    yield r
            result.pop()
    return _expand(l, 0, [])


def _distribute(top_conjunction):
    """Applies the law of distributivity to a conjunction that has disjunctions.
    
    Example:
    ``(d1 | d2) & d3 => (d1 & d3) | (d2 & d3)``
    
    The function also applies associatity laws, it will never produce
    nested expressions with the same operators.
    """
    disj, terms = split_children(top_conjunction, ast.Disjunction)
    
    terms = [
        ast.Conjunction(t + combination)
        for t in [terms]
        for combination in outer_product(disj)
    ]
    
    for t in terms:
        t.apply_associativity()
        
    return ast.Disjunction(terms)


def distribute_disjunctions(tree):
    """Distributes all disjunctions in `tree` and also applies the law of associativity.
    
    Example:
    ``A & (B & (C | D) | E) => (A & B & C) | (A & B & D) | (A & E)``
    """
    has_disj = False
    for child_name, child_node in tree.named_iter():
        if child_node.TYPE in (ast.Conjunction, ast.Disjunction):
            new = distribute_disjunctions(child_node)
            if new.TYPE is ast.Disjunction:
                has_disj = True
            
            new.apply_associativity()
            tree.set_child(child_name, new)

    if tree.TYPE is ast.Conjunction and has_disj:
        return _distribute(tree)
    else:
        return tree
    

def to_dnf(tree, feature_types):
    """Converts a boolean expression into Disjunctive Normal Form.

    After the transformation, the boolean expression will be a disjunction of terms, negation will
    be normalized as well, and associativity laws have been applied.
    
    Note that the `tree` node itself will not be considered to be a part of the boolean expression.
    
    *Parameters*:
     * `feature_types`: a dictionary that maps feature names to `NodeType`s, needed for negation of
       `FeatureRecords`.
    """
    n = NegNormalizer(feature_types)
    n.run(tree)
    return distribute_disjunctions(tree)


class NodeDescriptionNormalizer(AstVisitor):
    """Normalizes a node description.
    
    After normalization, a node description is in disjunctive normal form, 
    and no feature constraint expression contains any disjunctions.
    """
    
    def __init__(self, feature_types):
        super(self.__class__, self).__init__()
        self._feature_types = feature_types
        
    @node_handler(ast.FeatureConstraint)
    def distribute(self, child_node):
        """Normalizes feature constraint expressions.
        
        A feature constraint with disjunctions is turned into a disjunction of feature
        constraints.
        """
        to_dnf(child_node, self._feature_types)
        if child_node.expression.TYPE is ast.Disjunction:
            fc = partial(ast.FeatureConstraint, child_node.feature)
            return self.REPLACE(ast.Disjunction(
                [fc(term) for term in child_node.expression]))
        else:
            return self.CONTINUE(child_node)
    
    def result(self, query):
        """Normalizes the node description, after all feature constraints have been normalized."""
        to_dnf(query, self._feature_types)

########NEW FILE########
__FILENAME__ = ast_visitor
# -*- coding: utf-8 -*-
# Copyright © 2007-2008 Stockholm TreeAligner Project
# Author: Torsten Marek <shlomme@gmx.net>
# Licensed under the GNU GPLv2
"""Defines a class for traversing Abstract Syntax Trees as defined in `nltk_contrib.tiger.query.ast`.
"""

__all__ = ["node_handler", "post_child_handler", "AstVisitor"]

def node_handler(*node_types):
    """Sets a method as a handler for all AST node types list in `node_types`."""
    def _set_node_type(fun):
        """Stores the node types on the method to be used by the metaclass."""
        fun.node_types = node_types
        return fun
    return _set_node_type


def post_child_handler(node_type):
    """Sets a method as a handler for that is invoked for every child of an
    AST node of type `node_type`.
    
    The handler is invoked after the whole branch of a child has been processed."""
    def _set_post_child_handler(fun):
        """Stores the node type on the method to be used as a handler for further processing."""
        fun.post_child = node_type
        return fun
    return _set_post_child_handler


class AstVisitorType(type):
    """The meta class for AST visitors.
    
    During class creation, the meta class will look for methods annotated as node handlers
    and store them in a dictionary. The handler for node can be retrieved using the method
    `switch`.
    """
    def __new__(mcs, classname, bases, class_dict):
        switch = {}
        post_switch = {}
        for obj in class_dict.itervalues():
            for n in getattr(obj, "node_types", []):
                switch[n] = obj
            
            if hasattr(obj, "post_child"):
                post_switch[getattr(obj, "post_child")] = obj

        class_dict["switch"] = switch.get
        class_dict["post_switch"] = post_switch.get
        return type.__new__(mcs, classname, bases, class_dict)
    
    
class AstVisitor(object):
    """
    The base class for AST visitors.
    
    Example:
    >>> class SomeVisitor(AstVisitor):
    ...    @node_handler(ast.Negation):
    ...    def show_negations(self, node):
    ...        print node
    ...        return self.CONTINUE(node)
    
    This visitor will simply print all negation nodes and do nothing else.
    
    A handler can have the following return values defined on `AstVisitor`:
      `STOP`
        stops traversal for all nodes under the currently visited one
      `CONTINUE(n)`
       continues the traversal, uses `n` to get the child nodes. Usually, this will
       be the visitied node
      `REPLACE(n)`
       continues the traversal using `n`, but also replaces the visited node by `n`.
    
       
    The traversal will never visit the root node via a method annotated with `@node_handler`. The 
    root node of the tree cannot be replaced [#]_, and only the methods `setup` and `result` have 
    access to it.
    
    .. [#] Changing the visitor code to allow this is easy, but the use case has not come up yet.
    """
    __metaclass__ = AstVisitorType

    STOP = (0, None)
    CONTINUE = lambda s, n: (1, n)
    REPLACE = lambda s, n: (2, n)
    
    def switch(self, ast_node_type, default_handler): # reassigned by meta-class 
        """Returns the handler for `ast_node_type`, or the `default_handler`."""
        pass
    
    def post_switch(self, ast_node_type, default_handler): # reassigned by meta-class
        """Returns the handler for `ast_node_type`, or the `default_handler`."""
        pass
    
    def default(self, child_node):
        """The default handler.
        
        Returns `STOP` for leaf nodes, and `CONTINUE(child_node)` for everything else.
        
        May be reimplemented by clients.
        """
        return self.STOP if child_node.is_leaf() else self.CONTINUE(child_node)
    
    def post_default(self, parent_node, child_name):
        """The default post-handler.
        
        Does nothing by default, may be reimplemented by clients.
        """
        pass
    
    def setup(self, query, *args, **kwargs):
        """Setup method, called before the tree is traversed.
        
        Any arguments passed into the `run` method will be passed to this method as well.
        
        Does nothing by default, can be implemented by clients.
        """
        pass
    
    def result(self, query, *args, **kwargs):
        """Teardown/result processing methods, called after the tree is traversed.

        Any arguments passed into the `run` method will be passed to this method as well.
        
        The return value of the `result` method is also the return value of `run`.

        Does nothing by default, can be implemented by clients.
        """
        pass
    
    def call_handler(self, node):
        """Calls the handler method for `node`.
        
        Convenience method to be used by subclasses.
        """
        r = self.switch(node.TYPE, self.__class__.default)(self, node)
        assert len(r) == 2, "Wrong return value '%s' from %s handler." % (r, node.TYPE, )
        return r
    
    def _process(self, node, child_name, child_node):
        """Processes the node `child_node`.
        
        Calls the type handler and acts on the result.
        """
        action, new_child = self.call_handler(child_node)
        
        if action == 2:
            node.set_child(child_name, new_child)
            self._process(node, child_name, new_child)
        elif action == 1:
            self._traverse(new_child)
        
        self.post_switch(node.TYPE, self.__class__.post_default)(self, node, child_name)

    def _traverse(self, branch):
        """Calls `_process` for all child nodes of a given node."""
        for child_name, child_node in branch.named_iter():
            self._process(branch, child_name, child_node)
            
    def run(self, query, *args, **kwargs):
        """Visits the tree rooted at `query`.
        
        All arguments will be passed to `setup` and `result`.
        
        Returns the value returned by `result`.
        """
        self.setup(query, *args, **kwargs)
        self._traverse(query)        
        return self.result(query, *args, **kwargs)

########NEW FILE########
__FILENAME__ = constraints
# -*- coding: utf-8 -*-
# Copyright © 2007-2008 Stockholm TreeAligner Project
# Author: Torsten Marek <shlomme@gmx.net>
# Licensed under the GNU GPLv2
"""Contains the classes for constraint checking.

The code and the interfaces of this module are still subject to change. Please refer to the
inline comments for more information.
"""
from __future__ import with_statement

from nltk_contrib.tiger.query.exceptions import UndefinedNameError
from nltk_contrib.tiger.graph import NodeType
from nltk_contrib.tiger.index import ID, EDGE_LABEL, CONTINUITY, LEFT_CORNER, RIGHT_CORNER, TOKEN_ORDER, GORN_ADDRESS
from nltk_contrib.tiger.query import ast
from nltk_contrib.tiger.utils.enum import Enum, enum_member
from nltk_contrib.tiger.utils.factory import FactoryBase

DEFAULT_TYPES = (NodeType.UNKNOWN, NodeType.UNKNOWN)

# WARNING: This module is still subject to heavy change!
# FIXME: while the code is correct (well, the tests run through), it's 
#        also quite convoluted. There should be an easier way to do this,
#        ideally one that allows to combine constraints over the same 
#        pair of nodes.

# This is also the reason why this code is not properly documented.
# the short story:
# A constraint must implement the class method setup_context, which will 
# be called exactly once for each corpus, and may only be used for setting
# data in the evaluator context, which will also be handed into the
# from_op method.

# The plan is to rewrite this that each constraint takes a node and a list of nodes
# and returns those nodes that fulfill the constraint. This way, constraints
# can take advantage of natural ordering in some cases


# For evaluation, the check method will be called. For speed reasons,
# the check methods should not contain any branches.

class Direction(Enum):
    LEFT_TO_RIGHT = enum_member()
    RIGHT_TO_LEFT = enum_member()
    NONE = enum_member()
    BOTH = enum_member()
    

class Constraint(object):
    __converters__ = {}

    
    @classmethod
    def setup_context(cls, context):
        pass

    @classmethod
    def from_op(cls, op_node, var_types, ctx):
        kwargs = {}
        for modifier_name in cls.__attributes__:
            conv = cls.__converters__.get(modifier_name, lambda *x: x[0])
            kwargs[modifier_name] = conv(op_node.modifiers[modifier_name], cls, ctx)
            kwargs["types"] = var_types
        return cls(**kwargs)
    
    def __init__(self, types, *args):
        self._types = types
        self._modifiers = args
        
    def __ne__(self, other):
        return not self.__eq__(other)

    def __eq__(self, other):
        return self.__class__ is other.__class__ and \
               self._modifiers == other._modifiers
    
    def get_complement(self):
        assert self.__attributes__[-1] == "negated"
        args = list(self._modifiers)
        args[-1] = not args[-1]
        return self.__class__(self._types, *args)
    
    def __repr__(self):
        return "%s(%s)" % (self.__class__.__name__, ", ".join(str(x) for x in self._modifiers))

    def get_predicates(self, left, right):
        return [], []
    
    def get_node_variable_types(self):
        return NodeType.UNKNOWN, NodeType.UNKNOWN
    
    def get_singlematch_direction(self):
        return Direction.NONE


class PrecedenceConstraint(Constraint):
    __attributes__ = ("range", "negated")
    
    def __init__(self, types = DEFAULT_TYPES, range = (1, 1), negated = False):
        super(PrecedenceConstraint, self).__init__(types, range, negated)
        self._negated = not negated
        self._direction = Direction.NONE
        
        if range == (1, 1):
            if types == (NodeType.TERMINAL, NodeType.TERMINAL):
                self.check = self.check_immedidate_tt
                if self._negated:
                    self._direction = Direction.BOTH
            else:
                self.check = self.check_immediate

        elif range == (1, ):
            self.check = self.check_general
        else:
            self._min, self._max = range
            self.check = self.ranged_check

    def ranged_check(self, left_op, right_op, qc):
        l = left_op if left_op[CONTINUITY] == 0 else qc.get_node(left_op[LEFT_CORNER])
        r = right_op if right_op[CONTINUITY] == 0 else qc.get_node(right_op[LEFT_CORNER])
        return (self._min <= r[TOKEN_ORDER] - l[TOKEN_ORDER] <= self._max) is self._negated

    def check_general(self, left_op, right_op, qc):
        l = left_op if left_op[CONTINUITY] == 0 else qc.get_node(left_op[LEFT_CORNER])
        r = right_op if right_op[CONTINUITY] == 0 else qc.get_node(right_op[LEFT_CORNER])
        return (l[TOKEN_ORDER] < r[TOKEN_ORDER]) is self._negated
    
    def check_immediate(self, left_op, right_op, qc):
        l = left_op if left_op[CONTINUITY] == 0 else qc.get_node(left_op[LEFT_CORNER])
        r = right_op if right_op[CONTINUITY] == 0 else qc.get_node(right_op[LEFT_CORNER])
        return (l[TOKEN_ORDER] == r[TOKEN_ORDER] - 1) is self._negated
    
    def check_immedidate_tt(self, left_op, right_op, qc):
        return (left_op[TOKEN_ORDER] == right_op[TOKEN_ORDER] - 1) is self._negated

    def get_singlematch_direction(self):
        return self._direction
    

class SiblingConstraint(Constraint):
    __attributes__ = ("ordered", "negated")

    def __init__(self, types = DEFAULT_TYPES, ordered = False, negated = False):
        assert not (negated and ordered)
        super(SiblingConstraint, self).__init__(types, ordered, negated)
        if ordered:
            self.check = self.check_ordered
        elif negated:
            self.check = self.check_negated
        else:
            self.check = self.check_normal
    
    def check_negated(self, left_op, right_op, qc):
        return len(left_op[GORN_ADDRESS]) != len(right_op[GORN_ADDRESS]) \
               or left_op[GORN_ADDRESS][:-1] != right_op[GORN_ADDRESS][:-1]

    def check_normal(self, left_op, right_op, qc):
        return len(left_op[GORN_ADDRESS]) == len(right_op[GORN_ADDRESS]) \
               and left_op[GORN_ADDRESS][:-1] == right_op[GORN_ADDRESS][:-1]

    def check_ordered(self, left_op, right_op, qc):
        if len(left_op[GORN_ADDRESS]) == len(right_op[GORN_ADDRESS]) \
           and left_op[GORN_ADDRESS][:-1] == right_op[GORN_ADDRESS][:-1]:
            l = left_op if left_op[CONTINUITY] == 0 else qc.get_node(left_op[LEFT_CORNER])
            r = right_op if right_op[CONTINUITY] == 0 else qc.get_node(right_op[LEFT_CORNER])
            return l[TOKEN_ORDER] < r[TOKEN_ORDER]
        else:
            return False


def guarded(func, exc_type, new_exc_factory, *args, **kwargs):
    try:
        return func(*args, **kwargs)
    except exc_type, e:
        raise new_exc_factory(e)
    
from contextlib import contextmanager

@contextmanager
def convert_exception(exc_type, new_exc_type, args = lambda exc: exc.args):
    try:
        yield
    except exc_type, e:
        raise new_exc_type, args(e)
    
def _get_label_id(label, dct, domain):
    with convert_exception(KeyError, UndefinedNameError, lambda exc: (domain, exc.args[0])):
        return dct[label]

class DominanceConstraint(Constraint):
    class ChildrenTypePredicate(object):
        def get_query_fragment(self):
            return "node_data.token_order > 1"
    
        def __eq__(self, other):
            return self.__class__ is other.__class__

        def __ne__(self, other):
            return not self.__eq__(other)
    
        FOR_NODE = True
    
    class EdgeLabelPredicate(object):
        def __init__(self, label_id):
            self._label_id = label_id
        
        def get_query_fragment(self):
            return "edge_label = %i" % (self._label_id)
        
        def __eq__(self, other):
            return self.__class__ == other.__class__ and self._label_id == other._label_id
        
        def __ne__(self, other):
            return not self.__eq__(other)
        
        FOR_NODE = True

    __attributes__ = ("label", "range", "negated")
    __converters__ = {
        "label": lambda l, cls, ctx: _get_label_id(l, ctx.edge_label_map, 
                                                   UndefinedNameError.EDGELABEL)
    }

    
    @classmethod
    def setup_context(cls, context):
        context.edge_label_map = dict(context.db.execute("SELECT label, id FROM edge_labels"))
        context.edge_label_map[None] = None

    def __init__(self, types = DEFAULT_TYPES, label = None, range = (1, 1), negated = False):
        super(DominanceConstraint, self).__init__(types, label, range, negated)
        self._lbl = label
        self._negated = not negated
        self._direction = Direction.NONE
        if range == (1, 1):
            if self._negated:
                self._direction = Direction.RIGHT_TO_LEFT
            if self._lbl is None:
                self.check = self.check_immediate
            else:
                self.check = self.check_immediate_lbl
                
        elif range == (1, ):
            assert label is None
            if negated:
                self.check = self.check_general_ngt
            else:
                self.check = self.check_general
        else:
            assert label is None
            self._min, self._max = range
            self.check = self.ranged_check
        
    def ranged_check(self, left_op, right_op, qc):
        l = len(left_op[GORN_ADDRESS])
        r = len(right_op[GORN_ADDRESS])
        return (self._min <= r - l <= self._max and 
                buffer(right_op[GORN_ADDRESS][:l]) == left_op[GORN_ADDRESS]) is self._negated

    def check_general_ngt(self, left_op, right_op, qc):
        l = len(left_op[GORN_ADDRESS])
        r = len(right_op[GORN_ADDRESS])
        return not (r - l > 0 and buffer(right_op[GORN_ADDRESS][:l]) == left_op[GORN_ADDRESS])

    def check_general(self, left_op, right_op, qc):
        l = len(left_op[GORN_ADDRESS])
        r = len(right_op[GORN_ADDRESS])
        return (r - l > 0 and buffer(right_op[GORN_ADDRESS][:l]) == left_op[GORN_ADDRESS])

    def check_immediate(self, left_op, right_op, qc):
        l = len(left_op[GORN_ADDRESS])
        r = len(right_op[GORN_ADDRESS])
        return (r - l == 1 and buffer(right_op[GORN_ADDRESS][:l]) == left_op[GORN_ADDRESS]) \
               is self._negated

    def check_immediate_lbl(self, left_op, right_op, qc):
        l = len(left_op[GORN_ADDRESS])
        r = len(right_op[GORN_ADDRESS])
        return (r - l == 1 and buffer(right_op[GORN_ADDRESS][:l]) == left_op[GORN_ADDRESS] \
                and right_op[EDGE_LABEL] == self._lbl) is self._negated

    def get_predicates(self, left, right):
        l = []
        if right.var_type is NodeType.NONTERMINAL:
            l.append(self.ChildrenTypePredicate())
        return l, [self.EdgeLabelPredicate(self._lbl)] if self._lbl is not None else []
            
    def get_node_variable_types(self):
        return NodeType.NONTERMINAL, NodeType.UNKNOWN
    
    def get_singlematch_direction(self):
        return self._direction


class SecEdgeConstraint(Constraint):
    class SecEdgePredicate(object):
        # secedges.label_id is not indexed, therefore it is cheaper to load 
        # all secedges and then check for the correct labels later
        ORIGIN = 0
        TARGET = 1
        _ID_NAMES = ["origin_id", "target_id"]
        
        def __init__(self, node):
            self._node = node
            
        def get_query_fragment(self):
            return "(SELECT COUNT(*) FROM secedges WHERE secedges.%s = node_data.id) > 0" % (
                self._ID_NAMES[self._node], )
        
        def __eq__(self, other):
            return self.__class__ == other.__class__ and self._node == other._node
        
        def __ne__(self, other):
            return not self.__eq__(other)
        
        FOR_NODE = True
        
    __attributes__ = ("label", "negated")
    __converters__ = {
        "label": lambda l, cls, ctx: _get_label_id(l, ctx.secedge_label_map, 
                                                   UndefinedNameError.SECEDGELABEL)
    }
    
    @classmethod
    def setup_context(cls, ev_context):
        ev_context.secedge_label_map = \
                  dict(ev_context.db.execute("SELECT label, id FROM secedge_labels"))
        ev_context.secedge_label_map[None] = None

    def __init__(self, types = DEFAULT_TYPES, label = None, negated = False):
        super(SecEdgeConstraint, self).__init__(types, label, negated)
        self._lbl = label
        self._neg = negated

    def check(self, left_op, right_op, query_context):
        if self._lbl is not None:
            query_context.cursor.execute(
                """SELECT origin_id FROM secedges
                WHERE origin_id = ? AND target_id = ? AND label_id = ?""",
                (left_op[ID], right_op[ID], self._lbl))

        else:
            query_context.cursor.execute("""SELECT origin_id FROM secedges
            WHERE origin_id = ? AND target_id = ?""", (left_op[ID], right_op[ID]))
        return (query_context.cursor.fetchone() is None) is self._neg

    def get_predicates(self, left, right):
        return ([self.SecEdgePredicate(self.SecEdgePredicate.ORIGIN)],
                [self.SecEdgePredicate(self.SecEdgePredicate.TARGET)])


class CornerConstraint(Constraint):
    _IDX = {"l": LEFT_CORNER,
            "r": RIGHT_CORNER}

    __attributes__ = ("corner", "negated")
    __converters__ = {
        "corner": lambda c, cls, ctx: cls._IDX[c]
    }

    def __init__(self, types, corner, negated = False):
        super(CornerConstraint, self).__init__(types, corner, negated)
        if negated:
            self.check = lambda l, r, qc: l[corner] != r[ID] and not (r[ID] == l[ID] and \
                                                                      l[CONTINUITY] == 0)
        else:
            self.check = lambda l, r, qc: l[corner] == r[ID] or r[ID] == l[ID] and\
                l[CONTINUITY] == 0

    def get_node_variable_types(self):
        return NodeType.UNKNOWN, NodeType.TERMINAL


class ConstraintFactory(FactoryBase):
    __classes__ = {
        ast.SiblingOperator: SiblingConstraint,
        ast.PrecedenceOperator: PrecedenceConstraint,
        ast.CornerOperator: CornerConstraint,
        ast.SecEdgeOperator: SecEdgeConstraint,
        ast.DominanceOperator: DominanceConstraint
        }

    def _create_instance(self, cls, op_node, var_types, context):
        return cls.from_op(op_node, var_types, context)

    def _get_switch(self, op_node, var_types, context):
        return op_node.TYPE


########NEW FILE########
__FILENAME__ = evaluator
# -*- coding: utf-8 -*-
# Copyright © 2007-2008 Stockholm TreeAligner Project
# Author: Torsten Marek <shlomme@gmx.net>
# Licensed under the GNU GPLv2
"""This module contains high-level classes for evaluating TigerSearch queries.
"""
from nltk_contrib.tiger.query.factory import QueryFactory
from nltk_contrib.tiger.query.nodesearcher import NodeSearcher
from nltk_contrib.tiger.query.querybuilder import QueryBuilder
from nltk_contrib.tiger.query.result import ResultBuilder, ParallelResultBuilder
from nltk_contrib.tiger.query.tsqlparser import TsqlParser
from nltk_contrib.tiger.utils.parallel import use_parallel_processing

__all__ = ["TsqlQueryEvaluator"]

class TsqlQueryEvaluator(object):
    """The main class for preparing and evaluating TigerSearch queries.
    
    A façade class that pulls together the parser and the query builder.
    """
    
    class Context(object):
        """The evaluator context.
        
        Each query evaluator has one context, which is used during query
        preparation.
        """

        def __init__(self, db, db_provider, corpus_info):
            self.db = db
            self.db_provider = db_provider
            self._nodesearcher = None
            self.allow_parallel = True
            self.corpus_info = corpus_info
        
        def _use_parallel(self):
            return self.db_provider.can_reconnect() and use_parallel_processing and self.allow_parallel
        
        def get_result_builder_class(self, has_constraints):
            return ParallelResultBuilder if self._use_parallel() and has_constraints \
                   else ResultBuilder

        @property
        def nodesearcher(self):
            if self._nodesearcher is None:
                self._nodesearcher = NodeSearcher(self.db)
            return self._nodesearcher
                
        
    def __init__(self, db, db_provider, corpus_info):
        self._context = TsqlQueryEvaluator.Context(db, db_provider, corpus_info)
        self._parser = TsqlParser()
        self._query_factory = QueryFactory(self._context)

    def new_builder(self):
        """Returns a new query builder.
        
        Query builders can be used to define a TIGERSearch query programmatically rather
        than having to create a query string.
        """
        return QueryBuilder(self._query_factory)
    
    def set_allow_parallel(self, value):
        """Allow or prohibit use of parallelized processing for the query evaluator."""
        self._context.allow_parallel = value
        
    def prepare_query(self, query_str):
        """Creates a query object from a TIGER query string."""
        return self._query_factory.from_ast(
            self._parser.parse_query(query_str))
    
    def evaluate(self, query_str):
        """Immediately evaluates a TIGER query query."""
        return self.prepare_query(query_str).evaluate()

########NEW FILE########
__FILENAME__ = exceptions
# -*- coding: utf-8 -*-
# Copyright © 2007-2008 Stockholm TreeAligner Project
# Author: Torsten Marek <shlomme@gmx.net>
# Licensed under the GNU GPLv2
"""Query module exceptions

This module contains all exceptions that are thrown by the query evaluation engine.

All queries are subclasses of `TigerQueryError`, any other exception not caught by the 
evaluator methods are a bug.
"""

__all__ = ["TigerSyntaxError", "TigerQueryError", "ConflictError",
           "TigerTypeError", "PredicateTypeError", "MissingFeatureError"]

class TigerQueryError(Exception):
    """The base class of all TIGER query errors."""
    pass


class TigerSyntaxError(TigerQueryError):
    """An exception for query language syntax errors.
    
    Parameters:
     * `parse_exc`: the causing exception, a `pyparsing.ParseException`
    
    Attributes:
     * `cause`: the causing exception
    """
    def __init__(self, parse_exc):
        TigerQueryError.__init__(self)
        self.cause = parse_exc

    def __str__(self):
        return str(self.cause)
    

class UndefinedNameError(TigerQueryError):
    """An exception thrown on usage of an undefined name in a query.
    
    Parameters:
     * `name_domain`: the domain of the undefined name (see below)
     * `name`: the actual name
     
    Name domains:
     * features
     * predicates
     * edge labels
     * secondary edge labels
    """
    MSG = "Undefined %s '%s'."
    FEATURE = "feature"
    PREDICATE = "predicate"
    EDGELABEL = "edge label"
    SECEDGELABEL = "secondary edge label"

    def __init__(self, name_domain, name):
        TigerQueryError.__init__(
            self,
            self.MSG % (name_domain, name))
        
        
class ConflictError(TigerQueryError):
    """An exception for conflicting feature constraints.
    
    Parameters:
     * `variable_name`: the name of the variable where the conflict occurred
     
    Example: ``[cat="(NP & PP)"]``
    """
    pass


class TigerTypeError(TigerQueryError):
    """An exception for wrong types in node variables.
    
    Parameters:
     * `variable_name`: the name of the variable where the error occurred
     
    Example: ``[T & NT]``
    """
    def __init__(self, variable_name):
        TigerQueryError.__init__(
            self,
            "Conflicting type for node variable '%s'." % (variable_name, ))
        self.varname = variable_name


class PredicateTypeError(TigerQueryError):
    """An exception for type errors in predicate definitions.
    
    Example: ``arity(#a)``
    """
    pass


class MissingFeatureError(TigerQueryError):
    """An exception for signaling missing features.
    
    Exceptions of this type are feature requests.
    """
    pass

########NEW FILE########
__FILENAME__ = factory
# -*- coding: utf-8 -*-
# Copyright © 2007-2008 Stockholm TreeAligner Project
# Author: Torsten Marek <shlomme@gmx.net>
# Licensed under the GNU GPLv2

"""This module contains classes to create a result builder from a query AST.
"""
from collections import defaultdict
from itertools import count

from nltk_contrib.tiger.graph import NodeType
from nltk_contrib.tiger.query import ast_visitor
from nltk_contrib.tiger.query.ast_utils import create_varref, NodeDescriptionNormalizer
from nltk_contrib.tiger.query.node_variable import NodeVariable
from nltk_contrib.tiger.query import ast
from nltk_contrib.tiger.query.predicates import PredicateFactory, NodeTypePredicate
from nltk_contrib.tiger.query.constraints import ConstraintFactory
from nltk_contrib.tiger.query.exceptions import TigerTypeError, UndefinedNameError

__all__ = ["QueryFactory"]


class NodeTypeInferencer(ast_visitor.AstVisitor):
    """An AST visitor that processes a node description and infers the type 
    of the node variable using the feature constraints.
    
    *Parameters*:
     * `terminal_features`: the set of features on T nodes
     * `nonterminal_features`: the set of features on NT nodes
    """
    def __init__(self, feature_types):
        super(self.__class__, self).__init__()
        self._type_assoc = feature_types
        
    def setup(self, *args):
        """Prepares the typer for a new variable."""
        self._types = set()
        self._disjoints = [self._types]
        self._has_frec = False

    @ast_visitor.post_child_handler(ast.Disjunction)
    def after_disjunction_subexpr(self, node, child_idx):
        """Handles disjunctions."""
        if child_idx < len(node.children) - 1:
            self._types = set()
            self._disjoints.append(self._types)
        
    @ast_visitor.node_handler(ast.FeatureConstraint)
    def feature_constraints(self, node):
        """Adds type information based on the feature name."""
        try:
            self._types.add(self._type_assoc[node.feature])
        except KeyError:
            raise UndefinedNameError, (UndefinedNameError.FEATURE, node.feature)
        return self.STOP
    
    @ast_visitor.node_handler(ast.FeatureRecord)
    def feature_record(self, node):
        """Adds the type specified in a feature record."""
        self._types.add(node.type)
        self._has_frec = True
        return self.STOP
    
    def result(self, query_ast, node_variable):
        """Returns the type inferred type of the node variable.
        
        The return value is a member of the enum `nltk_contrib.tiger.graph.NodeType`.
        
        If the feature refer to conflicting node types, a `TigerTypeError` is raised.
        """
        node_var_type = set()

        for disj in self._disjoints:
            if len(disj) == 2:
                raise TigerTypeError, node_variable.name
            else:
                node_var_type.update(disj)

        if len(node_var_type) == 1:
            return (list(node_var_type)[0], self._has_frec)
        else:
            return (NodeType.UNKNOWN, False)


class QueryFactory(ast_visitor.AstVisitor):
    """Creates the internal representation from a query AST.
    
    A query AST is split into three parts:
     * node descriptions: a dictionary of `varname: AST`
     * predicates: a dictionary with `varname: nltk_contrib.tiger.query.predicates.Predicate` entries
     * constraints: a list of ((left, right), nltk_contrib.tiger.query.constraints.Constraint)` tuples
    
    These collections will be used to instantiate a `Query` object.
    Anonymous node descriptions will be wrapped into a variable definition with an 
    automatically generated, globally unique variable name.
    """
    get_anon_nodevar = (":anon:%i" % (c, ) for c in count()).next
    constraint_factory = ConstraintFactory()
    predicate_factory = PredicateFactory()
    
    def __init__(self, ev_context):
        super(self.__class__, self).__init__()
        self.nodedesc_normalizer = NodeDescriptionNormalizer(ev_context.corpus_info.feature_types)
        self._ev_context = ev_context
        for cls in self.constraint_factory:
            cls.setup_context(self._ev_context)
        self._ntyper = NodeTypeInferencer(ev_context.corpus_info.feature_types)
        
    @ast_visitor.node_handler(ast.NodeDescription)
    def handle_node_description(self, child_node):
        """Replaces an anonymous node description with a reference to a fresh node variable.
        
        The node description is stored for later reference.
        """
        variable = NodeVariable(self.get_anon_nodevar(), False)
        self.node_defs[variable] = child_node
        self.node_vars[variable.name] = variable
        return self.REPLACE(create_varref(variable.name))
    
    @ast_visitor.node_handler(ast.VariableDefinition)
    def handle_node_variable_def(self, child_node):
        """Replaces a node variable definition with a reference, and stores it.
        
        If the variable has already been defined, the node descriptions are merged.
        """
        assert child_node.variable.type == ast.VariableTypes.NodeIdentifier
        node_variable = NodeVariable.from_node(child_node.variable)
        self.node_vars[child_node.variable.name] = node_variable
        
        if node_variable in self.node_defs:
            self.node_defs[node_variable] = ast.NodeDescription(
                ast.Conjunction([self.node_defs[node_variable].expression, 
                                 child_node.expression.expression]))
        else:
            self.node_defs[node_variable] = child_node.expression
        
        return self.REPLACE(create_varref(child_node.variable.name, 
                                          container_type = child_node.variable.container))
    
    @ast_visitor.node_handler(ast.Predicate)
    def handle_predicate(self, child_node):
        """Stores the predicate in the list of predicates."""
        self.predicates.append(child_node)
        return self.CONTINUE(child_node)
    
    @ast_visitor.node_handler(ast.SiblingOperator, 
               ast.CornerOperator,
               ast.DominanceOperator,
               ast.PrecedenceOperator,
               ast.SecEdgeOperator)
    def constraint_op(self, child_node):
        """Stores the constraint in the list of constraints."""
        self.constraints.append(child_node)
        return self.CONTINUE(child_node)
    
    def setup(self, query_ast):
        """Creates the collections for the internal representation of the query."""
        self.predicates = []
        self.node_defs = {}
        self.node_vars = {}
        self.constraints = []

    def _get_variable(self, variable):
        """Returns a node variable object associated with the AST fragment `variable`.
        
        If `variable` is seen the first time, a new node variable is created using
        `NodeVariable.from_node`.
        """
        try:
            return self.node_vars[variable.name]
        except KeyError:
            node_variable = self.node_vars[variable.name] = NodeVariable.from_node(variable)
            self.node_defs[node_variable] = ast.NodeDescription(ast.Nop())
            return node_variable
        
    def _process_predicates(self, predicates):
        """Creates the predicate objects.
        
        The predicate objects are created from the AST nodes using the `predicate_factory`.
        """
        for pred_ast_node in self.predicates:
            ast_var, predicate = self.predicate_factory.create(pred_ast_node)
            predicates[self._get_variable(ast_var)].append(predicate)
    
    def _process_constraints(self, predicates):
        """Creates the constraint objects.
        
        The constraints are created from the AST representations using the `constraint_factory`.
        """
        result = []
        for constraint_ast_node in self.constraints:
            left_var = self._get_variable(constraint_ast_node.left_operand.variable)
            right_var = self._get_variable(constraint_ast_node.right_operand.variable)
            
            constraint = self.constraint_factory.create(
                constraint_ast_node, (left_var.var_type, right_var.var_type), self._ev_context)
            
            result.append(((left_var, right_var), constraint))
            
            for node_var, var_type in zip((left_var, right_var), 
                                          constraint.get_node_variable_types()):
                node_var.refine_type(var_type)
        
        for (left_var, right_var), constraint in result:
            left_p, right_p = constraint.get_predicates(left_var, right_var)
            predicates[left_var].extend(left_p)
            predicates[right_var].extend(right_p)
            
        return result

    def _add_type_predicates(self, predicates):
        """Adds type predicates to the predicate lists if necessary.
        
        A type predicate is only added for a node variable if all of the following conditions
        are true:
         * the node description is empty
         * no predicates are defined for the variable
         * the variable type is not `NodeType.UNKNOWN`
        
        This mechanism is different from handling of feature records. The type predicate
        is added to each disjunct, while the feature record can differ between each disjunct.
        """
        for node_variable, description in self.node_defs.iteritems():
            if description.expression.TYPE is ast.Nop and len(predicates[node_variable]) == 0 \
                and node_variable.var_type is not NodeType.UNKNOWN:
                predicates[node_variable].append(NodeTypePredicate(node_variable.var_type))

    def from_ast(self, query_ast):
        """Convert a query AST into a result builder object.
        
        Query ASTs are in the same state as returned by the parser.
        
        The result builder class is injected using the `get_result_builder_class`
        on the evaluator context.
        """
        return self.run(query_ast)
    
    def result(self, query_ast):
        """Processes the collected items and returns the query object."""        
        predicates = defaultdict(list)
        
        for node_variable, node_desc in self.node_defs.iteritems():
            self.nodedesc_normalizer.run(node_desc)
            node_var_type, has_frec = self._ntyper.run(node_desc, node_variable)
            node_variable.refine_type(node_var_type)

            if has_frec:
                predicates[node_variable].append(NodeTypePredicate(node_var_type))
                
        
        self._process_predicates(predicates)
        constraints = self._process_constraints(predicates)
        self._add_type_predicates(predicates)

        return self._ev_context.get_result_builder_class(len(constraints) > 0)(
            self._ev_context, self.node_defs, predicates, constraints)

########NEW FILE########
__FILENAME__ = nodesearcher
# -*- coding: utf-8 -*-
# Copyright © 2007-2008 Stockholm TreeAligner Project
# Author: Torsten Marek <shlomme@gmx.net>
# Licensed under the GNU GPLv2
"""This module contains the class for searching nodes that fit a given node description.

"""
import os
import operator
import re

from itertools import count
from functools import partial

from nltk_contrib.tiger.graph import NodeType
from nltk_contrib.tiger.index import IndexNodeId
from nltk_contrib.tiger.query.node_variable import NodeVariable
from nltk_contrib.tiger.query import ast
from nltk_contrib.tiger.query.predicates import NodeTypePredicate
from nltk_contrib.tiger.query.ast_visitor import AstVisitor, node_handler, post_child_handler
from nltk_contrib.tiger.query.ast_utils import split_children
from nltk_contrib.tiger.query.exceptions import ConflictError

__all__ = ["NodeSearcher"]


class EmptyResultException(Exception):
    """Escaping exception for queries that are trivially empty."""
    pass


class NullGraphFilter(object):
    """Graph filter that does not filter anything."""
    def get_initial_filters(self, corpus_size):
        """Returns the initial graph filter.
        
        In this implementation, there are no filters.
        """
        return []


class EqualPartitionsGraphFilter(object):
    """A graph filter to split a corpus into N equally large parts.
    
    *Parameters*:
     * `this_part`: the ID of the current part, 0-based
     * `part_count`: the number of total parts
    """
    def __init__(self, this_part, part_count):
        self._this_part = this_part
        self._part_count = part_count

    def get_initial_filters(self, corpus_size):
        """Returns the initial graph filter.
        
        In this implementation, returns a filter that will select all graphs
        from a given range of graph IDs.
        """
        f = []
        
        part_size = corpus_size // self._part_count
        
        if self._this_part > 0:
            f.append("node_data.id >= %i" % (
                (self._this_part * part_size) << IndexNodeId.NODE_BIT_WIDTH, ))
        if self._this_part + 1 < self._part_count:
            f.append("node_data.id < %i" % (
                ((self._this_part + 1) * part_size) << IndexNodeId.NODE_BIT_WIDTH, ))
        return f
        
        
class NodeQueryCompiler(AstVisitor):
    """An AST visitor that takes a node query and compiles it into an SQL query."""
    class PaddingCursor(object):
        """A fake node cursor that returns a node for each graph in the corpus.
        
        Used to pad queries that only contain set variables.
        
        *Parameters*:
         * `length`: the size of the corpus
        """
        def __init__(self, length):
            self._length = length
            
        def __iter__(self):
            return ((0, graph_id) for graph_id in xrange(self._length))
        
        
    get_temp_table = ("_temp_regex_table_%i_%i" % (os.getpid(), c) for c in count()).next

    MATCH = True
    NO_MATCH = False
    SQL_OPERATORS = {MATCH: "=", NO_MATCH: "!="}

    def __init__(self, db, graph_filter):
        super(self.__class__, self).__init__()
        self._featureids = dict(
            db.execute("SELECT name, id FROM features"))

        self._feature_caches = dict(
            (name, {}) for name in self._featureids)
        
        self._db = db
        self.corpus_size = self._db.execute("SELECT COUNT(id) FROM graphs").fetchone()[0]
        self._temp_tables = {}
        self.current_query = []
        self._graph_filter = graph_filter
        
    @post_child_handler(ast.Disjunction)
    def after_disjunction_subexpr(self, node, child_name):
        """Creates a new query for a new part of a toplevel disjunction."""
        if child_name < len(node.children) - 1:
            self.current_query = []
            self.queries.append(self.current_query)
            self.types.append(None)
            
    @node_handler(ast.FeatureRecord)
    def handle_feature_record(self, child_node):
        if self._node_type is NodeType.UNKNOWN:
            self.types[-1] = child_node.type
        return self.STOP
    
    @node_handler(ast.FeatureConstraint)
    def handle_feature_constraint(self, child_node):
        """Handles a feature constraint and stores it for later conversion into SQL."""
        expr = child_node.expression
        if expr.TYPE is ast.Negation:
            self.current_query.append((child_node.feature, self.NO_MATCH, expr.expression))
        elif expr.TYPE is ast.Conjunction:
            self._add_conjunction_constraints(child_node.feature, expr)
        else:
            self.current_query.append((child_node.feature, self.MATCH, expr))
            
        return self.STOP

    def _add_conjunction_constraints(self, feature_name, expr):
        """Adds all literals from a conjunction to the list of feature constraints.
        
        Conflict checking is performed, expressions like "A"&"B" will lead to a 
        `ConflictError`.
        """
        negated, literals = split_children(expr, ast.Negation)
            
        has_explicit_match = False
        
        for literal in literals:
            if literal.TYPE is ast.StringLiteral:
                if has_explicit_match:
                    raise ConflictError, \
                          "Feature '%s' has two conflicting constraints." % (feature_name, )
                
                has_explicit_match = True

                self.current_query.append((feature_name, self.MATCH, literal))
            
        if not has_explicit_match:
            for literal in literals:
                self.current_query.append((feature_name, self.MATCH, literal))
                    
            for n in negated:
                self.current_query.append((feature_name, self.NO_MATCH, n.expression))
        
    def setup(self, query, node_type, predicates):
        """Setup ast visitor for a new run."""
        self._node_type = node_type
        self.current_query = []
        self.queries = [self.current_query]
        self.types = [None]
        
    def cleanup_temporary_tables(self):
        """Drops all temporary tables created for regex matches."""
        for table_name in self._temp_tables.itervalues():
            self._db.execute("DROP TABLE %s" % (table_name, ))
        self._temp_tables = {}
        
    def _get_feature_value_id(self, feature_name, feature_value):
        """Returns the value id of `feature_value` from the feature `feature_name`.
        
        Uses a cache internally."""
        try:
            return self._feature_caches[feature_name][feature_value]
        except KeyError:
            cur = self._db.cursor()
            cur.execute("""SELECT value_id FROM feature_values 
            WHERE feature_id = ? AND value = ?""", 
                        (self._featureids[feature_name], feature_value))
            try:
                value_id = cur.fetchone()[0]
            except TypeError:
                raise EmptyResultException
            self._feature_caches[feature_name][feature_value] = value_id
            return value_id

    def _create_new_regex_table(self, feature_name, match_policy, regex_string):
        """Creates a temporary table for evaluation of regular expression constraints.
        
        A new temporary table is created, containing all values of a given feature, matching
        a regular expression with a given match policy. The table contains a single column, 
        named `id`, which is also the primary key, and contains the feature value ids.
        
        *Parameters*:
         * `feature_name`: the feature for which to create the table
         * `match_policy`: NodeSearcher.MATCH or NodeSearch.NO_MATCH
         * `regex_string`: the regular expression string to use
        
        Returns the name of the temporary table that contains the values.
        """
        expected = bool if match_policy is self.MATCH else partial(operator.is_, None)
        
        if not regex_string.endswith("$"):
            regex_string += "$"
            
        match = re.compile(regex_string, re.UNICODE).match

        table_name = self.get_temp_table()
                
        cursor = self._db.cursor()
        cursor.execute("CREATE TEMPORARY TABLE %s (id INTEGER PRIMARY KEY)" % (table_name, ))
        self._db.create_function("CURR_REGEX", 1, lambda feature: expected(match(feature)))
        cursor.execute(
            """INSERT INTO %s (id) 
            SELECT value_id FROM feature_values 
            WHERE feature_id = ? AND CURR_REGEX(value)""" % (table_name, ),
                                                          (self._featureids[feature_name], ))
        cursor.close()
        if cursor.rowcount == 0:
            self._db.rollback()
            raise EmptyResultException
        else:
            self._db.commit()
            return table_name
        
    def _get_temp_regex_table(self, feature_name, match_policy, regex_string):
        """Returns the temporary table for reguluar expression constraints.
        
        Creates the table, if necessary.
        """
        temp_table_key = (feature_name, match_policy, regex_string)
        try:
            return self._temp_tables[temp_table_key]
        except KeyError:
            temp_table_name = self._create_new_regex_table(feature_name, match_policy, regex_string)
            return self._temp_tables.setdefault(temp_table_key, temp_table_name)
            
    def _create_single_select_stmt(self, feature_constraints, node_type, predicates):
        """Creates an SQL select statement from feature constraints and predicates.
        
        Handles only a single disjunct of a query."""
        joins = set()
        wheres = self._graph_filter.get_initial_filters(self.corpus_size)

        if len(feature_constraints) == 0 and node_type is not None:
            wheres.append("(%s)" % (NodeTypePredicate(node_type).get_query_fragment(), ))
        
        for name, match_policy, value in feature_constraints:
            joins.add("JOIN feature_iidx_%s ON feature_iidx_%s.node_id = node_data.id" % (
                name, name))
            
            if value.TYPE is ast.StringLiteral:
                wheres.append("(feature_iidx_%s.value_id %s %s)" % (
                    name, self.SQL_OPERATORS[match_policy], 
                    self._get_feature_value_id(name, value.string)))
            elif value.TYPE is ast.RegexLiteral:
                temp_table = self._get_temp_regex_table(name, match_policy, value.regex)
                
                joins.add(
                    "JOIN %s ON %s.id = feature_iidx_%s.value_id" % (temp_table, temp_table, name))
        
        wheres.extend("(%s)" % (f.get_query_fragment(), ) for f in predicates if f.FOR_NODE)
        if not wheres:
            wheres = ["1"]
        return "SELECT node_data.id, node_data.id >> 12 AS graphid FROM node_data %s WHERE %s" % (
            " ".join(joins), " AND ".join(wheres))

    def result(self, query_ast, inferred_node_type, predicates):
        """Assembles the parts of a query and returns the final query string."""
        order_clause = " ORDER BY graphid" if len(self.queries) > 1 else ""

        return " UNION ".join(self._create_single_select_stmt(query, node_type, predicates) 
                              for query, node_type in zip(self.queries, self.types)) + order_clause


    def compile_query(self, query_ast, inferred_node_type, predicates):
        """Creates the complete SQL query for a single node description.
        
        Several disjuncts will be connected by an SQL UNION statement. Nodes from the same graph 
        are guaranteed to be consecutive in the result set."""
        return self.run(query_ast, inferred_node_type, predicates)
    
    def get_cursor(self, expression):
        return self._db.cursor().execute(expression)
    
    def get_padding_cursor(self):
        return iter(self.PaddingCursor(self.corpus_size))
    

class GraphIterator(object):
    def __init__(self, query_compiler, node_descriptions, predicates):
        self._node_descriptions = node_descriptions
        self._predicates = predicates

        self._shared_variables = self._get_shared_variables()
        self._node_vars = [v for v in node_descriptions if v not in self._shared_variables]
        self._query_compiler = query_compiler
        
        self._node_cursors = []
        self._node_tips = {}
        self._set_tips = {}
       
    def _compile_query(self, node_variable):
        return self._query_compiler.compile_query(
            self._node_descriptions[node_variable], 
            node_variable.var_type, 
            self._predicates.get(node_variable, []))    
    
    def _get_shared_variables(self):
        def can_share(var_a, var_b):
            return (var_b not in shared \
                    and self._node_descriptions[var_a] == self._node_descriptions[var_b]\
                    and not (var_a in self._predicates or var_b in self._predicates))
        def variable_combinations():
            variables = list(self._node_descriptions)
            return [(variables[idx1], variables[idx2])
                    for idx1 in range(len(variables) - 1)
                    for idx2 in range(idx1 + 1, len(variables))]
        
        shared = {}
        for var1, var2 in variable_combinations():
            if can_share(var1, var2):
                shared[var2] = var1
        return shared

    def _get_set_predicates(self):
        """Returns a list `(var_name, predicate)` containing all constraints on node sets."""
        return [(name, pred)
                for name, node_predicates in self._predicates.iteritems()
                if name.is_set
                for pred in node_predicates
                if not pred.FOR_NODE]
    
    def _create_node_iters(self):
        self._node_cursors = []
        self._node_tips = {}
        self._set_tips = {}
        
        # a query compilation might trigger the creation of a temporary table,
        # so we have to compile all queries first before we get the cursors
        exprs = [(node_variable, self._compile_query(node_variable))
                 for node_variable in self._node_vars]
        
        self._node_cursors = [
            (node_variable, self._query_compiler.get_cursor(expression), 
             self._set_tips if node_variable.is_set else self._node_tips)
            for node_variable, expression in exprs]
        self._pad_nodesets()
        
    def _pad_nodesets(self):
        if all(var.is_set for var in self._node_vars):
            self._node_cursors.append(
                (NodeVariable("", False), self._query_compiler.get_padding_cursor(), 
                 self._node_tips))

    def _remove_iter(self, node_var, node_cursor):
        if node_var.is_set:
            node_cursor.close()
            self._node_cursors = [x for x in self._node_cursors if x[0] != node_var]
            return False
        else:
            return True
        
    def _read_tips(self):
        for node_variable, node_iter, tips in self._node_cursors:
            try:
                tips[node_variable] = node_iter.next()
            except StopIteration:
                if self._remove_iter(node_variable, node_iter):
                    raise EmptyResultException

    def _new_empty_result(self):
        current_graph = {}
        for node_var in self._node_vars:
            current_graph[node_var] = []
            
        for target, source in self._shared_variables.iteritems():
            current_graph[target] = current_graph[source]
        return current_graph    

    @staticmethod
    def _dump_nodes(from_iter, current_tip, next_graph):
        while current_tip[1] < next_graph:
            current_tip = from_iter.next()
        return current_tip
    
    def _find_graphs(self):
        depleted = False
        max_graphid = max(self._node_tips.values())[1]
        
        while not depleted:
            min_graphid = min(self._node_tips.values())[1]
            if min_graphid == max_graphid:
                new_result = self._new_empty_result()
                    
                for varname, node_iter, tips in self._node_cursors:
                    try:
                        node_list = new_result[varname]
                    except KeyError:
                        node_list = []
                        
                    try:
                        current_tip = self._dump_nodes(node_iter, tips[varname], min_graphid)
                            
                        while current_tip[1] == min_graphid:
                            node_list.append(current_tip[0])
                            current_tip = node_iter.next()
                        tips[varname] = current_tip
                        if not varname.is_set and current_tip[1] > max_graphid:
                            max_graphid = current_tip[1]
                    except StopIteration:
                        depleted = self._remove_iter(varname, node_iter)
                
                yield min_graphid, new_result
                
            else:
                for varname, node_iter, tips in self._node_cursors:
                    try:
                        current_tip = tips[varname] = \
                                    self._dump_nodes(node_iter, tips[varname], max_graphid)
                        if not varname.is_set and current_tip[1] > max_graphid:
                            max_graphid = current_tip[1]
                    except StopIteration:
                        depleted = self._remove_iter(varname, node_iter)
        
        self._cleanup()

    def _check_set_constraints(self, set_constraints, graph_result):
        for node_variable, constraint in set_constraints:
            if not constraint.check_node_set(graph_result[node_variable]):
                return False
        else:
            return True
    
    def _cleanup(self):
        for node_variable, cursor, tips in self._node_cursors:
            cursor.close()
        self._node_cursors = []
        self._query_compiler.cleanup_temporary_tables()
        
    def __iter__(self):
        try:
            self._create_node_iters()
            self._read_tips()
        except EmptyResultException:
            self._cleanup()
            return iter([])
        
        set_constraints = self._get_set_predicates()
        
        if set_constraints:
            return ((graphid, node_cands) for graphid, node_cands in self._find_graphs()
                    if self._check_set_constraints(set_constraints, node_cands))
        else:
            return self._find_graphs()


class NodeSearcher(object):
    """Class for node searching based on node descriptions and predicates.

    :Parameters:
      `db`: the database connection to be used.
      `graph_filter`: graph filter for partial results in parallel processing, 
       `NullGraphFilter` by default.
     
    Parallel Processing
    ===================
    For parallel procesing, the parameter `graph_filter` can be used so that a single node 
    searcher only returns a fraction of the graphs from the corpus, and several node searchers
    can be used in parallel to return all results.
    """            
    def __init__(self, db, graph_filter = NullGraphFilter()):
        self._query_compiler = NodeQueryCompiler(db, graph_filter)
    
    def search_nodes(self, node_descriptions, predicates):
        return GraphIterator(self._query_compiler, node_descriptions, predicates)

########NEW FILE########
__FILENAME__ = node_variable
# -*- coding: utf-8 -*-
# Copyright © 2007-2008 Stockholm TreeAligner Project
# Author: Torsten Marek <shlomme@gmx.net>
# Licensed under the GNU GPLv2
"""Classes for handling node variables."""
from operator import attrgetter

from nltk_contrib.tiger.query import ast
from nltk_contrib.tiger.graph import NodeType
from nltk_contrib.tiger.query.exceptions import TigerTypeError

__all__ = ["NodeVariable"]


class NodeVariable(str):
    """A node variable used in a TIGER query.
    
    
    *Properties*
     * `name`: the name of the variable (read-only)
     * `is_set`: `True` if the variable is a set,`False` otherwise (read-only)
     * `var_type`: the type of the variable, a member of nltk_contrib.tiger.graph.NodeType
                   `UNKNOWN` by default.
     """
    @classmethod
    def from_node(cls, variable):
        """Creates a new node variable from an AST node `variable`"""
        return cls(variable.name, variable.container is ast.ContainerTypes.Set)
    
    def __new__(cls, name, *args):
        return str.__new__(cls, name)
    
    def __init__(self, name, is_set, var_type = NodeType.UNKNOWN):
        super(self.__class__, self).__init__()
        self._name = name
        self._is_set = is_set
        self.var_type = var_type
        
    def __reduce__(self):
        return (NodeVariable, (self._name, self._is_set, self.var_type))
    
    def __repr__(self):
        return "nv(%s)" % (self._name, )

    def refine_type(self, new_type):
        """Tries to further specify the type of a node variable.
        
        If `new_type` is `NodeType.UNKNOWN` or the same as the current type of 
        the node variable, nothing is changed. If the type of the variable
        of `NodeType.UNKNOWN`, the variable is updated. Otherwise,
        a `TigerTypeError` is raised.
        """
        if new_type is NodeType.UNKNOWN or new_type is self.var_type:
            return
        elif self.var_type is NodeType.UNKNOWN:
            self.var_type = new_type
        else:
            raise TigerTypeError, self._name
            
    name = property(attrgetter("_name"), doc = "The name of the variable")
    is_set = property(attrgetter("_is_set"), 
                      doc = "`True` if the variable is a set, `False` otherwise.")

########NEW FILE########
__FILENAME__ = predicates
# -*- coding: utf-8 -*-
# Copyright © 2007-2008 Stockholm TreeAligner Project
# Author: Torsten Marek <shlomme@gmx.net>
# Licensed under the GNU GPLv2
"""This module contains the node predicates that are supported in the query language.

Predicates are simple node filters, i.e. they evaluate to true or false for a given 
node id. They are implented by returning little SQL snippets that are added to the
``WHERE`` clause of the query that is used to retrieve all node ids that match
a given node predicate.

Predicates:
 - ``root``: true if a given node is the root of a graph
 - ``discontinuous``: true if the fringe of a node's subgraph does not have wholes, i.e. all 
   it's terminal successors are adjacent
 - ``continuous``: the negation of ``discontinuous.
 - ``token_arity(n, [m]): if the number of terminal children is ``n``, or between ``n`` and ``m``
 - ``arity(n, [m]): if the number of children is ``n``, or between ``n`` and ``m``

"""
from nltk_contrib.tiger.utils.factory import FactoryBase

from nltk_contrib.tiger.query import ast
from nltk_contrib.tiger.query.node_variable import NodeType
from nltk_contrib.tiger.query.exceptions import PredicateTypeError, UndefinedNameError
from nltk_contrib.tiger import index

__all__ = ["PredicateFactory", "NodeTypePredicate"]


class Predicate(object):
    """Base class for node predicates.
    
    Signatures
    ==========
    The signature of a predicate is defined in the attribute `__signature__`, which
    must be an iterable of `(ast_type, mandatory_argument)` tuples.
    
    Mandatory arguments must precede optional ones. To ensure this, use the function `signature`.
    
    The first member of a signature must be an `ast.VariableReference`, and a signature
    may only contain one variable reference. Other arguments should be literal values. However, the
    TSQL grammar currently only supports integer values, but is easily changed to support more 
    argument types.
    
    The attribute `__ref_types__` contains the set of allowable container types for the variable.
    
    Names
    =====
    The attribute `__names__` must be an iterable of strings that defines the different names
    a predicate can be used under.
    
    Creation
    ========
    Predicates are created using the function `create_from_ast`, which will first check the 
    argument types and then invoke the constructor. Subclasses constructors must take
    all their arguments as positional parameters. The first argument in the constructor
    call will be the name that was used in the query, to distinguish between different
    versions of the same predicate.
    """
    __signature__ = None
    __ref_types__ = None
    __names__ = None
    
    def __init__(self, name, ref):
        pass
    
    @classmethod
    def check_signature(cls, name, invocation_args):
        """Checks the argument list of a predicate against the signature of the class."""
        for idx, formal_arg in enumerate(cls.__signature__):
            expected_ast_type, mandatory = formal_arg
            try: 
                if invocation_args[idx].TYPE is not expected_ast_type:
                    raise PredicateTypeError, (
                        "Type Error in argument %i of '%s': Expected %s, got %s" % \
                        (idx, name, invocation_args[idx].TYPE, expected_ast_type))
            except IndexError:
                if mandatory:
                    raise PredicateTypeError, "Missing arguments for '%s'." % (name, )
                else:
                    break
        else:
            if idx + 1 != len(invocation_args):
                raise PredicateTypeError, "Too many arguments for predicate '%s'." % (name, )
        variable = invocation_args[0].variable
        if variable.container not in cls.__ref_types__:
            raise PredicateTypeError("Predicate '%s' not valid for container type '%s'." % (
                name, variable.container))
        return variable
    
    @classmethod
    def create_from_ast(cls, predicate_node):
        """Creates a predicate class `cls` from a predicate AST node."""
        assert predicate_node.name in cls.__names__
        variable = cls.check_signature(predicate_node.name, list(predicate_node))
        return variable, cls(predicate_node.name, *list(predicate_node))

    
    def __eq__(self, other): # pragma: nocover
        raise NotImplementedError

    def __ne__(self, other):
        return not self.__eq__(other)


class SetPredicate(Predicate):
    """Base class for set predicates."""
    def check_node_set(self, node_set): # pragma: nocover
        """Evaluates the predicate for a given node set.
        
        Returns a boolean. 
        
        Must be overridden in clients.
        """
        raise NotImplementedError

    FOR_NODE = False
    

class NodeQueryPredicate(Predicate):
    """Base class for predicates that modify the node query."""
    def get_query_fragment(self): # pragma: nocover
        """Returns the query fragment that should be added to the SQL query for retrieving node ids.
        
        The return value must be a correct and valid term to be used in a ``WHERE`` clause of an 
        SQL query. All fiels from the ``nodedata`` table may be used.
        
        Must be overridden in clients.
        """
        raise NotImplementedError

    FOR_NODE = True


def signature(*args):
    """Creates a signature list for the `__signature__` attribute in the `Predicate` class.
    
    If the last argument is a list, the members of this list are taken as optional arguments
    of the predicate.
    
    Example:
    >>> s = signature(ast.VariableReference, [ast.StringLiteral])
    >>> s == [(ast.VariableReference, True), (ast.StringLiteral, False)]
    """
    if isinstance(args[-1], (list, tuple)):
        args, optional = args[:-1], args[-1]
    else:
        optional = []
    return [(t, True) for t in args] + [(t, False) for t in optional]


class RootPredicate(NodeQueryPredicate):
    """Checks if a given node is the root of the graph.

    :Predicates: root
    """
    __names__ = ["root"]
    __signature__ = signature(ast.VariableReference)
    __ref_types__ = ast.ContainerTypes.enum_set()
    
    def get_query_fragment(self):
        """Checks if a node is a root by testing if the gorn address has zero length."""
        return "LENGTH(node_data.gorn_address) = 0"

    def __eq__(self, other):
        return self.__class__ is other.__class__
    

class ContinuityPredicate(NodeQueryPredicate):
    """Checks if a given node is continuous or not.
    
    :Predicates: continuous, discontinuous
    """
    __names__ = ["continuous", "discontinuous"]
    __signature__ = signature(ast.VariableReference)
    __ref_types__ = ast.ContainerTypes.enum_set()

    def __init__(self, name, ref):
        super(ContinuityPredicate, self).__init__(name, ref)
        self._continuity_type = (
            index.DISCONTINUOUS if name == "discontinuous" else index.CONTINUOUS)
        
    def get_query_fragment(self):
        """Checks (dis)continuity in the ``nodedata`` table."""
        return "node_data.continuity = %i" % (self._continuity_type,)
    
    def __eq__(self, other):
        return self.__class__ is other.__class__ and self._continuity_type == other._continuity_type


class XArityPredicate(NodeQueryPredicate):
    """Checks different arity predicates.
    
    :Predicates: arity, tokenarity
    """
    __names__ = ["arity", "tokenarity"]
    __signature__ = signature(ast.VariableReference, ast.IntegerLiteral, [ast.IntegerLiteral])
    __ref_types__ = ast.ContainerTypes.enum_set()
    
    def __init__(self, name, ref, lower, upper = None):
        super(XArityPredicate, self).__init__(self, ref)
        assert name in self.__names__
        self._field = name
        self._upper = upper.value if upper is not None else None
        self._lower = lower.value
        
    def get_query_fragment(self):
        """Checks token or child arity in the ``nodedata`` table."""
        if self._upper is None:
            return "node_data.%s = %i" % (self._field, self._lower)
        else:
            return "node_data.%s >= %i and node_data.%s <= %i" % \
                   (self._field, self._lower, self._field, self._upper)

    def __eq__(self, other):
        return self.__class__ is other.__class__ and self._field == other._field and \
               self._lower == other._lower and self._upper == other._upper

    
class SetEmptyPredicate(SetPredicate):
    """Checks node sets for emptiness.
    
    :Predicates: empty, nonempty
    """
    __names__ = ["empty", "nonempty"]
    __signature__ = signature(ast.VariableReference)
    __ref_types__ = ast.ContainerTypes.Set.elem_set()
    
    def __init__(self, name, ref):
        super(self.__class__, self).__init__(name, ref)
        self._name = name
        self._has_elems = name == "nonempty"
    
    def check_node_set(self, node_set):
        """Checks if `node_set` matches the required emptiness state."""
        return bool(node_set) == self._has_elems

    def __eq__(self, other):
        return self.__class__ is other.__class__ and self._name == other._name


# Special predicates
class NodeTypePredicate(NodeQueryPredicate):
    """A special non-user-constructible property for adding inferred node types."""
    def __init__(self, var_type):
        super(self.__class__, self).__init__(None, None)
        assert var_type is not NodeType.UNKNOWN
        self._var_type = var_type

    def get_query_fragment(self):
        """Adds a constraint on the node type."""
        if self._var_type is NodeType.TERMINAL:
            return "node_data.continuity = 0"
        else:
            return "node_data.continuity > 0"
    
    def __eq__(self, other):
        return self.__class__ is other.__class__ and self._var_type is other._var_type


class PredicateFactory(FactoryBase):
    """The factory class for predicates."""
    __classes__ = [RootPredicate, ContinuityPredicate, XArityPredicate, SetEmptyPredicate]
    
    def _get_class_switch(self, cls):
        """Uses the `__names__` as the list of type switches for `cls`."""
        return cls.__names__
    
    def _get_switch(self, pred_ast):
        """Returns the name of the predicate as the type switch."""
        return pred_ast.name
    
    def raise_error(self, pred_name):
        """Raises an `UndefinedNameError` for unknown predicates."""
        raise UndefinedNameError, (UndefinedNameError.PREDICATE, pred_name)
    
    def _create_instance(self, cls, pred_ast):
        """Creates a new predicate using the class factory method."""
        return cls.create_from_ast(pred_ast)

########NEW FILE########
__FILENAME__ = querybuilder
# -*- coding: utf-8 -*-
# Copyright © 2007-2008 Stockholm TreeAligner Project
# Author: Torsten Marek <shlomme@gmx.net>
# Licensed under the GNU GPLv2
from functools import partial
from nltk_contrib.tiger.query import ast, predicates
from nltk_contrib.tiger.query.ast_utils import create_varref, create_vardef
from nltk_contrib.tiger.query.tsqlparser import NODE_VAR_PREFIXES

# This interface is currently unused.
__all__ = ["QueryBuilder"]


class _BuilderBase(object):
    def _get_container_type(self, node_var):
        return NODE_VAR_PREFIXES.get(node_var[0], ast.ContainerTypes.Single)
    
    def _create_varref(self, node_var):
        return create_varref(node_var, container_type = self._get_container_type(node_var))


class _PredicateFactory(_BuilderBase):
    AST_NODES = {
        int: ast.IntegerLiteral,
    }
    def _get_ast_args(self, args):
        predicate_args = [self._create_varref(args[0])]
        predicate_args.extend(
            [self.AST_NODES[type(a)](a) 
             for a in args[1:]])
        return predicate_args
            
    def build(self, name, *args):
        return ast.Predicate(name, self._get_ast_args(args))
    
    def __getattr__(self, attr):
        return partial(self.build, attr)


class _ConstraintFactory(_BuilderBase):
    OPS = {
        "secedge": ast.SecEdgeOperator,
        "dominance": ast.DominanceOperator,
        "corner": ast.CornerOperator,
        "sibling": ast.SiblingOperator,
        "precedence": ast.PrecedenceOperator,
    }
    def build(self, name, left_op, right_op, **kwargs):
        op_cls = self.OPS[name]
        return op_cls(self._create_varref(left_op), self._create_varref(right_op),
                      **kwargs)
    
    def __getattr__(self, attr):
        return partial(self.build, attr)


class _Composite(object):
    def __and__(self, other):
        if other.__class__ == self.__class__:
            return And(self.children + other.children)
        else:
            self.children.append(other)
            return self
        
    def __or__(self, other):
        if other.__class__ == self.__class__:
            return Or(self.children + other.children)
        else:
            self.children.append(other)
            return self
    
    
class _ExprBase(object):
    def __and__(self, other):
        if isinstance(other, And):
            return other.__and__(self)
        else:
            return And([self, other])
        
    def __or__(self, other):
        if isinstance(other, Or):
            return other.__or__(self)
        else:
            return Or([self, other])


class And(_Composite, ast.Conjunction):
    TYPE = ast.Conjunction


class Or(_Composite, ast.Disjunction):
    TYPE = ast.Disjunction


class FeatureConstraint(_ExprBase, ast.FeatureConstraint):
    TYPE = ast.FeatureConstraint
    

class Feature(object):
    def __init__(self, feature_name):
        self._feature_name = feature_name

    def not_matches(self, regex):
        return FeatureConstraint(self._feature_name, ast.Negation(ast.RegexLiteral(regex.expr)))
                                  
    def matches(self, regex):
        return FeatureConstraint(self._feature_name, ast.RegexLiteral(regex.expr))
    
    def equals(self, literal):
        return FeatureConstraint(self._feature_name, ast.StringLiteral(literal))
        
    def not_equals(self, literal):
        return FeatureConstraint(self._feature_name, ast.Negation(ast.StringLiteral(literal)))


class _FeatureFactory(object):
    def __getattr__(self, attr):
        return Feature(attr)


class QueryBuilder(_BuilderBase):
    feature = _FeatureFactory()
    predicate = _PredicateFactory()
    constraint = _ConstraintFactory()

    def __init__(self, query_factory):
        super(self.__class__, self).__init__()
        self._factory = query_factory
        self._nodes = []
        self._predicates = []
        self._constraints = []
    
    def add_constraint(self, name, *args, **kwargs):
        self._constraints.append(self.constraint.build(name, *args, **kwargs))
        return self
    
    def add_predicate(self, name, *args):
        self._predicates.append(self.predicate.build(name, *args))
        return self
    
    def add_node(self, name, expression):
        a = create_vardef(name, ast.NodeDescription(expression),
                          container_type=self._get_container_type(name))
        self._nodes.append(a)
        return self
    
    def finish(self):
        return self._factory.from_ast(
            ast.TsqlExpression(ast.Conjunction(self._nodes + self._predicates + self._constraints)))

########NEW FILE########
__FILENAME__ = result
# -*- coding: utf-8 -*-
# Copyright © 2007-2008 Stockholm TreeAligner Project
# Author: Torsten Marek <shlomme@gmx.net>
# Licensed under the GNU GPLv2
"""This module contains classes for building result sets for TIGERSearch queries .

The result builder classes evaluate a TigerSearch query based on the internal
representation of the query.

The algorithm and the interfaces in this module are still subject to heavy change. For more
information, see the inline comments.
"""
import operator
import multiprocessing
from functools import partial
from itertools import count, izip
from collections import defaultdict

from nltk_contrib.tiger.index import IndexNodeId
from nltk_contrib.tiger.query.exceptions import MissingFeatureError
from nltk_contrib.tiger.query.constraints import Direction
from nltk_contrib.tiger.query.nodesearcher import NodeSearcher, EqualPartitionsGraphFilter

__all__ = ["ResultBuilder", "ParallelResultBuilder"]

product = partial(reduce, operator.mul)

def named_cross_product(items):
    def _outer_product(depth, combination):
        varname, nodes = items[-depth]
        for node in nodes:
            combination[varname] = node
            if depth == 1:
                yield combination.copy()
            else:
                for res in _outer_product(depth - 1, combination):
                    yield res
    
    if items:
        return _outer_product(len(items), {})
    else:
        return iter([{}])


def partition_variables(variables, constraints):
    var_connections = dict(izip(variables, count()))
    
    for l, r in constraints:
        new_id = var_connections[l]
        old_id = var_connections[r]
        for name, value in var_connections.iteritems():
            if value == old_id:
                var_connections[name] = new_id
        
    sets = defaultdict(set)
    for name, value in var_connections.iteritems():
        sets[value].add(name)
    return sets.values()


class ConstraintChecker(object):
    @classmethod
    def _nodevar_idx_combinations(cls, ordered_node_vars):
        return [(upper_key, lower_key) 
                for lower_key in xrange(1, len(ordered_node_vars))
                for upper_key in xrange(lower_key)]
    
    @classmethod
    def _get_node_variables(cls, constraints):
        return set(var for var_pair in constraints for var in var_pair)
    
    @classmethod
    def prepare(cls, constraints, sizes = {}):
        constraints = dict(constraints)
        
        set_weight = sum(sizes.values()) + 1
        
        ordered_node_vars = sorted(
            cls._get_node_variables(constraints),
            key = lambda k: set_weight if k.is_set else sizes.get(k, 0))
        
        ordered_constraints = []
        for (upper_idx, lower_idx) in cls._nodevar_idx_combinations(ordered_node_vars):
            var_pair = (ordered_node_vars[upper_idx], ordered_node_vars[lower_idx])
            
            fail_after_success = False
            
            if var_pair in constraints:
                constraint = constraints[var_pair].check
                direction = constraints[var_pair].get_singlematch_direction()

                if direction is Direction.BOTH or direction is Direction.LEFT_TO_RIGHT:
                    fail_after_success = True
                ordered_constraints.append((var_pair[0], var_pair[1], constraint, False, fail_after_success))

            elif var_pair[::-1] in constraints:
                constraint = constraints[var_pair[::-1]].check
                direction = constraints[var_pair[::-1]].get_singlematch_direction()

                if direction is Direction.BOTH or direction is Direction.RIGHT_TO_LEFT:
                    fail_after_success = True
                ordered_constraints.append((var_pair[0], var_pair[1], constraint, True, fail_after_success))


        return partial(ConstraintChecker, ordered_constraints)
        
    def __init__(self, constraints, nodes, query_context):
        self.ordered_constraints = constraints
        self.nodes = nodes
        self.ok = set()
        self.has_results = self.prefilter(query_context)

    def prefilter(self, query_context):
        for (left_var, right_var, constraint, exchange, fail_after_success) in self.ordered_constraints:
            l_success = set()
            r_success = set()
            for left in self.nodes[left_var]:
                ldata = query_context.get_node(left)
                
                for right in self.nodes[right_var]:
                    rdata = query_context.get_node(right)
                    
                    if exchange:
                        larg, rarg = rdata, ldata
                    else:
                        larg, rarg = ldata, rdata
                    
                    query_context.constraint_checks += 1
                    if constraint(larg, rarg, query_context):
                        self.ok.add((left, right))
                        if not right_var.is_set:
                            l_success.add(left)
                            r_success.add(right)
                            if fail_after_success:
                                break
                    elif right_var.is_set:
                        break
                else:
                    if right_var.is_set:
                        l_success.add(left)
                        r_success.update(self.nodes[right_var])
                       
            if not l_success:
                return False
            if not r_success and not right_var.is_set:
                return False
            self.nodes[left_var] = list(l_success)
            self.nodes[right_var] = list(r_success)
        return True
            
    def _nodeids(self, query_result):
        for node_var in query_result:
            query_result[node_var] = IndexNodeId.from_int(query_result[node_var])
        return query_result
    
    def extract(self):
        """Creates the result set.
        
        The function currently uses a brute-force attempt. Please see the TODOs at the top
        of the module.
        """
        if self.has_results:
            g = [item for item in self.nodes.items() if not item[0].is_set]
            
            return [self._nodeids(query_result)
                    for query_result in named_cross_product(g)
                    if self._check(query_result)]
        else:
            return []

    def _check(self, result):
        for (left_variable, right_variable, __, __, __) in self.ordered_constraints:
            if right_variable.is_set:
                for right_node in self.nodes[right_variable]:
                    if (result[left_variable], right_node) not in self.ok:
                        return False
            else:
                if (result[left_variable], result[right_variable]) not in self.ok:
                    return False
        return True
        
PREPARE_NEW_AFTER = 100

def cct_search(graph_results, query_context):
    query_context._ncache.clear()
    query_context.checked_graphs += 1
    if query_context.checked_graphs == PREPARE_NEW_AFTER:
        query_context.checker_factory = ConstraintChecker.prepare(query_context.constraints, query_context.node_counts)
    elif query_context.checked_graphs < PREPARE_NEW_AFTER:
        for node_var, node_ids in graph_results.iteritems():
            query_context.node_counts[node_var] += len(node_ids)
        
    c = query_context.checker_factory(graph_results, query_context)
    return c.extract()


class LazyResultSet(object):
    def __init__(self, nodes, query_context):
        query_context.checked_graphs += 1
        self._nodes = [(node_var.name, [IndexNodeId.from_int(nid) for nid in node_ids])
                       for node_var, node_ids in nodes.iteritems()
                       if not node_var.is_set]

        self._size = product((len(ids) for var, ids in self._nodes), 1)
        self._items = None
        
    def __len__(self):
        return self._size

    def __getitem__(self, idx):
        if self._items is None:
            self._items = list(iter(self))
        return self._items[idx]
    
    def __iter__(self):
        return named_cross_product(self._nodes)
        

class QueryContext(object):
    def __init__(self, db, constraints, nodevars):
        self.cursor = db.cursor()
        self._ncache = {}
        self.constraints = constraints
        self.node_counts = defaultdict(int)
        
        variable_partitions = partition_variables(nodevars, (c[0] for c in constraints))
        if len(variable_partitions) == len(nodevars):
            self.constraint_checker = LazyResultSet
        elif len(variable_partitions) == 1:
            self.checker_factory = ConstraintChecker.prepare(constraints)
            self.constraint_checker = cct_search
        else:
            raise MissingFeatureError, "Missing feature: disjoint constraint sets. Please file a bug report."
        self._reset_stats()
        
    def _reset_stats(self):
        self.node_cache_hits = 0
        self.node_cache_misses = 0
        self.checked_graphs = 0
        self.constraint_checks = 0
        
    def get_node(self, node_id):
        try:
            self.node_cache_hits += 1
            return self._ncache[node_id]
        except KeyError:
            self.node_cache_misses += 1
            self.cursor.execute("""SELECT id, edge_label,  
            continuity, left_corner, right_corner, token_order, gorn_address
            FROM node_data WHERE id = ?""", (node_id, ))
            rs = self._ncache[node_id] = self.cursor.fetchone()
            return rs    
    

class ResultBuilderBase(object):
    def __init__(self, node_descriptions, predicates):
        self._nodes = node_descriptions
        self._predicates = predicates
        
    def node_variable_names(self):
        """Returns the set of node variables defined in the query."""
        return frozenset(nv.name for nv in self._nodes)


class ResultBuilder(QueryContext, ResultBuilderBase):
    def __init__(self, ev_context, node_descriptions, predicates, constraints):
        QueryContext.__init__(self, ev_context.db, constraints, node_descriptions.keys())
        ResultBuilderBase.__init__(self, node_descriptions, predicates)
        self._nodesearcher = ev_context.nodesearcher
        
    def evaluate(self):
        """Evaluates the query.
        
        Returns a list of `(graph_number, results)` tuples, where `results` is a list of 
        dictionaries that contains `variable: node_id` pairs for all defined variable names.
        """
        self._reset_stats()
        matching_graphs = self._nodesearcher.search_nodes(self._nodes, self._predicates)

        return filter(operator.itemgetter(1),
                      ((graph_id, self.constraint_checker(nodes, self))
                       for graph_id, nodes in matching_graphs))


class ParallelEvaluatorContext(object):
    def __init__(self, db_provider, graph_filter):
        self.db = db_provider.connect()
        self.nodesearcher = NodeSearcher(self.db, graph_filter)
        self.db_provider = db_provider


def evaluate_parallel(db_provider, nodes, predicates, constraints, result_queue, graph_filter):
    ev_ctx = ParallelEvaluatorContext(db_provider, graph_filter)
    query = ResultBuilder(ev_ctx, nodes, predicates, constraints)

    result_set = query.evaluate()
    result_queue.put((result_set, (query.checked_graphs, query.constraint_checks, 
                      query.node_cache_hits, query.node_cache_misses)))
    result_queue.close()
    

class ParallelResultBuilder(ResultBuilderBase):
    def __init__(self, ev_context, node_descriptions, predicates, constraints):
        super(self.__class__, self).__init__(node_descriptions, predicates)
        self._constraints = constraints
        self._db_provider = ev_context.db_provider
        self._reset_stats()
        
    def _reset_stats(self):
        self.node_cache_hits = 0
        self.node_cache_misses = 0
        self.checked_graphs = 0
        self.constraint_checks = 0
        
    def evaluate(self):
        self._reset_stats()
        result_queue = multiprocessing.Queue()
        num_workers = multiprocessing.cpu_count()
        workers = []
        for i in range(num_workers):
            worker = multiprocessing.Process(
                target = evaluate_parallel,
                args = (self._db_provider, self._nodes, self._predicates,
                        self._constraints, result_queue, 
                        EqualPartitionsGraphFilter(i, num_workers)))
            worker.start()
            workers.append(worker)
        
        results = []
        running_workers = num_workers
        while running_workers > 0:
            partial_result, stats = result_queue.get()
            results.extend(partial_result)
            self.checked_graphs += stats[0]
            self.constraint_checks += stats[1]
            self.node_cache_hits += stats[2]
            self.node_cache_misses += stats[3]
            
            running_workers -= 1
        for worker in workers:
            worker.join()
        return results

########NEW FILE########
__FILENAME__ = tsqlparser
# -*- coding: utf-8 -*-
# Copyright © 2007-2008 Stockholm TreeAligner Project
# Author: Torsten Marek <shlomme@gmx.net>
# Licensed under the GNU GPLv2
"""Defines the grammar and parser class for the TigerSearch query language.

For a complete description, see the `TigerSearch Manual`_.

The grammar will create an abstract syntax tree from a TigerSearch query.

Please see the unit test and the `nltk_contrib.tiger.query.ast` module for 
the nodes and the structure of the ASTs.

If applicable, all fragments contain information about their respective AST node type, 
some example strings and the named results introduced by the fragment in question. This 
is not the full list of named results returned by the fragment, which can be obtained 
by examining which fragments are used in the expression builder in question.

The grammar does not check semantic correctness of the query, e.g. : 
 * valid predicate names
 * correct parameter types for predicates, as long as they are a node operand or 
   an integer literal.
 * type-correctness for variable references
 * conflicts in feature constraints (e.g. ``[cat="NP"&"NN"]``)
 * conflicts in relations (e.g. ``#a > #b & #b > #a``)
 * nonsensical distance modifiers (e.g. ``#a >5,1 #b``)
 
All these checks are done by the query builder and evaluator functions. 

The only normalization done in the parser is that ``cat!="NE"``
is turned into ``cat=!"NE"``.

.. _`TigerSearch Manual` : http://tinyurl.com/2jm24u
"""
import pyparsing

from nltk_contrib.tiger.graph import NodeType
from nltk_contrib.tiger.query import ast
from nltk_contrib.tiger.query.exceptions import TigerSyntaxError
__all__ = ["TsqlParser"]

# enable memoizing support in parser (speeds up parsing)
pyparsing.ParserElement.enablePackrat()

# convenience functions

NUMBER = pyparsing.Word(pyparsing.nums)
WORD = pyparsing.Word(pyparsing.alphas)

def single_value_holder(cls, conv = lambda n: n):
    """Creates a parse action that invokes `cls` with the first parse result.
    
    `conv` can be used to convert the parse result before invoking `cls`.
    """
    return lambda s, l, t: cls(conv(t[0]))


def suppressed_literal(s):
    """Creates a suppressed literal string `s`."""
    return pyparsing.Literal(s).suppress()


def boolean_expr(atom):
    """Creates a boolean expression grammar out of an expression `atom`.
    
    A boolean expression can contain the following operators, ordered by binding power:
     * negation: ``!term``
     * conjunction: ``term & term``
     * disjunction: ``term | term``
     
    and can have parentheses for grouping. 
    """
    ops = [
        (suppressed_literal(u"!"), 1, pyparsing.opAssoc.RIGHT,
         lambda s, l, t: ast.Negation(t[0][0])),

        (suppressed_literal(u"&"), 2, pyparsing.opAssoc.LEFT,
         lambda s, l, t: ast.Conjunction(t.asList()[0])),

        (suppressed_literal(u"|"), 2, pyparsing.opAssoc.LEFT,
         lambda s, l, t: ast.Disjunction(t.asList()[0]))]
    return pyparsing.operatorPrecedence(atom, ops)


def surround(left, expr, right):
    """Circumfixes the expression `expr` with `left` and `right`.
    
    Both `left` and `right` will be turned into suppressed literals.
    
    *Parameters*:
     `left`
        the left part of the circumfix
     `expr`
        the grammar expression to be circumfixed
     `right`
       the right part of the circumfix
    """
    return suppressed_literal(left) + expr + suppressed_literal(right)



def integer_literal():
    """Defines an expression for an integer literals.
    
    :AST Node: `IntegerLiteral`
    :Example: ``12345``
    """
    return NUMBER.setParseAction(single_value_holder(ast.IntegerLiteral, int))


def string_literal():
    """Defines an expression for string literals.
    
    A string literal can be enclosed in single (') or double (") quotes and can contain
    escaped characters using "\\".

    :AST Node: `StringLiteral`
    :Example: ``"word"``
    """
    string = (pyparsing.QuotedString("'", escChar = "\\") | 
              pyparsing.QuotedString('"', escChar = "\\"))
    return string.setParseAction(single_value_holder(ast.StringLiteral))  


def regex_literal():
    """"Defines an expression for regular expression literals.
    
    :AST Node: `RegexLiteral`
    :Example: ``/a+b+/``
    """
    regex = pyparsing.QuotedString("/")
    return regex.setParseAction(single_value_holder(ast.RegexLiteral))


def variable_name(type_prefixes):
    """Defines a reusable expression for all variable names.

    A variable name can only contain ASCII alphanumeric characters and the 
    underscore character, and must start with one of the characters listed in 
    in the dictionary `type_prefixes` (by default only "#"). The value in 
    `type_prefixes` determines the container type of the variable.
    
    :Named Results:
     - `varname`: the variable name
    """
    assert all(len(pfx) == 1 for pfx in type_prefixes), "prefix list may only contain characters"
    
    v_expr = pyparsing.Combine(pyparsing.oneOf(type_prefixes.keys()) + 
                               pyparsing.Word(pyparsing.alphanums + "_")).setResultsName("varname")
    v_expr.type_map = type_prefixes
    return v_expr


def variable_reference(variable_expr, variable_type):
    """Defines an expression for variable references of type `variable_type`.
    
    See `nltk_contrib.tiger.query.ast.VariableTypes` for the list of variable types.
    
    :AST Node: `VariableReference`
    :Example: ``#a``
    """
    return variable_expr.setParseAction(
        lambda s, l, t: ast.VariableReference(ast.Variable(
            t.varname, variable_type, variable_expr.type_map[t.varname[0]])))


def variable_definition(variable_expr, variable_type, right_hand):
    """Defines an expression for variable definitions of type `variable_type`.
    
    The referent expression is `right_hand`, and `variable_expr` contains the expression
    for variable names.
    
    :AST Node: `Variable`
    :Example: ``#a:...``
    :Named Results:
     - `expr`: the right-hand side of the definition
    """
    definition = (variable_expr + suppressed_literal(u":") + 
                  right_hand.setResultsName("expr"))
    return definition.setParseAction(
        lambda s, l, t: ast.VariableDefinition(
            ast.Variable(t.varname, variable_type, variable_expr.type_map[t.varname[0]]), t.expr))


def feature_value():
    """Defines an expression for the right hand side in feature constraints.
    
    A feature constraint can be a boolean expression of string and regex literals.
    
    :Example: ``"NE"``, ``"NE"|"NN"``, ``/h.*/ & !"haus"``
    """
    return boolean_expr(string_literal() | regex_literal())

FEATURE_VALUE = feature_value()

VAR_PREFIXES = {"#": ast.ContainerTypes.Single}

def feature_record():
    """Defines an expression for feature records.
    
    Valid feature records:
     * ``T``: all terminals
     * ``NT``: all nonterminals
     
    :AST Node: `FeatureRecord:
    :Example: ``T``, ``NT``
    """
    return pyparsing.oneOf("T NT").setParseAction(
        single_value_holder(ast.FeatureRecord, lambda v: NodeType.fromkey(v[0])))


def feature_constraint():
    """Defines a boolean expression for feature constraint.
    
    A feature constraint is a feature name, a match operator and a 
    feature value expression.
    
    Match operators:
     * ``=`?` (equality)
     * ``!=`` (inequality)
    
    If the match operator is `!=`, the feature value expression will be wrapped inside
    a `Negation` AST node.
    
    :AST Node: `FeatureConstraint`
    :Example: ``cat="NP"``, ``pos!=/N+/``, ``word="safe" & pos="NN"``
    """
    op = pyparsing.oneOf(u"= !=")
    v = FEATURE_VALUE
    
    constraint = (WORD + op + v)
    constraint.setParseAction(lambda s, l, t: ast.FeatureConstraint(t[0], t[2]) if t[1] == "=" 
                                              else ast.FeatureConstraint(t[0], ast.Negation(t[2])))
    return boolean_expr(constraint | feature_record())

FEATURE_CONSTRAINT = feature_constraint()


def node_description(): 
    """Defines an expression for node descriptions.
    
    Node descriptions can either be a boolean expression of  list of constraints, a 
    feature constraint variable definition or reference or a feature record.
    
    :AST Node: `NodeDescription`
    :Example: ``[pos="PREP" & word=("vor"|"vorm")]``, ``[T]``, ``[#a:(word = "safe")]``, ``[#b]``
    """
    node_desc = surround(u"[", FEATURE_CONSTRAINT, u"]")
    return node_desc.setParseAction(single_value_holder(ast.NodeDescription))

NODE_DESCRIPTION = node_description()

NODE_VAR_PREFIXES = {"#": ast.ContainerTypes.Single,
                     "%": ast.ContainerTypes.Set }


def node_variable_def():
    """Defines an expression for node variable definitions. 
    
    Node variables have the type `VariableTypes.NodeIdentifier`

    :AST Node: `VariableReference`
    :Example: ``#n1:[pos = "PREP" & word = ("vor", "vorm")]``
    """
    return variable_definition(variable_name(NODE_VAR_PREFIXES), 
                               ast.VariableTypes.NodeIdentifier, NODE_DESCRIPTION)


def node_variable_ref():
    """Defines an expression for node variable references. 
    
    Node variables have the type `VariableTypes.NodeIdentifier`
    
    :AST Node: `VariableReference`
    :Example: ``#n1``
    """
    return variable_reference(variable_name(NODE_VAR_PREFIXES), ast.VariableTypes.NodeIdentifier)


def node_operand():
    """Defines an expression for node operands in predicate functions or constraints.
    
    An operand can be a node variable definition, reference or a node description itself.
    """
    return (node_variable_def() | node_variable_ref() | NODE_DESCRIPTION)

NODE_OPERAND = node_operand()


class ConstraintModifiers(object):
    """A class that contains all possible constraint modifiers.
    
    Each modifier `MOD` is stored as a named result `res`.
    
    Modifiers:
     `NEGATION` : `negated`
       the ! symbol before a constraint operator
     `TRANSITIVE` : `indirect`
       the * symbol after an operator
     `DISTANCE` : `mindist` and `maxdist`
       a single or a pair of integers after an operator
     `EDGE_LABEL` : `label`
       a string after a dominance operator
    """
    NEGATION = pyparsing.Optional(pyparsing.Literal("!")).setResultsName("negated")
    TRANSITIVE = pyparsing.Literal("*").setResultsName("indirect")
    DISTANCE = (NUMBER("mindist") + pyparsing.Optional(suppressed_literal(",") + NUMBER("maxdist")))
    EDGE_LABEL = WORD("label")

    
def operator_symbol(s, ast_node_class):
    """Defines an operator symbol `s`.
    
    :Named Results:
     - `op_class`: the AST class of the operator
    """
    return pyparsing.Literal(s).setResultsName("op_class")\
                               .setParseAction(lambda s, l, t: ast_node_class)


def dominance_operator():
    """Defines an expression for dominance operators.

    All variantes of the dominance operator can be negated.
    
    :AST Node: `DominanceOperator`
    :Example: ``>``, ``>*``, ``>L``, ``>n``, ``>n,m``
    """
    return (ConstraintModifiers.NEGATION +
            operator_symbol(">", ast.DominanceOperator) +
            pyparsing.Optional(ConstraintModifiers.EDGE_LABEL | 
                               ConstraintModifiers.TRANSITIVE |
                               ConstraintModifiers.DISTANCE))


def corner_operator():
    """Defines an expression for left- and rightmost terminal successors (corners).

    All variants of the corner dominance operator can be negated.
    
    :AST Node: `CornerDominance`
    :Example: ``>@l``, ``>@r``
    :Named Results:
     - `corner`: the corner, either ``l`` or ``r``
    """
    return (ConstraintModifiers.NEGATION + 
            operator_symbol(">@", ast.CornerOperator) +
            pyparsing.oneOf("l r").setResultsName("corner"))


def precedence_operator():
    """Defines an expression for precendence operators.
    
    All variants can be negated.
    
    :AST Node: `PrecedenceOperator`
    :Example: ``.``, ``.*``, ``.n``, ``.n,m``
    """
    return (ConstraintModifiers.NEGATION +
            operator_symbol(".", ast.PrecedenceOperator) + 
            pyparsing.Optional(ConstraintModifiers.TRANSITIVE | 
                               ConstraintModifiers.DISTANCE))


def sec_edge_operator():
    """Defines an expression for secondary edge dominance operators.
    
    All variants can be negated.
    
    :AST Node: `SecEdgeOperator`
    :Example: ``>~``, ``>~L``
    """
    return (ConstraintModifiers.NEGATION + 
            operator_symbol(">~", ast.SecEdgeOperator) + 
            pyparsing.Optional(ConstraintModifiers.EDGE_LABEL))


def sibling_operator():
    """Defines an expression for sibling operators.
    
    The ``$.*`` operator for siblings with precendence cannot be negated.
    
    :AST Node: `SiblingOperator`
    :Example: ``$.*``, ``$``
    """
    return ((operator_symbol("$", ast.SiblingOperator) + 
             pyparsing.Optional(pyparsing.Literal(".*").setResultsName("ordered"))) | 
            (ConstraintModifiers.NEGATION + operator_symbol("$", ast.SiblingOperator)))


def node_relation_constraint():
    """Defines an expression for node relation constraints.
    
    Please see the documentation of the operator AST nodes.
    
    :Example: ``[cat="NP"] > #a``
    :Named Results:
     - `leftOperand`: the node operand on the left side
     - `operator` the operator symbol, without modifiers
     - `rightOperand` the node operand on the right side
    """
    constraint_op = pyparsing.Group(sec_edge_operator() | corner_operator() | dominance_operator() |
                                    precedence_operator() | sibling_operator())
    
    constraint = (NODE_OPERAND.setResultsName("leftOperand") + 
                  constraint_op.setResultsName("operator") +
                  NODE_OPERAND.setResultsName("rightOperand"))
    return constraint.setParseAction(lambda s, l, t: t.operator.op_class.create(t))


def node_predicate():
    """Defines an expression for node predicates.
    
    A node predicate is a function name and a parenthesized list of node operands or 
    integer literals. The list of supported predicates is not part of the parser.
    
    :AST Node: `Predicate`
    :Example: ``root(#a)``, ``arity([cat="NP", 5)``
    :Named Results:
     - `pred`: the name of the predicate
     - `args`: the list of arguments, either node operands or integer literals
    """
    arg = (NODE_OPERAND | integer_literal()).setResultsName("args", listAllMatches = True)
    identifier = WORD("pred")
    
    return (identifier + surround(u"(", pyparsing.delimitedList(arg), u")")
            ).setParseAction(lambda s, l, t: ast.Predicate(t.pred, t.args.asList()))
    

# the complete query language
def tsql_grammar():
    """Defines the expression for the complete TigerSearch query language.
    
    A query term is either a node operand, a node relation constraint or node predicate. An
    expression can be a single term or a conjunction of terms.
    
    Toplevel disjunctions are not currently not supported, toplevel disjunction is not supported,
    because it can always be represented by negations in the relations and node descriptions.
    
    The returned expression must match the whole input string.
    
    :AST Node: `TsqlExpression`
    :Example: ``#a:[cat="NP"] & root(#a) and #a > [word="safe"]``
    """
    atom = (node_predicate() | node_relation_constraint() | NODE_OPERAND)
    
    expr = pyparsing.Group(atom + pyparsing.OneOrMore(suppressed_literal(u"&") + atom)
                           ).setParseAction(lambda s, l, t: ast.Conjunction(t.asList()[0])) | atom
    
    expr.setParseAction(single_value_holder(ast.TsqlExpression))
    return expr + pyparsing.StringEnd()


class TsqlParser(object):
    """A simple façade for the PyParsing TSQL grammar."""
    def __init__(self):
        self._g = tsql_grammar()
    
    def parse_query(self, query_string):
        """Parses a query string and returns the AST.
        
        If the string cannot be parsed, a `TigerSyntaxError` will be raised.
        """
        try:
            return self._g.parseString(query_string)[0]
        except pyparsing.ParseException, e:
            raise TigerSyntaxError, e

########NEW FILE########
__FILENAME__ = tigerxml
# -*- coding: utf-8 -*-
# Copyright © 2007-2008 Stockholm TreeAligner Project
# Author: Torsten Marek <shlomme@gmx.net>
# Licensed under the GNU GPLv2
"""A simple, non-validating parser for TIGER-XML corpora.

The `TigerParser` simply reads out metadata information and creates instances of
`nltk_contrib.tiger.graph.TigerGraph` from the sentences contained in the corpus.

No checks for completeness or soundness of the graph specification are made, so the
parser should be able to handle problematic corpora as well. 
"""
from functools import partial

from nltk_contrib.tiger.utils.etree_xml import IterParseHandler, element_handler
from nltk_contrib.tiger.graph import NonterminalNode, TerminalNode, TigerGraph, NodeType

__all__ = ("parse_tiger_corpus", "TigerParser")

class TigerParser(IterParseHandler):
    """A parser for TIGER-XML corpora.
    
    Corpora are parsed using the `parse` method and fed into an indexer.
    
    Indexers will receive the following data:
    
     Corpus metadata: `set_metadata(d)`
      - `d`: a dictionary of `(key, value)` metadata entries
       
     Edge labels: `set_edge_labels(d)`:
      - `d`: a dictionary with `(edge_label, description)` entries. The descriptions may be `None`, 
        if not given in the corpus
     
     Secondary edge labels: `set_secedge_labels(d)`
      - `d`: a dictionary with `(secedge_label, description)` entries. The descriptions may 
        be `None`, if not given in the corpus
    
     Features: `add_feature(name, domain, d)`
      - `name`: the name of the feature
      - `domain`: the domain, a member of `nltk_contrib.tiger.graph.NodeType`
      - `d`: a dictionary with `(value, description)` entries. The descriptions may 
        be `None`, if not given in the corpus

     Graphs: `add_graph(g)`
      - `g`: an instance of `nltk_contrib.tiger.graph.Graph`
      
    Indexers may make the following assumptions:
     - `set_metadata` will be invoked before everything else.
     - after the first call to `add_graph`, only other `add_graph` calls will be made.
     
    These assumptions might not hold true in case of erroneous TIGER-XML files. No
    schema checks are currently done, mostly due to the fact that incremental parsing
    with the ElementTree API does not support this.
    """
    def __init__(self):
        super(TigerParser, self).__init__()
        self._indexer = None
        
    @element_handler("meta")
    def handle_meta(self, elem):
        """Reads out corpus metadata information and feeds it to the indexer."""
        self._indexer.set_metadata(
            dict((child.tag, child.text.strip()) for child in elem))

    @element_handler("feature")
    def handle_feature(self, elem):
        """Feeds feature information (values, descriptions) to the indexer."""
        self._handle_annotation(elem, 
                                partial(self._indexer.add_feature, elem.get("name"), 
                                        NodeType.fromkey(elem.get("domain")[0])))
    
    @element_handler("edgelabel")
    def handle_edgelabels(self, elem):
        """Feeds all edge labels (label strings, descriptions) to the indexer."""
        self._handle_annotation(elem, self._indexer.set_edge_labels)
        
    @element_handler("secedgelabel")
    def handle_secedgelabels(self, elem):
        """Feeds all secondary edge labels (label strings, descriptions) to the indexer."""
        self._handle_annotation(elem, self._indexer.set_secedge_labels)
    
    @classmethod
    def _handle_annotation(cls, elem, report_function):
        """Generic method to send {value: description} dictionaries to the indexer."""
        report_function(dict((value.get("name"), value.text.strip() if value.text else None)
                             for value in elem))

    @element_handler("s")
    def handle_sentence(self, sentence):
        """Creates a `TigerGraph` for a sentence specification and feeds it to the indexer."""
        graph = TigerGraph(sentence.get("id"))
        
        graph_elem = sentence.find("graph")
        
        self._read_nodes(graph_elem.getiterator("t"), graph.nodes, TerminalNode, 
                         self._postproc_terminal)
        self._read_nodes(graph_elem.getiterator("nt"), graph.nodes, NonterminalNode, 
                         self._postproc_nonterminal)
        
        graph.root_id = graph_elem.get("root")

        self._indexer.add_graph(graph)
        return self.DELETE_BRANCH

    def _postproc_terminal(self, node, node_elem):
        node.order = int(node_elem.get("id").split("_")[-1]) - 1
    
    def _postproc_nonterminal(self, node, node_elem):
        node.edges = [
            (edge.get("label"), edge.get("idref"))
            for edge in node_elem.getiterator("edge")]
        
    @classmethod
    def _read_nodes(cls, node_elems, graph, node_cls, pproc):
        """Reads all elements from `node_elems` and adds them to the graph.
        
        Tiger nodes will be created using `node_cls`, and store feature values, secondary edges,
        edges (nonterminals only) and token order (terminals only) in the node objects.
        """
        for node_elem in node_elems:
            node = node_cls(node_elem.get("id"))
            for feature_name, value in node_elem.attrib.iteritems():
                if feature_name == "id":
                    continue
                node.features[feature_name] = value
                
            graph[node.id] = node
            pproc(node, node_elem)

            secedges = [
                (edge.get("label"), edge.get("idref"))
                for edge in node_elem.getiterator("secedge")]
            
            if secedges:
                node.secedges = secedges
                
    def parse(self, path, indexer):
        """Parses the TIGER-XML corpus in `path` and sends all data to `indexer`."""
        self._indexer = indexer
        self._parse(path)

            
def parse_tiger_corpus(filename, indexer):
    """Parses the TIGER-XML corpus in `filename` and sends it to `indexer`."""
    t = TigerParser()
    t.parse(filename, indexer)

########NEW FILE########
__FILENAME__ = db
# -*- coding: utf-8 -*-
# Copyright © 2007-2008 Stockholm TreeAligner Project
# Author: Torsten Marek <shlomme@gmx.net>
# Licensed under the GNU GPLv2
"""Utilities for SQLite3 database handling."""
try:
    from pysqlite2 import dbapi2 as sqlite3
except ImportError:
    import sqlite3

def create_inmem_db():
    """Creates a new, empty in-memory SQlite3 database."""
    return sqlite3.connect(":memory:")

########NEW FILE########
__FILENAME__ = enum
# -*- coding: utf-8 -*-
# Copyright © 2007-2008 Stockholm TreeAligner Project
# Author: Torsten Marek <shlomme@gmx.net>
# Licensed under the GNU GPLv2

__all__ = ["Enum", "enum_member"]

# this class was heavily inspired by http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/521879

class _EnumMember(object):
    def __init__(self, fields):
        self.fields = fields

    def __get__(self, obj, type_):
        return type_.__members__[self._name]


def enum_member(*args):
    return _EnumMember(args)


class _EnumType(type):
    def __new__(mcs, class_name, bases, dct):
        dct.setdefault("__fields__", ())
        dct["__slots__"] = ("__value__", "__name__")
        dct["__members__"] = members = {}
        
        for name, obj in dct.iteritems():
            if isinstance(obj, _EnumMember):
                members[name] = obj.fields
                obj._name = name
        
        for idx, fname in enumerate(dct["__fields__"]):
            dct[fname] = property(lambda self, i = idx: self.__value__[i])

        return type.__new__(mcs, class_name, bases, dct)
    
    def __init__(mcs, name, bases, dct):
        type.__init__(mcs, name, bases, dct)

        decl = type.__getattribute__(mcs, '__members__')
        
        members = {}
        
        field_count = len(type.__getattribute__(mcs, "__fields__"))
        
        for member_name, field_values in decl.iteritems():
            members[member_name] = mcs()
            if len(field_values) != field_count:
                raise TypeError, (
                    "Wrong number of fields for enum member '%s'. Expected %i, got %i instead." % (
                    member_name, field_count, len(field_values)))
            else:
                object.__setattr__(members[member_name], "__value__", field_values)
            object.__setattr__(members[member_name], '__name__', member_name)
                
        type.__setattr__(mcs, "__members__", members)

    def __setattr__(mcs, name, value):
        raise TypeError, "enum types cannot be modified"
     
    def __delattr__(mcs, name):
        raise TypeError, "enum types cannot be modified"
    
    def names(mcs):
        return type.__getattribute__(mcs, '__members__').keys()
    
    def __len__(mcs):
        return len(type.__getattribute__(mcs, '__members__'))
    
    def __iter__(mcs):
        return type.__getattribute__(mcs, '__members__').itervalues()
    
    def __contains__(mcs, name):
        return name in type.__getattribute__(mcs, '__members__')
    
    def __repr__(mcs): # pragma: nocover
        return "<EnumType '%s.%s'>" % (mcs.__module__, mcs.__name__)

    
class Enum(object):
    __metaclass__ = _EnumType

    # assigned by metaclass
    __value__ = ()
    __name__ = ""
    
    def __repr__(self):
        return "%s.%s" % (self.__class__.__name__,  self.__name__)
    
    def __getstate__(self):
        return self.__name__
    
    def __setstate__(self, name):
        self.__name__ = name
        self.__value__ = getattr(self.__class__, name).__value__
    
    def __eq__(self, other):
        return self is other or type(self) is type(other) and self.__name__ == other.__name__
    
    def __ne__(self, other):
        return not (self == other)

    def elem_set(self):
        return set([self])

    @classmethod
    def enum_set(cls):
        return set(iter(cls))

########NEW FILE########
__FILENAME__ = etree_xml
# -*- coding: utf-8 -*-
# Copyright © 2007-2008 Stockholm TreeAligner Project
# Author: Torsten Marek <shlomme@gmx.net>
# Licensed under the GNU GPLv2
import logging

__all__ = ("element_handler", "IterParseHandler", "ET")

has_lxml = False
has_schema = False

try:
    import sys
    import lxml.etree as ET
    # lxml 1.3.4 is broken on win32, crashes on large inputs.
    if sys.platform == "win32" and ET.__version__ == "1.3.4":
        del ET
        raise ImportError
    has_lxml = True
    has_schema = ET.LXML_VERSION[:3] >= (2, 0, 3)
    import lxml._elementpath as DONTUSE # py2exe workaround
    del DONTUSE
except ImportError:
    import xml.etree.ElementTree # py2exe workaround
    import xml.etree.cElementTree as ET

HANDLER_ATTRIBUTE_NAME = "_handled"

def element_handler(tag, event = "end"):
    assert event in ("start", "end")
    def _inner_element_handler(method):
        setattr(method, HANDLER_ATTRIBUTE_NAME, (event, tag))
        return method
    return _inner_element_handler


class IterParseType(type):
    def __new__(mcs, classname, bases, class_dict):
        class_dict["__x_handlers__"] = handlers = {}
        for attr in class_dict.itervalues():
            if callable(attr) and hasattr(attr, HANDLER_ATTRIBUTE_NAME):
                handlers[getattr(attr, HANDLER_ATTRIBUTE_NAME)] = attr
                
        return type.__new__(mcs, classname, bases, class_dict)


class IterParseHandler(object):
    DELETE_BRANCH = True
    
    __metaclass__ = IterParseType
    
    __x_handlers__ = {}
    
    def __init__(self, schema = None):
        self._schema = None
        
        if schema:
            if has_lxml:
                if has_schema:
                    self._schema = ET.XMLSchema(ET.parse(schema))
                else:
                    logging.warning(
                        "XML schema validation is only supported with lxml 2.0.3 and higher.")
            else:
                logging.warning("Validation requested, but lxml is not installed, deactivating!")
           
    def _parse(self, filename):
        """Parses the XML file `filename`."""
        events = ("start", "end")
            
        if has_schema:
            event_source = ET.iterparse(filename, events,  schema=self._schema)
        else:
            event_source = ET.iterparse(filename, events)
        
        context = iter(event_source)
        
        event, root = context.next()
        self._handle_root(root)
        
        for event, elem in context:
            try:
                handler = self.__x_handlers__[(event, elem.tag)]
            except KeyError:
                continue
            result = handler(self, elem)
            if result == self.DELETE_BRANCH and event == "end":
                elem.clear()
        
    def _handle_root(self, elem):
        """Called with the root element before any other processing is done.
        
        Only the attributes of the `elem` may be accessed at this time, children
        might be present as a side-effect, but may not be used.
        
        The default implementation does nothing, should be overridden by subclasses.
        """
        pass

########NEW FILE########
__FILENAME__ = factory
# -*- coding: utf-8 -*-
# Copyright © 2007-2008 Stockholm TreeAligner Project
# Author: Torsten Marek <shlomme@gmx.net>
# Licensed under the GNU GPLv2
class MissingClassException(Exception):
    pass

class FactoryBase(object):
    __classes__ = []
    
    def __init__(self):
        self._switch = {}
        if isinstance(self.__classes__, dict):
            self._switch = self.__classes__
        else:
            for cls in self.__classes__:
                for switch in self._get_class_switch(cls):
                    self._switch[switch] = cls
    
    def create(self, *args):
        switch_value = self._get_switch(*args)
        try:
            cls = self._switch[switch_value]
        except KeyError, e:
            self.raise_error(e.args[0])
        return self._create_instance(cls, *args)

    def raise_error(self, switch_name):
        raise MissingClassException, switch_name
    
    def _create_instance(self, cls, *args):
        return cls(*args)

    def _get_class_switch(self, cls):
        raise NotImplementedError
    
    def _get_switch(self, *args):
        raise NotImplementedError
    
    def __iter__(self):
        return iter(set(self._switch.values()))

########NEW FILE########
__FILENAME__ = parallel
# -*- coding: utf-8 -*-
# Copyright © 2007-2008 Stockholm TreeAligner Project
# Author: Torsten Marek <shlomme@gmx.net>
# Licensed under the GNU GPLv2
"""Support module for using the `multiprocessing` module."""

try:
    import multiprocessing
    HAS_PROCESSING = True
except ImportError:
    HAS_PROCESSING = False    

__all__ = ["use_parallel_processing"]

if HAS_PROCESSING:
    CPU_COUNT = multiprocessing.cpu_count()
else:
    CPU_COUNT = 1
    multiprocessing = None


def use_parallel_processing():
    """Returns `True` if the query evaluator can run parallelized, `False` otherwise.
    
    For the parallelized version, two conditions have to be fulfilled:
     * the `multiprocessing` module is present
     * the system has more than one CPU
    """
    return HAS_PROCESSING and CPU_COUNT > 1


########NEW FILE########
__FILENAME__ = timex
# Code for tagging temporal expressions in text
# For details of the TIMEX format, see http://timex2.mitre.org/

import re
import string
import os
import sys

# Requires eGenix.com mx Base Distribution
# http://www.egenix.com/products/python/mxBase/
try:
    from mx.DateTime import *
except ImportError:
    print """
Requires eGenix.com mx Base Distribution
http://www.egenix.com/products/python/mxBase/"""

# Predefined strings.
numbers = "(^a(?=\s)|one|two|three|four|five|six|seven|eight|nine|ten| \
          eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen| \
          eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|eighty| \
          ninety|hundred|thousand)"
day = "(monday|tuesday|wednesday|thursday|friday|saturday|sunday)"
week_day = "(monday|tuesday|wednesday|thursday|friday|saturday|sunday)"
month = "(january|february|march|april|may|june|july|august|september| \
          october|november|december)"
dmy = "(year|day|week|month)"
rel_day = "(today|yesterday|tomorrow|tonight|tonite)"
exp1 = "(before|after|earlier|later|ago)"
exp2 = "(this|next|last)"
iso = "\d+[/-]\d+[/-]\d+ \d+:\d+:\d+\.\d+"
year = "((?<=\s)\d{4}|^\d{4})"
regxp1 = "((\d+|(" + numbers + "[-\s]?)+) " + dmy + "s? " + exp1 + ")"
regxp2 = "(" + exp2 + " (" + dmy + "|" + week_day + "|" + month + "))"

reg1 = re.compile(regxp1, re.IGNORECASE)
reg2 = re.compile(regxp2, re.IGNORECASE)
reg3 = re.compile(rel_day, re.IGNORECASE)
reg4 = re.compile(iso)
reg5 = re.compile(year)

def tag(text):

    # Initialization
    timex_found = []

    # re.findall() finds all the substring matches, keep only the full
    # matching string. Captures expressions such as 'number of days' ago, etc.
    found = reg1.findall(text)
    found = [a[0] for a in found if len(a) > 1]
    for timex in found:
        timex_found.append(timex)

    # Variations of this thursday, next year, etc
    found = reg2.findall(text)
    found = [a[0] for a in found if len(a) > 1]
    for timex in found:
        timex_found.append(timex)

    # today, tomorrow, etc
    found = reg3.findall(text)
    for timex in found:
        timex_found.append(timex)

    # ISO
    found = reg4.findall(text)
    for timex in found:
        timex_found.append(timex)

    # Year
    found = reg5.findall(text)
    for timex in found:
        timex_found.append(timex)

    # Tag only temporal expressions which haven't been tagged.
    for timex in timex_found:
        text = re.sub(timex + '(?!</TIMEX2>)', '<TIMEX2>' + timex + '</TIMEX2>', text)

    return text

# Hash function for week days to simplify the grounding task.
# [Mon..Sun] -> [0..6]
hashweekdays = {
    'Monday': 0,
    'Tuesday': 1,
    'Wednesday': 2,
    'Thursday': 3,
    'Friday': 4,
    'Saturday': 5,
    'Sunday': 6}

# Hash function for months to simplify the grounding task.
# [Jan..Dec] -> [1..12]
hashmonths = {
    'January': 1,
    'February': 2,
    'March': 3,
    'April': 4,
    'May': 5,
    'June': 6,
    'July': 7,
    'August': 8,
    'September': 9,
    'October': 10,
    'November': 11,
    'December': 12}

# Hash number in words into the corresponding integer value
def hashnum(number):
    if re.match(r'one|^a\b', number, re.IGNORECASE):
        return 1
    if re.match(r'two', number, re.IGNORECASE):
        return 2
    if re.match(r'three', number, re.IGNORECASE):
        return 3
    if re.match(r'four', number, re.IGNORECASE):
        return 4
    if re.match(r'five', number, re.IGNORECASE):
        return 5
    if re.match(r'six', number, re.IGNORECASE):
        return 6
    if re.match(r'seven', number, re.IGNORECASE):
        return 7
    if re.match(r'eight', number, re.IGNORECASE):
        return 8
    if re.match(r'nine', number, re.IGNORECASE):
        return 9
    if re.match(r'ten', number, re.IGNORECASE):
        return 10
    if re.match(r'eleven', number, re.IGNORECASE):
        return 11
    if re.match(r'twelve', number, re.IGNORECASE):
        return 12
    if re.match(r'thirteen', number, re.IGNORECASE):
        return 13
    if re.match(r'fourteen', number, re.IGNORECASE):
        return 14
    if re.match(r'fifteen', number, re.IGNORECASE):
        return 15
    if re.match(r'sixteen', number, re.IGNORECASE):
        return 16
    if re.match(r'seventeen', number, re.IGNORECASE):
        return 17
    if re.match(r'eighteen', number, re.IGNORECASE):
        return 18
    if re.match(r'nineteen', number, re.IGNORECASE):
        return 19
    if re.match(r'twenty', number, re.IGNORECASE):
        return 20
    if re.match(r'thirty', number, re.IGNORECASE):
        return 30
    if re.match(r'forty', number, re.IGNORECASE):
        return 40
    if re.match(r'fifty', number, re.IGNORECASE):
        return 50
    if re.match(r'sixty', number, re.IGNORECASE):
        return 60
    if re.match(r'seventy', number, re.IGNORECASE):
        return 70
    if re.match(r'eighty', number, re.IGNORECASE):
        return 80
    if re.match(r'ninety', number, re.IGNORECASE):
        return 90
    if re.match(r'hundred', number, re.IGNORECASE):
        return 100
    if re.match(r'thousand', number, re.IGNORECASE):
      return 1000

# Given a timex_tagged_text and a Date object set to base_date,
# returns timex_grounded_text
def ground(tagged_text, base_date):

    # Find all identified timex and put them into a list
    timex_regex = re.compile(r'<TIMEX2>.*?</TIMEX2>', re.DOTALL)
    timex_found = timex_regex.findall(tagged_text)
    timex_found = map(lambda timex:re.sub(r'</?TIMEX2.*?>', '', timex), \
                timex_found)

    # Calculate the new date accordingly
    for timex in timex_found:
        timex_val = 'UNKNOWN' # Default value

        timex_ori = timex   # Backup original timex for later substitution

        # If numbers are given in words, hash them into corresponding numbers.
        # eg. twenty five days ago --> 25 days ago
        if re.search(numbers, timex, re.IGNORECASE):
            split_timex = re.split(r'\s(?=days?|months?|years?|weeks?)', \
                                                              timex, re.IGNORECASE)
            value = split_timex[0]
            unit = split_timex[1]
            num_list = map(lambda s:hashnum(s),re.findall(numbers + '+', \
                                          value, re.IGNORECASE))
            timex = `sum(num_list)` + ' ' + unit

        # If timex matches ISO format, remove 'time' and reorder 'date'
        if re.match(r'\d+[/-]\d+[/-]\d+ \d+:\d+:\d+\.\d+', timex):
            dmy = re.split(r'\s', timex)[0]
            dmy = re.split(r'/|-', dmy)
            timex_val = str(dmy[2]) + '-' + str(dmy[1]) + '-' + str(dmy[0])

        # Specific dates
        elif re.match(r'\d{4}', timex):
            timex_val = str(timex)

        # Relative dates
        elif re.match(r'tonight|tonite|today', timex, re.IGNORECASE):
            timex_val = str(base_date)
        elif re.match(r'yesterday', timex, re.IGNORECASE):
            timex_val = str(base_date + RelativeDateTime(days=-1))
        elif re.match(r'tomorrow', timex, re.IGNORECASE):
            timex_val = str(base_date + RelativeDateTime(days=+1))

        # Weekday in the previous week.
        elif re.match(r'last ' + week_day, timex, re.IGNORECASE):
            day = hashweekdays[timex.split()[1]]
            timex_val = str(base_date + RelativeDateTime(weeks=-1, \
                            weekday=(day,0)))

        # Weekday in the current week.
        elif re.match(r'this ' + week_day, timex, re.IGNORECASE):
            day = hashweekdays[timex.split()[1]]
            timex_val = str(base_date + RelativeDateTime(weeks=0, \
                            weekday=(day,0)))

        # Weekday in the following week.
        elif re.match(r'next ' + week_day, timex, re.IGNORECASE):
            day = hashweekdays[timex.split()[1]]
            timex_val = str(base_date + RelativeDateTime(weeks=+1, \
                              weekday=(day,0)))

        # Last, this, next week.
        elif re.match(r'last week', timex, re.IGNORECASE):
            year = (base_date + RelativeDateTime(weeks=-1)).year

            # iso_week returns a triple (year, week, day) hence, retrieve
            # only week value.
            week = (base_date + RelativeDateTime(weeks=-1)).iso_week[1]
            timex_val = str(year) + 'W' + str(week)
        elif re.match(r'this week', timex, re.IGNORECASE):
            year = (base_date + RelativeDateTime(weeks=0)).year
            week = (base_date + RelativeDateTime(weeks=0)).iso_week[1]
            timex_val = str(year) + 'W' + str(week)
        elif re.match(r'next week', timex, re.IGNORECASE):
            year = (base_date + RelativeDateTime(weeks=+1)).year
            week = (base_date + RelativeDateTime(weeks=+1)).iso_week[1]
            timex_val = str(year) + 'W' + str(week)

        # Month in the previous year.
        elif re.match(r'last ' + month, timex, re.IGNORECASE):
            month = hashmonths[timex.split()[1]]
            timex_val = str(base_date.year - 1) + '-' + str(month)

        # Month in the current year.
        elif re.match(r'this ' + month, timex, re.IGNORECASE):
            month = hashmonths[timex.split()[1]]
            timex_val = str(base_date.year) + '-' + str(month)

        # Month in the following year.
        elif re.match(r'next ' + month, timex, re.IGNORECASE):
            month = hashmonths[timex.split()[1]]
            timex_val = str(base_date.year + 1) + '-' + str(month)
        elif re.match(r'last month', timex, re.IGNORECASE):

            # Handles the year boundary.
            if base_date.month == 1:
                timex_val = str(base_date.year - 1) + '-' + '12'
            else:
                timex_val = str(base_date.year) + '-' + str(base_date.month - 1)
        elif re.match(r'this month', timex, re.IGNORECASE):
                timex_val = str(base_date.year) + '-' + str(base_date.month)
        elif re.match(r'next month', timex, re.IGNORECASE):

            # Handles the year boundary.
            if base_date.month == 12:
                timex_val = str(base_date.year + 1) + '-' + '1'
            else:
                timex_val = str(base_date.year) + '-' + str(base_date.month + 1)
        elif re.match(r'last year', timex, re.IGNORECASE):
            timex_val = str(base_date.year - 1)
        elif re.match(r'this year', timex, re.IGNORECASE):
            timex_val = str(base_date.year)
        elif re.match(r'next year', timex, re.IGNORECASE):
            timex_val = str(base_date.year + 1)
        elif re.match(r'\d+ days? (ago|earlier|before)', timex, re.IGNORECASE):

            # Calculate the offset by taking '\d+' part from the timex.
            offset = int(re.split(r'\s', timex)[0])
            timex_val = str(base_date + RelativeDateTime(days=-offset))
        elif re.match(r'\d+ days? (later|after)', timex, re.IGNORECASE):
            offset = int(re.split(r'\s', timex)[0])
            timex_val = str(base_date + RelativeDateTime(days=+offset))
        elif re.match(r'\d+ weeks? (ago|earlier|before)', timex, re.IGNORECASE):
            offset = int(re.split(r'\s', timex)[0])
            year = (base_date + RelativeDateTime(weeks=-offset)).year
            week = (base_date + \
                            RelativeDateTime(weeks=-offset)).iso_week[1]
            timex_val = str(year) + 'W' + str(week)
        elif re.match(r'\d+ weeks? (later|after)', timex, re.IGNORECASE):
            offset = int(re.split(r'\s', timex)[0])
            year = (base_date + RelativeDateTime(weeks=+offset)).year
            week = (base_date + RelativeDateTime(weeks=+offset)).iso_week[1]
            timex_val = str(year) + 'W' + str(week)
        elif re.match(r'\d+ months? (ago|earlier|before)', timex, re.IGNORECASE):
            extra = 0
            offset = int(re.split(r'\s', timex)[0])

            # Checks if subtracting the remainder of (offset / 12) to the base month
            # crosses the year boundary.
            if (base_date.month - offset % 12) < 1:
                extra = 1

            # Calculate new values for the year and the month.
            year = str(base_date.year - offset // 12 - extra)
            month = str((base_date.month - offset % 12) % 12)

            # Fix for the special case.
            if month == '0':
                month = '12'
            timex_val = year + '-' + month
        elif re.match(r'\d+ months? (later|after)', timex, re.IGNORECASE):
            extra = 0
            offset = int(re.split(r'\s', timex)[0])
            if (base_date.month + offset % 12) > 12:
                extra = 1
            year = str(base_date.year + offset // 12 + extra)
            month = str((base_date.month + offset % 12) % 12)
            if month == '0':
                month = '12'
            timex_val = year + '-' + month
        elif re.match(r'\d+ years? (ago|earlier|before)', timex, re.IGNORECASE):
            offset = int(re.split(r'\s', timex)[0])
            timex_val = str(base_date.year - offset)
        elif re.match(r'\d+ years? (later|after)', timex, re.IGNORECASE):
            offset = int(re.split(r'\s', timex)[0])
            timex_val = str(base_date.year + offset)

        # Remove 'time' from timex_val.
        # For example, If timex_val = 2000-02-20 12:23:34.45, then
        # timex_val = 2000-02-20
        timex_val = re.sub(r'\s.*', '', timex_val)

        # Substitute tag+timex in the text with grounded tag+timex.
        tagged_text = re.sub('<TIMEX2>' + timex_ori + '</TIMEX2>', '<TIMEX2 val=\"' \
            + timex_val + '\">' + timex_ori + '</TIMEX2>', tagged_text)

    return tagged_text

####

def demo():
    import nltk
    text = nltk.corpus.abc.raw('rural.txt')[:10000]
    print tag(text)

if __name__ == '__main__':
    demo()

########NEW FILE########
__FILENAME__ = data
# -*- coding: utf-8 -*-

# Natural Language Toolkit: Toolbox data file parser
#
# Copyright (C) 2001-2006 NLTK Project
# Author: Greg Aumann <greg_aumann@sil.org>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

"""module for reading Toolbox data files
"""

from nltk.etree.ElementTree import Element, SubElement, TreeBuilder
from nltk import toolbox
import re
import os
import types
import logging
from datetime import date

class ToolboxData(toolbox.ToolboxData):
    def _make_parse_table(self, grammar):
        """
        Return parsing state information used by tree_parser.
        """

        first = dict()
        gram = dict()
        for sym, value in grammar.items():
            first[sym] = value[0]
            gram[sym] = value[0] + value[1]
        parse_table = dict()
        for state in gram.keys():
            parse_table[state] = dict()
            for to_sym in gram[state]:        
                if to_sym in grammar:
                    # is a nonterminal
                    # assume all firsts are terminals
                    for i in first[to_sym]:        
                        parse_table[state][i] = to_sym
                else:
                    parse_table[state][to_sym] = to_sym
        return (parse_table, first)

    def grammar_parse(self, startsym, grammar, no_blanks=True, **kwargs):
        """
        Returns an element tree structure corresponding to a toolbox data file
        parsed according to the grammar.
        
        @type startsym: string
        @param startsym: Start symbol used for the grammar
        @type grammar: dictionary of tuple of tuples
        @param grammar: Contains the set of rewrite rules used to parse the 
        database.  See the description below.
        @type no_blanks: boolean
        @param no_blanks: blank fields that are not important to the structure are deleted
        @type kwargs: keyword arguments dictionary
        @param kwargs: Keyword arguments passed to L{toolbox.StandardFormat.fields()}
        @rtype:   ElementTree._ElementInterface
        @return:  Contents of toolbox data parsed according to rules in grammar
        
        The rewrite rules in the grammar look similar to those usually used in 
        computer languages. The difference is that the ordering constraints 
        that are usually present are relaxed in this parser. The reason is that 
        toolbox databases seldom have consistent ordering of fields. Hence the 
        right side of each rule consists of a tuple with two parts. The 
        fields in the first part mark the start of nonterminal.
        Each of them can occur only once and all those must
        occur before any of the fields in the second part of that nonterminal.
        Otherwise they are interpreted as marking the start
        of another one of the same nonterminal. If there is more than one
        in the first part of the tuple they do not need to all appear in a parse.
        The fields in the second part of the tuple can occur in any order.

        Sample grammar::
        
            grammar = {
                'toolbox':  (('_sh',),      ('_DateStampHasFourDigitYear', 'entry')),
                'entry':    (('lx',),       ('hm', 'sense', 'dt')),
                'sense':    (('sn', 'ps'),  ('pn', 'gv', 'dv',
                                             'gn', 'gp', 'dn', 'rn',
                                             'ge', 'de', 're',
                                             'example', 'lexfunc')),
                'example':  (('rf', 'xv',), ('xn', 'xe')),
                'lexfunc':  (('lf',),       ('lexvalue',)),
                'lexvalue': (('lv',),       ('ln', 'le')),
            }
        """
        parse_table, first = self._make_parse_table(grammar)
        builder = TreeBuilder()
        pstack = list()
        state = startsym
        first_elems = list()
        pstack.append((state, first_elems))
        builder.start(state, {})
        field_iter = self.fields(**kwargs)
        loop = True
        try:
            mkr, value = field_iter.next()
        except StopIteration:
            loop = False
        while loop:
            (state, first_elems) = pstack[-1]
            if mkr in parse_table[state]:
                next_state = parse_table[state][mkr]
                if next_state == mkr:
                    if mkr in first[state]:
                        # may be start of a new nonterminal
                        if mkr not in first_elems:
                            # not a new nonterminal
                            first_elems.append(mkr)
                            add = True
                        else:
                            # a new nonterminal, second or subsequent instance
                            add = False
                            if len(pstack) > 1:
                                builder.end(state)
                                pstack.pop()
                            else:
                                raise ValueError, \
                                      'Line %d: syntax error, unexpected marker %s.' % (self.line_num, mkr)
                    else:
                        # start of terminal marker
                        add = True
                    if add:
                        if not no_blanks or value:
                            builder.start(mkr, dict())
                            builder.data(value)
                            builder.end(mkr)
                        try:
                            mkr, value = field_iter.next()
                        except StopIteration:
                            loop = False
                else:
                    # a non terminal, first instance
                    first_elems = list()
                    builder.start(next_state, dict())
                    pstack.append((next_state, first_elems))
            else:
                if len(pstack) > 1:
                    builder.end(state)
                    pstack.pop()
                else:
                    raise ValueError, \
                          'Line %d: syntax error, unexpected marker %s.' % (self.line_num, mkr)
        for state, first_elems in reversed(pstack):
            builder.end(state)
        return builder.close()

def parse_corpus(file_names,  directory='',  grammar=None,  **kwargs):
    """
    Returns an element tree structure corresponding to the toolbox data files
    parsed according to the grammar.
    
    @type file_names: C{string} or C{sequence} of C{string}s
    @param file_names: name or sequence of names of files to be parsed
    @type directory: C{string}
    @param directory: Name of directory to which the file_names are appended 
    @type grammar: C{string} or C{None}
    @param grammar: grammar describing the structure of the toolbox files.
    @rtype:   C{ElementTree._ElementInterface}
    @return:  Contents of toolbox data parsed according to rules in grammar
    return parses of all the dictionary files"""
    if isinstance(file_names, types.StringTypes):
        file_names = (file_names, )
    db =  toolbox.ToolboxData()
    all_data = data_header = None
    for fname in file_names:
        db.open(os.path.join(directory, fname))
        logging.info('about to parse %s' % fname)
        try:
            cur_data = db.parse(grammar, **kwargs)
        except ValueError, msg:
            logging.error('%s: %s' % (fname, msg))
            db.close()
            continue
        db.close()
        if all_data is not None:
            header = cur_data.find('header')
            if header != data_header:
                raise ValueError,  "cannot combine databases with different types"
            for elem in cur_data.findall('record'):
                all_data.append(elem)
        else:
            all_data = cur_data
    return all_data

def indent(elem, level=0):
    """
    Recursive function to indent an ElementTree._ElementInterface
    used for pretty printing. Code from 
    U{http://www.effbot.org/zone/element-lib.htm}. To use run indent
    on elem and then output in the normal way. 
    
    @param elem: element to be indented. will be modified. 
    @type elem: ElementTree._ElementInterface
    @param level: level of indentation for this element
    @type level: nonnegative integer
    @rtype:   ElementTree._ElementInterface
    @return:  Contents of elem indented to reflect its structure
    """
    i = "\n" + level*"  "
    if len(elem):
        if not elem.text or not elem.text.strip():
            elem.text = i + "  "
        for elem in elem:
            indent(elem, level+1)
        if not elem.tail or not elem.tail.strip():
            elem.tail = i
    else:
        if level and (not elem.tail or not elem.tail.strip()):
            elem.tail = i

def to_sfm_string(tree, encoding=None, errors='strict', unicode_fields=None):
    """Return a string with a standard format representation of the toolbox
    data in tree (tree can be a toolbox database or a single record). Should work for trees
    parsed by grammar_parse too.
    
    @param tree: flat representation of toolbox data (whole database or single record)
    @type tree: ElementTree._ElementInterface
    @param encoding: Name of an encoding to use.
    @type encoding: string
    @param errors: Error handling scheme for codec. Same as the C{encode} 
        inbuilt string method.
    @type errors: string
    @param unicode_fields:
    @type unicode_fields: string
    @rtype:   string
    @return:  string using standard format markup
    """
    # write SFM to file
    # unicode_fields parameter does nothing as yet
    l = list()
    _to_sfm_string(tree, l, encoding=encoding, errors=errors, unicode_fields=unicode_fields)
    s = ''.join(l)
    if encoding is not None:
        s = s.encode(encoding, errors)
    return s
    
_is_value = re.compile(r"\S")

def _to_sfm_string(node, l, **kwargs):
    # write SFM to file
    if len(node) == 0:
        tag = node.tag
        text = node.text
        if text is None:
            l.append('\\%s\n' % tag)
        elif re.search(_is_value, text):
            l.append('\\%s %s\n' % (tag, text))
        else:
            l.append('\\%s%s\n' % (tag, text))
    else:
        #l.append('\n')
        for n in node:
            _to_sfm_string(n, l, **kwargs)
    return

_months = None
_month_abbr = (
    'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
    'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec')

def _init_months():
    months = dict()
    for i, m in enumerate(_month_abbr):
        months[m] = i + 1
    return months

def to_date(s, four_digit_year=True):
    """return a date object corresponding to the Toolbox date in s.
    
    @param s: Toolbox date
    @type s: string
    @param four_digit_year: Do Toolbox dates use four digits for the year? 
        Defaults to True.
    @type four_digit_year: boolean
    @return: date
    @rtype: datetime.date
    """
    global _months
    if _months is None:
        _months = _init_months()
    fields = s.split('/')
    if len(fields) != 3:
        raise ValueError, 'Invalid Toolbox date "%s"' % s
    day = int(fields[0])
    try:
        month = _months[fields[1]]
    except KeyError:
        raise ValueError, 'Invalid Toolbox date "%s"' % s
    year = int(fields[2])
    return date(year, month, day)

def from_date(d, four_digit_year=True):
    """return a Toolbox date string corresponding to the date in d.
    
    @param d: date
    @type d: datetime.date
    @param four_digit_year: Do Toolbox dates use four digits for the year? 
        Defaults to True.
    @type four_digit_year: boolean
    @return: Toolbox date
    @rtype: string
    """
    return '%04d/%s/%02d' % (date.day, _month_abbr[date.month-1], date.year)

def elem_append_string(elem,  s):
    """Append s to C{elem}.
    
    @param elem: This is modified by the function.  It may have subelements.
    @type elem: C{ElementTree._ElementInterface}
    @param s: text to be appended to C{elem}
    @type s: C{String}
    """
    if len(elem):
        elem[-1].tail = (elem[-1].tail or "") + s
    else:
        elem.text = (elem.text or "") + s
    
char_code_attribs = {
    'fv':   {'lang':  'v'}, 
    'fe':   {'lang':  'e'}, 
    'fn':   {'lang':  'n'}, 
    'fr':   {'lang':  'r'}, 
    'fs':   {'class':  'fs'}, 
    'fl':   {'class':  'fl'}, 
    'fb':   {'class':  'fb'},       # bold
    'fi':   {'class':  'fi'},       # italic
    'u'  :   {'class':  'u'},         # general underlined
    'uc':   {'class':  'uc'},       # specific underlined
    'ub':   {'class':  'ub'},       # specific underlined bold
    'ui':   {'class':  'ui'},       # specific underlined italic
    'hm':   {'class':  'hm'},       # homonym
}

char_codes = '|'.join(sorted(char_code_attribs.keys(),  key=lambda s: (-len(s),  s)))
word_pat = re.compile(r'(%s):([^\s:;,.?!(){}\[\]]+)' % char_codes)
bar_pat = re.compile(r'\|(%s)\{([^}]*)(?:\}|$)'  % char_codes)
    
def _append_char_coded_text(elem,  s,  char_code_pat):
    """Append s to C{elem} with text in coded with character style codes  converted to span elements.
    
    @param elem: element corresponding to an MDF field. This is modified by the function. 
        It may already have 'span' subelements corresponding to character styled text earlier in the MDF field.
    @type elem: C{ElementTree._ElementInterface}
    @param s: field contents possibly including parts coded with MDF character style codes
    @type s: C{String}
    @param char_code_pat:  compiled regular expression describing the character styled text with the style 
        code. It must have two sets of capturing parentheses. The first set captures the style code and the 
        second the styled text.
    @type char_code_pat: compiled regular expression pattern
    """
    mobj = char_code_pat.search(s)
    pos = 0
    while mobj is not None:
        elem_append_string(elem,  s[pos:mobj.start()])
        attribs = char_code_attribs[mobj.group(1)]
        span_elem = SubElement(elem, 'span',  attribs)
        span_elem.text = mobj.group(2)
        pos = mobj.end()
        mobj = char_code_pat.search(s,  pos)
    elem_append_string(elem,  s[pos:])
    
def _inline_char_coded_text(elem,  char_code_pat):
    """replace char coded text in C{elem} with span elements with appropriate attributes
        
    @param elem: element corresponding to an MDF field. This is modified by the function. 
        It may already have 'span' subelements corresponding to character styled text earlier in the MDF field.
    @type elem: C{ElementTree._ElementInterface}
    @param char_code_pat:  compiled regular expression describing the character styled text with the style 
        code. It must have two sets of capturing parentheses. The first set captures the style code and the 
        second the styled text.
    @type char_code_pat: compiled regular expression pattern
    """
    text = elem.text
    elem.text = None
    sub_elems = elem[:]
    elem[:] = []
    _append_char_coded_text(elem,  text,  char_code_pat)
    for sub_elem in sub_elems:
        elem.append(sub_elem)
        tail = sub_elem.tail
        if tail:
            sub_elem.tail = None
            _append_char_coded_text(elem,  tail,  char_code_pat)
            
def inline_char_coded_elem(elem):
    """replace char coded text in C{elem} with span elements with appropriate attributes
        
    @param elem: element corresponding to an MDF field. This is modified by the function. 
        It may already have 'span' subelements corresponding to character styled text earlier in the MDF field.
    @type elem: C{ElementTree._ElementInterface}
    """
    _inline_char_coded_text(elem,  word_pat)
    # text coded with the word_pat uses underscores to seperate words
    for e in elem.getiterator('span'):
        e.text = e.text.replace('_',  ' ')
    _inline_char_coded_text(elem,  bar_pat)

def inline_char_coded_text(tag,  s):
    """return an element with the char coded text converted to span elements with appropriate attributes
        
    @param tag: tag for returned element
    @type tag: C{String}
    @param s: element corresponding to an MDF field. This is modified by the function. 
        It may already have 'span' subelements corresponding to character styled text earlier in the MDF field.
    @type s: C{String}
    @return: an element with the character code styles converted to spans elements.
    @rtype: C{ElementTree._ElementInterface}
    """
    elem = Element(tag)
    elem.text= s
    inline_char_coded_elem(elem)
    return elem

def text_lang_iterator(elem,  lang=None):
    """return a list of text in document order tagged with the contents of the 'lang' attribute
        
    @param elem: element corresponding to a char coded MDF field
    @type elem: C{String}
    @param lang: default value of the lang attribute for top level text.
    @type lang: C{String}
    @return: list of text in the element and its children in document order 
        tagged with the value of the 'lang' attribute.
    @rtype: list of tuples
    """
    text_segs = []
    text = elem.text
    if text:
        text_segs.append((text,  lang))
    for child in elem:
        text_segs.extend(text_lang_iterator(child,  child.get('lang')))
        tail = child.tail
        if tail:
            text_segs.append((tail,  lang))
    return text_segs

def demo_flat():
    from nltk.etree.ElementTree import ElementTree    
    import sys

    tree = ElementTree(toolbox.xml('iu_mien_samp.db', key='lx', encoding='utf8'))
    tree.write(sys.stdout)
    

if __name__ == '__main__':
    demo_flat()

########NEW FILE########
__FILENAME__ = analyse_toolbox
# -*- coding: utf-8 -*-

# Natural Language Toolkit: Toolbox Analyser
#
# Copyright (C) 2001-2011 NLTK Project
# Author: Greg Aumann <greg_aumann@sil.org>
# URL: <http://www.nltk.org>
# For license information, see LICENSE.TXT

"""A tool for developing a chunking grammar to parse toolbox lexical databases

to run it:
    python analyse_toolbox.py -g mdfsamp_grammar -x lexicon.xml path_to_MDFSamp.db
you need to give the full path to MDFSamp.db that comes with NLTK in
nltk_data/corpora/toolbox/MDF/MDFSampl.db

It assumes there is a chunking grammar in file called mdfsamp_grammar. The top level
nonterminal in this chunking grammar must be "entry".
A nicely indented XML version of the lexicon structured according to the grammar is
output to a file and the analysis report is sent to the standard output.

It outputs the number of occurences of each nonterminal in the grammar plus
the number of occurences of each structure that parses as that nonterminal.
The structures are listed in descending order of frequency with frequency counts. For
nonterminals that do not occur more than thirty times it lists the entries (including
homonym number) where that structure occurs. It also outputs a frequency count of
markers in the databases. Toolbox records that do not fully parse are listed under
"record" rather than under the "entry" nonterminal.
"""

import logging

from nltk import toolbox
from nltk.probability import FreqDist
from nltk_contrib.toolbox import parse_corpus, indent
import nltk.etree.ElementTree as ET

logging.basicConfig(level=logging.WARNING)

def string_from_entry(entry):
    lexeme = entry.findtext('lc')
    if not lexeme:
        lexeme = entry.findtext('lx')
    homonym_num = entry.findtext('hm')
    s = lexeme
    if homonym_num:
        s += '_%s' % homonym_num
    return s

def analyse_dict(structure):
    results = dict()
    for elem in structure.findall('record'):
        if elem[0].tag == 'entry' and len(elem) == 1:
            elem = elem[0]
        position = string_from_entry(elem)
        if len(elem) > 0:
            _analyse(elem, results, position)
    return results

def _analyse(structure, results, position):
    pattern = tuple([e.tag for e in structure])
    results.setdefault(structure.tag, dict()).setdefault(pattern, list()).append(position)
    for elem in structure:
        if len(elem) > 0:
            _analyse(elem, results, position)

def pattern_count(patt_dict):
    n = 0
    for value in patt_dict.values():
        n += len(value)
    return n

def count_mkrs(lexicon):
    mkr_count = FreqDist()
    nonblank_mkr_count = FreqDist()
    for record in lexicon.findall('record'):
        for e in record.getiterator():
            if len(e) == 0:
                tag = e.tag
                mkr_count.inc(tag)
                if e.text.strip():
                    nonblank_mkr_count.inc(tag)
    return (mkr_count, nonblank_mkr_count)

def process(dict_names, gram_fname, xml, encoding):
    """"""
    gram_file = open(gram_fname, 'r')
    gram = gram_file.read()
    gram_file.close()
    lexicon = parse_corpus(dict_names, grammar=gram, encoding=encoding, errors='replace')
    mkr_counts, nonblank_mkr_counts = count_mkrs(lexicon)
    analysis = analyse_dict(lexicon)
    if xml:
        indent(lexicon)
        out_file = open(xml, "w")
        out_file.write(ET.tostring(lexicon, encoding='UTF-8'))
        out_file.close()

    print 'analysing files\n%s\n' % '\n'.join(dict_names)
    if xml:
        print 'XML lexicon output in file "%s"\n' % xml
    print '====chunk grammar===='
    print gram
    print '\n'
    max_positions = 30
    for structure, patt_dict in analysis.items():
        print '\n\n===%s===: total= %d' %(structure, pattern_count(patt_dict))
        for pattern, positions in sorted(patt_dict.items(), key=lambda t: (-len(t[1]), t[0])):
            if len(positions) <= max_positions:
                pos_str = 'Entries: %s' % ', '.join(positions)
            else:
                pos_str = 'Too many entries to list.'
            print "\t%5d:  %s %s" % (len(positions), ':'.join(pattern), pos_str)
    print "\n\n"
    print 'mkr\tcount\tnonblank'
    for mkr in mkr_counts:
        print '%s\t%5d\t%5d' % (mkr, mkr_counts.get(mkr, 0), nonblank_mkr_counts.get(mkr, 0))


if __name__ == "__main__":
    from optparse import OptionParser
    usage = "usage: %prog [options] toolbox_db1 [toolbox_db2] ..."
    parser = OptionParser(usage=usage)
    parser.add_option("-e", "--enc", dest="encoding", default="cp1252",
                      help="encoding of toolbox database(s)")
    parser.add_option("-g", "--gram", dest="grammar", default="grammar",
                      help="file containing chunk grammar describing the structure of the toolbox database(s)")
    parser.add_option("-x", "--xml", dest="xml", default=None,
                      help="name of file for output of parsed contents of toolbox database(s) in indented xml")
    (options, args) = parser.parse_args()
    if len(args) == 0:
        parser.error("you must specify at least one toolbox_db file")
    process(args, gram_fname=options.grammar, xml=options.xml, encoding=options.encoding)

########NEW FILE########
__FILENAME__ = demo1
# -*- coding: utf-8 -*-

# Natural Language Toolkit: Toolbox Data demonstration
#
# Copyright (C) 2001-2006 NLTK Project
# Author: Greg Aumann <greg_aumann@sil.org>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

"""
corresponds to 
12.3.1   Accessing Toolbox Data
in http://nltk.sourceforge.net/lite/doc/en/data.html
"""

from nltk.corpus import toolbox

lexicon = toolbox.xml('rotokas.dic')

sum_size = num_entries = 0
for entry in lexicon.findall('record'):
    num_entries += 1
    sum_size += len(entry)
print sum_size/num_entries


from nltk.etree.ElementTree import ElementTree
import sys
fourth_entry = lexicon.findall('record')[3]
tree = ElementTree(fourth_entry)
tree.write(sys.stdout)

########NEW FILE########
__FILENAME__ = demo2
# -*- coding: utf-8 -*-

# Natural Language Toolkit: Toolbox Data demonstration
#
# Copyright (C) 2001-2006 NLTK Project
# Author: Greg Aumann <greg_aumann@sil.org>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

"""
corresponds to 
12.3.1   Accessing Toolbox Data
in http://nltk.sourceforge.net/lite/doc/en/data.html
"""

from nltk.corpus import toolbox

lexicon = toolbox.xml('rotokas.dic')
lexemes = []
for lexeme in lexicon.findall('record/lx'):
    normalised_lexeme = lexeme.text.lower()
    lexemes.append(normalised_lexeme)

# list comprehension approach
lexemes2=[lexeme.text.lower() for lexeme in lexicon.findall('record/lx')]

##if lexemes != lexemes2:
##    print 'error two lists not equal'
##else:
##    print repr(lexemes)

import re
def cv(s):
    s = s.lower()
    s = re.sub(r'[^a-z]',     r'_', s)
    s = re.sub(r'[aeiou]',    r'V', s)
    s = re.sub(r'[^V_]',     r'C', s)
    return (s)
    
for field in lexicon[50].getchildren():
    print "\\%s %s" % (field.tag, field.text)
    if field.tag == "lx":
        print "\\cv %s" % cv(field.text)

########NEW FILE########
__FILENAME__ = demo3
# -*- coding: utf-8 -*-

# Natural Language Toolkit: Toolbox Data demonstration
#
# Copyright (C) 2001-2006 NLTK Project
# Author: Greg Aumann <greg_aumann@sil.org>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

"""
demonstration of grammar parsing
"""

from nltk.etree.ElementTree import ElementTree
from nltk_contrib import toolbox
from nltk.corpus import find_corpus_file
import os.path

grammar = {
        'toolbox':      (('_sh',), ('_DateStampHasFourDigitYear', 'entry')),
        'entry':          (('lx',), ('hm', 'sense', 'dt')),
        'sense':          (('sn', 'ps'), ('pn', 'gv', 'dv',
                                   'gn', 'gp', 'dn', 'rn',
                                   'ge', 'de', 're',
                                   'example', 'lexfunc')),
        'example':      (('rf', 'xv',), ('xn', 'xe')),
        'lexfunc':      (('lf',), ('lexvalue',)),
        'lexvalue':    (('lv',), ('ln', 'le')),
}

db = toolbox.ToolboxData()
db.open(find_corpus_file('toolbox', 'iu_mien_samp.db'))
lexicon = db.grammar_parse('toolbox', grammar, encoding='utf8')
tree = ElementTree(lexicon)
tree.write('iu_mien_samp.xml', encoding='utf8')
num_lexemes = 0
num_senses = 0
num_examples = 0
for lexeme in lexicon.findall('entry'):
    num_lexemes += 1
    for sense in lexeme.findall('sense'):
        num_senses += 1
        for example in sense.findall('example'):
            num_examples += 1
print 'num. lexemes  =', num_lexemes
print 'num. senses   =', num_senses
print 'num. examples =', num_examples

#another approach
print 'num. examples =', len(lexicon.findall('.//example'))

########NEW FILE########
__FILENAME__ = demo4
# -*- coding: utf-8 -*-

# Natural Language Toolkit: Toolbox Data demonstration
#
# Copyright (C) 2001-2006 NLTK Project
# Author: Greg Aumann <greg_aumann@sil.org>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

"""
demonstration of grammar parsing
"""

from nltk.etree.ElementTree import ElementTree
from nltk_contrib import toolbox
from nltk.corpus import find_corpus_file
import os.path, sys

grammar = r"""
      lexfunc: {<lf>(<lv><ln|le>*)*}
      example: {<rf|xv><xn|xe>*}
      sense:   {<sn><ps><pn|gv|dv|gn|gp|dn|rn|ge|de|re>*<example>*<lexfunc>*}
      record:   {<lx><hm><sense>+<dt>}
    """

db = toolbox.ToolboxData()
db.open(find_corpus_file('toolbox', 'iu_mien_samp.db'))
lexicon = db.chunk_parse(grammar, encoding='utf8')
toolbox.data.indent(lexicon)
tree = ElementTree(lexicon)
tree.write(sys.stdout, encoding='utf8')

########NEW FILE########
__FILENAME__ = errors
# Natural Language Toolkit: Shoebox Errors
#
# Copyright (C) 2001-2006 NLTK Project
# Author: Stuart Robinson <Stuart.Robinson@mpi.nl>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

"""
This module provides Shoebox exceptions.
"""

# ---------------------------------------------------------------------
# CLASS:  ShoeboxError
# DESC:   ???
# ---------------------------------------------------------------------

class ShoeboxError(Exception):
    """
    This is the base class for all Shoebox errors.
    """
    def __init__(self):
        self._msg = ""

    
# ---------------------------------------------
# CLASS:  ValidationError
# DESC:   ???
# ---------------------------------------------

class NonUniqueEntryError(ShoeboxError):
    """
    ???
    """
    def __init__(self) :
        pass

class ValidationError(ShoeboxError):    

    def __init__(self):
        pass

    def setField(self, field):
       self._field = field
    
    def getField(self):
       return self._field


# ---------------------------------------------
# CLASS:  NoMetadataFound
# DESC:   ???
# ---------------------------------------------

class NoMetadataFound(ValidationError):

  def __init__(self, field):
    self._field = field


class FieldError(ShoeboxError):

    def __init__(self):
        pass

    def __str__(self) :
        return self.get_message()

  
class NonUniqueFieldError(FieldError):
  """
  Error raised when an attempt is made to retrieve a unique field which has more than one value
  """
  def __init__(self, entry):
    self._entry = entry

  def setEntry(self, entry):
    self._entry = entry
    
  def getEntry(self):
    return self._entry


# ---------------------------------------------
# CLASS:  BadFieldValue
# DESC:   ???
# ---------------------------------------------

class BadFieldValueError(ValidationError, FieldError):
  
  FIELD_VALUE_ERROR_RANGE_SET    = '1'
  FIELD_VALUE_ERROR_NO_WORD_WRAP = '2'
  FIELD_VALUE_ERROR_EMPTY_VALUE  = '3'
  FIELD_VALUE_ERROR_SINGLE_WORD  = '4'
  
  errorTypes = {
    '1': "Range Set",
    '2': "No Word Wrap",
    '3': "Empty Value",
    '4': "Single Word"
    }

  def __init__(self, errorType, entry, field, fmMetadata):
    self._entry       = entry
    self._errorType   = errorType
    self._field       = field
    self._fmMetadata  = fmMetadata

  def __str__(self):
    e   = self.getEntry()
    f   = self.getField()
    typ = self.getErrorDescription()
    s = "'%s' error in '\\%s' field of record %i!\nRecord:\n%s" % (typ, f.getMarker(), e.getNumber(), e.getRawText())
    return s

  def getFieldMarkerMetadata(self):
    return self._fmMetadata

  def setFieldMarkerMetadata(self, fmMetadata):
    self._fmMetadata = fmMetadata

  def getErrorDescription(self):
    try:
      return self.errorTypes[self.getErrorType()]
    except:
      return None
    
  def getErrorType(self):
    return self._errorType

  def setErrorType(self, errorType):
    self._errorType = errorType
    
  def getEntry(self):
    return self._entry

  def setEntry(self, entry):
    self._entry = entry

########NEW FILE########
__FILENAME__ = etreelib
# -*- coding: utf-8 -*-

# Natural Language Toolkit: Toolbox data file parser
#
# Copyright (C) Frederick Lundh
# Author: Frederick Lundh
# URL: <http://effbot.org>

"""
module of utility functions for ElementTree structures. Many
are from U{http://effbot.org/zone/element-lib.htm}
"""

import nltk.etree.ElementTree as ET

class _E(object):

    def __call__(self, tag, *children, **attrib):
        elem = ET.Element(tag, attrib)
        for item in children:
            if isinstance(item, dict):
                elem.attrib.update(item)
            elif isinstance(item, basestring):
                if len(elem):
                    elem[-1].tail = (elem[-1].tail or "") + item
                else:
                    elem.text = (elem.text or "") + item
            elif ET.iselement(item):
                elem.append(item)
            else:
                raise TypeError("bad argument: %r" % item)
        return elem

    def __getattr__(self, tag):
        return functools.partial(self, tag)

# create factory object
E = _E()


def append(elem, item):
    """append a string or an element to elem
    code from U{http://effbot.org/zone/element-lib.htm#append}

    @type elem: ElementTree.Element
    @param elem: parent element that item is appended to
    @type item: string or ElementTree.Element
    @param item: string or element appended to elem
    """
    if isinstance(item, basestring):
        if len(elem):
            elem[-1].tail = (elem[-1].tail or "") + item
        else:
            elem.text = (elem.text or "") + item
    else:
        elem.append(item)

def indent(elem, level=0):
    """
    Recursive function to indent an ElementTree._ElementInterface
    used for pretty printing. Based on code from 
    U{http://www.effbot.org/zone/element-lib.htm}. To use run indent
    on elem and then output in the normal way. 
    
    @param elem: element to be indented. will be modified. 
    @type elem: ElementTree._ElementInterface
    @param level: level of indentation for this element
    @type level: nonnegative integer
    @rtype:   ElementTree._ElementInterface
    @return:  Contents of elem indented to reflect its structure
    """
    i = "\n" + level*"  "
    if len(elem):
        if not elem.text or not elem.text.strip():
            elem.text = i + "  "
        for child in elem:
            indent(child, level+1)
        if not child.tail or not child.tail.strip():
            child.tail = i
        if not elem.tail or not elem.tail.strip():
            elem.tail = i
##        for elem in elem:
##            indent(elem, level+1)
##        if not elem.tail or not elem.tail.strip():
##            elem.tail = i
    else:
        if level and (not elem.tail or not elem.tail.strip()):
            elem.tail = i

if __name__ == '__main__':
    pass

########NEW FILE########
__FILENAME__ = iu_mien_hier
# -*- coding: utf-8 -*-

# Natural Language Toolkit: Toolbox data file parser
#
# Copyright (C) 2001-2006 NLTK Project
# Author: Greg Aumann <greg_aumann@sil.org>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

"""Grammar for the Toolbox MDF Alternate Hierarchy."""

# this dictionary lists all the markers that can occur in a given section
# of a shoebox record. The order is not used in parsing but may be when
# outputting a record. 

# the fields in the first tuple mark the start of nonterminal.
# Each field can occur only once and all those must
# occur before any other field in a nonterminal
# otherwise they are interpreted as marking the start
# of another one of the same nonterminal
# Fields in the second tuple alse can occur in that nonterminal.
# They must occur after those in the first tuple in 
# a given instance of the nonterminal

grammar = {
        'toolbox':   (('_sh',), ('_DateStampHasFourDigitYear', 'entry')),
        'entry':       (('lx',), ('hm', 'id', 'lc', 'ph', 'sh', 'mr', 'variant', 'sense', 'bw', 'etym',
                               'paradigm', 'st', 'subentry', 'dt')),
        'subentry': (('se',), ('hm', 'id', 'lc', 'ph', 'mr', 'variant', 'sense', 'bw', 'etym', 
                                'paradigm', 'st')),
        'variant':   (('va',), ('vn', 've', 'vr')),
        'sense':       (('sn', 'ps', 'pn'), ('gv', 'dv',
                                'chingloss', 'dn', 'chinrev', 'wn',
                                'ge', 'de', 're', 'we',
                                'gr', 'dr', 'rr', 'wr',
                                'lt', 'sc', 'example', 'usage', 'encyc', 'only',
                                'lexfunc', 'sy', 'an', 'crossref', 'mn', 'tb', 'sd', 'is', 'th', 'notes', 'so', 'bb')),
        'chingloss': (('gn',), ('gp',)),
        'chinrev':   (('rn',), ('rp',)),
        'example':   (('rf', 'xv'), ('xn', 'xe', 'xr')),
        'usage':       (('uv', 'un', 'ue'), ('ur',)),
        'encyc':       (('ev', 'en', 'ee'), ('er',)),
        'only':         (('ov', 'on', 'oe'), ('or',)),
        'lexfunc':   (('lf',), ('lexvalue',)),
        'lexvalue': (('lv',), ('ln', 'le', 'lr')),
        'crossref': (('cf',), ('cn', 'ce', 'cr')),
        'notes':       (('nt', 'np', 'ng', 'nd', 'na', 'ns', 'nq'), ()),
        'etym':         (('et',), ('eg', 'es', 'ec')),
        'paradigm': (('pd', 'pdl', 'pdv'), ('pdn', 'pde', 'pdr',
                               'sg', 'pl', 'rd', 
                               '1s', '2s', '3s', '4s', 
                               '1d', '2d', '3d', '4d',
                               '1p', '1i', '1e', '2p', '3p', '4p'))
        }

chunk_grammar = """
      etym: {<et><eg|es|ec>*}
      notes: {<nt|np|ng|nd|na|ns|nq>+}
      crossref: {<cf><cn|ce|cr>*}
      lexvalue: {<lv><ln|le|lr>*}
      lexfunc: {<lf><lexvalue>*}
      only: {<ov|on|oe>*<or>?}
      encyc: {<ev|en|ee>*<er>?}
      usage: {<uv|un|ue>*<ur>?}
      example: {<rf|xv><xn|xe>*}
      sense:   {<sn><ps|pn>*<gv|dv|gn|gp|dn|rn|wn|ge|de|re|we|lt|sc>*<example>*<usage>?<encyc>?<only>?<lexfunc>*<crossref>*<mn|tb|sd|is|th|>*<notes>*<so>*}
      variant:  { <va><vn|ve|vr>*}
      subentry:   {<se><hm><id>?<lc>?<ph>?<mr>?<variant>*<sense>+<bw>?<etym>?<paradigm>?<st>}
      entry:   {<lx><hm>?<id>?<lc>?<ph>?<sh>?<mr>?<variant>*<sense>+<bw>?<etym>?<paradigm>?<st>*<subentr>*<dt>}
"""

field_order = {
        'toolbox':   ('_sh', '_DateStampHasFourDigitYear', 'entry'),
        'entry':       ('lx', 'hm', 'sh', 'id', 'lc', 'ph', 'mr', 'variant', 'sense', 'bw', 'etym',
                               'paradigm', 'st', 'subentry', 'dt'),
        'subentry': ('se', 'hm', 'id', 'lc', 'ph', 'mr', 'variant', 'sense', 'bw', 'etym', 
                                'paradigm', 'st'),
        'variant':   ('va', 'vn', 've', 'vr'),
        'sense':       ('sn', 'ps', 'pn', 'gv', 'dv',
                                'chingloss', 'dn', 'chinrev', 'wn',
                                'ge', 'de', 're', 'we',
                                'gr', 'dr', 'rr', 'wr',
                                'lt', 'sc', 'example', 'usage', 'encyc', 'only',
                                'lexfunc', 'sy', 'an', 'crossref', 'mn', 'tb', 'sd', 'is', 'th', 'notes', 'so'),
        'chingloss': ('gn', 'gp'),
        'chinrev':   ('rn', 'rp'),
        'example':   ('rf', 'xv', 'xn', 'xe', 'xr'),
        'usage':       ('uv', 'un', 'ue', 'ur'),
        'encyc':       ('ev', 'en', 'ee', 'er'),
        'only':         ('ov', 'on', 'oe', 'or'),
        'lexfunc':   ('lf', 'lexvalue'),
        'lexvalue': ('lv', 'ln', 'le', 'lr'),
        'crossref': ('cf', 'cn', 'ce', 'cr'),
        'notes':       ('nt', 'np', 'ng', 'nd', 'na', 'ns', 'nq' ),
        'etym':         ('et', 'eg', 'es', 'ec'),
        'paradigm': ('pd', 'pdl', 'pdv', 'pdn', 'pde', 'pdr',
                               'sg', 'pl', 'rd', 
                               '1s', '2s', '3s', '4s', 
                               '1d', '2d', '3d', '4d',
                               '1p', '1i', '1e', '2p', '3p', '4p')
        }

default_fields = {
        'toolbox':   ('_sh', '_DateStampHasFourDigitYear'),
        'entry':       ('lx', 'hm', 'variant', 'sense', 'bw', 'st'),
        'subentry': ('se', 'hm', 'variant', 'sense', 'bw', 'st'),
        'variant':   ('va', ),
        'sense':       ('sn', 'ps', 'dv',
                                'chingloss', 'dn', 
                                'ge', 'de', 
                                'example', 'lexfunc'),
        'chingloss': ('gn', 'gp'),
        'chinrev':   ('rn', 'rp'),
        'example':   ('xv', 'xn', 'xe'),
        'usage':       ('uv', 'un', 'ue'),
        'encyc':       ('ev', 'en', 'ee'),
        'only':         ('ov', 'on', 'oe'),
        'lexfunc':   ('lf', 'lexvalue'),
        'lexvalue': ('lv', ),
        'crossref': ('cf', ),
        'notes':       ('nt', 'nq' ),
        'etym':         ('et', 'eg', ),
        }

blanks_before = {
        'toolbox':   ('entry',),
        'entry':       ('variant', 'sense', 'bw', 'paradigm', 'subentry', 'bw'),
        'subentry': ('variant', 'sense', 'bw', 'paradigm', 'bw'),
        'sense':       ('example', 'usage', 'encyc', 'only', 'lexfunc', 'crossref', 'is', 'notes', 'so'),
        }

blanks_between = {
        'toolbox':   ('entry',),
        'entry':       ('sense', 'bw', 'paradigm', 'subentry', 'bw'),
        'subentry': ('sense', 'bw', 'paradigm', 'bw'),
        'sense':       ('example', 'usage', 'encyc', 'only', 'lexfunc', 'crossref', 'so'),
        }

########NEW FILE########
__FILENAME__ = language
# -*- coding: utf-8 -*-

# Natural Language Toolkit: Toolbox Settings Parser
#
# Copyright (C) 2001-2006 NLTK Project
# Author: Greg Aumann <greg_aumann@sil.org>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

"""
This module provides functionality for reading language settings files for 
Toolbox. 
"""

from nltk.etree.ElementTree import TreeBuilder
from nltk.toolbox import ToolboxSettings
import re

class Letter(object):
    __slots__ = ('upper', 'lower')
    
    def __init__(self):
        self.upper = self.lower = None


class Language(object):
    """Class for Toolbox Language settings.
    """
    def __init__(self, fname, encoding=None):
        """Initialise from the settings file"""
        set = ToolboxSettings()
        set.open(fname)
        settings = set.parse(unwrap=False, encoding=encoding)

        self.init_case(settings.findtext('case'))
        self.sort_order = {}
        for sort_order in settings.findall('srtset/srt'):
            so = SortOrder(sort_order)
            self.sort_order[so.name] = so
        self.default_order = self.sort_order[settings.findtext('srtset/srtDefault')]

    def init_case(self, case_pairs):
        self.case = case = {}
        for c in case_pairs.splitlines():
            val = c.split()
            if len(val) != 2:
                raise ValueError, '"%s" is not a valid case association' % c
            u, l = val
            let_u = case[u] = Letter()
            let_l = case[l] = Letter()
            let_u.upper = let_l.upper = u
            let_u.lower = let_l.lower = l

    def lower(self, let):
        """return the lower case form of the letter.
        
        @rtype: string
        """
        return self.case[let].lower

    def upper(self, let):
        """return the upper case form of the letter.
        
        @rtype: string
        """
        return self.case[let].upper


class Graph(object):
    """"""
    __slots__ = ('order', 'type')
    
    def __init__(self):
        self.order = self.type = None


class SortOrder(object):
    """Class for Shoebox sort orders
    
    """
    
    def __init__(self, srt_order):
        self.name = srt_order.text
        self.desc = srt_order.findtext('desc')
        # if they don't exist make them a empty list so we don't need to test again
        try:    
            primary = srt_order.findtext('primary').splitlines()
        except AttributeError:
            primary = []
        try:
            sec_pre = srt_order.findtext('SecPreceding').split()
        except AttributeError:
            sec_pre = []
        try:
            sec_fol = srt_order.findtext('SecFollowing').split()
        except AttributeError:
            sec_fol = []
        try:
            ignore = srt_order.findtext('ignore').split()
        except AttributeError:
            ignore = []
        self.sec_after = srt_order.find('SecAfterBase') is not None

        primaries = [p.split() for p in primary]

        self.graphs = graphs = {}
        unmarked = len(sec_pre) + 1
        primaries[0:0] = [' '] #, '\t', '\n']
        i = 1
        for p in primaries:
            j = 1
            for m in p:
                if m in graphs:
                    raise ValueError, 'primary "%s" already in sort order' % m
                graphs[m] = g = Graph()
                g.type = 'p'
                g.order = (i, j, unmarked)
                j += 1
            i += 1
        prims = graphs.keys()
        prims.remove(' ')
        self.letter_pat = self.make_pattern(prims)

        i = 1
        for s in sec_pre:
            if s in graphs:
                raise ValueError, 'secondary preceding "%s" already in sort order' % s
            graphs[s] = g = Graph()
            g.type = 's'
            g.order = i
            i += 1

        # increment for unmarked case
        i += 1
        for s in sec_fol:
            if s in graphs:
                raise ValueError, 'secondary following "%s" already in sort order' % s
            graphs[s] = g = Graph()
            g.type = 's'
            g.order = i
            i += 1

        self.graph_pat = self.make_pattern(graphs.keys())
##~         graph_list = graphs.keys()
##~         
##~         # sort the longest first
##~         tmpl = [(len(x), x) for x in graph_list]
##~         tmpl.sort()
##~         tmpl.reverse()
##~         graph_list = [x[1] for x in tmpl]
##~         self.graph_pat = re.compile('|'.join([re.escape(g) for g in graph_list]))

    def make_pattern(self, slist):
        """Return a regular expression pattern to match the strings in slist"""
        # sort the longest first
        tmpl = [(len(x), x) for x in slist]
        tmpl.sort()
        tmpl.reverse()
        sorted_list = [x[1] for x in tmpl]
        escape = re.escape
        pat = re.compile('|'.join([re.escape(g) for g in sorted_list]))
        return pat

    def first_primary(self, s):
        """return the first primary in the string s"""
        match = self.letter_pat.search(s)
        if match is not None:
            return match.group()
        else:
            raise ValueError, 'no primary found in "%s"' % s

    def transform(self, s):
        graphs = self.graphs
        prim = []
        sec = []
        tert = []
        sec_order = None
        for g in self.graph_pat.findall(s):
            graph = graphs[g]
            order = graph.order
            type = graph.type
            if type == 'p':
                prim.append(order[0])
                tert.append(order[1])
                if sec_order is None:
                    sec.append(order[2])
                else:
                    sec.append(sec_order)
                    sec_order = None
            elif type == 's':
                # this ignores the situation of multiple consecutative secondaries
                if self.sec_after:
                    sec[-1] = order
                else:
                    # secondary is before the primary so save it for later
                    sec_order = order
        return (tuple(prim), tuple(sec), tuple(tert))

########NEW FILE########
__FILENAME__ = lexicon
# Natural Language Toolkit: Toolbox Lexicon
#
# Copyright (C) 2001-2006 NLTK Project
# Author: Stuart Robinson <stuart@zapata.org>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

"""
This modules provides functionality for parsing and manipulating the
contents of a Toolbox lexicon without reference to its metadata.
"""

import os, re, sys
import nltk.data
from nltk.corpus.reader.toolbox import StandardFormat
from utilities import Field, SequentialDictionary


class Lexicon(StandardFormat):

  """
  This class represents a Toolbox lexicon, which consists of an
  optional header and one or more Entry objects, saved in a dictionary
  whose keys are passed as a parameter to the parse() method.
  """

  def __init__(self, file):
    """
    This method construct a Lexicon object with a header and a dictionary of
    entries.
    """
    self._key_fields = ['lx']
    self._header  = ''
    self._entries = {}
    self._file = file
    
  def __str__(self):
    """
    This method defines the string representation of a Lexicon object
    """
    s = "%s\n" % self.get_header()
    for e in self.get_entries():
      s = "%s%s\n" % (s, e)
    return s
    
  def set_header(self, header):
    """
    This method sets the raw text of the header.
    @param header: header (as raw text)
    @type  header: string
    """
    self._header = header

  def get_header(self):
    """
    This method obtains the raw text of the header.
    @return: raw header
    @rtype: string
    """
    return self._header

  def get_entries(self):
    """
    This method obtains all of the entries found in a
    parsed Toolbox lexicon.
    
    @return: all of the entries in the Lexicon
    @rtype: list of Entry objects
    """
    keys = self._entries.keys()
    keys.sort()
    for k in keys :
      v = self._entries[k]
      for e in v :
        yield e

  def add_entry(self, entry, unique=False):
      """
      This method adds an Entry object to a Lexicon object. It adds the
      entry to the Lexicon keyed by the values of the fields specified
      by the I{key_fields} argument.

      @param entry: a parsed entry from a Toolbox lexicon
      @type entry: Entry object
      @param unique: raise exception if entry key already exists
      @type unique: boolean
      """
      key = ""
      for field_marker in self._key_fields:
          f = entry.get_field(field_marker)
          if f:
              values = f.get_values("/")
              key = key + "-" + values
          else:
              # Should this throw an error if a field with no values
              # is used in the list of key fields?
              pass
      if self._entries.has_key(key) :
          if unique :
              msg = "Non-unique entry! \nEntry: \n%s\nKey Fields: %s\nKey: '%s'\n" % (entry, self._key_fields, key)    
              raise ValueError, msg
      else :
          self._entries[key] = []
      # Now append entry to list of entries for key
      self._entries[key].append(entry)


  def parse(self,
            head_field_marker     = 'lx',
            subentry_field_marker = None,
            key_fields            = None,
            unique_entry          = True,
            unique_subentry       = False):
      """
      This method parses a Toolbox file in a Lexicon object. It will also parse
      subentries provided that the field marker identifying subentries is passed to it.
    
      @param head_field_marker:     field marker that identifies the start of an entry
      @type  head_field_marker:     string
      @param key_fields:            the field(s) to which entries are keyed
      @type  key_fields:            list of strings
      @param subentry_field_marker: field marker that identifies subentries
      @type  subentry_field_marker: string
      @param unique_entry:          raise warning if entries are non-unique according
                                      to I{key_fields} parameter
      @type  unique_entry:          boolean
      @param unique_subentry:       raise warning if entries are non-unique according to
                                      I{key_fields} parameter
      @type  unique_subentry:       boolean    
      @return:                      a parsed Lexicon object
      @rtype:                       dictionary object
      """

      if key_fields :
        self._key_fields = key_fields
        
      # Set up variables
      inside_entry = False
      inside_subentry = False
      e = None
      se = None
      # Use low-level functionality to get raw fields and walk through them
      self.open(self._file)
      for f in self.raw_fields() :
          fmarker, fvalue = f
          # What kind of field marker is it?
          if fmarker.startswith("_") :
              # TODO: Add field to header
              pass
          elif fmarker == head_field_marker :
              inside_entry = True
              inside_subentry = False
              if e :
                  self.add_entry(e, unique_entry)
              e = Entry()
          elif subentry_field_marker and fmarker == subentry_field_marker :
              inside_subentry = True
              if se :
                  e.add_subentry(se)
              se = Entry()               
          # Add field to entry or subentry
          if inside_subentry :
              se.add_field(fmarker, fvalue)
          elif inside_entry :
              e.add_field(fmarker, fvalue)
          else :
              pass
      # Deal with last entry
      if e :
          self.add_entry(e, unique_entry)
      self.close()

class Entry:
  """
  This class represents an entry (record) from a Toolbox lexicon. Each entry
  consists of a collection of fields, stored as a special type of dictionary
  which keeps track of the sequence in which its keys were entered.
  """

  def __init__(self):
    """
    This method constructs a new Entry object.
    """
    self._fields     = SequentialDictionary()
    self._rawText    = ""
    self._number     = None
    self._subentries = None

  def __str__(self):
    """
    This method defines the string representation of an entry.

    @rtype:  string
    @return: an entry as a string in Standard Format
    """
    s = ""
    fields = self.get_fields()
    for fm, fvs in self._fields.items():
      for fv in fvs:
        s = s + "\n\\%s %s" % (fm, fv)          
    return s
    
  def set_raw_text(self, rawText):
    """
    This method provides access to the raw text from which the
    Entry object was parsed.
    
    @param rawText: raw Toolbox text from which entry was parsed
    @type  rawText: string
    """
    self._rawText = rawText

  def get_raw_text(self):
    """
    This method sets the raw text from which the Entry object was parsed.

    @rtype: string
    """
    return self._rawText
  
  def get_subentries(self):
    """
    This method obtains all of the subentries for an entry.

    @rtype: list of Entry objects
    @returns: all of the subentries of an entry
    """
    return self._subentries

  def add_subentry(self, subentry):
    """
    This method adds to an entry a subentry, which is simply another
    Entry object.

    @param subentry: subentry
    @type  subentry: Entry object    : 
    """
    if not self._subentries:
      self._subentries = []
    self._subentries.append(subentry)

  def set_number(self, number):
    """
    This method sets the position of the entry in
    the dictionary as a cardinal number.
    
    @param number: number of entry
    @type  number: integer
    """
    self._number = number

  def get_number(self):
    """
    This method obtains the position of the entry in the dictionary
    as a cardinal number.
    
    @rtype: integer
    """
    return self._number
  
  def get_fields(self):
    """
    This method obtains all of the fields found in the Entry object.
    
    @rtype: list of Field objects
    """
    return self._fields.values()

  def get_field_markers(self):
    """
    This method obtains of the field markers found in the Entry object.

    @return: the field markers of an entry
    @rtype: list
    """
    return self._fields.keys()

  def get_values_by_marker(self, field_marker, sep=None) :
    return self.get_field_values_by_field_marker(field_marker, sep)

  def get_field_values_by_field_marker(self, field_marker, sep=None):
    """
    This method returns all of the field values for a given field marker.
    If the L(sep) is set, it will return a string; otherwise, it will
    return a list of Field objects.
    
    @param field_marker: marker of desired field
    @type  field_marker: string
    @param sep: separator for field values
    @type  sep: string    
    @rtype: string (if sep); otherwise, list of Field objects
    """
    try:
      values = self._fields[field_marker]
      if sep == None:
        return values
      else:
        return sep.join(values)
    except KeyError:
      return None

  def get_field_as_string(self,
                          field_marker,
                          join_string=""):
    """
    This method returns a particular field given a field marker.
    Returns a blank string if field is not found.
    
    @param field_marker: marker of desired field
    @type  field_marker: string
    @param join_string: string used to join field values (default to blank string)
    @type  join_string: string
    @rtype: string
    """
    try:
      return join_string.join(self._fields[field_marker])
    except KeyError:
      return ""

  def get_field(self, fieldMarker):
    """
    This method returns a particular field given a field marker.
    
    @param fieldMarker: marker of desired field
    @type  fieldMarker: string
    @rtype: Field object
    """
    try:
      return Field(fieldMarker, self._fields[fieldMarker])
    except KeyError:
      return None

  def set_field(self, fieldMarker, field):
    """
    This method sets a field, given a marker and its associated data.
    
    @param fieldMarker: field marker to set
    @type  fieldMarker: string
    @param field      : field object associated with field marker
    @type  field      : Field
    """
    fvs = []
    fvs.append(fieldData)
    self._fields[fieldMarker] = fvs

  def set_field_values(self, fieldMarker, fieldValues):
    """
    This method sets all of the values associated with a field.
    
    @param fieldMarker: field marker to set
    @type  fieldMarker: string
    @param fieldValues: list of field values
    @type  fieldValues: list
    """
    self._fields[fieldMarker] = fieldValues
  
  def add_field(self, marker, value):
    """
    This method adds a field to an entry if it does not already exist
    and adds a new value to the field of an entry if it does.
    
    @param marker: field marker
    @type  marker: string
    @param value : field value
    @type  value : string    
    """
    if self._fields.has_key(marker):
      fvs = self._fields[marker]
      fvs.append(value)
    else:
      fvs = []
      fvs.append(value)
    self._fields[marker] = fvs

  def remove_field(self, fieldMarker):
    """
    This method removes from an entry every field for a given
    field marker. It will not raise an error if the specified field
    does not exist.
    
    @param fieldMarker: field marker to be deleted
    @type  fieldMarker: string
    """
    if self._fields.has_key(fieldMarker):
      del self._fields[fieldMarker]

def demo() :
    path = nltk.data.find("corpora/toolbox/rotokas.dic")
    l = Lexicon(path)
    l.parse(key_fields=['lx','ps','sn'], unique_entry=False)
    h = l.get_header()
    for e in l.get_entries() :
        print "<%s><%s><%s>" % (e.get_field_as_string("lx", ""),
                                e.get_field_as_string("ps", ""),
                                e.get_field_as_string("sn", ""))
  
if __name__ == '__main__':
    demo()

########NEW FILE########
__FILENAME__ = normalise
# -*- coding: utf-8 -*-

# Natural Language Toolkit: Toolbox data file parser
#
# Copyright (C) 2001-2006 NLTK Project
# Author: Greg Aumann <greg_aumann@sil.org>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

"""
functions to normalise ElementTree structures.
"""

import nltk.etree.ElementTree as ET


def demo():
    from nltk_contrib.toolbox import ToolboxData, to_sfm_string
    from nltk_contrib.toolbox.data import iu_mien_hier as hierarchy
    import sys
    import os
    from nltk.data import ZipFilePathPointer

    file_path = data.find('corpora/toolbox/iu_mien_samp.db')
    settings.open(file_path)
#    zip_path = data.find('corpora/toolbox.zip')
#    db = ToolboxData(ZipFilePathPointer(zip_path, entry='toolbox/iu_mien_samp.db'))
    lexicon = db.grammar_parse('toolbox', hierarchy.grammar, unwrap=False, encoding='utf8')
    db.close()
    remove_blanks(lexicon)
    add_default_fields(lexicon, hierarchy.default_fields)
    sort_fields(lexicon, hierarchy.field_order)
    add_blank_lines(lexicon, hierarchy.blanks_before, hierarchy.blanks_between)
    print to_sfm_string(lexicon, encoding='utf8')
    

if __name__ == '__main__':
    demo()

########NEW FILE########
__FILENAME__ = settings
# -*- coding: utf-8 -*-

# Natural Language Toolkit: Toolbox Settings Parser
#
# Copyright (C) 2001-2006 NLTK Project
# Author: Stuart Robinson <stuart@zapata.org>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

"""
This module provides functionality for reading settings files for Toolbox. 
Settings files provide information (metadata) concerning lexicons and texts, 
such as which fields are found within them and what kind of values those 
fields can have.
"""

class MarkerSet :
    """This class is a container for FieldMetadata objects. A marker set
    contains a list of the fields in a database together with information
    about those files.

    The raw SFB looks like this::

        \\+mkrset 
        \\lngDefault Default
        \\mkrRecord lx

        \\+mkr dt
        \\nam Date Last Edited
        \\lng Default
        \\mkrOverThis lx
        \\-mkr

        \\+mkr lx
        \\nam Rotokas Word
        \\lng Rotokas
        \\-mkr
        \\-mkrset
        """             
    
    def __init__(self) :
        self._dict = {}

    def get_markers(self) :
        """Obtain a list of all of the field markers for the marker set.
        @returns: list of field markers
        @rtype: list of strings"""
        return self._dict.keys()

    def add_field_metadata(self, fmeta) :
        """Add FieldMetadata object to dictionary of marker sets, keyed by field marker.
        @param fmeta: field metadata to be added to collection for marker set
        @type  fmeta: FieldMetadata"""
        self._dict[fmeta.get_marker()] = fmeta
        
    def get_metadata_by_marker(self, mkr) :
        """Obtain a FieldMetadata object for the field marker provided.
        @param mkr: field to obtain metadata for
        @type  mkr: string
        @returns: metadata for field type associated with marker
        @rtype: FieldMetadata"""
        return self._dict[mkr]

    def get_field_marker_hierarchy(self) :
        # Find root field marker
        root = None
        for fm in self.get_markers() :
            fmmd = self.get_metadata_by_marker(fm)            
            if not fmmd.get_parent_marker() :
                root = fm

        # Build tree for field markers
        builder = TreeBuilder()
        builder.start(root, {})
        self.build_tree(root, builder)
        builder.end(root)
        return builder.close()
        
    def build_tree(self, mkr, builder) :
        markers = self.get_markers()
        markers.sort()
        for tmpmkr in markers :
            fmmd = self.get_metadata_by_marker(tmpmkr)
            # Field is child of current field
            if fmmd.get_parent_marker() == mkr :
                # Handle rangeset
                rangeset = fmmd.get_rangeset()
                if rangeset :
                    builder.start("rangeset", {})
                    for rsi in rangeset :
                        builder.start("value", {})
                        builder.data(rsi)
                        builder.end("value")
                    builder.end("rangeset")

                # Handle rangeset
                name = fmmd.get_name()
                if not name :
                    name = ""
                desc = fmmd.get_description()
                if not desc :
                    desc = ""
                d = {"name" : name,
                     "desc" : desc}
                #print fmmd.get_language()
                #print fmmd.is_multiword()
                #print fmmd.requires_value()
                builder.start(tmpmkr, d)
                self.build_tree(tmpmkr, builder)
                builder.end(tmpmkr)
        return builder
        
        
class FieldMetadata :
    """This class is a container for information about a field, including its marker, name,
    description, language, range set (valid values), and parent marker.

    The raw field metadata looks like this::

      \\+mkr dx
      \\nam Dialect
      \\desc dialects in which lexeme is found
      \\lng Default
      \\rngset Aita Atsilima Central Pipipaia
      \\mkrOverThis lx
      \\-mkr
    """
    
    def __init__(self,
                 marker     = None,
                 name       = None,
                 desc       = None,
                 lang       = None,
                 rangeset   = None,
                 multiword  = None,
                 required   = None,
                 parent_mkr = None) :
        self._marker     = marker
        self._name       = name
        self._desc       = desc
        self._lang       = lang
        self._rangeset   = rangeset
        self._parent_mkr = parent_mkr
        self._multiword  = multiword
        self._required   = required
        
    def get_marker(self) :
        """Obtain the marker for this field (e.g., 'dx').
        @returns: marker for field
        @rtype: string
        """
        return self._marker

    def get_name(self) :
        """Obtain the name for this field (e.g., 'Dialect').
        @returns: name of field
        @rtype: string
        """
        return self._name

    def get_description(self) :
        """Obtain the marker for this field (e.g., 'dialects in which lexeme is found').
        @returns: description of field
        @rtype: string
        """
        return self._desc

    def get_language(self) :
        """Obtain language in which field is encoded (e.g., 'Default').
        @returns: name of language used for field
        @rtype: string
        """
        return self._lang

    def get_rangeset(self) :
        """Obtain range set for field (e.g., ['Aita', 'Atsilima', 'Central', 'Pipipaia']).
        @returns: list of possible values for field
        @rtype: list of strings
        """
        return self._rangeset

    def set_rangeset(self, rangeset) :
        """Set list of valid values for field.
        @param rangeset: list of valid values for the field
        @type  rangeset: list
        """
        self._rangeset = rangeset
    
    def get_parent_marker(self) :
        """Obtain the marker for the parent of this field (e.g., 'lx').
        @returns: marker for parent field
        @rtype: string
        """
        return self._parent_mkr

    def is_multiword(self) :
        """Determine whether the value of the field consists of multiple words.
        @returns: whether field values can be multiword
        @rtype: boolean
        """
        return self._multiword

    def requires_value(self) :
        """Determine whether the field requires a value.
        @returns: whether field requires a value
        @rtype: boolean
        """
        return self._required


class LexiconSettings(ToolboxSettings) :
    """This class is used to parse and manipulate settings file for
    lexicons."""

    def __init__(self, file):
        self._file      = file
        self._markerset = MarkerSet()
        self._tree      = None
        
    def parse(self, encoding=None) :
        """Parse a settings file with lexicon metadata."""
        s = Settings()
        s.open(self._file)
        self._tree = s.parse(encoding=encoding)
        s.close()
        
        # Handle metadata for field markers (aka, marker set)
        for mkr in self._tree.findall('mkrset/mkr') :
            rangeset = None
            if self.__parse_value(mkr, "rngset") :
                rangeset = self.__parse_value(mkr, "rngset").split()
            fm = FieldMetadata(marker     = mkr.text,
                               name       = self.__parse_value(mkr, "nam"),
                               desc       = self.__parse_value(mkr, "desc"),
                               lang       = self.__parse_value(mkr, "lng"),
                               rangeset   = rangeset,
                               multiword  = self.__parse_boolean(mkr, "MultipleWordItems"),
                               required   = self.__parse_boolean(mkr, "MustHaveData"),
                               parent_mkr = self.__parse_value(mkr, "mkrOverThis"))
            self._markerset.add_field_metadata(fm)

        # Handle range sets defined outside of marker set
        # WARNING: Range sets outside the marker set override those inside the
        #          marker set
        for rs in self._tree.findall("rngset") :
            mkr = rs.findtext("mkr")
            fm = self._markerset.get_metadata_by_marker(mkr)
            fm.set_rangeset([d.text for d in rs.findall("dat") ])
            self._markerset.add_field_metadata(fm)
            
    def get_record_marker(self) :
        return self._tree.find('mkrset/mkrRecord').text

    def get_marker_set(self) :
        return self._markerset

    def __parse_boolean(self, mkr, name) :
        if mkr.find(name) == None :
            return False
        else :
            return True

    def __parse_value(self, mkr, name) :
        try :
            return mkr.find(name).text
        except :
            return None

class InterlinearProcess :
    """This class represents a process for text interlinearization."""

    def __init__(self,
                 from_mkr        = None,
                 to_mkr          = None,
                 out_mkr         = None,
                 gloss_sep       = None,
                 fail_mark       = None,
                 parse_proc      = None,
                 show_fail_mark  = None,
                 show_root_guess = None) :
        self.__from_mkr        = from_mkr
        self.__to_mkr          = to_mkr
        self.__out_mkr         = out_mkr
        self.__gloss_sep       = gloss_sep
        self.__fail_mark       = fail_mark
        self.__parse_proc      = parse_proc
        self.__show_fail_mark  = show_fail_mark
        self.__show_root_guess = show_root_guess

    def get_output_marker(self) :
        return self.__out_mkr
    
    def get_from_marker(self) :
        """The marker searched for in the lookup process."""
        return self.__from_mkr

    def get_to_marker(self) :
        """The marker found in the lookup process."""
        return self.__to_mkr

    def get_gloss_separator(self) :
        """???"""
        return self.__gloss_sep

    def get_failure_marker(self) :
        """The string used in the case of lookup failure,""" 
        return self.__fail_mark

    def is_parse_process(self) :
        """Determine whether this process is a parse process (as opposed to a lookup process)."""
        return self.__parse_proc

    def show_failure_marker(self) :
        """???"""
        return self.__show_fail_mark

    def show_root_guess(self) :
        """???"""
        return self.__show_root_guess


class LookupProcess(InterlinearProcess) :
    pass


class ParseProcess(InterlinearProcess) :
    pass


class TextSettings(ToolboxSettings) :
    """This class is used to parse and manipulate settings file for
    lexicons."""

    def __init__(self, file):
        self._file      = file
        self._markerset = MarkerSet()
        self._tree      = None
        
    def parse(self, encoding=None) :
        """Parse a settings file with lexicon metadata."""
        s = Settings()
        s.open(self._file)
        self._tree = s.parse(encoding=encoding)
        s.close()

        # Handle interlinear process list
        for proc in self._tree.findall("intprclst/intprc") :
            parseProcess  = self.__parse_boolean(proc, "bParseProc")
            showRootGuess = self.__parse_boolean(proc, "bShowRootGuess")
            showFailMark  = self.__parse_boolean(proc, "bShowFailMark")
            fromMkr       = self.__parse_value(proc, "mkrFrom")
            outMkr        = self.__parse_value(proc, "mkrOut")
            toMkr         = self.__parse_value(proc, "mkrTo").strip()
            glossSep      = self.__parse_value(proc, "GlossSeparator")
            failMark      = self.__parse_value(proc, "FailMark")
            ip = ParseProcess(from_mkr        = fromMkr,
                              to_mkr          = toMkr,
                              gloss_sep       = glossSep,
                              fail_mark       = failMark,
                              parse_proc      = parseProcess,
                              show_fail_mark  = showFailMark,
                              show_root_guess = showRootGuess,
                              out_mkr         = outMkr)                
            if parseProcess :
                pass
            else :
                pass

            print "----- Interlinear Process -----"
            print "  FROM:            [%s]" % ip.get_from_marker()
            print "  TO:              [%s]" % ip.get_to_marker()
            print "  GLOSS SEP:       [%s]" % ip.get_gloss_separator()
            print "  FAIL MARK:       [%s]" % ip.get_failure_marker()
            print "  SHOW FAIL MARK:  [%s]" % ip.show_failure_marker()
            print "  SHOW ROOT GUESS: [%s]" % ip.show_root_guess()
            print "  PARSE PROCESS:   [%s]" % ip.is_parse_process()            

            trilook = proc.find("triLook")
            if trilook :
                print "  -- trilook --"
                print "    DB TYPE:       [%s]" % self.__parse_value(trilook, "dbtyp")            
                print "    MKR OUTPUT:    [%s]" % self.__parse_value(trilook, "mkrOut")

            tripref = proc.find("triPref")
            if tripref :
                print "  -- tripref --"
                print "    DB TYPE:       [%s]" % self.__parse_value(tripref, "dbtyp")            
                print "    MKR OUTPUT:    [%s]" % self.__parse_value(tripref, "mkrOut")
                try :
                    for d in tripref.findall("drflst/drf") :
                        print "    DB:            [%s]" % self.__parse_value(d, "File")
                except :
                    pass
                try :
                    for d in tripref.find("mrflst") :
                        print "    MKR:           [%s]" % d.text
                except :
                    pass

            triroot = proc.find("triRoot")
            if triroot :
                print "  -- triroot --"
                print "    DB TYPE:       [%s]" % self.__parse_value(triroot, "dbtyp")
                print "    MKR OUTPUT:    [%s]" % self.__parse_value(triroot, "mkrOut")
                try :
                    for d in triroot.findall("drflst/drf") :
                        print "    DB:            [%s]" % self.__parse_value(d, "File")
                except :
                    pass
                try :
                    for d in triroot.find("mrflst") :
                        print "    MKR:           [%s]" % d.text
                except :
                    pass

            print ""
            
        # Handle metadata for field markers (aka, marker set)
        for mkr in self._tree.findall('mkrset/mkr') :
            rangeset = None
            if self.__parse_value(mkr, "rngset") :
                rangeset = self.__parse_value(mkr, "rngset").split()
            fm = FieldMetadata(marker     = mkr.text,
                               name       = self.__parse_value(mkr, "nam"),
                               desc       = self.__parse_value(mkr, "desc"),
                               lang       = self.__parse_value(mkr, "lng"),
                               rangeset   = rangeset,
                               multiword  = self.__parse_boolean(mkr, "MultipleWordItems"),
                               required   = self.__parse_boolean(mkr, "MustHaveData"),
                               parent_mkr = self.__parse_value(mkr, "mkrOverThis"))
            self._markerset.add_field_metadata(fm)

        # Handle range sets defined outside of marker set
        # WARNING: Range sets outside the marker set override those inside the
        #          marker set
        for rs in self._tree.findall("rngset") :
            mkr = rs.findtext("mkr")
            fm = self._markerset.get_metadata_by_marker(mkr)
            fm.set_rangeset([d.text for d in rs.findall("dat") ])
            self._markerset.add_field_metadata(fm)
            
    def get_record_marker(self) :
        return self._tree.find('mkrset/mkrRecord').text

    def get_version(self) :
        return self._tree.find('ver').text

    def get_description(self) :
        return self._tree.find('desc').text    

    def get_marker_set(self) :
        return self._markerset

    def __parse_boolean(self, mkr, name) :
        if mkr.find(name) == None :
            return False
        else :
            return True

    def __parse_value(self, mkr, name) :
        try :
            return mkr.find(name).text
        except :
            return None

def demo():
    pass

if __name__ == '__main__':
    demo()

########NEW FILE########
__FILENAME__ = text
# Natural Language Toolkit: Shoebox Text
#
# Author: Stuart Robinson <stuart@zapata.org>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

"""
This module provides tools for parsing and manipulating the contents
of a Shoebox text without reference to its metadata.
"""

import re
from utilities import Field, SequentialDictionary
from nltk.corpus.reader.toolbox import StandardFormat


# --------------------------------------------------------
# CLASS: Word
# DESC:  Object that represents a word.
# --------------------------------------------------------

class Word:
    """
    This class defines a word object, which consists of fixed number
    of attributes: a wordform, a gloss, a part of speech, and a list
    of morphemes.
    """

    def __init__(self,
                 form         = None,
                 gloss        = None,
                 morphemes    = None,
                 partOfSpeech = None):
        """Constructor that initializes Word object.

        @param form: the surface form for a word
        @type form: string
        @param gloss: the gloss for a word
        @type gloss: string
        @param morphemes: list of Morpheme objects for a word
        @type morphemes: list
        @param partOfSpeech: the part of speech for a word
        @type partOfSpeech: string
        """
        self._form            = form
        self._gloss           = gloss
        self._morphemes       = morphemes
        self._partOfSpeech    = partOfSpeech
        self._rawGloss        = None
        self._rawMorphemes    = None
        self._rawPartOfSpeech = None
        return

    def get_form(self):
        """Gives the surface form of a word."""
        return self._form

    def set_form(self, form):
        """Changes the surface form of a word."""
        self._form = form

    def get_gloss(self):
        """Gives the gloss for a word as a string (without alignment spacing)."""
        return self._gloss

    def set_gloss(self, gloss):
        """Change the gloss for a word."""
        self._gloss = gloss

    def get_morphemes(self):
        """Gives a list of Morpheme objects for a word."""
        return self._morphemes

    def set_morphemes(self, morphemes):
        """Change a list of Morpheme objects for a word."""
        self._morphemes = morphemes

    def get_part_of_speech(self):
        """Gives the part of speech for a word as a string (without alignment spacing)."""
        return self._partOfSpeech

    def set_part_of_speech(self, partOfSpeech):
        """Change the part of speech for a word."""
        self._partOfSpeech = partOfSpeech

    def get_raw_gloss(self):
      return self._rawGloss

    def set_raw_gloss(self, rawGloss):
        self._rawGloss = rawGloss

    def get_raw_morphemes(self):
        return self._rawMorphemes

    def set_raw_morphemes(self, rawMorphemes):
        self._rawMorphemes = rawMorphemes

    def get_raw_part_of_speech(self):
        return self._rawPartOfSpeech

    def set_raw_part_of_speech(self, rawPartOfSpeech):
        self._rawPartOfSpeech = rawPartOfSpeech


# --------------------------------------------------------
# CLASS: Morpheme
# DESC:  Object that represents a morpheme.
# --------------------------------------------------------

class Morpheme:
    """
    This class defines a morpheme object, which consists of fixed number
    of attributes: a surface form, an underlying form, a gloss, and a
    part of speech.
    """
  
    def __init__(self,
                 form         = None,
                 gloss        = None,
                 partOfSpeech = None):
        """Constructor that creates Morpheme object."""        
        self._form = form
        self._gloss = gloss
        self._partOfSpeech = partOfSpeech
        return

    def get_form(self):
        """Returns form for morpheme."""
        return self._form

    def set_form(self, form):
        """Change form for morpheme."""
        self._form = form

    def get_gloss(self):
        """Returns gloss for morpheme."""        
        return self._gloss

    def set_gloss(self, gloss):
        """Change gloss for morpheme."""        
        self._gloss = gloss

    def get_part_of_speech(self):
        """Returns part of speech for morpheme."""        
        return self._partOfSpeech

    def set_part_of_speech(self, partOfSpeech):
        """Change part of speech for morpheme."""
        self._partOfSpeech = partOfSpeech


# --------------------------------------------------------
# CLASS: Line
# DESC:  Object that represents a line from an interlinear
#         text.
# --------------------------------------------------------

class Line:
    """This class defines a line of interlinear glossing, such as::

        \\ref 9
        \\t Vigei    avapaviei                           atarisia.
        \\m vigei    ava -pa       -vi        -ei        atari -sia
        \\g 1.PL.INC go  -PROG     -1.PL.INCL -PRES      fish  -PURP
        \\p PRO.PERS V.I -SUFF.V.3 -SUFF.VI.4 -SUFF.VI.5 V.I   -SUFF.V.4
        \\fp Yumi bai go kisim pis.
        \\fe We're going fishing.

    The tiers of a line are saved as a sequential dictionary with
    all of its associated fields. Identified by the field marker \\ref
    by default."""
    
    def __init__(self,
                 label=None):
        """Constructor that initializes Line object."""
        self._fields = SequentialDictionary()
        self._label = label
        return

    def add_field(self, field):
        """Add field to line."""
        fm = field.get_marker()
        fv = field.get_values()
        self._fields[fm] = fv

    def get_field_markers(self):
        """Obtain list of unique fields for the line."""
        return self._fields.keys()

    def get_field_as_string(self,
                            field_marker,
                            join_string=""):
        """
        This method returns a particular field given a field marker.
        Returns a blank string if field is not found.
        
        @param field_marker: marker of desired field
        @type  field_marker: string
        @param join_string: string used to join field values (default to blank string)
        @type  join_string: string
        @rtype: string
        """
        try:
            return join_string.join(self._fields[field_marker])
        except KeyError:
            return ""

    def get_field_values_by_field_marker(self, field_marker, sep=None):
        """Obtain all fields for a line, given a field marker."""
        try:
            values = self._fields[field_marker]
            if sep == None:
                return values
            else:
                return sep.join(values)
        except KeyError:
            return None

  #   def getField(self, field_marker):
  #     try:
  #       return self._fields[field_marker]
  #     except:
  #       return None
      
    def get_field_values(self):
        """Obtain list of field values for the line."""
        return self._fields.values()

    def get_label(self):
        """Obtain identifier for line."""
        return self._label

    def get_raw_text(self):
        """Obtain original line of text."""
        return self._rawtext

    def set_label(self, label):
        """Set identifier for line."""
        self._label = label

    def set_raw_text(self, rawtext):
        """Set original line of text."""
        self._rawtext = rawtext

    def get_morphemes(self):
        """Obtain a list of morpheme objects for the line."""
        morphemes = []
        indices = get_indices(self.getFieldValueByFieldMarker("m"))
        print "%s" % indices
        morphemeFormField = self.getFieldValueByFieldMarker("m")
        morphemeGlossField = self.getFieldValueByFieldMarker("g")
        morphemeFormSlices = get_slices_by_indices(morphemeFormField, indices)
        morphemeGlossSlices = get_slices_by_indices(morphemeGlossField, indices)
        for i in range(0, len(morphemeFormSlices)):
            m = Morpheme()
            m.set_form(morphemeFormSlices[i].strip(" ").strip("-"))
            m.set_gloss(morphemeGlossSlices[i].strip(" ").strip("-"))
            morphemes.append(m)
        return morphemes
      
    def get_words(self, flagParseMorphemes=True):
        """Obtain a list of word objects for the line."""
        words = []

        # Obtain raw field values
        lineWordFormField      = self.get_field_values_by_field_marker("t")
        lineMorphemeFormField  = self.get_field_values_by_field_marker("m")
        lineMorphemeGlossField = self.get_field_values_by_field_marker("g")
        linePOSField           = self.get_field_values_by_field_marker("p")

        wordIndices = get_indices(lineWordFormField)
      
        # Slice raw field values by indices
        lineWordFormSlices      = get_slices_by_indices(lineWordFormField,      wordIndices)
        lineMorphemeFormSlices  = get_slices_by_indices(lineMorphemeFormField,  wordIndices)
        lineMorphemeGlossSlices = get_slices_by_indices(lineMorphemeGlossField, wordIndices)
        linePOSSlices           = get_slices_by_indices(linePOSField,           wordIndices)
          
        # Go through each slice
        for i in range(0, len(lineWordFormSlices)):
            wordForm            = lineWordFormSlices[i]
            wordMorphemeForms   = lineMorphemeFormSlices[i]
            wordMorphemeGlosses = lineMorphemeGlossSlices[i]
            wordPOS             = linePOSSlices[i]

            # Initialize word object and set raw fields
            w = Word()
            w.set_form(wordForm.strip(" ").strip("-"))
            w.set_raw_morphemes(wordMorphemeForms.strip(" ").strip("-"))
            w.set_raw_gloss(wordMorphemeGlosses.strip(" ").strip("-"))
            w.set_part_of_speech(wordPOS.strip(" ").strip("-"))

            # Should the word be inflated with morpheme objects?
            # If so, build morpheme object for each morpheme in word
            if flagParseMorphemes:
                morphemes = []

                # Get indices from morpheme-breakdown line in order to make slices
                morphemeIndices     = get_indices(wordMorphemeForms)
                morphemeFormSlices  = get_slices_by_indices(wordMorphemeForms,   morphemeIndices)
                morphemeGlossSlices = get_slices_by_indices(wordMorphemeGlosses, morphemeIndices)
                morphemePOSSlices   = get_slices_by_indices(wordPOS,             morphemeIndices)

                # Go through each morpheme
                for i in range(0, len(morphemeFormSlices)):
                    morphemeForm  = morphemeFormSlices[i].strip(" ")
                    morphemeGloss = morphemeGlossSlices[i].strip(" ")
                    morphemePOS   = morphemePOSSlices[i].strip(" ")

                    # Construct morpheme object from slices
                    m = Morpheme()
                    m.set_form(morphemeForm)
                    m.set_gloss(morphemeGloss)
                    m.set_part_of_speech(morphemePOS)
                    
                    # Add cooked morpheme to temporary collection for word
                    morphemes.append(m)

                # Inflate word with cooked morphemes
                w.set_morphemes(morphemes)

            words.append(w)
        return words

    def get_field_value_by_field_marker_and_column(self, field_marker, columnIndex):
        """Get values for line, given a field and column index."""
        fv = self.getFieldValueByFieldMarker(field_marker)
        field_markers = self.getFieldMarkers()
        sliceFieldMarker = field_markers[columnIndex-1]    
        indices = getIndices(self.getFieldValueByFieldMarker(field_marker))
        slices = get_slices_by_indices(fv, indices)
        return slices[columnIndex-1]


# --------------------------------------------------------
# CLASS: Paragraph
# DESC:  Object that represents a paragraph (i.e., a unit
#         larger than a line) from an interlinear text.
# --------------------------------------------------------

class Paragraph:
    """
    This class defines a unit of analysis above the line and below
    the text. Every text will have at least one paragraph and some
    will have more. Identified by the field marker \id by default.
    """

    def __init__(self,
                 label=None):
        """Constructor that initializes Paragraph object."""
        self._lines = []
        self._label = label
        return

    def add_line(self, line):
        """Add line object to list of line objects for paragraph."""
        self._lines.append(line)

    def get_label(self):
        """Obtain identifier for paragraph."""
        return self._label

    def get_lines(self):
        """Get list of line objects for paragraph."""
        return self._lines
    
    def set_label(self, label):
        """Set identifier for paragraph."""
        self._label = label


# --------------------------------------------------------
# CLASS: InterlinearText
# DESC:  Object that represents an interlinear text and
#         provides functionality for its querying and
#         manipulation.
# --------------------------------------------------------

class Text(StandardFormat) :
    """
    This class defines an interlinearized text, which consists of a collection of Paragraph objects.
    """
  
    def __init__(self,
                 file              = None,
                 fm_line           = "ref",
                 fm_paragraph      = "id",
                 fm_morpheme       = "m",
                 fm_morpheme_gloss = "g",
                 fm_word           = "w"
                 ):
        """Constructor for Text object. All arguments are optional. By default,
        the fields used to parse the Shoebox file are the following:
        @param file: filepath
        @type file: str
        @param fm_line: field marker identifying line (default: 'ref')
        @type fm_line: str
        @param fm_paragraph: field marker identifying paragraph (default: 'id')
        @type fm_paragraph: str
        @param fm_morpheme: field marker identifying morpheme tier (default: 'm')
        @type fm_morpheme: str
        @param fm_morpheme_gloss: field marker identifying morpheme gloss tier (default: 'g')
        @type fm_morpheme_gloss: str
        @param fm_word: field marker identifying word tier (???)
        @type fm_word: str 
        """
        self._file              = file
        self._fm_line           = fm_line
        self._fm_paragraph      = fm_paragraph
        self._fm_morpheme       = "m"
        self._fm_morpheme_gloss = "g"
        self._fm_word           = "w"
        #self._rawtext = rawtext
        self._paragraphs        = []
        return

    def get_lines(self):
        """Obtain a list of line objects (ignoring paragraph structure)."""
        lines = []
        for p in self.get_paragraphs():
            for l in p.get_lines():
                lines.append(l)
        return lines
        
    def get_paragraphs(self):
        """Obtain a list of paragraph objects."""
        return self._paragraphs

#     def set_paragraphs(self, paragraphs):
#       self._paragraphs = paragraphs

    def add_paragraph(self, paragraph):
        """Add paragraph object to list of paragraph objects.
        @param paragraph: paragraph to be added to text
        @type paragraph: Paragraph
        """
        self._paragraphs.append(paragraph)
      
#     def getRawText(self):
#       return self._rawtext

#     def setRawText(self, rawtext):
#       self._rawtext = rawtext

    def getLineFM(self):
        """Get field marker that identifies a new line."""
        return self._fm_line

    def setLineFM(self, lineHeadFieldMarker):
        """Change default field marker that identifies new line."""
        self._fm_line = lineHeadFieldMarker

    def getParagraphFM(self):
        """Get field marker that identifies a new paragraph."""
        return self._fm_paragraph
  
    def setParagraphFM(self, paragraphHeadFieldMarker):
        """Change default field marker that identifies new paragraph."""
        self._fm_paragraph = paragraphHeadFieldMarker

    def getWordFM(self):
        """Get field marker that identifies word tier."""
        return self._wordFieldMarker

    def setWordFM(self, wordFieldMarker):
        """Change default field marker that identifies word tier."""
        self._wordFieldMarker = wordFieldMarker

    def getMorphemeFM(self):
        """Get field marker that identifies morpheme tier."""
        return self._morphemeFieldMarker

    def setMorphemeFM(self, morphemeFieldMarker):
        """Change default field marker that identifies morpheme tier."""
        self._morphemeFieldMarker = morphemeFieldMarker

    def getMorphemeGlossFM(self):
        """Get field marker that identifies morpheme gloss tier."""
        return self._morphemeGlossFieldMarker

    def setMorphemeGlossFM(self, morphemeGlossFieldMarker):
        """Change default field marker that identifies morpheme gloss tier."""
        self._morphemeGlossFieldMarker = morphemeGlossFieldMarker    

    def get_file(self):
        """Get file path as string."""
        return self._file

    def set_file(self, file):
        """Change file path set upon initialization."""
        self._file = file

    def parse(self) :
        """Parse specified Shoebox file into Text object."""
        # Use low-level functionality to get raw fields and walk through them
        self.open(self._file)
        p, l = None, None
        for f in self.raw_fields() :
            fmarker, fvalue = f
            if fmarker == self.getParagraphFM() :
                if p :
                    self.add_paragraph(p)
                p = Paragraph(fvalue)
            elif fmarker == self.getLineFM() :
                if l :
                    p.add_line(l)
                l = Line(fvalue)
            else :
                if l :
                    l.add_field(Field(fmarker, fvalue))
        p.add_line(l)
        self.add_paragraph(p)


# -------------------------------------------------------------
# FUNCTION: get_indices
# ------------------------------------------------------------
def get_indices(str):
    """This method finds the indices for the leftmost boundaries
    of the units in a line of aligned text.

    Given the field \um, this function will find the
    indices identifing leftmost word boundaries, as
    follows::

            0    5  8   12              <- indices
            |    |  |   |               
            |||||||||||||||||||||||||||
        \sf dit  is een goede           <- surface form
        \um dit  is een goed      -e    <- underlying morphemes
        \mg this is a   good      -ADJ  <- morpheme gloss
        \gc DEM  V  ART ADJECTIVE -SUFF <- grammatical categories
        \ft This is a good explanation. <- free translation

    The function walks through the line char by char::
  
        c   flag.before  flag.after  index?
        --  -----------  ----------  ------
        0   1            0           yes
        1   0            1           no
        2   1            0           no
        3   0            1           no
        4   1            0           no   
        5   1            0           yes

    @param str: aligned text
    @type str: string
    """
    indices = []
    flag = 1
    for i in range(0, len(str)):
        c = str[i]
        if flag and c != ' ':
            indices.append(i)
            flag = 0
        elif not flag and c == ' ':
            flag = 1
    return indices


# -------------------------------------------------------------
# FUNCTION: get_slices_by_indices
# -------------------------------------------------------------
def get_slices_by_indices(str, indices):
    """Given a string and a list of indices, this function returns
    a list of the substrings defined by those indices. For example,
    given the arguments::
        str='antidisestablishmentarianism', indices=[4, 7, 16, 20, 25]
    this function returns the list::
        ['anti', 'dis', 'establish', 'ment', arian', 'ism']

    @param str: text
    @type str: string
    @param indices: indices 
    @type indices: list of integers
    """
    slices = []
    for i in range(0, len(indices)):
        slice = None
        start = indices[i]
        if i == len(indices)-1:
            slice = str[start: ]
        else:
            finish = indices[i+1]
            slice = str[start: finish]
        slices.append(slice)
    return slices

########NEW FILE########
__FILENAME__ = utilities
# Natural Language Toolkit: Shoebox Utilities
#
# Copyright (C) 2001-2006 NLTK Project
# Author: Stuart Robinson <stuart@zapata.org>
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

"""
This module provides basic functionality for handling shoebox format files.
These feed into the more sophisticated Shoebox tools available in the
modules I{lexicon}, I{text}, and I{metadata}.
"""

import re
from UserDict import UserDict


def parse_field(line):
  """
  This function returns the field marker and field value of a Shoebox field.

  @return: parses field as string and returns tuple with field marker and field value
  @rtype: tuple
  """
  mo = re.match(r"\\(.*?) (.*)", line)
  if mo:
    fm = mo.group(1)
    fv = mo.group(2)
    return (fm, fv)
  else:
    return None


class Field:
  """
  Class used to represent a standard fromat field. A field
  consists of a field marker and its value, stored as a tuple.
  """

  def __init__(self, fieldMarker, fieldValue):
    """
    This method constructs a Field object as a tuple of a field
    marker and a field value.
    @param fieldMarker: a field's marker
    @type  fieldMarker: string
    @param fieldValue : a field's value (the actual data)
    @type  fieldValue : string
    """
    self._field = (fieldMarker, fieldValue)

  def __str__(self):
    """
    This method returns the string representation of a Field object.
    
    @return: a Field object formatted as a string
    @rtype: string
    """
    return "\\%s %s" % (self.getMarker(), self.getValue())
  
  def get_marker(self):
    """
    This method returns the marker for a field.
    
    @return: a field's marker
    @rtype: string
    """
    return self._field[0]

  def has_unique_value(self):
    """
    This method checks whether a field has a single value, in
    which case it returns true, or multiple values, in which
    case it returns false.
    
    @return: whether the value for a given field is unique
    @rtype: boolean
    """
    if not self.get_values() or len(self.get_values()) > 1:
      return True
    else:
      return False
    
  def has_value(self):
    """
    This method checks whether a field has a value or not.

    @return: whether a given field has a value
    @rtype: boolean
    """
    if self.get_values():
      return True
    else:
      return False
    
  def get_values(self, sep=None):
    """
    This method returns the values for a field, either as a raw list of
    values or, if a separator string is provided, as a formatted string.
    
    @return: the values for a field; if sep provided, formatted as string
    @rtype: a list of values or a string of these values joined by I{sep}
    """    
    values = self._field[1]
    if sep == None:
      return values 
    else:
      return sep.join(values)


# class FieldParser:
#   """
#   Parses raw Shoebox field into a field object.
#   """
#   def __init__(self, rawText):
#     self._rawText = rawText

#   def getRawText(self):
#     """
#     This method returns the raw text to be parsed as a field by the parser.
    
#     @return: string
#     @rtype: a string with a standard format field as raw text
#     """    
#     return self._rawText

#   def setRawText(self, rawtext):
#     """
#     This method constructs a Field object as a tuple of a field
#     marker and a field value.
#     @param rawtext: the raw text to be parsed into a field object
#     @type  rawtext: string
#     """
#     self._rawtext = rawtext
#     return self._rawtext

#   def parse(self):
#     regex = r"\\([A-Za-z][A-Za-z0-9\_\-]*) (.*)"
#     mo = re.search(regex,
#                    self.getRawText())
#     fm = mo.group(1)
#     fv = mo.group(2)
#     return Field(fm, fv)


class SequentialDictionary(UserDict):
  """
  Dictionary that retains the order in which keys were added to it.
  """
  def __init__(self, dict=None):
    self._keys = []
    UserDict.__init__(self, dict)

  def __delitem__(self, key):
    UserDict.__delitem__(self, key)
    self._keys.remove(key)

  def __setitem__(self, key, item):
    UserDict.__setitem__(self, key, item)
    if key not in self._keys:
      self._keys.append(key)

  def clear(self):
    UserDict.clear(self)
    self._keys = []

  def copy(self):
    dict = UserDict.copy(self)
    dict._keys = self.keys[:]
    return dict

  def items(self):
    return zip(self._keys, self.values())

  def keys(self):
    return self._keys

  def popitem(self):
    try:
      key = self._keys[-1]
    except IndexError:
      raise KeyError('dictionary is empty')
    val = self[key]
    del self[key]

    return (key, val)

  def setdefault(self, key, failobj=None):
    if key not in self._keys:
      self._keys.append(key)
    return UserDict.setdefault(self, key, failobj)
  
  def update(self, dict):
    UserDict.update(self, dict)
    for key in dict.keys():
      if key not in self._keys:
        self._keys.append(key)

  def values(self):
    return map(self.get, self._keys)

########NEW FILE########
__FILENAME__ = wals
# Natural Language Toolkit: WALS interface
#
# Copyright (C) 2001-2011 NLTK Project
# Author: Michael Wayne Goodman <goodmami@uw.edu>
#
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT
#
# For more information about WALS (the World Atlas of Language Structures),
# see http://wals.info. WALS is curated by Matthew S. Dryer and Martin
# Haspelmath. WALS Online is hosted by the Max Planck Digital Library.
#
# The data used with this class can be obtained from http://wals.info/export

from collections import defaultdict
import csv
import os.path
import re

"""
Class for loading and interacting with the WALS database of linguistic
typological information.
"""

# Used for removing location information from language names.
_lg_variety_re = re.compile(r'\s*\(.*$')

# These are the columns for the feature, value, and language tables
feature_fields = ['id', 'name']
value_fields = ['feature_id', 'value_id', 'description', 'long_description']
language_fields = ['wals_code', 'name', 'latitude', 'longitude',
                   'genus', 'family', 'subfamily', 'iso_codes']

class LHNode(object):
    """
    Language hierarchy node, for storing genus/family/subfamily info.
    """

    def __init__(self, name):
        """
        @param name: The name of the genus/family/subfamily at this node.
        """
        self.name = name
        self.languages = {}
        self.subclasses = {}
        self.superclass = None

class WALS(object):
    """
    A container and interface for a WALS database.
    """

    def __init__(self, data_dir, dialect='excel-tab', encoding='utf-8'):
        """
        @param data_dir: The directory where the WALS data files exist.
        @param dialect: The CSV dialect used in the database (values are
                        'excel' and 'excel-tab'.
        """

        if dialect == 'tab': dialect = 'excel-tab'
        if dialect == 'csv': dialect = 'excel'
        self.dialect = dialect
        self.load(os.path.expanduser(data_dir), encoding)
        self._build_indices()
        self._build_language_hierarchy()

    def load(self, data_dir, encoding):
        """
        Read and store the data from the WALS database.
        
        @param data_dir: The directory where the WALS data files exist.
        @param encoding: The character encoding of the files.
        """

        # The features, values, and languages files are simply rows with an
        # equal number of columns per row. We can use csv.reader to parse.
        # For each file, the first line is header information. Discard it.
        file_ext = 'csv' if self.dialect == 'excel' else 'tab'
        def open_csv(filename, remove_header=True):
            filename = os.path.join(data_dir, filename + '.' + file_ext)
            wals_file = csv.reader(open(filename, 'r'), dialect=self.dialect)
            if remove_header: wals_file.next()
            for row in wals_file:
                yield [unicode(cell, encoding) for cell in row]

        def map_fields(vectors, fields):
            for vector in vectors:
                yield dict(zip(fields, vector))

        # Features
        self.features = dict((f['id'], f) for f in
                             map_fields(open_csv('features'),
                                        feature_fields))
        # Values
        self.values = defaultdict(dict)
        for v in map_fields(open_csv('values'), value_fields):
            self.values[v['feature_id']][v['value_id']] = v
        # Languages
        self.languages = dict((l['wals_code'], l) for l in
                              map_fields(open_csv('languages'),
                                         language_fields))
        # convert longitude and latitude to float from string
        for l in self.languages.values():
            l['latitude'] = float(l['latitude'])
            l['longitude'] = float(l['longitude'])
        # The datapoints file is more complicated. There is a column for
        # every feature, and a row for every language. Each cell is either
        # empty or contains a value dependent on the feature.
        rows = open_csv('datapoints', remove_header=False)
        header = rows.next()
        self.data = defaultdict(dict)
        self.feat_lg_map = defaultdict(list)
        for row in rows:
            lg = row[0]
            for i, val in enumerate(row[1:]):
                if val != '':
                    self.feat_lg_map[header[i+1]] += [self.languages[lg]]
                    self.data[lg][header[i+1]] = val

    def _build_indices(self):
        """
        For efficient lookup, build indices for language names and iso codes.
        """

        self.iso_index = defaultdict(list)
        self.language_name_index = defaultdict(list)
        for lg in self.languages.values():
            for iso in lg['iso_codes'].split():
                self.iso_index[iso] += [lg]
            name = lg['name'].lower()
            self.language_name_index[_lg_variety_re.sub('', name)] += [lg]

    def _build_language_hierarchy(self):
        """
        Use the 'family', 'subfamily', and 'genus' information to build
        a hierarchical representation of language relations.
        """

        # all languages in WALS have family and genus, but some also have
        # subfamily. This intervenes between family and genus:
        #    family -> genus
        #    family -> subfamily -> genus
        lg_hier = {}
        for lg in self.languages.values():
            family = lg_hier.setdefault(lg['family'],
                                 LHNode(lg['family']))
            family.languages[lg['wals_code']] = lg
            if lg['subfamily']:
                subfamily = family.subclasses.setdefault(lg['subfamily'],
                                                  LHNode(lg['subfamily']))
                subfamily.superclass = family
                subfamily.languages[lg['wals_code']] = lg
                # set family to subfamily so genus info is added correctly
                family = subfamily
            genus = family.subclasses.setdefault(lg['genus'],
                                            LHNode(['genus']))
            genus.languages[lg['wals_code']] = lg
            genus.superclass = family
        self.language_hierarchy = lg_hier

    def show_language(self, wals_code):
        """
        Given a WALS code, print the data for a language to STDOUT.

        @param wals_code: The WALS code for a language.
        """

        print self.languages[wals_code]['name'], '(%s):' % wals_code
        data = self.data[wals_code]
        for feat in sorted(data.keys()):
            print ' ', self.features[feat]['name'], '(%s):' % feat,\
                  self.values[feat][data[feat]]['description'],\
                  '(%s)' % self.values[feat][data[feat]]['value_id']

    def get_wals_codes_from_iso(self, iso_code):
        """
        Given a ISO-639 language code, return a list of WALS codes that
        match the ISO code. This may be a one-to-many mapping.

        @param iso_code: The ISO-639 code for a language.
        """

        return [lg['wals_code'] for lg in self.iso_index[iso_code]]

    def get_wals_codes_from_name(self, name):
        """
        Given a language name, return a list of WALS codes that match
        the name. This may be a one-to-many mapping.
        
        @param name: The name of a language.
        """

        name = _lg_variety_re.sub('', name).lower()
        return [lg['wals_code'] for lg in self.language_name_index[name]]

    def get_languages_with_feature(self, feature, value=None, superclass=None):
        """
        Given at least a feature index, find all languages that have
        the feature. Feature values and a language superclass (family,
        subfamily, or genus) may be used to narrow the results.

        @param feature: A WALS feature index.
        @param value: A WALS value index.
        @param superclass: A string that is matched against language family,
                           subfamily, and genus.
        """

        if value: value = str(value) # be robust to int values
        supermatch = lambda x: superclass in (x['genus'],
                                              x['subfamily'],
                                              x['family'])
        valmatch = lambda x: self.data[l['wals_code']].get(feature) == value
        return [l for l in self.feat_lg_map[feature]
                if (not value or valmatch(l)) and \
                   (not superclass or supermatch(l))]

def demo(wals_directory=None, dialect='excel-tab', encoding='utf-8'):
    if not wals_directory:
        import sys
        print >>sys.stderr, 'Error: No WALS data directory provided.'
        print >>sys.stderr, '       You may obtain the database from ' +\
            'http://wals.info/export'
        return
    w = WALS(wals_directory, dialect, encoding)
    
    # Basic statistics
    print 'In database:\n  %d\tlanguages\n  %d\tfeatures ' %\
        (len(w.languages), len(w.features))
    # values are a nested dictionary (w.values[feature_id][value_id])
    num_vals = sum(map(len, w.values.values()))
    print '  %d\ttotal values (%f avg. number per feature)' %\
        (num_vals, float(num_vals)/len(w.features))
    # More statistics
    print "  %d languages specify feature 81A (order of S, O, and V)" %\
        (len(w.get_languages_with_feature('81A')))
    print "  %d langauges have VOS order" %\
        (len(w.get_languages_with_feature('81A', value='4')))

    # Getting language data
    print "\nGetting data for languages named 'Irish'"
    for wals_code in w.get_wals_codes_from_name('Irish'):
        l = w.languages[wals_code]
        print '  %s (ISO-639 code: %s WALS code: %s)' %\
            (l['name'], l['iso_codes'], wals_code)
    print "\nGetting data for languages with ISO 'isl'"
    for wals_code in w.get_wals_codes_from_iso('isl'):
        w.show_language(wals_code)
    print "\nLocations of dialects for the Min Nan language (ISO 'nan'):"
    for wals_code in w.get_wals_codes_from_iso('nan'):
        l = w.languages[wals_code]
        print "  %s\tlat:%f\tlong:%f" %\
            (l['name'], l['latitude'], l['longitude'])

########NEW FILE########
