__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# Recommender documentation build configuration file, created by
# sphinx-quickstart on Fri Dec  3 01:04:43 2010.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.append(os.path.abspath('.'))

# -- General configuration -----------------------------------------------------

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.intersphinx', 'sphinx.ext.ifconfig', 'sphinx.ext.pngmath']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'python-recsys'
copyright = u'2010, Oscar Celma'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '1.0'
# The full version, including alpha/beta/rc tags.
release = '1.0'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of documents that shouldn't be included in the build.
#unused_docs = []

# List of directories, relative to source directory, that shouldn't be searched
# for source files.
exclude_trees = []

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  Major themes that come with
# Sphinx are currently 'default' and 'sphinxdoc'.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_use_modindex = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# If nonempty, this is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = ''

# Output file base name for HTML help builder.
htmlhelp_basename = 'Recommenderdoc'


# -- Options for LaTeX output --------------------------------------------------

# The paper size ('letter' or 'a4').
#latex_paper_size = 'letter'

# The font size ('10pt', '11pt' or '12pt').
#latex_font_size = '10pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'pyrecsys.tex', u'python-recsys - Documentation',
   u'Oscar Celma', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# Additional stuff for the LaTeX preamble.
#latex_preamble = ''

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_use_modindex = True


# Example configuration for intersphinx: refer to the Python standard library.
intersphinx_mapping = {'http://docs.python.org/': None}

########NEW FILE########
__FILENAME__ = baseline
from numpy import mean
from operator import itemgetter

from recsys.algorithm.baseclass import Algorithm
from recsys.algorithm import VERBOSE

class Baseline(Algorithm):
    def __init__(self):
        #Call parent constructor
        super(Baseline, self).__init__()

        # 'Cache' for user avg. rating
        self._user_avg_rating = dict()

    def predict(self, i, j, MIN_VALUE=None, MAX_VALUE=None, user_is_row=True):
        index = i
        if not user_is_row:
            index = j

        if not self._user_avg_rating.has_key(index):
            if user_is_row:
                vector = self.get_matrix().get_row(index).entries()
            else:
                vector = self.get_matrix().get_col(index).entries()
            # Vector is a list of tuples: (rating, index). E.g (3.0, 20)
            self._user_avg_rating[index] = mean(map(itemgetter(0), vector))
        predicted_value = self._user_avg_rating[index]

        if MIN_VALUE:
            predicted_value = max(predicted_value, MIN_VALUE)
        if MAX_VALUE:
            predicted_value = min(predicted_value, MAX_VALUE)
        return predicted_value

########NEW FILE########
__FILENAME__ = movielens
import sys

#To show some messages:
import recsys.algorithm
recsys.algorithm.VERBOSE = True

from recsys.algorithm.factorize import SVD
from recsys.datamodel.data import Data
from recsys.evaluation.prediction import RMSE, MAE

#Dataset
PERCENT_TRAIN = int(sys.argv[2])
data = Data()
data.load(sys.argv[1], sep='::', format={'col':0, 'row':1, 'value':2, 'ids':int})
#Train & Test data
train, test = data.split_train_test(percent=PERCENT_TRAIN)

#Create SVD
K=100
if len(sys.argv) == 4:
    K = int(sys.argv[3])
svd = SVD()
svd.set_data(train)
svd.compute(k=K, min_values=0, pre_normalize=None, mean_center=True, post_normalize=True)

#Evaluation using prediction-based metrics
print 'Evaluating...'
rmse = RMSE()
mae = MAE()
for rating, item_id, user_id in test.get():
    try:
        pred_rating = svd.predict(item_id, user_id)
        rmse.add(rating, pred_rating)
        mae.add(rating, pred_rating)
    except KeyError:
        continue

print 'RMSE=%s' % rmse.compute()
print 'MAE=%s' % mae.compute()

########NEW FILE########
__FILENAME__ = svdlibc_import
import sys
import recsys.algorithm
recsys.algorithm.VERBOSE = True
from recsys.utils.svdlibc import SVDLIBC

movielens = sys.argv[1] #ratings.dat movielens file path here
svdlibc = SVDLIBC(movielens)
svdlibc.to_sparse_matrix(sep='::', format={'col':0, 'row':1, 'value':2, 'ids': int})
svdlibc.compute()
svd = svdlibc.export()
svdlibc.remove_files()
MOVIEID = 1
print svd.similar(MOVIEID)
print svd.recommend(MOVIEID)

########NEW FILE########
__FILENAME__ = test_baseline
import sys

#To show some messages:
import recsys.algorithm
recsys.algorithm.VERBOSE = True

from recsys.evaluation.prediction import RMSE, MAE
from recsys.datamodel.data import Data

from baseline import Baseline #Import the test class we've just created

#Dataset
PERCENT_TRAIN = int(sys.argv[2])
data = Data()
data.load(sys.argv[1], sep='::', format={'col':0, 'row':1, 'value':2, 'ids': int})
#Train & Test data
train, test = data.split_train_test(percent=PERCENT_TRAIN)

baseline = Baseline()
baseline.set_data(train)
baseline.compute() # In this case, it does nothing

# Evaluate
rmse = RMSE()
mae = MAE()
for rating, item_id, user_id in test.get():
    try:
        pred_rating = baseline.predict(item_id, user_id, user_is_row=False)
        rmse.add(rating, pred_rating)
        mae.add(rating, pred_rating)
    except KeyError:
        continue

print 'RMSE=%s' % rmse.compute() # in my case (~80% train, ~20% test set) returns RMSE = 1.036374
print 'MAE=%s' % mae.compute()   # in my case (~80% train, ~20% test set) returns  MAE = 0.829024

########NEW FILE########
__FILENAME__ = test_simple_svd
from recsys.algorithm.factorize import SVD
from recsys.datamodel.data import Data

data = [(4.0, 'user1', 'item1'),
 (2.0, 'user1', 'item3'),
 (1.0, 'user2', 'item1'),
 (5.0, 'user2', 'item4')]

d = Data()
d.set(data)
svd = SVD()
svd.set_data(d)
m = svd.get_matrix()
svd.compute(k=2)
print svd.similar('user1')
print svd.predict('user1', 'item1')

########NEW FILE########
__FILENAME__ = test_svd
import sys
from numpy import nan, mean

#To show some messages:
import recsys.algorithm
recsys.algorithm.VERBOSE = True

from recsys.algorithm.factorize import SVD, SVDNeighbourhood
from recsys.datamodel.data import Data
from recsys.evaluation.prediction import RMSE, MAE

# Create SVD
K=100
svd = SVD()
svd_neig = SVDNeighbourhood()

#Dataset
PERCENT_TRAIN = int(sys.argv[2])
data = Data()
data.load(sys.argv[1], sep='::', format={'col':0, 'row':1, 'value':2, 'ids': int})

rmse_svd_all = []
mae_svd_all = []
rmse_svd_neig_all = []
mae_svd_neig_all = []

RUNS = [1,2,3,4,5,6,7,8,9,10]
for run in RUNS:
	print 'RUN(%d)' % run
	#Train & Test data
	train, test = data.split_train_test(percent=PERCENT_TRAIN)

	svd.set_data(train)
	svd_neig.set_data(train)

	#Compute SVD
	svd.compute(k=K, min_values=None, pre_normalize=None, mean_center=True, post_normalize=True)
	svd_neig.compute(k=K, min_values=None, pre_normalize=None, mean_center=True, post_normalize=True)

	# Evaluate
	rmse_svd = RMSE()
	mae_svd = MAE()
	rmse_svd_neig = RMSE()
	mae_svd_neig = MAE()

	i = 1
	total = len(test.get())
	print 'Total Test ratings: %s' % total
	for rating, item_id, user_id in test:
	    try:
		    pred_rating_svd = svd.predict(item_id, user_id)
		    rmse_svd.add(rating, pred_rating_svd)
		    mae_svd.add(rating, pred_rating_svd)

		    pred_rating_svd_neig = svd_neig.predict(item_id, user_id) #Koren & co.
		    if pred_rating_svd_neig is not nan:
		        rmse_svd_neig.add(rating, pred_rating_svd_neig)
		        mae_svd_neig.add(rating, pred_rating_svd_neig)

		    print "\rProcessed test rating %d" % i,                                        
		    sys.stdout.flush()

		    i += 1
	    except KeyError:
    		continue

	    rmse_svd_all.append(rmse_svd.compute())
	    mae_svd_all.append(mae_svd.compute())
	    rmse_svd_neig_all.append(rmse_svd_neig.compute())
	    mae_svd_neig_all.append(mae_svd_neig.compute())
print
print 'RMSE (SVD) = %s | STD = %s' % (mean(rmse_svd_all), std(rmse_svd_all))
print 'MAE  (SVD) = %s | STD = %s' % (mean(mae_svd_all), std(mae_svd_all))
print 'RMSE (SVD Neig.) = %s | STD = %s' % (mean(rmse_svd_neig_all), std(rmse_svd_neig_all))
print 'MAE  (SVD Neig.) = %s | STD = %s' % (mean(mae_svd_neig_all), std(mae_svd_neig_all))

########NEW FILE########
__FILENAME__ = baseclass
"""
.. module:: algorithm
   :synopsis: Base class Algorithm

.. moduleauthor:: Oscar Celma <ocelma@bmat.com>

"""
import sys
from scipy.cluster.vq import kmeans2 #for kmeans method
from random import randint #for kmeans++ (_kinit method)
#from scipy.linalg import norm #for kmeans++ (_kinit method)
from scipy import array #for kmeans method
from numpy import sum
from numpy.linalg import norm #for _cosine and kmeans++ (_kinit method)
try:
    import divisi2
except:
    from csc import divisi2

from recsys.algorithm import VERBOSE
from recsys.algorithm.matrix import SparseMatrix
from recsys.datamodel.data import Data

class Algorithm(object):
    """
    Base class Algorithm

    It has the basic methods to load a dataset, get the matrix and the raw input
    data, add more data (tuples), etc.

    Any other Algorithm derives from this base class
    """
    def __init__(self):
        self._data = Data()
        self._matrix = SparseMatrix()
        self._matrix_similarity = None #self-similarity matrix (only for the input Matrix rows)
        self._matrix_and_data_aligned = False #both Matrix and Data contain the same info?

    def __len__(self):
        return len(self.get_data())

    def __repr__(self):
        s = '%d rows.' % len(self.get_data())
        if len(self.get_data()):
            s += '\nE.g: %s' % str(self.get_data()[0])
        return s

    def get_matrix(self):
        """
        :returns: matrix *M*
        """
        if not self._matrix.get():
            self.create_matrix()
        return self._matrix

    def get_matrix_similarity(self):
        """
        :returns: the self-similarity matrix
        """
        return self._matrix_similarity

    def set_data(self, data):
        """
        Sets the raw dataset (input for matrix *M*)

        :param data: a Dataset class (list of tuples <value, row, col>)
        :type data: Data
        """
        #self._data = Data()
        #self._data.set(data)
        self._data = data
        self._matrix_and_data_aligned = False

    def get_data(self):
        """
        :returns: An instance of Data class. The raw dataset (input for matrix *M*). 
        """
        return self._data

    def add_tuple(self, tuple):
        """
        Add a tuple in the dataset

        :param tuple: a tuple containing <rating, user, item> information. Or, more general: <value, row, col>
        """
        self.get_data().add_tuple(tuple)
        self._matrix_and_data_aligned = False

    def load_data(self, filename, force=True, sep='\t', format={'value':0, 'row':1, 'col':2}, pickle=False):
        """
        Loads a dataset file

        See params definition in *datamodel.Data.load()*
        """
        if force:
            self._data = Data()
            self._matrix_similarity = None

        self._data.load(filename, force, sep, format, pickle)
    
    def save_data(self, filename, pickle=False):
        """
        Saves the dataset in divisi2 matrix format (i.e: value <tab> row <tab> col)

        :param filename: file to store the data
        :type filename: string
        :param pickle: save in pickle format?
        :type filename: boolean
        """
        self._data.save(filename, pickle)

    def create_matrix(self):
        if VERBOSE:
            sys.stdout.write('Creating matrix (%s tuples)\n' % len(self._data))
        try:
            self._matrix.create(self._data.get())
        except AttributeError:
            self._matrix.create(self._data)

        if VERBOSE:
            sys.stdout.write("Matrix density is: %s%%\n" % self._matrix.density())
        self._matrix_and_data_aligned = True

    def compute(self, min_values=None):
        if self._matrix.empty() and (not isinstance(self._data, list) and not self._data.get()):
            raise ValueError('No data set. Matrix is empty!')
        if self._matrix.empty() and (isinstance(self._data, list) and not self._data):
            raise ValueError('No data set. Matrix is empty!')
        if not self._matrix.empty() or not self._matrix_and_data_aligned:
            self.create_matrix()

        if min_values:
            if VERBOSE:
                sys.stdout.write('Updating matrix: squish to at least %s values\n' % min_values)
            self._matrix.set(self._matrix.get().squish(min_values))

    def _get_row_similarity(self, i):
        if not self.get_matrix_similarity() or self.get_matrix_similarity().get() is None:
            self.compute()
        try:
            return self.get_matrix_similarity().get_row(i)
        except KeyError:
            raise KeyError("%s not found!" % i)

    def similar(self, i, n=10):
        """
        :param i: a row in *M*
        :type i: user or item id
        :param n: number of similar elements
        :type n: int
        :returns: the most similar elements of *i*
        """
        if not self.get_matrix_similarity() or self.get_matrix_similarity().get() is None:
            self.compute()
        return self._get_row_similarity(i).top_items(n)

    def similarity(self, i, j):
        """
        :param i: a row in *M*
        :type i: user or item id
        :param j: a row in *M*
        :type j: user or item id
        :returns: the similarity between the two elements *i* and *j*
        """
        if not self.get_matrix_similarity() or self.get_matrix_similarity().get() is None:
            self.compute()
        return self.get_matrix_similarity().value(i, j)

    def predict(self, i, j, MIN_VALUE=None, MAX_VALUE=None):
        raise NotImplementedError("cannot instantiate Abstract Base Class")

    def recommend(self, i, n=10):
        raise NotImplementedError("cannot instantiate Abstract Base Class")

    ### OTHER METHODS ###
    def _cosine(self, v1, v2):
        return float(divisi2.dot(v1,v2) / (norm(v1) * norm(v2)))

    def centroid(self, ids, are_rows=True):
        if VERBOSE:
            sys.stdout.write('Computing centroid for ids=%s\n' % str(ids))
        points = []
        for id in ids:
            if are_rows:
                point = self.get_matrix().get_row(id)
            else:
                point = self.get_matrix().get_col(id)
            points.append(point)
        M = divisi2.SparseMatrix(points)
        return M.col_op(sum)/len(points) #TODO numpy.sum seems slower?

    def _kinit(self, X, k):
        #Init k seeds according to kmeans++
        n = X.shape[0]
        #Choose the 1st seed randomly, and store D(x)^2 in D[]
        centers = [X[randint(0, n-1)]]
        D = [norm(x-centers[0])**2 for x in X]

        for _ in range(k-1):
            bestDsum = bestIdx = -1
            for i in range(n):
                #Dsum = sum_{x in X} min(D(x)^2,||x-xi||^2)
                Dsum = reduce(lambda x,y:x+y,
                              (min(D[j], norm(X[j]-X[i])**2) for j in xrange(n)))
                if bestDsum < 0 or Dsum < bestDsum:
                    bestDsum, bestIdx = Dsum, i
            centers.append(X[bestIdx])
            D = [min(D[i], norm(X[i]-X[bestIdx])**2) for i in xrange(n)]
        return array(centers)

    def kmeans(self, id, k=5, is_row=True):
        """
        K-means clustering. http://en.wikipedia.org/wiki/K-means_clustering

        Clusterizes the (cols) values of a given row, or viceversa

        :param id: row (or col) id to cluster its values
        :param k: number of clusters
        :param is_row: is param *id* a row (or a col)?
        :type is_row: Boolean
        """
        # TODO: switch to Pycluster?
        # http://pypi.python.org/pypi/Pycluster
        if VERBOSE:
            sys.stdout.write('Computing k-means, k=%s, for id %s\n' % (k, id))
        point = None
        if is_row:
            point = self.get_matrix().get_row(id)
        else:
            point = self.get_matrix().get_col(id)
        points = []
        points_id = []
        for i in point.nonzero_entries():
            label = point.label(i)
            points_id.append(label)
            if not is_row:
                points.append(self.get_matrix().get_row(label))
            else:
                points.append(self.get_matrix().get_col(label))
        #return kmeans(array(points), k)
        if VERBOSE:
            sys.stdout.write('id %s has %s points\n' % (id, len(points)))
        M = array(points)

        MAX_POINTS = 150
        # Only apply Matrix initialization if num. points is not that big!
        if len(points) <= MAX_POINTS:
            centers = self._kinit(array(points), k)
            centroids, labels = kmeans2(M, centers, minit='matrix')
        else:
            centroids, labels = kmeans2(M, k, minit='random')
        i = 0
        clusters = dict()
        for cluster in labels:
            if not clusters.has_key(cluster): 
                clusters[cluster] = dict()
                clusters[cluster]['centroid'] = centroids[cluster]
                clusters[cluster]['points'] = []
            clusters[cluster]['points'].append(points_id[i])
            i += 1
        return clusters


########NEW FILE########
__FILENAME__ = factorize
# -*- coding: utf-8 -*-
"""
.. module:: algorithm
   :synopsis: Factorization recsys algorithms

.. moduleauthor:: Oscar Celma <ocelma@bmat.com>

"""
import os
import sys
import zipfile
try:
    import divisi2
except:
    from csc import divisi2
from numpy import loads, mean, sum, nan
from operator import itemgetter

from scipy.cluster.vq import kmeans2 #for kmeans method
from random import randint #for kmeans++ (_kinit method)
from scipy.linalg import norm #for kmeans++ (_kinit method)
from scipy import array #for kmeans method

from numpy import fromfile #for large files (U and V)
from divisi2 import DenseVector
from divisi2 import DenseMatrix
from divisi2.ordered_set import OrderedSet
                                        
from recsys.algorithm.baseclass import Algorithm
from recsys.algorithm.matrix import SimilarityMatrix
from recsys.algorithm import VERBOSE

TMPDIR = '/tmp'

class SVD(Algorithm):
    """
    Inherits from base class Algorithm. 
    It computes SVD (Singular Value Decomposition) on a matrix *M*

    It also provides recommendations and predictions using the reconstructed matrix *M'*

    :param filename: Path to a Zip file, containing an already computed SVD (U, Sigma, and V) for a matrix *M*
    :type filename: string
    """
    def __init__(self, filename=None):
        #Call parent constructor
        super(SVD, self).__init__()

        # self._U: Eigen vector. Relates the concepts of the input matrix to the principal axes
        # self._S (or \Sigma): Singular -or eigen- values. It represents the strength of each eigenvector.
        # self._V: Eigen vector. Relates features to the principal axes
        self._U, self._S, self._V = (None, None, None)
        # Mean centered Matrix: row and col shifts
        self._shifts = None
        # self._matrix_reconstructed: M' = U S V^t
        self._matrix_reconstructed = None

        # Similarity matrix: (U \Sigma)(U \Sigma)^T = U \Sigma^2 U^T
        # U \Sigma is concept_axes weighted by axis_weights.
        self._matrix_similarity = SimilarityMatrix()

        if filename:
            self.load_model(filename)

        # Row and Col ids. Only when importing from SVDLIBC
        self._file_row_ids = None
        self._file_col_ids = None

    def __repr__(self):
        try:
            s = '\n'.join(('M\':' + str(self._reconstruct_matrix()), \
                'A row (U):' + str(self._reconstruct_matrix().right[1]), \
                'A col (V):' + str(self._reconstruct_matrix().left[1])))
        except TypeError:
            s = self._data.__repr__()
        return s

    def load_model(self, filename):
        """
        Loads SVD transformation (U, Sigma and V matrices) from a ZIP file

        :param filename: path to the SVD matrix transformation (a ZIP file)
        :type filename: string
        """
        try:
            zip = zipfile.ZipFile(filename, allowZip64=True)
        except:
            zip = zipfile.ZipFile(filename + '.zip', allowZip64=True)
        # Options file
        options = dict()
        for line in zip.open('README'):
            data = line.strip().split('\t')
            options[data[0]] = data[1]
        try:
            k = int(options['k'])
        except:
            k = 100 #TODO: nasty!!!

        # Load U, S, and V
        """
        #Python 2.6 only:
        #self._U = loads(zip.open('.U').read())
        #self._S = loads(zip.open('.S').read())
        #self._V = loads(zip.open('.V').read())
        """
        try:
            self._U = loads(zip.read('.U'))
        except:
            matrix = fromfile(zip.extract('.U', TMPDIR))
            vectors = []
            i = 0
            while i < len(matrix) / k:
                v = DenseVector(matrix[k*i:k*(i+1)])
                vectors.append(v)
                i += 1
            try:
                idx = [ int(idx.strip()) for idx in zip.read('.row_ids').split('\n') if idx]
            except:
                idx = [ idx.strip() for idx in zip.read('.row_ids').split('\n') if idx]
            #self._U = DenseMatrix(vectors) 
            self._U = DenseMatrix(vectors, OrderedSet(idx), None)
        try:
            self._V = loads(zip.read('.V'))
        except:
            matrix = fromfile(zip.extract('.V', TMPDIR))
            vectors = []
            i = 0
            while i < len(matrix) / k:
                v = DenseVector(matrix[k*i:k*(i+1)])
                vectors.append(v)
                i += 1
            try:
                idx = [ int(idx.strip()) for idx in zip.read('.col_ids').split('\n') if idx]
            except:
                idx = [ idx.strip() for idx in zip.read('.col_ids').split('\n') if idx]
            #self._V = DenseMatrix(vectors) 
            self._V = DenseMatrix(vectors, OrderedSet(idx), None)

        self._S = loads(zip.read('.S'))

        # Shifts for Mean Centerer Matrix
        self._shifts = None
        if '.shifts.row' in zip.namelist():
            self._shifts = [loads(zip.read('.shifts.row')), 
                            loads(zip.read('.shifts.col')),
                            loads(zip.read('.shifts.total'))
                           ]
        self._reconstruct_matrix(shifts=self._shifts, force=True)
        self._reconstruct_similarity(force=True)

    def save_model(self, filename, options={}):
        """
        Saves SVD transformation (U, Sigma and V matrices) to a ZIP file

        :param filename: path to save the SVD matrix transformation (U, Sigma and V matrices)
        :type filename: string
        :param options: a dict() containing the info about the SVD transformation. E.g. {'k': 100, 'min_values': 5, 'pre_normalize': None, 'mean_center': True, 'post_normalize': True}
        :type options: dict
        """
        if VERBOSE:
            sys.stdout.write('Saving svd model to %s\n' % filename)

        f_opt = open(filename + '.config', 'w')
        for option, value in options.items():
            f_opt.write('\t'.join((option, str(value))) + '\n')
        f_opt.close()
        # U, S, and V
        MAX_VECTORS = 2**21
        if len(self._U) < MAX_VECTORS:
            self._U.dump(filename + '.U')
        else:
            self._U.tofile(filename + '.U')
        if len(self._V) < MAX_VECTORS:
            self._V.dump(filename + '.V')
        else:
            self._V.tofile(filename + '.V')
        self._S.dump(filename + '.S')

        # Shifts for Mean Centered Matrix
        if self._shifts:
            #(row_shift, col_shift, total_shift)
            self._shifts[0].dump(filename + '.shifts.row')
            self._shifts[1].dump(filename + '.shifts.col')
            self._shifts[2].dump(filename + '.shifts.total')

        zip = filename
        if not filename.endswith('.zip') and not filename.endswith('.ZIP'):
            zip += '.zip'
        fp = zipfile.ZipFile(zip, 'w', allowZip64=True)

        # Store Options in the ZIP file
        fp.write(filename=filename + '.config', arcname='README')
        os.remove(filename + '.config')
        
        # Store matrices in the ZIP file
        for extension in ['.U', '.S', '.V']:
            fp.write(filename=filename + extension, arcname=extension)
            os.remove(filename + extension)

        # Store mean center shifts in the ZIP file
        if self._shifts:
            for extension in ['.shifts.row', '.shifts.col', '.shifts.total']:
                fp.write(filename=filename + extension, arcname=extension)
                os.remove(filename + extension)

        # Store row and col ids file, if importing from SVDLIBC
        if self._file_row_ids:
            fp.write(filename=self._file_row_ids, arcname='.row_ids')
        if self._file_col_ids:
            fp.write(filename=self._file_col_ids, arcname='.col_ids')


    def _reconstruct_similarity(self, post_normalize=True, force=True):
        if not self.get_matrix_similarity() or force:
            self._matrix_similarity = SimilarityMatrix()
            self._matrix_similarity.create(self._U, self._S, post_normalize=post_normalize)
        return self._matrix_similarity

    def _reconstruct_matrix(self, shifts=None, force=True):
        if not self._matrix_reconstructed or force:
            if shifts:
                self._matrix_reconstructed = divisi2.reconstruct(self._U, self._S, self._V, shifts=shifts)
            else:
                self._matrix_reconstructed = divisi2.reconstruct(self._U, self._S, self._V)
        return self._matrix_reconstructed

    def compute(self, k=100, min_values=None, pre_normalize=None, mean_center=False, post_normalize=True, savefile=None):
        """
        Computes SVD on matrix *M*, :math:`M = U \Sigma V^T`

        :param k: number of dimensions
        :type k: int
        :param min_values: min. number of non-zeros (or non-empty values) any row or col must have
        :type min_values: int
        :param pre_normalize: normalize input matrix. Possible values are tfidf, rows, cols, all.
        :type pre_normalize: string
        :param mean_center: centering the input matrix (aka mean substraction)
        :type mean_center: Boolean
        :param post_normalize: Normalize every row of :math:`U \Sigma` to be a unit vector. Thus, row similarity (using cosine distance) returns :math:`[-1.0 .. 1.0]`
        :type post_normalize: Boolean
        :param savefile: path to save the SVD factorization (U, Sigma and V matrices)
        :type savefile: string
        """
        super(SVD, self).compute(min_values)

        if VERBOSE:
            sys.stdout.write('Computing svd k=%s, min_values=%s, pre_normalize=%s, mean_center=%s, post_normalize=%s\n' 
                            % (k, min_values, pre_normalize, mean_center, post_normalize))
            if not min_values:
                sys.stdout.write('[WARNING] min_values is set to None, meaning that some funky recommendations might appear!\n')

        # Get SparseMatrix
        matrix = self._matrix.get()

        # Mean center?
        shifts, row_shift, col_shift, total_shift = (None, None, None, None)
        if mean_center:
            if VERBOSE:
                sys.stdout.write("[WARNING] mean_center is True. svd.similar(...) might return nan's. If so, then do svd.compute(..., mean_center=False)\n")
            matrix, row_shift, col_shift, total_shift = matrix.mean_center() 
            self._shifts = (row_shift, col_shift, total_shift)

        # Pre-normalize input matrix?
        if pre_normalize:
            """
            Divisi2 divides each entry by the geometric mean of its row norm and its column norm. 
            The rows and columns don't actually become unit vectors, but they all become closer to unit vectors.
            """
            if pre_normalize == 'tfidf':
                matrix = matrix.normalize_tfidf() #TODO By default, treats the matrix as terms-by-documents; 
                                                  # pass cols_are_terms=True if the matrix is instead documents-by-terms.
            elif pre_normalize == 'rows':
                matrix = matrix.normalize_rows()
            elif pre_normalize == 'cols':
                matrix = matrix.normalize_cols()
            elif pre_normalize == 'all':
                matrix = matrix.normalize_all()
            else:
                raise ValueError("Pre-normalize option (%s) is not correct.\n \
                                  Possible values are: 'tfidf', 'rows', 'cols' or 'all'" % pre_normalize)
        #Compute SVD(M, k)
        self._U, self._S, self._V = matrix.svd(k)
        # Sim. matrix = U \Sigma^2 U^T
        self._reconstruct_similarity(post_normalize=post_normalize, force=True)
        # M' = U S V^t
        self._reconstruct_matrix(shifts=self._shifts, force=True)

        if savefile:
            options = {'k': k, 'min_values': min_values, 'pre_normalize': pre_normalize, 'mean_center': mean_center, 'post_normalize': post_normalize}
            self.save_model(savefile, options)

    def _get_row_reconstructed(self, i, zeros=None):
        if zeros:
            return self._matrix_reconstructed.row_named(i)[zeros]
        return self._matrix_reconstructed.row_named(i)

    def _get_col_reconstructed(self, j, zeros=None):
        if zeros:
            return self._matrix_reconstructed.col_named(j)[zeros]
        return self._matrix_reconstructed.col_named(j)

    def predict(self, i, j, MIN_VALUE=None, MAX_VALUE=None):
        """
        Predicts the value of :math:`M_{i,j}`, using reconstructed matrix :math:`M^\prime = U \Sigma_k V^T`

        :param i: row in M, :math:`M_{i \cdot}`
        :type i: user or item id
        :param j: col in M, :math:`M_{\cdot j}`
        :type j: item or user id
        :param MIN_VALUE: min. value in M (e.g. in ratings[1..5] => 1)
        :type MIN_VALUE: float
        :param MAX_VALUE: max. value in M (e.g. in ratings[1..5] => 5)
        :type MAX_VALUE: float
        """
        if not self._matrix_reconstructed:
            self.compute() #will use default values!
        predicted_value = self._matrix_reconstructed.entry_named(i, j) #M' = U S V^t
        if MIN_VALUE:
            predicted_value = max(predicted_value, MIN_VALUE)
        if MAX_VALUE:
            predicted_value = min(predicted_value, MAX_VALUE)
        return float(predicted_value)

    def recommend(self, i, n=10, only_unknowns=False, is_row=True):
        """
        Recommends items to a user (or users to an item) using reconstructed matrix :math:`M^\prime = U \Sigma_k V^T`

        E.g. if *i* is a row and *only_unknowns* is True, it returns the higher values of :math:`M^\prime_{i,\cdot}` :math:`\\forall_j{M_{i,j}=\emptyset}`

        :param i: row or col in M
        :type i: user or item id
        :param n: number of recommendations to return
        :type n: int
        :param only_unknowns: only return unknown values in *M*? (e.g. items not rated by the user)
        :type only_unknowns: Boolean
        :param is_row: is param *i* a row (or a col)?
        :type is_row: Boolean
        """
        if not self._matrix_reconstructed:
            self.compute() #will use default values!
        item = None
        zeros = []
        if only_unknowns and not self._matrix.get():
            raise ValueError("Matrix is empty! If you loaded an SVD model you can't use only_unknowns=True, unless svd.create_matrix() is called")
        if is_row:
            if only_unknowns:
                zeros = self._matrix.get().row_named(i).zero_entries()
            item = self._get_row_reconstructed(i, zeros)
        else:
            if only_unknowns:
                zeros = self._matrix.get().col_named(i).zero_entries()
            item = self._get_col_reconstructed(i, zeros)
        return item.top_items(n)

    def centroid(self, ids, is_row=True):
        points = []
        for id in ids:
            if is_row:
                point = self._U.row_named(id)
            else:
                point = self._V.row_named(id)
            points.append(point)
        M = divisi2.SparseMatrix(points)
        return M.col_op(sum)/len(points) #TODO Numpy.sum?

    def kmeans(self, ids, k=5, components=3, are_rows=True):
        """
        K-means clustering. It uses k-means++ (http://en.wikipedia.org/wiki/K-means%2B%2B) to choose the initial centroids of the clusters

        Clusterizes a list of IDs (either row or cols)

        :param ids: list of row (or col) ids to cluster
        :param k: number of clusters
        :param components: how many eigen values use (from SVD)
        :param are_rows: is param *ids* a list of rows (or cols)?
        :type are_rows: Boolean
        """
        if not isinstance(ids, list):
            # Cluster the whole row(or col) values. It's slow!
            return super(SVD, self).kmeans(ids, k=k, is_row=are_rows)
        if VERBOSE:
            sys.stdout.write('Computing k-means, k=%s for ids %s\n' % (k, ids))
        MAX_POINTS = 150
        points = []
        for id in ids:
            if are_rows:
                points.append(self._U.row_named(id)[:components])
            else:
                points.append(self._V.row_named(id)[:components])
        M = array(points)
        # Only apply Matrix initialization if num. points is not that big!
        if len(points) <= MAX_POINTS:
            centers = self._kinit(array(points), k)
            centroids, labels = kmeans2(M, centers, minit='matrix')
        else:
            centroids, labels = kmeans2(M, k, minit='random')
        i = 0
        clusters = dict()
        for cluster in labels:
            if not clusters.has_key(cluster): 
                clusters[cluster] = dict()
                clusters[cluster]['centroid'] = centroids[cluster]
                clusters[cluster]['points'] = []
            point = self._U.row_named(ids[i])[:components]
            centroid = clusters[cluster]['centroid']
            to_centroid = self._cosine(centroid, point)
            clusters[cluster]['points'].append((ids[i], to_centroid))
            clusters[cluster]['points'].sort(key=itemgetter(1), reverse=True)
            i += 1
        return clusters

    '''
    def kmeans(self, id, k=5, is_row=True):
        """
        K-means clustering. It uses k-means++ (http://en.wikipedia.org/wiki/K-means%2B%2B) for choosing the initial centroids of the clusters

        Clusterizes the (cols) values of a given row, or viceversa

        :param id: row (or col) id to cluster its values
        :param k: number of clusters
        :param is_row: is param *id* a row (or a col)?
        :type is_row: Boolean
        """
        if VERBOSE:
            sys.stdout.write('Computing k-means (from SVD) for %s, with k=%s\n' % (id, k))
        point = None
        if is_row:
            point = self.get_matrix().get_row(id)
        else:
            point = self.get_matrix().get_col(id)
        points = []
        for i in point.nonzero_entries():
            label = point.label(i)
            points.append(label)
        return self._kmeans(points, k, not is_row)
    '''

# SVDNeighbourhood
class SVDNeighbourhood(SVD):
    """
    Classic Neighbourhood plus Singular Value Decomposition. Inherits from SVD class

    Predicts the value of :math:`M_{i,j}`, using simple avg. (weighted) of
    all the ratings by the most similar users (or items). This similarity, *sim(i,j)* is derived from the SVD

    :param filename: Path to a Zip file, containing an already computed SVD (U, Sigma, and V) for a matrix *M*
    :type filename: string
    :param Sk: number of similar elements (items or users) to be used in *predict(i,j)*
    :type Sk: int
    """
    def __init__(self, filename=None, Sk=10):
        # Call parent constructor
        super(SVDNeighbourhood, self).__init__(filename)

        # Number of similar elements
        self._Sk = Sk #Length of Sk(i;u)

    def similar_neighbours(self, i, j, Sk=10):
        similars = self.similar(i, Sk*10) #Get 10 times Sk
        # Get only those items that user j has already rated
        current = 0
        _Sk = Sk
        for similar, weight in similars[1:]:
            if self.get_matrix().value(similars[current][0], j) == 0.0:
                similars.pop(current)
                current -= 1
                _Sk += 1
            current += 1
            _Sk -= 1
            if _Sk == 0: 
                break # We have enough elements to use
        return similars[:Sk]

    def predict(self, i, j, Sk=10, weighted=True, MIN_VALUE=None, MAX_VALUE=None):
        """
        Predicts the value of :math:`M_{i,j}`, using simple avg. (weighted) of
        all the ratings by the most similar users (or items)

        if *weighted*:
            :math:`\hat{r}_{ui} = \\frac{\sum_{j \in S^{k}(i;u)} sim(i, j) r_{uj}}{\sum_{j \in S^{k}(i;u)} sim(i, j)}`

        else:
            :math:`\hat{r}_{ui} = mean(\sum_{j \in S^{k}(i;u)} r_{uj})`

        :param i: row in M, :math:`M_{i \cdot}`
        :type i: user or item id
        :param j: col in M, :math:`M_{\cdot j}`
        :type j: item or user id
        :param Sk: number of k elements to be used in :math:`S^k(i; u)`
        :type Sk: int
        :param weighted: compute avg. weighted of all the ratings?
        :type weighted: Boolean
        :param MIN_VALUE: min. value in M (e.g. in ratings[1..5] => 1)
        :type MIN_VALUE: float
        :param MAX_VALUE: max. value in M (e.g. in ratings[1..5] => 5)
        :type MAX_VALUE: float
        """
        if not Sk:
            Sk = self._Sk
        similars = self.similar_neighbours(i, j, Sk)
        #Now, similars == S^k(i; u)

        sim_ratings = []
        sum_similarity = 0.0
        for similar, weight in similars:
            sim_rating = self.get_matrix().value(similar, j)
            if sim_rating is None: #== 0.0:
                continue
            sum_similarity += weight
            if weighted:
                sim_ratings.append(weight * sim_rating)
            else:
                sim_ratings.append(sim_rating)

        if not sum_similarity or not sim_ratings:
            return nan

        if weighted:
            predicted_value = sum(sim_ratings)/sum_similarity
        else:
            predicted_value = mean(sim_ratings)
        if MIN_VALUE:
            predicted_value = max(predicted_value, MIN_VALUE)
        if MAX_VALUE:
            predicted_value = min(predicted_value, MAX_VALUE)
        return float(predicted_value)


# SVDNeighbourhoodKoren
class __SVDNeighbourhoodKoren(SVDNeighbourhood):
    """
    Inherits from SVDNeighbourhood class. 

    Neighbourhood model, using Singular Value Decomposition.
    Based on 'Factorization Meets the Neighborhood: a Multifaceted
    Collaborative Filtering Model' (Yehuda Koren)
    http://public.research.att.com/~volinsky/netflix/kdd08koren.pdf

    :param filename: Path to a Zip file, containing an already computed SVD (U, Sigma, and V) for a matrix *M*
    :type filename: string
    :param Sk: number of similar elements (items or users) to be used in *predict(i,j)*
    :type Sk: int
    """
    def __init__(self, filename=None, Sk=10):
        # Call parent constructor
        super(SVDNeighbourhoodKoren, self).__init__(filename, Sk)

        # µ denotes the overall average rating
        self._Mu = None
        # Mean of all rows
        self._mean_rows = None
        # Mean of all cols
        self._mean_cols = None
        # Mean of each row / col
        self._mean_row = dict()
        self._mean_col = dict()

    def set_mu(self, mu):
        """
        Sets the :math:`\mu`. The overall average rating

        :param mu: overall average rating
        :type mu: float
        """
        self._Mu = mu

    def _set_mean_all(self, avg=None, is_row=True):
        m = self._mean_row.values()
        if not is_row:
            m = self._mean_col.values()
        return mean(m)

    def set_mean_rows(self, avg=None):
        """
        Sets the average value of all rows

        :param avg: the average value (if None, it computes *average(i)*)
        :type avg: float
        """
        self._mean_rows = self._set_mean_all(avg, is_row=True)

    def set_mean_cols(self, avg=None):
        """
        Sets the average value of all cols

        :param avg: the average value (if None, it computes *average(i)*)
        :type avg: float
        """
        self._mean_cols = self._set_mean_all(avg, is_row=False)

    def set_mean(self, i, avg=None, is_row=True):
        """
        Sets the average value of a row (or column).

        :param i: a row (or column)
        :type i: user or item id
        :param avg: the average value (if None, it computes *average(i)*)
        :type avg: float
        :param is_row: is param *i* a row (or a col)?
        :type is_row: Boolean
        """
        d = self._mean_row
        if not is_row:
            d = self._mean_col
        if avg is None: #Compute average
            m = self._matrix.get().row_named
            if not is_row:
                m = self._matrix.get().col_named
            avg = mean(m(i))
        d[i] = avg

    def predict(self, i, j, Sk=None, MIN_VALUE=None, MAX_VALUE=None):
        """
        Predicts the value of *M(i,j)*

        It is based on 'Factorization Meets the Neighborhood: a Multifaceted
        Collaborative Filtering Model' (Yehuda Koren). 
        Equation 3 (section 2.2):

        :math:`\hat{r}_{ui} = b_{ui} + \\frac{\sum_{j \in S^k(i;u)} s_{ij} (r_{uj} - b_{uj})}{\sum_{j \in S^k(i;u)} s_{ij}}`, where
        :math:`b_{ui} = \mu + b_u + b_i`

        http://public.research.att.com/~volinsky/netflix/kdd08koren.pdf

        :param i: row in M, M(i)
        :type i: user or item id
        :param j: col in M, M(j)
        :type j: user or item id
        :param Sk: number of k elements to be used in :math:`S^k(i; u)`
        :type Sk: int
        :param MIN_VALUE: min. value in M (e.g. in ratings[1..5] => 1)
        :type MIN_VALUE: float
        :param MAX_VALUE: max. value in M (e.g. in ratings[1..5] => 5)
        :type MAX_VALUE: float

        """
        # http://public.research.att.com/~volinsky/netflix/kdd08koren.pdf
        # bui = µ + bu + bi
        #   The parameters bu and bi indicate the observed deviations of user
        #   u and item i, respectively, from the average
        # 
        # S^k(i; u): 
        #   Using the similarity measure, we identify the k items rated
        #   by u, which are most similar to i.
        #
        # sij: similarity between i and j
        #
        # r^ui = bui + Sumj∈S^k(i;u) sij (ruj − buj) / Sumj∈S^k(i;u) sij
        if not Sk:
            Sk = self._Sk

        similars = self.similar_neighbours(i, j, Sk)
        #Now, similars == S^k(i; u)

        #bu = self._mean_col.get(j, self.set_mean(j, is_row=False)) - self._mean_cols
        #bi = self._mean_row.get(i, self.set_mean(i, is_row=True)) - self._mean_rows
        bu = self._mean_col[j] - self._mean_cols
        bi = self._mean_row[i] - self._mean_rows
        bui = bu + bi
        #if self._Mu: #TODO uncomment?
        #   bui += self._Mu
 
        sim_ratings = []
        sum_similarity = 0.0
        for similar, sij in similars[1:]:
            sim_rating = self.get_matrix().value(similar, j)
            if sim_rating is None:
                continue
            ruj = sim_rating
            sum_similarity += sij
            bj = self._mean_row[similar]- self._mean_rows
            buj = bu + bj
            sim_ratings.append(sij * (ruj - buj))

        if not sum_similarity or not sim_ratings:
            return nan

        Sumj_Sk = sum(sim_ratings)/sum_similarity
        rui = bui + Sumj_Sk
        predicted_value = rui
        
        if MIN_VALUE:
            predicted_value = max(predicted_value, MIN_VALUE)
        if MAX_VALUE:
            predicted_value = min(predicted_value, MAX_VALUE)
        return float(predicted_value)


########NEW FILE########
__FILENAME__ = matrix
try:
    from divisi2.sparse import SparseMatrix as divisiSparseMatrix
    from divisi2 import reconstruct_similarity
except:
    from csc.divisi2.sparse import SparseMatrix as divisiSparseMatrix
    from csc.divisi2 import reconstruct_similarity

from operator import itemgetter

class Matrix(object):
    def __init__(self):
        self._matrix = None

    def __repr__(self):
        return str(self._matrix)

    def create(self, data):
        raise NotImplementedError("cannot instantiate Abstract Base Class")

    def density(self, percent=True):
        if not self._matrix or not self._matrix.entries():
            return None
        density = self._matrix.density()
        if percent:
            density *= 100
        return round(density, 4)

    def empty(self):
        raise NotImplementedError("cannot instantiate Abstract Base Class")

    def get(self):
        return self._matrix

    def set(self, matrix):
        self._matrix = matrix

    def get_row(self, i):
        if self.empty() or not self._matrix.col_labels:
            raise ValueError('Matrix is empty (or has no columns!)')
        return self._matrix.row_named(i)

    def get_col(self, j):
        if self.empty() or not self._matrix.row_labels:
            raise ValueError('Matrix is empty (or has no rows!)')
        return self._matrix.col_named(j)

    def value(self, i, j):
        if self.empty():
            raise ValueError('Matrix is empty!')
        return self._matrix.entry_named(i, j)

    def get_value(self, i, j):
        if self.empty():
            raise ValueError('Matrix is empty!')
        return self.value(i, j)

    def set_value(self, i, j, value):
        if self.empty():
            raise ValueError('Matrix is empty!')
        self._matrix.set_entry_named(i, j, value)

    def get_row_len(self):
        if self.empty() or not self._matrix.col_labels:
            raise ValueError('Matrix is empty (or has no columns!)')
        return len(self._matrix.col_labels)

    def get_col_len(self):
        if self.empty() or not self._matrix.row_labels:
            raise ValueError('Matrix is empty (or has no rows!)')
        return len(self._matrix.row_labels)


class SparseMatrix(Matrix):
    def __init__(self):
        super(SparseMatrix, self).__init__()

    def create(self, data):
        values = map(itemgetter(0), data)
        rows = map(itemgetter(1), data)
        cols = map(itemgetter(2), data)
        self._matrix = divisiSparseMatrix.from_named_lists(values, rows, cols)

    def empty(self):
        return not self._matrix or not self._matrix.values()

class SimilarityMatrix(Matrix):
    def __init__(self):
        super(SimilarityMatrix, self).__init__()

    def create(self, U, S, post_normalize=False):
        self._matrix = reconstruct_similarity(U, S, post_normalize=post_normalize)

    def empty(self):
        nrows, ncols = (0, 0)
        if self._matrix:
            nrows, ncols = self._matrix.shape
        return not self._matrix or not (nrows and ncols)


########NEW FILE########
__FILENAME__ = data
import sys
import codecs
import pickle
#from random import shuffle
from exceptions import ValueError
from numpy.random import shuffle

from recsys.algorithm import VERBOSE

class Data:
    """
    Handles the relationshops among users and items
    """
    def __init__(self):
        #"""
        #:param data: a list of tuples
        #:type data: list
        #"""
        self._data = list([])

    def __repr__(self):
        s = '%d rows.' % len(self.get())
        if len(self.get()):
            s += '\nE.g: %s' % str(self.get()[0])
        return s

    def __len__(self):
        return len(self.get())

    def __getitem__(self, i):
        if i < len(self._data):
            return self._data[i]
        return None

    def __iter__(self):
        return iter(self.get())

    def set(self, data, extend=False):
        """
        Sets data to the dataset

        :param data: a list of tuples
        :type data: list
        """
        if extend:
            self._data.extend(data)
        else:
            self._data = data

    def get(self):
        """
        :returns: a list of tuples
        """
        return self._data

    def add_tuple(self, tuple):
        """
        :param tuple: a tuple containing <rating, user, item> information (e.g.  <value, row, col>)
        """
        #E.g: tuple = (25, "ocelma", "u2") -> "ocelma has played u2 25 times"
        if not len(tuple) == 3:
            raise ValueError('Tuple format not correct (should be: <value, row_id, col_id>)')
        value, row_id, col_id = tuple
        if not value and value != 0:
            raise ValueError('Value is empty %s' % (tuple,))
        if isinstance(value, basestring):
            raise ValueError('Value %s is a string (must be an int or float) %s' % (value, tuple,))
        if row_id is None or row_id == '':
            raise ValueError('Row id is empty %s' % (tuple,))
        if col_id is None or col_id == '':
            raise ValueError('Col id is empty %s' % (tuple,))
        self._data.append(tuple)

    def split_train_test(self, percent=80, shuffle_data=True):
        """
        Splits the data in two disjunct datasets: train and test

        :param percent: % of training set to be used (test set size = 100-percent)
        :type percent: int
        :param shuffle_data: shuffle dataset?
        :type shuffle_data: Boolean

        :returns: a tuple <Data, Data>
        """
        if shuffle_data:
            shuffle(self._data)
        length = len(self._data)
        train_list = self._data[:int(round(length*percent/100.0))]
        test_list = self._data[-int(round(length*(100-percent)/100.0)):]
        train = Data()
        train.set(train_list)
        test = Data()
        test.set(test_list)

        return train, test

    def load(self, path, force=True, sep='\t', format=None, pickle=False):
        """
        Loads data from a file

        :param path: filename
        :type path: string
        :param force: Cleans already added data
        :type force: Boolean
        :param sep: Separator among the fields of the file content
        :type sep: string
        :param format: Format of the file content. 
            Default format is 'value': 0 (first field), then 'row': 1, and 'col': 2.
            E.g: format={'row':0, 'col':1, 'value':2}. The row is in position 0, 
            then there is the column value, and finally the rating. 
            So, it resembles to a matrix in plain format
        :type format: dict()
        :param pickle: is input file in  pickle format?
        :type pickle: Boolean
        """
        if VERBOSE:
            sys.stdout.write('Loading %s\n' % path)
        if force:
            self._data = list([])
        if pickle:
            self._load_pickle(path)
        else:
            i = 0 
            for line in codecs.open(path, 'r', 'utf8'):
                data = line.strip('\r\n').split(sep)
                value = None
                if not data:
                    raise TypeError('Data is empty or None!')
                if not format:
                    # Default value is 1
                    try:
                        value, row_id, col_id = data
                    except:
                        value = 1
                        row_id, col_id = data
                else:
                    try:
                        # Default value is 1
                        try:
                            value = data[format['value']]
                        except KeyError, ValueError:
                            value = 1
                        try: 
                            row_id = data[format['row']]
                        except KeyError:
                            row_id = data[1]
                        try:
                            col_id = data[format['col']]
                        except KeyError:
                            col_id = data[2]
                        row_id = row_id.strip()
                        col_id = col_id.strip()
                        if format.has_key('ids') and (format['ids'] == int or format['ids'] == 'int'):
                            try:
                                row_id = int(row_id)
                            except:
                                print 'Error (ID is not int) while reading: %s' % data #Just ignore that line
                                continue
                            try:
                                col_id = int(col_id)
                            except:
                                print 'Error (ID is not int) while reading: %s' % data #Just ignore that line
                                continue
                    except IndexError:
                        #raise IndexError('while reading %s' % data)
                        print 'Error while reading: %s' % data #Just ignore that line
                        continue
                # Try to convert ids to int
                try:
                    row_id = int(row_id)
                except: pass
                try:
                    col_id = int(col_id)
                except: pass
                # Add tuple
                try:
                    self.add_tuple((float(value), row_id, col_id))
                except:
                    if VERBOSE:
                        sys.stdout.write('\nError while reading (%s, %s, %s). Skipping this tuple\n' % (value, row_id, col_id))
                    #raise ValueError('%s is not a float, while reading %s' % (value, data))
                i += 1
                if VERBOSE:
                    if i % 100000 == 0:
                        sys.stdout.write('.')
                    if i % 1000000 == 0:
                        sys.stdout.write('|')
                    if i % 10000000 == 0:
                        sys.stdout.write(' (%d M)\n' % int(i/1000000))
            if VERBOSE:
                sys.stdout.write('\n')

    def _load_pickle(self, path):
        """
        Loads data from a pickle file

        :param path: output filename
        :type param: string
        """
        self._data = pickle.load(codecs.open(path))

    def save(self, path, pickle=False):
        """
        Saves data in output file

        :param path: output filename
        :type param: string
        :param pickle: save in pickle format?
        :type pickle: Boolean
        """
        if VERBOSE:
            sys.stdout.write('Saving data to %s\n' % path)
        if pickle:
            self._save_pickle(path)
        else:
            out = codecs.open(path, 'w', 'utf8')
            for value, row_id, col_id in self._data:
                try:
                    value = unicode(value, 'utf8')
                except:
                    if not isinstance(value, unicode):
                        value = str(value)
                try:
                    row_id = unicode(row_id, 'utf8')
                except:
                    if not isinstance(row_id, unicode):
                        row_id = str(row_id)
                try:
                    col_id = unicode(col_id, 'utf8')
                except:
                    if not isinstance(col_id, unicode):
                        col_id = str(col_id)

                s = '\t'.join([value, row_id, col_id])
                out.write(s + '\n')
            out.close()

    def _save_pickle(self, path):
        """
        Saves data in output file, using pickle format

        :param path: output filename
        :type param: string
        """
        pickle.dump(self._data, open(path, "w"))

########NEW FILE########
__FILENAME__ = item
class Item:
    """
    An item, with its related metadata information

    :param id: item id
    :type id: string or int
    :returns: an item instance

    """
    def __init__(self, id):
        self._id = id
        self._data = None

    def __repr__(self):
        return str(self._id)

    def get_id(self):
        """
        Returns the Item id
        """
        return self._id

    def get_data(self):
        """
        Returns the associated information of the item
        """
        return self._data

    def add_data(self, data):
        """
        :param data: associated data for the item
        :type data: dict() or list()
        """
        self._data = data

########NEW FILE########
__FILENAME__ = user
class User:
    """
    User information, including her interaction with the items

    :param id: user id
    :type id: string or int
    :returns: a user instance

    """
    def __init__(self, id):
        self._id = id
        self._items = []

    def __repr__(self):
        return str(self._id)

    def get_id(self):
        """
        Returns the User id
        """
        return self._id

    def add_item(self, item_id, weight): 
        """
        :param item_id: An item ID
        :param weight: The weight (rating, views, plays, etc.) of the item_id for this user
        """
        self._items.append((item_id, weight))

    def get_items(self):
        """
        Returns the list of items for the user
        """
        return self._items

########NEW FILE########
__FILENAME__ = baseclass
from operator import itemgetter
from numpy import nan

class Evaluation(object):
    """
    Base class for Evaluation

    It has the basic methods to load ground truth and test data.
    Any other Evaluation class derives from this base class.

    :param data: A list of tuples, containing the real and the predicted value. E.g: [(3, 2.3), (1, 0.9), (5, 4.9), (2, 0.9), (3, 1.5)]
    :type data: list
    """
    def __init__(self, data=None):
        #data is a list of tuples. E.g: [(3, 2.3), (1, 0.9), (5, 4.9), (2, 0.9), (3, 1.5)]
        if data:
            self._ground_truth, self._test = map(itemgetter(0), data), map(itemgetter(1), data)
        else:
            self._ground_truth = []
            self._test = []

    def __repr__(self):
        gt = str(self._ground_truth)
        test = str(self._test)
        return 'GT  : %s\nTest: %s' % (gt, test)
        #return str('\n'.join((str(self._ground_truth), str(self._test))))

    def load_test(self, test):
        """
        Loads a test dataset

        :param test: a list of predicted values. E.g: [2.3, 0.9, 4.9, 0.9, 1.5] 
        :type test: list
        """
        if isinstance(test, list):
            self._test = list(test)
        else:
            self._test = test

    def get_test(self):
        """
        :returns: the test dataset (a list)
        """
        return self._test

    def load_ground_truth(self, ground_truth):
        """
        Loads a ground truth dataset

        :param ground_truth: a list of real values (aka ground truth). E.g: [3.0, 1.0, 5.0, 2.0, 3.0]
        :type ground_truth: list
        """
        if isinstance(ground_truth, list):
            self._ground_truth = list(ground_truth)
        else:
            self._ground_truth = ground_truth

    def get_ground_truth(self):
        """
        :returns: the ground truth list
        """
        return self._ground_truth

    def load(self, ground_truth, test):
        """
        Loads both the ground truth and the test lists. The two lists must have the same length.

        :param ground_truth: a list of real values (aka ground truth). E.g: [3.0, 1.0, 5.0, 2.0, 3.0]
        :type ground_truth: list
        :param test: a list of predicted values. E.g: [2.3, 0.9, 4.9, 0.9, 1.5] 
        :type test: list
        """
        self.load_ground_truth(ground_truth)
        self.load_test(test)

    def add(self, rating, rating_pred):
        """
        Adds a tuple <real rating, pred. rating>

        :param rating: a real rating value (the ground truth)
        :param rating_pred: the predicted rating
        """
        if rating is not nan and rating_pred is not nan:
            self._ground_truth.append(rating)
            self._test.append(rating_pred)

    def add_test(self, rating_pred):
        """
        Adds a predicted rating to the current test list

        :param rating_pred: the predicted rating
        """
        if rating_pred is not nan:
            self._test.append(rating_pred)

    def compute(self):
        """
        Computes the evaluation using the loaded ground truth and test lists
        """
        if len(self._ground_truth) == 0:
            raise ValueError('Ground Truth dataset is empty!')
        if len(self._test) == 0:
            raise ValueError('Test dataset is empty!')


########NEW FILE########
__FILENAME__ = decision
from recsys.evaluation.baseclass import Evaluation
from recsys.evaluation import ROUND_FLOAT

# Decision-Based Metrics. Evaluating Top-N recommendations
class PrecisionRecallF1(Evaluation):
    def __init__(self):
        super(PrecisionRecallF1, self).__init__()

    def add_predicted_value(self, rating_pred): # Synonym of self.add_test
        self.add_test(rating_pred)

    def compute(self):
        super(PrecisionRecallF1, self).compute()
        """
        precision, recall, f1 = (0.0, 0.0, 0.0)
        TP, FP, TN, FN = (0, 0, 0, 0)
        ground_truth = list(self._ground_truth)
        for item in self._test:
            if item in ground_truth:
                TP += 1
                ground_truth.pop(ground_truth.index(item))
            else:
                FP += 1
        FN = len(ground_truth)
        """
        hit_set = list(set(self._ground_truth) & set(self._test))
        precision = len(hit_set) / float(len(self._test)) #TP/float(TP+FP)
        recall = len(hit_set) / float(len(self._ground_truth)) #TP/float(TP+FN)
        if precision == 0.0 and recall == 0.0:
            return (0.0, 0.0, 0.0)
        f1 = 2 * ((precision*recall)/(precision+recall))
        return (round(precision, ROUND_FLOAT), round(recall, ROUND_FLOAT), round(f1, ROUND_FLOAT))


########NEW FILE########
__FILENAME__ = prediction
from math import sqrt
from scipy.stats import pearsonr

from recsys.evaluation import ROUND_FLOAT
from recsys.evaluation.baseclass import Evaluation

#Predictive-Based Metrics
class MAE(Evaluation):
    """
    Mean Absolute Error

    :param data: a tuple containing the Ground Truth data, and the Test data
    :type data: <list, list>
    """
    def __init__(self, data=None):
        super(MAE, self).__init__(data)

    def compute(self, r=None, r_pred=None):
        if r and r_pred:
            return round(abs(r - r_pred), ROUND_FLOAT)

        if not len(self._ground_truth) == len(self._test):
            raise ValueError('Ground truth and Test datasets have different sizes!')        

        #Compute for the whole test set
        super(MAE, self).compute()
        sum = 0.0 
        for i in range(0, len(self._ground_truth)):
            r = self._ground_truth[i]
            r_pred = self._test[i]
            sum += abs(r - r_pred)
        return round(abs(float(sum/len(self._test))), ROUND_FLOAT)

class RMSE(Evaluation):
    """
    Root Mean Square Error

    :param data: a tuple containing the Ground Truth data, and the Test data
    :type data: <list, list>
    """
    def __init__(self, data=None):
        super(RMSE, self).__init__(data)

    def compute(self, r=None, r_pred=None):
        if r and r_pred:
            return round(sqrt(abs((r - r_pred)*(r - r_pred))), ROUND_FLOAT)

        if not len(self._ground_truth) == len(self._test):
            raise ValueError('Ground truth and Test datasets have different sizes!')        

        #Compute for the whole test set
        super(RMSE, self).compute()
        sum = 0.0 
        for i in range(0, len(self._ground_truth)):
            r = self._ground_truth[i]
            r_pred = self._test[i]
            sum += abs((r - r_pred)*(r - r_pred))
        return round(sqrt(abs(float(sum/len(self._test)))), ROUND_FLOAT)

#Correlation-Based Metrics
class Pearson(Evaluation):
    """
    Pearson correlation

    :param data: a tuple containing the Ground Truth data, and the Test data
    :type data: <list, list>
    """
    def __init__(self, data=None):
        super(Pearson, self).__init__(data)

    def compute(self):
        super(Pearson, self).compute()
        return round(pearsonr(self._ground_truth, self._test)[0], ROUND_FLOAT)

########NEW FILE########
__FILENAME__ = ranking
from numpy import mean
from scipy.stats import kendalltau, spearmanr, rankdata
from operator import itemgetter

from recsys.evaluation import ROUND_FLOAT
from recsys.evaluation.baseclass import Evaluation

def _compute(f, ground_truth, test):
    elems = len(list(set(map(itemgetter(0), ground_truth)) & set(map(itemgetter(0), test))))
    if len(ground_truth) != elems or len(test) != elems:
        raise ValueError('Ground truth and Test datasets have different elements!')
    ground_truth.sort()
    test.sort()
    ground_truth, test = map(itemgetter(1), ground_truth), map(itemgetter(1), test)
    return round(f(ground_truth, test)[0], ROUND_FLOAT)

#Rank-Based Metrics:
class SpearmanRho(Evaluation):
    def __init__(self, data=None):
        super(SpearmanRho, self).__init__(data)

    def compute(self):
        super(SpearmanRho, self).compute()
        if not len(self._ground_truth) == len(self._test):
            raise ValueError('Ground truth and Test datasets have different sizes!')
        try:
            return _compute(spearmanr, self._ground_truth, self._test)
        except TypeError:
            return round(spearmanr(self._ground_truth, self._test)[0], ROUND_FLOAT)

class KendallTau(Evaluation):
    def __init__(self, data=None):
        super(KendallTau, self).__init__(data)

    def compute(self):
        super(KendallTau, self).compute()
        try:
            return _compute(kendalltau, self._ground_truth, self._test)
        except TypeError:
            return round(kendalltau(self._ground_truth, self._test)[0], ROUND_FLOAT)

class ReciprocalRank(Evaluation):
    def __init__(self):
        super(ReciprocalRank, self).__init__()

    def compute(self, ground_truth=None, query=None):
        if not query:
            query = self._test
        if not ground_truth:
            ground_truth = self._ground_truth
        try:
            rank_query = ground_truth.index(query) + 1
            rr = 1.0 / rank_query
            return rr
        except ValueError:
            return 0.0
        
class MeanReciprocalRank(Evaluation):
    def __init__(self):
        super(MeanReciprocalRank, self).__init__()
        # _rr stores partial ReciprocalRank results:
        self._rr = []

    def load(self, ranked_list, elem):
        if isinstance(elem, list):
            raise ValueError('2nd param must be an element not a list!. For example: load([1,2,3,4], 2)')
        self._ground_truth.append(ranked_list)
        self._test.append(elem)
        #Compute current ReciprocalRank
        rr = ReciprocalRank()
        rr.load(ranked_list, elem)
        self._rr.append(rr.compute())

    def load_test(self, elem):
        raise NotImplementedError("load_test() method not allowed. Use load(ground_truth, query) instead")

    def load_ground_truth(self, gt):
        raise NotImplementedError("load_ground_truth() method not allowed. Use load(ground_truth, query) instead")

    def get_reciprocal_rank_results(self):
        return self._rr

    def compute(self, ground_truth=None, query=None):
        if query and ground_truth:
            self.load(ground_truth, query)
            return self._rr[-1]
        return round(mean(self.get_reciprocal_rank_results()), ROUND_FLOAT)

"""
from recsys.evaluation.decision import PrecisionRecallF1
class PrecisionAtK(Evaluation):
    def __init__(self):
        super(PrecisionAtK, self).__init__()

    def compute(self, ground_truth=None, query=None):
        if not query:
            query = self._test
        if not ground_truth:
            ground_truth = self._ground_truth
        try:
            hit_set = list(set(self._ground_truth) & set(self._test))
            precision = len(hit_set) / float(len(self._test)) #TP/float(TP+FP)
        except ValueError:
            return 0.0
"""

class AveragePrecision(Evaluation):
    def __init__(self):
        super(AveragePrecision, self).__init__()

    def __compute(self):
        super(AveragePrecision, self).compute()
        i = 1
        hits = 0
        p_at_k = [0.0]*len(self._test)
        for item in self._test:
            try:
                hit = self._ground_truth.index(item) + 1
                hits += 1
                p = hits/float(i)
                p_at_k[i-1] = hits/float(i)
            except:
                pass
            i += 1
        return sum(p_at_k)/hits

    def compute(self):
        super(AveragePrecision, self).compute()
        from recsys.evaluation.decision import PrecisionRecallF1

        if not isinstance(self._test, list):
            self._test = [self._test]

        PRF1 = PrecisionRecallF1()
        p_at_k = []
        hits = 0
        for k in range(1, len(self._test)+1):
            test = self._test[:k]
            PRF1.load(self._ground_truth, test)
            if test[k-1] in self._ground_truth:
                p, r, f1 = PRF1.compute()
                hits += 1
            else:
                p = 0.0
            p_at_k.append(p)
        if not hits:
            return 0.0
        return sum(p_at_k)/hits

class MeanAveragePrecision(Evaluation):
    def __init__(self):
        super(MeanAveragePrecision, self).__init__()
        # _ap stores partial AveragePrecision results:
        self._ap = []

    def load(self, ground_truth, test):
        if not isinstance(test, list):
            test = [test]
        self._ground_truth.append(ground_truth)
        self._test.append(test)
        #Compute current AveragePrecision
        ap = AveragePrecision()
        ap.load(ground_truth, test)
        self._ap.append(ap.compute())

    def load_test(self, elem):
        raise NotImplementedError("load_test() method not allowed. Use load(ground_truth, query) instead")

    def load_ground_truth(self, gt):
        raise NotImplementedError("load_ground_truth() method not allowed. Use load(ground_truth, query) instead")

    def get_average_precision_results(self):
        return self._ap

    def compute(self, ground_truth=None, query=None):
        if query and ground_truth:
            self.load(ground_truth, query)
            return self._ap[-1]
        return round(mean(self.get_average_precision_results()), ROUND_FLOAT)

#TODO: how to define utility function g(u,i)? 
# Specially when there's no ratings (R(u,i) = 3, but another value (such as plays)?
#class DCG(Evaluation):
#    pass #TODO

########NEW FILE########
__FILENAME__ = svdlibc
import sys
import codecs
import os
from operator import itemgetter
import csv
from numpy import array
from divisi2.ordered_set import OrderedSet
from csc.divisi2.dense import DenseMatrix
from recsys.algorithm.factorize import SVD
from recsys.datamodel.data import Data
from recsys.algorithm import VERBOSE

# Path to 'svd' executable [ http://tedlab.mit.edu/~dr/SVDLIBC/ ]
PATH_SVDLIBC = '/usr/local/bin/'

class SVDLIBC(object):
    def __init__(self, datafile=None, matrix='matrix.dat', prefix='svd'):
        self._data_file = datafile
        self._matrix_file = matrix
        self._svd_prefix = prefix

    def compute(self, k=100, matrix=None, prefix=None):
        if matrix:
            self._matrix_file = matrix
        if prefix:
            self._svd_prefix = prefix
        if VERBOSE:
            sys.stdout.write('SVDLIBC: Computing svd(k=%s) from %s, saving it to %s\n' % (k, self._matrix_file, self._svd_prefix))
        error_code = os.spawnv(os.P_WAIT, PATH_SVDLIBC + 'svd', ['-r st', '-d%d' % k, '-o%s' % self._svd_prefix, self._matrix_file])
        if error_code == 127:
            raise IOError('svd executable not found in: %s. You might need to download it: %s' 
                    % (PATH_SVDLIBC + 'svd', 'http://tedlab.mit.edu/~dr/SVDLIBC/'))

    def set_matrix(self, matrix):
        self._matrix_file = matrix

    def remove_files(self):
        PREFIX = self._svd_prefix
        file_Ut = PREFIX + '-Ut'
        file_Vt = PREFIX + '-Vt'
        file_S = PREFIX + '-S'
        file_row_ids = '%s.ids.rows' % self._svd_prefix
        file_col_ids = '%s.ids.cols' % self._svd_prefix

        files = [self._matrix_file, file_Ut, file_Vt, file_S, file_row_ids, file_col_ids]
        for file in files:
            if not os.path.exists(file):
                raise IOError('could not delete file %s' % file)
            os.remove(file)

    def to_sparse_matrix(self, sep='\t', format=None):
        # http://tedlab.mit.edu/~dr/SVDLIBC/SVD_F_ST.html
        data = Data()
        data.load(self._data_file, sep=sep, format=format)

        f = open(self._matrix_file, 'w')
        f_row_ids = codecs.open('%s.ids.rows' % self._svd_prefix, 'w', 'utf8')
        f_col_ids = codecs.open('%s.ids.cols' % self._svd_prefix, 'w', 'utf8')

        num_rows = len(set(map(itemgetter(1), data)))
        num_cols = len(set(map(itemgetter(2), data)))
        non_zero = len(data)
        f.write('%s %s %s\n' % (num_rows, num_cols, non_zero))

        #print 'sorting data by col'
        l = data.get()
        #l.sort(key=itemgetter(2, 1)) #by col, and then row
        l.sort(key=itemgetter(2))

        rows = dict()
        cols = dict()
        prev_col_id = None
        col_values = []
        row, col = (0, 0)
        for value, row_id, col_id in l:
            #if not row_id or not col_id or not value:
            #    if VERBOSE:
            #        sys.stdout.write('Skipping: %s, %s, %s\n' % (value, row_id, col_id))
            #    continue
            if col_id != prev_col_id:
                if col_values:
                    f.write('%s\n' % len(col_values))
                    for col_row_id, col_value in col_values:
                        _row = rows[col_row_id]
                        f.write('%s %s\n' % (_row, col_value))
                col_values = []
                cols[col_id] = col
                col += 1
            if not rows.has_key(row_id):
                rows[row_id] = row
                row += 1
            col_values.append((row_id, value))
            prev_col_id = col_id
        if col_values:
            f.write('%s\n' % len(col_values))
            for col_row_id, col_value in col_values:
                row = rows[col_row_id]
                f.write('%s %s\n' % (row, col_value))
            cols[col_id] = col
        f.close()

        # Now write f_row_ids and f_col_ids
        rows = rows.items()
        rows.sort(key=itemgetter(1))
        for row_id, _ in rows:
            if row_id == '':
                continue
            if isinstance(row_id, int):
                row_id = str(row_id)
            f_row_ids.write(row_id + '\n')
        f_row_ids.close()

        cols = cols.items()
        cols.sort(key=itemgetter(1))
        for col_id, _ in cols:
            if col_id == '':
                continue
            if isinstance(col_id, int):
                col_id = str(col_id)
            f_col_ids.write(col_id + '\n')
        f_col_ids.close()

    def export(self):
        # http://tedlab.mit.edu/~dr/SVDLIBC/SVD_F_DT.html
        # only importing default 'dt' S, Ut and Vt (dense text output matrices)
        PREFIX = self._svd_prefix
        file_Ut = PREFIX + '-Ut'
        file_Vt = PREFIX + '-Vt'
        file_S = PREFIX + '-S'
        # Not really used:
        file_U = PREFIX + '-U'
        file_V = PREFIX + '-V'
        
        # Read matrices files (U, S, Vt), using CSV (it's much faster than numpy.loadtxt()!)
        if VERBOSE:
            sys.stdout.write('Reading files: %s, %s, %s\n' % (file_Ut, file_Vt, file_S))
        try:
            Ut = array(list(csv.reader(open(file_Ut),delimiter=' '))[1:]).astype('float')
            U = Ut.transpose()
        except:
            U = array(list(csv.reader(open(file_U),delimiter=' '))[1:]).astype('float')
        try:
            Vt = array(list(csv.reader(open(file_Vt),delimiter=' '))[1:]).astype('float')
            V = Vt.transpose()
        except:
            V = array(list(csv.reader(open(file_V),delimiter=' '))[1:]).astype('float')
            #Vt = V.transpose()
        _S = array(list(csv.reader(open(file_S),delimiter=' '))[1:]).astype('float')
        S = _S.reshape(_S.shape[0], )
        
        PREFIX_INDEXES = PREFIX + '.ids.'
        file_U_idx = PREFIX_INDEXES + 'rows'
        file_V_idx = PREFIX_INDEXES + 'cols'
        if VERBOSE:
            sys.stdout.write('Reading index files: %s, %s\n' % (file_U_idx, file_V_idx))
        try:
            U_idx = [ int(idx.strip()) for idx in open(file_U_idx)]
        except:
            U_idx = [ idx.strip() for idx in open(file_U_idx)]
        try:
            V_idx = [ int(idx.strip()) for idx in open(file_V_idx)]
        except:
            V_idx = [ idx.strip() for idx in open(file_V_idx)]
        
        #Check no duplicated IDs!!!
        assert(len(U_idx) == len(OrderedSet(U_idx))), 'There are duplicated row IDs!'
        assert(len(U_idx) == U.shape[0]), 'There are duplicated (or empty) row IDs!'
        assert(len(V_idx) == len(OrderedSet(V_idx))), 'There are duplicated col IDs!'
        assert(len(V_idx) == V.shape[0]), 'There are duplicated (or empty) col IDs'
 
        # Create SVD
        if VERBOSE:
            sys.stdout.write('Creating SVD() class\n')
        svd = SVD()
        svd._U = DenseMatrix(U, OrderedSet(U_idx), None)
        svd._S = S
        svd._V = DenseMatrix(V, OrderedSet(V_idx), None)
        svd._matrix_similarity = svd._reconstruct_similarity()
        svd._matrix_reconstructed = svd._reconstruct_matrix()

        # If save_model, then use row and col ids from SVDLIBC
        MAX_VECTORS = 2**21
        if len(svd._U) > MAX_VECTORS:
            svd._file_row_ids = file_U_idx
        if len(svd._V) > MAX_VECTORS:
            svd._file_col_ids = file_V_idx
        
        return svd

if __name__ == "__main__":
    import sys
    from recsys.algorithm.factorize import SVD

    datafile = sys.argv[1] #In default matrix format: value \t row \t col \n
    prefix = sys.argv[2]
    matrix = '/tmp/matrix.dat'
    k = int(sys.argv[3])

    svdlibc = SVDLIBC(datafile=datafile, matrix=matrix, prefix=prefix)
    print 'Loading', datafile
    svdlibc.to_sparse_matrix(format={'ids': int})
    svdlibc.compute(k)
    print '\nLoading SVD'
    svd = svdlibc.export()
    print svd
    svd.save_model('/tmp/svd-model', options={'k': k})

########NEW FILE########
__FILENAME__ = test_algorithm
# -*- coding: utf-8 -*-
import os
import codecs

from random import randrange
from nose import with_setup
from nose.tools import assert_equal, assert_not_equal, assert_raises, assert_true

import recsys.algorithm
recsys.algorithm.VERBOSE = True

from recsys.datamodel.data import Data
from recsys.datamodel.user import User
from recsys.datamodel.item import Item
from recsys.algorithm.factorize import SVD

from tests import MOVIELENS_DATA_PATH

FACTORS = 50 #The K for SVD

#Define some global vars.
ITEMID = 1
USERID1 = 1
USERID2 = 5
NUM_SIMILARS = 10

MIN_RATING = 1.0
MAX_RATING = 5.0

svd = None
users = None
items = None

def setup():
    global users, items, svd

    print 'Reading items...'
    items = _read_items(os.path.join(MOVIELENS_DATA_PATH, 'movies.dat'))
    users = []

    svd = SVD()
    svd.load_data(filename=os.path.join(MOVIELENS_DATA_PATH, 'ratings.dat'), sep='::', format={'col':0, 'row':1, 'value':2, 'ids':int})

def teardown():
    pass

# Read movie info
def _read_items(filename):
    items = dict()
    for line in codecs.open(filename, 'r', 'latin1'):
        #1::Toy Story (1995)::Animation|Children's|Comedy
        data =  line.strip('\r\n').split('::')
        item_id = int(data[0])
        item_name = data[1]
        str_genres = data[2]
        genres = []
        for genre in str_genres.split('|'):
            genres.append(genre)
        items[item_id] = Item(item_id)
        items[item_id].add_data({'name': item_name, 'genres': genres})
    return items

### TESTS ###
def test_matrix_get_row_len():
    row_len = 6040
    assert_equal(svd.get_matrix().get_row_len(), row_len)

def test_matrix_get_col_len():
    col_len = 3706
    assert_equal(svd.get_matrix().get_col_len(), col_len)

def test_matrix_density():
    density = 4.4684
    assert_equal(svd.get_matrix().density(), density)

def test_get_data():
    num_rows = 1000209
    assert_equal(len(svd.get_data()), num_rows)

def test_save_data():
    data_in = svd.get_data()
    svd.save_data(os.path.join(MOVIELENS_DATA_PATH, 'ratings.matrix.saved'))

    svd.load_data(os.path.join(MOVIELENS_DATA_PATH, 'ratings.matrix.saved'))
    data_saved = svd.get_data()

    assert_equal(len(data_in), len(data_saved))
    assert_true(isinstance(data_saved, Data))

def test_utf8_data():
    data_in = Data()

    NUM_PLAYS = 69
    ITEMID = u'Bj\xf6rk' 
    data_in.add_tuple([NUM_PLAYS, ITEMID, USERID1])

    NUM_PLAYS = 34
    ITEMID = 'Björk' 
    data_in.add_tuple([NUM_PLAYS, ITEMID, USERID2])

    data_in.save(os.path.join(MOVIELENS_DATA_PATH, 'ratings.matrix.saved.utf8'))

    data_saved = Data()
    data_saved.load(os.path.join(MOVIELENS_DATA_PATH, 'ratings.matrix.saved.utf8'))

    assert_equal(len(data_in), len(data_saved))

def test_save_pickle():
    data_in = svd.get_data()
    svd.save_data(os.path.join(MOVIELENS_DATA_PATH, 'ratings.matrix.pickle'), pickle=True)

    svd.load_data(os.path.join(MOVIELENS_DATA_PATH, 'ratings.matrix.pickle'), pickle=True)
    data_saved = svd.get_data()

    assert_equal(len(data_in), len(data_saved))
    assert_true(isinstance(data_saved, Data))

def test_load_pickle():
    svd = SVD()
    svd.load_data(os.path.join(MOVIELENS_DATA_PATH, 'ratings.matrix.pickle'), pickle=True)
    assert_true(isinstance(svd.get_data(), Data))

def test_compute_empty_matrix():
    svd = SVD()
    assert_raises(ValueError, svd.compute)

def test_compute_svd():
    svd.load_data(filename=os.path.join(MOVIELENS_DATA_PATH, 'ratings.dat'), sep='::', format={'col':0, 'row':1, 'value':2, 'ids':int})
    svd.compute(FACTORS, min_values=10, pre_normalize=None, mean_center=True, post_normalize=True)

    user_ids = [1, 2, 3]
    item_ids = [1, 48, 3114, 25, 44, 3142, 617]
    print
    for item_id in item_ids:
        item_name = items[item_id].get_data()['name']
        #Item similarity (in item_ids)
        for other_item_id in item_ids:
            other_item_name = items[other_item_id].get_data()['name']
            print item_name, other_item_name, svd.similarity(item_id, other_item_id)
        print
        #Similar items for item_id
        for sim_item, similarity in svd.similar(item_id, NUM_SIMILARS):
            print item_name, items[sim_item].get_data()['name'], similarity
        print
        #Predicted ratings for (user_id, item_id)
        for user_id in user_ids:
            pred_rating = svd.predict(user_id, item_id, MIN_RATING, MAX_RATING)
            print item_name, user_id, svd.get_matrix().value(item_id, user_id), pred_rating
        print

def test_save_model():
    svd.save_model(os.path.join(MOVIELENS_DATA_PATH, 'SVD_matrix'), options={'k': FACTORS})

def test_load_model():
    svd2 = SVD()
    svd2.load_model(os.path.join(MOVIELENS_DATA_PATH, 'SVD_matrix'))
    recs_svd = svd.recommend(USERID1, NUM_SIMILARS, is_row=False)
    recs_svd2 = svd2.recommend(USERID1, NUM_SIMILARS, is_row=False)
    assert_equal(recs_svd, recs_svd2)

def test_recommendations():
    # Recommendations are based on svd.compute() from test_compute_svd()
    print
    for item_id, relevance in svd.recommend(USERID1, NUM_SIMILARS, is_row=False): # Recommend items to USERID1
        print USERID1, items[item_id].get_data()['name'], relevance
    print
    for item_id, relevance in svd.recommend(USERID2, NUM_SIMILARS, only_unknowns=False, is_row=False): # Recommend (unknown) items to USERID2
        print USERID2, items[item_id].get_data()['name'], relevance

def test_predictions():
    # Predictions are based on previously svd.compute() from test_compute_svd(). That is:
    # svd.compute(FACTORS, min_values=10, pre_normalize=None, mean_center=True, post_normalize=True)
    print
    item_ids = [1, 48, 3114]
    for item_id in item_ids:
        print item_id, USERID1
        print 'real value     ', svd.get_matrix().value(item_id, USERID1)
        print 'predicted value', svd.predict(item_id, USERID1, MIN_RATING, MAX_RATING)

def test_centroid():
    item_ids = [1, 48, 3114]
    centroid = svd.centroid(item_ids)

    item1 = svd._U.row_named(item_ids[0])
    assert_true(svd._cosine(centroid, item1) > 0.85)
    item2 = svd._U.row_named(item_ids[1])
    assert_true(svd._cosine(centroid, item2) > 0.10)
    item3 = svd._U.row_named(item_ids[2])
    assert_true(svd._cosine(centroid, item3) > 0.80)

def test_largest_eigenvectors():
    #Look up the first column of svd._U, and ask for its top items.
    item_ids = svd._U[:,0].top_items(5)
    for item_id, relevance in item_ids:
        print item_id, items[item_id].get_data()['name'], relevance
    print
#    item_ids = (-svd._U[:,0]).top_items(5)
#    for item_id, relevance in item_ids:
#        print items[item_id].get_data()['name'], relevance
#    user_ids, relevance = svd._V[:,0].top_items(5)
#    for user_id in user_ids:
#        print user_id

def test_kmeans_kinit():
    k = 5
    col = svd._V.row_named(USERID1)
    print svd._kinit(col, k)

def test_kmeans():
    item_ids = [1, 48, 3114, 25, 44, 3142, 617, 1193, 3408]
    # K-means based on a list of ids
    clusters = svd.kmeans(item_ids, k=3, are_rows=True)
    #print clusters
    for cluster in clusters.values():
        for other_cluster in clusters.values():
            print svd._cosine(cluster['centroid'], other_cluster['centroid'])
        print
    clusters = svd.kmeans(USERID1, are_rows=False)
    print clusters

def test_add_tuple():
    #This test goes in the end. Else it destroys the other tests! (as it adds one more row to the original matrix)
    num_rows = len(svd.get_data())
    svd.add_tuple((5.0, USERID1, items[ITEMID].get_id()))
    assert_equal(len(svd.get_data()), num_rows+1)


########NEW FILE########
__FILENAME__ = test_datamodel
import os
from nose import with_setup
from nose.tools import assert_equal, assert_not_equal, assert_raises, assert_true

from recsys.datamodel.user import User
from recsys.datamodel.item import Item
from recsys.datamodel.data import Data

from tests import MOVIELENS_DATA_PATH

USERID = 'ocelma'
MOVIEID = 1
ARTISTID = 'u2'
PLAYS = 25

def setup():
    global user, items, data
    user = User(USERID)
    items = _read_items(os.path.join(MOVIELENS_DATA_PATH, 'movies.dat'))
    data = Data()
    data.load(os.path.join(MOVIELENS_DATA_PATH, 'ratings.dat'), sep='::', format={'col':0, 'row':1, 'value':2, 'ids':int})

def teardown():
    pass

# Read movie info
def _read_items(filename):
    items = dict()
    for line in open(filename):
        #1::Toy Story (1995)::Animation|Children's|Comedy
        data =  line.strip('\r\n').split('::')
        item_id = data[0]
        item_name = data[1]
        genres = data[2:]
        items[item_id] = Item(item_id)
        items[item_id].add_data({'name': item_name, 'genres': genres})
    return items

# DATA tests
def test_data_split_train_test():
    train, test = data.split_train_test()
    assert_equal(len(train), 800167)
    assert_equal(len(test), 200042)

def test_data_extend():
    dataset = [(1,2,3), (4,5,6)]
    dataset2 = [(7,8,9), (10,11,12)]
    data = Data()
    data.set(dataset)
    assert_equal(len(data), 2)

    data.set(dataset2, extend=True)
    assert_equal(len(data), 4)

def test_data_add_tuple():
    VALUE = 4.0
    tuple = (VALUE, 'row_id', 'col_id')
    data = Data()
    data.add_tuple(tuple)
    assert_equal(data[0][0], VALUE)

def test_data_add_tuple_error_format():
    tuple = (4, 'row_id', 'col_id', 'another error value!')
    assert_raises(ValueError, data.add_tuple, tuple)

def test_data_add_tuple_error_format2():
    tuple = ('row_id', 'col_id')
    assert_raises(ValueError, data.add_tuple, tuple)

def test_data_add_tuple_value_none():
    tuple = (None, 'row_id', 'col_id')
    assert_raises(ValueError, data.add_tuple, tuple)

def test_data_add_tuple_value_empty_string():
    tuple = ('', 'row_id', 'col_id')
    assert_raises(ValueError, data.add_tuple, tuple)

def test_data_add_tuple_value_string():
    tuple = ('1.0', 'row_id', 'col_id')
    assert_raises(ValueError, data.add_tuple, tuple)

#def test_data_add_tuple_zero_value():
#    tuple = (0, 'row_id', 'col_id')
#    assert_raises(ValueError, data.add_tuple, tuple)

def test_data_add_tuple_row_id_empty():
    tuple = (1, '', 'col_id')
    assert_raises(ValueError, data.add_tuple, tuple)

def test_data_add_tuple_col_id_empty():
    tuple = (1, 'row_id', '')
    assert_raises(ValueError, data.add_tuple, tuple)

def test_data_add_tuple_row_id_none():
    tuple = (1, None, 'col_id')
    assert_raises(ValueError, data.add_tuple, tuple)

def test_data_add_tuple_col_id_none():
    tuple = (1, 'row_id', None)
    assert_raises(ValueError, data.add_tuple, tuple)

#USER tests
def test_user_build():
    u = User(USERID)
    assert_equal(u.get_id(), USERID)

def test_user_add_item():
    u = User(USERID)
    item = Item(ARTISTID)
    item.add_data({'name': ARTISTID})
    u.add_item(item, PLAYS)
    assert_equal(str(u.get_items()), '[(u2, 25)]')

def test_user_get_items():
    for item, weight in user.get_items():
        assert_true(isinstance(item, Item))

#ITEM tests
def test_item_build():
    data = dict()
    data['name'] = 'u2'
    data['popularity'] = 5.0
    item = Item(MOVIEID)
    item.add_data(data)
    assert_true(isinstance(item, Item))
    assert_equal(str(item.get_data()), "{'popularity': 5.0, 'name': 'u2'}")

def test_item_get_data():
    item = items['1']
    assert_true(isinstance(item, Item))
    assert_equal(str(item.get_data()), "{\'genres\': [\"Animation|Children\'s|Comedy\"], \'name\': \'Toy Story (1995)\'}")


########NEW FILE########
__FILENAME__ = test_evaluation
from nose import with_setup
from nose.tools import assert_equal, assert_not_equal, assert_raises, assert_true
from numpy import nan, array

from operator import itemgetter

from recsys.evaluation.prediction import MAE, RMSE, Pearson
from recsys.evaluation.decision import PrecisionRecallF1
from recsys.evaluation.ranking import KendallTau, SpearmanRho, MeanReciprocalRank, ReciprocalRank, AveragePrecision, MeanAveragePrecision

class Test(object):
    def __init__(self):
        self.DATA_PRED = [(3, 2.3), (1, 0.9), (5, 4.9), (2, 0.9), (3, 1.5)]
        self.GT_DATA = map(itemgetter(0), self.DATA_PRED)
        self.TEST_DATA = map(itemgetter(1), self.DATA_PRED)

        self.TEST_DECISION = ['classical', 'invented', 'baroque', 'instrumental']
        self.GT_DECISION = ['classical', 'instrumental', 'piano', 'baroque']

        self.TEST_RANKING = [('classical', 25.0), ('piano', 75.0), ('baroque', 50.0), ('instrumental', 25.0)]
        self.GT_RANKING = [('classical', 50.0), ('piano', 100.0), ('baroque', 25.0), ('instrumental', 25.0)]


class TestPrediction(Test):
    def __init__(self):
        super(TestPrediction, self).__init__()
        # Prediction-based metrics: MAE, RMSE, Pearson
        self.mae = MAE(self.DATA_PRED)
        self.rmse = RMSE(self.DATA_PRED)

        self.R = 3        # Real Rating (ground truth)
        self.R_PRED = 2.1 # Predicted Rating

    # test_PRED MAE
    def test_PRED_MAE_compute_one(self):
        assert_equal(self.mae.compute(self.R, self.R_PRED), 0.9)

    def test_PRED_MAE_compute_one_empty_datasets(self):
        mae = MAE()
        assert_equal(mae.compute(self.R, self.R_PRED), 0.9)

    def test_PRED_MAE_compute_all(self):
        assert_equal(self.mae.compute(), 0.7)

    def test_PRED_MAE_nan(self):
        mae = MAE()
        mae.add(2.0, nan)
        assert_equal(mae.get_test(), [])
        assert_equal(mae.get_ground_truth(), [])

    def test_PRED_MAE_load(self):
        mae = MAE()
        mae.load(self.GT_DATA, self.TEST_DATA)
        assert_equal(mae.compute(), 0.7)

    def test_PRED_MAE_load_test(self):
        mae = MAE()
        mae.load_test(self.TEST_DATA)
        assert_equal(len(mae.get_test()), len(self.TEST_DATA))
        assert_equal(len(mae.get_ground_truth()), 0)
        assert_raises(ValueError, mae.compute) #Raise: GT is empty!

    def test_PRED_MAE_load_test_and_ground_truth(self):
        mae = MAE()
        mae.load_test(self.TEST_DATA)
        mae.load_ground_truth(self.GT_DATA)
        assert_equal(mae.compute(), 0.7)

    def test_PRED_MAE_add_entry(self):
        self.mae.add(1, 4) #1: GT rating, 4: Predicted rating
        assert_equal(len(self.mae.get_test()), len(self.DATA_PRED)+1)
        assert_equal(self.mae.compute(), 1.083333)

    def test_PRED_MAE_different_list_sizes(self):
        mae = MAE()
        GT = [3, 1, 5, 2]
        # GT list has one element less than self.TEST_DATA
        mae.load(GT, self.TEST_DATA)
        assert_raises(ValueError, mae.compute)

    # test_PRED RMSE
    def test_PRED_RMSE_compute_one(self):
        #Even though rmse has data, we only compute these two param values
        assert_equal(self.rmse.compute(self.R, self.R_PRED), 0.9)

    def test_PRED_RMSE_compute_one_empty_datasets(self):
        rmse = RMSE()
        assert_equal(rmse.compute(self.R, self.R_PRED), 0.9)

    def test_PRED_RMSE_compute_all(self):
        assert_equal(self.rmse.compute(), 0.891067)

    def test_PRED_RMSE_load_test(self):
        rmse = RMSE()
        self.TEST_DATA = [2.3, 0.9, 4.9, 0.9, 1.5]
        rmse.load_test(self.TEST_DATA)
        assert_equal(len(rmse.get_test()), len(self.TEST_DATA))

    def test_PRED_RMSE_add_entry(self):
        self.rmse.add(1,4)
        assert_equal(len(self.rmse.get_test()), len(self.DATA_PRED)+1)
        assert_equal(self.rmse.compute(), 1.470261)

    def test_PRED_RMSE_different_list_sizes(self):
        rmse = RMSE()
        GT = [3, 1, 5, 2]
        # GT list has one element less than self.TEST_DATA
        rmse.load(GT, self.TEST_DATA)
        assert_raises(ValueError, rmse.compute)

    def test_PRED_RMSE_numpy_array(self):
        rmse = RMSE()
        rmse.load(array(self.GT_DATA), array(self.TEST_DATA))
        assert(rmse.compute(), 0.891067)

# TEST_DECISION P/R/F1
class TestDecision(Test):
    def __init__(self):
        super(TestDecision, self).__init__()
        # Decision-based metrics: PrecisionRecallF1
        self.decision = PrecisionRecallF1()
        self.decision.load(self.GT_DECISION, self.TEST_DECISION)

    def test_decision_PRF1_compute_all(self):
        assert_equal(self.decision.compute(), (0.75, 0.75, 0.75)) #P, R, F1
        assert_equal(self.decision.compute(), (0.75, 0.75, 0.75))

    def test_decision_PRF1_empty(self):
        decision = PrecisionRecallF1()
        assert_raises(ValueError, decision.compute)

    def test_decision_PRF1_load_test(self):
        decision = PrecisionRecallF1()
        decision.load_test(self.TEST_DECISION)
        assert_equal(len(decision.get_test()), len(self.TEST_DECISION))

    def test_decision_PRF1_load_ground_truth(self):
        decision = PrecisionRecallF1()
        decision.load_ground_truth(self.GT_DECISION)
        assert_equal(len(decision.get_ground_truth()), len(self.GT_DECISION))

    def test_decision_PRF1_load_test_and_ground_truth(self):
        decision = PrecisionRecallF1()
        decision.load_test(self.TEST_DECISION)
        assert_equal(len(decision.get_test()), len(self.TEST_DECISION))
        decision.load_ground_truth(self.GT_DECISION)
        assert_equal(len(decision.get_ground_truth()), len(self.GT_DECISION))
        P, R, F1 = decision.compute()
        assert_equal(P, 0.75)
        assert_equal(R, 0.75)
        assert_equal(F1, 0.75)

    def test_decision_PRF1_add_entry(self):
        self.decision.add_predicted_value('guitar') #add_predicted_entry == add_test_entry
        assert_equal(len(self.decision.get_test()), len(self.TEST_DECISION)+1)
        assert_equal(len(self.decision.get_ground_truth()), len(self.GT_DECISION))
        P, R, F1 = self.decision.compute()
        assert_equal(P, 0.6)
        assert_equal(R, 0.75)
        assert_equal(F1, 0.666667)


# TEST_CORR Pearson
class TestCorrelation(Test):
    def __init__(self):
        super(TestCorrelation, self).__init__()
        self.pearson = Pearson(self.DATA_PRED)

    def test_CORR_Pearson_compute_all(self):
        assert_equal(self.pearson.compute(), 0.930024)

    def test_CORR_Pearson_load_test(self):
        pearson = Pearson()
        pearson.load_test(self.TEST_DATA)
        assert_equal(len(pearson.get_test()), len(self.TEST_DATA))

    def test_CORR_Pearson_load_ground_truth(self):
        pearson = Pearson()
        pearson.load_ground_truth(self.GT_DATA)
        assert_equal(len(pearson.get_ground_truth()), len(self.GT_DATA))

    def test_CORR_Pearson_add_entry(self):
        self.pearson.add(1, 4) #1: Real rating, 4: Predicted rating
        assert_equal(len(self.pearson.get_test()), len(self.DATA_PRED)+1)
        assert_equal(len(self.pearson.get_ground_truth()), len(self.DATA_PRED)+1)
        assert_equal(self.pearson.compute(), 0.498172)


class TestRanking(Test):
    def __init__(self):
        super(TestRanking, self).__init__()
        # Rank-based metrics:  KendallTau, SpearmanRho, MeanReciprocalRank, ReciprocalRank
        self.kendall = KendallTau()
        self.kendall.load(self.GT_RANKING, self.TEST_RANKING)
        self.spearman = SpearmanRho()
        self.spearman.load(self.GT_RANKING, self.TEST_RANKING)
        self.mrr = MeanReciprocalRank()

        for elem in self.TEST_DECISION:
            self.mrr.load(self.GT_DECISION, elem)

    # TEST_CORR Spearman
    def test_RANK_Spearman_compute_all(self):
        assert_equal(self.spearman.compute(), 0.5) #0.55 ?

    #def test_RANK_Spearman_compute_tied_ranks():
    #    assert_equal(spearman.compute(tied_ranks=True), 0.5) #In fact, it uses Pearsonr corr. of the ranks

    def test_RANK_Spearman_compute_floats(self):
        spearman = SpearmanRho(self.DATA_PRED)
        assert_equal(spearman.compute(), 0.947368) #0.95 ?

    #def test_RANK_Spearman_compute_floats_tied_ranks():
    #    spearman = SpearmanRho(self.DATA_PRED)
    #    assert_equal(spearman.compute(tied_ranks=True), 0.930024) #In fact, it uses Pearsonr corr. of the ranks

    def test_RANK_Spearman_load_test(self):
        spearman = SpearmanRho()
        spearman.load_test(self.TEST_DATA)
        assert_equal(len(spearman.get_test()), len(self.TEST_DATA))

    def test_RANK_Spearman_load_ground_truth(self):
        spearman = SpearmanRho()
        spearman.load_ground_truth(self.GT_DATA)
        assert_equal(len(spearman.get_ground_truth()), len(self.TEST_DATA))

    def test_RANK_Spearman_add_entry(self):
        self.spearman.add(('guitar', 4), ('guitar', 4)) #add tag 'guitar' at rank-4
        assert_equal(len(self.spearman.get_test()), len(self.TEST_RANKING)+1)
        assert_equal(len(self.spearman.get_ground_truth()), len(self.GT_RANKING)+1)
        assert_equal(self.spearman.compute(), 0.763158) #0.775 ?

    def test_RANK_Spearman_different_list_sizes(self):
        TEST_DATA = ['classical', 'invented', 'baroque']
        GT_DATA = ['classical', 'instrumental', 'piano', 'baroque']
        spearman = SpearmanRho()
        spearman.load_ground_truth(GT_DATA)
        spearman.load_test(TEST_DATA)
        assert_raises(ValueError, spearman.compute) #Raise: GT & TEST list have different sizes

    # TEST_CORR Kendall
    def test_RANK_Kendall_compute_all(self):
        assert_equal(self.kendall.compute(), 0.4)

    def test_RANK_Kendall_compute_floats(self):
        kendall = KendallTau(self.DATA_PRED)
        assert_equal(kendall.compute(), 0.888889)

    def test_RANK_Kendall_load_test(self):
        kendall = KendallTau()
        kendall.load_test(self.TEST_DATA)
        assert_equal(len(kendall.get_test()), len(self.TEST_DATA))

    def test_RANK_Kendall_load_ground_truth(self):
        kendall = KendallTau()
        kendall.load_ground_truth(self.GT_DATA)
        assert_equal(len(kendall.get_ground_truth()), len(self.GT_DATA))

    def test_RANK_Kendall_add_entry(self):
        self.kendall.add(('guitar', 4.0), ('guitar', 4.0)) #add tag 'guitar'
        assert_equal(len(self.kendall.get_test()), len(self.TEST_RANKING)+1)
        assert_equal(len(self.kendall.get_ground_truth()), len(self.GT_RANKING)+1)
        assert_equal(self.kendall.compute(), 0.666667)

    def test_RANK_Kendall_diff_elems(self):
        TEST_DECISION = ['class', 'invented', 'baro', 'instru']
        GT_DECISION = ['classical', 'instrumental', 'piano', 'baroque']
        kendall = KendallTau()
        kendall.load_ground_truth(self.GT_DECISION)
        kendall.load_test(self.TEST_DECISION)
        assert_raises(ValueError, kendall.compute) #Different elements

    # TEST_RANK ReciprocalRank
    def test_RANK_ReciprocalRank_compute(self):
        rr = ReciprocalRank()
        QUERY = 'instrumental'
        assert_equal(rr.compute(self.GT_DECISION, QUERY), 0.5)

    def test_RANK_ReciprocalRank_add_entry(self):
        rr= ReciprocalRank()
        QUERY = 'invented'
        rr.load(self.GT_DECISION, QUERY)
        assert_equal(rr.compute(), 0.0)

    # TEST_RANK MeanReciprocalRank
    # Internally, MeanReciprocalRank uses a list of ReciprocalRank results
    def test_RANK_MeanReciprocalRank_compute_all(self):
        assert_equal(self.mrr.compute(), 0.4375)

    def test_RANK_MeanReciprocalRank_compute_one(self):
        mrr  = MeanReciprocalRank()
        QUERY = 'instrumental'
        assert_equal(mrr.compute(self.GT_DECISION, QUERY), 0.5)

    def test_RANK_MeanReciprocalRank_load(self):
        mrr  = MeanReciprocalRank()
        assert_raises(ValueError, mrr.load, self.GT_DECISION, self.TEST_RANKING)

    def test_RANK_MeanReciprocalRank_load_test(self):
        mrr  = MeanReciprocalRank()
        assert_raises(NotImplementedError, mrr.load_test, self.TEST_RANKING)

    def test_RANK_MeanReciprocalRank_load_ground_truth(self):
        mrr  = MeanReciprocalRank()
        assert_raises(NotImplementedError, mrr.load_ground_truth, self.GT_RANKING)

    def test_RANK_MeanReciprocalRank_add_entry(self):
        mrr  = MeanReciprocalRank()
        QUERY = 'invented'
        mrr.load(self.GT_DECISION, QUERY)
        assert_equal(mrr.compute(), 0.0)

    #mAP tests
    def test_RANK_AveragePrecision(self):
        GT_DECISION = [1, 2, 4]
        TEST_DECISION = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
        avgp = AveragePrecision()
        avgp.load(GT_DECISION, TEST_DECISION)
        assert_equal(round(avgp.compute(), 4), 0.9167)

        GT_DECISION = [1, 4, 8]
        avgp = AveragePrecision()
        avgp.load(GT_DECISION, TEST_DECISION)
        assert_equal(round(avgp.compute(), 4), 0.625)

        GT_DECISION = [3, 5, 9, 25, 39, 44, 56, 71, 89, 123]
        TEST_DECISION = [123, 84, 56, 6, 8, 9, 511, 129, 187, 25, 38, 48, 250, 113, 3]
        avgp = AveragePrecision()
        avgp.load(GT_DECISION, TEST_DECISION)
        assert_equal(avgp.compute(), 0.58)

    #mAP tests
    def test_RANK_MeanAveragePrecision(self):
        mavgp = MeanAveragePrecision()
        GT_DECISION = [1, 2, 4]
        TEST_DECISION = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
        mavgp.load(GT_DECISION, TEST_DECISION)

        GT_DECISION = [1, 4, 8]
        mavgp.load(GT_DECISION, TEST_DECISION)

        GT_DECISION = [3, 5, 9, 25, 39, 44, 56, 71, 89, 123]
        TEST_DECISION = [123, 84, 56, 6, 8, 9, 511, 129, 187, 25, 38, 48, 250, 113, 3]
        mavgp.load(GT_DECISION, TEST_DECISION)

        assert_equal(mavgp.compute(), 0.707222)


########NEW FILE########
__FILENAME__ = test_matrix
from nose import with_setup
from nose.tools import assert_equal, assert_not_equal, assert_raises, assert_true

from recsys.algorithm.matrix import SparseMatrix as OurSparseMatrix
from divisi2 import make_sparse, SparseMatrix

def setup():
    global matrix, matrix_empty, data

    data = [
	  (4.0, "user1", "item1"),
	  (2.0, "user1", "item3"),
	  (1.0, "user2", "item1"),
	  (5.0, "user2", "item4")]
    matrix = OurSparseMatrix()
    matrix.create(data)

    matrix_empty = OurSparseMatrix()
    matrix_empty.create([])

#def teardown():
#    pass

def test_get_matrix():
    m = matrix.get()
    assert_true(isinstance(m, SparseMatrix))

def test_equal():
    m = make_sparse(data)
    assert_equal(matrix.get(), m)

def test_set_matrix():
    m = make_sparse(data)
    mm = OurSparseMatrix()
    mm.set(m)
    assert_equal(matrix.get(), mm.get())

def test_get_row_len():
    assert_equal(matrix.get_row_len(), 3)

def test_get_col_len():
    assert_equal(matrix.get_col_len(), 2)

def test_get_density():
    assert_equal(matrix.density(), 66.6667)

def test_get_value():
    assert_equal(matrix.value("user1", "item1"), 4.0)

def test_set_value():
    matrix.set_value("user1", "item1", -1.0)
    assert_equal(matrix.value("user1", "item1"), -1.0)

def test_get_value_synonym():
    assert_equal(matrix.value("user1", "item1"), matrix.get_value("user1", "item1"))

# Empty matrix tests
def test_empty_matrix():
    assert_true(matrix_empty.empty())

def test_empty_matrix_equals():
    data = []
    m = OurSparseMatrix()
    m.create(data)
    assert_equal(m.get(), matrix_empty.get())

def test_empty_matrix_get():
    assert_true(isinstance(matrix_empty.get(), SparseMatrix))

def test_empty_matrix_get_value():
    assert_raises(ValueError, matrix_empty.get_value, 1, 1)

def test_empty_matrix_set_value():
    assert_raises(ValueError, matrix_empty.set_value, 1, 1, 5.0)

def test_empty_matrix_get_row():
    assert_raises(ValueError, matrix_empty.get_row, 1)

def test_empty_matrix_get_col():
    assert_raises(ValueError, matrix_empty.get_col, 1)

def test_empty_matrix_get_row_len():
    assert_raises(ValueError, matrix_empty.get_row_len)

def test_empty_matrix_get_col_len():
    assert_raises(ValueError, matrix_empty.get_col_len)
 

########NEW FILE########
__FILENAME__ = test_utils
import os
from nose import with_setup
from nose.tools import assert_equal, assert_not_equal, assert_raises, assert_true
from operator import itemgetter

import recsys.algorithm
recsys.algorithm.VERBOSE = True

from recsys.utils.svdlibc import SVDLIBC

from tests import MOVIELENS_DATA_PATH


def setup():
    global svdlibc
    svdlibc = SVDLIBC(datafile=os.path.join(MOVIELENS_DATA_PATH, 'ratings.dat'))

def teardown():
    svdlibc.remove_files()

def test_to_sparse_matrix():
    svdlibc.to_sparse_matrix(sep='::', format={'col':0, 'row':1, 'value':2, 'ids': int})

def test_compute_svd():
    svdlibc.compute()

def test_export():
    global svd, MOVIEID
    svd = svdlibc.export()
    MOVIEID = 1

def test_similar():
    similars = svd.similar(MOVIEID)
    assert_true(len(similars) == 10)
    assert_true(588 in map(itemgetter(0), similars))

def test_similarity():
    MOVIEID2 = 3114
    assert_equal(round(svd.similarity(MOVIEID, MOVIEID2), 4), round(0.84099896392054219, 4))

########NEW FILE########
