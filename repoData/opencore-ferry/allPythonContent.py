__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

# -- General configuration -----------------------------------------------------



# Additional templates that should be rendered to pages, maps page names to
# template names.

html_additional_pages = {
    }

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinxcontrib.httpdomain', 'sphinx.ext.extlinks']

# Configure extlinks
extlinks = { }

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

html_add_permalinks = u'Â¶'

# The master toctree document.
master_doc = 'toctree'

# General information about the project.
project = u'Ferry'
copyright = u'2014 OpenCore LLC'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '0.1'
# The full version, including alpha/beta/rc tags.
release = '0'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'ferry'
# html_theme = 'nature'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
html_theme_path = ['../theme']

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.

# We use a png favicon. This is not compatible with internet explorer, but looks
# much better on all other browsers. However, sphynx doesn't like it (it likes
# .ico better) so we have just put it in the template rather than used this setting
# html_favicon = 'favicon.png'

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
# html_static_path = ['static_files']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
html_show_sourcelink = False

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'Ferrydoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'

########NEW FILE########
__FILENAME__ = cli
# Copyright 2014 OpenCore LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import errno
import ferry
import grp
import json
import logging
import logging.config
import os
import pwd
import requests
import re
import shutil
import StringIO
import sys
from termcolor import colored
import yaml
from requests.exceptions import ConnectionError
from ferry.table.prettytable import *
from ferry.options import CmdHelp
from ferry.install import Installer, FERRY_HOME, GUEST_DOCKER_REPO

class CLI(object):
    def __init__(self):
        self.cmds = CmdHelp()
        self.cmds.description = "Development environment for big data applications"
        self.cmds.version = ferry.__version__
        self.cmds.usage = "ferry COMMAND [arg...]"
        self.cmds.add_option("-c", "--conf", "Deployment configuration")
        self.cmds.add_option("-n", "--dns", "Use custom DNS")
        self.cmds.add_option("-l", "--log", "Log configuration file")
        self.cmds.add_option("-k", "--key", "Specify key directory")
        self.cmds.add_option("-m", "--mode", "Deployment mode")
        self.cmds.add_option("-u", "--upgrade", "Upgrade Ferry")
        self.cmds.add_option("-b", "--build", "Build Ferry images")
        self.cmds.add_cmd("clean", "Clean zombie Ferry processes")
        self.cmds.add_cmd("server", "Start all the servers")
        self.cmds.add_cmd("deploy", "Deploy a service to the cloud")
        self.cmds.add_cmd("help", "Print this help message")
        self.cmds.add_cmd("info", "Print version information")
        self.cmds.add_cmd("inspect", "Return low-level information on a service")
        self.cmds.add_cmd("install", "Install the Ferry images")
        self.cmds.add_cmd("logs", "Copy over the logs to the host")
        self.cmds.add_cmd("ps", "List deployed and running services")
        self.cmds.add_cmd("rm", "Remove a service or snapshot")
        self.cmds.add_cmd("snapshot", "Take a snapshot")
        self.cmds.add_cmd("snapshots", "List all snapshots")
        self.cmds.add_cmd("ssh", "Connect to a client/connector")
        self.cmds.add_cmd("start", "Start a new service or snapshot")
        self.cmds.add_cmd("stop", "Stop a running service")
        self.cmds.add_cmd("quit", "Stop the Ferry servers")

        self.ferry_server = 'http://127.0.0.1:4000'
        self.default_user = 'root'
        self.installer = Installer()

    """
    Create a new stack. 
    """
    def _create_stack(self, stack_description, args):
        mode = self._parse_deploy_arg('mode', args, default='local')
        conf = self._parse_deploy_arg('conf', args, default='default')
        payload = { 'payload' : json.dumps(stack_description),
                    'mode' : mode, 
                    'conf' : conf }
        try:
            res = requests.post(self.ferry_server + '/create', data=payload)
            return True, str(res.text)
        except ConnectionError:
            logging.error("could not connect to ferry server")
            return False, "It appears Ferry servers are not running.\nType sudo ferry server and try again."

    def _resubmit_create(self, reply, stack_description, args):
        values = {}
        for q in reply['questions']:
            question = colored(q['query']['text'], 'red')
            prompt = colored(' >> ', 'green')
            v = raw_input(question + prompt)
            values[q['query']['id']] = v
        stack_description['iparams'] = values
        posted, reply = self._create_stack(stack_description, args)
        if posted:
            try:
                reply = json.loads(reply)
                return reply['text']
            except ValueError as e:
                logging.error(reply)

    def _format_snapshots_query(self, json_data):
        bases = []
        date = []
        for uuid in json_data.keys():
            bases.append(json_data[uuid]['base'])
            if 'snapshot_ts' in json_data[uuid]:
                date.append(json_data[uuid]['snapshot_ts'])
            else:
                date.append(' ')

        t = PrettyTable()
        t.add_column("UUID", json_data.keys())
        t.add_column("Base", bases)
        t.add_column("Date", date)
        return t.get_string(sortby="Date",
                            padding_width=2)

    def _format_table_query(self, json_data):
        storage = []
        compute = []
        connectors = []
        status = []
        base = []
        time = []

        # Each additional row should include the actual data.
        for uuid in json_data.keys():
            for c in json_data[uuid]['connectors']:
                connectors.append(c)

            backends = json_data[uuid]['backends']
            for b in backends:
                if b['storage']:
                    storage.append(b['storage'])
                else:
                    storage.append(' ')

                if b['compute']:
                    compute.append(b['compute'])
                else:
                    compute.append(' ')

            status.append(json_data[uuid]['status'])
            base.append(json_data[uuid]['base'])
            time.append(json_data[uuid]['ts'])

        t = PrettyTable()
        t.add_column("UUID", json_data.keys())
        t.add_column("Storage", storage)
        t.add_column("Compute", compute)
        t.add_column("Connectors", connectors)
        t.add_column("Status", status)
        t.add_column("Base", base)
        t.add_column("Time", time)

        return t.get_string(sortby="UUID",
                            padding_width=2)

    def _stop_all(self):
        try:
            constraints = { 'status' : 'running' }
            payload = { 'constraints' : json.dumps(constraints) }
            res = requests.get(self.ferry_server + '/query', params=payload)
            stacks = json.loads(res.text)
            for uuid in stacks.keys():
                self._manage_stacks({'uuid' : uuid,
                                     'action' : 'stop'})
        except ConnectionError:
            logging.error("could not connect to ferry server")
        
    def _read_stacks(self, show_all=False, args=None):
        try:
            res = requests.get(self.ferry_server + '/query')
            query_reply = json.loads(res.text)

            deployed_reply = {}
            if show_all:
                mode = self._parse_deploy_arg('mode', args, default='local')
                conf = self._parse_deploy_arg('conf', args, default='default')
                payload = { 'mode' : mode,
                            'conf' : conf }

                res = requests.get(self.ferry_server + '/deployed', params=payload)
                deployed_reply = json.loads(res.text)

            # Merge the replies and format.
            return self._format_table_query(dict(query_reply.items() + deployed_reply.items()))
        except ConnectionError:
            logging.error("could not connect to ferry server")
            return "It appears Ferry servers are not running.\nType sudo ferry server and try again."

    def _list_snapshots(self):
        try:
            res = requests.get(self.ferry_server + '/snapshots')
            json_reply = json.loads(res.text)
            return self._format_snapshots_query(json_reply)
        except ConnectionError:
            logging.error("could not connect to ferry server")
            return "It appears Ferry servers are not running.\nType sudo ferry server and try again."

    def _format_stack_inspect(self, json_data):
        return json.dumps(json_data, 
                          sort_keys=True,
                          indent=2,
                          separators=(',',':'))
    """
    Inspect a specific stack. 
    """
    def _inspect_stack(self, stack_id):
        payload = { 'uuid':stack_id }
        try:
            res = requests.get(self.ferry_server + '/stack', params=payload)
            json_value = json.loads(str(res.text))
            return self._format_stack_inspect(json_value)
        except ConnectionError:
            logging.error("could not connect to ferry server")
            return "It appears Ferry servers are not running.\nType sudo ferry server and try again."
    """
    Copy over the logs. 
    """
    def _copy_logs(self, stack_id, to_dir):
        payload = {'uuid':stack_id,
                   'dir':to_dir}
        try:
            res = requests.get(self.ferry_server + '/logs', params=payload)
            json_value = json.loads(str(res.text))
            return self._format_stack_inspect(json_value)
        except ConnectionError:
            logging.error("could not connect to ferry server")
            return "It appears Ferry servers are not running.\nType sudo ferry server and try again."

    """
    Connector a specific client/connector via ssh. 
    """
    def _connect_stack(self, stack_id, connector_id):
        # Get the IP and default user information for this connector.
        payload = {'uuid':stack_id}
        try:
            res = requests.get(self.ferry_server + '/stack', params=payload)
            json_value = json.loads(str(res.text))
        except ConnectionError:
            logging.error("could not connect to ferry server")
            return "It appears Ferry servers are not running.\nType sudo ferry server and try again."

        connector_ip = None
        for cg in json_value['connectors']:
            if not connector_id:
                connector_ip = cg['entry']['ip']
                break
            elif connector_id == cg['uniq']:
                connector_ip = cg['entry']['ip']
                break
            else:
                logging.warning("no match: %s %s" % (connector_id, cg['uniq']))

        # Now form the ssh command. This just executes in the same shell. 
        if connector_ip:
            keydir, _ = self._read_key_dir()
            key_opt = '-o StrictHostKeyChecking=no'
            host_opt = '-o UserKnownHostsFile=/dev/null'
            ident = '-i %s/id_rsa' % keydir
            dest = '%s@%s' % (self.default_user, connector_ip)
            cmd = "ssh %s %s %s %s" % (key_opt, host_opt, ident, dest)
            logging.warning(cmd)
            os.execv('/usr/bin/ssh', cmd.split())

    def _parse_deploy_arg(self, param, args, default):
        pattern = re.compile('--%s=(\w+)' % param)
        for a in args:
            m = pattern.match(a)
            if m and m.group(0) != '':
                return m.group(1)
        return default

    """
    Deploy the stack. 
    """        
    def _deploy_stack(self, stack_id, args):
        mode = self._parse_deploy_arg('mode', args, default='local')
        conf = self._parse_deploy_arg('conf', args, default='default')

        payload = { 'uuid' : stack_id,
                    'mode' : mode,
                    'conf' : conf }
        try:
            res = requests.post(self.ferry_server + '/deploy', data=payload)
            return res.text
        except ConnectionError:
            logging.error("could not connect to ferry server")
            return "It appears Ferry servers are not running.\nType sudo ferry server and try again."

    """
    Manage the stack. 
    """
    def _manage_stacks(self, stack_info):
        try:
            res = requests.post(self.ferry_server + '/manage/stack', data=stack_info)
            return str(res.text)        
        except ConnectionError:
            logging.error("could not connect to ferry server")
            return "It appears Ferry servers are not running.\nType sudo ferry server and try again."
        
    """
    Output the help message.
    """
    def _print_help(self):
        return self.cmds.print_help()

    """
    Output version information.
    """
    def _print_info(self):
        try:
            res = requests.get(self.ferry_server + '/version')
            s = self.cmds.description + '\n'
            s += "Version: %s\n" % self.cmds.version
            s += "Docker: %s\n" % res.text.strip()

            return s
        except ConnectionError:
            logging.error("could not connect to ferry server")
            return "It appears Ferry servers are not running.\nType sudo ferry server and try again."
        
    """
    Helper method to read a file.
    """
    def _read_file_arg(self, file_name):
        json_file = open(os.path.abspath(file_name), 'r')
        json_text = ''

        for line in json_file:
            json_text += line.strip()

        return json_text

    """
    Read the location of the directory containing the keys
    used to communicate with the containers. 
    """
    def _read_key_dir(self):
        f = open(ferry.install.DEFAULT_DOCKER_KEY, 'r')
        k = f.read().strip().split("://")
        return k[1], k[0]

    def _using_tmp_ssh_key(self):
        keydir, tmp = self._read_key_dir()
        return tmp == "tmp"

    def _check_ssh_key(self):
        keydir, _ = self._read_key_dir()
        if keydir == ferry.install.DEFAULT_KEY_DIR:
            keydir = os.environ['HOME'] + '/.ssh/tmp-ferry'
            ferry.install.GLOBAL_KEY_DIR = 'tmp://' + keydir
            try:
                os.makedirs(keydir)
            except OSError as e:
                if e.errno != errno.EEXIST:
                    logging.error("Could not create ssh directory %s" % keydir)
                    exit(1)
            try:
                shutil.copy(ferry.install.DEFAULT_KEY_DIR + '/id_rsa', keydir + '/id_rsa')
                shutil.copy(ferry.install.DEFAULT_KEY_DIR + '/id_rsa.pub', keydir + '/id_rsa.pub')
                            
                uid = pwd.getpwnam(os.environ['USER']).pw_uid
                gid = grp.getgrnam(os.environ['USER']).gr_gid
                os.chown(keydir + '/id_rsa', uid, gid)
                os.chmod(keydir + '/id_rsa', 0400)
                os.chown(keydir + '/id_rsa.pub', uid, gid)
                os.chmod(keydir + '/id_rsa.pub', 0444)
            except OSError as e:
                logging.error("Could not copy ssh keys (%s)" % str(e))
                exit(1)
            except IOError as e:
                if e.errno == errno.EACCES:
                    logging.error("Could not override keys in %s, please delete those keys and try again." % keydir)
                exit(1)

            ferry.install._touch_file(ferry.install.DEFAULT_DOCKER_KEY, 
                                      ferry.install.GLOBAL_KEY_DIR)
            logging.warning("Copied ssh keys to " + ferry.install.GLOBAL_KEY_DIR)

    """
    This is the command dispatch table. 
    """
    def dispatch_cmd(self, cmd, args, options):
        if(cmd == 'start'):
            self._check_ssh_key()

            if '-b' in options:
                build_dir = options['-b'][0]
                dockerfile = build_dir + '/Dockerfile'
                names = self.installer._get_image(dockerfile)
                name = names.pop().split("/")
                if len(name) == 1:
                    repo = GUEST_DOCKER_REPO
                    image = name[0]
                else:
                    repo = name[0]
                    image = name[1]
                    
                self.installer._compile_image(image, repo, build_dir, build=True)
                if len(names) > 0:
                    self.installer._tag_images(image, repo, names)

            arg = args.pop(0)
            json_arg = {}
            if not os.path.exists(arg):
                # Check if the user wants to use one of the global plans.
                global_path = FERRY_HOME + '/data/plans/' + arg

                # If the user has not supplied a file extension, look for the
                # file with a YAML extension
                file_path = global_path
                n, e = os.path.splitext(global_path)
                if e == '':
                    file_path += '.yaml'
            else:
                file_path = arg

            if os.path.exists(file_path):
                file_path = os.path.abspath(file_path)
                n, e = os.path.splitext(file_path)
                if e == '.json':
                    json_string = self._read_file_arg(file_path)
                    json_arg = json.loads(json_string)
                elif e == '.yaml' or e == '.yml':
                    yaml_file = open(file_path, 'r')
                    json_arg = yaml.load(yaml_file)
                else:
                    # This is an unknown file type. We're going to try to
                    # pretend it is a JSON file and see if we fail.
                    try:
                        json_string = self._read_file_arg(file_path)
                        json_arg = json.loads(json_string)
                    except ValueError as e:
                        logging.error("could not load file " + file_path)
                        exit(1)

                json_arg['_file_path'] = file_path
            json_arg['_file'] = arg
            posted, reply = self._create_stack(json_arg, args)
            if posted:
                try:
                    reply = json.loads(reply)
                    if reply['status'] == 'query':
                        return self._resubmit_create(reply, json_arg, args)
                    elif reply['status'] == 'failed':
                        return 'could not create application'
                    else:
                        return reply['text']
                except ValueError as e:
                    logging.error(reply)
        elif(cmd == 'ps'):
            if len(args) > 0 and args[0] == '-a':
                opt = args.pop(0)
                return self._read_stacks(show_all=True, args = args)
            else:
                return self._read_stacks(show_all=False, args = args)
        elif(cmd == 'snapshots'):
            return self._list_snapshots()
        elif(cmd == 'install'):
            msg = self.installer.install(args, options)
            self.installer._stop_docker_daemon()
            return msg
        elif(cmd == 'clean'):
            self._check_ssh_key()
            self.installer.start_web()
            self.installer._clean_rules()
            self.installer.stop_web()
            self.installer._stop_docker_daemon(force=True)
            self.installer._reset_ssh_key()
            return 'cleaned ferry'
        elif(cmd == 'inspect'):
            return self._inspect_stack(args[0])
        elif(cmd == 'logs'):
            return self._copy_logs(args[0], args[1])
        elif(cmd == 'server'):
            self.installer.start_web(options)
            return 'started ferry'
        elif(cmd == 'ssh'):
            self._check_ssh_key()
            stack_id = args[0]
            connector_id = None
            if len(args) > 1:
                connector_id = args[1]

            return self._connect_stack(stack_id, connector_id)
        elif(cmd == 'quit'):
            self._check_ssh_key()
            self._stop_all()
            self.installer.stop_web()
            self.installer._stop_docker_daemon()

            if self._using_tmp_ssh_key():
                self.installer._reset_ssh_key()
            return 'stopped ferry'
        elif(cmd == 'deploy'):
            self._check_ssh_key()
            stack_id = args.pop(0)
            return self._deploy_stack(stack_id, args)
        elif(cmd == 'info'):
            return self._print_info()
        elif(cmd == 'help'):
            return self._print_help()
        else:
            stack_info = {'uuid' : args[0],
                          'action' : cmd}
            return self._manage_stacks(stack_info)
    
def main(argv=None):
    # Set up the various logging facilities 
    logging.config.fileConfig(FERRY_HOME + "/logging.conf")

    cli = CLI()
    if(sys.argv):
        if len(sys.argv) > 1:
            cli.cmds.parse_args(sys.argv)

            # Initialize the cli
            options = cli.cmds.get_options()
            if '-l' in options:
                logging.config.fileConfig(options['-l'][0])

            # Execute the commands
            all_cmds = cli.cmds.get_cmds()
            if len(all_cmds) > 0:
                for c in all_cmds.keys():
                    msg = cli.dispatch_cmd(c, all_cmds[c], options)
                    print msg
                    exit(0)
    print cli._print_help()

########NEW FILE########
__FILENAME__ = cassandraclientconfig
# Copyright 2014 OpenCore LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import sys
import sh
from string import Template
from ferry.docker.fabric import DockerFabric

class CassandraClientInitializer(object):
    """
    Create a new initializer
    Param user The user login for the git repo
    """
    def __init__(self):
        self.template_dir = None
        self.template_repo = None

        self.container_data_dir = CassandraClientConfig.data_directory
        self.container_log_dir = CassandraClientConfig.log_directory

    """
    Generate a new hostname
    """
    def new_host_name(self, instance_id):
        return 'cassandra_client' + str(instance_id)

    """
    Start the service on the containers. 
    """
    def _execute_service(self, containers, entry_point, fabric, cmd):
        output = fabric.cmd(containers, 
                            '/service/sbin/startnode %s %s' % (cmd, entry_point['storage_url']))
    def start_service(self, containers, entry_point, fabric):
        self._execute_service(containers, entry_point, fabric, "start")
    def restart_service(self, containers, entry_point, fabric):
        self._execute_service(containers, entry_point, fabric, "restart")
    def stop_service(self, containers, entry_point, fabric):
        self._execute_service(containers, entry_point, fabric, "stop")

    def _generate_config_dir(self, uuid):
        return 'cassandra_client' + str(uuid)

    """
    Get the ports necessary for Gluster. 
    """
    def get_necessary_ports(self, num_instances):
        return []

    """
    Get the internal ports. 
    """
    def get_exposed_ports(self, num_instances):
        return []

    """
    Get total number of instances.
    """
    def get_total_instances(self, num_instances, layers):
        instances = []

        for i in range(num_instances):
            instances.append('cassandra-client')

        return instances

    """
    Generate a new configuration
    """
    def generate(self, num):
        return CassandraClientConfig(num)

    def _apply_cassandra(self, host_dir, entry_point, config, container):
        yaml_in_file = open(self.template_dir + '/cassandra.yaml.template', 'r')
        yaml_out_file = open(host_dir + '/cassandra.yaml', 'w+')

        # Now make the changes to the template file. 
        changes = { "LOCAL_ADDRESS":container['data_ip'], 
                    "DATA_DIR":config.data_directory,
                    "CACHE_DIR":config.cache_directory,
                    "COMMIT_DIR":config.commit_directory,
                    "SEEDS":entry_point['storage_url']}

        for line in yaml_in_file:
            s = Template(line).substitute(changes)
            yaml_out_file.write(s)

        yaml_out_file.close()
        yaml_in_file.close()

    def _apply_titan(self, host_dir, storage_entry, container):
        in_file = open(self.template_dir + '/titan.properties', 'r')
        out_file = open(host_dir + '/titan.properties', 'w+')
        changes = { "BACKEND":"cassandrathrift", 
                    "DB":container['args']['db'],
                    "IP":storage_entry['ip']}
        for line in in_file:
            s = Template(line).substitute(changes)
            out_file.write(s)
        out_file.close()
        in_file.close()

    """
    Apply the configuration to the instances
    """
    def apply(self, config, containers):
        entry_point = { 'type' : 'cassandra-client' }
        entry_point['ip'] = containers[0]['data_ip']

        # Get the storage information. 
        storage_entry = containers[0]['storage'][0]
        if storage_entry['type'] != 'cassandra':
            # The Cassandra client is currently only compatible with a 
            # Cassandra backend. So just return an error.
            return None, None

        # Otherwise record the storage type and get the seed node. 
        entry_point['storage_type'] = storage_entry['type']
        seed = storage_entry['ip']
        entry_point['storage_url'] = seed

        # Create a new configuration directory, and place
        # into the template directory. 
        config_dirs = []

        try:
            host_dir = "/tmp/" + self._generate_config_dir(config.uuid)
            try:
                sh.mkdir('-p', host_dir)
            except:
                sys.stderr.write('could not create config dir ' + host_dir)

            self._apply_cassandra(host_dir, entry_point, config, containers[0])

            # See if we need to apply
            if 'titan' in storage_entry:
                self._apply_titan(host_dir, storage_entry, containers[0])
                out_file = open(host_dir + '/servers', 'w+')
                out_file.write("%s %s" % (storage_entry['titan']['ip'], 'rexserver'))
                out_file.close

            # The config dirs specifies what to transfer over. We want to 
            # transfer over specific files into a directory. 
            for c in containers:
                config_dirs.append([c['container'], 
                                    host_dir + '/*', 
                                    config.config_directory])
        except IOError as err:
            sys.stderr.write('' + str(err))

        return config_dirs, entry_point

class CassandraClientConfig(object):
    data_directory = '/service/data/main'
    log_directory = '/service/data/logs'
    commit_directory = '/service/data/commits'
    cache_directory = '/service/data/cache'
    config_directory = '/service/conf'

    def __init__(self, num):
        self.num = num
        self.data_directory = CassandraClientConfig.data_directory
        self.commit_directory = CassandraClientConfig.commit_directory
        self.cache_directory = CassandraClientConfig.cache_directory
        self.log_directory = CassandraClientConfig.log_directory
        self.config_directory = CassandraClientConfig.config_directory

########NEW FILE########
__FILENAME__ = cassandraconfig
# Copyright 2014 OpenCore LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import os
import sys
import sh
from string import Template
from ferry.install import FERRY_HOME
from ferry.docker.fabric import DockerFabric
from ferry.config.titan.titanconfig import *

class CassandraInitializer(object):
    """
    Create a new initializer
    Param user The user login for the git repo
    """
    def __init__(self):
        self.template_dir = None
        self.template_repo = None

        self.titan = TitanInitializer()
        self.titan.template_dir = FERRY_HOME + '/data/templates/titan'

        self.container_data_dir = CassandraConfig.data_directory
        self.container_log_dir = CassandraConfig.log_directory

    """
    Generate a new hostname
    """
    def new_host_name(self, instance_id):
        return 'cassandra' + str(instance_id)

    def _execute_service(self, containers, entry_point, fabric, cmd):
        for c in containers:
            if c.service_type == 'cassandra':
                output = fabric.cmd([c], '/service/sbin/startnode %s' % cmd)
            elif c.service_type == 'titan':
                self.titan._execute_service([c], entry_point, fabric, cmd)

    def start_service(self, containers, entry_point, fabric):
        self._execute_service(containers, entry_point, fabric, "start")
    def restart_service(self, containers, entry_point, fabric):
        self._execute_service(containers, entry_point, fabric, "restart")
    def stop_service(self, containers, entry_point, fabric):
        self._execute_service(containers, entry_point, fabric, "stop")

    def _generate_config_dir(self, uuid, container):
        return 'cassandra_' + str(uuid) + '_' + str(container['data_ip'])

    """
    Get the ports necessary. 
    """
    def get_necessary_ports(self, num_instances):
        return []

    """
    Get the internal ports. 
    """
    def get_exposed_ports(self, num_instances):
        ports = []
        
        ports.append(CassandraConfig.CLUSTER_COM_PORT)
        ports.append(CassandraConfig.CLUSTER_SSL_PORT)
        ports.append(CassandraConfig.THRIFT_COM_PORT)
        ports.append(CassandraConfig.NATIVE_COM_PORT)
        ports.append(CassandraConfig.JMX_PORT)

        return ports

    """
    Get total number of instances.
    """
    def get_total_instances(self, num_instances, layers):
        instances = []

        for i in range(num_instances):
            instances.append('cassandra')

        if len(layers) > 0 and layers[0] == "titan":
            instances.append('titan')

        return instances

    """
    Generate a new configuration
    """
    def generate(self, num):
        return CassandraConfig(num)

    """
    Generate Titan configuration. 
    """
    def _apply_titan(self, config, cass_entry, cass_containers):
        return self.titan.apply(config, cass_containers, cass_entry)

    def _generate_yaml_config(self, container, seed, host_dir, config):
        yaml_in_file = open(self.template_dir + '/cassandra.yaml.template', 'r')
        yaml_out_file = open(host_dir + '/cassandra.yaml', 'w+')

        changes = { "LOCAL_ADDRESS":container['data_ip'], 
                    "DATA_DIR":config.data_directory,
                    "CACHE_DIR":config.cache_directory,
                    "COMMIT_DIR":config.commit_directory,
                    "SEEDS":seed}

        for line in yaml_in_file:
            s = Template(line).substitute(changes)
            yaml_out_file.write(s)

        yaml_out_file.close()
        yaml_in_file.close()

    def _generate_log4j_config(self, host_dir, config):
        log4j_in_file = open(self.template_dir + '/log4j-server.properties', 'r')
        log4j_out_file = open(host_dir + '/log4j-server.properties', 'w+')

        changes = { "LOG_DIR":config.log_directory } 

        for line in log4j_in_file:
            s = Template(line).substitute(changes)
            log4j_out_file.write(s)

        log4j_out_file.close()
        log4j_in_file.close()

    """
    Apply the configuration to the instances
    """
    def apply(self, config, containers):
        # The "entry point" is the way to contact the storage service.
        entry_point = { 'type' : 'cassandra' }

        cass_containers = []
        titan_containers = []
        for c in containers:
            if c['type'] == 'cassandra':
                cass_containers.append(c)
            elif c['type'] == 'titan':
                titan_containers.append(c)

        # In Cassandra all nodes are equal, so just pick one
        # as the "entry" node
        entry_point['ip'] = str(containers[0]['data_ip'])

        # Generate list of the seed nodes. We don't need the entire
        # list of containers, just a few. 
        seed = ''
        for i in range(0, len(cass_containers)):
            if i < 6:
                if i != 0:
                    seed += ','
                seed += cass_containers[i]['data_ip']

        # Create a new configuration directory, and place
        # into the template directory. 
        config_dirs = []
        try:
            for c in cass_containers:
                host_dir = "/tmp/" + self._generate_config_dir(config.uuid, c)
                try:
                    sh.mkdir('-p', host_dir)
                except:
                    sys.stderr.write('could not create config dir ' + host_dir)

                # The config dirs specifies what to transfer over. We want to 
                # transfer over specific files into a directory. 
                config_dirs.append([c['container'], 
                                    host_dir + '/*', 
                                    config.config_directory])

                self._generate_yaml_config(c, seed, host_dir, config)
                self._generate_log4j_config(host_dir, config)
        except IOError as err:
            sys.stderr.write('' + str(err))

        if len(titan_containers) > 0:
            # We need to configure a Titan container. 
            titan_config = self.titan.generate(len(titan_containers))
            titan_config.uuid = config.uuid
            titan_dirs, titan_entry = self._apply_titan(titan_config, entry_point, titan_containers)
            config_dirs.extend(titan_dirs)
            entry_point['titan'] = titan_entry

        return config_dirs, entry_point

class CassandraConfig(object):
    data_directory = '/service/data/main'
    log_directory = '/service/logs'
    commit_directory = '/service/data/commits'
    cache_directory = '/service/data/cache'
    config_directory = '/service/conf'

    CLUSTER_COM_PORT = 7000
    CLUSTER_SSL_PORT = 7001
    THRIFT_COM_PORT  = 9160
    NATIVE_COM_PORT  = 9042
    JMX_PORT         = 7199

    def __init__(self, num):
        self.num = num
        self.data_directory = CassandraConfig.data_directory
        self.commit_directory = CassandraConfig.commit_directory
        self.cache_directory = CassandraConfig.cache_directory
        self.log_directory = CassandraConfig.log_directory
        self.config_directory = CassandraConfig.config_directory

########NEW FILE########
__FILENAME__ = glusterconfig
# Copyright 2014 OpenCore LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import sh
import os
import stat
import logging
from string import Template
from ferry.docker.fabric import DockerFabric

"""
Create Gluster configurations and apply them to a set of instances
"""
class GlusterInitializer(object):
    """
    Create a new initializer
    Param user The user login for the git repo
    """
    def __init__(self):
        self.template_dir = None
        self.template_repo = None

        self.MOUNT_VOLUME = '/gv0'
        self.MOUNT_ROOT = '/service'
        self.name = "GLUSTER"

        self.container_data_dir = GlusterConfig.data_directory 
        self.container_log_dir = GlusterConfig.log_directory 

    """
    Generate a new hostname
    """
    def new_host_name(self, instance_id):
        return 'gluster' + str(instance_id)

    def _execute_service(self, containers, entry_point, fabric, cmd):
        """
        Start the gluster service on the containers. We want to start/stop the
        slave nodes before the master, since the master assumes everything is
        waiting for it to start. 
        """
        master_ip = entry_point['ip']
        for c in containers:
            if c.internal_ip != master_ip:
                output = fabric.cmd([c], '/service/sbin/startnode %s slave' % cmd)
        for c in containers:
            if c.internal_ip == master_ip:
                output = fabric.cmd([c], '/service/sbin/startnode %s master' % cmd)
    def start_service(self, containers, entry_point, fabric):
        self._execute_service(containers, entry_point, fabric, "start")
    def restart_service(self, containers, entry_point, fabric):
        self._execute_service(containers, entry_point, fabric, "restart")
    def stop_service(self, containers, entry_point, fabric):
        self._execute_service(containers, entry_point, fabric, "stop")

    """
    Generate a new Gluster configuration repo for a new
    Gluster filesystem instantiation. 
    """
    def generate_config_dir(self, uuid):
        return 'gluster_' + str(uuid)

    """
    Get the ports necessary for Gluster. 
    """
    def get_necessary_ports(self, num_instances):
        return []

    """
    Get the internal ports. 
    """
    def get_exposed_ports(self, num_instances):
        ports = []
        
        ports.append(GlusterConfig.MANAGEMENT_PORT)
        for i in range(0, num_instances):
            ports.append(GlusterConfig.BRICK_PORT + i)

        return ports

    """
    Get total number of instances.
    """
    def get_total_instances(self, num_instances, layers):
        instances = []

        for i in range(num_instances):
            instances.append('gluster')

        return instances

    """
    Generate a new configuration
    Param num Number of instances that need to be configured
    Param image Image type of the instances
    """
    def generate(self, num):
        return GlusterConfig(num)

    """
    Apply the configuration to the instances
    """
    def apply(self, config, containers):
        # The "entry point" is the way to contact the storage service.
        # For gluster this is the IP address of the "master" and the volume name. 
        entry_point = { 'type' : 'gluster' }

        # Create a new configuration directory, and place
        # into the template directory. 
        new_config_dir = "/tmp/" + self.generate_config_dir(config.uuid)

        try:
            sh.mkdir('-p', new_config_dir)
        except:
            logging.warning("gluster " + new_config_dir + " already exists")

        # Choose one of the instances as the "head" node. 
        # The head node is special since it "runs" the installation. 
        config.head_node = containers[0]

        # Start making the changes. 
        try:
            # Write out the list of all addresses
            entry_point['ip'] = config.head_node['data_ip']
            entry_point['instances'] = []
            for server in containers:
                entry_point['instances'].append([server['data_ip'], server['host_name']])

            # These are the commands the head node will execute. 
            in_file = open(self.template_dir + '/configure.template', 'r')
            out_file = open(new_config_dir + '/configure', 'w+')

            probe = ""
            volume_id = "gluster-volume-" + str(config.uuid)
            volumes = "gluster volume create " + str(volume_id) + " "
            for server in containers:
                # The head node should not list itself in the peer probe.
                if server != config.head_node:
                    probe += "gluster peer probe " + str(server['data_ip']) + "\n"

                # All vumes get listed including the head node. 
                volumes += str(server['data_ip']) + ":/" + config.data_directory + " "

            # Now make the changes to the template file. 
            entry_point['volume'] = volume_id
            changes = { "BRICK_DIR":config.data_directory, 
                        "PEER_PROBE":probe,
                        "VOLUME_LIST":volumes,
                        "VOLUME_ID":volume_id }
            for line in in_file:
                s = Template(line).substitute(changes)
                out_file.write(s)

            in_file.close()
            out_file.close()

            # Change the permissions of the configure file to be executable.
            os.chmod(new_config_dir + '/configure', 
                     stat.S_IRUSR |
                     stat.S_IWUSR |
                     stat.S_IXUSR | 
                     stat.S_IRGRP |
                     stat.S_IWGRP |
                     stat.S_IXGRP |
                     stat.S_IROTH)
        except IOError as e:
            logging.error(e.strerror)

        # We need to assign a configuration to each container. 
        config_dirs = []
        for c in containers:
            config_dirs.append([c['container'],
                                new_config_dir, 
                                config.config_directory])

        return config_dirs, entry_point

"""
A GlusterFS configuration. 
"""
class GlusterConfig(object):
    data_directory = '/service/data'
    log_directory = '/service/logs'
    config_directory = '/service/configuration'

    BRICK_PORT = 24009
    MANAGEMENT_PORT = 24007

    def __init__(self, num):
        self.num = num
        self.data_directory = GlusterConfig.data_directory
        self.log_directory = GlusterConfig.log_directory
        self.config_directory = GlusterConfig.config_directory
        self.mode = 'stripe,replicate' # Use both striping and replication
        self.stripe_count = 2 * num
        self.stripe_size = '128kb'
        self.head_node = None
        self.uuid = None

########NEW FILE########
__FILENAME__ = hadoopclientconfig
# Copyright 2014 OpenCore LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import os
import sys
import sh
from string import Template
from ferry.install import FERRY_HOME
from ferry.docker.fabric import DockerFabric
from ferry.config.hadoop.hiveconfig import *

class HadoopClientInitializer(object):
    """
    Create a new initializer
    Param user The user login for the git repo
    """
    def __init__(self):
        self.template_dir = None
        self.template_repo = None

        self.hive_client = HiveClientInitializer()
        self.hive_client.template_dir = FERRY_HOME + '/data/templates/hive-metastore/'

        self.container_data_dir = None
        self.container_log_dir = HadoopClientConfig.log_directory

    """
    Generate a new hostname
    """
    def new_host_name(self, instance_id):
        return 'hadoop-client' + str(instance_id)

    """
    Start the service on the containers. 
    """
    def _execute_service(self, containers, entry_point, fabric, cmd):
        # We need to know what sort of storage backend we are
        # using, since this will help set up everything.
        if entry_point['storage_type'] == 'hadoop':
            output = fabric.cmd(containers, '/service/sbin/startnode %s hadoop' % cmd)
        elif entry_point['storage_type'] == 'gluster':
            mount_url = entry_point['storage_url']
            output = fabric.cmd(containers, 
                                '/service/sbin/startnode %s gluster %s' % (cmd, mount_url))
    def start_service(self, containers, entry_point, fabric):
        self._execute_service(containers, entry_point, fabric, "start")
    def restart_service(self, containers, entry_point, fabric):
        self._execute_service(containers, entry_point, fabric, "restart")
    def stop_service(self, containers, entry_point, fabric):        
        self._execute_service(containers, entry_point, fabric, "stop")

    """
    Generate a new configuration.
    """
    def _generate_config_dir(self, uuid):
        return 'hadoop_client_' + str(uuid)

    """
    Get the ports necessary. 
    """
    def get_necessary_ports(self, num_instances):
        return []

    """
    Get the internal ports. 
    """
    def get_exposed_ports(self, num_instances):
        return []

    """
    Generate a new configuration
    """
    def generate(self, num):
        return HadoopClientConfig(num)

    """
    Generate the core-site configuration for a local filesystem. 
    """
    def _generate_gluster_core_site(self, mount_point, new_config_dir):
        core_in_file = open(self.template_dir + '/core-site.xml.template', 'r')
        core_out_file = open(new_config_dir + '/core-site.xml', 'w+')

        changes = { "DEFAULT_NAME":"file:///", 
                    "DATA_TMP":"/service/data/client/tmp" }
        for line in core_in_file:
            s = Template(line).substitute(changes)
            core_out_file.write(s)

        core_in_file.close()
        core_out_file.close()

    """
    Generate the core-site configuration. 
    """
    def _generate_core_site(self, hdfs_master, new_config_dir):
        core_in_file = open(self.template_dir + '/core-site.xml.template', 'r')
        core_out_file = open(new_config_dir + '/core-site.xml', 'w+')

        default_name = "%s://%s:%s" % ("hdfs",
                                       hdfs_master,
                                       HadoopClientConfig.HDFS_MASTER)
        changes = { "DEFAULT_NAME":default_name,
                    "DATA_TMP":"/service/data/client/tmp" }
        for line in core_in_file:
            s = Template(line).substitute(changes)
            core_out_file.write(s)

        core_in_file.close()
        core_out_file.close()

    """
    Generate the yarn-site configuration. 
    """
    def _generate_yarn_site(self, yarn_master, new_config_dir):
        yarn_in_file = open(self.template_dir + '/yarn-site.xml.template', 'r')
        yarn_out_file = open(new_config_dir + '/yarn-site.xml', 'w+')

        changes = { "YARN_MASTER":yarn_master,
                    "DATA_STAGING":"/service/data/client/staging" }
        for line in yarn_in_file:
            s = Template(line).substitute(changes)
            yarn_out_file.write(s)

        yarn_in_file.close()
        yarn_out_file.close()


    """
    Generate the mapred-site configuration. 
    """
    def _generate_mapred_site(self, config, containers, new_config_dir):
        mapred_in_file = open(self.template_dir + '/mapred-site.xml.template', 'r')
        mapred_out_file = open(new_config_dir + '/mapred-site.xml', 'w+')

        # Most of these values aren't applicable for the client,
        # so just make up fake numbers. 
        changes = { "NODE_REDUCES":1, 
                    "NODE_MAPS":1,
                    "JOB_REDUCES":1,
                    "JOB_MAPS":1,
                    "HISTORY_SERVER":config.yarn_master, 
                    "DATA_TMP":"/service/data/client/tmp" }
        for line in mapred_in_file:
            s = Template(line).substitute(changes)
            mapred_out_file.write(s)

        mapred_in_file.close()
        mapred_out_file.close()

    """
    Apply the Hive client configuration
    """
    def _apply_hive_client(self, config, containers):
        return self.hive_client.apply(config, containers)

    """
    Apply the configuration to the instances
    """
    def apply(self, config, containers):
        entry_point = { 'type' : 'hadoop-client' }
        entry_point['ip'] = containers[0]['data_ip']

        # Create a new configuration directory, and place
        # into the template directory. 
        new_config_dir = "/tmp/" + self._generate_config_dir(config.uuid)
        try:
            sh.mkdir('-p', new_config_dir)
        except:
            sys.stderr.write('could not create config dir ' + new_config_dir)

        # Check if there is an explicit compute cluster. If there
        # is, then we use that for YARN information. 
        storage = containers[0]['storage'][0]
        compute = None
        if 'compute' in containers[0] and len(containers[0]['compute']) > 0:
            compute = containers[0]['compute'][0]

        if compute and 'yarn' in compute:
            config.yarn_master = compute['yarn']
            if 'db' in compute:
                config.hive_meta = compute['db']                
        else:
            # Use the storage backend for the YARN info. However, first
            # check if the storage is compatible.
            if 'yarn' in storage:
                config.yarn_master = storage['yarn']
            if 'db' in storage:
                config.hive_meta = storage['db']

        # Check what sort of storage we are using.
        entry_point['storage_type'] = storage['type']
        if storage['type'] == 'hadoop':
            config.hdfs_master = storage['hdfs']
            self._generate_core_site(config.hdfs_master, new_config_dir)
        elif storage['type'] == 'gluster':
            mount_url = "%s:/%s" % (storage['ip'], storage['volume'])
            entry_point['storage_url'] = mount_url
            self._generate_gluster_core_site('/data', new_config_dir)

        # Generate the Hadoop conf files.
        if config.yarn_master:
            self._generate_mapred_site(config, containers, new_config_dir)
            self._generate_yarn_site(config.yarn_master, new_config_dir)

        # Each container needs to point to a new config dir. 
        config_dirs = []
        for c in containers:
            config_dirs.append([c['container'],
                                new_config_dir + '/*',
                                config.config_directory])

        # Now configure the Hive client.
        if config.hive_meta:
            hive_config = HiveClientConfig(1)
            hive_config.uuid = config.uuid
            hive_config.hadoop_config_dir = config.config_directory
            hive_config.metastore = config.hive_meta
            hive_dirs, hive_entry = self._apply_hive_client(hive_config, containers)
            config_dirs.extend(hive_dirs)

        return config_dirs, entry_point

class HadoopClientConfig(object):
    log_directory = '/service/data/logs'
    config_directory = '/service/conf'

    HDFS_MASTER = 9000

    def __init__(self, num):
        self.num = num
        self.config_directory = HadoopClientConfig.config_directory
        self.system_info = None
        self.yarn_master = None
        self.hdfs_master = None
        self.hive_meta = None

########NEW FILE########
__FILENAME__ = hadoopconfig
# Copyright 2014 OpenCore LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import os
import sys
import time
import sh
from string import Template
from ferry.install import FERRY_HOME
from ferry.docker.fabric import DockerFabric
from ferry.config.hadoop.hiveconfig import *
from ferry.config.hadoop.metastore  import *

class HadoopInitializer(object):
    """
    Create a new initializer
    Param user The user login for the git repo
    """
    def __init__(self):
        self.template_dir = None
        self.template_repo = None

        self.container_data_dir = HadoopConfig.data_directory
        self.container_log_dir = HadoopConfig.log_directory

        self.hive_client = HiveClientInitializer()
        self.hive_ms = MetaStoreInitializer()
        self.hive_client.template_dir = FERRY_HOME + '/data/templates/hive-metastore/'
        self.hive_ms.template_dir = FERRY_HOME + '/data/templates/hive-metastore/'

    """
    Generate a new hostname
    """
    def new_host_name(self, instance_id):
        return 'hadoop' + str(instance_id)

    """
    Start the service on the containers. 
    """
    def _execute_service(self, containers, entry_point, fabric, cmd):
        yarn_master = entry_point['yarn']
        hdfs_master = None

        # Now start the HDFS cluster. 
        if entry_point['storage_type'] == 'hadoop':
            hdfs_master = entry_point['hdfs']
            for c in containers:
                if c.service_type == 'hadoop':
                    if c.internal_ip == hdfs_master:
                        output = fabric.cmd([c], '/service/sbin/startnode %s namenode' % cmd)
                    elif c.internal_ip != yarn_master:
                        output = fabric.cmd([c], '/service/sbin/startnode %s datanode' % cmd)

            # Now wait a couple seconds to make sure
            # everything has started.
            time.sleep(5)
        elif entry_point['storage_type'] == 'gluster':
            mount_url = entry_point['storage_url']
            output = fabric.cmd(containers, 
                                '/service/sbin/startnode %s gluster %s' % (cmd, mount_url))
                                
        # Now start the YARN cluster. 
        for c in containers:
            if c.service_type == 'hadoop' or c.service_type == 'yarn':
                if c.internal_ip == yarn_master:
                    output = fabric.cmd([c], '/service/sbin/startnode %s yarnmaster' % cmd)
                elif c.internal_ip != hdfs_master:
                    output = fabric.cmd([c], '/service/sbin/startnode %s yarnslave' % cmd)

        # Now start the Hive metastore. 
        for c in containers:
            if c.service_type == 'hive':
                self.hive_ms._execute_service([c], None, fabric, cmd)

    def start_service(self, containers, entry_point, fabric):
        self._execute_service(containers, entry_point, fabric, "start")
    def restart_service(self, containers, entry_point, fabric):
        self._execute_service(containers, entry_point, fabric, "restart")

    def stop_service(self, containers, entry_point, fabric):
        output = fabric.cmd(containers, '/service/sbin/startnode stop')

    """
    Generate a new configuration.
    """
    def _generate_config_dir(self, uuid, container):
        return 'hadoop_' + str(uuid) + '_' + str(container['data_ip'])

    """
    Get the ports necessary. 
    """
    def get_necessary_ports(self, num_instances):
        return []

    """
    Get the ports to expose internally between containers
    (but not outside containers). 
    """
    def get_exposed_ports(self, num_instances):
        ports = []
        ports.append(HadoopConfig.YARN_SCHEDULER)
        ports.append(HadoopConfig.YARN_ADMIN)
        ports.append(HadoopConfig.YARN_RESOURCE)
        ports.append(HadoopConfig.YARN_TRACKER)
        ports.append(HadoopConfig.YARN_HTTP)
        ports.append(HadoopConfig.YARN_HTTPS)
        ports.append(HadoopConfig.YARN_JOB_HISTORY)
        ports.append(HadoopConfig.HDFS_MASTER)
        ports.append(HadoopConfig.YARN_RPC_PORTS)
        ports.append(HadoopConfig.HIVE_META)
        ports.append(HadoopConfig.HIVE_SERVER)
        return ports

    """
    Get total number of instances. For Hadoop we must have additional containers
    for the YARN master, HDFS master, and possibly the Hive metastore. 
    """
    def get_total_instances(self, num_instances, layers):
        instances = []

        for i in range(num_instances + 2):
            instances.append('hadoop')

        if len(layers) > 0 and layers[0] == "hive":
            instances.append('hive')

        return instances

    """
    Generate a new configuration
    """
    def generate(self, num):
        return HadoopConfig(num)

    """
    Need to estimate the amount of memory available on
    one of the containers. 
    """
    def _estimate_mem_size(self, config):
        if config.system_info != None:
            return config.system_info['mem']
        else:
            # Estimate 2GB of available memory. 
            return 2

    """
    Generate the core-site configuration for a local filesystem. 
    """
    def _generate_gluster_core_site(self, new_config_dir, container):
        core_in_file = open(self.template_dir + '/core-site.xml.template', 'r')
        core_out_file = open(new_config_dir + '/core-site.xml', 'w+')

        changes = { "DEFAULT_NAME":"file:///", 
                    "DATA_TMP":"/service/data/%s/tmp" % container['host_name'] }
        for line in core_in_file:
            s = Template(line).substitute(changes)
            core_out_file.write(s)

        core_in_file.close()
        core_out_file.close()

    """
    Generate the core-site configuration. 
    """
    def _generate_core_site(self, hdfs_master, new_config_dir):
        core_in_file = open(self.template_dir + '/core-site.xml.template', 'r')
        core_out_file = open(new_config_dir + '/core-site.xml', 'w+')

        default_name = "%s://%s:%s" % ("hdfs",
                                       hdfs_master['data_ip'],
                                       HadoopConfig.HDFS_MASTER)
        changes = { "DEFAULT_NAME":default_name,
                    "DATA_TMP":"/service/data/tmp" }
        for line in core_in_file:
            s = Template(line).substitute(changes)
            core_out_file.write(s)

        core_in_file.close()
        core_out_file.close()

    """
    Generate the hdfs-site configuration. 
    """
    def _generate_hdfs_site(self, config, hdfs_master, new_config_dir):
        hdfs_in_file = open(self.template_dir + '/hdfs-site.xml.template', 'r')
        hdfs_out_file = open(new_config_dir + '/hdfs-site.xml', 'w+')

        changes = { "DATA_DIR":config.data_directory }
        for line in hdfs_in_file:
            s = Template(line).substitute(changes)
            hdfs_out_file.write(s)

        hdfs_in_file.close()
        hdfs_out_file.close()

    """
    Generate the yarn-site configuration. 
    """
    def _generate_yarn_site(self, yarn_master, new_config_dir, container=None):
        yarn_in_file = open(self.template_dir + '/yarn-site.xml.template', 'r')
        yarn_out_file = open(new_config_dir + '/yarn-site.xml', 'w+')

        changes = { "YARN_MASTER":yarn_master['data_ip'] } 

        # Generate the staging table. This differs depending on whether
        # we need to be container specific or not. 
        if container:
            changes['DATA_STAGING'] = '/service/data/%s/staging' % container['host_name']
        else:
            changes['DATA_STAGING'] = '/service/data/staging'

        for line in yarn_in_file:
            s = Template(line).substitute(changes)
            yarn_out_file.write(s)

        yarn_in_file.close()
        yarn_out_file.close()

    def _generate_log4j(self, new_config_dir):
        in_file = open(self.template_dir + '/log4j.properties', 'r')
        out_file = open(new_config_dir + '/log4j.properties', 'w+')

        for line in in_file:
            out_file.write(line)

        in_file.close()
        out_file.close()

    """
    Generate the yarn-env configuration. 
    """
    def _generate_yarn_env(self, yarn_master, new_config_dir):
        yarn_in_file = open(self.template_dir + '/yarn-env.sh.template', 'r')
        yarn_out_file = open(new_config_dir + '/yarn-env.sh', 'w+')

        for line in yarn_in_file:
            yarn_out_file.write(line)

        yarn_in_file.close()
        yarn_out_file.close()

    """
    Generate the yarn-env configuration. 
    """
    def _generate_mapred_env(self, yarn_master, new_config_dir):
        in_file = open(self.template_dir + '/mapred-env.sh', 'r')
        out_file = open(new_config_dir + '/mapred-env.sh', 'w+')

        for line in in_file:
            out_file.write(line)

        in_file.close()
        out_file.close()

    """
    Generate the mapred-site configuration. 
    """
    def _generate_mapred_site(self, config, containers, new_config_dir, container=None):
        mapred_in_file = open(self.template_dir + '/mapred-site.xml.template', 'r')
        mapred_out_file = open(new_config_dir + '/mapred-site.xml', 'w+')

        # These are the mapred variables. 
        mem_size = self._estimate_mem_size(config)
        reduce_per_node = (mem_size / len(containers) - 2) / 2
        map_per_node = reduce_per_node * 4
        job_maps = map_per_node * ( len(containers) - 2 )
        job_reduces = reduce_per_node * ( len(containers) - 2 )

        changes = { "NODE_REDUCES":reduce_per_node, 
                    "NODE_MAPS":map_per_node,
                    "JOB_REDUCES":job_reduces,
                    "JOB_MAPS":job_maps,
                    "HISTORY_SERVER":"0.0.0.0"}

        # Generate the temp area. This differs depending on whether
        # we need to be container specific or not. 
        if container:
            changes['DATA_TMP'] = '/service/data/%s/tmp' % container['host_name']
        else:
            changes['DATA_TMP'] = '/service/data/tmp'

        for line in mapred_in_file:
            s = Template(line).substitute(changes)
            mapred_out_file.write(s)

        mapred_in_file.close()
        mapred_out_file.close()

    """
    Apply the Hive metastore configuration
    """
    def _apply_hive_metastore(self, config, containers):
        return self.hive_ms.apply(config, containers)

    """
    Apply the Hive client configuration
    """
    def _apply_hive_client(self, config, containers):
        return self.hive_client.apply(config, containers)

    """
    Apply the Hadoop configuration
    """
    def _apply_hadoop(self, config, containers):
        entry_point = { 'type' : 'hadoop' }

        # Pick out the various master nodes. The Hadoop configuration assumes
        # that the first two containers are used for metadata purposes. 
        yarn_master = containers[0]
        hdfs_master = containers[1]
            
        # Remember the entry points
        entry_point['yarn'] = str(yarn_master['data_ip'])
        entry_point['hdfs'] = str(hdfs_master['data_ip'])
        entry_point['instances'] = []

        # Create a new configuration directory, and place
        # into the template directory. 
        config_dirs = []
        for c in containers:
            new_config_dir = "/tmp/" + self._generate_config_dir(config.uuid, c)
            try:
                sh.mkdir('-p', new_config_dir)
            except:
                sys.stderr.write('could not create config dir ' + new_config_dir)

            # Only add the container to the instances list once. 
            entry_point['instances'].append([c['data_ip'], c['host_name']])

            # Generate some mapred-site config
            self._generate_mapred_site(config, containers, new_config_dir)
            self._generate_mapred_env(yarn_master, new_config_dir)

            # Now generate the yarn config files
            self._generate_yarn_site(yarn_master, new_config_dir)
            self._generate_yarn_env(yarn_master, new_config_dir)

            # Now generate the core config
            self._generate_core_site(hdfs_master, new_config_dir)

            # Now generate the HDFS config
            self._generate_hdfs_site(config, hdfs_master, new_config_dir)

            # Generate the log4j config
            self._generate_log4j(new_config_dir)

            config_dirs.append([c['container'], 
                                new_config_dir + '/*',
                                config.config_directory])

        return config_dirs, entry_point

    """
    Apply the YARN-only configuration
    """
    def _apply_yarn(self, config, containers):
        entry_point = { 'type' : 'yarn' }

        # Pick out the various master nodes. The Hadoop configuration assumes
        # that the first two containers are used for metadata purposes. 
        yarn_master = containers[0]
        entry_point['yarn'] = str(yarn_master['data_ip'])
        entry_point['instances'] = []

        # Create a new configuration directory, and place
        # into the template directory. 
        config_dirs = []
        for c in containers:            
            new_config_dir = "/tmp/" + self._generate_config_dir(config.uuid, c)
            try:
                sh.mkdir('-p', new_config_dir)
            except:
                sys.stderr.write('could not create config dir ' + new_config_dir)

            # Slaves file used to figure out who hosts the actual work/data
            for server in containers:
                entry_point['instances'].append([server['data_ip'], server['host_name']])

            # Generate the log4j config
            self._generate_log4j(new_config_dir)

            # Generate some mapred-site config
            self._generate_mapred_site(config, containers, new_config_dir, c)
            self._generate_mapred_env(yarn_master, new_config_dir)

            # Now generate the yarn config files
            self._generate_yarn_site(yarn_master, new_config_dir, c)
            self._generate_yarn_env(yarn_master, new_config_dir)

            # Now we need to configure additional storage parameters. For example,
            # for Gluster, etc. 
            storage_entry = containers[0]['storage'][0]
            entry_point['storage_type'] = storage_entry['type']
            if storage_entry['type'] == 'gluster':
                url = self._apply_gluster(config, storage_entry, new_config_dir, c)
                entry_point['storage_url'] = url

            config_dirs.append([c['container'], 
                                new_config_dir + '/*',
                                config.config_directory])
        return config_dirs, entry_point

    def _apply_hive(self, config, hadoop_entry, hadoop_dirs, hadoop_containers, hive_containers):
        # First configure the metastore service
        ms_config = MetaStoreConfig(1)
        ms_config.uuid = config.uuid
        ms_config.hadoop_dirs = hadoop_dirs
        ms_dirs, ms_entry = self._apply_hive_metastore(ms_config, hive_containers)

        # Now configure the Hive client. This configuration
        # gets applied to the Hadoop containers. 
        hive_config = HiveClientConfig(1)
        hive_config.uuid = config.uuid
        hive_config.hadoop_config_dir = config.config_directory
        hive_config.metastore = ms_entry['db']
        hive_dirs, hive_entry = self._apply_hive_client(hive_config, hadoop_containers)
        hive_dirs.extend(ms_dirs)
        return hive_dirs, hive_config

    def _apply_gluster(self, config, storage_entry, new_config_dir, container):
        # We assume that the new configuration directory has already 
        # been created. In the future, may want to check for this. 
        self._generate_gluster_core_site(new_config_dir, container)

        # The mount URL specifies how to connect to Gluster. 
        mount_url = "%s:/%s" % (storage_entry['ip'], storage_entry['volume'])
        return mount_url

    """
    Apply the configuration to the instances
    """
    def apply(self, config, containers):
        # First separate the Hadoop and Hive containers.
        hadoop_containers = []
        hive_containers = []
        for c in containers:
            if c['type'] == 'hadoop' or c['type'] == 'yarn':
                hadoop_containers.append(c)
            elif c['type'] == 'hive':
                hive_containers.append(c)

        if 'storage' in hadoop_containers[0]:
            # This Hadoop instance is being applied to an existing
            # storage mechanism. So just configure yarn.
            hadoop_dirs, hadoop_entry = self._apply_yarn(config, hadoop_containers)
        else:
            # This Hadoop instance is being applied for both storage
            # and compute. Right now there's no way to just instantiate HDFS. 
            hadoop_dirs, hadoop_entry = self._apply_hadoop(config, hadoop_containers)
            hadoop_entry['storage_type'] = 'hadoop'

        hive_entry = {}
        if len(hive_containers) > 0:
            # We also need to configure some Hive services
            hive_dirs, hive_config = self._apply_hive(config, hadoop_entry, hadoop_dirs, hadoop_containers, hive_containers)

            # Now merge the configuration dirs.
            hadoop_dirs.extend(hive_dirs)
            hadoop_entry['db'] = hive_config.metastore

        return hadoop_dirs, hadoop_entry

class HadoopConfig(object):
    data_directory = '/service/data/main'
    log_directory = '/service/data/logs'
    tmp_directory = '/service/data/tmp'
    config_directory = '/service/conf'

    YARN_SCHEDULER = 8030
    YARN_ADMIN = 8033
    YARN_RESOURCE = 8041
    YARN_TRACKER = 8025
    YARN_HTTP = 8088
    YARN_HTTPS = 8090
    YARN_JOB_HISTORY = 10020
    HDFS_MASTER = 9000
    YARN_RPC_PORTS = '50100-50200'
    HIVE_META = 9083
    HIVE_SERVER = 10000

    def __init__(self, num):
        self.num = num
        self.data_directory = HadoopConfig.data_directory
        self.log_directory = HadoopConfig.log_directory
        self.tmp_directory = HadoopConfig.tmp_directory
        self.config_directory = HadoopConfig.config_directory
        self.system_info = None

########NEW FILE########
__FILENAME__ = hiveconfig
# Copyright 2014 OpenCore LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import sh
import sys
from string import Template
from ferry.docker.fabric import DockerFabric

class HiveClientInitializer(object):
    """
    Create a new initializer
    Param user The user login for the git repo
    """
    def __init__(self):
        self.template_dir = None
        self.template_repo = None

        self.container_data_dir = HiveClientConfig.data_directory
        self.container_log_dir = HiveClientConfig.log_directory

    """
    Generate a new hostname
    """
    def new_host_name(self, instance_id):
        return 'hive-client' + str(instance_id)

    """
    Start the service on the containers. 
    """
    def _execute_service(self, containers, entry_point, fabric, cmd):
        output = fabric.cmd(containers, '/service/sbin/startnode %s client' % cmd)

    """
    Generate a new configuration.
    """
    def _generate_config_dir(self, uuid):
        return 'hive_client_' + str(uuid)

    """
    Get the ports necessary. 
    """
    def get_necessary_ports(self, num_instances):
        return []

    """
    Get the internal ports. 
    """
    def get_exposed_ports(self, num_instances):
        return []

    """
    Generate a new configuration
    """
    def generate(self, num):
        return HiveClientConfig(num)

    """
    Generate the hive site configuration. 
    """
    def _generate_hive_site(self, config, new_config_dir):
        in_file = open(self.template_dir + '/hive-site.xml.template', 'r')
        out_file = open(new_config_dir + '/hive-site.xml', 'w+')
        
        changes = { "DB":config.metastore }
        for line in in_file:
            s = Template(line).substitute(changes)
            out_file.write(s)

        in_file.close()
        out_file.close()

    """
    Apply the configuration to the instances
    """
    def apply(self, config, containers):
        # The "entry point" is the way to contact the storage service.
        # For gluster this is the IP address of the "master" and the volume name. 
        entry_point = { 'type' : 'hive' }

        # Create a new configuration directory, and place
        # into the template directory. 
        config_dirs = []
        new_config_dir = "/tmp/" + self._generate_config_dir(config.uuid)
        try:
            sh.mkdir('-p', new_config_dir)
        except:
            sys.stderr.write('could not create config dir ' + new_config_dir)

        self._generate_hive_site(config, new_config_dir)

        # Each container needs to point to a new config dir. 
        for c in containers:
            config_files = new_config_dir + '/*'
            config_dirs.append([c['container'],
                                config_files, 
                                config.config_directory])

        return config_dirs, entry_point

class HiveClientConfig(object):
    data_directory = '/service/data/main'
    log_directory = '/service/data/logs'

    def __init__(self, num):
        self.num = num
        self.config_directory = '/service/packages/hive/conf'
        self.hadoop_config_dir = None
        self.metastore = None

########NEW FILE########
__FILENAME__ = metastore
# Copyright 2014 OpenCore LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import sh
import sys
from string import Template
from ferry.docker.fabric import DockerFabric

class MetaStoreInitializer(object):
    """
    Create a new initializer
    Param user The user login for the git repo
    """
    def __init__(self):
        self.template_dir = None
        self.template_repo = None

        self.container_data_dir = MetaStoreConfig.data_directory
        self.container_log_dir = MetaStoreConfig.log_directory

    """
    Generate a new hostname
    """
    def new_host_name(self, instance_id):
        return 'hive-metastore' + str(instance_id)

    """
    Start the service on the containers. 
    """
    def _execute_service(self, containers, entry_point, fabric, cmd):
        output = fabric.cmd(containers, '/service/sbin/startnode %s metastore' % cmd)
    def start_service(self, containers, entry_point, fabric):
        self._execute_service(containers, entry_point, fabric, "start")
    def restart_service(self, containers, entry_point, fabric):
        self._execute_service(containers, entry_point, fabric, "restart")
    def stop_service(self, containers, entry_point, fabric):
        self._execute_service(containers, entry_point, fabric, "stop")

    """
    Generate a new configuration.
    """
    def _generate_config_dir(self, uuid):
        return 'hive_ms_' + str(uuid)

    """
    Get the ports necessary. 
    """
    def get_necessary_ports(self, num_instances):
        return [MetaStoreConfig.POSTGRES_PORT, 
                MetaStoreConfig.THRIFT_PORT]

    """
    Get the internal ports. 
    """
    def get_exposed_ports(self, num_instances):
        return []

    """
    Generate a new configuration
    """
    def generate(self, num):
        return MetaStoreConfig(num)

    """
    Generate the postgres configuration. 
    """
    def _generate_postgres_site(self, new_config_dir):
        in_file = open(self.template_dir + '/postgresql.conf', 'r')
        out_file = open(new_config_dir + '/postgresql.conf', 'w+')

        for line in in_file:
            out_file.write(line)

        in_file.close()
        out_file.close()

    """
    Generate the security configuration. 
    """
    def _generate_security_site(self, entry_point, new_config_dir):
        in_file = open(self.template_dir + '/pg_hba.conf', 'r')
        out_file = open(new_config_dir + '/pg_hba.conf', 'w+')

        # We need to figure out the local mask so that clients can connect
        # to the statistics database. Right now we're guessing. 
        p = entry_point['db'].split(".")
        subnet = "%s.%s.%s.1/24" % (p[0], p[1], p[2])
        changes = { "LOCAL_IP" : entry_point['db'],
                    "LOCAL_MASK" : subnet }
        for line in in_file:
            s = Template(line).substitute(changes)
            out_file.write(s)

        in_file.close()
        out_file.close()

    """
    Generate the hive site configuration. 
    """
    def _generate_hive_site(self, entry_point, config, new_config_dir):
        in_file = open(self.template_dir + '/hive-site.xml.template', 'r')
        out_file = open(new_config_dir + '/hive-site.xml', 'w+')
        
        changes = { "DB":entry_point['db'] }
        for line in in_file:
            s = Template(line).substitute(changes)
            out_file.write(s)

        in_file.close()
        out_file.close()

    """
    Apply the configuration to the instances
    """
    def apply(self, config, containers):
        # The "entry point" is the way to contact the storage service.
        # For gluster this is the IP address of the "master" and the volume name. 
        entry_point = { 'type' : 'hive' }

        # Remember the entry points
        entry_point['db'] = str(containers[0]['data_ip'])

        # Create a new configuration directory, and place
        # into the template directory. 
        config_dirs = []
        new_config_dir = "/tmp/postgres/" + self._generate_config_dir(config.uuid)
        hive_config_dir = "/tmp/hive/" + self._generate_config_dir(config.uuid)
        try:
            sh.mkdir('-p', new_config_dir)
            sh.mkdir('-p', hive_config_dir)
        except:
            sys.stderr.write('could not create config dir ' + new_config_dir)

        self._generate_postgres_site(new_config_dir)
        self._generate_security_site(entry_point, new_config_dir)
        self._generate_hive_site(entry_point, config, hive_config_dir)

        # Each container needs to point to a new config dir. 
        for c in containers:
            config_files = new_config_dir + '/*'
            config_dirs.append([c['container'],
                                config_files, 
                                config.config_directory])

        # Transfer the Hive config
        for c in containers:
            config_files = hive_config_dir + '/*'
            config_dirs.append([c['container'],
                                config_files, 
                                config.hive_config])

        # Merge the Hadoop configuration
        from_files = config.hadoop_dirs[0][1]
        for c in containers:
            config_dirs.append([c['container'],
                                from_files, 
                                config.hadoop_config])
        return config_dirs, entry_point

class MetaStoreConfig(object):
    data_directory = '/service/data/main'
    log_directory = '/service/data/logs'
    tmp_directory = '/service/data/tmp'
    config_directory = '/etc/postgresql/9.1/main/'
    hive_config = '/service/packages/hive/conf'
    hadoop_config = '/service/packages/hadoop/etc/hadoop'

    POSTGRES_PORT = 5432
    THRIFT_PORT = 9083

    def __init__(self, num):
        self.data_directory = MetaStoreConfig.data_directory
        self.log_directory = MetaStoreConfig.log_directory
        self.tmp_directory = MetaStoreConfig.tmp_directory
        self.config_directory = MetaStoreConfig.config_directory
        self.hive_config = MetaStoreConfig.hive_config
        self.hadoop_config = MetaStoreConfig.hadoop_config

        self.num = num
        self.system_info = None
        self.hadoop_dirs = None
        self.metastore = None

########NEW FILE########
__FILENAME__ = mpiclientconfig
# Copyright 2014 OpenCore LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import logging
import sh
import sys
from string import Template
from ferry.config.openmpi.mpiconfig import *

class OpenMPIClientInitializer(object):
    """
    Create a new initializer
    Param user The user login for the git repo
    """
    def __init__(self):
        self.mpi = OpenMPIInitializer()
        self.container_data_dir = self.mpi.container_data_dir
        self.container_log_dir = self.mpi.container_log_dir

    @property
    def template_dir(self):
        self.mpi.template_dir

    @template_dir.setter
    def template_dir(self, value):
        self.mpi.template_dir = value

    @property
    def template_repo(self):
        self.mpi.template_repo

    @template_repo.setter
    def template_repo(self, value):
        self.mpi.template_repo = value

    """
    Generate a new hostname
    """
    def new_host_name(self, instance_id):
        return 'openmpi_client' + str(instance_id)

    """
    Start the service on the containers. 
    """
    def start_service(self, containers, entry_point, fabric):
        self.mpi.start_service(containers, entry_point, fabric)
    def restart_service(self, containers, entry_point, fabric):
        self.mpi.restart_service(containers, entry_point, fabric)
    def stop_service(self, containers, entry_point, fabric):        
        self.mpi.stop_service(containers, entry_point, fabric)

    """
    Get the ports necessary.
    """
    def get_necessary_ports(self, num_instances):
        return self.mpi.get_necessary_ports(num_instances)

    """
    Get the internal ports. 
    """
    def get_exposed_ports(self, num_instances):
        return self.mpi.get_exposed_ports(num_instances)

    """
    Generate a new configuration
    Param num Number of instances that need to be configured
    Param image Image type of the instances
    """
    def generate(self, num):
        return self.mpi.generate(num)

    """
    Apply the configuration to the instances
    """
    def apply(self, config, containers):
        config_dirs, entry_point = self.mpi.apply(config, containers)
        entry_point['type'] = 'mpi-client'
        return config_dirs, entry_point

########NEW FILE########
__FILENAME__ = mpiconfig
# Copyright 2014 OpenCore LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import logging
import sh
import sys
from string import Template

class OpenMPIInitializer(object):
    """
    Create a new initializer
    Param user The user login for the git repo
    """
    def __init__(self):
        self.template_dir = None
        self.template_repo = None

        self.container_data_dir = None
        self.container_log_dir = MPIConfig.log_directory

    """
    Generate a new hostname
    """
    def new_host_name(self, instance_id):
        return 'openmpi' + str(instance_id)

    """
    Start the service on the containers. 
    """
    def _execute_service(self, containers, entry_point, fabric, cmd):
        output = fabric.cmd(containers, '/service/sbin/startnode %s %s' % (cmd, entry_point['mount']))
    def start_service(self, containers, entry_point, fabric):
        self._execute_service(containers, entry_point, fabric, "start")
    def restart_service(self, containers, entry_point, fabric):
        self._execute_service(containers, entry_point, fabric, "restart")
    def stop_service(self, containers, entry_point, fabric):        
        self._execute_service(containers, entry_point, fabric, "stop")
    """
    Generate a new configuration.
    """
    def _generate_config_dir(self, uuid):
        return 'openmpi_' + str(uuid)

    """
    Get the ports necessary.
    """
    def get_necessary_ports(self, num_instances):
        return []

    """
    Get the internal ports. 
    """
    def get_exposed_ports(self, num_instances):
        BTL_PORT_END = MPIConfig.BTL_PORT_MIN + (MPIConfig.PORT_RANGE * num_instances)
        OOB_PORT_END = MPIConfig.OOB_PORT_MIN + (MPIConfig.PORT_RANGE * num_instances)
        BTL_PORTS = '%s-%s' % (MPIConfig.BTL_PORT_MIN, BTL_PORT_END)
        OOB_PORTS = '%s-%s' % (MPIConfig.OOB_PORT_MIN, OOB_PORT_END)
        return [BTL_PORTS, OOB_PORTS]

    def get_total_instances(self, num_instances, layers):
        instances = []

        for i in range(num_instances):
            instances.append('openmpi')

        return instances

    """
    Generate a new configuration
    Param num Number of instances that need to be configured
    Param image Image type of the instances
    """
    def generate(self, num):
        config = MPIConfig(num)
        config.btl_port_min = MPIConfig.BTL_PORT_MIN
        config.oob_port_min = MPIConfig.OOB_PORT_MIN
        config.btl_port_range = MPIConfig.PORT_RANGE * num
        config.oob_port_range = MPIConfig.PORT_RANGE * num

        return config

    """
    Generate the mca-params configuration. 
    """
    def _generate_mca_params(self, config, new_config_dir):
        in_file = open(self.template_dir + '/openmpi-mca-params.conf', 'r')
        out_file = open(new_config_dir + '/openmpi-mca-params.conf', 'w+')

        changes = { "BTL_PORT_MIN": config.btl_port_min,
                    "BTL_PORT_RANGE": config.btl_port_range,
                    "OOB_PORT_MIN": config.oob_port_min,
                    "OOB_PORT_RANGE": config.oob_port_range }
        for line in in_file:
            s = Template(line).substitute(changes)
            out_file.write(s)

        in_file.close()
        out_file.close()

    """
    Apply the configuration to the instances
    """
    def apply(self, config, containers):
        entry_point = { 'type' : 'openmpi' }
        config_dirs = []

        new_config_dir = "/tmp/" + self._generate_config_dir(config.uuid)
        try:
            sh.mkdir('-p', new_config_dir)
        except:
            sys.stderr.write('could not create config dir ' + new_config_dir)

        # For now the MPI client assumes there is only one storage and that it is
        # a Gluster end point. 
        storage = containers[0]['storage'][0]
        if storage['type'] == 'gluster':
            mount_ip = storage['ip']
            mount_dir = storage['volume']
            entry_point['mount'] = "%s:/%s" % (mount_ip, mount_dir)

            # Check if we are being called as a compute instance or client.
            if not 'compute' in containers[0]:
                entry_point['instances'] = []
                for server in containers:
                    entry_point['instances'].append([server['data_ip'], server['host_name']])
            else:
                # This is the MPI client. Create a "hosts" file that contains the
                # IP addresses of the compute nodes. 
                entry_point['ip'] = containers[0]['data_ip']
                compute = containers[0]['compute'][0]
                with open(new_config_dir + '/hosts', 'w+') as hosts_file:
                    for c in compute['instances']:
                        hosts_file.write(c[0] + "\n")

            self._generate_mca_params(config, new_config_dir)
            for c in containers:
                config_files = new_config_dir + '/*'
                config_dirs.append([c['container'],
                                    config_files, 
                                    config.config_directory])
        return config_dirs, entry_point

class MPIConfig(object):
    log_directory = '/service/logs'
    config_directory = '/usr/local/etc'

    BTL_PORT_MIN = 2000
    OOB_PORT_MIN = 6000
    PORT_RANGE = 4

    def __init__(self, num):
        self.num = num
        self.btl_port_min = 0
        self.btl_port_range = 0
        self.oob_port_min = 0
        self.oob_port_range = 0
        self.config_directory = MPIConfig.config_directory
        self.log_directory = MPIConfig.log_directory

########NEW FILE########
__FILENAME__ = sparkclientconfig
# Copyright 2014 OpenCore LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import logging
import sh
import sys
from string import Template
from ferry.config.spark.sparkconfig import *

class SparkClientInitializer(object):
    """
    Create a new initializer
    Param user The user login for the git repo
    """
    def __init__(self):
        self.spark = SparkInitializer()
        self.container_data_dir = self.spark.container_data_dir
        self.container_log_dir = self.spark.container_log_dir

    @property
    def template_dir(self):
        self.spark.template_dir

    @template_dir.setter
    def template_dir(self, value):
        self.spark.template_dir = value

    @property
    def template_repo(self):
        self.spark.template_repo

    @template_repo.setter
    def template_repo(self, value):
        self.spark.template_repo = value

    """
    Generate a new hostname
    """
    def new_host_name(self, instance_id):
        return 'spark_client' + str(instance_id)

    """
    Start the service on the containers. 
    """
    def start_service(self, containers, entry_point, fabric):
        self.spark.start_service(containers, entry_point, fabric)
    def restart_service(self, containers, entry_point, fabric):
        self.spark.restart_service(containers, entry_point, fabric)
    def stop_service(self, containers, entry_point, fabric):        
        self.spark.stop_service(containers, entry_point, fabric)

    """
    Get the ports necessary.
    """
    def get_necessary_ports(self, num_instances):
        return self.spark.get_necessary_ports(num_instances)

    """
    Get the internal ports. 
    """
    def get_exposed_ports(self, num_instances):
        return self.spark.get_exposed_ports(num_instances)

    """
    Generate a new configuration
    Param num Number of instances that need to be configured
    Param image Image type of the instances
    """
    def generate(self, num):
        return self.spark.generate(num)

    """
    Apply the configuration to the instances
    """
    def apply(self, config, containers):
        config_dirs, entry_point = self.spark.apply(config, containers)
        entry_point['type'] = 'spark-client'
        return config_dirs, entry_point

########NEW FILE########
__FILENAME__ = sparkconfig
# Copyright 2014 OpenCore LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import logging
import os
import sh
import sys
import time
from string import Template

class SparkInitializer(object):
    """
    Create a new initializer
    Param user The user login for the git repo
    """
    def __init__(self):
        self.template_dir = None
        self.template_repo = None

        self.container_data_dir = None
        self.container_log_dir = SparkConfig.log_directory

    """
    Generate a new hostname
    """
    def new_host_name(self, instance_id):
        return 'spark' + str(instance_id)

    """
    Start the service on the containers. 
    """
    def _execute_service(self, containers, entry_point, fabric, cmd):
        master = entry_point['master']
        for c in containers:
            if c.host_name == master:
                fabric.cmd([c], '/service/sbin/startnode %s master' % cmd)
            else:
                fabric.cmd([c], '/service/sbin/startnode %s slave' % cmd)

        # Now wait a couple seconds to make sure
        # everything has started.
        time.sleep(5)
    def start_service(self, containers, entry_point, fabric):
        self._execute_service(containers, entry_point, fabric, "start")
    def restart_service(self, containers, entry_point, fabric):
        self._execute_service(containers, entry_point, fabric, "restart")
    def stop_service(self, containers, entry_point, fabric):        
        self._execute_service(containers, entry_point, fabric, "stop")
    """
    Generate a new configuration.
    """
    def _generate_config_dir(self, uuid):
        return 'spark_' + str(uuid)

    """
    Get the ports necessary.
    """
    def get_necessary_ports(self, num_instances):
        return []

    """
    Get the internal ports. 
    """
    def get_exposed_ports(self, num_instances):
        return [SparkConfig.MASTER_PORT,
                SparkConfig.SLAVE_PORT,
                SparkConfig.WEBUI_MASTER,
                SparkConfig.WEBUI_SLAVE]

    def get_total_instances(self, num_instances, layers):
        instances = []

        for i in range(num_instances):
            instances.append('spark')

        return instances

    """
    Generate a new configuration
    Param num Number of instances that need to be configured
    Param image Image type of the instances
    """
    def generate(self, num):
        return SparkConfig(num)

    """
    Generate the core-site configuration for a local filesystem. 
    """
    def _generate_spark_env(self, new_config_dir, master):
        in_file = open(self.template_dir + '/spark_env.sh.template', 'r')
        out_file = open(new_config_dir + '/spark_env.sh', 'w+')

        changes = { "MASTER": master }
        for line in in_file:
            s = Template(line).substitute(changes)
            out_file.write(s)

        in_file.close()
        out_file.close()

        # The Spark env file is a shell script, so should be
        # executable by all. 
        os.chmod(new_config_dir + '/spark_env.sh', 0755)

    """
    Apply the configuration to the instances
    """
    def apply(self, config, containers):
        entry_point = { 'type' : 'spark' }
        config_dirs = []

        new_config_dir = "/tmp/" + self._generate_config_dir(config.uuid)
        try:
            sh.mkdir('-p', new_config_dir)
        except:
            sys.stderr.write('could not create config dir ' + new_config_dir)

        # This file records all instances so that we can
        # generate the hosts file. 
        entry_point['instances'] = []
        for server in containers:
            entry_point['instances'].append([server['data_ip'], server['host_name']])

        if not 'compute' in containers[0]:
            # This is being called as a compute service. 
            slave_file = open(new_config_dir + '/slaves', 'w+')
            entry_point['master'] = containers[0]['host_name']
            entry_point['instances'] = []
            master = entry_point['master']
            for server in containers:
                if server != master:
                    slave_file.write("%s\n" % server['host_name'])
            slave_file.close()
        else:
            # This is being called as a client service. 
            # For the client, also include the host/IP of the compute service. 
            compute = containers[0]['compute'][0]
            master = compute['master']
            entry_point['master'] = master

        # Transfer the configuration. 
        for c in containers:
            config_files = new_config_dir + '/*'
            config_dirs.append([c['container'],
                                config_files, 
                                config.config_directory])

        return config_dirs, entry_point

class SparkConfig(object):
    log_directory = '/service/packages/spark/logs'
    config_directory = '/service/packages/spark/conf'

    MASTER_PORT = 7077
    SLAVE_PORT = 7078
    WEBUI_MASTER = 8080
    WEBUI_SLAVE = 8081

    def __init__(self, num):
        self.num = num
        self.btl_port_min = 0
        self.btl_port_range = 0
        self.oob_port_min = 0
        self.oob_port_range = 0
        self.config_directory = SparkConfig.config_directory
        self.log_directory = SparkConfig.log_directory

########NEW FILE########
__FILENAME__ = titanconfig
# Copyright 2014 OpenCore LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import sh
import sys
from string import Template

class TitanInitializer(object):
    """
    Create a new initializer
    Param user The user login for the git repo
    """
    def __init__(self):
        self.template_dir = None
        self.template_repo = None

        self.container_data_dir = TitanConfig.data_directory 
        self.container_log_dir = TitanConfig.log_directory 

    """
    Generate a new hostname
    """
    def new_host_name(self, instance_id):
        return 'titan' + str(instance_id)

    """
    Start the service on the containers. 
    """
    def _execute_service(self, containers, entry_point, fabric, cmd):
        output = fabric.cmd(containers, '/service/sbin/startnode %s' % cmd)
    def start_service(self, containers, entry_point, fabric):
        self._execute_service(containers, entry_point, fabric, "start")
    def stop_service(self, containers, entry_point, fabric):
        self._execute_service(containers, entry_point, fabric, "stop")

    def _generate_config_dir(self, uuid, container):
        return 'titan_' + str(uuid) + '_' + str(container['data_ip'])

    """
    Get the ports necessary. 
    """
    def get_necessary_ports(self, num_instances):
        return []
    """
    Get the internal ports. 
    """
    def get_exposed_ports(self, num_instances):
        return [TitanConfig.REXSTER_PORT, 
                TitanConfig.REXPRO_PORT, 
                TitanConfig.THRIFT_COM_PORT,
                TitanConfig.NATIVE_COM_PORT]

    """
    Generate a new configuration
    """
    def generate(self, num):
        return TitanConfig(num)

    def _apply_rexster(self, host_dir, storage_entry, container):
        in_file = open(self.template_dir + '/rexster.xml.template', 'r')
        out_file = open(host_dir + '/rexster.xml', 'w+')
        changes = { "GRAPH_BACKEND":storage_entry['type'], 
                    "GRAPH_HOST":storage_entry['ip'],
                    "GRAPH_NAME":container['args']['db'],
                    "IP":container['data_ip']}
        for line in in_file:
            s = Template(line).substitute(changes)
            out_file.write(s)
        out_file.close()
        in_file.close()

    def _apply_titan(self, host_dir, storage_entry, container):
        in_file = open(self.template_dir + '/titan.properties', 'r')
        out_file = open(host_dir + '/titan.properties', 'w+')
        changes = { "BACKEND":"cassandrathrift", 
                    "DB":container['args']['db'],
                    "IP":storage_entry['ip']}
        for line in in_file:
            s = Template(line).substitute(changes)
            out_file.write(s)
        out_file.close()
        in_file.close()

    """
    Apply the configuration to the instances
    """
    def apply(self, config, containers, storage_entry=None):
        # The "entry point" is the way to contact the storage service.
        entry_point = { 'type' : 'titan' }
    
        # List all the containers in the entry point so that
        # clients can connect any of them. 
        entry_point['ip'] = str(containers[0]['data_ip'])

        config_dirs = []
        try:
            for c in containers:
                host_dir = "/tmp/" + self._generate_config_dir(config.uuid, c)
                try:
                    sh.mkdir('-p', host_dir)
                except:
                    sys.stderr.write('could not create config dir ' + host_dir)

                self._apply_rexster(host_dir, storage_entry, c)
                self._apply_titan(host_dir, storage_entry, c)

                # The config dirs specifies what to transfer over. We want to 
                # transfer over specific files into a directory. 
                config_dirs.append([c['container'], 
                                    host_dir + '/*', 
                                    config.config_directory])

        except IOError as err:
            sys.stderr.write('' + str(err))

        return config_dirs, entry_point

class TitanConfig(object):
    data_directory = '/service/data/main'
    log_directory = '/service/data/logs'

    REXSTER_PORT = 8182
    REXPRO_PORT = 8184
    THRIFT_COM_PORT  = 9160
    NATIVE_COM_PORT  = 9042

    def __init__(self, num):
        self.num = num
        self.config_directory = '/service/conf'

########NEW FILE########
__FILENAME__ = mounthelper
import json
import os
import sys
import logging
from subprocess import Popen, PIPE

def mkdir(directory):
    if not os.path.isdir(directory):
        cmd = 'mkdir -p %s' % directory
        output = Popen(cmd, stdout=PIPE, shell=True).stdout.read()
        logging.info(cmd)
        logging.info(output)

def mount(entry_point, mount_point):
    # Check if the mount point exists. If not
    # go ahead and create it. 
    # mount -t glusterfs entry_point mount_point
    cmd = 'mount -t glusterfs %s %s' % (entry_point,
                                        mount_point)
    output = Popen(cmd, stdout=PIPE, shell=True).stdout.read()
    logging.info(cmd)
    logging.info(output)

def umount(mount_point):
    cmd = 'umount %s' % mount_point
    output = Popen(cmd, stdout=PIPE, shell=True).stdout.read()
    logging.info(cmd)
    logging.info(output)

cmd = sys.argv[1]
if cmd == "mount":
    entry = sys.argv[2]
    mkdir('/service/data')
    mount(entry, '/service/data')
elif cmd == "umount":
    umount('/service/data')

########NEW FILE########
__FILENAME__ = mounthelper
import json
import os
import sys
import logging
from subprocess import Popen, PIPE

def mkdir(directory):
    if not os.path.isdir(directory):
        cmd = 'mkdir -p %s' % directory
        output = Popen(cmd, stdout=PIPE, shell=True).stdout.read()
        logging.info(cmd)
        logging.info(output)

def mount(entry_point, mount_point):
    # Check if the mount point exists. If not
    # go ahead and create it. 
    # mount -t glusterfs entry_point mount_point
    cmd = 'mount -t glusterfs %s %s' % (entry_point,
                                        mount_point)
    output = Popen(cmd, stdout=PIPE, shell=True).stdout.read()
    logging.info(cmd)
    logging.info(output)
    print output

def umount(mount_point):
    cmd = 'umount %s' % mount_point
    output = Popen(cmd, stdout=PIPE, shell=True).stdout.read()
    logging.info(cmd)
    logging.info(output)

cmd = sys.argv[1]
if cmd == "mount":
    entry = sys.argv[2]
    mkdir('/service/data')
    mount(entry, '/service/data')
elif cmd == "umount":
    umount('/service/data')

########NEW FILE########
__FILENAME__ = mounthelper
import json
import os
import sys
import logging
from subprocess import Popen, PIPE

def mkdir(directory):
    if not os.path.isdir(directory):
        cmd = 'mkdir -p %s' % directory
        output = Popen(cmd, stdout=PIPE, shell=True).stdout.read()
        logging.info(cmd)
        logging.info(output)

def mount(entry_point, mount_point):
    # Check if the mount point exists. If not
    # go ahead and create it. 
    cmd = 'mount -t glusterfs %s %s' % (entry_point,
                                        mount_point)
    output = Popen(cmd, stdout=PIPE, shell=True).stdout.read()
    logging.info(cmd)
    logging.info(output)

def umount(mount_point):
    cmd = 'umount %s' % mount_point
    output = Popen(cmd, stdout=PIPE, shell=True).stdout.read()
    logging.info(cmd)
    logging.info(output)

cmd = sys.argv[1]
if cmd == "mount":
    entry = sys.argv[2]
    mount(entry, '/service/data')
elif cmd == "umount":
    umount('/service/data')

########NEW FILE########
__FILENAME__ = classification
import os
import os.path
import sys
from pyspark import SparkContext
from pyspark.mllib.classification import LogisticRegressionWithSGD
from numpy import array

if __name__ == "__main__":
    data_file = '/spark/data/svm.txt'

    if len(sys.argv) == 1:
        print >> sys.stderr, "Usage: classification.py <master>"
        exit(-1)
    else:
        # Load and parse the data
        sc = SparkContext(sys.argv[1], "Binary Classification")
        data = sc.textFile(data_file)
        parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))
        model = LogisticRegressionWithSGD.train(parsedData)

        # Build the model
        labelsAndPreds = parsedData.map(lambda point: (int(point.item(0)),
                                                       model.predict(point.take(range(1, point.size)))))

        # Evaluating the model on training data
        trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(parsedData.count())
        print("Training Error = " + str(trainErr))

########NEW FILE########
__FILENAME__ = clustering
import os
import os.path
import sys
from pyspark import SparkContext
from pyspark.mllib.clustering import KMeans
from numpy import array
from math import sqrt

# Evaluate clustering by computing Within Set Sum of Squared Errors
def error(point):
    center = clusters.centers[clusters.predict(point)]
    return sqrt(sum([x**2 for x in (point - center)]))

if __name__ == "__main__":
    data_file = '/spark/data/kmeans.txt'

    if len(sys.argv) == 1:
        print >> sys.stderr, "Usage: filtering.py <master>"
        exit(-1)
    else:
        # Load and parse the data
        sc = SparkContext(sys.argv[1], "KMeans Clustering")
        data = sc.textFile(data_file)
        parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))

        # Build the model (cluster the data)
        clusters = KMeans.train(parsedData, 2, maxIterations=10,
                                runs=30, initialization_mode="random")


        WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)
        print("Within Set Sum of Squared Error = " + str(WSSSE))

########NEW FILE########
__FILENAME__ = filtering
import os
import os.path
import sys
from pyspark import SparkContext
from pyspark.mllib.recommendation import ALS
from numpy import array

if __name__ == "__main__":
    data_file = '/spark/data/als.data'

    if len(sys.argv) == 1:
        print >> sys.stderr, "Usage: filtering.py <master>"
        exit(-1)
    else:
        sc = SparkContext(sys.argv[1], "Collaborative Filtering")
        data = sc.textFile(data_file)
        ratings = data.map(lambda line: array([float(x) for x in line.split(',')]))

        # Build the recommendation model using Alternating Least Squares
        model = ALS.train(ratings, 1, 20)

        # Evaluate the model on training data
        testdata = ratings.map(lambda p: (int(p[0]), int(p[1])))
        predictions = model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))
        ratesAndPreds = ratings.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)
        MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).reduce(lambda x, y: x + y)/ratesAndPreds.count()
        print("Mean Squared Error = " + str(MSE))

########NEW FILE########
__FILENAME__ = mounthelper
import json
import os
import sys
import logging
from subprocess import Popen, PIPE

def mkdir(directory):
    if not os.path.isdir(directory):
        cmd = 'mkdir -p %s' % directory
        output = Popen(cmd, stdout=PIPE, shell=True).stdout.read()
        logging.info(cmd)
        logging.info(output)

def mount(entry_point, mount_point):
    # Check if the mount point exists. If not
    # go ahead and create it. 
    # mount -t glusterfs entry_point mount_point
    cmd = 'mount -t glusterfs %s %s' % (entry_point,
                                        mount_point)
    output = Popen(cmd, stdout=PIPE, shell=True).stdout.read()
    logging.info(cmd)
    logging.info(output)
    print output

def umount(mount_point):
    cmd = 'umount %s' % mount_point
    output = Popen(cmd, stdout=PIPE, shell=True).stdout.read()
    logging.info(cmd)
    logging.info(output)

cmd = sys.argv[1]
if cmd == "mount":
    entry = sys.argv[2]
    mkdir('/service/data')
    mount(entry, '/service/data')
elif cmd == "umount":
    umount('/service/data')

########NEW FILE########
__FILENAME__ = regression
import os
import os.path
import sys
from pyspark import SparkContext
from pyspark.mllib.regression import LinearRegressionWithSGD
from numpy import array

if __name__ == "__main__":
    data_file = '/spark/data/lpsa.data'

    if len(sys.argv) == 1:
        print >> sys.stderr, "Usage: regression.py <master>"
        exit(-1)
    else:
        # Load and parse the data
        sc = SparkContext(sys.argv[1], "Logistic Regression")
        data = sc.textFile(data_file)
        parsedData = data.map(lambda line: array([float(x) for x in line.replace(',', ' ').split(' ')]))

        # Build the model
        model = LinearRegressionWithSGD.train(parsedData)

        # Evaluate the model on training data
        valuesAndPreds = parsedData.map(lambda point: (point.item(0),
                                                       model.predict(point.take(range(1, point.size)))))
        MSE = valuesAndPreds.map(lambda (v, p): (v - p)**2).reduce(lambda x, y: x + y)/valuesAndPreds.count()
        print("Mean Squared Error = " + str(MSE))

########NEW FILE########
__FILENAME__ = simpleapp
import os
import os.path
from pyspark import SparkContext

if 'BACKEND_COMPUTE_MASTER' in os.environ:
    master = os.environ['BACKEND_COMPUTE_MASTER']
else:
    master = 'localhost'

logFile = os.path.join("/tmp/data/README.md")

from pyspark import SparkConf, SparkContext
conf = SparkConf()
conf.setMaster('spark://' + master + ':7077')
conf.setAppName("simpleapp")
sc = SparkContext(conf = conf)
logData = sc.textFile(logFile).cache()

numAs = logData.filter(lambda s: 'a' in s).count()
numBs = logData.filter(lambda s: 'b' in s).count()

print "Lines with a: %i, lines with b: %i" % (numAs, numBs)

########NEW FILE########
__FILENAME__ = local
# Copyright 2014 OpenCore LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import os
import json
import uuid
import datetime
import time
import logging
from ferry.docker.fabric import DockerFabric
from ferry.docker.docker import DockerInstance
from pymongo import MongoClient
from pymongo.errors import ConnectionFailure

class LocalDeploy(object):
    def __init__(self, docker):
        self.type = 'local'
        self.docker = docker
        self._try_connect()

    """
    Try connecting to mongo.
    """
    def _try_connect(self):
        for i in range(0, 3):
            try:
                self.mongo = MongoClient(os.environ['MONGODB'], 27017, connectTimeoutMS=6000)
                self.deploy_collection = self.mongo['state']['deploy']
                break
            except ConnectionFailure:
                time.sleep(2)

    """
    Generate a new deployment UUID
    """
    def _new_deploy_uuid(self, cluster_uuid):
        return "dp-%s-%s" % (cluster_uuid, str(uuid.uuid4()))

    """
    Deploy the containers. This is a local version that
    uses either the local registry or a private registry. 
    """
    def deploy(self, cluster_uuid, containers, conf=None):
        # Deploy the containers. 
        deployed = self.docker.deploy(containers)

        # Register the containers.
        deploy_uuid = self._new_deploy_uuid(cluster_uuid)
        deploy_ts = datetime.datetime.now()
        deploy_state = { 'ts' : deploy_ts, 
                         'uuid' : deploy_uuid,
                         'cluster_uuid' : cluster_uuid,
                         'connectors' : deployed }
        self.deploy_collection.insert( deploy_state )

    """
    Find the deployed application. 
    """
    def find(self, one=False, spec=None, conf=None):
        if one:
            return self.deploy_collection.find_one(spec)
        else:
            return self.deploy_collection.find(spec)

########NEW FILE########
__FILENAME__ = remote
# Copyright 2014 OpenCore LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import os
import json
import uuid
import datetime
import logging
from pymongo import MongoClient
from ferry.docker.fabric import DockerFabric
from ferry.docker.docker import DockerInstance

class RemoteDeploy(object):
    def __init__(self, docker):
        self.type = 'remote'
        self.docker = docker
        self.mongo = None
        self.registry = None

    """
    Generate a new deployment UUID
    """
    def _new_deploy_uuid(self, cluster_uuid):
        return "dp-%s-%s" % (cluster_uuid, str(uuid.uuid4()))

    """
    Try connecting to mongo.
    """
    def _try_connect(self, mongo_url, mongo_port):
        for i in range(0, 3):
            try:
                self.mongo = MongoClient(mongo_url, 
                                         mongo_port, 
                                         connectTimeoutMS=6000)
                self.deploy_collection = self.mongo['state']['deploy']
                break
            except ConnectionFailure:
                time.sleep(2)

    """
    Initialize the registry and mongo information.
    """
    def _init_db(self, mongo, registry):
        self.registry = registry
        mongo_info = mongo.split(":")
        self._try_connect(mongo_info[0], 
                          int(mongo_info[1]))

    """
    Deploy the containers. This version uploads the images to a
    remote docker registry and a remote Mongo DB. 
    """
    def deploy(self, cluster_uuid, containers, conf=None):
        if conf:
            if 'registry' in conf and 'mongo' in conf:
                self._init_db(conf['mongo'], conf['registry'])

        if mongo and registry:
            # Deploy the containers. 
            deployed = self.docker.deploy(containers, self.registry)

            # Register the containers.
            deploy_uuid = self._new_deploy_uuid(cluster_uuid)
            deploy_ts = datetime.datetime.now()
            deploy_state = { 'ts' : deploy_ts, 
                             'uuid' : deploy_uuid,
                             'cluster_uuid' : cluster_uuid,
                             'connectors' : deployed }
            self.deploy_collection.insert( deploy_state )

    """
    Find the deployed application. 
    """
    def find(self, one=False, spec=None, conf=None):
        if conf:
            if 'registry' in conf and 'mongo' in conf:
                self._init_db(conf['mongo'], conf['registry'])

            if one:
                return self.deploy_collection.find_one(spec)
            else:
                return self.deploy_collection.find(spec)

########NEW FILE########
__FILENAME__ = configfactory
# Copyright 2014 OpenCore LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import os
import logging
from pymongo import MongoClient
from ferry.install import FERRY_HOME
from ferry.docker.docker import DockerInstance
from ferry.docker.fabric import DockerFabric
from ferry.config.gluster.glusterconfig     import *
from ferry.config.hadoop.hadoopconfig       import *
from ferry.config.hadoop.hadoopclientconfig import *
from ferry.config.hadoop.metastore          import *
from ferry.config.spark.sparkconfig         import *
from ferry.config.spark.sparkclientconfig   import *
from ferry.config.openmpi.mpiconfig         import *
from ferry.config.openmpi.mpiclientconfig   import *
from ferry.config.titan.titanconfig         import *
from ferry.config.cassandra.cassandraconfig import *
from ferry.config.cassandra.cassandraclientconfig import *

class ConfigFactory(object):
    def __init__(self):
        # Storage configuration tools
        self.gluster = GlusterInitializer()
        self.hadoop = HadoopInitializer()
        self.yarn = HadoopInitializer()
        self.hive = MetaStoreInitializer()
        self.spark = SparkInitializer()
        self.cassandra = CassandraInitializer()
        self.titan = TitanInitializer()
        self.mpi = OpenMPIInitializer()
        self.cassandra_client = CassandraClientInitializer()
        self.mpi_client = OpenMPIClientInitializer()
        self.hadoop_client = HadoopClientInitializer()
        self.spark_client = SparkClientInitializer()

        # Get the Ferry home to find the templates.
        template_dir = FERRY_HOME + '/data/templates'
        self.hadoop.template_dir =        template_dir + '/hadoop/'
        self.yarn.template_dir =          template_dir + '/hadoop/'
        self.spark.template_dir =         template_dir + '/spark/'
        self.spark_client.template_dir =  template_dir + '/spark/'
        self.hadoop_client.template_dir = template_dir + '/hadoop/'
        self.hive.template_dir =          template_dir + '/hive-metastore/'
        self.gluster.template_dir =       template_dir + '/gluster/'
        self.cassandra.template_dir =     template_dir + '/cassandra/'
        self.titan.template_dir =         template_dir + '/titan/'
        self.cassandra_client.template_dir =   template_dir + '/cassandra/'
        self.mpi.template_dir =           template_dir + '/openmpi/'
        self.mpi_client.template_dir =    template_dir + '/openmpi/'

    """
    Helper method to generate and copy over the configuration. 
    """
    def _generate_configuration(self, uuid, container_info, config_factory):
        config = config_factory.generate(len(container_info))
        config.uuid = uuid
        return config_factory.apply(config, container_info)

    """
    Generagte a compute-specific configuration. This configuration
    lives in its own directory that gets copied in each container. 
    """
    def generate_compute_configuration(self, 
                                       uuid,
                                       containers,
                                       service,
                                       args, 
                                       storage_info):
        container_info = []
        for c in containers:
            s = {'data_dev':'eth0', 
                 'data_ip':c.internal_ip, 
                 'manage_ip':c.internal_ip,
                 'host_name':c.host_name,
                 'type':c.service_type}
            s['container'] = c
            s['storage'] = storage_info
            s['args'] = args

            container_info.append(s)

        return self._generate_configuration(uuid, 
                                                container_info, 
                                            service)

    """
    Generagte a storage-specific configuration. This configuration
    lives in its own directory that gets copied in each container. 
    """
    def generate_storage_configuration(self, 
                                       uuid,
                                       containers,
                                       service, 
                                       args=None):
        container_info = []
        for c in containers:
            s = {'data_dev':'eth0', 
                 'data_ip':c.internal_ip, 
                 'manage_ip':c.internal_ip,
                 'host_name':c.host_name,
                 'type':c.service_type}
            s['container'] = c
            s['args'] = args

            # Specify the data volume. There should only be one. 
            for v in c.volumes.keys():
                s['ebs_block'] = c.volumes[v]

            container_info.append(s)
        return self._generate_configuration(uuid, container_info, service) 
    """
    Generate a connector specific configuration. 
    """
    def generate_connector_configuration(self, 
                                         uuid,
                                         containers,
                                         service, 
                                         storage_info=None,
                                         compute_info=None,
                                         args=None):
        container_info = []
        for c in containers:
            s = {'data_dev':'eth0', 
                 'data_ip':c.internal_ip, 
                 'manage_ip':c.internal_ip,
                 'host_name':c.host_name}
            s['container'] = c
            s['args'] = args

            # Specify the entry point
            s['storage'] = storage_info
            s['compute'] = compute_info
            container_info.append(s)
        return self._generate_configuration(uuid, container_info, service) 

    """
    Helper method to generate some environment variables. 
    """
    def _generate_key_value(self,
                            json_data,
                            base_key):
        env = {}
        if type(json_data) is list:
            for j in json_data:
                values = self._generate_key_value(j, base_key)
                env = dict(env.items() + values.items())
        else:
            for k in json_data.keys():
                if type(json_data[k]) is unicode:
                    key = "%s_%s" % (base_key, k.upper())
                    value = json_data[k]
                    env[key] = value
                elif type(json_data[k]) is dict:
                    values = self._generate_key_value(json_data[k],
                                                      base_key + "_LAYER")
                    env = dict(env.items() + values.items())
        return env

    """
    Generate some environment variables for the connectors. 
    These variables help the connectors query the backend. 
    """
    def generate_env_vars(self,
                          storage_info=None,
                          compute_info=None):
        storage_values = {}
        compute_values = {}
        if storage_info:
            storage_values = self._generate_key_value(storage_info[0],
                                              "BACKEND_STORAGE")
        if compute_info:
            compute_values = self._generate_key_value(compute_info[0],
                                              "BACKEND_COMPUTE")
        return dict(storage_values.items() + compute_values.items())

########NEW FILE########
__FILENAME__ = deploy
# Copyright 2014 OpenCore LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import os
import json
import uuid
import datetime
import logging
import importlib
import inspect
from pymongo import MongoClient
from ferry.docker.fabric import DockerFabric
from ferry.docker.docker import DockerInstance

class DeployEngine(object):
    def __init__(self, docker):
        self.docker = docker
        self.engines = {}
        self._load_engines()

    """
    Deploy the containers. This is a local version that
    uses either the local registry or a private registry. 
    """
    def deploy(self, cluster_uuid, containers, conf=None):
        if conf['_mode'] in self.engines:
            engine = self.engines[conf['_mode']]
            engine.deploy(cluster_uuid, containers, conf)

    """
    Dynamically load all the deployment engines
    """
    def _load_engines(self):
        engine_dir = os.environ['FERRY_HOME'] + '/deploy'
        files = os.listdir(engine_dir)
        for f in files:
            p = engine_dir + os.sep + f
            c = self._load_class(p)
            if c:
                self.engines[c.type] = c

    """
    Dynamically load a class from a string
    """
    def _load_class(self, full_name):
        class_info = full_name.split("/")[-1].split(".")
        module_name = class_info[0]
        file_extension = class_info[1]

        # Ignore the init and compiled files. 
        if module_name == "__init__" or file_extension == "pyc":
            return None

        # Construct the full module path. This lets us filter out only
        # the deployment engines and ignore all the other imported packages. 
        module_path = "ferry.deploy.%s" % module_name
        module = importlib.import_module(module_path)
        for n, o in inspect.getmembers(module):
            if inspect.isclass(o):
                if o.__module__ == module_path:
                    return o(self.docker)
        return None

    """
    Find the deployed application. 
    """
    def find(self, one=False, spec=None, conf=None):
        all_engines = []
        if conf and conf['_mode'] in self.engines:
            all_engines.append(self.engines[conf['_mode']])
        else:
            all_engines = self.engines.values()

        all_v = []
        for e in all_engines:
            v = e.find(one, spec, conf)
            if v and one:
                return v
            elif v:
                all_v.append(v)
                
        if one:
            return None
        else:
            return all_v

########NEW FILE########
__FILENAME__ = docker
# Copyright 2014 OpenCore LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import os
import json
import logging
import re
from subprocess import Popen, PIPE

DOCKER_SOCK='unix:////var/run/ferry.sock'

""" Docker instance """
class DockerInstance(object):
    def __init__(self, json_data=None):
        if not json_data:
            self.container = ''
            self.service_type = None
            self.host_name = None
            self.external_ip = None
            self.internal_ip = None
            self.ports = {}
            self.image = ''
            self.keys = None
            self.volumes = None
            self.default_user = None
            self.name = None
            self.args = None
        else:
            self.container = json_data['container']
            self.service_type = json_data['type']
            self.host_name = json_data['hostname']
            self.external_ip = json_data['external_ip']
            self.internal_ip = json_data['internal_ip']
            self.ports = json_data['ports']
            self.image = json_data['image']
            self.default_user = json_data['user']
            self.name = json_data['name']
            self.args = json_data['args']
            self.keys = json_data['keys']
            self.volumes = json_data['volumes']

    """
    Return in JSON format. 
    """
    def json(self):
        json_reply = { '_type' : 'docker',
                       'external_ip' : self.external_ip,
                       'internal_ip' : self.internal_ip,
                       'ports' : self.ports,
                       'hostname' : self.host_name,
                       'container' : self.container,
                       'image' : self.image,
                       'type': self.service_type, 
                       'keys' : self.keys,
                       'volumes' : self.volumes,
                       'user' : self.default_user,
                       'name' : self.name,
                       'args' : self.args }
        return json_reply


""" Alternative API for Docker that uses external commands """
class DockerCLI(object):
    def __init__(self):
        self.docker = 'docker-ferry -H=' + DOCKER_SOCK
        self.version_cmd = 'version'
        self.start_cmd = 'start'
        self.run_cmd = 'run -privileged'
        self.build_cmd = 'build -privileged'
        self.inspect_cmd = 'inspect'
        self.images_cmd = 'images'
        self.commit_cmd = 'commit'
        self.push_cmd = 'push'
        self.stop_cmd = 'stop'
        self.tag_cmd = 'tag'
        self.rm_cmd = 'rm'
        self.ps_cmd = 'ps'
        self.info_cmd = 'info'
        self.daemon = '-d'
        self.interactive = '-i'
        self.tty = '-t'
        self.port_flag = ' -p'
        self.expose_flag = ' -expose'
        self.volume_flag = ' -v'
        self.lxc_flag = ' -lxc-conf'
        self.disable_net = ' -n=false'
        self.host_flag = ' -h'
        self.fs_flag = ' -s'

    """
    Get the backend driver docker is using. 
    """
    def get_fs_type(self):
        cmd = self.docker + ' ' + self.info_cmd + ' | grep Driver | awk \'{print $2}\''
        logging.warning(cmd)

        output = Popen(cmd, stdout=PIPE, shell=True).stdout.read()
        return output.strip()
        
    """
    Fetch the current docker version.
    """
    def version(self):
        cmd = self.docker + ' ' + self.version_cmd + ' | grep Client | awk \'{print $3}\''
        logging.warning(cmd)

        output = Popen(cmd, stdout=PIPE, shell=True).stdout.read()
        return output.strip()
        
    """
    List all the containers. 
    """
    def list(self):
        cmd = self.docker + ' ' + self.ps_cmd + ' -q' 
        logging.warning(cmd)

        output = Popen(cmd, stdout=PIPE, shell=True).stdout.read()
        output = output.strip()

        # There is a container ID for each line
        return output.split()

    """
    List all images that match the image name
    """
    def images(self, image_name=None):
        if not image_name:
            cmd = self.docker + ' ' + self.images_cmd + ' | awk \'{print $1}\''
            output = Popen(cmd, stdout=PIPE, shell=True).stdout.read()
        else:
            cmd = self.docker + ' ' + self.images_cmd + ' | awk \'{print $1}\'' + ' | grep ' + image_name
            output = Popen(cmd, stdout=PIPE, shell=True).stdout.read()

        logging.warning(cmd)
        return output.strip()
    """
    Build a new image from a Dockerfile
    """
    def build(self, image, docker_file=None):
        path = '.'
        if docker_file != None:
            path = docker_file

        cmd = self.docker + ' ' + self.build_cmd + ' -t %s %s' % (image, path)
        logging.warning(cmd)
        output = Popen(cmd, stdout=PIPE, shell=True).stdout.read()

    def _get_default_run(self, container):
        cmd = self.docker + ' ' + self.inspect_cmd + ' ' + container.container
        logging.warning(cmd)

        output = Popen(cmd, stdout=PIPE, shell=True).stdout.read()
        data = json.loads(output.strip())
        
        cmd = data[0]['Config']['Cmd']
        return json.dumps( {'Cmd' : cmd} )

    """
    Push an image to a remote registry.
    """
    def push(self, container, registry):
        raw_image_name = container.image.split("/")[1]
        new_image = "%s/%s" % (registry, raw_image_name)

        tag = self.docker + ' ' + self.tag_cmd + ' ' + container.image + ' ' + new_image
        push = self.docker + ' ' + self.push_cmd + ' ' + new_image
        logging.warning(tag)
        logging.warning(push)
        
        Popen(tag, stdout=PIPE, shell=True).stdout.read()
        Popen(push, stdout=PIPE, shell=True).stdout.read()

    """
    Commit a container
    """
    def commit(self, container, snapshot_name):
        default_run = self._get_default_run(container)
        run_cmd = "-run='%s'" % default_run

        # Construct a new container using the given snapshot name. 
        cmd = self.docker + ' ' + self.commit_cmd + ' ' + run_cmd + ' ' + container.container + ' ' + snapshot_name
        logging.warning(cmd)
        output = Popen(cmd, stdout=PIPE, shell=True).stdout.read()

    """
    Stop a running container
    """
    def stop(self, container):
        cmd = self.docker + ' ' + self.stop_cmd + ' ' + container
        logging.warning(cmd)
        output = Popen(cmd, stdout=PIPE, shell=True).stdout.read()

    """
    Remove a container
    """
    def remove(self, container):
        cmd = self.docker + ' ' + self.rm_cmd + ' ' + container
        logging.warning(cmd)
        output = Popen(cmd, stdout=PIPE, shell=True).stdout.read()

    """
    Start a stopped container. 
    """
    def start(self, container, service_type, keys, volumes, args):
        cmd = self.docker + ' ' + self.start_cmd + ' ' + container
        logging.warning(cmd)
        output = Popen(cmd, stdout=PIPE, shell=True).stdout.read()

        # Now parse the output to get the IP and port
        container = output.strip()
        return self.inspect(container = container, 
                            keys = keys,
                            volumes = volumes,
                            service_type = service_type, 
                            args = args)

    """
    Run a command in a virtualized container
    The Docker allocator will ignore subnet, volumes, instance_name, and key
    information since everything runs locally. 
    """
    def run(self, service_type, image, volumes, keys, open_ports, host_map=None, expose_group=None, hostname=None, default_cmd=None, args=None, lxc_opts=None):
        flags = self.daemon 

        # Specify the hostname (this is optional)
        if hostname != None:
            flags += self.host_flag
            flags += ' %s ' % hostname

        # Add the port value if provided a valid port. 
        if expose_group != None and len(expose_group) > 0:
            for p in expose_group:
                flags += self.expose_flag
                flags += ' %s' % str(p)

        # Add all the bind mounts
        if volumes != None:
            for v in volumes.keys():
                flags += self.volume_flag
                flags += ' %s:%s' % (v, volumes[v])

        # Add the key directory
        if keys != None:
            for v in keys.keys():
                flags += self.volume_flag
                # flags += ' %s:%s' % (v, keys[v])
                flags += ' %s:%s' % (keys[v], v)

        # Add the lxc options
        if lxc_opts != None:
            flags += self.disable_net
            for o in lxc_opts:
                flags += self.lxc_flag
                flags += ' \"%s\"' % o

        if not default_cmd:
            default_cmd = ''

        # Now construct the final docker command. 
        cmd = self.docker + ' ' + self.run_cmd + ' ' + flags + ' ' + image + ' ' + default_cmd
        logging.warning(cmd)
        child = Popen(cmd, stdout=PIPE, stderr=PIPE, shell=True)

        err = child.stderr.read().strip()
        if re.compile('[/:\s\w]*Can\'t connect[\'\s\w]*').match(err):
            logging.error("Ferry docker daemon does not appear to be running")
            return None
        elif re.compile('Unable to find image[\'\s\w]*').match(err):
            logging.error("%s not present" % image)
            return None

        container = child.stdout.read().strip()
        return self.inspect(container, keys, volumes, hostname, open_ports, host_map, service_type, args)

    def _get_lxc_net(self, lxc_tuples):
        for l in lxc_tuples:
            if l['Key'] == 'lxc.network.ipv4':
                ip = l['Value'].split('/')[0]
                return ip
        return None

    """
    Inspect a container and return information on how
    to connect to the container. 
    """
    def inspect(self, container, keys=None, volumes=None, hostname=None, open_ports=[], host_map=None, service_type=None, args=None):
        cmd = self.docker + ' ' + self.inspect_cmd + ' ' + container
        logging.warning(cmd)

        output = Popen(cmd, stdout=PIPE, shell=True).stdout.read()

        data = json.loads(output.strip())
        instance = DockerInstance()

        if type(data) is list:
            data = data[0]

        instance.image = data['Config']['Image']
        instance.container = data['ID']
        instance.internal_ip = data['NetworkSettings']['IPAddress']

        # If we've used the lxc config, then the networking information
        # will be located somewhere else. 
        if instance.internal_ip == "":
            instance.internal_ip = self._get_lxc_net(data['HostConfig']['LxcConf'])

        if hostname:
            instance.host_name = hostname
        else:
            # Need to inspect to get the hostname.
            instance.host_name = data['Config']['Hostname']

        instance.service_type = service_type
        instance.args = args

        if len(open_ports) == 0:
            port_mapping = data['HostConfig']['PortBindings']
            if port_mapping:
                instance.ports = port_mapping
        else:
            for p in open_ports:
                if host_map and p in host_map:
                    instance.ports[p] = host_map[p]
                else:
                    instance.ports[p] = []

        # Add any data volume information. 
        if volumes:
            instance.volumes = volumes
        else:
            instance.volumes = data['Volumes']

        if keys:
            instance.keys = keys
        return instance

########NEW FILE########
__FILENAME__ = fabric
# Copyright 2014 OpenCore LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import ferry.install
import json
import logging
import time
from subprocess import Popen, PIPE
from ferry.docker.docker import DockerCLI
from ferry.ip.client import DHCPClient

"""
Allocate local docker instances
"""
class DockerFabric(object):
    def __init__(self):
        self.repo = 'public'
        self.docker_user = 'root'
        self.cli = DockerCLI()
        self.network = DHCPClient(self._get_gateway())

    def _get_gateway(self):
        cmd = "LC_MESSAGES=C ifconfig drydock0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}'"
        gw = Popen(cmd, stdout=PIPE, shell=True).stdout.read().strip()

        cmd = "LC_MESSAGES=C ifconfig drydock0 | grep 'inet addr:' | cut -d: -f4 | awk '{ print $1}'"
        netmask = Popen(cmd, stdout=PIPE, shell=True).stdout.read().strip()
        mask = map(int, netmask.split("."))
        cidr = 1
        if mask[3] == 0:
            cidr = 8
        if mask[2] == 0:
            cidr *= 2

        return "%s/%d" % (gw, 32 - cidr)

    def _get_host(self):
        cmd = "ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}'"
        return Popen(cmd, stdout=PIPE, shell=True).stdout.read().strip()

    """
    Read the location of the directory containing the keys
    used to communicate with the containers. 
    """
    def _read_key_dir(self):
        f = open(ferry.install.DEFAULT_DOCKER_KEY, 'r')
        k = f.read().strip().split("://")
        return k[1], k[0]
 
    """
    Fetch the current docker version.
    """
    def version(self):
        return self.cli.version()

    """
    Get the filesystem type associated with docker. 
    """
    def get_fs_type(self):
        return self.cli.get_fs_type()

    """
    Restart the stopped containers.
    """
    def restart(self, containers):
        new_containers = []
        for c in containers:
            container = self.cli.start(c.container,
                                       c.service_type,
                                       c.keys,
                                       c.volumes,
                                       c.args)
            container.default_user = self.docker_user
            new_containers.append(container)

        # We should wait for a second to let the ssh server start
        # on the containers (otherwise sometimes we get a connection refused)
        time.sleep(2)
        return new_containers

    """
    Allocate several instances.
    """
    def alloc(self, container_info):
        containers = []
        mounts = {}
        for c in container_info:
            # Get a new IP address for this container and construct
            # a default command. 
            ip = self.network.assign_ip(c)
            gw = self._get_gateway().split("/")[0]

            lxc_opts = ["lxc.network.type = veth",
                        "lxc.network.ipv4 = %s/24" % ip, 
                        "lxc.network.ipv4.gateway = %s" % gw,
                        "lxc.network.link = drydock0",
                        "lxc.network.name = eth0",
                        "lxc.network.flags = up"]

            c['default_cmd'] = "/service/sbin/startnode init"

            # Check if we need to forward any ports. 
            host_map = {}
            for p in c['ports']:
                p = str(p)
                s = p.split(":")
                if len(s) > 1:
                    host = s[0]
                    dest = s[1]
                else:
                    host = self.network.random_port()
                    dest = s[0]
                host_map[dest] = [{'HostIp' : '0.0.0.0',
                                   'HostPort' : host}]
                self.network.forward_rule('0.0.0.0/0', host, ip, dest)

            # Start a container with a specific image, in daemon mode,
            # without TTY, and on a specific port
            container = self.cli.run(service_type = c['type'], 
                                     image = c['image'], 
                                     volumes = c['volumes'],
                                     keys = c['keys'], 
                                     open_ports = host_map.keys(),
                                     host_map = host_map, 
                                     expose_group = c['exposed'], 
                                     hostname = c['hostname'],
                                     default_cmd = c['default_cmd'],
                                     args= c['args'],
                                     lxc_opts = lxc_opts)
            if container:
                container.default_user = self.docker_user
                container.internal_ip = ip
                containers.append(container)
                self.network.set_owner(ip, container.container)

                if 'name' in c:
                    container.name = c['name']

                if 'volume_user' in c:
                    mounts[container] = {'user':c['volume_user'],
                                         'vols':c['volumes'].items()}

                # We should wait for a second to let the ssh server start
                # on the containers (otherwise sometimes we get a connection refused)
                time.sleep(2)

                # Check if we need to set the file permissions
                # for the mounted volumes. 
                for c, i in mounts.items():
                    for _, v in i['vols']:
                        self.cmd([c], 'chown -R %s %s' % (i['user'], v))

        return containers

    def stop(self, containers):
        """
        Forceably stop the running containers
        """
        for c in containers:
            self.cli.stop(c['container'])

    def remove(self, containers):
        """
        Remove the running instances
        """
        for c in containers:
            for p in c.ports.keys():
                self.network.delete_rule(c.internal_ip, p)
            self.network.free_ip(c.internal_ip)
            self.cli.remove(c.container)

    def snapshot(self, containers, cluster_uuid, num_snapshots):
        """
        Save/commit the running instances
        """
        snapshots = []
        for c in containers:
            snapshot_name = '%s-%s-%s:SNAPSHOT-%s' % (c.image, 
                                                      cluster_uuid,
                                                      c.host_name,
                                                      num_snapshots)
            snapshots.append( {'image' : snapshot_name,
                               'base' : c.image,
                               'type' : c.service_type, 
                               'name' : c.name, 
                               'args' : c.args,
                               'ports': c.ports} )
            self.cli.commit(c, snapshot_name)
        return snapshots

    def deploy(self, containers, registry=None):
        """
        Upload these containers to the specified registry.
        """
        deployed = []
        for c in containers:
            image_name = '%s-%s:DEPLOYED' % (c.image, 
                                             c.host_name)
            deployed.append( {'image' : image_name,
                              'base' : c.image,
                              'type' : c.service_type, 
                              'name' : c.name, 
                              'args' : c.args,
                              'ports': c.ports} )
            if not registry:
                self.cli.commit(c, image_name)
            else:
                self.cli.push(c, registry)
        return deployed

    def halt(self, containers):
        """
        Safe stop the containers. 
        """
        cmd = '/service/sbin/startnode halt'
        for c in containers:
            self.cmd_raw(c.internal_ip, cmd)

    def copy(self, containers, from_dir, to_dir):
        """
        Copy over the contents to each container
        """
        for c in containers:
            self.copy_raw(c.internal_ip, from_dir, to_dir)

    def copy_raw(self, ip, from_dir, to_dir):
        keydir, _ = self._read_key_dir()
        opts = '-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null'
        key = '-i ' + keydir + '/id_rsa'
        scp = 'scp ' + opts + ' ' + key + ' -r ' + from_dir + ' ' + self.docker_user + '@' + ip + ':' + to_dir
        logging.warning(scp)
        output = Popen(scp, stdout=PIPE, shell=True).stdout.read()

    def cmd(self, containers, cmd):
        """
        Run a command on all the containers and collect the output. 
        """
        all_output = {}
        for c in containers:
            output = self.cmd_raw(c.internal_ip, cmd)
            all_output[c] = output.strip()
        return all_output

    def cmd_raw(self, ip, cmd):
        keydir, _ = self._read_key_dir()
        key = keydir + '/id_rsa'
        ip = self.docker_user + '@' + ip
        ssh = 'ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i ' + key + ' -t -t ' + ip + ' \'%s\'' % cmd
        logging.warning(ssh)
        output = Popen(ssh, stdout=PIPE, shell=True).stdout.read()
        return output

########NEW FILE########
__FILENAME__ = manager
# Copyright 2014 OpenCore LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
import datetime
import grp
import json
import logging
import os
import os.path
import pwd
import sh
import shutil
import stat
import sys
import time
import uuid
from pymongo import MongoClient
from sets import Set
from ferry.install import *
from ferry.docker.docker        import DockerInstance
from ferry.docker.fabric        import DockerFabric
from ferry.docker.configfactory import ConfigFactory
from ferry.docker.deploy        import DeployEngine

class DockerManager(object):
    SSH_PORT = 22

    def __init__(self):
        # Generate configuration.
        self.config = ConfigFactory()

        # Service mappings
        self.service = {
            'openmpi' : { 
                'server' : self.config.mpi,
                'client' : self.config.mpi_client },
            'yarn': { 
                'server' : self.config.yarn,
                'client' : self.config.hadoop_client },
            'spark': { 
                'server' : self.config.spark,
                'client' : self.config.spark_client },
            'gluster': { 
                'server' : self.config.gluster },
            'cassandra': { 
                'server' : self.config.cassandra,
                'client' : self.config.cassandra_client},
            'titan': { 
                'server' : self.config.titan,
                'client' : self.config.cassandra_client},
            'hadoop': { 
                'server' : self.config.hadoop,
                'client' : self.config.hadoop_client},
            'hive': { 
                'server' : self.config.hadoop,
                'client' : self.config.hadoop_client}
            }

        # Docker tools
        self.docker = DockerFabric()
        self.deploy = DeployEngine(self.docker)

        # Initialize the state. 
        self._init_state_db()
        self._clean_state_db()

    """
    Contact the state database. 
    """
    def _init_state_db(self):
        self.mongo = MongoClient(os.environ['MONGODB'], 27017, connectTimeoutMS=6000)

        self.cluster_collection = self.mongo['state']['clusters']
        self.service_collection = self.mongo['state']['services']
        self.snapshot_collection = self.mongo['state']['snapshots']

    """
    Remove all the services that are "terminated". 
    """
    def _clean_state_db(self):
        self.cluster_collection.remove( {'status':'removed'} )

    def _serialize_containers(self, containers):
        info = []
        for c in containers:
            info.append(c.json())
        return info

    """
    Update the service configuration. 
    """
    def _update_service_configuration(self, service_uuid, service_info):
        service = self.service_collection.find_one( {'uuid':service_uuid} )
        if not service:
            self.service_collection.insert( service_info )
        else:
            self.service_collection.update( {'uuid' : service_uuid},
                                            {'$set': service_info} )
    """
    Get the storage information. 
    """
    def _get_service_configuration(self, service_uuid, detailed=False):
        info = self.service_collection.find_one( {'uuid':service_uuid}, {'_id':False} )
        if info:
            if detailed:
                return info
            else:
                return info['entry']
        else:
            return None

    def _get_inspect_info(self, service_uuid):
        json_reply = {'uuid' : service_uuid}

        # Get the service information. If we can't find it,
        # return an empty reply (this shouldn't happen btw). 
        raw_info = self._get_service_configuration(service_uuid, detailed=True)
        if not raw_info:
            return json_reply

        # Get individual container information
        json_reply['containers'] = []
        for c in raw_info['containers']:
            json_reply['containers'].append(c)

        # Now get the entry information
        json_reply['entry'] = raw_info['entry']

        # Check if this service has a user-defined
        # unique name.
        if 'uniq' in raw_info:
            json_reply['uniq'] = raw_info['uniq']
        else:
            json_reply['uniq'] = None

        return json_reply

    def _get_snapshot_info(self, stack_uuid):
        v = self.cluster_collection.find_one( {'uuid' : stack_uuid} )
        if v:
            s = self.snapshot_collection.find_one( {'snapshot_uuid':v['snapshot_uuid']} )
            if s:
                time = s['snapshot_ts'].strftime("%m/%w/%Y (%I:%M %p)")
                return { 'snapshot_ts' : time,
                         'snapshot_uuid' : v['snapshot_uuid'] }

    def _get_service(self, service_type):
        if service_type in self.service:
            return self.service[service_type]['server']
        else:
            logging.error("unknown service " + service_type)
            return None

    def _get_client_services(self, storage_entry, compute_entry):
        """
        Get a list of all the client types that are needed for the supplied
        storage and compute backends. So, for example, if the user has specified
        a Hadoop backend, then we'll need to supply the Hadoop client, etc. 
        """
        client_services = Set()
        service_names = []
        for storage in storage_entry:
            if storage['type'] in self.service:
                if 'client' in self.service[storage['type']]:
                    client_services.add(self.service[storage['type']]['client'])
                service_names.append(storage['type'])
        for compute in compute_entry:
            if compute['type'] in self.service:
                if 'client' in self.service[compute['type']]:
                    client_services.add(self.service[compute['type']]['client'])
                service_names.append(compute['type'])
        return client_services, service_names

    """
    Helper method to copy directories. shutil fails if the 
    destination already exists. 
    """
    def _copytree(src, dst):
        for item in os.listdir(src):
            s = os.path.join(src, item)
            d = os.path.join(dst, item)
            if os.path.isdir(s):
                shutil.copytree(s, d)
            else:
                shutil.copy2(s, d)

    def _copy_instance_logs(self, instance, to_dir):
        service = self._get_service(instance.service_type)
        log_dir = service.container_log_dir

        # We're performing a reverse lookup. 
        for d in instance.volumes.keys():
            if instance.volumes[d] == log_dir:
                self._copytree(d, to_dir)
                return

    def copy_logs(self, stack_uuid, to_dir):
        _, compute, storage = self._get_cluster_instances(stack_uuid)
        storage_dir = to_dir + '/' + stack_uuid + '/storage'
        compute_dir = to_dir + '/' + stack_uuid + '/compute'

        for c in compute:
            for i in c['instances']:
                self._copy_instance_logs(i, compute_dir)
        for c in storage:
            for i in c['instances']:
                self._copy_instance_logs(i, storage_dir)

    """
    Inspect a deployed stack. 
    """
    def inspect_deployed(self, uuid, registry):
        json_reply = {}

        # Need to inspect the registry to make sure
        # that all the images are available. 

        return json.dumps(json_reply, 
                          sort_keys=True,
                          indent=2,
                          separators=(',',':'))

    """
    Inspect a running stack. 
    """
    def inspect_stack(self, stack_uuid):
        json_reply = {}

        # Get the collection of all backends and connector UUIDS.
        cluster = self.cluster_collection.find_one( {'uuid': stack_uuid} )

        connector_uuids = []
        if cluster and 'connectors' in cluster:
            connector_uuids = cluster['connectors']

        storage_uuids = []
        compute_uuids = []
        if cluster and 'backends' in cluster:
            for b in cluster['backends']['uuids']:
                if b['storage'] != None:
                    storage_uuids.append(b['storage'])

                if b['compute'] != None:
                    for c in b['compute']:
                        compute_uuids.append(c)

        # For each UUID, collect the detailed service information. 
        json_reply['connectors'] = []            
        for uuid in connector_uuids:
            json_reply['connectors'].append(self._get_inspect_info(uuid))

        json_reply['storage'] = []
        for uuid in storage_uuids:
            json_reply['storage'].append(self._get_inspect_info(uuid))

        json_reply['compute'] = []
        for uuid in compute_uuids:
            json_reply['compute'].append(self._get_inspect_info(uuid))

        # Now append some snapshot info. 
        json_reply['snapshots'] = self._get_snapshot_info(stack_uuid)

        return json.dumps(json_reply, 
                          sort_keys=True,
                          indent=2,
                          separators=(',',':'))

    """
    Query the available snapshots. 
    """
    def query_snapshots(self, constraints=None):
        json_reply = {}

        values = self.snapshot_collection.find()
        for v in values:
            c = self.cluster_collection.find_one( {'uuid':v['cluster_uuid']} )
            if c:
                time = v['snapshot_ts'].strftime("%m/%w/%Y (%I:%M %p)")
                json_reply[v['snapshot_uuid']] = { 'uuid' : v['snapshot_uuid'],
                                                   'base' : c['base'], 
                                                   'snapshot_ts' : time }
        return json.dumps(json_reply, 
                          sort_keys=True,
                          indent=2,
                          separators=(',',':'))
    
    """
    Query the available stacks. 
    """
    def query_stacks(self, constraints=None):
        json_reply = {}

        if constraints:
            values = self.cluster_collection.find(constraints)
        else:
            values = self.cluster_collection.find()

        for v in values:
            time = ''
            s = self.snapshot_collection.find_one( {'snapshot_uuid':v['snapshot_uuid']} )
            if s:
                time = v['ts'].strftime("%m/%w/%Y (%I:%M %p)")
            json_reply[v['uuid']] = { 'uuid' : v['uuid'],
                                      'base' : v['base'], 
                                      'ts' : time,
                                      'backends' : v['backends']['uuids'],
                                      'connectors': v['connectors'],
                                      'status' : v['status']}
        return json.dumps(json_reply, 
                          sort_keys=True,
                          indent=2,
                          separators=(',',':'))

    """
    Query the deployed applications. 
    """
    def query_deployed(self, conf=None):
        json_reply = {}

        cursors = self.deploy.find(conf=conf)
        for c in cursors:
            for v in c:
                time = v['ts'].strftime("%m/%w/%Y (%I:%M %p)")
                c = self.cluster_collection.find_one( {'uuid':v['cluster_uuid']} )
                
                json_reply[v['uuid']] = { 'uuid' : v['uuid'],
                                          'base' : c['base'], 
                                          'ts' : time,
                                          'backends' : c['backends']['uuids'],
                                          'connectors': c['connectors'],
                                          'status': 'deployed' }
        return json.dumps(json_reply, 
                          sort_keys=True,
                          indent=2,
                          separators=(',',':'))

    """
    Allocate new UUIDs. 
    """
    def _new_service_uuid(self):
        while True:
            longid = str(uuid.uuid4())
            shortid = 'se-' + longid.split('-')[0]
            services = self.service_collection.find_one( {'uuid' : shortid} )
            if not services:
                return shortid

    def _new_stack_uuid(self):
        while True:
            longid = str(uuid.uuid4())
            shortid = 'sa-' + longid.split('-')[0]
            services = self.cluster_collection.find_one( {'uuid' : shortid} )
            if not services:
                return shortid

    def _new_snapshot_uuid(self, cluster_uuid):
        while True:
            longid = str(uuid.uuid4())
            shortid = 'sn-' + longid.split('-')[0]
            services = self.snapshot_collection.find_one( {'snapshot_uuid' : shortid} )
            if not services:
                return shortid

    """
    Determine if the supplied UUID is a valid snapshot. 
    """
    def is_snapshot(self, snapshot_uuid):
        v = self.snapshot_collection.find_one( {'snapshot_uuid':snapshot_uuid} )
        if v:
            return True
        else:
            return False

    """
    Check if the application status
    """
    def _confirm_status(self, uuid, status):
        cluster = self.cluster_collection.find_one( {'uuid':uuid} )
        if cluster:
            return cluster['status'] == status
        return False
    def is_running(self, uuid, conf=None):
        return self._confirm_status(uuid, 'running')
    def is_stopped(self, uuid, conf=None):
        return self._confirm_status(uuid, 'stopped')
    def is_removed(self, uuid, conf=None):
        return self._confirm_status(uuid, 'removed')

    """
    Check if the UUID is of a deployed application. 
    """
    def is_deployed(self, uuid, conf=None):
        v = self.deploy.find( one=True, 
                              spec = {'uuid':uuid},
                              conf = conf )
        if v:
            return True
        else:
            return False

    """
    Get the base image of this cluster. 
    """
    def get_base_image(self, uuid):
        cluster = self.cluster_collection.find_one( {'uuid':uuid} )
        if cluster:
            return cluster['base']
        return None

    """
    Create a new data directory
    """
    def _new_data_dir(self, service_uuid, storage_type, storage_id):
        # Check the location of the scratch directory. If not defined
        # use the current directory
        if 'FERRY_SCRATCH' in os.environ:
            scratch_dir = os.environ['FERRY_SCRATCH'] + '/'
        else:
            scratch_dir = ''

        # First check if this data directory already exists. If so,
        # go ahead and delete it (this will hopefully get rid of all xattr stuff)
        new_dir = scratch_dir + 'tmp/%s/data_%s' % (service_uuid, storage_type + '_' + str(storage_id))
        return self._create_dir(new_dir, replace=True)

    """
    Create a new log directory
    """
    def _new_log_dir(self, service_uuid, storage_type, storage_id, replace = False):
        # Check the location of the scratch directory. If not defined
        # use the current directory
        if 'FERRY_SCRATCH' in os.environ:
            scratch_dir = os.environ['FERRY_SCRATCH'] + '/'
        else:
            scratch_dir = ''

        # First check if this data directory already exists. If so,
        # go ahead and delete it (this will hopefully get rid of all xattr stuff)
        new_dir = scratch_dir + 'tmp/%s/log_%s' % (service_uuid, storage_type + '_' + str(storage_id))
        return self._create_dir(new_dir, replace=replace)

    def _create_dir(self, new_dir, replace=False):
        # See if we need to delete an existing data dir.
        if os.path.exists(new_dir) and replace:
            logging.warning("deleting dir " + new_dir)
            shutil.rmtree(new_dir)

        try:
            # Now create the new directory and assign
            # the right permissions. 
            sh.mkdir('-p', new_dir)
        except:
            logging.warning(new_dir + " already exists")

        try:
            uid, gid = ferry.install._get_ferry_user()
            os.chown(new_dir, uid, gid)
            os.chmod(new_dir, 0774)
        except OSError as e:
            logging.warning("could not change permissions for " + new_dir)

        return os.path.abspath(new_dir)

    def _get_service_environment(self,
                                 service, 
                                 instance, 
                                 num_instances):
        container_dir = service.container_data_dir
        log_dir = service.container_log_dir
        host_name = service.new_host_name(instance)
        ports = service.get_necessary_ports(num_instances)
        exposed = service.get_exposed_ports(num_instances)
        
        # Add SSH port for management purposes. 
        exposed.append(DockerManager.SSH_PORT)

        return container_dir, log_dir, host_name, ports, exposed

    """
    Read the directory containing the key we should use. 
    """
    def _read_key_dir(self):
        with open(ferry.install.DEFAULT_DOCKER_KEY, 'r') as f:
            k = f.read().strip().split("://")
            return { '/service/keys' : k[1]  }

    """
    Prepare the environment for storage containers.
    """
    def _prepare_storage_environment(self, 
                                     service_uuid, 
                                     num_instances, 
                                     storage_type, 
                                     layers,
                                     args = None,
                                     replace = False):
        # Generate the data volumes. This basically defines which
        # directories on the host get mounted in the container. 
        ports = []
        exposed = []
        plan = {'localhost':{'containers':[]}}

        # Get the actual number of containers needed. 
        storage_service = self._get_service(storage_type)
        instances = storage_service.get_total_instances(num_instances, layers)

        # Now get the new container-specific information. 
        i = 0
        for t in instances:
            instance_type = self._get_instance_image(t)
            service = self._get_service(t)
            container_dir, log_dir, host_name, ports, exposed = self._get_service_environment(service, i, num_instances)
            new_log_dir = self._new_log_dir(service_uuid, t, i, replace=replace)
            dir_info = { new_log_dir : log_dir }

            # Only use a data directory mapping if we're not
            # using BTRFS (this is to get around the xattr problem). 
            if self.docker.get_fs_type() != "btrfs":
                new_data_dir = self._new_data_dir(service_uuid, t, i)
                dir_info[new_data_dir] = container_dir

            container_info = {'image':instance_type,
                              'type':t, 
                              'volumes':dir_info,
                              'volume_user':DEFAULT_FERRY_OWNER, 
                              'keys': self._read_key_dir(), 
                              'ports':ports,
                              'exposed':exposed, 
                              'hostname':host_name,
                              'args':args}
            plan['localhost']['containers'].append(container_info)
            i += 1

        return plan

    """
    Prepare the environment for compute containers.
    """
    def _prepare_compute_environment(self, 
                                     service_uuid, 
                                     num_instances, 
                                     compute_type,
                                     layers, 
                                     args = None):
        # Generate the data volumes. This basically defines which
        # directories on the host get mounted in the container. 
        ports = []
        exposed = []
        instance_type = ''
        plan = {'localhost':{'containers':[]}}

        # Get the actual number of containers needed. 
        compute_service = self._get_service(compute_type)
        instances = compute_service.get_total_instances(num_instances, layers)

        i = 0
        for t in instances:
            instance_type = self._get_instance_image(t)
            service = self._get_service(t)
            container_dir, log_dir, host_name, ports, exposed = self._get_service_environment(service, i, num_instances)
            new_log_dir = self._new_log_dir(service_uuid, t, i)
            dir_info = { new_log_dir : log_dir }
            container_info = {'image':instance_type,
                              'volumes':dir_info,
                              'volume_user':DEFAULT_FERRY_OWNER, 
                              'keys': self._read_key_dir(), 
                              'type':t, 
                              'ports':ports,
                              'exposed':exposed, 
                              'hostname':host_name,
                              'args':args}
            plan['localhost']['containers'].append(container_info)
            i += 1
        return plan

    """
    Fetch the instance type. If the UUID is not associated with a running
    service, then just use the raw image. Otherwise, look for a snapshot image. 
    """
    def _get_instance_image(self, instance_type, uuid=None):
        s = instance_type.split('/')
        if len(s) == 1:
            instance_type = DEFAULT_DOCKER_REPO + '/' + instance_type
        return instance_type

    """
    Prepare the environment for connector containers.
    """
    def _prepare_connector_environment(self, 
                                       service_uuid, 
                                       connector_type, 
                                       instance_type=None,
                                       name=None, 
                                       ports=[],
                                       args=None):
        plan = {'localhost':{'containers':[]}}

        # Determine the instance type from the connector type. 
        if not instance_type:
            instance_type = self._get_instance_image(connector_type)

        if not name:
            host_name = "client-%s" % str(service_uuid)
        else:
            host_name = name
        container_info = { 'image':instance_type,
                           'keys': self._read_key_dir(), 
                           'volumes':{},
                           'type':connector_type, 
                           'ports':ports,
                           'exposed':[], 
                           'hostname':host_name,
                           'name':name, 
                           'args':args}

        plan['localhost']['containers'].append(container_info)
        return plan

    def _transfer_config(self, config_dirs):
        """
        Transfer the configuration to the containers. 
        """
        for c in config_dirs:
            container = c[0]
            from_dir = c[1]
            to_dir = c[2]
            logging.warning("transfer config %s -> %s" % (from_dir, to_dir))
            self.docker.copy([container], from_dir, to_dir)

    def _transfer_ip(self, ips):
        """
        Transfer the hostname/IP addresses to all the containers. 
        """
        with open('/tmp/instances', 'w+') as hosts_file:
            for ip in ips:
                hosts_file.write("%s %s\n" % (ip[0], ip[1]))
        for ip in ips:
            self.docker.copy_raw(ip[0], '/tmp/instances', '/service/sconf/instances')
            self.docker.cmd_raw(ip[0], '/service/sbin/startnode hosts')
        
    """
    Transfer these environment variables to the containers.
    Since the user normally interacts with these containers by 
    logging in (via ssh), we must place these variables in the profile. 
    """
    def _transfer_env_vars(self, containers, env_vars):
        for k in env_vars.keys():
            self.docker.cmd(containers, 
                            "echo export %s=%s >> /etc/profile" % (k, env_vars[k]))
    """
    Start the containers on the specified environment
    """
    def _start_containers(self, plan):
        return self.docker.alloc(plan['localhost']['containers']);

    """
    Restart the stopped containers. 
    """
    def _restart_containers(self, containers):
        return self.docker.restart(containers)

    def cancel_stack(self, backends, connectors):
        """
        The stack could not be instantiated correctly. Just get rid
        of these containers. 
        """
        for b in backends['uuids']:
            conf = self._get_service_configuration(b['storage'], detailed=True)
            if conf and 'containers' in conf:
                for c in conf['containers']:
                    self.docker.stop([c])
            for compute in b['compute']:
                conf = self._get_service_configuration(compute, detailed=True)
                if conf and 'containers' in conf:
                    for c in conf['containers']:
                        self.docker.stop([c])

        for b in connectors:
            s = self._get_service_configuration(b, detailed=True)
            if s and 'containers' in s:
                for c in s['containers']:
                    self.docker.stop([c])

    """
    Register the set of services under a single cluster identifier. 
    """
    def register_stack(self, backends, connectors, base, uuid=None):
        if not uuid:
            cluster_uuid = self._new_stack_uuid()
        else:
            cluster_uuid = uuid

        ts = datetime.datetime.now()
        cluster = { 'uuid' : cluster_uuid,
                    'backends':backends,
                    'connectors':connectors,
                    'num_snapshots':0,
                    'snapshot_ts':'', 
                    'snapshot_uuid':base, 
                    'base':base,
                    'status': 'running',
                    'ts':ts }

        if not uuid:
            self.cluster_collection.insert( cluster )
        else:
            self._update_stack(uuid, cluster)

        return cluster_uuid

    """
    Helper method to update a cluster's status. 
    """
    def _update_stack(self, cluster_uuid, state):
        self.cluster_collection.update( {'uuid' : cluster_uuid},
                                        {'$set' : state} )

    def _get_cluster_instances(self, cluster_uuid):
        all_connectors = []
        all_storage = []
        all_compute = []
        cluster = self.cluster_collection.find_one( {'uuid':cluster_uuid} )
        if cluster:
            backends = cluster['backends']
            connector_uuids = cluster['connectors']
            for c in connector_uuids:
                connectors = {'uuid' : c,
                              'instances' : []}
                connector_info = self._get_service_configuration(c, detailed=True)
                if connector_info:
                    for connector in connector_info['containers']:
                        connector_instance = DockerInstance(connector)
                        connectors['instances'].append(connector_instance)
                        connectors['type'] = connector_instance.service_type
                    all_connectors.append(connectors)

            # Collect all the UUIDs of the backend containers. 
            # and stop them. The backend is considered ephemeral!
            for b in backends['uuids']:
                if b['storage'] != None:
                    storage = {'uuid' : b['storage'],
                               'instances' : []}
                    storage_info = self._get_service_configuration(b['storage'], detailed=True)
                    if storage_info:
                        for s in storage_info['containers']:
                            storage_instance = DockerInstance(s)
                            storage['instances'].append(storage_instance)
                            storage['type'] = storage_instance.service_type
                        all_storage.append(storage)

                if b['compute'] != None:
                    for c in b['compute']:
                        compute = {'uuid' : c,
                                   'instances' : []}
                        compute_info = self._get_service_configuration(c, detailed=True)
                        if compute_info:
                            for container in compute_info['containers']:
                                compute_instance = DockerInstance(container)
                                compute['instances'].append(compute_instance)
                                compute['type'] = compute_instance.service_type
                            all_compute.append(compute)
            return all_connectors, all_compute, all_storage

    """
    Stop a running cluster.
    """
    def _stop_stack(self, cluster_uuid):
        # First stop all the running services. 
        connectors, compute, storage = self._get_cluster_instances(cluster_uuid)
        for c in connectors:
            self._stop_service(c['uuid'], c['instances'], c['type'])
        for c in compute:
            self._stop_service(c['uuid'], c['instances'], c['type'])
        for c in storage:
            self._stop_service(c['uuid'], c['instances'], c['type'])

        # Then actually stop the containers. 
        for c in connectors:
            self.docker.halt(c['instances'])
        for c in compute:
            self.docker.halt(c['instances'])
        for c in storage:
            self.docker.halt(c['instances'])

    def _purge_stack(self, cluster_uuid):
        volumes = []
        connectors, compute, storage = self._get_cluster_instances(cluster_uuid)
        for c in connectors:
            self.docker.remove(c['instances'])
        for c in compute:
            self.docker.remove(c['instances'])
        for s in storage:
            for i in s['instances']:
                for v in i.volumes.keys():
                    volumes.append(v)
            self.docker.remove(s['instances'])

        # Now remove the data directories. 
        for v in volumes:
            shutil.rmtree(v)
    
    """
    Take a snapshot of an existing stack. 
    """
    def _snapshot_stack(self, cluster_uuid):
        cluster = self.cluster_collection.find_one( {'uuid':cluster_uuid} )
        if cluster:
            # We need to deserialize the docker containers from the cluster/service
            # description so that the snapshot code has access to certain pieces
            # of information (service type, etc.). 
            connectors = []
            connector_uuids = cluster['connectors']
            for c in connector_uuids:
                connector_info = self._get_service_configuration(c, detailed=True)
                if connector_info:
                    connectors.append(DockerInstance(connector_info['containers'][0]))
            cs_snapshots = self.docker.snapshot(connectors, 
                                                cluster_uuid, 
                                                cluster['num_snapshots'])

            # Register the snapshot in the snapshot state. 
            snapshot_uuid = self._new_snapshot_uuid(cluster_uuid)
            snapshot_ts = datetime.datetime.now()
            snapshot_state = { 'snapshot_ts' : snapshot_ts, 
                               'snapshot_uuid' : snapshot_uuid,
                               'snapshot_cs' : cs_snapshots,
                               'cluster_uuid' : cluster_uuid}
            self.snapshot_collection.insert( snapshot_state )

            # Now update the cluster state. 
            cluster_state = { 'num_snapshots' : cluster['num_snapshots'] + 1,
                              'snapshot_uuid' : snapshot_uuid }
            self.cluster_collection.update( {'uuid':cluster_uuid}, 
                                             {"$set": cluster_state } )

    def start_service(self, uuid, containers):
        """
        Start the service.
        """
        service_info = self._get_service_configuration(uuid, detailed=True)
        self._start_service(uuid, containers, service_info)

    """
    Allocate a new compute cluster.
    """
    def allocate_compute(self,
                         compute_type, 
                         storage_uuid,
                         args, 
                         num_instances=1,
                         layers=[]):
        # Allocate a UUID.
        service_uuid = self._new_service_uuid()
        service = self._get_service(compute_type)

        # Generate the data volumes. This basically defines which
        # directories on the host get mounted in the container. 
        plan = self._prepare_compute_environment(service_uuid, num_instances, compute_type, layers, args)

        # Get the entry point for the storage layer. 
        storage_entry = self._get_service_configuration(storage_uuid)

        # Allocate all the containers. 
        containers = self._start_containers(plan)

        # Generate a configuration dir.
        config_dirs, entry_point = self.config.generate_compute_configuration(service_uuid, 
                                                                              containers, 
                                                                              service, 
                                                                              args, 
                                                                              [storage_entry])

        # Now copy over the configuration.
        self._transfer_config(config_dirs)
        for k in self._read_key_dir().keys():
            keydir = k

        container_info = self._serialize_containers(containers)
        service = {'uuid':service_uuid, 
                   'containers':container_info, 
                   'class':'compute',
                   'keys': keydir, 
                   'type':compute_type,
                   'entry':entry_point,
                   'storage':storage_uuid, 
                   'status':'running'}
        self._update_service_configuration(service_uuid, service)

        # After the docker instance start, we need to start the
        # actual storage service (gluster, etc.). 
        # self._start_service(service_uuid, containers, compute_type, entry_point)
        return service_uuid, containers
        
    def _start_service(self,
                       uuid,
                       containers,
                       service_info): 
        if service_info['class'] != 'connector':
            service = self._get_service(service_info['type'])
            service.start_service(containers, service_info['entry'], self.docker)
        else:
            storage_entry = service_info['storage']
            compute_entry = service_info['compute']
            services, backend_names = self._get_client_services(storage_entry, compute_entry)
            for service in services:
                service.start_service(containers, service_info['entry'], self.docker)

    def _restart_service(self,
                         uuid,
                         containers,
                         service_type):
        service_info = self._get_service_configuration(uuid, detailed=True)
        entry_point = service_info['entry']        
        if service_info['class'] != 'connector':
            service = self._get_service(service_type)
            service.restart_service(containers, entry_point, self.docker)
        else:
            for backend in service_info['backends']:
                if 'client' in self.service[backend]:
                    service = self.service[backend]['client']
                    service.restart_service(containers, entry_point, self.docker)

    def _stop_service(self,
                      uuid,
                      containers,
                      service_type):
        service_info = self._get_service_configuration(uuid, detailed=True)
        entry_point = service_info['entry']        
        if service_info['class'] != 'connector':
            service = self._get_service(service_type)
            service.stop_service(containers, entry_point, self.docker)
        else:
            for backend in service_info['backends']:
                if 'client' in self.service[backend]:
                    service = self.service[backend]['client']
                    service.stop_service(containers, entry_point, self.docker)
        
    def restart_containers(self, service_uuid, containers):
        """
        Restart an stopped storage cluster. This does not
        re-initialize the container. It just starts an empty
        container. 
        """
        self._restart_containers(containers)
        container_info = self._serialize_containers(containers)

        service = {'uuid':service_uuid, 
                   'containers':container_info, 
                   'status':'running'}
        self._update_service_configuration(service_uuid, service)
                        
    """
    Create a storage cluster and start a particular
    personality on that cluster. 
    """
    def allocate_storage(self, 
                         storage_type, 
                         num_instances=1,
                         layers=[], 
                         args=None,
                         replace=False):
        # Allocate a UUID.
        service_uuid = self._new_service_uuid()
        service = self._get_service(storage_type)

        # Generate the data volumes. This basically defines which
        # directories on the host get mounted in the container. 
        plan = self._prepare_storage_environment(service_uuid, num_instances, storage_type, layers, args, replace)

        # Allocate all the containers. 
        containers = self._start_containers(plan)

        # Generate a configuration dir.
        config_dirs, entry_point = self.config.generate_storage_configuration(service_uuid, 
                                                                              containers, 
                                                                              service, 
                                                                              args)

        # Now copy over the configuration.
        self._transfer_config(config_dirs)

        container_info = self._serialize_containers(containers)
        service = {'uuid':service_uuid, 
                   'containers':container_info, 
                   'class':'storage',
                   'type':storage_type,
                   'entry':entry_point,
                   'status':'running'}
        self._update_service_configuration(service_uuid, service)
        return service_uuid, containers

    """
    Get the default deployment conf file. 
    """
    def _get_default_conf(self):        
        return FERRY_HOME + '/data/conf/deploy_default.json'

    """
    Get the deployment configuration parameters. 
    """
    def _get_deploy_params(self, mode, conf):
        # First just find and read the configuration file. 
        if conf == 'default':
            conf = self._get_default_conf()

        # Read the configuration file.
        if os.path.exists(conf):
            with open(conf, 'r') as f: 
                j = json.loads(f.read())

                # Now find the right configuration.
                if mode in j:
                    j[mode]['_mode'] = mode
                    return j[mode]
        return None

    """
    Deploy an existing stack. 
    """
    def deploy_stack(self, cluster_uuid, params=None):
        containers = []
        cluster = self.cluster_collection.find_one( {'uuid':cluster_uuid} )
        if cluster:
            connector_uuids = cluster['connectors']
            for c in connector_uuids:
                connector_info = self._get_service_configuration(c, detailed=True)
                if connector_info:
                    containers.append(DockerInstance(connector_info['containers'][0]))

            self.deploy.deploy(cluster_uuid, containers, params)

            # Check if we need to try starting the
            # stack right away. 
            if params and 'start-on-create' in params:
                return True
        return False
    """
    Manage the stack.
    """
    def manage_stack(self,
                     stack_uuid,
                     action):
        status = 'running'
        if(action == 'snapshot'):        
            # The user wants to take a snapshot of the current stack. This
            # doesn't actually stop anything.
            self._snapshot_stack(stack_uuid)
        elif(action == 'stop'):
            if self.is_running(stack_uuid):
                self._stop_stack(stack_uuid)
                status = 'stopped'
                service_status = { 'uuid':stack_uuid, 'status':status }
                self._update_stack(stack_uuid, service_status)
        elif(action == 'rm'):
            # First need to check if the stack is stopped.
            if self.is_stopped(stack_uuid):
                self._purge_stack(stack_uuid)
                status = 'removed'
                service_status = { 'uuid':stack_uuid, 'status':status }
                self._update_stack(stack_uuid, service_status)
            else:
                return { 'uuid' : stack_uuid,
                         'status' : False,
                         'msg': 'Stack is running. Please stop first' }

        return { 'uuid' : stack_uuid,
                 'status' : True,
                 'msg': status }

    """
    Lookup the stopped backend info. 
    """
    def fetch_stopped_backend(self, uuid):
        cluster = self.cluster_collection.find_one( {'uuid':uuid} )
        if cluster:
            backends = []
            for i, uuid in enumerate(cluster['backends']['uuids']):
                storage_uuid = uuid['storage']
                storage_conf = self._get_service_configuration(storage_uuid, 
                                                               detailed=True)

                compute_confs = []
                if 'compute' in uuid:
                    for c in uuid['compute']:
                        compute_conf = self._get_service_configuration(c, detailed=True)
                        compute_confs.append(compute_conf)
                
                backends.append( {'storage' : storage_conf,
                                  'compute' : compute_confs} )
            return backends
            

    """
    Lookup the snapshot backend info. 
    """
    def fetch_snapshot_backend(self, snapshot_uuid):
        snapshot = self.cluster_collection.find_one( {'snapshot_uuid':snapshot_uuid} )
        if snapshot:
            logging.warning("SNAPSHOT BACKEND: " + str(snapshot['backends']))
            return snapshot['backends']['backend']

    """
    Lookup the deployed backend info. 
    """
    def fetch_deployed_backend(self, app_uuid, conf=None):
        app = self.deploy.find( one = True,
                                spec = { 'uuid' : app_uuid },
                                conf = conf )
        stack = self.cluster_collection.find_one( {'uuid':app['cluster_uuid'] } )
        if stack:
            return stack['backends']['backend']

    """
    Lookup the deployed application connector info and instantiate. 
    """
    def allocate_stopped_connectors(self, 
                                     app_uuid, 
                                     backend_info,
                                     conf = None):
        connector_info = []
        connector_plan = []
        cluster = self.cluster_collection.find_one( {'uuid':app_uuid} )
        if cluster:
            for cuid in cluster['connectors']:
                # Retrieve the actual service information. This will
                # contain the container ID. 
                s = self._get_service_configuration(cuid, detailed=True)
                if s and 'containers' in s:
                    containers = [DockerInstance(j) for j in s['containers']]
                    connector_plan.append( { 'uuid' : cuid,
                                             'containers' : containers,
                                             'type' : s['type'], 
                                             'backend' : backend_info, 
                                             'start' : 'restart' } )
                    connector_info.append(cuid)
                    self.restart_containers(cuid, containers)
        return connector_info, connector_plan

    """
    Lookup the deployed application connector info and instantiate. 
    """
    def allocate_deployed_connectors(self, 
                                     app_uuid, 
                                     backend_info,
                                     conf = None):
        connector_info = []
        connector_plan = []
        app = self.deploy.find( one = True,
                                spec = { 'uuid' : app_uuid },
                                conf = conf)
        if app:
            for c in app['connectors']:
                uuid, containers = self.allocate_connector(connector_type = c['type'],
                                                           backend = backend_info,
                                                           name = c['name'], 
                                                           args = c['args'],
                                                           ports = c['ports'].keys(),
                                                           image = c['image'])
                connector_info.append(uuid)
                connector_plan.append( { 'uuid' : uuid,
                                         'containers' : containers,
                                         'type' : c['type'], 
                                         'start' : 'start' } )
        return connector_info, connector_plan
                
    """
    Lookup the snapshot connector info and instantiate. 
    """
    def allocate_snapshot_connectors(self, 
                                     snapshot_uuid, 
                                     backend_info):
        connector_info = []
        connector_plan = []
        snapshot = self.snapshot_collection.find_one( {'snapshot_uuid':snapshot_uuid} )
        if snapshot:
            for s in snapshot['snapshot_cs']:
                uuid, containers = self.allocate_connector(connector_type = s['type'],
                                                           backend = backend_info,
                                                           name = s['name'], 
                                                           args = s['args'],
                                                           ports = s['ports'].keys(),
                                                           image = s['image'])
                connector_info.append(uuid)
                connector_plan.append( { 'uuid' : uuid,
                                         'containers' : containers,
                                         'type' : s['type'], 
                                         'start' : 'start' } )
        return connector_info, connector_plan
                

    """
    Restart a stopped connector with an existing storage service. 
    """
    def _restart_connectors(self,
                            service_uuid, 
                            connectors, 
                            backend=None):
        # Initialize the connector and connect to the storage. 
        storage_entry = []
        compute_entry = []
        for b in backend:
            if b['storage']:
                storage_entry.append(self._get_service_configuration(b['storage']))
            if b['compute']:
                for c in b['compute']:
                    compute_entry.append(self._get_service_configuration(c))

        # Generate the environment variables that will be 
        # injected into the containers. 
        env_vars = self.config.generate_env_vars(storage_entry,
                                                 compute_entry)
        services, backend_names = self._get_client_services(storage_entry, compute_entry)
        entry_points = {}
        for service in services:
            config_dirs, entry_point = self.config.generate_connector_configuration(service_uuid, 
                                                                                    connectors, 
                                                                                    service,
                                                                                    storage_entry,
                                                                                    compute_entry,
                                                                                    connectors[0].args)
            # Merge all the entry points. 
            entry_points = dict(entry_point.items() + entry_points.items())

            # Now copy over the configuration.
            self._transfer_config(config_dirs)
            self._transfer_env_vars(connectors, env_vars)

        # Start the containers and update the state. 
        self._restart_service(service_uuid, connectors, connectors[0].service_type)
        container_info = self._serialize_containers(connectors)
        service = {'uuid':service_uuid, 
                   'containers':container_info, 
                   'backends':backend_names, 
                   'entry':entry_points,
                   'status':'running'}
        self._update_service_configuration(service_uuid, service)
                
    """
    Allocate a new connector and associate with an existing storage service. 
    """
    def allocate_connector(self,
                           connector_type, 
                           backend=None,
                           name=None, 
                           args=None,
                           ports=None, 
                           image=None):
        # Initialize the connector and connect to the storage. 
        storage_entry = []
        compute_entry = []
        if backend:
            for b in backend:
                if b['storage']:
                    storage_entry.append(self._get_service_configuration(b['storage']))
                if b['compute']:
                    for c in b['compute']:
                        compute_entry.append(self._get_service_configuration(c))

        # Generate the environment variables that will be 
        # injected into the containers. 
        env_vars = self.config.generate_env_vars(storage_entry,
                                                 compute_entry)

        # Allocate a UUID.
        service_uuid = self._new_service_uuid()
        plan = self._prepare_connector_environment(service_uuid = service_uuid, 
                                                   connector_type = connector_type, 
                                                   instance_type = image,
                                                   name = name,
                                                   ports = ports, 
                                                   args = args)
        containers = self._start_containers(plan)

        # Now generate the configuration files that will be
        # transferred to the containers. 
        entry_points = {}
        services, backend_names = self._get_client_services(storage_entry, compute_entry)
        for service in services:
            config_dirs, entry_point = self.config.generate_connector_configuration(service_uuid, 
                                                                                    containers, 
                                                                                    service,
                                                                                    storage_entry,
                                                                                    compute_entry,
                                                                                    args)
            # Merge all the entry points. 
            entry_points = dict(entry_point.items() + entry_points.items())

            # Now copy over the configuration.
            self._transfer_config(config_dirs)
            self._transfer_env_vars(containers, env_vars)

        # Update the connector state. 
        container_info = self._serialize_containers(containers)
        service_info = {'uuid':service_uuid, 
                        'containers':container_info, 
                        'backends':backend_names, 
                        'storage': storage_entry, 
                        'compute': compute_entry, 
                        'class':'connector',
                        'type':connector_type,
                        'entry':entry_points,
                        'uniq': name, 
                        'status':'running'}
        self._update_service_configuration(service_uuid, service_info)
        return service_uuid, containers

    """
    Fetch the current docker version.
    """
    def version(self):
        return self.docker.version()

########NEW FILE########
__FILENAME__ = httpapi
# Copyright 2014 OpenCore LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import json
import logging
from flask import Flask, request
from ferry.install import Installer
from ferry.docker.manager import DockerManager
from ferry.docker.docker import DockerInstance

# Initialize Flask
app = Flask(__name__)

# Initialize the storage driver
docker = DockerManager()
installer = Installer()

"""
Fetch the current information for a particular filesystem. 
"""
@app.route('/storage', methods=['GET'])
def query_storage():
    status = AllocationResponse()
    status.uuid = request.args['uuid']
    status.status = status.NOT_EXISTS

    # Get the time the storage cluster was created, 
    # along with basic usage information. 
    info = storage.query_storage(status.uuid)
    if info != None:
        status.status = info

    # Return the JSON reply.
    return status.json()


"""
Fetch the current docker version
"""
@app.route('/version', methods=['GET'])
def get_version():
    return docker.version()

"""
Allocate the backend from a snapshot. 
"""
def _allocate_backend_from_snapshot(payload):
    snapshot_uuid = payload['_file']
    backends = docker.fetch_snapshot_backend(snapshot_uuid)

    if backends:
        return _allocate_backend(payload = None,
                                 backends = backends)

"""
Allocate the backend from a deployment. 
"""
def _allocate_backend_from_deploy(payload, params=None):
    app_uuid = payload['_file']
    backends = docker.fetch_deployed_backend(app_uuid, params)
    
    if backends:
        return _allocate_backend(payload = None,
                                 backends = backends)

"""
Allocate the backend from a stopped service. 
"""
def _allocate_backend_from_stopped(payload):
    app_uuid = payload['_file']
    backends = docker.fetch_stopped_backend(app_uuid)
    
    if backends:
        return _allocate_backend(payload = None,
                                 backends = backends,
                                 uuid = app_uuid)

def _fetch_num_instances(instance_arg, instance_type, options=None):
    reply = {}
    try:
        num_instances = int(instance_arg)
        reply['num'] = num_instances
    except ValueError:
        # This was not an integer, check if it was a string.
        # First remove any extra white spaces.
        instance_arg = instance_arg.replace(" ","")
        if instance_arg[0] == '>':
            if instance_arg[1] == '=':
                min_instances = int(instance_arg[2])
            else:
                min_instances = int(instance_arg[1]) + 1

            if options and instance_type in options:
                num_instances = int(options[instance_type])
                if num_instances < min_instances:
                    reply['num'] = min_instances
                else:
                    reply['num'] = num_instances
            else:
                reply['query'] = { 'id' : instance_type,
                                   'text' : 'Number of instances for %s' % instance_type  }
    return reply

def _allocate_compute(computes, storage_uuid, options=None):
    """
    Allocate a new compute backend. This method assumes that every
    compute backend already has a specific instance count associated
    with it. After creating the compute backend, it sends back a list
    of all UUIDs that were created in the process. 
    """
    uuids = []
    compute_plan = []
    for c in computes:
        compute_type = c['personality']
        reply = _fetch_num_instances(c['instances'], compute_type, options)
        num_instances = reply['num']
        c['instances'] = num_instances
        args = {}
        if 'args' in c:
            args = c['args']

        layers = []
        if 'layers' in c:
            layers = c['layers']

        compute_uuid, compute_containers = docker.allocate_compute(compute_type = compute_type,
                                                                   storage_uuid = storage_uuid, 
                                                                   args = args, 
                                                                   num_instances = num_instances,
                                                                   layers = layers)
        compute_plan.append( { 'uuid' : compute_uuid,
                               'containers' : compute_containers,
                               'type' : compute_type, 
                               'start' : 'start' } )
        uuids.append( compute_uuid )
    return uuids, compute_plan

def _query_backend_params(backends, options=None):
    """
    Helps allocate a storage/compute backend. It must first determine though if
    there are any missing value parameters. If there are, it sends back a
    list of questions to the client to get those values. The client then
    has to resubmit the request. 
    """
    all_queries = []
    for b in backends:
        backend_type = b['personality']
        query = _fetch_num_instances(b['instances'], backend_type, options)
        if 'query' in query:
            all_queries.append(query)
    return all_queries

def _restart_compute(computes):
    uuids = []
    compute_plan = []
    for c in computes:
        service_uuid = c['uuid']
        compute_type = c['type']

        # Transform the containers into proper container objects.
        compute_containers = c['containers']
        containers = [DockerInstance(j) for j in compute_containers] 

        uuids.append(service_uuid)
        compute_plan.append( { 'uuid' : service_uuid,
                               'containers' : containers,
                               'type' : compute_type, 
                               'start' : 'restart' } )
        docker.restart_containers(service_uuid, containers)
    return uuids, compute_plan

"""
Allocate a brand new backend
"""
def _allocate_backend(payload,
                      backends=None,
                      replace=False,
                      uuid=None):
    iparams = None
    if not backends:
        # The 'iparams' specifies the number of instances to use
        # for the various stack components. It is optional since
        # the application stack may already specify these values. 
        if 'iparams' in payload:
            iparams = payload['iparams']
            
        # We should find the backend information in the payload. 
        if 'backend' in payload:
            backends = payload['backend']
        else:
            backends = []

    # This is the reply we send back. The 'status' denotes whether
    # everything was created/started fine. The UUIDs are a list of 
    # tuples (storage, compute) IDs. The 'backends' just keeps track of
    # the backends we used for allocation purposes. 
    backend_info = { 'status' : 'ok', 
                     'uuids' : [],
                     'backend' : backends }
    storage_plan = []

    # First we need to check if either the storage or the compute 
    # have unspecified number of instances. If so, we'll need to abort
    # the creating the backend, and send a query. 
    if not uuid:
        all_questions = []
        for b in backends:
            storage = b['storage']
            storage_params = _query_backend_params([storage], iparams)
            compute_params = []
            if 'compute' in b:
                compute_params = _query_backend_params(b['compute'], iparams)
            all_questions = storage_params + compute_params + all_questions

        # Looks like there are unfilled parameters. Send back a list of
        # questions for the user to fill out. 
        if len(all_questions) > 0:
            backend_info['status'] = 'query'
            backend_info['query'] = all_questions
            return backend_info, None
    
    # Go ahead and create the actual backend stack. If the user has passed in
    # an existing backend UUID, that means we should restart that backend. Otherwise
    # we create a fresh backend. 
    for b in backends:
        storage = b['storage']
        if not uuid:
            args = None
            if 'args' in storage:
                args = storage['args']
            storage_type = storage['personality']
            reply = _fetch_num_instances(storage['instances'], storage_type, iparams)
            num_instances = reply['num']
            storage['instances'] = num_instances
            layers = []
            if 'layers' in storage:
                layers = storage['layers']
            storage_uuid, storage_containers = docker.allocate_storage(storage_type = storage_type, 
                                                                       num_instances = num_instances,
                                                                       layers = layers,
                                                                       args = args,
                                                                       replace = replace)
            storage_plan.append( { 'uuid' : storage_uuid,
                                   'containers' : storage_containers,
                                   'type' : storage_type, 
                                   'start' : 'start' } )
        else:
            storage_uuid = storage['uuid']
            storage_type = storage['type']
            storage_containers = storage['containers']

            # Transform the containers into proper container objects.
            containers = [DockerInstance(j) for j in storage_containers]
            storage_plan.append( { 'uuid' : storage_uuid,
                                   'containers' : containers,
                                   'type' : storage_type, 
                                   'start' : 'restart' } )
            docker.restart_containers(storage_uuid, containers)
                                                  
        # Now allocate the compute backend. The compute is optional so
        # we should check if it even exists first. 
        compute_uuids = []
        compute_plan = []
        if 'compute' in b:
            if not uuid:
                compute_uuids, compute_plan = _allocate_compute(b['compute'], storage_uuid, iparams)
            else:
                compute_uuids, compute_plan = _restart_compute(b['compute'])
        
        backend_info['uuids'].append( {'storage':storage_uuid,
                                       'compute':compute_uuids} )
    return backend_info, { 'storage' : storage_plan,
                           'compute' : compute_plan }

def _allocate_connectors(payload, backend_info):
    connector_info = []
    connector_plan = []
    if 'connectors' in payload:
        connectors = payload['connectors']
        for c in connectors:
            connector_type = c['personality']
            
            # Check if this connector type has already been pulled
            # into the local index. If not, manually pull it. 
            if not installer._check_and_pull_image(connector_type):
                # We could not fetch this connetor. Instead of 
                # finishing, just return an error.
                return False, connector_info, None

            # Connector names are created by the user 
            # to help identify particular instances. 
            connector_name = None
            if 'name' in c:
                connector_name = c['name']

            args = {}
            if 'args' in c:
                args = c['args']

            ports = []
            if 'ports' in c:
                ports = c['ports']

            uuid, containers = docker.allocate_connector(connector_type = connector_type,
                                                         backend = backend_info, 
                                                         name = connector_name, 
                                                         args = args,
                                                         ports = ports)
            connector_plan.append( { 'uuid' : uuid,
                                     'containers' : containers,
                                     'type' : connector_type, 
                                     'start' : 'start' } )
            connector_info.append(uuid)
    return True, connector_info, connector_plan

"""
Allocate the connectors from a snapshot. 
"""
def _allocate_connectors_from_snapshot(payload, backend_info):
    snapshot_uuid = payload['_file']
    return docker.allocate_snapshot_connectors(snapshot_uuid,
                                               backend_info)

"""
Allocate the connectors from a deployment. 
"""
def _allocate_connectors_from_deploy(payload, backend_info, params=None):
    app_uuid = payload['_file']
    return docker.allocate_deployed_connectors(app_uuid,
                                               backend_info,
                                               params)

"""
Allocate the connectors from a stopped application. 
"""
def _allocate_connectors_from_stopped(payload, backend_info, params=None):
    app_uuid = payload['_file']
    return docker.allocate_stopped_connectors(app_uuid,
                                              backend_info,
                                              params)

def _register_ip_addresses(backend_plan, connector_plan):
    """
    Helper function to register the hostname/IP addresses
    of all the containers. 
    """
    ips = []
    for s in backend_plan['storage']:
        for c in s['containers']:
            if isinstance(c, dict):
                ips.append( [c['internal_ip'], c['hostname']] )
            else:
                ips.append( [c.internal_ip, c.host_name] )
    for s in backend_plan['compute']:
        for c in s['containers']:
            if isinstance(c, dict):
                ips.append( [c['internal_ip'], c['hostname']] )
            else:
                ips.append( [c.internal_ip, c.host_name] )
    for s in connector_plan:
        for c in s['containers']:
            # This is slightly awkward. It is because when starting
            # a new stack, we get proper "container" objects. However,
            # when restarting we get dictionary descriptions. Should just
            # fix at the restart level! 
            if isinstance(c, dict):
                logging.warning("CONTAINER: " + str(c))
                ips.append( [c['internal_ip'], c['hostname']] )
            else:
                ips.append( [c.internal_ip, c.host_name] )
    docker._transfer_ip(ips)

def _start_all_services(backend_plan, connector_plan):
    """
    Helper function to start both the backend and
    frontend. Depending on the plan, this will either
    do a fresh start or a restart on an existing cluster. 
    """

    # Make sure that all the hosts have the current set
    # of IP addresses. 
    _register_ip_addresses(backend_plan, connector_plan)

    # Now we need to start/restart all the services. 
    for s in backend_plan['storage']:
        if s['start'] == 'start':
            docker.start_service(s['uuid'], 
                                 s['containers'])
        else:
            docker._restart_service(s['uuid'], s['containers'], s['type'])

    for c in backend_plan['compute']:
        if c['start'] == 'start':
            docker.start_service(c['uuid'], c['containers'])
        else:
            docker._restart_service(c['uuid'], c['containers'], c['type'])

    for c in connector_plan:
        if c['start'] == 'start':
            docker.start_service(c['uuid'], c['containers'])
        else:
            docker._restart_connectors(c['uuid'], c['containers'], c['backend'])

def _allocate_new(payload):
    """
    Helper function to allocate and start a new stack. 
    """
    reply = {}
    backend_info, backend_plan = _allocate_backend(payload, replace=True)
    reply['status'] = backend_info['status']
    logging.warning("NEW BACKEND: " + str(backend_info))
    if backend_info['status'] == 'ok':
        success, connector_info, connector_plan = _allocate_connectors(payload, backend_info['uuids'])

        if success:
            _start_all_services(backend_plan, connector_plan)
            uuid = docker.register_stack(backend_info, connector_info, payload['_file'])
            reply['text'] = str(uuid)
        else:
            # One or more connectors was not instantiated properly. 
            docker.cancel_stack(backend_info, connector_info)
            reply['status'] = 'failed'
    else:
        reply['questions'] = backend_info['query']
    return json.dumps(reply)

def _allocate_stopped(payload):
    """
    Helper function to allocate and start a stopped stack. 
    """
    base = docker.get_base_image(payload['_file'])
    backend_info, backend_plan = _allocate_backend_from_stopped(payload)
    if backend_info['status'] == 'ok':
        connector_info, connector_plan = _allocate_connectors_from_stopped(payload, backend_info['uuids'])
        _start_all_services(backend_plan, connector_plan)        
        uuid = docker.register_stack(backends = backend_info,
                                     connectors = connector_info,
                                     base = base,
                                     uuid = payload['_file'])
        return json.dumps({'status' : 'ok',
                           'text' : str(uuid)})
    else:
        return json.dumps({'status' : 'failed'})

def _allocate_snapshot(payload):
    """
    Helper function to allocate and start a snapshot.
    """
    logging.warning("ALLOCATE SNAPSHOT " + str(payload['_file']))
    backend_info, backend_plan = _allocate_backend_from_snapshot(payload)
    if backend_info['status'] == 'ok':
        connector_info, connector_plan = _allocate_connectors_from_snapshot(payload, 
                                                                            backend_info['uuids'])
        _start_all_services(backend_plan, connector_plan)
        uuid = docker.register_stack(backend_info, connector_info, payload['_file'])
        return json.dumps({'status' : 'ok',
                           'text' : str(uuid)})
    else:
        return json.dumps({'status' : 'failed'})

def _allocate_deployed(payload, params=None):
    """
    Helper function to allocate and start a deployed application. 
    """
    backend_info, backend_plan = _allocate_backend_from_deploy(payload, params)
    if backend_info['status'] == 'ok':
        connector_info, connector_plan = _allocate_connectors_from_deploy(payload, 
                                                                          backend_info['uuids'],
                                                                          params)
        _start_all_services(backend_plan, connector_plan)
        uuid = docker.register_stack(backend_info, connector_info, payload['_file'])
        return json.dumps({'status' : 'ok',
                           'text' : str(uuid)})

"""
Create some new storage infrastructure
"""
@app.route('/create', methods=['POST'])
def allocate_stack():
    payload = json.loads(request.form['payload'])
    mode = request.form['mode']
    conf = request.form['conf']
    params = docker._get_deploy_params(mode, conf)

    # Check whether the user wants to start from fresh or
    # start with a snapshot.
    if docker.is_stopped(payload['_file']):
        return _allocate_stopped(payload)
    elif docker.is_snapshot(payload['_file']):
        return _allocate_snapshot(payload)
    elif docker.is_deployed(payload['_file'], params):
        return _allocate_deployed(payload, params)
    elif '_file_path' in payload:
        return _allocate_new(payload)
    else:
        return "Could not start " + payload['_file']

"""
Query the deployed applications.
"""
@app.route('/deployed', methods=['GET'])
def query_deployed():
    mode = request.args['mode']
    conf = request.args['conf']
    params = docker._get_deploy_params(mode, conf)

    return docker.query_deployed(params)

"""
Query the stacks.
"""
@app.route('/query', methods=['GET'])
def query_stacks():
    if 'constraints' in request.args:
        constraints = json.loads(request.args['constraints'])
        return docker.query_stacks(constraints)
    else:
        return docker.query_stacks()
"""
Query the snapshots
"""
@app.route('/snapshots', methods=['GET'])
def snapshots():
    return docker.query_snapshots()

"""
Inspect a particular stack.
"""
@app.route('/stack', methods=['GET'])
def inspect():
    uuid = request.args['uuid']
    return docker.inspect_stack(uuid)

"""
Copy over logs
"""
@app.route('/logs', methods=['GET'])
def logs():
    stack_uuid = request.args['uuid']
    to_dir = request.args['dir']
    return docker.copy_logs(stack_uuid, to_dir)

"""
"""
@app.route('/deploy', methods=['POST'])
def deploy_stack():
    stack_uuid = request.form['uuid']
    mode = request.form['mode']
    conf = request.form['conf']

    # Deploy the stack and check if we need to
    # automatically start the stack as well. 
    params = docker._get_deploy_params(mode, conf)
    if docker.deploy_stack(stack_uuid, params):
        _allocate_deployed( { '_file' : stack_uuid } )

"""
Manage the stacks.
"""
@app.route('/manage/stack', methods=['POST'])
def manage_stack():
    stack_uuid = request.form['uuid']
    stack_action = request.form['action']
    reply = docker.manage_stack(stack_uuid, stack_action)

    # Format the message to make more sense.
    if reply['status']:
        return reply['msg'] + ' ' + reply['uuid']
    else:
        return reply['msg']

########NEW FILE########
__FILENAME__ = install
# Copyright 2014 OpenCore LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import ferry
import grp
import json
import logging
import os
import os.path
import pwd
import re
import shutil
import stat
import struct
import sys
import time
from distutils import spawn
from string import Template
from subprocess import Popen, PIPE
from ferry.ip.client import DHCPClient

def _get_ferry_home():
    if 'FERRY_HOME' in os.environ:
        return os.environ['FERRY_HOME']
    else:
        return os.path.dirname(__file__)

def _get_ferry_user():
    uid = pwd.getpwnam("root").pw_uid
    gid = grp.getgrnam("docker").gr_gid
    return uid, gid
    
def _has_ferry_user():
    try:
        uid = pwd.getpwnam("root").pw_uid
        gid = grp.getgrnam("docker").gr_gid
    except KeyError:
        return False
    return True

def _supported_arch():
    return struct.calcsize("P") * 8 == 64

def _supported_lxc():
    output = Popen("(lxc-version 2>/dev/null || lxc-start --version) | sed 's/.* //'", stdout=PIPE, shell=True).stdout.read()

    # Ignore all non-numeric strings in the
    # versioning information. 
    cleaned = []
    tuples = output.strip().split(".")[:3]
    for t in tuples: 
        m = re.compile('(\d)*').match(t)
        if m and len(m.groups()) > 0 and m.group(1) != '':
            cleaned.append(m.group(1))
        else:
            cleaned.append(str(0))

    # We need our tuples to be exactly three values. If
    # there are more values, add a zero. 
    for i in range(3 - len(cleaned)):
        cleaned.append(0)

    # Now compare the tuples. We need at least version 0.7.5. This
    # assumes that lxc-version info is consistent across distributions
    # which may not be true...
    ver = tuple(map(int, cleaned))
    return ver > (0, 7, 5)

def _supported_python():
    return sys.version_info[0] == 2

def _touch_file(file_name, content, root=False):
    try:
        f = open(file_name, 'w+')
        f.write(content)
        f.close()

        if root:
            uid, gid = _get_ferry_user()
            os.chown(file_name, uid, gid)
            os.chmod(file_name, 0664)
    except IOError as e:
        logging.error("Could not create %s.\n" % file_name)

FERRY_HOME=_get_ferry_home()
DEFAULT_IMAGE_DIR=FERRY_HOME + '/data/dockerfiles'
DEFAULT_KEY_DIR=FERRY_HOME + '/data/key'
GLOBAL_KEY_DIR=DEFAULT_KEY_DIR
DEFAULT_DOCKER_REPO='ferry'
GUEST_DOCKER_REPO='ferry-user'
DEFAULT_FERRY_OWNER='ferry:docker'
DOCKER_CMD='docker-ferry'
DOCKER_SOCK='unix:////var/run/ferry.sock'
DOCKER_DIR='/var/lib/ferry'
DOCKER_PID='/var/run/ferry.pid'
DEFAULT_MONGO_DB='/var/lib/ferry/mongo'
DEFAULT_MONGO_LOG='/var/lib/ferry/mongolog'
DEFAULT_REGISTRY_DB='/var/lib/ferry/registry'
DEFAULT_DOCKER_LOG='/var/lib/ferry/docker.log'
DEFAULT_DOCKER_KEY='/var/lib/ferry/keydir'

class Installer(object):
    def __init__(self):
        self.network = DHCPClient()

    def _read_key_dir(self):
        f = open(ferry.install.DEFAULT_DOCKER_KEY, 'r')
        k = f.read().strip().split("://")
        return k[1], k[0]

    def _clean_rules(self):
        self.network.clean_rules()

    def _reset_ssh_key(self):
        keydir, tmp = self._read_key_dir()

        # Only reset temporary keys. User-defined key directories
        # shouldn't be touched. 
        if keydir != DEFAULT_KEY_DIR and tmp == "tmp":
            shutil.rmtree(keydir)

        # Mark that we are using the default package keys
        global GLOBAL_KEY_DIR
        GLOBAL_KEY_DIR = 'tmp://' + DEFAULT_KEY_DIR
        _touch_file(DEFAULT_DOCKER_KEY, GLOBAL_KEY_DIR, root=True)
        logging.warning("reset key directory " + GLOBAL_KEY_DIR)
        
    def _process_ssh_key(self, options):
        global GLOBAL_KEY_DIR
        if options and '-k' in options:
            GLOBAL_KEY_DIR = 'user://' + self.fetch_image_keys(options['-k'][0])
        else:
            GLOBAL_KEY_DIR = 'tmp://' + DEFAULT_KEY_DIR
        logging.warning("using key directory " + GLOBAL_KEY_DIR)
        _touch_file(DEFAULT_DOCKER_KEY, GLOBAL_KEY_DIR, root=True)

    def install(self, args, options):
        # Check if the host is actually 64-bit. If not raise a warning and quit.
        if not _supported_arch():
            return 'Your architecture appears to be 32-bit.\nOnly 64-bit architectures are supported at the moment.'

        if not _supported_python():
            return 'You appear to be running Python3.\nOnly Python2 is supported at the moment.'

        if not _supported_lxc():
            return 'You appear to be running an older version of LXC.\nOnly versions > 0.7.5 are supported.'

        if not _has_ferry_user():
            return 'You do not appear to have the \'docker\' group configured. Please create the \'docker\' group and try again.'

        # Create the various directories.
        try:
            if not os.path.isdir(DOCKER_DIR):
                os.makedirs(DOCKER_DIR)
                self._change_permission(DOCKER_DIR)
        except OSError as e:
            logging.error("Could not install Ferry.\n") 
            logging.error(e.strerror)
            sys.exit(1)

        # Start the Ferry docker daemon. If it does not successfully
        # start, print out a msg. 
        logging.warning("all prerequisites met...")
        start, msg = self._start_docker_daemon(options)
        if not start:
            logging.error('ferry docker daemon not started')
            return msg

        # Normally we don't want to build the Dockerfiles,
        # but sometimes we may for testing, etc. 
        build = False
        if options and '-b' in options:
            build = True

        if options and '-u' in options:
            if len(options['-u']) > 0 and options['-u'][0] != True:
                logging.warning("performing select rebuild (%s)" % str(options['-u']))
                self.build_from_list(options['-u'], 
                                     DEFAULT_IMAGE_DIR,
                                     DEFAULT_DOCKER_REPO, build, recurse=False)
            else:
                logging.warning("performing forced rebuild")
                self.build_from_dir(DEFAULT_IMAGE_DIR, DEFAULT_DOCKER_REPO, build)
        else:
            # We want to be selective about which images
            # to rebuild. Useful if one image breaks, etc. 
            to_build = self.check_images(DEFAULT_IMAGE_DIR,
                                         DEFAULT_DOCKER_REPO)
            if len(to_build) > 0:
                logging.warning("performing select rebuild (%s)" % str(to_build))
                self.build_from_list(to_build, 
                                     DEFAULT_IMAGE_DIR,
                                     DEFAULT_DOCKER_REPO, build)

        # Check that all the images were built.
        not_installed = self._check_all_images()
        if len(not_installed) == 0:
            return 'installed ferry'
        else:
            logging.error('images not built: ' + str(not_installed))
            return 'Some images were not installed. Please type \'ferry install\' again.'

    def _check_all_images(self):
        not_installed = []
        images = ['mongodb', 'ferry-base', 'hadoop-base', 'hadoop', 'hadoop-client',
                  'hive-metastore', 'gluster', 'openmpi', 'cassandra', 'cassandra-client', 
                  'titan', 'spark']
        for i in images:
            if not self._check_image_installed("%s/%s" % (DEFAULT_DOCKER_REPO, i)):
                not_installed.append(i)
        return not_installed

    def _check_and_pull_image(self, image_name):
        if not self._check_image_installed(image_name):
            self._pull_image(image_name, on_client=False)

        return self._check_image_installed(image_name)

    def _check_image_installed(self, image_name):
        cmd = DOCKER_CMD + ' -H=' + DOCKER_SOCK + ' inspect %s 2> /dev/null' % image_name
        output = Popen(cmd, stdout=PIPE, shell=True).stdout.read()
        if output.strip() == '[]':
            return False
        else:
            return True

    def start_web(self, options=None, clean=False):
        start, msg = self._start_docker_daemon(options)
        if not clean and not start:
            # We are trying to start the web services but the Docker
            # daemon won't start. If we're cleaning, it's not a big deal. 
            logging.error(msg) 
            sys.exit(1)

        # Check if the user wants to use a specific key directory. 
        self._process_ssh_key(options)

        # Check if the Mongo directory exists yet. If not
        # go ahead and create it. 
        try:
            if not os.path.isdir(DEFAULT_MONGO_DB):
                os.makedirs(DEFAULT_MONGO_DB)
                self._change_permission(DEFAULT_MONGO_DB)
            if not os.path.isdir(DEFAULT_MONGO_LOG):
                os.makedirs(DEFAULT_MONGO_LOG)
                self._change_permission(DEFAULT_MONGO_LOG)
        except OSError as e:
            logging.error("Could not start ferry servers.\n") 
            logging.error(e.strerror)
            sys.exit(1)

        # Check if the Mongo image is built.
        if not self._check_image_installed('%s/mongodb' % DEFAULT_DOCKER_REPO):
            logging.error("Could not start ferry servers.\n") 
            logging.error("MongoDB images not found. Try executing 'ferry install'.")
            sys.exit(1)

        # Check if there are any other Mongo instances runnig.
        self._clean_web()

        # Start the Mongo server.
        keydir, _ = self._read_key_dir()
        cmd = DOCKER_CMD + ' -H=' + DOCKER_SOCK + ' run -d -v %s:%s -v %s:%s -v %s:%s %s/mongodb' % (keydir, '/service/keys',
                                                                                                     DEFAULT_MONGO_DB, '/service/data',
                                                                                                     DEFAULT_MONGO_LOG, '/service/logs',
                                                                                                     DEFAULT_DOCKER_REPO)
        logging.warning(cmd)
        child = Popen(cmd, stdout=PIPE, stderr=PIPE, shell=True)
        output = child.stderr.read().strip()
        if re.compile('[/:\s\w]*Can\'t connect[\'\s\w]*').match(output):
            logging.error("Ferry docker daemon does not appear to be running")
            sys.exit(1)
        elif re.compile('Unable to find image[\'\s\w]*').match(output):
            logging.error("Ferry mongo image not present")
            sys.exit(1)

        # Need to get Mongo connection info and store in temp file. 
        container = child.stdout.read().strip()
        cmd = DOCKER_CMD + ' -H=' + DOCKER_SOCK + ' inspect %s' % container
        logging.warning(cmd)
        output = Popen(cmd, stdout=PIPE, shell=True).stdout.read()
        output_json = json.loads(output.strip())
        ip = output_json[0]['NetworkSettings']['IPAddress']
        _touch_file('/tmp/mongodb.ip', ip, root=True)

        # Set the MongoDB env. variable. 
        my_env = os.environ.copy()
        my_env['MONGODB'] = ip

        # Sleep a little while to let Mongo start receiving.
        time.sleep(2)

        # Start the DHCP server
        logging.warning("starting dhcp server")
        cmd = 'gunicorn -t 3600 -b 127.0.0.1:5000 -w 1 ferry.ip.dhcp:app &'
        Popen(cmd, stdout=PIPE, shell=True, env=my_env)
        time.sleep(2)

        # Reserve the Mongo IP.
        self.network.reserve_ip(ip)

        # Start the Ferry HTTP servers
        logging.warning("starting http servers on port 4000 and mongo %s" % ip)
        cmd = 'gunicorn -e FERRY_HOME=%s -t 3600 -w 3 -b 127.0.0.1:4000 ferry.http.httpapi:app &' % FERRY_HOME
        Popen(cmd, stdout=PIPE, shell=True, env=my_env)

    def stop_web(self):
        # Shutdown the mongo instance
        if os.path.exists('/tmp/mongodb.ip'):
            f = open('/tmp/mongodb.ip', 'r')
            ip = f.read().strip()
            f.close()

            keydir, tmp = self._read_key_dir()
            key = keydir + "/id_rsa"
            cmd = 'ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i %s root@%s /service/bin/mongodb stop' % (key, ip)
            logging.warning(cmd)
            output = Popen(cmd, stdout=PIPE, shell=True).stdout.read()
            os.remove('/tmp/mongodb.ip')

        # Kill all the gunicorn instances. 
        logging.warning("stopping http servers")
        cmd = 'ps -eaf | grep httpapi | awk \'{print $2}\' | xargs kill -15'
        Popen(cmd, stdout=PIPE, shell=True)
        cmd = 'ps -eaf | grep ferry.ip.dhcp | awk \'{print $2}\' | xargs kill -15'
        Popen(cmd, stdout=PIPE, shell=True)

    def _clean_web(self):
        docker = DOCKER_CMD + ' -H=' + DOCKER_SOCK
        cmd = docker + ' ps | grep ferry/mongodb | awk \'{print $1}\' | xargs ' + docker + ' stop '
        logging.warning("cleaning previous mongo resources")
        logging.warning(cmd)
        child = Popen(cmd, stdout=PIPE, stderr=PIPE, shell=True)
        child.stdout.read()
        child.stderr.read()

    def _copytree(self, src, dst):
        for item in os.listdir(src):
            s = os.path.join(src, item)
            d = os.path.join(dst, item)
            if os.path.isdir(s):
                shutil.copytree(s, d)
            else:
                shutil.copy2(s, d)

    def _change_permission(self, location):
        uid, gid = _get_ferry_user()
        os.chown(location, uid, gid)

        if os.path.isdir(location):        
            os.chmod(location, 0774)
            for entry in os.listdir(location):
                self._change_permission(os.path.join(location, entry))
        else:
            # Check if this file has a file extension. If not,
            # then assume it's a binary.
            s = stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IWGRP | stat.S_IROTH
            if len(location.split(".")) == 1:
                s |= stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH
            os.chmod(location, s)

    """
    Ask for the key directory.
    """
    def fetch_image_keys(self, key_dir=None):
        if key_dir and os.path.exists(key_dir):
            return key_dir
        else:
            return DEFAULT_KEY_DIR

    """
    Check if the dockerfiles are already built. 
    """
    def check_images(self, image_dir, repo):
        if self._docker_running():
            build_images = []
            for f in os.listdir(image_dir):
                dockerfile = image_dir + '/' + f + '/Dockerfile'
                image_names = self._check_dockerfile(dockerfile, repo)
                if len(image_names) > 0:
                    build_images += image_names
            return build_images
        else:
            logging.error("ferry daemon not started")

    """
    Build the docker images
    """
    def build_from_list(self, to_build, image_dir, repo, build=False, recurse=True):
        if self._docker_running():
            built_images = {}
            for f in os.listdir(image_dir):
                logging.warning("transforming dockerfile")
                self._transform_dockerfile(image_dir, f, repo)

            for f in os.listdir("/tmp/dockerfiles/"):
                dockerfile = '/tmp/dockerfiles/' + f + '/Dockerfile'
                images = self._get_image(dockerfile)
                intersection = [i for i in images if i in to_build]
                if len(intersection) > 0:
                    image = images.pop(0)
                    logging.warning("building image " + image)
                    self._build_image(image, dockerfile, repo, built_images, recurse=recurse, build=build)

                    if len(images) > 0:
                        logging.warning("tagging images " + image)
                        self._tag_images(image, repo, images)

            # After building everything, get rid of the temp dir.
            shutil.rmtree("/tmp/dockerfiles")
        else:
            logging.error("ferry daemon not started")

    """
    Build the docker images
    """
    def build_from_dir(self, image_dir, repo, build=False):
        if self._docker_running():
            built_images = {}
            for f in os.listdir(image_dir):
                self._transform_dockerfile(image_dir, f, repo)
            for f in os.listdir("/tmp/dockerfiles/"):
                dockerfile = "/tmp/dockerfiles/" + f + "/Dockerfile"
                images = self._get_image(dockerfile)
                image = images.pop(0)
                self._build_image(image, dockerfile, repo, built_images, recurse=True, build=build)

                if len(images) > 0:
                    logging.warning("tagging images " + image)
                    self._tag_images(image, repo, images)

            # After building everything, get rid of the temp dir.
            shutil.rmtree("/tmp/dockerfiles")
        else:
            logging.error("ferry daemon not started")

    def _docker_running(self):
        return os.path.exists('/var/run/ferry.sock')

    def _check_dockerfile(self, dockerfile, repo):
        not_installed = []
        images = self._get_image(dockerfile)
        for image in images:
            qualified = DEFAULT_DOCKER_REPO + '/' + image
            cmd = DOCKER_CMD + ' -H=' + DOCKER_SOCK + ' inspect ' + qualified + ' 2> /dev/null'
            output = Popen(cmd, stdout=PIPE, shell=True).stdout.read()
            if output.strip() == '[]':
                not_installed.append(image)
        return not_installed

    def _transform_dockerfile(self, image_dir, f, repo):
        if not os.path.exists("/tmp/dockerfiles/" + f):
            shutil.copytree(image_dir + '/' + f, '/tmp/dockerfiles/' + f)
    
        out_file = "/tmp/dockerfiles/" + f + "/Dockerfile"
        out = open(out_file, "w+")
        uid, gid = _get_ferry_user()
        changes = { "USER" : repo,
                    "DOCKER" : gid }
        for line in open(image_dir + '/' + f + '/Dockerfile', "r"):
            s = Template(line).substitute(changes)
            out.write(s)
        out.close()

    def _build_image(self, image, f, repo, built_images, recurse=False, build=False):
        base = self._get_base(f)
        if recurse and base != "base":
            image_dir = os.path.dirname(os.path.dirname(f))
            dockerfile = image_dir + '/' + base + '/Dockerfile'
            self._build_image(base, dockerfile, repo, built_images, recurse, build)

        if not image in built_images:
            if base == "base":
                self._pull_image(base, tag='latest')

            built_images[image] = True
            self._compile_image(image, repo, os.path.dirname(f), build)

    def _get_image(self, dockerfile):
        names = []
        for l in open(dockerfile, 'r'):
            if l.strip() != '':
                s = l.split()
                if len(s) > 0:
                    if s[0].upper() == 'NAME':
                        names.append(s[1].strip())
        return names

    def _get_base(self, dockerfile):
        base = None
        for l in open(dockerfile, 'r'):
            s = l.split()
            if len(s) > 0:
                if s[0].upper() == 'FROM':
                    base = s[1].strip().split("/")
                    return base[-1]
        return base

    def _continuous_print(self, process, on_client=True):
        while True:
            try:
                out = process.stdout.read(15)
                if out == '':
                    break
                else:
                    if on_client:
                        sys.stdout.write(out)
                        sys.stdout.flush()
                    else:
                        logging.warning("downloading image...")
            except IOError as e:
                logging.warning(e)

        try:
            errmsg = process.stderr.readline()
            if errmsg and errmsg != '':
                logging.warning(errmsg)
            else:
                logging.warning("downloaded image!")
        except IOError:
            pass

    def _pull_image(self, image, tag=None, on_client=True):
        if not tag:
            cmd = DOCKER_CMD + ' -H=' + DOCKER_SOCK + ' pull %s' % image
        else:
            cmd = DOCKER_CMD + ' -H=' + DOCKER_SOCK + ' pull %s:%s' % (image, tag)

        logging.warning(cmd)
        child = Popen(cmd, stdout=PIPE, stderr=PIPE, shell=True)
        self._continuous_print(child, on_client=on_client)

        # Now tag the image with the 'latest' tag. 
        if tag and tag != 'latest':
            cmd = DOCKER_CMD + ' -H=' + DOCKER_SOCK + ' tag' + ' %s:%s %s:%s' % (image, tag, image, 'latest')
            logging.warning(cmd)
            Popen(cmd, stdout=PIPE, shell=True)
        
    def _compile_image(self, image, repo, image_dir, build=False):
        # Now build the image. 
        if build:
            cmd = DOCKER_CMD + ' -H=' + DOCKER_SOCK + ' build --rm=true -t' + ' %s/%s %s' % (repo, image, image_dir)
            logging.warning(cmd)
            child = Popen(cmd, stdout=PIPE, stderr=PIPE, shell=True)
            self._continuous_print(child)

            # Now tag the image. 
            cmd = DOCKER_CMD + ' -H=' + DOCKER_SOCK + ' tag' + ' %s/%s %s/%s:%s' % (repo, image, repo, image, ferry.__version__)
            logging.warning(cmd)
            child = Popen(cmd, stdout=PIPE, shell=True)
        else:
            # Just pull the image from the public repo. 
            image_name = "%s/%s" % (repo, image)
            self._pull_image(image_name, tag=ferry.__version__)

    def _tag_images(self, image, repo, alternatives):
        for a in alternatives:
            cmd = DOCKER_CMD + ' -H=' + DOCKER_SOCK + ' tag' + ' %s/%s:%s %s/%s:%s' % (repo, image, ferry.__version__, repo, a, ferry.__version__)
            logging.warning(cmd)
            child = Popen(cmd, stdout=PIPE, shell=True)
            cmd = DOCKER_CMD + ' -H=' + DOCKER_SOCK + ' tag' + ' %s/%s:latest %s/%s:latest' % (repo, image, repo, a)
            logging.warning(cmd)
            child = Popen(cmd, stdout=PIPE, shell=True)

    def _clean_images(self):
        cmd = DOCKER_CMD + ' -H=' + DOCKER_SOCK + ' | grep none | awk \'{print $1}\' | xargs ' + DOCKER_CMD + ' -H=' + DOCKER_SOCK + ' rmi'
        Popen(cmd, stdout=PIPE, shell=True)

    def _is_running_btrfs(self):
        logging.warning("checking for btrfs")
        cmd = 'cat /etc/mtab | grep %s | awk \'{print $3}\'' % DOCKER_DIR
        output = Popen(cmd, stdout=PIPE, shell=True).stdout.read()
        return output.strip() == "btrfs"

    def _start_docker_daemon(self, options=None):
        # Check if the docker daemon is already running
        try:
            if not self._docker_running():
                bflag = ''
                if self._is_running_btrfs():
                    logging.warning("using btrfs backend")
                    bflag = ' -s btrfs'

                # Explicitly supply the DNS.
                if options and '-n' in options:
                    logging.warning("using custom dns")
                    dflag = ''
                    for d in options['-n']:
                        dflag += ' --dns %s' % d
                else:
                    logging.warning("using public dns")
                    dflag = ' --dns 8.8.8.8 --dns 8.8.4.4'

                # We need to fix this so that ICC is set to false. 
                icc = ' --icc=true'
                cmd = 'nohup ' + DOCKER_CMD + ' -d' + ' -H=' + DOCKER_SOCK + ' -g=' + DOCKER_DIR + ' -p=' + DOCKER_PID + dflag + bflag + icc + ' 1>%s  2>&1 &' % DEFAULT_DOCKER_LOG
                logging.warning(cmd)
                Popen(cmd, stdout=PIPE, shell=True)

                # Wait a second to let the docker daemon do its thing.
                time.sleep(2)
                return True, "Ferry daemon running on /var/run/ferry.sock"
            else:
                return False, "Ferry appears to be already running. If this is an error, please type \'ferry clean\' and try again."
        except OSError as e:
            logging.error("could not start docker daemon.\n") 
            logging.error(e.strerror)
            sys.exit(1)

    def _stop_docker_daemon(self, force=False):
        if force or self._docker_running():
            logging.warning("stopping docker daemon")
            cmd = 'pkill -f docker-ferry'
            Popen(cmd, stdout=PIPE, shell=True)
            try:
                os.remove('/var/run/ferry.sock')
            except OSError:
                pass

    def _get_gateway(self):
        cmd = "LC_MESSAGES=C ifconfig drydock0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}'"
        gw = Popen(cmd, stdout=PIPE, shell=True).stdout.read().strip()

        cmd = "LC_MESSAGES=C ifconfig drydock0 | grep 'inet addr:' | cut -d: -f4 | awk '{ print $1}'"
        netmask = Popen(cmd, stdout=PIPE, shell=True).stdout.read().strip()
        mask = map(int, netmask.split("."))
        cidr = 1
        if mask[3] == 0:
            cidr = 8
        if mask[2] == 0:
            cidr *= 2

        return "%s/%d" % (gw, 32 - cidr)

########NEW FILE########
__FILENAME__ = client
# Copyright 2014 OpenCore LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import json
import logging
import requests

DHCP_SERVER = 'http://localhost:5000'
class DHCPClient(object):
    def __init__(self, cidr_block=None):
        if cidr_block:
            payload = { 'cidr' : cidr_block }
            res = requests.post(DHCP_SERVER + '/cidr', data=payload)

    def assign_ip(self, container):
        payload = { 'container' : json.dumps(container) }
        res = requests.get(DHCP_SERVER + '/ip', params=payload)
        j = json.loads(res.text)
        return j['ip']

    def reserve_ip(self, ip):
        payload = { 'ip' : ip }
        res = requests.put(DHCP_SERVER + '/ip', data=payload)

    def set_owner(self, ip, container):
        payload = { 'args' : json.dumps({ 'ip' : ip,
                                          'container' : container}) }
        requests.post(DHCP_SERVER + '/node', data=payload)

    def random_port(self):
        res = requests.get(DHCP_SERVER + '/port')
        return res.text

    def forward_rule(self, source_ip, source_port, dest_ip, dest_port):
        payload = { 'src_ip' : source_ip,
                    'src_port' : source_port,
                    'dest_ip' : dest_ip,
                    'dest_port' : dest_port }
        requests.post(DHCP_SERVER + '/port', data={'args': json.dumps(payload)})

    def delete_rule(self, dest_ip, dest_port):
        payload = { 'dest_ip' : dest_ip,
                    'dest_port' : dest_port }
        requests.delete(DHCP_SERVER + '/port', data={'args': json.dumps(payload)})

    def clean_rules(self):
        requests.delete(DHCP_SERVER + '/ports')

    def stop_ip(self, ip):
        payload = { 'ip' : ip }
        requests.post(DHCP_SERVER + '/ip', data=payload)

    def free_ip(self, ip):
        payload = { 'ip' : ip }
        requests.delete(DHCP_SERVER + '/ip', data=payload)

########NEW FILE########
__FILENAME__ = dhcp
# Copyright 2014 OpenCore LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import json
import logging
import os
from flask import Flask, request
from pymongo import MongoClient
from ferry.ip.nat import NAT

class DHCP(object):
    def __init__(self):
        self.free_ips = []
        self.reserved_ips = []
        self.ips = {}
        self.num_ips = 1
        self.num_addrs = 0
        self.nat = NAT()
        self._init_state_db()

    def assign_cidr(self, cidr_block):
        if self.num_addrs == 0:
            self.cidr_collection.insert( { 'cidr' : cidr_block } )
            self._parse_cidr(cidr_block)

    def _parse_cidr(self, cidr_block):
        self.gw_ip, self.prefix = self._parse_cidr_address(cidr_block)
        addr = map(int, self.gw_ip.split("."))
        if self.prefix == 24:
            self.latest_ip = "%d.%d.%d.0" % (addr[0], addr[1], addr[2])
        elif self.prefix == 16:
            self.latest_ip = "%d.%d.0.0" % (addr[0], addr[1])
        self.num_addrs = 2**(32 - self.prefix)

    def _init_state_db(self):
        self.mongo = MongoClient(os.environ['MONGODB'], 27017, connectTimeoutMS=6000)
        self.dhcp_collection = self.mongo['network']['dhcp']
        self.cidr_collection = self.mongo['network']['cidr']

        cidr = self.cidr_collection.find_one()
        if cidr:
            logging.warning("recovering network gateway: " + str(cidr['cidr']))
            self._parse_cidr(cidr['cidr'])

        all_ips = self.dhcp_collection.find()
        if all_ips:
            logging.warning("recovering assigned IP addresses")
            for ip_status in all_ips:
                ip = ip_status['ip']
                status = ip_status['status']
                self.num_ips += 1
                self.ips[ip] = { 'status' : status }

                if status == 'free':
                    self.free_ips.append(ip)
                else:
                    self.ips[ip]['container'] = ip_status['container']

                self._recover_latest_ip(ip)

    def _recover_latest_ip(self, ip):
        l = map(int, self.latest_ip.split("."))
        s = map(int, ip.split("."))

        for i in [0, 1, 2, 3]:
            if s[i] > l[i]:
                self.latest_ip = ip
                break
            elif l[i] > s[i]:
                break
                
    def _parse_cidr_address(self, block):
        s = block.split("/")
        return s[0], int(s[1])

    def _increment_ip(self):
        if self.num_ips < self.num_addrs:
            s = map(int, self.latest_ip.split("."))
            for i in [3, 2, 1, 0]:
                if s[i] < 255:
                    s[i] += 1
                    break
                else:
                    s[i] = 0

            # Make sure we skip over the gateway IP. 
            self.latest_ip = "%d.%d.%d.%d" % (s[0], s[1], s[2], s[3])
            if self.latest_ip == self.gw_ip:
                return self._increment_ip()
            elif self.latest_ip in self.reserved_ips:
                return self._increment_ip()
            else:
                self.num_ips += 1
                return self.latest_ip

    def _get_new_ip(self):
        if len(self.free_ips) > 0:
            return self.free_ips.pop(0)
        else:
            return self._increment_ip()

    def random_port(self):
        """
        Get a random port
        """
        return self.nat.random_port()

    def clean_rules(self):
        """
        Clean all rules
        """
        self.nat._clear_nat()

    def delete_rule(self, dest_ip, dest_port):
        """
        Delete port forwarding
        """
        self.nat.delete_rule(dest_ip, dest_port)

    def forward_rule(self, source_ip, source_port, dest_ip, dest_port):
        """
        Port forwarding
        """
        self.nat.forward_rule(source_ip, source_port, dest_ip, dest_port)

    def stop_ip(self, ip):
        """
        Store the container's IP for future use. 
        """
        self.ips[ip]['status'] = 'stopped'
        self.dhcp_collection.update( { 'ip' : ip },
                                     { '$set' : self.ips[ip] },
                                     upsert = True )

    def reserve_ip(self, ip):
        """
        Reserve an IP. This basically takes this IP out of commission. 
        """
        self.reserved_ips.append(ip)

    def assign_ip(self, container):
        """
        Assign a new IP address. If the container is on the stopped list
        then re-assign the same IP address. 
        """
        if 'container' in container:
            for k in self.ips.keys():
                v = self.ips[k]
                if v['container'] == container['container']:
                    self.ips[k]['status'] = 'active'
                    self.dhcp_collection.update( { 'ip' : k },
                                                 { '$set' : { 'status' : 'active' }} )
                                                 
                    return k
            
        new_ip = self._get_new_ip()
        self.ips[new_ip] = { 'status': 'active',
                             'container': None }
        self.dhcp_collection.update( { 'ip' : new_ip },
                                     { '$set' : self.ips[new_ip]},
                                     upsert = True )
        return new_ip

    def free_ip(self, ip):
        """
        Container is being removed and the IP address should be freed. 
        """
        self.free_ips.append(ip)
        self.ips[ip] = { 'status': 'free' }
        self.dhcp_collection.update( { 'ip' : ip },
                                     { '$set' : self.ips[ip] } )

    def set_owner(self, ip, container):
        """
        Set the owner of this IP address. 
        """
        self.ips[ip]['container'] = container
        self.dhcp_collection.update( { 'ip' : ip },
                                     { '$set' : { 'container' : container}} )

dhcp = DHCP()
app = Flask(__name__)

@app.route('/cidr', methods=['POST'])
def assign_cidr():
    cidr = request.form['cidr']
    dhcp.assign_cidr(cidr)
    return ""

@app.route('/ip', methods=['GET'])
def assign_ip():
    container = json.loads(request.args['container'])
    ip = dhcp.assign_ip(container)
    return json.dumps( { 'ip' : ip } )

@app.route('/ip', methods=['POST'])
def stop_ip():
    ip = request.form['ip']
    dhcp.stop_ip(ip)
    return ""

@app.route('/ip', methods=['PUT'])
def reserve_ip():
    ip = request.form['ip']
    dhcp.reserve_ip(ip)
    return ""

@app.route('/port', methods=['GET'])
def random_port():
    return dhcp.random_port()

@app.route('/port', methods=['POST'])
def forward_rule():
    args = json.loads(request.form['args'])
    dhcp.forward_rule(args['src_ip'], args['src_port'], args['dest_ip'], args['dest_port'])
    return ""

@app.route('/port', methods=['DELETE'])
def delete_rule():
    args = json.loads(request.form['args'])
    dhcp.delete_rule(args['dest_ip'], args['dest_port'])
    return ""

@app.route('/ports', methods=['DELETE'])
def clean_rules():
    dhcp.clean_rules()
    return ""

@app.route('/ip', methods=['DELETE'])
def free_ip():
    ip = request.form['ip']
    dhcp.free_ip(ip)
    return ""

@app.route('/node', methods=['POST'])
def set_owner():
    args = json.loads(request.form['args'])
    dhcp.set_owner(args['ip'], args['container'])
    return ""

########NEW FILE########
__FILENAME__ = nat
# Copyright 2014 OpenCore LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import logging
import os
from pymongo import MongoClient
from subprocess import Popen, PIPE

class NAT(object):
    def __init__(self):
        self._current_port = 999
        self.reserved_ports = [4000, 5000]
        self._init_state_db()
        self._clear_nat()
        self._init_nat()
        self._repop_nat()
        
    def _init_state_db(self):
        self.mongo = MongoClient(os.environ['MONGODB'], 27017, connectTimeoutMS=6000)
        self.nat_collection = self.mongo['network']['nat']


    def _clear_nat(self):
        logging.warning("clearing nat")
        
        cmds = ['iptables -t nat -D PREROUTING -m addrtype --dst-type LOCAL -j FERRY_CHAIN',
                'iptables -t nat -D OUTPUT -m addrtype --dst-type LOCAL ! --dst 127.0.0.0/8 -j FERRY_CHAIN',
                'iptables -t nat -D OUTPUT -m addrtype --dst-type LOCAL -j FERRY_CHAIN',
                'iptables -t nat -D OUTPUT -j FERRY_CHAIN',
                'iptables -t nat -F FERRY_CHAIN',
                'iptables -t nat -D PREROUTING -j FERRY_CHAIN',
                'iptables -t nat -X FERRY_CHAIN']
        for c in cmds:
            logging.warning(c)
            Popen(c, shell=True)

    def _init_nat(self):
        logging.warning("init nat")
        cmds = ['iptables -t nat -N FERRY_CHAIN',
                'iptables -t nat -A OUTPUT -m addrtype --dst-type LOCAL ! --dst 127.0.0.0/8 -j FERRY_CHAIN',
                'iptables -t nat -A PREROUTING  -m addrtype --dst-type LOCAL -j FERRY_CHAIN']
        for c in cmds:
            logging.warning(c)
            Popen(c, shell=True)

    def _repop_nat(self):
        rules = self.nat_collection.find()
        for r in rules:
            self._save_nat(r['src_ip'], r['src_port'],r['ip'], r['port'])
                              
    def _save_nat(self, source_ip, source_port, dest_ip, dest_port):
        cmds = ['iptables -I FORWARD 1 ! -i drydock0 -o drydock0  -p tcp --dport %s -d %s -j ACCEPT' % (str(dest_port), dest_ip),
                'iptables -t nat -A FERRY_CHAIN -d %s -p tcp --dport %s -j DNAT --to-destination %s:%s' % (source_ip, str(source_port), dest_ip, str(dest_port))]
        for c in cmds:
            logging.warning(c)
            Popen(c, shell=True)

    def _delete_nat(self, source_ip, source_port, dest_ip, dest_port):
        cmds = ['iptables -D FORWARD ! -i drydock0 -o drydock0  -p tcp --dport %s -d %s -j ACCEPT' % (str(dest_port), dest_ip),
                'iptables -t nat -D FERRY_CHAIN -d %s -p tcp --dport %s -j DNAT --to-destination %s:%s' % (source_ip, str(source_port), dest_ip, str(dest_port))]
                
        for c in cmds:
            logging.warning(c)
            Popen(c, shell=True)

    def _save_forwarding_rule(self, source_ip, source_port, dest_ip, dest_port):
        self.nat_collection.insert({ 'ip' : dest_ip,
                                     'port' : dest_port,
                                     'src_ip' : source_ip,
                                     'src_port' : source_port })

    def _delete_forwarding_rule(self, dest_ip, dest_port):
        self.nat_collection.remove( { 'ip' : dest_ip,
                                      'port' : dest_port } )

    def random_port(self):
        while True:
            port = self._current_port
            self._current_port += 1
            if not port in self.reserved_ports:
                return str(port)

    def has_rule(self, dest_ip, dest_port):
        rule = self.nat_collection.find_one( { 'ip' : dest_ip,
                                               'port' : dest_port } )
        if rule:
            return rule['src_ip'], rule['src_port']
        else:
            rules = self.nat_collection.find( {'ip' : dest_ip} )
            for r in rules:
                logging.warning("r: " + str(r))
            return None, None

    def delete_rule(self, dest_ip, dest_port):
        """
        Delete the forwarding rule. 
        """
        src_ip, src_port = self.has_rule(dest_ip, dest_port)
        if src_ip:
            self._delete_forwarding_rule(dest_ip, dest_port)
            self._delete_nat(src_ip, src_port, dest_ip, dest_port)
        else:
            logging.warning("no such dest %s:%s" % (dest_ip, dest_port))

    def forward_rule(self, source_ip, source_port, dest_ip, dest_port):
        """
        Add a new forwarding rule. 
        """
        if source_port in self.reserved_ports:
            logging.warning("cannot use reserved port " + source_port)
            return False

        src_ip, src_port = self.has_rule(dest_ip, dest_port)
        if not src_ip:
            self._save_forwarding_rule(source_ip, source_port, dest_ip, dest_port)
            self._save_nat(source_ip, source_port, dest_ip, dest_port)
            return True
        else:
            logging.warning("port " + source_port + " already reserved")
            return False

########NEW FILE########
__FILENAME__ = options
# Copyright 2014 OpenCore LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

class CmdHelp(object):
    def __init__(self):
        self.options = {}
        self.cmds = {}
        self.usage = ''
        self.description = ''

    def add_option(self, short_flag, long_flag, help):
        self.options[short_flag] = { 'short' : short_flag,
                                     'long' : long_flag,
                                     'help' : help,
                                     'args' : [] }

    def add_cmd(self, cmd, help):
        self.cmds[cmd] = { 'cmd' : cmd,
                           'help' : help,
                           'args' : [] }

    def _parse_values(self, i, args):
        values = []
        if i == len(args):
            return i - 1, values
        elif args[i] in self.options or args[i] in self.cmds:
            return i - 1, values
        elif i < len(args):
            values.append(args[i])
            j, v = self._parse_values(i + 1, args)

            if i + 1 == j:
                i = j
                values += v

        return i, values

    def _is_option(self, flag):
        if flag in self.options:
            return True
        else:
            for f in self.options.keys():
                if self.options[f]['long'] == flag:
                    return True
        return False

    def _get_canonical_option(self, flag):
        if flag in self.options:
            return flag
        else:
            for f in self.options.keys():
                if self.options[f]['long'] == flag:
                    return f

    def parse_args(self, args):
        i = 0
        while i < len(args):
            s = args[i].strip()
            if self._is_option(s):
                j, values = self._parse_values(i + 1, args)
                s = self._get_canonical_option(s)
                if len(values) > 0:
                    i = j
                    self.options[s]['args'] += values
                else:
                    i += 1
                    self.options[s]['args'].append(True)
            elif s in self.cmds:
                j, values = self._parse_values(i + 1, args)
                if len(values) > 0:
                    i = j
                    self.cmds[s]['args'] += values
                else:
                    i += 1
                    self.cmds[s]['args'].append(True)
            else:
                i += 1

    def get_cmds(self):
        ac = {}
        for c in self.cmds:
            a = self.cmds[c]['args']
            if len(a) > 0:
                ac[c] = a
        return ac

    def get_options(self):
        ac = {}
        for c in self.options:
            a = self.options[c]['args']
            if len(a) > 0:
                ac[c] = a
        return ac

    def print_help(self):
        help_string = 'Usage: ' + self.usage + '\n'
        help_string += '\n'
        help_string += self.description + '\n'
        help_string += '\n'

        help_string += 'Options:\n'

        for k in sorted(self.options.iterkeys()):
            cmd_string = '    {:10s} {:13s} {:10s}'.format(k, self.options[k]['long'], self.options[k]['help'])
            help_string += cmd_string + '\n'
        help_string += '\n'

        help_string += 'Commands:\n'
        for k in sorted(self.cmds.iterkeys()):
            cmd_string = '    {:10s} {:10s}'.format(k, self.cmds[k]['help'])
            help_string += cmd_string + '\n'

        return help_string

########NEW FILE########
__FILENAME__ = prettytable
#
# Copyright (c) 2009-2013, Luke Maurits <luke@maurits.id.au>
# All rights reserved.
# With contributions from:
#  * Chris Clark
#  * Klein Stephane
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are met:
#
# * Redistributions of source code must retain the above copyright notice,
#   this list of conditions and the following disclaimer.
# * Redistributions in binary form must reproduce the above copyright notice,
#   this list of conditions and the following disclaimer in the documentation
#   and/or other materials provided with the distribution.
# * The name of the author may not be used to endorse or promote products
#   derived from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
# POSSIBILITY OF SUCH DAMAGE.

import copy
import itertools
import math
import random
import re
import sys
import textwrap
import unicodedata

py3k = sys.version_info[0] >= 3
if py3k:
    unicode = str
    basestring = str
    itermap = map
    iterzip = zip
    uni_chr = chr
else: 
    itermap = itertools.imap
    iterzip = itertools.izip
    uni_chr = unichr

# hrule styles
FRAME = 0
ALL   = 1
NONE  = 2
HEADER = 3

# Table styles
DEFAULT = 10
MSWORD_FRIENDLY = 11
PLAIN_COLUMNS = 12
RANDOM = 20

_re = re.compile("\033\[[0-9;]*m")

def _get_size(text):
    lines = text.split("\n")
    height = len(lines)
    width = max([_str_block_width(line) for line in lines])
    return (width, height)
        
class PrettyTable(object):

    def __init__(self, field_names=None, **kwargs):

        """Return a new PrettyTable instance

        Arguments:

        encoding - Unicode encoding scheme used to decode any encoded input
        title - optional table title
        field_names - list or tuple of field names
        fields - list or tuple of field names to include in displays
        start - index of first data row to include in output
        end - index of last data row to include in output PLUS ONE (list slice style)
        header - print a header showing field names (True or False)
        header_style - stylisation to apply to field names in header ("cap", "title", "upper", "lower" or None)
        border - print a border around the table (True or False)
        hrules - controls printing of horizontal rules after rows.  Allowed values: FRAME, HEADER, ALL, NONE
        vrules - controls printing of vertical rules between columns.  Allowed values: FRAME, ALL, NONE
        int_format - controls formatting of integer data
        float_format - controls formatting of floating point data
        min_table_width - minimum desired table width, in characters
        max_table_width - maximum desired table width, in characters
        padding_width - number of spaces on either side of column data (only used if left and right paddings are None)
        left_padding_width - number of spaces on left hand side of column data
        right_padding_width - number of spaces on right hand side of column data
        vertical_char - single character string used to draw vertical lines
        horizontal_char - single character string used to draw horizontal lines
        junction_char - single character string used to draw line junctions
        sortby - name of field to sort rows by
        sort_key - sorting key function, applied to data points before sorting
        valign - default valign for each row (None, "t", "m" or "b")
        reversesort - True or False to sort in descending or ascending order
        oldsortslice - Slice rows before sorting in the "old style" """

        self.encoding = kwargs.get("encoding", "UTF-8")

        # Data
        self._field_names = []
        self._align = {}
        self._valign = {}
        self._max_width = {}
        self._min_width = {}
        self._rows = []
        if field_names:
            self.field_names = field_names
        else:
            self._widths = []

        # Options
        self._options = "title start end fields header border sortby reversesort sort_key attributes format hrules vrules".split()
        self._options.extend("int_format float_format min_table_width max_table_width padding_width left_padding_width right_padding_width".split())
        self._options.extend("vertical_char horizontal_char junction_char header_style valign xhtml print_empty oldsortslice".split())
        for option in self._options:
            if option in kwargs:
                self._validate_option(option, kwargs[option])
            else:
                kwargs[option] = None

        self._title = kwargs["title"] or None
        self._start = kwargs["start"] or 0
        self._end = kwargs["end"] or None
        self._fields = kwargs["fields"] or None
        
        if kwargs["header"] in (True, False):
            self._header = kwargs["header"]
        else:
            self._header = True
        self._header_style = kwargs["header_style"] or None
        if kwargs["border"] in (True, False):
            self._border = kwargs["border"]
        else:
            self._border = True
        self._hrules = kwargs["hrules"] or FRAME
        self._vrules = kwargs["vrules"] or ALL

        self._sortby = kwargs["sortby"] or None
        if kwargs["reversesort"] in (True, False):
            self._reversesort = kwargs["reversesort"]
        else:
            self._reversesort = False
        self._sort_key = kwargs["sort_key"] or (lambda x: x)

        self._int_format = kwargs["int_format"] or {}
        self._float_format = kwargs["float_format"] or {}
        self._min_table_width = kwargs["min_table_width"] or None
        self._max_table_width = kwargs["max_table_width"] or None
        self._padding_width = kwargs["padding_width"] or 1
        self._left_padding_width = kwargs["left_padding_width"] or None
        self._right_padding_width = kwargs["right_padding_width"] or None

        self._vertical_char = kwargs["vertical_char"] or self._unicode("|")
        self._horizontal_char = kwargs["horizontal_char"] or self._unicode("-")
        self._junction_char = kwargs["junction_char"] or self._unicode("+")
        
        if kwargs["print_empty"] in (True, False):
            self._print_empty = kwargs["print_empty"]
        else:
            self._print_empty = True
        if kwargs["oldsortslice"] in (True, False):
            self._oldsortslice = kwargs["oldsortslice"]
        else:
            self._oldsortslice = False
        self._format = kwargs["format"] or False
        self._xhtml = kwargs["xhtml"] or False
        self._attributes = kwargs["attributes"] or {}
   
    def _unicode(self, value):
        if not isinstance(value, basestring):
            value = str(value)
        if not isinstance(value, unicode):
            value = unicode(value, self.encoding, "strict")
        return value

    def _justify(self, text, width, align):
        excess = width - _str_block_width(text)
        if align == "l":
            return text + excess * " "
        elif align == "r":
            return excess * " " + text
        else:
            if excess % 2:
                # Uneven padding
                # Put more space on right if text is of odd length...
                if _str_block_width(text) % 2:
                    return (excess//2)*" " + text + (excess//2 + 1)*" "
                # and more space on left if text is of even length
                else:
                    return (excess//2 + 1)*" " + text + (excess//2)*" "
                # Why distribute extra space this way?  To match the behaviour of
                # the inbuilt str.center() method.
            else:
                # Equal padding on either side
                return (excess//2)*" " + text + (excess//2)*" "

    def __getattr__(self, name):

        if name == "rowcount":
            return len(self._rows)
        elif name == "colcount":
            if self._field_names:
                return len(self._field_names)
            elif self._rows:
                return len(self._rows[0])
            else:
                return 0
        else:
            raise AttributeError(name)
 
    def __getitem__(self, index):

        new = PrettyTable()
        new.field_names = self.field_names
        for attr in self._options:
            setattr(new, "_"+attr, getattr(self, "_"+attr))
        setattr(new, "_align", getattr(self, "_align"))
        if isinstance(index, slice):
            for row in self._rows[index]:
                new.add_row(row)
        elif isinstance(index, int):
            new.add_row(self._rows[index])
        else:
            raise Exception("Index %s is invalid, must be an integer or slice" % str(index))
        return new

    if py3k:
        def __str__(self):
           return self.__unicode__()
    else:
        def __str__(self):
           return self.__unicode__().encode(self.encoding)

    def __unicode__(self):
        return self.get_string()

    ##############################
    # ATTRIBUTE VALIDATORS       #
    ##############################

    # The method _validate_option is all that should be used elsewhere in the code base to validate options.
    # It will call the appropriate validation method for that option.  The individual validation methods should
    # never need to be called directly (although nothing bad will happen if they *are*).
    # Validation happens in TWO places.
    # Firstly, in the property setters defined in the ATTRIBUTE MANAGMENT section.
    # Secondly, in the _get_options method, where keyword arguments are mixed with persistent settings

    def _validate_option(self, option, val):
        if option in ("field_names"):
            self._validate_field_names(val)
        elif option in ("start", "end", "max_width", "min_width", "min_table_width", "max_table_width", "padding_width", "left_padding_width", "right_padding_width", "format"):
            self._validate_nonnegative_int(option, val)
        elif option in ("sortby"):
            self._validate_field_name(option, val)
        elif option in ("sort_key"):
            self._validate_function(option, val)
        elif option in ("hrules"):
            self._validate_hrules(option, val)
        elif option in ("vrules"):
            self._validate_vrules(option, val)
        elif option in ("fields"):
            self._validate_all_field_names(option, val)
        elif option in ("header", "border", "reversesort", "xhtml", "print_empty", "oldsortslice"):
            self._validate_true_or_false(option, val)
        elif option in ("header_style"):
            self._validate_header_style(val)
        elif option in ("int_format"):
            self._validate_int_format(option, val)
        elif option in ("float_format"):
            self._validate_float_format(option, val)
        elif option in ("vertical_char", "horizontal_char", "junction_char"):
            self._validate_single_char(option, val)
        elif option in ("attributes"):
            self._validate_attributes(option, val)

    def _validate_field_names(self, val):
        # Check for appropriate length
        if self._field_names:
            try:
               assert len(val) == len(self._field_names)
            except AssertionError:
               raise Exception("Field name list has incorrect number of values, (actual) %d!=%d (expected)" % (len(val), len(self._field_names)))
        if self._rows:
            try:
               assert len(val) == len(self._rows[0])
            except AssertionError:
               raise Exception("Field name list has incorrect number of values, (actual) %d!=%d (expected)" % (len(val), len(self._rows[0])))
        # Check for uniqueness
        try:
            assert len(val) == len(set(val))
        except AssertionError:
            raise Exception("Field names must be unique!")

    def _validate_header_style(self, val):
        try:
            assert val in ("cap", "title", "upper", "lower", None)
        except AssertionError:
            raise Exception("Invalid header style, use cap, title, upper, lower or None!")

    def _validate_align(self, val):
        try:
            assert val in ["l","c","r"]
        except AssertionError:
            raise Exception("Alignment %s is invalid, use l, c or r!" % val)

    def _validate_valign(self, val):
        try:
            assert val in ["t","m","b",None]
        except AssertionError:
            raise Exception("Alignment %s is invalid, use t, m, b or None!" % val)

    def _validate_nonnegative_int(self, name, val):
        try:
            assert int(val) >= 0
        except AssertionError:
            raise Exception("Invalid value for %s: %s!" % (name, self._unicode(val)))

    def _validate_true_or_false(self, name, val):
        try:
            assert val in (True, False)
        except AssertionError:
            raise Exception("Invalid value for %s!  Must be True or False." % name)

    def _validate_int_format(self, name, val):
        if val == "":
            return
        try:
            assert type(val) in (str, unicode)
            assert val.isdigit()
        except AssertionError:
            raise Exception("Invalid value for %s!  Must be an integer format string." % name)

    def _validate_float_format(self, name, val):
        if val == "":
            return
        try:
            assert type(val) in (str, unicode)
            assert "." in val
            bits = val.split(".")
            assert len(bits) <= 2
            assert bits[0] == "" or bits[0].isdigit()
            assert bits[1] == "" or bits[1].isdigit()
        except AssertionError:
            raise Exception("Invalid value for %s!  Must be a float format string." % name)

    def _validate_function(self, name, val):
        try:
            assert hasattr(val, "__call__")
        except AssertionError:
            raise Exception("Invalid value for %s!  Must be a function." % name)

    def _validate_hrules(self, name, val):
        try:
            assert val in (ALL, FRAME, HEADER, NONE)
        except AssertionError:
            raise Exception("Invalid value for %s!  Must be ALL, FRAME, HEADER or NONE." % name)

    def _validate_vrules(self, name, val):
        try:
            assert val in (ALL, FRAME, NONE)
        except AssertionError:
            raise Exception("Invalid value for %s!  Must be ALL, FRAME, or NONE." % name)

    def _validate_field_name(self, name, val):
        try:
            assert (val in self._field_names) or (val is None)
        except AssertionError:
            raise Exception("Invalid field name: %s!" % val)

    def _validate_all_field_names(self, name, val):
        try:
            for x in val:
                self._validate_field_name(name, x)
        except AssertionError:
            raise Exception("fields must be a sequence of field names!")

    def _validate_single_char(self, name, val):
        try:
            assert _str_block_width(val) == 1
        except AssertionError:
            raise Exception("Invalid value for %s!  Must be a string of length 1." % name)

    def _validate_attributes(self, name, val):
        try:
            assert isinstance(val, dict)
        except AssertionError:
            raise Exception("attributes must be a dictionary of name/value pairs!")

    ##############################
    # ATTRIBUTE MANAGEMENT       #
    ##############################

    @property
    def field_names(self):
        """List or tuple of field names"""
        return self._field_names

    @field_names.setter
    def field_names(self, val):
        val = [self._unicode(x) for x in val]
        self._validate_option("field_names", val)
        if self._field_names:
            old_names = self._field_names[:]
        self._field_names = val
        if self._align and old_names:
            for old_name, new_name in zip(old_names, val):
                self._align[new_name] = self._align[old_name]
            for old_name in old_names:
                if old_name not in self._align:
                    self._align.pop(old_name)
        else:
            for field in self._field_names:
                self._align[field] = "c"
        if self._valign and old_names:
            for old_name, new_name in zip(old_names, val):
                self._valign[new_name] = self._valign[old_name]
            for old_name in old_names:
                if old_name not in self._valign:
                    self._valign.pop(old_name)
        else:
            for field in self._field_names:
                self._valign[field] = "t"
    @property
    def align(self):
        return self._align

    @align.setter
    def align(self, val):
        self._validate_align(val)
        for field in self._field_names:
            self._align[field] = val

    @property
    def valign(self):
        return self._valign

    @valign.setter
    def valign(self, val):
        self._validate_valign(val)
        for field in self._field_names:
            self._valign[field] = val

    @property
    def max_width(self):
        return self._max_width

    @max_width.setter
    def max_width(self, val):
        self._validate_option("max_width", val)
        for field in self._field_names:
            self._max_width[field] = val
   
    @property
    def min_width(self):
        return self._min_width

    @min_width.setter
    def min_width(self, val):
        self._validate_option("min_width", val)
        for field in self._field_names:
            self._min_width[field] = val

    @property
    def min_table_width(self):
        return self._min_table_width

    @min_table_width.setter
    def min_table_width(self, val):
        self._validate_option("min_table_width", val)
        self._min_table_width = val

    @property
    def max_table_width(self):
        return self._max_table_width

    @max_table_width.setter
    def max_table_width(self, val):
        self._validate_option("max_table_width", val)
        self._max_table_width = val

    @property
    def fields(self):
        """List or tuple of field names to include in displays"""
        return self._fields

    @fields.setter
    def fields(self, val):
        self._validate_option("fields", val)
        self._fields = val

    @property
    def title(self):
        """Optional table title

        Arguments:

        title - table title"""
        return self._title

    @title.setter
    def title(self, val):
        self._title = self._unicode(val)

    @property
    def start(self):
        """Start index of the range of rows to print

        Arguments:

        start - index of first data row to include in output"""
        return self._start

    @start.setter
    def start(self, val):
        self._validate_option("start", val)
        self._start = val

    @property
    def end(self):
        """End index of the range of rows to print

        Arguments:

        end - index of last data row to include in output PLUS ONE (list slice style)"""
        return self._end
    @end.setter
    def end(self, val):
        self._validate_option("end", val)
        self._end = val

    @property
    def sortby(self):
        """Name of field by which to sort rows

        Arguments:

        sortby - field name to sort by"""
        return self._sortby
    @sortby.setter
    def sortby(self, val):
        self._validate_option("sortby", val)
        self._sortby = val

    @property
    def reversesort(self):
        """Controls direction of sorting (ascending vs descending)

        Arguments:

        reveresort - set to True to sort by descending order, or False to sort by ascending order"""
        return self._reversesort
    @reversesort.setter
    def reversesort(self, val):
        self._validate_option("reversesort", val)
        self._reversesort = val

    @property
    def sort_key(self):
        """Sorting key function, applied to data points before sorting

        Arguments:

        sort_key - a function which takes one argument and returns something to be sorted"""
        return self._sort_key
    @sort_key.setter
    def sort_key(self, val):
        self._validate_option("sort_key", val)
        self._sort_key = val
 
    @property
    def header(self):
        """Controls printing of table header with field names

        Arguments:

        header - print a header showing field names (True or False)"""
        return self._header
    @header.setter
    def header(self, val):
        self._validate_option("header", val)
        self._header = val

    @property
    def header_style(self):
        """Controls stylisation applied to field names in header

        Arguments:

        header_style - stylisation to apply to field names in header ("cap", "title", "upper", "lower" or None)"""
        return self._header_style
    @header_style.setter
    def header_style(self, val):
        self._validate_header_style(val)
        self._header_style = val

    @property
    def border(self):
        """Controls printing of border around table

        Arguments:

        border - print a border around the table (True or False)"""
        return self._border
    @border.setter
    def border(self, val):
        self._validate_option("border", val)
        self._border = val

    @property
    def hrules(self):
        """Controls printing of horizontal rules after rows

        Arguments:

        hrules - horizontal rules style.  Allowed values: FRAME, ALL, HEADER, NONE"""
        return self._hrules
    @hrules.setter
    def hrules(self, val):
        self._validate_option("hrules", val)
        self._hrules = val

    @property
    def vrules(self):
        """Controls printing of vertical rules between columns

        Arguments:

        vrules - vertical rules style.  Allowed values: FRAME, ALL, NONE"""
        return self._vrules
    @vrules.setter
    def vrules(self, val):
        self._validate_option("vrules", val)
        self._vrules = val

    @property
    def int_format(self):
        """Controls formatting of integer data
        Arguments:

        int_format - integer format string"""
        return self._int_format
    @int_format.setter
    def int_format(self, val):
        for field in self._field_names:
            self._int_format[field] = val

    @property
    def float_format(self):
        """Controls formatting of floating point data
        Arguments:

        float_format - floating point format string"""
        return self._float_format
    @float_format.setter
    def float_format(self, val):
        for field in self._field_names:
            self._float_format[field] = val

    @property
    def padding_width(self):
        """The number of empty spaces between a column's edge and its content

        Arguments:

        padding_width - number of spaces, must be a positive integer"""
        return self._padding_width
    @padding_width.setter
    def padding_width(self, val):
        self._validate_option("padding_width", val)
        self._padding_width = val

    @property
    def left_padding_width(self):
        """The number of empty spaces between a column's left edge and its content

        Arguments:

        left_padding - number of spaces, must be a positive integer"""
        return self._left_padding_width
    @left_padding_width.setter
    def left_padding_width(self, val):
        self._validate_option("left_padding_width", val)
        self._left_padding_width = val

    @property
    def right_padding_width(self):
        """The number of empty spaces between a column's right edge and its content

        Arguments:

        right_padding - number of spaces, must be a positive integer"""
        return self._right_padding_width
    @right_padding_width.setter
    def right_padding_width(self, val):
        self._validate_option("right_padding_width", val)
        self._right_padding_width = val

    @property
    def vertical_char(self):
        """The charcter used when printing table borders to draw vertical lines

        Arguments:

        vertical_char - single character string used to draw vertical lines"""
        return self._vertical_char
    @vertical_char.setter
    def vertical_char(self, val):
        val = self._unicode(val)
        self._validate_option("vertical_char", val)
        self._vertical_char = val

    @property
    def horizontal_char(self):
        """The charcter used when printing table borders to draw horizontal lines

        Arguments:

        horizontal_char - single character string used to draw horizontal lines"""
        return self._horizontal_char
    @horizontal_char.setter
    def horizontal_char(self, val):
        val = self._unicode(val)
        self._validate_option("horizontal_char", val)
        self._horizontal_char = val

    @property
    def junction_char(self):
        """The charcter used when printing table borders to draw line junctions

        Arguments:

        junction_char - single character string used to draw line junctions"""
        return self._junction_char
    @junction_char.setter
    def junction_char(self, val):
        val = self._unicode(val)
        self._validate_option("vertical_char", val)
        self._junction_char = val

    @property
    def format(self):
        """Controls whether or not HTML tables are formatted to match styling options

        Arguments:

        format - True or False"""
        return self._format
    @format.setter
    def format(self, val):
        self._validate_option("format", val)
        self._format = val

    @property
    def print_empty(self):
        """Controls whether or not empty tables produce a header and frame or just an empty string

        Arguments:

        print_empty - True or False"""
        return self._print_empty
    @print_empty.setter
    def print_empty(self, val):
        self._validate_option("print_empty", val)
        self._print_empty = val

    @property
    def attributes(self):
        """A dictionary of HTML attribute name/value pairs to be included in the <table> tag when printing HTML

        Arguments:

        attributes - dictionary of attributes"""
        return self._attributes

    @attributes.setter
    def attributes(self, val):
        self._validate_option("attributes", val)
        self._attributes = val

    @property
    def oldsortslice(self):
        """ oldsortslice - Slice rows before sorting in the "old style" """
        return self._oldsortslice
    @oldsortslice.setter
    def oldsortslice(self):
        self._validate_option("oldsortslice", val)
        self._oldsortslice = val

    ##############################
    # OPTION MIXER               #
    ##############################

    def _get_options(self, kwargs):

        options = {}
        for option in self._options:
            if option in kwargs:
                self._validate_option(option, kwargs[option])
                options[option] = kwargs[option]
            else:
                options[option] = getattr(self, "_"+option)
        return options

    ##############################
    # PRESET STYLE LOGIC         #
    ##############################

    def set_style(self, style):

        if style == DEFAULT:
            self._set_default_style()
        elif style == MSWORD_FRIENDLY:
            self._set_msword_style()
        elif style == PLAIN_COLUMNS:
            self._set_columns_style()
        elif style == RANDOM:
            self._set_random_style()
        else:
            raise Exception("Invalid pre-set style!")

    def _set_default_style(self):

        self.header = True
        self.border = True
        self._hrules = FRAME
        self._vrules = ALL
        self.padding_width = 1
        self.left_padding_width = 1
        self.right_padding_width = 1
        self.vertical_char = "|"
        self.horizontal_char = "-"
        self.junction_char = "+"

    def _set_msword_style(self):

        self.header = True
        self.border = True
        self._hrules = NONE
        self.padding_width = 1
        self.left_padding_width = 1
        self.right_padding_width = 1
        self.vertical_char = "|"

    def _set_columns_style(self):

        self.header = True
        self.border = False
        self.padding_width = 1
        self.left_padding_width = 0
        self.right_padding_width = 8

    def _set_random_style(self):

        # Just for fun!
        self.header = random.choice((True, False))
        self.border = random.choice((True, False))
        self._hrules = random.choice((ALL, FRAME, HEADER, NONE))
        self._vrules = random.choice((ALL, FRAME, NONE))
        self.left_padding_width = random.randint(0,5)
        self.right_padding_width = random.randint(0,5)
        self.vertical_char = random.choice("~!@#$%^&*()_+|-=\{}[];':\",./;<>?")
        self.horizontal_char = random.choice("~!@#$%^&*()_+|-=\{}[];':\",./;<>?")
        self.junction_char = random.choice("~!@#$%^&*()_+|-=\{}[];':\",./;<>?")

    ##############################
    # DATA INPUT METHODS         #
    ##############################

    def add_row(self, row):

        """Add a row to the table

        Arguments:

        row - row of data, should be a list with as many elements as the table
        has fields"""

        if self._field_names and len(row) != len(self._field_names):
            raise Exception("Row has incorrect number of values, (actual) %d!=%d (expected)" %(len(row),len(self._field_names)))
        if not self._field_names:
            self.field_names = [("Field %d" % (n+1)) for n in range(0,len(row))]
        self._rows.append(list(row))

    def del_row(self, row_index):

        """Delete a row to the table

        Arguments:

        row_index - The index of the row you want to delete.  Indexing starts at 0."""

        if row_index > len(self._rows)-1:
            raise Exception("Cant delete row at index %d, table only has %d rows!" % (row_index, len(self._rows)))
        del self._rows[row_index]

    def add_column(self, fieldname, column, align="c", valign="t"):

        """Add a column to the table.

        Arguments:

        fieldname - name of the field to contain the new column of data
        column - column of data, should be a list with as many elements as the
        table has rows
        align - desired alignment for this column - "l" for left, "c" for centre and "r" for right
        valign - desired vertical alignment for new columns - "t" for top, "m" for middle and "b" for bottom"""

        if len(self._rows) in (0, len(column)):
            self._validate_align(align)
            self._validate_valign(valign)
            self._field_names.append(fieldname)
            self._align[fieldname] = align
            self._valign[fieldname] = valign
            for i in range(0, len(column)):
                if len(self._rows) < i+1:
                    self._rows.append([])
                self._rows[i].append(column[i])
        else:
            raise Exception("Column length %d does not match number of rows %d!" % (len(column), len(self._rows)))

    def clear_rows(self):

        """Delete all rows from the table but keep the current field names"""

        self._rows = []

    def clear(self):

        """Delete all rows and field names from the table, maintaining nothing but styling options"""

        self._rows = []
        self._field_names = []
        self._widths = []

    ##############################
    # MISC PUBLIC METHODS        #
    ##############################

    def copy(self):
        return copy.deepcopy(self)

    ##############################
    # MISC PRIVATE METHODS       #
    ##############################

    def _format_value(self, field, value):
        if isinstance(value, int) and field in self._int_format:
            value = self._unicode(("%%%sd" % self._int_format[field]) % value)
        elif isinstance(value, float) and field in self._float_format:
            value = self._unicode(("%%%sf" % self._float_format[field]) % value)
        return self._unicode(value)

    def _compute_table_width(self, options):
        table_width = 2 if options["vrules"] in (FRAME, ALL) else 0
        per_col_padding = sum(self._get_padding_widths(options))
        for index, fieldname in enumerate(self.field_names):
            if not options["fields"] or (options["fields"] and fieldname in options["fields"]):
                table_width += self._widths[index] + per_col_padding
        return table_width

    def _compute_widths(self, rows, options):
        if options["header"]:
            widths = [_get_size(field)[0] for field in self._field_names]
        else:
            widths = len(self.field_names) * [0]

        for row in rows:
            for index, value in enumerate(row):
                fieldname = self.field_names[index]
                if fieldname in self.max_width:
                    widths[index] = max(widths[index], min(_get_size(value)[0], self.max_width[fieldname]))
                else:
                    widths[index] = max(widths[index], _get_size(value)[0])
                if fieldname in self.min_width:
                    widths[index] = max(widths[index], self.min_width[fieldname])
        self._widths = widths

        # Are we exceeding max_table_width?
        if self._max_table_width:
            table_width = self._compute_table_width(options)
            if table_width > self._max_table_width:
                # Shrink widths in proportion
                scale = 1.0*self._max_table_width / table_width
                widths = [int(math.floor(w*scale)) for w in widths]
                self._widths = widths

        # Are we under min_table_width or title width?
        if self._min_table_width or options["title"]:
            if options["title"]:
                title_width = len(options["title"])+sum(self._get_padding_widths(options))
                if options["vrules"] in (FRAME, ALL):
                    title_width += 2
            else:
                title_width = 0
            min_table_width = self.min_table_width or 0
            min_width = max(title_width, min_table_width)
            table_width = self._compute_table_width(options)
            if table_width < min_width:
                # Grow widths in proportion
                scale = 1.0*min_width / table_width
                widths = [int(math.ceil(w*scale)) for w in widths]
                self._widths = widths

    def _get_padding_widths(self, options):

        if options["left_padding_width"] is not None:
            lpad = options["left_padding_width"]
        else:
            lpad = options["padding_width"]
        if options["right_padding_width"] is not None:
            rpad = options["right_padding_width"]
        else:
            rpad = options["padding_width"]
        return lpad, rpad

    def _get_rows(self, options):
        """Return only those data rows that should be printed, based on slicing and sorting.

        Arguments:

        options - dictionary of option settings."""
      
        if options["oldsortslice"]:
            rows = copy.deepcopy(self._rows[options["start"]:options["end"]])
        else:
            rows = copy.deepcopy(self._rows)

        # Sort
        if options["sortby"]:
            sortindex = self._field_names.index(options["sortby"])
            # Decorate
            rows = [[row[sortindex]]+row for row in rows]
            # Sort
            rows.sort(reverse=options["reversesort"], key=options["sort_key"])
            # Undecorate
            rows = [row[1:] for row in rows]

        # Slice if necessary
        if not options["oldsortslice"]:
            rows = rows[options["start"]:options["end"]]

        return rows
        
    def _format_row(self, row, options):
        return [self._format_value(field, value) for (field, value) in zip(self._field_names, row)]

    def _format_rows(self, rows, options):
        return [self._format_row(row, options) for row in rows]
 
    ##############################
    # PLAIN TEXT STRING METHODS  #
    ##############################

    def get_string(self, **kwargs):

        """Return string representation of table in current state.

        Arguments:

        title - optional table title
        start - index of first data row to include in output
        end - index of last data row to include in output PLUS ONE (list slice style)
        fields - names of fields (columns) to include
        header - print a header showing field names (True or False)
        border - print a border around the table (True or False)
        hrules - controls printing of horizontal rules after rows.  Allowed values: ALL, FRAME, HEADER, NONE
        vrules - controls printing of vertical rules between columns.  Allowed values: FRAME, ALL, NONE
        int_format - controls formatting of integer data
        float_format - controls formatting of floating point data
        padding_width - number of spaces on either side of column data (only used if left and right paddings are None)
        left_padding_width - number of spaces on left hand side of column data
        right_padding_width - number of spaces on right hand side of column data
        vertical_char - single character string used to draw vertical lines
        horizontal_char - single character string used to draw horizontal lines
        junction_char - single character string used to draw line junctions
        sortby - name of field to sort rows by
        sort_key - sorting key function, applied to data points before sorting
        reversesort - True or False to sort in descending or ascending order
        print empty - if True, stringify just the header for an empty table, if False return an empty string """

        options = self._get_options(kwargs)

        lines = []

        # Don't think too hard about an empty table
        # Is this the desired behaviour?  Maybe we should still print the header?
        if self.rowcount == 0 and (not options["print_empty"] or not options["border"]):
            return ""

        # Get the rows we need to print, taking into account slicing, sorting, etc.
        rows = self._get_rows(options)

        # Turn all data in all rows into Unicode, formatted as desired
        formatted_rows = self._format_rows(rows, options)

        # Compute column widths
        self._compute_widths(formatted_rows, options)

        # Add header or top of border
        lines.append(self._stringify_header(options))

        # Add rows
        for row in formatted_rows:
            lines.append(self._stringify_row(row, options))

        return self._unicode("\n").join(lines)

    def _stringify_header(self, options):
        bits = []
        lines = []
        lpad, rpad = self._get_padding_widths(options)
        bits.append(" ")
        lines.append(" ")

        # For tables with no data or field names
        if not self._field_names:
            bits.append(" ")
            lines.append(" ")
        for field, width, in zip(self._field_names, self._widths):
            if options["fields"] and field not in options["fields"]:
                continue
            if self._header_style == "cap":
                fieldname = field.capitalize()
            elif self._header_style == "title":
                fieldname = field.title()
            elif self._header_style == "upper":
                fieldname = field.upper()
            elif self._header_style == "lower":
                fieldname = field.lower()
            else:
                fieldname = field
            fieldline = '-' * len(fieldname)
            bits.append(" " * lpad + self._justify(fieldname, width, self._align[field]) + " " * rpad)
            lines.append(" " * lpad + self._justify(fieldline, width, self._align[field]) + " " * rpad)

            bits.append(" ")
            lines.append(" ")

        bits.append("\n")
        bits.append("".join(lines))
        return "".join(bits)

    def _stringify_row(self, row, options):
       
        for index, field, value, width, in zip(range(0,len(row)), self._field_names, row, self._widths):
            # Enforce max widths
            lines = value.split("\n")
            new_lines = []
            for line in lines: 
                if _str_block_width(line) > width:
                    line = textwrap.fill(line, width)
                new_lines.append(line)
            lines = new_lines
            value = "\n".join(lines)
            row[index] = value

        row_height = 0
        for c in row:
            h = _get_size(c)[1]
            if h > row_height:
                row_height = h

        bits = []
        lpad, rpad = self._get_padding_widths(options)
        for y in range(0, row_height):
            bits.append([])
            bits[y].append(" ")

        for field, value, width, in zip(self._field_names, row, self._widths):

            valign = self._valign[field]
            lines = value.split("\n")
            dHeight = row_height - len(lines)
            if dHeight:
                if valign == "m":
                  lines = [""] * int(dHeight / 2) + lines + [""] * (dHeight - int(dHeight / 2))
                elif valign == "b":
                  lines = [""] * dHeight + lines
                else:
                  lines = lines + [""] * dHeight

            y = 0
            for l in lines:
                if options["fields"] and field not in options["fields"]:
                    continue

                bits[y].append(" " * lpad + self._justify(l, width, self._align[field]) + " " * rpad)
                bits[y].append(" ")
                y += 1

        for y in range(0, row_height):
            bits[y] = "".join(bits[y])

        return "\n".join(bits)

##############################
# UNICODE WIDTH FUNCTIONS    #
##############################

def _char_block_width(char):
    # Basic Latin, which is probably the most common case
    #if char in xrange(0x0021, 0x007e):
    #if char >= 0x0021 and char <= 0x007e:
    if 0x0021 <= char <= 0x007e:
        return 1
    # Chinese, Japanese, Korean (common)
    if 0x4e00 <= char <= 0x9fff:
        return 2
    # Hangul
    if 0xac00 <= char <= 0xd7af:
        return 2
    # Combining?
    if unicodedata.combining(uni_chr(char)):
        return 0
    # Hiragana and Katakana
    if 0x3040 <= char <= 0x309f or 0x30a0 <= char <= 0x30ff:
        return 2
    # Full-width Latin characters
    if 0xff01 <= char <= 0xff60:
        return 2
    # CJK punctuation
    if 0x3000 <= char <= 0x303e:
        return 2
    # Backspace and delete
    if char in (0x0008, 0x007f):
        return -1
    # Other control characters
    elif char in (0x0000, 0x001f):
        return 0
    # Take a guess
    return 1

def _str_block_width(val):

    return sum(itermap(_char_block_width, itermap(ord, _re.sub("", val))))


########NEW FILE########
