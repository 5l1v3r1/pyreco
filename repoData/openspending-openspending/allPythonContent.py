__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# Open Spending documentation build configuration file, created by
# sphinx-quickstart on Fri Mar 11 20:25:54 2011.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.append(os.path.abspath('.'))
sys.path.insert(0, os.path.abspath('_exts'))

# -- General configuration -----------------------------------------------------

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.doctest', 'edit_on_github' ]

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'OpenSpending'
project_short_name = u'OpenSpending'
copyright = u'''&copy; 2009-2014, <a href="http://okfn.org/">Open Knowledge Foundation</a>.
    Licensed under <a
    href="http://creativecommons.org/licenses/by-sa/3.0/">Creative Commons
    Attribution ShareAlike (Unported) v3.0 License</a>.<br />
    <img src="http://i.creativecommons.org/l/by-sa/3.0/80x15.png" alt="CC License Logo" />
    <a href="http://opendefinition.org/"><img src="http://assets.okfn.org/images/ok_buttons/oc_80x15_blue.png" border="0"
      alt="{{ _('Open Content') }}" /></a>
  '''
html_show_sphinx = False

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
import openspending
version = openspending.__version__.rstrip('abcdefgh')
# The full version, including alpha/beta/rc tags.
release = openspending.__version__

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of documents that shouldn't be included in the build.
#unused_docs = []

# List of directories, relative to source directory, that shouldn't be searched
# for source files.
exclude_trees = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  Major themes that come with
# Sphinx are currently 'default' and 'sphinxdoc'.
sys.path.append(os.path.abspath('_themes'))
html_theme_path = ['_themes']
html_theme = 'sphinx-theme-okfn'
html_theme_options = {
    'logo_icon': 'http://assets.okfn.org/p/opendatahandbook/img/data-wrench-inverted.png',
    'show_version': True
    }

html_sidebars = {
    '**':  ['localtoc.html', 'globaltoc.html', 'sourcelink.html']
}


# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
html_title = '%s v%s Docs' % (project, release)

# A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = "%s Docs" % (project_short_name)

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_use_modindex = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# If nonempty, this is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = ''

# Output file base name for HTML help builder.
htmlhelp_basename = 'OpenSpendingdoc'


# -- Options for LaTeX output --------------------------------------------------

# The paper size ('letter' or 'a4').
#latex_paper_size = 'letter'

# The font size ('10pt', '11pt' or '12pt').
#latex_font_size = '10pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'OpenSpending.tex', u'Open Spending Documentation',
   u'Open Knowledge Foundation', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# Additional stuff for the LaTeX preamble.
#latex_preamble = ''

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_use_modindex = True

# -- Options for edit_on_github extension --------------------------------------

edit_on_github_project = 'openspending/openspending'
edit_on_github_branch = 'master'
edit_on_github_root_url = 'https://github.com'
edit_on_github_doc_root = 'doc'

########NEW FILE########
__FILENAME__ = edit_on_github
# Licensed under a 3-clause BSD style license - see LICENSE.rst
"""
This extension makes it easy to edit documentation on GitHub. It is heavily
inspired by the Sphinx extension of exactly the same name in the Astropy
repository:

  https://github.com/astropy/astropy/blob/master/astropy/sphinx/ext/edit_on_github.py

It adds a field to the page template context called ``edit_on_github`` which
can be used to link to the GitHub edit page.

It has the following configuration options (to be set in the project's
``conf.py``):

* `edit_on_github_project`
    The name of the github project, in the form "username/projectname".

* `edit_on_github_branch`
    The name of the branch to edit.  If this is a released version, this should
    be a git tag referring to that version.  For a dev version, it often makes
    sense for it to be "master".  It may also be a git hash.

* `edit_on_github_root_url`
    The root URL of the GitHub instance on which you wish to edit your
    documentation.

* `edit_on_github_doc_root`
    The location within the source tree of the root of the
    documentation source.  Defaults to "doc", but it may make sense to
    set it to "doc/source" if the project uses a separate source
    directory.
"""
import os


def get_url_base(app):
    return '%s/%s/edit/%s/' % (
        app.config.edit_on_github_root_url,
        app.config.edit_on_github_project,
        app.config.edit_on_github_branch)


def html_page_context(app, pagename, templatename, context, doctree):
    if templatename == 'page.html':
        doc_root = app.config.edit_on_github_doc_root
        if doc_root != '' and not doc_root.endswith('/'):
            doc_root += '/'
        doc_path = os.path.relpath(doctree.get('source'), app.builder.srcdir)
        url = get_url_base(app)

        context['edit_on_github'] = url + doc_root + doc_path


def setup(app):
    app.add_config_value('edit_on_github_project', 'REQUIRED', True)
    app.add_config_value('edit_on_github_branch', 'master', True)
    app.add_config_value('edit_on_github_root_url', 'https://github.com', True)
    app.add_config_value('edit_on_github_doc_root', 'doc', True)

    app.connect('html-page-context', html_page_context)

########NEW FILE########
__FILENAME__ = 001_account_dataset
from datetime import datetime

from sqlalchemy import *
from migrate import *

meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine
    dataset = Table('dataset', meta,
                    Column('id', Integer, primary_key=True),
                    Column('name', Unicode(255), unique=True),
                    Column('label', Unicode(2000)),
                    Column('description', Unicode),
                    Column('currency', Unicode),
                    Column('default_time', Unicode),
                    Column('data', Text),
                    )

    account = Table('account', meta,
                    Column('id', Integer, primary_key=True),
                    Column('name', Unicode(255), unique=True),
                    Column('fullname', Unicode(2000)),
                    Column('email', Unicode(2000)),
                    Column('password', Unicode(2000)),
                    Column('api_key', Unicode(2000)),
                    Column('admin', Boolean, default=False),
                    )

    dataset.create()
    account.create()

def downgrade(migrate_engine):
    raise NotImplementedError()

########NEW FILE########
__FILENAME__ = 002_dataset_managers
from datetime import datetime

from sqlalchemy import *
from migrate import *

meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine
    dataset = Table('dataset', meta, autoload=True)
    account = Table('account', meta, autoload=True)

    account_dataset_table = Table('account_dataset', meta,
            Column('dataset_id', Integer, ForeignKey('dataset.id'),
                primary_key=True),
            Column('account_id', Integer, ForeignKey('account.id'),
                primary_key=True)
        )

    account_dataset_table.create()
    private = Column('private', Boolean, default=False)
    private.create(dataset)
    u = dataset.update(values={'private': False})
    migrate_engine.execute(u)


def downgrade(migrate_engine):
    raise NotImplementedError()


########NEW FILE########
__FILENAME__ = 003_sources
from datetime import datetime

from sqlalchemy import *
from migrate import *

meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine
    dataset = Table('dataset', meta, autoload=True)
    account = Table('account', meta, autoload=True)

    source_table = Table('source', meta,
        Column('id', Integer, primary_key=True),
        Column('url', Unicode),
        Column('analysis', Unicode, default=dict),
        Column('created_at', DateTime, default=datetime.utcnow),
        Column('dataset_id', Integer, ForeignKey('dataset.id')),
        Column('creator_id', Integer, ForeignKey('account.id'))
        )

    source_table.create()

def downgrade(migrate_engine):
    raise NotImplementedError()




########NEW FILE########
__FILENAME__ = 004_run_and_log_record
from sqlalchemy import *
from migrate import *

meta = MetaData()

def upgrade(migrate_engine):
    meta.bind = migrate_engine
    dataset = Table('dataset', meta, autoload=True)
    source = Table('source', meta, autoload=True)

    run_table = Table('run', meta,
        Column('id', Integer, primary_key=True),
        Column('operation', Unicode(2000)),
        Column('status', Unicode(2000)),
        Column('time_start', DateTime),
        Column('time_end', DateTime),
        Column('dataset_id', Integer, 
            ForeignKey('dataset.id'), nullable=True),
        Column('source_id', Integer, 
            ForeignKey('source.id'), nullable=True)
        )
    run_table.create()

    run_log_record = Table('log_record', meta,
        Column('id', Integer, primary_key=True),
        Column('run_id', Integer, ForeignKey('run.id')),
        Column('category', Unicode),
        Column('level', Unicode),
        Column('message', Unicode),
        Column('timestamp', DateTime),
        Column('error', Unicode),
        Column('row', Integer),
        Column('attribute', Unicode),
        Column('column', Unicode),
        Column('data_type', Unicode),
        Column('value', Unicode)
        )
    run_log_record.create()


########NEW FILE########
__FILENAME__ = 005_dataset_properties
from sqlalchemy import *
from migrate import *

meta = MetaData()

def upgrade(migrate_engine):
    meta.bind = migrate_engine
    dataset = Table('dataset', meta, autoload=True)

    entry_custom_html = Column('entry_custom_html', Unicode())
    entry_custom_html.create(dataset) 

########NEW FILE########
__FILENAME__ = 006_timestamps
from datetime import datetime

from sqlalchemy import *
from migrate import *

meta = MetaData()

def upgrade(migrate_engine):
    meta.bind = migrate_engine

    dataset = Table('dataset', meta, autoload=True)
    created_at = Column('created_at', DateTime, default=datetime.utcnow)
    created_at.create(dataset) 
    updated_at = Column('updated_at', DateTime, onupdate=datetime.utcnow)
    updated_at.create(dataset) 

    source = Table('source', meta, autoload=True)
    updated_at = Column('updated_at', DateTime, onupdate=datetime.utcnow)
    updated_at.create(source) 

########NEW FILE########
__FILENAME__ = 007_ds_language_and_country
from datetime import datetime

from sqlalchemy import *
from migrate import *

meta = MetaData()

def upgrade(migrate_engine):
    meta.bind = migrate_engine
    dataset = Table('dataset', meta, autoload=True)

    dataset_language = Table('dataset_language', meta,
        Column('id', Integer, primary_key=True),
        Column('code', Unicode),
        Column('created_at', DateTime, default=datetime.utcnow),
        Column('updated_at', DateTime, onupdate=datetime.utcnow),
        Column('dataset_id', Integer, ForeignKey('dataset.id'))
        )
    dataset_language.create()
    
    dataset_territory = Table('dataset_territory', meta,
        Column('id', Integer, primary_key=True),
        Column('code', Unicode),
        Column('created_at', DateTime, default=datetime.utcnow),
        Column('updated_at', DateTime, onupdate=datetime.utcnow),
        Column('dataset_id', Integer, ForeignKey('dataset.id'))
        )
    dataset_territory.create()


########NEW FILE########
__FILENAME__ = 008_dataset_schema_version
from sqlalchemy import *
from migrate import *

meta = MetaData()

def upgrade(migrate_engine):
    meta.bind = migrate_engine
    dataset = Table('dataset', meta, autoload=True)

    v = Column('schema_version', Unicode())
    v.create(dataset) 


########NEW FILE########
__FILENAME__ = 009_ckan_uri
from sqlalchemy import *
from migrate import *

meta = MetaData()

def upgrade(migrate_engine):
    meta.bind = migrate_engine
    dataset = Table('dataset', meta, autoload=True)

    ckan_uri = Column('ckan_uri', Unicode())
    ckan_uri.create(dataset) 


########NEW FILE########
__FILENAME__ = 010_saved_views
from datetime import datetime

from sqlalchemy import *
from migrate import *

meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine
    Table('dataset', meta, autoload=True)
    Table('account', meta, autoload=True)

    view = Table('view', meta,
        Column('id', Integer, primary_key=True),
        Column('widget', Unicode(2000)),
        Column('name', Unicode(2000)),
        Column('label', Unicode(2000)),
        Column('description', Unicode()),
        Column('state', Unicode()),
        Column('public', Boolean),
        Column('created_at', DateTime),
        Column('updated_at', DateTime),
        Column('dataset_id', Integer, ForeignKey('dataset.id')),
        Column('account_id', Integer, ForeignKey('account.id'),
            nullable=True)
        )
    view.create()


def downgrade(migrate_engine):
    raise NotImplementedError()

########NEW FILE########
__FILENAME__ = 011_account_script_root
from sqlalchemy import *
from migrate import *

meta = MetaData()

def upgrade(migrate_engine):
    meta.bind = migrate_engine
    dataset = Table('account', meta, autoload=True)

    v = Column('script_root', Unicode())
    v.create(dataset)


########NEW FILE########
__FILENAME__ = 012_dataset_category
from sqlalchemy import *
from migrate import *

meta = MetaData()

def upgrade(migrate_engine):
    meta.bind = migrate_engine
    dataset = Table('dataset', meta, autoload=True)

    category = Column('category', Unicode())
    category.create(dataset) 

    u = dataset.update(values={'category': 'other'})
    migrate_engine.execute(u)

########NEW FILE########
__FILENAME__ = 013_dataset_serp
from sqlalchemy import *
from migrate import *

meta = MetaData()

def upgrade(migrate_engine):
    meta.bind = migrate_engine
    dataset = Table('dataset', meta, autoload=True)

    serp_title = Column('serp_title', Unicode())
    serp_title.create(dataset)

    serp_teaser = Column('serp_teaser', Unicode())
    serp_teaser.create(dataset) 


########NEW FILE########
__FILENAME__ = 014_Add_terms_column
from sqlalchemy import Boolean, Column, MetaData, Table


meta = MetaData()


def upgrade(migrate_engine):
    meta.bind = migrate_engine
    account = Table('account', meta, autoload=True)

    termsc = Column('terms', Boolean, default=False)
    termsc.create(account, populate_default=True)


def downgrade(migrate_engine):
    meta.bind = migrate_engine
    account = Table('account', meta, autoload=True)

    account.c.terms.drop()

########NEW FILE########
__FILENAME__ = 015_badges
from datetime import datetime

from sqlalchemy import *
from migrate import *

meta = MetaData()

def upgrade(migrate_engine):
    meta.bind = migrate_engine
    dataset = Table('dataset', meta, autoload=True)
    account = Table('account', meta, autoload=True)

    badge = Table('badge', meta,
        Column('id', Integer, primary_key=True),
        Column('label', Unicode),
        Column('image', Unicode),
        Column('description', Unicode),
        Column('creator_id', Integer, ForeignKey('account.id')),
        Column('created_at', DateTime, default=datetime.utcnow),
        Column('updated_at', DateTime, default=datetime.utcnow),
        )

    badge.create()

    badges_on_datasets = Table('badges_on_datasets', meta,
        Column('badge_id', Integer, ForeignKey('badge.id')),
        Column('dataset_id', Integer, ForeignKey('dataset.id'))
        )

    badges_on_datasets.create()

def downgrade(migrate_engine):
    raise NotImplementedError()




########NEW FILE########
__FILENAME__ = 016_contact_info
from datetime import datetime

from sqlalchemy import *
from migrate import *

meta = MetaData()

def upgrade(migrate_engine):
    meta.bind = migrate_engine

    account = Table('account', meta, autoload=True)

    # Column that stores the user's twitter handle
    twitter_handle = Column('twitter_handle', Unicode)
    twitter_handle.create(account)

    # Should email address be public?
    public_email = Column('public_email', Boolean, default=False)
    public_email.create(account)

    # Should twitter handle be public?
    public_twitter = Column('public_twitter', Boolean, default=False)
    public_twitter.create(account)

def downgrade(migrate_engine):
    raise NotImplementedError()

########NEW FILE########
__FILENAME__ = 017_secret_key
from datetime import datetime

from sqlalchemy import *
from migrate import *
import uuid

meta = MetaData()

def upgrade(migrate_engine):
    meta.bind = migrate_engine

    account = Table('account', meta, autoload=True)

    # Column that stores the user's secret api key
    secret_api_key = Column('secret_api_key', Unicode, default=unicode(uuid.uuid4()))
    secret_api_key.create(account)


def downgrade(migrate_engine):
    raise NotImplementedError()

########NEW FILE########
__FILENAME__ = 018_drop_secret_key
from sqlalchemy import *
from migrate import *

meta = MetaData()

def upgrade(migrate_engine):
    meta.bind = migrate_engine
    account = Table('account', meta, autoload=True)
    account.c.secret_api_key.drop()

def downgrade(migrate_engine):
    raise NotImplementedError()

########NEW FILE########
__FILENAME__ = account
from pylons import tmpl_context


def logged_in():
    return hasattr(
        tmpl_context, 'account') and tmpl_context.account is not None


def create():
    return True


def read(account):
    return True


def update(account):
    return logged_in()


def delete(account):
    return False

########NEW FILE########
__FILENAME__ = badge
from account import logged_in
from pylons import tmpl_context


def create():
    """
    Permission to create a new badge. Only administrators can create badges.
    """
    return logged_in() and tmpl_context.account.admin


def give(badge, dataset):
    """
    Permission to give a badge to a dataset. Currently only administrators
    can reward datasets with badges.
    """
    return logged_in() and tmpl_context.account.admin

########NEW FILE########
__FILENAME__ = dataset
from account import logged_in
from pylons import tmpl_context


def create():
    return logged_in()


def read(dataset):
    if not dataset.private:
        return True
    return update(dataset)


def update(dataset):
    if logged_in():
        if tmpl_context.account.admin:
            return True
        elif tmpl_context.account in dataset.managers:
            return True
    return False


def delete(dataset):
    if logged_in():
        if tmpl_context.account.admin:
            return True
        elif tmpl_context.account in dataset.managers:
            return True
    return False

########NEW FILE########
__FILENAME__ = view
from account import logged_in
from pylons import tmpl_context

import dataset as ds


def create(dataset):
    return logged_in() and ds.read(dataset)


def read(dataset, view):
    return ds.read(dataset)


def update(dataset, view):
    if logged_in() and tmpl_context.account == view.account:
        return True
    return ds.update(dataset)


def delete(dataset, view):
    return update(dataset, view)

########NEW FILE########
__FILENAME__ = archive
from __future__ import print_function

import logging
import os
import posixpath
import sys
import json
import hashlib
import urllib

from openspending.model.dataset import Dataset
from openspending.model.source import Source

log = logging.getLogger(__name__)


def exit_with_error(message):
    """
    Exit the program after printing out a message to the stderr
    """

    # Print error message
    print(message, file=sys.stderr)
    # Exit program
    sys.exit(1)


def get_confirmation(message):
    """
    Get a y/n answer from the user (return True/False respectively).
    """

    # Get an answer from the user as lowercase
    answer = raw_input(message + " (y/n) ").lower()

    # Return True if user answered yes
    if answer == 'y':
        return True
    # Return False if user answered no
    if answer == 'n':
        return False

    # User answered something else so we ask again
    return get_confirmation(message)


def get_url_filename(url):
    """
    Get a base filename for a url. Returns short hashed url appended by the
    urn filename (basefile)
    """

    # Return the first 10 hexdigest chars of sha1 appended by the basename
    # we use posixpath so that getting the basename of the url will work on
    # non-unix (posix-compliant) systems as well
    return '-'.join([hashlib.sha1(url).hexdigest()[:10],
                     posixpath.basename(url)])


def file_name(path, source):
    """
    Return a filename based on the source url located at the relative or
    absolute path provided
    """

    file_name = get_url_filename(source.url)
    return os.path.join(path, file_name)


def sizeof_fmt(num):
    for x in ['bytes', 'KB', 'MB', 'GB']:
        if num < 1024.0:
            return "%3.1f%s" % (num, x)
        num /= 1024.0
    return "%3.1f%s" % (num, 'TB')


def update_source(archive_dir, source):
    if source.dataset is None:
        return
    fname = file_name(archive_dir, source)
    fname_tmp = fname + '.tmp'
    log.info("Fetching %s to %s", source.url, fname)
    if not os.path.isfile(fname):
        try:
            urllib.urlretrieve(source.url, fname_tmp)
            os.rename(fname_tmp, fname)
        except Exception as e:
            log.exception(e)
    if os.path.isfile(fname):
        log.info("OK: %s", sizeof_fmt(os.path.getsize(fname)))


def date_handler(obj):
    return obj.isoformat() if hasattr(obj, 'isoformat') else obj


def update(archive_dir, dataset=None):
    """
    Download all sources into an archive directory. If dataset parameter
    is provided only sources for that dataset will be fetched (otherwise
    all source in the database will be fetched)
    """

    # Create archive directory if it doesn't exist
    if not os.path.isdir(archive_dir):
        os.makedirs(archive_dir)

    # If a dataset is provided we limit to only its sources (else we take all)
    sources = Source.all() if dataset is None else dataset.sources

    # Update each source
    for source in sources:
        update_source(archive_dir, source)


def _update(args):
    return update(args.archive_dir)


def archive_model(dataset, archive_dir):
    """
    Archive the dataset model (sources, views, mapping, dataset metadata)
    and put it into model.json in the archive directory
    """

    # Create the dataset model (returns a dict with keys: mapping and dataset)
    model = dataset.model
    # Add sources to the dataset metadata
    model['dataset']['sources'] = [s.url for s in dataset.sources]

    # Let user know we're about to create model.json
    log.info("Creating %s/model.json for %s", archive_dir, dataset.name)

    # Write the result to a file
    with open(os.path.join(archive_dir, 'model.json'), 'w') as f:
        print(json.dumps(model, indent=2, default=date_handler), file=f)


def archive_visualisations(dataset, archive_dir):
    """
    Archive the visualisations for a dataset and put it into a special
    visualisations.json file in the archive directory
    """

    visualisations = {'visualisations': [v.as_dict() for v in dataset.views]}
    log.info('Creating %s/visualisations.json for %s',
             archive_dir, dataset.name)

    # Write the result to a file
    with open(os.path.join(archive_dir, 'visualisations.json'), 'w') as f:
        print(json.dumps(visualisations, indent=2, default=date_handler),
              file=f)


def archive_one(dataset_name, archive_dir):
    """
    Find the dataset, create the archive directory and start archiving
    """

    # Find the dataset
    dataset = Dataset.by_name(dataset_name)
    # If no dataset found, exit with error message
    if dataset is None:
        exit_with_error("Dataset not found. Unable to archive it.")

    # If the archive_dir exists we have to ask the user if we should overwrite
    if os.path.exists(archive_dir):
        # If user doesn't want to write over it we exit
        if not get_confirmation("%s exists. Do you want to overwrite?"
                                % archive_dir):
            sys.exit(0)
        # If the archive dir is a file we don't do anything
        if os.path.isfile(archive_dir):
            exit_with_error("Cannot overwrite a file (need a directory).")
    # If the archive_dir doesn't exist we create it
    else:
        try:
            os.makedirs(archive_dir)
        except OSError:
            # If we couldn't create it, we exit with an error message
            exit_with_error("Couldn't create archive directory.")

    # Archive the model (dataset metadata)
    archive_model(dataset, archive_dir)
    # Archive the visualisations
    archive_visualisations(dataset, archive_dir)
    # Download all sources
    update(os.path.join(archive_dir, 'sources'), dataset)


def _archive_one(args):
    """
    Kick off archiving based on args object and check if all arguments are
    provided.
    """

    # Check if dataset parameter has been provided and exit with error
    # message if not
    if not args.dataset:
        exit_with_error("You must provide a dataset")

    # Check if archive directory has been provided and set it as the dataset
    # name if not
    if not args.output:
        args.output = args.dataset

    # Archive the dataset
    archive_one(args.dataset, args.output)


def configure_parser(subparsers):
    parser = subparsers.add_parser('archive', help='Archival of source data')
    sp = parser.add_subparsers(title='subcommands')

    p = sp.add_parser('update',
                      help='Create a source data archive')
    p.add_argument('archive_dir', help="Archive folder path")
    p.set_defaults(func=_update)

    # Add a subcommand to archive one specific dataset
    dataset = sp.add_parser('one',
                            help='Archive a single dataset')
    # Identifier of dataset must be provided
    dataset.add_argument('--output', action="store", dest='output',
                         default=None, metavar='output directory',
                         help="Archive folder path")
    # Where to store the archive (directory should not exist)
    dataset.add_argument('dataset', help="Dataset Identifier")
    dataset.set_defaults(func=_archive_one)

########NEW FILE########
__FILENAME__ = db
import logging
import os

from pylons import config

from openspending.model import meta as db
from openspending.model.dataset import Dataset

import migrate.versioning.api as migrate_api
from migrate.exceptions import DatabaseNotControlledError

log = logging.getLogger(__name__)


def drop():
    log.warn("Dropping database")
    db.metadata.reflect()
    db.metadata.drop_all()
    return 0


def drop_collections():
    # Kept for backwards compatibility
    return drop()


def drop_dataset(name):
    log.warn("Dropping dataset '%s'", name)
    dataset = db.session.query(Dataset).filter_by(name=name).first()
    if dataset is None:
        log.warn("Dataset does not exist: '%s'", name)
        return 1
    dataset.drop()
    db.session.delete(dataset)
    db.session.commit()
    return 0


def migrate():
    url = config.get('openspending.db.url')
    repo = config.get('openspending.migrate_dir',
                      os.path.join(os.path.dirname(config['__file__']),
                                   'migration'))

    try:
        migrate_api.upgrade(url, repo)
    except DatabaseNotControlledError:
        # Assume it's a new database, and try the migration again
        migrate_api.version_control(url, repo)
        migrate_api.upgrade(url, repo)

    diff = migrate_api.compare_model_to_db(url, repo, db.metadata)
    if diff:
        # Oh dear! The database we migrated to doesn't match openspending.model
        print diff
        return 1

    return 0


def modelmigrate():
    from openspending.validation.model.migration import migrate_model
    dataset = db.Table('dataset', db.metadata, autoload=True)
    rp = db.engine.execute(dataset.select())
    while True:
        ds = rp.fetchone()
        if ds is None:
            break
        print ds['name'], '...'
        model = migrate_model(ds['data'])
        version = model.get('dataset').get('schema_version')
        if 'dataset' in model:
            del model['dataset']
        q = dataset.update().where(dataset.c.id == ds['id'])
        q = q.values({'data': model, 'schema_version': version})
        db.engine.execute(q)
    return 0


def init():
    migrate()


def _init(args):
    return init()


def _drop(args):
    return drop()


def _drop_collections(args):
    return drop_collections()


def _drop_dataset(args):
    return drop_dataset(args.name)


def _migrate(args):
    return migrate()


def _modelmigrate(args):
    return modelmigrate()


def configure_parser(subparsers):
    parser = subparsers.add_parser('db', help='Database operations')
    sp = parser.add_subparsers(title='subcommands')

    p = sp.add_parser('drop', help='Drop database')
    p.set_defaults(func=_drop)

    p = sp.add_parser('dropcollections',
                      help='Drop collections within database')
    p.set_defaults(func=_drop_collections)

    p = sp.add_parser('dropdataset',
                      help='Drop a dataset from the database')
    p.add_argument('name')
    p.set_defaults(func=_drop_dataset)

    p = sp.add_parser('migrate',
                      help='Run pending data migrations')
    p.set_defaults(func=_migrate)

    p = sp.add_parser('modelmigrate',
                      help='Run pending data model migrations')
    p.set_defaults(func=_modelmigrate)

    p = sp.add_parser('init',
                      help='Initialize the database')
    p.set_defaults(func=_init)

########NEW FILE########
__FILENAME__ = importer
from __future__ import print_function

import argparse
import logging
import sys
import os
import urllib2
import urlparse

from openspending.lib import json

from openspending.model.source import Source
from openspending.model.dataset import Dataset
from openspending.model.account import Account
from openspending.model.view import View
from openspending.model import meta as db

from openspending.importer import CSVImporter
from openspending.importer.analysis import analyze_csv

from openspending.command.archive import get_url_filename

from openspending.validation.model import validate_model
from openspending.validation.model import Invalid

log = logging.getLogger(__name__)

SHELL_USER = 'system'

import_parser = argparse.ArgumentParser(add_help=False)

import_parser.add_argument('-n', '--dry-run',
                           action="store_true", dest='dry_run', default=False,
                           help="Perform a dry run, don't load any data.")

import_parser.add_argument('--no-index', action="store_false",
                           dest='build_indices', default=True,
                           help='Suppress Solr index build.')

import_parser.add_argument('--max-lines', action="store", dest='max_lines',
                           type=int, default=None, metavar='N',
                           help="Number of lines to import.")

import_parser.add_argument('--raise-on-error', action="store_true",
                           dest='raise_errors', default=False,
                           help='Get full traceback on first error.')


def shell_account():
    account = Account.by_name(SHELL_USER)
    if account is not None:
        return account
    account = Account()
    account.name = SHELL_USER
    db.session.add(account)
    return account


def _is_local_file(url):
    """
    Check to see if the provided url is a local file. Returns True if it is
    and False if it isn't. This method only checks if their is a scheme
    associated with the url or not (so file:location will be regarded as a url)
    """

    # Parse the url and check if scheme is '' (no scheme)
    parsed_result = urlparse.urlparse(url)
    return parsed_result.scheme == ''


def json_of_url(url):
    # Check if it's a local file
    if _is_local_file(url):
        # If it is we open it as a normal file
        return json.load(open(url, 'r'))
    else:
        # If it isn't we open the url as a file
        return json.load(urllib2.urlopen(url))


def create_view(dataset, view_config):
    """
    Create view for a provided dataset from a view provided as dict
    """

    # Check if it exists (if not we create it)
    existing = View.by_name(dataset, view_config['name'])
    if existing is None:
        # Create the view
        view = View()

        # Set saved configurations
        view.widget = view_config['widget']
        view.state = view_config['state']
        view.name = view_config['name']
        view.label = view_config['label']
        view.description = view_config['description']
        view.public = view_config['public']

        # Set the dataset as the current dataset
        view.dataset = dataset

        # Try and set the account provided but if it doesn't exist
        # revert to shell account
        view.account = Account.by_name(view_config['account'])
        if view.account is None:
            view.account = shell_account()

        # Commit view to database
        db.session.add(view)
        db.session.commit()


def get_model(model):
    """
    Get and validate the model. If the model doesn't validate we exit the
    program.
    """

    # Get and parse the model
    model = json_of_url(model)

    # Validate the model
    try:
        log.info("Validating model")
        model = validate_model(model)
    except Invalid as i:
        log.error("Errors occured during model validation:")
        for field, error in i.asdict().items():
            log.error("%s: %s", field, error)
        sys.exit(1)

    # Return the model
    return model


def get_or_create_dataset(model):
    """
    Based on a provided model we get the model (if it doesn't exist we
    create it).
    """

    # Get the dataset by the name provided in the model
    dataset = Dataset.by_name(model['dataset']['name'])

    # If the dataset wasn't found we create it
    if dataset is None:
        dataset = Dataset(model)
        db.session.add(dataset)
        db.session.commit()

    # Log information about the dataset and return it
    log.info("Dataset: %s", dataset.name)
    return dataset


def import_csv(dataset, url, args):
    """
    Import the csv data into the dataset
    """

    csv_data_url, source_url = url
    source = Source(dataset, shell_account(),
                    csv_data_url)
    # Analyse the csv data and add it to the source
    # If we don't analyse it we'll be left with a weird message
    source.analysis = analyze_csv(csv_data_url)
    # Check to see if the dataset already has this source
    for source_ in dataset.sources:
        if source_.url == csv_data_url:
            source = source_
            break
    db.session.add(source)
    db.session.commit()

    dataset.generate()
    importer = CSVImporter(source)
    importer.run(**vars(args))

    # Check if imported from the file system (source and data url differ)
    if csv_data_url != source_url:
        # If we did, then we must update the source url based on the
        # sources in the dataset model (so we need to fetch the source again
        # or else we'll add a new one)
        source = Source.by_id(source.id)
        source.url = source_url
        db.session.commit()


def import_views(dataset, views_url):
    """
    Import views into the provided dataset which are defined in a json object
    located at the views_url
    """

    # Load the json and loop over its 'visualisations' property
    for view in json_of_url(views_url)['visualisations']:
        create_view(dataset, view)


def map_source_urls(model, urls):
    """
    Go through the source urls of the dataset model and map them to the
    files or urls. Returns a dict where the key is the url and the value
    is how it should be represented in the dataset.
    """

    # Create map from file to model sources
    source_files = {get_url_filename(s): s
                    for s in model['dataset'].get('sources', [])}

    # Return a map for the representation of csv urls
    return {u: source_files.get(os.path.basename(u), u) for u in urls}


def _csvimport(args):
    """
    Parse the arguments and pass them to the processing functions
    """
    # Get the model
    model = get_model(args.model)

    # Get the source map (data urls to models)
    source_map = map_source_urls(model, args.dataset_urls)

    # Get the dataset for the model
    dataset = get_or_create_dataset(model)

    # For every url in mapped dataset_urls (arguments) we import it
    for urlmap in source_map.iteritems():
        import_csv(dataset, urlmap, args)

    # Import visualisations if there are any
    if args.views:
        import_views(dataset, args.views)


def configure_parser(subparser):
    p = subparser.add_parser('csvimport',
                             help='Load a CSV dataset',
                             description='You must specify --model.',
                             parents=[import_parser])
    # Add the model argument which is required
    p.add_argument('--model', action="store", dest='model',
                   default=None, metavar='url', required=True,
                   help="URL of JSON format model (metadata and mapping).")
    # Allow user to define url or file with visualisations
    p.add_argument('--visualisations', action="store", dest="views",
                   default=None, metavar='url/file',
                   help="URL/file of JSON format visualisations.")
    # Load multiple sources via the dataset_urls (all remaining arguments)
    p.add_argument('dataset_urls', nargs=argparse.REMAINDER,
                   help="Dataset file URL")
    p.set_defaults(func=_csvimport)

########NEW FILE########
__FILENAME__ = solr
from openspending.lib import solr_util as solr


def load(dataset):
    solr.build_index(dataset)
    return 0


def loadall():
    solr.build_all_index()
    return 0


def delete(dataset):
    solr.drop_index(dataset)
    return 0


def optimize():
    solr.get_connection().optimize()
    return 0


def clean():
    s = solr.get_connection()
    s.delete_query('*:*')
    s.commit()
    return 0


def _load(args):
    return load(args.dataset)


def _loadall(args):
    return loadall()


def _delete(args):
    return delete(args.dataset)


def _optimize(args):
    return optimize()


def _clean(args):
    return clean()


def configure_parser(subparsers):
    parser = subparsers.add_parser('solr',
                                   help='Solr index operations')
    sp = parser.add_subparsers(title='subcommands')

    p = sp.add_parser('load', help='Load data for dataset into Solr')
    p.add_argument('dataset')
    p.set_defaults(func=_load)

    p = sp.add_parser('loadall', help='Load data for all datasets into Solr')
    p.set_defaults(func=_loadall)

    p = sp.add_parser('delete', help='Delete data for dataset from Solr')
    p.add_argument('dataset')
    p.set_defaults(func=_delete)

    p = sp.add_parser('optimize', help='Optimize the Solr index')
    p.set_defaults(func=_optimize)

    p = sp.add_parser('clean', help='Empty/reset the Solr index')
    p.set_defaults(func=_clean)

########NEW FILE########
__FILENAME__ = user
def grant_admin(username):
    from openspending.model import meta as db
    from openspending.model.account import Account

    a = Account.by_name(username)

    if a is None:
        print "Account `%s` not found." % username
        return 1

    a.admin = True
    db.session.add(a)
    db.session.commit()

    return 0


def _grant_admin(args):
    return grant_admin(args.username)


def configure_parser(subparsers):
    parser = subparsers.add_parser('user', help='User operations')
    sp = parser.add_subparsers(title='subcommands')

    p = sp.add_parser('grantadmin',
                      help='Grant admin privileges to given user')
    p.add_argument('username')
    p.set_defaults(func=_grant_admin)

########NEW FILE########
__FILENAME__ = analysis
from collections import defaultdict
from urllib import urlopen
import logging

from messytables import CSVRowSet, type_guess
from messytables.types import TYPES, DateType
from openspending.lib.util import slugify

log = logging.getLogger(__name__)


class LocalDateType(DateType):
    formats = ["%Y", "%Y-%m", "%Y-%m-%d"]

LIMITED_TYPES = list(TYPES)
LIMITED_TYPES.remove(DateType)
LIMITED_TYPES.append(LocalDateType)


def frequent_values(sample):
    values = defaultdict(lambda: defaultdict(int))
    for row in sample:
        for i, value in enumerate(row):
            values[i][value.value] += 1
    sorted_values = []
    for idx, column in values.items():
        frequent = sorted(column.items(), key=lambda v_c: v_c[1], reverse=True)
        sorted_values.append(frequent[:5])
    return sorted_values


def analyze_csv(url, sample=1000):
    try:
        fileobj = urlopen(url)
        row_set = CSVRowSet('data', fileobj, window=sample)
        sample = list(row_set.sample)
        headers, sample = sample[0], sample[1:]
        # values = frequent_values(sample)
        types = type_guess(sample[500:], types=LIMITED_TYPES)
        mapping = {}
        for header, type_ in zip(headers, types):
            type_ = repr(type_).lower()
            name = slugify(header.value).lower()
            meta = {
                'label': header.value,
                'column': header.value,
                'datatype': type_
            }
            if type_ in ['decimal', 'integer', 'float']:
                meta['type'] = 'measure'
                meta['datatype'] = 'float'
            elif type_.startswith('date'):
                meta['type'] = 'date'
                meta['datatype'] = 'date'
            else:
                meta['type'] = 'attribute'
            mapping[name] = meta
        return {'columns': [h.value for h in headers],
                'mapping': mapping}
    except Exception as e:
        log.exception(e)
        return {'error': unicode(e)}

########NEW FILE########
__FILENAME__ = browser
import json
from collections import defaultdict

from openspending.model.dataset import Dataset
from openspending.lib import solr_util as solr
from openspending.lib import util


class Browser(object):

    def __init__(self,
                 q='',
                 filter=None,
                 page=1,
                 pagesize=100,
                 order=None,
                 stats=False,
                 facet_field=None,
                 facet_page=1,
                 facet_pagesize=100):

        self.params = {
            'q': q,
            'filter': filter if filter is not None else {},
            'page': page,
            'pagesize': pagesize,
            'order': order if order is not None else [('score', True),
                                                      ('amount', True)],
            'stats': stats,
            'facet_field': facet_field if facet_field is not None else [],
            'facet_page': facet_page,
            'facet_pagesize': facet_pagesize
        }

    def execute(self):
        """
        Run the query for this browser instance.

        Returns a tuple of dicts: (stats, facets, entries)
        """
        q = self.query()

        self.stats = {
            'results_count_query': q['response']['numFound'],
            'results_count': len(q['response']['docs'])
        }

        if self.params['stats']:
            self.stats.update(q.get('stats', {}).get('stats_fields', {}).
                              get('amount', {}))

        self.facets = q.get('facet_counts', {}).get('facet_fields', {})
        for k in self.facets.keys():
            self.facets[k] = _parse_facets(self.facets[k])

        self.entries = _get_entries(q['response']['docs'])

    def get_stats(self):
        return self.stats

    def get_facets(self):
        return self.facets

    def get_expanded_facets(self, dataset):
        return util.expand_facets(self.facets, dataset)

    def get_entries(self):
        return self.entries

    def query(self):
        data = solr.get_connection().raw_query(**_build_query(self.params))
        return json.loads(data)


def _build_query(params):
    query = {
        'q': params['q'] or '*:*',
        'fq': _build_fq(params['filter']),
        'wt': 'json',
        'fl': 'id, dataset',
        'sort': _build_sort(params['order']),
        'stats': str(params['stats']).lower(),
        'stats.field': 'amount',
        # FIXME: In a future version of the API, we really should use
        #        offset/limit rather than page/pagesize.
        #
        # NB: we permit fractional page sizes to overcome the limits of
        # page/pagesize vs offset/limit
        'start': int((params['page'] - 1) * params['pagesize']),
        'rows': params['pagesize'],
    }
    if params['facet_field']:
        query.update({
            'facet': 'true',
            'facet.field': params['facet_field'],
            'facet.mincount': 1,
            'facet.sort': 'count',
            # NB: we permit fractional page sizes to overcome the limits of
            # page/pagesize vs offset/limit
            'facet.offset': int((params['facet_page'] - 1)
                                * params['facet_pagesize']),
            'facet.limit': params['facet_pagesize']
        })
    return query


def _build_fq(filters):
    """
    Make a Solr 'fq' object from a filters dict.

    Returns a list, suitable for passing as the 'fq' keyword
    argument to ``raw_query()``
    """
    def fq_for(key, value):
        return "+%s:\"%s\"" % (key, value.replace('"', '\\"'))
    fq = []
    for key, value in filters.iteritems():
        if isinstance(value, basestring):
            fq.append(fq_for(key, value))
        else:
            fq.append(' OR '.join(map(lambda v: fq_for(key, v), value)))
    return fq


def _build_sort(order):
    sort = []
    for field, reverse in order:
        sort.append('{0} {1}'.format(field, 'desc' if reverse else 'asc'))
    return ', '.join(sort)


def _parse_facets(facets):
    out = []

    for i in xrange(0, len(facets), 2):
        out.append([facets[i], facets[i + 1]])

    return out


def _get_entries(docs):
    # List of ids in Solr return order
    # print [docs]
    ids = [d['id'] for d in docs]

    # partition the entries by dataset (so we make only N queries
    # for N datasets)
    by_dataset = defaultdict(list)
    for d in docs:
        by_dataset[d['dataset']].append(d['id'])

    # create a great big list of entries, one per doc
    entries = []
    for ds_name, ds_ids in by_dataset.iteritems():
        dataset = Dataset.by_name(ds_name)
        query = dataset.alias.c.id.in_(ds_ids)
        entries.extend([(dataset, e) for e in dataset.entries(query)])

    entries = util.sort_by_reference(ids, entries, lambda x: x[1]['id'])
    for dataset, entry in entries:
        yield dataset, entry

########NEW FILE########
__FILENAME__ = calculator
import csv
from StringIO import StringIO

# National statistics for tax paid per household income decile, 2008/9.
# Taken from Table 14 in http://www.statistics.gov.uk/CCI/article.asp?ID=2440
# Row 1 is average gross income per decile - including benefits, pensions etc
# Row 2 is average direct taxation - income tax, employee NI and council tax
# Row 3 is average total indirect taxation MINUS VAT, tobacco, alcohol,
# & car costs
# Row 4 is average total VAT paid
# Row 5 is average total tobacco tax paid
# Row 6 is average total alcohol tax (beer, cider, wine & spirits) paid
# Row 7 is average total car-related costs (petrol & car tax)
# We connect the data points by linear interpolation.
reader = csv.reader(StringIO('''
0,9219,13583,17204,22040,25190,32995,37592,46268,56889,94341
0,1172,1368,1939,3108,3973,6118,7423,10172,13463,23047
0,1016,969,1125,1262,1319,1507,1630,1884,2148,2622
0,1101,1085,1295,1562,1609,1927,2155,2616,2871,3747
0,288,310,317,320,295,341,286,311,235,251
0,150,167,182,243,222,261,306,392,450,526
0,349,289,373,505,519,632,727,851,909,949
'''))
income_table = list(reader)
income_table = [[(float(income)) for income in row] for row in income_table]


class TaxCalculator2010(object):

    def total_tax(self,
                  income,
                  spending=None,
                  is_smoker=True,
                  is_drinker=True,
                  is_driver=True,
                  ):
        '''Estimates a person's tax contribution based on the following
        information.

        :param income: Total household income. This is used to estimate both
          direct and indirect tax paid.
        :param spending: Household expenditure. No longer used, but included
          for back-compatibility.
        :param is_smoker: - `True` if the person smokes, or `False` if not.
          Default is True.
        :param is_drinker: - `True` if the person drinks, or `False` if not.
          Default is True.
        :param is_driver: - `True` if the person drives a car, `False` if not.
          Default is True.

        :returns: a pair `(total_tax, explanation)`.
            The `explanation` is a list of strings describing the steps of
            the calculation.
        '''
        lower = 0.0
        income = float(income)
        explanation = []
        tax_results = {}
        if income <= 0.0:
            tax_results['tax'] = 0.0
            explanation.append('Incomes must be positive.')
            return tax_results, explanation

        # First, find the relevant income deciles.
        for i, upper in enumerate(income_table[1]):
            if income < upper:
                # Found the right band. Use linear interpolation.
                explanation.append('''\
This household income falls between national average income decile %s \
(which has average gross household income of %.2f, and pays %.2f in \
direct tax, %.2f in VAT, %.2f in smoking taxes, %.2f in alcohol-related \
taxes, %.2f in car-related taxes, and %.2f in other indirect taxes), \
and decile %s (which has average gross household income of %.2f, and \
pays %.2f in direct tax, %.2f in VAT, %.2f in smoking taxes, %.2f in \
alcohol-related taxes, %.2f in car-related taxes, and %.2f in other \
indirect taxes).'''
                                   % (i - 1, lower,
                                      income_table[2][i - 1],
                                      income_table[3][i - 1],
                                      income_table[4][i - 1],
                                      income_table[5][i - 1],
                                      income_table[6][i - 1],
                                      income_table[7][i - 1],
                                      i, upper,
                                      income_table[2][i], income_table[3][i],
                                      income_table[4][i], income_table[5][i],
                                      income_table[6][i], income_table[7][i]))
                # Linear interpolation.
                multiplier = (income - lower) / (upper - lower)
                tax_results['total_direct_tax'] = income_table[2][i - 1] + \
                    (income_table[2][i] - income_table[2][i - 1]) * multiplier
                indirect_tax_minus_extras = income_table[3][i - 1] + \
                    (income_table[3][i] - income_table[3][i - 1]) * multiplier
                tax_results['vat'] = income_table[4][i - 1] + \
                    (income_table[4][i] - income_table[4][i - 1]) * multiplier
                if is_smoker:
                    tax_results['tobacco_tax'] = income_table[5][i - 1] + \
                        (income_table[5][i] - income_table[5]
                         [i - 1]) * multiplier
                else:
                    tax_results['tobacco_tax'] = 0
                if is_drinker:
                    tax_results['alcohol_tax'] = income_table[6][i - 1] + \
                        (income_table[6][i] - income_table[6]
                         [i - 1]) * multiplier
                else:
                    tax_results['alcohol_tax'] = 0
                if is_driver:
                    tax_results['car_related_tax'] = income_table[7][i - 1] + \
                        (income_table[7][i] - income_table[7]
                         [i - 1]) * multiplier
                else:
                    tax_results['car_related_tax'] = 0
                break
            else:
                lower = upper
        else:
            # Income is above all the bands. Use constant tax rates.
            direct_top_rate = income_table[2][-1] / income_table[1][-1]
            indirect_top_rate = income_table[3][-1] / income_table[1][-1]
            vat_top_rate = income_table[4][-1] / income_table[1][-1]
            tax_results['total_direct_tax'] = income * direct_top_rate
            indirect_tax_minus_extras = income * indirect_top_rate
            tax_results['vat'] = income * vat_top_rate
            if is_smoker:
                tax_results['tobacco_tax'] = income * \
                    (income_table[5][-1] / income_table[1][-1])
            else:
                tax_results['tobacco_tax'] = 0
            if is_drinker:
                tax_results['alcohol_tax'] = income * \
                    (income_table[6][-1] / income_table[1][-1])
            else:
                tax_results['alcohol_tax'] = 0
            if is_drinker:
                tax_results['car_related_tax'] = income * \
                    (income_table[7][-1] / income_table[1][-1])
            else:
                tax_results['car_related_tax'] = 0
            explanation.append('''\
For very high-earning households, in the top income decile, we don't use \
linear interpolation, but assume the fractions of income paid as tax are \
the average for the top decile.''')

        # Calculate indirect tax by adding up all the other kinds of tax.
        tax_results['total_indirect_tax'] = indirect_tax_minus_extras + \
            tax_results['vat'] + tax_results['tobacco_tax'] + \
            tax_results['alcohol_tax'] + tax_results['car_related_tax']

        # Set up the explanation text.
        explanation_text = 'Therefore, a'
        if not is_smoker:
            explanation_text += ' non-smoking'
        if not is_drinker:
            explanation_text += ' non-driving'
        if not is_driver:
            explanation_text += ' non-driving'
        explanation_text += ' household with an income of %.2f pays '\
            'approximately %.2f in direct tax and %.2f in total '\
            'indirect tax.' % \
            (income,
             tax_results['total_direct_tax'],
             tax_results['total_indirect_tax'])
        explanation.append(explanation_text)

        tax_results['tax'] = tax_results['total_direct_tax'] + \
            tax_results['total_indirect_tax']

        return tax_results, explanation

########NEW FILE########
__FILENAME__ = csvexport
import csv

from datetime import datetime
from StringIO import StringIO

from openspending.lib.util import flatten


def write_csv(entries, response, filename=None):
    csv_headers(response, filename)
    return generate_csv(entries)


def csv_headers(response, filename='download.csv'):
    response.content_type = 'text/csv'
    if filename:
        response.content_disposition = 'attachment; filename=%s' % filename


def generate_csv(entries, generate_headers=True):
    for entry in entries:
        yield generate_csv_row(entry, generate_headers)
        generate_headers = False


def generate_csv_row(entry, generate_headers=True):
    row = {}
    for k, v in flatten(entry).items():
        if isinstance(v, (list, tuple, dict)):
            continue
        elif isinstance(v, datetime):
            v = v.isoformat()
        elif isinstance(v, float):
            v = u'%.2f' % v
        row[unicode(k).encode('utf8')] = unicode(v).encode('utf8')

    fields = sorted(row.keys())
    sio = StringIO()
    writer = csv.DictWriter(sio, fields)
    if generate_headers:
        header = dict(zip(fields, fields))
        writer.writerow(header)

    writer.writerow(row)
    return sio.getvalue()

########NEW FILE########
__FILENAME__ = jsonexport
from datetime import datetime
import logging

from decorator import decorator

from pylons import request, response

from openspending.lib import json

log = logging.getLogger(__name__)


def default_json(obj):
    '''\
    Return a json representations for some custom objects.
    Used for the *default* parameter to json.dump[s](),
    see http://docs.python.org/library/json.html#json.dump

    Raises :exc:`TypeError` if it can't handle the object.
    '''
    if isinstance(obj, datetime):
        return obj.isoformat()
    if hasattr(obj, 'as_dict'):
        return obj.as_dict()
    raise TypeError("%r is not JSON serializable" % obj)


def write_json(entries, response, filename=None):
    response.content_type = 'application/json'
    if filename:
        response.content_disposition = 'attachment; filename=%s' % filename
    return generate_json(entries)


def generate_json(entries):
    for e in entries:
        yield to_json(e, indent=None) + '\n'


def write_browser_json(entries, stats, facets, response):
    """ Streaming support for large result sets, specific to the browser as
    the data is enveloped. """
    response.content_type = 'application/json'
    callback = None
    if 'callback' in request.params:
        response.content_type = 'text/javascript'
        callback = str(request.params['callback'])
    return generate_browser_json(entries, stats, facets, callback)


def generate_browser_json(entries, stats, facets, callback):
    yield callback + '({' if callback else '{'
    yield '"stats": %s, "facets": %s, "results": [' % (
        to_json(stats), to_json(facets))
    iter = entries.__iter__()
    has_next, first = True, True
    while has_next:
        try:
            row = iter.next()
        except StopIteration:
            has_next = False
        if has_next:
            if not first:
                yield ', '
            yield to_json(row)
        first = False
    yield ']})' if callback else ']}'


def to_json(data, indent=2):
    return json.dumps(data, default=default_json, indent=indent)


def json_headers(filename=None):
    if 'callback' in request.params:
        response.headers['Content-Type'] = 'text/javascript'
    else:
        response.headers['Content-Type'] = 'application/json'
    if filename:
        response.content_disposition = 'attachment; filename=%s' % filename


def generate_jsonp(data, indent=2, callback=None):
    result = to_json(data, indent=indent)
    if callback:
        # The parameter is a unicode object, which we don't want (as it
        # causes Pylons to complain when we return a unicode object from
        # this function).  All reasonable values of this parameter will
        # "str" with no problem (ASCII clean).  So we do that then.
        cbname = str(request.params['callback'])
        result = '%s(%s);' % (cbname, result)
    return result


def to_jsonp(data):
    is_xhr = request.headers.get(
        'x-requested-with',
        '').lower() == 'xmlhttprequest'
    indent = None if is_xhr else 2
    json_headers()
    return generate_jsonp(
        data, indent=indent, callback=request.params.get('callback'))


@decorator
def jsonpify(func, *args, **kwargs):
    """\
    A decorator that reformats the output as JSON; or, if the
    *callback* parameter is specified (in the HTTP request), as JSONP.

    Modelled after pylons.decorators.jsonify.
    """
    data = func(*args, **kwargs)
    return to_jsonp(data)

########NEW FILE########
__FILENAME__ = mailer
import smtplib
import logging
from time import time
from email.mime.text import MIMEText
from email.header import Header
from email import Utils

from pylons.i18n.translation import _
from pylons import config, app_globals
from openspending.ui.lib import helpers as h

log = logging.getLogger(__name__)


class MailerException(Exception):
    pass


def add_msg_niceties(recipient_name, body, sender_name):
    return _(u"Dear %s,") % recipient_name \
        + u"\r\n\r\n%s\r\n\r\n" % body \
        + u"--\r\n%s" % sender_name


def mail_recipient(recipient_name, recipient_email,
                   subject, body, headers=None):
    mail_from = config.get(
        'openspending.mail_from',
        'noreply@openspending.org')
    body = add_msg_niceties(recipient_name, body, app_globals.site_title)
    msg = MIMEText(body.encode('utf-8'), 'plain', 'utf-8')
    if headers:
        msg.update(headers)
    subject = Header(subject.encode('utf-8'), 'utf-8')
    msg['Subject'] = subject
    msg['From'] = _("%s <%s>") % (app_globals.site_title, mail_from)
    recipient = u"%s <%s>" % (recipient_name, recipient_email)
    msg['To'] = Header(recipient, 'utf-8')
    msg['Date'] = Utils.formatdate(time())
    msg['X-Mailer'] = "OpenSpending"
    try:
        server = smtplib.SMTP(config.get('smtp_server', 'localhost'))
        server.sendmail(mail_from, [recipient_email], msg.as_string())
        server.quit()
    except Exception as e:
        msg = '%r' % e
        log.exception(msg)
        raise MailerException(msg)


def mail_account(recipient, subject, body, headers=None):
    if (recipient.email is None) or not len(recipient.email):
        raise MailerException(_("No recipient email address available!"))
    mail_recipient(recipient.display_name, recipient.email, subject,
                   body, headers=headers)


RESET_LINK_MESSAGE = _(
    '''You have requested your password on %(site_title)s to be reset.

Please click the following link to confirm this request:

   %(reset_link)s
''')


def get_reset_body(account):
    reset_link = h.url_for(controller='account',
                           action='do_reset',
                           email=account.email,
                           token=account.token,
                           qualified=True)
    d = {
        'reset_link': reset_link,
        'site_title': app_globals.site_title
    }
    return RESET_LINK_MESSAGE % d


def send_reset_link(account):
    body = get_reset_body(account)
    mail_account(account, _('Reset your password'), body)

########NEW FILE########
__FILENAME__ = paramparser
from ordereddict import OrderedDict
import hashlib

from openspending.model.dataset import Dataset
from openspending.model.dimension import CompoundDimension

from openspending.reference.category import CATEGORIES
from openspending.reference.country import COUNTRIES
from openspending.reference.language import LANGUAGES


class ParamParser(object):
    defaults = OrderedDict()
    defaults['page'] = 1
    defaults['pagesize'] = 10000
    defaults['order'] = None

    def __init__(self, params):
        self.params = self.defaults.copy()
        self.params.update(params)

    def parse(self):
        self._output = {}
        self._errors = []

        for key in self.params.keys():
            if key not in self.defaults:
                continue

            parser = 'parse_{0}'.format(key)
            if hasattr(self, parser):
                result = getattr(self, parser)(self.params[key])
            else:
                result = self.params[key]

            if result is not None:
                self._output[key] = result

        return self._output, self._errors

    def key(self):
        params = sorted(self.params.items())
        return hashlib.sha1(repr(params)).hexdigest()

    def _error(self, msg):
        self._errors.append(msg)

    def parse_page(self, page):
        return max(1, self._to_float('page', page))

    def parse_pagesize(self, pagesize):
        return self._to_int('pagesize', pagesize)

    def parse_order(self, order):
        if not order:
            return []

        result = []
        for part in order.split('|'):
            try:
                dimension, direction = part.split(':')
            except ValueError:
                self._error('Wrong format for "order". It has to be '
                            'specified with request parameters in the form '
                            '"order=dimension:direction|dimension:direction". '
                            'We got: "order=%s"' % order)
                return
            else:
                if direction not in ('asc', 'desc'):
                    self._error('Order direction can be "asc" or "desc". We '
                                'got "%s" in "order=%s"' %
                                (direction, order))
                    return

                if direction == 'asc':
                    reverse = False
                else:
                    reverse = True

                result.append((dimension, reverse))
        return result

    def _to_float(self, name, value):
        try:
            return float(value)
        except ValueError:
            self._error('"%s" has to be a number, it is: %s' %
                        (name, value))

    def _to_int(self, name, value):
        try:
            return int(value)
        except ValueError:
            self._error('"%s" has to be an integer, it is: %s' %
                        (name, value))

    def _to_bool(self, value):
        if value.lower().strip() in ['true', '1', 'yes', 'on']:
            return True
        return False


class DatasetIndexParamParser(ParamParser):

    """
    Parameter parser for the dataset index page (which is served
    differently based on languages, territories and category chosen.
    """

    # We cannot use the defaults from ParamParser since that includes
    # order.
    defaults = OrderedDict()
    defaults['languages'] = []
    defaults['territories'] = []
    defaults['category'] = None
    # Used for pagination in html pages only
    defaults['page'] = 1
    defaults['pagesize'] = 25

    def __init__(self, params):
        """
        Initialize dataset index parameter parser, and make
        the initial params available as part of the instance
        """
        self.request_params = params
        super(DatasetIndexParamParser, self).__init__(params)

    def parse_languages(self, language):
        """
        Get the languages. This ignores the language supplied since multiple
        languages can be provided with multiple parameters and ParamParser
        does not support that.
        """
        # We force the language codes to lowercase and strip whitespace
        languages = [l.lower().strip()
                     for l in self.request_params.getall('languages')]
        # Check if this language is supported by OpenSpending
        # If not we add an error
        for lang in languages:
            if lang.lower().strip() not in LANGUAGES:
                self._error('Language %s not found' % lang)

        return languages

    def parse_territories(self, territory):
        """
        Get the territories. This ignores the territory supplied since multiple
        territories can be provided with multiple parameters and ParamParser
        does not support that.
        """
        # We force the territory codes to uppercase and strip whitespace
        # Isn't it great that we're so consistent with uppercase and lowercase
        # (uppercase here, lowercase in languages and categories)
        territories = [t.upper().strip()
                       for t in self.request_params.getall('territories')]

        # Check if this territory is supported by OpenSpending
        # If not we add an error
        for country in territories:
            if country not in COUNTRIES:
                self._error('Territory %s not found' % country)

        return territories

    def parse_category(self, category):
        """
        Get the category and check if it exists in
        supported categories. If so we return it.
        """
        if category:
            # We want the category to be lowercase and stripped of whitespace
            category = category.lower().strip()
            # Check if category is supported, if not add an error
            if category in CATEGORIES:
                return category
            else:
                self._error('Category %s not found' % category)

        # We return None if there's an error of no category
        return None


class AggregateParamParser(ParamParser):
    defaults = ParamParser.defaults.copy()
    defaults['dataset'] = None
    defaults['drilldown'] = None
    defaults['cut'] = None
    defaults['order'] = None
    defaults['inflate'] = None
    defaults['format'] = 'json'
    defaults['measure'] = 'amount'

    def parse_dataset(self, dataset_name):
        if not dataset_name:
            self._error('dataset name not provided')
            return

        dataset = Dataset.by_name(dataset_name)
        if dataset is None:
            self._error('no dataset with name "%s"' % dataset_name)
            return

        return dataset

    def parse_drilldown(self, drilldown):
        if not drilldown:
            return []
        return drilldown.split('|')

    def parse_format(self, format):
        format = format.lower().strip()
        if not format or format not in ('json', 'csv'):
            return 'json'
        return format

    def parse_cut(self, cuts):
        if not cuts:
            return []

        result = []
        for cut in cuts.split('|'):
            try:
                dimension, value = cut.split(':')
            except ValueError:
                self._error('Wrong format for "cut". It has to be specified '
                            'with request cut_parameters in the form '
                            '"cut=dimension:value|dimension:value". '
                            'We got: "cut=%s"' %
                            cuts)
                return
            else:
                result.append((dimension, value))
        return result

    def parse_measure(self, measures):
        """
        Parse the measure parameter which can be either a single measure or
        multiple measures separated by a pipe ('|'). The parser also checks
        to see if the measure is in fact a measure in the dataset model.

        Returns a list of measures even if it is only a single measure.
        Returns None if noe dataset or measure is not in the dataset's model
        (along with an error).
        """

        # Get the dataset which should already have been parsed
        if self._output.get('dataset') is None:
            return

        # Get a list of all measurement names for the given dataset
        measure_names = [m.name for m in self._output['dataset'].measures]

        result = []

        # Split the measures on | and check if it is in dataset if so append
        # it to our results, if not raise and error and return None
        for measure in measures.split('|'):
            if measure not in measure_names:
                self._error('no measure with name "%s"' % measure)
                return

            result.append(measure)

        return result


class EntryIndexParamParser(ParamParser):

    """
    This class extends the ParamParser to parse and provide an empty string
    default for the q parameter.
    """
    defaults = ParamParser.defaults.copy()
    defaults['q'] = ''
    defaults['filter'] = None
    defaults['facet_page'] = 1
    defaults['facet_pagesize'] = 100

    def parse_filter(self, filters):
        if not filters:
            return {}

        parsed_filters = {}

        for _filter in filters.split('|'):
            try:
                key, value = _filter.split(':')
                parsed_filters[key] = value
            except ValueError:
                self._error('Wrong format for "filter". It has to be '
                            'specified with request parameters in the form '
                            '"filter=key1:value1". '
                            'We got: "filter=%s"' % filter)
                break

        return parsed_filters


class SearchParamParser(ParamParser):
    defaults = ParamParser.defaults.copy()
    defaults['q'] = ''
    defaults['filter'] = None
    defaults['category'] = None
    defaults['dataset'] = None
    defaults['page'] = 1
    defaults['stats'] = 'false'
    defaults['pagesize'] = 100
    defaults['order'] = None
    defaults['facet_field'] = None
    defaults['facet_page'] = 1
    defaults['facet_pagesize'] = 100
    defaults['expand_facet_dimensions'] = None
    defaults['format'] = 'json'

    MAX_FACET_PAGESIZE = 100

    def parse_filter(self, filter):
        if not filter:
            return {}

        filters = {}
        for f in filter.split('|'):
            try:
                key, value = f.split(':')
            except ValueError:
                self._error('Wrong format for "filter". It has to be '
                            'specified with request parameters in the form '
                            '"filter=key1:value1|key2:value2". '
                            'We got: "filter=%s"' % filter)
                break
            else:
                filters[key] = value

        return filters

    def parse_format(self, format):
        format = format.lower().strip()
        if not format or format not in ('json', 'csv'):
            return 'json'
        return format

    def parse_dataset(self, dataset):
        datasets = []

        if dataset:
            for name in dataset.split('|'):
                dataset = Dataset.by_name(name)
                if dataset is None:
                    self._error('no dataset with name "%s"' % name)
                    return
                datasets.append(dataset)
        return datasets

    def parse_pagesize(self, pagesize):
        return self._to_int('pagesize', pagesize)

    def parse_category(self, category):
        category = category.lower().strip() if category else None
        if category in CATEGORIES:
            return category
        return None

    def parse_facet_field(self, facet_field):
        if not facet_field:
            return

        return facet_field.split('|')

    def parse_facet_page(self, page):
        return max(1, self._to_float('facet_page', page))

    def parse_stats(self, stats):
        return self._to_bool(stats)

    def parse_facet_pagesize(self, pagesize):
        return min(
            self.MAX_FACET_PAGESIZE, self._to_int('facet_pagesize', pagesize))

    def parse_expand_facet_dimensions(self, expand_facet_dimensions):
        return expand_facet_dimensions is not None


class DistinctParamParser(ParamParser):
    defaults = ParamParser.defaults.copy()
    defaults['q'] = ''
    defaults['page'] = 1
    defaults['pagesize'] = 100

    def parse_pagesize(self, pagesize):
        return min(100, self._to_int('pagesize', pagesize))


class DistinctFieldParamParser(DistinctParamParser):
    defaults = DistinctParamParser.defaults.copy()
    defaults['attribute'] = None

    def __init__(self, dimension, params):
        self.dimension = dimension
        super(DistinctFieldParamParser, self).__init__(params)

    def parse_attribute(self, attribute):
        if not isinstance(self.dimension, CompoundDimension):
            return self.dimension
        try:
            return self.dimension[attribute]
        except KeyError:
            return self.dimension['label']

########NEW FILE########
__FILENAME__ = solr_util
'''\
Helper methods for using Solr.
'''

import datetime
import logging
import json
from unicodedata import category

from solr import SolrConnection

from openspending.model.dataset import Dataset
from openspending.model import meta as db
from openspending.lib.util import flatten

log = logging.getLogger(__name__)

url = 'http://localhost:8983/solr'
http_user = None
http_pass = None

_client = None

# Solr connection singleton
_solr = None


# Helper class to represent the UTC timezone.
class UTC(datetime.tzinfo):

    def utcoffset(self, dt):
        return datetime.timedelta(0)

    def dst(self, dt):
        return datetime.timedelta(0)

    def tzname(self, dt):
        return "UTC"

    __reduce__ = object.__reduce__


def configure(config=None):
    global url
    global http_user
    global http_pass

    if not config:
        config = {}

    url = config.get('openspending.solr.url', url)
    http_user = config.get('openspending.solr.http_user', http_user)
    http_pass = config.get('openspending.solr.http_pass', http_pass)


def get_connection():
    """Returns the global Solr connection, or creates one, as required."""
    global _solr

    if _solr:
        return _solr

    _solr = SolrConnection(url,
                           http_user=http_user,
                           http_pass=http_pass)

    return _solr


def drop_index(dataset_name):
    solr = get_connection()
    solr.delete_query('dataset:%s' % dataset_name)
    solr.commit()


def dataset_entries(dataset_name):
    solr = get_connection()
    f = 'dataset:"%s"' % dataset_name if dataset_name else ''
    res = solr.raw_query(q='*:*', fq=f, rows=0, wt='json')
    res = json.loads(res)
    return res.get('response', {}).get('numFound')


def extend_entry(entry, dataset):
    entry['dataset'] = dataset.name
    entry['dataset.id'] = dataset.id
    entry = flatten(entry)
    entry['_id'] = dataset.name + '::' + unicode(entry['id'])
    for k, v in entry.items():
        if k.endswith(".taxonomy") or k.endswith('.color'):
            continue
        # this is similar to json encoding, but not the same.
        if isinstance(v, datetime.datetime) and not v.tzinfo:
            entry[k] = datetime.datetime(v.year, v.month, v.day, v.hour,
                                         v.minute, v.second, tzinfo=UTC())
        elif '.' in k and isinstance(v, (list, tuple)):
            entry[k] = " ".join([unicode(vi) for vi in v])
        else:
            entry[k] = _safe_unicode(entry[k])
        if k.endswith(".name"):
            vk = k[:len(k) - len(".name")]
            entry[vk] = v
        if k.endswith(".label"):
            entry[k + "_facet"] = entry[k]
    return entry


def build_index(dataset_name):
    solr = get_connection()
    dataset_ = Dataset.by_name(dataset_name)
    if dataset_ is None:
        raise ValueError("No such dataset: %s" % dataset_name)
    buf = []
    for i, entry in enumerate(dataset_.entries()):
        ourdata = extend_entry(entry, dataset_)
        # from pprint import pprint
        # pprint(ourdata)
        buf.append(ourdata)
        if i and i % 1000 == 0:
            solr.add_many(buf)
            solr.commit()
            log.info("Indexed %d entries", i)
            buf = []
    solr.add_many(buf)
    solr.commit()
    dataset_.updated_at = datetime.datetime.utcnow()
    db.session.commit()


def build_all_index():
    for dataset in db.session.query(Dataset):
        try:
            count = len(dataset)
        except:
            count = 0
        if count is 0:
            continue
        log.info("Indexing: %s (%s entries)", dataset.name, count)
        drop_index(dataset.name)
        build_index(dataset.name)


def _safe_unicode(s):
    if not isinstance(s, basestring):
        return s
    return u"".join([c for c in unicode(s) if not category(c)[0] == 'C'])

########NEW FILE########
__FILENAME__ = streaming
import json
import math

from openspending.lib import solr_util as solr
from openspending.lib.browser import Browser
from openspending.lib.csvexport import generate_csv_row
from openspending.lib.jsonexport import generate_jsonp, to_json
from openspending.ui.lib.hypermedia import (
    entry_apply_links,
    dataset_apply_links)


class StreamingResponse(object):

    def __init__(self, datasets, params, pagesize=100):
        self.datasets = datasets
        self.params = params
        param_page = params['page']
        param_pagesize = params['pagesize']
        self.start_page = (param_page - 1) * param_pagesize / float(pagesize)
        self.start_page = int(math.floor(self.start_page)) + 1
        self.start_offset = int(((param_page - 1) * param_pagesize) % pagesize)
        self.pagesize = pagesize

    def get_browser(self, page):
        current = dict(self.params)
        current['pagesize'] = self.pagesize
        current['page'] = page
        self.browser = Browser(**current)
        return self.browser

    def make_entries(self, entries):
        for dataset, entry in entries:
            entry = entry_apply_links(dataset.name, entry)
            entry['dataset'] = dataset_apply_links(dataset.as_dict())
            yield entry

    def entries_iterator(self, initial_page=None):
        if initial_page is None:
            initial_page = self.start_page
        i = initial_page - 1
        total_count = 0
        while True:
            i += 1
            b = self.get_browser(i)
            try:
                b.execute()
            except solr.SolrException as e:
                yield json.dumps({'errors': [unicode(e)]})
                raise StopIteration

            count = 0
            for entry in self.make_entries(b.get_entries()):
                count += 1
                if total_count == 0 and count <= self.start_offset:
                    continue
                total_count += 1
                if total_count > self.params['pagesize']:
                    raise StopIteration
                yield entry

            if count < self.pagesize:
                # There are no more results for the next page
                raise StopIteration
            if total_count >= self.params['pagesize']:
                # We have enough results
                raise StopIteration


class CSVStreamingResponse(StreamingResponse):

    def response(self):
        header = True
        for entry in self.entries_iterator():
            yield generate_csv_row(entry, header)
            header = False


class JSONStreamingResponse(StreamingResponse):

    def __init__(self, *args, **kwargs):
        self.expand_facets = kwargs.pop('expand_facets', None)
        self.callback = kwargs.pop('callback', None)
        super(JSONStreamingResponse, self).__init__(*args, **kwargs)

    def generate_json_frame(self):
        facets = self.browser.get_facets()
        stats = self.browser.get_stats()
        stats['results_count'] = self.params

        if self.expand_facets and len(self.datasets) == 1:
            facets = self.expand_facets(facets, self.datasets[0])

        template = generate_jsonp({
            'stats': stats,
            'facets': facets,
            'results': [None]
        }, indent=0, callback=self.callback)
        self.parts = template.split('null')

    def response(self):
        first = True
        for entry in self.entries_iterator():
            if first:
                self.generate_json_frame()
                yield self.parts[0]
                json_dict = to_json(entry)
            else:
                json_dict = ',' + to_json(entry)
            yield json_dict
            first = False
        yield self.parts[1]

########NEW FILE########
__FILENAME__ = util
import re
from hashlib import sha1
from unidecode import unidecode


def flatten(data, sep='.'):
    out = {}
    for k, v in data.items():
        ksep = k + sep
        if isinstance(v, dict):
            for ik, iv in flatten(v, sep).items():
                out[ksep + ik] = iv
        else:
            out[k] = v
    return out


def hash_values(iterable):
    """Return a cryptographic hash of an iterable."""
    return sha1(''.join(sha1(unicode(val).encode('utf-8')).hexdigest()
                        for val in iterable)).hexdigest()


def check_rest_suffix(name):
    '''\
    Assert that the ``name`` does not end with a string like
    '.csv', '.json'. Read the source for a list of all recognized
    extensions.
    '''
    for sfx in ['csv', 'json', 'xml', 'rdf', 'html', 'htm', 'n3', 'nt']:
        assert not name.lower().endswith('.' + sfx), \
            "Names cannot end in .%s" % sfx


SLUG_RE = re.compile(r'[\t !"#$%&\'()*\-/<=>?@\[\\\]^_`{|},.]+')


def slugify(text, delimiter='-'):
    '''\
    Generate an ascii only slug from the text that can be
    used in urls or as a name.
    '''
    result = []
    for word in SLUG_RE.split(unicode(text).lower()):
        result.extend(unidecode(word).split())
    return unicode(delimiter.join(result))


def sort_by_reference(ref, sort, sort_fn=None):
    """

    Sort the iterable ``sort`` by ``sort_fn`` (if omitted, the whole object
    will be used to sort) according the order defined by the list given in
    ``ref``.

    Will raise nasty errors if ``ref`` and ``sort`` aren't 1-to-1, and doesn't
    currently perform any error-checking to ensure that they are.

    Example:

        ids = [4, 7, 1, 3]
        objs = [{'id': 1}, {'id': 7}, {'id': 4}, {'id': 3}]

        sorted = sort_list_pair(ids, objs, lambda x: x['id'])
        # => [{'id': 4}, {'id': 7}, {'id': 1}, {'id': 3}]

    """
    if sort_fn is None:
        sort_fn = lambda x: x

    ref_map = dict((r, idx) for idx, r in enumerate(ref))

    ordered = [None] * len(ref)
    for x in sort:
        key = sort_fn(x)
        if key in ref_map:
            ordered[ref_map[key]] = x

    return filter(lambda x: x is not None, ordered)


def expand_facets(facets, dataset):
    """
    For the given dataset we return the facets as a dict with facet
    names for keys and the value is the list of its members along with
    the total count (facet_values).
    """

    # We'll fill in and return this dict
    expanded_facets = {}

    # Find dimension names in the dataset
    dimension_names = [d.name for d in dataset.dimensions]

    # Loop over all facets (their names)
    for (facet_name, facet_members) in facets.iteritems():
        # We only act on facets which are compound dimensions
        if facet_name in dimension_names and dataset[facet_name].is_compound:
            # Get the dimension from the dataset
            dimension = dataset[facet_name]
            # We get the member names and their facet values into
            # their own variables because we need to work more with
            # the member names
            member_names = []
            facet_values = []
            for member in facet_members:
                # We've processed the members so that they're tuples
                # that look like: (name,count)
                member_names.append(member[0])
                facet_values.append(member[1])

            # Get all the members for this dimension
            members = dimension.members(dimension.alias.c.name.
                                        in_(member_names))
            # We need to sort them by the member names so that they retain
            # the same order as the facet_alues
            members = sort_by_reference(member_names, members,
                                        lambda x: x['name'])

            # Now we zip them all up into tuples and add into the output dict
            expanded_facets[facet_name] = zip(members, facet_values)
        else:
            # If the facet isn't a compound dimension we still want to keep
            # it around
            expanded_facets[facet_name] = facet_members

    # ... and return it
    return expanded_facets

########NEW FILE########
__FILENAME__ = account
import colander
import uuid
import hmac

from sqlalchemy.orm import relationship, backref
from sqlalchemy.schema import Table, Column, ForeignKey
from sqlalchemy.types import Integer, Unicode, Boolean

from openspending.model import meta as db
from openspending.model.dataset import Dataset

REGISTER_NAME_RE = r"^[a-zA-Z0-9_\-]{3,255}$"


def make_uuid():
    return unicode(uuid.uuid4())


account_dataset_table = Table(
    'account_dataset', db.metadata,
    Column('dataset_id', Integer, ForeignKey('dataset.id'),
           primary_key=True),
    Column('account_id', Integer, ForeignKey('account.id'),
           primary_key=True)
)


class Account(db.Model):
    __tablename__ = 'account'

    id = Column(Integer, primary_key=True)
    name = Column(Unicode(255), unique=True)
    fullname = Column(Unicode(2000))
    email = Column(Unicode(2000))
    public_email = Column(Boolean, default=False)
    twitter_handle = Column(Unicode(140))
    public_twitter = Column(Boolean, default=False)
    password = Column(Unicode(2000))
    api_key = Column(Unicode(2000), default=make_uuid)
    admin = Column(Boolean, default=False)
    script_root = Column(Unicode(2000))
    terms = Column(Boolean, default=False)

    datasets = relationship(Dataset,
                            secondary=account_dataset_table,
                            backref=backref('managers', lazy='dynamic'))

    def __init__(self):
        pass

    @property
    def display_name(self):
        return self.fullname or self.name

    @property
    def token(self):
        h = hmac.new('')
        h.update(self.api_key)
        if self.password:
            h.update(self.password)
        return h.hexdigest()

    @classmethod
    def by_name(cls, name):
        return db.session.query(cls).filter_by(name=name).first()

    @classmethod
    def by_email(cls, email):
        return db.session.query(cls).filter_by(email=email).first()

    @classmethod
    def by_api_key(cls, api_key):
        return db.session.query(cls).filter_by(api_key=api_key).first()

    def as_dict(self):
        """
        Return the dictionary representation of the account
        """

        # Dictionary will include name, fullname, email and the admin bit
        account_dict = {
            'name': self.name,
            'fullname': self.fullname,
            'email': self.email,
            'admin': self.admin
        }

        # If the user has a twitter handle we add it
        if self.twitter_handle is not None:
            account_dict['twitter'] = self.twitter_handle

        # Return the dictionary representation
        return account_dict


class AccountRegister(colander.MappingSchema):
    name = colander.SchemaNode(colander.String(),
                               validator=colander.Regex(REGISTER_NAME_RE))

    fullname = colander.SchemaNode(colander.String())
    email = colander.SchemaNode(colander.String(),
                                validator=colander.Email())
    public_email = colander.SchemaNode(colander.Boolean(), missing=False)
    password1 = colander.SchemaNode(colander.String(),
                                    validator=colander.Length(min=4))
    password2 = colander.SchemaNode(colander.String(),
                                    validator=colander.Length(min=4))
    terms = colander.SchemaNode(colander.Bool())
    subscribe_community = colander.SchemaNode(colander.Boolean(),
                                              missing=False)
    subscribe_developer = colander.SchemaNode(colander.Boolean(),
                                              missing=False)


class AccountSettings(colander.MappingSchema):
    fullname = colander.SchemaNode(colander.String())
    email = colander.SchemaNode(colander.String(),
                                validator=colander.Email())
    public_email = colander.SchemaNode(colander.Boolean(), missing=False)
    twitter = colander.SchemaNode(colander.String(), missing=None,
                                  validator=colander.Length(max=140))
    public_twitter = colander.SchemaNode(colander.Boolean(), missing=False)
    password1 = colander.SchemaNode(colander.String(),
                                    missing=None, default=None)
    password2 = colander.SchemaNode(colander.String(),
                                    missing=None, default=None)
    script_root = colander.SchemaNode(colander.String(),
                                      missing=None, default=None)

########NEW FILE########
__FILENAME__ = attribute
from sqlalchemy.schema import Column
from sqlalchemy.types import UnicodeText, Float


class Attribute(object):

    """ An attribute describes some concrete value stored in the data model.
    This value can either be stored directly on the facts table or on a
    separate dimension table, which is associated to the facts table through
    a reference. """

    def __init__(self, parent, name, data):
        self._data = data
        self.parent = parent
        self.name = name
        self.key = data.get('key', False)
        self.source_column = data.get('column')
        self.default_value = data.get('default_value')
        self.constant = data.get('constant')
        self.description = data.get('description')
        self.datatype = data.get('datatype', 'value')

    @property
    def selectable(self):
        return self.column_alias

    @property
    def column_alias(self):
        return self.parent.alias.c[self.column.name]

    def init(self, meta, table, make_table=False):
        """ Make a model for this attribute, selecting the proper
        data type from attribute metadata.
        """
        # TODO: fetch this from AttributeType system?
        types = {
            'string': UnicodeText,
            'constant': UnicodeText,
            'date': UnicodeText,
            'float': Float,
        }
        type_ = types.get(self.datatype, UnicodeText)
        column = Column(self.name, type_)
        table.append_column(column)
        return column

    def generate(self, meta, table):
        """ Create the column on a given table. """
        pass

    def load(self, bind, value):
        return {self.column.name: value}

    def __repr__(self):
        return "<Attribute(%s)>" % self.name

    def as_dict(self):
        return self._data

########NEW FILE########
__FILENAME__ = badge
from datetime import datetime

from sqlalchemy.orm import relationship, backref
from sqlalchemy.schema import Table, Column, ForeignKey
from sqlalchemy.types import Integer, Unicode, DateTime

from openspending.model.account import Account
from openspending.model import meta as db

# Badges and dataset share a many to many relationship
# therefore we need to create an associate table
badges_on_datasets = Table('badges_on_datasets', db.metadata,
                           Column('badge_id',
                                  Integer,
                                  ForeignKey('badge.id')),
                           Column('dataset_id',
                                  Integer,
                                  ForeignKey('dataset.id'))
                           )


class Badge(db.Model):

    """
    This model allows marking datasets with various badges.
    Examples could be "World Bank" - data verified by the World bank.

    Each badge has a name, a representative image and a description.
    Also stored for historical reasons are badge creator, creation time
    and modification date.
    """
    __tablename__ = 'badge'

    id = Column(Integer, primary_key=True)

    # Primary information for this badge
    label = Column(Unicode)
    image = Column(Unicode)
    description = Column(Unicode)

    # Define relationship with datasets via the associate table
    datasets = relationship("Dataset",
                            secondary=badges_on_datasets,
                            backref=backref('badges', lazy='dynamic'))

    # Creator (and relationship)
    creator_id = Column(Integer, ForeignKey('account.id'))
    creator = relationship(Account,
                           backref=backref('badge_creations',
                                           lazy='dynamic'))

    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow,
                        onupdate=datetime.utcnow)

    def __init__(self, label, image, description, creator):
        """
        Initialize a badge object.
        Badge label should be a representative title for the badge
        Image should be a small, representative image for the badge
        Description describes the purpose of the badge in more detail
        Creator is the user who created the badge.
        """

        self.label = label
        self.image = image
        self.description = description
        self.creator = creator

    def __repr__(self):
        return "<Badge(%s)>" % self.label

    @classmethod
    def by_id(cls, id):
        """
        Find one badge given the id
        """
        return db.session.query(cls).filter_by(id=id).first()

    @classmethod
    def all(cls):
        """
        Find all badges
        """
        return db.session.query(cls)

    def as_dict(self, short=False):
        """
        A dictionary representation of the badge. This can return a long
        version containing all interesting fields or a short version containing
        only id, label and image.
        """
        badge = {
            "id": self.id,
            "label": self.label,
            "image": self.image,
        }
        if not short:
            badge.update({
                "description": self.description,
                "datasets": [ds.name for ds in self.datasets],
                "created_at": self.created_at
            })

        return badge

########NEW FILE########
__FILENAME__ = common
# coding=utf-8
from json import dumps, loads
from sqlalchemy.types import Text, TypeDecorator, Integer
from sqlalchemy.schema import Table, Column
from sqlalchemy.sql.expression import and_, select, func
from sqlalchemy.ext.mutable import Mutable
from openspending.model import meta as db

ALIAS_PLACEHOLDER = u'‽'


def decode_row(row, dataset):
    from openspending.model.dimension import CompoundDimension

    result = {}
    for key, value in row.items():
        if '_' in key:
            dimension, attribute = key.split('_', 1)
            dimension = dimension.replace(ALIAS_PLACEHOLDER, '_')
            if dimension == 'entry':
                result[attribute] = value
            else:
                if dimension not in result:
                    result[dimension] = {}

                    # TODO: backwards-compat?
                    if isinstance(dataset[dimension], CompoundDimension):
                        result[dimension]['taxonomy'] = \
                            dataset[dimension].taxonomy
                result[dimension][attribute] = value
        else:
            if key == 'entries':
                key = 'num_entries'
            result[key] = value
    return result


class MutableDict(Mutable, dict):

    """
    Create a mutable dictionary to track mutable values
    and notify listeners upon change.
    """

    @classmethod
    def coerce(cls, key, value):
        """
        Convert plain dictionaries to MutableDict
        """

        # If it isn't a MutableDict already we conver it
        if not isinstance(value, MutableDict):
            # If it is a dictionary we can convert it
            if isinstance(value, dict):
                return MutableDict(value)

            # Try to coerce but it will probably return a ValueError
            return Mutable.coerce(key, value)
        else:
            # Since we already have a MutableDict we can just return it
            return value

    def __setitem__(self, key, value):
        """
        Set a value to a key and notify listeners of change
        """

        dict.__setitem__(self, key, value)
        self.changed()

    def __delitem__(self, key):
        """
        Delete a key and notify listeners of change
        """

        dict.__delitem__(self, key)
        self.changed()


class JSONType(TypeDecorator):
    impl = Text

    def __init__(self):
        super(JSONType, self).__init__()

    def process_bind_param(self, value, dialect):
        return dumps(value)

    def process_result_value(self, value, dialiect):
        return loads(value)

    def copy_value(self, value):
        return loads(dumps(value))


class TableHandler(object):

    """ Used by automatically generated objects such as datasets
    and dimensions to generate, write and clear the table under
    its management. """

    def _init_table(self, meta, namespace, name, id_type=Integer):
        """ Create the given table if it does not exist, otherwise
        reflect the current table schema from the database.
        """
        name = namespace + '__' + name
        self.table = Table(name, meta)
        if id_type is not None:
            col = Column('id', id_type, primary_key=True)
            self.table.append_column(col)

    def _generate_table(self):
        """ Create the given table if it does not exist. """
        # TODO: make this support some kind of migration?
        if not db.engine.has_table(self.table.name):
            self.table.create(db.engine)

    def _upsert(self, bind, data, unique_columns):
        """ Upsert a set of values into the table. This will
        query for the set of unique columns and either update an
        existing row or create a new one. In both cases, the ID
        of the changed row will be returned. """
        key = and_(*[self.table.c[c] == data.get(c)
                     for c in unique_columns])
        q = self.table.update(key, data)
        if bind.execute(q).rowcount == 0:
            q = self.table.insert(data)
            rs = bind.execute(q)
            return rs.inserted_primary_key[0]
        else:
            q = self.table.select(key)
            row = bind.execute(q).fetchone()
            return row['id']

    def _flush(self, bind):
        """ Delete all rows in the table. """
        q = self.table.delete()
        bind.execute(q)

    def _drop(self, bind):
        """ Drop the table and the local reference to it. """
        if db.engine.has_table(self.table.name):
            self.table.drop()
        del self.table


class DatasetFacetMixin(object):

    @classmethod
    def dataset_counts(cls, datasets):
        ds_ids = [d.id for d in datasets]
        if not len(ds_ids):
            return []
        q = select([cls.code, func.count(cls.dataset_id)],
                   cls.dataset_id.in_(ds_ids), group_by=cls.code,
                   order_by=func.count(cls.dataset_id).desc())
        return db.session.bind.execute(q).fetchall()

########NEW FILE########
__FILENAME__ = dataset
"""
The ``Dataset`` serves as double function in OpenSpending: on one hand, it is
a simple domain object that can be created, modified and deleted as any other
On the other hand it serves as a controller object for the dataset-specific
data model which it represents, handling the creation, filling and migration of
the table schema associated with the dataset. As such, it holds the key set
of logic functions upon which all other queries and loading functions rely.
"""
import math
import logging
from collections import defaultdict
from datetime import datetime
from itertools import count
from sqlalchemy import ForeignKeyConstraint, MetaData
from sqlalchemy.orm import reconstructor, relationship, backref
from sqlalchemy.schema import Column, ForeignKey
from sqlalchemy.types import Integer, Unicode, Boolean, DateTime
from sqlalchemy.sql.expression import false, and_, or_, select, func
from sqlalchemy.ext.associationproxy import association_proxy

from openspending.model import meta as db
from openspending.lib.util import hash_values

from openspending.model.common import (TableHandler, MutableDict, JSONType,
                                       DatasetFacetMixin, decode_row)
from openspending.model.dimension import (CompoundDimension, DateDimension,
                                          AttributeDimension, Measure)

log = logging.getLogger(__name__)


class Dataset(TableHandler, db.Model):

    """ The dataset is the core entity of any access to data. All
    requests to the actual data store are routed through it, as well
    as data loading and model generation.

    The dataset keeps an in-memory representation of the data model
    (including all dimensions and measures) which can be used to
    generate necessary queries.
    """
    __tablename__ = 'dataset'

    id = Column(Integer, primary_key=True)
    name = Column(Unicode(255), unique=True)
    label = Column(Unicode(2000))
    description = Column(Unicode())
    currency = Column(Unicode())
    default_time = Column(Unicode())
    schema_version = Column(Unicode())
    entry_custom_html = Column(Unicode())
    ckan_uri = Column(Unicode())
    category = Column(Unicode())
    serp_title = Column(Unicode(), nullable=True)
    serp_teaser = Column(Unicode(), nullable=True)
    private = Column(Boolean, default=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow,
                        onupdate=datetime.utcnow)
    data = Column(MutableDict.as_mutable(JSONType), default=dict)

    languages = association_proxy('_languages', 'code')
    territories = association_proxy('_territories', 'code')

    def __init__(self, data):
        self.data = data.copy()
        dataset = self.data['dataset']
        del self.data['dataset']
        self.label = dataset.get('label')
        self.name = dataset.get('name')
        self.description = dataset.get('description')
        self.currency = dataset.get('currency')
        self.category = dataset.get('category')
        self.serp_title = dataset.get('serp_title')
        self.serp_teaser = dataset.get('serp_teaser')
        self.default_time = dataset.get('default_time')
        self.entry_custom_html = dataset.get('entry_custom_html')
        self.languages = dataset.get('languages', [])
        self.territories = dataset.get('territories', [])
        self.ckan_uri = dataset.get('ckan_uri')
        self._load_model()

    @property
    def model(self):
        model = self.data.copy()
        model['dataset'] = self.as_dict()
        return model

    @property
    def mapping(self):
        return self.data.get('mapping', {})

    @reconstructor
    def _load_model(self):
        """ Construct the in-memory object representation of this
        dataset's dimension and measures model.

        This is called upon initialization and deserialization of
        the dataset from the SQLAlchemy store.
        """
        self.dimensions = []
        self.measures = []
        for dim, data in self.mapping.items():
            if data.get('type') == 'measure' or dim == 'amount':
                self.measures.append(Measure(self, dim, data))
                continue
            elif data.get('type') == 'date' or \
                    (dim == 'time' and data.get('datatype') == 'date'):
                dimension = DateDimension(self, dim, data)
            elif data.get('type') in ['value', 'attribute']:
                dimension = AttributeDimension(self, dim, data)
            else:
                dimension = CompoundDimension(self, dim, data)
            self.dimensions.append(dimension)
        self.init()
        self._is_generated = None

    def __getitem__(self, name):
        """ Access a field (dimension or measure) by name. """
        for field in self.fields:
            if field.name == name:
                return field
        raise KeyError()

    def __contains__(self, name):
        try:
            self[name]
            return True
        except KeyError:
            return False

    @property
    def fields(self):
        """ Both the dimensions and metrics in this dataset. """
        return self.dimensions + self.measures

    @property
    def compounds(self):
        """ Return only compound dimensions. """
        return filter(lambda d: isinstance(d, CompoundDimension),
                      self.dimensions)

    @property
    def facet_dimensions(self):
        return [d for d in self.dimensions if d.facet]

    def init(self):
        """ Create a SQLAlchemy model for the current dataset model,
        without creating the tables and columns. This needs to be
        called both for access to the data and in order to generate
        the model physically. """
        self.bind = db.engine
        self.meta = MetaData()
        self.meta.bind = db.engine

        self._init_table(self.meta, self.name, 'entry',
                         id_type=Unicode(42))
        for field in self.fields:
            field.column = field.init(self.meta, self.table)
        self.alias = self.table.alias('entry')

    def generate(self):
        """ Create the tables and columns necessary for this dataset
        to keep data.
        """
        for field in self.fields:
            field.generate(self.meta, self.table)
        for dim in self.dimensions:
            if isinstance(dim, CompoundDimension):
                self.table.append_constraint(ForeignKeyConstraint(
                    [dim.name + '_id'], [dim.table.name + '.id'],
                    # use_alter=True,
                    name='fk_' + self.name + '_' + dim.name
                ))
        self._generate_table()
        self._is_generated = True

    @property
    def is_generated(self):
        if self._is_generated is None:
            self._is_generated = self.table.exists()
        return self._is_generated

    @property
    def has_badges(self):
        """
        Property that returns True if the dataset has been given any badges
        """
        # Cast the badge count as a boolean and return it
        return bool(self.badges.count())

    def commit(self):
        pass
        # self.tx.commit()
        # self.tx = self.bind.begin()

    def _make_key(self, data):
        """ Generate a unique identifier for an entry. This is better
        than SQL auto-increment because it is stable across mutltiple
        loads and thus creates stable URIs for entries.
        """
        uniques = [self.name]
        for field in self.fields:
            if not field.key:
                continue
            obj = data.get(field.name)
            if isinstance(obj, dict):
                obj = obj.get('name', obj.get('id'))
            uniques.append(obj)
        return hash_values(uniques)

    def load(self, data):
        """ Handle a single entry of data in the mapping source format,
        i.e. with all needed columns. This will propagate to all dimensions
        and set values as appropriate. """
        entry = dict()
        for field in self.fields:
            field_data = data[field.name]
            entry.update(field.load(self.bind, field_data))
        entry['id'] = self._make_key(data)
        self._upsert(self.bind, entry, ['id'])

    def flush(self):
        """ Delete all data from the dataset tables but leave the table
        structure intact.
        """
        for dimension in self.dimensions:
            dimension.flush(self.bind)
        self._flush(self.bind)

    def drop(self):
        """ Drop all tables created as part of this dataset, i.e. by calling
        ``generate()``. This will of course also delete the data itself.
        """
        self._drop(self.bind)
        for dimension in self.dimensions:
            dimension.drop(self.bind)
        self._is_generated = False

    def key(self, key):
        """ For a given ``key``, find a column to indentify it in a query.
        A ``key`` is either the name of a simple attribute (e.g. ``time``)
        or of an attribute of a complex dimension (e.g. ``to.label``). The
        returned key is using an alias, so it can be used in a query
        directly. """
        attr = None
        if '.' in key:
            key, attr = key.split('.', 1)
        dimension = self[key]
        if hasattr(dimension, 'alias'):
            attr_name = dimension[attr].column.name if attr else 'name'
            return dimension.alias.c[attr_name]
        return self.alias.c[dimension.column.name]

    def entries(self, conditions="1=1", order_by=None, limit=None,
                offset=0, step=10000, fields=None):
        """ Generate a fully denormalized view of the entries on this
        table. This view is nested so that each dimension will be a hash
        of its attributes.

        This is somewhat similar to the entries collection in the fully
        denormalized schema before OpenSpending 0.11 (MongoDB).
        """
        if not self.is_generated:
            return

        if fields is None:
            fields = self.fields

        joins = self.alias
        for d in self.dimensions:
            if d in fields:
                joins = d.join(joins)
        selects = [f.selectable for f in fields] + [self.alias.c.id]

        # enforce stable sorting:
        if order_by is None:
            order_by = [self.alias.c.id.asc()]

        for i in count():
            qoffset = offset + (step * i)
            qlimit = step
            if limit is not None:
                qlimit = min(limit - (step * i), step)
            if qlimit <= 0:
                break

            query = select(selects, conditions, joins, order_by=order_by,
                           use_labels=True, limit=qlimit, offset=qoffset)
            rp = self.bind.execute(query)

            first_row = True
            while True:
                row = rp.fetchone()
                if row is None:
                    if first_row:
                        return
                    break
                first_row = False
                yield decode_row(row, self)

    def aggregate(self, measures=['amount'], drilldowns=[], cuts=[],
                  page=1, pagesize=10000, order=[]):
        """ Query the dataset for a subset of cells based on cuts and
        drilldowns. It returns a structure with a list of drilldown items
        and a summary about the slice cutted by the query.

        ``measures``
            The numeric units to be aggregated over, defaults to
            [``amount``]. (type: `list`)
        ``drilldowns``
            Dimensions to drill down to. (type: `list`)
        ``cuts``
            Specification what to cut from the cube. This is a
            `list` of `two-tuples` where the first item is the dimension
            and the second item is the value to cut from. It is turned into
            a query where multible cuts for the same dimension are combined
            to an *OR* query and then the queries for the different
            dimensions are combined to an *AND* query.
        ``page``
            Page the drilldown result and return page number *page*.
            type: `int`
        ``pagesize``
            Page the drilldown result into page of size *pagesize*.
            type: `int`
        ``order``
            Sort the result based on the dimension *sort_dimension*.
            This may be `None` (*default*) or a `list` of two-`tuples`
            where the first element is the *dimension* and the second
            element is the order (`False` for ascending, `True` for
            descending).
            Type: `list` of two-`tuples`.

        Raises:

        :exc:`ValueError`
            If a cube is not yet computed. Call :meth:`compute` to compute
            the cube.
        :exc:`KeyError`
            If a drilldown, cut or order dimension is not part of this
            cube or the order dimensions are not a subset of the drilldown
            dimensions.

        Returns: A `dict` containing the drilldown and the summary:

          {"drilldown": [
              {"num_entries": 5545,
               "amount": 41087379002.0,
               "cofog1": {"description": "",
                          "label": "Economic affairs"}},
              ... ]
           "summary": {"amount": 7353306450299.0,
                       "num_entries": 133612}}

        """

        # Get the joins (aka alias) and the dataset
        joins = alias = self.alias
        dataset = self

        # Aggregation fields are all of the measures, so we create individual
        # summary fields with the sum function of SQLAlchemy
        fields = [func.sum(alias.c[m]).label(m) for m in measures]
        # We append an aggregation field that counts the number of entries
        fields.append(func.count(alias.c.id).label("entries"))
        # Create a copy of the statistics fields (for later)
        stats_fields = list(fields)

        # Create label map for time columns (year and month) for lookup
        # since they are found under the time attribute
        labels = {
            'year': dataset['time']['year'].column_alias.label('year'),
            'month': dataset['time']['yearmonth'].column_alias.label('month'),
        }

        # Get the dimensions we're interested in. These would be the drilldowns
        # and the cuts. For compound dimensions we are only interested in the
        # most significant one (e.g. for from.name we're interested in from)
        dimensions = drilldowns + [k for k, v in cuts]
        dimensions = [d.split('.')[0] for d in dimensions]

        # Loop over the dimensions as a set (to avoid multiple occurances)
        for dimension in set(dimensions):
            # If the dimension is year or month we're interested in 'time'
            if dimension in labels:
                dimension = 'time'
            # If the dimension table isn't in the automatic joins we add it
            if dimension not in [c.table.name for c in joins.columns]:
                joins = dataset[dimension].join(joins)

        # Drilldowns are performed using group_by SQL functions
        group_by = []
        for key in drilldowns:
            # If drilldown is in labels we append its mapped column to fields
            if key in labels:
                column = labels[key]
                group_by.append(column)
                fields.append(column)
            else:
                # Get the column from the dataset
                column = dataset.key(key)
                # If the drilldown is a compound dimension or the columns table
                # is in the joins we're already fetching the column so we just
                # append it to fields and the group_by
                if '.' in key or column.table == alias:
                    fields.append(column)
                    group_by.append(column)
                else:
                    # If not we add the column table to the fields and add all
                    # of that tables columns to the group_by
                    fields.append(column.table)
                    for col in column.table.columns:
                        group_by.append(col)

        # Cuts are managed using AND statements and we use a dict with set as
        # the default value to create the filters (cut on various values)
        conditions = and_()
        filters = defaultdict(set)

        for key, value in cuts:
            # If the key is in labels (year or month) we get the mapped column
            # else we get the column from the dataset
            if key in labels:
                column = labels[key]
            else:
                column = dataset.key(key)
            # We add the value to the set for that particular column
            filters[column].add(value)

        # Loop over the columns in the filter and add that to the conditions
        # For every value in the set we create and OR statement so we get e.g.
        # year=2007 AND (from.who == 'me' OR from.who == 'you')
        for attr, values in filters.items():
            conditions.append(or_(*[attr == v for v in values]))

        # Ordering can be set by a parameter or ordered by measures by default
        order_by = []
        # If no order is defined we default to order of the measures in the
        # order they occur (furthest to the left is most significant)
        if order is None or not len(order):
            order = [(m, True) for m in measures]

        # We loop through the order list to add the columns themselves
        for key, direction in order:
            # If it's a part of the measures we have to order by the
            # aggregated values (the sum of the measure)
            if key in measures:
                column = func.sum(alias.c[key]).label(key)
            # If it's in the labels we have to get the mapped column
            elif key in labels:
                column = labels[key]
            # ...if not we just get the column from the dataset
            else:
                column = dataset.key(key)
            # We append the column and set the direction (True == descending)
            order_by.append(column.desc() if direction else column.asc())

        # query 1: get overall sums.
        # Here we use the stats_field we saved earlier
        query = select(stats_fields, conditions, joins)
        rp = dataset.bind.execute(query)
        # Execute the query and turn them to a list so we can pop the
        # entry count and then zip the measurements and the totals together
        stats = list(rp.fetchone())
        num_entries = stats.pop()
        total = zip(measures, stats)

        # query 2: get total count of drilldowns
        if len(group_by):
            # Select 1 for each group in the group_by and count them
            query = select(['1'], conditions, joins, group_by=group_by)
            query = select([func.count('1')], '1=1', query.alias('q'))
            rp = dataset.bind.execute(query)
            num_drilldowns, = rp.fetchone()
        else:
            # If there are no drilldowns we still have to do one
            num_drilldowns = 1

        # The drilldown result list
        drilldown = []
        # The offset in the db, based on the page and pagesize (we have to
        # modify it since page counts starts from 1 but we count from 0
        offset = int((page - 1) * pagesize)

        # query 3: get the actual data
        query = select(fields, conditions, joins, order_by=order_by,
                       group_by=group_by, use_labels=True,
                       limit=pagesize, offset=offset)
        rp = dataset.bind.execute(query)

        while True:
            # Get each row in the db result and append it, decoded, to the
            # drilldown result. The decoded version is a json represenation
            row = rp.fetchone()
            if row is None:
                break
            result = decode_row(row, dataset)
            drilldown.append(result)

        # Create the summary based on the stats_fields and other things
        # First we add a the total for each measurement in the root of the
        # summary (watch out!) and then we add various other, self-explanatory
        # statistics such as page, number of entries. The currency value is
        # strange since it's redundant for multiple measures but is left as is
        # for backwards compatibility
        summary = {key: value for (key, value) in total}
        summary.update({
            'num_entries': num_entries,
            'currency': {m: dataset.currency for m in measures},
            'num_drilldowns': num_drilldowns,
            'page': page,
            'pages': int(math.ceil(num_drilldowns / float(pagesize))),
            'pagesize': pagesize
        })

        return {'drilldown': drilldown, 'summary': summary}

    def timerange(self):
        """
        Get the timerange of the dataset (based on the time attribute).
        Returns a tuple of (first timestamp, last timestamp) where timestamp
        is a datetime object
        """
        try:
            # Get the time column
            time = self.key('time')
            # We use SQL's min and max functions to get the timestamps
            query = db.session.query(func.min(time), func.max(time))
            # We just need one result to get min and max time
            return [datetime.strptime(date, '%Y-%m-%d') if date else None
                    for date in query.one()]
        except:
            return (None, None)

    def __repr__(self):
        return "<Dataset(%s:%s:%s)>" % (self.name, self.dimensions,
                                        self.measures)

    def __len__(self):
        if not self.is_generated:
            return 0
        rp = self.bind.execute(self.alias.count())
        return rp.fetchone()[0]

    def as_dict(self):
        return {
            'label': self.label,
            'name': self.name,
            'description': self.description,
            'default_time': self.default_time,
            'schema_version': self.schema_version,
            'currency': self.currency,
            'category': self.category,
            'serp_title': self.serp_title,
            'serp_teaser': self.serp_teaser,
            'timestamps': {
                'created': self.created_at,
                'last_modified': self.updated_at
            },
            'languages': list(self.languages),
            'territories': list(self.territories),
            'badges': [b.as_dict(short=True) for b in self.badges]
        }

    @classmethod
    def all_by_account(cls, account):
        """ Query available datasets based on dataset visibility. """
        criteria = [cls.private == false()]
        if account is not None:
            criteria += ["1=1" if account.admin else "1=2",
                         cls.managers.any(type(account).id == account.id)]
        q = db.session.query(cls).filter(or_(*criteria))
        q = q.order_by(cls.label.asc())
        return q

    @classmethod
    def by_name(cls, name):
        return db.session.query(cls).filter_by(name=name).first()


class DatasetLanguage(db.Model, DatasetFacetMixin):
    __tablename__ = 'dataset_language'

    id = Column(Integer, primary_key=True)
    code = Column(Unicode)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, onupdate=datetime.utcnow)

    dataset_id = Column(Integer, ForeignKey('dataset.id'))
    dataset = relationship(Dataset, backref=backref('_languages', lazy=False))

    def __init__(self, code):
        self.code = code


class DatasetTerritory(db.Model, DatasetFacetMixin):
    __tablename__ = 'dataset_territory'

    id = Column(Integer, primary_key=True)
    code = Column(Unicode)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, onupdate=datetime.utcnow)

    dataset_id = Column(Integer, ForeignKey('dataset.id'))
    dataset = relationship(Dataset, backref=backref('_territories',
                                                    lazy=False))

    def __init__(self, code):
        self.code = code

########NEW FILE########
__FILENAME__ = dimension
from sqlalchemy.schema import Column
from sqlalchemy.types import Integer
from sqlalchemy.sql.expression import select, func

from openspending.model.attribute import Attribute
from openspending.model.common import TableHandler, ALIAS_PLACEHOLDER


class Dimension(object):

    """ A base class for dimensions. A dimension is any property of an entry
    that can serve to describe it beyond its purely numeric ``Measure``.  """

    def __init__(self, dataset, name, data):
        self._data = data
        self.dataset = dataset
        self.name = name
        self.key = data.get('key', False)
        self.label = data.get('label', name)
        self.type = data.get('type', name)
        self.description = data.get('description', name)
        self.facet = data.get('facet')

    def join(self, from_clause):
        return from_clause

    def flush(self, bind):
        pass

    def drop(self, bind):
        del self.column

    @property
    def is_compound(self):
        """ Test whether or not this dimension object is compound. """
        return isinstance(self, CompoundDimension)

    def __getitem__(self, name):
        raise KeyError()

    def __repr__(self):
        return "<Dimension(%s)>" % self.name

    def as_dict(self):
        # FIXME: legacy support
        d = self._data.copy()
        d['key'] = self.name
        d['name'] = self.name
        return d

    def has_attribute(self, attribute):
        """
        Check whether an instance has a given attribute.
        This methods exposes the hasattr for parts of OpenSpending
        where hasattr isn't accessible (e.g. in templates)
        """
        return hasattr(self, attribute)


class AttributeDimension(Dimension, Attribute):

    """ A simple dimension that does not create its own values table
    but keeps its values directly as columns on the facts table. This is
    somewhat unusual for a star schema but appropriate for properties such as
    transaction identifiers whose cardinality roughly equals that of the facts
    table.
    """

    def __init__(self, dataset, name, data):
        Attribute.__init__(self, dataset, name, data)
        Dimension.__init__(self, dataset, name, data)

    def __repr__(self):
        return "<AttributeDimension(%s)>" % self.name

    def members(self, conditions="1=1", limit=None, offset=0):
        """ Get a listing of all the members of the dimension (i.e. all the
        distinct values) matching the filter in ``conditions``. """
        query = select([self.column_alias], conditions,
                       limit=limit, offset=offset, distinct=True)
        rp = self.dataset.bind.execute(query)
        while True:
            row = rp.fetchone()
            if row is None:
                break
            yield row[0]

    def num_entries(self, conditions="1=1"):
        """ Return the count of entries on the dataset fact table having the
        dimension set to a value matching the filter given by ``conditions``.
        """
        query = select([func.count(func.distinct(self.column_alias))],
                       conditions)
        rp = self.dataset.bind.execute(query)
        return rp.fetchone()[0]


class Measure(Attribute):

    """ A value on the facts table that can be subject to aggregation,
    and is specific to this one fact. This would typically be some
    financial unit, i.e. the amount associated with the transaction or
    a specific portion thereof (i.e. co-financed amounts). """

    def __init__(self, dataset, name, data):
        Attribute.__init__(self, dataset, name, data)
        self.label = data.get('label', name)

    def __getitem__(self, name):
        raise KeyError()

    def join(self, from_clause):
        return from_clause

    def __repr__(self):
        return "<Measure(%s)>" % self.name


class CompoundDimension(Dimension, TableHandler):

    """ A compound dimension is an outer table on the star schema, i.e. an
    associated table that is referenced from the fact table. It can have
    any number of attributes but in the case of OpenSpending it will not
    have sub-dimensions (i.e. snowflake schema).
    """

    def __init__(self, dataset, name, data):
        Dimension.__init__(self, dataset, name, data)
        self.taxonomy = data.get('taxonomy', name)

        self.attributes = []
        for name, attr in data.get('attributes', {}).items():
            self.attributes.append(Attribute(self, name, attr))

        # TODO: possibly use a LRU later on?
        self._pk_cache = {}

    def join(self, from_clause):
        """ This will return a query fragment that can be used to establish
        an aliased join between the fact table and the dimension table.
        """
        return from_clause.join(
            self.alias, self.alias.c.id == self.column_alias)

    def flush(self, bind):
        """ Clear all data in the dimension table but keep the table structure
        intact. """
        self._flush(bind)

    def drop(self, bind):
        """ Drop the dimension table and all data within it. """
        self._drop(bind)
        del self.column

    @property
    def column_alias(self):
        """ This an aliased pointer to the FK column on the fact table. """
        return self.dataset.alias.c[self.column.name]

    @property
    def selectable(self):
        return self.alias

    def __getitem__(self, name):
        for attr in self.attributes:
            if attr.name == name:
                return attr
        raise KeyError()

    def init(self, meta, fact_table, make_table=True):
        column = Column(self.name + '_id', Integer, index=True)
        fact_table.append_column(column)
        if make_table is True:
            self._init_table(meta, self.dataset.name, self.name)
            for attr in self.attributes:
                attr.column = attr.init(meta, self.table)
            alias_name = self.name.replace('_', ALIAS_PLACEHOLDER)
            self.alias = self.table.alias(alias_name)
        return column

    def generate(self, meta, entry_table):
        """ Create the table and column associated with this dimension
        if it does not already exist and propagate this call to the
        associated attributes.
        """
        for attr in self.attributes:
            attr.generate(meta, self.table)
        self._generate_table()

    def load(self, bind, row):
        """ Load a row of data into this dimension by upserting the attribute
        values. """
        dim = dict()
        for attr in self.attributes:
            attr_data = row[attr.name]
            dim.update(attr.load(bind, attr_data))
        name = dim['name']
        if name in self._pk_cache:
            pk = self._pk_cache[name]
        else:
            pk = self._upsert(bind, dim, ['name'])
            self._pk_cache[name] = pk
        return {self.column.name: pk}

    def members(self, conditions="1=1", limit=None, offset=0):
        """ Get a listing of all the members of the dimension (i.e. all the
        distinct values) matching the filter in ``conditions``. This can also
        be used to find a single individual member, e.g. a dimension value
        identified by its name. """
        query = select([self.alias], conditions,
                       limit=limit, offset=offset,
                       distinct=True)
        rp = self.dataset.bind.execute(query)
        while True:
            row = rp.fetchone()
            if row is None:
                break
            member = dict(row.items())
            member['taxonomy'] = self.taxonomy
            yield member

    def num_entries(self, conditions="1=1"):
        """ Return the count of entries on the dataset fact table having the
        dimension set to a value matching the filter given by ``conditions``.
        """
        joins = self.join(self.dataset.alias)
        query = select([func.count(func.distinct(self.column_alias))],
                       conditions, joins)
        rp = self.dataset.bind.execute(query)
        return rp.fetchone()[0]

    def __len__(self):
        rp = self.dataset.bind.execute(self.alias.count())
        return rp.fetchone()[0]

    def __repr__(self):
        return "<CompoundDimension(%s:%s)>" % (self.name, self.attributes)


class DateDimension(CompoundDimension):

    """ DateDimensions are closely related to :py:class:`CompoundDimensions`
    but the value is set up from a Python date object to automatically contain
    several properties of the date in their own attributes (e.g. year, month,
    quarter, day). """

    DATE_ATTRIBUTES = {
        'name': {'datatype': 'string'},
        'label': {'datatype': 'string'},
        'year': {'datatype': 'string'},
        'quarter': {'datatype': 'string'},
        'month': {'datatype': 'string'},
        'week': {'datatype': 'string'},
        'day': {'datatype': 'string'},
        # legacy query support:
        'yearmonth': {'datatype': 'string'},
    }

    def __init__(self, dataset, name, data):
        Dimension.__init__(self, dataset, name, data)
        self.taxonomy = name

        self.attributes = []
        for name, attr in self.DATE_ATTRIBUTES.items():
            self.attributes.append(Attribute(self, name, attr))

        self._pk_cache = {}

    def load(self, bind, value):
        """ Given a Python datetime.date, generate a date dimension with the
        following attributes automatically set:

        * name - a human-redable representation
        * year - the year only (e.g. 2011)
        * quarter - a number to identify the quarter of the year (zero-based)
        * month - the month of the date (e.g. 01)
        * week - calendar week of the year (e.g. 42)
        * day - day of the month (e.g. 8)
        * yearmonth - combined year and month (e.g. 201112)
        """
        data = {
            'name': value.isoformat(),
            'label': value.strftime("%d. %B %Y"),
            'year': value.strftime('%Y'),
            'quarter': str(value.month / 4),
            'month': value.strftime('%m'),
            'week': value.strftime('%W'),
            'day': value.strftime('%d'),
            'yearmonth': value.strftime('%Y%m')
        }
        return super(DateDimension, self).load(bind, data)

    def __repr__(self):
        return "<DateDimension(%s:%s)>" % (self.name, self.attributes)

########NEW FILE########
__FILENAME__ = log_record
from datetime import datetime

from sqlalchemy.orm import relationship, backref
from sqlalchemy.schema import Column, ForeignKey
from sqlalchemy.types import Integer, Unicode, DateTime

from openspending.model import meta as db
from openspending.model.run import Run


class LogRecord(db.Model):
    __tablename__ = 'log_record'

    CATEGORY_SYSTEM = 'system'
    CATEGORY_MODEL = 'model'
    CATEGORY_DATA = 'data'

    id = Column(Integer, primary_key=True)
    run_id = Column(Integer, ForeignKey('run.id'))

    category = Column(Unicode)
    level = Column(Unicode)
    message = Column(Unicode)
    error = Column(Unicode)
    timestamp = Column(DateTime, default=datetime.utcnow)

    row = Column(Integer)
    attribute = Column(Unicode)
    column = Column(Unicode)
    data_type = Column(Unicode)
    value = Column(Unicode)

    run = relationship(Run, backref=backref('records', lazy='dynamic'))

    def __init__(self, run, category, level, message):
        self.run = run
        self.category = category
        self.level = level
        self.message = message

    @classmethod
    def by_id(cls, id):
        return db.session.query(cls).filter_by(id=id).first()

    def __repr__(self):
        return "<LogRecord(%s:%s:%s:%s:%s)>" % (self.category, self.level,
                                                self.error, self.timestamp,
                                                self.message)

########NEW FILE########
__FILENAME__ = meta
"""SQLAlchemy Metadata and Session object"""

from sqlalchemy import MetaData
from sqlalchemy import orm
from sqlalchemy.ext.declarative import declarative_base

# SQLAlchemy database engine.  Updated by model.init_model()
engine = None

# SQLAlchemy session manager.  Updated by model.init_model()
session = None

# Global metadata. If you have multiple databases with overlapping table
# names, you'll need a metadata for each database
metadata = MetaData()


class Model(object):

    """Baseclass for custom user models."""

    query_class = orm.Query
    query = None

Model = declarative_base(cls=Model, name='Model', metadata=metadata)

########NEW FILE########
__FILENAME__ = run
from datetime import datetime

from sqlalchemy.orm import relationship, backref
from sqlalchemy.schema import Column, ForeignKey
from sqlalchemy.types import Integer, Unicode, DateTime

from openspending.model import meta as db
from openspending.model.dataset import Dataset
from openspending.model.source import Source


class Run(db.Model):

    """ A run is a generic grouping object for background operations
    that perform logging to the frontend. """

    __tablename__ = 'run'

    # Status values
    STATUS_RUNNING = 'running'
    STATUS_COMPLETE = 'complete'
    STATUS_FAILED = 'failed'
    STATUS_REMOVED = 'removed'

    # Operation values for database, two operations possible
    OPERATION_SAMPLE = 'sample'
    OPERATION_IMPORT = 'import'

    id = Column(Integer, primary_key=True)
    operation = Column(Unicode(2000))
    status = Column(Unicode(2000))
    time_start = Column(DateTime, default=datetime.utcnow)
    time_end = Column(DateTime)
    dataset_id = Column(Integer, ForeignKey('dataset.id'), nullable=True)
    source_id = Column(Integer, ForeignKey('source.id'), nullable=True)

    dataset = relationship(Dataset,
                           backref=backref('runs',
                                           order_by='Run.time_start.desc()',
                                           lazy='dynamic'))
    source = relationship(Source,
                          backref=backref('runs',
                                          order_by='Run.time_start.desc()',
                                          lazy='dynamic'))

    def __init__(self, operation, status, dataset, source):
        self.operation = operation
        self.status = status
        self.dataset = dataset
        self.source = source

    @property
    def successful_sample(self):
        """
        Returns True if the run was a sample operation (not full import)
        and ran without failures.
        """
        return self.operation == self.OPERATION_SAMPLE and \
            self.status == self.STATUS_COMPLETE

    @property
    def successful_load(self):
        """
        Returns True if the run was an import operation (not a sample)
        and ran without failures.
        """
        return self.operation == self.OPERATION_IMPORT and \
            self.status == self.STATUS_COMPLETE

    @property
    def is_running(self):
        """
        Returns True if the run is currently running
        """
        return self.status == self.STATUS_RUNNING

    @classmethod
    def by_id(cls, id):
        return db.session.query(cls).filter_by(id=id).first()

    def __repr__(self):
        return "<Run(%s,%s)>" % (self.source.id, self.id)

########NEW FILE########
__FILENAME__ = source
from datetime import datetime

from sqlalchemy.orm import relationship, backref
from sqlalchemy.schema import Column, ForeignKey
from sqlalchemy.types import Integer, Unicode, DateTime

from openspending.model import meta as db
from openspending.model.common import MutableDict, JSONType
from openspending.model.dataset import Dataset
from openspending.model.account import Account


class Source(db.Model):
    __tablename__ = 'source'

    id = Column(Integer, primary_key=True)
    url = Column(Unicode)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, onupdate=datetime.utcnow)
    analysis = Column(MutableDict.as_mutable(JSONType), default=dict)

    dataset_id = Column(Integer, ForeignKey('dataset.id'))
    dataset = relationship(
        Dataset,
        backref=backref('sources', lazy='dynamic',
                        order_by='Source.created_at.desc()'))

    creator_id = Column(Integer, ForeignKey('account.id'))
    creator = relationship(Account,
                           backref=backref('sources', lazy='dynamic'))

    def __init__(self, dataset, creator, url):
        self.dataset = dataset
        self.creator = creator
        self.url = url

    @property
    def loadable(self):
        """
        Returns True if the source is ready to be imported into the
        database. Does not not require a sample run although it
        probably should.
        """
        # It shouldn't be loaded again into the database
        if self.successfully_loaded:
            return False
        # It needs mapping to be loadable
        if not len(self.dataset.mapping):
            return False
        # There can be no errors in the analysis of the source
        if 'error' in self.analysis:
            return False
        # All is good... proceed
        return True

    @property
    def successfully_sampled(self):
        """
        Returns True if any of this source's runs have been
        successfully sampled (a complete sample run). This shows
        whether the source is ready to be imported into the database
        """
        return True in [r.successful_sample for r in self.runs]

    @property
    def is_running(self):
        """
        Returns True if any of this source's runs have the status
        'running'. This shows whether the loading has been started or not
        to help avoid multiple loads of the same resource.
        """
        return True in [r.is_running for r in self.runs]

    @property
    def successfully_loaded(self):
        """
        Returns True if any of this source's runs have been
        successfully loaded (not a sample and no errors). This
        shows whether the source has been loaded into the database
        """
        return True in [r.successful_load for r in self.runs]

    def __repr__(self):
        try:
            return "<Source(%s,%s)>" % (self.dataset.name, self.url)
        except:
            return ''

    @classmethod
    def by_id(cls, id):
        return db.session.query(cls).filter_by(id=id).first()

    @classmethod
    def all(cls):
        return db.session.query(cls)

    def as_dict(self):
        return {
            "id": self.id,
            "url": self.url,
            "dataset": self.dataset.name,
            "created_at": self.created_at
        }

########NEW FILE########
__FILENAME__ = view
from datetime import datetime

from sqlalchemy.orm import relationship, backref
from sqlalchemy.schema import Column, ForeignKey
from sqlalchemy.types import Integer, Unicode, Boolean, DateTime

from openspending.model import meta as db
from openspending.model.dataset import Dataset
from openspending.model.account import Account
from openspending.model.common import MutableDict, JSONType


class View(db.Model):

    """ A view stores a specific configuration of a visualisation widget. """

    __tablename__ = 'view'

    id = Column(Integer, primary_key=True)
    widget = Column(Unicode(2000))
    name = Column(Unicode(2000))
    label = Column(Unicode(2000))
    description = Column(Unicode())
    state = Column(MutableDict.as_mutable(JSONType), default=dict)
    public = Column(Boolean, default=False)

    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, onupdate=datetime.utcnow)

    dataset_id = Column(Integer, ForeignKey('dataset.id'))
    account_id = Column(Integer, ForeignKey('account.id'), nullable=True)

    dataset = relationship(Dataset,
                           backref=backref('views',
                                           cascade='all,delete,delete-orphan',
                                           lazy='dynamic'))

    account = relationship(Account,
                           backref=backref('views',
                                           cascade='all,delete,delete-orphan',
                                           lazy='dynamic'))

    def __init__(self):
        pass

    @classmethod
    def by_id(cls, id):
        return db.session.query(cls).filter_by(id=id).first()

    @classmethod
    def by_name(cls, dataset, name):
        q = db.session.query(cls).filter_by(name=name)
        return q.filter_by(dataset=dataset).first()

    @classmethod
    def all_by_dataset(cls, dataset):
        return db.session.query(cls).filter_by(dataset=dataset)

    def as_dict(self):
        return {
            'id': self.id,
            'widget': self.widget,
            'name': self.name,
            'label': self.label,
            'description': self.description,
            'state': self.state,
            'public': self.public,
            'dataset': self.dataset.name,
            'account': self.account.name if self.account else None
        }

    def __repr__(self):
        return "<View(%s,%s)>" % (self.dataset.name, self.name)

########NEW FILE########
__FILENAME__ = celery
from __future__ import absolute_import
import os

from celery import Celery
from celery import signals
from celery.bin import Option

from openspending.command import _configure_pylons

# Get celery broker and backend from the environment with localhost
# RabbitMQ (or other AMQP message queue on 5672) as default
BROKER = os.environ.get('BROKER_URL', 'amqp://guest:guest@localhost:5672//')
BACKEND = os.environ.get('BACKEND_BROKER_URL', BROKER)

# Create Celery app for tasks, this is imported where we set the tasks
# and used by the celery commandline tool
celery = Celery('openspending.tasks', broker=BROKER, backend=BACKEND,
                include=['openspending.tasks.generic',
                         'openspending.tasks.dataset'])

# Add a user option to celery where the configuration file for pylons
# can be provided.
celery.user_options['preload'].add(
    Option('-p', '--pylons-ini-file',
           help='Pylons .ini file to use for environment configuration'),
)


@signals.user_preload_options.connect
def on_preload_parsed(options, **kwargs):
    """
    Parse user options for celery. It only configures the environment
    if a pylons ini file is provided, else it does nothing.
    """
    if 'pylons_ini_file' in options:
        # Get the filename of the pylons ini file provided
        pylons_config = os.path.realpath(options['pylons_ini_file'])
        # Configure the pylons environment with the pylons config
        _configure_pylons(pylons_config)

########NEW FILE########
__FILENAME__ = dataset
from __future__ import absolute_import
from openspending.tasks.celery import celery

from celery.utils.log import get_task_logger
log = get_task_logger(__name__)


@celery.task(ignore_result=True)
def analyze_all_sources():
    from openspending.model import meta as db
    from openspending.model.source import Source
    for source in db.session.query(Source):
        analyze_source.delay(source.id)


@celery.task(ignore_result=True)
def analyze_source(source_id):
    from openspending.model import meta as db
    from openspending.model.source import Source
    from openspending.importer.analysis import analyze_csv
    source = Source.by_id(source_id)
    if not source:
        log.error("No such source: %s", source_id)
        return
    log.info("Analyzing: %s", source.url)
    source.analysis = analyze_csv(source.url)
    if 'error' in source.analysis:
        log.error(source.analysis.get('error'))
    else:
        log.info("Columns: %r", source.analysis.get('columns'))
    db.session.commit()


@celery.task(ignore_result=True)
def load_source(source_id, sample=False):
    from openspending.model.source import Source
    from openspending.importer import CSVImporter
    source = Source.by_id(source_id)
    if not source:
        log.error("No such source: %s", source_id)

    if not source.loadable:
        log.error("Dataset has no mapping.")
        return

    source.dataset.generate()
    importer = CSVImporter(source)
    if sample:
        importer.run(dry_run=True, max_lines=1000, max_errors=1000)
    else:
        importer.run()
        index_dataset.delay(source.dataset.name)


@celery.task(ignore_result=True)
def index_dataset(dataset_name):
    from openspending.lib.solr_util import build_index
    build_index(dataset_name)

########NEW FILE########
__FILENAME__ = generic
from __future__ import absolute_import
from openspending.tasks.celery import celery

from celery.utils.log import get_task_logger
log = get_task_logger(__name__)


@celery.task(ignore_result=True)
def ping():
    log.info("Pong.")


@celery.task(ignore_result=True)
def clean_sessions():
    import os
    import subprocess
    from pylons import config

    cache_dir = config.get('pylons.cache_dir')

    if cache_dir is None:
        log.warn(
            "No 'cache_dir' found in pylons config," +
            "unable to clean session files!")
        return

    sessions_dir = os.path.join(cache_dir, 'sessions')
    if not os.path.isdir(sessions_dir):
        log.warn(
            "No 'sessions' directory found in %s," +
            "skipping clean_sessions task!",
            cache_dir)
        return

    # remove all session files with an atime more than 1 day ago
    return subprocess.call(
        ['find', sessions_dir, '-type', 'f', '!', '-atime', '0', '-delete'])

########NEW FILE########
__FILENAME__ = base
from routes.util import URLGenerator
from pylons import url, config
from pylons.test import pylonsapp
from webtest import TestApp

from openspending.model import meta, init_model
from openspending.tests.helpers import clean_db

from sqlalchemy import engine_from_config
from migrate.versioning.util import construct_engine


class TestCase(object):

    def setup(self):
        pass

    def teardown(self):
        pass


class DatabaseTestCase(TestCase):

    def setup_database(self):
        """
        Configure the database based on the provided configuration
        file, but be sure to overwrite the url so that it will use
        sqlite in memory, irrespective of what the user has set in
        test.ini. Construct the sqlalchemy engine with versioning
        and initialise everything.
        """

        config['openspending.db.url'] = 'sqlite:///:memory:'
        engine = engine_from_config(config, 'openspending.db.')
        engine = construct_engine(engine)
        init_model(engine)

    def setup(self):
        self.setup_database()
        meta.metadata.create_all(meta.engine)

    def teardown(self):
        clean_db()
        super(DatabaseTestCase, self).teardown()


class ControllerTestCase(DatabaseTestCase):

    def __init__(self, *args, **kwargs):
        self.app = TestApp(pylonsapp)
        url._push_object(URLGenerator(config['routes.map'], {}))
        super(DatabaseTestCase, self).__init__(*args, **kwargs)

########NEW FILE########
__FILENAME__ = test_importer
from openspending.model.account import Account
from openspending.model import meta as db
from openspending.command.importer import shell_account

from openspending.tests.base import TestCase
from mock import patch


class TestImporter(TestCase):

    @patch.object(Account, 'by_name', return_value='the account')
    def test_shell_account_when_it_exists(self, account_mock):
        assert shell_account() == 'the account'
        Account.by_name.assert_called_once_with('system')

    @patch.object(Account, 'by_name', return_value=None)
    def test_shell_account_when_it_doesnt_exists(self, account_mock):
        with patch.object(db, 'session'):
            account = shell_account()
            assert account.name == 'system'
            db.session.add.assert_called_once_with(account)

########NEW FILE########
__FILENAME__ = test_account
from openspending.tests.base import ControllerTestCase
from openspending.tests.helpers import make_account, load_fixture
from nose.tools import raises
from mock import patch

from openspending.model import meta as db
from openspending.model.account import Account
from openspending.lib.mailer import MailerException
from pylons import config, url

import json
import urllib2


class TestAccountController(ControllerTestCase):

    def test_login(self):
        self.app.get(url(controller='account', action='login'))

    def test_register(self):
        self.app.get(url(controller='account', action='register'))

    def test_account_create_gives_api_key(self):
        account = make_account()
        assert len(account.api_key) == 36

    @patch('openspending.auth.account.update')
    @patch('openspending.model.account.Account.by_name')
    def test_settings(self, model_mock, update_mock):
        account = Account()
        account.name = 'mockaccount'
        db.session.add(account)
        db.session.commit()
        model_mock.return_value = account
        update_mock.return_value = True
        self.app.get(url(controller='account', action='settings'),
                     extra_environ={'REMOTE_USER': 'mockaccount'})

    def test_after_login(self):
        self.app.get(url(controller='account', action='after_login'))

    def test_after_logout(self):
        self.app.get(url(controller='account', action='after_logout'))

    def test_trigger_reset_get(self):
        response = self.app.get(
            url(controller='account', action='trigger_reset'))
        assert 'email address you used to register your account'\
            in response.body, response.body

    def test_trigger_reset_post_fail(self):
        response = self.app.post(url(controller='account',
                                     action='trigger_reset'),
                                 params={'emailx': "foo@bar"})
        assert 'Please enter an email address' in response.body, response.body
        response = self.app.post(url(controller='account',
                                     action='trigger_reset'),
                                 params={'email': "foo@bar"})
        assert 'No user is registered' in response.body, response.body

    @raises(MailerException)
    def test_trigger_reset_post_ok(self):
        try:
            original_smtp_server = config.get('smtp_server')
            config['smtp_server'] = 'non-existent-smtp-server'
            make_account()
            self.app.post(url(controller='account',
                              action='trigger_reset'),
                          params={'email': "test@example.com"})
        finally:
            config['smtp_server'] = original_smtp_server

    def test_reset_get(self):
        response = self.app.get(url(controller='account', action='do_reset',
                                    token='huhu',
                                    email='huhu@example.com'))
        assert '/login' in response.headers['location'], response.headers
        account = make_account()
        response = self.app.get(url(controller='account', action='do_reset',
                                    token=account.token,
                                    email=account.email))
        assert '/settings' in response.headers['location'], response.headers

    def test_completion_access_check(self):
        response = self.app.get(url(controller='account', action='complete'),
                                expect_errors=True)
        obj = json.loads(response.body)
        assert u'You are not authorized to see that page' == obj['errors']

    def test_distinct_json(self):
        test = make_account()
        response = self.app.get(url(controller='account', action='complete'),
                                extra_environ={'REMOTE_USER': str(test.name)})
        obj = json.loads(response.body)['results']
        assert obj[0].keys() == [u'fullname', u'name']
        assert len(obj) == 1, obj
        assert obj[0]['name'] == 'test', obj[0]

        response = self.app.get(url(controller='account', action='complete'),
                                params={'q': 'tes'},
                                extra_environ={'REMOTE_USER': str(test.name)})
        obj = json.loads(response.body)['results']
        assert len(obj) == 1, obj

        response = self.app.get(url(controller='account', action='complete'),
                                params={'q': 'foo'},
                                extra_environ={'REMOTE_USER': str(test.name)})
        obj = json.loads(response.body)['results']
        assert len(obj) == 0, obj

    def test_dashboard_not_logged_in(self):
        response = self.app.get(url(controller='account', action='dashboard'),
                                status=403)
        assert '403' in response.status, response.status

    def test_dashboard(self):
        test = make_account('test')
        cra = load_fixture('cra', manager=test)
        response = self.app.get(url(controller='account', action='dashboard'),
                                extra_environ={'REMOTE_USER': str(test.name)})
        assert '200' in response.status, response.status
        assert cra.label in response, response

    def test_profile(self):
        """
        Profile page test
        """

        # Create the test user account using default
        # username is test, fullname is 'Test User',
        # email is test@example.com and twitter handle is testuser
        test = make_account('test')

        # Get the user profile for an anonymous user
        response = self.app.get(url(controller='account', action='profile',
                                    name='test'))

        assert '200' in response.status, \
            'Profile not successfully returned for anonymous user'
        assert '<dt>Name</dt>' in response.body, \
            'Name heading is not in profile for anonymous user'
        assert '<dd>Test User</dd>' in response.body, \
            'User fullname is not in profile for anonymous user'
        assert '<dt>Username</dt>' in response.body, \
            'Username heading is not in profile for anonymous user'
        assert '<dd>test</dd>' in response.body, \
            'Username is not in profile for anonymous user'
        assert '<dt>Email</dt>' not in response.body, \
            'Email heading is in profile for anonymous user'
        assert '<dd>test@example.com</dd>' not in response.body, \
            'Email of user is in profile for anonymous user'
        assert '<dt>Twitter</dt>' not in response.body, \
            'Twitter heading is in profile for anonymous user'
        assert '<dd>@testuser</dd>' not in response.body, \
            'Twitter handle is in profile for anonymous user'

        # Display email and twitter handle for the user
        response = self.app.get(url(controller='account', action='profile',
                                    name='test'), extra_environ={'REMOTE_USER':
                                                                 'test'})

        assert '200' in response.status, \
            'Profile not successfully returned for user'
        assert '<dt>Email</dt>' in response.body, \
            'Email heading is not in profile for the user'
        assert '<dd>test@example.com</dd>' in response.body, \
            'Email of user is not in profile for the user'
        assert '<dt>Twitter</dt>' in response.body, \
            'Twitter heading is not in profile for the user'
        assert '@testuser' in response.body, \
            'Twitter handle of user is not in profile for the user'

        # Immitate that the user now makes email address and twitter handle
        # public to all
        test.public_email = True
        test.public_twitter = True
        db.session.add(test)
        db.session.commit()

        # Get the site as an anonymous user
        response = self.app.get(url(controller='account', action='profile',
                                    name='test'))

        assert '200' in response.status, \
            'Profile with public contact info not returned to anonymous user'
        assert '<dt>Email</dt>' in response.body, \
            'Public email heading not in profile for anonymous user'
        assert '<dd>test@example.com</dd>' in response.body, \
            'Public email not in profile for anonymous user'
        assert '<dt>Twitter</dt>' in response.body, \
            'Public Twitter heading not in profile for anonymous user'
        assert '@testuser' in response.body, \
            'Public Twitter handle not in profile for anonymous user'

        # We take it back and hide the email and the twitter handle
        test.public_email = False
        test.public_twitter = False
        db.session.add(test)
        db.session.commit()

        # Create admin user
        admin_user = make_account('admin', 'Admin', 'admin@os.com')
        admin_user.admin = True
        db.session.add(admin_user)
        db.session.commit()

        # Display email for admins
        response = self.app.get(url(controller='account', action='profile',
                                    name='test'), extra_environ={'REMOTE_USER':
                                                                 'admin'})

        assert '200' in response.status, \
            'Profile not successfully returned for admins'
        assert '<dt>Name</dt>' in response.body, \
            'Full name heading not in profile for admins'
        assert '<dd>Test User</dd>' in response.body, \
            'Full name of user not in profile for admins'
        assert '<dt>Username</dt>' in response.body, \
            'Username heading not in profile for admins'
        assert '<dd>test</dd>' in response.body, \
            'Username of user not in profile for admins'
        assert '<dt>Email</dt>' in response.body, \
            'Email heading not in profile for admins'
        assert '<dd>test@example.com</dd>' in response.body, \
            'Email of user not in profile for admins'
        assert '<dt>Twitter</dt>' in response.body, \
            'Twitter heading not in profile for admins'
        assert '@testuser' in response.body, \
            'Twitter handle of user not in profile for admins'

        # Do not display fullname if it's empty
        test.fullname = ''
        db.session.add(test)
        db.session.commit()

        response = self.app.get(url(controller='account', action='profile',
                                    name='test'))

        assert '200' in response.status, \
            'Profile page not successfully returned without full name'
        assert '<dt>Name</dt>' not in response.body, \
            'Name heading is in profile even though full name is empty'
        # Test if the information is missing or just the full name
        assert '<dt>Username</dt>' in response.body, \
            'Username heading is not in profile when full name is empty'
        assert '<dd>test</dd>' in response.body, \
            'Username for user is not in profile when full name is empty'

        # Do not display twitter handle if it's empty
        test.twitter_handle = None
        db.session.add(test)
        db.session.commit()

        response = self.app.get(url(controller='account', action='profile',
                                    name='test'), extra_environ={'REMOTE_USER':
                                                                 'test'})
        # Check if the Twitter heading is there
        assert '<dt>Twitter</dt>' not in response.body, \
            'Twitter heading is in profile even though twitter handle is empty'
        # Test if the other information is missing
        assert '<dt>Username</dt>' in response.body, \
            'Username heading is not in profile when Twitter handle is empty'
        assert '<dd>test</dd>' in response.body, \
            'Username for user is not in profile when Twitter handle is empty'

    def test_terms_check(self):
        """
        Test whether terms of use are present on the signup page (login) page
        and whether they are a required field.
        """

        # Get the login page
        response = self.app.get(url(controller='account', action='login'))
        assert '200' in response.status, \
            'Error (status is not 200) while retrieving the login/signup page'

        # Check if user can send an input field for terms of use/privacy
        assert 'name="terms"' in response.body, \
            'Terms of use input field not present'

        # Check whether the terms of use url is in the response
        # For now we rely on an external terms of use page and we therefore
        # check whether that page also exists.
        assert 'http://okfn.org/terms-of-use' in response.body, \
            'Terms of use url not in response'

        # Headers to immitate a browser
        headers = {'User-Agent': 'OpenSpending in-site browser',
                   'Accept': ','.join(['text/html', 'application/xhtml+xml',
                                       'application/xml;q=0.9', '*/*;q=0.8'])
                   }

        # We use urllib2 instead of webtest's get because of redirects
        request = urllib2.Request('http://okfn.org/terms-of-use',
                                  headers=headers)
        external_response = urllib2.urlopen(request)
        assert 200 == external_response.getcode(), \
            'External terms of use page not found'

        # Check whether the privay policy url is in the response
        # For now we rely on an external privacy policy and we therefore
        # check whether that page also exists.
        assert 'http://okfn.org/privacy-policy' in response.body, \
            'Privacy policy url not in response'

        request = urllib2.Request('http://okfn.org/privacy-policy',
                                  headers=headers)
        external_response = urllib2.urlopen(request)
        assert 200 == external_response.getcode(), \
            'External privacy policy page not found'

        # Check that not filling up the field throws a 'required' response
        # if the terms box is not in the post request (not checked)
        response = self.app.post(url(controller='account', action='register'),
                                 params={'name': 'termschecker',
                                         'fullname': 'Term Checker',
                                         'email': 'termchecker@test.com',
                                         'password1': 'secret',
                                         'password2': 'secret'})
        assert 'name="terms"' in response.body, \
            'Terms of use checkbox not present after registering without tick'
        # Check if user is told it is required (this can be anywhere on the
        # page, and might not even be tied to terms of use checkbox but it
        # should be present nonetheless)
        assert 'Required' in response.body, \
            'User is not told that a field is "Required"'

        # Check that terms input field is not present after a successful
        # register
        response = self.app.post(url(controller='account', action='register'),
                                 params={'name': 'termschecker',
                                         'fullname': 'Term Checker',
                                         'email': 'termchecker@test.com',
                                         'password1': 'secret',
                                         'password2': 'secret',
                                         'terms': True})
        assert 'name="terms"' not in response.body, \
            'Terms of use checkbox is present even after a successful register'

    def test_vary_header(self):
        """
        Test whether the Vary header is set to change on Cookies and whether
        the ETag gets a different value based on the cookies. This allows
        intermediate caches to serve different content based on whether the
        user is logged in or not
        """

        # We need to perform a get to get the cache settings from app globals
        response = self.app.get(url(controller='home', action='index'))

        # Get the cache settings from the app globals
        cache_settings = response.app_globals.cache_enabled
        # Enable cache
        response.app_globals.cache_enabled = True

        # Get the view page again (now with cache enabled)
        response = self.app.get(url(controller='home', action='index'))

        # Enforce check based on cookies
        assert 'Vary' in response.headers, \
            'Vary header is not present in response'
        assert 'Cookie' in response.headers.get('Vary'), \
            'Cookie is not in the vary header'

        # Save the ETag for an assertion
        etag_for_no_cookie = response.headers.get('etag')

        # Set a dummy login cookie
        self.app.cookies['openspending.login'] = 'testcookie'
        # Do a get (we don't need the remote user but let's try to immitate
        # a GET as closely as possible)
        response = self.app.get(url(controller='home', action='index'),
                                extra_environ={'REMOTE_USER': 'test'})

        # Get the ETag for the login cookie based GET
        etag_for_cookie = response.headers.get('etag')
        # Check if ETag is different in a login cookie based GET
        assert etag_for_cookie != etag_for_no_cookie, \
            'ETags for login cookie and no login cookie are the same'

        # Remove the login cookie from the cookiejar
        del self.app.cookies['openspending.login']

        # Reset cache setting
        response.app_globals.cache_enabled = cache_settings

    def test_user_scoreboard(self):
        """
        Test if the user scoreboard works and is only accessible by
        administrators
        """

        # Create dataset and users and make normal user owner of dataset
        admin_user = make_account('test_admin', admin=True)

        dataset = load_fixture('cra')
        normal_user = make_account('test_user')
        normal_user.datasets.append(dataset)
        db.session.add(normal_user)
        db.session.commit()

        # Refetch the accounts into scope after the commit
        admin_user = Account.by_name('test_admin')
        normal_user = Account.by_name('test_user')

        # Get the URL to user scoreboard
        scoreboard_url = url(controller='account', action='scoreboard')
        # Get the home page (could be just any page
        user_response = self.app.get(url(controller='home', action='index'),
                                     extra_environ={'REMOTE_USER':
                                                    str(normal_user.name)})
        admin_response = self.app.get(url(controller='home', action='index'),
                                      extra_environ={'REMOTE_USER':
                                                     str(admin_user.name)})

        # Admin user should be the only one to see a link
        # to the user scoreboard (not the normal user)
        assert scoreboard_url not in user_response.body, \
            "Normal user can see scoreboard url on the home page"

        assert scoreboard_url in admin_response.body, \
            "Admin user cannot see the scoreboard url on the home page"

        # Normal user should not be able to access the scoreboard url
        user_response = self.app.get(scoreboard_url,
                                     expect_errors=True,
                                     extra_environ={'REMOTE_USER':
                                                    str(normal_user.name)})
        assert '403' in user_response.status, \
            "Normal user is authorized to see user scoreboard"

        # Administrator should see scoreboard and users should be there in
        # in the following order normal user - admin user (with 10 and 0 points
        # respectively)
        admin_response = self.app.get(scoreboard_url,
                                      extra_environ={'REMOTE_USER':
                                                     str(admin_user.name)})

        assert '200' in admin_response.status, \
            "Administrator did not get a 200 status for user scoreboard"

        # We need to remove everything before an 'Dataset Maintainers' because
        # the admin user name comes first because of the navigational bar
        heading_index = admin_response.body.find('Dataset Maintainers')
        check_body = admin_response.body[heading_index:]

        admin_index = check_body.find(admin_user.name)
        user_index = check_body.find(normal_user.name)
        assert admin_index > user_index, \
            "Admin index comes before normal user index"

        # Same thing as with the username we check for the scores
        # they are represented as <p>10</p> and <p>0</p>
        admin_index = check_body.find('<p>0</p>')
        user_index = check_body.find('<p>10</p>')
        assert admin_index > user_index, \
            "Admin score does not come before the user score"

########NEW FILE########
__FILENAME__ = test_api_version1
from openspending.lib import json

from openspending.tests.base import ControllerTestCase
from openspending.tests.helpers import load_fixture, clean_and_reindex_solr
from pylons import url


class TestApiController(ControllerTestCase):

    def setup(self):
        super(TestApiController, self).setup()
        load_fixture('cra')

    def test_aggregate(self):
        response = self.app.get(url(controller='api/version1',
                                    action='aggregate',
                                    dataset='cra'))
        assert '"metadata": {' in response, response
        assert '"dataset": "cra"' in response, response
        assert '"include": []' in response, response
        assert '"dates":' in response, response
        assert '"axes": []' in response, response
        assert '"results": [' in response

    def test_aggregate_with_breakdown(self):
        u = url(controller='api/version1', action='aggregate', **{
            'dataset': 'cra',
            'breakdown-region': 'yes',
        })
        response = self.app.get(u)
        assert '"region"' in response, response
        assert '"ENGLAND_London"' in response, response

    def test_jsonp_aggregate(self):
        # Copied from test_aggregate_with_breakdown.
        callback = randomjsonpcallback()
        u = url(controller='api/version1',
                callback=callback, action='aggregate', **{
                    'dataset': 'cra',
                    'breakdown-region': 'yes',
                })
        response = self.app.get(u)
        assert '"region"' in response, response
        assert '"ENGLAND_London"' in response, response
        assert valid_jsonp(response, callback)

    def test_aggregate_with_per_region(self):
        u = url(controller='api/version1', action='aggregate', **{
            'dataset': 'cra',
            'breakdown-region': 'yes',
            'per-population2006': 'region'
        })
        response = self.app.get(u)
        assert '"region"' in response, response
        assert '"ENGLAND_London"' in response, response
        assert '0.1' in response, response

    def test_mytax(self):
        u = url(controller='api/version1', action='mytax', income=20000)
        response = self.app.get(u)
        assert '"tax": ' in response, response
        assert '"explanation": ' in response, response
        # TODO: check amounts still work.

    def test_jsonp_mytax(self):
        # Copied from test_mytax.
        callback = randomjsonpcallback()
        u = url(controller='api/version1', action='mytax', income=20000,
                callback=callback)
        response = self.app.get(u)
        assert '"tax": ' in response, response
        assert '"explanation": ' in response, response
        assert valid_jsonp(response, callback)


def randomjsonpcallback(prefix='cb'):
    """Generate a random identifier suitable, beginning with *prefix*,
    for using as a JSONP callback name."""

    import random
    import string
    return prefix + ''.join(random.choice(string.letters) for
                            _ in range(6))


def valid_jsonp(response, callback):
    """True if *response* is valid JSONP using *callback* as the
    callback name.  Currently does not completely validate
    everything."""

    return (
        ((callback + '(') in response, response) and
        (str(response)[-2:] == ');' or str(response)[-1] == ')')
    )


class TestApiSearch(ControllerTestCase):

    def setup(self):

        super(TestApiSearch, self).setup()
        load_fixture('cra')
        clean_and_reindex_solr()

    def test_search_01_no_query(self):
        response = self.app.get(
            url(controller='api/version1', action='search'))
        out = json.loads(str(response.body))['response']
        assert out['numFound'] == 36, out['numFound']
        assert out['docs'][0]['dataset'] == 'cra', out

    def test_search_02_query(self):
        response = self.app.get(url(controller='api/version1', action='search',
                                    q='Children'))
        out = json.loads(str(response.body))['response']
        assert out['numFound'] == 7, out['numFound']
        exp_entity = 'Department for Children, Schools and Families'

        assert out['docs'][0]['from.label_facet'] == exp_entity, out['docs'][0]

    def test_search_03_jsonpify(self):
        callback = 'mycallback'
        response = self.app.get(url(controller='api/version1', action='search',
                                    q='children', callback=callback))
        assert response.body.startswith('%s({"responseHeader"'
                                        % callback), response.body

    def test_search_04_invalid_query(self):
        response = self.app.get(url(controller='api/version1', action='search',
                                    q='time:'), expect_errors=True)
        assert "400" in response.status, response.status

########NEW FILE########
__FILENAME__ = test_api_version2
from openspending.lib import json
from openspending.model import meta as db
from openspending.model.dataset import Dataset
from openspending.model.account import Account

from csv import DictReader
from openspending.tests.base import ControllerTestCase
from openspending.tests.helpers import (make_account, load_fixture,
                                        clean_and_reindex_solr)

from pylons import url


class TestApi2Controller(ControllerTestCase):

    def setup(self):
        super(TestApi2Controller, self).setup()
        load_fixture('cra')
        clean_and_reindex_solr()

    def test_aggregate(self):
        response = self.app.get(url(controller='api/version2',
                                    action='aggregate',
                                    dataset='cra'))
        assert response.status == '200 OK'
        assert response.content_type == 'application/json'
        result = json.loads(response.body)
        assert sorted(result.keys()) == [u'drilldown', u'summary']

        expected_result = [(u'amount', -371500000.0),
                           (u'currency', {u'amount': u'GBP'}),
                           (u'num_drilldowns', 1),
                           (u'num_entries', 36),
                           (u'page', 1),
                           (u'pages', 1),
                           (u'pagesize', 10000)]
        assert sorted(result['summary'].items()) == expected_result

    def test_aggregate_drilldown(self):
        response = self.app.get(url(controller='api/version2',
                                    action='aggregate',
                                    dataset='cra', drilldown='cofog1|cofog2'))
        assert response.status == '200 OK'

        result = json.loads(response.body)
        assert result['summary']['num_drilldowns'] == 6
        assert result['summary']['amount'] == -371500000.0

    def test_aggregate_drilldown_format_csv(self):
        response = self.app.get(url(controller='api/version2',
                                    action='aggregate',
                                    dataset='cra', drilldown='cofog1|cofog2',
                                    format='csv'))
        assert response.status == '200 OK'
        result = list(DictReader(response.body.split('\n')))
        assert len(result) == 6
        assert result[0]['cofog2.name'] == '10.1'

    def test_aggregate_measures(self):
        response = self.app.get(url(controller='api/version2',
                                    action='aggregate',
                                    dataset='cra', cut='year:2009',
                                    measure='total'))
        assert response.status == '200 OK'
        result = json.loads(response.body)
        assert result['summary']['num_drilldowns'] == 1
        assert result['summary']['total'] == 57300000.0

    def test_aggregate_multiple_measures(self):
        """
        Test whether multiple measures work. Multiple measures are
        separated with | in the measure url parameter.
        """

        # Get the aggregated amount and total values for year 2009
        response = self.app.get(url(controller='api/version2',
                                    action='aggregate',
                                    dataset='cra', cut='year:2009',
                                    measure='amount|total'))

        # This should return a status code 200.
        assert '200' in response.status, \
            'Aggregation for multiple measures did not return successfully'

        # Load the json body into a dict
        result = json.loads(response.body)

        # Only one drilldown should be made even if there are two measures
        assert result['summary']['num_drilldowns'] == 1, \
            "Aggregation of multiple measures wasn't done with one drilldown"

        # Since amount measure and total measure are two different measures
        # for the same field in the csv file they should contain the same
        # amount but still be distinct.
        assert result['summary']['total'] == 57300000.0, \
            'Multiple measure aggregation of total measure is not correct'
        assert result['summary']['amount'] == 57300000.0, \
            'Multiple measure aggregation of amount measure is not correct'

    def test_aggregate_cut(self):
        response = self.app.get(url(controller='api/version2',
                                    action='aggregate',
                                    dataset='cra', cut='year:2009'))
        assert response.status == '200 OK'
        result = json.loads(response.body)
        assert result['summary']['num_drilldowns'] == 1
        assert result['summary']['amount'] == 57300000.0

    def test_aggregate_order(self):
        def unique(seq):
            result = []
            for item in seq:
                if item not in result:
                    result.append(item)
            return result
        response = self.app.get(url(controller='api/version2',
                                    action='aggregate',
                                    dataset='cra', order='year:asc',
                                    drilldown='year'))
        assert response.status == '200 OK'
        result = json.loads(response.body)
        order = [cell['year'] for cell in result['drilldown']]
        expected_result = map(unicode, [2003, 2004, 2005, 2006, 2007,
                                        2008, 2009, 2010])
        assert unique(order) == expected_result

        response = self.app.get(url(controller='api/version2',
                                    action='aggregate',
                                    dataset='cra', order='year:desc',
                                    drilldown='year'))
        assert response.status == '200 OK'
        result = json.loads(response.body)
        order = [cell['year'] for cell in result['drilldown']]
        expected_result = map(unicode, [2010, 2009, 2008, 2007, 2006,
                                        2005, 2004, 2003])
        assert unique(order) == expected_result

    def test_search(self):
        response = self.app.get(
            url(controller='api/version2', action='search'))
        result = json.loads(response.body)

        assert result['stats']['results_count'] == 36
        assert result['stats']['results_count_query'] == 36
        assert result['facets'] == {}
        assert len(result['results']) == 36

    def test_search_results_dataset(self):
        response = self.app.get(
            url(controller='api/version2', action='search'))
        result = json.loads(response.body)

        assert result['results'][0]['dataset']['name'] == 'cra'
        expected_label = 'Country Regional Analysis v2009'
        assert result['results'][0]['dataset']['label'] == expected_label

    def test_search_page_pagesize(self):
        response = self.app.get(url(controller='api/version2', action='search',
                                    page=2, pagesize=10))
        result = json.loads(response.body)

        assert result['stats']['results_count'] == 10
        assert result['stats']['results_count_query'] == 36

    def test_search_q(self):
        response = self.app.get(url(controller='api/version2', action='search',
                                    q="Ministry of Justice"))
        result = json.loads(response.body)

        assert result['stats']['results_count'] == 5
        assert result['stats']['results_count_query'] == 5

        id_value = '06dafa7250420ab1dc616d2bbbe310c9ad6e485e'
        assert result['results'][0]['id'] == id_value

    def test_search_filter(self):
        response = self.app.get(url(controller='api/version2', action='search',
                                    filter="pog:P13 S091105"))
        result = json.loads(response.body)

        assert result['stats']['results_count'] == 5
        assert result['stats']['results_count_query'] == 5

        id_value = '06dafa7250420ab1dc616d2bbbe310c9ad6e485e'
        assert result['results'][0]['id'] == id_value

    def test_search_facet(self):
        response = self.app.get(url(controller='api/version2', action='search',
                                    pagesize=0, facet_field="dataset"))
        result = json.loads(response.body)

        assert len(result['facets']['dataset']) == 1
        assert result['facets']['dataset'][0] == ['cra', 36]

    def test_search_expand_facet_dimensions(self):
        response = self.app.get(url(controller='api/version2',
                                    action='search',
                                    dataset='cra',
                                    pagesize=0,
                                    facet_field="from|to.name",
                                    expand_facet_dimensions="1"))
        result = json.loads(response.body)

        hra = {
            "taxonomy": "from",
            "description": "",
            "id": 5,
            "name": "999",
            "label": "ENG_HRA"}

        assert result['facets']['from'][0][0] == hra
        assert result['facets']['to.name'][0][0] == 'society'

    def test_search_expand_facet_dimensions_no_dataset(self):
        response = self.app.get(url(controller='api/version2',
                                    action='search',
                                    pagesize=0,
                                    facet_field="from",
                                    expand_facet_dimensions="1"))
        result = json.loads(response.body)

        # facets should *NOT* be expanded unless exactly 1 dataset was
        # specified
        assert result['facets']['from'][0][0] == '999'

    def test_search_order(self):
        response = self.app.get(url(controller='api/version2', action='search',
                                    order="amount:asc"))
        result = json.loads(response.body)

        amounts = [r['amount'] for r in result['results']]
        assert amounts == sorted(amounts)

        response = self.app.get(url(controller='api/version2', action='search',
                                    order="amount:desc"))
        result = json.loads(response.body)

        amounts = [r['amount'] for r in result['results']]
        assert amounts == sorted(amounts)[::-1]

    def test_inflation(self):
        """
        Test for inflation support in the aggregation api. Inflation works
        by adding a url parameter containing the target year of inflation.

        This test has hard coded values based on existing inflation data used
        by an external module. This may therefore need updating should the
        inflation data become more accurate with better data.
        """

        response = self.app.get(url(controller='api/version2',
                                    action='aggregate',
                                    dataset='cra', cut='year:2009',
                                    inflate='2011'))
        assert '200' in response.status, \
            "Inflation request didn't return successfully (status isn't 200)"

        result = json.loads(response.body)

        # Check for inflated amount
        assert 'amount' in result['summary'], \
            "Amount is absent for the result summary"
        assert int(result['summary']['amount']) == 61836609, \
            "Inflation amount is incorrect"

        # Check for original amount
        assert 'original' in result['summary'], \
            "Original amount not in inflation request"
        assert result['summary']['original'] == 57300000.0, \
            "Original amount (for inflation) is incorrect"

        # Check for inflation adjustment object in drilldown results
        assert len(result['drilldown']) == 1, \
            "Drilldown results were not of length 1"
        assert 'inflation_adjustment' in result['drilldown'][0], \
            "Inflation adjustment is not present in drilldown results"

        # Check for what happens when inflation is not possible
        response = self.app.get(url(controller='api/version2',
                                    action='aggregate',
                                    dataset='cra', cut='year:2009',
                                    inflate='1000'))
        assert '200' in response.status, \
            "Incorrect inflation did not return sucessfully (status isn't 200)"

        result = json.loads(response.body)

        assert 'warning' in result, \
            "No warning given when inflation not possible"
        assert result['summary']['amount'] == 57300000.0, \
            "Amount does not fall back to the original amount"

    def test_permissions(self):
        """
        Test permissions API which tells users if they are allowed to
        perform CRUD operations on a given dataset
        """

        # Create our users
        make_account('test_admin', admin=True)
        maintainer = make_account('maintainer')
        make_account('test_user')

        # Set maintainer as maintainer of cra dataset
        dataset = Dataset.by_name('cra')
        dataset.managers.append(maintainer)
        db.session.add(dataset)
        db.session.commit()

        # Make the url reusable
        permission = url(controller='api/version2', action='permissions')

        # First we try to get permissions without dataset parameter
        # This should return a 200 but include an error message and nothing
        # else
        response = self.app.get(permission)
        json_response = json.loads(response.body)
        assert len(json_response.keys()) == 1, \
            'Parameterless call response includes more than one properties'
        assert 'error' in json_response, \
            'Error property not present in parameterless call response'

        # Dataset is public by default

        # Anonymous user
        response = self.app.get(permission, params={'dataset': 'cra'})
        anon_response = json.loads(response.body)
        assert not anon_response['create'], \
            'Anonymous user can create existing dataset'
        assert anon_response['read'], \
            'Anonymous user cannot read public dataset'
        assert not anon_response['update'], \
            'Anonymous user can update existing dataset'
        assert not anon_response['delete'], \
            'Anonymous user can delete existing dataset'
        # Normal user
        response = self.app.get(permission, params={'dataset': 'cra'},
                                extra_environ={'REMOTE_USER': 'test_user'})
        normal_response = json.loads(response.body)
        assert anon_response == normal_response, \
            'Normal user has wrong permissions for a public dataset'
        # Maintainer
        response = self.app.get(permission, params={'dataset': 'cra'},
                                extra_environ={'REMOTE_USER': 'maintainer'})
        main_response = json.loads(response.body)
        assert not main_response['create'], \
            'Maintainer can create a dataset with an existing (public) name'
        assert main_response['read'], \
            'Maintainer is not able to read public dataset'
        assert main_response['update'], \
            'Maintainer is not able to update public dataset'
        assert main_response['delete'], \
            'Maintainer is not able to delete public dataset'
        # Administrator
        response = self.app.get(permission, params={'dataset': 'cra'},
                                extra_environ={'REMOTE_USER': 'test_admin'})
        admin_response = json.loads(response.body)
        # Permissions for admins should be the same as for maintainer
        assert main_response == admin_response, \
            'Admin and maintainer permissions differ on public datasets'

        # Set cra dataset as private so only maintainer and admin should be
        # able to 'read' (and 'update' and 'delete'). All others should get
        # False on everything
        dataset = Dataset.by_name('cra')
        dataset.private = True
        db.session.add(dataset)
        db.session.commit()

        # Anonymous user
        response = self.app.get(permission, params={'dataset': 'cra'})
        anon_response = json.loads(response.body)
        assert True not in anon_response.values(), \
            'Anonymous user has access to a private dataset'
        # Normal user
        response = self.app.get(permission, params={'dataset': 'cra'},
                                extra_environ={'REMOTE_USER': 'test_user'})
        normal_response = json.loads(response.body)
        assert anon_response == normal_response, \
            'Normal user has access to a private dataset'
        # Maintainer
        response = self.app.get(permission, params={'dataset': 'cra'},
                                extra_environ={'REMOTE_USER': 'maintainer'})
        main_response = json.loads(response.body)
        assert not main_response['create'], \
            'Maintainer can create a dataset with an existing (private) name'
        assert main_response['read'], \
            'Maintainer is not able to read private dataset'
        assert main_response['update'], \
            'Maintainer is not able to update private dataset'
        assert main_response['delete'], \
            'Maintainer is not able to delete private dataset'
        # Administrator
        response = self.app.get(permission, params={'dataset': 'cra'},
                                extra_environ={'REMOTE_USER': 'test_admin'})
        admin_response = json.loads(response.body)
        # Permissions for admins should be the same as for maintainer
        assert main_response == admin_response, \
            'Admin does not have the same permissions as maintainer'

        # Now we try accessing a nonexistent dataset
        # Everyone except anonymous user should have the same permissions
        # We don't need to check with maintainer or admin now since this
        # applies to all logged in users
        response = self.app.get(permission, params={'dataset': 'nonexistent'})
        anon_response = json.loads(response.body)
        assert True not in anon_response.values(), \
            'Anonymous users has permissions on a nonexistent datasets'
        # Any logged in user (we use normal user)
        response = self.app.get(permission, params={'dataset': 'nonexistent'},
                                extra_environ={'REMOTE_USER': 'test_user'})
        normal_response = json.loads(response.body)
        assert normal_response['create'], \
            'User cannot create a nonexistent dataset'
        assert not normal_response['read'], \
            'User can read a nonexistent dataset'
        assert not normal_response['update'], \
            'User can update a nonexistent dataset'
        assert not normal_response['delete'], \
            'User can delete a nonexistent dataset'


class TestApiNewDataset(ControllerTestCase):

    """
    This checks for authentication with a header api key while also
    testing for loading of data via the api
    """

    def setup(self):
        super(TestApiNewDataset, self).setup()
        self.user = make_account('test_new')
        self.user.api_key = 'd0610659-627b-4403-8b7f-6e2820ebc95d'

        self.user2 = make_account('test_new2')
        self.user2.api_key = 'c011c340-8dad-419c-8138-1c6ded86ead5'

    def test_new_dataset(self):
        user = Account.by_name('test_new')
        assert user.api_key == 'd0610659-627b-4403-8b7f-6e2820ebc95d'

        u = url(controller='api/version2', action='create')
        params = {
            'metadata':
            'https://dl.dropbox.com/u/3250791/sample-openspending-model.json',
            'csv_file':
            'http://mk.ucant.org/info/data/sample-openspending-dataset.csv'
        }
        apikey_header = 'apikey {0}'.format(user.api_key)
        response = self.app.post(u, params, {'Authorization': apikey_header})
        assert "200" in response.status
        assert Dataset.by_name('openspending-example') is not None

    def test_new_no_apikey(self):
        u = url(controller='api/version2', action='create')
        params = {
            'metadata':
            'https://dl.dropbox.com/u/3250791/sample-openspending-model.json',
            'csv_file':
            'http://mk.ucant.org/info/data/sample-openspending-dataset.csv'
        }
        response = self.app.post(u, params, expect_errors=True)
        assert "400" in response.status
        assert Dataset.by_name('openspending-example') is None

    def test_new_wrong_user(self):
        # First we add a Dataset with user 'test_new'
        user = Account.by_name('test_new')
        assert user.api_key == 'd0610659-627b-4403-8b7f-6e2820ebc95d'

        u = url(controller='api/version2', action='create')
        params = {
            'metadata':
            'https://dl.dropbox.com/u/3250791/sample-openspending-model.json',
            'csv_file':
            'http://mk.ucant.org/info/data/sample-openspending-dataset.csv'
        }
        apikey_header = 'apikey {0}'.format(user.api_key)
        response = self.app.post(u, params, {'Authorization': apikey_header})

        assert "200" in response.status
        assert Dataset.by_name('openspending-example') is not None

        # After that we try to update the Dataset with user 'test_new2'
        user = Account.by_name('test_new2')
        assert user.api_key == 'c011c340-8dad-419c-8138-1c6ded86ead5'

        u = url(controller='api/version2', action='create')
        params = {
            'metadata':
            'https://dl.dropbox.com/u/3250791/sample-openspending-model.json',
            'csv_file':
            'http://mk.ucant.org/info/data/sample-openspending-dataset.csv'
        }
        apikey_header = 'apikey {0}'.format(user.api_key)
        response = self.app.post(u, params, {'Authorization': apikey_header},
                                 expect_errors=True)
        assert '403' in response.status

########NEW FILE########
__FILENAME__ = test_badge
from openspending.tests.base import ControllerTestCase
from openspending.tests.helpers import make_account, load_fixture
from openspending.ui.lib import helpers

from openspending.model import meta as db
from openspending.model.badge import Badge
from pylons import config, url
import json
import os


class TestBadgeController(ControllerTestCase):

    def setup(self):
        """
        Set up the TestBadgeController. Setup creates two users, one regular
        user (test) and one administrator (admin).
        """
        super(TestBadgeController, self).setup()

        # Create test user
        self.user = make_account('test')

        # Create admin user
        self.admin = make_account('admin')
        self.admin.admin = True
        db.session.commit()

        # Load dataset we use for tests
        self.dataset = load_fixture('cra')

    def test_create_badge_form(self):
        """
        Test to see if create badge form is present on badge index page.
        The test should only be available to administrators. For this we
        check whether badge creation url is present on the html sites.
        """

        # Get badge create url
        create_url = url(controller='badge', action='create')

        # Check for non-users (visitors/guests)
        response = self.app.get(url(controller='badge', action='index'))
        assert create_url not in response.body, \
            "URL to create a badge is present in badge index for non-users"

        # Check for normal users
        response = self.app.get(url(controller='badge', action='index'),
                                extra_environ={'REMOTE_USER': 'test'})
        assert create_url not in response.body, \
            "URL to create a badge is present in badge index for normal users"

        response = self.app.get(url(controller='badge', action='index'),
                                extra_environ={'REMOTE_USER': 'admin'})
        assert create_url in response.body, \
            "URL to create a badge is not present in badge index for admins"

    def test_create_badge(self):
        """
        Test badge creation. Only administrators can create badges.
        To create a badge user must provide label, description and image
        """

        # Get all existing badges (should be zero but we never know)
        badge_json = self.app.get(url(controller='badge', action='index',
                                      format='json'))
        badge_index = json.loads(badge_json.body)
        existing_badges = len(badge_index['badges'])

        # The dummy files we'll upload
        files = [("badge-image", "badge.png", "Test badge file")]

        # Create upload directory if it doesn't exist
        object_upload_dir = os.path.join(
            config['pylons.paths']['static_files'],
            config.get('openspending.upload_directory', 'test_uploads'))

        if os.path.isdir(object_upload_dir):
            # Upload dir exists (so we won't change a thing)
            upload_dir_created = False
        else:
            # Doesn't exist (so we'll remove it afterwards
            os.mkdir(object_upload_dir, 0o744)
            upload_dir_created = True

        # Create a new badge (should return unauthorized)
        response = self.app.post(
            url(controller='badge', action='create'),
            params={'badge-label': 'testbadge',
                    'badge-description': 'testdescription'},
            upload_files=files,
            expect_errors=True)

        # Check if it returned Forbidden (which is http status code 403)
        # This should actually return 401 Unauthorized but that's an
        # authentication implementation failure (which should be fixed)
        assert '403' in response.status, \
            "Non-user should get an error when trying to create a badge"

        # Check to see that badge list didn't change
        badge_json = self.app.get(url(controller='badge', action='index',
                                      format='json'))
        assert badge_index == json.loads(badge_json.body), \
            "A non-user was able to change the existing badges"

        # Create a new badge (should return forbidden)
        response = self.app.post(
            url(controller='badge', action='create'),
            params={'badge-label': 'testbadge',
                    'badge-description': 'testdescription'},
            upload_files=files,
            extra_environ={'REMOTE_USER': 'test'},
            expect_errors=True)

        # Check if it returned Forbidden (which is http status code 403)
        assert '403' in response.status, \
            "Non-admin user should get an error when trying to create a badge"

        # Check to see that badge list didn't change
        badge_json = self.app.get(url(controller='badge', action='index',
                                      format='json'))
        assert badge_index == json.loads(badge_json.body), \
            "A non-admin user was able to change the existing badges"

        response = self.app.post(
            url(controller='badge', action='create'),
            params={'badge-label': 'testbadge',
                    'badge-description': 'testdescription'},
            upload_files=files,
            extra_environ={'REMOTE_USER': 'admin'})

        # Check to see there is now badge more in the list than to begin with
        badge_json = self.app.get(url(controller='badge', action='index',
                                      format='json'))
        badge_index = json.loads(badge_json.body)
        assert len(badge_index['badges']) == existing_badges + 1, \
            "One badge should have been added but it wasn't"

        # Check image exists
        # Get image filename from url
        image = badge_index['badges'][0]['image'].split('/')[-1]
        # Get the uploaded file on the system
        upload_dir = helpers.get_object_upload_dir(Badge)
        uploaded_file = os.path.join(upload_dir, image)
        # Check if file exists
        assert os.path.exists(uploaded_file), \
            "Uploaded badge image isn't present on the file system"
        # Remove the file or we'll have a lot of random files after test runs
        os.remove(uploaded_file)

        # If we have created the upload directory we should remove upload_dir
        if upload_dir_created:
            os.rmdir(upload_dir)
            os.rmdir(object_upload_dir)

        # Check to be certain both label and description are present
        assert badge_index['badges'][0]['label'] == 'testbadge', \
            "Uploaded badge label isn't correct"
        assert badge_index['badges'][0]['description'] == 'testdescription', \
            "Uploaded badge description isn't correct"

        # No datasets should be present for the badge just after creation
        assert len(badge_index['badges'][0]['datasets']) == 0, \
            "Newly created badge shouldn't have been awarded to datasets"

    def test_give_badge_form(self):
        """
        Test if badge giving form is only present for admin users on about
        page. Only administrators should see a form to award a badge on the
        about page for a given dataset.
        """

        # Get badge create url
        badge_give_url = url(controller='badge', action='give',
                             dataset=self.dataset.name)

        # Check for non-users (visitors/guests)
        response = self.app.get(url(controller='dataset', action='about',
                                    dataset=self.dataset.name))
        assert badge_give_url not in response.body, \
            "URL to give dataset a badge is in about page for non-users"

        # Check for normal users
        response = self.app.get(url(controller='dataset', action='about',
                                    dataset=self.dataset.name),
                                extra_environ={'REMOTE_USER': 'test'})
        assert badge_give_url not in response.body, \
            "URL to give dataset a badge is in about page for normal users"

        response = self.app.get(url(controller='dataset', action='about',
                                    dataset=self.dataset.name),
                                extra_environ={'REMOTE_USER': 'admin'})
        assert badge_give_url in response.body, \
            "URL to give dataset a badge isn't in about page for admins"

    def test_give_badge(self):
        """
        Test giving dataset a badge. Only administrators should be able to
        give datasets a badge.
        """

        badge = Badge('give-me', 'testimage', 'give me', self.admin)
        db.session.add(badge)
        db.session.commit()

        # Check if non-user can award badges
        response = self.app.post(url(controller='badge', action='give',
                                     dataset='cra'),
                                 params={'badge': badge.id},
                                 expect_errors=True)
        # Check if it returned Forbidden (which is http status code 403)
        # This should actually return 401 Unauthorized but that's an
        # authentication implementation failure (which should be fixed)
        assert '403' in response.status, \
            "Non-user should get an error when trying to give a badge"

        # Check to see that badge hasn't been awarded to any datasets
        badge_json = self.app.get(url(controller='badge', action='information',
                                      id=badge.id, format='json'))
        badge_info = json.loads(badge_json.body)
        assert len(badge_info['badge']['datasets']) == 0, \
            "A non-user was able to award a badge"

        # Check if normal user can award badges
        response = self.app.post(url(controller='badge', action='give',
                                     dataset='cra'),
                                 params={'badge': badge.id},
                                 extra_environ={'REMOTE_USER': 'test'},
                                 expect_errors=True)
        # Check if it returned Forbidden (which is http status code 403)
        assert '403' in response.status, \
            "A normal user should get an error when trying to give a badge"

        # Check to see that badge hasn't been awarded to any datasets
        badge_json = self.app.get(url(controller='badge', action='information',
                                      id=badge.id, format='json'))
        badge_info = json.loads(badge_json.body)
        assert len(badge_info['badge']['datasets']) == 0, \
            "A normal user was able to award a badge"

        # Finally we check if admin user can award badges
        response = self.app.post(url(controller='badge', action='give',
                                     dataset='cra'),
                                 params={'badge': 'not an id'},
                                 extra_environ={'REMOTE_USER': 'admin'},
                                 expect_errors=True)

        # Check to see that badge hasn't been awarded to the dataset
        badge_json = self.app.get(url(controller='badge', action='information',
                                      id=badge.id, format='json'))
        badge_info = json.loads(badge_json.body)
        # Check if admin was able to give the badge to a dataset
        assert len(badge_info['badge']['datasets']) == 0, \
            "Admin user was able to award a badge"

        # Finally we check if admin user can award badges
        response = self.app.post(url(controller='badge', action='give',
                                     dataset='cra'),
                                 params={'badge': badge.id},
                                 extra_environ={'REMOTE_USER': 'admin'})

        # Check to see that badge has been awarded to the dataset
        badge_json = self.app.get(url(controller='badge', action='information',
                                      id=badge.id, format='json'))
        badge_info = json.loads(badge_json.body)
        # Check if admin was able to give the badge to a dataset
        assert len(badge_info['badge']['datasets']) == 1, \
            "Admin user wasn't able to award a badge"
        # Check if admin gave it to the write dataset
        assert self.dataset.name in badge_info['badge']['datasets'], \
            "Admin user gave the badge to the incorrect dataset"

########NEW FILE########
__FILENAME__ = test_dataset
import csv
import json
import datetime
from StringIO import StringIO

from openspending.model import meta as db
from openspending.model.dataset import Dataset
from openspending.tests.base import ControllerTestCase
from openspending.tests.helpers import (make_account, load_fixture,
                                        clean_and_reindex_solr)

from pylons import url


class TestDatasetController(ControllerTestCase):

    def setup(self):

        super(TestDatasetController, self).setup()
        self.dataset = load_fixture('cra')
        self.user = make_account('test')
        clean_and_reindex_solr()

    def test_index(self):
        response = self.app.get(url(controller='dataset', action='index'))
        assert 'The database contains the following datasets' in response
        assert 'cra' in response

    def test_index_json(self):
        response = self.app.get(
            url(controller='dataset', action='index', format='json'))
        obj = json.loads(response.body)
        assert len(obj['datasets']) == 1
        assert obj['datasets'][0]['name'] == 'cra'
        assert obj['datasets'][0]['label'] == 'Country Regional Analysis v2009'

    def test_index_hide_private(self):
        cra = Dataset.by_name('cra')
        cra.private = True
        db.session.commit()
        response = self.app.get(
            url(controller='dataset', action='index', format='json'))
        obj = json.loads(response.body)
        assert len(obj['datasets']) == 0

    def test_index_csv(self):
        response = self.app.get(
            url(controller='dataset', action='index', format='csv'))
        r = csv.DictReader(StringIO(response.body))
        obj = [l for l in r]
        assert len(obj) == 1
        assert obj[0]['name'] == 'cra'
        assert obj[0]['label'] == 'Country Regional Analysis v2009'

    def test_view(self):
        """
        Test view page for a dataset
        """

        # Get the view page for the dataset
        response = self.app.get(
            url(controller='dataset', action='view', dataset='cra'))
        # The dataset label should be present in the response
        assert 'Country Regional Analysis v2009' in response, \
            "'Country Regional Analysis v2009' not in response!"

        # Assertions about time range
        assert 'Time range' in response.body, \
            'Time range is not present on view page for dataset'
        # Start date comes from looking at the test fixture for cra
        assert '2003-01-01' in response.body, \
            'Starting date of time range not on view page for dataset'
        # End date comes from looking at the test fixture for cra
        assert '2010-01-01' in response.body, \
            'End date of time range not on view page for dataset'

    def test_view_private(self):
        cra = Dataset.by_name('cra')
        cra.private = True
        db.session.commit()
        response = self.app.get(url(controller='dataset', action='view',
                                    dataset='cra'), status=403)
        assert 'Country Regional Analysis v2009' not in response, \
            "'Country Regional Analysis v2009' in response!"
        assert 'openspending_browser' not in response, \
            "'openspending_browser' in response!"

    def test_about_has_format_links(self):
        url_ = url(controller='dataset', action='about', dataset='cra')
        response = self.app.get(url_)

        url_ = url(controller='dataset', action='model', dataset='cra',
                   format='json')

        assert url_ in response, \
            "Link to view page (JSON format) not in response!"

    def test_about_has_profile_links(self):
        self.dataset.managers.append(self.user)
        db.session.add(self.dataset)
        db.session.commit()
        response = self.app.get(url(controller='dataset', action='about',
                                    dataset='cra'))
        profile_url = url(controller='account', action='profile',
                          name=self.user.name)
        profile_link = '<li><a href="{url}">{fullname}</a></li>'.format(
            url=profile_url, fullname=self.user.fullname)
        assert profile_link in response.body

    def test_about_has_timestamps(self):
        """
        Test whether about page includes timestamps when dataset was created
        and when it was last updated
        """

        # Get the about page
        response = self.app.get(url(controller='dataset', action='about',
                                    dataset='cra'))

        # Check assertions for timestamps
        assert 'Timestamps' in response.body, \
            'Timestamp header is not on about page'
        assert 'created on' in response.body, \
            'No line for "created on" on about page'
        assert 'last modified on' in response.body, \
            'No line for "last modified on" on about page'
        assert datetime.datetime.now().strftime('%Y-%m-%d') in response.body, \
            'Created (and update) timestamp is not on about page'

    def test_view_json(self):
        response = self.app.get(url(controller='dataset', action='view',
                                    dataset='cra', format='json'))
        obj = json.loads(response.body)
        assert obj['name'] == 'cra'
        assert obj['label'] == 'Country Regional Analysis v2009'

    def test_model_json(self):
        response = self.app.get(url(controller='dataset', action='model',
                                    dataset='cra', format='json'))
        obj = json.loads(response.body)
        assert 'dataset' in obj.keys(), obj
        assert obj['dataset']['name'] == 'cra'
        assert obj['dataset']['label'] == 'Country Regional Analysis v2009'

    def test_entries_json_export(self):
        response = self.app.get(url(controller='entry',
                                    action='index',
                                    dataset='cra',
                                    format='json'))
        assert '/api/2/search' in response.headers['Location'], \
            response.headers
        assert 'format=json' in response.headers['Location'], response.headers

    def test_entries_csv_export(self):
        response = self.app.get(url(controller='entry',
                                    action='index',
                                    dataset='cra',
                                    format='csv'))
        assert '/api/2/search' in response.headers['Location'], \
            response.headers
        assert 'format=csv' in response.headers['Location'], response.headers
        response = response.follow()
        r = csv.DictReader(StringIO(response.body))
        obj = [l for l in r]
        assert len(obj) == 36

    def test_new_form(self):
        response = self.app.get(url(controller='dataset', action='new'),
                                params={'limit': '20'},
                                extra_environ={'REMOTE_USER': 'test'})
        assert "Import a dataset" in response.body

    def test_create_dataset(self):
        response = self.app.post(url(controller='dataset', action='create'),
                                 extra_environ={'REMOTE_USER': 'test'})
        assert "Import a dataset" in response.body
        assert "Required" in response.body

        params = {'name': 'testds', 'label': 'Test Dataset',
                  'category': 'budget', 'description': 'I\'m a banana!',
                  'currency': 'EUR'}

        response = self.app.post(url(controller='dataset', action='create'),
                                 params=params,
                                 extra_environ={'REMOTE_USER': 'test'})
        assert "302" in response.status

        ds = Dataset.by_name('testds')
        assert ds.label == params['label'], ds

    def test_feeds(self):
        # Anonymous user with one public dataset
        response = self.app.get(url(controller='dataset', action='feed_rss'),
                                expect_errors=True)
        assert 'application/xml' in response.content_type
        assert '<title>Recently Created Datasets</title>' in response
        assert '<item><title>Country Regional Analysis v2009' in response
        cra = Dataset.by_name('cra')
        cra.private = True
        db.session.add(cra)
        db.session.commit()

        # Anonymous user with one private dataset
        response = self.app.get(url(controller='dataset', action='feed_rss'),
                                expect_errors=True)
        assert 'application/xml' in response.content_type
        assert '<title>Recently Created Datasets</title>' in response
        assert '<item><title>Country Regional Analysis v2009' not in response

        # Logged in user with one public dataset
        cra.private = False
        db.session.add(cra)
        db.session.commit()
        response = self.app.get(url(controller='dataset', action='feed_rss'),
                                expect_errors=True,
                                extra_environ={'REMOTE_USER': 'test'})
        assert 'application/xml' in response.content_type
        assert '<title>Recently Created Datasets</title>' in response
        assert '<item><title>Country Regional Analysis v2009' in response

        # Logged in user with one private dataset
        cra.private = True
        db.session.add(cra)
        db.session.commit()
        response = self.app.get(url(controller='dataset', action='feed_rss'),
                                expect_errors=True,
                                extra_environ={'REMOTE_USER': 'test'})
        assert 'application/xml' in response.content_type
        assert '<title>Recently Created Datasets</title>' in response
        assert '<item><title>Country Regional Analysis v2009' not in response

        # Logged in admin user with one private dataset
        admin_user = make_account('admin')
        admin_user.admin = True
        db.session.add(admin_user)
        db.session.commit()
        response = self.app.get(url(controller='dataset', action='feed_rss'),
                                extra_environ={'REMOTE_USER': 'admin'})
        assert '<title>Recently Created Datasets</title>' in response
        assert '<item><title>Country Regional Analysis v2009' in response
        assert 'application/xml' in response.content_type

        response = self.app.get(url(controller='dataset', action='index'))
        assert ('<link rel="alternate" type="application/rss+xml" title="'
                'Latest Datasets on OpenSpending" href="/datasets.rss"' in
                response)

########NEW FILE########
__FILENAME__ = test_dimension
import json

from openspending.tests.base import ControllerTestCase
from openspending.tests.helpers import load_fixture, clean_and_reindex_solr
from openspending.ui.lib.helpers import member_url
from openspending.model.dataset import Dataset
from openspending.model.dimension import CompoundDimension

from pylons import url


class TestDimensionController(ControllerTestCase):

    def setup(self):
        super(TestDimensionController, self).setup()
        load_fixture('cra')
        clean_and_reindex_solr()
        self.cra = Dataset.by_name('cra')
        for dimension in self.cra.dimensions:
            if isinstance(dimension, CompoundDimension) and \
                    dimension.name == 'cofog1':
                members = list(dimension.members(
                    dimension.alias.c.name == '3',
                    limit=1))
                self.member = members.pop()
                break

    def test_index(self):
        response = self.app.get(url(controller='dimension', dataset='cra',
                                    action='index'))
        assert 'Paid by' in response, "'Paid by' not in response!"
        assert 'Paid to' in response, "'Paid to' not in response!"
        assert 'Programme Object Group' in response, \
            "'Programme Object Group' not in response!"
        assert 'CG, LG or PC' in response, "'CG, LG or PC' not in response!"

    def test_index_json(self):
        response = self.app.get(url(controller='dimension', dataset='cra',
                                    action='index', format='json'))
        obj = json.loads(response.body)
        assert len(obj) == 12
        assert obj[0]['key'] == 'cap_or_cur'
        assert obj[0]['label'] == 'CG, LG or PC'

    def test_view(self):
        response = self.app.get(url(controller='dimension', dataset='cra',
                                    action='view', dimension='from'))
        assert 'Paid by' in response, "'Paid by' not in response!"
        assert 'The entity that the money was paid from.' in response.body, \
            "'The entity that the money was paid from.' not in response!"

    def test_view_json(self):
        response = self.app.get(url(controller='dimension', dataset='cra',
                                    action='view', dimension='from',
                                    format='json'))
        obj = json.loads(response.body)
        assert obj['key'] == 'from'

    def test_distinct_json(self):
        response = self.app.get(url(controller='dimension', dataset='cra',
                                    action='distinct', dimension='from',
                                    format='json'))
        obj = json.loads(response.body)['results']
        assert len(obj) == 5, obj
        assert obj[0]['name'] == 'Dept032', obj[0]

        q = 'Ministry of Ju'
        response = self.app.get(url(controller='dimension', dataset='cra',
                                    action='distinct', dimension='from',
                                    format='json', q=q))
        obj = json.loads(response.body)['results']
        assert len(obj) == 1, obj
        assert obj[0]['label'].startswith(q), obj[0]

    def test_view_member_html(self):
        url_ = member_url(self.cra.name, 'cofog1', self.member)
        result = self.app.get(url_)

        assert result.status == '200 OK'

        # Links to entries json and csv and entries listing
        assert '<a href="/cra/cofog1/3.json">' in result
        assert '<a href="/cra/cofog1/3/entries">Search</a>' in result

    def test_view_member_json(self):
        url_ = member_url(self.cra.name, 'cofog1', self.member, format='json')
        result = self.app.get(url_)

        assert result.status == '200 OK'
        assert result.content_type == 'application/json'

        json_data = json.loads(result.body)
        assert json_data['name'] == u'3'
        assert json_data['label'] == self.member['label']
        assert json_data['id'] == self.member['id']

    def test_view_entries_json(self):
        url_ = url(controller='dimension', action='entries', format='json',
                   dataset=self.cra.name,
                   dimension='cofog1',
                   name=self.member['name'])
        result = self.app.get(url_)
        result = result.follow()
        assert result.status == '200 OK'
        assert result.content_type == 'application/json'

        json_data = json.loads(result.body).get('results')
        assert len(json_data) == 5

    def test_view_entries_csv(self):
        url_ = url(controller='dimension', action='entries', format='csv',
                   dataset=self.cra.name,
                   dimension='cofog1',
                   name=self.member['name'])
        result = self.app.get(url_)
        result = result.follow()
        assert result.status == '200 OK'
        assert result.content_type == 'text/csv'
        assert 'amount,' in result.body  # csv headers
        assert 'id,' in result.body  # csv headers

    def test_view_entries_html(self):
        url_ = url(controller='dimension', action='entries', format='html',
                   dataset=self.cra.name,
                   dimension='cofog1',
                   name=self.member['name'])
        result = self.app.get(url_)
        assert result.status == '200 OK'
        assert result.content_type == 'text/html'
        # Content is filled in by client-side code.

########NEW FILE########
__FILENAME__ = test_editor
import json

from openspending.tests.base import ControllerTestCase
from openspending.tests.helpers import make_account, load_fixture
from openspending.model import meta as db
from openspending.model.dataset import Dataset
from openspending.model.source import Source

from pylons import url


class TestEditorController(ControllerTestCase):

    def setup(self):

        super(TestEditorController, self).setup()
        self.user = make_account('test')
        load_fixture('cra', self.user)

    def test_overview(self):
        response = self.app.get(url(controller='editor',
                                    action='index', dataset='cra'),
                                extra_environ={'REMOTE_USER': 'test'})
        assert 'Manage the dataset' in response.body

    def test_core_edit_mask(self):
        response = self.app.get(url(controller='editor',
                                    action='core_edit', dataset='cra'),
                                extra_environ={'REMOTE_USER': 'test'})
        assert 'EUR' in response.body
        assert 'Update' in response.body

    def test_core_update(self):
        self.app.post(url(controller='editor',
                          action='core_update', dataset='cra'),
                      params={'name': 'cra', 'label': 'Common Rough Act',
                              'description': 'I\'m a banana',
                              'currency': 'EUR', 'languages': 'en',
                              'territories': 'gb',
                              'category': 'budget', 'default_time': 2009},
                      extra_environ={'REMOTE_USER': 'test'})
        cra = Dataset.by_name('cra')
        assert cra.label == 'Common Rough Act', cra.label
        assert cra.currency == 'EUR', cra.currency

    def test_core_update_invalid_category(self):
        response = self.app.post(url(controller='editor',
                                     action='core_update', dataset='cra'),
                                 params={'name': 'cra',
                                         'label': 'Common Rough Act',
                                         'description': 'I\'m a banana',
                                         'currency': 'EUR', 'languages': 'en',
                                         'territories': 'gb',
                                         'category': 'foo',
                                         'default_time': 2009},
                                 extra_environ={'REMOTE_USER': 'test'})
        assert 'valid category' in response.body
        cra = Dataset.by_name('cra')
        assert cra.label != 'Common Rough Act', cra.label

    def test_core_update_invalid_label(self):
        response = self.app.post(url(controller='editor',
                                     action='core_update', dataset='cra'),
                                 params={'name': 'cra', 'label': '',
                                         'description': 'I\'m a banana',
                                         'currency': 'GBP'},
                                 extra_environ={'REMOTE_USER': 'test'})
        assert 'Required' in response.body
        cra = Dataset.by_name('cra')
        assert cra.label != '', cra.label

    def test_core_update_invalid_language(self):
        response = self.app.post(url(controller='editor',
                                     action='core_update', dataset='cra'),
                                 params={'name': 'cra', 'label': 'CRA',
                                         'languages': 'esperanto',
                                         'description': 'I\'m a banana',
                                         'currency': 'GBP',
                                         'default_time': 2009},
                                 extra_environ={'REMOTE_USER': 'test'})
        assert 'updated' not in response.body
        cra = Dataset.by_name('cra')
        assert 'esperanto' not in cra.languages

    def test_core_update_invalid_territory(self):
        response = self.app.post(url(controller='editor',
                                     action='core_update', dataset='cra'),
                                 params={'name': 'cra', 'label': 'CRA',
                                         'territories': 'su',
                                         'description': 'I\'m a banana',
                                         'currency': 'GBP',
                                         'default_time': 2009},
                                 extra_environ={'REMOTE_USER': 'test'})
        assert 'updated' not in response.body
        cra = Dataset.by_name('cra')
        assert 'su' not in cra.territories

    def test_core_update_invalid_currency(self):
        response = self.app.post(url(controller='editor',
                                     action='core_update', dataset='cra'),
                                 params={'name': 'cra',
                                         'label': 'Common Rough Act',
                                         'description': 'I\'m a banana',
                                         'currency': 'glass pearls',
                                         'default_time': 2009},
                                 extra_environ={'REMOTE_USER': 'test'})
        assert 'not a valid currency' in response.body
        cra = Dataset.by_name('cra')
        assert cra.currency == 'GBP', cra.label

    def test_dimensions_edit_mask(self):
        cra = Dataset.by_name('cra')
        cra.drop()
        cra.init()
        cra.generate()
        src = Source(cra, self.user, 'file:///dev/null')
        src.analysis = {'columns': ['amount', 'etc']}
        db.session.add(src)
        db.session.commit()
        response = self.app.get(url(controller='editor',
                                    action='dimensions_edit', dataset='cra'),
                                extra_environ={'REMOTE_USER': 'test'})
        assert '"amount"' in response.body
        assert 'Update' in response.body

    def test_dimensions_edit_mask_with_data(self):
        cra = Dataset.by_name('cra')
        src = Source(cra, self.user, 'file:///dev/null')
        src.analysis = {'columns': ['amount', 'etc']}
        db.session.add(src)
        db.session.commit()
        response = self.app.get(url(controller='editor',
                                    action='dimensions_edit', dataset='cra'),
                                extra_environ={'REMOTE_USER': 'test'})
        assert 'cannot edit dimensions' in response.body
        assert '"amount"' not in response.body
        assert 'Update' not in response.body

    def test_dimensions_update_invalid_json(self):
        cra = Dataset.by_name('cra')
        cra.drop()
        cra.init()
        cra.generate()
        response = self.app.post(url(controller='editor',
                                     action='dimensions_update',
                                     dataset='cra'),
                                 params={'mapping': 'banana'},
                                 extra_environ={'REMOTE_USER': 'test'},
                                 expect_errors=True)
        assert '400' in response.status, response.status

    def test_views_edit_mask(self):
        response = self.app.get(url(controller='editor',
                                    action='views_edit', dataset='cra'),
                                extra_environ={'REMOTE_USER': 'test'})
        assert '"default"' in response.body
        assert 'Update' in response.body

    def test_views_update(self):
        cra = Dataset.by_name('cra')
        views = cra.data['views']
        views[0]['label'] = 'Banana'
        response = self.app.post(url(controller='editor',
                                     action='views_update', dataset='cra'),
                                 params={'views': json.dumps(views)},
                                 extra_environ={'REMOTE_USER': 'test'},
                                 expect_errors=True)
        assert '200' in response.status, response.status
        cra = Dataset.by_name('cra')
        assert 'Banana' in repr(cra.data['views'])

    def test_views_update_invalid_json(self):
        response = self.app.post(url(controller='editor',
                                     action='views_update', dataset='cra'),
                                 params={'views': 'banana'},
                                 extra_environ={'REMOTE_USER': 'test'},
                                 expect_errors=True)
        assert '400' in response.status, response.status

    def test_team_edit_mask(self):
        response = self.app.get(url(controller='editor',
                                    action='team_edit', dataset='cra'),
                                extra_environ={'REMOTE_USER': 'test'})
        assert 'Add someone' in response.body
        assert 'Save' in response.body

    def test_team_update(self):
        response = self.app.post(url(controller='editor',
                                     action='team_update', dataset='cra'),
                                 params={},
                                 extra_environ={'REMOTE_USER': 'test'},
                                 expect_errors=True)
        assert '200' in response.status, response.status
        cra = Dataset.by_name('cra')
        assert len(cra.managers.all()) == 1, cra.managers

    def test_templates_edit_mask(self):
        response = self.app.get(url(controller='editor',
                                    action='templates_edit', dataset='cra'),
                                extra_environ={'REMOTE_USER': 'test'})
        assert 'Update' in response.body

    def test_templates_update(self):
        response = self.app.post(url(controller='editor',
                                     action='templates_update', dataset='cra'),
                                 params={'serp_title': 'BANANA'},
                                 extra_environ={'REMOTE_USER': 'test'},
                                 expect_errors=True)
        assert '200' in response.status, response.status
        cra = Dataset.by_name('cra')
        assert cra.serp_title == 'BANANA', cra.serp_title

    def test_drop(self):
        cra = Dataset.by_name('cra')
        assert len(cra) == 36, len(cra)
        # double-check authz
        response = self.app.post(url(controller='editor',
                                     action='drop', dataset='cra'),
                                 expect_errors=True)
        assert '403' in response.status
        cra = Dataset.by_name('cra')
        assert len(cra) == 36, len(cra)

        response = self.app.post(url(controller='editor',
                                     action='drop', dataset='cra'),
                                 extra_environ={'REMOTE_USER': 'test'})
        cra = Dataset.by_name('cra')
        assert len(cra) == 0, len(cra)

    def test_delete(self):
        cra = Dataset.by_name('cra')
        assert len(cra) == 36, len(cra)
        # double-check authz
        response = self.app.post(url(controller='editor',
                                     action='delete', dataset='cra'),
                                 expect_errors=True)
        assert '403' in response.status
        cra = Dataset.by_name('cra')
        assert len(cra) == 36, len(cra)

        response = self.app.post(url(controller='editor',
                                     action='delete', dataset='cra'),
                                 extra_environ={'REMOTE_USER': 'test'})
        cra = Dataset.by_name('cra')
        assert cra is None, cra

    def test_publish(self):
        cra = Dataset.by_name('cra')
        cra.private = True
        db.session.commit()
        response = self.app.post(url(controller='editor',
                                     action='publish', dataset='cra'),
                                 extra_environ={'REMOTE_USER': 'test'})
        cra = Dataset.by_name('cra')
        assert cra.private is False, cra.private
        response = self.app.post(url(controller='editor',
                                     action='publish', dataset='cra'),
                                 extra_environ={'REMOTE_USER': 'test'},
                                 expect_errors=True)
        assert '400' in response.status, response.status

    def test_retract(self):
        cra = Dataset.by_name('cra')
        assert cra.private is False, cra.private
        response = self.app.post(url(controller='editor',
                                     action='retract', dataset='cra'),
                                 extra_environ={'REMOTE_USER': 'test'})
        cra = Dataset.by_name('cra')
        assert cra.private is True, cra.private
        response = self.app.post(url(controller='editor',
                                     action='retract', dataset='cra'),
                                 extra_environ={'REMOTE_USER': 'test'},
                                 expect_errors=True)
        assert '400' in response.status, response.status

########NEW FILE########
__FILENAME__ = test_entry
from openspending.model import meta as db
from openspending.model.dataset import Dataset

from openspending.tests.base import ControllerTestCase
from openspending.tests.helpers import load_fixture
from pylons import url


class TestEntryController(ControllerTestCase):

    def setup(self):
        super(TestEntryController, self).setup()
        load_fixture('cra')
        self.cra = Dataset.by_name('cra')

    def test_view(self):
        t = list(self.cra.entries(limit=1)).pop()
        response = self.app.get(url(controller='entry', action='view',
                                    dataset='cra', id=t['id']))

        assert 'cra' in response

    def test_inflated_view(self):
        """
        Test whether a view of an entry can be inflated. This is done by
        adding a url parameter inflate containing the target year
        for inflation.

        This test has hard coded values based on existing inflation data used
        by an external module. This may therefore need updating should the
        inflation data become more accurate with better data.
        """

        # There is only a single entry with this amount so we can safely
        # pop it and know that it is for the year 2010 but we assert it
        # to be absolutely sure
        t = list(self.cra.entries('amount=-22400000')).pop()
        assert t['time']['year'] == '2010', \
            "Test entry isn't from the year 2010"

        # Get an inflated response
        response = self.app.get(url(controller='entry', action='view',
                                    dataset='cra', id=t['id'],
                                    inflate='2011'))
        assert '200' in response.status, \
            "Inflated entry isn't successful (status code isn't 200)"

        # Check for inflation adjustments
        assert 'Adjusted for inflation' in response.body, \
            "Entry is not adjusted for inflation"
        assert 'Original amount: -22,400,000' in response.body, \
            "Original amount not in inflated entry response"
        assert '-23,404,469' in response.body, \
            "Inflated amount is not in inflated entry response"

        # Try a non-working inflation (bad year)
        response = self.app.get(url(controller='entry', action='view',
                                    dataset='cra', id=t['id'],
                                    inflate='1000'))
        assert '200' in response.status, \
            "Inflated entry (bad year) unsuccessful (status code isn't 200)"
        assert 'Unable to adjust for inflation' in response.body, \
            "Inflation warning not present in inflated entry response (bad)"

    # TODO: 2013-11-17 reinstate
    # disabled as we disabled custom html stuff as part of genshi removal
    def _test_entry_custom_html(self):
        tpl = '<a href="/custom/path/%s">%s</a>'
        tpl_c = tpl % ('${entry["id"]}', '${entry["name"]}')
        self.cra.entry_custom_html = tpl_c
        db.session.commit()

        t = list(self.cra.entries(limit=1)).pop()

        response = self.app.get(url(controller='entry', action='view',
                                    dataset=self.cra.name,
                                    id=t['id']))

        assert tpl % (t['id'], t['name']) in response, \
            'Custom HTML not present in rendered page!'

########NEW FILE########
__FILENAME__ = test_error
from openspending.tests.base import ControllerTestCase

from pylons import url


class TestErrorController(ControllerTestCase):

    def test_403(self):
        response = self.app.get(
            url(controller='error_test', action='not_authorised'),
            status=403
        )
        assert "Access was denied" in response, \
            "'Access was denied' not in response that should give a 403!"

        assert "OpenSpending" in response, \
            "'OpenSpending' not in 403 page! Is this not a custom 403?"

        assert "Custom 403 error message" in response, \
            "Custom error message was not passed through to 403 error page."

    def test_404(self):
        response = self.app.get(
            url(controller='error_test', action='not_found'),
            status=404
        )
        assert "Error 404" in response, \
            "'Not Found' not in response to request that should give a 404!"

        assert "OpenSpending" in response, \
            "'OpenSpending' not in 404 page! Is this not a custom 404?"

        assert "Custom 404 error message" in response, \
            "Custom error message was not passed through to 404 error page."

    def test_500(self):
        # NB: This test will fail if the tests are run in debug mode. Add the
        # following line to your test.ini under [app:main]:
        #
        #   set debug = False

        response = self.app.get(
            url(controller='error_test', action='server_error'),
            status=500
        )
        assert "Server Error" in response, \
            "'Server Error' not in response to request that should give a 500!"

        assert "OpenSpending" in response, \
            "'OpenSpending' not in 500 page! Is this not a custom 500?"

        assert "Custom 500 error message" in response, \
            "Custom error message was not passed through to 500 error page."

########NEW FILE########
__FILENAME__ = test_home
from openspending.tests.base import ControllerTestCase

from pylons import url


class TestHomeController(ControllerTestCase):

    def test_index(self):
        response = self.app.get(url(controller='home', action='index'))
        assert 'OpenSpending' in response

    def test_locale(self):
        self.app.post(url(controller='home', action='set_locale'))

########NEW FILE########
__FILENAME__ = test_rest
from openspending.model.dataset import Dataset
from openspending.tests.base import ControllerTestCase
from openspending.tests.helpers import load_fixture

from pylons import url


class TestRestController(ControllerTestCase):

    def setup(self):
        super(TestRestController, self).setup()
        load_fixture('cra')
        self.cra = Dataset.by_name('cra')

    def test_index(self):
        response = self.app.get(url(controller='rest', action='index'))
        for word in ['/cra', 'entries']:
            assert word in response, response

    def test_dataset(self):
        response = self.app.get(url(controller='dataset',
                                    action='view',
                                    format='json',
                                    dataset=self.cra.name))

        assert '"name": "cra"' in response, response

    def test_entry(self):
        q = self.cra['from'].alias.c.name == 'Dept047'
        example = list(self.cra.entries(q, limit=1)).pop()

        response = self.app.get(url(controller='entry',
                                    action='view',
                                    dataset=self.cra.name,
                                    format='json',
                                    id=str(example['id'])))

        assert '"id":' in response, response
        assert '"cofog1":' in response, response
        assert '"from":' in response, response
        assert '"Dept047"' in response, response

########NEW FILE########
__FILENAME__ = test_run
from openspending.tests.base import ControllerTestCase
from openspending.tests.importer.test_csv import csvimport_fixture
from openspending.model.account import Account
from openspending.importer import CSVImporter
from openspending.ui.lib.helpers import readable_url

from pylons import url


class TestRunController(ControllerTestCase):

    def setup(self):

        super(TestRunController, self).setup()
        self.source = csvimport_fixture('import_errors')
        self.source.dataset.managers.append(Account.by_name('test'))
        self.importer = CSVImporter(self.source)
        self.importer.run()

    def test_view_run(self):
        response = self.app.get(url(controller='run',
                                    action='view',
                                    dataset=self.source.dataset.name,
                                    source=self.source.id,
                                    id=self.importer._run.id),
                                extra_environ={'REMOTE_USER': 'test'},
                                expect_errors=True)
        assert readable_url(self.source.url).encode('utf-8') in response.body

    def test_view_run_does_not_exist(self):
        response = self.app.get(url(controller='run',
                                    action='view',
                                    dataset=self.source.dataset.name,
                                    source=self.source.id,
                                    id=47347893),
                                extra_environ={'REMOTE_USER': 'test'},
                                expect_errors=True)
        assert '404' in response.status, response.status

########NEW FILE########
__FILENAME__ = test_source
from openspending.tests.base import ControllerTestCase
from openspending.tests.helpers import make_account, load_fixture
from openspending.tests.importer.test_csv import csvimport_fixture

from openspending.model import meta as db
from openspending.model.source import Source
from openspending.model.account import Account
from openspending.importer import CSVImporter

from pylons import url


class TestSourceController(ControllerTestCase):

    def setup(self):

        super(TestSourceController, self).setup()
        self.user = make_account('test')
        self.dataset = load_fixture('cra', self.user)

    def test_view_source(self):
        url_ = 'http://banana.com/split.csv'
        source = Source(self.dataset, self.user, url_)
        db.session.add(source)
        db.session.commit()
        response = self.app.get(url(controller='source',
                                    action='view', dataset='cra',
                                    id=source.id),
                                extra_environ={'REMOTE_USER': 'test'})
        assert response.headers['Location'] == url_, response.headers

    def test_view_source_does_not_exist(self):
        response = self.app.get(url(controller='source',
                                    action='view', dataset='cra', id=47347893),
                                extra_environ={'REMOTE_USER': 'test'},
                                expect_errors=True)
        assert '404' in response.status, response.status

    def test_new_source(self):
        response = self.app.get(url(controller='source',
                                    action='new', dataset='cra'),
                                extra_environ={'REMOTE_USER': 'test'})
        assert 'Create a data source' in response.body

    def test_create_source(self):
        url_ = 'http://banana.com/split.csv'
        response = self.app.post(url(controller='source',
                                     action='create', dataset='cra'),
                                 params={'url': url_},
                                 extra_environ={'REMOTE_USER': 'test'})

        response = self.app.get(url(controller='editor',
                                    action='index', dataset='cra'),
                                extra_environ={'REMOTE_USER': 'test'})
        assert url_ in response.body, response.body

    def test_create_source_invalid_url(self):
        url_ = 'banana'
        response = self.app.post(url(controller='source',
                                     action='create', dataset='cra'),
                                 params={'url': url_},
                                 extra_environ={'REMOTE_USER': 'test'})
        assert 'HTTP/HTTPS' in response.body

        response = self.app.get(url(controller='editor',
                                    action='index', dataset='cra'),
                                extra_environ={'REMOTE_USER': 'test'})
        assert url_ not in response.body, response.body

    def test_delete_source(self):
        """
        Test source removal with a source that includes errors
        """

        # Add and import source with errors (we want to remove it)
        # The source is added to a dataset called 'test-csv' (but
        # we'll just use source.dataset.name in case it changes)
        source = csvimport_fixture('import_errors')
        source.dataset.managers.append(Account.by_name('test'))
        importer = CSVImporter(source)
        importer.run()

        # Make sure the source is imported
        assert db.session.query(Source).filter_by(id=source.id).count() == 1, \
            "Import of csv failed. Source not found"

        # Delete the source
        self.app.post(url(controller='source',
                          action='delete',
                          dataset=source.dataset.name,
                          id=source.id),
                      extra_environ={'REMOTE_USER': 'test'})

        # Check if source has been deleted
        assert db.session.query(Source).filter_by(id=source.id).count() == 0, \
            "Deleting source unsuccessful. Source still exists."

    def test_delete_successfully_loaded_source(self):
        """
        Test source removal with a source that has been successfully loaded.
        Removing a source that has been successfully loaded should not be
        possible.
        """

        # Add and import source without errors.
        # The source is added to a dataset called 'test-csv' (but
        # we'll just use source.dataset.name in case it changes)
        source = csvimport_fixture('successful_import')
        source.dataset.managers.append(Account.by_name('test'))
        importer = CSVImporter(source)
        importer.run()

        # Make sure the source is imported
        assert db.session.query(Source).filter_by(id=source.id).count() == 1, \
            "Import of csv failed. Source not found"

        # Delete the source
        self.app.post(url(controller='source',
                          action='delete',
                          dataset=source.dataset.name,
                          id=source.id),
                      extra_environ={'REMOTE_USER': 'test'})

        # Check if source has been deleted
        assert db.session.query(Source).filter_by(id=source.id).count() == 1, \
            "Deleting source succeeded. The source is gone."

########NEW FILE########
__FILENAME__ = test_view
import json

from openspending.tests.base import ControllerTestCase
from openspending.tests.helpers import make_account, load_fixture

from openspending.model.dataset import Dataset
from openspending.model.view import View


from pylons import url


class TestViewController(ControllerTestCase):

    def setup(self):
        super(TestViewController, self).setup()
        self.user = make_account('test')
        load_fixture('cra', self.user)

    def test_index(self):
        response = self.app.get(url(controller='view',
                                    action='index', dataset='cra'),
                                extra_environ={'REMOTE_USER': 'test'})
        assert 'Library of visualisations' in response.body

    def test_delete(self):
        # TODO: Create the view using a fixture
        self.app.post(url(controller='view', action='create',
                          dataset='cra'),
                      params={'widget': 'treemap',
                              'label': 'I am a banana!',
                              'state': '{"foo":"banana"}'},
                      extra_environ={'REMOTE_USER': 'test'})
        response = self.app.delete(url(controller='view',
                                       action='delete', dataset='cra',
                                       name='i-am-a-banana'),
                                   extra_environ={'REMOTE_USER': 'test'})
        dataset = Dataset.by_name('cra')
        view = View.by_name(dataset, 'i-am-a-banana')
        assert view is None
        assert '302' in response.status

    def test_delete_by_unauthorized_user(self):
        # TODO: Create the view using a fixture
        self.app.post(url(controller='view', action='create',
                          dataset='cra'),
                      params={'widget': 'treemap',
                              'label': 'I am a banana!',
                              'state': '{"foo":"banana"}'},
                      extra_environ={'REMOTE_USER': 'test'})
        response = self.app.delete(
            url(controller='view',
                action='delete', dataset='cra',
                name='i-am-a-banana'),
            expect_errors=True,
            extra_environ={
                'REMOTE_USER': 'unauthorized_user'})

        dataset = Dataset.by_name('cra')
        view = View.by_name(dataset, 'i-am-a-banana')
        assert view is not None
        assert '403' in response.status

    def test_new(self):
        response = self.app.get(url(controller='view',
                                    action='new', dataset='cra'),
                                extra_environ={'REMOTE_USER': 'test'})
        assert 'widgets.js' in response.body

    def test_create_noauth(self):
        response = self.app.post(url(controller='view', action='create',
                                     dataset='cra'),
                                 params={'widget': 'treemap',
                                         'label': 'I am a banana!',
                                         'state': '{"foo":"banana"}'},
                                 expect_errors=True)
        assert '403' in response.status, response.status

    def test_create(self):
        response = self.app.post(url(controller='view', action='create',
                                     dataset='cra'),
                                 params={'widget': 'treemap',
                                         'label': 'I am a banana!',
                                         'state': '{"foo":"banana"}'},
                                 extra_environ={'REMOTE_USER': 'test'})
        assert '302' in response.status, response.status
        assert '/cra/views/i-am-a-banana' \
            in response.headers.get('location'), response.headers

        response = self.app.get(url(controller='view', action='view',
                                    dataset='cra', name='i-am-a-banana',
                                    format='json'))
        data = json.loads(response.body)
        assert data['widget'] == 'treemap', data

        response = self.app.get(url(controller='view', action='view',
                                    dataset='cra', name='i-am-a-banana'))
        assert 'title>I am a banana!' in response.body, response

    def test_update(self):
        """
        Test the update function of a view.
        """
        # Create the view (we do it via a controller but it would be
        # better to create it manually (or via a fixture)
        self.app.post(url(controller='view', action='create',
                          dataset='cra'),
                      params={'widget': 'treemap',
                              'label': 'I am a banana!',
                              'state': '{"foo":"banana"}'},
                      extra_environ={'REMOTE_USER': 'test'})

        # Check whether a non-user can update the view
        response = self.app.post(url(controller='view', action='update',
                                     dataset='cra', name='i-am-a-banana'),
                                 params={'label': 'I am an apple',
                                         'state': '{"foo":"apple"}',
                                         'description': 'An apple!'},
                                 expect_errors=True)
        # The user should receive a 403 Forbidden (actually should get 401)
        assert '403' in response.status, \
            "A non-user was able to update a view"

        dataset = Dataset.by_name('cra')
        view = View.by_name(dataset, 'i-am-a-banana')
        assert view.label == 'I am a banana!', \
            "View's label was changed by a non-user"
        assert view.state['foo'] == 'banana', \
            "View's state was changed by a non-user"
        assert view.description is None, \
            "View's description was changed by a non-user"

        # Check whether an unauthorized user can update the view
        response = self.app.post(url(controller='view', action='update',
                                     dataset='cra', name='i-am-a-banana'),
                                 params={'label': 'I am an apple',
                                         'state': '{"foo":"apple"}',
                                         'description': 'An apple!'},
                                 expect_errors=True,
                                 extra_environ={'REMOTE_USER': 'anotheruser'})
        # The user should receive a 403 (Forbidden)
        assert '403' in response.status, \
            "Unauthorized user was able to update a view"

        dataset = Dataset.by_name('cra')
        view = View.by_name(dataset, 'i-am-a-banana')
        assert view.label == 'I am a banana!', \
            "View's label was changed by an unauthorized user"
        assert view.state['foo'] == 'banana', \
            "View's state was changed by an unauthorized user"
        assert view.description is None, \
            "View's description was changed by an unauthorized user"

        # Check whether a managing user can update the view
        response = self.app.post(url(controller='view', action='update',
                                     dataset='cra', name='i-am-a-banana'),
                                 params={'label': 'I am an apple',
                                         'name': 'can-i-be-an-apple',
                                         'state': '{"foo":"apple"}',
                                         'description': 'An apple!'},
                                 extra_environ={'REMOTE_USER': 'test'})

        dataset = Dataset.by_name('cra')
        view = View.by_name(dataset, 'i-am-a-banana')
        # Name cannot have been changed because the view might have been
        # embedded elsewhere (cannot be changed by params nor be re-slugified)
        assert view is not None, \
            "View's name was changed by update"
        assert view.label == 'I am an apple', \
            "View's label wasn't changed by the managing user"
        assert view.state['foo'] == 'apple', \
            "View's state wasn't changed by the managing user"
        assert view.description == 'An apple!', \
            "View's description wasn't changed by the managing user"

    def test_embed(self):
        response = self.app.get(url(controller='view', action='embed',
                                    dataset='cra'),
                                params={'widget': 'treemap'})
        assert u"Embedded" in response.body, response.body
        response = self.app.get(url(controller='view', action='embed',
                                    dataset='cra'),
                                expect_errors=True)
        assert "400" in response.status, response.status

    def test_embed_state(self):
        response = self.app.get(url(controller='view', action='embed',
                                    dataset='cra'),
                                params={'widget': 'treemap',
                                        'state': '{"foo":"banana"}'})
        assert u"banana" in response.body, response.body
        response = self.app.get(url(controller='view', action='embed',
                                    dataset='cra'),
                                params={'widget': 'treemap',
                                        'state': '{"foo:"banana"}'},
                                expect_errors=True)
        assert "400" in response.status, response.status

########NEW FILE########
__FILENAME__ = helpers
from openspending.validation.data import convert_types
from openspending.model.dataset import Dataset
from openspending.model import meta as db
from openspending.lib import solr_util as solr

from datetime import datetime
import os.path
import json
import csv


def fixture_file(name):
    """Return a file-like object pointing to a named fixture."""
    return open(fixture_path(name))


def model_fixture(name):
    model_fp = fixture_file('model/' + name + '.json')
    model = json.load(model_fp)
    model_fp.close()
    return model


def data_fixture(name):
    return fixture_file('data/' + name + '.csv')


def fixture_path(name):
    """
    Return the full path to a named fixture.

    Use fixture_file rather than this method wherever possible.
    """
    # Get the directory of this file (helpers is placed in the test directory)
    test_directory = os.path.dirname(__file__)
    # Fixture is a directory in the test directory
    return os.path.join(test_directory, 'fixtures', name)


def load_fixture(name, manager=None):
    """
    Load fixture data into the database.
    """
    model = model_fixture(name)
    dataset = Dataset(model)
    dataset.updated_at = datetime.utcnow()
    if manager is not None:
        dataset.managers.append(manager)
    db.session.add(dataset)
    db.session.commit()
    dataset.generate()
    data = data_fixture(name)
    reader = csv.DictReader(data)
    for row in reader:
        entry = convert_types(model['mapping'], row)
        dataset.load(entry)
    data.close()
    dataset.commit()
    return dataset


def load_dataset(dataset):
    simple_model = model_fixture('simple')
    data = data_fixture('simple')
    reader = csv.DictReader(data)
    for row in reader:
        row = convert_types(simple_model['mapping'], row)
        dataset.load(row)
    data.close()


def make_account(name='test', fullname='Test User',
                 email='test@example.com', twitter='testuser',
                 admin=False):
    from openspending.model.account import Account

    # First see if the account already exists and if so, return it
    account = Account.by_name(name)
    if account:
        return account

    # Account didn't exist so we create it and return it
    account = Account()
    account.name = name
    account.fullname = fullname
    account.email = email
    account.twitter_handle = twitter
    account.admin = admin
    db.session.add(account)
    db.session.commit()

    return account


def clean_db():
    db.session.rollback()
    db.metadata.drop_all()


def clean_solr():
    '''Clean all entries from Solr.'''
    s = solr.get_connection()
    s.delete_query('*:*')
    s.commit()


def clean_and_reindex_solr():
    '''Clean Solr and reindex all entries in the database.'''
    clean_solr()
    for dataset in db.session.query(Dataset):
        solr.build_index(dataset.name)

########NEW FILE########
__FILENAME__ = test_csv
from openspending.model.dataset import Dataset
from openspending.model.source import Source
from openspending.model import meta as db
from openspending.lib import json

from openspending.importer import CSVImporter

from openspending.tests.base import DatabaseTestCase
from openspending.tests.helpers import fixture_path, make_account


def csvimport_fixture_path(name, path):
    return fixture_path('csv_import/%s/%s' % (name, path))


def csvimport_fixture_file(name, path):
    try:
        fp = open(csvimport_fixture_path(name, path))
    except IOError:
        if name == 'default':
            fp = None
        else:
            fp = csvimport_fixture_file('default', path)

    return fp


def csvimport_fixture(name):
    model_fp = csvimport_fixture_file(name, 'model.json')
    mapping_fp = csvimport_fixture_file(name, 'mapping.json')
    model = json.load(model_fp)
    if mapping_fp:
        model['mapping'] = json.load(mapping_fp)
    dataset = Dataset(model)
    dataset.generate()
    db.session.add(dataset)
    data_path = csvimport_fixture_path(name, 'data.csv')
    user = make_account()
    source = Source(dataset, user, data_path)
    db.session.add(source)
    db.session.commit()
    return source


class TestCSVImporter(DatabaseTestCase):

    def test_successful_import(self):
        source = csvimport_fixture('successful_import')
        importer = CSVImporter(source)
        importer.run()
        dataset = db.session.query(Dataset).first()

        assert dataset is not None, "Dataset should not be None"
        assert dataset.name == "test-csv"

        entries = dataset.entries()
        assert len(list(entries)) == 4

        # TODO: provenance
        entry = list(dataset.entries(limit=1, offset=1)).pop()
        assert entry is not None, "Entry with name could not be found"
        assert entry['amount'] == 66097.77

    def test_no_dimensions_for_measures(self):
        source = csvimport_fixture('simple')
        importer = CSVImporter(source)
        importer.run()
        dataset = db.session.query(Dataset).first()

        dimensions = [str(d.name) for d in dataset.dimensions]
        assert sorted(dimensions) == ['entry_id', 'from', 'time', 'to']

    def test_successful_import_with_simple_testdata(self):
        source = csvimport_fixture('simple')
        importer = CSVImporter(source)
        importer.run()
        assert importer.errors == 0

        dataset = db.session.query(Dataset).first()
        assert dataset is not None, "Dataset should not be None"

        entries = list(dataset.entries())
        assert len(entries) == 5

        entry = entries[0]
        assert entry['from']['label'] == 'Test From'
        assert entry['to']['label'] == 'Test To'
        assert entry['time']['name'] == '2010-01-01'
        assert entry['amount'] == 100.00

    def test_import_errors(self):
        source = csvimport_fixture('import_errors')

        importer = CSVImporter(source)
        importer.run(dry_run=True)
        assert importer.errors > 1, "Should have errors"

        records = list(importer._run.records)
        assert records[0].row == 1, \
            "Should detect missing date colum in line 1"

    def test_empty_csv(self):
        source = csvimport_fixture('default')
        source.url = 'file:///dev/null'
        importer = CSVImporter(source)
        importer.run(dry_run=True)

        assert importer.errors == 2

        records = list(importer._run.records)
        assert records[0].row == 0
        assert records[1].row == 0
        assert "Didn't read any lines of data" in str(records[1].message)

    def test_malformed_csv(self):
        source = csvimport_fixture('malformed')
        importer = CSVImporter(source)
        importer.run(dry_run=True)
        assert importer.errors == 1

    def test_erroneous_values(self):
        source = csvimport_fixture('erroneous_values')
        importer = CSVImporter(source)
        importer.run(dry_run=True)

        # Expected failures:
        # * unique key constraint not met (2x)
        # * amount cannot be parsed
        # * time cannot be parse
        assert importer.errors == 4

        records = list(importer._run.records)
        # The fourth record should be about badly formed date
        assert "time" in records[3].attribute, \
            "Should find badly formatted date"

        # The row number of the badly formed date should be 5
        assert records[3].row == 5

    def test_error_with_empty_additional_date(self):
        source = csvimport_fixture('empty_additional_date')
        importer = CSVImporter(source)
        importer.run()
        assert importer.errors == 1

    def test_quoting(self):
        source = csvimport_fixture('quoting')
        importer = CSVImporter(source)
        importer.run()
        assert importer.errors == 0


class TestCSVImportDatasets(DatabaseTestCase):

    def count_lines_in_stream(self, f):
        try:
            return len(f.read().splitlines())
        finally:
            f.seek(0)

    def _test_import(self, name):
        source = csvimport_fixture(name)
        data = open(source.url)
        lines = self.count_lines_in_stream(data) - 1  # -1 for header row

        importer = CSVImporter(source)
        importer.run()

        assert importer.errors == 0

        # check correct number of entries
        dataset = db.session.query(Dataset).first()
        entries = list(dataset.entries())
        assert len(entries) == lines

    def test_all_imports(self):
        for dir in ('lbhf', 'mexico', 'sample', 'uganda'):
            yield self._test_import, dir

########NEW FILE########
__FILENAME__ = test_browser
from openspending.tests.base import TestCase

from openspending.lib.browser import Browser

import json
from mock import Mock, patch


def make_entries(ids):
    return [{'id': id, 'dataset': 'mock_dataset'} for id in ids]


def make_response(ids):
    entries = make_entries(ids)
    return json.dumps({
        'response': {
            'numFound': 1234,
            'docs': entries
        }
    })


class TestBrowser(TestCase):

    def setup(self):
        super(TestBrowser, self).setup()

        self.conn = Mock()
        self.dataset = Mock()
        self.dataset.name = 'mock_dataset'

        self.solr_patcher = patch('openspending.lib.browser.solr')
        mock_solr = self.solr_patcher.start()
        mock_solr.get_connection.return_value = self.conn

        self.conn.raw_query.return_value = make_response([])

        self.dataset_patcher = patch('openspending.lib.browser.Dataset')
        mock_dataset = self.dataset_patcher.start()
        mock_dataset.by_name.return_value = self.dataset

    def teardown(self):
        self.solr_patcher.stop()
        self.dataset_patcher.stop()

    def test_defaults(self):
        b = Browser()
        assert b.params['q'] == ''
        assert b.params['page'] == 1
        assert b.params['pagesize'] == 100
        assert b.params['filter'] == {}
        assert b.params['facet_field'] == []

    def test_simple_query(self):
        b = Browser()
        b.execute()

        ignore, solr_args = self.conn.raw_query.call_args

        assert solr_args['q'] == '*:*'
        assert solr_args['fq'] == []
        assert solr_args['wt'] == 'json'
        assert solr_args['fl'] == 'id, dataset'
        assert solr_args['sort'] == 'score desc, amount desc'
        assert solr_args['start'] == 0
        assert solr_args['rows'] == 100

    def test_entries_order(self):
        self.conn.raw_query.return_value = make_response([1, 2, 3])
        self.dataset.entries.return_value = make_entries([3, 1, 2])

        b = Browser()
        b.execute()
        entries = b.get_entries()

        assert map(lambda a_b: a_b[1], entries) == make_entries([1, 2, 3])

    def test_entries_stats(self):
        self.conn.raw_query.return_value = make_response([1, 2, 3])
        self.dataset.entries.return_value = make_entries([3, 1, 2])

        b = Browser()
        b.execute()
        stats = b.get_stats()

        assert stats['results_count'] == 3
        assert stats['results_count_query'] == 1234

    def test_filter(self):
        f = {'foo': 'bar', 'baz': 'with "quotes"'}
        b = Browser(filter=f)
        b.execute()

        ignore, solr_args = self.conn.raw_query.call_args
        assert '+foo:"bar"' in solr_args['fq']
        assert '+baz:"with \\"quotes\\""' in solr_args['fq']

    def test_filter_union(self):
        f = {'foo': ['bar', 'baz']}
        b = Browser(filter=f)
        b.execute()

        ignore, solr_args = self.conn.raw_query.call_args
        assert '+foo:"bar" OR +foo:"baz"' in solr_args['fq']

    def test_page_pagesize(self):
        b = Browser(page=2, pagesize=50)
        b.execute()

        ignore, solr_args = self.conn.raw_query.call_args
        assert solr_args['start'] == 50
        assert solr_args['rows'] == 50

    def test_fractional_page_pagesize(self):
        b = Browser(page=2.5, pagesize=50)
        b.execute()

        ignore, solr_args = self.conn.raw_query.call_args
        assert solr_args['start'] == 75
        assert solr_args['rows'] == 50

    def test_facets(self):
        b = Browser(facet_field=['foo', 'bar'])
        b.execute()

        ignore, solr_args = self.conn.raw_query.call_args
        assert solr_args['facet'] == 'true'
        assert solr_args['facet.mincount'] == 1
        assert solr_args['facet.sort'] == 'count'
        assert solr_args['facet.field'] == ['foo', 'bar']

    def test_facets_page_pagesize(self):
        b = Browser(facet_field=['one'], facet_page=2, facet_pagesize=50)
        b.execute()

        ignore, solr_args = self.conn.raw_query.call_args
        assert solr_args['facet.offset'] == 50
        assert solr_args['facet.limit'] == 50

    def test_order(self):
        b = Browser(order=[('amount', False), ('something.id', True)])
        b.execute()

        ignore, solr_args = self.conn.raw_query.call_args
        assert solr_args['sort'] == 'amount asc, something.id desc'

########NEW FILE########
__FILENAME__ = test_calculator
from openspending.lib.calculator import TaxCalculator2010

calculator = TaxCalculator2010()


def test_income_tax():
    # Check tax is never more than income.
    def test(income):
        tax, explanation = calculator.total_tax(income)
        tax = tax['tax']
        assert abs(income - tax) >= 0.0, (tax, income, explanation)
        assert (text != '' for text in explanation), (tax, income, explanation)

    # Low incomes.
    yield test, 0
    yield test, 5225

    # High incomes.
    yield test, 2264285.71
    yield test, 10e6

    # Mid-ranking incomes, requiring interpolation.
    yield test, 25837.04

########NEW FILE########
__FILENAME__ = test_helpers
from openspending.tests.base import TestCase
from openspending.ui.lib import helpers


class TestFormatNumber(TestCase):

    def _check(self, ourmethod, testsets):
        for inp, res in testsets:
            out = ourmethod(inp)
            assert out == res, (out, res)

    def test_01_positive(self):
        testsets = [
            [200, '200'],
            [2000, '2,000'],
            [2000000, '2,000,000'],
        ]
        self._check(helpers.format_number_with_commas, testsets)

    def test_02_negative(self):
        testsets = [
            [-200, '-200'],
            [-2000, '-2,000'],
            [-2000000, '-2,000,000'],
        ]
        self._check(helpers.format_number_with_commas, testsets)

    def test_03_format_number(self):
        testsets = [
            [200, '200'],
            [2000, '2.0k'],
            [200000, '200.0k'],
            [2109400, '2.11m'],
            [-2103400, '-2.1m'],
            [2109400000, '2.11b'],
        ]
        self._check(helpers.format_number, testsets)

########NEW FILE########
__FILENAME__ = test_paramparser
from openspending.lib.paramparser import (ParamParser, AggregateParamParser,
                                          SearchParamParser)

from openspending.tests.base import TestCase
from mock import Mock, patch


class TestParamParser(TestCase):

    def test_defaults(self):
        out, err = ParamParser({}).parse()
        assert out['page'] == 1
        assert out['pagesize'] == 10000
        assert out['order'] == []

    def test_page(self):
        out, err = ParamParser({'page': 'foo'}).parse()
        assert len(err) == 1
        assert '"page" has to be a number' in err[0]

    def test_page_fractional(self):
        out, err = ParamParser({'page': '1.2'}).parse()
        assert out['page'] == 1.2

        out, err = ParamParser({'page': '0.2'}).parse()
        assert out['page'] == 1

    def test_pagesize(self):
        out, err = ParamParser({'pagesize': 'foo'}).parse()
        assert len(err) == 1
        assert '"pagesize" has to be an integer' in err[0]

    def test_order(self):
        out, err = ParamParser({'order': 'foo:asc|amount:desc'}).parse()
        assert out['order'] == [('foo', False), ('amount', True)]

        out, err = ParamParser({'order': 'foo'}).parse()
        assert 'Wrong format for "order"' in err[0]

        out, err = ParamParser({'order': 'foo:boop'}).parse()
        assert 'Order direction can be "asc" or "desc"' in err[0]


class TestAggregateParamParser(TestCase):

    def test_defaults(self):
        out, err = AggregateParamParser({}).parse()
        assert out['page'] == 1
        assert out['pagesize'] == 10000
        assert err[0] == 'dataset name not provided'

    @patch('openspending.lib.paramparser.Dataset')
    def test_dataset(self, model_mock):
        ds = Mock()
        ds.measures = []
        model_mock.by_name.return_value = ds

        out, err = AggregateParamParser({'dataset': 'foo'}).parse()
        assert out['dataset'] == ds

    def test_drilldown(self):
        out, err = AggregateParamParser({'drilldown': 'foo|bar|baz'}).parse()
        assert out['drilldown'] == ['foo', 'bar', 'baz']

    def test_format(self):
        out, err = AggregateParamParser({'format': 'json'}).parse()
        assert out['format'] == 'json'

        out, err = AggregateParamParser({'format': 'csv'}).parse()
        assert out['format'] == 'csv'

        out, err = AggregateParamParser({'format': 'html'}).parse()
        assert out['format'] == 'json'

    @patch('openspending.lib.paramparser.Dataset')
    def test_cut(self, model_mock):
        ds = Mock()
        ds.measures = []
        model_mock.by_name.return_value = ds

        out, err = AggregateParamParser(
            {'dataset': 'foo', 'cut': 'foo:one|bar:two'}).parse()
        assert out['cut'] == [('foo', 'one'), ('bar', 'two')]

        out, err = AggregateParamParser(
            {'dataset': 'foo', 'cut': 'foo:one|bar'}).parse()
        assert 'Wrong format for "cut"' in err[0]

    @patch('openspending.lib.paramparser.Dataset')
    def test_measure(self, model_mock):
        ds = Mock()
        amt = Mock()
        amt.name = 'amount'
        bar = Mock()
        bar.name = 'bar'
        ds.measures = [amt, bar]
        model_mock.by_name.return_value = ds

        out, err = AggregateParamParser({'dataset': 'foo'}).parse()
        assert out['measure'] == ['amount']

        out, err = AggregateParamParser(
            {'dataset': 'foo', 'measure': 'bar'}).parse()
        assert out['measure'] == ['bar']

        out, err = AggregateParamParser(
            {'dataset': 'foo', 'measure': 'amount|bar'}).parse()
        assert 'amount' in out['measure'], \
            "AggregateParamParser doesn't return amount measure"
        assert 'bar' in out['measure'], \
            "AggregateParamParser doesn't return bar measure"

        out, err = AggregateParamParser(
            {'dataset': 'foo', 'measure': 'baz'}).parse()
        assert 'no measure with name "baz"' in err[0]


class TestSearchParamParser(TestCase):

    def test_filter(self):
        out, err = SearchParamParser({'filter': 'foo:one|bar:two'}).parse()
        assert out['filter'] == {'foo': 'one', 'bar': 'two'}

        out, err = SearchParamParser({'filter': 'foo:one|bar'}).parse()
        assert 'Wrong format for "filter"' in err[0]

    @patch('openspending.lib.paramparser.Dataset')
    def test_dataset(self, model_mock):
        def _mock_dataset(name):
            if name == 'baz':
                return None
            ds = Mock()
            ds.name = name
            return ds

        model_mock.by_name.side_effect = _mock_dataset

        out, err = SearchParamParser({'dataset': 'foo|bar'}).parse()
        assert [x.name for x in out['dataset']] == ['foo', 'bar']

        out, err = SearchParamParser({'dataset': 'baz'}).parse()
        assert 'no dataset with name "baz"' in err[0]

    # cf test_facet_pagesize
    def test_pagesize(self):
        out, err = SearchParamParser({'pagesize': '73'}).parse()
        assert out['pagesize'] == 73

        out, err = SearchParamParser({'pagesize': '140'}).parse()
        assert out['pagesize'] == 140

    def test_facet_pagesize(self):
        out, err = SearchParamParser({'facet_pagesize': '73'}).parse()
        assert out['facet_pagesize'] == 73

        out, err = SearchParamParser({'facet_pagesize': '140'}).parse()
        assert out['facet_pagesize'] == 100

    def test_facet_field(self):
        out, err = SearchParamParser({'facet_field': 'foo|bar|baz'}).parse()
        assert out['facet_field'] == ['foo', 'bar', 'baz']

    def test_category(self):
        out, err = SearchParamParser({'category': 'banana'}).parse()
        assert 'category' not in out

        out, err = SearchParamParser({'category': 'spending'}).parse()
        assert out['category'] == 'spending'

    def test_facet_page(self):
        out, err = SearchParamParser({'facet_page': '14'}).parse()
        assert out['facet_page'] == 14

    def test_facet_page_fractional(self):
        out, err = SearchParamParser({'facet_page': '1.7'}).parse()
        assert out['facet_page'] == 1.7

        out, err = SearchParamParser({'facet_page': '0.6'}).parse()
        assert out['facet_page'] == 1

    def test_expand_facet_dimensions(self):
        # Expand facet dimensions should default to False
        out, err = SearchParamParser({}).parse()
        assert not out['expand_facet_dimensions']

        # If expand_facet_dimension param is provided we should return True
        out, err = SearchParamParser({'expand_facet_dimensions': ''}).parse()
        assert out['expand_facet_dimensions']

########NEW FILE########
__FILENAME__ = test_solr_util
# coding: UTF-8
from datetime import datetime
from openspending.lib import solr_util as solr

from openspending.tests.base import TestCase
from nose.tools import assert_raises
from mock import Mock, patch


class TestSolrUtil(TestCase):

    def setup(self):
        super(TestSolrUtil, self).setup()
        reload(solr)
        self.patcher = patch('openspending.lib.solr_util.SolrConnection')
        self.mock_solr = self.patcher.start()

    def teardown(self):
        self.patcher.stop()
        super(TestSolrUtil, self).teardown()

    def test_configure_defaults(self):
        solr.configure()

        assert solr.url == 'http://localhost:8983/solr'
        assert solr.http_user is None
        assert solr.http_pass is None

    def test_configure(self):
        config = Mock()
        config.get.side_effect = ['myurl', 'myuser', 'mypass']
        solr.configure(config)

        assert solr.url == 'myurl'
        assert solr.http_user == 'myuser'
        assert solr.http_pass == 'mypass'

    def test_get_connection(self):
        conn = solr.get_connection()
        conn = solr.get_connection()
        self.mock_solr.assert_called_once_with(
            'http://localhost:8983/solr',
            http_user=None,
            http_pass=None)
        assert conn == self.mock_solr.return_value

    def test_drop_index(self):
        solr.drop_index('foo')
        self.mock_solr.return_value.delete_query.assert_called_once_with(
            'dataset:foo')
        self.mock_solr.return_value.commit.assert_called_once()

    def test_dataset_entries(self):
        self.mock_solr.return_value.raw_query.return_value = \
            '{"response":{"numFound":42}}'
        assert solr.dataset_entries('foo') == 42

    def test_extend_entry(self):
        dataset = Mock()
        dataset.id = 123
        dataset.name = 'mydataset'

        now = datetime.now()

        entry = {
            'id': 456,
            'time': now,
            'foo.name': 'uber',
            'foo.label': 'UberLabel',
            'foo.tags': ['one', 'two', 'three']
        }

        expected = {
            '_id': 'mydataset::456',
            'id': 456,
            'time': datetime(now.year, now.month, now.day, now.hour,
                             now.minute, now.second, 0, solr.UTC()),
            'dataset.id': 123,
            'dataset': 'mydataset',
            'foo.name': u'uber',
            'foo': u'uber',
            'foo.label': 'UberLabel',
            'foo.label_facet': 'UberLabel',
            'foo.tags': 'one two three'
        }

        res = solr.extend_entry(entry, dataset)
        assert res == expected

    @patch('openspending.lib.solr_util.Dataset')
    def test_build_index_no_dataset(self, mock_ds):
        mock_ds.by_name.return_value = None
        assert_raises(ValueError, solr.build_index, 'foobar')

    @patch('openspending.lib.solr_util.Dataset')
    @patch('openspending.lib.solr_util.extend_entry')
    def test_build_index(self, mock_ee, mock_ds):
        ds = mock_ds.by_name.return_value
        ds.entries.return_value = [{'foo': 123}, {'foo': 456}, {'foo': 789}]

        mock_ee.side_effect = lambda e, d: e['foo']

        solr.build_index('mydataset')
        conn = self.mock_solr.return_value
        conn.add_many.assert_called_once_with([123, 456, 789])
        conn.commit.assert_called_once()

    @patch('openspending.lib.solr_util.Dataset')
    @patch('openspending.lib.solr_util.extend_entry')
    def test_build_index_batch(self, mock_ee, mock_ds):
        ds = mock_ds.by_name.return_value
        ds.entries.return_value = [{'foo': 'bar'}] * 2500

        solr.build_index('mydataset')
        conn = self.mock_solr.return_value
        assert conn.add_many.call_count == 3
        assert conn.commit.call_count == 3

########NEW FILE########
__FILENAME__ = test_util
# -*- coding: utf-8 -*-
from openspending.lib import util
from openspending.tests.base import TestCase


class TestUtils(TestCase):

    def test_slugify(self):
        assert util.slugify(u'foo') == 'foo'
        assert util.slugify(u'fóo') == 'foo'
        assert util.slugify(u'fóo&bañ') == 'foo-ban'

    def test_hash_values(self):
        hash_value = util.hash_values([u'fóo&bañ'])
        assert hash_value == 'a2a4c050e75206e5fe84dbb7fe525c5dde8c848d'

    def test_sort_by_reference(self):
        ids = [4, 7, 1, 3]
        objs = [{'id': 1}, {'id': 7}, {'id': 4}, {'id': 3}]

        sorted_objs = util.sort_by_reference(ids, objs, lambda x: x['id'])
        assert sorted_objs == [{'id': 4}, {'id': 7}, {'id': 1}, {'id': 3}]

########NEW FILE########
__FILENAME__ = test_views
from openspending.tests.base import DatabaseTestCase
from openspending.tests.helpers import load_fixture
from nose.tools import assert_raises

from openspending.model.dataset import Dataset
from openspending.ui.lib.views import View


class TestViews(DatabaseTestCase):

    def setup(self):
        super(TestViews, self).setup()
        load_fixture('cra')
        self.dataset = Dataset.by_name('cra')

    def get_view(self, name='default', **kwargs):
        return View(self.dataset, kwargs)

    def test_by_name(self):
        assert len(self.dataset.data['views']) == 4

        default = View.by_name(self.dataset, self.dataset, 'default')
        assert default is not None, default
        assert default.entity == 'dataset', default.entity
        assert default.name == 'default', default.name
        assert View.by_name(self.dataset, self.dataset, 'region')

        cf = {'taxonomy': 'cofog'}
        cfa = View.by_name(self.dataset, cf, 'default', 'cofog1')
        assert cfa, cfa
        assert cfa.dimension == 'cofog1', cfa.dimension

        assert_raises(ValueError, View.by_name, self.dataset,
                      cfa, 'not-there')

########NEW FILE########
__FILENAME__ = test_dataset
from sqlalchemy import Integer, UnicodeText, Float, Unicode
from nose.tools import assert_raises

from openspending.tests.helpers import model_fixture, load_dataset
from openspending.tests.base import DatabaseTestCase

from openspending.model import meta as db
from openspending.model.dataset import Dataset
from openspending.model.dimension import (AttributeDimension, Measure,
                                          CompoundDimension, DateDimension)


class TestDataset(DatabaseTestCase):

    def setup(self):
        super(TestDataset, self).setup()
        self.model = model_fixture('simple')
        self.ds = Dataset(self.model)

    def test_load_model_properties(self):
        assert self.ds.name == self.model['dataset']['name'], self.ds.name
        assert self.ds.label == self.model['dataset']['label'], self.ds.label

    def test_load_model_dimensions(self):
        assert len(self.ds.dimensions) == 4, self.ds.dimensions
        assert isinstance(self.ds['time'], DateDimension), self.ds['time']
        assert isinstance(
            self.ds['field'], AttributeDimension), self.ds['field']
        assert isinstance(self.ds['to'], CompoundDimension), self.ds['to']
        assert isinstance(self.ds['function'], CompoundDimension), \
            self.ds['function']
        assert len(self.ds.measures) == 1, self.ds.measures
        assert isinstance(self.ds['amount'], Measure), self.ds['amount']

    def test_value_dimensions_as_attributes(self):
        dim = self.ds['field']
        assert isinstance(dim.column.type, UnicodeText), dim.column
        assert 'field' == dim.column.name, dim.column
        assert dim.name == 'field', dim.name
        assert dim.source_column == self.model['mapping']['field']['column'],\
            dim.source_column
        assert dim.label == self.model['mapping']['field']['label'], \
            dim.label
        assert dim.constant is None, dim.constant
        assert dim.default_value is None, dim.default_value
        assert dim.constant is None, dim.constant
        assert dim.dataset == self.ds, dim.dataset
        assert dim.datatype == 'string', dim.datatype
        assert not hasattr(dim, 'table')
        assert not hasattr(dim, 'alias')

    def test_generate_db_entry_table(self):
        assert self.ds.table.name == 'test__entry', self.ds.table.name
        cols = self.ds.table.c
        assert 'id' in cols
        assert isinstance(cols['id'].type, Unicode)

        assert 'time_id' in cols
        assert isinstance(cols['time_id'].type, Integer)
        assert 'amount' in cols
        assert isinstance(cols['amount'].type, Float)
        assert 'field' in cols
        assert isinstance(cols['field'].type, UnicodeText)
        assert 'to_id' in cols
        assert isinstance(cols['to_id'].type, Integer)
        assert 'function_id' in cols
        assert isinstance(cols['function_id'].type, Integer)
        assert_raises(KeyError, cols.__getitem__, 'foo')

    def test_facet_dimensions(self):
        assert [d.name for d in self.ds.facet_dimensions] == ['to']


class TestDatasetLoad(DatabaseTestCase):

    def setup(self):
        super(TestDatasetLoad, self).setup()
        self.ds = Dataset(model_fixture('simple'))
        self.ds.generate()
        self.engine = db.engine

    def test_load_all(self):
        load_dataset(self.ds)
        resn = self.engine.execute(self.ds.table.select()).fetchall()
        assert len(resn) == 6, resn
        row0 = resn[0]
        assert row0['amount'] == 200, row0.items()
        assert row0['field'] == 'foo', row0.items()

    def test_flush(self):
        load_dataset(self.ds)
        resn = self.engine.execute(self.ds.table.select()).fetchall()
        assert len(resn) == 6, resn
        self.ds.flush()
        resn = self.engine.execute(self.ds.table.select()).fetchall()
        assert len(resn) == 0, resn

    def test_drop(self):
        tn = self.engine.table_names()
        assert 'test__entry' in tn, tn
        assert 'test__to' in tn, tn
        assert 'test__function' in tn, tn

        self.ds.drop()
        tn = self.engine.table_names()
        assert 'test__entry' not in tn, tn
        assert 'test__to' not in tn, tn
        assert 'test__function' not in tn, tn

    def test_dataset_count(self):
        load_dataset(self.ds)
        assert len(self.ds) == 6, len(self.ds)

    def test_aggregate_simple(self):
        load_dataset(self.ds)
        res = self.ds.aggregate()
        assert res['summary']['num_entries'] == 6, res
        assert res['summary']['amount'] == 2690.0, res

    def test_aggregate_basic_cut(self):
        load_dataset(self.ds)
        res = self.ds.aggregate(cuts=[('field', u'foo')])
        assert res['summary']['num_entries'] == 3, res
        assert res['summary']['amount'] == 1000, res

    def test_aggregate_or_cut(self):
        load_dataset(self.ds)
        res = self.ds.aggregate(cuts=[('field', u'foo'),
                                      ('field', u'bar')])
        assert res['summary']['num_entries'] == 4, res
        assert res['summary']['amount'] == 1190, res

    def test_aggregate_dimensions_drilldown(self):
        load_dataset(self.ds)
        res = self.ds.aggregate(drilldowns=['function'])
        assert res['summary']['num_entries'] == 6, res
        assert res['summary']['amount'] == 2690, res
        assert len(res['drilldown']) == 2, res['drilldown']

    def test_aggregate_two_dimensions_drilldown(self):
        load_dataset(self.ds)
        res = self.ds.aggregate(drilldowns=['function', 'field'])
        assert res['summary']['num_entries'] == 6, res
        assert res['summary']['amount'] == 2690, res
        assert len(res['drilldown']) == 5, res['drilldown']

    def test_aggregate_by_attribute(self):
        load_dataset(self.ds)
        res = self.ds.aggregate(drilldowns=['function.label'])
        assert len(res['drilldown']) == 2, res['drilldown']

    def test_aggregate_two_attributes_same_dimension(self):
        load_dataset(self.ds)
        res = self.ds.aggregate(drilldowns=['function.name', 'function.label'])
        assert len(res['drilldown']) == 2, res['drilldown']

    def test_materialize_table(self):
        load_dataset(self.ds)
        itr = self.ds.entries()
        tbl = list(itr)
        assert len(tbl) == 6, len(tbl)
        row = tbl[0]
        assert isinstance(row['field'], unicode), row
        assert isinstance(row['function'], dict), row
        assert isinstance(row['to'], dict), row

########NEW FILE########
__FILENAME__ = test_dimension
from nose.tools import assert_raises

from openspending.tests.helpers import model_fixture, load_fixture
from openspending.tests.base import DatabaseTestCase

from openspending.model import meta as db
from openspending.model.dataset import Dataset


class TestAttributeDimension(DatabaseTestCase):

    def setup(self):
        super(TestAttributeDimension, self).setup()
        self.engine = db.engine
        self.meta = db.metadata
        self.meta.bind = self.engine
        self.ds = Dataset(model_fixture('simple'))
        self.field = self.ds['field']

    def test_is_compound(self):
        assert not self.field.is_compound


class TestCompoundDimension(DatabaseTestCase):

    def setup(self):
        super(TestCompoundDimension, self).setup()
        self.engine = db.engine
        self.meta = db.metadata
        self.meta.bind = self.engine
        self.ds = load_fixture('cra')
        self.entity = self.ds['from']
        self.classifier = self.ds['cofog1']

    def test_is_compound(self):
        assert self.entity.is_compound

    def test_basic_properties(self):
        assert self.entity.name == 'from', self.entity.name
        assert self.classifier.name == 'cofog1', self.classifier.name

    def test_generated_tables(self):
        assert hasattr(self.entity, 'table'), self.entity
        assert self.entity.table.name == 'cra__' + \
            self.entity.taxonomy, self.entity.table.name
        assert hasattr(self.entity, 'alias')
        assert self.entity.alias.name == self.entity.name, \
            self.entity.alias.name
        cols = self.entity.table.c
        assert 'id' in cols
        assert_raises(KeyError, cols.__getitem__, 'field')

    def test_attributes_exist_on_object(self):
        assert len(self.entity.attributes) == 3, self.entity.attributes
        assert_raises(KeyError, self.entity.__getitem__, 'field')
        assert self.entity['name'].name == 'name'
        assert self.entity['name'].datatype == 'string'

    def test_attributes_exist_on_table(self):
        assert hasattr(self.entity, 'table'), self.entity
        assert 'name' in self.entity.table.c, self.entity.table.c
        assert 'label' in self.entity.table.c, self.entity.table.c

    def test_members(self):
        members = list(self.entity.members())
        assert len(members) == 5

        members = list(
            self.entity.members(
                self.entity.alias.c.name == 'Dept032'))
        assert len(members) == 1

########NEW FILE########
__FILENAME__ = templating
import os
from pylons import tmpl_context as c
from pylons import app_globals
from pylons import config
from pylons import i18n
from webhelpers import paginate

from openspending import auth as can
from openspending.ui.lib import helpers as h

from jinja2 import FileSystemLoader
from jinja2.environment import Environment
import formencode_jinja2

import lxml.html
from lxml.html import builder as E

# Set the directory where this file is as the template root directory
template_rootdir = os.path.abspath(os.path.dirname(__file__))


class Page(paginate.Page):
    # Overwrite the pager method of the webhelpers.paginate.Page class,
    # so we have our custom layout set as default.

    def pager(self, *args, **kwargs):
        kwargs.update(
            format="<div class='pager'>$link_previous ~2~ $link_next</div>",
            symbol_previous=u'\xab Prev', symbol_next=u'Next \xbb'
        )
        return super(Page, self).pager(*args, **kwargs)


def languages(detected_languages, current_language):
    def lang_triple(lang):
        return {
            "lang_code": lang[0],
            "lang_name": lang[1],
            "current_locale": {
                True: "current_locale",
                False: ""
            }[current_language == lang[0]]
        }
    return [lang_triple(l) for l in detected_languages]


def section_active(section):
    sections = ["blog", "dataset", "search", "resources", "help", "about"]
    tmp = dict([(s, section == s)for s in sections])
    tmp["dataset"] = bool(c.dataset)

    return dict([(k, {
                True: "active",
                False: ""
                }[v]) for k, v in tmp.iteritems()])


def postprocess_forms(s, form_errors):
    def tag_errors(tag, root):
        for i in root.cssselect(tag):
            name = i.attrib.get('name', None)
            value = form_errors.get(name, None)
            if value is not None:
                p = E.P(value)
                p.set('class', 'help-block error')
                i.addnext(p)

    def input_errors(root):
        return tag_errors('input', root)

    def select_errors(root):
        return tag_errors('select', root)

    def textarea_errors(root):
        return tag_errors('textarea', root)

    root = lxml.html.fromstring(s)
    processors = [input_errors, select_errors, textarea_errors]
    [process(root) for process in processors]
    return lxml.html.tostring(root, doctype=root.getroottree().docinfo.doctype)


def render(path, **kwargs):
    """Render a template with jinja2

    Args:
      path (str): the path to the template; should be of the form
      "dir/filename.html"

    """

    env = Environment(loader=FileSystemLoader(template_rootdir),
                      extensions=[formencode_jinja2.formfill,
                                  'jinja2.ext.i18n'])
    env.install_gettext_translations(i18n)

    template = env.get_template(path)

    static_cache_version = config.get("openspending.static_cache_version", "")
    if static_cache_version != "":
        static_cache_version = "?" + static_cache_version

    params = {
        "script_root": h.script_root(),
        "script_boot": h.script_tag('prod/boot'),
        "bootstrap_css": h.static('style/bootstrap.css'),
        "style_css": h.static('style/style.css'),
        "number_symbols_group": c.locale.number_symbols.get('group'),
        "number_symbols_decimal": c.locale.number_symbols.get('decimal'),
        "site_title": app_globals.site_title,
        "static": config.get("openspending.static_path", "/static/"),
        "static_cache_version": static_cache_version,
        "messages": list(h._flash.pop_messages()),
        "languages": languages(c.detected_l10n_languages, c.language),
        "section_active": section_active(c.content_section),
        "account": c.account is not None,
        "h": h,
        "c": c,
        "g": app_globals,
        "can": can,
        "show_rss": hasattr(c, 'show_rss') and c.show_rss or None
    }
    params.update(kwargs)
    form_errors = params.get('form_errors', {})
    return postprocess_forms(template.render(params), form_errors)

########NEW FILE########
__FILENAME__ = environment
"""Pylons environment configuration"""
import logging
import os

from pylons import config

from sqlalchemy import engine_from_config
from migrate.versioning.util import construct_engine

from webhelpers import markdown

from openspending.model import init_model

from openspending.ui.config import routing
from openspending.ui.lib import app_globals
from openspending.ui.lib import helpers


def load_environment(global_conf, app_conf):
    """\
    Configure the Pylons environment via the ``pylons.config`` object
    """

    # Pylons paths
    root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    paths = dict(root=root,
                 controllers=os.path.join(root, 'controllers'),
                 static_files=os.path.join(root, 'public'),
                 templates=[os.path.join(root, 'templates')])

    # Initialize config with the basic options
    config.init_app(
        global_conf,
        app_conf,
        package='openspending.ui',
        paths=paths)

    config['routes.map'] = routing.make_map()
    config['pylons.app_globals'] = app_globals.Globals()
    config['pylons.h'] = helpers

    # set log level in markdown
    markdown.logger.setLevel(logging.WARN)

    # SQLAlchemy
    engine = engine_from_config(config, 'openspending.db.')
    engine = construct_engine(engine)
    init_model(engine)

    # Configure Solr
    import openspending.lib.solr_util as solr
    solr.configure(config)

########NEW FILE########
__FILENAME__ = middleware
"""Pylons middleware initialization"""
import logging
import sys

from beaker.middleware import CacheMiddleware, SessionMiddleware
from paste.cascade import Cascade
from paste.registry import RegistryManager
from paste.urlparser import StaticURLParser
from paste.deploy.converters import asbool

from pylons import config
from pylons.middleware import ErrorHandler, StatusCodeRedirect
from pylons.wsgiapp import PylonsApp
from routes.middleware import RoutesMiddleware

from repoze.who.middleware import PluggableAuthenticationMiddleware
from repoze.who.plugins.basicauth import BasicAuthPlugin
from repoze.who.plugins.auth_tkt import AuthTktCookiePlugin
from repoze.who.classifiers import (default_request_classifier,
                                    default_challenge_decider)
from repoze.who.plugins.friendlyform import FriendlyFormPlugin

from openspending.ui.config.environment import load_environment
from openspending.ui.lib.authenticator import (UsernamePasswordAuthenticator,
                                               ApiKeyIdentifier,
                                               ApiKeyAuthenticator)


def make_app(global_conf, full_stack=True, static_files=True, **app_conf):
    """Create a Pylons WSGI application and return it

    ``global_conf``
        The inherited configuration for this application. Normally from
        the [DEFAULT] section of the Paste ini file.

    ``full_stack``
        Whether this application provides a full WSGI stack (by default,
        meaning it handles its own exceptions and errors). Disable
        full_stack when this application is "managed" by another WSGI
        middleware.

    ``static_files``
        Whether this application serves its own static files; disable
        when another web server is responsible for serving them.

    ``app_conf``
        The application's local configuration. Normally specified in
        the [app:<name>] section of the Paste ini file (where <name>
        defaults to main).

    """
    # Configure the Pylons environment
    load_environment(global_conf, app_conf)

    # The Pylons WSGI app
    app = PylonsApp()

    # Routing/Session/Cache Middleware
    app = RoutesMiddleware(app, config['routes.map'])
    app = SessionMiddleware(app, config)
    app = CacheMiddleware(app, config)

    # CUSTOM MIDDLEWARE HERE (filtered by error handling middlewares)
    basicauth = BasicAuthPlugin('OpenSpending')
    auth_tkt = AuthTktCookiePlugin(
        'RANDOM_KEY_THAT_ONLY_LOOKS_LIKE_A_PLACEHOLDER',
        cookie_name='openspending_login', timeout=86400 * 90,
        reissue_time=3600)

    form = FriendlyFormPlugin(
        '/login',
        '/perform_login',
        '/after_login',
        '/logout',
        '/after_logout',
        rememberer_name='auth_tkt')
    identifiers = [('auth_tkt', auth_tkt),
                   ('basicauth', basicauth),
                   ('form', form),
                   ('apikey', ApiKeyIdentifier())]
    authenticators = [('auth_tkt', auth_tkt),
                      ('username', UsernamePasswordAuthenticator()),
                      ('apikey', ApiKeyAuthenticator())]
    challengers = [('form', form),
                   ('basicauth', basicauth)]
    log_stream = sys.stdout
    app = PluggableAuthenticationMiddleware(
        app, identifiers, authenticators, challengers, [],
        default_request_classifier,
        default_challenge_decider,
        log_stream=log_stream,
        log_level=logging.WARNING
    )

    if asbool(full_stack):
        # Handle Python exceptions
        app = ErrorHandler(app, global_conf, **config['pylons.errorware'])

        # Display error documents for 401, 403, 404 status codes (and
        # 500 when debug is disabled)
        if asbool(config['debug']):
            app = StatusCodeRedirect(app)
        else:
            app = StatusCodeRedirect(app, [400, 401, 403, 404, 500])

    # Establish the Registry for this application
    app = RegistryManager(app)

    if asbool(static_files):
        max_age = None if asbool(config['debug']) else 3600

        # Serve static files
        static_app = StaticURLParser(
            config['pylons.paths']['static_files'],
            cache_max_age=max_age
        )
        static_parsers = [static_app, app]
        app = Cascade(static_parsers)

    return app

########NEW FILE########
__FILENAME__ = routing
"""Routes configuration

The more specific and detailed routes should be defined first so they
may take precedent over the more generic routes. For more information
refer to the routes manual at http://routes.groovie.org/docs/
"""
from pylons import config
from routes import Mapper


def make_map():
    """Create, configure and return the routes Mapper"""
    map = Mapper(directory=config['pylons.paths']['controllers'],
                 always_scan=config['debug'], explicit=True)
    map.minimization = False

    # The ErrorController route (handles 404/500 error pages); it should
    # likely stay at the top, ensuring it can always be resolved
    map.connect('/error/{action}', controller='error')
    map.connect('/error/{action}/{id}', controller='error')
    # The ErrorTestController is used to test our custom error pages.
    map.connect('/_error_test/{action}', controller='error_test')

    # CUSTOM ROUTES HERE
    map.connect('/', controller='home', action='index')

    map.connect(
        '/set-locale',
        controller='home',
        action='set_locale',
        conditions=dict(
            method=['POST']))
    map.connect('/favicon.ico', controller='home', action='favicon')

    map.connect('/login', controller='account', action='login')
    map.connect('/register', controller='account', action='register')
    map.connect('/settings', controller='account', action='settings')
    map.connect('/dashboard', controller='account', action='dashboard')
    map.connect(
        '/accounts/scoreboard',
        controller='account',
        action='scoreboard')
    map.connect('/accounts/_complete', controller='account', action='complete')
    map.connect('/after_login', controller='account', action='after_login')
    map.connect('/after_logout', controller='account', action='after_logout')
    map.connect(
        '/account/forgotten',
        controller='account',
        action='trigger_reset')
    map.connect('/account/reset', controller='account', action='do_reset')
    map.connect('/account/{name}', controller='account', action='profile')

    map.connect('/badges', controller='badge', action='index')
    map.connect('/badges.{format}', controller='badge', action='index')
    map.connect('/badges/create', controller='badge', action='create',
                conditions=dict(method=['POST']))
    map.connect(
        '/badge/{id}.{format}',
        controller='badge',
        action='information')
    map.connect('/badge/{id}', controller='badge', action='information')

    map.connect('/datasets.rss', controller='dataset', action='feed_rss')
    map.connect('/datasets.{format}', controller='dataset', action='index')
    map.connect('/datasets/cta', controller='dataset', action='cta')
    map.connect('/datasets/territories', controller='dataset',
                action='territories')
    map.connect('/datasets/new', controller='dataset', action='new')
    map.connect('/datasets', controller='dataset', action='create',
                conditions=dict(method=['POST']))
    map.connect('/datasets', controller='dataset', action='index')

    map.connect('/search', controller='entry', action='search')

    map.connect('/api', controller='api/version1', action='index')
    map.connect('/api/search', controller='api/version1', action='search')
    map.connect(
        '/api/aggregate',
        controller='api/version1',
        action='aggregate')
    map.connect('/api/mytax', controller='api/version1', action='mytax')

    map.connect('/api/rest/', controller='rest', action='index')
    map.connect(
        '/api/2/aggregate',
        controller='api/version2',
        action='aggregate')
    map.connect('/api/2/search', controller='api/version2', action='search')
    map.connect(
        '/api/2/new',
        controller='api/version2',
        action='create',
        conditions=dict(
            method=['POST']))
    map.connect(
        '/api/2/permissions',
        controller='api/version2',
        action='permissions')

    map.connect('/500', controller='error', action='render', code="500")

    map.connect('/__version__', controller='home', action='version')
    map.connect('/__ping__', controller='home', action='ping')

    map.connect('/{dataset}.{format}', controller='dataset', action='view')
    map.connect('/{dataset}', controller='dataset', action='view')
    map.connect('/{dataset}/explorer', controller='dataset', action='explorer')
    map.connect(
        '/{dataset}/model.{format}',
        controller='dataset',
        action='model')
    map.connect('/{dataset}/model', controller='dataset', action='model')
    map.connect('/{dataset}/meta', controller='dataset', action='about')
    map.connect('/{dataset}/timeline', controller='dataset', action='timeline')

    map.connect('/{dataset}/views/new', controller='view', action='new')
    map.connect('/{dataset}/views', controller='view', action='create',
                conditions=dict(method=['POST']))
    map.connect('/{dataset}/views.{format}', controller='view', action='index')
    map.connect('/{dataset}/views', controller='view', action='index')
    map.connect(
        '/{dataset}/views/{name}.{format}',
        controller='view',
        action='view')
    map.connect('/{dataset}/views/{name}', controller='view', action='update',
                conditions=dict(method=['POST']))
    map.connect('/{dataset}/views/{name}', controller='view', action='delete',
                conditions=dict(method=['DELETE']))
    map.connect('/{dataset}/views/{name}', controller='view', action='view')
    map.connect('/{dataset}/embed', controller='view', action='embed')

    map.connect('/{dataset}/editor', controller='editor', action='index')
    map.connect('/{dataset}/editor/core', controller='editor',
                action='core_update', conditions=dict(method=['POST']))
    map.connect(
        '/{dataset}/editor/core',
        controller='editor',
        action='core_edit')
    map.connect('/{dataset}/editor/dimensions', controller='editor',
                action='dimensions_update', conditions=dict(method=['POST']))
    map.connect('/{dataset}/editor/dimensions', controller='editor',
                action='dimensions_edit')
    map.connect('/{dataset}/editor/dimensions_src', controller='editor',
                action='dimensions_edit', mode='source')
    map.connect('/{dataset}/editor/views', controller='editor',
                action='views_update', conditions=dict(method=['POST']))
    map.connect('/{dataset}/editor/views', controller='editor',
                action='views_edit')
    map.connect('/{dataset}/editor/team', controller='editor',
                action='team_update', conditions=dict(method=['POST']))
    map.connect('/{dataset}/editor/team', controller='editor',
                action='team_edit')
    map.connect('/{dataset}/editor/templates', controller='editor',
                action='templates_update', conditions=dict(method=['POST']))
    map.connect('/{dataset}/editor/templates', controller='editor',
                action='templates_edit')
    map.connect('/{dataset}/editor/publish', controller='editor',
                action='publish', conditions=dict(method=['POST']))
    map.connect('/{dataset}/editor/retract', controller='editor',
                action='retract', conditions=dict(method=['POST']))
    map.connect('/{dataset}/editor/drop', controller='editor',
                action='drop', conditions=dict(method=['POST']))
    map.connect('/{dataset}/editor/delete', controller='editor',
                action='delete', conditions=dict(method=['POST']))

    map.connect('/{dataset}/sources', controller='source',
                action='create', conditions=dict(method=['POST']))
    map.connect('/{dataset}/sources.{format}', controller='source',
                action='index')
    map.connect('/{dataset}/sources/new', controller='source', action='new')
    map.connect('/{dataset}/sources/{id}', controller='source', action='view')
    map.connect('/{dataset}/sources/{id}/load', controller='source',
                action='load', conditions=dict(method=['POST']))
    map.connect('/{dataset}/sources/{id}/delete', controller='source',
                action='delete', conditions=dict(method=['POST']))
    map.connect('/{dataset}/sources/{source}/runs/{id}',
                controller='run', action='view')
    map.connect('/{dataset}/sources/{source}/analysis.{format}',
                controller='source', action='analysis')

    map.connect(
        '/{dataset}/entries.{format}',
        controller='entry',
        action='index')
    map.connect('/{dataset}/entries', controller='entry', action='index')
    map.connect(
        '/{dataset}/entries/{id}.{format}',
        controller='entry',
        action='view')
    map.connect('/{dataset}/entries/{id}', controller='entry', action='view')
    map.connect('/{dataset}/entries/{id}/{action}', controller='entry')

    map.connect('/{dataset}/give', controller='badge', action='give',
                conditions=dict(method=['POST']))

    map.connect('/{dataset}/dimensions.{format}',
                controller='dimension', action='index')
    map.connect('/{dataset}/dimensions',
                controller='dimension', action='index')

    map.connect('/{dataset}/{dimension}.distinct.json',
                controller='dimension', action='distinct', format='json')
    map.connect('/{dataset}/{dimension}.distinct',
                controller='dimension', action='distinct')

    map.connect('/{dataset}/{dimension}.json',
                controller='dimension', action='view', format='json')
    map.connect('/{dataset}/{dimension}',
                controller='dimension', action='view')

    map.connect('/{dataset}/{dimension}/{name}.json',
                controller='dimension', action='member', format='json')
    map.connect('/{dataset}/{dimension}/{name}.csv',
                controller='dimension', action='member', format='csv')
    map.connect('/{dataset}/{dimension}/{name}',
                controller='dimension', action='member')

    map.connect('/{dataset}/{dimension}/{name}/entries.{format}',
                controller='dimension', action='entries')
    map.connect('/{dataset}/{dimension}/{name}/entries',
                controller='dimension', action='entries')

    map.redirect('/*(url)/', '/{url}', _redirect_code='301 Moved Permanently')
    return map

########NEW FILE########
__FILENAME__ = account
from pylons import config

import logging

import colander

from pylons import request, response, tmpl_context as c
from pylons.controllers.util import abort, redirect
from pylons.i18n import _

from repoze.who.api import get_api

from openspending.model import meta as db
from openspending.model.dataset import Dataset
from openspending.model.account import (Account, AccountRegister,
                                        AccountSettings)
from openspending.lib.paramparser import DistinctParamParser
from openspending.ui.lib import helpers as h
from openspending.ui.lib.base import BaseController, require
from openspending.ui.lib.security import generate_password_hash
from openspending.ui.lib.mailman import subscribe_lists
from openspending.lib.jsonexport import to_jsonp
from openspending.lib.mailer import send_reset_link
from openspending.ui.alttemplates import templating
from sqlalchemy.sql.expression import desc, func, or_
log = logging.getLogger(__name__)


class AccountController(BaseController):

    def login(self):
        """
        Render the login page (which is also the registration page)
        """

        # Disable cache
        self._disable_cache()

        # Add config (so we can offer users to subscribe to mailing lists)
        c.config = config

        # Return the rendered template
        return templating.render('account/login.html')

    def register(self):
        """
        Perform registration of a new user
        """

        # We must allow account creation
        require.account.create()

        # We add the config as a context variable in case anything happens
        # (like with login we need this for subscriptions to mailing lists)
        c.config = config

        # Disable the cache (don't want anything getting in the way)
        self._disable_cache()

        # Initial values and errors
        errors, values = {}, None

        # If this is a POST operation somebody is trying to register
        if request.method == 'POST':
            try:
                # Get the account register schema (for validation)
                schema = AccountRegister()

                # Set values from the request parameters
                # (for validation and so we can autofill forms)
                values = request.params

                # Grab the actual data and validate it
                data = schema.deserialize(values)

                # Check if the username already exists, return an error if so
                if Account.by_name(data['name']):
                    raise colander.Invalid(
                        AccountRegister.name,
                        _("Login name already exists, please choose a "
                          "different one"))

                # Check if passwords match, return error if not
                if not data['password1'] == data['password2']:
                    raise colander.Invalid(AccountRegister.password1,
                                           _("Passwords don't match!"))

                # Create the account
                account = Account()

                # Set username and full name
                account.name = data['name']
                account.fullname = data['fullname']

                # Set email and if email address should be public
                account.email = data['email']
                account.public_email = data['public_email']

                # Hash the password and store the hash
                account.password = generate_password_hash(data['password1'])

                # Commit the new user account to the database
                db.session.add(account)
                db.session.commit()

                # Perform a login for the user
                who_api = get_api(request.environ)
                authenticated, headers = who_api.login({
                    "login": account.name,
                    "password": data['password1']
                })
                # Add the login headers
                response.headers.extend(headers)

                # Subscribe the user to the mailing lists
                errors = subscribe_lists(('community', 'developer'), data)
                # Notify if the mailing list subscriptions failed
                if errors:
                    h.flash_notice(
                        _("Subscription to the following mailing " +
                          "lists probably failed: %s.") % ', '.join(errors))

                # Registration successful - Redirect to the front page
                return redirect("/")
            except colander.Invalid as i:
                # Mark colander errors
                errors = i.asdict()

        # Show the templates (with possible errors and form values)
        return templating.render('account/login.html', form_fill=values,
                                 form_errors=errors)

    def settings(self):
        """
        Change settings for the logged in user
        """

        # The logged in user must be able to update the account
        require.account.update(c.account)

        # Disable the cache
        self._disable_cache()

        # Initial values and errors
        errors, values = {}, c.account

        # If POST the user is trying to update the settings
        if request.method == 'POST':
            try:
                # Get the account settings schema (for validation)
                schema = AccountSettings()

                # Set values from the request parameters
                # (for validation and so we can autofill forms)
                values = request.params

                # Grab the actual data and validate it
                data = schema.deserialize(values)

                # If the passwords don't match we notify the user
                if not data['password1'] == data['password2']:
                    raise colander.Invalid(AccountSettings.password1,
                                           _("Passwords don't match!"))

                # Update full name
                c.account.fullname = data['fullname']

                # Update the script root
                c.account.script_root = data['script_root']

                # Update email and whether email should be public
                c.account.email = data['email']
                c.account.public_email = data['public_email']

                # If twitter handle is provided we update it
                # (and if it should be public)
                if data['twitter'] is not None:
                    c.account.twitter_handle = data['twitter'].lstrip('@')
                    c.account.public_twitter = data['public_twitter']

                # If a new password was provided we update it as well
                if data['password1'] is not None and len(data['password1']):
                    c.account.password = generate_password_hash(
                        data['password1'])

                # Do the actual update in the database
                db.session.add(c.account)
                db.session.commit()

                # Let the user know we've updated successfully
                h.flash_success(_("Your settings have been updated."))
            except colander.Invalid as i:
                # Load errors if we get here
                errors = i.asdict()
        else:
            # Get the account values to autofill the form
            values = c.account.as_dict()

            # We need to put public checks separately because they're not
            # a part of the dictionary representation of the account
            if c.account.public_email:
                values['public_email'] = c.account.public_email
            if c.account.public_twitter:
                values['public_twitter'] = c.account.public_twitter

        # Return the rendered template
        return templating.render('account/settings.html',
                                 form_fill=values,
                                 form_errors=errors)

    def dashboard(self, format='html'):
        """
        Show the user profile for the logged in user
        """

        # To be able to show it, the user must be logged in
        require.account.logged_in()

        # Disable caching
        self._disable_cache()

        # Return the profile page for the user
        return self.profile(c.account.name)

    def scoreboard(self, format='html'):
        """
        A list of users ordered by their score. The score is computed by
        by assigning every dataset a score (10 divided by no. of maintainers)
        and then adding that score up for all maintainers.

        This does give users who maintain a single dataset a higher score than
        those who are a part of a maintenance team, which is not really what
        we want (since that rewards single points of failure in the system).

        But this is an adequate initial score and this will only be accessible
        to administrators (who may be interested in findin these single points
        of failures).
        """

        # If user is not an administrator we abort
        if not (c.account and c.account.admin):
            abort(403, _("You are not authorized to view this page"))

        # Assign scores to each dataset based on number of maintainers
        score = db.session.query(Dataset.id,
                                 (10 / func.count(Account.id)).label('sum'))
        score = score.join('managers').group_by(Dataset.id).subquery()

        # Order users based on their score which is the sum of the dataset
        # scores they maintain
        user_score = db.session.query(
            Account.name, Account.email,
            func.coalesce(func.sum(score.c.sum), 0).label('score'))
        user_score = user_score.outerjoin(Account.datasets).outerjoin(score)
        user_score = user_score.group_by(Account.name, Account.email)
        # We exclude the system user
        user_score = user_score.filter(Account.name != 'system')
        user_score = user_score.order_by(desc('score'))

        # Fetch all and assign to a context variable score and paginate them
        # We paginate 42 users per page, just because that's an awesome number
        scores = user_score.all()
        c.page = templating.Page(scores, items_per_page=42,
                                 item_count=len(scores),
                                 **request.params)

        return templating.render('account/scoreboard.html')

    def complete(self, format='json'):
        self._disable_cache()
        parser = DistinctParamParser(request.params)
        params, errors = parser.parse()
        if errors:
            response.status = 400
            return {'errors': errors}
        if not c.account:
            response.status = 403
            return to_jsonp({'errors': _("You are not authorized to see that "
                                         "page")})

        query = db.session.query(Account)
        filter_string = params.get('q') + '%'
        query = query.filter(or_(Account.name.ilike(filter_string),
                                 Account.fullname.ilike(filter_string)))
        count = query.count()
        query = query.limit(params.get('pagesize'))
        query = query.offset(int((params.get('page') - 1) *
                                 params.get('pagesize')))
        results = [dict(fullname=x.fullname, name=x.name) for x in list(query)]

        return to_jsonp({
            'results': results,
            'count': count
        })

    def after_login(self):
        self._disable_cache()
        if c.account is not None:
            h.flash_success(_("Welcome back, %s!") % c.account.name)
            redirect(h.url_for(controller='account', action='dashboard'))
        else:
            h.flash_error(_("Incorrect user name or password!"))
            redirect(h.url_for(controller='account', action='login'))

    def after_logout(self):
        self._disable_cache()
        h.flash_success(_("You have been logged out."))
        redirect("/")

    def trigger_reset(self):
        """
        Allow user to trigger a reset of the password in case they forget it
        """

        # Disable the cache
        self._disable_cache()

        # If it's a simple GET method we return the form
        if request.method == 'GET':
            return templating.render('account/trigger_reset.html')

        # Get the email
        email = request.params.get('email')

        # Simple check to see if the email was provided. Flash error if not
        if email is None or not len(email):
            h.flash_error(_("Please enter an email address!"))
            return templating.render('account/trigger_reset.html')

        # Get the account for this email
        account = Account.by_email(email)

        # If no account is found we let the user know that it's not registered
        if account is None:
            h.flash_error(_("No user is registered under this address!"))
            return templating.render('account/trigger_reset.html')

        # Send the reset link to the email of this account
        send_reset_link(account)

        # Let the user know that email with link has been sent
        h.flash_success(_("You've received an email with a link to reset your "
                          + "password. Please check your inbox."))

        # Redirect to the login page
        redirect(h.url_for(controller='account', action='login'))

    def do_reset(self):
        email = request.params.get('email')
        if email is None or not len(email):
            h.flash_error(_("The reset link is invalid!"))
            redirect(h.url_for(controller='account', action='login'))
        account = Account.by_email(email)
        if account is None:
            h.flash_error(_("No user is registered under this address!"))
            redirect(h.url_for(controller='account', action='login'))
        if request.params.get('token') != account.token:
            h.flash_error(_("The reset link is invalid!"))
            redirect(h.url_for(controller='account', action='login'))
        who_api = request.environ['repoze.who.plugins']['auth_tkt']
        headers = who_api.remember(request.environ,
                                   {'repoze.who.userid': account.name})
        response.headers.extend(headers)
        h.flash_success(
            _("Thanks! You have now been signed in - please change "
              + "your password!"))
        redirect(h.url_for(controller='account', action='settings'))

    def profile(self, name=None):
        """
        Generate a profile page for a user (from the provided name)
        """

        # Get the account, if it's none we return a 404
        account = Account.by_name(name)
        if account is None:
            response.status = 404
            return None

        # Set the account we got as the context variable 'profile'
        # Note this is not the same as the context variable 'account'
        # which is the account for a logged in user
        c.profile = account

        # Set a context boo if email/twitter should be shown, it is only shown
        # to administrators and to owner (account is same as context account)
        show_info = (c.account and c.account.admin) or (c.account == account)

        # ..or if the user has chosen to make it public
        c.show_email = show_info or account.public_email
        c.show_twitter = show_info or account.public_twitter

        # Collect and sort the account's datasets and views
        c.account_datasets = sorted(account.datasets, key=lambda d: d.label)
        c.account_views = sorted(account.views, key=lambda d: d.label)

        # Render the profile
        return templating.render('account/profile.html')

########NEW FILE########
__FILENAME__ = version1
import logging

from collections import defaultdict

from pylons import request, response, tmpl_context as c
from pylons.controllers.util import abort

from openspending import model
from openspending.lib import calculator
from openspending.lib import solr_util as solr
from solr import SolrException
from openspending.ui.lib.base import BaseController, require
from openspending.ui.lib.cache import AggregationCache
from openspending.lib.jsonexport import jsonpify

log = logging.getLogger(__name__)

__controller__ = 'APIv1Controller'


def statistic_normalize(dataset, result, per, statistic):
    drilldowns = []
    values = {}
    for drilldown in result['drilldown']:
        per_value = drilldown.get(per)
        if per_value not in values:
            entries = list(dataset.entries(dataset.table.c[per] == per_value,
                                           limit=1))
            if len(entries):
                values[per_value] = entries[0].get(statistic, 0.0)
            else:
                values[per_value] = 0.0
        if values[per_value]:  # skip division by zero oppprtunities
            drilldown['amount'] /= values[per_value]
            drilldowns.append(drilldown)
    result['drilldown'] = drilldowns
    return result


def cellget(cell, key):
    val = cell.get(key)
    if isinstance(val, dict):
        return val.get('name', val.get('id'))
    return val


class APIv1Controller(BaseController):

    @jsonpify
    def index(self):
        out = {
            'doc': 'http://openspending.org/help/api.html'
        }
        return out

    def search(self):
        solrargs = dict(request.params)
        rows = min(1000, int(request.params.get('rows', 10)))
        q = request.params.get('q', '*:*')
        solrargs['q'] = q
        solrargs['rows'] = rows
        solrargs['wt'] = 'json'

        datasets = model.Dataset.all_by_account(c.account)
        fq = ' OR '.join(map(lambda d: '+dataset:"%s"' % d.name, datasets))
        solrargs['fq'] = '(%s)' % fq

        if 'callback' in solrargs and 'json.wrf' not in solrargs:
            solrargs['json.wrf'] = solrargs['callback']
        if 'sort' not in solrargs:
            solrargs['sort'] = 'score desc,amount desc'
        try:
            query = solr.get_connection().raw_query(**solrargs)
        except SolrException as se:
            response.status_int = se.httpcode
            return se.body
        response.content_type = 'application/json'
        return query

    @jsonpify
    def aggregate(self):
        dataset_name = request.params.get(
            'dataset',
            request.params.get('slice'))
        dataset = model.Dataset.by_name(dataset_name)
        if dataset is None:
            abort(400, "Dataset %s not found" % dataset_name)
        require.dataset.read(dataset)

        drilldowns, cuts, statistics = [], [], []
        for key, value in sorted(request.params.items()):
            if '-' not in key:
                continue
            op, key = key.split('-', 1)
            if 'include' == op:
                cuts.append((key, value))
            elif 'per' == op:
                if 'time' == key:
                    abort(400, "Time series are no longer supported")
                statistics.append((key, value))
            elif 'breakdown' == op:
                drilldowns.append(key)
        cache = AggregationCache(dataset)
        result = cache.aggregate(drilldowns=drilldowns + ['time'],
                                 cuts=cuts)
        # TODO: handle statistics as key-values ??? what's the point?
        for k, v in statistics:
            result = statistic_normalize(dataset, result, v, k)

        # translate to old format: group by drilldown, then by date.
        translated_result = defaultdict(dict)
        for cell in result['drilldown']:
            key = tuple([cellget(cell, d) for d in drilldowns])
            translated_result[key][cell['time']['name']] = \
                cell['amount']
        dates = sorted(set([d['time']['name'] for d in
                            result['drilldown']]))
        # give a value (or 0) for each present date in sorted order
        translated_result = [(k, [v.get(d, 0.0) for d in dates])
                             for k, v in translated_result.items()]
        return {'results': translated_result,
                'metadata': {
                    'dataset': dataset.name,
                    'include': cuts,
                    'dates': map(unicode, dates),
                    'axes': drilldowns,
                    'per': statistics,
                    'per_time': []
                }
                }

    @jsonpify
    def mytax(self):

        def float_param(name, required=False):
            if name not in request.params:
                if required:
                    abort(status_code=400,
                          detail='parameter %s is missing' % name)
                return None
            ans = request.params[name]
            try:
                return float(ans)
            except ValueError:
                abort(status_code=400, detail='%r is not a number' % ans)

        def bool_param(name, default=True, required=False):
            if name not in request.params:
                if required:
                    abort(status_code=400,
                          detail='parameter %s is missing' % name)
                return default

            ans = request.params[name].lower()
            if ans == 'yes':
                return True
            elif ans == 'no':
                return False
            else:
                abort(status_code=400,
                      detail='%r is not %r or %r' % (ans, 'yes', 'no'))

        tax, explanation = calculator.TaxCalculator2010().total_tax(
            float_param('income', required=True),
            float_param('spending'),
            bool_param('smoker'),
            bool_param('drinker'),
            bool_param('driver'))

        result = {'explanation': explanation}
        for k, v in tax.items():
            result[k] = v

        return result

########NEW FILE########
__FILENAME__ = version2
import logging
import urllib2
import json

from pylons import request, response, tmpl_context as c
from pylons.controllers.util import abort, etag_cache

from openspending import auth as can
from openspending.model import meta as db
from openspending.model.dataset import Dataset
from openspending.model.source import Source
from openspending.lib import util
from openspending.lib.browser import Browser
from openspending.lib.streaming import (JSONStreamingResponse,
                                        CSVStreamingResponse)
from solr import SolrException
from openspending.lib.jsonexport import to_jsonp, json_headers
from openspending.lib.csvexport import write_csv, csv_headers
from openspending.lib.paramparser import (AggregateParamParser,
                                          SearchParamParser)
from openspending.ui.lib.base import BaseController, require
from openspending.ui.lib.base import etag_cache_keygen
from openspending.ui.lib.cache import AggregationCache
from openspending.ui.lib.hypermedia import (entry_apply_links,
                                            drilldowns_apply_links,
                                            dataset_apply_links)
from openspending.tasks.dataset import load_source
from openspending.validation.model import validate_model
from colander import Invalid

log = logging.getLogger(__name__)

__controller__ = 'APIv2Controller'


class APIv2Controller(BaseController):

    def _response_params(self, params):
        """
        Create response headers based on parameters. Headers will be something
        like "X-Drilldowns: [u'from']"
        """

        # Loop over the parameters and and add each to the response headers
        for k, v in params.items():
            # Replace both _ and - with space and then split the string on
            # whitespace, then join it together, capitalizing each part and
            # append and X. So a parameter called "this-is a_header" will
            # become X-This-Is-A-Header"
            k = k.replace('_', ' ').replace('-', ' ').split()
            k = '-'.join(['X'] + [l.capitalize() for l in k])

            # Add the header along with it's value encoded as ascii but
            # ignore all errors in encoding
            response.headers[k] = unicode(v).encode('ascii', 'ignore')

    def aggregate(self):
        """
        Aggregation of a dataset based on URL parameters. It serves the
        aggregation from a cache if possible, and if not it computes it (it's
        performed in the aggregation cache for some reason).
        """

        # Parse the aggregation parameters to get them into the right format
        parser = AggregateParamParser(request.params)
        params, errors = parser.parse()

        # If there were parsing errors we return them with status code 400
        # as jsonp, irrespective of what format was asked for.
        if errors:
            response.status = 400
            return to_jsonp({'errors': errors})

        # URL parameters are always singular nouns but we work with some
        # as plural nouns so we pop them into the plural version
        params['cuts'] = params.pop('cut')
        params['drilldowns'] = params.pop('drilldown')
        params['measures'] = params.pop('measure')

        # Get the dataset and the format and remove from the parameters
        dataset = params.pop('dataset')
        format = params.pop('format')

        # User must have the right to read the dataset to perform aggregation
        require.dataset.read(dataset)

        # Create response headers from the parameters
        self._response_params(params)

        try:
            # Create an aggregation cache for the dataset and aggregate its
            # results. The cache will perform the aggreagation if it doesn't
            # have a cached result
            cache = AggregationCache(dataset)
            result = cache.aggregate(**params)

            # If the result has drilldown we create html_url values for its
            # dimensions (linked data).
            if 'drilldown' in result:
                result['drilldown'] = drilldowns_apply_links(
                    dataset.name, result['drilldown'])

            # Do the ETag caching based on the cache_key in the summary
            # this is a weird place to do it since the heavy lifting has
            # already been performed above. TODO: Needs rethinking.
            response.last_modified = dataset.updated_at
            if cache.cache_enabled and 'cache_key' in result['summary']:
                etag_cache(result['summary']['cache_key'])

        except (KeyError, ValueError) as ve:
            # We log possible errors and return them with status code 400
            log.exception(ve)
            response.status = 400
            return to_jsonp({'errors': [unicode(ve)]})

        # If the requested format is csv we write the drilldown results into
        # a csv file and return it, if not we return a jsonp result (default)
        if format == 'csv':
            return write_csv(result['drilldown'], response,
                             filename=dataset.name + '.csv')
        return to_jsonp(result)

    def search(self):
        parser = SearchParamParser(request.params)
        params, errors = parser.parse()

        if errors:
            response.status = 400
            return to_jsonp({'errors': errors})

        expand_facets = params.pop('expand_facet_dimensions')

        format = params.pop('format')
        if format == 'csv':
            params['stats'] = False
            params['facet_field'] = None

        datasets = params.pop('dataset', None)
        if datasets is None or not datasets:
            q = Dataset.all_by_account(c.account)
            if params.get('category'):
                q = q.filter_by(category=params.pop('category'))
            datasets = q.all()
            expand_facets = False

        if not datasets:
            return {'errors': ["No dataset available."]}

        params['filter']['dataset'] = []
        for dataset in datasets:
            require.dataset.read(dataset)
            params['filter']['dataset'].append(dataset.name)

        response.last_modified = max([d.updated_at for d in datasets])
        etag_cache_keygen(parser.key(), response.last_modified)

        self._response_params(params)

        if params['pagesize'] > parser.defaults['pagesize']:

            # http://wiki.nginx.org/X-accel#X-Accel-Buffering
            response.headers['X-Accel-Buffering'] = 'no'

            if format == 'csv':
                csv_headers(response, 'entries.csv')
                streamer = CSVStreamingResponse(
                    datasets,
                    params,
                    pagesize=parser.defaults['pagesize']
                )
                return streamer.response()
            else:
                json_headers(filename='entries.json')
                streamer = JSONStreamingResponse(
                    datasets,
                    params,
                    pagesize=parser.defaults['pagesize'],
                    expand_facets=util.expand_facets
                    if expand_facets else None,
                    callback=request.params.get('callback')
                )
                return streamer.response()

        solr_browser = Browser(**params)
        try:
            solr_browser.execute()
        except SolrException as e:
            return {'errors': [unicode(e)]}

        entries = []
        for dataset, entry in solr_browser.get_entries():
            entry = entry_apply_links(dataset.name, entry)
            entry['dataset'] = dataset_apply_links(dataset.as_dict())
            entries.append(entry)

        if format == 'csv':
            return write_csv(entries, response,
                             filename='entries.csv')

        if expand_facets and len(datasets) == 1:
            facets = solr_browser.get_expanded_facets(datasets[0])
        else:
            facets = solr_browser.get_facets()

        return to_jsonp({
            'stats': solr_browser.get_stats(),
            'facets': facets,
            'results': entries
        })

    def create(self):
        """
        Adds a new dataset dynamically through a POST request
        """

        # User must be authenticated so we should have a user object in
        # c.account, if not abort with error message
        if not c.account:
            abort(status_code=400, detail='user not authenticated')

        # Check if the params are there ('metadata', 'csv_file')
        if len(request.params) != 2:
            abort(status_code=400, detail='incorrect number of params')

        metadata = request.params['metadata'] \
            if 'metadata' in request.params \
            else abort(status_code=400, detail='metadata is missing')

        csv_file = request.params['csv_file'] \
            if 'csv_file' in request.params \
            else abort(status_code=400, detail='csv_file is missing')

        # We proceed with the dataset
        try:
            model = json.load(urllib2.urlopen(metadata))
        except:
            abort(status_code=400, detail='JSON model could not be parsed')
        try:
            log.info("Validating model")
            model = validate_model(model)
        except Invalid as i:
            log.error("Errors occured during model validation:")
            for field, error in i.asdict().items():
                log.error("%s: %s", field, error)
            abort(status_code=400, detail='Model is not well formed')
        dataset = Dataset.by_name(model['dataset']['name'])
        if dataset is None:
            dataset = Dataset(model)
            require.dataset.create()
            dataset.managers.append(c.account)
            dataset.private = True  # Default value
            db.session.add(dataset)
        else:
            require.dataset.update(dataset)

        log.info("Dataset: %s", dataset.name)
        source = Source(dataset=dataset, creator=c.account, url=csv_file)

        log.info(source)
        for source_ in dataset.sources:
            if source_.url == csv_file:
                source = source_
                break
        db.session.add(source)
        db.session.commit()

        # Send loading of source into celery queue
        load_source.delay(source.id)
        return to_jsonp(dataset_apply_links(dataset.as_dict()))

    def permissions(self):
        """
        Check a user's permissions for a given dataset. This could also be
        done via request to the user, but since we're not really doing a
        RESTful service we do this via the api instead.
        """

        # Check the parameters. Since we only use one parameter we check it
        # here instead of creating a specific parameter parser
        if len(request.params) != 1 or 'dataset' not in request.params:
            return to_jsonp({'error': 'Parameter dataset missing'})

        # Get the dataset we want to check permissions for
        dataset = Dataset.by_name(request.params['dataset'])

        # Return permissions
        return to_jsonp(
            {
                "create": can.dataset.create() and dataset is None,
                "read": False if dataset is None
                else can.dataset.read(dataset),
                "update": False if dataset is None
                else can.dataset.update(dataset),
                "delete": False if dataset is None
                else can.dataset.delete(dataset)})

########NEW FILE########
__FILENAME__ = badge
import logging
import os
from pylons import request, tmpl_context as c
from pylons.controllers.util import redirect
from pylons.i18n import _

from openspending.model import meta as db
from openspending.model.badge import Badge
from openspending.ui.lib.base import require
from openspending.lib.jsonexport import to_jsonp
from openspending.ui.lib import helpers as h
from openspending.ui.lib.hypermedia import (badges_apply_links,
                                            badge_apply_links)
from openspending.ui.lib.base import BaseController
from openspending.ui.alttemplates import templating

log = logging.getLogger(__name__)


class BadgeController(BaseController):

    def index(self, format='html'):
        """
        List all badges in the system. Default is to present the
        user with an html site, but the user can request a json list
        of badges.
        """
        c.badges = Badge.all()

        # If the requested format is json return a list of badges
        if format == 'json':
            return to_jsonp({"badges": badges_apply_links([b.as_dict()
                                                          for b in c.badges])})

        # Return html representation
        return templating.render('badge/index.html')

    def information(self, id, format='html'):
        """
        Show information about the badge. Default is to present the
        user with the badge on an html site, but the user can request a
        json representation of the badge
        """

        # Get the badge
        c.badge = Badge.by_id(id=id)

        # Return a json representation if the format requested is 'json'
        if format == 'json':
            return to_jsonp({"badge": badge_apply_links(c.badge.as_dict())})

        # Return html representation
        return templating.render('badge/information.html')

    def create(self):
        """
        Create a new badge in the system
        """
        # Check if user is allowed to create a badge
        require.badge.create()

        import shutil

        label = request.params['badge-label']
        description = request.params['badge-description']
        image = request.POST['badge-image']

        try:
            # Get upload directory for Badge and generate a random filename
            upload_dir = h.get_object_upload_dir(Badge)
            random_filename = h.get_uuid_filename(image.filename)

            # Open the filename and copy the uploaded image
            permanent_filename = os.path.join(upload_dir, random_filename)
            permanent_image = open(permanent_filename, 'w')
            shutil.copyfileobj(image.file, permanent_image)

            upload_image_path = h.upload(random_filename, Badge)
            # Close image files
            image.file.close()
            permanent_image.close()
        except OSError:
            upload_image_path = ''
            h.flash_error(_('Uploading files not supported at the moment.'))

        badge = Badge(label, upload_image_path, description, c.account)
        db.session.add(badge)
        db.session.commit()

        redirect(h.url_for(controller='badge', action='information',
                           id=badge.id))

    def give(self, dataset):
        """
        Award a given badge to a given dataset.
        """
        # Get the dataset
        self._get_dataset(dataset)

        # Get the badge
        badge_id = request.params.get('badge', None)
        badge = Badge.by_id(id=badge_id)

        if badge:
            # See if user can award this badge to a this dataset
            require.badge.give(badge, c.dataset)
            # Add the dataset to the badge datasets and commit to database
            badge.datasets.append(c.dataset)
            db.session.commit()
        else:
            # If we don't find the badge id we flash an error message
            h.flash_error(_('Badge not found.'))

        # Go to the dataset's main page
        redirect(h.url_for(controller='dataset', action='view',
                           dataset=c.dataset.name))

########NEW FILE########
__FILENAME__ = dataset
import json
import logging
from StringIO import StringIO
from urllib import urlencode

from webhelpers.feedgenerator import Rss201rev2Feed

from pylons import request, response, tmpl_context as c, url
from pylons.controllers.util import abort, redirect
from pylons.i18n import _
from colander import SchemaNode, String, Invalid

from openspending.model.dataset import Dataset
from openspending.model.badge import Badge
from openspending.model import meta as db
from openspending.lib.csvexport import write_csv
from openspending.lib.jsonexport import to_jsonp
from openspending.lib.paramparser import DatasetIndexParamParser
from openspending import auth as has

from openspending.ui.lib import helpers as h
from openspending.ui.lib.base import BaseController
from openspending.ui.lib.cache import DatasetIndexCache
from openspending.ui.lib.base import require, etag_cache_keygen
from openspending.ui.lib.views import handle_request
from openspending.ui.lib.hypermedia import dataset_apply_links
from openspending.reference.currency import CURRENCIES
from openspending.reference.country import COUNTRIES
from openspending.reference.category import CATEGORIES
from openspending.reference.language import LANGUAGES
from openspending.validation.model.dataset import dataset_schema
from openspending.validation.model.common import ValidationState
from openspending.ui.controllers.entry import EntryController
from openspending.ui.alttemplates import templating

log = logging.getLogger(__name__)


class DatasetController(BaseController):

    def index(self, format='html'):
        """
        Get a list of all datasets along with territory, language, and
        category counts (amount of datasets for each).
        """

        # Create facet filters (so we can look at a single country,
        # language etc.)
        c.query = request.params.items()
        c.add_filter = lambda f, v: \
            '?' + urlencode(c.query +
                            [(f, v)] if (f, v) not in c.query else c.query)
        c.del_filter = lambda f, v: \
            '?' + urlencode([(k, x) for k, x in
                             c.query if (k, x) != (f, v)])

        # Parse the request parameters to get them into the right format
        parser = DatasetIndexParamParser(request.params)
        params, errors = parser.parse()
        if errors:
            concatenated_errors = ', '.join(errors)
            abort(400,
                  _('Parameter values not supported: %s') %
                  concatenated_errors)

        # We need to pop the page and pagesize parameters since they're not
        # used for the cache (we have to get all of the datasets to do the
        # language, territory, and category counts (these are then only used
        # for the html response)
        params.pop('page')
        pagesize = params.pop('pagesize')

        # Get cached indices (this will also generate them if there are no
        # cached results (the cache is invalidated when a dataset is published
        # or retracted
        cache = DatasetIndexCache()
        results = cache.index(**params)

        # Generate the ETag from the last modified timestamp of the first
        # dataset (since they are ordered in descending order by last
        # modified). It doesn't matter that this happens if it has (possibly)
        # generated the index (if not cached) since if it isn't cached then
        # the ETag is definitely modified. We wrap it in a try clause since
        # if there are no public datasets we'll get an index error.
        # We also don't set c._must_revalidate to True since we don't care
        # if the index needs a hard refresh
        try:
            etag_cache_keygen(
                results['datasets'][0]['timestamps']['last_modified'])
        except IndexError:
            etag_cache_keygen(None)

        # Assign the results to template context variables
        c.language_options = results['languages']
        c.territory_options = results['territories']
        c.category_options = results['categories']

        if format == 'json':
            # Apply links to the dataset lists before returning the json
            results['datasets'] = [dataset_apply_links(r)
                                   for r in results['datasets']]
            return to_jsonp(results)
        elif format == 'csv':
            # The CSV response only shows datasets, not languages,
            # territories, etc.
            return write_csv(results['datasets'], response)

        # If we're here then it's an html format so we show rss, do the
        # pagination and render the template
        c.show_rss = True
        # The page parameter we popped earlier is part of request.params but
        # we now know it was parsed. We have to send in request.params to
        # retain any parameters already supplied (filters)
        c.page = templating.Page(results['datasets'], items_per_page=pagesize,
                                 item_count=len(results['datasets']),
                                 **request.params)
        return templating.render('dataset/index.html')

    def new(self, errors={}):
        self._disable_cache()
        if not has.dataset.create():
            return templating.render('dataset/new_cta.html')
        require.dataset.create()
        c.key_currencies = sorted(
            [(r, n) for (r, (n, k)) in CURRENCIES.items() if k],
            key=lambda k_v: k_v[1])
        c.all_currencies = sorted(
            [(r, n) for (r, (n, k)) in CURRENCIES.items() if not k],
            key=lambda k_v1: k_v1[1])
        c.languages = sorted(LANGUAGES.items(), key=lambda k_v2: k_v2[1])
        c.territories = sorted(COUNTRIES.items(), key=lambda k_v3: k_v3[1])
        c.categories = sorted(CATEGORIES.items(), key=lambda k_v4: k_v4[1])
        errors = [(k[len('dataset.'):], v) for k, v in errors.items()]
        return templating.render(
            'dataset/new.html', form_errors=dict(errors),
            form_fill=request.params if errors else {'currency': 'USD'})

    def create(self):
        require.dataset.create()
        try:
            dataset = dict(request.params)
            dataset['territories'] = request.params.getall('territories')
            dataset['languages'] = request.params.getall('languages')
            model = {'dataset': dataset}
            schema = dataset_schema(ValidationState(model))
            data = schema.deserialize(dataset)
            if Dataset.by_name(data['name']) is not None:
                raise Invalid(
                    SchemaNode(String(), name='dataset.name'),
                    _("A dataset with this identifer already exists!"))
            dataset = Dataset({'dataset': data})
            dataset.private = True
            dataset.managers.append(c.account)
            db.session.add(dataset)
            db.session.commit()
            redirect(h.url_for(controller='editor', action='index',
                               dataset=dataset.name))
        except Invalid as i:
            errors = i.asdict()
            return self.new(errors)

    def view(self, dataset, format='html'):
        """
        Dataset viewer. Default format is html. This will return either
        an entry index if there is no default view or the defaul view.
        If a request parameter embed is given the default view is
        returned as an embeddable page.

        If json is provided as a format the json representation of the
        dataset is returned.
        """

        # Get the dataset (will be placed in c.dataset)
        self._get_dataset(dataset)

        # Generate the etag for the cache based on updated_at value
        etag_cache_keygen(c.dataset.updated_at)

        # Compute the number of entries in the dataset
        c.num_entries = len(c.dataset)

        # Handle the request for the dataset, this will return
        # a default view in c.view if there is any
        handle_request(request, c, c.dataset)

        if format == 'json':
            # If requested format is json we return the json representation
            return to_jsonp(dataset_apply_links(c.dataset.as_dict()))
        else:
            (earliest_timestamp, latest_timestamp) = c.dataset.timerange()
            if earliest_timestamp is not None:
                c.timerange = {'from': earliest_timestamp,
                               'to': latest_timestamp}

            if c.view is None:
                # If handle request didn't return a view we return the
                # entry index
                return EntryController().index(dataset, format)
            if 'embed' in request.params:
                # If embed is requested using the url parameters we return
                # a redirect to an embed page for the default view
                return redirect(
                    h.url_for(controller='view',
                              action='embed', dataset=c.dataset.name,
                              widget=c.view.vis_widget.get('name'),
                              state=json.dumps(c.view.vis_state)))
                # Return the dataset view (for the default view)
            return templating.render('dataset/view.html')

    def about(self, dataset, format='html'):
        self._get_dataset(dataset)
        etag_cache_keygen(c.dataset.updated_at)
        handle_request(request, c, c.dataset)
        c.sources = list(c.dataset.sources)
        c.managers = list(c.dataset.managers)

        # Get all badges if user is admin because they can then
        # give badges to the dataset on its about page.
        if c.account and c.account.admin:
            c.badges = list(Badge.all())

        return templating.render('dataset/about.html')

    def explorer(self, dataset):
        redirect(h.url_for(controller='view', action='new',
                           dataset=dataset))

    def model(self, dataset, format='json'):
        self._get_dataset(dataset)
        etag_cache_keygen(c.dataset.updated_at)
        model = c.dataset.model
        model['dataset'] = dataset_apply_links(model['dataset'])
        return to_jsonp(model)

    def feed_rss(self):
        q = db.session.query(Dataset)
        if not (c.account and c.account.admin):
            q = q.filter_by(private=False)
        feed_items = q.order_by(Dataset.created_at.desc()).limit(20)
        items = []
        for feed_item in feed_items:
            items.append({
                'title': feed_item.label,
                'pubdate': feed_item.updated_at,
                'link': url(controller='dataset', action='view',
                            dataset=feed_item.name, qualified=True),
                'description': feed_item.description,
                'author_name': ', '.join([person.fullname for person in
                                          feed_item.managers if
                                          person.fullname]),
            })
        feed = Rss201rev2Feed(_('Recently Created Datasets'), url(
            controller='home', action='index', qualified=True),
            _('Recently created datasets in the OpenSpending Platform'),
            author_name='Openspending')
        for item in items:
            feed.add_item(**item)
        sio = StringIO()
        feed.write(sio, 'utf-8')
        response.content_type = 'application/xml'
        return sio.getvalue()

########NEW FILE########
__FILENAME__ = dimension
import logging
import json

from pylons import request, tmpl_context as c, response
from pylons.controllers.util import abort, redirect
from pylons.i18n import _

from openspending.model.dimension import Dimension
from openspending.ui.lib.base import BaseController
from openspending.ui.lib.base import etag_cache_keygen
from openspending.ui.lib.views import handle_request
from openspending.ui.lib.helpers import url_for
from openspending.ui.lib.widgets import get_widget
from openspending.lib.paramparser import DistinctFieldParamParser
from openspending.ui.lib.hypermedia import dimension_apply_links, \
    member_apply_links, entry_apply_links
from openspending.lib.csvexport import write_csv
from openspending.lib.jsonexport import write_json, to_jsonp
from openspending.ui.alttemplates import templating
log = logging.getLogger(__name__)

PAGE_SIZE = 100


class DimensionController(BaseController):

    def _get_dimension(self, dataset, dimension):
        self._get_dataset(dataset)
        try:
            c.dimension = c.dataset[dimension]
        except KeyError:
            abort(404, _('This is not a dimension'))
        if not isinstance(c.dimension, Dimension):
            abort(404, _('This is not a dimension'))

    def _get_member(self, dataset, dimension_name, name):
        self._get_dataset(dataset)
        c.dimension = dimension_name
        for dimension in c.dataset.compounds:
            if dimension.name == dimension_name:
                cond = dimension.alias.c.name == name
                members = list(dimension.members(cond, limit=1))
                if not len(members):
                    abort(404, _('Sorry, there is no member named %r')
                          % name)
                c.dimension = dimension
                c.member = members.pop()
                c.num_entries = dimension.num_entries(cond)
                return
        abort(404,
              _('Sorry, there is no dimension named %r') % dimension_name)

    def index(self, dataset, format='html'):
        self._get_dataset(dataset)
        etag_cache_keygen(c.dataset.updated_at, format)
        if format == 'json':
            dimensions = [dimension_apply_links(dataset, d.as_dict())
                          for d in c.dataset.dimensions]
            return to_jsonp(dimensions)
        else:
            return templating.render('dimension/index.html')

    def view(self, dataset, dimension, format='html'):
        self._get_dimension(dataset, dimension)
        etag_cache_keygen(c.dataset.updated_at, format)
        if format == 'json':
            dimension = dimension_apply_links(dataset, c.dimension.as_dict())
            return to_jsonp(dimension)
        c.widget = get_widget('aggregate_table')
        c.widget_state = {'drilldowns': [c.dimension.name]}
        return templating.render('dimension/view.html')

    def distinct(self, dataset, dimension, format='json'):
        self._get_dimension(dataset, dimension)
        parser = DistinctFieldParamParser(c.dimension, request.params)
        params, errors = parser.parse()
        etag_cache_keygen(c.dataset.updated_at, format, parser.key())

        if errors:
            response.status = 400
            return {'errors': errors}

        q = params.get('attribute').column_alias.ilike(params.get('q') + '%')
        offset = int((params.get('page') - 1) * params.get('pagesize'))
        members = c.dimension.members(
            q,
            offset=offset,
            limit=params.get('pagesize'))
        return to_jsonp({
            'results': list(members),
            'count': c.dimension.num_entries(q)
        })

    def member(self, dataset, dimension, name, format="html"):
        self._get_member(dataset, dimension, name)
        handle_request(request, c, c.member, c.dimension.name)
        member = [member_apply_links(dataset, dimension, c.member)]
        if format == 'json':
            return write_json(member, response)
        elif format == 'csv':
            return write_csv(member, response)
        else:
            # If there are no views set up, then go direct to the entries
            # search page
            if c.view is None:
                return redirect(
                    url_for(controller='dimension', action='entries',
                            dataset=c.dataset.name, dimension=dimension,
                            name=name))
            if 'embed' in request.params:
                return redirect(
                    url_for(controller='view',
                            action='embed', dataset=c.dataset.name,
                            widget=c.view.vis_widget.get('name'),
                            state=json.dumps(c.view.vis_state)))
            return templating.render('dimension/member.html')

    def entries(self, dataset, dimension, name, format='html'):
        self._get_member(dataset, dimension, name)
        if format in ['json', 'csv']:
            return redirect(
                url_for(controller='api/version2', action='search',
                        format=format, dataset=dataset,
                        filter='%s.name:%s' % (dimension, name),
                        **request.params))

        handle_request(request, c, c.member, c.dimension.name)
        entries = c.dataset.entries(
            c.dimension.alias.c.name == c.member['name'])
        entries = (entry_apply_links(dataset, e) for e in entries)
        return templating.render('dimension/entries.html')

########NEW FILE########
__FILENAME__ = editor
import logging
import json
from datetime import datetime

from pylons.controllers.util import redirect
from pylons import request, tmpl_context as c
from pylons.i18n import _
from colander import Invalid

from openspending.model.account import Account
from openspending.model.run import Run
from openspending.model import meta as db
from openspending.lib import solr_util as solr
from openspending.ui.lib import helpers as h
from openspending.ui.lib.base import BaseController
from openspending.ui.lib.base import require, abort
from openspending.ui.lib.cache import AggregationCache, DatasetIndexCache
from openspending.reference.currency import CURRENCIES
from openspending.reference.country import COUNTRIES
from openspending.reference.category import CATEGORIES
from openspending.reference.language import LANGUAGES
from openspending.validation.model.dataset import dataset_schema
from openspending.validation.model.mapping import mapping_schema
from openspending.validation.model.views import views_schema
from openspending.validation.model.common import ValidationState

from openspending.ui.alttemplates import templating

log = logging.getLogger(__name__)


class EditorController(BaseController):

    def index(self, dataset, format='html'):
        self._get_dataset(dataset)
        self._disable_cache()
        require.dataset.update(c.dataset)
        c.entries_count = len(c.dataset)
        c.has_sources = c.dataset.sources.count() > 0
        c.source = c.dataset.sources.first()
        c.index_count = solr.dataset_entries(c.dataset.name)
        c.index_percentage = 0 if not c.entries_count else \
            int((float(c.index_count) / float(c.entries_count)) * 1000)
        return templating.render('editor/index.html')

    def core_edit(self, dataset, errors={}, format='html'):
        self._get_dataset(dataset)
        self._disable_cache()
        require.dataset.update(c.dataset)
        c.key_currencies = sorted(
            [(r, n) for (r, (n, k)) in CURRENCIES.items() if k],
            key=lambda k_v: k_v[1])
        c.all_currencies = sorted(
            [(r, n) for (r, (n, k)) in CURRENCIES.items() if not k],
            key=lambda k_v1: k_v1[1])
        c.languages = sorted(LANGUAGES.items(), key=lambda k_v2: k_v2[1])
        c.territories = sorted(COUNTRIES.items(), key=lambda k_v3: k_v3[1])
        c.categories = sorted(CATEGORIES.items(), key=lambda k_v4: k_v4[1])

        if 'time' in c.dataset:
            c.available_times = [m['year']
                                 for m in c.dataset['time'].members()]
            c.available_times = sorted(set(c.available_times), reverse=True)
        else:
            c.available_times = []

        errors = [(k[len('dataset.'):], v) for k, v in errors.items()]
        fill = c.dataset.as_dict()
        if errors:
            fill.update(request.params)
        return templating.render('editor/core.html', form_errors=dict(errors),
                                 form_fill=fill)

    def core_update(self, dataset, format='html'):
        self._get_dataset(dataset)
        require.dataset.update(c.dataset)
        errors = {}
        try:
            schema = dataset_schema(ValidationState(c.dataset.model))
            data = dict(request.params)
            data['territories'] = request.params.getall('territories')
            data['languages'] = request.params.getall('languages')
            data = schema.deserialize(data)
            c.dataset.label = data['label']
            c.dataset.currency = data['currency']
            c.dataset.category = data['category']
            c.dataset.description = data['description']
            c.dataset.default_time = data['default_time']
            c.dataset.territories = data['territories']
            c.dataset.languages = data['languages']
            db.session.commit()
            h.flash_success(_("The dataset has been updated."))
        except Invalid as i:
            errors = i.asdict()
        return self.core_edit(dataset, errors=errors)

    def dimensions_edit(self, dataset, errors={}, mapping=None,
                        format='html', saved=False):

        self._get_dataset(dataset)
        self._disable_cache()
        require.dataset.update(c.dataset)
        # TODO: really split up dimensions and mapping editor.
        c.source = c.dataset.sources.first()
        if c.source is None:
            return templating.render('editor/dimensions_errors.html')
        mapping = mapping or c.dataset.data.get('mapping', {})
        if not len(mapping) and c.source and 'mapping' in c.source.analysis:
            mapping = c.source.analysis['mapping']
        c.fill = {'mapping': json.dumps(mapping, indent=2)}
        c.errors = errors
        c.saved = saved
        if len(c.dataset):
            return templating.render('editor/dimensions_errors.html')
        return templating.render('editor/dimensions.html', form_fill=c.fill)

    def dimensions_update(self, dataset, format='html'):
        self._get_dataset(dataset)

        require.dataset.update(c.dataset)
        if len(c.dataset):
            abort(400, _("You cannot edit the dimensions model when "
                         "data is loaded for the dataset."))

        errors, mapping, saved = {}, None, False
        try:
            mapping = json.loads(request.params.get('mapping'))
            model = c.dataset.model
            model['mapping'] = mapping
            schema = mapping_schema(ValidationState(model))
            new_mapping = schema.deserialize(mapping)
            c.dataset.data['mapping'] = new_mapping
            c.dataset.drop()
            c.dataset._load_model()
            c.dataset.generate()
            db.session.commit()
            # h.flash_success(_("The mapping has been updated."))
            saved = True
        except (ValueError, TypeError, AttributeError):
            abort(400, _("The mapping data could not be decoded as JSON!"))
        except Invalid as i:
            errors = i.asdict()
        return self.dimensions_edit(dataset, errors=errors,
                                    mapping=mapping, saved=saved)

    def templates_edit(self, dataset, errors={}, values=None,
                       format='html'):
        self._get_dataset(dataset)
        self._disable_cache()
        require.dataset.update(c.dataset)
        c.fill = values or {'serp_title': c.dataset.serp_title,
                            'serp_teaser': c.dataset.serp_teaser}
        c.errors = errors
        return templating.render('editor/templates.html', form_fill=c.fill)

    def templates_update(self, dataset, format='html'):
        self._get_dataset(dataset)
        require.dataset.update(c.dataset)
        errors, values = {}, None
        try:
            values = dict(request.params.items())
            c.dataset.serp_title = values.get('serp_title', None)
            c.dataset.serp_teaser = values.get('serp_teaser', None)
            db.session.commit()
            h.flash_success(_("The templates have been updated."))
        except Invalid as i:
            errors = i.asdict()
        return self.templates_edit(dataset, errors=errors, values=values)

    def views_edit(self, dataset, errors={}, views=None,
                   format='html'):
        self._get_dataset(dataset)
        self._disable_cache()
        require.dataset.update(c.dataset)
        views = views or c.dataset.data.get('views', [])
        c.fill = {'views': json.dumps(views, indent=2)}
        c.errors = errors
        return templating.render('editor/views.html', form_fill=c.fill)

    def views_update(self, dataset, format='html'):
        self._get_dataset(dataset)
        require.dataset.update(c.dataset)
        errors, views = {}, None
        try:
            views = json.loads(request.params.get('views'))
            schema = views_schema(ValidationState(c.dataset.model))
            c.dataset.data['views'] = schema.deserialize(views)
            db.session.commit()
            h.flash_success(_("The views have been updated."))
        except (ValueError, TypeError):
            abort(400, _("The views could not be decoded as JSON!"))
        except Invalid as i:
            errors = i.asdict()
        return self.views_edit(dataset, errors=errors, views=views)

    def team_edit(self, dataset, errors={}, accounts=None,
                  format='html'):
        self._get_dataset(dataset)
        self._disable_cache()
        require.dataset.update(c.dataset)
        accounts = accounts or c.dataset.managers
        c.accounts = json.dumps([a.as_dict() for a in accounts], indent=2)
        c.errors = errors
        return templating.render('editor/team.html')

    def team_update(self, dataset, format='html'):
        self._get_dataset(dataset)
        require.dataset.update(c.dataset)
        errors, accounts = {}, []
        for account_name in request.params.getall('accounts'):
            account = Account.by_name(account_name)
            if account is None:
                errors[account_name] = _("User account cannot be found.")
            else:
                accounts.append(account)
        if c.account not in accounts:
            accounts.append(c.account)
        if not len(errors):
            c.dataset.managers = accounts
            c.dataset.updated_at = datetime.utcnow()
            db.session.commit()
            h.flash_success(_("The team has been updated."))
        return self.team_edit(dataset, errors=errors, accounts=accounts)

    def drop(self, dataset):
        self._get_dataset(dataset)
        require.dataset.update(c.dataset)
        c.dataset.updated_at = datetime.utcnow()
        c.dataset.drop()
        solr.drop_index(c.dataset.name)
        c.dataset.init()
        c.dataset.generate()
        AggregationCache(c.dataset).invalidate()

        # For every source in the dataset we set the status to removed
        for source in c.dataset.sources:
            for run in source.runs:
                run.status = Run.STATUS_REMOVED

        db.session.commit()
        h.flash_success(_("The dataset has been cleared."))
        redirect(h.url_for(controller='editor', action='index',
                           dataset=c.dataset.name))

    def publish(self, dataset):
        self._get_dataset(dataset)
        require.dataset.update(c.dataset)
        if not c.dataset.private:
            abort(400, _("This dataset is already public!"))
        c.dataset.private = False
        c.dataset.updated_at = datetime.utcnow()
        db.session.commit()

        # Need to invalidate the cache of the dataset index
        cache = DatasetIndexCache()
        cache.invalidate()

        public_url = h.url_for(controller='dataset', action='view',
                               dataset=c.dataset.name, qualified=True)
        h.flash_success(
            _("Congratulations, the dataset has been "
              "published. It is now available at: %s") % public_url)
        redirect(h.url_for(controller='editor', action='index',
                           dataset=c.dataset.name))

    def retract(self, dataset):
        self._get_dataset(dataset)
        require.dataset.update(c.dataset)
        if c.dataset.private:
            abort(400, _("This dataset is already private!"))
        c.dataset.private = True
        c.dataset.updated_at = datetime.utcnow()
        AggregationCache(c.dataset).invalidate()
        db.session.commit()

        # Need to invalidate the cache of the dataset index
        cache = DatasetIndexCache()
        cache.invalidate()

        h.flash_success(_("The dataset has been retracted. "
                          "It is no longer visible to others."))
        redirect(h.url_for(controller='editor', action='index',
                           dataset=c.dataset.name))

    def delete(self, dataset):
        self._get_dataset(dataset)
        require.dataset.delete(c.dataset)
        c.dataset.drop()
        solr.drop_index(c.dataset.name)
        db.session.delete(c.dataset)
        db.session.commit()
        h.flash_success(_("The dataset has been deleted."))
        redirect(h.url_for(controller='dataset', action='index'))

########NEW FILE########
__FILENAME__ = entry
import logging

from pylons import request, response, tmpl_context as c
from pylons.controllers.util import abort, redirect
from pylons.i18n import _

from openspending.ui.lib.base import BaseController
from openspending.ui.lib.views import handle_request
from openspending.ui.lib.hypermedia import entry_apply_links
from openspending.lib.browser import Browser
from openspending.lib.csvexport import write_csv
from openspending.lib.jsonexport import to_jsonp
from openspending.ui.lib import helpers as h
from openspending.ui.alttemplates import templating
from openspending.lib.paramparser import EntryIndexParamParser

from solr import SolrException

log = logging.getLogger(__name__)


class EntryController(BaseController):

    def index(self, dataset, format='html'):
        # Get the dataset into the context variable 'c'
        self._get_dataset(dataset)

        # If the format is either json or csv we direct the user to the search
        # API instead
        if format in ['json', 'csv']:
            return redirect(h.url_for(controller='api/version2',
                                      action='search',
                                      format=format, dataset=dataset,
                                      **request.params))

        # Get the default view
        handle_request(request, c, c.dataset)

        # Parse the parameters using the SearchParamParser (used by the API)
        parser = EntryIndexParamParser(request.params)
        params, errors = parser.parse()

        # We have to remove page from the parameters because that's also
        # used in the Solr browser (which fetches the queries)
        params.pop('page')

        # We limit ourselve to only our dataset
        params['filter']['dataset'] = [c.dataset.name]
        facet_dimensions = {field.name: field
                            for field in c.dataset.dimensions
                            if field.facet}
        params['facet_field'] = facet_dimensions.keys()

        # Create a Solr browser and execute it
        b = Browser(**params)
        try:
            b.execute()
        except SolrException as e:
            return {'errors': [unicode(e)]}

        # Get the entries, each item is a tuple of (dataset, entry)
        solr_entries = b.get_entries()
        # We are only interested in the entry in the tuple since  we know
        # the dataset
        entries = [entry[1] for entry in solr_entries]

        # Get expanded facets for this dataset,
        c.facets = b.get_expanded_facets(c.dataset)

        # Create a pager for the entries
        c.entries = templating.Page(entries, **request.params)

        # Set the search word and default to empty string
        c.search = params.get('q', '')

        # Set filters (but remove the dataset as we don't need it)
        c.filters = params['filter']
        del c.filters['dataset']

        # We also make the facet dimensions and dimension names available
        c.facet_dimensions = facet_dimensions
        c.dimensions = [dimension.name for dimension in c.dataset.dimensions]

        # Render the entries page
        return templating.render('entry/index.html')

    def view(self, dataset, id, format='html'):
        """
        Get a specific entry in the dataset, identified by the id. Entry
        can be return as html (default), json or csv.
        """

        # Generate the dataset
        self._get_dataset(dataset)
        # Get the entry that matches the given id. c.dataset.entries is
        # a generator so we create a list from it's responses based on the
        # given constraint
        entries = list(c.dataset.entries(c.dataset.alias.c.id == id))
        # Since we're trying to get a single entry the list should only
        # contain one entry, if not then we return an error
        if not len(entries) == 1:
            abort(404, _('Sorry, there is no entry %r') % id)
        # Add urls to the dataset and assign assign it as a context variable
        c.entry = entry_apply_links(dataset, entries.pop())

        # Get and set some context variables from the entry
        # This shouldn't really be necessary but it's here so nothing gets
        # broken
        c.id = c.entry.get('id')
        c.from_ = c.entry.get('from')
        c.to = c.entry.get('to')
        c.currency = c.entry.get('currency', c.dataset.currency).upper()
        c.time = c.entry.get('time')

        # Get the amount for the entry
        amount = c.entry.get('amount')
        # We adjust for inflation if the user as asked for this to be inflated
        if 'inflate' in request.params:
            try:
                # Inflate the amount. Target date is provided in request.params
                # as value for inflate and reference date is the date of the
                # entry. We also provide a list of the territories to extract
                # a single country for which to do the inflation
                c.inflation = h.inflate(amount, request.params['inflate'],
                                        c.time, c.dataset.territories)

                # The amount to show should be the inflated amount
                # and overwrite the entry's amount as well
                c.amount = c.inflation['inflated']
                c.entry['amount'] = c.inflation['inflated']

                # We include the inflation response in the entry's dict
                # HTML description assumes every dict value for the entry
                # includes a label so we include a default "Inflation
                # adjustment" for it to work.
                c.inflation['label'] = 'Inflation adjustment'
                c.entry['inflation_adjustment'] = c.inflation
            except:
                # If anything goes wrong in the try clause (and there's a lot
                # that can go wrong). We just say that we can't adjust for
                # inflation and set the context amount as the original amount
                h.flash_notice(_('Unable to adjust for inflation'))
                c.amount = amount
        else:
            # If we haven't been asked to inflate then we just use the
            # original amount
            c.amount = amount

        # Add custom html for the dataset entry if the dataset has some
        # custom html
        # 2013-11-17 disabled this as part of removal of genshi as depended on
        # a genshi specific helper.
        # TODO: reinstate if important
        # c.custom_html = h.render_entry_custom_html(c.dataset, c.entry)

        # Add the rest of the dimensions relating to this entry into a
        # extras dictionary. We first need to exclude all dimensions that
        # are already shown and then we can loop through the dimensions
        excluded_keys = ('time', 'amount', 'currency', 'from',
                         'to', 'dataset', 'id', 'name', 'description')

        c.extras = {}
        if c.dataset:
            # Create a dictionary of the dataset dimensions
            c.desc = dict([(d.name, d) for d in c.dataset.dimensions])
            # Loop through dimensions of the entry
            for key in c.entry:
                # Entry dimension must be a dataset dimension and not in
                # the predefined excluded keys
                if key in c.desc and \
                        key not in excluded_keys:
                    c.extras[key] = c.entry[key]

        # Return entry based on
        if format == 'json':
            return to_jsonp(c.entry)
        elif format == 'csv':
            return write_csv([c.entry], response)
        else:
            return templating.render('entry/view.html')

    def search(self):
        c.content_section = 'search'
        return templating.render('entry/search.html')

########NEW FILE########
__FILENAME__ = error
import cgi

from paste.urlparser import PkgResourcesParser
from pylons import request, response, tmpl_context as c
from pylons.controllers.util import forward, abort
from pylons.middleware import error_document_template
from webhelpers.html.builder import literal

from openspending.ui.lib.base import BaseController
from openspending.ui.alttemplates import templating


class ErrorController(BaseController):

    """Generates error documents as and when they are required.

    The ErrorDocuments middleware forwards to ErrorController when error
    related status codes are returned from the application.

    This behaviour can be altered by changing the parameters to the
    ErrorDocuments middleware in your config/middleware.py file.

    """

    rendered_error_codes = ("404", "403", "400", "500")

    def document(self):
        """Render the error document - show custom template for 404"""
        self._disable_cache()
        resp = request.environ.get('pylons.original_response')

        # Don't do fancy error documents for JSON
        if resp.headers['Content-Type'] in ['text/javascript',
                                            'application/json', 'text/csv']:
            response.headers['Content-Type'] = resp.headers['Content-Type']
            return resp.body

        code = cgi.escape(request.GET.get('code', str(resp.status_int)))
        content = (literal(resp.body) or
                   cgi.escape(request.GET.get('message', '')))

        if code in self.rendered_error_codes:
            c.code = code
            message = content
            message = message.split('</h1>', 1)[-1]
            message = message.split('</body>', 1)[0]
            c.message = message.split('\n', 2)[-1]
            return templating.render('errors/%s.html' % code)
        else:
            page = error_document_template % \
                dict(prefix=request.environ.get('SCRIPT_NAME', ''),
                     code=code,
                     message=content)
            return page

    def render(self, code):
        if code in self.rendered_error_codes:
            c.code = code
            c.message = code
            return templating.render('errors/%s.html' % code)
        abort(404)

    def img(self, id):
        """Serve Pylons' stock images"""
        return self._serve_file('/'.join(['media/img', id]))

    def style(self, id):
        """Serve Pylons' stock stylesheets"""
        return self._serve_file('/'.join(['media/style', id]))

    def _serve_file(self, path):
        """Call Paste's FileApp (a WSGI application) to serve the file
        at the specified path
        """
        request.environ['PATH_INFO'] = '/%s' % path
        return forward(PkgResourcesParser('pylons', 'pylons'))

########NEW FILE########
__FILENAME__ = error_test
"""
ErrorTestController is used solely in functional testing of the custom error
documents.
"""
from openspending.ui.lib.base import BaseController, abort


class ErrorTestController(BaseController):

    def not_found(self):
        abort(404, "Custom 404 error message")

    def not_authorised(self):
        abort(403, "Custom 403 error message")

    def server_error(self):
        abort(500, "Custom 500 error message")

########NEW FILE########
__FILENAME__ = home
import logging
import os
import subprocess

from pylons import request, tmpl_context as c
from pylons.controllers.util import redirect
from pylons.i18n import _

from openspending.model.dataset import Dataset, DatasetTerritory
from openspending.lib.solr_util import dataset_entries
from openspending.ui.i18n import set_session_locale
from openspending.ui.lib.base import BaseController
from openspending.ui.lib.helpers import flash_success
from openspending.ui.alttemplates import templating

log = logging.getLogger(__name__)


class HomeController(BaseController):

    def index(self):
        # Get all of the datasets available to the account of the logged in
        # or an anonymous user (if c.account is None)
        c.datasets = Dataset.all_by_account(c.account)
        c.territories = DatasetTerritory.dataset_counts(c.datasets)

        c.num_entries = dataset_entries(None)
        return templating.render('home/index.html')

    def set_locale(self):
        locale = request.params.get('locale')
        if locale is not None:
            set_session_locale(locale)

    def version(self):
        cwd = os.path.dirname(__file__)
        process = subprocess.Popen('git rev-parse --verify HEAD'.split(' '),
                                   cwd=cwd,
                                   stdout=subprocess.PIPE)
        output = process.communicate()[0]
        if process.returncode == 0:
            return output
        else:
            from openspending._version import __version__
            return __version__

    def favicon(self):
        return redirect('/static/img/favicon.ico', code=301)

    def ping(self):
        from openspending.tasks.generic import ping
        ping.delay()
        flash_success(_("Sent ping!"))
        redirect('/')

########NEW FILE########
__FILENAME__ = rest
import logging

from pylons import tmpl_context as c, url

from openspending.model.dataset import Dataset
from openspending.model import meta as db
from openspending.ui.lib.base import BaseController
from openspending.ui.alttemplates import templating

log = logging.getLogger(__name__)


class RestController(BaseController):

    def index(self):
        dataset = db.session.query(Dataset).filter_by(private=False).first()
        entry = list(dataset.entries(limit=1)).pop()
        c.urls = [
            url(controller='dataset', action='view', dataset=dataset.name,
                format='json'),
            url(controller='entry', action='view', dataset=dataset.name,
                id=entry['id'], format='json')]

        return templating.render('home/rest.html')

########NEW FILE########
__FILENAME__ = run
import logging

from pylons import tmpl_context as c
from pylons.i18n import _

from openspending.model.source import Source
from openspending.model.run import Run
from openspending.model.log_record import LogRecord

from openspending.ui.lib.base import BaseController
from openspending.ui.lib.base import abort, require
from openspending.ui.alttemplates import templating

log = logging.getLogger(__name__)


class RunController(BaseController):

    def _get_run(self, dataset, source, id):
        self._get_dataset(dataset)
        require.dataset.update(c.dataset)
        c.source = Source.by_id(source)
        if c.source is None or c.source.dataset != c.dataset:
            abort(404, _("There is no source '%s'") % source)
        c.run = Run.by_id(id)
        if c.run is None or c.run.source != c.source:
            abort(404, _("There is no run '%s'") % id)

    def view(self, dataset, source, id, format='html'):
        self._get_run(dataset, source, id)
        system = c.run.records.filter_by(category=LogRecord.CATEGORY_SYSTEM)
        c.num_system = system.count()
        c.system_page = templating.Page(
            system.order_by(LogRecord.timestamp.asc()),
            page=self._get_page('system_page'),
            items_per_page=10)
        data = c.run.records.filter_by(category=LogRecord.CATEGORY_DATA)
        c.num_data = data.count()
        c.data_page = templating.Page(data.order_by(LogRecord.timestamp.asc()),
                                      page=self._get_page('data_page'),
                                      items_per_page=20)
        return templating.render('run/view.html')

########NEW FILE########
__FILENAME__ = source
import logging

from pylons import request, tmpl_context as c
from pylons.controllers.util import redirect
from paste.deploy.converters import asbool
from pylons.i18n import _
from colander import Invalid

from openspending.model import meta as db
from openspending.model.source import Source

from openspending.lib.jsonexport import to_jsonp
from openspending.ui.lib import helpers as h
from openspending.ui.lib.base import BaseController
from openspending.ui.lib.base import abort, require
from openspending.tasks.dataset import analyze_source, load_source
from openspending.ui.alttemplates import templating

from openspending.ui.validation.source import source_schema

log = logging.getLogger(__name__)


class SourceController(BaseController):

    def new(self, dataset, errors={}):
        self._get_dataset(dataset)
        require.dataset.update(c.dataset)
        params_dict = dict(request.params) if errors else {}
        return templating.render('source/new.html', form_errors=errors,
                                 form_fill=params_dict)

    def create(self, dataset):
        self._get_dataset(dataset)
        require.dataset.update(c.dataset)
        try:
            schema = source_schema()
            data = schema.deserialize(request.params)
            source = Source(c.dataset, c.account, data['url'])
            db.session.add(source)
            db.session.commit()
            analyze_source.apply_async(args=[source.id], countdown=2)
            h.flash_success(_("The source has been created."))
            redirect(h.url_for(controller='editor', action='index',
                               dataset=c.dataset.name))
        except Invalid as i:
            errors = i.asdict()
            errors = [(k[len('source.'):], v) for k, v in errors.items()]
            return self.new(dataset, dict(errors))

    def index(self, dataset, format='json'):
        self._get_dataset(dataset)
        return to_jsonp([src.as_dict() for src in c.dataset.sources])

    def _get_source(self, dataset, id):
        self._get_dataset(dataset)
        c.source = Source.by_id(id)
        if c.source is None or c.source.dataset != c.dataset:
            abort(404, _("There is no source '%s'") % id)

    def view(self, dataset, id):
        self._get_source(dataset, id)
        redirect(c.source.url)

    def load(self, dataset, id):
        """
        Load the dataset into the database. If a url parameter 'sample'
        is provided then its value is converted into a boolean. If the value
        equals true we only perform a sample run, else we do a full load.
        """

        # Get our source (and dataset)
        self._get_source(dataset, id)

        # We require that the user can update the dataset
        require.dataset.update(c.dataset)

        # If the source is already running we flash an error declaring that
        # we're already running this source
        if c.source.is_running:
            h.flash_error(_("Already running!"))
        # If the source isn't already running we try to load it (or sample it)
        else:
            try:
                sample = asbool(request.params.get('sample', 'false'))
                load_source.delay(c.source.id, sample)
                # Let the user know we're loading the source
                h.flash_success(_("Now loading..."))
            except Exception as e:
                abort(400, e)

        # Send the user to the editor index page for this dataset
        redirect(h.url_for(controller='editor', action='index',
                           dataset=c.dataset.name))

    def delete(self, dataset, id):
        # Get our source (and dataset)
        self._get_source(dataset, id)

        # We require that the user can update the dataset
        require.dataset.update(c.dataset)

        # Delete the source if hasn't been sucessfully loaded
        # If it is successfully loaded we don't return an error
        # message because the user is then going around the normal
        # user interface
        if not c.source.successfully_loaded:
            db.session.delete(c.source)
            db.session.commit()

        redirect(h.url_for(controller='editor', action='index',
                           dataset=c.dataset.name))

    def analysis(self, dataset, source, format='json'):
        self._get_source(dataset, source)
        return to_jsonp(c.source.analysis)

########NEW FILE########
__FILENAME__ = view
import logging

import colander

from pylons import request, tmpl_context as c
from pylons.controllers.util import redirect, abort
from pylons.i18n import _

from openspending.model import meta as db
from openspending.model.view import View
from openspending.ui.lib import helpers as h, widgets
from openspending.lib import json
from openspending.ui.lib.views import handle_request
from openspending.ui.lib.base import BaseController, require
from openspending import auth as can
from openspending.lib.jsonexport import to_jsonp
from openspending.ui.alttemplates import templating

log = logging.getLogger(__name__)


class JSONSchemaType(colander.SchemaType):

    def serialize(self, node, appstruct):
        return json.dumps(appstruct)

    def deserialize(self, node, cstruct):
        try:
            return json.loads(cstruct)
        except Exception as exc:
            raise colander.Invalid(node, unicode(exc))


def valid_widget_name(widget):
    if widget in widgets.list_widgets():
        return True
    return _("Invalid widget type: %r") % widget


class CreateView(colander.MappingSchema):
    label = colander.SchemaNode(colander.String())
    widget = colander.SchemaNode(
        colander.String(),
        validator=colander.Function(valid_widget_name))
    description = colander.SchemaNode(colander.String(),
                                      missing=None)
    state = colander.SchemaNode(JSONSchemaType())


def make_name(dataset, label):
    from openspending.lib.util import slugify
    from itertools import count
    name = name_orig = slugify(label)
    view = View.by_name(dataset, name)
    for i in count():
        if view is None:
            return name
        name = name_orig + str(i)
        view = View.by_name(dataset, name)


class ViewController(BaseController):

    def _get_named_view(self, dataset, name):
        self._get_dataset(dataset)
        c.named_view = View.by_name(c.dataset, name)
        if c.named_view is None:
            abort(404, _('Sorry, there is no view %r') % name)
        require.view.read(c.dataset, c.named_view)

    def index(self, dataset, format='html'):
        self._get_dataset(dataset)
        handle_request(request, c, c.dataset)
        c.views = View.all_by_dataset(c.dataset)
        if format == 'json':
            return to_jsonp([v.as_dict() for v in c.views])
        else:
            return templating.render('view/index.html')

    def new(self, dataset, errors={}):
        self._get_dataset(dataset)
        self._disable_cache()
        handle_request(request, c, c.dataset)
        c.widgets = dict([(n, widgets.get_widget(n))
                          for n in widgets.list_widgets()])
        if 'dev_widget' in request.params and \
                request.params.get('dev_widget') not in widgets.list_widgets():
            n = request.params.get('dev_widget')
            c.widgets[n] = widgets.get_widget(n, force=True)
        c.errors = errors
        c.can_save = can.view.create(c.dataset)
        return templating.render('view/new.html')

    def delete(self, dataset, name):
        self._get_named_view(dataset, name)
        if not can.view.delete(c.dataset, c.named_view):
            abort(403, _("You are not authorized to delete this view."))
        h.flash_success(_("'%s' has been deleted.") % c.named_view.label)
        db.session.delete(c.named_view)
        db.session.commit()
        return redirect(h.url_for(controller='view',
                                  action='index', dataset=c.dataset.name))

    def create(self, dataset):
        self._get_dataset(dataset)
        require.view.create(c.dataset)
        handle_request(request, c, c.dataset)
        try:
            data = CreateView().deserialize(request.params)
            view = View()
            view.dataset = c.dataset
            view.account = c.account
            view.widget = data['widget']
            view.state = data['state']
            view.name = make_name(c.dataset, data['label'])
            view.label = data['label']
            view.description = data['description']
            view.public = True
            db.session.add(view)
            db.session.commit()
            redirect(h.url_for(controller='view', action='view',
                               dataset=c.dataset.name, name=view.name))
        except colander.Invalid as inv:
            return self.new(dataset, errors=inv.asdict())

    def update(self, dataset, name):
        """
        Update dataset. Does nothing at the moment.
        """
        # Get the dataset for the view
        self._get_dataset(dataset)

        # Get the named view
        view = View.by_name(c.dataset, name)
        # User must be allowed to update the named view
        require.view.update(c.dataset, view)

        # Possible update values
        # We don't update the view's name because it might have been embedded
        view.label = request.params.get('label', view.label)
        try:
            # Try to load the state
            view.state = json.loads(request.params['state'])
        except:
            pass
        view.description = request.params.get('description', view.description)

        # Commit the changes
        db.session.commit()

        # Redirect to the view page for this view
        redirect(h.url_for(controller='view', action='view',
                           dataset=c.dataset.name, name=view.name))

    def view(self, dataset, name, format='html'):
        self._get_named_view(dataset, name)
        handle_request(request, c, c.dataset)
        c.widget = widgets.get_widget(c.named_view.widget)
        if format == 'json':
            return to_jsonp(c.named_view.as_dict())
        else:
            return templating.render('view/view.html')

    def embed(self, dataset):
        self._get_dataset(dataset)
        c.widget = request.params.get('widget')
        if c.widget is None:
            abort(400, _("No widget type has been specified."))
        try:
            c.widget = widgets.get_widget(c.widget)
            c.state = json.loads(request.params.get('state', '{}'))
        except ValueError as ve:
            abort(400, unicode(ve))
        return templating.render('view/embed.html')

########NEW FILE########
__FILENAME__ = aggregation
from openspending.ui.lib import helpers


def get_value(key, item):
    """
    Get value of key in an item dict where it can be nested with . as a
    separator. This should really be solved using a specific data structure
    common to all aggregation results
    """

    # Split the key to get all possible keys and subkeys
    keys = key.split('.')

    # Loop through and replace item with the result of the key
    for split_key in keys:
        item = item[split_key]

    # Item now contains the the value of key/subkey in item
    return item


def remove_excessive_time(keys, item):
    """
    Checks for time.year in keys and if not present deletes either time.year
    or time (if no time variable is present)
    """

    try:
        # First we check if there is no time at all in the keys and delete
        # time from item (and break out early since the next text would raise
        # a KeyError
        if 'time' not in [k.split('.')[0] for k in keys]:
            del item['time']
            return item

        # Delete only year if time.year is not in keys (this happens in the
        # rare case that a user drills down into time.month to compare months
        # and not years (or something similar)
        if 'time.year' not in keys:
            del item['time']['year']
    except KeyError:
        # We do nothing if a key error is raised (then time or its subitem
        # year didn't exist
        pass

    return item


def aggregate(dataset, measures=['amount'], drilldowns=None, cuts=None,
              page=1, pagesize=10000, order=None, inflate=None):
    """
    Do an aggregation over a single dataset. This method calls the aggregation
    method of the dataset but in case inflation is asked for, this method takes
    care of those computations
    """

    # If we have to inflate we need to add a year to the drilldowns
    # (since inflation only supports years at the moment).
    if inflate:
        drilldowns.append('time.year')

    # Aggregate the dataset via its own aggregate function
    result = dataset.aggregate(measures=measures, drilldowns=drilldowns,
                               cuts=cuts, page=page, pagesize=pagesize,
                               order=order)

    # If we have to inflate we do some inflation calculations
    if inflate:
        try:
            # Since different parts of the drilldowns are going to change
            # (differently) we need to recompute the sumamry total
            summary_total = 0

            # We collect adjusted drilldowns into a new dict since if
            # inflation fails we can still serve the uninflated content
            adjusted_drilldowns = {}
            # We remove the time.year we added to the drilldowns
            hash_key = drilldowns[:]
            hash_key.remove('time.year')

            for item in result['drilldown']:
                # Get the inflated amount for this year (returns an inflation
                # dictionary with values for reference and target dates along
                # with original and inflated amounts)
                adjustment = helpers.inflate(item['amount'],
                                             inflate, item['time'],
                                             dataset.territories)

                # We make the reference and target datestrings with ISO format
                # (ISO format is yyyy-mm-dd).
                adjustment['reference'] = adjustment['reference'].isoformat()
                adjustment['target'] = adjustment['target'].isoformat()

                # Get the inflated amount into its own variable
                inflated_amount = adjustment['inflated']

                # Get the item key
                item_key = unicode([get_value(k, item) for k in hash_key])
                if item_key not in adjusted_drilldowns:
                    # We copy the item in case something happens (then we
                    # catch it and serve the original aggregation result)
                    adjusted_drilldown = item.copy()
                    remove_excessive_time(hash_key, adjusted_drilldown)
                    adjusted_drilldown['inflation_adjustment'] = [adjustment]
                    adjusted_drilldown['amount'] = inflated_amount
                    adjusted_drilldowns[item_key] = adjusted_drilldown
                else:
                    adjusted_drilldowns[item_key]['inflation_adjustment'] \
                        .append(adjustment)
                    adjusted_drilldowns[item_key]['amount'] += inflated_amount

                summary_total += inflated_amount

            result['drilldown'] = adjusted_drilldowns.values()
            result['summary']['original'] = result['summary']['amount']
            result['summary']['amount'] = summary_total
        except KeyError as error:
            # If inflation fails because a date wasn't found in the inflation
            # data, then it raises a KeyError (returning the date in the
            # exception message

            # Note we do not remove time.year from the drilldown here since
            # it can help resolve the warning (and it's bothersome)
            result['warning'] = {
                'inflation': 'Unable to do inflation adjustment',
                'error': 'Inflation error: %s' % error}

    return result

########NEW FILE########
__FILENAME__ = app_globals
from pylons import config
from paste.deploy.converters import asbool
from economics import Inflation


class Globals(object):

    """\
    Globals acts as a container for objects available throughout the
    life of the application

    One instance of Globals is created during application
    initialization and is available during requests via the
    'app_globals' variable
    """

    def __init__(self):
        self.debug = asbool(config.get('debug', False))

        self.site_title = config.get(
            'openspending.site_title',
            'OpenSpending'
        )
        self.cache_enabled = asbool(config.get(
            'openspending.cache_enabled',
            True
        ))

        self.script_root = config.get('openspending.script_root', '/static/js')

        if asbool(config.get('openspending.fake_inflation', False)):
            from .fake_inflation import Inflation as FakeInflation
            self.inflation = FakeInflation()
        else:
            self.inflation = Inflation()

########NEW FILE########
__FILENAME__ = authenticator
from zope.interface import implements
from repoze.who.interfaces import IAuthenticator, IIdentifier
from paste.httpheaders import AUTHORIZATION

from openspending.model.account import Account
from openspending.ui.lib.security import check_password_hash

import logging
log = logging.getLogger(__name__)


class UsernamePasswordAuthenticator(object):
    implements(IAuthenticator)

    def authenticate(self, environ, identity):
        if 'login' not in identity or 'password' not in identity:
            return None
        account = Account.by_name(identity['login'])
        if account is None:
            return None
        if account.password is None:
            return None
        if check_password_hash(account.password, identity['password']):
            return account.name
        return None


class ApiKeyIdentifier(object):
    implements(IIdentifier)

    def identify(self, environ):
        """
        Try to identify user based on api key authorization in header
        """

        # Get the authorization header as passed through paster
        authorization = AUTHORIZATION(environ)
        log.debug(authorization)
        # Split the authorization header value by whitespace
        try:
            method, auth = authorization.split(' ', 1)
        except ValueError:
            # not enough values to unpack
            return None

        # If authentication method is apikey we return the identity
        if method.lower() == 'apikey':
            return {'apikey': auth.strip()}

        # Return None if we get here (identity not found)
        return None

    def remember(self, environ, identity):
        """
        API key authentication headers user can use to make the system
        remember the user
        """

        # User cannot ask to be remembered since API key is the authentication
        # mechanism and must be used on every request (thus, no headers)
        return None

    def forget(self, environ, identity):
        """
        API key authentication headers user can use to make the system
        forget the user
        """

        # User cannot be remembered so there is no need to forget the user
        # either (and thus no header mechanism)
        return None


class ApiKeyAuthenticator(object):
    implements(IAuthenticator)

    def authenticate(self, environ, identity):
        """
        Try to authenticate user based on api key identity
        """

        # If identity has apikey we get the account by the api key
        # and return none if no account or apikey is found is found
        if 'apikey' in identity:
            acc = Account.by_api_key(identity.get('apikey'))
            if acc is not None:
                return acc.name

        return None

########NEW FILE########
__FILENAME__ = base
"""The base Controller API

Provides the BaseController class for subclassing.
"""
from time import time
import hashlib

from pylons.controllers import WSGIController
from pylons.controllers.util import etag_cache
from pylons import tmpl_context as c, request, response
from pylons import app_globals, session
from pylons.controllers.util import abort
from pylons.i18n import _

from openspending.model import meta as db
from openspending.model.account import Account
from openspending.model.dataset import Dataset
from openspending.auth import require
from openspending.ui import i18n

import logging
log = logging.getLogger(__name__)

ACCEPT_MIMETYPES = {
    "application/json": "json",
    "text/javascript": "json",
    "application/javascript": "json",
    "text/csv": "csv"
}


def etag_cache_keygen(*a):
    """
    Generate ETag key for the cache.
    This automatically includes the username taken from the session cookie
    with the help of pylons
    """
    # Get the account name (authentication in pylons sets it to the
    # environment variable REMOTE_USER)
    account_name = request.environ.get('REMOTE_USER', None)
    etag = hashlib.sha1(repr(a) + repr(account_name)).hexdigest()
    etag_cache(etag)


def set_vary_header():
    """
    Set the vary header to force intermediate caching to be controlled by
    different request headers. This only sets Cookie as the value of the Vary
    header (because of user credentials in top navigation bar) but it could
    also include other request headers like Accept-Language if the site should
    serve different locales based on request headers.
    """
    # Set the vary header as Cookie
    response.vary = ['Cookie']

    # If ETag hasn't been generated we generate it
    if not response.etag:
        etag_cache_keygen()


class BaseController(WSGIController):

    def __call__(self, environ, start_response):
        """Invoke the Controller"""
        # WSGIController.__call__ dispatches to the Controller method
        # the request is routed to. This routing information is
        # available in environ['pylons.routes_dict']
        begin = time()
        try:
            return WSGIController.__call__(self, environ, start_response)
        finally:
            db.session.remove()
            db.session.close()
            log.debug("Request to %s took %sms" %
                      (request.path, int((time() - begin) * 1000)))

    def __before__(self, action, **params):
        account_name = request.environ.get('REMOTE_USER', None)
        if account_name:
            c.account = Account.by_name(account_name)
        else:
            c.account = None

        i18n.handle_request(request, c)

        c._cache_disabled = False
        c._must_revalidate = False
        c.content_section = c.dataset = None

        c.detected_l10n_languages = i18n.get_language_pairs()

    def __after__(self):
        db.session.close()
        response.pragma = None

        if not app_globals.cache_enabled or 'flash' in session._session():
            return

        if c._cache_disabled:
            return

        del response.cache_control.no_cache
        if len(session._session().keys()) == 2 and \
                not len(request.cookies.keys()):
            session._current_obj().__dict__['_sess'] = None
            response.cache_control.public = True
        else:
            response.cache_control.private = True

        # Set vary header (this will set Cookie as a value for the vary header
        # so different content can be served to logged in users
        set_vary_header()

        response.cache_control.must_revalidate = c._must_revalidate
        if not c._must_revalidate:
            response.cache_control.max_age = 3600 * 6

    def _detect_format(self, format):
        for mimetype, mimeformat in self.accept_mimetypes.items():
            if format == mimeformat or mimetype \
                    in request.headers.get("Accept", ""):
                return mimeformat
        return "html"

    def _disable_cache(self):
        c._cache_disabled = True

    def _get_dataset(self, dataset):
        c.dataset = Dataset.by_name(dataset)
        if c.dataset is None:
            abort(404, _('Sorry, there is no dataset named %r') % dataset)
        require.dataset.read(c.dataset)

    def _get_page(self, param='page'):
        try:
            return int(request.params.get(param))
        except:
            return 1

########NEW FILE########
__FILENAME__ = cache
import hashlib
import logging

from pylons import cache, app_globals
from openspending.ui.lib.aggregation import aggregate
from openspending.ui.lib.indices import dataset_index, language_index, \
    territory_index, category_index

log = logging.getLogger(__name__)


class DatasetIndexCache(object):

    """
    A proxy object to run cached calls against the dataset
    index (dataset index page and dataset.json)
    """

    def __init__(self, type='dbm'):
        """
        Initialise a dataset index cache
        """
        self.cache_enabled = app_globals.cache_enabled
        self.cache = cache.get_cache('DATASET_INDEX_CACHE',
                                     type=type)

    def invalidate(self):
        """
        Clear the cache. This should be called whenever the index changes.
        """
        self.cache.clear()

    def index(self, languages=[], territories=[], category=None):
        """
        Get an index of all public datasets, preferably served via
        the cache. Returns a DatasetIndices tuple if cache is hit.
        """

        # If caching is enabled we try to fetch this from the cache
        if self.cache_enabled:
            # Key parts of the request, used to compute the cache key
            key_parts = (category, sorted(languages),
                         sorted(territories))

            # Return a hash of the key parts as the cache key
            key = hashlib.sha1(repr(key_parts)).hexdigest()

            # If the cache key exists we serve directly from the cache
            if key in self.cache:
                log.debug("Index cache hit: %s", key)
                return self.cache.get(key)
        else:
            # Debug message to show caching is disabled
            log.debug("Caching is disabled.")

        # Get all of the datasets
        log.debug(languages)
        datasets = dataset_index(languages, territories, category)

        # Count territories, languages, and categories
        territories = territory_index(datasets)
        languages = language_index(datasets)
        categories = category_index(datasets)

        # Create a results dictionary. We need to transform the datasets
        # into dict for the caching (since the datasets are returned as
        # classes with functions that cannot be cached).
        results = {'datasets': map(lambda d: d.as_dict(), datasets),
                   'languages': languages, 'territories': territories,
                   'categories': categories}

        # Cache the results if we have caching enabled and then return
        # the results
        if self.cache_enabled:
            self.cache.put(key, results)

        return results


class AggregationCache(object):

    """ A proxy object to run cached calls against the dataset
    aggregation function. This is neither a concern of the data
    model itself, nor should it be repeated at each location
    where caching of aggregates should occur - thus it ends up
    here. """

    def __init__(self, dataset, type='dbm'):
        self.dataset = dataset
        self.cache_enabled = app_globals.cache_enabled and \
            not self.dataset.private
        self.cache = cache.get_cache('DSCACHE_' + dataset.name,
                                     type=type)

    def aggregate(self, measures=['amount'], drilldowns=None, cuts=None,
                  page=1, pagesize=10000, order=None, inflate=None):
        """ For call docs, see ``model.Dataset.aggregate``. """

        # Initialise cache key
        key = None
        # If caching is enabled we try to fetch this from the cache
        if self.cache_enabled:
            # Key parts of the request, used to compute the cache key
            key_parts = (self.dataset.updated_at.isoformat(),
                         sorted(measures),
                         sorted(drilldowns or []),
                         sorted(cuts or []),
                         order, page, pagesize, inflate)
            key = hashlib.sha1(repr(key_parts)).hexdigest()

            # If the cache key exists we serve directly from the cache
            if key in self.cache:
                log.debug("Cache hit: %s", key)
                return self.cache.get(key)
        else:
            # Debug message to show caching is disabled
            log.debug("Caching is disabled.")

        # If it didn't exist in the cache we perform the aggregation and
        # store it in the cache with the given cache key
        result = aggregate(self.dataset, measures=measures,
                           drilldowns=drilldowns, cuts=cuts,
                           page=page, pagesize=pagesize,
                           order=order, inflate=inflate)

        # If caching is enabled we add the result to the cache
        if self.cache_enabled:
            log.debug("Generating key: %s", key)
            result['summary']['cached'] = True
            result['summary']['cache_key'] = key
            self.cache.put(key, result)

        return result

    def invalidate(self):
        """ Clear the cache. """
        self.cache.clear()

########NEW FILE########
__FILENAME__ = fake_inflation
import collections

InflationResult = collections.namedtuple('Inflation', 'factor value')


class Inflation(object):

    def __init__(self, source=None, reference=None, country=None):
        pass

    def get(self, target=None, reference=None, country=None):
        return InflationResult(factor=1.0, value=0.0)

    def inflate(self, amount, target=None, reference=None, country=None):
        return amount

########NEW FILE########
__FILENAME__ = helpers
# -*- coding: utf-8 -*-
"""Helper functions

Consists of functions to typically be used within templates, but also
available to Controllers. This module is available to templates as 'h'.
"""

from pylons import config, tmpl_context, app_globals
from routes import url_for as routes_url_for
from lxml import html
from webhelpers.html import literal
from webhelpers.html.tags import link_to
from webhelpers.markdown import markdown as _markdown
from webhelpers.pylonslib import Flash as _Flash
from webhelpers.text import truncate

from openspending.reference import country

import math
import os
import uuid
import json
import hashlib
import datetime
import babel.numbers


def markdown(*args, **kwargs):
    return literal(_markdown(*args, **kwargs))


def markdown_preview(text, length=150):
    if not text:
        return ''
    try:
        md = html.fromstring(unicode(markdown(text)))
        text = md.text_content()
    except:
        pass
    if length:
        text = truncate(text, length=length, whole_word=True)
    return text.replace('\n', ' ')


_flash = _Flash()


def flash_success(message):
    _flash(message, category='success')


def flash_error(message):
    _flash(message, category='error')


def flash_notice(message):
    _flash(message, category='notice')


def render_value(value):
    if isinstance(value, dict):
        return value.get('label', value.get('name', value))
    return value


def readable_url(url):
    if len(url) > 55:
        return url[:15] + " .. " + url[len(url) - 25:]
    return url


def url_for(*args, **kwargs):
    """
    Overwrite routes url_for so that we can set the protocol based on
    the config (in case Varnish or other software messes with the headers
    """

    # Since Varnish or other software can mess with the headers and
    # cause us to lose the protocol of the request we need to fetch it
    # from a config and set it
    protocol = config.get('openspending.enforced_protocol', None)
    if protocol:
        kwargs.update({'protocol': protocol})
    return routes_url_for(*args, **kwargs)


def site_url():
    return url_for(
        controller='home', action='index', qualified=True).rstrip('/')


def gravatar(email, size=None, default='mm'):
    """
    Generate a gravatar url based on a provided email. If email is none we
    spit out a default gravatar. The default gravatar is the mystery man (mm).
    """

    # Gravatar url structure
    gravatar_url = '//www.gravatar.com/avatar/{digest}?d={default}{query}'

    # If email is None we spit out a dummy digest
    if email is None:
        digest = '00000000000000000000000000000000'
    # else we spit out and md5 digest as required by Gravatar
    else:
        digest = hashlib.md5(email.strip().lower()).hexdigest()

    # Generate the Gravatar url
    url = gravatar_url.format(digest=digest,
                              default=default,
                              query='&s=' + str(size) if size else '')

    # Return it
    return url


def twitter_uri(handle):
    return '//twitter.com/{handle}'.format(handle=handle.lstrip('@'))


def script_root():
    c = tmpl_context
    if c.account and c.account.script_root\
            and len(c.account.script_root.strip()):
        return c.account.script_root
    return app_globals.script_root


def static(url, obj=None):
    """
    Get the static uri based on the static_path configuration.
    """

    static_path = config.get("openspending.static_path", "/static/")

    if obj:
        # We append the lowercase object name and a forward slash
        static_path = '%s%s/' % (static_path, obj.__name__.lower())

    url_ = "%s%s" % (static_path, url)
    version = config.get("openspending.static_cache_version", "")
    if version:
        url_ = "%s?%s" % (url_, version)
    return url_


def upload(url, obj):
    """
    Get upload uri based on either the upload_uri configurations (set when
    an external web server serves the files).
    The upload uri is appended with a directory based on the provided object
    """

    # Create the uri from the upload_uri configuration (we need to remove the
    # rightmost '/' if it's there so we can avoid importing urljoin for such
    # as simple task), the lowercased object name and the provided url
    uri_ = '%s/%s/%s' % (config.get("openspending.upload_uri",
                                    "/files").rstrip('/'),
                         obj.__name__.lower(), url)

    # We use versioning so that the cache won't serve removed images
    version = config.get("openspending.static_cache_version", "")
    if version:
        uri_ = "%s?%s" % (uri_, version)

    return uri_


def get_object_upload_dir(obj):
    """
    Generate filesystem path of a dynamic upload directory based on object
    name. The dynamic directory is created in the static file folder and is
    assigned the lowercased name of the object.

    If for some reason it is not possible to get or create the directory,
    the method raises OSError.

    Use this method sparingly since it creates directories in the filesystem.
    """

    # We wrap important configuration value fetching in try. If any of them
    # fail we raise OSError to indicate we don't support upload directories
    # The only one likely to fail is the first one (getting pylons.paths)
    # since the static directory has a default value and the other value is
    # just a lowercased object name
    try:
        # Get public directory on filesystem
        pylons_upload = config['pylons.paths']['static_files']
        # Get upload directory as defined in config file (we default to a
        # folder called files (also default for
        upload_path = config.get('openspending.upload_directory', 'files')
        # Check to see the upload dir exists. If not we raise OSError
        upload_dir = os.path.join(pylons_upload, upload_path)
        if not os.path.isdir(upload_dir):
            raise OSError
    except:
        # Upload isn't supported if something happens when retrieving directory
        raise OSError("Upload not supported.")

    # Create the object's upload dir from upload dir and object name
    object_upload_dir = os.path.join(upload_dir, obj.__name__.lower())

    # Check if the directory exists, if so return the path
    if os.path.isdir(object_upload_dir):
        return object_upload_dir

    # Since the directory didn't exist we try to create it
    # We don't have to create any parent directories since we're either
    # creating this in the static folder or raising OSError (therefore we
    # use os.mkdir, not os.makedirs
    try:
        os.mkdir(object_upload_dir, 0o744)
    except OSError as exception:
        # Highly unlikely we end up here, but we will if there's a race
        # condition. We might also end up here if there's a regular file
        # in the static directory with the same name as the intended
        # object directory (errno.EEXIST will let us know).
        import errno
        if exception.errno != errno.EEXIST:
            raise

    # Successfully created, return it
    return object_upload_dir


def get_uuid_filename(filename):
    """
    Return a random uuid based path for a specific object.
    The method generates a filename (but keeps the same file extension)
    This reduces the likelyhood that a preexisting file might get overwritten.
    """

    # Get the hex value of a uuid4 generated value as new filename
    uuid_name = uuid.uuid4().get_hex()
    # Split out the extension and append it to the uuid name
    return ''.join([uuid_name, os.path.splitext(filename)[1]])


def format_currency(amount, dataset):
    """
    Wrapper around babel's format_currency which fetches the currency
    from the dataset. Uses the current locale to format the number.
    """
    try:
        return babel.numbers.format_currency(amount, dataset.currency,
                                             locale=tmpl_context.locale)
    except:
        return amount


def join_filters(filters, append=None, remove=None):
    """
    Join filters which are used to filter Solr entries according to
    the OpenSpending convention. The conventions is that each key/value
    pair is joined with a colon : and the filters are joined with a
    pipe | so the output should be key1:value1|key2:value2

    The function allows users to append more values from a list to
    the output and remove values in a list from the output
    """

    if append is None:
        append = []

    if remove is None:
        remove = []

    # Join filter dictionary but skip pairs with key in remove
    filter_values = [u'%s:%s' % (key, value)
                     for (key, value) in filters.iteritems()
                     if key not in remove]
    # Extend the filters with pairs from append
    for (key, item) in append:
        # We expect the item to be a dictionary with a key name who's value
        # is the filter we want to add. If it isn't we try to add it as a
        # string and if that fails we just don't do anything
        try:
            filter_values.append('%s:%s' % (key, item.get('name', item)))
        except:
            pass

    # Return the joined filters
    return '|'.join(filter_values)


def entry_description(entry):
    fragments = []
    if 'from' in entry and 'to' in entry:
        fragments.extend([
            entry.get('from').get('label'),
            entry.get('to').get('label')
        ])
    if isinstance(entry.get('description'), basestring):
        fragments.append(entry.get('description'))
    else:
        for k, v in entry.items():
            if k in ['from', 'to', 'taxonomy', 'html_url']:
                continue
            if isinstance(v, dict):
                fragments.append(v.get('label'))
            elif isinstance(v, basestring):
                fragments.append(v)
    description = " - ".join(fragments)
    return markdown_preview(description)


def member_url(dataset, dimension, member, **kwargs):
    return url_for(controller='dimension',
                   action='member',
                   dataset=dataset,
                   name=member.get('name'),
                   dimension=dimension,
                   **kwargs)


def dataset_url(dataset, **kwargs):
    return url_for(controller='dataset',
                   action='view', dataset=dataset.name, **kwargs)


def entry_url(dataset, entry, **kwargs):
    kwargs.setdefault('action', 'view')
    return url_for(controller='entry', id=str(entry['id']),
                   dataset=dataset, **kwargs)


def entry_link(dataset, entry, **kwargs):
    kwargs['class'] = 'entry-link'
    return link_to(entry.get('label', entry.get('name', "(Unnamed)")),
                   entry_url(dataset, entry), **kwargs)


def dimension_link(dataset, dimension, data):
    text = render_value(data)
    if isinstance(data, dict) and data['name']:
        text = link_to(text, member_url(dataset, dimension, data))
    return text


def format_number(number):
    '''Format a number with m,b,k etc.

    '''
    if not number:
        return '-'
    # round to 3 significant figures
    tnumber = float('%.2e' % number)
    if abs(tnumber) > 1e9:
        return '%sb' % (tnumber / 1e9)
    elif abs(tnumber) > 1e6:
        return '%sm' % (tnumber / 1e6)
    elif abs(tnumber) > 1e3:
        return '%sk' % (tnumber / 1e3)
    else:
        return '%s' % number


def format_number_with_commas(number):
    '''Format a number with commas.

    NB: will convert to integer e.g. 2010.13 -> 2,010
    '''
    if number is None:
        return "-"
    if number == 'NaN':
        return "-"
    try:
        if math.isnan(number):
            return "-"
        s = str(int(number))
    except TypeError:
        msg = "Value was not numeric: %s (type: %s)" \
              % (repr(number), type(number))
        raise TypeError(msg)

    groups = []
    while s and s[-1].isdigit():
        groups.append(s[-3:])
        s = s[:-3]
    return s + ','.join(reversed(groups))


def get_date_object(unparsed_date):
    """
    Parse either a dict or a string to retreive a datetime.date object.
    The dictionary has to be like the one returned by the database, i.e.
    it must have at least three keys, year, month and day.
    The string can be of any of three formats: yyyy, yyyy-mm-dd, or dd-mm-yyyy.
    This method raises a ValueError if it's unable to parse the given object.
    """

    # If unparsed_date is a dict it's probably a time entry as returned by
    # the database so we try to get the data
    if isinstance(unparsed_date, dict):
        try:
            # Year is necessary, month and day can default to 1
            return datetime.date(int(unparsed_date['year']),
                                 int(unparsed_date.get('month', 1)),
                                 int(unparsed_date.get('day', 1)))
        except KeyError:
            # The creation of the datetime.date might return a KeyError but we
            # catch it and just ignore the error. This will mean that we will
            # return a ValueError instead (last line of the method). We do that
            # since the problem is in fact the value of unparsed_date
            pass

    # If unparsed_date is a string (unicode or str) we try to parse it using
    # datetime's builtin date parser
    if isinstance(unparsed_date, basestring):
        # Supported date formats we can parse
        supported_formats = ['%Y', '%Y-%m-%d', '%d-%m-%Y']

        # Loop through the supported formats and try to parse the datestring
        for dateformat in supported_formats:
            try:
                # From parsing we get a datetime object so we call .date() to
                # return a date object
                date = datetime.datetime.strptime(unparsed_date, dateformat)
                return date.date()
            except ValueError:
                # If we get a ValueError we were unable to parse the string
                # with the format so we just pass and go to the next one
                pass

    # If we get here, we don't support the date format unparsed_date is in
    # so we raise a value error to notify of faulty argument.
    raise ValueError('Unable to parse date')


def get_sole_country(territories):
    """
    Get a single country from a list of dataset territories.
    This returns the name of the country as defined in openspending.reference
    """

    # At the moment we only support returning a country if there is only a
    # single territory. If not we raise an error since we don't know which
    # country is the right one
    if len(territories) != 1:
        raise IndexError('Cannot find a sole country')

    return country.COUNTRIES.get(territories[0])


def inflate(amount, target, reference, territories):
    """
    Inflate an amount from a reference year to a target year for a given
    country. Access a global inflation object which is created at startup
    """

    # Get both target and reference dates. We then need to create a new date
    # object since datetime.dates are immutable and we need it to be January 1
    # in order to work.
    target_date = get_date_object(target)
    target_date = datetime.date(target_date.year, 1, 1)
    reference_date = get_date_object(reference)
    reference_date = datetime.date(reference_date.year, 1, 1)

    # Get the country from the list of territories provided
    dataset_country = get_sole_country(territories)

    # Inflate the amount from reference date to the target date
    inflated_amount = app_globals.inflation.inflate(amount, target_date,
                                                    reference_date,
                                                    dataset_country)

    # Return an inflation dictionary where we show the reference and target
    # dates along with original and inflated amounts.
    return {'reference': reference_date, 'target': target_date,
            'original': amount, 'inflated': inflated_amount}


def script_tag(name):
    return '''<script type="text/javascript" src="''' + \
           '%s/%s.js' % (script_root(), name) + \
           '''"></script>'''


def style_tag(name):
    return '''<link rel="stylesheet" href="''' + \
           '%s/%s.css' % (script_root(), name) + \
           '''" />'''


def has_datatype_attr(c, key):
    return c.desc.get(key) and \
        hasattr(c.desc.get(key), 'datatype') and \
        c.desc.get(key).datatype == 'url'


def json_to_string(json_object, indent=2):
    """
    Wrapper around json.dumps which exposes a helper function
    to dump a json object as string with default indent of 2.
    """
    return json.dumps(json_object, indent=indent)

########NEW FILE########
__FILENAME__ = hypermedia
from openspending.ui.lib.helpers import url_for


def dataset_apply_links(dataset):
    dataset['html_url'] = url_for(controller='dataset',
                                  action='view', dataset=dataset['name'],
                                  qualified=True)
    dataset['badges'] = badges_apply_links(dataset['badges'])
    return dataset


def entry_apply_links(dataset_name, entry):
    if isinstance(entry, dict) and 'id' in entry:
        entry['html_url'] = url_for(controller='entry',
                                    action='view', dataset=dataset_name,
                                    id=entry['id'], qualified=True)
        for k, v in entry.items():
            entry[k] = member_apply_links(dataset_name, k, v)
    return entry


def dimension_apply_links(dataset_name, dimension):
    name = dimension.get('name', dimension.get('key'))
    dimension['html_url'] = url_for(controller='dimension',
                                    action='view', dataset=dataset_name,
                                    dimension=name, qualified=True)
    return dimension


def member_apply_links(dataset_name, dimension, data):
    if isinstance(data, dict) and 'name' in data:
        data['html_url'] = url_for(controller='dimension',
                                   action='member', dataset=dataset_name,
                                   dimension=dimension, name=data['name'],
                                   qualified=True)
    return data


def drilldowns_apply_links(dataset_name, drilldowns):
    linked_data = []
    for drilldown in drilldowns:
        for k, v in drilldown.items():
            drilldown[k] = member_apply_links(dataset_name, k, v)
        linked_data.append(drilldown)
    return linked_data


def badges_apply_links(badges):
    """
    From a list of badges, generate a linked representation of each badge
    in the list.
    """
    linked_badges = []
    # Generate links for each badge and append to linked_badges and return it
    for badge in badges:
        linked_badges.append(badge_apply_links(badge))
    return linked_badges


def badge_apply_links(badge):
    """
    Add links or to badge dictionary representation or modify a dictionary
    representation to include a fully qualified domain
    """
    # Add an html_url to represent the html representation of the badge
    badge['html_url'] = url_for(controller='badge', action='information',
                                id=badge['id'], qualified=True)
    # Change the image url to be a fully qualified url if it isn't already
    needs_qualified = not str(badge['image']).startswith('http://')
    badge['image'] = url_for(str(badge['image']), qualified=needs_qualified)

    return badge

########NEW FILE########
__FILENAME__ = indices
from sqlalchemy.sql.expression import select, func
from sqlalchemy.orm import aliased
from openspending.model import meta as db
from openspending.model.dataset import (Dataset, DatasetLanguage,
                                        DatasetTerritory)

from openspending.ui.lib import helpers as h
from openspending.reference.country import COUNTRIES
from openspending.reference.category import CATEGORIES
from openspending.reference.language import LANGUAGES

import logging
log = logging.getLogger(__name__)


def language_index(datasets):
    """
    Get a list of languages by count of datasets
    """
    # Get a list of languages in the current list of datasets
    languages = DatasetLanguage.dataset_counts(datasets)
    # Return a list of languages as dicts with code, count, url and label
    return [{'code': code, 'count': count,
             'url': h.url_for(controller='dataset',
                              action='index', languages=code),
             'label': LANGUAGES.get(code, code)}
            for (code, count) in languages]


def territory_index(datasets):
    """
    Get a list of territories by count of datasets
    """
    # Get a list of territories in the current list of datasets
    territories = DatasetTerritory.dataset_counts(datasets)
    # Return a list of territories as dicts with code, count, url and label
    return [{'code': code, 'count': count,
             'url': h.url_for(controller='dataset',
                              action='index', territories=code),
             'label': COUNTRIES.get(code, code)}
            for (code, count) in territories]


def category_index(datasets):
    """
    Get a list of categories by count of datasets
    """
    # Get the dataset ids in the current list of datasets
    ds_ids = [d.id for d in datasets]
    if len(ds_ids):
        # If we have dataset ids we count the dataset by category
        q = select([Dataset.category, func.count(Dataset.id)],
                   Dataset.id.in_(ds_ids), group_by=Dataset.category,
                   order_by=func.count(Dataset.id).desc())

        # Execute the queery to the the list of categories
        categories = db.session.bind.execute(q).fetchall()
        # Return a list of categories as dicts with category, count, url
        # and label
        return [{'category': category, 'count': count,
                 'url': h.url_for(controller='dataset',
                                  action='index', category=category),
                 'label': CATEGORIES.get(category, category)}
                for (category, count) in categories if category is not None]

    # We return an empty string if no datasets found
    return []


def dataset_index(languages=[], territories=[], category=None):

    # Get all of the public datasets ordered by when they were last updated
    results = db.session.query(Dataset)
    results = results.filter_by(private=False)
    results = results.order_by(Dataset.updated_at.desc())

    # Filter by languages if they have been provided
    for language in languages:
        l = aliased(DatasetLanguage)
        results = results.join(l, Dataset._languages)
        results = results.filter(l.code == language)

    # Filter by territories if they have been provided
    for territory in territories:
        t = aliased(DatasetTerritory)
        results = results.join(t, Dataset._territories)
        results = results.filter(t.code == territory)

    # Filter category if that has been provided
    if category:
        results = results.filter(Dataset.category == category)

    return list(results)

########NEW FILE########
__FILENAME__ = mailman
from pylons import config

import requests


def subscribe_lists(listnames, data):
    errors = []
    for listname in listnames:
        url = config.get('openspending.subscribe_{0}'.format(listname), False)
        if url and data.get('subscribe_{0}'.format(listname)):
            if not subscribe(url, data):
                errors.append(listname)
    return errors


def subscribe(listserver, data):
    data = {
        'email': data['email'],
        'fullname': data['fullname'],
        'pw': '',
        'pw-conf': '',
        'language': 'en',
        'digest': '1',
        'email-button': 'Subscribe'
    }
    try:
        response = requests.post(listserver, data=data)
        response.raise_for_status()
        return True
    except requests.exceptions.RequestException:
        return False

########NEW FILE########
__FILENAME__ = security
# -*- coding: utf-8 -*-
"""
    werkzeug.security
    ~~~~~~~~~~~~~~~~~

    Security related helpers such as secure password hashing tools.

    :copyright: (c) 2010 by the Werkzeug Team, see AUTHORS for more details.
    :license: BSD, see LICENSE for more details.
"""
import hmac
import string
from random import SystemRandom

# because the API of hmac changed with the introduction of the
# new hashlib module, we have to support both.  This sets up a
# mapping to the digest factory functions and the digest modules
# (or factory functions with changed API)
try:
    from hashlib import sha1, md5
    _hash_funcs = _hash_mods = {'sha1': sha1, 'md5': md5}
    _sha1_mod = sha1
    _md5_mod = md5
except ImportError:
    import sha as _sha1_mod
    import md5 as _md5_mod
    _hash_mods = {'sha1': _sha1_mod, 'md5': _md5_mod}
    _hash_funcs = {'sha1': _sha1_mod.new, 'md5': _md5_mod.new}


SALT_CHARS = string.letters + string.digits


_sys_rng = SystemRandom()


def gen_salt(length):
    """Generate a random string of SALT_CHARS with specified ``length``."""
    if length <= 0:
        raise ValueError('requested salt of length <= 0')
    return ''.join(_sys_rng.choice(SALT_CHARS) for _ in xrange(length))


def _hash_internal(method, salt, password):
    """Internal password hash helper.  Supports plaintext without salt,
    unsalted and salted passwords.  In case salted passwords are used
    hmac is used.
    """
    if method == 'plain':
        return password
    if salt:
        if method not in _hash_mods:
            return None
        if isinstance(salt, unicode):
            salt = salt.encode('utf-8')
        h = hmac.new(salt, None, _hash_mods[method])
    else:
        if method not in _hash_funcs:
            return None
        h = _hash_funcs[method]()
    if isinstance(password, unicode):
        password = password.encode('utf-8')
    h.update(password)
    return h.hexdigest()


def generate_password_hash(password, method='sha1', salt_length=8):
    """Hash a password with the given method and salt with with a string of
    the given length.  The format of the string returned includes the method
    that was used so that :func:`check_password_hash` can check the hash.

    The format for the hashed string looks like this::

        method$salt$hash

    This method can **not** generate unsalted passwords but it is possible
    to set the method to plain to enforce plaintext passwords.  If a salt
    is used, hmac is used internally to salt the password.

    :param password: the password to hash
    :param method: the hash method to use (``'md5'`` or ``'sha1'``)
    :param salt_length: the lengt of the salt in letters
    """
    salt = method != 'plain' and gen_salt(salt_length) or ''
    h = _hash_internal(method, salt, password)
    if h is None:
        raise TypeError('invalid method %r' % method)
    return '%s$%s$%s' % (method, salt, h)


def check_password_hash(pwhash, password):
    """check a password against a given salted and hashed password value.
    In order to support unsalted legacy passwords this method supports
    plain text passwords, md5 and sha1 hashes (both salted and unsalted).

    Returns `True` if the password matched, `False` otherwise.

    :param pwhash: a hashed string like returned by
                   :func:`generate_password_hash`
    :param password: the plaintext password to compare against the hash
    """
    if pwhash.count('$') < 2:
        return False
    method, salt, hashval = pwhash.split('$', 2)
    return _hash_internal(method, salt, password) == hashval

########NEW FILE########
__FILENAME__ = views
'''
This module implements views on the database.
'''
import logging
from datetime import datetime

from pylons.i18n import _

from openspending.model.dataset import Dataset
from openspending.ui.lib import widgets

log = logging.getLogger(__name__)


def default_year(dataset):
    """ Guess a reasonable default year for this dataset or use
    the year specified on the dataset object. """
    current_year = str(datetime.utcnow().year)
    times = [m['year'] for m in dataset['time'].members()]
    times = list(set(times))
    if dataset.default_time:
        return dataset.default_time
    if not len(times) or current_year in times:
        return current_year
    return max(times)


class View(object):

    def __init__(self, dataset, obj, view):
        self.dataset = dataset
        self.obj = obj
        self.entity = view.get('entity').lower().strip()
        self.filters = view.get('filters', {})
        self.name = view.get('name') or 'untitled'
        self.label = view.get('label') or _('Untitled')
        self.dimension = view.get('dimension')
        self.drilldown = view.get('drilldown',
                                  view.get('breakdown'))
        self.cuts = view.get('cuts',
                             view.get('view_filters', {}))
        self.year = default_year(dataset)

    def match(self, obj, dimension=None):
        if isinstance(obj, Dataset):
            return self.entity == 'dataset'
        if self.dimension != dimension:
            return False
        for k, v in self.filters.items():
            if obj.get(k) != v:
                return False
        return True

    @classmethod
    def by_name(cls, dataset, obj, name, dimension=None):
        """get a``View`` with the name ``name`` from the object

        ``obj``
            a model object (instance of :class:`openspending.model.Base`)
        ``name``
            The name of the ``View``

        returns: An instance of :class:`View` if the view could
        be found.

        raises: ``ValueError`` if the view does not exist.
        """
        for data in dataset.data.get('views', []):
            view = cls(dataset, obj, data)
            if view.name == name and view.match(obj, dimension):
                return view
        raise ValueError("View %s does not exist." % name)

    @classmethod
    def available(cls, dataset, obj, dimension=None):
        views = []
        for data in dataset.data.get('views', []):
            view = View(dataset, obj, data)
            if view.match(obj, dimension):
                views.append(view)
        return views

    @property
    def full_cuts(self):
        cuts = dict(self.cuts.items())
        if self.dimension.lower() != 'dataset' and \
                self.entity.lower() != 'dataset':
            cuts[self.dimension] = self.obj.get('name')
        return cuts

    @property
    def vis_state(self):
        return {
            'drilldown': self.drilldown,
            'cuts': self.full_cuts,
            'year': self.year
        }

    @property
    def vis_widget(self):
        return widgets.get_widget('treemap')

    @property
    def table_state(self):
        return {
            'drilldowns': [self.drilldown],
            'cuts': self.full_cuts,
            'year': self.year
        }

    @property
    def table_widget(self):
        return widgets.get_widget('aggregate_table')


def handle_request(request, c, obj, dimension=None):
    view_name = request.params.get('_view', 'default')
    c.available_views = View.available(c.dataset, obj, dimension)
    try:
        c.view = View.by_name(c.dataset, obj, view_name,
                              dimension=dimension)
    except ValueError:
        c.view = None
        return

########NEW FILE########
__FILENAME__ = widgets
# widgets config
import logging
from urlparse import urljoin
from pylons.i18n import _
from pylons import config

from openspending.ui.lib.helpers import script_root

log = logging.getLogger(__name__)


def get_widget(name, force=False):
    """ Get a dict to describe various properties of a named widget. """
    if not force and name not in list_widgets():
        raise ValueError(_("No widget named '%s' exists.") % name)
    base_url = urljoin(script_root() + '/', 'widgets/')
    prefix = urljoin(base_url, name)

    widget_class = ''.join([p.capitalize() for p in name.split('_')])
    widget_class = 'OpenSpending.' + widget_class
    return {
        'js': '%s/main.js' % prefix,
        'base': prefix,
        'class_name': widget_class,
        'name': name,
        'preview': prefix + '/preview.png'
    }


def list_widgets():
    """ List of widgets registered in configuration file. """
    widgets = config.get('openspending.widgets', '').split()
    return map(lambda w: w.strip(), widgets)

########NEW FILE########
__FILENAME__ = source
from urlparse import urlparse

from openspending.validation.model.common import mapping
from openspending.validation.model.common import key
from openspending.validation.model.predicates import chained, \
    nonempty_string


def valid_url(url):
    parsed = urlparse(url)
    if parsed.scheme.lower() not in ('http', 'https'):
        return "Only HTTP/HTTPS web addresses are supported " \
               "at the moment."
    return True


def source_schema():
    schema = mapping('source')
    schema.add(key('url', validator=chained(
        nonempty_string,
        valid_url
    )))
    return schema

########NEW FILE########
__FILENAME__ = _version
__version__ = '0.13.0'

########NEW FILE########
