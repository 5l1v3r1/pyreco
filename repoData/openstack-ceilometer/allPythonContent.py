__FILENAME__ = ceilometer-test-event
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Rackspace Hosting.
#
# Author: Monsyne Dragon <mdragon@rackspace.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

"""Command line tool help you debug your event definitions.

Feed it a list of test notifications in json format, and it will show
you what events will be generated.
"""

import json
import sys

from oslo.config import cfg
from stevedore import extension

from ceilometer.event import converter
from ceilometer import service


cfg.CONF.register_cli_opts([
    cfg.StrOpt('input-file',
               short='i',
               help='File to read test notifications from.'
               ' (Containing a json list of notifications.)'
               ' defaults to stdin.'),
    cfg.StrOpt('output-file',
               short='o',
               help='File to write results to. Defaults to stdout.'),
])

TYPES = {1: 'text',
         2: 'int',
         3: 'float',
         4: 'datetime'}


service.prepare_service()

config_file = converter.get_config_file()
output_file = cfg.CONF.output_file
input_file = cfg.CONF.input_file

if output_file is None:
    out = sys.stdout
else:
    out = open(output_file, 'w')

if input_file is None:
    notifications = json.load(sys.stdin)
else:
    with open(input_file, 'r') as f:
        notifications = json.load(f)

out.write("Definitions file: %s\n" % config_file)
out.write("Notifications tested: %s\n" % len(notifications))

event_converter = converter.setup_events(
    extension.ExtensionManager(
        namespace='ceilometer.event.trait_plugin'))

for notification in notifications:
    event = event_converter.to_event(notification)
    if event is None:
        out.write("Dropped notification: %s\n" %
                  notification['message_id'])
        continue
    out.write("Event: %s at %s\n" % (event.event_name, event.generated))
    for trait in event.traits:
        dtype = TYPES[trait.dtype]
        out.write("    Trait: name: %s, type: %s, value: %s\n" % (
            trait.name, dtype, trait.value))

########NEW FILE########
__FILENAME__ = agent
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Julien Danjou
# Copyright © 2014 Red Hat, Inc
#
# Authors: Julien Danjou <julien@danjou.info>
#          Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import collections
import itertools
import urlparse

from stevedore import extension

from ceilometer.openstack.common import context
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer.openstack.common import service as os_service
from ceilometer import pipeline

LOG = log.getLogger(__name__)


class Resources(object):
    def __init__(self, agent_manager):
        self.agent_manager = agent_manager
        self._resources = []
        self._discovery = []

    def extend(self, pipeline):
        self._resources.extend(pipeline.resources)
        self._discovery.extend(pipeline.discovery)

    @property
    def resources(self):
        source_discovery = (self.agent_manager.discover(self._discovery)
                            if self._discovery else [])
        return self._resources + source_discovery


class PollingTask(object):
    """Polling task for polling samples and inject into pipeline.
    A polling task can be invoked periodically or only once.
    """

    def __init__(self, agent_manager):
        self.manager = agent_manager
        self.pollsters = set()
        # we extend the amalgamation of all static resources for this
        # set of pollsters with a common interval, so as to also
        # include any dynamically discovered resources specific to
        # the matching pipelines (if either is present, the per-agent
        # default discovery is overridden)
        resource_factory = lambda: Resources(agent_manager)
        self.resources = collections.defaultdict(resource_factory)
        self.publish_context = pipeline.PublishContext(
            agent_manager.context)

    def add(self, pollster, pipelines):
        self.publish_context.add_pipelines(pipelines)
        for pipeline in pipelines:
            self.resources[pollster.name].extend(pipeline)
        self.pollsters.update([pollster])

    def poll_and_publish(self):
        """Polling sample and publish into pipeline."""
        agent_resources = self.manager.discover()
        with self.publish_context as publisher:
            cache = {}
            for pollster in self.pollsters:
                key = pollster.name
                LOG.info(_("Polling pollster %s"), key)
                source_resources = list(self.resources[key].resources)
                try:
                    samples = list(pollster.obj.get_samples(
                        self.manager,
                        cache,
                        resources=source_resources or agent_resources,
                    ))
                    publisher(samples)
                except Exception as err:
                    LOG.warning(_(
                        'Continue after error from %(name)s: %(error)s')
                        % ({'name': pollster.name, 'error': err}),
                        exc_info=True)


class AgentManager(os_service.Service):

    def __init__(self, namespace, default_discovery=None):
        super(AgentManager, self).__init__()
        default_discovery = default_discovery or []
        self.default_discovery = default_discovery
        self.pollster_manager = self._extensions('poll', namespace)
        self.discovery_manager = self._extensions('discover')
        self.context = context.RequestContext('admin', 'admin', is_admin=True)

    @staticmethod
    def _extensions(category, agent_ns=None):
        namespace = ('ceilometer.%s.%s' % (category, agent_ns) if agent_ns
                     else 'ceilometer.%s' % category)
        return extension.ExtensionManager(
            namespace=namespace,
            invoke_on_load=True,
        )

    def create_polling_task(self):
        """Create an initially empty polling task."""
        return PollingTask(self)

    def setup_polling_tasks(self):
        polling_tasks = {}
        for pipeline, pollster in itertools.product(
                self.pipeline_manager.pipelines,
                self.pollster_manager.extensions):
            if pipeline.support_meter(pollster.name):
                polling_task = polling_tasks.get(pipeline.get_interval())
                if not polling_task:
                    polling_task = self.create_polling_task()
                    polling_tasks[pipeline.get_interval()] = polling_task
                polling_task.add(pollster, [pipeline])

        return polling_tasks

    def start(self):
        self.pipeline_manager = pipeline.setup_pipeline()

        for interval, task in self.setup_polling_tasks().iteritems():
            self.tg.add_timer(interval,
                              self.interval_task,
                              task=task)

    @staticmethod
    def interval_task(task):
        task.poll_and_publish()

    @staticmethod
    def _parse_discoverer(url):
        s = urlparse.urlparse(url)
        return (s.scheme or s.path), (s.netloc + s.path if s.scheme else None)

    def _discoverer(self, name):
        for d in self.discovery_manager:
            if d.name == name:
                return d.obj
        return None

    def discover(self, discovery=None):
        resources = []
        for url in (discovery or self.default_discovery):
            name, param = self._parse_discoverer(url)
            discoverer = self._discoverer(name)
            if discoverer:
                try:
                    discovered = discoverer.discover(param)
                    resources.extend(discovered)
                except Exception as err:
                    LOG.exception(_('Unable to discover resources: %s') % err)
            else:
                LOG.warning(_('Unknown discovery extension: %s') % name)
        return resources

########NEW FILE########
__FILENAME__ = combination
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 eNovance <licensing@enovance.com>
#
# Authors: Mehdi Abaakouk <mehdi.abaakouk@enovance.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.


import itertools

from ceilometer.alarm import evaluator
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log

LOG = log.getLogger(__name__)

COMPARATORS = {'and': all, 'or': any}


class CombinationEvaluator(evaluator.Evaluator):

    def _get_alarm_state(self, alarm_id):
        try:
            alarm = self._client.alarms.get(alarm_id)
        except Exception:
            LOG.exception(_('alarm retrieval failed'))
            return None
        return alarm.state

    def _sufficient_states(self, alarm, states):
        """Ensure there is sufficient data for evaluation,
        transitioning to unknown otherwise.
        """
        #note(sileht): alarm can be evaluated only with
        #stable state of other alarm
        alarms_missing_states = [alarm_id for alarm_id, state in states
                                 if not state or state == evaluator.UNKNOWN]
        sufficient = len(alarms_missing_states) == 0
        if not sufficient and alarm.state != evaluator.UNKNOWN:
            reason = _('Alarms %(alarm_ids)s'
                       ' are in unknown state') % \
                {'alarm_ids': ",".join(alarms_missing_states)}
            reason_data = self._reason_data(alarms_missing_states)
            self._refresh(alarm, evaluator.UNKNOWN, reason, reason_data)
        return sufficient

    @staticmethod
    def _reason_data(alarm_ids):
        """Create a reason data dictionary for this evaluator type.
        """
        return {'type': 'combination', 'alarm_ids': alarm_ids}

    @classmethod
    def _reason(cls, alarm, state, underlying_states):
        """Fabricate reason string."""
        transition = alarm.state != state

        alarms_to_report = [alarm_id for alarm_id, alarm_state
                            in underlying_states
                            if alarm_state == state]
        reason_data = cls._reason_data(alarms_to_report)
        if transition:
            return (_('Transition to %(state)s due to alarms'
                      ' %(alarm_ids)s in state %(state)s') %
                    {'state': state,
                     'alarm_ids': ",".join(alarms_to_report)}), reason_data
        return (_('Remaining as %(state)s due to alarms'
                  ' %(alarm_ids)s in state %(state)s') %
                {'state': state,
                 'alarm_ids': ",".join(alarms_to_report)}), reason_data

    def _transition(self, alarm, underlying_states):
        """Transition alarm state if necessary.
        """
        op = alarm.rule['operator']
        if COMPARATORS[op](s == evaluator.ALARM
                           for __, s in underlying_states):
            state = evaluator.ALARM
        else:
            state = evaluator.OK

        continuous = alarm.repeat_actions
        reason, reason_data = self._reason(alarm, state, underlying_states)
        if alarm.state != state or continuous:
            self._refresh(alarm, state, reason, reason_data)

    def evaluate(self, alarm):
        if not self.within_time_constraint(alarm):
            LOG.debug(_('Attempted to evaluate alarm %s, but it is not '
                        'within its time constraint.') % alarm.alarm_id)
            return

        states = zip(alarm.rule['alarm_ids'],
                     itertools.imap(self._get_alarm_state,
                                    alarm.rule['alarm_ids']))

        if self._sufficient_states(alarm, states):
            self._transition(alarm, states)

########NEW FILE########
__FILENAME__ = threshold
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Red Hat, Inc
#
# Author: Eoghan Glynn <eglynn@redhat.com>
# Author: Mehdi Abaakouk <mehdi.abaakouk@enovance.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import datetime
import operator

from ceilometer.alarm import evaluator
from ceilometer.alarm.evaluator import utils
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer.openstack.common import timeutils

LOG = log.getLogger(__name__)

COMPARATORS = {
    'gt': operator.gt,
    'lt': operator.lt,
    'ge': operator.ge,
    'le': operator.le,
    'eq': operator.eq,
    'ne': operator.ne,
}


class ThresholdEvaluator(evaluator.Evaluator):

    # the sliding evaluation window is extended to allow
    # for reporting/ingestion lag
    look_back = 1

    # minimum number of datapoints within sliding window to
    # avoid unknown state
    quorum = 1

    @classmethod
    def _bound_duration(cls, alarm, constraints):
        """Bound the duration of the statistics query."""
        now = timeutils.utcnow()
        # when exclusion of weak datapoints is enabled, we extend
        # the look-back period so as to allow a clearer sample count
        # trend to be established
        look_back = (cls.look_back if not alarm.rule.get('exclude_outliers')
                     else alarm.rule['evaluation_periods'])
        window = (alarm.rule['period'] *
                  (alarm.rule['evaluation_periods'] + look_back))
        start = now - datetime.timedelta(seconds=window)
        LOG.debug(_('query stats from %(start)s to '
                    '%(now)s') % {'start': start, 'now': now})
        after = dict(field='timestamp', op='ge', value=start.isoformat())
        before = dict(field='timestamp', op='le', value=now.isoformat())
        constraints.extend([before, after])
        return constraints

    @staticmethod
    def _sanitize(alarm, statistics):
        """Sanitize statistics.
        """
        LOG.debug(_('sanitize stats %s') % statistics)
        if alarm.rule.get('exclude_outliers'):
            key = operator.attrgetter('count')
            mean = utils.mean(statistics, key)
            stddev = utils.stddev(statistics, key, mean)
            lower = mean - 2 * stddev
            upper = mean + 2 * stddev
            inliers, outliers = utils.anomalies(statistics, key, lower, upper)
            if outliers:
                LOG.debug(_('excluded weak datapoints with sample counts %s'),
                          [s.count for s in outliers])
                statistics = inliers
            else:
                LOG.debug('no excluded weak datapoints')

        # in practice statistics are always sorted by period start, not
        # strictly required by the API though
        statistics = statistics[-alarm.rule['evaluation_periods']:]
        LOG.debug(_('pruned statistics to %d') % len(statistics))
        return statistics

    def _statistics(self, alarm, query):
        """Retrieve statistics over the current window."""
        LOG.debug(_('stats query %s') % query)
        try:
            return self._client.statistics.list(
                meter_name=alarm.rule['meter_name'], q=query,
                period=alarm.rule['period'])
        except Exception:
            LOG.exception(_('alarm stats retrieval failed'))
            return []

    def _sufficient(self, alarm, statistics):
        """Ensure there is sufficient data for evaluation,
           transitioning to unknown otherwise.
        """
        sufficient = len(statistics) >= self.quorum
        if not sufficient and alarm.state != evaluator.UNKNOWN:
            reason = _('%d datapoints are unknown') % alarm.rule[
                'evaluation_periods']
            reason_data = self._reason_data('unknown',
                                            alarm.rule['evaluation_periods'],
                                            None)
            self._refresh(alarm, evaluator.UNKNOWN, reason, reason_data)
        return sufficient

    @staticmethod
    def _reason_data(disposition, count, most_recent):
        """Create a reason data dictionary for this evaluator type.
        """
        return {'type': 'threshold', 'disposition': disposition,
                'count': count, 'most_recent': most_recent}

    @classmethod
    def _reason(cls, alarm, statistics, distilled, state):
        """Fabricate reason string."""
        count = len(statistics)
        disposition = 'inside' if state == evaluator.OK else 'outside'
        last = getattr(statistics[-1], alarm.rule['statistic'])
        transition = alarm.state != state
        reason_data = cls._reason_data(disposition, count, last)
        if transition:
            return (_('Transition to %(state)s due to %(count)d samples'
                      ' %(disposition)s threshold, most recent:'
                      ' %(most_recent)s')
                    % dict(reason_data, state=state)), reason_data
        return (_('Remaining as %(state)s due to %(count)d samples'
                  ' %(disposition)s threshold, most recent: %(most_recent)s')
                % dict(reason_data, state=state)), reason_data

    def _transition(self, alarm, statistics, compared):
        """Transition alarm state if necessary.

           The transition rules are currently hardcoded as:

           - transitioning from a known state requires an unequivocal
             set of datapoints

           - transitioning from unknown is on the basis of the most
             recent datapoint if equivocal

           Ultimately this will be policy-driven.
        """
        distilled = all(compared)
        unequivocal = distilled or not any(compared)
        unknown = alarm.state == evaluator.UNKNOWN
        continuous = alarm.repeat_actions

        if unequivocal:
            state = evaluator.ALARM if distilled else evaluator.OK
            reason, reason_data = self._reason(alarm, statistics,
                                               distilled, state)
            if alarm.state != state or continuous:
                self._refresh(alarm, state, reason, reason_data)
        elif unknown or continuous:
            trending_state = evaluator.ALARM if compared[-1] else evaluator.OK
            state = trending_state if unknown else alarm.state
            reason, reason_data = self._reason(alarm, statistics,
                                               distilled, state)
            self._refresh(alarm, state, reason, reason_data)

    def evaluate(self, alarm):
        if not self.within_time_constraint(alarm):
            LOG.debug(_('Attempted to evaluate alarm %s, but it is not '
                        'within its time constraint.') % alarm.alarm_id)
            return

        query = self._bound_duration(
            alarm,
            alarm.rule['query']
        )

        statistics = self._sanitize(
            alarm,
            self._statistics(alarm, query)
        )

        if self._sufficient(alarm, statistics):
            def _compare(stat):
                op = COMPARATORS[alarm.rule['comparison_operator']]
                value = getattr(stat, alarm.rule['statistic'])
                limit = alarm.rule['threshold']
                LOG.debug(_('comparing value %(value)s against threshold'
                            ' %(limit)s') %
                          {'value': value, 'limit': limit})
                return op(value, limit)

            self._transition(alarm,
                             statistics,
                             map(_compare, statistics))

########NEW FILE########
__FILENAME__ = utils
# -*- encoding: utf-8 -*-
#
# Copyright © 2014 Red Hat, Inc
#
# Author: Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import math


def mean(s, key=lambda x: x):
    """Calculate the mean of a numeric list.
    """
    count = float(len(s))
    if count:
        return math.fsum(map(key, s)) / count
    return 0.0


def deltas(s, key, m=None):
    """Calculate the squared distances from mean for a numeric list.
    """
    m = m or mean(s, key)
    return [(key(i) - m) ** 2 for i in s]


def variance(s, key, m=None):
    """Calculate the variance of a numeric list.
    """
    return mean(deltas(s, key, m))


def stddev(s, key, m=None):
    """Calculate the standard deviation of a numeric list.
    """
    return math.sqrt(variance(s, key, m))


def outside(s, key, lower=0.0, upper=0.0):
    """Determine if value falls outside upper and lower bounds.
    """
    v = key(s)
    return v < lower or v > upper


def anomalies(s, key, lower=0.0, upper=0.0):
    """Separate anomalous data points from the in-liers.
    """
    inliers = []
    outliers = []
    for i in s:
        if outside(i, key, lower, upper):
            outliers.append(i)
        else:
            inliers.append(i)
    return inliers, outliers

########NEW FILE########
__FILENAME__ = log
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 eNovance
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Log alarm notifier."""

from ceilometer.alarm import notifier
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log

LOG = log.getLogger(__name__)


class LogAlarmNotifier(notifier.AlarmNotifier):
    "Log alarm notifier."""

    @staticmethod
    def notify(action, alarm_id, previous, current, reason, reason_data):
        LOG.info(_(
            "Notifying alarm %(alarm_id)s from %(previous)s "
            "to %(current)s with action %(action)s because "
            "%(reason)s") % ({'alarm_id': alarm_id, 'previous': previous,
                              'current': current, 'action': action,
                              'reason': reason}))

########NEW FILE########
__FILENAME__ = rest
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 eNovance
#
# Author: Mehdi Abaakouk <mehdi.abaakouk@enovance.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Rest alarm notifier."""

import eventlet
import requests
import six.moves.urllib.parse as urlparse

from oslo.config import cfg

from ceilometer.alarm import notifier
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import jsonutils
from ceilometer.openstack.common import log

LOG = log.getLogger(__name__)

REST_NOTIFIER_OPTS = [
    cfg.StrOpt('rest_notifier_certificate_file',
               default='',
               help='SSL Client certificate for REST notifier.'
               ),
    cfg.StrOpt('rest_notifier_certificate_key',
               default='',
               help='SSL Client private key for REST notifier.'
               ),
    cfg.BoolOpt('rest_notifier_ssl_verify',
                default=True,
                help='Whether to verify the SSL Server certificate when '
                'calling alarm action.'
                ),

]

cfg.CONF.register_opts(REST_NOTIFIER_OPTS, group="alarm")


class RestAlarmNotifier(notifier.AlarmNotifier):
    """Rest alarm notifier."""

    @staticmethod
    def notify(action, alarm_id, previous, current, reason, reason_data,
               headers=None):
        LOG.info(_(
            "Notifying alarm %(alarm_id)s from %(previous)s "
            "to %(current)s with action %(action)s because "
            "%(reason)s") % ({'alarm_id': alarm_id, 'previous': previous,
                              'current': current, 'action': action,
                              'reason': reason}))
        body = {'alarm_id': alarm_id, 'previous': previous,
                'current': current, 'reason': reason,
                'reason_data': reason_data}
        kwargs = {'data': jsonutils.dumps(body)}
        if headers:
            kwargs['headers'] = headers

        if action.scheme == 'https':
            default_verify = int(cfg.CONF.alarm.rest_notifier_ssl_verify)
            options = urlparse.parse_qs(action.query)
            verify = bool(int(options.get('ceilometer-alarm-ssl-verify',
                                          [default_verify])[-1]))
            kwargs['verify'] = verify

            cert = cfg.CONF.alarm.rest_notifier_certificate_file
            key = cfg.CONF.alarm.rest_notifier_certificate_key
            if cert:
                kwargs['cert'] = (cert, key) if key else cert

        eventlet.spawn_n(requests.post, action.geturl(), **kwargs)

########NEW FILE########
__FILENAME__ = test
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 eNovance
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Test alarm notifier."""

from ceilometer.alarm import notifier


class TestAlarmNotifier(notifier.AlarmNotifier):
    "Test alarm notifier."""

    def __init__(self):
        self.notifications = []

    def notify(self, action, alarm_id, previous, current, reason, reason_data):
        self.notifications.append((action,
                                   alarm_id,
                                   previous,
                                   current,
                                   reason,
                                   reason_data))

########NEW FILE########
__FILENAME__ = trust
# -*- encoding: utf-8 -*-
#
# Copyright © 2014 eNovance
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Rest alarm notifier with trusted authentication."""

from keystoneclient.v3 import client as keystone_client
from oslo.config import cfg
from six.moves.urllib import parse

from ceilometer.alarm import notifier


class TrustRestAlarmNotifier(notifier.rest.RestAlarmNotifier):
    """Notifier supporting keystone trust authentication.

    This alarm notifier is intended to be used to call an endpoint using
    keystone authentication. It uses the ceilometer service user to
    authenticate using the trust ID provided.

    The URL must be in the form trust+http://trust-id@host/action.
    """

    @staticmethod
    def notify(action, alarm_id, previous, current, reason, reason_data):
        trust_id = action.username

        auth_url = cfg.CONF.service_credentials.os_auth_url.replace(
            "v2.0", "v3")
        client = keystone_client.Client(
            username=cfg.CONF.service_credentials.os_username,
            password=cfg.CONF.service_credentials.os_password,
            cacert=cfg.CONF.service_credentials.os_cacert,
            auth_url=auth_url,
            region_name=cfg.CONF.service_credentials.os_region_name,
            insecure=cfg.CONF.service_credentials.insecure,
            trust_id=trust_id)

        # Remove the fake user
        netloc = action.netloc.split("@")[1]
        # Remove the trust prefix
        scheme = action.scheme[6:]

        action = parse.SplitResult(scheme, netloc, action.path, action.query,
                                   action.fragment)

        headers = {'X-Auth-Token': client.auth_token}
        notifier.rest.RestAlarmNotifier.notify(
            action, alarm_id, previous, current, reason, reason_data, headers)

########NEW FILE########
__FILENAME__ = coordination
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Red Hat, Inc
#
# Authors: Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import math
import random
import uuid

from ceilometer.alarm import rpc as rpc_alarm
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer.openstack.common import timeutils


LOG = log.getLogger(__name__)


class PartitionIdentity(object):
    """Representation of a partition's identity for age comparison."""

    def __init__(self, uuid, priority):
        self.uuid = uuid
        self.priority = priority

    def __repr__(self):
        return '%s:%s' % (self.uuid, self.priority)

    def __hash__(self):
        return hash((self.uuid, self.priority))

    def __eq__(self, other):
        if not isinstance(other, PartitionIdentity):
            return False
        return self.priority == other.priority and self.uuid == other.uuid

    def __ne__(self, other):
        return not self.__eq__(other)

    def __lt__(self, other):
        if not other:
            return True
        if not isinstance(other, PartitionIdentity):
            return False
        older = self.priority < other.priority
        tie_broken = (self.priority == other.priority and
                      self.uuid < other.uuid)
        return older or tie_broken

    def __gt__(self, other):
        return not (self.__lt__(other) or self.__eq__(other))


class PartitionCoordinator(object):
    """Implements the alarm partition coordination protocol.

    A simple protocol based on AMQP fanout RPC is used.

    All available partitions report their presence periodically.

    The priority of each partition in terms of assuming mastership
    is determined by earliest start-time (with a UUID-based tiebreaker
    in the unlikely event of a time clash).

    A single partition assumes mastership at any given time, taking
    responsibility for allocating the alarms to be evaluated across
    the set of currently available partitions.

    When a partition lifecycle event is detected (i.e. a pre-existing
    partition fails to report its presence, or a new one is started
    up), a complete rebalance of the alarms is initiated.

    Individual alarm lifecycle events, on the other hand, do not
    require a full re-balance. Instead new alarms are allocated as
    they are detected, whereas deleted alarms are initially allowed to
    remain within the allocation (as the individual evaluators are tolerant
    of assigned alarms not existing, and the deleted alarms should be
    randomly distributed over the partitions). However once the number of
    alarms deleted since the last rebalance reaches a certain limit, a
    rebalance will be initiated to maintain equity.

    As presence reports are received, each partition keeps track of the
    oldest partition it currently knows about, allowing an assumption of
    mastership to be aborted if an older partition belatedly reports.
    """

    def __init__(self):
        # uniqueness is based on a combination of starting timestamp
        # and UUID
        self.start = timeutils.utcnow()
        self.this = PartitionIdentity(str(uuid.uuid4()),
                                      float(self.start.strftime('%s.%f')))
        self.oldest = None

        # fan-out RPC
        self.coordination_rpc = rpc_alarm.RPCAlarmPartitionCoordination()

        # state maintained by the master
        self.is_master = False
        self.presence_changed = False
        self.reports = {}
        self.last_alarms = set()
        self.deleted_alarms = set()

        # alarms for evaluation, relevant to all partitions regardless
        # of role
        self.assignment = []

    def _distribute(self, alarms, rebalance):
        """Distribute alarms over known set of evaluators.

        :param alarms: the alarms to distribute
        :param rebalance: true if this is a full rebalance
        :return: true if the distribution completed, false if aborted
        """
        verb = 'assign' if rebalance else 'allocate'
        method = (self.coordination_rpc.assign if rebalance
                  else self.coordination_rpc.allocate)
        LOG.debug(_('triggering %s') % verb)
        LOG.debug(_('known evaluators %s') % self.reports)
        per_evaluator = int(math.ceil(len(alarms) /
                            float(len(self.reports) + 1)))
        LOG.debug(_('per evaluator allocation %s') % per_evaluator)
        # for small distributions (e.g. of newly created alarms)
        # we deliberately skew to non-master evaluators
        evaluators = self.reports.keys()
        random.shuffle(evaluators)
        offset = 0
        for evaluator in evaluators:
            # TODO(eglynn): use pagination in the alarms API to chunk large
            # large allocations
            if self.oldest < self.this:
                LOG.warn(_('%(this)s bailing on distribution cycle '
                           'as older partition detected: %(older)s') %
                         dict(this=self.this, older=self.oldest))
                return False
            allocation = alarms[offset:offset + per_evaluator]
            if allocation:
                LOG.debug(_('%(verb)s-ing %(alloc)s to %(eval)s') %
                          dict(verb=verb, alloc=allocation, eval=evaluator))
                method(evaluator.uuid, allocation)
            offset += per_evaluator
        LOG.debug(_('master taking %s for self') % alarms[offset:])
        if rebalance:
            self.assignment = alarms[offset:]
        else:
            self.assignment.extend(alarms[offset:])
        return True

    def _deletion_requires_rebalance(self, alarms):
        """Track the level of deletion activity since the last full rebalance.

        We delay rebalancing until a certain threshold of deletion activity
        has occurred.

        :param alarms: current set of alarms
        :return: True if the level of alarm deletion since the last rebalance
                 is sufficient so as to require a full rebalance
        """
        deleted_alarms = self.last_alarms - set(alarms)
        LOG.debug(_('newly deleted alarms %s') % deleted_alarms)
        self.deleted_alarms.update(deleted_alarms)
        if len(self.deleted_alarms) > len(alarms) / 5:
            LOG.debug(_('alarm deletion activity requires rebalance'))
            self.deleted_alarms = set()
            return True
        return False

    def _record_oldest(self, partition, stale=False):
        """Check if reported partition is the oldest we know about.

        :param partition: reported partition
        :param stale: true if reported partition detected as stale.
        """
        if stale and self.oldest == partition:
            # current oldest partition detected as stale
            self.oldest = None
        elif not self.oldest:
            # no known oldest partition
            self.oldest = partition
        elif partition < self.oldest:
            # new oldest
            self.oldest = partition

    def _is_master(self, interval):
        """Determine if the current partition is the master."""
        now = timeutils.utcnow()
        if timeutils.delta_seconds(self.start, now) < interval * 2:
            LOG.debug(_('%s still warming up') % self.this)
            return False
        is_master = True
        for partition, last_heard in self.reports.items():
            delta = timeutils.delta_seconds(last_heard, now)
            LOG.debug(_('last heard from %(report)s %(delta)s seconds ago') %
                      dict(report=partition, delta=delta))
            if delta > interval * 2:
                del self.reports[partition]
                self._record_oldest(partition, stale=True)
                LOG.debug(_('%(this)s detects stale evaluator: %(stale)s') %
                          dict(this=self.this, stale=partition))
                self.presence_changed = True
            elif partition < self.this:
                is_master = False
                LOG.info(_('%(this)s sees older potential master: %(older)s')
                         % dict(this=self.this, older=partition))
        LOG.info(_('%(this)s is master?: %(is_master)s') %
                 dict(this=self.this, is_master=is_master))
        return is_master

    def _master_role(self, assuming, api_client):
        """Carry out the master role, initiating a distribution if required.

        :param assuming: true if newly assumed mastership
        :param api_client: the API client to use for alarms.
        :return: True if not overtaken by an older partition
        """
        alarms = [a.alarm_id for a in api_client.alarms.list()]
        created_alarms = list(set(alarms) - self.last_alarms)
        LOG.debug(_('newly created alarms %s') % created_alarms)
        sufficient_deletion = self._deletion_requires_rebalance(alarms)
        if (assuming or sufficient_deletion or self.presence_changed):
            still_ahead = self._distribute(alarms, rebalance=True)
        elif created_alarms:
            still_ahead = self._distribute(list(created_alarms),
                                           rebalance=False)
        else:
            # nothing to distribute, but check anyway if overtaken
            still_ahead = self.this < self.oldest
        self.last_alarms = set(alarms)
        LOG.info(_('%(this)s not overtaken as master? %(still_ahead)s') %
                ({'this': self.this, 'still_ahead': still_ahead}))
        return still_ahead

    def check_mastership(self, eval_interval, api_client):
        """Periodically check if the mastership role should be assumed.

        :param eval_interval: the alarm evaluation interval
        :param api_client: the API client to use for alarms.
        """
        LOG.debug(_('%s checking mastership status') % self.this)
        try:
            assuming = not self.is_master
            self.is_master = (self._is_master(eval_interval) and
                              self._master_role(assuming, api_client))
            self.presence_changed = False
        except Exception:
            LOG.exception(_('mastership check failed'))

    def presence(self, uuid, priority):
        """Accept an incoming report of presence."""
        report = PartitionIdentity(uuid, priority)
        if report != self.this:
            if report not in self.reports:
                self.presence_changed = True
            self._record_oldest(report)
            self.reports[report] = timeutils.utcnow()
            LOG.debug(_('%(this)s knows about %(reports)s') %
                      dict(this=self.this, reports=self.reports))

    def assign(self, uuid, alarms):
        """Accept an incoming alarm assignment."""
        if uuid == self.this.uuid:
            LOG.debug(_('%(this)s got assignment: %(alarms)s') %
                      dict(this=self.this, alarms=alarms))
            self.assignment = alarms

    def allocate(self, uuid, alarms):
        """Accept an incoming alarm allocation."""
        if uuid == self.this.uuid:
            LOG.debug(_('%(this)s got allocation: %(alarms)s') %
                      dict(this=self.this, alarms=alarms))
            self.assignment.extend(alarms)

    def report_presence(self):
        """Report the presence of the current partition."""
        LOG.debug(_('%s reporting presence') % self.this)
        try:
            self.coordination_rpc.presence(self.this.uuid, self.this.priority)
        except Exception:
            LOG.exception(_('presence reporting failed'))

    def assigned_alarms(self, api_client):
        """Return the alarms assigned to the current partition."""
        if not self.assignment:
            LOG.debug(_('%s has no assigned alarms to evaluate') % self.this)
            return []

        try:
            LOG.debug(_('%(this)s alarms for evaluation: %(alarms)s') %
                      dict(this=self.this, alarms=self.assignment))
            return [a for a in api_client.alarms.list(q=[{'field': 'enabled',
                                                          'value': True}])
                    if a.alarm_id in self.assignment]
        except Exception:
            LOG.exception(_('assignment retrieval failed'))
            return []

########NEW FILE########
__FILENAME__ = rpc
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 eNovance <licensing@enovance.com>
#
# Authors: Mehdi Abaakouk <mehdi.abaakouk@enovance.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from oslo.config import cfg

from ceilometer import messaging
from ceilometer.openstack.common import context
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer.storage import models

OPTS = [
    cfg.StrOpt('notifier_rpc_topic',
               default='alarm_notifier',
               help='The topic that ceilometer uses for alarm notifier '
                    'messages.'),
    cfg.StrOpt('partition_rpc_topic',
               default='alarm_partition_coordination',
               help='The topic that ceilometer uses for alarm partition '
                    'coordination messages.'),
]

cfg.CONF.register_opts(OPTS, group='alarm')

LOG = log.getLogger(__name__)


class RPCAlarmNotifier(object):
    def __init__(self):
        self.client = messaging.get_rpc_client(
            topic=cfg.CONF.alarm.notifier_rpc_topic,
            version="1.0")

    def notify(self, alarm, previous, reason, reason_data):
        actions = getattr(alarm, models.Alarm.ALARM_ACTIONS_MAP[alarm.state])
        if not actions:
            LOG.debug(_('alarm %(alarm_id)s has no action configured '
                        'for state transition from %(previous)s to '
                        'state %(state)s, skipping the notification.') %
                      {'alarm_id': alarm.alarm_id,
                       'previous': previous,
                       'state': alarm.state})
            return
        self.client.cast(context.get_admin_context(),
                         'notify_alarm', data={
                             'actions': actions,
                             'alarm_id': alarm.alarm_id,
                             'previous': previous,
                             'current': alarm.state,
                             'reason': unicode(reason),
                             'reason_data': reason_data})


class RPCAlarmPartitionCoordination(object):
    def __init__(self):
        self.client = messaging.get_rpc_client(
            topic=cfg.CONF.alarm.partition_rpc_topic,
            version="1.0")

    def presence(self, uuid, priority):
        cctxt = self.client.prepare(fanout=True)
        return cctxt.cast(context.get_admin_context(),
                          'presence', data={'uuid': uuid,
                                            'priority': priority})

    def assign(self, uuid, alarms):
        cctxt = self.client.prepare(fanout=True)
        return cctxt.cast(context.get_admin_context(),
                          'assign', data={'uuid': uuid,
                                          'alarms': alarms})

    def allocate(self, uuid, alarms):
        cctxt = self.client.prepare(fanout=True)
        return cctxt.cast(context.get_admin_context(),
                          'allocate', data={'uuid': uuid,
                                            'alarms': alarms})

########NEW FILE########
__FILENAME__ = service
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Red Hat, Inc
# Copyright © 2013 eNovance <licensing@enovance.com>
#
# Authors: Eoghan Glynn <eglynn@redhat.com>
#          Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import abc

from ceilometerclient import client as ceiloclient
from oslo.config import cfg
import six
from stevedore import extension

from ceilometer.alarm.partition import coordination
from ceilometer.alarm import rpc as rpc_alarm
from ceilometer import messaging
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer.openstack.common import network_utils
from ceilometer.openstack.common import service as os_service


OPTS = [
    cfg.IntOpt('evaluation_interval',
               default=60,
               help='Period of evaluation cycle, should'
                    ' be >= than configured pipeline interval for'
                    ' collection of underlying metrics.',
               deprecated_opts=[cfg.DeprecatedOpt(
                   'threshold_evaluation_interval', group='alarm')]),
]

cfg.CONF.register_opts(OPTS, group='alarm')
cfg.CONF.import_opt('notifier_rpc_topic', 'ceilometer.alarm.rpc',
                    group='alarm')
cfg.CONF.import_opt('partition_rpc_topic', 'ceilometer.alarm.rpc',
                    group='alarm')

LOG = log.getLogger(__name__)


@six.add_metaclass(abc.ABCMeta)
class AlarmService(object):

    EXTENSIONS_NAMESPACE = "ceilometer.alarm.evaluator"

    def _load_evaluators(self):
        self.evaluators = extension.ExtensionManager(
            namespace=self.EXTENSIONS_NAMESPACE,
            invoke_on_load=True,
            invoke_args=(rpc_alarm.RPCAlarmNotifier(),)
        )
        self.supported_evaluators = [ext.name for ext in
                                     self.evaluators.extensions]

    @property
    def _client(self):
        """Construct or reuse an authenticated API client."""
        if not self.api_client:
            auth_config = cfg.CONF.service_credentials
            creds = dict(
                os_auth_url=auth_config.os_auth_url,
                os_region_name=auth_config.os_region_name,
                os_tenant_name=auth_config.os_tenant_name,
                os_password=auth_config.os_password,
                os_username=auth_config.os_username,
                os_cacert=auth_config.os_cacert,
                os_endpoint_type=auth_config.os_endpoint_type,
                insecure=auth_config.insecure,
            )
            self.api_client = ceiloclient.get_client(2, **creds)
        return self.api_client

    def _evaluate_assigned_alarms(self):
        try:
            alarms = self._assigned_alarms()
            LOG.info(_('initiating evaluation cycle on %d alarms') %
                     len(alarms))
            for alarm in alarms:
                self._evaluate_alarm(alarm)
        except Exception:
            LOG.exception(_('alarm evaluation cycle failed'))

    def _evaluate_alarm(self, alarm):
        """Evaluate the alarms assigned to this evaluator."""
        if alarm.type not in self.supported_evaluators:
            LOG.debug(_('skipping alarm %s: type unsupported') %
                      alarm.alarm_id)
            return

        LOG.debug(_('evaluating alarm %s') % alarm.alarm_id)
        self.evaluators[alarm.type].obj.evaluate(alarm)

    @abc.abstractmethod
    def _assigned_alarms(self):
        pass


class SingletonAlarmService(AlarmService, os_service.Service):

    def __init__(self):
        super(SingletonAlarmService, self).__init__()
        self._load_evaluators()
        self.api_client = None

    def start(self):
        super(SingletonAlarmService, self).start()
        if self.evaluators:
            interval = cfg.CONF.alarm.evaluation_interval
            self.tg.add_timer(
                interval,
                self._evaluate_assigned_alarms,
                0)
        # Add a dummy thread to have wait() working
        self.tg.add_timer(604800, lambda: None)

    def _assigned_alarms(self):
        return self._client.alarms.list(q=[{'field': 'enabled',
                                            'value': True}])


cfg.CONF.import_opt('host', 'ceilometer.service')


class PartitionedAlarmService(AlarmService, os_service.Service):

    def __init__(self):
        super(PartitionedAlarmService, self).__init__()
        self.rpc_server = messaging.get_rpc_server(
            cfg.CONF.alarm.partition_rpc_topic, self)

        self._load_evaluators()
        self.api_client = None
        self.partition_coordinator = coordination.PartitionCoordinator()

    def start(self):
        super(PartitionedAlarmService, self).start()
        if self.evaluators:
            eval_interval = cfg.CONF.alarm.evaluation_interval
            self.tg.add_timer(
                eval_interval / 4,
                self.partition_coordinator.report_presence,
                0)
            self.tg.add_timer(
                eval_interval / 2,
                self.partition_coordinator.check_mastership,
                eval_interval,
                *[eval_interval, self._client])
            self.tg.add_timer(
                eval_interval,
                self._evaluate_assigned_alarms,
                eval_interval)
        self.rpc_server.start()
        # Add a dummy thread to have wait() working
        self.tg.add_timer(604800, lambda: None)

    def stop(self):
        self.rpc_server.stop()
        super(PartitionedAlarmService, self).stop()

    def _assigned_alarms(self):
        return self.partition_coordinator.assigned_alarms(self._client)

    def presence(self, context, data):
        self.partition_coordinator.presence(data.get('uuid'),
                                            data.get('priority'))

    def assign(self, context, data):
        self.partition_coordinator.assign(data.get('uuid'),
                                          data.get('alarms'))

    def allocate(self, context, data):
        self.partition_coordinator.allocate(data.get('uuid'),
                                            data.get('alarms'))


class AlarmNotifierService(os_service.Service):

    EXTENSIONS_NAMESPACE = "ceilometer.alarm.notifier"

    def __init__(self):
        super(AlarmNotifierService, self).__init__()
        self.rpc_server = messaging.get_rpc_server(
            cfg.CONF.alarm.notifier_rpc_topic, self)
        self.notifiers = extension.ExtensionManager(self.EXTENSIONS_NAMESPACE,
                                                    invoke_on_load=True)

    def start(self):
        super(AlarmNotifierService, self).start()
        self.rpc_server.start()
        # Add a dummy thread to have wait() working
        self.tg.add_timer(604800, lambda: None)

    def stop(self):
        self.rpc_server.stop()
        super(AlarmNotifierService, self).stop()

    def _handle_action(self, action, alarm_id, previous,
                       current, reason, reason_data):
        try:
            action = network_utils.urlsplit(action)
        except Exception:
            LOG.error(
                _("Unable to parse action %(action)s for alarm %(alarm_id)s"),
                {'action': action, 'alarm_id': alarm_id})
            return

        try:
            notifier = self.notifiers[action.scheme].obj
        except KeyError:
            scheme = action.scheme
            LOG.error(
                _("Action %(scheme)s for alarm %(alarm_id)s is unknown, "
                  "cannot notify"),
                {'scheme': scheme, 'alarm_id': alarm_id})
            return

        try:
            LOG.debug(_("Notifying alarm %(id)s with action %(act)s") % (
                      {'id': alarm_id, 'act': action}))
            notifier.notify(action, alarm_id, previous,
                            current, reason, reason_data)
        except Exception:
            LOG.exception(_("Unable to notify alarm %s"), alarm_id)
            return

    def notify_alarm(self, context, data):
        """Notify that alarm has been triggered.

        data should be a dict with the following keys:
        - actions, the URL of the action to run;
          this is a mapped to extensions automatically
        - alarm_id, the ID of the alarm that has been triggered
        - previous, the previous state of the alarm
        - current, the new state the alarm has transitioned to
        - reason, the reason the alarm changed its state
        - reason_data, a dict representation of the reason

        :param context: Request context.
        :param data: A dict as described above.
        """
        actions = data.get('actions')
        if not actions:
            LOG.error(_("Unable to notify for an alarm with no action"))
            return

        for action in actions:
            self._handle_action(action,
                                data.get('alarm_id'),
                                data.get('previous'),
                                data.get('current'),
                                data.get('reason'),
                                data.get('reason_data'))

########NEW FILE########
__FILENAME__ = acl
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

"""Access Control Lists (ACL's) control access the API server."""

from ceilometer.openstack.common import policy

_ENFORCER = None


def get_limited_to(headers):
    """Return the user and project the request should be limited to.

    :param headers: HTTP headers dictionary
    :return: A tuple of (user, project), set to None if there's no limit on
    one of these.

    """
    global _ENFORCER
    if not _ENFORCER:
        _ENFORCER = policy.Enforcer()
    if not _ENFORCER.enforce('context_is_admin',
                             {},
                             {'roles': headers.get('X-Roles', "").split(",")}):
        return headers.get('X-User-Id'), headers.get('X-Project-Id')
    return None, None


def get_limited_to_project(headers):
    """Return the project the request should be limited to.

    :param headers: HTTP headers dictionary
    :return: A project, or None if there's no limit on it.

    """
    return get_limited_to(headers)[1]

########NEW FILE########
__FILENAME__ = app
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import logging
import os
import socket
from wsgiref import simple_server

import netaddr
from oslo.config import cfg
from paste import deploy
import pecan

from ceilometer.api import config as api_config
from ceilometer.api import hooks
from ceilometer.api import middleware
from ceilometer.openstack.common import log
from ceilometer import storage

LOG = log.getLogger(__name__)

auth_opts = [
    cfg.StrOpt('api_paste_config',
               default="api_paste.ini",
               help="Configuration file for WSGI definition of API."
               ),
]

CONF = cfg.CONF
CONF.register_opts(auth_opts)


def get_pecan_config():
    # Set up the pecan configuration
    filename = api_config.__file__.replace('.pyc', '.py')
    return pecan.configuration.conf_from_file(filename)


def setup_app(pecan_config=None, extra_hooks=None):
    # FIXME: Replace DBHook with a hooks.TransactionHook
    app_hooks = [hooks.ConfigHook(),
                 hooks.DBHook(
                     storage.get_connection_from_config(cfg.CONF),
                 ),
                 hooks.PipelineHook(),
                 hooks.TranslationHook()]
    if extra_hooks:
        app_hooks.extend(extra_hooks)

    if not pecan_config:
        pecan_config = get_pecan_config()

    pecan.configuration.set_config(dict(pecan_config), overwrite=True)

    app = pecan.make_app(
        pecan_config.app.root,
        static_root=pecan_config.app.static_root,
        template_path=pecan_config.app.template_path,
        debug=CONF.debug,
        force_canonical=getattr(pecan_config.app, 'force_canonical', True),
        hooks=app_hooks,
        wrap_app=middleware.ParsableErrorMiddleware,
        guess_content_type_from_ext=False
    )

    return app


class VersionSelectorApplication(object):
    def __init__(self):
        pc = get_pecan_config()
        pc.app.debug = CONF.debug

        def not_found(environ, start_response):
            start_response('404 Not Found', [])
            return []

        self.v1 = not_found
        self.v2 = setup_app(pecan_config=pc)

    def __call__(self, environ, start_response):
        if environ['PATH_INFO'].startswith('/v1/'):
            return self.v1(environ, start_response)
        return self.v2(environ, start_response)


def get_server_cls(host):
    """Return an appropriate WSGI server class base on provided host

    :param host: The listen host for the ceilometer API server.
    """
    server_cls = simple_server.WSGIServer
    if netaddr.valid_ipv6(host):
        # NOTE(dzyu) make sure use IPv6 sockets if host is in IPv6 pattern
        if getattr(server_cls, 'address_family') == socket.AF_INET:
            class server_cls(server_cls):
                address_family = socket.AF_INET6
    return server_cls


def get_handler_cls():
    cls = simple_server.WSGIRequestHandler

    # old-style class doesn't support super
    class CeilometerHandler(cls, object):
        def address_string(self):
            if cfg.CONF.api.enable_reverse_dns_lookup:
                return super(CeilometerHandler, self).address_string()
            else:
                # disable reverse dns lookup, directly return ip adress
                return self.client_address[0]

    return CeilometerHandler


def load_app():
    # Build the WSGI app
    cfg_file = cfg.CONF.api_paste_config
    LOG.info("WSGI config requested: %s" % cfg_file)
    if not os.path.exists(cfg_file):
        # this code is to work around chicken-egg dependency between
        # ceilometer gate jobs use of devstack and this change.
        # The gate job uses devstack to run tempest.
        # devstack does not copy api_paste.ini into /etc/ceilometer because it
        # is introduced in this change. Once this is merged, we will change
        # devstack to copy api_paste.ini and once that is merged will remove
        # this code.
        root = os.path.abspath(os.path.join(os.path.dirname(__file__),
                                            '..', '..', 'etc', 'ceilometer'
                                            )
                               )
        cfg_file = os.path.join(root, cfg_file)
    if not os.path.exists(cfg_file):
        raise Exception('api_paste_config file not found')
    LOG.info("Full WSGI config used: %s" % cfg_file)
    return deploy.loadapp("config:" + cfg_file)


def build_server():
    app = load_app()
    # Create the WSGI server and start it
    host, port = cfg.CONF.api.host, cfg.CONF.api.port
    server_cls = get_server_cls(host)

    srv = simple_server.make_server(host, port, app,
                                    server_cls, get_handler_cls())

    LOG.info(_('Starting server in PID %s') % os.getpid())
    LOG.info(_("Configuration:"))
    cfg.CONF.log_opt_values(LOG, logging.INFO)

    if host == '0.0.0.0':
        LOG.info(_(
            'serving on 0.0.0.0:%(sport)s, view at http://127.0.0.1:%(vport)s')
            % ({'sport': port, 'vport': port}))
    else:
        LOG.info(_("serving on http://%(host)s:%(port)s") % (
                 {'host': host, 'port': port}))

    return srv


def app_factory(global_config, **local_conf):
    return VersionSelectorApplication()

########NEW FILE########
__FILENAME__ = config
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

# Server Specific Configurations
server = {
    'port': '8777',
    'host': '0.0.0.0'
}

# Pecan Application Configurations
app = {
    'root': 'ceilometer.api.controllers.root.RootController',
    'modules': ['ceilometer.api'],
    'static_root': '%(confdir)s/public',
    'template_path': '%(confdir)s/ceilometer/api/templates',
}

# Custom Configurations must be in Python dictionary format::
#
# foo = {'bar':'baz'}
#
# All configurations are accessible at::
# pecan.conf

########NEW FILE########
__FILENAME__ = root
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import pecan

from ceilometer.api.controllers import v2


class RootController(object):

    v2 = v2.V2Controller()

    @pecan.expose(generic=True, template='index.html')
    def index(self):
        # FIXME: Return version information
        return dict()

########NEW FILE########
__FILENAME__ = v2
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
# Copyright 2013 IBM Corp.
# Copyright © 2013 eNovance <licensing@enovance.com>
# Copyright Ericsson AB 2013. All rights reserved
#
# Authors: Doug Hellmann <doug.hellmann@dreamhost.com>
#          Angus Salkeld <asalkeld@redhat.com>
#          Eoghan Glynn <eglynn@redhat.com>
#          Julien Danjou <julien@danjou.info>
#          Ildiko Vancsa <ildiko.vancsa@ericsson.com>
#          Balazs Gibizer <balazs.gibizer@ericsson.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Version 2 of the API.
"""
import ast
import base64
import copy
import croniter
import datetime
import functools
import inspect
import json
import jsonschema
import pytz
import uuid

from oslo.config import cfg
import pecan
from pecan import rest
import six
import wsme
from wsme import types as wtypes
import wsmeext.pecan as wsme_pecan

from ceilometer.api import acl
from ceilometer import messaging
from ceilometer.openstack.common import context
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer.openstack.common import strutils
from ceilometer.openstack.common import timeutils
from ceilometer import sample
from ceilometer import storage
from ceilometer import utils


LOG = log.getLogger(__name__)


ALARM_API_OPTS = [
    cfg.BoolOpt('record_history',
                default=True,
                help='Record alarm change events.'
                ),
]

cfg.CONF.register_opts(ALARM_API_OPTS, group='alarm')

state_kind = ["ok", "alarm", "insufficient data"]
state_kind_enum = wtypes.Enum(str, *state_kind)
operation_kind = wtypes.Enum(str, 'lt', 'le', 'eq', 'ne', 'ge', 'gt')


class ClientSideError(wsme.exc.ClientSideError):
    def __init__(self, error, status_code=400):
        pecan.response.translatable_error = error
        super(ClientSideError, self).__init__(error, status_code)


class EntityNotFound(ClientSideError):
    def __init__(self, entity, id):
        super(EntityNotFound, self).__init__(
            _("%(entity)s %(id)s Not Found") % {'entity': entity,
                                                'id': id},
            status_code=404)


class AdvEnum(wtypes.wsproperty):
    """Handle default and mandatory for wtypes.Enum
    """
    def __init__(self, name, *args, **kwargs):
        self._name = '_advenum_%s' % name
        self._default = kwargs.pop('default', None)
        mandatory = kwargs.pop('mandatory', False)
        enum = wtypes.Enum(*args, **kwargs)
        super(AdvEnum, self).__init__(datatype=enum, fget=self._get,
                                      fset=self._set, mandatory=mandatory)

    def _get(self, parent):
        if hasattr(parent, self._name):
            value = getattr(parent, self._name)
            return value or self._default
        return self._default

    def _set(self, parent, value):
        if self.datatype.validate(value):
            setattr(parent, self._name, value)


class CronType(wtypes.UserType):
    """A user type that represents a cron format."""
    basetype = six.string_types
    name = 'cron'

    @staticmethod
    def validate(value):
        # raises ValueError if invalid
        croniter.croniter(value)
        return value


class _Base(wtypes.Base):

    @classmethod
    def from_db_model(cls, m):
        return cls(**(m.as_dict()))

    @classmethod
    def from_db_and_links(cls, m, links):
        return cls(links=links, **(m.as_dict()))

    def as_dict(self, db_model):
        valid_keys = inspect.getargspec(db_model.__init__)[0]
        if 'self' in valid_keys:
            valid_keys.remove('self')
        return self.as_dict_from_keys(valid_keys)

    def as_dict_from_keys(self, keys):
        return dict((k, getattr(self, k))
                    for k in keys
                    if hasattr(self, k) and
                    getattr(self, k) != wsme.Unset)


class Link(_Base):
    """A link representation
    """

    href = wtypes.text
    "The url of a link"

    rel = wtypes.text
    "The name of a link"

    @classmethod
    def sample(cls):
        return cls(href=('http://localhost:8777/v2/meters/volume?'
                         'q.field=resource_id&'
                         'q.value=bd9431c1-8d69-4ad3-803a-8d4a6b89fd36'),
                   rel='volume'
                   )


class Query(_Base):
    """Query filter.
    """

    # The data types supported by the query.
    _supported_types = ['integer', 'float', 'string', 'boolean']

    # Functions to convert the data field to the correct type.
    _type_converters = {'integer': int,
                        'float': float,
                        'boolean': functools.partial(
                            strutils.bool_from_string, strict=True),
                        'string': six.text_type,
                        'datetime': timeutils.parse_isotime}

    _op = None  # provide a default

    def get_op(self):
        return self._op or 'eq'

    def set_op(self, value):
        self._op = value

    field = wtypes.text
    "The name of the field to test"

    #op = wsme.wsattr(operation_kind, default='eq')
    # this ^ doesn't seem to work.
    op = wsme.wsproperty(operation_kind, get_op, set_op)
    "The comparison operator. Defaults to 'eq'."

    value = wtypes.text
    "The value to compare against the stored data"

    type = wtypes.text
    "The data type of value to compare against the stored data"

    def __repr__(self):
        # for logging calls
        return '<Query %r %s %r %s>' % (self.field,
                                        self.op,
                                        self.value,
                                        self.type)

    @classmethod
    def sample(cls):
        return cls(field='resource_id',
                   op='eq',
                   value='bd9431c1-8d69-4ad3-803a-8d4a6b89fd36',
                   type='string'
                   )

    def as_dict(self):
        return self.as_dict_from_keys(['field', 'op', 'type', 'value'])

    def _get_value_as_type(self, forced_type=None):
        """Convert metadata value to the specified data type.

        This method is called during metadata query to help convert the
        querying metadata to the data type specified by user. If there is no
        data type given, the metadata will be parsed by ast.literal_eval to
        try to do a smart converting.

        NOTE (flwang) Using "_" as prefix to avoid an InvocationError raised
        from wsmeext/sphinxext.py. It's OK to call it outside the Query class.
        Because the "public" side of that class is actually the outside of the
        API, and the "private" side is the API implementation. The method is
        only used in the API implementation, so it's OK.

        :returns: metadata value converted with the specified data type.
        """
        type = forced_type or self.type
        try:
            converted_value = self.value
            if not type:
                try:
                    converted_value = ast.literal_eval(self.value)
                except (ValueError, SyntaxError):
                    # Unable to convert the metadata value automatically
                    # let it default to self.value
                    pass
            else:
                if type not in self._supported_types:
                    # Types must be explicitly declared so the
                    # correct type converter may be used. Subclasses
                    # of Query may define _supported_types and
                    # _type_converters to define their own types.
                    raise TypeError()
                converted_value = self._type_converters[type](self.value)
        except ValueError:
            msg = _('Unable to convert the value %(value)s'
                    ' to the expected data type %(type)s.') % \
                {'value': self.value, 'type': type}
            raise ClientSideError(msg)
        except TypeError:
            msg = _('The data type %(type)s is not supported. The supported'
                    ' data type list is: %(supported)s') % \
                {'type': type, 'supported': self._supported_types}
            raise ClientSideError(msg)
        except Exception:
            msg = _('Unexpected exception converting %(value)s to'
                    ' the expected data type %(type)s.') % \
                {'value': self.value, 'type': type}
            raise ClientSideError(msg)
        return converted_value


class ProjectNotAuthorized(ClientSideError):
    def __init__(self, id, aspect='project'):
        params = dict(aspect=aspect, id=id)
        super(ProjectNotAuthorized, self).__init__(
            _("Not Authorized to access %(aspect)s %(id)s") % params,
            status_code=401)


def _get_auth_project(on_behalf_of=None):
    # when an alarm is created by an admin on behalf of another tenant
    # we must ensure for:
    # - threshold alarm, that an implicit query constraint on project_id is
    #   added so that admin-level visibility on statistics is not leaked
    # - combination alarm, that alarm ids verification is scoped to
    #   alarms owned by the alarm project.
    # hence for null auth_project (indicating admin-ness) we check if
    # the creating tenant differs from the tenant on whose behalf the
    # alarm is being created
    auth_project = acl.get_limited_to_project(pecan.request.headers)
    created_by = pecan.request.headers.get('X-Project-Id')
    is_admin = auth_project is None

    if is_admin and on_behalf_of != created_by:
        auth_project = on_behalf_of
    return auth_project


def _sanitize_query(query, db_func, on_behalf_of=None):
    '''Check the query to see if:
    1) the request is coming from admin - then allow full visibility
    2) non-admin - make sure that the query includes the requester's
    project.
    '''
    q = copy.copy(query)

    auth_project = _get_auth_project(on_behalf_of)
    if auth_project:
        _verify_query_segregation(q, auth_project)

        proj_q = [i for i in q if i.field == 'project_id']
        valid_keys = inspect.getargspec(db_func)[0]
        if not proj_q and 'on_behalf_of' not in valid_keys:
            # The user is restricted, but they didn't specify a project
            # so add it for them.
            q.append(Query(field='project_id',
                           op='eq',
                           value=auth_project))
    return q


def _verify_query_segregation(query, auth_project=None):
    '''Ensure non-admin queries are not constrained to another project.'''
    auth_project = (auth_project or
                    acl.get_limited_to_project(pecan.request.headers))
    if auth_project:
        for q in query:
            if q.field == 'project_id' and (auth_project != q.value or
                                            q.op != 'eq'):
                raise ProjectNotAuthorized(q.value)


def _validate_query(query, db_func, internal_keys=None,
                    allow_timestamps=True):
    """Validates the syntax of the query and verifies that the query
    request is authorized for the included project.

    :param query: Query expression that should be validated
    :param db_func: the function on the storage level, of which arguments
        will form the valid_keys list, which defines the valid fields for a
        query expression
    :param internal_keys: internally used field names, that should not be
        used for querying
    :param allow_timestamps: defines whether the timestamp-based constraint is
        applicable for this query or not

    :returns: None, if the query is valid

    :raises InvalidInput: if an operator is not supported for a given field
    :raises InvalidInput: if timestamp constraints are allowed, but
        search_offset was included without timestamp constraint
    :raises: UnknownArgument: if a field name is not a timestamp field, nor
        in the list of valid keys

    """

    internal_keys = internal_keys or []
    _verify_query_segregation(query)

    valid_keys = inspect.getargspec(db_func)[0]
    internal_keys.append('self')
    valid_keys = set(valid_keys) - set(internal_keys)
    translation = {'user_id': 'user',
                   'project_id': 'project',
                   'resource_id': 'resource'}

    has_timestamp_query = _validate_timestamp_fields(query,
                                                     'timestamp',
                                                     ('lt', 'le', 'gt', 'ge'),
                                                     allow_timestamps)
    has_search_offset_query = _validate_timestamp_fields(query,
                                                         'search_offset',
                                                         ('eq'),
                                                         allow_timestamps)

    if has_search_offset_query and not has_timestamp_query:
        raise wsme.exc.InvalidInput('field', 'search_offset',
                                    "search_offset cannot be used without " +
                                    "timestamp")

    def _is_field_metadata(field):
        return (field.startswith('metadata.') or
                field.startswith('resource_metadata.'))

    for i in query:
        if i.field not in ('timestamp', 'search_offset'):
            key = translation.get(i.field, i.field)
            operator = i.op
            if (key in valid_keys or _is_field_metadata(i.field)):
                if operator == 'eq':
                    if key == 'enabled':
                        i._get_value_as_type('boolean')
                    elif _is_field_metadata(key):
                        i._get_value_as_type()
                else:
                    raise wsme.exc.InvalidInput('op', i.op,
                                                'unimplemented operator for '
                                                '%s' % i.field)
            else:
                msg = ("unrecognized field in query: %s, "
                       "valid keys: %s") % (query, valid_keys)
                raise wsme.exc.UnknownArgument(key, msg)


def _validate_timestamp_fields(query, field_name, operator_list,
                               allow_timestamps):
    """Validates the timestamp related constraints in a query expression, if
    there are any.

    :param query: query expression that may contain the timestamp fields
    :param field_name: timestamp name, which should be checked (timestamp,
        search_offset)
    :param operator_list: list of operators that are supported for that
        timestamp, which was specified in the parameter field_name
    :param allow_timestamps: defines whether the timestamp-based constraint is
        applicable to this query or not

    :returns: True, if there was a timestamp constraint, containing
        a timestamp field named as defined in field_name, in the query and it
        was allowed and syntactically correct.
    :returns: False, if there wasn't timestamp constraint, containing a
        timestamp field named as defined in field_name, in the query

    :raises InvalidInput: if an operator is unsupported for a given timestamp
        field
    :raises UnknownArgument: if the timestamp constraint is not allowed in
        the query

    """

    for item in query:
        if item.field == field_name:
            #If *timestamp* or *search_offset* field was specified in the
            #query, but timestamp is not supported on that resource, on
            #which the query was invoked, then raise an exception.
            if not allow_timestamps:
                raise wsme.exc.UnknownArgument(field_name,
                                               "not valid for " +
                                               "this resource")
            if item.op not in operator_list:
                raise wsme.exc.InvalidInput('op', item.op,
                                            'unimplemented operator for %s' %
                                            item.field)
            return True
    return False


def _query_to_kwargs(query, db_func, internal_keys=None,
                     allow_timestamps=True):
    internal_keys = internal_keys or []
    _validate_query(query, db_func, internal_keys=internal_keys,
                    allow_timestamps=allow_timestamps)
    query = _sanitize_query(query, db_func)
    internal_keys.append('self')
    valid_keys = set(inspect.getargspec(db_func)[0]) - set(internal_keys)
    translation = {'user_id': 'user',
                   'project_id': 'project',
                   'resource_id': 'resource'}
    stamp = {}
    metaquery = {}
    kwargs = {}
    for i in query:
        if i.field == 'timestamp':
            if i.op in ('lt', 'le'):
                stamp['end_timestamp'] = i.value
                stamp['end_timestamp_op'] = i.op
            elif i.op in ('gt', 'ge'):
                stamp['start_timestamp'] = i.value
                stamp['start_timestamp_op'] = i.op
        else:
            if i.op == 'eq':
                if i.field == 'search_offset':
                    stamp['search_offset'] = i.value
                elif i.field == 'enabled':
                    kwargs[i.field] = i._get_value_as_type('boolean')
                elif i.field.startswith('metadata.'):
                    metaquery[i.field] = i._get_value_as_type()
                elif i.field.startswith('resource_metadata.'):
                    metaquery[i.field[9:]] = i._get_value_as_type()
                else:
                    key = translation.get(i.field, i.field)
                    kwargs[key] = i.value

    if metaquery and 'metaquery' in valid_keys:
        kwargs['metaquery'] = metaquery
    if stamp:
        q_ts = _get_query_timestamps(stamp)
        if 'start' in valid_keys:
            kwargs['start'] = q_ts['query_start']
            kwargs['end'] = q_ts['query_end']
        elif 'start_timestamp' in valid_keys:
            kwargs['start_timestamp'] = q_ts['query_start']
            kwargs['end_timestamp'] = q_ts['query_end']
        if 'start_timestamp_op' in stamp:
            kwargs['start_timestamp_op'] = stamp['start_timestamp_op']
        if 'end_timestamp_op' in stamp:
            kwargs['end_timestamp_op'] = stamp['end_timestamp_op']

    return kwargs


def _validate_groupby_fields(groupby_fields):
    """Checks that the list of groupby fields from request is valid and
    if all fields are valid, returns fields with duplicates removed

    """
    # NOTE(terriyu): Currently, metadata fields are not supported in our
    # group by statistics implementation
    valid_fields = set(['user_id', 'resource_id', 'project_id', 'source'])

    invalid_fields = set(groupby_fields) - valid_fields
    if invalid_fields:
        raise wsme.exc.UnknownArgument(invalid_fields,
                                       "Invalid groupby fields")

    # Remove duplicate fields
    # NOTE(terriyu): This assumes that we don't care about the order of the
    # group by fields.
    return list(set(groupby_fields))


def _get_query_timestamps(args=None):
    """Return any optional timestamp information in the request.

    Determine the desired range, if any, from the GET arguments. Set
    up the query range using the specified offset.

    [query_start ... start_timestamp ... end_timestamp ... query_end]

    Returns a dictionary containing:

    query_start: First timestamp to use for query
    start_timestamp: start_timestamp parameter from request
    query_end: Final timestamp to use for query
    end_timestamp: end_timestamp parameter from request
    search_offset: search_offset parameter from request

    """
    if args is None:
        return {'query_start': None,
                'query_end': None,
                'start_timestamp': None,
                'end_timestamp': None,
                'search_offset': 0}
    search_offset = int(args.get('search_offset', 0))

    start_timestamp = args.get('start_timestamp')
    if start_timestamp:
        start_timestamp = timeutils.parse_isotime(start_timestamp)
        start_timestamp = start_timestamp.replace(tzinfo=None)
        query_start = (start_timestamp -
                       datetime.timedelta(minutes=search_offset))
    else:
        query_start = None

    end_timestamp = args.get('end_timestamp')
    if end_timestamp:
        end_timestamp = timeutils.parse_isotime(end_timestamp)
        end_timestamp = end_timestamp.replace(tzinfo=None)
        query_end = end_timestamp + datetime.timedelta(minutes=search_offset)
    else:
        query_end = None

    return {'query_start': query_start,
            'query_end': query_end,
            'start_timestamp': start_timestamp,
            'end_timestamp': end_timestamp,
            'search_offset': search_offset,
            }


def _flatten_metadata(metadata):
    """Return flattened resource metadata with flattened nested
    structures (except nested sets) and with all values converted
    to unicode strings.
    """
    if metadata:
        # After changing recursive_keypairs` output we need to keep
        # flattening output unchanged.
        # Example: recursive_keypairs({'a': {'b':{'c':'d'}}}, '.')
        # output before: a.b:c=d
        # output now: a.b.c=d
        # So to keep the first variant just replace all dots except the first
        return dict((k.replace('.', ':').replace(':', '.', 1), unicode(v))
                    for k, v in utils.recursive_keypairs(metadata,
                                                         separator='.')
                    if type(v) is not set)
    return {}


def _make_link(rel_name, url, type, type_arg, query=None):
    query_str = ''
    if query:
        query_str = '?q.field=%s&q.value=%s' % (query['field'],
                                                query['value'])
    return Link(href=('%s/v2/%s/%s%s') % (url, type, type_arg, query_str),
                rel=rel_name)


def _send_notification(event, payload):
    notification = event.replace(" ", "_")
    notification = "alarm.%s" % notification
    notifier = messaging.get_notifier(publisher_id="ceilometer.api")
    # FIXME(sileht): perhaps we need to copy some infos from the
    # pecan request headers like nova does
    notifier.info(context.RequestContext(), notification, payload)


class OldSample(_Base):
    """A single measurement for a given meter and resource.

    This class is deprecated in favor of Sample.
    """

    source = wtypes.text
    "The ID of the source that identifies where the sample comes from"

    counter_name = wsme.wsattr(wtypes.text, mandatory=True)
    "The name of the meter"
    # FIXME(dhellmann): Make this meter_name?

    counter_type = wsme.wsattr(wtypes.text, mandatory=True)
    "The type of the meter (see :ref:`measurements`)"
    # FIXME(dhellmann): Make this meter_type?

    counter_unit = wsme.wsattr(wtypes.text, mandatory=True)
    "The unit of measure for the value in counter_volume"
    # FIXME(dhellmann): Make this meter_unit?

    counter_volume = wsme.wsattr(float, mandatory=True)
    "The actual measured value"

    user_id = wtypes.text
    "The ID of the user who last triggered an update to the resource"

    project_id = wtypes.text
    "The ID of the project or tenant that owns the resource"

    resource_id = wsme.wsattr(wtypes.text, mandatory=True)
    "The ID of the :class:`Resource` for which the measurements are taken"

    timestamp = datetime.datetime
    "UTC date and time when the measurement was made"

    recorded_at = datetime.datetime
    "When the sample has been recorded."

    resource_metadata = {wtypes.text: wtypes.text}
    "Arbitrary metadata associated with the resource"

    message_id = wtypes.text
    "A unique identifier for the sample"

    def __init__(self, counter_volume=None, resource_metadata=None,
                 timestamp=None, **kwds):
        resource_metadata = resource_metadata or {}
        if counter_volume is not None:
            counter_volume = float(counter_volume)
        resource_metadata = _flatten_metadata(resource_metadata)
        # this is to make it easier for clients to pass a timestamp in
        if timestamp and isinstance(timestamp, basestring):
            timestamp = timeutils.parse_isotime(timestamp)

        super(OldSample, self).__init__(counter_volume=counter_volume,
                                        resource_metadata=resource_metadata,
                                        timestamp=timestamp, **kwds)

        if self.resource_metadata in (wtypes.Unset, None):
            self.resource_metadata = {}

    @classmethod
    def sample(cls):
        return cls(source='openstack',
                   counter_name='instance',
                   counter_type='gauge',
                   counter_unit='instance',
                   counter_volume=1,
                   resource_id='bd9431c1-8d69-4ad3-803a-8d4a6b89fd36',
                   project_id='35b17138-b364-4e6a-a131-8f3099c5be68',
                   user_id='efd87807-12d2-4b38-9c70-5f5c2ac427ff',
                   recorded_at=datetime.datetime.utcnow(),
                   timestamp=datetime.datetime.utcnow(),
                   resource_metadata={'name1': 'value1',
                                      'name2': 'value2'},
                   message_id='5460acce-4fd6-480d-ab18-9735ec7b1996',
                   )


class Statistics(_Base):
    """Computed statistics for a query.
    """

    groupby = {wtypes.text: wtypes.text}
    "Dictionary of field names for group, if groupby statistics are requested"

    unit = wtypes.text
    "The unit type of the data set"

    min = float
    "The minimum volume seen in the data"

    max = float
    "The maximum volume seen in the data"

    avg = float
    "The average of all of the volume values seen in the data"

    sum = float
    "The total of all of the volume values seen in the data"

    count = int
    "The number of samples seen"

    aggregate = {wtypes.text: float}
    "The selectable aggregate value(s)"

    duration = float
    "The difference, in seconds, between the oldest and newest timestamp"

    duration_start = datetime.datetime
    "UTC date and time of the earliest timestamp, or the query start time"

    duration_end = datetime.datetime
    "UTC date and time of the oldest timestamp, or the query end time"

    period = int
    "The difference, in seconds, between the period start and end"

    period_start = datetime.datetime
    "UTC date and time of the period start"

    period_end = datetime.datetime
    "UTC date and time of the period end"

    def __init__(self, start_timestamp=None, end_timestamp=None, **kwds):
        super(Statistics, self).__init__(**kwds)
        self._update_duration(start_timestamp, end_timestamp)

    def _update_duration(self, start_timestamp, end_timestamp):
        # "Clamp" the timestamps we return to the original time
        # range, excluding the offset.
        if (start_timestamp and
                self.duration_start and
                self.duration_start < start_timestamp):
            self.duration_start = start_timestamp
            LOG.debug(_('clamping min timestamp to range'))
        if (end_timestamp and
                self.duration_end and
                self.duration_end > end_timestamp):
            self.duration_end = end_timestamp
            LOG.debug(_('clamping max timestamp to range'))

        # If we got valid timestamps back, compute a duration in seconds.
        #
        # If the min > max after clamping then we know the
        # timestamps on the samples fell outside of the time
        # range we care about for the query, so treat them as
        # "invalid."
        #
        # If the timestamps are invalid, return None as a
        # sentinel indicating that there is something "funny"
        # about the range.
        if (self.duration_start and
                self.duration_end and
                self.duration_start <= self.duration_end):
            self.duration = timeutils.delta_seconds(self.duration_start,
                                                    self.duration_end)
        else:
            self.duration_start = self.duration_end = self.duration = None

    @classmethod
    def sample(cls):
        return cls(unit='GiB',
                   min=1,
                   max=9,
                   avg=4.5,
                   sum=45,
                   count=10,
                   duration_start=datetime.datetime(2013, 1, 4, 16, 42),
                   duration_end=datetime.datetime(2013, 1, 4, 16, 47),
                   period=7200,
                   period_start=datetime.datetime(2013, 1, 4, 16, 00),
                   period_end=datetime.datetime(2013, 1, 4, 18, 00),
                   )


class Aggregate(_Base):

    func = wsme.wsattr(wtypes.text, mandatory=True)
    "The aggregation function name"

    param = wsme.wsattr(wtypes.text, default=None)
    "The paramter to the aggregation function"

    def __init__(self, **kwargs):
        super(Aggregate, self).__init__(**kwargs)

    @staticmethod
    def validate(aggregate):
        return aggregate

    @classmethod
    def sample(cls):
        return cls(func='cardinality',
                   param='resource_id')


class MeterController(rest.RestController):
    """Manages operations on a single meter.
    """
    _custom_actions = {
        'statistics': ['GET'],
    }

    def __init__(self, meter_name):
        pecan.request.context['meter_name'] = meter_name
        self.meter_name = meter_name

    @wsme_pecan.wsexpose([OldSample], [Query], int)
    def get_all(self, q=None, limit=None):
        """Return samples for the meter.

        :param q: Filter rules for the data to be returned.
        :param limit: Maximum number of samples to return.
        """
        q = q or []
        if limit and limit < 0:
            raise ClientSideError(_("Limit must be positive"))
        kwargs = _query_to_kwargs(q, storage.SampleFilter.__init__)
        kwargs['meter'] = self.meter_name
        f = storage.SampleFilter(**kwargs)
        return [OldSample.from_db_model(e)
                for e in pecan.request.storage_conn.get_samples(f, limit=limit)
                ]

    @wsme_pecan.wsexpose([OldSample], body=[OldSample])
    def post(self, samples):
        """Post a list of new Samples to Telemetry.

        :param samples: a list of samples within the request body.
        """
        now = timeutils.utcnow()
        auth_project = acl.get_limited_to_project(pecan.request.headers)
        def_source = pecan.request.cfg.sample_source
        def_project_id = pecan.request.headers.get('X-Project-Id')
        def_user_id = pecan.request.headers.get('X-User-Id')

        published_samples = []
        for s in samples:
            if self.meter_name != s.counter_name:
                raise wsme.exc.InvalidInput('counter_name', s.counter_name,
                                            'should be %s' % self.meter_name)

            if s.message_id:
                raise wsme.exc.InvalidInput('message_id', s.message_id,
                                            'The message_id must not be set')

            if s.counter_type not in sample.TYPES:
                raise wsme.exc.InvalidInput('counter_type', s.counter_type,
                                            'The counter type must be: ' +
                                            ', '.join(sample.TYPES))

            s.user_id = (s.user_id or def_user_id)
            s.project_id = (s.project_id or def_project_id)
            s.source = '%s:%s' % (s.project_id, (s.source or def_source))
            s.timestamp = (s.timestamp or now)

            if auth_project and auth_project != s.project_id:
                # non admin user trying to cross post to another project_id
                auth_msg = 'can not post samples to other projects'
                raise wsme.exc.InvalidInput('project_id', s.project_id,
                                            auth_msg)

            published_sample = sample.Sample(
                name=s.counter_name,
                type=s.counter_type,
                unit=s.counter_unit,
                volume=s.counter_volume,
                user_id=s.user_id,
                project_id=s.project_id,
                resource_id=s.resource_id,
                timestamp=s.timestamp.isoformat(),
                resource_metadata=utils.restore_nesting(s.resource_metadata,
                                                        separator='.'),
                source=s.source)
            published_samples.append(published_sample)

            s.message_id = published_sample.id

        with pecan.request.pipeline_manager.publisher(
                context.get_admin_context()) as publisher:
            publisher(published_samples)

        return samples

    @wsme_pecan.wsexpose([Statistics], [Query], [unicode], int, [Aggregate])
    def statistics(self, q=None, groupby=None, period=None, aggregate=None):
        """Computes the statistics of the samples in the time range given.

        :param q: Filter rules for the data to be returned.
        :param groupby: Fields for group by aggregation
        :param period: Returned result will be an array of statistics for a
                       period long of that number of seconds.
        :param aggregate: The selectable aggregation functions to be applied.
        """
        q = q or []
        groupby = groupby or []
        aggregate = aggregate or []

        if period and period < 0:
            raise ClientSideError(_("Period must be positive."))

        kwargs = _query_to_kwargs(q, storage.SampleFilter.__init__)
        kwargs['meter'] = self.meter_name
        f = storage.SampleFilter(**kwargs)
        g = _validate_groupby_fields(groupby)

        aggregate = utils.uniq(aggregate, ['func', 'param'])
        computed = pecan.request.storage_conn.get_meter_statistics(f,
                                                                   period,
                                                                   g,
                                                                   aggregate)
        LOG.debug(_('computed value coming from %r'),
                  pecan.request.storage_conn)
        # Find the original timestamp in the query to use for clamping
        # the duration returned in the statistics.
        start = end = None
        for i in q:
            if i.field == 'timestamp' and i.op in ('lt', 'le'):
                end = timeutils.parse_isotime(i.value).replace(tzinfo=None)
            elif i.field == 'timestamp' and i.op in ('gt', 'ge'):
                start = timeutils.parse_isotime(i.value).replace(tzinfo=None)

        return [Statistics(start_timestamp=start,
                           end_timestamp=end,
                           **c.as_dict())
                for c in computed]


class Meter(_Base):
    """One category of measurements.
    """

    name = wtypes.text
    "The unique name for the meter"

    type = wtypes.Enum(str, *sample.TYPES)
    "The meter type (see :ref:`measurements`)"

    unit = wtypes.text
    "The unit of measure"

    resource_id = wtypes.text
    "The ID of the :class:`Resource` for which the measurements are taken"

    project_id = wtypes.text
    "The ID of the project or tenant that owns the resource"

    user_id = wtypes.text
    "The ID of the user who last triggered an update to the resource"

    source = wtypes.text
    "The ID of the source that identifies where the meter comes from"

    meter_id = wtypes.text
    "The unique identifier for the meter"

    def __init__(self, **kwargs):
        meter_id = base64.encodestring('%s+%s' % (kwargs['resource_id'],
                                                  kwargs['name']))
        kwargs['meter_id'] = meter_id
        super(Meter, self).__init__(**kwargs)

    @classmethod
    def sample(cls):
        return cls(name='instance',
                   type='gauge',
                   unit='instance',
                   resource_id='bd9431c1-8d69-4ad3-803a-8d4a6b89fd36',
                   project_id='35b17138-b364-4e6a-a131-8f3099c5be68',
                   user_id='efd87807-12d2-4b38-9c70-5f5c2ac427ff',
                   source='openstack',
                   )


class MetersController(rest.RestController):
    """Works on meters."""

    @pecan.expose()
    def _lookup(self, meter_name, *remainder):
        return MeterController(meter_name), remainder

    @wsme_pecan.wsexpose([Meter], [Query])
    def get_all(self, q=None):
        """Return all known meters, based on the data recorded so far.

        :param q: Filter rules for the meters to be returned.
        """
        q = q or []

        #Timestamp field is not supported for Meter queries
        kwargs = _query_to_kwargs(q, pecan.request.storage_conn.get_meters,
                                  allow_timestamps=False)
        return [Meter.from_db_model(m)
                for m in pecan.request.storage_conn.get_meters(**kwargs)]


class Sample(_Base):
    """One measurement."""

    id = wtypes.text
    "The unique identifier for the sample."

    meter = wtypes.text
    "The meter name this sample is for."

    type = wtypes.Enum(str, *sample.TYPES)
    "The meter type (see :ref:`measurements`)"

    unit = wtypes.text
    "The unit of measure."

    volume = float
    "The metered value."

    user_id = wtypes.text
    "The user this sample was taken for."

    project_id = wtypes.text
    "The project this sample was taken for."

    resource_id = wtypes.text
    "The :class:`Resource` this sample was taken for."

    source = wtypes.text
    "The source that identifies where the sample comes from."

    timestamp = datetime.datetime
    "When the sample has been generated."

    recorded_at = datetime.datetime
    "When the sample has been recorded."

    metadata = {wtypes.text: wtypes.text}
    "Arbitrary metadata associated with the sample."

    @classmethod
    def from_db_model(cls, m):
        return cls(id=m.message_id,
                   meter=m.counter_name,
                   type=m.counter_type,
                   unit=m.counter_unit,
                   volume=m.counter_volume,
                   user_id=m.user_id,
                   project_id=m.project_id,
                   resource_id=m.resource_id,
                   source=m.source,
                   timestamp=m.timestamp,
                   recorded_at=m.recorded_at,
                   metadata=_flatten_metadata(m.resource_metadata))

    @classmethod
    def sample(cls):
        return cls(id=str(uuid.uuid1()),
                   meter='instance',
                   type='gauge',
                   unit='instance',
                   volume=1,
                   resource_id='bd9431c1-8d69-4ad3-803a-8d4a6b89fd36',
                   project_id='35b17138-b364-4e6a-a131-8f3099c5be68',
                   user_id='efd87807-12d2-4b38-9c70-5f5c2ac427ff',
                   timestamp=timeutils.utcnow(),
                   recorded_at=datetime.datetime.utcnow(),
                   source='openstack',
                   metadata={'name1': 'value1',
                             'name2': 'value2'},
                   )


class SamplesController(rest.RestController):
    """Controller managing the samples."""

    @wsme_pecan.wsexpose([Sample], [Query], int)
    def get_all(self, q=None, limit=None):
        """Return all known samples, based on the data recorded so far.

        :param q: Filter rules for the samples to be returned.
        :param limit: Maximum number of samples to be returned.
        """
        q = q or []

        if limit and limit < 0:
            raise ClientSideError(_("Limit must be positive"))
        kwargs = _query_to_kwargs(q, storage.SampleFilter.__init__)
        f = storage.SampleFilter(**kwargs)
        return map(Sample.from_db_model,
                   pecan.request.storage_conn.get_samples(f, limit=limit))

    @wsme_pecan.wsexpose(Sample, wtypes.text)
    def get_one(self, sample_id):
        """Return a sample

        :param sample_id: the id of the sample
        """
        f = storage.SampleFilter(message_id=sample_id)

        samples = list(pecan.request.storage_conn.get_samples(f))
        if len(samples) < 1:
            raise EntityNotFound(_('Sample'), sample_id)

        return Sample.from_db_model(samples[0])


class ComplexQuery(_Base):
    """Holds a sample query encoded in json."""

    filter = wtypes.text
    "The filter expression encoded in json."

    orderby = wtypes.text
    "List of single-element dicts for specifing the ordering of the results."

    limit = int
    "The maximum number of results to be returned."

    @classmethod
    def sample(cls):
        return cls(filter='{"and": [{"and": [{"=": ' +
                          '{"counter_name": "cpu_util"}}, ' +
                          '{">": {"counter_volume": 0.23}}, ' +
                          '{"<": {"counter_volume": 0.26}}]}, ' +
                          '{"or": [{"and": [{">": ' +
                          '{"timestamp": "2013-12-01T18:00:00"}}, ' +
                          '{"<": ' +
                          '{"timestamp": "2013-12-01T18:15:00"}}]}, ' +
                          '{"and": [{">": ' +
                          '{"timestamp": "2013-12-01T18:30:00"}}, ' +
                          '{"<": ' +
                          '{"timestamp": "2013-12-01T18:45:00"}}]}]}]}',
                   orderby='[{"counter_volume": "ASC"}, ' +
                           '{"timestamp": "DESC"}]',
                   limit=42
                   )


def _list_to_regexp(items, regexp_prefix=""):
    regexp = ["^%s$" % item for item in items]
    regexp = regexp_prefix + "|".join(regexp)
    return regexp


class ValidatedComplexQuery(object):
    complex_operators = ["and", "or"]
    order_directions = ["asc", "desc"]
    simple_ops = ["=", "!=", "<", ">", "<=", "=<", ">=", "=>"]
    regexp_prefix = "(?i)"

    complex_ops = _list_to_regexp(complex_operators, regexp_prefix)
    simple_ops = _list_to_regexp(simple_ops, regexp_prefix)
    order_directions = _list_to_regexp(order_directions, regexp_prefix)

    timestamp_fields = ["timestamp", "state_timestamp"]

    def __init__(self, query, db_model, additional_name_mapping=None,
                 metadata_allowed=False):
        additional_name_mapping = additional_name_mapping or {}
        self.name_mapping = {"user": "user_id",
                             "project": "project_id"}
        self.name_mapping.update(additional_name_mapping)
        valid_keys = db_model.get_field_names()
        valid_keys = list(valid_keys) + self.name_mapping.keys()
        valid_fields = _list_to_regexp(valid_keys)

        if metadata_allowed:
            valid_filter_fields = valid_fields + "|^metadata\.[\S]+$"
        else:
            valid_filter_fields = valid_fields

        schema_value = {
            "oneOf": [{"type": "string"},
                      {"type": "number"},
                      {"type": "boolean"}],
            "minProperties": 1,
            "maxProperties": 1}

        schema_value_in = {
            "type": "array",
            "items": {"oneOf": [{"type": "string"},
                                {"type": "number"}]},
            "minItems": 1}

        schema_field = {
            "type": "object",
            "patternProperties": {valid_filter_fields: schema_value},
            "additionalProperties": False,
            "minProperties": 1,
            "maxProperties": 1}

        schema_field_in = {
            "type": "object",
            "patternProperties": {valid_filter_fields: schema_value_in},
            "additionalProperties": False,
            "minProperties": 1,
            "maxProperties": 1}

        schema_leaf_in = {
            "type": "object",
            "patternProperties": {"(?i)^in$": schema_field_in},
            "additionalProperties": False,
            "minProperties": 1,
            "maxProperties": 1}

        schema_leaf_simple_ops = {
            "type": "object",
            "patternProperties": {self.simple_ops: schema_field},
            "additionalProperties": False,
            "minProperties": 1,
            "maxProperties": 1}

        schema_and_or_array = {
            "type": "array",
            "items": {"$ref": "#"},
            "minItems": 2}

        schema_and_or = {
            "type": "object",
            "patternProperties": {self.complex_ops: schema_and_or_array},
            "additionalProperties": False,
            "minProperties": 1,
            "maxProperties": 1}

        schema_not = {
            "type": "object",
            "patternProperties": {"(?i)^not$": {"$ref": "#"}},
            "additionalProperties": False,
            "minProperties": 1,
            "maxProperties": 1}

        self.schema = {
            "oneOf": [{"$ref": "#/definitions/leaf_simple_ops"},
                      {"$ref": "#/definitions/leaf_in"},
                      {"$ref": "#/definitions/and_or"},
                      {"$ref": "#/definitions/not"}],
            "minProperties": 1,
            "maxProperties": 1,
            "definitions": {"leaf_simple_ops": schema_leaf_simple_ops,
                            "leaf_in": schema_leaf_in,
                            "and_or": schema_and_or,
                            "not": schema_not}}

        self.orderby_schema = {
            "type": "array",
            "items": {
                "type": "object",
                "patternProperties":
                    {valid_fields:
                        {"type": "string",
                         "pattern": self.order_directions}},
                "additionalProperties": False,
                "minProperties": 1,
                "maxProperties": 1}}

        self.original_query = query

    def validate(self, visibility_field):
        """Validates the query content and does the necessary transformations.
        """
        if self.original_query.filter is wtypes.Unset:
            self.filter_expr = None
        else:
            self.filter_expr = json.loads(self.original_query.filter)
            self._validate_filter(self.filter_expr)
            self._replace_isotime_with_datetime(self.filter_expr)
            self._convert_operator_to_lower_case(self.filter_expr)
            self._normalize_field_names_for_db_model(self.filter_expr)

        self._force_visibility(visibility_field)

        if self.original_query.orderby is wtypes.Unset:
            self.orderby = None
        else:
            self.orderby = json.loads(self.original_query.orderby)
            self._validate_orderby(self.orderby)
            self._convert_orderby_to_lower_case(self.orderby)
            self._normalize_field_names_in_orderby(self.orderby)

        if self.original_query.limit is wtypes.Unset:
            self.limit = None
        else:
            self.limit = self.original_query.limit

        if self.limit is not None and self.limit <= 0:
            msg = _('Limit should be positive')
            raise ClientSideError(msg)

    @staticmethod
    def _convert_orderby_to_lower_case(orderby):
        for orderby_field in orderby:
            utils.lowercase_values(orderby_field)

    def _normalize_field_names_in_orderby(self, orderby):
        for orderby_field in orderby:
            self._replace_field_names(orderby_field)

    def _traverse_postorder(self, tree, visitor):
        op = tree.keys()[0]
        if op.lower() in self.complex_operators:
            for i, operand in enumerate(tree[op]):
                self._traverse_postorder(operand, visitor)
        if op.lower() == "not":
            self._traverse_postorder(tree[op], visitor)

        visitor(tree)

    def _check_cross_project_references(self, own_project_id,
                                        visibility_field):
        """Do not allow other than own_project_id
        """
        def check_project_id(subfilter):
            op = subfilter.keys()[0]
            if (op.lower() not in self.complex_operators
                    and subfilter[op].keys()[0] == visibility_field
                    and subfilter[op][visibility_field] != own_project_id):
                raise ProjectNotAuthorized(subfilter[op][visibility_field])

        self._traverse_postorder(self.filter_expr, check_project_id)

    def _force_visibility(self, visibility_field):
        """If the tenant is not admin insert an extra
        "and <visibility_field>=<tenant's project_id>" clause to the query
        """
        authorized_project = acl.get_limited_to_project(pecan.request.headers)
        is_admin = authorized_project is None
        if not is_admin:
            self._restrict_to_project(authorized_project, visibility_field)
            self._check_cross_project_references(authorized_project,
                                                 visibility_field)

    def _restrict_to_project(self, project_id, visibility_field):
        restriction = {"=": {visibility_field: project_id}}
        if self.filter_expr is None:
            self.filter_expr = restriction
        else:
            self.filter_expr = {"and": [restriction, self.filter_expr]}

    def _replace_isotime_with_datetime(self, filter_expr):
        def replace_isotime(subfilter):
            op = subfilter.keys()[0]
            if (op.lower() not in self.complex_operators
                    and subfilter[op].keys()[0] in self.timestamp_fields):
                field = subfilter[op].keys()[0]
                date_time = self._convert_to_datetime(subfilter[op][field])
                subfilter[op][field] = date_time

        self._traverse_postorder(filter_expr, replace_isotime)

    def _normalize_field_names_for_db_model(self, filter_expr):
        def _normalize_field_names(subfilter):
            op = subfilter.keys()[0]
            if op.lower() not in self.complex_operators:
                self._replace_field_names(subfilter.values()[0])
        self._traverse_postorder(filter_expr,
                                 _normalize_field_names)

    def _replace_field_names(self, subfilter):
        field = subfilter.keys()[0]
        value = subfilter[field]
        if field in self.name_mapping:
            del subfilter[field]
            subfilter[self.name_mapping[field]] = value
        if field.startswith("metadata."):
            del subfilter[field]
            subfilter["resource_" + field] = value

    def _convert_operator_to_lower_case(self, filter_expr):
        self._traverse_postorder(filter_expr, utils.lowercase_keys)

    @staticmethod
    def _convert_to_datetime(isotime):
        try:
            date_time = timeutils.parse_isotime(isotime)
            date_time = date_time.replace(tzinfo=None)
            return date_time
        except ValueError:
            LOG.exception(_("String %s is not a valid isotime") % isotime)
            msg = _('Failed to parse the timestamp value %s') % isotime
            raise ClientSideError(msg)

    def _validate_filter(self, filter_expr):
        jsonschema.validate(filter_expr, self.schema)

    def _validate_orderby(self, orderby_expr):
        jsonschema.validate(orderby_expr, self.orderby_schema)


class Resource(_Base):
    """An externally defined object for which samples have been received.
    """

    resource_id = wtypes.text
    "The unique identifier for the resource"

    project_id = wtypes.text
    "The ID of the owning project or tenant"

    user_id = wtypes.text
    "The ID of the user who created the resource or updated it last"

    first_sample_timestamp = datetime.datetime
    "UTC date & time not later than the first sample known for this resource"

    last_sample_timestamp = datetime.datetime
    "UTC date & time not earlier than the last sample known for this resource"

    metadata = {wtypes.text: wtypes.text}
    "Arbitrary metadata associated with the resource"

    links = [Link]
    "A list containing a self link and associated meter links"

    source = wtypes.text
    "The source where the resource come from"

    def __init__(self, metadata=None, **kwds):
        metadata = metadata or {}
        metadata = _flatten_metadata(metadata)
        super(Resource, self).__init__(metadata=metadata, **kwds)

    @classmethod
    def sample(cls):
        return cls(resource_id='bd9431c1-8d69-4ad3-803a-8d4a6b89fd36',
                   project_id='35b17138-b364-4e6a-a131-8f3099c5be68',
                   user_id='efd87807-12d2-4b38-9c70-5f5c2ac427ff',
                   timestamp=datetime.datetime.utcnow(),
                   source="openstack",
                   metadata={'name1': 'value1',
                             'name2': 'value2'},
                   links=[Link(href=('http://localhost:8777/v2/resources/'
                                     'bd9431c1-8d69-4ad3-803a-8d4a6b89fd36'),
                               rel='self'),
                          Link(href=('http://localhost:8777/v2/meters/volume?'
                                     'q.field=resource_id&'
                                     'q.value=bd9431c1-8d69-4ad3-803a-'
                                     '8d4a6b89fd36'),
                               rel='volume')],
                   )


class ResourcesController(rest.RestController):
    """Works on resources."""

    def _resource_links(self, resource_id, meter_links=1):
        links = [_make_link('self', pecan.request.host_url, 'resources',
                            resource_id)]
        if meter_links:
            for meter in pecan.request.storage_conn.get_meters(resource=
                                                               resource_id):
                query = {'field': 'resource_id', 'value': resource_id}
                links.append(_make_link(meter.name, pecan.request.host_url,
                                        'meters', meter.name, query=query))
        return links

    @wsme_pecan.wsexpose(Resource, unicode)
    def get_one(self, resource_id):
        """Retrieve details about one resource.

        :param resource_id: The UUID of the resource.
        """
        authorized_project = acl.get_limited_to_project(pecan.request.headers)
        resources = list(pecan.request.storage_conn.get_resources(
            resource=resource_id, project=authorized_project))
        if not resources:
            raise EntityNotFound(_('Resource'), resource_id)
        return Resource.from_db_and_links(resources[0],
                                          self._resource_links(resource_id))

    @wsme_pecan.wsexpose([Resource], [Query], int)
    def get_all(self, q=None, meter_links=1):
        """Retrieve definitions of all of the resources.

        :param q: Filter rules for the resources to be returned.
        :param meter_links: option to include related meter links
        """
        q = q or []
        kwargs = _query_to_kwargs(q, pecan.request.storage_conn.get_resources)
        resources = [
            Resource.from_db_and_links(r,
                                       self._resource_links(r.resource_id,
                                                            meter_links))
            for r in pecan.request.storage_conn.get_resources(**kwargs)]
        return resources


class AlarmThresholdRule(_Base):
    meter_name = wsme.wsattr(wtypes.text, mandatory=True)
    "The name of the meter"

    #FIXME(sileht): default doesn't work
    #workaround: default is set in validate method
    query = wsme.wsattr([Query], default=[])
    """The query to find the data for computing statistics.
    Ownership settings are automatically included based on the Alarm owner.
    """

    period = wsme.wsattr(wtypes.IntegerType(minimum=1), default=60)
    "The time range in seconds over which query"

    comparison_operator = AdvEnum('comparison_operator', str,
                                  'lt', 'le', 'eq', 'ne', 'ge', 'gt',
                                  default='eq')
    "The comparison against the alarm threshold"

    threshold = wsme.wsattr(float, mandatory=True)
    "The threshold of the alarm"

    statistic = AdvEnum('statistic', str, 'max', 'min', 'avg', 'sum',
                        'count', default='avg')
    "The statistic to compare to the threshold"

    evaluation_periods = wsme.wsattr(wtypes.IntegerType(minimum=1), default=1)
    "The number of historical periods to evaluate the threshold"

    exclude_outliers = wsme.wsattr(bool, default=False)
    "Whether datapoints with anomalously low sample counts are excluded"

    def __init__(self, query=None, **kwargs):
        if query:
            query = [Query(**q) for q in query]
        super(AlarmThresholdRule, self).__init__(query=query, **kwargs)

    @staticmethod
    def validate(threshold_rule):
        #note(sileht): wsme default doesn't work in some case
        #workaround for https://bugs.launchpad.net/wsme/+bug/1227039
        if not threshold_rule.query:
            threshold_rule.query = []

        #Timestamp is not allowed for AlarmThresholdRule query, as the alarm
        #evaluator will construct timestamp bounds for the sequence of
        #statistics queries as the sliding evaluation window advances
        #over time.
        _validate_query(threshold_rule.query, storage.SampleFilter.__init__,
                        allow_timestamps=False)
        return threshold_rule

    @property
    def default_description(self):
        return _(
            'Alarm when %(meter_name)s is %(comparison_operator)s a '
            '%(statistic)s of %(threshold)s over %(period)s seconds') % \
            dict(comparison_operator=self.comparison_operator,
                 statistic=self.statistic,
                 threshold=self.threshold,
                 meter_name=self.meter_name,
                 period=self.period)

    def as_dict(self):
        rule = self.as_dict_from_keys(['period', 'comparison_operator',
                                       'threshold', 'statistic',
                                       'evaluation_periods', 'meter_name',
                                       'exclude_outliers'])
        rule['query'] = [q.as_dict() for q in self.query]
        return rule

    @classmethod
    def sample(cls):
        return cls(meter_name='cpu_util',
                   period=60,
                   evaluation_periods=1,
                   threshold=300.0,
                   statistic='avg',
                   comparison_operator='gt',
                   query=[{'field': 'resource_id',
                           'value': '2a4d689b-f0b8-49c1-9eef-87cae58d80db',
                           'op': 'eq',
                           'type': 'string'}])


class AlarmCombinationRule(_Base):
    operator = AdvEnum('operator', str, 'or', 'and', default='and')
    "How to combine the sub-alarms"

    alarm_ids = wsme.wsattr([wtypes.text], mandatory=True)
    "List of alarm identifiers to combine"

    @property
    def default_description(self):
        joiner = ' %s ' % self.operator
        return _('Combined state of alarms %s') % joiner.join(self.alarm_ids)

    def as_dict(self):
        return self.as_dict_from_keys(['operator', 'alarm_ids'])

    @staticmethod
    def validate(rule):
        rule.alarm_ids = sorted(set(rule.alarm_ids), key=rule.alarm_ids.index)
        if len(rule.alarm_ids) <= 1:
            raise ClientSideError(_('Alarm combination rule should contain at'
                                    ' least two different alarm ids.'))
        return rule

    @classmethod
    def sample(cls):
        return cls(operator='or',
                   alarm_ids=['739e99cb-c2ec-4718-b900-332502355f38',
                              '153462d0-a9b8-4b5b-8175-9e4b05e9b856'])


class AlarmTimeConstraint(_Base):
    """Representation of a time constraint on an alarm."""

    name = wsme.wsattr(wtypes.text, mandatory=True)
    "The name of the constraint"

    _description = None  # provide a default

    def get_description(self):
        if not self._description:
            return 'Time constraint at %s lasting for %s seconds' \
                   % (self.start, self.duration)
        return self._description

    def set_description(self, value):
        self._description = value

    description = wsme.wsproperty(wtypes.text, get_description,
                                  set_description)
    "The description of the constraint"

    start = wsme.wsattr(CronType(), mandatory=True)
    "Start point of the time constraint, in cron format"

    duration = wsme.wsattr(wtypes.IntegerType(minimum=0), mandatory=True)
    "How long the constraint should last, in seconds"

    timezone = wsme.wsattr(wtypes.text, default="")
    "Timezone of the constraint"

    def as_dict(self):
        return self.as_dict_from_keys(['name', 'description', 'start',
                                       'duration', 'timezone'])

    @staticmethod
    def validate(tc):
        if tc.timezone:
            try:
                pytz.timezone(tc.timezone)
            except Exception:
                raise ClientSideError(_("Timezone %s is not valid")
                                      % tc.timezone)
        return tc

    @classmethod
    def sample(cls):
        return cls(name='SampleConstraint',
                   description='nightly build every night at 23h for 3 hours',
                   start='0 23 * * *',
                   duration=10800,
                   timezone='Europe/Ljubljana')


class Alarm(_Base):
    """Representation of an alarm.

    .. note::
        combination_rule and threshold_rule are mutually exclusive. The *type*
        of the alarm should be set to *threshold* or *combination* and the
        appropriate rule should be filled.
    """

    alarm_id = wtypes.text
    "The UUID of the alarm"

    name = wsme.wsattr(wtypes.text, mandatory=True)
    "The name for the alarm"

    _description = None  # provide a default

    def get_description(self):
        rule = getattr(self, '%s_rule' % self.type, None)
        if not self._description and rule:
            return six.text_type(rule.default_description)
        return self._description

    def set_description(self, value):
        self._description = value

    description = wsme.wsproperty(wtypes.text, get_description,
                                  set_description)
    "The description of the alarm"

    enabled = wsme.wsattr(bool, default=True)
    "This alarm is enabled?"

    ok_actions = wsme.wsattr([wtypes.text], default=[])
    "The actions to do when alarm state change to ok"

    alarm_actions = wsme.wsattr([wtypes.text], default=[])
    "The actions to do when alarm state change to alarm"

    insufficient_data_actions = wsme.wsattr([wtypes.text], default=[])
    "The actions to do when alarm state change to insufficient data"

    repeat_actions = wsme.wsattr(bool, default=False)
    "The actions should be re-triggered on each evaluation cycle"

    type = AdvEnum('type', str, 'threshold', 'combination', mandatory=True)
    "Explicit type specifier to select which rule to follow below."

    threshold_rule = AlarmThresholdRule
    "Describe when to trigger the alarm based on computed statistics"

    combination_rule = AlarmCombinationRule
    """Describe when to trigger the alarm based on combining the state of
    other alarms"""

    time_constraints = wtypes.wsattr([AlarmTimeConstraint], default=[])
    """Describe time constraints for the alarm"""

    # These settings are ignored in the PUT or POST operations, but are
    # filled in for GET
    project_id = wtypes.text
    "The ID of the project or tenant that owns the alarm"

    user_id = wtypes.text
    "The ID of the user who created the alarm"

    timestamp = datetime.datetime
    "The date of the last alarm definition update"

    state = AdvEnum('state', str, *state_kind,
                    default='insufficient data')
    "The state offset the alarm"

    state_timestamp = datetime.datetime
    "The date of the last alarm state changed"

    def __init__(self, rule=None, time_constraints=None, **kwargs):
        super(Alarm, self).__init__(**kwargs)

        if rule:
            if self.type == 'threshold':
                self.threshold_rule = AlarmThresholdRule(**rule)
            elif self.type == 'combination':
                self.combination_rule = AlarmCombinationRule(**rule)
        if time_constraints:
            self.time_constraints = [AlarmTimeConstraint(**tc)
                                     for tc in time_constraints]

    @staticmethod
    def validate(alarm):

        Alarm.check_rule(alarm)
        if alarm.threshold_rule:
            # ensure an implicit constraint on project_id is added to
            # the query if not already present
            alarm.threshold_rule.query = _sanitize_query(
                alarm.threshold_rule.query,
                storage.SampleFilter.__init__,
                on_behalf_of=alarm.project_id
            )
        elif alarm.combination_rule:
            project = _get_auth_project(alarm.project_id
                                        if alarm.project_id != wtypes.Unset
                                        else None)
            for id in alarm.combination_rule.alarm_ids:
                alarms = list(pecan.request.storage_conn.get_alarms(
                    alarm_id=id, project=project))
                if not alarms:
                    raise EntityNotFound(_('Alarm'), id)

        tc_names = [tc.name for tc in alarm.time_constraints]
        if len(tc_names) > len(set(tc_names)):
            error = _("Time constraint names must be "
                      "unique for a given alarm.")
            raise ClientSideError(error)

        return alarm

    @staticmethod
    def check_rule(alarm):
        rule = '%s_rule' % alarm.type
        if getattr(alarm, rule) in (wtypes.Unset, None):
            error = _("%(rule)s must be set for %(type)s"
                      " type alarm") % {"rule": rule, "type": alarm.type}
            raise ClientSideError(error)
        if alarm.threshold_rule and alarm.combination_rule:
            error = _("threshold_rule and combination_rule "
                      "cannot be set at the same time")
            raise ClientSideError(error)

    @classmethod
    def sample(cls):
        return cls(alarm_id=None,
                   name="SwiftObjectAlarm",
                   description="An alarm",
                   type='combination',
                   threshold_rule=None,
                   combination_rule=AlarmCombinationRule.sample(),
                   time_constraints=[AlarmTimeConstraint.sample().as_dict()],
                   user_id="c96c887c216949acbdfbd8b494863567",
                   project_id="c96c887c216949acbdfbd8b494863567",
                   enabled=True,
                   timestamp=datetime.datetime.utcnow(),
                   state="ok",
                   state_timestamp=datetime.datetime.utcnow(),
                   ok_actions=["http://site:8000/ok"],
                   alarm_actions=["http://site:8000/alarm"],
                   insufficient_data_actions=["http://site:8000/nodata"],
                   repeat_actions=False,
                   )

    def as_dict(self, db_model):
        d = super(Alarm, self).as_dict(db_model)
        for k in d:
            if k.endswith('_rule'):
                del d[k]
        d['rule'] = getattr(self, "%s_rule" % self.type).as_dict()
        d['time_constraints'] = [tc.as_dict() for tc in self.time_constraints]
        return d


class AlarmChange(_Base):
    """Representation of an event in an alarm's history
    """

    event_id = wtypes.text
    "The UUID of the change event"

    alarm_id = wtypes.text
    "The UUID of the alarm"

    type = wtypes.Enum(str,
                       'creation',
                       'rule change',
                       'state transition',
                       'deletion')
    "The type of change"

    detail = wtypes.text
    "JSON fragment describing change"

    project_id = wtypes.text
    "The project ID of the initiating identity"

    user_id = wtypes.text
    "The user ID of the initiating identity"

    on_behalf_of = wtypes.text
    "The tenant on behalf of which the change is being made"

    timestamp = datetime.datetime
    "The time/date of the alarm change"

    @classmethod
    def sample(cls):
        return cls(alarm_id='e8ff32f772a44a478182c3fe1f7cad6a',
                   type='rule change',
                   detail='{"threshold": 42.0, "evaluation_periods": 4}',
                   user_id="3e5d11fda79448ac99ccefb20be187ca",
                   project_id="b6f16144010811e387e4de429e99ee8c",
                   on_behalf_of="92159030020611e3b26dde429e99ee8c",
                   timestamp=datetime.datetime.utcnow(),
                   )


class AlarmController(rest.RestController):
    """Manages operations on a single alarm.
    """

    _custom_actions = {
        'history': ['GET'],
        'state': ['PUT', 'GET'],
    }

    def __init__(self, alarm_id):
        pecan.request.context['alarm_id'] = alarm_id
        self._id = alarm_id

    def _alarm(self):
        self.conn = pecan.request.storage_conn
        auth_project = acl.get_limited_to_project(pecan.request.headers)
        alarms = list(self.conn.get_alarms(alarm_id=self._id,
                                           project=auth_project))
        if not alarms:
            raise EntityNotFound(_('Alarm'), self._id)
        return alarms[0]

    def _record_change(self, data, now, on_behalf_of=None, type=None):
        if not cfg.CONF.alarm.record_history:
            return
        type = type or storage.models.AlarmChange.RULE_CHANGE
        scrubbed_data = utils.stringify_timestamps(data)
        detail = json.dumps(scrubbed_data)
        user_id = pecan.request.headers.get('X-User-Id')
        project_id = pecan.request.headers.get('X-Project-Id')
        on_behalf_of = on_behalf_of or project_id
        payload = dict(event_id=str(uuid.uuid4()),
                       alarm_id=self._id,
                       type=type,
                       detail=detail,
                       user_id=user_id,
                       project_id=project_id,
                       on_behalf_of=on_behalf_of,
                       timestamp=now)

        try:
            self.conn.record_alarm_change(payload)
        except NotImplementedError:
            pass

        # Revert to the pre-json'ed details ...
        payload['detail'] = scrubbed_data
        _send_notification(type, payload)

    @wsme_pecan.wsexpose(Alarm)
    def get(self):
        """Return this alarm.
        """
        return Alarm.from_db_model(self._alarm())

    @wsme_pecan.wsexpose(Alarm, body=Alarm)
    def put(self, data):
        """Modify this alarm.

        :param data: an alarm within the request body.
        """
        # Ensure alarm exists
        alarm_in = self._alarm()

        now = timeutils.utcnow()

        data.alarm_id = self._id
        user, project = acl.get_limited_to(pecan.request.headers)
        if user:
            data.user_id = user
        elif data.user_id == wtypes.Unset:
            data.user_id = alarm_in.user_id
        if project:
            data.project_id = project
        elif data.project_id == wtypes.Unset:
            data.project_id = alarm_in.project_id
        data.timestamp = now
        if alarm_in.state != data.state:
            data.state_timestamp = now
        else:
            data.state_timestamp = alarm_in.state_timestamp

        # make sure alarms are unique by name per project.
        if alarm_in.name != data.name:
            alarms = list(self.conn.get_alarms(name=data.name,
                                               project=data.project_id))
            if alarms:
                raise ClientSideError(
                    _("Alarm with name=%s exists") % data.name,
                    status_code=409)

        # should check if there is any circle in the dependency, but for
        # efficiency reason, here only check alarm cannot depend on itself
        if data.type == 'combination':
            if self._id in data.combination_rule.alarm_ids:
                raise ClientSideError(_('Cannot specify alarm %s itself in '
                                        'combination rule') % self._id)

        old_alarm = Alarm.from_db_model(alarm_in).as_dict(storage.models.Alarm)
        updated_alarm = data.as_dict(storage.models.Alarm)
        try:
            alarm_in = storage.models.Alarm(**updated_alarm)
        except Exception:
            LOG.exception(_("Error while putting alarm: %s") % updated_alarm)
            raise ClientSideError(_("Alarm incorrect"))

        alarm = self.conn.update_alarm(alarm_in)

        change = dict((k, v) for k, v in updated_alarm.items()
                      if v != old_alarm[k] and k not in
                      ['timestamp', 'state_timestamp'])
        self._record_change(change, now, on_behalf_of=alarm.project_id)
        return Alarm.from_db_model(alarm)

    @wsme_pecan.wsexpose(None, status_code=204)
    def delete(self):
        """Delete this alarm.
        """
        # ensure alarm exists before deleting
        alarm = self._alarm()
        self.conn.delete_alarm(alarm.alarm_id)
        change = Alarm.from_db_model(alarm).as_dict(storage.models.Alarm)
        self._record_change(change,
                            timeutils.utcnow(),
                            type=storage.models.AlarmChange.DELETION)

    # TODO(eglynn): add pagination marker to signature once overall
    #               API support for pagination is finalized
    @wsme_pecan.wsexpose([AlarmChange], [Query])
    def history(self, q=None):
        """Assembles the alarm history requested.

        :param q: Filter rules for the changes to be described.
        """
        q = q or []
        # allow history to be returned for deleted alarms, but scope changes
        # returned to those carried out on behalf of the auth'd tenant, to
        # avoid inappropriate cross-tenant visibility of alarm history
        auth_project = acl.get_limited_to_project(pecan.request.headers)
        conn = pecan.request.storage_conn
        kwargs = _query_to_kwargs(q, conn.get_alarm_changes, ['on_behalf_of',
                                                              'alarm_id'])
        return [AlarmChange.from_db_model(ac)
                for ac in conn.get_alarm_changes(self._id, auth_project,
                                                 **kwargs)]

    @wsme.validate(state_kind_enum)
    @wsme_pecan.wsexpose(state_kind_enum, body=state_kind_enum)
    def put_state(self, state):
        """Set the state of this alarm.

        :param state: an alarm state within the request body.
        """
        # note(sileht): body are not validated by wsme
        # Workaround for https://bugs.launchpad.net/wsme/+bug/1227229
        if state not in state_kind:
            raise ClientSideError(_("state invalid"))
        now = timeutils.utcnow()
        alarm = self._alarm()
        alarm.state = state
        alarm.state_timestamp = now
        alarm = self.conn.update_alarm(alarm)
        change = {'state': alarm.state}
        self._record_change(change, now, on_behalf_of=alarm.project_id,
                            type=storage.models.AlarmChange.STATE_TRANSITION)
        return alarm.state

    @wsme_pecan.wsexpose(state_kind_enum)
    def get_state(self):
        """Get the state of this alarm.
        """
        alarm = self._alarm()
        return alarm.state


class AlarmsController(rest.RestController):
    """Manages operations on the alarms collection.
    """

    @pecan.expose()
    def _lookup(self, alarm_id, *remainder):
        return AlarmController(alarm_id), remainder

    def _record_creation(self, conn, data, alarm_id, now):
        if not cfg.CONF.alarm.record_history:
            return
        type = storage.models.AlarmChange.CREATION
        scrubbed_data = utils.stringify_timestamps(data)
        detail = json.dumps(scrubbed_data)
        user_id = pecan.request.headers.get('X-User-Id')
        project_id = pecan.request.headers.get('X-Project-Id')
        payload = dict(event_id=str(uuid.uuid4()),
                       alarm_id=alarm_id,
                       type=type,
                       detail=detail,
                       user_id=user_id,
                       project_id=project_id,
                       on_behalf_of=project_id,
                       timestamp=now)

        try:
            conn.record_alarm_change(payload)
        except NotImplementedError:
            pass

        # Revert to the pre-json'ed details ...
        payload['detail'] = scrubbed_data
        _send_notification(type, payload)

    @wsme_pecan.wsexpose(Alarm, body=Alarm, status_code=201)
    def post(self, data):
        """Create a new alarm.

        :param data: an alarm within the request body.
        """
        conn = pecan.request.storage_conn
        now = timeutils.utcnow()

        data.alarm_id = str(uuid.uuid4())
        user_limit, project_limit = acl.get_limited_to(pecan.request.headers)

        def _set_ownership(aspect, owner_limitation, header):
            attr = '%s_id' % aspect
            requested_owner = getattr(data, attr)
            explicit_owner = requested_owner != wtypes.Unset
            caller = pecan.request.headers.get(header)
            if (owner_limitation and explicit_owner
                    and requested_owner != caller):
                raise ProjectNotAuthorized(requested_owner, aspect)

            actual_owner = (owner_limitation or
                            requested_owner if explicit_owner else caller)
            setattr(data, attr, actual_owner)

        _set_ownership('user', user_limit, 'X-User-Id')
        _set_ownership('project', project_limit, 'X-Project-Id')

        data.timestamp = now
        data.state_timestamp = now

        change = data.as_dict(storage.models.Alarm)

        # make sure alarms are unique by name per project.
        alarms = list(conn.get_alarms(name=data.name,
                                      project=data.project_id))
        if alarms:
            raise ClientSideError(
                _("Alarm with name='%s' exists") % data.name,
                status_code=409)

        try:
            alarm_in = storage.models.Alarm(**change)
        except Exception:
            LOG.exception(_("Error while posting alarm: %s") % change)
            raise ClientSideError(_("Alarm incorrect"))

        alarm = conn.create_alarm(alarm_in)
        self._record_creation(conn, change, alarm.alarm_id, now)
        return Alarm.from_db_model(alarm)

    @wsme_pecan.wsexpose([Alarm], [Query])
    def get_all(self, q=None):
        """Return all alarms, based on the query provided.

        :param q: Filter rules for the alarms to be returned.
        """
        q = q or []
        #Timestamp is not supported field for Simple Alarm queries
        kwargs = _query_to_kwargs(q,
                                  pecan.request.storage_conn.get_alarms,
                                  allow_timestamps=False)
        return [Alarm.from_db_model(m)
                for m in pecan.request.storage_conn.get_alarms(**kwargs)]


class TraitDescription(_Base):
    """A description of a trait, with no associated value."""

    type = wtypes.text
    "the data type, defaults to string"

    name = wtypes.text
    "the name of the trait"

    @classmethod
    def sample(cls):
        return cls(name='service',
                   type='string'
                   )


class EventQuery(Query):
    """Query arguments for Event Queries."""

    _supported_types = ['integer', 'float', 'string', 'datetime']

    type = wsme.wsattr(wtypes.text, default='string')
    "the type of the trait filter, defaults to string"

    def __repr__(self):
        # for logging calls
        return '<EventQuery %r %s %r %s>' % (self.field,
                                             self.op,
                                             self._get_value_as_type(),
                                             self.type)


class Trait(_Base):
    """A Trait associated with an event."""

    name = wtypes.text
    "The name of the trait"

    value = wtypes.text
    "the value of the trait"

    type = wtypes.text
    "the type of the trait (string, integer, float or datetime)"

    @classmethod
    def sample(cls):
        return cls(name='service',
                   type='string',
                   value='compute.hostname'
                   )


class Event(_Base):
    """A System event."""

    message_id = wtypes.text
    "The message ID for the notification"

    event_type = wtypes.text
    "The type of the event"

    _traits = None

    def get_traits(self):
        return self._traits

    @staticmethod
    def _convert_storage_trait(t):
        """Helper method to convert a storage model into an API trait
        instance. If an API trait instance is passed in, just return it.
        """
        if isinstance(t, Trait):
            return t
        value = (six.text_type(t.value)
                 if not t.dtype == storage.models.Trait.DATETIME_TYPE
                 else t.value.isoformat())
        type = storage.models.Trait.get_name_by_type(t.dtype)
        return Trait(name=t.name, type=type, value=value)

    def set_traits(self, traits):
        self._traits = map(self._convert_storage_trait, traits)

    traits = wsme.wsproperty(wtypes.ArrayType(Trait),
                             get_traits,
                             set_traits)
    "Event specific properties"

    generated = datetime.datetime
    "The time the event occurred"

    @classmethod
    def sample(cls):
        return cls(
            event_type='compute.instance.update',
            generated='2013-11-11T20:00:00',
            message_id='94834db1-8f1b-404d-b2ec-c35901f1b7f0',
            traits={
                'request_id': 'req-4e2d67b8-31a4-48af-bb2f-9df72a353a72',
                'service': 'conductor.tem-devstack-01',
                'tenant_id': '7f13f2b17917463b9ee21aa92c4b36d6'
            }
        )


def requires_admin(func):

    @functools.wraps(func)
    def wrapped(*args, **kwargs):
        usr_limit, proj_limit = acl.get_limited_to(pecan.request.headers)
        # If User and Project are None, you have full access.
        if usr_limit and proj_limit:
            raise ProjectNotAuthorized(proj_limit)
        return func(*args, **kwargs)

    return wrapped


def _event_query_to_event_filter(q):
    evt_model_filter = {
        'event_type': None,
        'message_id': None,
        'start_time': None,
        'end_time': None
    }
    traits_filter = []

    for i in q:
        # FIXME(herndon): Support for operators other than
        # 'eq' will come later.
        if i.op != 'eq':
            error = _("operator %s not supported") % i.op
            raise ClientSideError(error)
        if i.field in evt_model_filter:
            evt_model_filter[i.field] = i.value
        else:
            traits_filter.append({"key": i.field,
                                  i.type: i._get_value_as_type()})
    return storage.EventFilter(traits_filter=traits_filter, **evt_model_filter)


class TraitsController(rest.RestController):
    """Works on Event Traits."""

    @requires_admin
    @wsme_pecan.wsexpose([Trait], wtypes.text, wtypes.text)
    def get_one(self, event_type, trait_name):
        """Return all instances of a trait for an event type.

        :param event_type: Event type to filter traits by
        :param trait_name: Trait to return values for
        """
        LOG.debug(_("Getting traits for %s") % event_type)
        return [Trait(name=t.name, type=t.get_type_name(), value=t.value)
                for t in pecan.request.storage_conn
                .get_traits(event_type, trait_name)]

    @requires_admin
    @wsme_pecan.wsexpose([TraitDescription], wtypes.text)
    def get_all(self, event_type):
        """Return all trait names for an event type.

        :param event_type: Event type to filter traits by
        """
        get_trait_name = storage.models.Trait.get_name_by_type
        return [TraitDescription(name=t['name'],
                                 type=get_trait_name(t['data_type']))
                for t in pecan.request.storage_conn
                .get_trait_types(event_type)]


class EventTypesController(rest.RestController):
    """Works on Event Types in the system."""

    traits = TraitsController()

    @pecan.expose()
    def get_one(self, event_type):
        pecan.abort(404)

    @requires_admin
    @wsme_pecan.wsexpose([unicode])
    def get_all(self):
        """Get all event types.
        """
        return list(pecan.request.storage_conn.get_event_types())


class EventsController(rest.RestController):
    """Works on Events."""

    @requires_admin
    @wsme_pecan.wsexpose([Event], [EventQuery])
    def get_all(self, q=None):
        """Return all events matching the query filters.

        :param q: Filter arguments for which Events to return
        """
        q = q or []
        event_filter = _event_query_to_event_filter(q)
        return [Event(message_id=event.message_id,
                      event_type=event.event_type,
                      generated=event.generated,
                      traits=event.traits)
                for event in
                pecan.request.storage_conn.get_events(event_filter)]

    @requires_admin
    @wsme_pecan.wsexpose(Event, wtypes.text)
    def get_one(self, message_id):
        """Return a single event with the given message id.

        :param message_id: Message ID of the Event to be returned
        """
        event_filter = storage.EventFilter(message_id=message_id)
        events = pecan.request.storage_conn.get_events(event_filter)
        if not events:
            raise EntityNotFound(_("Event"), message_id)

        if len(events) > 1:
            LOG.error(_("More than one event with "
                        "id %s returned from storage driver") % message_id)

        event = events[0]

        return Event(message_id=event.message_id,
                     event_type=event.event_type,
                     generated=event.generated,
                     traits=event.traits)


class QuerySamplesController(rest.RestController):
    """Provides complex query possibilities for samples
    """

    @wsme_pecan.wsexpose([Sample], body=ComplexQuery)
    def post(self, body):
        """Define query for retrieving Sample data.

        :param body: Query rules for the samples to be returned.
        """
        sample_name_mapping = {"resource": "resource_id",
                               "meter": "counter_name",
                               "type": "counter_type",
                               "unit": "counter_unit",
                               "volume": "counter_volume"}

        query = ValidatedComplexQuery(body,
                                      storage.models.Sample,
                                      sample_name_mapping,
                                      metadata_allowed=True)
        query.validate(visibility_field="project_id")
        conn = pecan.request.storage_conn
        return [Sample.from_db_model(s)
                for s in conn.query_samples(query.filter_expr,
                                            query.orderby,
                                            query.limit)]


class QueryAlarmHistoryController(rest.RestController):
    """Provides complex query possibilites for alarm history
    """
    @wsme_pecan.wsexpose([AlarmChange], body=ComplexQuery)
    def post(self, body):
        """Define query for retrieving AlarmChange data.

        :param body: Query rules for the alarm history to be returned.
        """
        query = ValidatedComplexQuery(body,
                                      storage.models.AlarmChange)
        query.validate(visibility_field="on_behalf_of")
        conn = pecan.request.storage_conn
        return [AlarmChange.from_db_model(s)
                for s in conn.query_alarm_history(query.filter_expr,
                                                  query.orderby,
                                                  query.limit)]


class QueryAlarmsController(rest.RestController):
    """Provides complex query possibilities for alarms
    """
    history = QueryAlarmHistoryController()

    @wsme_pecan.wsexpose([Alarm], body=ComplexQuery)
    def post(self, body):
        """Define query for retrieving Alarm data.

        :param body: Query rules for the alarms to be returned.
        """
        query = ValidatedComplexQuery(body,
                                      storage.models.Alarm)
        query.validate(visibility_field="project_id")
        conn = pecan.request.storage_conn
        return [Alarm.from_db_model(s)
                for s in conn.query_alarms(query.filter_expr,
                                           query.orderby,
                                           query.limit)]


class QueryController(rest.RestController):

    samples = QuerySamplesController()
    alarms = QueryAlarmsController()


def _flatten_capabilities(capabilities):
    return dict((k, v) for k, v in utils.recursive_keypairs(capabilities))


class Capabilities(_Base):
    """A representation of the API capabilities, usually constrained
       by restrictions imposed by the storage driver.
    """

    api = {wtypes.text: bool}
    "A flattened dictionary of API capabilities"

    @classmethod
    def sample(cls):
        return cls(
            api=_flatten_capabilities({
                'meters': {'pagination': True,
                           'query': {'simple': True,
                                     'metadata': True,
                                     'complex': False}},
                'resources': {'pagination': False,
                              'query': {'simple': True,
                                        'metadata': True,
                                        'complex': False}},
                'samples': {'pagination': True,
                            'groupby': True,
                            'query': {'simple': True,
                                      'metadata': True,
                                      'complex': True}},
                'statistics': {'pagination': True,
                               'groupby': True,
                               'query': {'simple': True,
                                         'metadata': True,
                                         'complex': False},
                               'aggregation': {'standard': True,
                                               'selectable': {
                                                   'max': True,
                                                   'min': True,
                                                   'sum': True,
                                                   'avg': True,
                                                   'count': True,
                                                   'stddev': True,
                                                   'cardinality': True,
                                                   'quartile': False}}},
                'alarms': {'query': {'simple': True,
                                     'complex': True},
                           'history': {'query': {'simple': True,
                                                 'complex': True}}},
                'events': {'query': {'simple': True}},
            })
        )


class CapabilitiesController(rest.RestController):
    """Manages capabilities queries.
    """

    @wsme_pecan.wsexpose(Capabilities)
    def get(self):
        """Returns a flattened dictionary of API capabilities supported
           by the currently configured storage driver.
        """
        # variation in API capabilities is effectively determined by
        # the lack of strict feature parity across storage drivers
        driver_capabilities = pecan.request.storage_conn.get_capabilities()
        return Capabilities(api=_flatten_capabilities(driver_capabilities))


class V2Controller(object):
    """Version 2 API controller root."""

    resources = ResourcesController()
    meters = MetersController()
    samples = SamplesController()
    alarms = AlarmsController()
    event_types = EventTypesController()
    events = EventsController()
    query = QueryController()
    capabilities = CapabilitiesController()

########NEW FILE########
__FILENAME__ = hooks
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#         Angus Salkeld <asalkeld@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import threading

from oslo.config import cfg
from pecan import hooks

from ceilometer import pipeline


class ConfigHook(hooks.PecanHook):
    """Attach the configuration object to the request
    so controllers can get to it.
    """

    def before(self, state):
        state.request.cfg = cfg.CONF


class DBHook(hooks.PecanHook):

    def __init__(self, storage_connection):
        self.storage_connection = storage_connection

    def before(self, state):
        state.request.storage_conn = self.storage_connection


class PipelineHook(hooks.PecanHook):
    '''Create and attach a pipeline to the request so that
    new samples can be posted via the /v2/meters/ API.
    '''

    pipeline_manager = None

    def __init__(self):
        if self.__class__.pipeline_manager is None:
            # this is done here as the cfg options are not available
            # when the file is imported.
            self.__class__.pipeline_manager = pipeline.setup_pipeline()

    def before(self, state):
        state.request.pipeline_manager = self.pipeline_manager


class TranslationHook(hooks.PecanHook):

    def __init__(self):
        # Use thread local storage to make this thread safe in situations
        # where one pecan instance is being used to serve multiple request
        # threads.
        self.local_error = threading.local()
        self.local_error.translatable_error = None

    def before(self, state):
        self.local_error.translatable_error = None

    def after(self, state):
        if hasattr(state.response, 'translatable_error'):
            self.local_error.translatable_error = (
                state.response.translatable_error)

########NEW FILE########
__FILENAME__ = middleware
# -*- encoding: utf-8 -*-
#
# Copyright 2013 IBM Corp.
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Middleware to replace the plain text message body of an error
response with one formatted so the client can parse it.

Based on pecan.middleware.errordocument
"""

import json

from lxml import etree
import webob

from ceilometer.api import hooks
from ceilometer.openstack.common import gettextutils
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log

LOG = log.getLogger(__name__)


class ParsableErrorMiddleware(object):
    """Replace error body with something the client can parse.
    """

    @staticmethod
    def best_match_language(accept_language):
        """Determines best available locale from the Accept-Language
        header.

        :returns: the best language match or None if the 'Accept-Language'
                  header was not available in the request.
        """
        if not accept_language:
            return None
        all_languages = gettextutils.get_available_languages('ceilometer')
        return accept_language.best_match(all_languages)

    def __init__(self, app):
        self.app = app

    def __call__(self, environ, start_response):
        # Request for this state, modified by replace_start_response()
        # and used when an error is being reported.
        state = {}

        def replacement_start_response(status, headers, exc_info=None):
            """Overrides the default response to make errors parsable.
            """
            try:
                status_code = int(status.split(' ')[0])
                state['status_code'] = status_code
            except (ValueError, TypeError):  # pragma: nocover
                raise Exception((
                    'ErrorDocumentMiddleware received an invalid '
                    'status %s' % status
                ))
            else:
                if (state['status_code'] / 100) not in (2, 3):
                    # Remove some headers so we can replace them later
                    # when we have the full error message and can
                    # compute the length.
                    headers = [(h, v)
                               for (h, v) in headers
                               if h not in ('Content-Length', 'Content-Type')
                               ]
                # Save the headers in case we need to modify them.
                state['headers'] = headers
                return start_response(status, headers, exc_info)

        app_iter = self.app(environ, replacement_start_response)
        if (state['status_code'] / 100) not in (2, 3):
            req = webob.Request(environ)
            # Find the first TranslationHook in the array of hooks and use the
            # translatable_error object from it
            error = None
            for hook in self.app.hooks:
                if isinstance(hook, hooks.TranslationHook):
                    error = hook.local_error.translatable_error
                    break
            user_locale = self.best_match_language(req.accept_language)
            if (req.accept.best_match(['application/json', 'application/xml'])
               == 'application/xml'):
                try:
                    # simple check xml is valid
                    fault = etree.fromstring('\n'.join(app_iter))
                    # Add the translated error to the xml data
                    if error is not None:
                        for fault_string in fault.findall('faultstring'):
                            fault_string.text = (
                                gettextutils.translate(
                                    error, user_locale))
                    body = ['<error_message>' + etree.tostring(fault)
                            + '</error_message>']
                except etree.XMLSyntaxError as err:
                    LOG.error(_('Error parsing HTTP response: %s') % err)
                    body = ['<error_message>%s' % state['status_code']
                            + '</error_message>']
                state['headers'].append(('Content-Type', 'application/xml'))
            else:
                try:
                    fault = json.loads('\n'.join(app_iter))
                    if error is not None and 'faultstring' in fault:
                        fault['faultstring'] = (
                            gettextutils.translate(
                                error, user_locale))
                    body = [json.dumps({'error_message': fault})]
                except ValueError as err:
                    body = [json.dumps({'error_message': '\n'.join(app_iter)})]
                state['headers'].append(('Content-Type', 'application/json'))
            state['headers'].append(('Content-Length', str(len(body[0]))))
        else:
            body = app_iter
        return body

########NEW FILE########
__FILENAME__ = manager
# -*- encoding: utf-8 -*-
#
# Copyright © 2012-2013 eNovance <licensing@enovance.com>
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from keystoneclient.v2_0 import client as ksclient
from oslo.config import cfg

from ceilometer import agent
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log

cfg.CONF.import_group('service_credentials', 'ceilometer.service')

LOG = log.getLogger(__name__)


class AgentManager(agent.AgentManager):

    def __init__(self):
        super(AgentManager, self).__init__('central')

    def interval_task(self, task):
        try:
            self.keystone = ksclient.Client(
                username=cfg.CONF.service_credentials.os_username,
                password=cfg.CONF.service_credentials.os_password,
                tenant_id=cfg.CONF.service_credentials.os_tenant_id,
                tenant_name=cfg.CONF.service_credentials.os_tenant_name,
                cacert=cfg.CONF.service_credentials.os_cacert,
                auth_url=cfg.CONF.service_credentials.os_auth_url,
                region_name=cfg.CONF.service_credentials.os_region_name,
                insecure=cfg.CONF.service_credentials.insecure)
        except Exception as e:
            LOG.error(_('Skip interval_task because Keystone error: %s'), e)
            return

        super(AgentManager, self).interval_task(task)

########NEW FILE########
__FILENAME__ = plugin
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Base class for plugins used by the central agent.
"""

from ceilometer import plugin


class CentralPollster(plugin.PollsterBase):
    """Base class for plugins that support the polling API."""

########NEW FILE########
__FILENAME__ = cli
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
#
# Copyright © 2012-2014 Julien Danjou
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

"""Command line tool for creating meter for Ceilometer.
"""
import logging
import sys

import eventlet
# NOTE(jd) We need to monkey patch the socket and select module for,
# at least, oslo.messaging, otherwise everything's blocked on its
# first read() or select(), thread need to be patched too, because
# oslo.messaging use threading.local
eventlet.monkey_patch(socket=True, select=True, thread=True)


from oslo.config import cfg

from ceilometer.alarm import service as alarm_service
from ceilometer.api import app
from ceilometer.central import manager as central_manager
from ceilometer import collector
from ceilometer.compute import manager as compute_manager
from ceilometer import notification
from ceilometer.openstack.common import context
from ceilometer.openstack.common import importutils
from ceilometer.openstack.common import service as os_service
from ceilometer.openstack.common import timeutils
from ceilometer import pipeline
from ceilometer import sample
from ceilometer import service
from ceilometer import storage
from ceilometer import transformer

OPTS = [
    cfg.StrOpt('evaluation_service',
               default='ceilometer.alarm.service.SingletonAlarmService',
               help='Class to launch as alarm evaluation service.'),
]

cfg.CONF.register_opts(OPTS, group='alarm')
cfg.CONF.import_opt('time_to_live', 'ceilometer.storage',
                    group='database')

LOG = logging.getLogger(__name__)


def alarm_notifier():
    service.prepare_service()
    os_service.launch(alarm_service.AlarmNotifierService()).wait()


def alarm_evaluator():
    service.prepare_service()
    eval_service = importutils.import_object(cfg.CONF.alarm.evaluation_service)
    os_service.launch(eval_service).wait()


def agent_central():
    service.prepare_service()
    os_service.launch(central_manager.AgentManager()).wait()


def agent_compute():
    service.prepare_service()
    os_service.launch(compute_manager.AgentManager()).wait()


def agent_notification():
    service.prepare_service()
    launcher = os_service.ProcessLauncher()
    launcher.launch_service(
        notification.NotificationService(),
        workers=service.get_workers('notification'))
    launcher.wait()


def api():
    service.prepare_service()
    srv = app.build_server()
    srv.serve_forever()


def collector_service():
    service.prepare_service()
    launcher = os_service.ProcessLauncher()
    launcher.launch_service(
        collector.CollectorService(),
        workers=service.get_workers('collector'))
    launcher.wait()


def storage_dbsync():
    service.prepare_service()
    storage.get_connection_from_config(cfg.CONF).upgrade()


def storage_expirer():
    service.prepare_service()
    if cfg.CONF.database.time_to_live > 0:
        LOG.debug(_("Clearing expired metering data"))
        storage_conn = storage.get_connection_from_config(cfg.CONF)
        storage_conn.clear_expired_metering_data(
            cfg.CONF.database.time_to_live)
    else:
        LOG.info(_("Nothing to clean, database time to live is disabled"))


def send_sample():
    cfg.CONF.register_cli_opts([
        cfg.StrOpt('sample-name',
                   short='n',
                   help='Meter name.',
                   required=True),
        cfg.StrOpt('sample-type',
                   short='y',
                   help='Meter type (gauge, delta, cumulative).',
                   default='gauge',
                   required=True),
        cfg.StrOpt('sample-unit',
                   short='U',
                   help='Meter unit.',
                   default=None),
        cfg.IntOpt('sample-volume',
                   short='l',
                   help='Meter volume value.',
                   default=1),
        cfg.StrOpt('sample-resource',
                   short='r',
                   help='Meter resource id.',
                   required=True),
        cfg.StrOpt('sample-user',
                   short='u',
                   help='Meter user id.'),
        cfg.StrOpt('sample-project',
                   short='p',
                   help='Meter project id.'),
        cfg.StrOpt('sample-timestamp',
                   short='i',
                   help='Meter timestamp.',
                   default=timeutils.utcnow().isoformat()),
        cfg.StrOpt('sample-metadata',
                   short='m',
                   help='Meter metadata.'),
    ])

    service.prepare_service()

    # Set up logging to use the console
    console = logging.StreamHandler(sys.stderr)
    console.setLevel(logging.DEBUG)
    formatter = logging.Formatter('%(message)s')
    console.setFormatter(formatter)
    root_logger = logging.getLogger('')
    root_logger.addHandler(console)
    root_logger.setLevel(logging.DEBUG)

    pipeline_manager = pipeline.setup_pipeline(
        transformer.TransformerExtensionManager(
            'ceilometer.transformer',
        ),
    )

    with pipeline_manager.publisher(context.get_admin_context()) as p:
        p([sample.Sample(
            name=cfg.CONF.sample_name,
            type=cfg.CONF.sample_type,
            unit=cfg.CONF.sample_unit,
            volume=cfg.CONF.sample_volume,
            user_id=cfg.CONF.sample_user,
            project_id=cfg.CONF.sample_project,
            resource_id=cfg.CONF.sample_resource,
            timestamp=cfg.CONF.sample_timestamp,
            resource_metadata=cfg.CONF.sample_metadata and eval(
                cfg.CONF.sample_metadata))])

########NEW FILE########
__FILENAME__ = collector
# -*- encoding: utf-8 -*-
#
# Copyright © 2012-2013 eNovance <licensing@enovance.com>
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import socket

import msgpack
from oslo.config import cfg

from ceilometer import dispatcher
from ceilometer import messaging
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer.openstack.common import service as os_service
from ceilometer.openstack.common import units

OPTS = [
    cfg.StrOpt('udp_address',
               default='0.0.0.0',
               help='Address to which the UDP socket is bound. Set to '
               'an empty string to disable.'),
    cfg.IntOpt('udp_port',
               default=4952,
               help='Port to which the UDP socket is bound.'),
]

cfg.CONF.register_opts(OPTS, group="collector")
cfg.CONF.import_opt('metering_topic', 'ceilometer.publisher.rpc',
                    group="publisher_rpc")


LOG = log.getLogger(__name__)


class CollectorService(os_service.Service):
    """Listener for the collector service."""

    @staticmethod
    def rpc_enabled():
        # cfg.CONF opt from oslo.messaging.transport
        return cfg.CONF.rpc_backend or cfg.CONF.transport_url

    def start(self):
        """Bind the UDP socket and handle incoming data."""
        # ensure dispatcher is configured before starting other services
        self.dispatcher_manager = dispatcher.load_dispatcher_manager()
        super(CollectorService, self).start()
        if cfg.CONF.collector.udp_address:
            self.tg.add_thread(self.start_udp)

        if self.rpc_enabled():
            self.rpc_server = messaging.get_rpc_server(
                cfg.CONF.publisher_rpc.metering_topic, self)
            self.rpc_server.start()

            if not cfg.CONF.collector.udp_address:
                # Add a dummy thread to have wait() working
                self.tg.add_timer(604800, lambda: None)

    def start_udp(self):
        udp = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        udp.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        udp.bind((cfg.CONF.collector.udp_address,
                  cfg.CONF.collector.udp_port))

        self.udp_run = True
        while self.udp_run:
            # NOTE(jd) Arbitrary limit of 64K because that ought to be
            # enough for anybody.
            data, source = udp.recvfrom(64 * units.Ki)
            try:
                sample = msgpack.loads(data, encoding='utf-8')
            except Exception:
                LOG.warn(_("UDP: Cannot decode data sent by %s"), str(source))
            else:
                try:
                    LOG.debug(_("UDP: Storing %s"), str(sample))
                    self.dispatcher_manager.map_method('record_metering_data',
                                                       sample)
                except Exception:
                    LOG.exception(_("UDP: Unable to store meter"))

    def stop(self):
        self.udp_run = False
        if self.rpc_enabled():
            self.rpc_server.stop()
        super(CollectorService, self).stop()

    def record_metering_data(self, context, data):
        """RPC endpoint for messages we send to ourselves.

        When the notification messages are re-published through the
        RPC publisher, this method receives them for processing.
        """
        self.dispatcher_manager.map_method('record_metering_data', data=data)

########NEW FILE########
__FILENAME__ = discovery
# -*- encoding: utf-8 -*-
#
# Copyright © 2014 Red Hat, Inc
#
# Author: Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from oslo.config import cfg

from ceilometer import nova_client
from ceilometer import plugin


class InstanceDiscovery(plugin.DiscoveryBase):
    def __init__(self):
        super(InstanceDiscovery, self).__init__()
        self.nova_cli = nova_client.Client()

    def discover(self, param=None):
        """Discover resources to monitor.
        """
        instances = self.nova_cli.instance_get_all_by_host(cfg.CONF.host)
        return [i for i in instances
                if getattr(i, 'OS-EXT-STS:vm_state', None) != 'error']

########NEW FILE########
__FILENAME__ = manager
# -*- encoding: utf-8 -*-
#
# Copyright © 2012-2013 eNovance <licensing@enovance.com>
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ceilometer import agent
from ceilometer.compute.virt import inspector as virt_inspector
from ceilometer.openstack.common import log

LOG = log.getLogger(__name__)


class AgentManager(agent.AgentManager):

    def __init__(self):
        super(AgentManager, self).__init__('compute', ['local_instances'])
        self._inspector = virt_inspector.get_hypervisor_inspector()

    @property
    def inspector(self):
        return self._inspector

########NEW FILE########
__FILENAME__ = cpu
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Intel
#
# Author: Shuangtai Tian <shuangtai.tian@intel.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Converters for producing compute CPU sample messages from notification
events.
"""

from ceilometer.compute import notifications
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer.openstack.common import timeutils
from ceilometer import sample

LOG = log.getLogger(__name__)


class ComputeMetricsNotificationBase(notifications.ComputeNotificationBase):
    """Convert compute.metrics.update notifications into Samples
    """
    event_types = ['compute.metrics.update']
    metric = None
    sample_type = None
    unit = None

    @staticmethod
    def _get_sample(message, name):
        try:
            for metric in message['payload']['metrics']:
                if name == metric['name']:
                    info = {}
                    info['payload'] = metric
                    info['event_type'] = message['event_type']
                    info['publisher_id'] = message['publisher_id']
                    info['resource_id'] = '%s_%s' % (
                        message['payload']['host'],
                        message['payload']['nodename'])
                    info['timestamp'] = str(timeutils.parse_strtime(
                        metric['timestamp']))
                    return info
        except Exception as err:
            LOG.warning(_('An error occurred while building %(m)s '
                          'sample: %(e)s') % {'m': name, 'e': err})

    def process_notification(self, message):
        info = self._get_sample(message, self.metric)
        if info:
            yield sample.Sample.from_notification(
                name='compute.node.%s' % self.metric,
                type=self.sample_type,
                unit=self.unit,
                volume=(info['payload']['value'] * 100 if self.unit == '%'
                        else info['payload']['value']),
                user_id=None,
                project_id=None,
                resource_id=info['resource_id'],
                message=info)


class CpuFrequency(ComputeMetricsNotificationBase):
    """Handle CPU current frequency message."""
    metric = 'cpu.frequency'
    sample_type = sample.TYPE_GAUGE
    unit = 'MHz'


class CpuUserTime(ComputeMetricsNotificationBase):
    """Handle CPU user mode time message."""
    metric = 'cpu.user.time'
    sample_type = sample.TYPE_CUMULATIVE
    unit = 'ns'


class CpuKernelTime(ComputeMetricsNotificationBase):
    """Handle CPU kernel time message."""
    metric = 'cpu.kernel.time'
    unit = 'ns'
    sample_type = sample.TYPE_CUMULATIVE


class CpuIdleTime(ComputeMetricsNotificationBase):
    """Handle CPU idle time message."""
    metric = 'cpu.idle.time'
    unit = 'ns'
    sample_type = sample.TYPE_CUMULATIVE


class CpuIowaitTime(ComputeMetricsNotificationBase):
    """Handle CPU I/O wait time message."""
    metric = 'cpu.iowait.time'
    unit = 'ns'
    sample_type = sample.TYPE_CUMULATIVE


class CpuKernelPercent(ComputeMetricsNotificationBase):
    """Handle CPU kernel percentage message."""
    metric = 'cpu.kernel.percent'
    unit = '%'
    sample_type = sample.TYPE_GAUGE


class CpuIdlePercent(ComputeMetricsNotificationBase):
    """Handle CPU idle percentage message."""
    metric = 'cpu.idle.percent'
    unit = '%'
    sample_type = sample.TYPE_GAUGE


class CpuUserPercent(ComputeMetricsNotificationBase):
    """Handle CPU user mode percentage message."""
    metric = 'cpu.user.percent'
    unit = '%'
    sample_type = sample.TYPE_GAUGE


class CpuIowaitPercent(ComputeMetricsNotificationBase):
    """Handle CPU I/O wait percentage message."""
    metric = 'cpu.iowait.percent'
    unit = '%'
    sample_type = sample.TYPE_GAUGE


class CpuPercent(ComputeMetricsNotificationBase):
    """Handle generic CPU utilization message."""
    metric = 'cpu.percent'
    unit = '%'
    sample_type = sample.TYPE_GAUGE

########NEW FILE########
__FILENAME__ = instance
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
# Copyright © 2013 eNovance
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#         Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Converters for producing compute sample messages from notification events.
"""

import abc
import six

from ceilometer.compute import notifications
from ceilometer.compute import util
from ceilometer import sample


@six.add_metaclass(abc.ABCMeta)
class UserMetadataAwareInstanceNotificationBase(
        notifications.ComputeNotificationBase):
    """Consumes notifications containing instance user metadata.
    """

    def process_notification(self, message):
        instance_properties = self.get_instance_properties(message)
        if isinstance(instance_properties.get('metadata'), dict):
            src_metadata = instance_properties['metadata']
            del instance_properties['metadata']
            util.add_reserved_user_metadata(src_metadata, instance_properties)
        return self.get_sample(message)

    def get_instance_properties(self, message):
        """Retrieve instance properties from notification payload."""
        return message['payload']

    @abc.abstractmethod
    def get_sample(self, message):
        """Derive sample from notification payload."""


class InstanceScheduled(UserMetadataAwareInstanceNotificationBase):
    event_types = ['scheduler.run_instance.scheduled']

    def get_instance_properties(self, message):
        """Retrieve instance properties from notification payload."""
        return message['payload']['request_spec']['instance_properties']

    def get_sample(self, message):
        yield sample.Sample.from_notification(
            name='instance.scheduled',
            type=sample.TYPE_DELTA,
            volume=1,
            unit='instance',
            user_id=None,
            project_id=
            message['payload']['request_spec']
            ['instance_properties']['project_id'],
            resource_id=message['payload']['instance_id'],
            message=message)


class ComputeInstanceNotificationBase(
        UserMetadataAwareInstanceNotificationBase):
    """Convert compute.instance.* notifications into Samples
    """
    event_types = ['compute.instance.*']


class Instance(ComputeInstanceNotificationBase):
    def get_sample(self, message):
        yield sample.Sample.from_notification(
            name='instance',
            type=sample.TYPE_GAUGE,
            unit='instance',
            volume=1,
            user_id=message['payload']['user_id'],
            project_id=message['payload']['tenant_id'],
            resource_id=message['payload']['instance_id'],
            message=message)


class Memory(ComputeInstanceNotificationBase):
    def get_sample(self, message):
        yield sample.Sample.from_notification(
            name='memory',
            type=sample.TYPE_GAUGE,
            unit='MB',
            volume=message['payload']['memory_mb'],
            user_id=message['payload']['user_id'],
            project_id=message['payload']['tenant_id'],
            resource_id=message['payload']['instance_id'],
            message=message)


class VCpus(ComputeInstanceNotificationBase):
    def get_sample(self, message):
        yield sample.Sample.from_notification(
            name='vcpus',
            type=sample.TYPE_GAUGE,
            unit='vcpu',
            volume=message['payload']['vcpus'],
            user_id=message['payload']['user_id'],
            project_id=message['payload']['tenant_id'],
            resource_id=message['payload']['instance_id'],
            message=message)


class RootDiskSize(ComputeInstanceNotificationBase):
    def get_sample(self, message):
        yield sample.Sample.from_notification(
            name='disk.root.size',
            type=sample.TYPE_GAUGE,
            unit='GB',
            volume=message['payload']['root_gb'],
            user_id=message['payload']['user_id'],
            project_id=message['payload']['tenant_id'],
            resource_id=message['payload']['instance_id'],
            message=message)


class EphemeralDiskSize(ComputeInstanceNotificationBase):
    def get_sample(self, message):
        yield sample.Sample.from_notification(
            name='disk.ephemeral.size',
            type=sample.TYPE_GAUGE,
            unit='GB',
            volume=message['payload']['ephemeral_gb'],
            user_id=message['payload']['user_id'],
            project_id=message['payload']['tenant_id'],
            resource_id=message['payload']['instance_id'],
            message=message)


class InstanceFlavor(ComputeInstanceNotificationBase):
    def get_sample(self, message):
        instance_type = message.get('payload', {}).get('instance_type')
        if instance_type:
            yield sample.Sample.from_notification(
                name='instance:%s' % instance_type,
                type=sample.TYPE_GAUGE,
                unit='instance',
                volume=1,
                user_id=message['payload']['user_id'],
                project_id=message['payload']['tenant_id'],
                resource_id=message['payload']['instance_id'],
                message=message)


class InstanceDelete(ComputeInstanceNotificationBase):
    """Handle the messages sent by the nova notifier plugin
    when an instance is being deleted.
    """

    event_types = ['compute.instance.delete.samples']

    def get_sample(self, message):
        for s in message['payload'].get('samples', []):
            yield sample.Sample.from_notification(
                name=s['name'],
                type=s['type'],
                unit=s['unit'],
                volume=s['volume'],
                user_id=message['payload']['user_id'],
                project_id=message['payload']['tenant_id'],
                resource_id=message['payload']['instance_id'],
                message=message)

########NEW FILE########
__FILENAME__ = nova_notifier
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import sys

from nova import notifications
from nova.openstack.common import log as logging
from nova.openstack.common.notifier import api as notifier_api

# HACK(dhellmann): Insert the nova version of openstack.common into
# sys.modules as though it was the copy from ceilometer, so that when
# we use modules from ceilometer below they do not re-define options.
# use the real ceilometer base package
import ceilometer  # noqa
for name in ['openstack', 'openstack.common', 'openstack.common.log']:
    sys.modules['ceilometer.' + name] = sys.modules['nova.' + name]

from nova.compute import flavors
from nova import conductor
from nova import utils

from stevedore import extension

from ceilometer.compute.virt import inspector
from ceilometer.openstack.common.gettextutils import _


# This module runs inside the nova compute
# agent, which only configures the "nova" logger.
# We use a fake logger name in that namespace
# so that messages from this module appear
# in the log file.
LOG = logging.getLogger('nova.ceilometer.notifier')

_gatherer = None
conductor_api = conductor.API()


class DeletedInstanceStatsGatherer(object):

    def __init__(self, extensions):
        self.mgr = extensions
        self.inspector = inspector.get_hypervisor_inspector()

    def __call__(self, instance):
        cache = {}
        samples = self.mgr.map_method('get_samples',
                                      self,
                                      cache,
                                      instance)
        # samples is a list of lists, so flatten it before returning
        # the results
        results = []
        for slist in samples:
            results.extend(slist)
        return results


def initialize_gatherer(gatherer=None):
    """Set the callable used to gather stats for the instance.

    gatherer should be a callable accepting one argument (the instance
    ref), or None to have a default gatherer used
    """
    global _gatherer
    if gatherer is not None:
        LOG.debug(_('using provided stats gatherer %r'), gatherer)
        _gatherer = gatherer
    if _gatherer is None:
        LOG.debug(_('making a new stats gatherer'))
        mgr = extension.ExtensionManager(
            namespace='ceilometer.poll.compute',
            invoke_on_load=True,
        )
        _gatherer = DeletedInstanceStatsGatherer(mgr)
    return _gatherer


class Instance(object):
    """Model class for instances

    The pollsters all expect an instance that looks like what the
    novaclient gives them, but the conductor API gives us a
    dictionary. This class makes an object from the dictionary so we
    can pass it to the pollsters.
    """
    def __init__(self, context, info):
        for k, v in info.iteritems():
            if k == 'name':
                setattr(self, 'OS-EXT-SRV-ATTR:instance_name', v)
            elif k == 'metadata':
                setattr(self, k, utils.metadata_to_dict(v))
            else:
                setattr(self, k, v)

        instance_type = flavors.extract_flavor(info)
        self.flavor_name = instance_type.get('name', 'UNKNOWN')
        self.instance_flavor_id = instance_type.get('flavorid', '')
        LOG.debug(_('INFO %r'), info)

    @property
    def tenant_id(self):
        return self.project_id

    @property
    def flavor(self):
        return {
            'id': self.instance_type_id,
            'flavor_id': self.instance_flavor_id,
            'name': self.flavor_name,
            'vcpus': self.vcpus,
            'ram': self.memory_mb,
            'disk': self.root_gb + self.ephemeral_gb,
            'ephemeral': self.ephemeral_gb
        }

    @property
    def hostId(self):
        return self.host

    @property
    def image(self):
        return {'id': self.image_ref}

    @property
    def name(self):
        return self.display_name


def notify(context, message):
    if message['event_type'] != 'compute.instance.delete.start':
        LOG.debug(_('ignoring %s'), message['event_type'])
        return
    LOG.info(_('processing %s'), message['event_type'])
    gatherer = initialize_gatherer()

    instance_id = message['payload']['instance_id']
    LOG.debug(_('polling final stats for %r'), instance_id)

    # Ask for the instance details
    instance_ref = conductor_api.instance_get_by_uuid(
        context,
        instance_id,
    )

    # Get the default notification payload
    payload = notifications.info_from_instance(
        context, instance_ref, None, None)

    # Extend the payload with samples from our plugins.  We only need
    # to send some of the data from the sample objects, since a lot
    # of the fields are the same.
    instance = Instance(context, instance_ref)
    samples = gatherer(instance)
    payload['samples'] = [{'name': s.name,
                           'type': s.type,
                           'unit': s.unit,
                           'volume': s.volume}
                          for s in samples]

    publisher_id = notifier_api.publisher_id('compute', None)

    # We could simply modify the incoming message payload, but we
    # can't be sure that this notifier will be called before the RPC
    # notifier. Modifying the content may also break the message
    # signature. So, we start a new message publishing. We will be
    # called again recursively as a result, but we ignore the event we
    # generate so it doesn't matter.
    notifier_api.notify(context, publisher_id,
                        'compute.instance.delete.samples',
                        notifier_api.INFO, payload)

########NEW FILE########
__FILENAME__ = plugin
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Base class for plugins used by the compute agent.
"""

import abc
import six

from ceilometer.openstack.common import timeutils
from ceilometer import plugin


@six.add_metaclass(abc.ABCMeta)
class ComputePollster(plugin.PollsterBase):
    """Base class for plugins that support the polling API on the compute node.
    """

    @abc.abstractmethod
    def get_samples(self, manager, cache, resources):
        """Return a sequence of Counter instances from polling the resources.

        :param manager: The service manager invoking the plugin
        :param cache: A dictionary for passing data between plugins
        :param resources: The resources to examine (expected to be instances)
        """

    def _record_poll_time(self):
        """Method records current time as the poll time.

        :return: time in seconds since the last poll time was recorded
        """
        current_time = timeutils.utcnow()
        duration = None
        if hasattr(self, '_last_poll_time'):
            duration = timeutils.delta_seconds(self._last_poll_time,
                                               current_time)
        self._last_poll_time = current_time
        return duration

########NEW FILE########
__FILENAME__ = cpu
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 eNovance <licensing@enovance.com>
# Copyright © 2012 Red Hat, Inc
#
# Author: Julien Danjou <julien@danjou.info>
# Author: Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ceilometer.compute import plugin
from ceilometer.compute.pollsters import util
from ceilometer.compute.virt import inspector as virt_inspector
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer import sample

LOG = log.getLogger(__name__)


class CPUPollster(plugin.ComputePollster):

    def get_samples(self, manager, cache, resources):
        for instance in resources:
            LOG.info(_('checking instance %s'), instance.id)
            instance_name = util.instance_name(instance)
            try:
                cpu_info = manager.inspector.inspect_cpus(instance_name)
                LOG.info(_("CPUTIME USAGE: %(instance)s %(time)d"),
                         {'instance': instance.__dict__,
                          'time': cpu_info.time})
                cpu_num = {'cpu_number': cpu_info.number}
                yield util.make_sample_from_instance(
                    instance,
                    name='cpu',
                    type=sample.TYPE_CUMULATIVE,
                    unit='ns',
                    volume=cpu_info.time,
                    additional_metadata=cpu_num,
                )
            except virt_inspector.InstanceNotFoundException as err:
                # Instance was deleted while getting samples. Ignore it.
                LOG.debug(_('Exception while getting samples %s'), err)
            except NotImplementedError:
                # Selected inspector does not implement this pollster.
                LOG.debug(_('Obtaining CPU time is not implemented for %s'
                            ), manager.inspector.__class__.__name__)
            except Exception as err:
                LOG.exception(_('could not get CPU time for %(id)s: %(e)s'),
                              {'id': instance.id, 'e': err})


class CPUUtilPollster(plugin.ComputePollster):

    def get_samples(self, manager, cache, resources):
        self._inspection_duration = self._record_poll_time()
        for instance in resources:
            LOG.debug(_('Checking CPU util for instance %s'), instance.id)
            try:
                cpu_info = manager.inspector.inspect_cpu_util(
                    instance, self._inspection_duration)
                LOG.debug(_("CPU UTIL: %(instance)s %(util)d"),
                          ({'instance': instance.__dict__,
                            'util': cpu_info.util}))
                yield util.make_sample_from_instance(
                    instance,
                    name='cpu_util',
                    type=sample.TYPE_GAUGE,
                    unit='%',
                    volume=cpu_info.util,
                )
            except virt_inspector.InstanceNotFoundException as err:
                # Instance was deleted while getting samples. Ignore it.
                LOG.debug(_('Exception while getting samples %s'), err)
            except NotImplementedError:
                # Selected inspector does not implement this pollster.
                LOG.debug(_('Obtaining CPU Util is not implemented for %s'),
                          manager.inspector.__class__.__name__)
            except Exception as err:
                LOG.exception(_('Could not get CPU Util for %(id)s: %(e)s'),
                              {'id': instance.id, 'e': err})

########NEW FILE########
__FILENAME__ = disk
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 eNovance <licensing@enovance.com>
# Copyright © 2012 Red Hat, Inc
#
# Author: Julien Danjou <julien@danjou.info>
# Author: Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import abc
import collections

import six

from ceilometer.compute import plugin
from ceilometer.compute.pollsters import util
from ceilometer.compute.virt import inspector as virt_inspector
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer import sample

LOG = log.getLogger(__name__)


DiskIOData = collections.namedtuple(
    'DiskIOData',
    'r_bytes r_requests w_bytes w_requests',
)


@six.add_metaclass(abc.ABCMeta)
class _Base(plugin.ComputePollster):

    DISKIO_USAGE_MESSAGE = ' '.join(["DISKIO USAGE:",
                                     "%s %s:",
                                     "read-requests=%d",
                                     "read-bytes=%d",
                                     "write-requests=%d",
                                     "write-bytes=%d",
                                     "errors=%d",
                                     ])

    CACHE_KEY_DISK = 'diskio'

    def _populate_cache(self, inspector, cache, instance, instance_name):
        i_cache = cache.setdefault(self.CACHE_KEY_DISK, {})
        if instance_name not in i_cache:
            r_bytes = 0
            r_requests = 0
            w_bytes = 0
            w_requests = 0
            for disk, info in inspector.inspect_disks(instance_name):
                LOG.info(self.DISKIO_USAGE_MESSAGE,
                         instance, disk.device, info.read_requests,
                         info.read_bytes, info.write_requests,
                         info.write_bytes, info.errors)
                r_bytes += info.read_bytes
                r_requests += info.read_requests
                w_bytes += info.write_bytes
                w_requests += info.write_requests
            i_cache[instance_name] = DiskIOData(
                r_bytes=r_bytes,
                r_requests=r_requests,
                w_bytes=w_bytes,
                w_requests=w_requests,
            )
        return i_cache[instance_name]

    @abc.abstractmethod
    def _get_sample(instance, c_data):
        """Return one Sample."""

    def get_samples(self, manager, cache, resources):
        for instance in resources:
            instance_name = util.instance_name(instance)
            try:
                c_data = self._populate_cache(
                    manager.inspector,
                    cache,
                    instance,
                    instance_name,
                )
                yield self._get_sample(instance, c_data)
            except virt_inspector.InstanceNotFoundException as err:
                # Instance was deleted while getting samples. Ignore it.
                LOG.debug(_('Exception while getting samples %s'), err)
            except NotImplementedError:
                # Selected inspector does not implement this pollster.
                LOG.debug(_('%(inspector)s does not provide data for '
                            ' %(pollster)s'),
                          {'inspector': manager.inspector.__class__.__name__,
                           'pollster': self.__class__.__name__})
            except Exception as err:
                LOG.exception(_('Ignoring instance %(name)s: %(error)s'),
                              {'name': instance_name, 'error': err})


class ReadRequestsPollster(_Base):

    @staticmethod
    def _get_sample(instance, c_data):
        return util.make_sample_from_instance(
            instance,
            name='disk.read.requests',
            type=sample.TYPE_CUMULATIVE,
            unit='request',
            volume=c_data.r_requests,
        )


class ReadBytesPollster(_Base):

    @staticmethod
    def _get_sample(instance, c_data):
        return util.make_sample_from_instance(
            instance,
            name='disk.read.bytes',
            type=sample.TYPE_CUMULATIVE,
            unit='B',
            volume=c_data.r_bytes,
        )


class WriteRequestsPollster(_Base):

    @staticmethod
    def _get_sample(instance, c_data):
        return util.make_sample_from_instance(
            instance,
            name='disk.write.requests',
            type=sample.TYPE_CUMULATIVE,
            unit='request',
            volume=c_data.w_requests,
        )


class WriteBytesPollster(_Base):

    @staticmethod
    def _get_sample(instance, c_data):
        return util.make_sample_from_instance(
            instance,
            name='disk.write.bytes',
            type=sample.TYPE_CUMULATIVE,
            unit='B',
            volume=c_data.w_bytes,
        )


@six.add_metaclass(abc.ABCMeta)
class _DiskRatesPollsterBase(plugin.ComputePollster):

    CACHE_KEY_DISK_RATE = 'diskio-rate'

    def _populate_cache(self, inspector, cache, instance):
        i_cache = cache.setdefault(self.CACHE_KEY_DISK_RATE, {})
        if instance.id not in i_cache:
            r_bytes_rate = 0
            r_requests_rate = 0
            w_bytes_rate = 0
            w_requests_rate = 0
            disk_rates = inspector.inspect_disk_rates(
                instance, self._inspection_duration)
            for disk, info in disk_rates:
                r_bytes_rate += info.read_bytes_rate
                r_requests_rate += info.read_requests_rate
                w_bytes_rate += info.write_bytes_rate
                w_requests_rate += info.write_requests_rate
            i_cache[instance.id] = virt_inspector.DiskRateStats(
                r_bytes_rate,
                r_requests_rate,
                w_bytes_rate,
                w_requests_rate
            )
        return i_cache[instance.id]

    @abc.abstractmethod
    def _get_sample(self, instance, disk_rates_info):
        """Return one Sample."""

    def get_samples(self, manager, cache, resources):
        self._inspection_duration = self._record_poll_time()
        for instance in resources:
            try:
                disk_rates_info = self._populate_cache(
                    manager.inspector,
                    cache,
                    instance,
                )
                yield self._get_sample(instance, disk_rates_info)
            except virt_inspector.InstanceNotFoundException as err:
                # Instance was deleted while getting samples. Ignore it.
                LOG.debug(_('Exception while getting samples %s'), err)
            except NotImplementedError:
                # Selected inspector does not implement this pollster.
                LOG.debug(_('%(inspector)s does not provide data for '
                            ' %(pollster)s'),
                          {'inspector': manager.inspector.__class__.__name__,
                           'pollster': self.__class__.__name__})
            except Exception as err:
                instance_name = util.instance_name(instance)
                LOG.exception(_('Ignoring instance %(name)s: %(error)s'),
                              {'name': instance_name, 'error': err})


class ReadBytesRatePollster(_DiskRatesPollsterBase):

    def _get_sample(self, instance, disk_rates_info):
        return util.make_sample_from_instance(
            instance,
            name='disk.read.bytes.rate',
            type=sample.TYPE_GAUGE,
            unit='B/s',
            volume=disk_rates_info.read_bytes_rate,
        )


class ReadRequestsRatePollster(_DiskRatesPollsterBase):

    def _get_sample(self, instance, disk_rates_info):
        return util.make_sample_from_instance(
            instance,
            name='disk.read.requests.rate',
            type=sample.TYPE_GAUGE,
            unit='requests/s',
            volume=disk_rates_info.read_requests_rate,
        )


class WriteBytesRatePollster(_DiskRatesPollsterBase):

    def _get_sample(self, instance, disk_rates_info):
        return util.make_sample_from_instance(
            instance,
            name='disk.write.bytes.rate',
            type=sample.TYPE_GAUGE,
            unit='B/s',
            volume=disk_rates_info.write_bytes_rate,
        )


class WriteRequestsRatePollster(_DiskRatesPollsterBase):

    def _get_sample(self, instance, disk_rates_info):
        return util.make_sample_from_instance(
            instance,
            name='disk.write.requests.rate',
            type=sample.TYPE_GAUGE,
            unit='requests/s',
            volume=disk_rates_info.write_requests_rate,
        )

########NEW FILE########
__FILENAME__ = instance
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 eNovance <licensing@enovance.com>
# Copyright © 2012 Red Hat, Inc
#
# Author: Julien Danjou <julien@danjou.info>
# Author: Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ceilometer.compute import plugin
from ceilometer.compute.pollsters import util
from ceilometer import sample


class InstancePollster(plugin.ComputePollster):

    @staticmethod
    def get_samples(manager, cache, resources):
        for instance in resources:
            yield util.make_sample_from_instance(
                instance,
                name='instance',
                type=sample.TYPE_GAUGE,
                unit='instance',
                volume=1,
            )


class InstanceFlavorPollster(plugin.ComputePollster):

    @staticmethod
    def get_samples(manager, cache, resources):
        for instance in resources:
            yield util.make_sample_from_instance(
                instance,
                # Use the "meter name + variable" syntax
                name='instance:%s' %
                instance.flavor['name'],
                type=sample.TYPE_GAUGE,
                unit='instance',
                volume=1,
            )

########NEW FILE########
__FILENAME__ = memory
# Copyright (c) 2014 VMware, Inc.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from ceilometer.compute import plugin
from ceilometer.compute.pollsters import util
from ceilometer.compute.virt import inspector as virt_inspector
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer import sample

LOG = log.getLogger(__name__)


class MemoryUsagePollster(plugin.ComputePollster):

    def get_samples(self, manager, cache, resources):
        self._inspection_duration = self._record_poll_time()
        for instance in resources:
            LOG.debug(_('Checking memory usage for instance %s'), instance.id)
            try:
                memory_info = manager.inspector.inspect_memory_usage(
                    instance, self._inspection_duration)
                LOG.debug(_("MEMORY USAGE: %(instance)s %(usage)f"),
                          ({'instance': instance.__dict__,
                            'usage': memory_info.usage}))
                yield util.make_sample_from_instance(
                    instance,
                    name='memory.usage',
                    type=sample.TYPE_GAUGE,
                    unit='MB',
                    volume=memory_info.usage,
                )
            except virt_inspector.InstanceNotFoundException as err:
                # Instance was deleted while getting samples. Ignore it.
                LOG.debug(_('Exception while getting samples %s'), err)
            except NotImplementedError:
                # Selected inspector does not implement this pollster.
                LOG.debug(_('Obtaining Memory Usage is not implemented for %s'
                            ), manager.inspector.__class__.__name__)
            except Exception as err:
                LOG.exception(_('Could not get Memory Usage for '
                                '%(id)s: %(e)s'), {'id': instance.id,
                                                   'e': err})

########NEW FILE########
__FILENAME__ = net
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 eNovance <licensing@enovance.com>
# Copyright © 2012 Red Hat, Inc
#
# Author: Julien Danjou <julien@danjou.info>
# Author: Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import copy

from ceilometer.compute import plugin
from ceilometer.compute.pollsters import util
from ceilometer.compute.virt import inspector as virt_inspector
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer.openstack.common import timeutils
from ceilometer import sample

LOG = log.getLogger(__name__)


class _Base(plugin.ComputePollster):

    NET_USAGE_MESSAGE = ' '.join(["NETWORK USAGE:", "%s %s:", "read-bytes=%d",
                                  "write-bytes=%d"])

    @staticmethod
    def make_vnic_sample(instance, name, type, unit, volume, vnic_data):
        metadata = copy.copy(vnic_data)
        resource_metadata = dict(zip(metadata._fields, metadata))
        resource_metadata['instance_id'] = instance.id
        resource_metadata['instance_type'] = \
            instance.flavor['id'] if instance.flavor else None

        if vnic_data.fref is not None:
            rid = vnic_data.fref
        else:
            instance_name = util.instance_name(instance)
            rid = "%s-%s-%s" % (instance_name, instance.id, vnic_data.name)

        return sample.Sample(
            name=name,
            type=type,
            unit=unit,
            volume=volume,
            user_id=instance.user_id,
            project_id=instance.tenant_id,
            resource_id=rid,
            timestamp=timeutils.isotime(),
            resource_metadata=resource_metadata
        )

    CACHE_KEY_VNIC = 'vnics'

    def _get_vnic_info(self, inspector, instance):
        instance_name = util.instance_name(instance)
        return inspector.inspect_vnics(instance_name)

    @staticmethod
    def _get_rx_info(info):
        return info.rx_bytes

    @staticmethod
    def _get_tx_info(info):
        return info.tx_bytes

    def _get_vnics_for_instance(self, cache, inspector, instance):
        instance_name = util.instance_name(instance)
        i_cache = cache.setdefault(self.CACHE_KEY_VNIC, {})
        if instance_name not in i_cache:
            i_cache[instance_name] = list(
                self._get_vnic_info(inspector, instance)
            )
        return i_cache[instance_name]

    def get_samples(self, manager, cache, resources):
        self._inspection_duration = self._record_poll_time()
        for instance in resources:
            instance_name = util.instance_name(instance)
            LOG.debug(_('checking net info for instance %s'), instance.id)
            try:
                vnics = self._get_vnics_for_instance(
                    cache,
                    manager.inspector,
                    instance,
                )
                for vnic, info in vnics:
                    LOG.debug(self.NET_USAGE_MESSAGE, instance_name,
                              vnic.name, self._get_rx_info(info),
                              self._get_tx_info(info))
                    yield self._get_sample(instance, vnic, info)
            except virt_inspector.InstanceNotFoundException as err:
                # Instance was deleted while getting samples. Ignore it.
                LOG.debug(_('Exception while getting samples %s'), err)
            except NotImplementedError:
                # Selected inspector does not implement this pollster.
                LOG.debug(_('%(inspector)s does not provide data for '
                            ' %(pollster)s'),
                          {'inspector': manager.inspector.__class__.__name__,
                           'pollster': self.__class__.__name__})
            except Exception as err:
                LOG.exception(_('Ignoring instance %(name)s: %(error)s'),
                              {'name': instance_name, 'error': err})


class _RateBase(_Base):

    NET_USAGE_MESSAGE = ' '.join(["NETWORK RATE:", "%s %s:",
                                 "read-bytes-rate=%d",
                                 "write-bytes-rate=%d"])

    CACHE_KEY_VNIC = 'vnic-rates'

    def _get_vnic_info(self, inspector, instance):
        return inspector.inspect_vnic_rates(instance,
                                            self._inspection_duration)

    @staticmethod
    def _get_rx_info(info):
        return info.rx_bytes_rate

    @staticmethod
    def _get_tx_info(info):
        return info.tx_bytes_rate


class IncomingBytesPollster(_Base):

    def _get_sample(self, instance, vnic, info):
        return self.make_vnic_sample(
            instance,
            name='network.incoming.bytes',
            type=sample.TYPE_CUMULATIVE,
            unit='B',
            volume=info.rx_bytes,
            vnic_data=vnic,
        )


class IncomingPacketsPollster(_Base):

    def _get_sample(self, instance, vnic, info):
        return self.make_vnic_sample(
            instance,
            name='network.incoming.packets',
            type=sample.TYPE_CUMULATIVE,
            unit='packet',
            volume=info.rx_packets,
            vnic_data=vnic,
        )


class OutgoingBytesPollster(_Base):

    def _get_sample(self, instance, vnic, info):
        return self.make_vnic_sample(
            instance,
            name='network.outgoing.bytes',
            type=sample.TYPE_CUMULATIVE,
            unit='B',
            volume=info.tx_bytes,
            vnic_data=vnic,
        )


class OutgoingPacketsPollster(_Base):

    def _get_sample(self, instance, vnic, info):
        return self.make_vnic_sample(
            instance,
            name='network.outgoing.packets',
            type=sample.TYPE_CUMULATIVE,
            unit='packet',
            volume=info.tx_packets,
            vnic_data=vnic,
        )


class IncomingBytesRatePollster(_RateBase):

    def _get_sample(self, instance, vnic, info):
        return self.make_vnic_sample(
            instance,
            name='network.incoming.bytes.rate',
            type=sample.TYPE_GAUGE,
            unit='B/s',
            volume=info.rx_bytes_rate,
            vnic_data=vnic,
        )


class OutgoingBytesRatePollster(_RateBase):

    def _get_sample(self, instance, vnic, info):
        return self.make_vnic_sample(
            instance,
            name='network.outgoing.bytes.rate',
            type=sample.TYPE_GAUGE,
            unit='B/s',
            volume=info.tx_bytes_rate,
            vnic_data=vnic,
        )

########NEW FILE########
__FILENAME__ = util
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 eNovance <licensing@enovance.com>
# Copyright © 2012 Red Hat, Inc
#
# Author: Julien Danjou <julien@danjou.info>
# Author: Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ceilometer.compute import util as compute_util
from ceilometer.openstack.common import timeutils
from ceilometer import sample


INSTANCE_PROPERTIES = [
    # Identity properties
    'reservation_id',
    # Type properties
    'architecture',
    'OS-EXT-AZ:availability_zone',
    'kernel_id',
    'os_type',
    'ramdisk_id',
]


def _get_metadata_from_object(instance):
    """Return a metadata dictionary for the instance.
    """
    metadata = {
        'display_name': instance.name,
        'name': getattr(instance, 'OS-EXT-SRV-ATTR:instance_name', u''),
        'instance_type': (instance.flavor['id'] if instance.flavor else None),
        'host': instance.hostId,
        'flavor': instance.flavor,
        'status': instance.status.lower(),
    }

    # Image properties
    if instance.image:
        metadata['image'] = instance.image
        metadata['image_ref'] = instance.image['id']
        # Images that come through the conductor API in the nova notifier
        # plugin will not have links.
        if instance.image.get('links'):
            metadata['image_ref_url'] = instance.image['links'][0]['href']
        else:
            metadata['image_ref_url'] = None
    else:
        metadata['image'] = None
        metadata['image_ref'] = None
        metadata['image_ref_url'] = None

    for name in INSTANCE_PROPERTIES:
        if hasattr(instance, name):
            metadata[name] = getattr(instance, name)

    metadata['vcpus'] = instance.flavor['vcpus']
    metadata['memory_mb'] = instance.flavor['ram']
    metadata['disk_gb'] = instance.flavor['disk']
    metadata['ephemeral_gb'] = instance.flavor['ephemeral']
    metadata['root_gb'] = int(metadata['disk_gb']) - \
        int(metadata['ephemeral_gb'])

    return compute_util.add_reserved_user_metadata(instance.metadata, metadata)


def make_sample_from_instance(instance, name, type, unit, volume,
                              additional_metadata=None):
    additional_metadata = additional_metadata or {}
    resource_metadata = _get_metadata_from_object(instance)
    resource_metadata.update(additional_metadata)
    return sample.Sample(
        name=name,
        type=type,
        unit=unit,
        volume=volume,
        user_id=instance.user_id,
        project_id=instance.tenant_id,
        resource_id=instance.id,
        timestamp=timeutils.isotime(),
        resource_metadata=resource_metadata,
    )


def instance_name(instance):
    """Shortcut to get instance name."""
    return getattr(instance, 'OS-EXT-SRV-ATTR:instance_name', None)

########NEW FILE########
__FILENAME__ = util
# -*- encoding: utf-8 -*-
#
# Copyright © 2014 Red Hat, Inc
#
# Author: Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from oslo.config import cfg


OPTS = [
    cfg.ListOpt('reserved_metadata_namespace',
                default=['metering.'],
                help='List of metadata prefixes reserved for metering use.'),
    cfg.IntOpt('reserved_metadata_length',
               default=256,
               help='Limit on length of reserved metadata values.'),
]

cfg.CONF.register_opts(OPTS)


def add_reserved_user_metadata(src_metadata, dest_metadata):
    limit = cfg.CONF.reserved_metadata_length
    user_metadata = {}
    for prefix in cfg.CONF.reserved_metadata_namespace:
        md = dict(
            (k[len(prefix):].replace('.', '_'),
             v[:limit] if isinstance(v, basestring) else v)
            for k, v in src_metadata.items()
            if (k.startswith(prefix) and
                k[len(prefix):].replace('.', '_') not in dest_metadata)
        )
        user_metadata.update(md)
    if user_metadata:
        dest_metadata['user_metadata'] = user_metadata

    return dest_metadata

########NEW FILE########
__FILENAME__ = inspector
# Copyright 2013 Cloudbase Solutions Srl
#
# Author: Claudiu Belu <cbelu@cloudbasesolutions.com>
#         Alessandro Pilotti <apilotti@cloudbasesolutions.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Implementation of Inspector abstraction for Hyper-V"""

from oslo.config import cfg

from ceilometer.compute.virt.hyperv import utilsv2
from ceilometer.compute.virt import inspector as virt_inspector
from ceilometer.openstack.common import log
from ceilometer.openstack.common import units

CONF = cfg.CONF
LOG = log.getLogger(__name__)


class HyperVInspector(virt_inspector.Inspector):

    def __init__(self):
        super(HyperVInspector, self).__init__()
        self._utils = utilsv2.UtilsV2()

    def inspect_instances(self):
        for element_name, name in self._utils.get_all_vms():
            yield virt_inspector.Instance(
                name=element_name,
                UUID=name)

    def inspect_cpus(self, instance_name):
        (cpu_clock_used,
         cpu_count, uptime) = self._utils.get_cpu_metrics(instance_name)
        host_cpu_clock, host_cpu_count = self._utils.get_host_cpu_info()

        cpu_percent_used = (cpu_clock_used /
                            float(host_cpu_clock * cpu_count))
        # Nanoseconds
        cpu_time = (long(uptime * cpu_percent_used) * units.k)

        return virt_inspector.CPUStats(number=cpu_count, time=cpu_time)

    def inspect_vnics(self, instance_name):
        for vnic_metrics in self._utils.get_vnic_metrics(instance_name):
            interface = virt_inspector.Interface(
                name=vnic_metrics["element_name"],
                mac=vnic_metrics["address"],
                fref=None,
                parameters=None)

            stats = virt_inspector.InterfaceStats(
                rx_bytes=vnic_metrics['rx_mb'] * units.Mi,
                rx_packets=0,
                tx_bytes=vnic_metrics['tx_mb'] * units.Mi,
                tx_packets=0)

            yield (interface, stats)

    def inspect_disks(self, instance_name):
        for disk_metrics in self._utils.get_disk_metrics(instance_name):
            device = dict([(i, disk_metrics[i])
                          for i in ['instance_id', 'host_resource']
                          if i in disk_metrics])

            disk = virt_inspector.Disk(device=device)
            stats = virt_inspector.DiskStats(
                read_requests=0,
                # Return bytes
                read_bytes=disk_metrics['read_mb'] * units.Mi,
                write_requests=0,
                write_bytes=disk_metrics['write_mb'] * units.Mi,
                errors=0)

            yield (disk, stats)

########NEW FILE########
__FILENAME__ = utilsv2
# Copyright 2013 Cloudbase Solutions Srl
#
# Author: Claudiu Belu <cbelu@cloudbasesolutions.com>
#         Alessandro Pilotti <apilotti@cloudbasesolutions.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""
Utility class for VM related operations.
Based on the "root/virtualization/v2" namespace available starting with
Hyper-V Server / Windows Server 2012.
"""

import sys

if sys.platform == 'win32':
    import wmi

from oslo.config import cfg

from ceilometer.compute.virt import inspector
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log as logging

CONF = cfg.CONF
LOG = logging.getLogger(__name__)


class HyperVException(inspector.InspectorException):
    pass


class UtilsV2(object):

    _VIRTUAL_SYSTEM_TYPE_REALIZED = 'Microsoft:Hyper-V:System:Realized'

    _PROC_SETTING = 'Msvm_ProcessorSettingData'
    _SYNTH_ETH_PORT = 'Msvm_SyntheticEthernetPortSettingData'
    _ETH_PORT_ALLOC = 'Msvm_EthernetPortAllocationSettingData'
    _PORT_ACL_SET_DATA = 'Msvm_EthernetSwitchPortAclSettingData'
    _STORAGE_ALLOC = 'Msvm_StorageAllocationSettingData'
    _VS_SETTING_DATA = 'Msvm_VirtualSystemSettingData'
    _METRICS_ME = 'Msvm_MetricForME'
    _BASE_METRICS_VALUE = 'Msvm_BaseMetricValue'

    _CPU_METRIC_NAME = 'Aggregated Average CPU Utilization'
    _NET_IN_METRIC_NAME = 'Filtered Incoming Network Traffic'
    _NET_OUT_METRIC_NAME = 'Filtered Outgoing Network Traffic'
    # Disk metrics are supported from Hyper-V 2012 R2
    _DISK_RD_METRIC_NAME = 'Disk Data Read'
    _DISK_WR_METRIC_NAME = 'Disk Data Written'

    def __init__(self, host='.'):
        if sys.platform == 'win32':
            self._init_hyperv_wmi_conn(host)
            self._init_cimv2_wmi_conn(host)
        self._host_cpu_info = None

    def _init_hyperv_wmi_conn(self, host):
        self._conn = wmi.WMI(moniker='//%s/root/virtualization/v2' % host)

    def _init_cimv2_wmi_conn(self, host):
        self._conn_cimv2 = wmi.WMI(moniker='//%s/root/cimv2' % host)

    def get_host_cpu_info(self):
        if not self._host_cpu_info:
            host_cpus = self._conn_cimv2.Win32_Processor()
            self._host_cpu_info = (host_cpus[0].MaxClockSpeed, len(host_cpus))
        return self._host_cpu_info

    def get_all_vms(self):
        vms = [(v.ElementName, v.Name) for v in
               self._conn.Msvm_ComputerSystem(['ElementName', 'Name'],
                                              Caption="Virtual Machine")]
        return vms

    def get_cpu_metrics(self, vm_name):
        vm = self._lookup_vm(vm_name)
        cpu_sd = self._get_vm_resources(vm, self._PROC_SETTING)[0]
        cpu_metrics_def = self._get_metric_def(self._CPU_METRIC_NAME)
        cpu_metric_aggr = self._get_metrics(vm, cpu_metrics_def)

        cpu_used = 0
        if cpu_metric_aggr:
            cpu_used = long(cpu_metric_aggr[0].MetricValue)

        return (cpu_used,
                int(cpu_sd.VirtualQuantity),
                long(vm.OnTimeInMilliseconds))

    def get_vnic_metrics(self, vm_name):
        vm = self._lookup_vm(vm_name)
        ports = self._get_vm_resources(vm, self._ETH_PORT_ALLOC)
        vnics = self._get_vm_resources(vm, self._SYNTH_ETH_PORT)

        metric_def_in = self._get_metric_def(self._NET_IN_METRIC_NAME)
        metric_def_out = self._get_metric_def(self._NET_OUT_METRIC_NAME)

        for port in ports:
            vnic = [v for v in vnics if port.Parent == v.path_()][0]

            metric_value_instances = self._get_metric_value_instances(
                port.associators(wmi_result_class=self._PORT_ACL_SET_DATA),
                self._BASE_METRICS_VALUE)
            metric_values = self._sum_metric_values_by_defs(
                metric_value_instances, [metric_def_in, metric_def_out])

            yield {
                'rx_mb': metric_values[0],
                'tx_mb': metric_values[1],
                'element_name': vnic.ElementName,
                'address': vnic.Address
            }

    def get_disk_metrics(self, vm_name):
        vm = self._lookup_vm(vm_name)
        metric_def_r = self._get_metric_def(self._DISK_RD_METRIC_NAME)
        metric_def_w = self._get_metric_def(self._DISK_WR_METRIC_NAME)

        disks = self._get_vm_resources(vm, self._STORAGE_ALLOC)
        for disk in disks:
            metric_values = self._get_metric_values(
                disk, [metric_def_r, metric_def_w])

            # Thi sis e.g. the VHD file location
            if disk.HostResource:
                host_resource = disk.HostResource[0]

            yield {
                # Values are in megabytes
                'read_mb': metric_values[0],
                'write_mb': metric_values[1],
                'instance_id': disk.InstanceID,
                'host_resource': host_resource
            }

    def _sum_metric_values(self, metrics):
        tot_metric_val = 0
        for metric in metrics:
            tot_metric_val += long(metric.MetricValue)
        return tot_metric_val

    def _sum_metric_values_by_defs(self, element_metrics, metric_defs):
        metric_values = []
        for metric_def in metric_defs:
            if metric_def:
                metrics = self._filter_metrics(element_metrics, metric_def)
                metric_values.append(self._sum_metric_values(metrics))
            else:
                # In case the metric is not defined on this host
                metric_values.append(0)
        return metric_values

    def _get_metric_value_instances(self, elements, result_class):
        instances = []
        for el in elements:
            associators = el.associators(wmi_result_class=result_class)
            if associators:
                instances.append(associators[0])

        return instances

    def _get_metric_values(self, element, metric_defs):
        element_metrics = element.associators(
            wmi_association_class=self._METRICS_ME)
        return self._sum_metric_values_by_defs(element_metrics, metric_defs)

    def _lookup_vm(self, vm_name):
        vms = self._conn.Msvm_ComputerSystem(ElementName=vm_name)
        n = len(vms)
        if n == 0:
            raise inspector.InstanceNotFoundException(
                _('VM %s not found on Hyper-V') % vm_name)
        elif n > 1:
            raise HyperVException(_('Duplicate VM name found: %s') % vm_name)
        else:
            return vms[0]

    def _get_metrics(self, element, metric_def):
        return self._filter_metrics(
            element.associators(
                wmi_association_class=self._METRICS_ME), metric_def)

    def _filter_metrics(self, all_metrics, metric_def):
        return [v for v in all_metrics if
                v.MetricDefinitionId == metric_def.Id]

    def _get_metric_def(self, metric_def):
        metric = self._conn.CIM_BaseMetricDefinition(ElementName=metric_def)
        if metric:
            return metric[0]

    def _get_vm_setting_data(self, vm):
        vm_settings = vm.associators(
            wmi_result_class=self._VS_SETTING_DATA)
        # Avoid snapshots
        return [s for s in vm_settings if
                s.VirtualSystemType == self._VIRTUAL_SYSTEM_TYPE_REALIZED][0]

    def _get_vm_resources(self, vm, resource_class):
        setting_data = self._get_vm_setting_data(vm)
        return setting_data.associators(wmi_result_class=resource_class)

########NEW FILE########
__FILENAME__ = inspector
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 Red Hat, Inc
#
# Author: Eoghan Glynn <eglynn@redhat.com>
#         Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Inspector abstraction for read-only access to hypervisors."""

import collections

from oslo.config import cfg
from stevedore import driver

from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log


OPTS = [
    cfg.StrOpt('hypervisor_inspector',
               default='libvirt',
               help='Inspector to use for inspecting the hypervisor layer.'),
]

cfg.CONF.register_opts(OPTS)


LOG = log.getLogger(__name__)

# Named tuple representing instances.
#
# name: the name of the instance
# uuid: the UUID associated with the instance
#
Instance = collections.namedtuple('Instance', ['name', 'UUID'])


# Named tuple representing CPU statistics.
#
# number: number of CPUs
# time: cumulative CPU time
#
CPUStats = collections.namedtuple('CPUStats', ['number', 'time'])

# Named tuple representing CPU Utilization statistics.
#
# util: CPU utilization in percentage
#
CPUUtilStats = collections.namedtuple('CPUUtilStats', ['util'])

# Named tuple representing Memory usage statistics.
#
# usage: Amount of memory used
#
MemoryUsageStats = collections.namedtuple('MemoryUsageStats', ['usage'])


# Named tuple representing vNICs.
#
# name: the name of the vNIC
# mac: the MAC address
# fref: the filter ref
# parameters: miscellaneous parameters
#
Interface = collections.namedtuple('Interface', ['name', 'mac',
                                                 'fref', 'parameters'])


# Named tuple representing vNIC statistics.
#
# rx_bytes: number of received bytes
# rx_packets: number of received packets
# tx_bytes: number of transmitted bytes
# tx_packets: number of transmitted packets
#
InterfaceStats = collections.namedtuple('InterfaceStats',
                                        ['rx_bytes', 'rx_packets',
                                         'tx_bytes', 'tx_packets'])


# Named tuple representing vNIC rate statistics.
#
# rx_bytes_rate: rate of received bytes
# tx_bytes_rate: rate of transmitted bytes
#
InterfaceRateStats = collections.namedtuple('InterfaceRateStats',
                                            ['rx_bytes_rate', 'tx_bytes_rate'])


# Named tuple representing disks.
#
# device: the device name for the disk
#
Disk = collections.namedtuple('Disk', ['device'])


# Named tuple representing disk statistics.
#
# read_bytes: number of bytes read
# read_requests: number of read operations
# write_bytes: number of bytes written
# write_requests: number of write operations
# errors: number of errors
#
DiskStats = collections.namedtuple('DiskStats',
                                   ['read_bytes', 'read_requests',
                                    'write_bytes', 'write_requests',
                                    'errors'])

# Named tuple representing disk rate statistics.
#
# read_bytes_rate: number of bytes read per second
# read_requests_rate: number of read operations per second
# write_bytes_rate: number of bytes written per second
# write_requests_rate: number of write operations per second
#
DiskRateStats = collections.namedtuple('DiskRateStats',
                                       ['read_bytes_rate',
                                        'read_requests_rate',
                                        'write_bytes_rate',
                                        'write_requests_rate'])


# Exception types
#
class InspectorException(Exception):
    def __init__(self, message=None):
        super(InspectorException, self).__init__(message)


class InstanceNotFoundException(InspectorException):
    pass


# Main virt inspector abstraction layering over the hypervisor API.
#
class Inspector(object):

    def inspect_instances(self):
        """List the instances on the current host."""
        raise NotImplementedError()

    def inspect_cpus(self, instance_name):
        """Inspect the CPU statistics for an instance.

        :param instance_name: the name of the target instance
        :return: the number of CPUs and cumulative CPU time
        """
        raise NotImplementedError()

    def inspect_cpu_util(self, instance, duration=None):
        """Inspect the CPU Utilization (%) for an instance.

        :param instance: the target instance
        :param duration: the last 'n' seconds, over which the value should be
               inspected
        :return: the percentage of CPU utilization
        """
        raise NotImplementedError()

    def inspect_vnics(self, instance_name):
        """Inspect the vNIC statistics for an instance.

        :param instance_name: the name of the target instance
        :return: for each vNIC, the number of bytes & packets
                 received and transmitted
        """
        raise NotImplementedError()

    def inspect_vnic_rates(self, instance, duration=None):
        """Inspect the vNIC rate statistics for an instance.

        :param instance: the target instance
        :param duration: the last 'n' seconds, over which the value should be
               inspected
        :return: for each vNIC, the rate of bytes & packets
                 received and transmitted
        """
        raise NotImplementedError()

    def inspect_disks(self, instance_name):
        """Inspect the disk statistics for an instance.

        :param instance_name: the name of the target instance
        :return: for each disk, the number of bytes & operations
                 read and written, and the error count
        """
        raise NotImplementedError()

    def inspect_memory_usage(self, instance, duration=None):
        """Inspect the memory usage statistics for an instance.

        :param instance: the target instance
        :param duration: the last 'n' seconds, over which the value should be
               inspected
        :return: the amount of memory used
        """
        raise NotImplementedError()

    def inspect_disk_rates(self, instance, duration=None):
        """Inspect the disk statistics as rates for an instance.

        :param instance: the target instance
        :param duration: the last 'n' seconds, over which the value should be
               inspected
        :return: for each disk, the number of bytes & operations
                 read and written per second, with the error count
        """
        raise NotImplementedError()


def get_hypervisor_inspector():
    try:
        namespace = 'ceilometer.compute.virt'
        mgr = driver.DriverManager(namespace,
                                   cfg.CONF.hypervisor_inspector,
                                   invoke_on_load=True)
        return mgr.driver
    except ImportError as e:
        LOG.error(_("Unable to load the hypervisor inspector: %s") % (e))
        return Inspector()

########NEW FILE########
__FILENAME__ = inspector
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 Red Hat, Inc
#
# Author: Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Implementation of Inspector abstraction for libvirt."""

from lxml import etree
from oslo.config import cfg

from ceilometer.compute.virt import inspector as virt_inspector
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log as logging

libvirt = None

LOG = logging.getLogger(__name__)

libvirt_opts = [
    cfg.StrOpt('libvirt_type',
               default='kvm',
               help='Libvirt domain type (valid options are: '
                    'kvm, lxc, qemu, uml, xen).'),
    cfg.StrOpt('libvirt_uri',
               default='',
               help='Override the default libvirt URI '
                    '(which is dependent on libvirt_type).'),
]

CONF = cfg.CONF
CONF.register_opts(libvirt_opts)


class LibvirtInspector(virt_inspector.Inspector):

    per_type_uris = dict(uml='uml:///system', xen='xen:///', lxc='lxc:///')

    def __init__(self):
        self.uri = self._get_uri()
        self.connection = None

    def _get_uri(self):
        return CONF.libvirt_uri or self.per_type_uris.get(CONF.libvirt_type,
                                                          'qemu:///system')

    def _get_connection(self):
        if not self.connection or not self._test_connection():
            global libvirt
            if libvirt is None:
                libvirt = __import__('libvirt')

            LOG.debug(_('Connecting to libvirt: %s'), self.uri)
            self.connection = libvirt.openReadOnly(self.uri)

        return self.connection

    def _test_connection(self):
        try:
            self.connection.getCapabilities()
            return True
        except libvirt.libvirtError as e:
            if (e.get_error_code() == libvirt.VIR_ERR_SYSTEM_ERROR and
                e.get_error_domain() in (libvirt.VIR_FROM_REMOTE,
                                         libvirt.VIR_FROM_RPC)):
                LOG.debug(_('Connection to libvirt broke'))
                return False
            raise

    def _lookup_by_name(self, instance_name):
        try:
            return self._get_connection().lookupByName(instance_name)
        except Exception as ex:
            if not libvirt or not isinstance(ex, libvirt.libvirtError):
                raise virt_inspector.InspectorException(unicode(ex))
            error_code = ex.get_error_code()
            msg = ("Error from libvirt while looking up %(instance_name)s: "
                   "[Error Code %(error_code)s] "
                   "%(ex)s" % {'instance_name': instance_name,
                               'error_code': error_code,
                               'ex': ex})
            raise virt_inspector.InstanceNotFoundException(msg)

    def inspect_instances(self):
        if self._get_connection().numOfDomains() > 0:
            for domain_id in self._get_connection().listDomainsID():
                try:
                    # We skip domains with ID 0 (hypervisors).
                    if domain_id != 0:
                        domain = self._get_connection().lookupByID(domain_id)
                        yield virt_inspector.Instance(name=domain.name(),
                                                      UUID=domain.UUIDString())
                except libvirt.libvirtError:
                    # Instance was deleted while listing... ignore it
                    pass

    def inspect_cpus(self, instance_name):
        domain = self._lookup_by_name(instance_name)
        dom_info = domain.info()
        return virt_inspector.CPUStats(number=dom_info[3], time=dom_info[4])

    def inspect_vnics(self, instance_name):
        domain = self._lookup_by_name(instance_name)
        state = domain.info()[0]
        if state == libvirt.VIR_DOMAIN_SHUTOFF:
            LOG.warn(_('Failed to inspect vnics of %(instance_name)s, '
                       'domain is in state of SHUTOFF'),
                     {'instance_name': instance_name})
            return
        tree = etree.fromstring(domain.XMLDesc(0))
        for iface in tree.findall('devices/interface'):
            target = iface.find('target')
            if target is not None:
                name = target.get('dev')
            else:
                continue
            mac = iface.find('mac')
            if mac is not None:
                mac_address = mac.get('address')
            else:
                continue
            fref = iface.find('filterref')
            if fref is not None:
                fref = fref.get('filter')

            params = dict((p.get('name').lower(), p.get('value'))
                          for p in iface.findall('filterref/parameter'))
            interface = virt_inspector.Interface(name=name, mac=mac_address,
                                                 fref=fref, parameters=params)
            dom_stats = domain.interfaceStats(name)
            stats = virt_inspector.InterfaceStats(rx_bytes=dom_stats[0],
                                                  rx_packets=dom_stats[1],
                                                  tx_bytes=dom_stats[4],
                                                  tx_packets=dom_stats[5])
            yield (interface, stats)

    def inspect_disks(self, instance_name):
        domain = self._lookup_by_name(instance_name)
        state = domain.info()[0]
        if state == libvirt.VIR_DOMAIN_SHUTOFF:
            LOG.warn(_('Failed to inspect disks of %(instance_name)s, '
                       'domain is in state of SHUTOFF'),
                     {'instance_name': instance_name})
            return
        tree = etree.fromstring(domain.XMLDesc(0))
        for device in filter(
                bool,
                [target.get("dev")
                 for target in tree.findall('devices/disk/target')]):
            disk = virt_inspector.Disk(device=device)
            block_stats = domain.blockStats(device)
            stats = virt_inspector.DiskStats(read_requests=block_stats[0],
                                             read_bytes=block_stats[1],
                                             write_requests=block_stats[2],
                                             write_bytes=block_stats[3],
                                             errors=block_stats[4])
            yield (disk, stats)

########NEW FILE########
__FILENAME__ = inspector
# Copyright (c) 2014 VMware, Inc.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Implementation of Inspector abstraction for VMware vSphere"""

from oslo.config import cfg
from oslo.vmware import api
from oslo.vmware import vim

from ceilometer.compute.virt import inspector as virt_inspector
from ceilometer.compute.virt.vmware import vsphere_operations
from ceilometer.openstack.common import units


opt_group = cfg.OptGroup(name='vmware',
                         title='Options for VMware')

OPTS = [
    cfg.StrOpt('host_ip',
               default='',
               help='IP address of the VMware Vsphere host'),
    cfg.StrOpt('host_username',
               default='',
               help='Username of VMware Vsphere'),
    cfg.StrOpt('host_password',
               default='',
               help='Password of VMware Vsphere'),
    cfg.IntOpt('api_retry_count',
               default=10,
               help='Number of times a VMware Vsphere API must be retried'),
    cfg.FloatOpt('task_poll_interval',
                 default=0.5,
                 help='Sleep time in seconds for polling an ongoing async '
                 'task')
]

cfg.CONF.register_group(opt_group)
cfg.CONF.register_opts(OPTS, group=opt_group)

VC_AVERAGE_MEMORY_CONSUMED_CNTR = 'mem:consumed:average'
VC_AVERAGE_CPU_CONSUMED_CNTR = 'cpu:usage:average'
VC_NETWORK_RX_COUNTER = 'net:received:average'
VC_NETWORK_TX_COUNTER = 'net:transmitted:average'
VC_DISK_READ_RATE_CNTR = "disk:read:average"
VC_DISK_READ_REQUESTS_RATE_CNTR = "disk:numberReadAveraged:average"
VC_DISK_WRITE_RATE_CNTR = "disk:write:average"
VC_DISK_WRITE_REQUESTS_RATE_CNTR = "disk:numberWriteAveraged:average"


def get_api_session():
    hostIp = cfg.CONF.vmware.host_ip
    wsdl_loc = vim.Vim._get_wsdl_loc("https", hostIp)
    api_session = api.VMwareAPISession(
        hostIp,
        cfg.CONF.vmware.host_username,
        cfg.CONF.vmware.host_password,
        cfg.CONF.vmware.api_retry_count,
        cfg.CONF.vmware.task_poll_interval,
        wsdl_loc=wsdl_loc)
    return api_session


class VsphereInspector(virt_inspector.Inspector):

    def __init__(self):
        super(VsphereInspector, self).__init__()
        self._ops = vsphere_operations.VsphereOperations(
            get_api_session(), 1000)

    def inspect_cpu_util(self, instance, duration=None):
        vm_moid = self._ops.get_vm_moid(instance.id)
        if vm_moid is None:
            raise virt_inspector.InstanceNotFoundException(
                _('VM %s not found in VMware Vsphere') % instance.id)
        cpu_util_counter_id = self._ops.get_perf_counter_id(
            VC_AVERAGE_CPU_CONSUMED_CNTR)
        cpu_util = self._ops.query_vm_aggregate_stats(
            vm_moid, cpu_util_counter_id, duration)

        # For this counter vSphere returns values scaled-up by 100, since the
        # corresponding API can't return decimals, but only longs.
        # For e.g. if the utilization is 12.34%, the value returned is 1234.
        # Hence, dividing by 100.
        cpu_util = cpu_util / 100
        return virt_inspector.CPUUtilStats(util=cpu_util)

    def inspect_vnic_rates(self, instance, duration=None):
        vm_moid = self._ops.get_vm_moid(instance.id)
        if not vm_moid:
            raise virt_inspector.InstanceNotFoundException(
                _('VM %s not found in VMware Vsphere') % instance.id)

        vnic_stats = {}
        vnic_ids = set()

        for net_counter in (VC_NETWORK_RX_COUNTER, VC_NETWORK_TX_COUNTER):
            net_counter_id = self._ops.get_perf_counter_id(net_counter)
            vnic_id_to_stats_map = self._ops.query_vm_device_stats(
                vm_moid, net_counter_id, duration)
            vnic_stats[net_counter] = vnic_id_to_stats_map
            vnic_ids.update(vnic_id_to_stats_map.iterkeys())

        # Stats provided from vSphere are in KB/s, converting it to B/s.
        for vnic_id in vnic_ids:
            rx_bytes_rate = (vnic_stats[VC_NETWORK_RX_COUNTER]
                             .get(vnic_id, 0) * units.Ki)
            tx_bytes_rate = (vnic_stats[VC_NETWORK_TX_COUNTER]
                             .get(vnic_id, 0) * units.Ki)

            stats = virt_inspector.InterfaceRateStats(rx_bytes_rate,
                                                      tx_bytes_rate)
            interface = virt_inspector.Interface(
                name=vnic_id,
                mac=None,
                fref=None,
                parameters=None)
            yield (interface, stats)

    def inspect_memory_usage(self, instance, duration=None):
        vm_moid = self._ops.get_vm_moid(instance.id)
        if vm_moid is None:
            raise virt_inspector.InstanceNotFoundException(
                _('VM %s not found in VMware Vsphere') % instance.id)
        mem_counter_id = self._ops.get_perf_counter_id(
            VC_AVERAGE_MEMORY_CONSUMED_CNTR)
        memory = self._ops.query_vm_aggregate_stats(
            vm_moid, mem_counter_id, duration)
        # Stat provided from vSphere is in KB, converting it to MB.
        memory = memory / units.Ki
        return virt_inspector.MemoryUsageStats(usage=memory)

    def inspect_disk_rates(self, instance, duration=None):
        vm_moid = self._ops.get_vm_moid(instance.id)
        if not vm_moid:
            raise virt_inspector.InstanceNotFoundException(
                _('VM %s not found in VMware Vsphere') % instance.id)

        disk_stats = {}
        disk_ids = set()
        disk_counters = [
            VC_DISK_READ_RATE_CNTR,
            VC_DISK_READ_REQUESTS_RATE_CNTR,
            VC_DISK_WRITE_RATE_CNTR,
            VC_DISK_WRITE_REQUESTS_RATE_CNTR
        ]

        for disk_counter in disk_counters:
            disk_counter_id = self._ops.get_perf_counter_id(disk_counter)
            disk_id_to_stat_map = self._ops.query_vm_device_stats(
                vm_moid, disk_counter_id, duration)
            disk_stats[disk_counter] = disk_id_to_stat_map
            disk_ids.update(disk_id_to_stat_map.iterkeys())

        for disk_id in disk_ids:

            def stat_val(counter_name):
                return disk_stats[counter_name].get(disk_id, 0)

            disk = virt_inspector.Disk(device=disk_id)
            # Stats provided from vSphere are in KB/s, converting it to B/s.
            disk_rate_info = virt_inspector.DiskRateStats(
                read_bytes_rate=stat_val(VC_DISK_READ_RATE_CNTR) * units.Ki,
                read_requests_rate=stat_val(VC_DISK_READ_REQUESTS_RATE_CNTR),
                write_bytes_rate=stat_val(VC_DISK_WRITE_RATE_CNTR) * units.Ki,
                write_requests_rate=stat_val(VC_DISK_WRITE_REQUESTS_RATE_CNTR)
            )
            yield(disk, disk_rate_info)

########NEW FILE########
__FILENAME__ = vsphere_operations
# Copyright (c) 2014 VMware, Inc.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from oslo.vmware import vim_util


PERF_MANAGER_TYPE = "PerformanceManager"
PERF_COUNTER_PROPERTY = "perfCounter"
VM_INSTANCE_ID_PROPERTY = 'config.extraConfig["nvp.vm-uuid"].value'

# ESXi Servers sample performance data every 20 seconds. 20-second interval
# data is called instance data or real-time data. To retrieve instance data,
# we need to specify a value of 20 seconds for the "PerfQuerySpec.intervalId"
# property. In that case the "QueryPerf" method operates as a raw data feed
# that bypasses the vCenter database and instead retrieves performance data
# from an ESXi host.
# The following value is time interval for real-time performance stats
# in seconds and it is not configurable.
VC_REAL_TIME_SAMPLING_INTERVAL = 20


class VsphereOperations(object):
    """Class to invoke vSphere APIs calls required by various
       pollsters, collecting data from VMware infrastructure.
    """
    def __init__(self, api_session, max_objects):
        self._api_session = api_session
        self._max_objects = max_objects
        # Mapping between "VM's Nova instance Id" -> "VM's MOID"
        # In case a VM is deployed by Nova, then its name is instance ID.
        # So this map essentially has VM names as keys.
        self._vm_moid_lookup_map = {}

        # Mapping from full name -> ID, for VC Performance counters
        self._perf_counter_id_lookup_map = None

    def _init_vm_moid_lookup_map(self):
        session = self._api_session
        result = session.invoke_api(vim_util, "get_objects", session.vim,
                                    "VirtualMachine", self._max_objects,
                                    [VM_INSTANCE_ID_PROPERTY],
                                    False)
        while result:
            for vm_object in result.objects:
                vm_moid = vm_object.obj.value
                # propSet will be set only if the server provides value
                if hasattr(vm_object, 'propSet') and vm_object.propSet:
                    vm_instance_id = vm_object.propSet[0].val
                    if vm_instance_id:
                        self._vm_moid_lookup_map[vm_instance_id] = vm_moid

            result = session.invoke_api(vim_util, "continue_retrieval",
                                        session.vim, result)

    def get_vm_moid(self, vm_instance_id):
        """Method returns VC MOID of the VM by its NOVA instance ID.
        """
        if vm_instance_id not in self._vm_moid_lookup_map:
            self._init_vm_moid_lookup_map()

        return self._vm_moid_lookup_map.get(vm_instance_id, None)

    def _init_perf_counter_id_lookup_map(self):

        # Query details of all the performance counters from VC
        session = self._api_session
        client_factory = session.vim.client.factory
        perf_manager = session.vim.service_content.perfManager

        prop_spec = vim_util.build_property_spec(
            client_factory, PERF_MANAGER_TYPE, [PERF_COUNTER_PROPERTY])

        obj_spec = vim_util.build_object_spec(
            client_factory, perf_manager, None)

        filter_spec = vim_util.build_property_filter_spec(
            client_factory, [prop_spec], [obj_spec])

        options = client_factory.create('ns0:RetrieveOptions')
        options.maxObjects = 1

        prop_collector = session.vim.service_content.propertyCollector
        result = session.invoke_api(session.vim, "RetrievePropertiesEx",
                                    prop_collector, specSet=[filter_spec],
                                    options=options)

        perf_counter_infos = result.objects[0].propSet[0].val.PerfCounterInfo

        # Extract the counter Id for each counter and populate the map
        self._perf_counter_id_lookup_map = {}
        for perf_counter_info in perf_counter_infos:

            counter_group = perf_counter_info.groupInfo.key
            counter_name = perf_counter_info.nameInfo.key
            counter_rollup_type = perf_counter_info.rollupType
            counter_id = perf_counter_info.key

            counter_full_name = (counter_group + ":" + counter_name + ":" +
                                 counter_rollup_type)
            self._perf_counter_id_lookup_map[counter_full_name] = counter_id

    def get_perf_counter_id(self, counter_full_name):
        """Method returns the ID of VC performance counter by its full name.

        A VC performance counter is uniquely identified by the
        tuple {'Group Name', 'Counter Name', 'Rollup Type'}.
        It will have an id - counter ID (changes from one VC to another),
        which is required to query performance stats from that VC.
        This method returns the ID for a counter,
        assuming 'CounterFullName' => 'Group Name:CounterName:RollupType'.
        """
        if not self._perf_counter_id_lookup_map:
            self._init_perf_counter_id_lookup_map()
        return self._perf_counter_id_lookup_map[counter_full_name]

    # TODO(akhils@vmware.com) Move this method to common library
    # when it gets checked-in
    def query_vm_property(self, vm_moid, property_name):
        """Method returns the value of specified property for a VM.

        :param vm_moid: moid of the VM whose property is to be queried
        :param property_name: path of the property
        """
        vm_mobj = vim_util.get_moref(vm_moid, "VirtualMachine")
        session = self._api_session
        return session.invoke_api(vim_util, "get_object_property",
                                  session.vim, vm_mobj, property_name)

    def query_vm_aggregate_stats(self, vm_moid, counter_id, duration):
        """Method queries the aggregated real-time stat value for a VM.

        This method should be used for aggregate counters.

        :param vm_moid: moid of the VM
        :param counter_id: id of the perf counter in VC
        :param duration: in seconds from current time,
            over which the stat value was applicable
        :return: the aggregated stats value for the counter
        """
        # For aggregate counters, device_name should be ""
        stats = self._query_vm_perf_stats(vm_moid, counter_id, "", duration)

        # Performance manager provides the aggregated stats value
        # with device name -> None
        return stats.get(None, 0)

    def query_vm_device_stats(self, vm_moid, counter_id, duration):
        """Method queries the real-time stat values for a VM, for all devices.

        This method should be used for device(non-aggregate) counters.

        :param vm_moid: moid of the VM
        :param counter_id: id of the perf counter in VC
        :param duration: in seconds from current time,
            over which the stat value was applicable
        :return: a map containing the stat values keyed by the device ID/name
        """
        # For device counters, device_name should be "*" to get stat values
        # for all devices.
        stats = self._query_vm_perf_stats(vm_moid, counter_id, "*", duration)

        # For some device counters, in addition to the per device value
        # the Performance manager also returns the aggregated value.
        # Just to be consistent, deleting the aggregated value if present.
        stats.pop(None, None)
        return stats

    def _query_vm_perf_stats(self, vm_moid, counter_id, device_name, duration):
        """Method queries the real-time stat values for a VM.

        :param vm_moid: moid of the VM for which stats are needed
        :param counter_id: id of the perf counter in VC
        :param device_name: name of the device for which stats are to be
            queried. For aggregate counters pass empty string ("").
            For device counters pass "*", if stats are required over all
            devices.
        :param duration: in seconds from current time,
            over which the stat value was applicable
        :return: a map containing the stat values keyed by the device ID/name
        """

        session = self._api_session
        client_factory = session.vim.client.factory

        # Construct the QuerySpec
        metric_id = client_factory.create('ns0:PerfMetricId')
        metric_id.counterId = counter_id
        metric_id.instance = device_name

        query_spec = client_factory.create('ns0:PerfQuerySpec')
        query_spec.entity = vim_util.get_moref(vm_moid, "VirtualMachine")
        query_spec.metricId = [metric_id]
        query_spec.intervalId = VC_REAL_TIME_SAMPLING_INTERVAL
        # We query all samples which are applicable over the specified duration
        samples_cnt = (duration / VC_REAL_TIME_SAMPLING_INTERVAL if duration
                       else 1)
        query_spec.maxSample = samples_cnt

        perf_manager = session.vim.service_content.perfManager
        perf_stats = session.invoke_api(session.vim, 'QueryPerf', perf_manager,
                                        querySpec=[query_spec])

        stat_values = {}
        if perf_stats:
            entity_metric = perf_stats[0]
            sample_infos = entity_metric.sampleInfo

            if len(sample_infos) > 0:
                for metric_series in entity_metric.value:
                    # Take the average of all samples to improve the accuracy
                    # of the stat value
                    stat_value = float(sum(metric_series.value)) / samples_cnt
                    device_id = metric_series.id.instance
                    stat_values[device_id] = stat_value

        return stat_values

########NEW FILE########
__FILENAME__ = database
# -*- encoding: utf-8 -*-
#
# Copyright 2013 IBM Corp
#
# Author: Tong Li <litong01@us.ibm.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ceilometer import dispatcher
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer.openstack.common import timeutils
from ceilometer.publisher import utils as publisher_utils
from ceilometer import storage

LOG = log.getLogger(__name__)


class DatabaseDispatcher(dispatcher.Base):
    '''Dispatcher class for recording metering data into database.

    The dispatcher class which records each meter into a database configured
    in ceilometer configuration file.

    To enable this dispatcher, the following section needs to be present in
    ceilometer.conf file

    dispatchers = database
    '''
    def __init__(self, conf):
        super(DatabaseDispatcher, self).__init__(conf)
        self.storage_conn = storage.get_connection_from_config(conf)

    def record_metering_data(self, data):
        # We may have receive only one counter on the wire
        if not isinstance(data, list):
            data = [data]

        for meter in data:
            LOG.debug(_(
                'metering data %(counter_name)s '
                'for %(resource_id)s @ %(timestamp)s: %(counter_volume)s')
                % ({'counter_name': meter['counter_name'],
                    'resource_id': meter['resource_id'],
                    'timestamp': meter.get('timestamp', 'NO TIMESTAMP'),
                    'counter_volume': meter['counter_volume']}))
            if publisher_utils.verify_signature(
                    meter,
                    self.conf.publisher.metering_secret):
                try:
                    # Convert the timestamp to a datetime instance.
                    # Storage engines are responsible for converting
                    # that value to something they can store.
                    if meter.get('timestamp'):
                        ts = timeutils.parse_isotime(meter['timestamp'])
                        meter['timestamp'] = timeutils.normalize_time(ts)
                    self.storage_conn.record_metering_data(meter)
                except Exception as err:
                    LOG.exception(_('Failed to record metering data: %s'),
                                  err)
            else:
                LOG.warning(_(
                    'message signature invalid, discarding message: %r'),
                    meter)

    def record_events(self, events):
        if not isinstance(events, list):
            events = [events]

        return self.storage_conn.record_events(events)

########NEW FILE########
__FILENAME__ = file
# -*- encoding: utf-8 -*-
#
# Copyright 2013 IBM Corp
#
# Author: Tong Li <litong01@us.ibm.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import logging
import logging.handlers

from oslo.config import cfg

from ceilometer import dispatcher

file_dispatcher_opts = [
    cfg.StrOpt('file_path',
               default=None,
               help='Name and the location of the file to record '
                    'meters.'),
    cfg.IntOpt('max_bytes',
               default=0,
               help='The max size of the file.'),
    cfg.IntOpt('backup_count',
               default=0,
               help='The max number of the files to keep.'),
]

cfg.CONF.register_opts(file_dispatcher_opts, group="dispatcher_file")


class FileDispatcher(dispatcher.Base):
    '''Dispatcher class for recording metering data to a file.

    The dispatcher class which logs each meter into a file configured in
    ceilometer configuration file. An example configuration may look like the
    following:

    [dispatcher_file]
    file_path = /tmp/meters

    To enable this dispatcher, the following section needs to be present in
    ceilometer.conf file

    [collector]
    dispatchers = file
    '''

    def __init__(self, conf):
        super(FileDispatcher, self).__init__(conf)
        self.log = None

        # if the directory and path are configured, then log to the file
        if self.conf.dispatcher_file.file_path:
            dispatcher_logger = logging.Logger('dispatcher.file')
            dispatcher_logger.setLevel(logging.INFO)
            # create rotating file handler which logs meters
            rfh = logging.handlers.RotatingFileHandler(
                self.conf.dispatcher_file.file_path,
                maxBytes=self.conf.dispatcher_file.max_bytes,
                backupCount=self.conf.dispatcher_file.backup_count,
                encoding='utf8')

            rfh.setLevel(logging.INFO)
            # Only wanted the meters to be saved in the file, not the
            # project root logger.
            dispatcher_logger.propagate = False
            dispatcher_logger.addHandler(rfh)
            self.log = dispatcher_logger

    def record_metering_data(self, data):
        if self.log:
            self.log.info(data)

    def record_events(self, events):
        if self.log:
            self.log.info(events)
        return []

########NEW FILE########
__FILENAME__ = kwapi
# -*- coding: utf-8 -*-
#
# Author: François Rossigneux <francois.rossigneux@inria.fr>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import datetime

from keystoneclient import exceptions
from oslo.config import cfg
import requests

from ceilometer.central import plugin
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer import sample

LOG = log.getLogger(__name__)


class KwapiClient(object):
    """Kwapi API client."""

    def __init__(self, url, token=None):
        """Initializes client."""
        self.url = url
        self.token = token

    def iter_probes(self):
        """Returns a list of dicts describing all probes."""
        probes_url = self.url + '/probes/'
        headers = {}
        if self.token is not None:
            headers = {'X-Auth-Token': self.token}
        request = requests.get(probes_url, headers=headers)
        message = request.json()
        probes = message['probes']
        for key, value in probes.iteritems():
            probe_dict = value
            probe_dict['id'] = key
            yield probe_dict


class _Base(plugin.CentralPollster):
    """Base class for the Kwapi pollster, derived from CentralPollster."""

    @staticmethod
    def get_kwapi_client(ksclient):
        """Returns a KwapiClient configured with the proper url and token."""
        endpoint = ksclient.service_catalog.url_for(
            service_type='energy',
            endpoint_type=cfg.CONF.service_credentials.os_endpoint_type)
        return KwapiClient(endpoint, ksclient.auth_token)

    CACHE_KEY_PROBE = 'kwapi.probes'

    def _iter_probes(self, ksclient, cache):
        """Iterate over all probes."""
        if self.CACHE_KEY_PROBE not in cache:
            cache[self.CACHE_KEY_PROBE] = self._get_probes(ksclient)
        return iter(cache[self.CACHE_KEY_PROBE])

    def _get_probes(self, ksclient):
        try:
            client = self.get_kwapi_client(ksclient)
        except exceptions.EndpointNotFound:
            LOG.debug(_("Kwapi endpoint not found"))
            return []
        return list(client.iter_probes())


class EnergyPollster(_Base):
    """Measures energy consumption."""

    def get_samples(self, manager, cache, resources=None):
        """Returns all samples."""
        for probe in self._iter_probes(manager.keystone, cache):
            yield sample.Sample(
                name='energy',
                type=sample.TYPE_CUMULATIVE,
                unit='kWh',
                volume=probe['kwh'],
                user_id=None,
                project_id=None,
                resource_id=probe['id'],
                timestamp=datetime.datetime.fromtimestamp(
                    probe['timestamp']).isoformat(),
                resource_metadata={}
            )


class PowerPollster(_Base):
    """Measures power consumption."""

    def get_samples(self, manager, cache, resources=None):
        """Returns all samples."""
        for probe in self._iter_probes(manager.keystone, cache):
            yield sample.Sample(
                name='power',
                type=sample.TYPE_GAUGE,
                unit='W',
                volume=probe['w'],
                user_id=None,
                project_id=None,
                resource_id=probe['id'],
                timestamp=datetime.datetime.fromtimestamp(
                    probe['timestamp']).isoformat(),
                resource_metadata={}
            )

########NEW FILE########
__FILENAME__ = converter
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Rackspace Hosting.
#
# Author: Monsyne Dragon <mdragon@rackspace.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import fnmatch
import os

import jsonpath_rw
from oslo.config import cfg
import six
import yaml

from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer.openstack.common import timeutils
from ceilometer.storage import models

OPTS = [
    cfg.StrOpt('definitions_cfg_file',
               default="event_definitions.yaml",
               help="Configuration file for event definitions."
               ),
    cfg.BoolOpt('drop_unmatched_notifications',
                default=False,
                help='Drop notifications if no event definition matches. '
                '(Otherwise, we convert them with just the default traits)'),

]

cfg.CONF.register_opts(OPTS, group='event')

LOG = log.getLogger(__name__)


class EventDefinitionException(Exception):
    def __init__(self, message, definition_cfg):
        super(EventDefinitionException, self).__init__(message)
        self.definition_cfg = definition_cfg

    def __str__(self):
        return '%s %s: %s' % (self.__class__.__name__,
                              self.definition_cfg, self.message)


class TraitDefinition(object):

    def __init__(self, name, trait_cfg, plugin_manager):
        self.cfg = trait_cfg
        self.name = name

        type_name = trait_cfg.get('type', 'text')

        if 'plugin' in trait_cfg:
            plugin_cfg = trait_cfg['plugin']
            if isinstance(plugin_cfg, six.string_types):
                plugin_name = plugin_cfg
                plugin_params = {}
            else:
                try:
                    plugin_name = plugin_cfg['name']
                except KeyError:
                    raise EventDefinitionException(
                        _('Plugin specified, but no plugin name supplied for '
                          'trait %s') % name, self.cfg)
                plugin_params = plugin_cfg.get('parameters')
                if plugin_params is None:
                    plugin_params = {}
            try:
                plugin_ext = plugin_manager[plugin_name]
            except KeyError:
                raise EventDefinitionException(
                    _('No plugin named %(plugin)s available for '
                      'trait %(trait)s') % dict(plugin=plugin_name,
                                                trait=name), self.cfg)
            plugin_class = plugin_ext.plugin
            self.plugin = plugin_class(**plugin_params)
        else:
            self.plugin = None

        if 'fields' not in trait_cfg:
            raise EventDefinitionException(
                _("Required field in trait definition not specified: "
                  "'%s'") % 'fields',
                self.cfg)

        fields = trait_cfg['fields']
        if not isinstance(fields, six.string_types):
            # NOTE(mdragon): if not a string, we assume a list.
            if len(fields) == 1:
                fields = fields[0]
            else:
                fields = '|'.join('(%s)' % path for path in fields)
        try:
            self.fields = jsonpath_rw.parse(fields)
        except Exception as e:
            raise EventDefinitionException(
                _("Parse error in JSONPath specification "
                  "'%(jsonpath)s' for %(trait)s: %(err)s")
                % dict(jsonpath=fields, trait=name, err=e), self.cfg)
        self.trait_type = models.Trait.get_type_by_name(type_name)
        if self.trait_type is None:
            raise EventDefinitionException(
                _("Invalid trait type '%(type)s' for trait %(trait)s")
                % dict(type=type_name, trait=name), self.cfg)

    def _get_path(self, match):
        if match.context is not None:
            for path_element in self._get_path(match.context):
                yield path_element
            yield str(match.path)

    def to_trait(self, notification_body):
        values = [match for match in self.fields.find(notification_body)
                  if match.value is not None]

        if self.plugin is not None:
            value_map = [('.'.join(self._get_path(match)), match.value) for
                         match in values]
            value = self.plugin.trait_value(value_map)
        else:
            value = values[0].value if values else None

        if value is None:
            return None

        # NOTE(mdragon): some openstack projects (mostly Nova) emit ''
        # for null fields for things like dates.
        if self.trait_type != models.Trait.TEXT_TYPE and value == '':
            return None

        value = models.Trait.convert_value(self.trait_type, value)
        return models.Trait(self.name, self.trait_type, value)


class EventDefinition(object):

    DEFAULT_TRAITS = dict(
        service=dict(type='text', fields='publisher_id'),
        request_id=dict(type='text', fields='_context_request_id'),
        tenant_id=dict(type='text', fields=['payload.tenant_id',
                                            '_context_tenant']),
    )

    def __init__(self, definition_cfg, trait_plugin_mgr):
        self._included_types = []
        self._excluded_types = []
        self.traits = dict()
        self.cfg = definition_cfg

        try:
            event_type = definition_cfg['event_type']
            traits = definition_cfg['traits']
        except KeyError as err:
            raise EventDefinitionException(
                _("Required field %s not specified") % err.args[0], self.cfg)

        if isinstance(event_type, six.string_types):
            event_type = [event_type]

        for t in event_type:
            if t.startswith('!'):
                self._excluded_types.append(t[1:])
            else:
                self._included_types.append(t)

        if self._excluded_types and not self._included_types:
            self._included_types.append('*')

        for trait_name in self.DEFAULT_TRAITS:
            self.traits[trait_name] = TraitDefinition(
                trait_name,
                self.DEFAULT_TRAITS[trait_name],
                trait_plugin_mgr)
        for trait_name in traits:
            self.traits[trait_name] = TraitDefinition(
                trait_name,
                traits[trait_name],
                trait_plugin_mgr)

    def included_type(self, event_type):
        for t in self._included_types:
            if fnmatch.fnmatch(event_type, t):
                return True
        return False

    def excluded_type(self, event_type):
        for t in self._excluded_types:
            if fnmatch.fnmatch(event_type, t):
                return True
        return False

    def match_type(self, event_type):
        return (self.included_type(event_type)
                and not self.excluded_type(event_type))

    @property
    def is_catchall(self):
        return '*' in self._included_types and not self._excluded_types

    @staticmethod
    def _extract_when(body):
        """Extract the generated datetime from the notification.
        """
        # NOTE: I am keeping the logic the same as it was in the collector,
        # However, *ALL* notifications should have a 'timestamp' field, it's
        # part of the notification envelope spec. If this was put here because
        # some openstack project is generating notifications without a
        # timestamp, then that needs to be filed as a bug with the offending
        # project (mdragon)
        when = body.get('timestamp', body.get('_context_timestamp'))
        if when:
            return timeutils.normalize_time(timeutils.parse_isotime(when))

        return timeutils.utcnow()

    def to_event(self, notification_body):
        event_type = notification_body['event_type']
        message_id = notification_body['message_id']
        when = self._extract_when(notification_body)

        traits = (self.traits[t].to_trait(notification_body)
                  for t in self.traits)
        # Only accept non-None value traits ...
        traits = [trait for trait in traits if trait is not None]
        event = models.Event(message_id, event_type, when, traits)
        return event


class NotificationEventsConverter(object):
    """Notification Event Converter

    The NotificationEventsConverter handles the conversion of Notifications
    from openstack systems into Ceilometer Events.

    The conversion is handled according to event definitions in a config file.

    The config is a list of event definitions. Order is significant, a
    notification will be processed according to the LAST definition that
    matches it's event_type. (We use the last matching definition because that
    allows you to use YAML merge syntax in the definitions file.)
    Each definition is a dictionary with the following keys (all are required):
        event_type: this is a list of notification event_types this definition
                    will handle. These can be wildcarded with unix shell glob
                    (not regex!) wildcards.
                    An exclusion listing (starting with a '!') will exclude any
                    types listed from matching. If ONLY exclusions are listed,
                    the definition will match anything not matching the
                    exclusions.
                    This item can also be a string, which will be taken as
                    equivalent to 1 item list.

                    Examples:
                    *   ['compute.instance.exists'] will only match
                            compute.intance.exists notifications
                    *   "compute.instance.exists"   Same as above.
                    *   ["image.create", "image.delete"]  will match
                         image.create and image.delete, but not anything else.
                    *   'compute.instance.*" will match
                        compute.instance.create.start but not image.upload
                    *   ['*.start','*.end', '!scheduler.*'] will match
                        compute.instance.create.start, and image.delete.end,
                        but NOT compute.instance.exists or
                        scheduler.run_instance.start
                    *   '!image.*' matches any notification except image
                        notifications.
                    *   ['*', '!image.*']  same as above.
        traits:  dictionary, The keys are trait names, the values are the trait
                 definitions
            Each trait definition is a dictionary with the following keys:
                type (optional): The data type for this trait. (as a string)
                    Valid options are: 'text', 'int', 'float' and 'datetime'
                    defaults to 'text' if not specified.
                fields:  a path specification for the field(s) in the
                    notification you wish to extract. The paths can be
                    specified with a dot syntax (e.g. 'payload.host').
                    dictionary syntax (e.g. 'payload[host]') is also supported.
                    in either case, if the key for the field you are looking
                    for contains special charecters, like '.', it will need to
                    be quoted (with double or single quotes) like so:

                          "payload.image_meta.'org.openstack__1__architecture'"

                    The syntax used for the field specification is a variant
                    of JSONPath, and is fairly flexible.
                    (see: https://github.com/kennknowles/python-jsonpath-rw
                    for more info)  Specifications can be written to match
                    multiple possible fields, the value for the trait will
                    be derived from the matching fields that exist and have
                    a non-null (i.e. is not None) values in the notification.
                    By default the value will be the first such field.
                    (plugins can alter that, if they wish)

                    This configuration value is normally a string, for
                    convenience, it can be specified as a list of
                    specifications, which will be OR'ed together (a union
                    query in jsonpath terms)
                plugin (optional): (dictionary) with the following keys:
                    name: (string) name of a plugin to load
                    parameters: (optional) Dictionary of keyword args to pass
                                to the plugin on initialization.
                                See documentation on each plugin to see what
                                arguments it accepts.
                    For convenience, this value can also be specified as a
                    string, which is interpreted as a plugin name, which will
                    be loaded with no parameters.

    """

    def __init__(self, events_config, trait_plugin_mgr, add_catchall=True):
        self.definitions = [
            EventDefinition(event_def, trait_plugin_mgr)
            for event_def in reversed(events_config)]
        if add_catchall and not any(d.is_catchall for d in self.definitions):
            event_def = dict(event_type='*', traits={})
            self.definitions.append(EventDefinition(event_def,
                                                    trait_plugin_mgr))

    def to_event(self, notification_body):
        event_type = notification_body['event_type']
        message_id = notification_body['message_id']
        edef = None
        for d in self.definitions:
            if d.match_type(event_type):
                edef = d
                break

        if edef is None:
            msg = (_('Dropping Notification %(type)s (uuid:%(msgid)s)')
                   % dict(type=event_type, msgid=message_id))
            if cfg.CONF.event.drop_unmatched_notifications:
                LOG.debug(msg)
            else:
                # If drop_unmatched_notifications is False, this should
                # never happen. (mdragon)
                LOG.error(msg)
            return None

        return edef.to_event(notification_body)


def get_config_file():
    config_file = cfg.CONF.event.definitions_cfg_file
    if not os.path.exists(config_file):
        config_file = cfg.CONF.find_file(config_file)
    return config_file


def setup_events(trait_plugin_mgr):
    """Setup the event definitions from yaml config file."""
    config_file = get_config_file()
    if config_file is not None:
        LOG.debug(_("Event Definitions configuration file: %s"), config_file)

        with open(config_file) as cf:
            config = cf.read()

        try:
            events_config = yaml.safe_load(config)
        except yaml.YAMLError as err:
            if hasattr(err, 'problem_mark'):
                mark = err.problem_mark
                errmsg = (_("Invalid YAML syntax in Event Definitions file "
                            "%(file)s at line: %(line)s, column: %(column)s.")
                          % dict(file=config_file,
                                 line=mark.line + 1,
                                 column=mark.column + 1))
            else:
                errmsg = (_("YAML error reading Event Definitions file "
                            "%(file)s")
                          % dict(file=config_file))
            LOG.error(errmsg)
            raise

    else:
        LOG.debug(_("No Event Definitions configuration file found!"
                  " Using default config."))
        events_config = []

    LOG.info(_("Event Definitions: %s"), events_config)

    allow_drop = cfg.CONF.event.drop_unmatched_notifications
    return NotificationEventsConverter(events_config,
                                       trait_plugin_mgr,
                                       add_catchall=not allow_drop)

########NEW FILE########
__FILENAME__ = endpoint
# -*- encoding: utf-8 -*-
#
# Copyright © 2012-2014 eNovance <licensing@enovance.com>
#
# Author: Mehdi Abaakouk <mehdi.abaakouk@enovance.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import logging

from oslo.config import cfg
import oslo.messaging
from stevedore import extension

from ceilometer import dispatcher
from ceilometer.event import converter as event_converter
from ceilometer import messaging
from ceilometer.openstack.common.gettextutils import _
from ceilometer.storage import models

LOG = logging.getLogger(__name__)


class EventsNotificationEndpoint(object):
    def __init__(self):
        super(EventsNotificationEndpoint, self).__init__()
        self.dispatcher_manager = dispatcher.load_dispatcher_manager()
        LOG.debug(_('Loading event definitions'))
        self.event_converter = event_converter.setup_events(
            extension.ExtensionManager(
                namespace='ceilometer.event.trait_plugin'))

    def info(self, ctxt, publisher_id, event_type, payload, metadata):
        """Convert message to Ceilometer Event.

        :param ctxt: oslo.messaging context
        :param publisher_id: publisher of the notification
        :param event_type: type of notification
        :param payload: notification payload
        :param metadata: metadata about the notification
        """

        # NOTE: the rpc layer currently rips out the notification
        # delivery_info, which is critical to determining the
        # source of the notification. This will have to get added back later.
        notification = messaging.convert_to_old_notification_format(
            'info', ctxt, publisher_id, event_type, payload, metadata)
        self.process_notification(notification)

    def process_notification(self, notification):
        event = self.event_converter.to_event(notification)

        if event is not None:
            LOG.debug(_('Saving event "%s"'), event.event_type)
            problem_events = []
            for dispatcher_ext in self.dispatcher_manager:
                problem_events.extend(dispatcher_ext.obj.record_events(event))
            if models.Event.UNKNOWN_PROBLEM in [x[0] for x in problem_events]:
                if not cfg.CONF.notification.ack_on_event_error:
                    return oslo.messaging.NotificationResult.REQUEUE
        return oslo.messaging.NotificationResult.HANDLED

########NEW FILE########
__FILENAME__ = trait_plugins
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Rackspace Hosting.
#
# Author: Monsyne Dragon <mdragon@rackspace.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import abc
import six


@six.add_metaclass(abc.ABCMeta)
class TraitPluginBase(object):
    """Base class for plugins that convert notification fields to
       Trait values.
    """

    def __init__(self, **kw):
        """Setup the trait plugin.

        For each Trait definition a plugin is used on in a conversion
        definition, a new instance of the plugin will be created, and
        initialized with the parameters (if any) specified in the
        config file.

        :param kw: the parameters specified in the event definitions file.

        """
        super(TraitPluginBase, self).__init__()

    @abc.abstractmethod
    def trait_value(self, match_list):
        """Convert a set of fields to a Trait value.

        This method is called each time a trait is attempted to be extracted
        from a notification. It will be called *even if* no matching fields
        are found in the notification (in that case, the match_list will be
        empty). If this method returns None, the trait *will not* be added to
        the event. Any other value returned by this method will be used as
        the value for the trait. Values returned will be coerced to the
        appropriate type for the trait.

        :param match_list: A list (may be empty if no matches) of *tuples*.
                          Each tuple is (field_path, value) where field_path
                          is the jsonpath for that specific field,

        Example:
            trait's fields definition: ['payload.foobar',
                                        'payload.baz',
                                        'payload.thing.*']
            notification body:
                        {
                            'message_id': '12345',
                            'publisher':    'someservice.host',
                            'payload':  {
                                            'foobar': 'test',
                                            'thing':    {
                                                            'bar': 12,
                                                            'boing': 13,
                                                        }
                                        }
                        }
            match_list will be: [('payload.foobar','test'),
                                 ('payload.thing.bar',12),
                                 ('payload.thing.boing',13)]

        Here is a plugin that emulates the default (no plugin) behavior:

        class DefaultPlugin(TraitPluginBase):
            "Plugin that returns the first field value"

            def __init__(self, **kw):
                super(DefaultPlugin, self).__init__()

            def trait_value(self, match_list):
                if not match_list:
                    return None
                return match_list[0][1]
        """


class SplitterTraitPlugin(TraitPluginBase):
    """Plugin that splits a piece off of a string value."""

    def __init__(self, separator=".", segment=0, max_split=None, **kw):
        """Setup how do split the field.

        :param  separator:  String to split on. default "."
        :param  segment:    Which segment to return. (int) default 0
        :param  max_split: Limit number of splits. Default: None (no limit)
        """
        self.separator = separator
        self.segment = segment
        self.max_split = max_split
        super(SplitterTraitPlugin, self).__init__(**kw)

    def trait_value(self, match_list):
        if not match_list:
            return None
        value = str(match_list[0][1])
        if self.max_split is not None:
            values = value.split(self.separator, self.max_split)
        else:
            values = value.split(self.separator)
        try:
            return values[self.segment]
        except IndexError:
            return None


class BitfieldTraitPlugin(TraitPluginBase):
    """Plugin to set flags on a bitfield."""
    def __init__(self, initial_bitfield=0, flags=None, **kw):
        """Setup bitfield trait.

        :param initial_bitfield: (int) initial value for the bitfield
                                 Flags that are set will be OR'ed with this.
        :param flags: List of dictionaries defining bitflags to set depending
                      on data in the notification. Each one has the following
                      keys:
                            path: jsonpath of field to match.
                            bit: (int) number of bit to set (lsb is bit 0)
                            value: set bit if corresponding field's value
                                   matches this. If value is not provided,
                                   bit will be set if the field exists (and
                                   is non-null), regardless of it's value.

        """
        self.initial_bitfield = initial_bitfield
        if flags is None:
            flags = []
        self.flags = flags
        super(BitfieldTraitPlugin, self).__init__(**kw)

    def trait_value(self, match_list):
        matches = dict(match_list)
        bitfield = self.initial_bitfield
        for flagdef in self.flags:
            path = flagdef['path']
            bit = 2 ** int(flagdef['bit'])
            if path in matches:
                if 'value' in flagdef:
                    if matches[path] == flagdef['value']:
                        bitfield |= bit
                else:
                    bitfield |= bit
        return bitfield

########NEW FILE########
__FILENAME__ = base
# -*- encoding: utf-8 -*-
#
# Copyright © 2014 ZHAW SoE
#
# Authors: Lucas Graf <graflu0@students.zhaw.ch>
#          Toni Zehnder <zehndton@students.zhaw.ch>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Inspector abstraction for read-only access to hardware components"""

import abc
import collections

import six

# Named tuple representing CPU statistics.
#
# cpu1MinLoad: 1 minute load
# cpu5MinLoad: 5 minute load
# cpu15MinLoad: 15 minute load
#
CPUStats = collections.namedtuple(
    'CPUStats',
    ['cpu_1_min', 'cpu_5_min', 'cpu_15_min'])

# Named tuple representing RAM statistics.
#
# total: Total Memory (bytes)
# used: Used Memory (bytes)
#
MemoryStats = collections.namedtuple('MemoryStats', ['total', 'used'])

# Named tuple representing disks.
#
# device: the device name for the disk
# path: the path from the disk
#
Disk = collections.namedtuple('Disk', ['device', 'path'])

# Named tuple representing disk statistics.
#
# size: storage size (bytes)
# used: storage used (bytes)
#
DiskStats = collections.namedtuple('DiskStats', ['size', 'used'])


# Named tuple representing an interface.
#
# name: the name of the interface
# mac: the MAC of the interface
# ip: the IP of the interface
#
Interface = collections.namedtuple('Interface', ['name', 'mac', 'ip'])


# Named tuple representing network interface statistics.
#
# bandwidth: current bandwidth (bytes/s)
# rx_bytes: total number of octets received (bytes)
# tx_bytes: total number of octets transmitted (bytes)
# error: number of outbound packets not transmitted because of errors
#
InterfaceStats = collections.namedtuple(
    'InterfaceStats',
    ['bandwidth', 'rx_bytes', 'tx_bytes', 'error'])


@six.add_metaclass(abc.ABCMeta)
class Inspector(object):
    @abc.abstractmethod
    def inspect_cpu(self, host):
        """Inspect the CPU statistics for a host.

        :param host: the target host
        :return: iterator of CPUStats
        """

    @abc.abstractmethod
    def inspect_disk(self, host):
        """Inspect the disk statistics for a host.

        :param : the target host
        :return: iterator of tuple (Disk, DiskStats)
        """

    @abc.abstractmethod
    def inspect_memory(self, host):
        """Inspect the ram statistics for a host.

        :param : the target host
        :return: iterator of MemoryStats
        """

    @abc.abstractmethod
    def inspect_network(self, host):
        """Inspect the network interfaces for a host.

        :param : the target host
        :return: iterator of tuple (Interface, InterfaceStats)
        """

########NEW FILE########
__FILENAME__ = snmp
# -*- encoding: utf-8 -*-
#
# Copyright © 2014 ZHAW SoE
#
# Authors: Lucas Graf <graflu0@students.zhaw.ch>
#          Toni Zehnder <zehndton@students.zhaw.ch>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Inspector for collecting data over SNMP"""

import urlparse

from ceilometer.hardware.inspector import base
from pysnmp.entity.rfc3413.oneliner import cmdgen


class SNMPException(Exception):
    pass


def parse_snmp_return(ret):
    """Check the return value of snmp operations

    :param ret: a tuple of (errorIndication, errorStatus, errorIndex, data)
                returned by pysnmp
    :return: a tuple of (err, data)
             err: True if error found, or False if no error found
             data: a string of error description if error found, or the
                   actual return data of the snmp operation
    """
    err = True
    (errIndication, errStatus, errIdx, varBinds) = ret
    if errIndication:
        data = errIndication
    elif errStatus:
        data = "%s at %s" % (errStatus.prettyPrint(),
                             errIdx and varBinds[int(errIdx) - 1] or "?")
    else:
        err = False
        data = varBinds
    return (err, data)


class SNMPInspector(base.Inspector):
    #CPU OIDs
    _cpu_1_min_load_oid = "1.3.6.1.4.1.2021.10.1.3.1"
    _cpu_5_min_load_oid = "1.3.6.1.4.1.2021.10.1.3.2"
    _cpu_15_min_load_oid = "1.3.6.1.4.1.2021.10.1.3.3"
    #Memory OIDs
    _memory_total_oid = "1.3.6.1.4.1.2021.4.5.0"
    _memory_used_oid = "1.3.6.1.4.1.2021.4.6.0"
    #Disk OIDs
    _disk_index_oid = "1.3.6.1.4.1.2021.9.1.1"
    _disk_path_oid = "1.3.6.1.4.1.2021.9.1.2"
    _disk_device_oid = "1.3.6.1.4.1.2021.9.1.3"
    _disk_size_oid = "1.3.6.1.4.1.2021.9.1.6"
    _disk_used_oid = "1.3.6.1.4.1.2021.9.1.8"
    #Network Interface OIDs
    _interface_index_oid = "1.3.6.1.2.1.2.2.1.1"
    _interface_name_oid = "1.3.6.1.2.1.2.2.1.2"
    _interface_bandwidth_oid = "1.3.6.1.2.1.2.2.1.5"
    _interface_mac_oid = "1.3.6.1.2.1.2.2.1.6"
    _interface_ip_oid = "1.3.6.1.2.1.4.20.1.2"
    _interface_received_oid = "1.3.6.1.2.1.2.2.1.10"
    _interface_transmitted_oid = "1.3.6.1.2.1.2.2.1.16"
    _interface_error_oid = "1.3.6.1.2.1.2.2.1.20"
    #Default port and security name
    _port = 161
    _security_name = 'public'

    def __init__(self):
        super(SNMPInspector, self).__init__()
        self._cmdGen = cmdgen.CommandGenerator()

    def _get_or_walk_oid(self, oid, host, get=True):
        if get:
            func = self._cmdGen.getCmd
            ret_func = lambda x: x[0][1]
        else:
            func = self._cmdGen.nextCmd
            ret_func = lambda x: x
        ret = func(cmdgen.CommunityData(self._get_security_name(host)),
                   cmdgen.UdpTransportTarget((host.hostname,
                                              host.port or self._port)),
                   oid)
        (error, data) = parse_snmp_return(ret)
        if error:
            raise SNMPException("An error occurred, oid %(oid)s, "
                                "host %(host)s, %(err)s" % dict(oid=oid,
                                host=host.hostname, err=data))
        else:
            return ret_func(data)

    def _get_value_from_oid(self, oid, host):
        return self._get_or_walk_oid(oid, host, True)

    def _walk_oid(self, oid, host):
        return self._get_or_walk_oid(oid, host, False)

    def inspect_cpu(self, host):
        #get 1 minute load
        cpu_1_min_load = \
            str(self._get_value_from_oid(self._cpu_1_min_load_oid, host))
        #get 5 minute load
        cpu_5_min_load = \
            str(self._get_value_from_oid(self._cpu_5_min_load_oid, host))
        #get 15 minute load
        cpu_15_min_load = \
            str(self._get_value_from_oid(self._cpu_15_min_load_oid, host))

        yield base.CPUStats(cpu_1_min=float(cpu_1_min_load),
                            cpu_5_min=float(cpu_5_min_load),
                            cpu_15_min=float(cpu_15_min_load))

    def inspect_memory(self, host):
        #get total memory
        total = self._get_value_from_oid(self._memory_total_oid, host)
        #get used memory
        used = self._get_value_from_oid(self._memory_used_oid, host)

        yield base.MemoryStats(total=int(total), used=int(used))

    def inspect_disk(self, host):
        disks = self._walk_oid(self._disk_index_oid, host)

        for disk in disks:
            for object_name, value in disk:
                path_oid = "%s.%s" % (self._disk_path_oid, str(value))
                path = self._get_value_from_oid(path_oid, host)
                device_oid = "%s.%s" % (self._disk_device_oid, str(value))
                device = self._get_value_from_oid(device_oid, host)
                size_oid = "%s.%s" % (self._disk_size_oid, str(value))
                size = self._get_value_from_oid(size_oid, host)
                used_oid = "%s.%s" % (self._disk_used_oid, str(value))
                used = self._get_value_from_oid(used_oid, host)

                disk = base.Disk(device=str(device),
                                 path=str(path))
                stats = base.DiskStats(size=int(size),
                                       used=int(used))

                yield (disk, stats)

    def inspect_network(self, host):
        net_interfaces = self._walk_oid(self._interface_index_oid, host)

        for interface in net_interfaces:
            for object_name, value in interface:
                ip = self._get_ip_for_interface(host, value)
                name_oid = "%s.%s" % (self._interface_name_oid,
                                      str(value))
                name = self._get_value_from_oid(name_oid, host)
                mac_oid = "%s.%s" % (self._interface_mac_oid,
                                     str(value))
                mac = self._get_value_from_oid(mac_oid, host)
                bw_oid = "%s.%s" % (self._interface_bandwidth_oid,
                                    str(value))
                # bits/s to byte/s
                bandwidth = self._get_value_from_oid(bw_oid, host) / 8
                rx_oid = "%s.%s" % (self._interface_received_oid,
                                    str(value))
                rx_bytes = self._get_value_from_oid(rx_oid, host)
                tx_oid = "%s.%s" % (self._interface_transmitted_oid,
                                    str(value))
                tx_bytes = self._get_value_from_oid(tx_oid, host)
                error_oid = "%s.%s" % (self._interface_error_oid,
                                       str(value))
                error = self._get_value_from_oid(error_oid, host)

                adapted_mac = mac.prettyPrint().replace('0x', '')
                interface = base.Interface(name=str(name),
                                           mac=adapted_mac,
                                           ip=str(ip))
                stats = base.InterfaceStats(bandwidth=int(bandwidth),
                                            rx_bytes=int(rx_bytes),
                                            tx_bytes=int(tx_bytes),
                                            error=int(error))
                yield (interface, stats)

    def _get_security_name(self, host):
        options = urlparse.parse_qs(host.query)
        return options.get('security_name', [self._security_name])[-1]

    def _get_ip_for_interface(self, host, interface_id):
        ip_addresses = self._walk_oid(self._interface_ip_oid, host)
        for ip in ip_addresses:
            for name, value in ip:
                if value == interface_id:
                    return str(name).replace(self._interface_ip_oid + ".", "")

########NEW FILE########
__FILENAME__ = plugin
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 ZHAW SoE
# Copyright © 2014 Intel Corp.
#
# Authors: Lucas Graf <graflu0@students.zhaw.ch>
#          Toni Zehnder <zehndton@students.zhaw.ch>
#          Lianhao Lu <lianhao.lu@intel.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Base class for plugins used by the hardware agent."""

import abc
import itertools
import six

from ceilometer.central import plugin
from ceilometer.hardware import inspector as insloader
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer.openstack.common import network_utils

LOG = log.getLogger(__name__)


@six.add_metaclass(abc.ABCMeta)
class HardwarePollster(plugin.CentralPollster):
    """Base class for plugins that support the polling API."""

    CACHE_KEY = None
    INSPECT_METHOD = None

    def __init__(self):
        super(HardwarePollster, self).__init__()
        self.inspectors = {}

    def get_samples(self, manager, cache, resources=None):
        """Return an iterable of Sample instances from polling the resources.

        :param manager: The service manager invoking the plugin
        :param cache: A dictionary for passing data between plugins
        :param resources: end point to poll data from
        """
        resources = resources or []
        h_cache = cache.setdefault(self.CACHE_KEY, {})
        sample_iters = []
        for res in resources:
            parsed_url = network_utils.urlsplit(res)
            inspector = self._get_inspector(parsed_url)
            func = getattr(inspector, self.INSPECT_METHOD)

            try:
                # Call hardware inspector to poll for the data
                i_cache = h_cache.setdefault(res, {})
                if res not in i_cache:
                    i_cache[res] = list(func(parsed_url))
                # Generate samples
                if i_cache[res]:
                    sample_iters.append(self.generate_samples(parsed_url,
                                                              i_cache[res]))
            except Exception as err:
                LOG.exception(_('inspector call %(func)r failed for '
                                'host %(host)s: %(err)s'),
                              dict(func=func,
                                   host=parsed_url.hostname,
                                   err=err))
        return itertools.chain(*sample_iters)

    def generate_samples(self, host_url, data):
        """Generate an iterable Sample from the data returned by inspector

        :param host_url: host url of the endpoint
        :param data: list of data returned by the corresponding inspector

        """
        return (self.generate_one_sample(host_url, datum) for datum in data)

    @abc.abstractmethod
    def generate_one_sample(self, host_url, c_data):
        """Return one Sample.

        :param host_url: host url of the endpoint
        :param c_data: data returned by the corresponding inspector, of
                       one of the types defined in the file
                       ceilometer.hardware.inspector.base.CPUStats
        """

    def _get_inspector(self, parsed_url):
        if parsed_url.scheme not in self.inspectors:
            try:
                driver = insloader.get_inspector(parsed_url)
                self.inspectors[parsed_url.scheme] = driver
            except Exception as err:
                LOG.exception(_("Can NOT load inspector %(name)s: %(err)s"),
                              dict(name=parsed_url.scheme,
                                   err=err))
                raise err
        return self.inspectors[parsed_url.scheme]

########NEW FILE########
__FILENAME__ = cpu
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 ZHAW SoE
# Copyright © 2014 Intel Corp.
#
# Authors: Lucas Graf <graflu0@students.zhaw.ch>
#          Toni Zehnder <zehndton@students.zhaw.ch>
#          Lianhao Lu <lianhao.lu@intel.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ceilometer.hardware import plugin
from ceilometer.hardware.pollsters import util
from ceilometer import sample


class _Base(plugin.HardwarePollster):

    CACHE_KEY = 'cpu'
    INSPECT_METHOD = 'inspect_cpu'


class CPULoad1MinPollster(_Base):

    @staticmethod
    def generate_one_sample(host, c_data):
        return util.make_sample_from_host(host,
                                          name='cpu.load.1min',
                                          type=sample.TYPE_GAUGE,
                                          unit='process',
                                          volume=c_data.cpu_1_min,
                                          )


class CPULoad5MinPollster(_Base):

    @staticmethod
    def generate_one_sample(host, c_data):
        return util.make_sample_from_host(host,
                                          name='cpu.load.5min',
                                          type=sample.TYPE_GAUGE,
                                          unit='process',
                                          volume=c_data.cpu_5_min,
                                          )


class CPULoad15MinPollster(_Base):

    @staticmethod
    def generate_one_sample(host, c_data):
        return util.make_sample_from_host(host,
                                          name='cpu.load.15min',
                                          type=sample.TYPE_GAUGE,
                                          unit='process',
                                          volume=c_data.cpu_15_min,
                                          )

########NEW FILE########
__FILENAME__ = disk
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 ZHAW SoE
# Copyright © 2014 Intel Corp.
#
# Authors: Lucas Graf <graflu0@students.zhaw.ch>
#          Toni Zehnder <zehndton@students.zhaw.ch>
#          Lianhao Lu <lianhao.lu@intel.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ceilometer.hardware import plugin
from ceilometer.hardware.pollsters import util
from ceilometer import sample


class _Base(plugin.HardwarePollster):

    CACHE_KEY = 'disk'
    INSPECT_METHOD = 'inspect_disk'


class DiskTotalPollster(_Base):

    @staticmethod
    def generate_one_sample(host, c_data):
        (disk, info) = c_data
        return util.make_sample_from_host(host,
                                          name='disk.size.total',
                                          type=sample.TYPE_GAUGE,
                                          unit='B',
                                          volume=info.size,
                                          res_metadata=disk,
                                          )


class DiskUsedPollster(_Base):

    @staticmethod
    def generate_one_sample(host, c_data):
        (disk, info) = c_data
        return util.make_sample_from_host(host,
                                          name='disk.size.used',
                                          type=sample.TYPE_GAUGE,
                                          unit='B',
                                          volume=info.used,
                                          res_metadata=disk,
                                          )

########NEW FILE########
__FILENAME__ = memory
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 ZHAW SoE
# Copyright © 2014 Intel Corp.
#
# Authors: Lucas Graf <graflu0@students.zhaw.ch>
#          Toni Zehnder <zehndton@students.zhaw.ch>
#          Lianhao Lu <lianhao.lu@intel.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ceilometer.hardware import plugin
from ceilometer.hardware.pollsters import util
from ceilometer import sample


class _Base(plugin.HardwarePollster):

    CACHE_KEY = 'memory'
    INSPECT_METHOD = 'inspect_memory'


class MemoryTotalPollster(_Base):

    @staticmethod
    def generate_one_sample(host, c_data):
        return util.make_sample_from_host(host,
                                          name='memory.total',
                                          type=sample.TYPE_GAUGE,
                                          unit='B',
                                          volume=c_data.total,
                                          )


class MemoryUsedPollster(_Base):

    @staticmethod
    def generate_one_sample(host, c_data):
        return util.make_sample_from_host(host,
                                          name='memory.used',
                                          type=sample.TYPE_GAUGE,
                                          unit='B',
                                          volume=c_data.used,
                                          )

########NEW FILE########
__FILENAME__ = net
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 ZHAW SoE
# Copyright © 2014 Intel Corp.
#
# Authors: Lucas Graf <graflu0@students.zhaw.ch>
#          Toni Zehnder <zehndton@students.zhaw.ch>
#          Lianhao Lu <lianhao.lu@intel.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ceilometer.hardware import plugin
from ceilometer.hardware.pollsters import util
from ceilometer import sample


class _Base(plugin.HardwarePollster):

    CACHE_KEY = 'nic'
    INSPECT_METHOD = 'inspect_network'


class BandwidthBytesPollster(_Base):

    @staticmethod
    def generate_one_sample(host, c_data):
        (nic, info) = c_data
        return util.make_sample_from_host(host,
                                          name='network.bandwidth.bytes',
                                          type=sample.TYPE_CUMULATIVE,
                                          unit='B',
                                          volume=info.bandwidth,
                                          res_metadata=nic,
                                          )


class IncomingBytesPollster(_Base):

    @staticmethod
    def generate_one_sample(host, c_data):
        (nic, info) = c_data
        return util.make_sample_from_host(host,
                                          name='network.incoming.bytes',
                                          type=sample.TYPE_CUMULATIVE,
                                          unit='B',
                                          volume=info.rx_bytes,
                                          res_metadata=nic,
                                          )


class OutgoingBytesPollster(_Base):

    @staticmethod
    def generate_one_sample(host, c_data):
        (nic, info) = c_data
        return util.make_sample_from_host(host,
                                          name='network.outgoing.bytes',
                                          type=sample.TYPE_CUMULATIVE,
                                          unit='B',
                                          volume=info.tx_bytes,
                                          res_metadata=nic,
                                          )


class OutgoingErrorsPollster(_Base):

    @staticmethod
    def generate_one_sample(host, c_data):
        (nic, info) = c_data
        return util.make_sample_from_host(host,
                                          name='network.outgoing.errors',
                                          type=sample.TYPE_CUMULATIVE,
                                          unit='packet',
                                          volume=info.error,
                                          res_metadata=nic,
                                          )

########NEW FILE########
__FILENAME__ = util
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 ZHAW SoE
# Copyright © 2014 Intel Corp.
#
# Authors: Lucas Graf <graflu0@students.zhaw.ch>
#          Toni Zehnder <zehndton@students.zhaw.ch>
#          Lianhao Lu <lianhao.lu@intel.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import copy
import urlparse

from ceilometer.openstack.common import timeutils
from ceilometer import sample


def get_metadata_from_host(host_url):
    return {'resource_url': urlparse.urlunsplit(host_url)}


def make_sample_from_host(host_url, name, type, unit, volume,
                          project_id=None, user_id=None, res_metadata=None):
    resource_metadata = dict()
    if res_metadata is not None:
        metadata = copy.copy(res_metadata)
        resource_metadata = dict(zip(metadata._fields, metadata))
    resource_metadata.update(get_metadata_from_host(host_url))

    return sample.Sample(
        name='hardware.' + name,
        type=type,
        unit=unit,
        volume=volume,
        user_id=project_id,
        project_id=user_id,
        resource_id=host_url.hostname,
        timestamp=timeutils.isotime(),
        resource_metadata=resource_metadata,
        source='hardware',
    )

########NEW FILE########
__FILENAME__ = glance
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Common code for working with images
"""

from __future__ import absolute_import
import itertools

import glanceclient
from oslo.config import cfg

from ceilometer.openstack.common import timeutils
from ceilometer import plugin
from ceilometer import sample


class _Base(plugin.PollsterBase):

    @staticmethod
    def get_glance_client(ksclient):
        endpoint = ksclient.service_catalog.url_for(
            service_type='image',
            endpoint_type=cfg.CONF.service_credentials.os_endpoint_type)

        # hard-code v1 glance API version selection while v2 API matures
        service_credentials = cfg.CONF.service_credentials
        return glanceclient.Client('1', endpoint,
                                   token=ksclient.auth_token,
                                   cacert=service_credentials.os_cacert,
                                   insecure=service_credentials.insecure)

    def _get_images(self, ksclient):
        client = self.get_glance_client(ksclient)
        #TODO(eglynn): use pagination to protect against unbounded
        #              memory usage
        rawImageList = list(itertools.chain(
            client.images.list(filters={"is_public": True}),
            #TODO(eglynn): extend glance API with all_tenants logic to
            #              avoid second call to retrieve private images
            client.images.list(filters={"is_public": False})))

        # When retrieving images from glance, glance will check
        # whether the user is of 'admin_role' which is
        # configured in glance-api.conf. If the user is of
        # admin_role, and is querying public images(which means
        # that the 'is_public' param is set to be True),
        # glance will ignore 'is_public' parameter and returns
        # all the public images together with private images.
        # As a result, if the user/tenant has an admin role
        # for ceilometer to collect image list,
        # the _Base.iter_images method will return a image list
        # which contains duplicate images. Add the following
        # code to avoid recording down duplicate image events.
        imageIdSet = set(image.id for image in rawImageList)

        for image in rawImageList:
            if image.id in imageIdSet:
                imageIdSet -= set([image.id])
                yield image

    def _iter_images(self, ksclient, cache):
        """Iterate over all images."""
        if 'images' not in cache:
            cache['images'] = list(self._get_images(ksclient))
        return iter(cache['images'])

    @staticmethod
    def extract_image_metadata(image):
        return dict((k, getattr(image, k))
                    for k in
                    [
                        "status",
                        "is_public",
                        "name",
                        "deleted",
                        "container_format",
                        "created_at",
                        "disk_format",
                        "updated_at",
                        "properties",
                        "min_disk",
                        "protected",
                        "checksum",
                        "deleted_at",
                        "min_ram",
                        "size",
                    ])


class ImagePollster(_Base):

    def get_samples(self, manager, cache, resources=None):
        for image in self._iter_images(manager.keystone, cache):
            yield sample.Sample(
                name='image',
                type=sample.TYPE_GAUGE,
                unit='image',
                volume=1,
                user_id=None,
                project_id=image.owner,
                resource_id=image.id,
                timestamp=timeutils.isotime(),
                resource_metadata=self.extract_image_metadata(image),
            )


class ImageSizePollster(_Base):

    def get_samples(self, manager, cache, resources=None):
        for image in self._iter_images(manager.keystone, cache):
            yield sample.Sample(
                name='image.size',
                type=sample.TYPE_GAUGE,
                unit='B',
                volume=image.size,
                user_id=None,
                project_id=image.owner,
                resource_id=image.id,
                timestamp=timeutils.isotime(),
                resource_metadata=self.extract_image_metadata(image),
            )

########NEW FILE########
__FILENAME__ = notifications
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 Red Hat, Inc
#
# Author: Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Handler for producing image metering messages from glance notification
   events.
"""

from oslo.config import cfg
import oslo.messaging

from ceilometer import plugin
from ceilometer import sample

OPTS = [
    cfg.StrOpt('glance_control_exchange',
               default='glance',
               help="Exchange name for Glance notifications."),
]


cfg.CONF.register_opts(OPTS)


class ImageBase(plugin.NotificationBase):
    """Base class for image counting."""

    @staticmethod
    def get_targets(conf):
        """Return a sequence of oslo.messaging.Target defining the exchange and
        topics to be connected for this plugin.
        """
        return [oslo.messaging.Target(topic=topic,
                                      exchange=conf.glance_control_exchange)
                for topic in conf.notification_topics]


class ImageCRUDBase(ImageBase):
    event_types = [
        'image.update',
        'image.upload',
        'image.delete',
    ]


class ImageCRUD(ImageCRUDBase):
    def process_notification(self, message):
        yield sample.Sample.from_notification(
            name=message['event_type'],
            type=sample.TYPE_DELTA,
            unit='image',
            volume=1,
            resource_id=message['payload']['id'],
            user_id=None,
            project_id=message['payload']['owner'],
            message=message)


class Image(ImageCRUDBase):
    def process_notification(self, message):
        yield sample.Sample.from_notification(
            name='image',
            type=sample.TYPE_GAUGE,
            unit='image',
            volume=1,
            resource_id=message['payload']['id'],
            user_id=None,
            project_id=message['payload']['owner'],
            message=message)


class ImageSize(ImageCRUDBase):
    def process_notification(self, message):
        yield sample.Sample.from_notification(
            name='image.size',
            type=sample.TYPE_GAUGE,
            unit='B',
            volume=message['payload']['size'],
            resource_id=message['payload']['id'],
            user_id=None,
            project_id=message['payload']['owner'],
            message=message)


class ImageDownload(ImageBase):
    """Emit image_download sample when an image is downloaded."""
    event_types = ['image.send']

    def process_notification(self, message):
        yield sample.Sample.from_notification(
            name='image.download',
            type=sample.TYPE_DELTA,
            unit='B',
            volume=message['payload']['bytes_sent'],
            resource_id=message['payload']['image_id'],
            user_id=message['payload']['receiver_user_id'],
            project_id=message['payload']['receiver_tenant_id'],
            message=message)


class ImageServe(ImageBase):
    """Emit image_serve sample when an image is served out."""
    event_types = ['image.send']

    def process_notification(self, message):
        yield sample.Sample.from_notification(
            name='image.serve',
            type=sample.TYPE_DELTA,
            unit='B',
            volume=message['payload']['bytes_sent'],
            resource_id=message['payload']['image_id'],
            user_id=None,
            project_id=message['payload']['owner_id'],
            message=message)

########NEW FILE########
__FILENAME__ = messaging
# -*- encoding: utf-8 -*-
# Copyright © 2013 eNovance <licensing@enovance.com>
#
# Author: Mehdi Abaakouk <mehdi.abaakouk@enovance.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import eventlet
from oslo.config import cfg
import oslo.messaging

from ceilometer.openstack.common import context
from ceilometer.openstack.common import jsonutils


TRANSPORT = None
NOTIFIER = None

_ALIASES = {
    'ceilometer.openstack.common.rpc.impl_kombu': 'rabbit',
    'ceilometer.openstack.common.rpc.impl_qpid': 'qpid',
    'ceilometer.openstack.common.rpc.impl_zmq': 'zmq',
}


class RequestContextSerializer(oslo.messaging.Serializer):
    def __init__(self, base):
        self._base = base

    def serialize_entity(self, ctxt, entity):
        if not self._base:
            return entity
        return self._base.serialize_entity(ctxt, entity)

    def deserialize_entity(self, ctxt, entity):
        if not self._base:
            return entity
        return self._base.deserialize_entity(ctxt, entity)

    @staticmethod
    def serialize_context(ctxt):
        return ctxt.to_dict()

    @staticmethod
    def deserialize_context(ctxt):
        return context.RequestContext(ctxt)


class JsonPayloadSerializer(oslo.messaging.NoOpSerializer):
    @classmethod
    def serialize_entity(cls, context, entity):
        return jsonutils.to_primitive(entity, convert_instances=True)


def setup(url=None):
    """Initialise the oslo.messaging layer."""
    global TRANSPORT, NOTIFIER

    if url and url.startswith("fake://"):
        # NOTE(sileht): oslo.messaging fake driver uses time.sleep
        # for task switch, so we need to monkey_patch it
        eventlet.monkey_patch(time=True)

    if not TRANSPORT:
        oslo.messaging.set_transport_defaults('ceilometer')
        TRANSPORT = oslo.messaging.get_transport(cfg.CONF, url,
                                                 aliases=_ALIASES)
    if not NOTIFIER:
        serializer = RequestContextSerializer(JsonPayloadSerializer())
        NOTIFIER = oslo.messaging.Notifier(TRANSPORT, serializer=serializer)


def cleanup():
    """Cleanup the oslo.messaging layer."""
    global TRANSPORT, NOTIFIER
    assert TRANSPORT is not None
    assert NOTIFIER is not None
    TRANSPORT.cleanup()
    TRANSPORT = NOTIFIER = None


def get_rpc_server(topic, endpoint):
    """Return a configured oslo.messaging rpc server."""
    global TRANSPORT
    target = oslo.messaging.Target(server=cfg.CONF.host, topic=topic)
    serializer = RequestContextSerializer(JsonPayloadSerializer())
    return oslo.messaging.get_rpc_server(TRANSPORT, target, [endpoint],
                                         executor='eventlet',
                                         serializer=serializer)


def get_rpc_client(**kwargs):
    """Return a configured oslo.messaging RPCClient."""
    global TRANSPORT
    target = oslo.messaging.Target(**kwargs)
    serializer = RequestContextSerializer(JsonPayloadSerializer())
    return oslo.messaging.RPCClient(TRANSPORT, target,
                                    serializer=serializer)


def get_notification_listener(targets, endpoints, url=None):
    """Return a configured oslo.messaging notification listener."""
    global TRANSPORT
    if url:
        transport = oslo.messaging.get_transport(cfg.CONF, url,
                                                 _ALIASES)
    else:
        transport = TRANSPORT
    return oslo.messaging.get_notification_listener(
        transport, targets, endpoints, executor='eventlet')


def get_notifier(publisher_id):
    """Return a configured oslo.messaging notifier."""
    global NOTIFIER
    return NOTIFIER.prepare(publisher_id=publisher_id)


def convert_to_old_notification_format(priority, ctxt, publisher_id,
                                       event_type, payload, metadata):
    # FIXME(sileht): temporary convert notification to old format
    # to focus on oslo.messaging migration before refactoring the code to
    # use the new oslo.messaging facilities
    notification = {'priority': priority,
                    'payload': payload,
                    'event_type': event_type,
                    'publisher_id': publisher_id}
    notification.update(metadata)
    for k in ctxt:
        notification['_context_' + k] = ctxt[k]
    return notification

########NEW FILE########
__FILENAME__ = middleware
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 eNovance
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from oslo.config import cfg
import oslo.messaging

from ceilometer import plugin
from ceilometer import sample

cfg.CONF.import_opt('nova_control_exchange',
                    'ceilometer.compute.notifications')
cfg.CONF.import_opt('glance_control_exchange',
                    'ceilometer.image.notifications')
cfg.CONF.import_opt('neutron_control_exchange',
                    'ceilometer.network.notifications')
cfg.CONF.import_opt('cinder_control_exchange',
                    'ceilometer.volume.notifications')

OPTS = [
    cfg.MultiStrOpt('http_control_exchanges',
                    default=[cfg.CONF.nova_control_exchange,
                             cfg.CONF.glance_control_exchange,
                             cfg.CONF.neutron_control_exchange,
                             cfg.CONF.cinder_control_exchange],
                    help="Exchanges name to listen for notifications."),
]

cfg.CONF.register_opts(OPTS)


class HTTPRequest(plugin.NotificationBase):
    event_types = ['http.request']

    @staticmethod
    def get_targets(conf):
        """Return a sequence of oslo.messaging.Target defining the exchange and
        topics to be connected for this plugin.
        """
        return [oslo.messaging.Target(topic=topic, exchange=exchange)
                for topic in conf.notification_topics
                for exchange in conf.http_control_exchanges]

    def process_notification(self, message):
        yield sample.Sample.from_notification(
            name=message['event_type'],
            type=sample.TYPE_DELTA,
            volume=1,
            unit=message['event_type'].split('.')[1],
            user_id=message['payload']['request'].get('HTTP_X_USER_ID'),
            project_id=message['payload']['request'].get('HTTP_X_PROJECT_ID'),
            resource_id=message['payload']['request'].get(
                'HTTP_X_SERVICE_NAME'),
            message=message)


class HTTPResponse(HTTPRequest):
    event_types = ['http.response']

########NEW FILE########
__FILENAME__ = floatingip
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 eNovance <licensing@enovance.com>
#
# Copyright 2013 IBM Corp
# All Rights Reserved.
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ceilometer.central import plugin
from ceilometer import nova_client
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer.openstack.common import timeutils
from ceilometer import sample


class FloatingIPPollster(plugin.CentralPollster):

    LOG = log.getLogger(__name__ + '.floatingip')

    def _get_floating_ips(self):
        nv = nova_client.Client()
        return nv.floating_ip_get_all()

    def _iter_floating_ips(self, cache):
        if 'floating_ips' not in cache:
            cache['floating_ips'] = list(self._get_floating_ips())
        return iter(cache['floating_ips'])

    def get_samples(self, manager, cache, resources=None):
        for ip in self._iter_floating_ips(cache):
            self.LOG.info(_("FLOATING IP USAGE: %s") % ip.ip)
            # FIXME (flwang) Now Nova API /os-floating-ips can't provide those
            # attributes were used by Ceilometer, such as project id, host.
            # In this fix, those attributes usage will be removed temporarily.
            # And they will be back after fix the Nova bug 1174802.
            yield sample.Sample(
                name='ip.floating',
                type=sample.TYPE_GAUGE,
                unit='ip',
                volume=1,
                user_id=None,
                project_id=None,
                resource_id=ip.id,
                timestamp=timeutils.utcnow().isoformat(),
                resource_metadata={
                    'address': ip.ip,
                    'pool': ip.pool
                })

########NEW FILE########
__FILENAME__ = notifications
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Handler for producing network counter messages from Neutron notification
   events.

"""
import copy

from oslo.config import cfg
import oslo.messaging

from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer import plugin
from ceilometer import sample

OPTS = [
    cfg.StrOpt('neutron_control_exchange',
               default='neutron',
               help="Exchange name for Neutron notifications.",
               deprecated_name='quantum_control_exchange'),
]

cfg.CONF.register_opts(OPTS)

LOG = log.getLogger(__name__)


class NetworkNotificationBase(plugin.NotificationBase):

    resource_name = None

    @property
    def event_types(self):
        return [
            # NOTE(flwang): When the *.create.start notification sending,
            # there is no resource id assigned by Neutron yet. So we ignore
            # the *.create.start notification for now and only listen the
            # *.create.end to make sure the resource id is existed.
            '%s.create.end' % (self.resource_name),
            '%s.update.*' % (self.resource_name),
            '%s.exists' % (self.resource_name),
            # FIXME(dhellmann): Neutron delete notifications do
            # not include the same metadata as the other messages,
            # so we ignore them for now. This isn't ideal, since
            # it may mean we miss charging for some amount of time,
            # but it is better than throwing away the existing
            # metadata for a resource when it is deleted.
            ##'%s.delete.start' % (self.resource_name),
        ]

    @staticmethod
    def get_targets(conf):
        """Return a sequence of oslo.messaging.Target defining the exchange and
        topics to be connected for this plugin.
        """
        return [oslo.messaging.Target(topic=topic,
                                      exchange=conf.neutron_control_exchange)
                for topic in conf.notification_topics]

    def process_notification(self, message):
        LOG.info(_('network notification %r') % message)
        counter_name = getattr(self, 'counter_name', self.resource_name)
        unit_value = getattr(self, 'unit', self.resource_name)

        payload = message['payload'].get(self.resource_name)
        payloads = message['payload'].get(self.resource_name + 's')
        payload_list = copy.copy([payload] if payload else payloads)
        for p in payload_list:
            message['payload'] = p
            yield sample.Sample.from_notification(
                name=counter_name,
                type=sample.TYPE_GAUGE,
                unit=unit_value,
                volume=1,
                user_id=message['_context_user_id'],
                project_id=message['_context_tenant_id'],
                resource_id=message['payload']['id'],
                message=message)
            event_type_split = message['event_type'].split('.')
            if len(event_type_split) > 2:
                yield sample.Sample.from_notification(
                    name=counter_name
                    + "." + event_type_split[1],
                    type=sample.TYPE_DELTA,
                    unit=unit_value,
                    volume=1,
                    user_id=message['_context_user_id'],
                    project_id=message['_context_tenant_id'],
                    resource_id=message['payload']['id'],
                    message=message)


class Network(NetworkNotificationBase):
    """Listen for Neutron network notifications in order to mediate with the
    metering framework.

    """
    resource_name = 'network'


class Subnet(NetworkNotificationBase):
    """Listen for Neutron notifications in order to mediate with the
    metering framework.

    """
    resource_name = 'subnet'


class Port(NetworkNotificationBase):
    """Listen for Neutron notifications in order to mediate with the
    metering framework.

    """
    resource_name = 'port'


class Router(NetworkNotificationBase):
    """Listen for Neutron notifications in order to mediate with the
    metering framework.

    """
    resource_name = 'router'


class FloatingIP(NetworkNotificationBase):
    """Listen for Neutron notifications in order to mediate with the
    metering framework.

    """
    resource_name = 'floatingip'
    counter_name = 'ip.floating'
    unit = 'ip'


class Bandwidth(NetworkNotificationBase):
    """Listen for Neutron notifications in order to mediate with the
    metering framework.

    """
    event_types = ['l3.meter']

    def process_notification(self, message):
        yield sample.Sample.from_notification(
            name='bandwidth',
            type=sample.TYPE_DELTA,
            unit='B',
            volume=message['payload']['bytes'],
            user_id=None,
            project_id=message['payload']['tenant_id'],
            resource_id=message['payload']['label_id'],
            message=message)

########NEW FILE########
__FILENAME__ = driver
#
# Copyright 2014 NEC Corporation.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import abc

import six


@six.add_metaclass(abc.ABCMeta)
class Driver():

    @abc.abstractmethod
    def get_sample_data(self, meter_name, parse_url, params, cache):
        '''Return volume, resource_id, resource_metadata, timestamp in tuple.

        If not implemented for meter_name, returns None
        '''

########NEW FILE########
__FILENAME__ = flow
#
# Copyright 2014 NEC Corporation.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.


from ceilometer.network import statistics
from ceilometer import sample


class FlowPollster(statistics._Base):

    meter_name = 'switch.flow'
    meter_type = sample.TYPE_GAUGE
    meter_unit = 'flow'


class FlowPollsterDurationSeconds(statistics._Base):

    meter_name = 'switch.flow.duration_seconds'
    meter_type = sample.TYPE_GAUGE
    meter_unit = 's'


class FlowPollsterDurationNanoseconds(statistics._Base):

    meter_name = 'switch.flow.duration_nanoseconds'
    meter_type = sample.TYPE_GAUGE
    meter_unit = 'ns'


class FlowPollsterPackets(statistics._Base):

    meter_name = 'switch.flow.packets'
    meter_type = sample.TYPE_CUMULATIVE
    meter_unit = 'packet'


class FlowPollsterBytes(statistics._Base):

    meter_name = 'switch.flow.bytes'
    meter_type = sample.TYPE_CUMULATIVE
    meter_unit = 'B'

########NEW FILE########
__FILENAME__ = client
# Copyright (C) 2014 eNovance SAS <licensing@enovance.com>
#
# Author: Sylvain Afchain <sylvain.afchain@enovance.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from oslo.config import cfg
import requests
import six
from six.moves.urllib import parse as url_parse

from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log


CONF = cfg.CONF


LOG = log.getLogger(__name__)


class OpencontrailAPIFailed(Exception):
    pass


class AnalyticsAPIBaseClient(object):
    """Opencontrail Base Statistics REST API Client."""

    def __init__(self, endpoint, username, password, domain, verify_ssl=True):
        self.endpoint = endpoint
        self.username = username
        self.password = password
        self.domain = domain
        self.verify_ssl = verify_ssl
        self.sid = None

    def authenticate(self):
        path = '/authenticate'
        data = {'username': self.username,
                'password': self.password,
                'domain': self.domain}

        req_params = self._get_req_params(data=data)
        url = url_parse.urljoin(self.endpoint, path)
        resp = requests.post(url, **req_params)
        if resp.status_code != 302:
            raise OpencontrailAPIFailed(
                _('Opencontrail API returned %(status)s %(reason)s') %
                {'status': resp.status_code, 'reason': resp.reason})
        self.sid = resp.cookies['connect.sid']

    def request(self, path, fqdn_uuid, data, retry=True):
        if not self.sid:
            self.authenticate()

        if not data:
            data = {'fqnUUID': fqdn_uuid}
        else:
            data['fqnUUID'] = fqdn_uuid

        req_params = self._get_req_params(data=data,
                                          cookies={'connect.sid': self.sid})

        url = url_parse.urljoin(self.endpoint, path)
        self._log_req(url, req_params)
        resp = requests.get(url, **req_params)
        self._log_res(resp)

        # it seems that the sid token has to be renewed
        if resp.status_code == 302:
            self.sid = 0
            if retry:
                return self.request(path, fqdn_uuid, data,
                                    retry=False)

        if resp.status_code != 200:
            raise OpencontrailAPIFailed(
                _('Opencontrail API returned %(status)s %(reason)s') %
                {'status': resp.status_code, 'reason': resp.reason})

        return resp

    def _get_req_params(self, params=None, data=None, cookies=None):
        req_params = {
            'headers': {
                'Accept': 'application/json'
            },
            'data': data,
            'verify': self.verify_ssl,
            'allow_redirects': False,
            'cookies': cookies
        }

        return req_params

    @staticmethod
    def _log_req(url, req_params):
        if not CONF.debug:
            return

        curl_command = ['REQ: curl -i -X GET ']

        params = []
        for name, value in six.iteritems(req_params['data']):
            params.append("%s=%s" % (name, value))

        curl_command.append('"%s?%s" ' % (url, '&'.join(params)))

        for name, value in six.iteritems(req_params['headers']):
            curl_command.append('-H "%s: %s" ' % (name, value))

        LOG.debug(''.join(curl_command))

    @staticmethod
    def _log_res(resp):
        if not CONF.debug:
            return

        dump = ['RES: \n']
        dump.append('HTTP %.1f %s %s\n' % (resp.raw.version,
                                           resp.status_code,
                                           resp.reason))
        dump.extend(['%s: %s\n' % (k, v)
                     for k, v in six.iteritems(resp.headers)])
        dump.append('\n')
        if resp.content:
            dump.extend([resp.content, '\n'])

        LOG.debug(''.join(dump))


class NetworksAPIClient(AnalyticsAPIBaseClient):
    """Opencontrail Statistics REST API Client."""

    def get_port_statistics(self, fqdn_uuid):
        """Get port statistics of a network

        URL:
            /tenant/networking/virtual-machines/details
        PARAMS:
            fqdnUUID=fqdn_uuid
            type=vn
        """

        path = '/api/tenant/networking/virtual-machines/details'
        resp = self.request(path, fqdn_uuid, {'type': 'vn'})

        return resp.json()


class Client(object):

    def __init__(self, endpoint, username, password, domain, verify_ssl=True):
        self.networks = NetworksAPIClient(endpoint, username, password,
                                          domain, verify_ssl)

########NEW FILE########
__FILENAME__ = driver
# Copyright (C) 2014 eNovance SAS <licensing@enovance.com>
#
# Author: Sylvain Afchain <sylvain.afchain@enovance.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from six.moves.urllib import parse as url_parse

from ceilometer.network.statistics import driver
from ceilometer.network.statistics.opencontrail import client
from ceilometer import neutron_client
from ceilometer.openstack.common import timeutils


class OpencontrailDriver(driver.Driver):
    """Driver of network analytics of Opencontrail.

    This driver uses resources in "pipeline.yaml".
    Resource requires below conditions:
    * resource is url
    * scheme is "opencontrail"

    This driver can be configured via query parameters.
    Supported parameters:
    * scheme:
        The scheme of request url to Opencontrail Analytics endpoint.
        (default http)
    * username:
        This is username used by Opencontrail Analytics.(default None)
    * password:
        This is password used by Opencontrail Analytics.(default None)
    * domain
        This is domain used by Opencontrail Analytics.(default None)
    * verify_ssl
        Specify if the certificate will be checked for https request.
        (default false)

    e.g.
        opencontrail://localhost:8143/?username=admin&password=admin&
        scheme=https&domain=&verify_ssl=true
    """
    @staticmethod
    def _prepare_cache(endpoint, params, cache):

        if 'network.statistics.opencontrail' in cache:
            return cache['network.statistics.opencontrail']

        data = {
            'o_client': client.Client(endpoint,
                                      params['username'],
                                      params['password'],
                                      params.get('domain'),
                                      params.get('verify_ssl') == 'true'),
            'n_client': neutron_client.Client()
        }

        cache['network.statistics.opencontrail'] = data

        return data

    def get_sample_data(self, meter_name, parse_url, params, cache):

        parts = url_parse.ParseResult(params.get('scheme', ['http'])[0],
                                      parse_url.netloc,
                                      parse_url.path,
                                      None,
                                      None,
                                      None)
        endpoint = url_parse.urlunparse(parts)

        iter = self._get_iter(meter_name)
        if iter is None:
            # The extractor for this meter is not implemented or the API
            # doesn't have method to get this meter.
            return

        extractor = self._get_extractor(meter_name)
        if extractor is None:
            # The extractor for this meter is not implemented or the API
            # doesn't have method to get this meter.
            return

        data = self._prepare_cache(endpoint, params, cache)

        ports = data['n_client'].port_get_all()
        ports_map = dict((port['id'], port['tenant_id']) for port in ports)

        networks = data['n_client'].network_get_all()

        for network in networks:
            net_id = network['id']

            timestamp = timeutils.utcnow().isoformat()
            statistics = data['o_client'].networks.get_port_statistics(net_id)
            if not statistics:
                continue

            for value in statistics['value']:
                for sample in iter(extractor, value, ports_map):
                    if sample is not None:
                        sample[2]['network_id'] = net_id
                        yield sample + (timestamp, )

    def _get_iter(self, meter_name):
        if meter_name.startswith('switch.port'):
            return self._iter_port

    def _get_extractor(self, meter_name):
        method_name = '_' + meter_name.replace('.', '_')
        return getattr(self, method_name, None)

    @staticmethod
    def _iter_port(extractor, value, ports_map):
        ifstats = value['value']['UveVirtualMachineAgent']['if_stats_list']
        for ifstat in ifstats:
            name = ifstat['name']
            device_owner_id, port_id = name.split(':')

            tenant_id = ports_map.get(port_id)

            resource_meta = {'device_owner_id': device_owner_id,
                             'tenant_id': tenant_id}
            yield extractor(ifstat, port_id, resource_meta)

    @staticmethod
    def _switch_port_receive_packets(statistic, resource_id, resource_meta):
        return (int(statistic['in_pkts']), resource_id, resource_meta)

    @staticmethod
    def _switch_port_transmit_packets(statistic, resource_id, resource_meta):
        return (int(statistic['out_pkts']), resource_id, resource_meta)

    @staticmethod
    def _switch_port_receive_bytes(statistic, resource_id, resource_meta):
        return (int(statistic['in_bytes']), resource_id, resource_meta)

    @staticmethod
    def _switch_port_transmit_bytes(statistic, resource_id, resource_meta):
        return (int(statistic['out_bytes']), resource_id, resource_meta)

########NEW FILE########
__FILENAME__ = client
#
# Copyright 2013 NEC Corporation.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import abc

from oslo.config import cfg
import requests
from requests import auth
import six

from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log


CONF = cfg.CONF


LOG = log.getLogger(__name__)


@six.add_metaclass(abc.ABCMeta)
class _Base():
    """Base class of OpenDaylight REST APIs Clients."""

    @abc.abstractproperty
    def base_url(self):
        """Returns base url for each REST API."""

    def __init__(self, client):
        self.client = client

    def request(self, path, container_name):
        return self.client.request(self.base_url + path, container_name)


class OpenDaylightRESTAPIFailed(Exception):
    pass


class StatisticsAPIClient(_Base):
    """OpenDaylight Statistics REST API Client

    Base URL:
      {endpoint}/statistics/{containerName}
    """

    base_url = '/statistics/%(container_name)s'

    def get_port_statistics(self, container_name):
        """Get port statistics

        URL:
            {Base URL}/port
        """
        return self.request('/port', container_name)

    def get_flow_statistics(self, container_name):
        """Get flow statistics

        URL:
            {Base URL}/flow
        """
        return self.request('/flow', container_name)

    def get_table_statistics(self, container_name):
        """Get table statistics

        URL:
            {Base URL}/table
        """
        return self.request('/table', container_name)


class TopologyAPIClient(_Base):
    """OpenDaylight Topology REST API Client

    Base URL:
      {endpoint}/topology/{containerName}
    """

    base_url = '/topology/%(container_name)s'

    def get_topology(self, container_name):
        """Get topology

        URL:
            {Base URL}
        """
        return self.request('', container_name)

    def get_user_links(self, container_name):
        """Get user links

        URL:
            {Base URL}/userLinks
        """
        return self.request('/userLinks', container_name)


class SwitchManagerAPIClient(_Base):
    """OpenDaylight Switch Manager REST API Client

    Base URL:
      {endpoint}/switchmanager/{containerName}
    """

    base_url = '/switchmanager/%(container_name)s'

    def get_nodes(self, container_name):
        """Get node informations

        URL:
            {Base URL}/nodes
        """
        return self.request('/nodes', container_name)


class HostTrackerAPIClient(_Base):
    """OpenDaylight Host Tracker REST API Client

    Base URL:
      {endpoint}/hosttracker/{containerName}
    """

    base_url = '/hosttracker/%(container_name)s'

    def get_active_hosts(self, container_name):
        """Get active hosts informatinos

        URL:
            {Base URL}/hosts/active
        """
        return self.request('/hosts/active', container_name)

    def get_inactive_hosts(self, container_name):
        """Get inactive hosts informations

        URL:
            {Base URL}/hosts/inactive
        """
        return self.request('/hosts/inactive', container_name)


class Client():

    def __init__(self, endpoint, params):
        self.statistics = StatisticsAPIClient(self)
        self.topology = TopologyAPIClient(self)
        self.switch_manager = SwitchManagerAPIClient(self)
        self.host_tracker = HostTrackerAPIClient(self)

        self._endpoint = endpoint

        self._req_params = self._get_req_params(params)

    @staticmethod
    def _get_req_params(params):
        req_params = {
            'headers': {
                'Accept': 'application/json'
            }
        }

        auth_way = params.get('auth')
        if auth_way in ['basic', 'digest']:
            user = params.get('user')
            password = params.get('password')

            if auth_way == 'basic':
                auth_class = auth.HTTPBasicAuth
            else:
                auth_class = auth.HTTPDigestAuth

            req_params['auth'] = auth_class(user, password)
        return req_params

    def _log_req(self, url):

        curl_command = ['REQ: curl -i -X GET ']
        curl_command.append('"%s" ' % (url))

        if 'auth' in self._req_params:
            auth_class = self._req_params['auth']
            if isinstance(auth_class, auth.HTTPBasicAuth):
                curl_command.append('--basic ')
            else:
                curl_command.append('--digest ')

            curl_command.append('--user "%s":"%s" ' % (auth_class.username,
                                                       auth_class.password))

        for name, value in six.iteritems(self._req_params['headers']):
            curl_command.append('-H "%s: %s" ' % (name, value))

        LOG.debug(''.join(curl_command))

    @staticmethod
    def _log_res(resp):

        dump = ['RES: \n']
        dump.append('HTTP %.1f %s %s\n' % (resp.raw.version,
                                           resp.status_code,
                                           resp.reason))
        dump.extend(['%s: %s\n' % (k, v)
                     for k, v in six.iteritems(resp.headers)])
        dump.append('\n')
        if resp.content:
            dump.extend([resp.content, '\n'])

        LOG.debug(''.join(dump))

    def _http_request(self, url):
        if CONF.debug:
            self._log_req(url)
        resp = requests.get(url, **self._req_params)
        if CONF.debug:
            self._log_res(resp)
        if resp.status_code / 100 != 2:
            raise OpenDaylightRESTAPIFailed(
                _('OpenDaylitght API returned %(status)s %(reason)s') %
                {'status': resp.status_code, 'reason': resp.reason})

        return resp.json()

    def request(self, path, container_name):

        url = self._endpoint + path % {'container_name': container_name}
        return self._http_request(url)

########NEW FILE########
__FILENAME__ = driver
#
# Copyright 2013 NEC Corporation.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import six
from six import moves
from six.moves.urllib import parse as url_parse

from ceilometer.network.statistics import driver
from ceilometer.network.statistics.opendaylight import client
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer.openstack.common import timeutils
from ceilometer import utils


LOG = log.getLogger(__name__)


def _get_properties(properties, prefix='properties'):
    resource_meta = {}
    if properties is not None:
        for k, v in six.iteritems(properties):
            value = v['value']
            key = prefix + '_' + k
            if 'name' in v:
                key += '_' + v['name']
            resource_meta[key] = value
    return resource_meta


def _get_int_sample(key, statistic, resource_id, resource_meta):
    if key not in statistic:
        return None
    return (int(statistic[key]), resource_id, resource_meta)


class OpenDayLightDriver(driver.Driver):
    """Driver of network info collector from OpenDaylight.

    This driver uses resources in "pipeline.yaml".
    Resource requires below conditions:
    * resource is url
    * scheme is "opendaylight"

    This driver can be configured via query parameters.
    Supported parameters:
    * scheme:
        The scheme of request url to OpenDaylight REST API endpoint.
        (default http)
    * auth:
        Auth strategy of http.
        This parameter can be set basic and digest.(default None)
    * user:
        This is username that is used by auth.(default None)
    * password:
        This is password that is used by auth.(default None)
    * container_name:
        Name of container of OpenDaylight.(default "default")
        This parameter allows multi vaues.

    e.g.
        opendaylight://127.0.0.1:8080/controller/nb/v2?container_name=default&
            container_name=egg&auth=basic&user=admin&password=admin&scheme=http

        In this case, the driver send request to below URL:
            http://127.0.0.1:8080/controller/nb/v2/statistics/default/flow
            and
            http://127.0.0.1:8080/controller/nb/v2/statistics/egg/flow
    """
    @staticmethod
    def _prepare_cache(endpoint, params, cache):

        if 'network.statistics.opendaylight' in cache:
            return cache['network.statistics.opendaylight']

        data = {}

        container_names = params.get('container_name', ['default'])

        odl_params = {}
        if 'auth' in params:
            odl_params['auth'] = params['auth'][0]
        if 'user' in params:
            odl_params['user'] = params['user'][0]
        if 'password' in params:
            odl_params['password'] = params['password'][0]
        cs = client.Client(endpoint, odl_params)

        for container_name in container_names:
            try:
                container_data = {}

                # get flow statistics
                container_data['flow'] = cs.statistics.get_flow_statistics(
                    container_name)

                # get port statistics
                container_data['port'] = cs.statistics.get_port_statistics(
                    container_name)

                # get table statistics
                container_data['table'] = cs.statistics.get_table_statistics(
                    container_name)

                # get topology
                container_data['topology'] = cs.topology.get_topology(
                    container_name)

                # get switch informations
                container_data['switch'] = cs.switch_manager.get_nodes(
                    container_name)

                # get and optimize user links
                # e.g.
                # before:
                #   "OF|2@OF|00:00:00:00:00:00:00:02"
                # after:
                #   {
                #       'port': {
                #           'type': 'OF',
                #           'id': '2'},
                #       'node': {
                #           'type': 'OF',
                #           'id': '00:00:00:00:00:00:00:02'
                #       }
                #   }
                user_links_raw = cs.topology.get_user_links(container_name)
                user_links = []
                container_data['user_links'] = user_links
                for user_link_row in user_links_raw['userLinks']:
                    user_link = {}
                    for k, v in six.iteritems(user_link_row):
                        if (k == "dstNodeConnector" or
                                k == "srcNodeConnector"):
                            port_raw, node_raw = v.split('@')
                            port = {}
                            port['type'], port['id'] = port_raw.split('|')
                            node = {}
                            node['type'], node['id'] = node_raw.split('|')
                            v = {'port': port, 'node': node}
                        user_link[k] = v
                    user_links.append(user_link)

                # get link status to hosts
                container_data['active_hosts'] = cs.host_tracker.\
                    get_active_hosts(container_name)
                container_data['inactive_hosts'] = cs.host_tracker.\
                    get_inactive_hosts(container_name)

                container_data['timestamp'] = timeutils.isotime()

                data[container_name] = container_data
            except Exception:
                LOG.exception(_('Request failed to connect to OpenDaylight'
                                ' with NorthBound REST API'))

        cache['network.statistics.opendaylight'] = data

        return data

    def get_sample_data(self, meter_name, parse_url, params, cache):

        extractor = self._get_extractor(meter_name)
        if extractor is None:
            # The way to getting meter is not implemented in this driver or
            # OpenDaylight REST API has not api to getting meter.
            return None

        iter = self._get_iter(meter_name)
        if iter is None:
            # The way to getting meter is not implemented in this driver or
            # OpenDaylight REST API has not api to getting meter.
            return None

        parts = url_parse.ParseResult(params.get('scheme', ['http'])[0],
                                      parse_url.netloc,
                                      parse_url.path,
                                      None,
                                      None,
                                      None)
        endpoint = url_parse.urlunparse(parts)

        data = self._prepare_cache(endpoint, params, cache)

        samples = []
        for name, value in six.iteritems(data):
            timestamp = value['timestamp']
            for sample in iter(extractor, value):
                if sample is not None:
                    # set controller name and container name
                    # to resource_metadata
                    sample[2]['controller'] = 'OpenDaylight'
                    sample[2]['container'] = name

                    samples.append(sample + (timestamp, ))

        return samples

    def _get_iter(self, meter_name):
        if meter_name == 'switch':
            return self._iter_switch
        elif meter_name.startswith('switch.flow'):
            return self._iter_flow
        elif meter_name.startswith('switch.table'):
            return self._iter_table
        elif meter_name.startswith('switch.port'):
            return self._iter_port

    def _get_extractor(self, meter_name):
        method_name = '_' + meter_name.replace('.', '_')
        return getattr(self, method_name, None)

    @staticmethod
    def _iter_switch(extractor, data):
        for switch in data['switch']['nodeProperties']:
            yield extractor(switch, switch['node']['id'], {})

    @staticmethod
    def _switch(statistic, resource_id, resource_meta):

        resource_meta.update(_get_properties(statistic.get('properties')))

        return (1, resource_id, resource_meta)

    @staticmethod
    def _iter_port(extractor, data):
        for port_statistic in data['port']['portStatistics']:
            for statistic in port_statistic['portStatistic']:
                resource_meta = {'port': statistic['nodeConnector']['id']}
                yield extractor(statistic, port_statistic['node']['id'],
                                resource_meta, data)

    @staticmethod
    def _switch_port(statistic, resource_id, resource_meta, data):
        my_node_id = resource_id
        my_port_id = statistic['nodeConnector']['id']

        # link status from topology
        edge_properties = data['topology']['edgeProperties']
        for edge_property in edge_properties:
            edge = edge_property['edge']

            if (edge['headNodeConnector']['node']['id'] == my_node_id and
                    edge['headNodeConnector']['id'] == my_port_id):
                target_node = edge['tailNodeConnector']
            elif (edge['tailNodeConnector']['node']['id'] == my_node_id and
                    edge['tailNodeConnector']['id'] == my_port_id):
                target_node = edge['headNodeConnector']
            else:
                continue

            resource_meta['topology_node_id'] = target_node['node']['id']
            resource_meta['topology_node_port'] = target_node['id']

            resource_meta.update(_get_properties(
                edge_property.get('properties'),
                prefix='topology'))

            break

        # link status from user links
        for user_link in data['user_links']:
            if (user_link['dstNodeConnector']['node']['id'] == my_node_id and
                    user_link['dstNodeConnector']['port']['id'] == my_port_id):
                target_node = user_link['srcNodeConnector']
            elif (user_link['srcNodeConnector']['node']['id'] == my_node_id and
                    user_link['srcNodeConnector']['port']['id'] == my_port_id):
                target_node = user_link['dstNodeConnector']
            else:
                continue

            resource_meta['user_link_node_id'] = target_node['node']['id']
            resource_meta['user_link_node_port'] = target_node['port']['id']
            resource_meta['user_link_status'] = user_link['status']
            resource_meta['user_link_name'] = user_link['name']

            break

        # link status to hosts
        for hosts, status in moves.zip(
                [data['active_hosts'], data['inactive_hosts']],
                ['active', 'inactive']):
            for host_config in hosts['hostConfig']:
                if (host_config['nodeId'] != my_node_id or
                        host_config['nodeConnectorId'] != my_port_id):
                    continue

                resource_meta['host_status'] = status
                for key in ['dataLayerAddress', 'vlan', 'staticHost',
                            'networkAddress']:
                    if key in host_config:
                        resource_meta['host_' + key] = host_config[key]

                break

        return (1, resource_id, resource_meta)

    @staticmethod
    def _switch_port_receive_packets(statistic, resource_id,
                                     resource_meta, data):
        return _get_int_sample('receivePackets', statistic, resource_id,
                               resource_meta)

    @staticmethod
    def _switch_port_transmit_packets(statistic, resource_id,
                                      resource_meta, data):
        return _get_int_sample('transmitPackets', statistic, resource_id,
                               resource_meta)

    @staticmethod
    def _switch_port_receive_bytes(statistic, resource_id,
                                   resource_meta, data):
        return _get_int_sample('receiveBytes', statistic, resource_id,
                               resource_meta)

    @staticmethod
    def _switch_port_transmit_bytes(statistic, resource_id,
                                    resource_meta, data):
        return _get_int_sample('transmitBytes', statistic, resource_id,
                               resource_meta)

    @staticmethod
    def _switch_port_receive_drops(statistic, resource_id,
                                   resource_meta, data):
        return _get_int_sample('receiveDrops', statistic, resource_id,
                               resource_meta)

    @staticmethod
    def _switch_port_transmit_drops(statistic, resource_id,
                                    resource_meta, data):
        return _get_int_sample('transmitDrops', statistic, resource_id,
                               resource_meta)

    @staticmethod
    def _switch_port_receive_errors(statistic, resource_id,
                                    resource_meta, data):
        return _get_int_sample('receiveErrors', statistic, resource_id,
                               resource_meta)

    @staticmethod
    def _switch_port_transmit_errors(statistic, resource_id,
                                     resource_meta, data):
        return _get_int_sample('transmitErrors', statistic, resource_id,
                               resource_meta)

    @staticmethod
    def _switch_port_receive_frame_error(statistic, resource_id,
                                         resource_meta, data):
        return _get_int_sample('receiveFrameError', statistic, resource_id,
                               resource_meta)

    @staticmethod
    def _switch_port_receive_overrun_error(statistic, resource_id,
                                           resource_meta, data):
        return _get_int_sample('receiveOverRunError', statistic, resource_id,
                               resource_meta)

    @staticmethod
    def _switch_port_receive_crc_error(statistic, resource_id,
                                       resource_meta, data):
        return _get_int_sample('receiveCrcError', statistic, resource_id,
                               resource_meta)

    @staticmethod
    def _switch_port_collision_count(statistic, resource_id,
                                     resource_meta, data):
        return _get_int_sample('collisionCount', statistic, resource_id,
                               resource_meta)

    @staticmethod
    def _iter_table(extractor, data):
        for table_statistic in data['table']['tableStatistics']:
            for statistic in table_statistic['tableStatistic']:
                resource_meta = {'table_id': statistic['nodeTable']['id']}
                yield extractor(statistic,
                                table_statistic['node']['id'],
                                resource_meta)

    @staticmethod
    def _switch_table(statistic, resource_id, resource_meta):
        return (1, resource_id, resource_meta)

    @staticmethod
    def _switch_table_active_entries(statistic, resource_id,
                                     resource_meta):
        return _get_int_sample('activeCount', statistic, resource_id,
                               resource_meta)

    @staticmethod
    def _switch_table_lookup_packets(statistic, resource_id,
                                     resource_meta):
        return _get_int_sample('lookupCount', statistic, resource_id,
                               resource_meta)

    @staticmethod
    def _switch_table_matched_packets(statistic, resource_id,
                                      resource_meta):
        return _get_int_sample('matchedCount', statistic, resource_id,
                               resource_meta)

    @staticmethod
    def _iter_flow(extractor, data):
        for flow_statistic in data['flow']['flowStatistics']:
            for statistic in flow_statistic['flowStatistic']:
                resource_meta = {'flow_id': statistic['flow']['id'],
                                 'table_id': statistic['tableId']}
                for key, value in utils.dict_to_keyval(statistic['flow'],
                                                       'flow'):
                    resource_meta[key.replace('.', '_')] = value
                yield extractor(statistic,
                                flow_statistic['node']['id'],
                                resource_meta)

    @staticmethod
    def _switch_flow(statistic, resource_id, resource_meta):
        return (1, resource_id, resource_meta)

    @staticmethod
    def _switch_flow_duration_seconds(statistic, resource_id,
                                      resource_meta):
        return _get_int_sample('durationSeconds', statistic, resource_id,
                               resource_meta)

    @staticmethod
    def _switch_flow_duration_nanoseconds(statistic, resource_id,
                                          resource_meta):
        return _get_int_sample('durationNanoseconds', statistic, resource_id,
                               resource_meta)

    @staticmethod
    def _switch_flow_packets(statistic, resource_id, resource_meta):
        return _get_int_sample('packetCount', statistic, resource_id,
                               resource_meta)

    @staticmethod
    def _switch_flow_bytes(statistic, resource_id, resource_meta):
        return _get_int_sample('byteCount', statistic, resource_id,
                               resource_meta)

########NEW FILE########
__FILENAME__ = port
#
# Copyright 2014 NEC Corporation.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.


from ceilometer.network import statistics
from ceilometer import sample


class PortPollster(statistics._Base):

    meter_name = 'switch.port'
    meter_type = sample.TYPE_GAUGE
    meter_unit = 'port'


class PortPollsterReceivePackets(statistics._Base):

    meter_name = 'switch.port.receive.packets'
    meter_type = sample.TYPE_CUMULATIVE
    meter_unit = 'packet'


class PortPollsterTransmitPackets(statistics._Base):

    meter_name = 'switch.port.transmit.packets'
    meter_type = sample.TYPE_CUMULATIVE
    meter_unit = 'packet'


class PortPollsterReceiveBytes(statistics._Base):

    meter_name = 'switch.port.receive.bytes'
    meter_type = sample.TYPE_CUMULATIVE
    meter_unit = 'B'


class PortPollsterTransmitBytes(statistics._Base):

    meter_name = 'switch.port.transmit.bytes'
    meter_type = sample.TYPE_CUMULATIVE
    meter_unit = 'B'


class PortPollsterReceiveDrops(statistics._Base):

    meter_name = 'switch.port.receive.drops'
    meter_type = sample.TYPE_CUMULATIVE
    meter_unit = 'packet'


class PortPollsterTransmitDrops(statistics._Base):

    meter_name = 'switch.port.transmit.drops'
    meter_type = sample.TYPE_CUMULATIVE
    meter_unit = 'packet'


class PortPollsterReceiveErrors(statistics._Base):

    meter_name = 'switch.port.receive.errors'
    meter_type = sample.TYPE_CUMULATIVE
    meter_unit = 'packet'


class PortPollsterTransmitErrors(statistics._Base):

    meter_name = 'switch.port.transmit.errors'
    meter_type = sample.TYPE_CUMULATIVE
    meter_unit = 'packet'


class PortPollsterReceiveFrameErrors(statistics._Base):

    meter_name = 'switch.port.receive.frame_error'
    meter_type = sample.TYPE_CUMULATIVE
    meter_unit = 'packet'


class PortPollsterReceiveOverrunErrors(statistics._Base):

    meter_name = 'switch.port.receive.overrun_error'
    meter_type = sample.TYPE_CUMULATIVE
    meter_unit = 'packet'


class PortPollsterReceiveCRCErrors(statistics._Base):

    meter_name = 'switch.port.receive.crc_error'
    meter_type = sample.TYPE_CUMULATIVE
    meter_unit = 'packet'


class PortPollsterCollisionCount(statistics._Base):

    meter_name = 'switch.port.collision.count'
    meter_type = sample.TYPE_CUMULATIVE
    meter_unit = 'packet'

########NEW FILE########
__FILENAME__ = switch
#
# Copyright 2014 NEC Corporation.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.


from ceilometer.network import statistics
from ceilometer import sample


class SWPollster(statistics._Base):

    meter_name = 'switch'
    meter_type = sample.TYPE_GAUGE
    meter_unit = 'switch'

########NEW FILE########
__FILENAME__ = table
#
# Copyright 2014 NEC Corporation.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.


from ceilometer.network import statistics
from ceilometer import sample


class TablePollster(statistics._Base):

    meter_name = 'switch.table'
    meter_type = sample.TYPE_GAUGE
    meter_unit = 'table'


class TablePollsterActiveEntries(statistics._Base):

    meter_name = 'switch.table.active.entries'
    meter_type = sample.TYPE_GAUGE
    meter_unit = 'entry'


class TablePollsterLookupPackets(statistics._Base):

    meter_name = 'switch.table.lookup.packets'
    meter_type = sample.TYPE_GAUGE
    meter_unit = 'packet'


class TablePollsterMatchedPackets(statistics._Base):

    meter_name = 'switch.table.matched.packets'
    meter_type = sample.TYPE_GAUGE
    meter_unit = 'packet'

########NEW FILE########
__FILENAME__ = neutron_client
# Copyright (C) 2014 eNovance SAS <licensing@enovance.com>
#
# Author: Sylvain Afchain <sylvain.afchain@enovance.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import functools

from neutronclient.v2_0 import client as clientv20
from oslo.config import cfg

from ceilometer.openstack.common import log

cfg.CONF.import_group('service_credentials', 'ceilometer.service')

LOG = log.getLogger(__name__)


def logged(func):

    @functools.wraps(func)
    def with_logging(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            LOG.exception(e)
            raise

    return with_logging


class Client(object):
    """A client which gets information via python-neutronclient."""

    def __init__(self):
        conf = cfg.CONF.service_credentials
        params = {
            'insecure': conf.insecure,
            'ca_cert': conf.os_cacert,
            'username': conf.os_username,
            'password': conf.os_password,
            'auth_url': conf.os_auth_url,
            'region_name': conf.os_region_name,
            'endpoint_type': conf.os_endpoint_type
        }

        if conf.os_tenant_id:
            params['tenant_id'] = conf.os_tenant_id
        else:
            params['tenant_name'] = conf.os_tenant_name

        self.client = clientv20.Client(**params)

    @logged
    def network_get_all(self):
        """Returns all networks."""
        resp = self.client.list_networks()
        return resp.get('networks')

    @logged
    def port_get_all(self):
        resp = self.client.list_ports()
        return resp.get('ports')

########NEW FILE########
__FILENAME__ = notification
# -*- encoding: utf-8 -*-
#
# Copyright © 2012-2013 eNovance <licensing@enovance.com>
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from oslo.config import cfg
from stevedore import extension

from ceilometer.event import endpoint as event_endpoint
from ceilometer import messaging
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer.openstack.common import service as os_service
from ceilometer import pipeline


LOG = log.getLogger(__name__)


OPTS = [
    cfg.BoolOpt('ack_on_event_error',
                default=True,
                deprecated_group='collector',
                help='Acknowledge message when event persistence fails.'),
    cfg.BoolOpt('store_events',
                deprecated_group='collector',
                default=False,
                help='Save event details.'),
    cfg.MultiStrOpt('messaging_urls',
                    default=[],
                    help="Messaging URLs to listen for notifications. "
                         "Example: transport://user:pass@host1:port"
                         "[,hostN:portN]/virtual_host "
                         "(DEFAULT/transport_url is used if empty)"),
]

cfg.CONF.register_opts(OPTS, group="notification")


class NotificationService(os_service.Service):

    NOTIFICATION_NAMESPACE = 'ceilometer.notification'

    @classmethod
    def _get_notifications_manager(cls, pm):
        return extension.ExtensionManager(
            namespace=cls.NOTIFICATION_NAMESPACE,
            invoke_on_load=True,
            invoke_args=(pm, )
        )

    def start(self):
        super(NotificationService, self).start()
        self.pipeline_manager = pipeline.setup_pipeline()

        self.notification_manager = self._get_notifications_manager(
            self.pipeline_manager)
        if not list(self.notification_manager):
            LOG.warning(_('Failed to load any notification handlers for %s'),
                        self.NOTIFICATION_NAMESPACE)

        ack_on_error = cfg.CONF.notification.ack_on_event_error

        endpoints = []
        if cfg.CONF.notification.store_events:
            endpoints = [event_endpoint.EventsNotificationEndpoint()]

        targets = []
        for ext in self.notification_manager:
            handler = ext.obj
            LOG.debug(_('Event types from %(name)s: %(type)s'
                        ' (ack_on_error=%(error)s)') %
                      {'name': ext.name,
                       'type': ', '.join(handler.event_types),
                       'error': ack_on_error})
            targets.extend(handler.get_targets(cfg.CONF))
            endpoints.append(handler)

        urls = cfg.CONF.notification.messaging_urls or [None]
        self.listeners = []
        for url in urls:
            listener = messaging.get_notification_listener(targets,
                                                           endpoints,
                                                           url)
            listener.start()
            self.listeners.append(listener)

        # Add a dummy thread to have wait() working
        self.tg.add_timer(604800, lambda: None)

    def stop(self):
        map(lambda x: x.stop(), self.listeners)
        super(NotificationService, self).stop()

########NEW FILE########
__FILENAME__ = notifier
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 eNovance
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ceilometer.openstack.common import context as req_context
from ceilometer.openstack.common import log as logging
from ceilometer import pipeline
from ceilometer import transformer

from ceilometer.openstack.common.gettextutils import _
from stevedore import extension


LOG = logging.getLogger(__name__)


_notification_manager = None
_pipeline_manager = None


def _load_notification_manager():
    global _notification_manager, _pipeline_manager

    namespace = 'ceilometer.notification'

    LOG.debug(_('loading notification handlers from %s'), namespace)

    _notification_manager = extension.ExtensionManager(
        namespace=namespace,
        invoke_on_load=True,
        invoke_args=(_pipeline_manager, )
    )

    if not list(_notification_manager):
        LOG.warning(_('Failed to load any notification handlers for %s'),
                    namespace)


def _load_pipeline_manager():
    global _pipeline_manager

    _pipeline_manager = pipeline.setup_pipeline(
        transformer.TransformerExtensionManager(
            'ceilometer.transformer',
        ),
    )


def notify(context, message):
    """Sends a notification as a meter using Ceilometer pipelines."""
    if not _pipeline_manager:
        _load_pipeline_manager()
    if not _notification_manager:
        _load_notification_manager()
    _notification_manager.map_method(
        'to_samples_and_publish',
        context=context or req_context.get_admin_context(),
        notification=message)

########NEW FILE########
__FILENAME__ = nova_client
# -*- encoding: utf-8 -*-
#
# Author: John Tran <jhtran@att.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import functools

import novaclient
from novaclient.v1_1 import client as nova_client
from oslo.config import cfg

from ceilometer.openstack.common import log

cfg.CONF.import_group('service_credentials', 'ceilometer.service')

LOG = log.getLogger(__name__)


def logged(func):

    @functools.wraps(func)
    def with_logging(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            LOG.exception(e)
            raise

    return with_logging


class Client(object):
    """A client which gets information via python-novaclient."""

    def __init__(self):
        """Initialize a nova client object."""
        conf = cfg.CONF.service_credentials
        tenant = conf.os_tenant_id or conf.os_tenant_name
        self.nova_client = nova_client.Client(
            username=conf.os_username,
            api_key=conf.os_password,
            project_id=tenant,
            auth_url=conf.os_auth_url,
            region_name=conf.os_region_name,
            endpoint_type=conf.os_endpoint_type,
            cacert=conf.os_cacert,
            insecure=conf.insecure,
            no_cache=True)

    def _with_flavor_and_image(self, instances):
        for instance in instances:
            self._with_flavor(instance)
            self._with_image(instance)

        return instances

    def _with_flavor(self, instance):
        fid = instance.flavor['id']
        try:
            flavor = self.nova_client.flavors.get(fid)
        except novaclient.exceptions.NotFound:
            flavor = None

        attr_defaults = [('name', 'unknown-id-%s' % fid),
                         ('vcpus', 0), ('ram', 0), ('disk', 0),
                         ('ephemeral', 0)]

        for attr, default in attr_defaults:
            if not flavor:
                instance.flavor[attr] = default
                continue
            instance.flavor[attr] = getattr(flavor, attr, default)

    def _with_image(self, instance):
        try:
            iid = instance.image['id']
        except TypeError:
            instance.image = None
            instance.kernel_id = None
            instance.ramdisk_id = None
            return

        try:
            image = self.nova_client.images.get(iid)
        except novaclient.exceptions.NotFound:
            instance.image['name'] = 'unknown-id-%s' % iid
            instance.kernel_id = None
            instance.ramdisk_id = None
            return

        instance.image['name'] = getattr(image, 'name')
        image_metadata = getattr(image, 'metadata', None)

        for attr in ['kernel_id', 'ramdisk_id']:
            ameta = image_metadata.get(attr) if image_metadata else None
            setattr(instance, attr, ameta)

    @logged
    def instance_get_all_by_host(self, hostname):
        """Returns list of instances on particular host."""
        search_opts = {'host': hostname, 'all_tenants': True}
        return self._with_flavor_and_image(self.nova_client.servers.list(
            detailed=True,
            search_opts=search_opts))

    @logged
    def floating_ip_get_all(self):
        """Returns all floating ips."""
        return self.nova_client.floating_ips.list()

########NEW FILE########
__FILENAME__ = swift
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 eNovance
#
# Author: Guillaume Pernot <gpernot@praksys.org>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Common code for working with object stores
"""

from __future__ import absolute_import
import six.moves.urllib.parse as urlparse

from keystoneclient import exceptions
from oslo.config import cfg
from swiftclient import client as swift

from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer.openstack.common import timeutils
from ceilometer import plugin
from ceilometer import sample


LOG = log.getLogger(__name__)

OPTS = [
    cfg.StrOpt('reseller_prefix',
               default='AUTH_',
               help="Swift reseller prefix. Must be on par with "
               "reseller_prefix in proxy-server.conf."),
]

cfg.CONF.register_opts(OPTS)


class _Base(plugin.PollsterBase):

    CACHE_KEY_TENANT = 'tenants'
    METHOD = 'head'

    @property
    def CACHE_KEY_METHOD(self):
        return 'swift.%s_account' % self.METHOD

    def _iter_accounts(self, ksclient, cache):
        if self.CACHE_KEY_TENANT not in cache:
            cache[self.CACHE_KEY_TENANT] = ksclient.tenants.list()
        if self.CACHE_KEY_METHOD not in cache:
            cache[self.CACHE_KEY_METHOD] = list(self._get_account_info(
                                                ksclient, cache))
        return iter(cache[self.CACHE_KEY_METHOD])

    def _get_account_info(self, ksclient, cache):
        try:
            endpoint = ksclient.service_catalog.url_for(
                service_type='object-store',
                endpoint_type=cfg.CONF.service_credentials.os_endpoint_type)
        except exceptions.EndpointNotFound:
            LOG.debug(_("Swift endpoint not found"))
            raise StopIteration()

        for t in cache[self.CACHE_KEY_TENANT]:
            api_method = '%s_account' % self.METHOD
            yield (t.id, getattr(swift, api_method)
                                (self._neaten_url(endpoint, t.id),
                                 ksclient.auth_token))

    @staticmethod
    def _neaten_url(endpoint, tenant_id):
        """Transform the registered url to standard and valid format.
        """
        return urlparse.urljoin(endpoint,
                                '/v1/' + cfg.CONF.reseller_prefix + tenant_id)


class ObjectsPollster(_Base):
    """Iterate over all accounts, using keystone.
    """

    def get_samples(self, manager, cache, resources=None):
        for tenant, account in self._iter_accounts(manager.keystone, cache):
            yield sample.Sample(
                name='storage.objects',
                type=sample.TYPE_GAUGE,
                volume=int(account['x-account-object-count']),
                unit='object',
                user_id=None,
                project_id=tenant,
                resource_id=tenant,
                timestamp=timeutils.isotime(),
                resource_metadata=None,
            )


class ObjectsSizePollster(_Base):
    """Iterate over all accounts, using keystone.
    """

    def get_samples(self, manager, cache, resources=None):
        for tenant, account in self._iter_accounts(manager.keystone, cache):
            yield sample.Sample(
                name='storage.objects.size',
                type=sample.TYPE_GAUGE,
                volume=int(account['x-account-bytes-used']),
                unit='B',
                user_id=None,
                project_id=tenant,
                resource_id=tenant,
                timestamp=timeutils.isotime(),
                resource_metadata=None,
            )


class ObjectsContainersPollster(_Base):
    """Iterate over all accounts, using keystone.
    """

    def get_samples(self, manager, cache, resources=None):
        for tenant, account in self._iter_accounts(manager.keystone, cache):
            yield sample.Sample(
                name='storage.objects.containers',
                type=sample.TYPE_GAUGE,
                volume=int(account['x-account-container-count']),
                unit='container',
                user_id=None,
                project_id=tenant,
                resource_id=tenant,
                timestamp=timeutils.isotime(),
                resource_metadata=None,
            )


class ContainersObjectsPollster(_Base):
    """Get info about containers using Swift API
    """

    METHOD = 'get'

    def get_samples(self, manager, cache, resources=None):
        for project, account in self._iter_accounts(manager.keystone, cache):
            containers_info = account[1]
            for container in containers_info:
                yield sample.Sample(
                    name='storage.containers.objects',
                    type=sample.TYPE_GAUGE,
                    volume=int(container['count']),
                    unit='object',
                    user_id=None,
                    project_id=project,
                    resource_id=project + '/' + container['name'],
                    timestamp=timeutils.isotime(),
                    resource_metadata=None,
                )


class ContainersSizePollster(_Base):
    """Get info about containers using Swift API
    """

    METHOD = 'get'

    def get_samples(self, manager, cache, resources=None):
        for project, account in self._iter_accounts(manager.keystone, cache):
            containers_info = account[1]
            for container in containers_info:
                yield sample.Sample(
                    name='storage.containers.objects.size',
                    type=sample.TYPE_GAUGE,
                    volume=int(container['bytes']),
                    unit='B',
                    user_id=None,
                    project_id=project,
                    resource_id=project + '/' + container['name'],
                    timestamp=timeutils.isotime(),
                    resource_metadata=None,
                )

########NEW FILE########
__FILENAME__ = swift_middleware
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 eNovance <licensing@enovance.com>
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied.
# See the License for the specific language governing permissions and
# limitations under the License.


"""
Ceilometer Middleware for Swift Proxy

Configuration:

In /etc/swift/proxy-server.conf on the main pipeline add "ceilometer" just
before "proxy-server" and add the following filter in the file:

[filter:ceilometer]
use = egg:ceilometer#swift

# Some optional configuration
# this allow to publish additional metadata
metadata_headers = X-TEST

# Set reseller prefix (defaults to "AUTH_" if not set)
reseller_prefix = AUTH_
"""

from __future__ import absolute_import

from swift.common import utils

try:
    # Swift >= 1.7.5
    import swift.common.swob
    REQUEST = swift.common.swob
except ImportError:
    import webob
    REQUEST = webob

try:
    # Swift > 1.7.5 ... module exists but doesn't contain class.
    from swift.common.utils import InputProxy
except ImportError:
    # Swift <= 1.7.5 ... module exists and has class.
    from swift.common.middleware.proxy_logging import InputProxy

from ceilometer.openstack.common import context
from ceilometer.openstack.common import timeutils
from ceilometer import pipeline
from ceilometer import sample
from ceilometer import service


class CeilometerMiddleware(object):
    """Ceilometer middleware used for counting requests."""

    def __init__(self, app, conf):
        self.app = app
        self.logger = utils.get_logger(conf, log_route='ceilometer')

        self.metadata_headers = [h.strip().replace('-', '_').lower()
                                 for h in conf.get(
                                     "metadata_headers",
                                     "").split(",") if h.strip()]

        service.prepare_service([])

        self.pipeline_manager = pipeline.setup_pipeline()
        self.reseller_prefix = conf.get('reseller_prefix', 'AUTH_')
        if self.reseller_prefix and self.reseller_prefix[-1] != '_':
            self.reseller_prefix += '_'

    def __call__(self, env, start_response):
        start_response_args = [None]
        input_proxy = InputProxy(env['wsgi.input'])
        env['wsgi.input'] = input_proxy

        def my_start_response(status, headers, exc_info=None):
            start_response_args[0] = (status, list(headers), exc_info)

        def iter_response(iterable):
            if start_response_args[0]:
                start_response(*start_response_args[0])
            bytes_sent = 0
            try:
                for chunk in iterable:
                    if chunk:
                        bytes_sent += len(chunk)
                    yield chunk
            finally:
                try:
                    self.publish_sample(env,
                                        input_proxy.bytes_received,
                                        bytes_sent)
                except Exception:
                    self.logger.exception('Failed to publish samples')

        try:
            iterable = self.app(env, my_start_response)
        except Exception:
            self.publish_sample(env, input_proxy.bytes_received, 0)
            raise
        else:
            return iter_response(iterable)

    def publish_sample(self, env, bytes_received, bytes_sent):
        req = REQUEST.Request(env)
        try:
            version, account, container, obj = utils.split_path(req.path, 2,
                                                                4, True)
        except ValueError:
            return
        now = timeutils.utcnow().isoformat()

        resource_metadata = {
            "path": req.path,
            "version": version,
            "container": container,
            "object": obj,
        }

        for header in self.metadata_headers:
            if header.upper() in req.headers:
                resource_metadata['http_header_%s' % header] = req.headers.get(
                    header.upper())

        with self.pipeline_manager.publisher(
                context.get_admin_context()) as publisher:
            if bytes_received:
                publisher([sample.Sample(
                    name='storage.objects.incoming.bytes',
                    type=sample.TYPE_DELTA,
                    unit='B',
                    volume=bytes_received,
                    user_id=env.get('HTTP_X_USER_ID'),
                    project_id=env.get('HTTP_X_TENANT_ID'),
                    resource_id=account.partition(self.reseller_prefix)[2],
                    timestamp=now,
                    resource_metadata=resource_metadata)])

            if bytes_sent:
                publisher([sample.Sample(
                    name='storage.objects.outgoing.bytes',
                    type=sample.TYPE_DELTA,
                    unit='B',
                    volume=bytes_sent,
                    user_id=env.get('HTTP_X_USER_ID'),
                    project_id=env.get('HTTP_X_TENANT_ID'),
                    resource_id=account.partition(self.reseller_prefix)[2],
                    timestamp=now,
                    resource_metadata=resource_metadata)])

            # publish the event for each request
            # request method will be recorded in the metadata
            resource_metadata['method'] = req.method.lower()
            publisher([sample.Sample(
                name='storage.api.request',
                type=sample.TYPE_DELTA,
                unit='request',
                volume=1,
                user_id=env.get('HTTP_X_USER_ID'),
                project_id=env.get('HTTP_X_TENANT_ID'),
                resource_id=account.partition(self.reseller_prefix)[2],
                timestamp=now,
                resource_metadata=resource_metadata)])


def filter_factory(global_conf, **local_conf):
    conf = global_conf.copy()
    conf.update(local_conf)

    def ceilometer_filter(app):
        return CeilometerMiddleware(app, conf)
    return ceilometer_filter

########NEW FILE########
__FILENAME__ = generator
# Copyright 2012 SINA Corporation
# Copyright 2014 Cisco Systems, Inc.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

"""Extracts OpenStack config option info from module(s)."""

from __future__ import print_function

import argparse
import imp
import os
import re
import socket
import sys
import textwrap

from oslo.config import cfg
import six
import stevedore.named

from ceilometer.openstack.common import gettextutils
from ceilometer.openstack.common import importutils

gettextutils.install('ceilometer')

STROPT = "StrOpt"
BOOLOPT = "BoolOpt"
INTOPT = "IntOpt"
FLOATOPT = "FloatOpt"
LISTOPT = "ListOpt"
DICTOPT = "DictOpt"
MULTISTROPT = "MultiStrOpt"

OPT_TYPES = {
    STROPT: 'string value',
    BOOLOPT: 'boolean value',
    INTOPT: 'integer value',
    FLOATOPT: 'floating point value',
    LISTOPT: 'list value',
    DICTOPT: 'dict value',
    MULTISTROPT: 'multi valued',
}

OPTION_REGEX = re.compile(r"(%s)" % "|".join([STROPT, BOOLOPT, INTOPT,
                                              FLOATOPT, LISTOPT, DICTOPT,
                                              MULTISTROPT]))

PY_EXT = ".py"
BASEDIR = os.path.abspath(os.path.join(os.path.dirname(__file__),
                                       "../../../../"))
WORDWRAP_WIDTH = 60


def raise_extension_exception(extmanager, ep, err):
    raise


def generate(argv):
    parser = argparse.ArgumentParser(
        description='generate sample configuration file',
    )
    parser.add_argument('-m', dest='modules', action='append')
    parser.add_argument('-l', dest='libraries', action='append')
    parser.add_argument('srcfiles', nargs='*')
    parsed_args = parser.parse_args(argv)

    mods_by_pkg = dict()
    for filepath in parsed_args.srcfiles:
        pkg_name = filepath.split(os.sep)[1]
        mod_str = '.'.join(['.'.join(filepath.split(os.sep)[:-1]),
                            os.path.basename(filepath).split('.')[0]])
        mods_by_pkg.setdefault(pkg_name, list()).append(mod_str)
    # NOTE(lzyeval): place top level modules before packages
    pkg_names = sorted(pkg for pkg in mods_by_pkg if pkg.endswith(PY_EXT))
    ext_names = sorted(pkg for pkg in mods_by_pkg if pkg not in pkg_names)
    pkg_names.extend(ext_names)

    # opts_by_group is a mapping of group name to an options list
    # The options list is a list of (module, options) tuples
    opts_by_group = {'DEFAULT': []}

    if parsed_args.modules:
        for module_name in parsed_args.modules:
            module = _import_module(module_name)
            if module:
                for group, opts in _list_opts(module):
                    opts_by_group.setdefault(group, []).append((module_name,
                                                                opts))

    # Look for entry points defined in libraries (or applications) for
    # option discovery, and include their return values in the output.
    #
    # Each entry point should be a function returning an iterable
    # of pairs with the group name (or None for the default group)
    # and the list of Opt instances for that group.
    if parsed_args.libraries:
        loader = stevedore.named.NamedExtensionManager(
            'oslo.config.opts',
            names=list(set(parsed_args.libraries)),
            invoke_on_load=False,
            on_load_failure_callback=raise_extension_exception
        )
        for ext in loader:
            for group, opts in ext.plugin():
                opt_list = opts_by_group.setdefault(group or 'DEFAULT', [])
                opt_list.append((ext.name, opts))

    for pkg_name in pkg_names:
        mods = mods_by_pkg.get(pkg_name)
        mods.sort()
        for mod_str in mods:
            if mod_str.endswith('.__init__'):
                mod_str = mod_str[:mod_str.rfind(".")]

            mod_obj = _import_module(mod_str)
            if not mod_obj:
                raise RuntimeError("Unable to import module %s" % mod_str)

            for group, opts in _list_opts(mod_obj):
                opts_by_group.setdefault(group, []).append((mod_str, opts))

    print_group_opts('DEFAULT', opts_by_group.pop('DEFAULT', []))
    for group in sorted(opts_by_group.keys()):
        print_group_opts(group, opts_by_group[group])


def _import_module(mod_str):
    try:
        if mod_str.startswith('bin.'):
            imp.load_source(mod_str[4:], os.path.join('bin', mod_str[4:]))
            return sys.modules[mod_str[4:]]
        else:
            return importutils.import_module(mod_str)
    except Exception as e:
        sys.stderr.write("Error importing module %s: %s\n" % (mod_str, str(e)))
        return None


def _is_in_group(opt, group):
    "Check if opt is in group."
    for value in group._opts.values():
        # NOTE(llu): Temporary workaround for bug #1262148, wait until
        # newly released oslo.config support '==' operator.
        if not(value['opt'] != opt):
            return True
    return False


def _guess_groups(opt, mod_obj):
    # is it in the DEFAULT group?
    if _is_in_group(opt, cfg.CONF):
        return 'DEFAULT'

    # what other groups is it in?
    for value in cfg.CONF.values():
        if isinstance(value, cfg.CONF.GroupAttr):
            if _is_in_group(opt, value._group):
                return value._group.name

    raise RuntimeError(
        "Unable to find group for option %s, "
        "maybe it's defined twice in the same group?"
        % opt.name
    )


def _list_opts(obj):
    def is_opt(o):
        return (isinstance(o, cfg.Opt) and
                not isinstance(o, cfg.SubCommandOpt))

    opts = list()
    for attr_str in dir(obj):
        attr_obj = getattr(obj, attr_str)
        if is_opt(attr_obj):
            opts.append(attr_obj)
        elif (isinstance(attr_obj, list) and
              all(map(lambda x: is_opt(x), attr_obj))):
            opts.extend(attr_obj)

    ret = {}
    for opt in opts:
        ret.setdefault(_guess_groups(opt, obj), []).append(opt)
    return ret.items()


def print_group_opts(group, opts_by_module):
    print("[%s]" % group)
    print('')
    for mod, opts in opts_by_module:
        print('#')
        print('# Options defined in %s' % mod)
        print('#')
        print('')
        for opt in opts:
            _print_opt(opt)
        print('')


def _get_my_ip():
    try:
        csock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        csock.connect(('8.8.8.8', 80))
        (addr, port) = csock.getsockname()
        csock.close()
        return addr
    except socket.error:
        return None


def _sanitize_default(name, value):
    """Set up a reasonably sensible default for pybasedir, my_ip and host."""
    if value.startswith(sys.prefix):
        # NOTE(jd) Don't use os.path.join, because it is likely to think the
        # second part is an absolute pathname and therefore drop the first
        # part.
        value = os.path.normpath("/usr/" + value[len(sys.prefix):])
    elif value.startswith(BASEDIR):
        return value.replace(BASEDIR, '/usr/lib/python/site-packages')
    elif BASEDIR in value:
        return value.replace(BASEDIR, '')
    elif value == _get_my_ip():
        return '10.0.0.1'
    elif value in (socket.gethostname(), socket.getfqdn()) and 'host' in name:
        return 'ceilometer'
    elif value.strip() != value:
        return '"%s"' % value
    return value


def _print_opt(opt):
    opt_name, opt_default, opt_help = opt.dest, opt.default, opt.help
    if not opt_help:
        sys.stderr.write('WARNING: "%s" is missing help string.\n' % opt_name)
        opt_help = ""
    opt_type = None
    try:
        opt_type = OPTION_REGEX.search(str(type(opt))).group(0)
    except (ValueError, AttributeError) as err:
        sys.stderr.write("%s\n" % str(err))
        sys.exit(1)
    opt_help = u'%s (%s)' % (opt_help,
                             OPT_TYPES[opt_type])
    print('#', "\n# ".join(textwrap.wrap(opt_help, WORDWRAP_WIDTH)))
    if opt.deprecated_opts:
        for deprecated_opt in opt.deprecated_opts:
            if deprecated_opt.name:
                deprecated_group = (deprecated_opt.group if
                                    deprecated_opt.group else "DEFAULT")
                print('# Deprecated group/name - [%s]/%s' %
                      (deprecated_group,
                       deprecated_opt.name))
    try:
        if opt_default is None:
            print('#%s=<None>' % opt_name)
        elif opt_type == STROPT:
            assert(isinstance(opt_default, six.string_types))
            print('#%s=%s' % (opt_name, _sanitize_default(opt_name,
                                                          opt_default)))
        elif opt_type == BOOLOPT:
            assert(isinstance(opt_default, bool))
            print('#%s=%s' % (opt_name, str(opt_default).lower()))
        elif opt_type == INTOPT:
            assert(isinstance(opt_default, int) and
                   not isinstance(opt_default, bool))
            print('#%s=%s' % (opt_name, opt_default))
        elif opt_type == FLOATOPT:
            assert(isinstance(opt_default, float))
            print('#%s=%s' % (opt_name, opt_default))
        elif opt_type == LISTOPT:
            assert(isinstance(opt_default, list))
            print('#%s=%s' % (opt_name, ','.join(opt_default)))
        elif opt_type == DICTOPT:
            assert(isinstance(opt_default, dict))
            opt_default_strlist = [str(key) + ':' + str(value)
                                   for (key, value) in opt_default.items()]
            print('#%s=%s' % (opt_name, ','.join(opt_default_strlist)))
        elif opt_type == MULTISTROPT:
            assert(isinstance(opt_default, list))
            if not opt_default:
                opt_default = ['']
            for default in opt_default:
                print('#%s=%s' % (opt_name, default))
        print('')
    except Exception:
        sys.stderr.write('Error in option "%s"\n' % opt_name)
        sys.exit(1)


def main():
    generate(sys.argv[1:])

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = context
# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Simple class that stores security context information in the web request.

Projects should subclass this class if they wish to enhance the request
context or provide additional information in their specific WSGI pipeline.
"""

import itertools
import uuid


def generate_request_id():
    return 'req-%s' % str(uuid.uuid4())


class RequestContext(object):

    """Helper class to represent useful information about a request context.

    Stores information about the security context under which the user
    accesses the system, as well as additional request information.
    """

    user_idt_format = '{user} {tenant} {domain} {user_domain} {p_domain}'

    def __init__(self, auth_token=None, user=None, tenant=None, domain=None,
                 user_domain=None, project_domain=None, is_admin=False,
                 read_only=False, show_deleted=False, request_id=None,
                 instance_uuid=None):
        self.auth_token = auth_token
        self.user = user
        self.tenant = tenant
        self.domain = domain
        self.user_domain = user_domain
        self.project_domain = project_domain
        self.is_admin = is_admin
        self.read_only = read_only
        self.show_deleted = show_deleted
        self.instance_uuid = instance_uuid
        if not request_id:
            request_id = generate_request_id()
        self.request_id = request_id

    def to_dict(self):
        user_idt = (
            self.user_idt_format.format(user=self.user or '-',
                                        tenant=self.tenant or '-',
                                        domain=self.domain or '-',
                                        user_domain=self.user_domain or '-',
                                        p_domain=self.project_domain or '-'))

        return {'user': self.user,
                'tenant': self.tenant,
                'domain': self.domain,
                'user_domain': self.user_domain,
                'project_domain': self.project_domain,
                'is_admin': self.is_admin,
                'read_only': self.read_only,
                'show_deleted': self.show_deleted,
                'auth_token': self.auth_token,
                'request_id': self.request_id,
                'instance_uuid': self.instance_uuid,
                'user_identity': user_idt}


def get_admin_context(show_deleted=False):
    context = RequestContext(None,
                             tenant=None,
                             is_admin=True,
                             show_deleted=show_deleted)
    return context


def get_context_from_function_and_args(function, args, kwargs):
    """Find an arg of type RequestContext and return it.

       This is useful in a couple of decorators where we don't
       know much about the function we're wrapping.
    """

    for arg in itertools.chain(kwargs.values(), args):
        if isinstance(arg, RequestContext):
            return arg

    return None


def is_user_context(context):
    """Indicates if the request context is a normal user."""
    if not context:
        return False
    if context.is_admin:
        return False
    if not context.user_id or not context.project_id:
        return False
    return True

########NEW FILE########
__FILENAME__ = api
# Copyright (c) 2013 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Multiple DB API backend support.

A DB backend module should implement a method named 'get_backend' which
takes no arguments.  The method can return any object that implements DB
API methods.
"""

import functools
import logging
import threading
import time

from ceilometer.openstack.common.db import exception
from ceilometer.openstack.common.gettextutils import _LE
from ceilometer.openstack.common import importutils


LOG = logging.getLogger(__name__)


def safe_for_db_retry(f):
    """Enable db-retry for decorated function, if config option enabled."""
    f.__dict__['enable_retry'] = True
    return f


class wrap_db_retry(object):
    """Retry db.api methods, if DBConnectionError() raised

    Retry decorated db.api methods. If we enabled `use_db_reconnect`
    in config, this decorator will be applied to all db.api functions,
    marked with @safe_for_db_retry decorator.
    Decorator catchs DBConnectionError() and retries function in a
    loop until it succeeds, or until maximum retries count will be reached.
    """

    def __init__(self, retry_interval, max_retries, inc_retry_interval,
                 max_retry_interval):
        super(wrap_db_retry, self).__init__()

        self.retry_interval = retry_interval
        self.max_retries = max_retries
        self.inc_retry_interval = inc_retry_interval
        self.max_retry_interval = max_retry_interval

    def __call__(self, f):
        @functools.wraps(f)
        def wrapper(*args, **kwargs):
            next_interval = self.retry_interval
            remaining = self.max_retries

            while True:
                try:
                    return f(*args, **kwargs)
                except exception.DBConnectionError as e:
                    if remaining == 0:
                        LOG.exception(_LE('DB exceeded retry limit.'))
                        raise exception.DBError(e)
                    if remaining != -1:
                        remaining -= 1
                        LOG.exception(_LE('DB connection error.'))
                    # NOTE(vsergeyev): We are using patched time module, so
                    #                  this effectively yields the execution
                    #                  context to another green thread.
                    time.sleep(next_interval)
                    if self.inc_retry_interval:
                        next_interval = min(
                            next_interval * 2,
                            self.max_retry_interval
                        )
        return wrapper


class DBAPI(object):
    def __init__(self, backend_name, backend_mapping=None, lazy=False,
                 **kwargs):
        """Initialize the chosen DB API backend.

        :param backend_name: name of the backend to load
        :type backend_name: str

        :param backend_mapping: backend name -> module/class to load mapping
        :type backend_mapping: dict

        :param lazy: load the DB backend lazily on the first DB API method call
        :type lazy: bool

        Keyword arguments:

        :keyword use_db_reconnect: retry DB transactions on disconnect or not
        :type use_db_reconnect: bool

        :keyword retry_interval: seconds between transaction retries
        :type retry_interval: int

        :keyword inc_retry_interval: increase retry interval or not
        :type inc_retry_interval: bool

        :keyword max_retry_interval: max interval value between retries
        :type max_retry_interval: int

        :keyword max_retries: max number of retries before an error is raised
        :type max_retries: int

        """

        self._backend = None
        self._backend_name = backend_name
        self._backend_mapping = backend_mapping or {}
        self._lock = threading.Lock()

        if not lazy:
            self._load_backend()

        self.use_db_reconnect = kwargs.get('use_db_reconnect', False)
        self.retry_interval = kwargs.get('retry_interval', 1)
        self.inc_retry_interval = kwargs.get('inc_retry_interval', True)
        self.max_retry_interval = kwargs.get('max_retry_interval', 10)
        self.max_retries = kwargs.get('max_retries', 20)

    def _load_backend(self):
        with self._lock:
            if not self._backend:
                # Import the untranslated name if we don't have a mapping
                backend_path = self._backend_mapping.get(self._backend_name,
                                                         self._backend_name)
                backend_mod = importutils.import_module(backend_path)
                self._backend = backend_mod.get_backend()

    def __getattr__(self, key):
        if not self._backend:
            self._load_backend()

        attr = getattr(self._backend, key)
        if not hasattr(attr, '__call__'):
            return attr
        # NOTE(vsergeyev): If `use_db_reconnect` option is set to True, retry
        #                  DB API methods, decorated with @safe_for_db_retry
        #                  on disconnect.
        if self.use_db_reconnect and hasattr(attr, 'enable_retry'):
            attr = wrap_db_retry(
                retry_interval=self.retry_interval,
                max_retries=self.max_retries,
                inc_retry_interval=self.inc_retry_interval,
                max_retry_interval=self.max_retry_interval)(attr)

        return attr

########NEW FILE########
__FILENAME__ = exception
# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""DB related custom exceptions."""

import six

from ceilometer.openstack.common.gettextutils import _


class DBError(Exception):
    """Wraps an implementation specific exception."""
    def __init__(self, inner_exception=None):
        self.inner_exception = inner_exception
        super(DBError, self).__init__(six.text_type(inner_exception))


class DBDuplicateEntry(DBError):
    """Wraps an implementation specific exception."""
    def __init__(self, columns=[], inner_exception=None):
        self.columns = columns
        super(DBDuplicateEntry, self).__init__(inner_exception)


class DBDeadlock(DBError):
    def __init__(self, inner_exception=None):
        super(DBDeadlock, self).__init__(inner_exception)


class DBInvalidUnicodeParameter(Exception):
    message = _("Invalid Parameter: "
                "Unicode is not supported by the current database.")


class DbMigrationError(DBError):
    """Wraps migration specific exception."""
    def __init__(self, message=None):
        super(DbMigrationError, self).__init__(message)


class DBConnectionError(DBError):
    """Wraps connection specific exception."""
    pass

########NEW FILE########
__FILENAME__ = options
#  Licensed under the Apache License, Version 2.0 (the "License"); you may
#  not use this file except in compliance with the License. You may obtain
#  a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#  WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#  License for the specific language governing permissions and limitations
#  under the License.

import copy

from oslo.config import cfg


database_opts = [
    cfg.StrOpt('sqlite_db',
               deprecated_group='DEFAULT',
               default='ceilometer.sqlite',
               help='The file name to use with SQLite'),
    cfg.BoolOpt('sqlite_synchronous',
                deprecated_group='DEFAULT',
                default=True,
                help='If True, SQLite uses synchronous mode'),
    cfg.StrOpt('backend',
               default='sqlalchemy',
               deprecated_name='db_backend',
               deprecated_group='DEFAULT',
               help='The backend to use for db'),
    cfg.StrOpt('connection',
               help='The SQLAlchemy connection string used to connect to the '
                    'database',
               secret=True,
               deprecated_opts=[cfg.DeprecatedOpt('sql_connection',
                                                  group='DEFAULT'),
                                cfg.DeprecatedOpt('sql_connection',
                                                  group='DATABASE'),
                                cfg.DeprecatedOpt('connection',
                                                  group='sql'), ]),
    cfg.StrOpt('mysql_sql_mode',
               default='TRADITIONAL',
               help='The SQL mode to be used for MySQL sessions. '
                    'This option, including the default, overrides any '
                    'server-set SQL mode. To use whatever SQL mode '
                    'is set by the server configuration, '
                    'set this to no value. Example: mysql_sql_mode='),
    cfg.IntOpt('idle_timeout',
               default=3600,
               deprecated_opts=[cfg.DeprecatedOpt('sql_idle_timeout',
                                                  group='DEFAULT'),
                                cfg.DeprecatedOpt('sql_idle_timeout',
                                                  group='DATABASE'),
                                cfg.DeprecatedOpt('idle_timeout',
                                                  group='sql')],
               help='Timeout before idle sql connections are reaped'),
    cfg.IntOpt('min_pool_size',
               default=1,
               deprecated_opts=[cfg.DeprecatedOpt('sql_min_pool_size',
                                                  group='DEFAULT'),
                                cfg.DeprecatedOpt('sql_min_pool_size',
                                                  group='DATABASE')],
               help='Minimum number of SQL connections to keep open in a '
                    'pool'),
    cfg.IntOpt('max_pool_size',
               default=None,
               deprecated_opts=[cfg.DeprecatedOpt('sql_max_pool_size',
                                                  group='DEFAULT'),
                                cfg.DeprecatedOpt('sql_max_pool_size',
                                                  group='DATABASE')],
               help='Maximum number of SQL connections to keep open in a '
                    'pool'),
    cfg.IntOpt('max_retries',
               default=10,
               deprecated_opts=[cfg.DeprecatedOpt('sql_max_retries',
                                                  group='DEFAULT'),
                                cfg.DeprecatedOpt('sql_max_retries',
                                                  group='DATABASE')],
               help='Maximum db connection retries during startup. '
                    '(setting -1 implies an infinite retry count)'),
    cfg.IntOpt('retry_interval',
               default=10,
               deprecated_opts=[cfg.DeprecatedOpt('sql_retry_interval',
                                                  group='DEFAULT'),
                                cfg.DeprecatedOpt('reconnect_interval',
                                                  group='DATABASE')],
               help='Interval between retries of opening a sql connection'),
    cfg.IntOpt('max_overflow',
               default=None,
               deprecated_opts=[cfg.DeprecatedOpt('sql_max_overflow',
                                                  group='DEFAULT'),
                                cfg.DeprecatedOpt('sqlalchemy_max_overflow',
                                                  group='DATABASE')],
               help='If set, use this value for max_overflow with sqlalchemy'),
    cfg.IntOpt('connection_debug',
               default=0,
               deprecated_opts=[cfg.DeprecatedOpt('sql_connection_debug',
                                                  group='DEFAULT')],
               help='Verbosity of SQL debugging information. 0=None, '
                    '100=Everything'),
    cfg.BoolOpt('connection_trace',
                default=False,
                deprecated_opts=[cfg.DeprecatedOpt('sql_connection_trace',
                                                   group='DEFAULT')],
                help='Add python stack traces to SQL as comment strings'),
    cfg.IntOpt('pool_timeout',
               default=None,
               deprecated_opts=[cfg.DeprecatedOpt('sqlalchemy_pool_timeout',
                                                  group='DATABASE')],
               help='If set, use this value for pool_timeout with sqlalchemy'),
    cfg.BoolOpt('use_db_reconnect',
                default=False,
                help='Enable the experimental use of database reconnect '
                     'on connection lost'),
    cfg.IntOpt('db_retry_interval',
               default=1,
               help='seconds between db connection retries'),
    cfg.BoolOpt('db_inc_retry_interval',
                default=True,
                help='Whether to increase interval between db connection '
                     'retries, up to db_max_retry_interval'),
    cfg.IntOpt('db_max_retry_interval',
               default=10,
               help='max seconds between db connection retries, if '
                    'db_inc_retry_interval is enabled'),
    cfg.IntOpt('db_max_retries',
               default=20,
               help='maximum db connection retries before error is raised. '
                    '(setting -1 implies an infinite retry count)'),
]

CONF = cfg.CONF
CONF.register_opts(database_opts, 'database')


def set_defaults(sql_connection, sqlite_db, max_pool_size=None,
                 max_overflow=None, pool_timeout=None):
    """Set defaults for configuration variables."""
    cfg.set_defaults(database_opts,
                     connection=sql_connection,
                     sqlite_db=sqlite_db)
    # Update the QueuePool defaults
    if max_pool_size is not None:
        cfg.set_defaults(database_opts,
                         max_pool_size=max_pool_size)
    if max_overflow is not None:
        cfg.set_defaults(database_opts,
                         max_overflow=max_overflow)
    if pool_timeout is not None:
        cfg.set_defaults(database_opts,
                         pool_timeout=pool_timeout)


def list_opts():
    """Returns a list of oslo.config options available in the library.

    The returned list includes all oslo.config options which may be registered
    at runtime by the library.

    Each element of the list is a tuple. The first element is the name of the
    group under which the list of elements in the second element will be
    registered. A group name of None corresponds to the [DEFAULT] group in
    config files.

    The purpose of this is to allow tools like the Oslo sample config file
    generator to discover the options exposed to users by this library.

    :returns: a list of (group_name, opts) tuples
    """
    return [('database', copy.deepcopy(database_opts))]

########NEW FILE########
__FILENAME__ = migration
# coding: utf-8
#
# Copyright (c) 2013 OpenStack Foundation
# All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
# Base on code in migrate/changeset/databases/sqlite.py which is under
# the following license:
#
# The MIT License
#
# Copyright (c) 2009 Evan Rosson, Jan Dittberner, Domen Kožar
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.

import os
import re

from migrate.changeset import ansisql
from migrate.changeset.databases import sqlite
from migrate import exceptions as versioning_exceptions
from migrate.versioning import api as versioning_api
from migrate.versioning.repository import Repository
import sqlalchemy
from sqlalchemy.schema import UniqueConstraint

from ceilometer.openstack.common.db import exception
from ceilometer.openstack.common.gettextutils import _


def _get_unique_constraints(self, table):
    """Retrieve information about existing unique constraints of the table

    This feature is needed for _recreate_table() to work properly.
    Unfortunately, it's not available in sqlalchemy 0.7.x/0.8.x.

    """

    data = table.metadata.bind.execute(
        """SELECT sql
           FROM sqlite_master
           WHERE
               type='table' AND
               name=:table_name""",
        table_name=table.name
    ).fetchone()[0]

    UNIQUE_PATTERN = "CONSTRAINT (\w+) UNIQUE \(([^\)]+)\)"
    return [
        UniqueConstraint(
            *[getattr(table.columns, c.strip(' "')) for c in cols.split(",")],
            name=name
        )
        for name, cols in re.findall(UNIQUE_PATTERN, data)
    ]


def _recreate_table(self, table, column=None, delta=None, omit_uniques=None):
    """Recreate the table properly

    Unlike the corresponding original method of sqlalchemy-migrate this one
    doesn't drop existing unique constraints when creating a new one.

    """

    table_name = self.preparer.format_table(table)

    # we remove all indexes so as not to have
    # problems during copy and re-create
    for index in table.indexes:
        index.drop()

    # reflect existing unique constraints
    for uc in self._get_unique_constraints(table):
        table.append_constraint(uc)
    # omit given unique constraints when creating a new table if required
    table.constraints = set([
        cons for cons in table.constraints
        if omit_uniques is None or cons.name not in omit_uniques
    ])

    self.append('ALTER TABLE %s RENAME TO migration_tmp' % table_name)
    self.execute()

    insertion_string = self._modify_table(table, column, delta)

    table.create(bind=self.connection)
    self.append(insertion_string % {'table_name': table_name})
    self.execute()
    self.append('DROP TABLE migration_tmp')
    self.execute()


def _visit_migrate_unique_constraint(self, *p, **k):
    """Drop the given unique constraint

    The corresponding original method of sqlalchemy-migrate just
    raises NotImplemented error

    """

    self.recreate_table(p[0].table, omit_uniques=[p[0].name])


def patch_migrate():
    """A workaround for SQLite's inability to alter things

    SQLite abilities to alter tables are very limited (please read
    http://www.sqlite.org/lang_altertable.html for more details).
    E. g. one can't drop a column or a constraint in SQLite. The
    workaround for this is to recreate the original table omitting
    the corresponding constraint (or column).

    sqlalchemy-migrate library has recreate_table() method that
    implements this workaround, but it does it wrong:

        - information about unique constraints of a table
          is not retrieved. So if you have a table with one
          unique constraint and a migration adding another one
          you will end up with a table that has only the
          latter unique constraint, and the former will be lost

        - dropping of unique constraints is not supported at all

    The proper way to fix this is to provide a pull-request to
    sqlalchemy-migrate, but the project seems to be dead. So we
    can go on with monkey-patching of the lib at least for now.

    """

    # this patch is needed to ensure that recreate_table() doesn't drop
    # existing unique constraints of the table when creating a new one
    helper_cls = sqlite.SQLiteHelper
    helper_cls.recreate_table = _recreate_table
    helper_cls._get_unique_constraints = _get_unique_constraints

    # this patch is needed to be able to drop existing unique constraints
    constraint_cls = sqlite.SQLiteConstraintDropper
    constraint_cls.visit_migrate_unique_constraint = \
        _visit_migrate_unique_constraint
    constraint_cls.__bases__ = (ansisql.ANSIColumnDropper,
                                sqlite.SQLiteConstraintGenerator)


def db_sync(engine, abs_path, version=None, init_version=0, sanity_check=True):
    """Upgrade or downgrade a database.

    Function runs the upgrade() or downgrade() functions in change scripts.

    :param engine:       SQLAlchemy engine instance for a given database
    :param abs_path:     Absolute path to migrate repository.
    :param version:      Database will upgrade/downgrade until this version.
                         If None - database will update to the latest
                         available version.
    :param init_version: Initial database version
    :param sanity_check: Require schema sanity checking for all tables
    """

    if version is not None:
        try:
            version = int(version)
        except ValueError:
            raise exception.DbMigrationError(
                message=_("version should be an integer"))

    current_version = db_version(engine, abs_path, init_version)
    repository = _find_migrate_repo(abs_path)
    if sanity_check:
        _db_schema_sanity_check(engine)
    if version is None or version > current_version:
        return versioning_api.upgrade(engine, repository, version)
    else:
        return versioning_api.downgrade(engine, repository,
                                        version)


def _db_schema_sanity_check(engine):
    """Ensure all database tables were created with required parameters.

    :param engine:  SQLAlchemy engine instance for a given database

    """

    if engine.name == 'mysql':
        onlyutf8_sql = ('SELECT TABLE_NAME,TABLE_COLLATION '
                        'from information_schema.TABLES '
                        'where TABLE_SCHEMA=%s and '
                        'TABLE_COLLATION NOT LIKE "%%utf8%%"')

        # NOTE(morganfainberg): exclude the sqlalchemy-migrate and alembic
        # versioning tables from the tables we need to verify utf8 status on.
        # Non-standard table names are not supported.
        EXCLUDED_TABLES = ['migrate_version', 'alembic_version']

        table_names = [res[0] for res in
                       engine.execute(onlyutf8_sql, engine.url.database) if
                       res[0].lower() not in EXCLUDED_TABLES]

        if len(table_names) > 0:
            raise ValueError(_('Tables "%s" have non utf8 collation, '
                               'please make sure all tables are CHARSET=utf8'
                               ) % ','.join(table_names))


def db_version(engine, abs_path, init_version):
    """Show the current version of the repository.

    :param engine:  SQLAlchemy engine instance for a given database
    :param abs_path: Absolute path to migrate repository
    :param version:  Initial database version
    """
    repository = _find_migrate_repo(abs_path)
    try:
        return versioning_api.db_version(engine, repository)
    except versioning_exceptions.DatabaseNotControlledError:
        meta = sqlalchemy.MetaData()
        meta.reflect(bind=engine)
        tables = meta.tables
        if len(tables) == 0 or 'alembic_version' in tables:
            db_version_control(engine, abs_path, version=init_version)
            return versioning_api.db_version(engine, repository)
        else:
            raise exception.DbMigrationError(
                message=_(
                    "The database is not under version control, but has "
                    "tables. Please stamp the current version of the schema "
                    "manually."))


def db_version_control(engine, abs_path, version=None):
    """Mark a database as under this repository's version control.

    Once a database is under version control, schema changes should
    only be done via change scripts in this repository.

    :param engine:  SQLAlchemy engine instance for a given database
    :param abs_path: Absolute path to migrate repository
    :param version:  Initial database version
    """
    repository = _find_migrate_repo(abs_path)
    versioning_api.version_control(engine, repository, version)
    return version


def _find_migrate_repo(abs_path):
    """Get the project's change script repository

    :param abs_path: Absolute path to migrate repository
    """
    if not os.path.exists(abs_path):
        raise exception.DbMigrationError("Path %s not found" % abs_path)
    return Repository(abs_path)

########NEW FILE########
__FILENAME__ = models
# Copyright (c) 2011 X.commerce, a business unit of eBay Inc.
# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# Copyright 2011 Piston Cloud Computing, Inc.
# Copyright 2012 Cloudscaling Group, Inc.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
"""
SQLAlchemy models.
"""

import six

from sqlalchemy import Column, Integer
from sqlalchemy import DateTime
from sqlalchemy.orm import object_mapper

from ceilometer.openstack.common import timeutils


class ModelBase(six.Iterator):
    """Base class for models."""
    __table_initialized__ = False

    def save(self, session):
        """Save this object."""

        # NOTE(boris-42): This part of code should be look like:
        #                       session.add(self)
        #                       session.flush()
        #                 But there is a bug in sqlalchemy and eventlet that
        #                 raises NoneType exception if there is no running
        #                 transaction and rollback is called. As long as
        #                 sqlalchemy has this bug we have to create transaction
        #                 explicitly.
        with session.begin(subtransactions=True):
            session.add(self)
            session.flush()

    def __setitem__(self, key, value):
        setattr(self, key, value)

    def __getitem__(self, key):
        return getattr(self, key)

    def get(self, key, default=None):
        return getattr(self, key, default)

    @property
    def _extra_keys(self):
        """Specifies custom fields

        Subclasses can override this property to return a list
        of custom fields that should be included in their dict
        representation.

        For reference check tests/db/sqlalchemy/test_models.py
        """
        return []

    def __iter__(self):
        columns = dict(object_mapper(self).columns).keys()
        # NOTE(russellb): Allow models to specify other keys that can be looked
        # up, beyond the actual db columns.  An example would be the 'name'
        # property for an Instance.
        columns.extend(self._extra_keys)
        self._i = iter(columns)
        return self

    # In Python 3, __next__() has replaced next().
    def __next__(self):
        n = six.advance_iterator(self._i)
        return n, getattr(self, n)

    def next(self):
        return self.__next__()

    def update(self, values):
        """Make the model object behave like a dict."""
        for k, v in six.iteritems(values):
            setattr(self, k, v)

    def iteritems(self):
        """Make the model object behave like a dict.

        Includes attributes from joins.
        """
        local = dict(self)
        joined = dict([(k, v) for k, v in six.iteritems(self.__dict__)
                      if not k[0] == '_'])
        local.update(joined)
        return six.iteritems(local)


class TimestampMixin(object):
    created_at = Column(DateTime, default=lambda: timeutils.utcnow())
    updated_at = Column(DateTime, onupdate=lambda: timeutils.utcnow())


class SoftDeleteMixin(object):
    deleted_at = Column(DateTime)
    deleted = Column(Integer, default=0)

    def soft_delete(self, session):
        """Mark this object as deleted."""
        self.deleted = self.id
        self.deleted_at = timeutils.utcnow()
        self.save(session=session)

########NEW FILE########
__FILENAME__ = provision
# Copyright 2013 Mirantis.inc
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Provision test environment for specific DB backends"""

import argparse
import logging
import os
import random
import string

from six import moves
import sqlalchemy

from ceilometer.openstack.common.db import exception as exc


LOG = logging.getLogger(__name__)


def get_engine(uri):
    """Engine creation

    Call the function without arguments to get admin connection. Admin
    connection required to create temporary user and database for each
    particular test. Otherwise use existing connection to recreate connection
    to the temporary database.
    """
    return sqlalchemy.create_engine(uri, poolclass=sqlalchemy.pool.NullPool)


def _execute_sql(engine, sql, driver):
    """Initialize connection, execute sql query and close it."""
    try:
        with engine.connect() as conn:
            if driver == 'postgresql':
                conn.connection.set_isolation_level(0)
            for s in sql:
                conn.execute(s)
    except sqlalchemy.exc.OperationalError:
        msg = ('%s does not match database admin '
               'credentials or database does not exist.')
        LOG.exception(msg % engine.url)
        raise exc.DBConnectionError(msg % engine.url)


def create_database(engine):
    """Provide temporary user and database for each particular test."""
    driver = engine.name

    auth = {
        'database': ''.join(random.choice(string.ascii_lowercase)
                            for i in moves.range(10)),
        'user': engine.url.username,
        'passwd': engine.url.password,
    }

    sqls = [
        "drop database if exists %(database)s;",
        "create database %(database)s;"
    ]

    if driver == 'sqlite':
        return 'sqlite:////tmp/%s' % auth['database']
    elif driver in ['mysql', 'postgresql']:
        sql_query = map(lambda x: x % auth, sqls)
        _execute_sql(engine, sql_query, driver)
    else:
        raise ValueError('Unsupported RDBMS %s' % driver)

    params = auth.copy()
    params['backend'] = driver
    return "%(backend)s://%(user)s:%(passwd)s@localhost/%(database)s" % params


def drop_database(admin_engine, current_uri):
    """Drop temporary database and user after each particular test."""

    engine = get_engine(current_uri)
    driver = engine.name
    auth = {'database': engine.url.database, 'user': engine.url.username}

    if driver == 'sqlite':
        try:
            os.remove(auth['database'])
        except OSError:
            pass
    elif driver in ['mysql', 'postgresql']:
        sql = "drop database if exists %(database)s;"
        _execute_sql(admin_engine, [sql % auth], driver)
    else:
        raise ValueError('Unsupported RDBMS %s' % driver)


def main():
    """Controller to handle commands

    ::create: Create test user and database with random names.
    ::drop: Drop user and database created by previous command.
    """
    parser = argparse.ArgumentParser(
        description='Controller to handle database creation and dropping'
        ' commands.',
        epilog='Under normal circumstances is not used directly.'
        ' Used in .testr.conf to automate test database creation'
        ' and dropping processes.')
    subparsers = parser.add_subparsers(
        help='Subcommands to manipulate temporary test databases.')

    create = subparsers.add_parser(
        'create',
        help='Create temporary test '
        'databases and users.')
    create.set_defaults(which='create')
    create.add_argument(
        'instances_count',
        type=int,
        help='Number of databases to create.')

    drop = subparsers.add_parser(
        'drop',
        help='Drop temporary test databases and users.')
    drop.set_defaults(which='drop')
    drop.add_argument(
        'instances',
        nargs='+',
        help='List of databases uri to be dropped.')

    args = parser.parse_args()

    connection_string = os.getenv('OS_TEST_DBAPI_ADMIN_CONNECTION',
                                  'sqlite://')
    engine = get_engine(connection_string)
    which = args.which

    if which == "create":
        for i in range(int(args.instances_count)):
            print(create_database(engine))
    elif which == "drop":
        for db in args.instances:
            drop_database(engine, db)


if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = session
# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Session Handling for SQLAlchemy backend.

Recommended ways to use sessions within this framework:

* Don't use them explicitly; this is like running with ``AUTOCOMMIT=1``.
  `model_query()` will implicitly use a session when called without one
  supplied. This is the ideal situation because it will allow queries
  to be automatically retried if the database connection is interrupted.

  .. note:: Automatic retry will be enabled in a future patch.

  It is generally fine to issue several queries in a row like this. Even though
  they may be run in separate transactions and/or separate sessions, each one
  will see the data from the prior calls. If needed, undo- or rollback-like
  functionality should be handled at a logical level. For an example, look at
  the code around quotas and `reservation_rollback()`.

  Examples:

  .. code:: python

    def get_foo(context, foo):
        return (model_query(context, models.Foo).
                filter_by(foo=foo).
                first())

    def update_foo(context, id, newfoo):
        (model_query(context, models.Foo).
                filter_by(id=id).
                update({'foo': newfoo}))

    def create_foo(context, values):
        foo_ref = models.Foo()
        foo_ref.update(values)
        foo_ref.save()
        return foo_ref


* Within the scope of a single method, keep all the reads and writes within
  the context managed by a single session. In this way, the session's
  `__exit__` handler will take care of calling `flush()` and `commit()` for
  you. If using this approach, you should not explicitly call `flush()` or
  `commit()`. Any error within the context of the session will cause the
  session to emit a `ROLLBACK`. Database errors like `IntegrityError` will be
  raised in `session`'s `__exit__` handler, and any try/except within the
  context managed by `session` will not be triggered. And catching other
  non-database errors in the session will not trigger the ROLLBACK, so
  exception handlers should  always be outside the session, unless the
  developer wants to do a partial commit on purpose. If the connection is
  dropped before this is possible, the database will implicitly roll back the
  transaction.

  .. note:: Statements in the session scope will not be automatically retried.

  If you create models within the session, they need to be added, but you
  do not need to call `model.save()`:

  .. code:: python

    def create_many_foo(context, foos):
        session = sessionmaker()
        with session.begin():
            for foo in foos:
                foo_ref = models.Foo()
                foo_ref.update(foo)
                session.add(foo_ref)

    def update_bar(context, foo_id, newbar):
        session = sessionmaker()
        with session.begin():
            foo_ref = (model_query(context, models.Foo, session).
                        filter_by(id=foo_id).
                        first())
            (model_query(context, models.Bar, session).
                        filter_by(id=foo_ref['bar_id']).
                        update({'bar': newbar}))

  .. note:: `update_bar` is a trivially simple example of using
     ``with session.begin``. Whereas `create_many_foo` is a good example of
     when a transaction is needed, it is always best to use as few queries as
     possible.

  The two queries in `update_bar` can be better expressed using a single query
  which avoids the need for an explicit transaction. It can be expressed like
  so:

  .. code:: python

    def update_bar(context, foo_id, newbar):
        subq = (model_query(context, models.Foo.id).
                filter_by(id=foo_id).
                limit(1).
                subquery())
        (model_query(context, models.Bar).
                filter_by(id=subq.as_scalar()).
                update({'bar': newbar}))

  For reference, this emits approximately the following SQL statement:

  .. code:: sql

    UPDATE bar SET bar = ${newbar}
        WHERE id=(SELECT bar_id FROM foo WHERE id = ${foo_id} LIMIT 1);

  .. note:: `create_duplicate_foo` is a trivially simple example of catching an
     exception while using ``with session.begin``. Here create two duplicate
     instances with same primary key, must catch the exception out of context
     managed by a single session:

  .. code:: python

    def create_duplicate_foo(context):
        foo1 = models.Foo()
        foo2 = models.Foo()
        foo1.id = foo2.id = 1
        session = sessionmaker()
        try:
            with session.begin():
                session.add(foo1)
                session.add(foo2)
        except exception.DBDuplicateEntry as e:
            handle_error(e)

* Passing an active session between methods. Sessions should only be passed
  to private methods. The private method must use a subtransaction; otherwise
  SQLAlchemy will throw an error when you call `session.begin()` on an existing
  transaction. Public methods should not accept a session parameter and should
  not be involved in sessions within the caller's scope.

  Note that this incurs more overhead in SQLAlchemy than the above means
  due to nesting transactions, and it is not possible to implicitly retry
  failed database operations when using this approach.

  This also makes code somewhat more difficult to read and debug, because a
  single database transaction spans more than one method. Error handling
  becomes less clear in this situation. When this is needed for code clarity,
  it should be clearly documented.

  .. code:: python

    def myfunc(foo):
        session = sessionmaker()
        with session.begin():
            # do some database things
            bar = _private_func(foo, session)
        return bar

    def _private_func(foo, session=None):
        if not session:
            session = sessionmaker()
        with session.begin(subtransaction=True):
            # do some other database things
        return bar


There are some things which it is best to avoid:

* Don't keep a transaction open any longer than necessary.

  This means that your ``with session.begin()`` block should be as short
  as possible, while still containing all the related calls for that
  transaction.

* Avoid ``with_lockmode('UPDATE')`` when possible.

  In MySQL/InnoDB, when a ``SELECT ... FOR UPDATE`` query does not match
  any rows, it will take a gap-lock. This is a form of write-lock on the
  "gap" where no rows exist, and prevents any other writes to that space.
  This can effectively prevent any INSERT into a table by locking the gap
  at the end of the index. Similar problems will occur if the SELECT FOR UPDATE
  has an overly broad WHERE clause, or doesn't properly use an index.

  One idea proposed at ODS Fall '12 was to use a normal SELECT to test the
  number of rows matching a query, and if only one row is returned,
  then issue the SELECT FOR UPDATE.

  The better long-term solution is to use
  ``INSERT .. ON DUPLICATE KEY UPDATE``.
  However, this can not be done until the "deleted" columns are removed and
  proper UNIQUE constraints are added to the tables.


Enabling soft deletes:

* To use/enable soft-deletes, the `SoftDeleteMixin` must be added
  to your model class. For example:

  .. code:: python

      class NovaBase(models.SoftDeleteMixin, models.ModelBase):
          pass


Efficient use of soft deletes:

* There are two possible ways to mark a record as deleted:
  `model.soft_delete()` and `query.soft_delete()`.

  The `model.soft_delete()` method works with a single already-fetched entry.
  `query.soft_delete()` makes only one db request for all entries that
  correspond to the query.

* In almost all cases you should use `query.soft_delete()`. Some examples:

  .. code:: python

        def soft_delete_bar():
            count = model_query(BarModel).find(some_condition).soft_delete()
            if count == 0:
                raise Exception("0 entries were soft deleted")

        def complex_soft_delete_with_synchronization_bar(session=None):
            if session is None:
                session = sessionmaker()
            with session.begin(subtransactions=True):
                count = (model_query(BarModel).
                            find(some_condition).
                            soft_delete(synchronize_session=True))
                            # Here synchronize_session is required, because we
                            # don't know what is going on in outer session.
                if count == 0:
                    raise Exception("0 entries were soft deleted")

* There is only one situation where `model.soft_delete()` is appropriate: when
  you fetch a single record, work with it, and mark it as deleted in the same
  transaction.

  .. code:: python

        def soft_delete_bar_model():
            session = sessionmaker()
            with session.begin():
                bar_ref = model_query(BarModel).find(some_condition).first()
                # Work with bar_ref
                bar_ref.soft_delete(session=session)

  However, if you need to work with all entries that correspond to query and
  then soft delete them you should use the `query.soft_delete()` method:

  .. code:: python

        def soft_delete_multi_models():
            session = sessionmaker()
            with session.begin():
                query = (model_query(BarModel, session=session).
                            find(some_condition))
                model_refs = query.all()
                # Work with model_refs
                query.soft_delete(synchronize_session=False)
                # synchronize_session=False should be set if there is no outer
                # session and these entries are not used after this.

  When working with many rows, it is very important to use query.soft_delete,
  which issues a single query. Using `model.soft_delete()`, as in the following
  example, is very inefficient.

  .. code:: python

        for bar_ref in bar_refs:
            bar_ref.soft_delete(session=session)
        # This will produce count(bar_refs) db requests.

"""

import functools
import logging
import re
import time

import six
from sqlalchemy import exc as sqla_exc
from sqlalchemy.interfaces import PoolListener
import sqlalchemy.orm
from sqlalchemy.pool import NullPool, StaticPool
from sqlalchemy.sql.expression import literal_column

from ceilometer.openstack.common.db import exception
from ceilometer.openstack.common.gettextutils import _LE, _LW
from ceilometer.openstack.common import timeutils


LOG = logging.getLogger(__name__)


class SqliteForeignKeysListener(PoolListener):
    """Ensures that the foreign key constraints are enforced in SQLite.

    The foreign key constraints are disabled by default in SQLite,
    so the foreign key constraints will be enabled here for every
    database connection
    """
    def connect(self, dbapi_con, con_record):
        dbapi_con.execute('pragma foreign_keys=ON')


# note(boris-42): In current versions of DB backends unique constraint
# violation messages follow the structure:
#
# sqlite:
# 1 column - (IntegrityError) column c1 is not unique
# N columns - (IntegrityError) column c1, c2, ..., N are not unique
#
# sqlite since 3.7.16:
# 1 column - (IntegrityError) UNIQUE constraint failed: tbl.k1
#
# N columns - (IntegrityError) UNIQUE constraint failed: tbl.k1, tbl.k2
#
# postgres:
# 1 column - (IntegrityError) duplicate key value violates unique
#               constraint "users_c1_key"
# N columns - (IntegrityError) duplicate key value violates unique
#               constraint "name_of_our_constraint"
#
# mysql:
# 1 column - (IntegrityError) (1062, "Duplicate entry 'value_of_c1' for key
#               'c1'")
# N columns - (IntegrityError) (1062, "Duplicate entry 'values joined
#               with -' for key 'name_of_our_constraint'")
#
# ibm_db_sa:
# N columns - (IntegrityError) SQL0803N  One or more values in the INSERT
#                statement, UPDATE statement, or foreign key update caused by a
#                DELETE statement are not valid because the primary key, unique
#                constraint or unique index identified by "2" constrains table
#                "NOVA.KEY_PAIRS" from having duplicate values for the index
#                key.
_DUP_KEY_RE_DB = {
    "sqlite": (re.compile(r"^.*columns?([^)]+)(is|are)\s+not\s+unique$"),
               re.compile(r"^.*UNIQUE\s+constraint\s+failed:\s+(.+)$")),
    "postgresql": (re.compile(r"^.*duplicate\s+key.*\"([^\"]+)\"\s*\n.*$"),),
    "mysql": (re.compile(r"^.*\(1062,.*'([^\']+)'\"\)$"),),
    "ibm_db_sa": (re.compile(r"^.*SQL0803N.*$"),),
}


def _raise_if_duplicate_entry_error(integrity_error, engine_name):
    """Raise exception if two entries are duplicated.

    In this function will be raised DBDuplicateEntry exception if integrity
    error wrap unique constraint violation.
    """

    def get_columns_from_uniq_cons_or_name(columns):
        # note(vsergeyev): UniqueConstraint name convention: "uniq_t0c10c2"
        #                  where `t` it is table name and columns `c1`, `c2`
        #                  are in UniqueConstraint.
        uniqbase = "uniq_"
        if not columns.startswith(uniqbase):
            if engine_name == "postgresql":
                return [columns[columns.index("_") + 1:columns.rindex("_")]]
            return [columns]
        return columns[len(uniqbase):].split("0")[1:]

    if engine_name not in ["ibm_db_sa", "mysql", "sqlite", "postgresql"]:
        return

    # FIXME(johannes): The usage of the .message attribute has been
    # deprecated since Python 2.6. However, the exceptions raised by
    # SQLAlchemy can differ when using unicode() and accessing .message.
    # An audit across all three supported engines will be necessary to
    # ensure there are no regressions.
    for pattern in _DUP_KEY_RE_DB[engine_name]:
        match = pattern.match(integrity_error.message)
        if match:
            break
    else:
        return

    # NOTE(mriedem): The ibm_db_sa integrity error message doesn't provide the
    # columns so we have to omit that from the DBDuplicateEntry error.
    columns = ''

    if engine_name != 'ibm_db_sa':
        columns = match.group(1)

    if engine_name == "sqlite":
        columns = [c.split('.')[-1] for c in columns.strip().split(", ")]
    else:
        columns = get_columns_from_uniq_cons_or_name(columns)
    raise exception.DBDuplicateEntry(columns, integrity_error)


# NOTE(comstud): In current versions of DB backends, Deadlock violation
# messages follow the structure:
#
# mysql:
# (OperationalError) (1213, 'Deadlock found when trying to get lock; try '
#                     'restarting transaction') <query_str> <query_args>
_DEADLOCK_RE_DB = {
    "mysql": re.compile(r"^.*\(1213, 'Deadlock.*")
}


def _raise_if_deadlock_error(operational_error, engine_name):
    """Raise exception on deadlock condition.

    Raise DBDeadlock exception if OperationalError contains a Deadlock
    condition.
    """
    re = _DEADLOCK_RE_DB.get(engine_name)
    if re is None:
        return
    # FIXME(johannes): The usage of the .message attribute has been
    # deprecated since Python 2.6. However, the exceptions raised by
    # SQLAlchemy can differ when using unicode() and accessing .message.
    # An audit across all three supported engines will be necessary to
    # ensure there are no regressions.
    m = re.match(operational_error.message)
    if not m:
        return
    raise exception.DBDeadlock(operational_error)


def _wrap_db_error(f):
    @functools.wraps(f)
    def _wrap(self, *args, **kwargs):
        try:
            assert issubclass(
                self.__class__, sqlalchemy.orm.session.Session
            ), ('_wrap_db_error() can only be applied to methods of '
                'subclasses of sqlalchemy.orm.session.Session.')

            return f(self, *args, **kwargs)
        except UnicodeEncodeError:
            raise exception.DBInvalidUnicodeParameter()
        except sqla_exc.OperationalError as e:
            _raise_if_db_connection_lost(e, self.bind)
            _raise_if_deadlock_error(e, self.bind.dialect.name)
            # NOTE(comstud): A lot of code is checking for OperationalError
            # so let's not wrap it for now.
            raise
        # note(boris-42): We should catch unique constraint violation and
        # wrap it by our own DBDuplicateEntry exception. Unique constraint
        # violation is wrapped by IntegrityError.
        except sqla_exc.IntegrityError as e:
            # note(boris-42): SqlAlchemy doesn't unify errors from different
            # DBs so we must do this. Also in some tables (for example
            # instance_types) there are more than one unique constraint. This
            # means we should get names of columns, which values violate
            # unique constraint, from error message.
            _raise_if_duplicate_entry_error(e, self.bind.dialect.name)
            raise exception.DBError(e)
        except Exception as e:
            LOG.exception(_LE('DB exception wrapped.'))
            raise exception.DBError(e)
    return _wrap


def _synchronous_switch_listener(dbapi_conn, connection_rec):
    """Switch sqlite connections to non-synchronous mode."""
    dbapi_conn.execute("PRAGMA synchronous = OFF")


def _add_regexp_listener(dbapi_con, con_record):
    """Add REGEXP function to sqlite connections."""

    def regexp(expr, item):
        reg = re.compile(expr)
        return reg.search(six.text_type(item)) is not None
    dbapi_con.create_function('regexp', 2, regexp)


def _thread_yield(dbapi_con, con_record):
    """Ensure other greenthreads get a chance to be executed.

    If we use eventlet.monkey_patch(), eventlet.greenthread.sleep(0) will
    execute instead of time.sleep(0).
    Force a context switch. With common database backends (eg MySQLdb and
    sqlite), there is no implicit yield caused by network I/O since they are
    implemented by C libraries that eventlet cannot monkey patch.
    """
    time.sleep(0)


def _ping_listener(engine, dbapi_conn, connection_rec, connection_proxy):
    """Ensures that MySQL and DB2 connections are alive.

    Borrowed from:
    http://groups.google.com/group/sqlalchemy/msg/a4ce563d802c929f
    """
    cursor = dbapi_conn.cursor()
    try:
        ping_sql = 'select 1'
        if engine.name == 'ibm_db_sa':
            # DB2 requires a table expression
            ping_sql = 'select 1 from (values (1)) AS t1'
        cursor.execute(ping_sql)
    except Exception as ex:
        if engine.dialect.is_disconnect(ex, dbapi_conn, cursor):
            msg = _LW('Database server has gone away: %s') % ex
            LOG.warning(msg)

            # if the database server has gone away, all connections in the pool
            # have become invalid and we can safely close all of them here,
            # rather than waste time on checking of every single connection
            engine.dispose()

            # this will be handled by SQLAlchemy and will force it to create
            # a new connection and retry the original action
            raise sqla_exc.DisconnectionError(msg)
        else:
            raise


def _set_session_sql_mode(dbapi_con, connection_rec, sql_mode=None):
    """Set the sql_mode session variable.

    MySQL supports several server modes. The default is None, but sessions
    may choose to enable server modes like TRADITIONAL, ANSI,
    several STRICT_* modes and others.

    Note: passing in '' (empty string) for sql_mode clears
    the SQL mode for the session, overriding a potentially set
    server default.
    """

    cursor = dbapi_con.cursor()
    cursor.execute("SET SESSION sql_mode = %s", [sql_mode])


def _mysql_get_effective_sql_mode(engine):
    """Returns the effective SQL mode for connections from the engine pool.

    Returns ``None`` if the mode isn't available, otherwise returns the mode.

    """
    # Get the real effective SQL mode. Even when unset by
    # our own config, the server may still be operating in a specific
    # SQL mode as set by the server configuration.
    # Also note that the checkout listener will be called on execute to
    # set the mode if it's registered.
    row = engine.execute("SHOW VARIABLES LIKE 'sql_mode'").fetchone()
    if row is None:
        return
    return row[1]


def _mysql_check_effective_sql_mode(engine):
    """Logs a message based on the effective SQL mode for MySQL connections."""
    realmode = _mysql_get_effective_sql_mode(engine)

    if realmode is None:
        LOG.warning(_LW('Unable to detect effective SQL mode'))
        return

    LOG.debug('MySQL server mode set to %s', realmode)
    # 'TRADITIONAL' mode enables several other modes, so
    # we need a substring match here
    if not ('TRADITIONAL' in realmode.upper() or
            'STRICT_ALL_TABLES' in realmode.upper()):
        LOG.warning(_LW("MySQL SQL mode is '%s', "
                        "consider enabling TRADITIONAL or STRICT_ALL_TABLES"),
                    realmode)


def _mysql_set_mode_callback(engine, sql_mode):
    if sql_mode is not None:
        mode_callback = functools.partial(_set_session_sql_mode,
                                          sql_mode=sql_mode)
        sqlalchemy.event.listen(engine, 'connect', mode_callback)
    _mysql_check_effective_sql_mode(engine)


def _is_db_connection_error(args):
    """Return True if error in connecting to db."""
    # NOTE(adam_g): This is currently MySQL specific and needs to be extended
    #               to support Postgres and others.
    # For the db2, the error code is -30081 since the db2 is still not ready
    conn_err_codes = ('2002', '2003', '2006', '2013', '-30081')
    for err_code in conn_err_codes:
        if args.find(err_code) != -1:
            return True
    return False


def _raise_if_db_connection_lost(error, engine):
    # NOTE(vsergeyev): Function is_disconnect(e, connection, cursor)
    #                  requires connection and cursor in incoming parameters,
    #                  but we have no possibility to create connection if DB
    #                  is not available, so in such case reconnect fails.
    #                  But is_disconnect() ignores these parameters, so it
    #                  makes sense to pass to function None as placeholder
    #                  instead of connection and cursor.
    if engine.dialect.is_disconnect(error, None, None):
        raise exception.DBConnectionError(error)


def create_engine(sql_connection, sqlite_fk=False, mysql_sql_mode=None,
                  idle_timeout=3600,
                  connection_debug=0, max_pool_size=None, max_overflow=None,
                  pool_timeout=None, sqlite_synchronous=True,
                  connection_trace=False, max_retries=10, retry_interval=10):
    """Return a new SQLAlchemy engine."""

    connection_dict = sqlalchemy.engine.url.make_url(sql_connection)

    engine_args = {
        "pool_recycle": idle_timeout,
        'convert_unicode': True,
    }

    logger = logging.getLogger('sqlalchemy.engine')

    # Map SQL debug level to Python log level
    if connection_debug >= 100:
        logger.setLevel(logging.DEBUG)
    elif connection_debug >= 50:
        logger.setLevel(logging.INFO)
    else:
        logger.setLevel(logging.WARNING)

    if "sqlite" in connection_dict.drivername:
        if sqlite_fk:
            engine_args["listeners"] = [SqliteForeignKeysListener()]
        engine_args["poolclass"] = NullPool

        if sql_connection == "sqlite://":
            engine_args["poolclass"] = StaticPool
            engine_args["connect_args"] = {'check_same_thread': False}
    else:
        if max_pool_size is not None:
            engine_args['pool_size'] = max_pool_size
        if max_overflow is not None:
            engine_args['max_overflow'] = max_overflow
        if pool_timeout is not None:
            engine_args['pool_timeout'] = pool_timeout

    engine = sqlalchemy.create_engine(sql_connection, **engine_args)

    sqlalchemy.event.listen(engine, 'checkin', _thread_yield)

    if engine.name in ['mysql', 'ibm_db_sa']:
        ping_callback = functools.partial(_ping_listener, engine)
        sqlalchemy.event.listen(engine, 'checkout', ping_callback)
        if engine.name == 'mysql':
            if mysql_sql_mode:
                _mysql_set_mode_callback(engine, mysql_sql_mode)
    elif 'sqlite' in connection_dict.drivername:
        if not sqlite_synchronous:
            sqlalchemy.event.listen(engine, 'connect',
                                    _synchronous_switch_listener)
        sqlalchemy.event.listen(engine, 'connect', _add_regexp_listener)

    if connection_trace and engine.dialect.dbapi.__name__ == 'MySQLdb':
        _patch_mysqldb_with_stacktrace_comments()

    try:
        engine.connect()
    except sqla_exc.OperationalError as e:
        if not _is_db_connection_error(e.args[0]):
            raise

        remaining = max_retries
        if remaining == -1:
            remaining = 'infinite'
        while True:
            msg = _LW('SQL connection failed. %s attempts left.')
            LOG.warning(msg % remaining)
            if remaining != 'infinite':
                remaining -= 1
            time.sleep(retry_interval)
            try:
                engine.connect()
                break
            except sqla_exc.OperationalError as e:
                if (remaining != 'infinite' and remaining == 0) or \
                        not _is_db_connection_error(e.args[0]):
                    raise
    return engine


class Query(sqlalchemy.orm.query.Query):
    """Subclass of sqlalchemy.query with soft_delete() method."""
    def soft_delete(self, synchronize_session='evaluate'):
        return self.update({'deleted': literal_column('id'),
                            'updated_at': literal_column('updated_at'),
                            'deleted_at': timeutils.utcnow()},
                           synchronize_session=synchronize_session)


class Session(sqlalchemy.orm.session.Session):
    """Custom Session class to avoid SqlAlchemy Session monkey patching."""
    @_wrap_db_error
    def query(self, *args, **kwargs):
        return super(Session, self).query(*args, **kwargs)

    @_wrap_db_error
    def flush(self, *args, **kwargs):
        return super(Session, self).flush(*args, **kwargs)

    @_wrap_db_error
    def execute(self, *args, **kwargs):
        return super(Session, self).execute(*args, **kwargs)


def get_maker(engine, autocommit=True, expire_on_commit=False):
    """Return a SQLAlchemy sessionmaker using the given engine."""
    return sqlalchemy.orm.sessionmaker(bind=engine,
                                       class_=Session,
                                       autocommit=autocommit,
                                       expire_on_commit=expire_on_commit,
                                       query_cls=Query)


def _patch_mysqldb_with_stacktrace_comments():
    """Adds current stack trace as a comment in queries.

    Patches MySQLdb.cursors.BaseCursor._do_query.
    """
    import MySQLdb.cursors
    import traceback

    old_mysql_do_query = MySQLdb.cursors.BaseCursor._do_query

    def _do_query(self, q):
        stack = ''
        for filename, line, method, function in traceback.extract_stack():
            # exclude various common things from trace
            if filename.endswith('session.py') and method == '_do_query':
                continue
            if filename.endswith('api.py') and method == 'wrapper':
                continue
            if filename.endswith('utils.py') and method == '_inner':
                continue
            if filename.endswith('exception.py') and method == '_wrap':
                continue
            # db/api is just a wrapper around db/sqlalchemy/api
            if filename.endswith('db/api.py'):
                continue
            # only trace inside ceilometer
            index = filename.rfind('ceilometer')
            if index == -1:
                continue
            stack += "File:%s:%s Method:%s() Line:%s | " \
                     % (filename[index:], line, method, function)

        # strip trailing " | " from stack
        if stack:
            stack = stack[:-3]
            qq = "%s /* %s */" % (q, stack)
        else:
            qq = q
        old_mysql_do_query(self, qq)

    setattr(MySQLdb.cursors.BaseCursor, '_do_query', _do_query)


class EngineFacade(object):
    """A helper class for removing of global engine instances from ceilometer.db.

    As a library, ceilometer.db can't decide where to store/when to create engine
    and sessionmaker instances, so this must be left for a target application.

    On the other hand, in order to simplify the adoption of ceilometer.db changes,
    we'll provide a helper class, which creates engine and sessionmaker
    on its instantiation and provides get_engine()/get_session() methods
    that are compatible with corresponding utility functions that currently
    exist in target projects, e.g. in Nova.

    engine/sessionmaker instances will still be global (and they are meant to
    be global), but they will be stored in the app context, rather that in the
    ceilometer.db context.

    Note: using of this helper is completely optional and you are encouraged to
    integrate engine/sessionmaker instances into your apps any way you like
    (e.g. one might want to bind a session to a request context). Two important
    things to remember:

    1. An Engine instance is effectively a pool of DB connections, so it's
       meant to be shared (and it's thread-safe).
    2. A Session instance is not meant to be shared and represents a DB
       transactional context (i.e. it's not thread-safe). sessionmaker is
       a factory of sessions.

    """

    def __init__(self, sql_connection,
                 sqlite_fk=False, autocommit=True,
                 expire_on_commit=False, **kwargs):
        """Initialize engine and sessionmaker instances.

        :param sqlite_fk: enable foreign keys in SQLite
        :type sqlite_fk: bool

        :param autocommit: use autocommit mode for created Session instances
        :type autocommit: bool

        :param expire_on_commit: expire session objects on commit
        :type expire_on_commit: bool

        Keyword arguments:

        :keyword mysql_sql_mode: the SQL mode to be used for MySQL sessions.
                                 (defaults to TRADITIONAL)
        :keyword idle_timeout: timeout before idle sql connections are reaped
                               (defaults to 3600)
        :keyword connection_debug: verbosity of SQL debugging information.
                                   0=None, 100=Everything (defaults to 0)
        :keyword max_pool_size: maximum number of SQL connections to keep open
                                in a pool (defaults to SQLAlchemy settings)
        :keyword max_overflow: if set, use this value for max_overflow with
                               sqlalchemy (defaults to SQLAlchemy settings)
        :keyword pool_timeout: if set, use this value for pool_timeout with
                               sqlalchemy (defaults to SQLAlchemy settings)
        :keyword sqlite_synchronous: if True, SQLite uses synchronous mode
                                     (defaults to True)
        :keyword connection_trace: add python stack traces to SQL as comment
                                   strings (defaults to False)
        :keyword max_retries: maximum db connection retries during startup.
                              (setting -1 implies an infinite retry count)
                              (defaults to 10)
        :keyword retry_interval: interval between retries of opening a sql
                                 connection (defaults to 10)

        """

        super(EngineFacade, self).__init__()

        self._engine = create_engine(
            sql_connection=sql_connection,
            sqlite_fk=sqlite_fk,
            mysql_sql_mode=kwargs.get('mysql_sql_mode', 'TRADITIONAL'),
            idle_timeout=kwargs.get('idle_timeout', 3600),
            connection_debug=kwargs.get('connection_debug', 0),
            max_pool_size=kwargs.get('max_pool_size'),
            max_overflow=kwargs.get('max_overflow'),
            pool_timeout=kwargs.get('pool_timeout'),
            sqlite_synchronous=kwargs.get('sqlite_synchronous', True),
            connection_trace=kwargs.get('connection_trace', False),
            max_retries=kwargs.get('max_retries', 10),
            retry_interval=kwargs.get('retry_interval', 10))
        self._session_maker = get_maker(
            engine=self._engine,
            autocommit=autocommit,
            expire_on_commit=expire_on_commit)

    def get_engine(self):
        """Get the engine instance (note, that it's shared)."""

        return self._engine

    def get_session(self, **kwargs):
        """Get a Session instance.

        If passed, keyword arguments values override the ones used when the
        sessionmaker instance was created.

        :keyword autocommit: use autocommit mode for created Session instances
        :type autocommit: bool

        :keyword expire_on_commit: expire session objects on commit
        :type expire_on_commit: bool

        """

        for arg in kwargs:
            if arg not in ('autocommit', 'expire_on_commit'):
                del kwargs[arg]

        return self._session_maker(**kwargs)

    @classmethod
    def from_config(cls, connection_string, conf,
                    sqlite_fk=False, autocommit=True, expire_on_commit=False):
        """Initialize EngineFacade using oslo.config config instance options.

        :param connection_string: SQLAlchemy connection string
        :type connection_string: string

        :param conf: oslo.config config instance
        :type conf: oslo.config.cfg.ConfigOpts

        :param sqlite_fk: enable foreign keys in SQLite
        :type sqlite_fk: bool

        :param autocommit: use autocommit mode for created Session instances
        :type autocommit: bool

        :param expire_on_commit: expire session objects on commit
        :type expire_on_commit: bool

        """

        return cls(sql_connection=connection_string,
                   sqlite_fk=sqlite_fk,
                   autocommit=autocommit,
                   expire_on_commit=expire_on_commit,
                   **dict(conf.database.items()))

########NEW FILE########
__FILENAME__ = test_base
# Copyright (c) 2013 OpenStack Foundation
# All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import abc
import functools
import os

import fixtures
import six

from ceilometer.openstack.common.db.sqlalchemy import session
from ceilometer.openstack.common.db.sqlalchemy import utils
from ceilometer.openstack.common.fixture import lockutils
from ceilometer.openstack.common import test


class DbFixture(fixtures.Fixture):
    """Basic database fixture.

    Allows to run tests on various db backends, such as SQLite, MySQL and
    PostgreSQL. By default use sqlite backend. To override default backend
    uri set env variable OS_TEST_DBAPI_CONNECTION with database admin
    credentials for specific backend.
    """

    def _get_uri(self):
        return os.getenv('OS_TEST_DBAPI_CONNECTION', 'sqlite://')

    def __init__(self, test):
        super(DbFixture, self).__init__()

        self.test = test

    def setUp(self):
        super(DbFixture, self).setUp()

        self.test.engine = session.create_engine(self._get_uri())
        self.test.sessionmaker = session.get_maker(self.test.engine)
        self.addCleanup(self.test.engine.dispose)


class DbTestCase(test.BaseTestCase):
    """Base class for testing of DB code.

    Using `DbFixture`. Intended to be the main database test case to use all
    the tests on a given backend with user defined uri. Backend specific
    tests should be decorated with `backend_specific` decorator.
    """

    FIXTURE = DbFixture

    def setUp(self):
        super(DbTestCase, self).setUp()
        self.useFixture(self.FIXTURE(self))


ALLOWED_DIALECTS = ['sqlite', 'mysql', 'postgresql']


def backend_specific(*dialects):
    """Decorator to skip backend specific tests on inappropriate engines.

    ::dialects: list of dialects names under which the test will be launched.
    """
    def wrap(f):
        @functools.wraps(f)
        def ins_wrap(self):
            if not set(dialects).issubset(ALLOWED_DIALECTS):
                raise ValueError(
                    "Please use allowed dialects: %s" % ALLOWED_DIALECTS)
            if self.engine.name not in dialects:
                msg = ('The test "%s" can be run '
                       'only on %s. Current engine is %s.')
                args = (f.__name__, ' '.join(dialects), self.engine.name)
                self.skip(msg % args)
            else:
                return f(self)
        return ins_wrap
    return wrap


@six.add_metaclass(abc.ABCMeta)
class OpportunisticFixture(DbFixture):
    """Base fixture to use default CI databases.

    The databases exist in OpenStack CI infrastructure. But for the
    correct functioning in local environment the databases must be
    created manually.
    """

    DRIVER = abc.abstractproperty(lambda: None)
    DBNAME = PASSWORD = USERNAME = 'openstack_citest'

    def _get_uri(self):
        return utils.get_connect_string(backend=self.DRIVER,
                                        user=self.USERNAME,
                                        passwd=self.PASSWORD,
                                        database=self.DBNAME)


@six.add_metaclass(abc.ABCMeta)
class OpportunisticTestCase(DbTestCase):
    """Base test case to use default CI databases.

    The subclasses of the test case are running only when openstack_citest
    database is available otherwise a tests will be skipped.
    """

    FIXTURE = abc.abstractproperty(lambda: None)

    def setUp(self):
        # TODO(bnemec): Remove this once infra is ready for
        # https://review.openstack.org/#/c/74963/ to merge.
        self.useFixture(lockutils.LockFixture('opportunistic-db'))
        credentials = {
            'backend': self.FIXTURE.DRIVER,
            'user': self.FIXTURE.USERNAME,
            'passwd': self.FIXTURE.PASSWORD,
            'database': self.FIXTURE.DBNAME}

        if self.FIXTURE.DRIVER and not utils.is_backend_avail(**credentials):
            msg = '%s backend is not available.' % self.FIXTURE.DRIVER
            return self.skip(msg)

        super(OpportunisticTestCase, self).setUp()


class MySQLOpportunisticFixture(OpportunisticFixture):
    DRIVER = 'mysql'


class PostgreSQLOpportunisticFixture(OpportunisticFixture):
    DRIVER = 'postgresql'


class MySQLOpportunisticTestCase(OpportunisticTestCase):
    FIXTURE = MySQLOpportunisticFixture


class PostgreSQLOpportunisticTestCase(OpportunisticTestCase):
    FIXTURE = PostgreSQLOpportunisticFixture

########NEW FILE########
__FILENAME__ = test_migrations
# Copyright 2010-2011 OpenStack Foundation
# Copyright 2012-2013 IBM Corp.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import functools
import logging
import os
import subprocess

import lockfile
from six import moves
from six.moves.urllib import parse
import sqlalchemy
import sqlalchemy.exc

from ceilometer.openstack.common.db.sqlalchemy import utils
from ceilometer.openstack.common.gettextutils import _LE
from ceilometer.openstack.common import test

LOG = logging.getLogger(__name__)


def _have_mysql(user, passwd, database):
    present = os.environ.get('TEST_MYSQL_PRESENT')
    if present is None:
        return utils.is_backend_avail(backend='mysql',
                                      user=user,
                                      passwd=passwd,
                                      database=database)
    return present.lower() in ('', 'true')


def _have_postgresql(user, passwd, database):
    present = os.environ.get('TEST_POSTGRESQL_PRESENT')
    if present is None:
        return utils.is_backend_avail(backend='postgres',
                                      user=user,
                                      passwd=passwd,
                                      database=database)
    return present.lower() in ('', 'true')


def _set_db_lock(lock_path=None, lock_prefix=None):
    def decorator(f):
        @functools.wraps(f)
        def wrapper(*args, **kwargs):
            try:
                path = lock_path or os.environ.get("CEILOMETER_LOCK_PATH")
                lock = lockfile.FileLock(os.path.join(path, lock_prefix))
                with lock:
                    LOG.debug('Got lock "%s"' % f.__name__)
                    return f(*args, **kwargs)
            finally:
                LOG.debug('Lock released "%s"' % f.__name__)
        return wrapper
    return decorator


class BaseMigrationTestCase(test.BaseTestCase):
    """Base class fort testing of migration utils."""

    def __init__(self, *args, **kwargs):
        super(BaseMigrationTestCase, self).__init__(*args, **kwargs)

        self.DEFAULT_CONFIG_FILE = os.path.join(os.path.dirname(__file__),
                                                'test_migrations.conf')
        # Test machines can set the TEST_MIGRATIONS_CONF variable
        # to override the location of the config file for migration testing
        self.CONFIG_FILE_PATH = os.environ.get('TEST_MIGRATIONS_CONF',
                                               self.DEFAULT_CONFIG_FILE)
        self.test_databases = {}
        self.migration_api = None

    def setUp(self):
        super(BaseMigrationTestCase, self).setUp()

        # Load test databases from the config file. Only do this
        # once. No need to re-run this on each test...
        LOG.debug('config_path is %s' % self.CONFIG_FILE_PATH)
        if os.path.exists(self.CONFIG_FILE_PATH):
            cp = moves.configparser.RawConfigParser()
            try:
                cp.read(self.CONFIG_FILE_PATH)
                defaults = cp.defaults()
                for key, value in defaults.items():
                    self.test_databases[key] = value
            except moves.configparser.ParsingError as e:
                self.fail("Failed to read test_migrations.conf config "
                          "file. Got error: %s" % e)
        else:
            self.fail("Failed to find test_migrations.conf config "
                      "file.")

        self.engines = {}
        for key, value in self.test_databases.items():
            self.engines[key] = sqlalchemy.create_engine(value)

        # We start each test case with a completely blank slate.
        self._reset_databases()

    def tearDown(self):
        # We destroy the test data store between each test case,
        # and recreate it, which ensures that we have no side-effects
        # from the tests
        self._reset_databases()
        super(BaseMigrationTestCase, self).tearDown()

    def execute_cmd(self, cmd=None):
        process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE,
                                   stderr=subprocess.STDOUT)
        output = process.communicate()[0]
        LOG.debug(output)
        self.assertEqual(0, process.returncode,
                         "Failed to run: %s\n%s" % (cmd, output))

    def _reset_pg(self, conn_pieces):
        (user,
         password,
         database,
         host) = utils.get_db_connection_info(conn_pieces)
        os.environ['PGPASSWORD'] = password
        os.environ['PGUSER'] = user
        # note(boris-42): We must create and drop database, we can't
        # drop database which we have connected to, so for such
        # operations there is a special database template1.
        sqlcmd = ("psql -w -U %(user)s -h %(host)s -c"
                  " '%(sql)s' -d template1")

        sql = ("drop database if exists %s;") % database
        droptable = sqlcmd % {'user': user, 'host': host, 'sql': sql}
        self.execute_cmd(droptable)

        sql = ("create database %s;") % database
        createtable = sqlcmd % {'user': user, 'host': host, 'sql': sql}
        self.execute_cmd(createtable)

        os.unsetenv('PGPASSWORD')
        os.unsetenv('PGUSER')

    @_set_db_lock(lock_prefix='migration_tests-')
    def _reset_databases(self):
        for key, engine in self.engines.items():
            conn_string = self.test_databases[key]
            conn_pieces = parse.urlparse(conn_string)
            engine.dispose()
            if conn_string.startswith('sqlite'):
                # We can just delete the SQLite database, which is
                # the easiest and cleanest solution
                db_path = conn_pieces.path.strip('/')
                if os.path.exists(db_path):
                    os.unlink(db_path)
                # No need to recreate the SQLite DB. SQLite will
                # create it for us if it's not there...
            elif conn_string.startswith('mysql'):
                # We can execute the MySQL client to destroy and re-create
                # the MYSQL database, which is easier and less error-prone
                # than using SQLAlchemy to do this via MetaData...trust me.
                (user, password, database, host) = \
                    utils.get_db_connection_info(conn_pieces)
                sql = ("drop database if exists %(db)s; "
                       "create database %(db)s;") % {'db': database}
                cmd = ("mysql -u \"%(user)s\" -p\"%(password)s\" -h %(host)s "
                       "-e \"%(sql)s\"") % {'user': user, 'password': password,
                                            'host': host, 'sql': sql}
                self.execute_cmd(cmd)
            elif conn_string.startswith('postgresql'):
                self._reset_pg(conn_pieces)


class WalkVersionsMixin(object):
    def _walk_versions(self, engine=None, snake_walk=False, downgrade=True):
        # Determine latest version script from the repo, then
        # upgrade from 1 through to the latest, with no data
        # in the databases. This just checks that the schema itself
        # upgrades successfully.

        # Place the database under version control
        self.migration_api.version_control(engine, self.REPOSITORY,
                                           self.INIT_VERSION)
        self.assertEqual(self.INIT_VERSION,
                         self.migration_api.db_version(engine,
                                                       self.REPOSITORY))

        LOG.debug('latest version is %s' % self.REPOSITORY.latest)
        versions = range(self.INIT_VERSION + 1, self.REPOSITORY.latest + 1)

        for version in versions:
            # upgrade -> downgrade -> upgrade
            self._migrate_up(engine, version, with_data=True)
            if snake_walk:
                downgraded = self._migrate_down(
                    engine, version - 1, with_data=True)
                if downgraded:
                    self._migrate_up(engine, version)

        if downgrade:
            # Now walk it back down to 0 from the latest, testing
            # the downgrade paths.
            for version in reversed(versions):
                # downgrade -> upgrade -> downgrade
                downgraded = self._migrate_down(engine, version - 1)

                if snake_walk and downgraded:
                    self._migrate_up(engine, version)
                    self._migrate_down(engine, version - 1)

    def _migrate_down(self, engine, version, with_data=False):
        try:
            self.migration_api.downgrade(engine, self.REPOSITORY, version)
        except NotImplementedError:
            # NOTE(sirp): some migrations, namely release-level
            # migrations, don't support a downgrade.
            return False

        self.assertEqual(
            version, self.migration_api.db_version(engine, self.REPOSITORY))

        # NOTE(sirp): `version` is what we're downgrading to (i.e. the 'target'
        # version). So if we have any downgrade checks, they need to be run for
        # the previous (higher numbered) migration.
        if with_data:
            post_downgrade = getattr(
                self, "_post_downgrade_%03d" % (version + 1), None)
            if post_downgrade:
                post_downgrade(engine)

        return True

    def _migrate_up(self, engine, version, with_data=False):
        """migrate up to a new version of the db.

        We allow for data insertion and post checks at every
        migration version with special _pre_upgrade_### and
        _check_### functions in the main test.
        """
        # NOTE(sdague): try block is here because it's impossible to debug
        # where a failed data migration happens otherwise
        try:
            if with_data:
                data = None
                pre_upgrade = getattr(
                    self, "_pre_upgrade_%03d" % version, None)
                if pre_upgrade:
                    data = pre_upgrade(engine)

            self.migration_api.upgrade(engine, self.REPOSITORY, version)
            self.assertEqual(version,
                             self.migration_api.db_version(engine,
                                                           self.REPOSITORY))
            if with_data:
                check = getattr(self, "_check_%03d" % version, None)
                if check:
                    check(engine, data)
        except Exception:
            LOG.error(_LE("Failed to migrate to version %s on engine %s") %
                      (version, engine))
            raise

########NEW FILE########
__FILENAME__ = utils
# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# Copyright 2010-2011 OpenStack Foundation.
# Copyright 2012 Justin Santa Barbara
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import logging
import re

import sqlalchemy
from sqlalchemy import Boolean
from sqlalchemy import CheckConstraint
from sqlalchemy import Column
from sqlalchemy.engine import reflection
from sqlalchemy.ext.compiler import compiles
from sqlalchemy import func
from sqlalchemy import Index
from sqlalchemy import Integer
from sqlalchemy import MetaData
from sqlalchemy import or_
from sqlalchemy.sql.expression import literal_column
from sqlalchemy.sql.expression import UpdateBase
from sqlalchemy import String
from sqlalchemy import Table
from sqlalchemy.types import NullType

from ceilometer.openstack.common import context as request_context
from ceilometer.openstack.common.db.sqlalchemy import models
from ceilometer.openstack.common.gettextutils import _, _LI, _LW
from ceilometer.openstack.common import timeutils


LOG = logging.getLogger(__name__)

_DBURL_REGEX = re.compile(r"[^:]+://([^:]+):([^@]+)@.+")


def sanitize_db_url(url):
    match = _DBURL_REGEX.match(url)
    if match:
        return '%s****:****%s' % (url[:match.start(1)], url[match.end(2):])
    return url


class InvalidSortKey(Exception):
    message = _("Sort key supplied was not valid.")


# copy from glance/db/sqlalchemy/api.py
def paginate_query(query, model, limit, sort_keys, marker=None,
                   sort_dir=None, sort_dirs=None):
    """Returns a query with sorting / pagination criteria added.

    Pagination works by requiring a unique sort_key, specified by sort_keys.
    (If sort_keys is not unique, then we risk looping through values.)
    We use the last row in the previous page as the 'marker' for pagination.
    So we must return values that follow the passed marker in the order.
    With a single-valued sort_key, this would be easy: sort_key > X.
    With a compound-values sort_key, (k1, k2, k3) we must do this to repeat
    the lexicographical ordering:
    (k1 > X1) or (k1 == X1 && k2 > X2) or (k1 == X1 && k2 == X2 && k3 > X3)

    We also have to cope with different sort_directions.

    Typically, the id of the last row is used as the client-facing pagination
    marker, then the actual marker object must be fetched from the db and
    passed in to us as marker.

    :param query: the query object to which we should add paging/sorting
    :param model: the ORM model class
    :param limit: maximum number of items to return
    :param sort_keys: array of attributes by which results should be sorted
    :param marker: the last item of the previous page; we returns the next
                    results after this value.
    :param sort_dir: direction in which results should be sorted (asc, desc)
    :param sort_dirs: per-column array of sort_dirs, corresponding to sort_keys

    :rtype: sqlalchemy.orm.query.Query
    :return: The query with sorting/pagination added.
    """

    if 'id' not in sort_keys:
        # TODO(justinsb): If this ever gives a false-positive, check
        # the actual primary key, rather than assuming its id
        LOG.warning(_LW('Id not in sort_keys; is sort_keys unique?'))

    assert(not (sort_dir and sort_dirs))

    # Default the sort direction to ascending
    if sort_dirs is None and sort_dir is None:
        sort_dir = 'asc'

    # Ensure a per-column sort direction
    if sort_dirs is None:
        sort_dirs = [sort_dir for _sort_key in sort_keys]

    assert(len(sort_dirs) == len(sort_keys))

    # Add sorting
    for current_sort_key, current_sort_dir in zip(sort_keys, sort_dirs):
        try:
            sort_dir_func = {
                'asc': sqlalchemy.asc,
                'desc': sqlalchemy.desc,
            }[current_sort_dir]
        except KeyError:
            raise ValueError(_("Unknown sort direction, "
                               "must be 'desc' or 'asc'"))
        try:
            sort_key_attr = getattr(model, current_sort_key)
        except AttributeError:
            raise InvalidSortKey()
        query = query.order_by(sort_dir_func(sort_key_attr))

    # Add pagination
    if marker is not None:
        marker_values = []
        for sort_key in sort_keys:
            v = getattr(marker, sort_key)
            marker_values.append(v)

        # Build up an array of sort criteria as in the docstring
        criteria_list = []
        for i in range(len(sort_keys)):
            crit_attrs = []
            for j in range(i):
                model_attr = getattr(model, sort_keys[j])
                crit_attrs.append((model_attr == marker_values[j]))

            model_attr = getattr(model, sort_keys[i])
            if sort_dirs[i] == 'desc':
                crit_attrs.append((model_attr < marker_values[i]))
            else:
                crit_attrs.append((model_attr > marker_values[i]))

            criteria = sqlalchemy.sql.and_(*crit_attrs)
            criteria_list.append(criteria)

        f = sqlalchemy.sql.or_(*criteria_list)
        query = query.filter(f)

    if limit is not None:
        query = query.limit(limit)

    return query


def _read_deleted_filter(query, db_model, read_deleted):
    if 'deleted' not in db_model.__table__.columns:
        raise ValueError(_("There is no `deleted` column in `%s` table. "
                           "Project doesn't use soft-deleted feature.")
                         % db_model.__name__)

    default_deleted_value = db_model.__table__.c.deleted.default.arg
    if read_deleted == 'no':
        query = query.filter(db_model.deleted == default_deleted_value)
    elif read_deleted == 'yes':
        pass  # omit the filter to include deleted and active
    elif read_deleted == 'only':
        query = query.filter(db_model.deleted != default_deleted_value)
    else:
        raise ValueError(_("Unrecognized read_deleted value '%s'")
                         % read_deleted)
    return query


def _project_filter(query, db_model, context, project_only):
    if project_only and 'project_id' not in db_model.__table__.columns:
        raise ValueError(_("There is no `project_id` column in `%s` table.")
                         % db_model.__name__)

    if request_context.is_user_context(context) and project_only:
        if project_only == 'allow_none':
            is_none = None
            query = query.filter(or_(db_model.project_id == context.project_id,
                                     db_model.project_id == is_none))
        else:
            query = query.filter(db_model.project_id == context.project_id)

    return query


def model_query(context, model, session, args=None, project_only=False,
                read_deleted=None):
    """Query helper that accounts for context's `read_deleted` field.

    :param context:      context to query under

    :param model:        Model to query. Must be a subclass of ModelBase.
    :type model:         models.ModelBase

    :param session:      The session to use.
    :type session:       sqlalchemy.orm.session.Session

    :param args:         Arguments to query. If None - model is used.
    :type args:          tuple

    :param project_only: If present and context is user-type, then restrict
                         query to match the context's project_id. If set to
                         'allow_none', restriction includes project_id = None.
    :type project_only:  bool

    :param read_deleted: If present, overrides context's read_deleted field.
    :type read_deleted:   bool

    Usage:

    ..code:: python

        result = (utils.model_query(context, models.Instance, session=session)
                       .filter_by(uuid=instance_uuid)
                       .all())

        query = utils.model_query(
                    context, Node,
                    session=session,
                    args=(func.count(Node.id), func.sum(Node.ram))
                    ).filter_by(project_id=project_id)

    """

    if not read_deleted:
        if hasattr(context, 'read_deleted'):
            # NOTE(viktors): some projects use `read_deleted` attribute in
            # their contexts instead of `show_deleted`.
            read_deleted = context.read_deleted
        else:
            read_deleted = context.show_deleted

    if not issubclass(model, models.ModelBase):
        raise TypeError(_("model should be a subclass of ModelBase"))

    query = session.query(model) if not args else session.query(*args)
    query = _read_deleted_filter(query, model, read_deleted)
    query = _project_filter(query, model, context, project_only)

    return query


def get_table(engine, name):
    """Returns an sqlalchemy table dynamically from db.

    Needed because the models don't work for us in migrations
    as models will be far out of sync with the current data.
    """
    metadata = MetaData()
    metadata.bind = engine
    return Table(name, metadata, autoload=True)


class InsertFromSelect(UpdateBase):
    """Form the base for `INSERT INTO table (SELECT ... )` statement."""
    def __init__(self, table, select):
        self.table = table
        self.select = select


@compiles(InsertFromSelect)
def visit_insert_from_select(element, compiler, **kw):
    """Form the `INSERT INTO table (SELECT ... )` statement."""
    return "INSERT INTO %s %s" % (
        compiler.process(element.table, asfrom=True),
        compiler.process(element.select))


class ColumnError(Exception):
    """Error raised when no column or an invalid column is found."""


def _get_not_supported_column(col_name_col_instance, column_name):
    try:
        column = col_name_col_instance[column_name]
    except KeyError:
        msg = _("Please specify column %s in col_name_col_instance "
                "param. It is required because column has unsupported "
                "type by sqlite).")
        raise ColumnError(msg % column_name)

    if not isinstance(column, Column):
        msg = _("col_name_col_instance param has wrong type of "
                "column instance for column %s It should be instance "
                "of sqlalchemy.Column.")
        raise ColumnError(msg % column_name)
    return column


def drop_unique_constraint(migrate_engine, table_name, uc_name, *columns,
                           **col_name_col_instance):
    """Drop unique constraint from table.

    DEPRECATED: this function is deprecated and will be removed from ceilometer.db
    in a few releases. Please use UniqueConstraint.drop() method directly for
    sqlalchemy-migrate migration scripts.

    This method drops UC from table and works for mysql, postgresql and sqlite.
    In mysql and postgresql we are able to use "alter table" construction.
    Sqlalchemy doesn't support some sqlite column types and replaces their
    type with NullType in metadata. We process these columns and replace
    NullType with the correct column type.

    :param migrate_engine: sqlalchemy engine
    :param table_name:     name of table that contains uniq constraint.
    :param uc_name:        name of uniq constraint that will be dropped.
    :param columns:        columns that are in uniq constraint.
    :param col_name_col_instance:   contains pair column_name=column_instance.
                            column_instance is instance of Column. These params
                            are required only for columns that have unsupported
                            types by sqlite. For example BigInteger.
    """

    from migrate.changeset import UniqueConstraint

    meta = MetaData()
    meta.bind = migrate_engine
    t = Table(table_name, meta, autoload=True)

    if migrate_engine.name == "sqlite":
        override_cols = [
            _get_not_supported_column(col_name_col_instance, col.name)
            for col in t.columns
            if isinstance(col.type, NullType)
        ]
        for col in override_cols:
            t.columns.replace(col)

    uc = UniqueConstraint(*columns, table=t, name=uc_name)
    uc.drop()


def drop_old_duplicate_entries_from_table(migrate_engine, table_name,
                                          use_soft_delete, *uc_column_names):
    """Drop all old rows having the same values for columns in uc_columns.

    This method drop (or mark ad `deleted` if use_soft_delete is True) old
    duplicate rows form table with name `table_name`.

    :param migrate_engine:  Sqlalchemy engine
    :param table_name:      Table with duplicates
    :param use_soft_delete: If True - values will be marked as `deleted`,
                            if False - values will be removed from table
    :param uc_column_names: Unique constraint columns
    """
    meta = MetaData()
    meta.bind = migrate_engine

    table = Table(table_name, meta, autoload=True)
    columns_for_group_by = [table.c[name] for name in uc_column_names]

    columns_for_select = [func.max(table.c.id)]
    columns_for_select.extend(columns_for_group_by)

    duplicated_rows_select = sqlalchemy.sql.select(
        columns_for_select, group_by=columns_for_group_by,
        having=func.count(table.c.id) > 1)

    for row in migrate_engine.execute(duplicated_rows_select):
        # NOTE(boris-42): Do not remove row that has the biggest ID.
        delete_condition = table.c.id != row[0]
        is_none = None  # workaround for pyflakes
        delete_condition &= table.c.deleted_at == is_none
        for name in uc_column_names:
            delete_condition &= table.c[name] == row[name]

        rows_to_delete_select = sqlalchemy.sql.select(
            [table.c.id]).where(delete_condition)
        for row in migrate_engine.execute(rows_to_delete_select).fetchall():
            LOG.info(_LI("Deleting duplicated row with id: %(id)s from table: "
                         "%(table)s") % dict(id=row[0], table=table_name))

        if use_soft_delete:
            delete_statement = table.update().\
                where(delete_condition).\
                values({
                    'deleted': literal_column('id'),
                    'updated_at': literal_column('updated_at'),
                    'deleted_at': timeutils.utcnow()
                })
        else:
            delete_statement = table.delete().where(delete_condition)
        migrate_engine.execute(delete_statement)


def _get_default_deleted_value(table):
    if isinstance(table.c.id.type, Integer):
        return 0
    if isinstance(table.c.id.type, String):
        return ""
    raise ColumnError(_("Unsupported id columns type"))


def _restore_indexes_on_deleted_columns(migrate_engine, table_name, indexes):
    table = get_table(migrate_engine, table_name)

    insp = reflection.Inspector.from_engine(migrate_engine)
    real_indexes = insp.get_indexes(table_name)
    existing_index_names = dict(
        [(index['name'], index['column_names']) for index in real_indexes])

    # NOTE(boris-42): Restore indexes on `deleted` column
    for index in indexes:
        if 'deleted' not in index['column_names']:
            continue
        name = index['name']
        if name in existing_index_names:
            column_names = [table.c[c] for c in existing_index_names[name]]
            old_index = Index(name, *column_names, unique=index["unique"])
            old_index.drop(migrate_engine)

        column_names = [table.c[c] for c in index['column_names']]
        new_index = Index(index["name"], *column_names, unique=index["unique"])
        new_index.create(migrate_engine)


def change_deleted_column_type_to_boolean(migrate_engine, table_name,
                                          **col_name_col_instance):
    if migrate_engine.name == "sqlite":
        return _change_deleted_column_type_to_boolean_sqlite(
            migrate_engine, table_name, **col_name_col_instance)
    insp = reflection.Inspector.from_engine(migrate_engine)
    indexes = insp.get_indexes(table_name)

    table = get_table(migrate_engine, table_name)

    old_deleted = Column('old_deleted', Boolean, default=False)
    old_deleted.create(table, populate_default=False)

    table.update().\
        where(table.c.deleted == table.c.id).\
        values(old_deleted=True).\
        execute()

    table.c.deleted.drop()
    table.c.old_deleted.alter(name="deleted")

    _restore_indexes_on_deleted_columns(migrate_engine, table_name, indexes)


def _change_deleted_column_type_to_boolean_sqlite(migrate_engine, table_name,
                                                  **col_name_col_instance):
    insp = reflection.Inspector.from_engine(migrate_engine)
    table = get_table(migrate_engine, table_name)

    columns = []
    for column in table.columns:
        column_copy = None
        if column.name != "deleted":
            if isinstance(column.type, NullType):
                column_copy = _get_not_supported_column(col_name_col_instance,
                                                        column.name)
            else:
                column_copy = column.copy()
        else:
            column_copy = Column('deleted', Boolean, default=0)
        columns.append(column_copy)

    constraints = [constraint.copy() for constraint in table.constraints]

    meta = table.metadata
    new_table = Table(table_name + "__tmp__", meta,
                      *(columns + constraints))
    new_table.create()

    indexes = []
    for index in insp.get_indexes(table_name):
        column_names = [new_table.c[c] for c in index['column_names']]
        indexes.append(Index(index["name"], *column_names,
                             unique=index["unique"]))

    c_select = []
    for c in table.c:
        if c.name != "deleted":
            c_select.append(c)
        else:
            c_select.append(table.c.deleted == table.c.id)

    ins = InsertFromSelect(new_table, sqlalchemy.sql.select(c_select))
    migrate_engine.execute(ins)

    table.drop()
    [index.create(migrate_engine) for index in indexes]

    new_table.rename(table_name)
    new_table.update().\
        where(new_table.c.deleted == new_table.c.id).\
        values(deleted=True).\
        execute()


def change_deleted_column_type_to_id_type(migrate_engine, table_name,
                                          **col_name_col_instance):
    if migrate_engine.name == "sqlite":
        return _change_deleted_column_type_to_id_type_sqlite(
            migrate_engine, table_name, **col_name_col_instance)
    insp = reflection.Inspector.from_engine(migrate_engine)
    indexes = insp.get_indexes(table_name)

    table = get_table(migrate_engine, table_name)

    new_deleted = Column('new_deleted', table.c.id.type,
                         default=_get_default_deleted_value(table))
    new_deleted.create(table, populate_default=True)

    deleted = True  # workaround for pyflakes
    table.update().\
        where(table.c.deleted == deleted).\
        values(new_deleted=table.c.id).\
        execute()
    table.c.deleted.drop()
    table.c.new_deleted.alter(name="deleted")

    _restore_indexes_on_deleted_columns(migrate_engine, table_name, indexes)


def _change_deleted_column_type_to_id_type_sqlite(migrate_engine, table_name,
                                                  **col_name_col_instance):
    # NOTE(boris-42): sqlaclhemy-migrate can't drop column with check
    #                 constraints in sqlite DB and our `deleted` column has
    #                 2 check constraints. So there is only one way to remove
    #                 these constraints:
    #                 1) Create new table with the same columns, constraints
    #                 and indexes. (except deleted column).
    #                 2) Copy all data from old to new table.
    #                 3) Drop old table.
    #                 4) Rename new table to old table name.
    insp = reflection.Inspector.from_engine(migrate_engine)
    meta = MetaData(bind=migrate_engine)
    table = Table(table_name, meta, autoload=True)
    default_deleted_value = _get_default_deleted_value(table)

    columns = []
    for column in table.columns:
        column_copy = None
        if column.name != "deleted":
            if isinstance(column.type, NullType):
                column_copy = _get_not_supported_column(col_name_col_instance,
                                                        column.name)
            else:
                column_copy = column.copy()
        else:
            column_copy = Column('deleted', table.c.id.type,
                                 default=default_deleted_value)
        columns.append(column_copy)

    def is_deleted_column_constraint(constraint):
        # NOTE(boris-42): There is no other way to check is CheckConstraint
        #                 associated with deleted column.
        if not isinstance(constraint, CheckConstraint):
            return False
        sqltext = str(constraint.sqltext)
        return (sqltext.endswith("deleted in (0, 1)") or
                sqltext.endswith("deleted IN (:deleted_1, :deleted_2)"))

    constraints = []
    for constraint in table.constraints:
        if not is_deleted_column_constraint(constraint):
            constraints.append(constraint.copy())

    new_table = Table(table_name + "__tmp__", meta,
                      *(columns + constraints))
    new_table.create()

    indexes = []
    for index in insp.get_indexes(table_name):
        column_names = [new_table.c[c] for c in index['column_names']]
        indexes.append(Index(index["name"], *column_names,
                             unique=index["unique"]))

    ins = InsertFromSelect(new_table, table.select())
    migrate_engine.execute(ins)

    table.drop()
    [index.create(migrate_engine) for index in indexes]

    new_table.rename(table_name)
    deleted = True  # workaround for pyflakes
    new_table.update().\
        where(new_table.c.deleted == deleted).\
        values(deleted=new_table.c.id).\
        execute()

    # NOTE(boris-42): Fix value of deleted column: False -> "" or 0.
    deleted = False  # workaround for pyflakes
    new_table.update().\
        where(new_table.c.deleted == deleted).\
        values(deleted=default_deleted_value).\
        execute()


def get_connect_string(backend, database, user=None, passwd=None):
    """Get database connection

    Try to get a connection with a very specific set of values, if we get
    these then we'll run the tests, otherwise they are skipped
    """
    args = {'backend': backend,
            'user': user,
            'passwd': passwd,
            'database': database}
    if backend == 'sqlite':
        template = '%(backend)s:///%(database)s'
    else:
        template = "%(backend)s://%(user)s:%(passwd)s@localhost/%(database)s"
    return template % args


def is_backend_avail(backend, database, user=None, passwd=None):
    try:
        connect_uri = get_connect_string(backend=backend,
                                         database=database,
                                         user=user,
                                         passwd=passwd)
        engine = sqlalchemy.create_engine(connect_uri)
        connection = engine.connect()
    except Exception:
        # intentionally catch all to handle exceptions even if we don't
        # have any backend code loaded.
        return False
    else:
        connection.close()
        engine.dispose()
        return True


def get_db_connection_info(conn_pieces):
    database = conn_pieces.path.strip('/')
    loc_pieces = conn_pieces.netloc.split('@')
    host = loc_pieces[1]

    auth_pieces = loc_pieces[0].split(':')
    user = auth_pieces[0]
    password = ""
    if len(auth_pieces) > 1:
        password = auth_pieces[1].strip()

    return (user, password, database, host)

########NEW FILE########
__FILENAME__ = eventlet_backdoor
# Copyright (c) 2012 OpenStack Foundation.
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from __future__ import print_function

import errno
import gc
import os
import pprint
import socket
import sys
import traceback

import eventlet
import eventlet.backdoor
import greenlet
from oslo.config import cfg

from ceilometer.openstack.common.gettextutils import _LI
from ceilometer.openstack.common import log as logging

help_for_backdoor_port = (
    "Acceptable values are 0, <port>, and <start>:<end>, where 0 results "
    "in listening on a random tcp port number; <port> results in listening "
    "on the specified port number (and not enabling backdoor if that port "
    "is in use); and <start>:<end> results in listening on the smallest "
    "unused port number within the specified range of port numbers.  The "
    "chosen port is displayed in the service's log file.")
eventlet_backdoor_opts = [
    cfg.StrOpt('backdoor_port',
               default=None,
               help="Enable eventlet backdoor.  %s" % help_for_backdoor_port)
]

CONF = cfg.CONF
CONF.register_opts(eventlet_backdoor_opts)
LOG = logging.getLogger(__name__)


class EventletBackdoorConfigValueError(Exception):
    def __init__(self, port_range, help_msg, ex):
        msg = ('Invalid backdoor_port configuration %(range)s: %(ex)s. '
               '%(help)s' %
               {'range': port_range, 'ex': ex, 'help': help_msg})
        super(EventletBackdoorConfigValueError, self).__init__(msg)
        self.port_range = port_range


def _dont_use_this():
    print("Don't use this, just disconnect instead")


def _find_objects(t):
    return [o for o in gc.get_objects() if isinstance(o, t)]


def _print_greenthreads():
    for i, gt in enumerate(_find_objects(greenlet.greenlet)):
        print(i, gt)
        traceback.print_stack(gt.gr_frame)
        print()


def _print_nativethreads():
    for threadId, stack in sys._current_frames().items():
        print(threadId)
        traceback.print_stack(stack)
        print()


def _parse_port_range(port_range):
    if ':' not in port_range:
        start, end = port_range, port_range
    else:
        start, end = port_range.split(':', 1)
    try:
        start, end = int(start), int(end)
        if end < start:
            raise ValueError
        return start, end
    except ValueError as ex:
        raise EventletBackdoorConfigValueError(port_range, ex,
                                               help_for_backdoor_port)


def _listen(host, start_port, end_port, listen_func):
    try_port = start_port
    while True:
        try:
            return listen_func((host, try_port))
        except socket.error as exc:
            if (exc.errno != errno.EADDRINUSE or
               try_port >= end_port):
                raise
            try_port += 1


def initialize_if_enabled():
    backdoor_locals = {
        'exit': _dont_use_this,      # So we don't exit the entire process
        'quit': _dont_use_this,      # So we don't exit the entire process
        'fo': _find_objects,
        'pgt': _print_greenthreads,
        'pnt': _print_nativethreads,
    }

    if CONF.backdoor_port is None:
        return None

    start_port, end_port = _parse_port_range(str(CONF.backdoor_port))

    # NOTE(johannes): The standard sys.displayhook will print the value of
    # the last expression and set it to __builtin__._, which overwrites
    # the __builtin__._ that gettext sets. Let's switch to using pprint
    # since it won't interact poorly with gettext, and it's easier to
    # read the output too.
    def displayhook(val):
        if val is not None:
            pprint.pprint(val)
    sys.displayhook = displayhook

    sock = _listen('localhost', start_port, end_port, eventlet.listen)

    # In the case of backdoor port being zero, a port number is assigned by
    # listen().  In any case, pull the port number out here.
    port = sock.getsockname()[1]
    LOG.info(
        _LI('Eventlet backdoor listening on %(port)s for process %(pid)d') %
        {'port': port, 'pid': os.getpid()}
    )
    eventlet.spawn_n(eventlet.backdoor.backdoor_server, sock,
                     locals=backdoor_locals)
    return port

########NEW FILE########
__FILENAME__ = excutils
# Copyright 2011 OpenStack Foundation.
# Copyright 2012, Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Exception related utilities.
"""

import logging
import sys
import time
import traceback

import six

from ceilometer.openstack.common.gettextutils import _LE


class save_and_reraise_exception(object):
    """Save current exception, run some code and then re-raise.

    In some cases the exception context can be cleared, resulting in None
    being attempted to be re-raised after an exception handler is run. This
    can happen when eventlet switches greenthreads or when running an
    exception handler, code raises and catches an exception. In both
    cases the exception context will be cleared.

    To work around this, we save the exception state, run handler code, and
    then re-raise the original exception. If another exception occurs, the
    saved exception is logged and the new exception is re-raised.

    In some cases the caller may not want to re-raise the exception, and
    for those circumstances this context provides a reraise flag that
    can be used to suppress the exception.  For example::

      except Exception:
          with save_and_reraise_exception() as ctxt:
              decide_if_need_reraise()
              if not should_be_reraised:
                  ctxt.reraise = False

    If another exception occurs and reraise flag is False,
    the saved exception will not be logged.

    If the caller wants to raise new exception during exception handling
    he/she sets reraise to False initially with an ability to set it back to
    True if needed::

      except Exception:
          with save_and_reraise_exception(reraise=False) as ctxt:
              [if statements to determine whether to raise a new exception]
              # Not raising a new exception, so reraise
              ctxt.reraise = True
    """
    def __init__(self, reraise=True):
        self.reraise = reraise

    def __enter__(self):
        self.type_, self.value, self.tb, = sys.exc_info()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if exc_type is not None:
            if self.reraise:
                logging.error(_LE('Original exception being dropped: %s'),
                              traceback.format_exception(self.type_,
                                                         self.value,
                                                         self.tb))
            return False
        if self.reraise:
            six.reraise(self.type_, self.value, self.tb)


def forever_retry_uncaught_exceptions(infunc):
    def inner_func(*args, **kwargs):
        last_log_time = 0
        last_exc_message = None
        exc_count = 0
        while True:
            try:
                return infunc(*args, **kwargs)
            except Exception as exc:
                this_exc_message = six.u(str(exc))
                if this_exc_message == last_exc_message:
                    exc_count += 1
                else:
                    exc_count = 1
                # Do not log any more frequently than once a minute unless
                # the exception message changes
                cur_time = int(time.time())
                if (cur_time - last_log_time > 60 or
                        this_exc_message != last_exc_message):
                    logging.exception(
                        _LE('Unexpected exception occurred %d time(s)... '
                            'retrying.') % exc_count)
                    last_log_time = cur_time
                    last_exc_message = this_exc_message
                    exc_count = 0
                # This should be a very rare event. In case it isn't, do
                # a sleep.
                time.sleep(1)
    return inner_func

########NEW FILE########
__FILENAME__ = fileutils
# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import contextlib
import errno
import os
import tempfile

from ceilometer.openstack.common import excutils
from ceilometer.openstack.common import log as logging

LOG = logging.getLogger(__name__)

_FILE_CACHE = {}


def ensure_tree(path):
    """Create a directory (and any ancestor directories required)

    :param path: Directory to create
    """
    try:
        os.makedirs(path)
    except OSError as exc:
        if exc.errno == errno.EEXIST:
            if not os.path.isdir(path):
                raise
        else:
            raise


def read_cached_file(filename, force_reload=False):
    """Read from a file if it has been modified.

    :param force_reload: Whether to reload the file.
    :returns: A tuple with a boolean specifying if the data is fresh
              or not.
    """
    global _FILE_CACHE

    if force_reload and filename in _FILE_CACHE:
        del _FILE_CACHE[filename]

    reloaded = False
    mtime = os.path.getmtime(filename)
    cache_info = _FILE_CACHE.setdefault(filename, {})

    if not cache_info or mtime > cache_info.get('mtime', 0):
        LOG.debug("Reloading cached file %s" % filename)
        with open(filename) as fap:
            cache_info['data'] = fap.read()
        cache_info['mtime'] = mtime
        reloaded = True
    return (reloaded, cache_info['data'])


def delete_if_exists(path, remove=os.unlink):
    """Delete a file, but ignore file not found error.

    :param path: File to delete
    :param remove: Optional function to remove passed path
    """

    try:
        remove(path)
    except OSError as e:
        if e.errno != errno.ENOENT:
            raise


@contextlib.contextmanager
def remove_path_on_error(path, remove=delete_if_exists):
    """Protect code that wants to operate on PATH atomically.
    Any exception will cause PATH to be removed.

    :param path: File to work with
    :param remove: Optional function to remove passed path
    """

    try:
        yield
    except Exception:
        with excutils.save_and_reraise_exception():
            remove(path)


def file_open(*args, **kwargs):
    """Open file

    see built-in file() documentation for more details

    Note: The reason this is kept in a separate module is to easily
    be able to provide a stub module that doesn't alter system
    state at all (for unit tests)
    """
    return file(*args, **kwargs)


def write_to_tempfile(content, path=None, suffix='', prefix='tmp'):
    """Create temporary file or use existing file.

    This util is needed for creating temporary file with
    specified content, suffix and prefix. If path is not None,
    it will be used for writing content. If the path doesn't
    exist it'll be created.

    :param content: content for temporary file.
    :param path: same as parameter 'dir' for mkstemp
    :param suffix: same as parameter 'suffix' for mkstemp
    :param prefix: same as parameter 'prefix' for mkstemp

    For example: it can be used in database tests for creating
    configuration files.
    """
    if path:
        ensure_tree(path)

    (fd, path) = tempfile.mkstemp(suffix=suffix, dir=path, prefix=prefix)
    try:
        os.write(fd, content)
    finally:
        os.close(fd)
    return path

########NEW FILE########
__FILENAME__ = config
#
# Copyright 2013 Mirantis, Inc.
# Copyright 2013 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import fixtures
from oslo.config import cfg
import six


class Config(fixtures.Fixture):
    """Allows overriding configuration settings for the test.

    `conf` will be reset on cleanup.

    """

    def __init__(self, conf=cfg.CONF):
        self.conf = conf

    def setUp(self):
        super(Config, self).setUp()
        # NOTE(morganfainberg): unregister must be added to cleanup before
        # reset is because cleanup works in reverse order of registered items,
        # and a reset must occur before unregistering options can occur.
        self.addCleanup(self._unregister_config_opts)
        self.addCleanup(self.conf.reset)
        self._registered_config_opts = {}

    def config(self, **kw):
        """Override configuration values.

        The keyword arguments are the names of configuration options to
        override and their values.

        If a `group` argument is supplied, the overrides are applied to
        the specified configuration option group, otherwise the overrides
        are applied to the ``default`` group.

        """

        group = kw.pop('group', None)
        for k, v in six.iteritems(kw):
            self.conf.set_override(k, v, group)

    def _unregister_config_opts(self):
        for group in self._registered_config_opts:
            self.conf.unregister_opts(self._registered_config_opts[group],
                                      group=group)

    def register_opt(self, opt, group=None):
        """Register a single option for the test run.

        Options registered in this manner will automatically be unregistered
        during cleanup.

        If a `group` argument is supplied, it will register the new option
        to that group, otherwise the option is registered to the ``default``
        group.
        """
        self.conf.register_opt(opt, group=group)
        self._registered_config_opts.setdefault(group, set()).add(opt)

    def register_opts(self, opts, group=None):
        """Register multiple options for the test run.

        This works in the same manner as register_opt() but takes a list of
        options as the first argument. All arguments will be registered to the
        same group if the ``group`` argument is supplied, otherwise all options
        will be registered to the ``default`` group.
        """
        for opt in opts:
            self.register_opt(opt, group=group)

########NEW FILE########
__FILENAME__ = lockutils
# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import fixtures

from ceilometer.openstack.common import lockutils


class LockFixture(fixtures.Fixture):
    """External locking fixture.

    This fixture is basically an alternative to the synchronized decorator with
    the external flag so that tearDowns and addCleanups will be included in
    the lock context for locking between tests. The fixture is recommended to
    be the first line in a test method, like so::

        def test_method(self):
            self.useFixture(LockFixture)
                ...

    or the first line in setUp if all the test methods in the class are
    required to be serialized. Something like::

        class TestCase(testtools.testcase):
            def setUp(self):
                self.useFixture(LockFixture)
                super(TestCase, self).setUp()
                    ...

    This is because addCleanups are put on a LIFO queue that gets run after the
    test method exits. (either by completing or raising an exception)
    """
    def __init__(self, name, lock_file_prefix=None):
        self.mgr = lockutils.lock(name, lock_file_prefix, True)

    def setUp(self):
        super(LockFixture, self).setUp()
        self.addCleanup(self.mgr.__exit__, None, None, None)
        self.lock = self.mgr.__enter__()

########NEW FILE########
__FILENAME__ = logging
# All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import fixtures


def get_logging_handle_error_fixture():
    """returns a fixture to make logging raise formatting exceptions.

    Usage:
    self.useFixture(logging.get_logging_handle_error_fixture())
    """
    return fixtures.MonkeyPatch('logging.Handler.handleError',
                                _handleError)


def _handleError(self, record):
    """Monkey patch for logging.Handler.handleError.

    The default handleError just logs the error to stderr but we want
    the option of actually raising an exception.
    """
    raise

########NEW FILE########
__FILENAME__ = mockpatch
# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

##############################################################################
##############################################################################
##
## DO NOT MODIFY THIS FILE
##
## This file is being graduated to the ceilometertest library. Please make all
## changes there, and only backport critical fixes here. - dhellmann
##
##############################################################################
##############################################################################

import fixtures
import mock


class PatchObject(fixtures.Fixture):
    """Deal with code around mock."""

    def __init__(self, obj, attr, new=mock.DEFAULT, **kwargs):
        self.obj = obj
        self.attr = attr
        self.kwargs = kwargs
        self.new = new

    def setUp(self):
        super(PatchObject, self).setUp()
        _p = mock.patch.object(self.obj, self.attr, self.new, **self.kwargs)
        self.mock = _p.start()
        self.addCleanup(_p.stop)


class Patch(fixtures.Fixture):

    """Deal with code around mock.patch."""

    def __init__(self, obj, new=mock.DEFAULT, **kwargs):
        self.obj = obj
        self.kwargs = kwargs
        self.new = new

    def setUp(self):
        super(Patch, self).setUp()
        _p = mock.patch(self.obj, self.new, **self.kwargs)
        self.mock = _p.start()
        self.addCleanup(_p.stop)

########NEW FILE########
__FILENAME__ = moxstubout
# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

##############################################################################
##############################################################################
##
## DO NOT MODIFY THIS FILE
##
## This file is being graduated to the ceilometertest library. Please make all
## changes there, and only backport critical fixes here. - dhellmann
##
##############################################################################
##############################################################################

import fixtures
from six.moves import mox


class MoxStubout(fixtures.Fixture):
    """Deal with code around mox and stubout as a fixture."""

    def setUp(self):
        super(MoxStubout, self).setUp()
        # emulate some of the mox stuff, we can't use the metaclass
        # because it screws with our generators
        self.mox = mox.Mox()
        self.stubs = self.mox.stubs
        self.addCleanup(self.mox.UnsetStubs)
        self.addCleanup(self.mox.VerifyAll)

########NEW FILE########
__FILENAME__ = gettextutils
# Copyright 2012 Red Hat, Inc.
# Copyright 2013 IBM Corp.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
gettext for openstack-common modules.

Usual usage in an openstack.common module:

    from ceilometer.openstack.common.gettextutils import _
"""

import copy
import functools
import gettext
import locale
from logging import handlers
import os

from babel import localedata
import six

_localedir = os.environ.get('ceilometer'.upper() + '_LOCALEDIR')
_t = gettext.translation('ceilometer', localedir=_localedir, fallback=True)

# We use separate translation catalogs for each log level, so set up a
# mapping between the log level name and the translator. The domain
# for the log level is project_name + "-log-" + log_level so messages
# for each level end up in their own catalog.
_t_log_levels = dict(
    (level, gettext.translation('ceilometer' + '-log-' + level,
                                localedir=_localedir,
                                fallback=True))
    for level in ['info', 'warning', 'error', 'critical']
)

_AVAILABLE_LANGUAGES = {}
USE_LAZY = False


def enable_lazy():
    """Convenience function for configuring _() to use lazy gettext

    Call this at the start of execution to enable the gettextutils._
    function to use lazy gettext functionality. This is useful if
    your project is importing _ directly instead of using the
    gettextutils.install() way of importing the _ function.
    """
    global USE_LAZY
    USE_LAZY = True


def _(msg):
    if USE_LAZY:
        return Message(msg, domain='ceilometer')
    else:
        if six.PY3:
            return _t.gettext(msg)
        return _t.ugettext(msg)


def _log_translation(msg, level):
    """Build a single translation of a log message
    """
    if USE_LAZY:
        return Message(msg, domain='ceilometer' + '-log-' + level)
    else:
        translator = _t_log_levels[level]
        if six.PY3:
            return translator.gettext(msg)
        return translator.ugettext(msg)

# Translators for log levels.
#
# The abbreviated names are meant to reflect the usual use of a short
# name like '_'. The "L" is for "log" and the other letter comes from
# the level.
_LI = functools.partial(_log_translation, level='info')
_LW = functools.partial(_log_translation, level='warning')
_LE = functools.partial(_log_translation, level='error')
_LC = functools.partial(_log_translation, level='critical')


def install(domain, lazy=False):
    """Install a _() function using the given translation domain.

    Given a translation domain, install a _() function using gettext's
    install() function.

    The main difference from gettext.install() is that we allow
    overriding the default localedir (e.g. /usr/share/locale) using
    a translation-domain-specific environment variable (e.g.
    NOVA_LOCALEDIR).

    :param domain: the translation domain
    :param lazy: indicates whether or not to install the lazy _() function.
                 The lazy _() introduces a way to do deferred translation
                 of messages by installing a _ that builds Message objects,
                 instead of strings, which can then be lazily translated into
                 any available locale.
    """
    if lazy:
        # NOTE(mrodden): Lazy gettext functionality.
        #
        # The following introduces a deferred way to do translations on
        # messages in OpenStack. We override the standard _() function
        # and % (format string) operation to build Message objects that can
        # later be translated when we have more information.
        def _lazy_gettext(msg):
            """Create and return a Message object.

            Lazy gettext function for a given domain, it is a factory method
            for a project/module to get a lazy gettext function for its own
            translation domain (i.e. nova, glance, cinder, etc.)

            Message encapsulates a string so that we can translate
            it later when needed.
            """
            return Message(msg, domain=domain)

        from six import moves
        moves.builtins.__dict__['_'] = _lazy_gettext
    else:
        localedir = '%s_LOCALEDIR' % domain.upper()
        if six.PY3:
            gettext.install(domain,
                            localedir=os.environ.get(localedir))
        else:
            gettext.install(domain,
                            localedir=os.environ.get(localedir),
                            unicode=True)


class Message(six.text_type):
    """A Message object is a unicode object that can be translated.

    Translation of Message is done explicitly using the translate() method.
    For all non-translation intents and purposes, a Message is simply unicode,
    and can be treated as such.
    """

    def __new__(cls, msgid, msgtext=None, params=None,
                domain='ceilometer', *args):
        """Create a new Message object.

        In order for translation to work gettext requires a message ID, this
        msgid will be used as the base unicode text. It is also possible
        for the msgid and the base unicode text to be different by passing
        the msgtext parameter.
        """
        # If the base msgtext is not given, we use the default translation
        # of the msgid (which is in English) just in case the system locale is
        # not English, so that the base text will be in that locale by default.
        if not msgtext:
            msgtext = Message._translate_msgid(msgid, domain)
        # We want to initialize the parent unicode with the actual object that
        # would have been plain unicode if 'Message' was not enabled.
        msg = super(Message, cls).__new__(cls, msgtext)
        msg.msgid = msgid
        msg.domain = domain
        msg.params = params
        return msg

    def translate(self, desired_locale=None):
        """Translate this message to the desired locale.

        :param desired_locale: The desired locale to translate the message to,
                               if no locale is provided the message will be
                               translated to the system's default locale.

        :returns: the translated message in unicode
        """

        translated_message = Message._translate_msgid(self.msgid,
                                                      self.domain,
                                                      desired_locale)
        if self.params is None:
            # No need for more translation
            return translated_message

        # This Message object may have been formatted with one or more
        # Message objects as substitution arguments, given either as a single
        # argument, part of a tuple, or as one or more values in a dictionary.
        # When translating this Message we need to translate those Messages too
        translated_params = _translate_args(self.params, desired_locale)

        translated_message = translated_message % translated_params

        return translated_message

    @staticmethod
    def _translate_msgid(msgid, domain, desired_locale=None):
        if not desired_locale:
            system_locale = locale.getdefaultlocale()
            # If the system locale is not available to the runtime use English
            if not system_locale[0]:
                desired_locale = 'en_US'
            else:
                desired_locale = system_locale[0]

        locale_dir = os.environ.get(domain.upper() + '_LOCALEDIR')
        lang = gettext.translation(domain,
                                   localedir=locale_dir,
                                   languages=[desired_locale],
                                   fallback=True)
        if six.PY3:
            translator = lang.gettext
        else:
            translator = lang.ugettext

        translated_message = translator(msgid)
        return translated_message

    def __mod__(self, other):
        # When we mod a Message we want the actual operation to be performed
        # by the parent class (i.e. unicode()), the only thing  we do here is
        # save the original msgid and the parameters in case of a translation
        params = self._sanitize_mod_params(other)
        unicode_mod = super(Message, self).__mod__(params)
        modded = Message(self.msgid,
                         msgtext=unicode_mod,
                         params=params,
                         domain=self.domain)
        return modded

    def _sanitize_mod_params(self, other):
        """Sanitize the object being modded with this Message.

        - Add support for modding 'None' so translation supports it
        - Trim the modded object, which can be a large dictionary, to only
        those keys that would actually be used in a translation
        - Snapshot the object being modded, in case the message is
        translated, it will be used as it was when the Message was created
        """
        if other is None:
            params = (other,)
        elif isinstance(other, dict):
            # Merge the dictionaries
            # Copy each item in case one does not support deep copy.
            params = {}
            if isinstance(self.params, dict):
                for key, val in self.params.items():
                    params[key] = self._copy_param(val)
            for key, val in other.items():
                params[key] = self._copy_param(val)
        else:
            params = self._copy_param(other)
        return params

    def _copy_param(self, param):
        try:
            return copy.deepcopy(param)
        except Exception:
            # Fallback to casting to unicode this will handle the
            # python code-like objects that can't be deep-copied
            return six.text_type(param)

    def __add__(self, other):
        msg = _('Message objects do not support addition.')
        raise TypeError(msg)

    def __radd__(self, other):
        return self.__add__(other)

    def __str__(self):
        # NOTE(luisg): Logging in python 2.6 tries to str() log records,
        # and it expects specifically a UnicodeError in order to proceed.
        msg = _('Message objects do not support str() because they may '
                'contain non-ascii characters. '
                'Please use unicode() or translate() instead.')
        raise UnicodeError(msg)


def get_available_languages(domain):
    """Lists the available languages for the given translation domain.

    :param domain: the domain to get languages for
    """
    if domain in _AVAILABLE_LANGUAGES:
        return copy.copy(_AVAILABLE_LANGUAGES[domain])

    localedir = '%s_LOCALEDIR' % domain.upper()
    find = lambda x: gettext.find(domain,
                                  localedir=os.environ.get(localedir),
                                  languages=[x])

    # NOTE(mrodden): en_US should always be available (and first in case
    # order matters) since our in-line message strings are en_US
    language_list = ['en_US']
    # NOTE(luisg): Babel <1.0 used a function called list(), which was
    # renamed to locale_identifiers() in >=1.0, the requirements master list
    # requires >=0.9.6, uncapped, so defensively work with both. We can remove
    # this check when the master list updates to >=1.0, and update all projects
    list_identifiers = (getattr(localedata, 'list', None) or
                        getattr(localedata, 'locale_identifiers'))
    locale_identifiers = list_identifiers()

    for i in locale_identifiers:
        if find(i) is not None:
            language_list.append(i)

    # NOTE(luisg): Babel>=1.0,<1.3 has a bug where some OpenStack supported
    # locales (e.g. 'zh_CN', and 'zh_TW') aren't supported even though they
    # are perfectly legitimate locales:
    #     https://github.com/mitsuhiko/babel/issues/37
    # In Babel 1.3 they fixed the bug and they support these locales, but
    # they are still not explicitly "listed" by locale_identifiers().
    # That is  why we add the locales here explicitly if necessary so that
    # they are listed as supported.
    aliases = {'zh': 'zh_CN',
               'zh_Hant_HK': 'zh_HK',
               'zh_Hant': 'zh_TW',
               'fil': 'tl_PH'}
    for (locale, alias) in six.iteritems(aliases):
        if locale in language_list and alias not in language_list:
            language_list.append(alias)

    _AVAILABLE_LANGUAGES[domain] = language_list
    return copy.copy(language_list)


def translate(obj, desired_locale=None):
    """Gets the translated unicode representation of the given object.

    If the object is not translatable it is returned as-is.
    If the locale is None the object is translated to the system locale.

    :param obj: the object to translate
    :param desired_locale: the locale to translate the message to, if None the
                           default system locale will be used
    :returns: the translated object in unicode, or the original object if
              it could not be translated
    """
    message = obj
    if not isinstance(message, Message):
        # If the object to translate is not already translatable,
        # let's first get its unicode representation
        message = six.text_type(obj)
    if isinstance(message, Message):
        # Even after unicoding() we still need to check if we are
        # running with translatable unicode before translating
        return message.translate(desired_locale)
    return obj


def _translate_args(args, desired_locale=None):
    """Translates all the translatable elements of the given arguments object.

    This method is used for translating the translatable values in method
    arguments which include values of tuples or dictionaries.
    If the object is not a tuple or a dictionary the object itself is
    translated if it is translatable.

    If the locale is None the object is translated to the system locale.

    :param args: the args to translate
    :param desired_locale: the locale to translate the args to, if None the
                           default system locale will be used
    :returns: a new args object with the translated contents of the original
    """
    if isinstance(args, tuple):
        return tuple(translate(v, desired_locale) for v in args)
    if isinstance(args, dict):
        translated_dict = {}
        for (k, v) in six.iteritems(args):
            translated_v = translate(v, desired_locale)
            translated_dict[k] = translated_v
        return translated_dict
    return translate(args, desired_locale)


class TranslationHandler(handlers.MemoryHandler):
    """Handler that translates records before logging them.

    The TranslationHandler takes a locale and a target logging.Handler object
    to forward LogRecord objects to after translating them. This handler
    depends on Message objects being logged, instead of regular strings.

    The handler can be configured declaratively in the logging.conf as follows:

        [handlers]
        keys = translatedlog, translator

        [handler_translatedlog]
        class = handlers.WatchedFileHandler
        args = ('/var/log/api-localized.log',)
        formatter = context

        [handler_translator]
        class = openstack.common.log.TranslationHandler
        target = translatedlog
        args = ('zh_CN',)

    If the specified locale is not available in the system, the handler will
    log in the default locale.
    """

    def __init__(self, locale=None, target=None):
        """Initialize a TranslationHandler

        :param locale: locale to use for translating messages
        :param target: logging.Handler object to forward
                       LogRecord objects to after translation
        """
        # NOTE(luisg): In order to allow this handler to be a wrapper for
        # other handlers, such as a FileHandler, and still be able to
        # configure it using logging.conf, this handler has to extend
        # MemoryHandler because only the MemoryHandlers' logging.conf
        # parsing is implemented such that it accepts a target handler.
        handlers.MemoryHandler.__init__(self, capacity=0, target=target)
        self.locale = locale

    def setFormatter(self, fmt):
        self.target.setFormatter(fmt)

    def emit(self, record):
        # We save the message from the original record to restore it
        # after translation, so other handlers are not affected by this
        original_msg = record.msg
        original_args = record.args

        try:
            self._translate_and_log_record(record)
        finally:
            record.msg = original_msg
            record.args = original_args

    def _translate_and_log_record(self, record):
        record.msg = translate(record.msg, self.locale)

        # In addition to translating the message, we also need to translate
        # arguments that were passed to the log method that were not part
        # of the main message e.g., log.info(_('Some message %s'), this_one))
        record.args = _translate_args(record.args, self.locale)

        self.target.emit(record)

########NEW FILE########
__FILENAME__ = importutils
# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Import related utilities and helper functions.
"""

import sys
import traceback


def import_class(import_str):
    """Returns a class from a string including module and class."""
    mod_str, _sep, class_str = import_str.rpartition('.')
    try:
        __import__(mod_str)
        return getattr(sys.modules[mod_str], class_str)
    except (ValueError, AttributeError):
        raise ImportError('Class %s cannot be found (%s)' %
                          (class_str,
                           traceback.format_exception(*sys.exc_info())))


def import_object(import_str, *args, **kwargs):
    """Import a class and return an instance of it."""
    return import_class(import_str)(*args, **kwargs)


def import_object_ns(name_space, import_str, *args, **kwargs):
    """Tries to import object from default namespace.

    Imports a class and return an instance of it, first by trying
    to find the class in a default namespace, then failing back to
    a full path if not found in the default namespace.
    """
    import_value = "%s.%s" % (name_space, import_str)
    try:
        return import_class(import_value)(*args, **kwargs)
    except ImportError:
        return import_class(import_str)(*args, **kwargs)


def import_module(import_str):
    """Import a module."""
    __import__(import_str)
    return sys.modules[import_str]


def import_versioned_module(version, submodule=None):
    module = 'ceilometer.v%s' % version
    if submodule:
        module = '.'.join((module, submodule))
    return import_module(module)


def try_import(import_str, default=None):
    """Try to import a module and if it fails return default."""
    try:
        return import_module(import_str)
    except ImportError:
        return default

########NEW FILE########
__FILENAME__ = jsonutils
# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# Copyright 2011 Justin Santa Barbara
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

'''
JSON related utilities.

This module provides a few things:

    1) A handy function for getting an object down to something that can be
    JSON serialized.  See to_primitive().

    2) Wrappers around loads() and dumps().  The dumps() wrapper will
    automatically use to_primitive() for you if needed.

    3) This sets up anyjson to use the loads() and dumps() wrappers if anyjson
    is available.
'''


import datetime
import functools
import inspect
import itertools
import sys

if sys.version_info < (2, 7):
    # On Python <= 2.6, json module is not C boosted, so try to use
    # simplejson module if available
    try:
        import simplejson as json
    except ImportError:
        import json
else:
    import json

import six
import six.moves.xmlrpc_client as xmlrpclib

from ceilometer.openstack.common import gettextutils
from ceilometer.openstack.common import importutils
from ceilometer.openstack.common import timeutils

netaddr = importutils.try_import("netaddr")

_nasty_type_tests = [inspect.ismodule, inspect.isclass, inspect.ismethod,
                     inspect.isfunction, inspect.isgeneratorfunction,
                     inspect.isgenerator, inspect.istraceback, inspect.isframe,
                     inspect.iscode, inspect.isbuiltin, inspect.isroutine,
                     inspect.isabstract]

_simple_types = (six.string_types + six.integer_types
                 + (type(None), bool, float))


def to_primitive(value, convert_instances=False, convert_datetime=True,
                 level=0, max_depth=3):
    """Convert a complex object into primitives.

    Handy for JSON serialization. We can optionally handle instances,
    but since this is a recursive function, we could have cyclical
    data structures.

    To handle cyclical data structures we could track the actual objects
    visited in a set, but not all objects are hashable. Instead we just
    track the depth of the object inspections and don't go too deep.

    Therefore, convert_instances=True is lossy ... be aware.

    """
    # handle obvious types first - order of basic types determined by running
    # full tests on nova project, resulting in the following counts:
    # 572754 <type 'NoneType'>
    # 460353 <type 'int'>
    # 379632 <type 'unicode'>
    # 274610 <type 'str'>
    # 199918 <type 'dict'>
    # 114200 <type 'datetime.datetime'>
    #  51817 <type 'bool'>
    #  26164 <type 'list'>
    #   6491 <type 'float'>
    #    283 <type 'tuple'>
    #     19 <type 'long'>
    if isinstance(value, _simple_types):
        return value

    if isinstance(value, datetime.datetime):
        if convert_datetime:
            return timeutils.strtime(value)
        else:
            return value

    # value of itertools.count doesn't get caught by nasty_type_tests
    # and results in infinite loop when list(value) is called.
    if type(value) == itertools.count:
        return six.text_type(value)

    # FIXME(vish): Workaround for LP bug 852095. Without this workaround,
    #              tests that raise an exception in a mocked method that
    #              has a @wrap_exception with a notifier will fail. If
    #              we up the dependency to 0.5.4 (when it is released) we
    #              can remove this workaround.
    if getattr(value, '__module__', None) == 'mox':
        return 'mock'

    if level > max_depth:
        return '?'

    # The try block may not be necessary after the class check above,
    # but just in case ...
    try:
        recursive = functools.partial(to_primitive,
                                      convert_instances=convert_instances,
                                      convert_datetime=convert_datetime,
                                      level=level,
                                      max_depth=max_depth)
        if isinstance(value, dict):
            return dict((k, recursive(v)) for k, v in six.iteritems(value))
        elif isinstance(value, (list, tuple)):
            return [recursive(lv) for lv in value]

        # It's not clear why xmlrpclib created their own DateTime type, but
        # for our purposes, make it a datetime type which is explicitly
        # handled
        if isinstance(value, xmlrpclib.DateTime):
            value = datetime.datetime(*tuple(value.timetuple())[:6])

        if convert_datetime and isinstance(value, datetime.datetime):
            return timeutils.strtime(value)
        elif isinstance(value, gettextutils.Message):
            return value.data
        elif hasattr(value, 'iteritems'):
            return recursive(dict(value.iteritems()), level=level + 1)
        elif hasattr(value, '__iter__'):
            return recursive(list(value))
        elif convert_instances and hasattr(value, '__dict__'):
            # Likely an instance of something. Watch for cycles.
            # Ignore class member vars.
            return recursive(value.__dict__, level=level + 1)
        elif netaddr and isinstance(value, netaddr.IPAddress):
            return six.text_type(value)
        else:
            if any(test(value) for test in _nasty_type_tests):
                return six.text_type(value)
            return value
    except TypeError:
        # Class objects are tricky since they may define something like
        # __iter__ defined but it isn't callable as list().
        return six.text_type(value)


def dumps(value, default=to_primitive, **kwargs):
    return json.dumps(value, default=default, **kwargs)


def loads(s):
    return json.loads(s)


def load(s):
    return json.load(s)


try:
    import anyjson
except ImportError:
    pass
else:
    anyjson._modules.append((__name__, 'dumps', TypeError,
                                       'loads', ValueError, 'load'))
    anyjson.force_implementation(__name__)

########NEW FILE########
__FILENAME__ = local
# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Local storage of variables using weak references"""

import threading
import weakref


class WeakLocal(threading.local):
    def __getattribute__(self, attr):
        rval = super(WeakLocal, self).__getattribute__(attr)
        if rval:
            # NOTE(mikal): this bit is confusing. What is stored is a weak
            # reference, not the value itself. We therefore need to lookup
            # the weak reference and return the inner value here.
            rval = rval()
        return rval

    def __setattr__(self, attr, value):
        value = weakref.ref(value)
        return super(WeakLocal, self).__setattr__(attr, value)


# NOTE(mikal): the name "store" should be deprecated in the future
store = WeakLocal()

# A "weak" store uses weak references and allows an object to fall out of scope
# when it falls out of scope in the code that uses the thread local storage. A
# "strong" store will hold a reference to the object so that it never falls out
# of scope.
weak_store = WeakLocal()
strong_store = threading.local()

########NEW FILE########
__FILENAME__ = lockutils
# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import contextlib
import errno
import fcntl
import functools
import os
import shutil
import subprocess
import sys
import tempfile
import threading
import time
import weakref

from oslo.config import cfg

from ceilometer.openstack.common import fileutils
from ceilometer.openstack.common.gettextutils import _, _LE, _LI
from ceilometer.openstack.common import log as logging


LOG = logging.getLogger(__name__)


util_opts = [
    cfg.BoolOpt('disable_process_locking', default=False,
                help='Whether to disable inter-process locks'),
    cfg.StrOpt('lock_path',
               default=os.environ.get("CEILOMETER_LOCK_PATH"),
               help=('Directory to use for lock files.'))
]


CONF = cfg.CONF
CONF.register_opts(util_opts)


def set_defaults(lock_path):
    cfg.set_defaults(util_opts, lock_path=lock_path)


class _FileLock(object):
    """Lock implementation which allows multiple locks, working around
    issues like bugs.debian.org/cgi-bin/bugreport.cgi?bug=632857 and does
    not require any cleanup. Since the lock is always held on a file
    descriptor rather than outside of the process, the lock gets dropped
    automatically if the process crashes, even if __exit__ is not executed.

    There are no guarantees regarding usage by multiple green threads in a
    single process here. This lock works only between processes. Exclusive
    access between local threads should be achieved using the semaphores
    in the @synchronized decorator.

    Note these locks are released when the descriptor is closed, so it's not
    safe to close the file descriptor while another green thread holds the
    lock. Just opening and closing the lock file can break synchronisation,
    so lock files must be accessed only using this abstraction.
    """

    def __init__(self, name):
        self.lockfile = None
        self.fname = name

    def acquire(self):
        basedir = os.path.dirname(self.fname)

        if not os.path.exists(basedir):
            fileutils.ensure_tree(basedir)
            LOG.info(_LI('Created lock path: %s'), basedir)

        self.lockfile = open(self.fname, 'w')

        while True:
            try:
                # Using non-blocking locks since green threads are not
                # patched to deal with blocking locking calls.
                # Also upon reading the MSDN docs for locking(), it seems
                # to have a laughable 10 attempts "blocking" mechanism.
                self.trylock()
                LOG.debug('Got file lock "%s"', self.fname)
                return True
            except IOError as e:
                if e.errno in (errno.EACCES, errno.EAGAIN):
                    # external locks synchronise things like iptables
                    # updates - give it some time to prevent busy spinning
                    time.sleep(0.01)
                else:
                    raise threading.ThreadError(_("Unable to acquire lock on"
                                                  " `%(filename)s` due to"
                                                  " %(exception)s") %
                                                {
                                                    'filename': self.fname,
                                                    'exception': e,
                                                })

    def __enter__(self):
        self.acquire()
        return self

    def release(self):
        try:
            self.unlock()
            self.lockfile.close()
            LOG.debug('Released file lock "%s"', self.fname)
        except IOError:
            LOG.exception(_LE("Could not release the acquired lock `%s`"),
                          self.fname)

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.release()

    def exists(self):
        return os.path.exists(self.fname)

    def trylock(self):
        raise NotImplementedError()

    def unlock(self):
        raise NotImplementedError()


class _WindowsLock(_FileLock):
    def trylock(self):
        msvcrt.locking(self.lockfile.fileno(), msvcrt.LK_NBLCK, 1)

    def unlock(self):
        msvcrt.locking(self.lockfile.fileno(), msvcrt.LK_UNLCK, 1)


class _FcntlLock(_FileLock):
    def trylock(self):
        fcntl.lockf(self.lockfile, fcntl.LOCK_EX | fcntl.LOCK_NB)

    def unlock(self):
        fcntl.lockf(self.lockfile, fcntl.LOCK_UN)


class _PosixLock(object):
    def __init__(self, name):
        # Hash the name because it's not valid to have POSIX semaphore
        # names with things like / in them. Then use base64 to encode
        # the digest() instead taking the hexdigest() because the
        # result is shorter and most systems can't have shm sempahore
        # names longer than 31 characters.
        h = hashlib.sha1()
        h.update(name.encode('ascii'))
        self.name = str((b'/' + base64.urlsafe_b64encode(
            h.digest())).decode('ascii'))

    def acquire(self, timeout=None):
        self.semaphore = posix_ipc.Semaphore(self.name,
                                             flags=posix_ipc.O_CREAT,
                                             initial_value=1)
        self.semaphore.acquire(timeout)
        return self

    def __enter__(self):
        self.acquire()
        return self

    def release(self):
        self.semaphore.release()
        self.semaphore.close()

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.release()

    def exists(self):
        try:
            semaphore = posix_ipc.Semaphore(self.name)
        except posix_ipc.ExistentialError:
            return False
        else:
            semaphore.close()
        return True


if os.name == 'nt':
    import msvcrt
    InterProcessLock = _WindowsLock
    FileLock = _WindowsLock
else:
    import base64
    import hashlib
    import posix_ipc
    InterProcessLock = _PosixLock
    FileLock = _FcntlLock

_semaphores = weakref.WeakValueDictionary()
_semaphores_lock = threading.Lock()


def _get_lock_path(name, lock_file_prefix, lock_path=None):
    # NOTE(mikal): the lock name cannot contain directory
    # separators
    name = name.replace(os.sep, '_')
    if lock_file_prefix:
        sep = '' if lock_file_prefix.endswith('-') else '-'
        name = '%s%s%s' % (lock_file_prefix, sep, name)

    local_lock_path = lock_path or CONF.lock_path

    if not local_lock_path:
        # NOTE(bnemec): Create a fake lock path for posix locks so we don't
        # unnecessarily raise the RequiredOptError below.
        if InterProcessLock is not _PosixLock:
            raise cfg.RequiredOptError('lock_path')
        local_lock_path = 'posixlock:/'

    return os.path.join(local_lock_path, name)


def external_lock(name, lock_file_prefix=None, lock_path=None):
    LOG.debug('Attempting to grab external lock "%(lock)s"',
              {'lock': name})

    lock_file_path = _get_lock_path(name, lock_file_prefix, lock_path)

    # NOTE(bnemec): If an explicit lock_path was passed to us then it
    # means the caller is relying on file-based locking behavior, so
    # we can't use posix locks for those calls.
    if lock_path:
        return FileLock(lock_file_path)
    return InterProcessLock(lock_file_path)


def remove_external_lock_file(name, lock_file_prefix=None):
    """Remove a external lock file when it's not used anymore
    This will be helpful when we have a lot of lock files
    """
    with internal_lock(name):
        lock_file_path = _get_lock_path(name, lock_file_prefix)
        try:
            os.remove(lock_file_path)
        except OSError:
            LOG.info(_LI('Failed to remove file %(file)s'),
                     {'file': lock_file_path})


def internal_lock(name):
    with _semaphores_lock:
        try:
            sem = _semaphores[name]
        except KeyError:
            sem = threading.Semaphore()
            _semaphores[name] = sem

    LOG.debug('Got semaphore "%(lock)s"', {'lock': name})
    return sem


@contextlib.contextmanager
def lock(name, lock_file_prefix=None, external=False, lock_path=None):
    """Context based lock

    This function yields a `threading.Semaphore` instance (if we don't use
    eventlet.monkey_patch(), else `semaphore.Semaphore`) unless external is
    True, in which case, it'll yield an InterProcessLock instance.

    :param lock_file_prefix: The lock_file_prefix argument is used to provide
      lock files on disk with a meaningful prefix.

    :param external: The external keyword argument denotes whether this lock
      should work across multiple processes. This means that if two different
      workers both run a a method decorated with @synchronized('mylock',
      external=True), only one of them will execute at a time.
    """
    int_lock = internal_lock(name)
    with int_lock:
        if external and not CONF.disable_process_locking:
            ext_lock = external_lock(name, lock_file_prefix, lock_path)
            with ext_lock:
                yield ext_lock
        else:
            yield int_lock


def synchronized(name, lock_file_prefix=None, external=False, lock_path=None):
    """Synchronization decorator.

    Decorating a method like so::

        @synchronized('mylock')
        def foo(self, *args):
           ...

    ensures that only one thread will execute the foo method at a time.

    Different methods can share the same lock::

        @synchronized('mylock')
        def foo(self, *args):
           ...

        @synchronized('mylock')
        def bar(self, *args):
           ...

    This way only one of either foo or bar can be executing at a time.
    """

    def wrap(f):
        @functools.wraps(f)
        def inner(*args, **kwargs):
            try:
                with lock(name, lock_file_prefix, external, lock_path):
                    LOG.debug('Got semaphore / lock "%(function)s"',
                              {'function': f.__name__})
                    return f(*args, **kwargs)
            finally:
                LOG.debug('Semaphore / lock released "%(function)s"',
                          {'function': f.__name__})
        return inner
    return wrap


def synchronized_with_prefix(lock_file_prefix):
    """Partial object generator for the synchronization decorator.

    Redefine @synchronized in each project like so::

        (in nova/utils.py)
        from nova.openstack.common import lockutils

        synchronized = lockutils.synchronized_with_prefix('nova-')


        (in nova/foo.py)
        from nova import utils

        @utils.synchronized('mylock')
        def bar(self, *args):
           ...

    The lock_file_prefix argument is used to provide lock files on disk with a
    meaningful prefix.
    """

    return functools.partial(synchronized, lock_file_prefix=lock_file_prefix)


def main(argv):
    """Create a dir for locks and pass it to command from arguments

    If you run this:
    python -m openstack.common.lockutils python setup.py testr <etc>

    a temporary directory will be created for all your locks and passed to all
    your tests in an environment variable. The temporary dir will be deleted
    afterwards and the return value will be preserved.
    """

    lock_dir = tempfile.mkdtemp()
    os.environ["CEILOMETER_LOCK_PATH"] = lock_dir
    try:
        ret_val = subprocess.call(argv[1:])
    finally:
        shutil.rmtree(lock_dir, ignore_errors=True)
    return ret_val


if __name__ == '__main__':
    sys.exit(main(sys.argv))

########NEW FILE########
__FILENAME__ = log
# Copyright 2011 OpenStack Foundation.
# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""OpenStack logging handler.

This module adds to logging functionality by adding the option to specify
a context object when calling the various log methods.  If the context object
is not specified, default formatting is used. Additionally, an instance uuid
may be passed as part of the log message, which is intended to make it easier
for admins to find messages related to a specific instance.

It also allows setting of formatting information through conf.

"""

import inspect
import itertools
import logging
import logging.config
import logging.handlers
import os
import re
import sys
import traceback

from oslo.config import cfg
import six
from six import moves

from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import importutils
from ceilometer.openstack.common import jsonutils
from ceilometer.openstack.common import local


_DEFAULT_LOG_DATE_FORMAT = "%Y-%m-%d %H:%M:%S"

_SANITIZE_KEYS = ['adminPass', 'admin_pass', 'password', 'admin_password']

# NOTE(ldbragst): Let's build a list of regex objects using the list of
# _SANITIZE_KEYS we already have. This way, we only have to add the new key
# to the list of _SANITIZE_KEYS and we can generate regular expressions
# for XML and JSON automatically.
_SANITIZE_PATTERNS = []
_FORMAT_PATTERNS = [r'(%(key)s\s*[=]\s*[\"\']).*?([\"\'])',
                    r'(<%(key)s>).*?(</%(key)s>)',
                    r'([\"\']%(key)s[\"\']\s*:\s*[\"\']).*?([\"\'])',
                    r'([\'"].*?%(key)s[\'"]\s*:\s*u?[\'"]).*?([\'"])']

for key in _SANITIZE_KEYS:
    for pattern in _FORMAT_PATTERNS:
        reg_ex = re.compile(pattern % {'key': key}, re.DOTALL)
        _SANITIZE_PATTERNS.append(reg_ex)


common_cli_opts = [
    cfg.BoolOpt('debug',
                short='d',
                default=False,
                help='Print debugging output (set logging level to '
                     'DEBUG instead of default WARNING level).'),
    cfg.BoolOpt('verbose',
                short='v',
                default=False,
                help='Print more verbose output (set logging level to '
                     'INFO instead of default WARNING level).'),
]

logging_cli_opts = [
    cfg.StrOpt('log-config-append',
               metavar='PATH',
               deprecated_name='log-config',
               help='The name of logging configuration file. It does not '
                    'disable existing loggers, but just appends specified '
                    'logging configuration to any other existing logging '
                    'options. Please see the Python logging module '
                    'documentation for details on logging configuration '
                    'files.'),
    cfg.StrOpt('log-format',
               default=None,
               metavar='FORMAT',
               help='DEPRECATED. '
                    'A logging.Formatter log message format string which may '
                    'use any of the available logging.LogRecord attributes. '
                    'This option is deprecated.  Please use '
                    'logging_context_format_string and '
                    'logging_default_format_string instead.'),
    cfg.StrOpt('log-date-format',
               default=_DEFAULT_LOG_DATE_FORMAT,
               metavar='DATE_FORMAT',
               help='Format string for %%(asctime)s in log records. '
                    'Default: %(default)s'),
    cfg.StrOpt('log-file',
               metavar='PATH',
               deprecated_name='logfile',
               help='(Optional) Name of log file to output to. '
                    'If no default is set, logging will go to stdout.'),
    cfg.StrOpt('log-dir',
               deprecated_name='logdir',
               help='(Optional) The base directory used for relative '
                    '--log-file paths'),
    cfg.BoolOpt('use-syslog',
                default=False,
                help='Use syslog for logging. '
                     'Existing syslog format is DEPRECATED during I, '
                     'and then will be changed in J to honor RFC5424'),
    cfg.BoolOpt('use-syslog-rfc-format',
                # TODO(bogdando) remove or use True after existing
                #    syslog format deprecation in J
                default=False,
                help='(Optional) Use syslog rfc5424 format for logging. '
                     'If enabled, will add APP-NAME (RFC5424) before the '
                     'MSG part of the syslog message.  The old format '
                     'without APP-NAME is deprecated in I, '
                     'and will be removed in J.'),
    cfg.StrOpt('syslog-log-facility',
               default='LOG_USER',
               help='Syslog facility to receive log lines')
]

generic_log_opts = [
    cfg.BoolOpt('use_stderr',
                default=True,
                help='Log output to standard error')
]

log_opts = [
    cfg.StrOpt('logging_context_format_string',
               default='%(asctime)s.%(msecs)03d %(process)d %(levelname)s '
                       '%(name)s [%(request_id)s %(user_identity)s] '
                       '%(instance)s%(message)s',
               help='Format string to use for log messages with context'),
    cfg.StrOpt('logging_default_format_string',
               default='%(asctime)s.%(msecs)03d %(process)d %(levelname)s '
                       '%(name)s [-] %(instance)s%(message)s',
               help='Format string to use for log messages without context'),
    cfg.StrOpt('logging_debug_format_suffix',
               default='%(funcName)s %(pathname)s:%(lineno)d',
               help='Data to append to log format when level is DEBUG'),
    cfg.StrOpt('logging_exception_prefix',
               default='%(asctime)s.%(msecs)03d %(process)d TRACE %(name)s '
               '%(instance)s',
               help='Prefix each line of exception output with this format'),
    cfg.ListOpt('default_log_levels',
                default=[
                    'amqp=WARN',
                    'amqplib=WARN',
                    'boto=WARN',
                    'qpid=WARN',
                    'sqlalchemy=WARN',
                    'suds=INFO',
                    'oslo.messaging=INFO',
                    'iso8601=WARN',
                    'requests.packages.urllib3.connectionpool=WARN'
                ],
                help='List of logger=LEVEL pairs'),
    cfg.BoolOpt('publish_errors',
                default=False,
                help='Publish error events'),
    cfg.BoolOpt('fatal_deprecations',
                default=False,
                help='Make deprecations fatal'),

    # NOTE(mikal): there are two options here because sometimes we are handed
    # a full instance (and could include more information), and other times we
    # are just handed a UUID for the instance.
    cfg.StrOpt('instance_format',
               default='[instance: %(uuid)s] ',
               help='If an instance is passed with the log message, format '
                    'it like this'),
    cfg.StrOpt('instance_uuid_format',
               default='[instance: %(uuid)s] ',
               help='If an instance UUID is passed with the log message, '
                    'format it like this'),
]

CONF = cfg.CONF
CONF.register_cli_opts(common_cli_opts)
CONF.register_cli_opts(logging_cli_opts)
CONF.register_opts(generic_log_opts)
CONF.register_opts(log_opts)

# our new audit level
# NOTE(jkoelker) Since we synthesized an audit level, make the logging
#                module aware of it so it acts like other levels.
logging.AUDIT = logging.INFO + 1
logging.addLevelName(logging.AUDIT, 'AUDIT')


try:
    NullHandler = logging.NullHandler
except AttributeError:  # NOTE(jkoelker) NullHandler added in Python 2.7
    class NullHandler(logging.Handler):
        def handle(self, record):
            pass

        def emit(self, record):
            pass

        def createLock(self):
            self.lock = None


def _dictify_context(context):
    if context is None:
        return None
    if not isinstance(context, dict) and getattr(context, 'to_dict', None):
        context = context.to_dict()
    return context


def _get_binary_name():
    return os.path.basename(inspect.stack()[-1][1])


def _get_log_file_path(binary=None):
    logfile = CONF.log_file
    logdir = CONF.log_dir

    if logfile and not logdir:
        return logfile

    if logfile and logdir:
        return os.path.join(logdir, logfile)

    if logdir:
        binary = binary or _get_binary_name()
        return '%s.log' % (os.path.join(logdir, binary),)

    return None


def mask_password(message, secret="***"):
    """Replace password with 'secret' in message.

    :param message: The string which includes security information.
    :param secret: value with which to replace passwords.
    :returns: The unicode value of message with the password fields masked.

    For example:

    >>> mask_password("'adminPass' : 'aaaaa'")
    "'adminPass' : '***'"
    >>> mask_password("'admin_pass' : 'aaaaa'")
    "'admin_pass' : '***'"
    >>> mask_password('"password" : "aaaaa"')
    '"password" : "***"'
    >>> mask_password("'original_password' : 'aaaaa'")
    "'original_password' : '***'"
    >>> mask_password("u'original_password' :   u'aaaaa'")
    "u'original_password' :   u'***'"
    """
    message = six.text_type(message)

    # NOTE(ldbragst): Check to see if anything in message contains any key
    # specified in _SANITIZE_KEYS, if not then just return the message since
    # we don't have to mask any passwords.
    if not any(key in message for key in _SANITIZE_KEYS):
        return message

    secret = r'\g<1>' + secret + r'\g<2>'
    for pattern in _SANITIZE_PATTERNS:
        message = re.sub(pattern, secret, message)
    return message


class BaseLoggerAdapter(logging.LoggerAdapter):

    def audit(self, msg, *args, **kwargs):
        self.log(logging.AUDIT, msg, *args, **kwargs)


class LazyAdapter(BaseLoggerAdapter):
    def __init__(self, name='unknown', version='unknown'):
        self._logger = None
        self.extra = {}
        self.name = name
        self.version = version

    @property
    def logger(self):
        if not self._logger:
            self._logger = getLogger(self.name, self.version)
        return self._logger


class ContextAdapter(BaseLoggerAdapter):
    warn = logging.LoggerAdapter.warning

    def __init__(self, logger, project_name, version_string):
        self.logger = logger
        self.project = project_name
        self.version = version_string
        self._deprecated_messages_sent = dict()

    @property
    def handlers(self):
        return self.logger.handlers

    def deprecated(self, msg, *args, **kwargs):
        """Call this method when a deprecated feature is used.

        If the system is configured for fatal deprecations then the message
        is logged at the 'critical' level and :class:`DeprecatedConfig` will
        be raised.

        Otherwise, the message will be logged (once) at the 'warn' level.

        :raises: :class:`DeprecatedConfig` if the system is configured for
                 fatal deprecations.

        """
        stdmsg = _("Deprecated: %s") % msg
        if CONF.fatal_deprecations:
            self.critical(stdmsg, *args, **kwargs)
            raise DeprecatedConfig(msg=stdmsg)

        # Using a list because a tuple with dict can't be stored in a set.
        sent_args = self._deprecated_messages_sent.setdefault(msg, list())

        if args in sent_args:
            # Already logged this message, so don't log it again.
            return

        sent_args.append(args)
        self.warn(stdmsg, *args, **kwargs)

    def process(self, msg, kwargs):
        # NOTE(mrodden): catch any Message/other object and
        #                coerce to unicode before they can get
        #                to the python logging and possibly
        #                cause string encoding trouble
        if not isinstance(msg, six.string_types):
            msg = six.text_type(msg)

        if 'extra' not in kwargs:
            kwargs['extra'] = {}
        extra = kwargs['extra']

        context = kwargs.pop('context', None)
        if not context:
            context = getattr(local.store, 'context', None)
        if context:
            extra.update(_dictify_context(context))

        instance = kwargs.pop('instance', None)
        instance_uuid = (extra.get('instance_uuid') or
                         kwargs.pop('instance_uuid', None))
        instance_extra = ''
        if instance:
            instance_extra = CONF.instance_format % instance
        elif instance_uuid:
            instance_extra = (CONF.instance_uuid_format
                              % {'uuid': instance_uuid})
        extra['instance'] = instance_extra

        extra.setdefault('user_identity', kwargs.pop('user_identity', None))

        extra['project'] = self.project
        extra['version'] = self.version
        extra['extra'] = extra.copy()
        return msg, kwargs


class JSONFormatter(logging.Formatter):
    def __init__(self, fmt=None, datefmt=None):
        # NOTE(jkoelker) we ignore the fmt argument, but its still there
        #                since logging.config.fileConfig passes it.
        self.datefmt = datefmt

    def formatException(self, ei, strip_newlines=True):
        lines = traceback.format_exception(*ei)
        if strip_newlines:
            lines = [moves.filter(
                lambda x: x,
                line.rstrip().splitlines()) for line in lines]
            lines = list(itertools.chain(*lines))
        return lines

    def format(self, record):
        message = {'message': record.getMessage(),
                   'asctime': self.formatTime(record, self.datefmt),
                   'name': record.name,
                   'msg': record.msg,
                   'args': record.args,
                   'levelname': record.levelname,
                   'levelno': record.levelno,
                   'pathname': record.pathname,
                   'filename': record.filename,
                   'module': record.module,
                   'lineno': record.lineno,
                   'funcname': record.funcName,
                   'created': record.created,
                   'msecs': record.msecs,
                   'relative_created': record.relativeCreated,
                   'thread': record.thread,
                   'thread_name': record.threadName,
                   'process_name': record.processName,
                   'process': record.process,
                   'traceback': None}

        if hasattr(record, 'extra'):
            message['extra'] = record.extra

        if record.exc_info:
            message['traceback'] = self.formatException(record.exc_info)

        return jsonutils.dumps(message)


def _create_logging_excepthook(product_name):
    def logging_excepthook(exc_type, value, tb):
        extra = {}
        if CONF.verbose or CONF.debug:
            extra['exc_info'] = (exc_type, value, tb)
        getLogger(product_name).critical(
            "".join(traceback.format_exception_only(exc_type, value)),
            **extra)
    return logging_excepthook


class LogConfigError(Exception):

    message = _('Error loading logging config %(log_config)s: %(err_msg)s')

    def __init__(self, log_config, err_msg):
        self.log_config = log_config
        self.err_msg = err_msg

    def __str__(self):
        return self.message % dict(log_config=self.log_config,
                                   err_msg=self.err_msg)


def _load_log_config(log_config_append):
    try:
        logging.config.fileConfig(log_config_append,
                                  disable_existing_loggers=False)
    except moves.configparser.Error as exc:
        raise LogConfigError(log_config_append, str(exc))


def setup(product_name, version='unknown'):
    """Setup logging."""
    if CONF.log_config_append:
        _load_log_config(CONF.log_config_append)
    else:
        _setup_logging_from_conf(product_name, version)
    sys.excepthook = _create_logging_excepthook(product_name)


def set_defaults(logging_context_format_string):
    cfg.set_defaults(log_opts,
                     logging_context_format_string=
                     logging_context_format_string)


def _find_facility_from_conf():
    facility_names = logging.handlers.SysLogHandler.facility_names
    facility = getattr(logging.handlers.SysLogHandler,
                       CONF.syslog_log_facility,
                       None)

    if facility is None and CONF.syslog_log_facility in facility_names:
        facility = facility_names.get(CONF.syslog_log_facility)

    if facility is None:
        valid_facilities = facility_names.keys()
        consts = ['LOG_AUTH', 'LOG_AUTHPRIV', 'LOG_CRON', 'LOG_DAEMON',
                  'LOG_FTP', 'LOG_KERN', 'LOG_LPR', 'LOG_MAIL', 'LOG_NEWS',
                  'LOG_AUTH', 'LOG_SYSLOG', 'LOG_USER', 'LOG_UUCP',
                  'LOG_LOCAL0', 'LOG_LOCAL1', 'LOG_LOCAL2', 'LOG_LOCAL3',
                  'LOG_LOCAL4', 'LOG_LOCAL5', 'LOG_LOCAL6', 'LOG_LOCAL7']
        valid_facilities.extend(consts)
        raise TypeError(_('syslog facility must be one of: %s') %
                        ', '.join("'%s'" % fac
                                  for fac in valid_facilities))

    return facility


class RFCSysLogHandler(logging.handlers.SysLogHandler):
    def __init__(self, *args, **kwargs):
        self.binary_name = _get_binary_name()
        super(RFCSysLogHandler, self).__init__(*args, **kwargs)

    def format(self, record):
        msg = super(RFCSysLogHandler, self).format(record)
        msg = self.binary_name + ' ' + msg
        return msg


def _setup_logging_from_conf(project, version):
    log_root = getLogger(None).logger
    for handler in log_root.handlers:
        log_root.removeHandler(handler)

    if CONF.use_syslog:
        facility = _find_facility_from_conf()
        # TODO(bogdando) use the format provided by RFCSysLogHandler
        #   after existing syslog format deprecation in J
        if CONF.use_syslog_rfc_format:
            syslog = RFCSysLogHandler(address='/dev/log',
                                      facility=facility)
        else:
            syslog = logging.handlers.SysLogHandler(address='/dev/log',
                                                    facility=facility)
        log_root.addHandler(syslog)

    logpath = _get_log_file_path()
    if logpath:
        filelog = logging.handlers.WatchedFileHandler(logpath)
        log_root.addHandler(filelog)

    if CONF.use_stderr:
        streamlog = ColorHandler()
        log_root.addHandler(streamlog)

    elif not logpath:
        # pass sys.stdout as a positional argument
        # python2.6 calls the argument strm, in 2.7 it's stream
        streamlog = logging.StreamHandler(sys.stdout)
        log_root.addHandler(streamlog)

    if CONF.publish_errors:
        handler = importutils.import_object(
            "ceilometer.openstack.common.log_handler.PublishErrorsHandler",
            logging.ERROR)
        log_root.addHandler(handler)

    datefmt = CONF.log_date_format
    for handler in log_root.handlers:
        # NOTE(alaski): CONF.log_format overrides everything currently.  This
        # should be deprecated in favor of context aware formatting.
        if CONF.log_format:
            handler.setFormatter(logging.Formatter(fmt=CONF.log_format,
                                                   datefmt=datefmt))
            log_root.info('Deprecated: log_format is now deprecated and will '
                          'be removed in the next release')
        else:
            handler.setFormatter(ContextFormatter(project=project,
                                                  version=version,
                                                  datefmt=datefmt))

    if CONF.debug:
        log_root.setLevel(logging.DEBUG)
    elif CONF.verbose:
        log_root.setLevel(logging.INFO)
    else:
        log_root.setLevel(logging.WARNING)

    for pair in CONF.default_log_levels:
        mod, _sep, level_name = pair.partition('=')
        level = logging.getLevelName(level_name)
        logger = logging.getLogger(mod)
        logger.setLevel(level)

_loggers = {}


def getLogger(name='unknown', version='unknown'):
    if name not in _loggers:
        _loggers[name] = ContextAdapter(logging.getLogger(name),
                                        name,
                                        version)
    return _loggers[name]


def getLazyLogger(name='unknown', version='unknown'):
    """Returns lazy logger.

    Creates a pass-through logger that does not create the real logger
    until it is really needed and delegates all calls to the real logger
    once it is created.
    """
    return LazyAdapter(name, version)


class WritableLogger(object):
    """A thin wrapper that responds to `write` and logs."""

    def __init__(self, logger, level=logging.INFO):
        self.logger = logger
        self.level = level

    def write(self, msg):
        self.logger.log(self.level, msg.rstrip())


class ContextFormatter(logging.Formatter):
    """A context.RequestContext aware formatter configured through flags.

    The flags used to set format strings are: logging_context_format_string
    and logging_default_format_string.  You can also specify
    logging_debug_format_suffix to append extra formatting if the log level is
    debug.

    For information about what variables are available for the formatter see:
    http://docs.python.org/library/logging.html#formatter

    If available, uses the context value stored in TLS - local.store.context

    """

    def __init__(self, *args, **kwargs):
        """Initialize ContextFormatter instance

        Takes additional keyword arguments which can be used in the message
        format string.

        :keyword project: project name
        :type project: string
        :keyword version: project version
        :type version: string

        """

        self.project = kwargs.pop('project', 'unknown')
        self.version = kwargs.pop('version', 'unknown')

        logging.Formatter.__init__(self, *args, **kwargs)

    def format(self, record):
        """Uses contextstring if request_id is set, otherwise default."""

        # store project info
        record.project = self.project
        record.version = self.version

        # store request info
        context = getattr(local.store, 'context', None)
        if context:
            d = _dictify_context(context)
            for k, v in d.items():
                setattr(record, k, v)

        # NOTE(sdague): default the fancier formatting params
        # to an empty string so we don't throw an exception if
        # they get used
        for key in ('instance', 'color', 'user_identity'):
            if key not in record.__dict__:
                record.__dict__[key] = ''

        if record.__dict__.get('request_id'):
            self._fmt = CONF.logging_context_format_string
        else:
            self._fmt = CONF.logging_default_format_string

        if (record.levelno == logging.DEBUG and
                CONF.logging_debug_format_suffix):
            self._fmt += " " + CONF.logging_debug_format_suffix

        # Cache this on the record, Logger will respect our formatted copy
        if record.exc_info:
            record.exc_text = self.formatException(record.exc_info, record)
        return logging.Formatter.format(self, record)

    def formatException(self, exc_info, record=None):
        """Format exception output with CONF.logging_exception_prefix."""
        if not record:
            return logging.Formatter.formatException(self, exc_info)

        stringbuffer = moves.StringIO()
        traceback.print_exception(exc_info[0], exc_info[1], exc_info[2],
                                  None, stringbuffer)
        lines = stringbuffer.getvalue().split('\n')
        stringbuffer.close()

        if CONF.logging_exception_prefix.find('%(asctime)') != -1:
            record.asctime = self.formatTime(record, self.datefmt)

        formatted_lines = []
        for line in lines:
            pl = CONF.logging_exception_prefix % record.__dict__
            fl = '%s%s' % (pl, line)
            formatted_lines.append(fl)
        return '\n'.join(formatted_lines)


class ColorHandler(logging.StreamHandler):
    LEVEL_COLORS = {
        logging.DEBUG: '\033[00;32m',  # GREEN
        logging.INFO: '\033[00;36m',  # CYAN
        logging.AUDIT: '\033[01;36m',  # BOLD CYAN
        logging.WARN: '\033[01;33m',  # BOLD YELLOW
        logging.ERROR: '\033[01;31m',  # BOLD RED
        logging.CRITICAL: '\033[01;31m',  # BOLD RED
    }

    def format(self, record):
        record.color = self.LEVEL_COLORS[record.levelno]
        return logging.StreamHandler.format(self, record)


class DeprecatedConfig(Exception):
    message = _("Fatal call to deprecated config: %(msg)s")

    def __init__(self, msg):
        super(Exception, self).__init__(self.message % dict(msg=msg))

########NEW FILE########
__FILENAME__ = log_handler
# Copyright 2013 IBM Corp.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import logging

from oslo.config import cfg

from ceilometer.openstack.common import notifier


class PublishErrorsHandler(logging.Handler):
    def emit(self, record):
        if ('ceilometer.openstack.common.notifier.log_notifier' in
                cfg.CONF.notification_driver):
            return
        notifier.api.notify(None, 'error.publisher',
                            'error_notification',
                            notifier.api.ERROR,
                            dict(error=record.getMessage()))

########NEW FILE########
__FILENAME__ = loopingcall
# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# Copyright 2011 Justin Santa Barbara
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import sys

from eventlet import event
from eventlet import greenthread

from ceilometer.openstack.common.gettextutils import _LE, _LW
from ceilometer.openstack.common import log as logging
from ceilometer.openstack.common import timeutils

LOG = logging.getLogger(__name__)


class LoopingCallDone(Exception):
    """Exception to break out and stop a LoopingCall.

    The poll-function passed to LoopingCall can raise this exception to
    break out of the loop normally. This is somewhat analogous to
    StopIteration.

    An optional return-value can be included as the argument to the exception;
    this return-value will be returned by LoopingCall.wait()

    """

    def __init__(self, retvalue=True):
        """:param retvalue: Value that LoopingCall.wait() should return."""
        self.retvalue = retvalue


class LoopingCallBase(object):
    def __init__(self, f=None, *args, **kw):
        self.args = args
        self.kw = kw
        self.f = f
        self._running = False
        self.done = None

    def stop(self):
        self._running = False

    def wait(self):
        return self.done.wait()


class FixedIntervalLoopingCall(LoopingCallBase):
    """A fixed interval looping call."""

    def start(self, interval, initial_delay=None):
        self._running = True
        done = event.Event()

        def _inner():
            if initial_delay:
                greenthread.sleep(initial_delay)

            try:
                while self._running:
                    start = timeutils.utcnow()
                    self.f(*self.args, **self.kw)
                    end = timeutils.utcnow()
                    if not self._running:
                        break
                    delay = interval - timeutils.delta_seconds(start, end)
                    if delay <= 0:
                        LOG.warn(_LW('task run outlasted interval by %s sec') %
                                 -delay)
                    greenthread.sleep(delay if delay > 0 else 0)
            except LoopingCallDone as e:
                self.stop()
                done.send(e.retvalue)
            except Exception:
                LOG.exception(_LE('in fixed duration looping call'))
                done.send_exception(*sys.exc_info())
                return
            else:
                done.send(True)

        self.done = done

        greenthread.spawn_n(_inner)
        return self.done


# TODO(mikal): this class name is deprecated in Havana and should be removed
# in the I release
LoopingCall = FixedIntervalLoopingCall


class DynamicLoopingCall(LoopingCallBase):
    """A looping call which sleeps until the next known event.

    The function called should return how long to sleep for before being
    called again.
    """

    def start(self, initial_delay=None, periodic_interval_max=None):
        self._running = True
        done = event.Event()

        def _inner():
            if initial_delay:
                greenthread.sleep(initial_delay)

            try:
                while self._running:
                    idle = self.f(*self.args, **self.kw)
                    if not self._running:
                        break

                    if periodic_interval_max is not None:
                        idle = min(idle, periodic_interval_max)
                    LOG.debug('Dynamic looping call sleeping for %.02f '
                              'seconds', idle)
                    greenthread.sleep(idle)
            except LoopingCallDone as e:
                self.stop()
                done.send(e.retvalue)
            except Exception:
                LOG.exception(_LE('in dynamic looping call'))
                done.send_exception(*sys.exc_info())
                return
            else:
                done.send(True)

        self.done = done

        greenthread.spawn(_inner)
        return self.done

########NEW FILE########
__FILENAME__ = audit
# Copyright (c) 2013 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Attach open standard audit information to request.environ

AuditMiddleware filter should be place after Keystone's auth_token middleware
in the pipeline so that it can utilise the information Keystone provides.

"""
from pycadf.audit import api as cadf_api

from ceilometer.openstack.common.middleware import notifier


class AuditMiddleware(notifier.RequestNotifier):

    def __init__(self, app, **conf):
        super(AuditMiddleware, self).__init__(app, **conf)
        self.cadf_audit = cadf_api.OpenStackAuditApi()

    @notifier.log_and_ignore_error
    def process_request(self, request):
        self.cadf_audit.append_audit_event(request)
        super(AuditMiddleware, self).process_request(request)

    @notifier.log_and_ignore_error
    def process_response(self, request, response,
                         exception=None, traceback=None):
        self.cadf_audit.mod_audit_event(request, response)
        super(AuditMiddleware, self).process_response(request, response,
                                                      exception, traceback)

########NEW FILE########
__FILENAME__ = base
# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Base class(es) for WSGI Middleware."""

import webob.dec


class Middleware(object):
    """Base WSGI middleware wrapper.

    These classes require an application to be initialized that will be called
    next.  By default the middleware will simply call its wrapped app, or you
    can override __call__ to customize its behavior.
    """

    @classmethod
    def factory(cls, global_conf, **local_conf):
        """Factory method for paste.deploy."""
        return cls

    def __init__(self, application):
        self.application = application

    def process_request(self, req):
        """Called on each request.

        If this returns None, the next application down the stack will be
        executed. If it returns a response then that response will be returned
        and execution will stop here.
        """
        return None

    def process_response(self, response):
        """Do whatever you'd like to the response."""
        return response

    @webob.dec.wsgify
    def __call__(self, req):
        response = self.process_request(req)
        if response:
            return response
        response = req.get_response(self.application)
        return self.process_response(response)

########NEW FILE########
__FILENAME__ = catch_errors
# Copyright (c) 2013 NEC Corporation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Middleware that provides high-level error handling.

It catches all exceptions from subsequent applications in WSGI pipeline
to hide internal errors from API response.
"""

import webob.dec
import webob.exc

from ceilometer.openstack.common.gettextutils import _LE
from ceilometer.openstack.common import log as logging
from ceilometer.openstack.common.middleware import base


LOG = logging.getLogger(__name__)


class CatchErrorsMiddleware(base.Middleware):

    @webob.dec.wsgify
    def __call__(self, req):
        try:
            response = req.get_response(self.application)
        except Exception:
            LOG.exception(_LE('An error occurred during '
                              'processing the request: %s'))
            response = webob.exc.HTTPInternalServerError()
        return response

########NEW FILE########
__FILENAME__ = correlation_id
# Copyright (c) 2013 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Middleware that attaches a correlation id to WSGI request"""

import uuid

from ceilometer.openstack.common.middleware import base


class CorrelationIdMiddleware(base.Middleware):

    def process_request(self, req):
        correlation_id = (req.headers.get("X_CORRELATION_ID") or
                          str(uuid.uuid4()))
        req.headers['X_CORRELATION_ID'] = correlation_id

########NEW FILE########
__FILENAME__ = debug
# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Debug middleware"""

from __future__ import print_function

import sys

import six
import webob.dec

from ceilometer.openstack.common.middleware import base


class Debug(base.Middleware):
    """Helper class that returns debug information.

    Can be inserted into any WSGI application chain to get information about
    the request and response.
    """

    @webob.dec.wsgify
    def __call__(self, req):
        print(("*" * 40) + " REQUEST ENVIRON")
        for key, value in req.environ.items():
            print(key, "=", value)
        print()
        resp = req.get_response(self.application)

        print(("*" * 40) + " RESPONSE HEADERS")
        for (key, value) in six.iteritems(resp.headers):
            print(key, "=", value)
        print()

        resp.app_iter = self.print_generator(resp.app_iter)

        return resp

    @staticmethod
    def print_generator(app_iter):
        """Prints the contents of a wrapper string iterator when iterated."""
        print(("*" * 40) + " BODY")
        for part in app_iter:
            sys.stdout.write(part)
            sys.stdout.flush()
            yield part
        print()

########NEW FILE########
__FILENAME__ = notifier
# Copyright (c) 2013 eNovance
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Send notifications on request

"""
import os.path
import sys
import traceback as tb

import six
import webob.dec

from ceilometer.openstack.common import context
from ceilometer.openstack.common.gettextutils import _LE
from ceilometer.openstack.common import log as logging
from ceilometer.openstack.common.middleware import base
from ceilometer.openstack.common.notifier import api

LOG = logging.getLogger(__name__)


def log_and_ignore_error(fn):
    def wrapped(*args, **kwargs):
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            LOG.exception(_LE('An exception occurred processing '
                              'the API call: %s ') % e)
    return wrapped


class RequestNotifier(base.Middleware):
    """Send notification on request."""

    @classmethod
    def factory(cls, global_conf, **local_conf):
        """Factory method for paste.deploy."""
        conf = global_conf.copy()
        conf.update(local_conf)

        def _factory(app):
            return cls(app, **conf)
        return _factory

    def __init__(self, app, **conf):
        self.service_name = conf.get('service_name')
        self.ignore_req_list = [x.upper().strip() for x in
                                conf.get('ignore_req_list', '').split(',')]
        super(RequestNotifier, self).__init__(app)

    @staticmethod
    def environ_to_dict(environ):
        """Following PEP 333, server variables are lower case, so don't
        include them.

        """
        return dict((k, v) for k, v in six.iteritems(environ)
                    if k.isupper())

    @log_and_ignore_error
    def process_request(self, request):
        request.environ['HTTP_X_SERVICE_NAME'] = \
            self.service_name or request.host
        payload = {
            'request': self.environ_to_dict(request.environ),
        }

        api.notify(context.get_admin_context(),
                   api.publisher_id(os.path.basename(sys.argv[0])),
                   'http.request',
                   api.INFO,
                   payload)

    @log_and_ignore_error
    def process_response(self, request, response,
                         exception=None, traceback=None):
        payload = {
            'request': self.environ_to_dict(request.environ),
        }

        if response:
            payload['response'] = {
                'status': response.status,
                'headers': response.headers,
            }

        if exception:
            payload['exception'] = {
                'value': repr(exception),
                'traceback': tb.format_tb(traceback)
            }

        api.notify(context.get_admin_context(),
                   api.publisher_id(os.path.basename(sys.argv[0])),
                   'http.response',
                   api.INFO,
                   payload)

    @webob.dec.wsgify
    def __call__(self, req):
        if req.method in self.ignore_req_list:
            return req.get_response(self.application)
        else:
            self.process_request(req)
            try:
                response = req.get_response(self.application)
            except Exception:
                exc_type, value, traceback = sys.exc_info()
                self.process_response(req, None, value, traceback)
                raise
            else:
                self.process_response(req, response)
            return response

########NEW FILE########
__FILENAME__ = request_id
# Copyright (c) 2013 NEC Corporation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Middleware that ensures request ID.

It ensures to assign request ID for each API request and set it to
request environment. The request ID is also added to API response.
"""

import webob.dec

from ceilometer.openstack.common import context
from ceilometer.openstack.common.middleware import base


ENV_REQUEST_ID = 'openstack.request_id'
HTTP_RESP_HEADER_REQUEST_ID = 'x-openstack-request-id'


class RequestIdMiddleware(base.Middleware):

    @webob.dec.wsgify
    def __call__(self, req):
        req_id = context.generate_request_id()
        req.environ[ENV_REQUEST_ID] = req_id
        response = req.get_response(self.application)
        if HTTP_RESP_HEADER_REQUEST_ID not in response.headers:
            response.headers.add(HTTP_RESP_HEADER_REQUEST_ID, req_id)
        return response

########NEW FILE########
__FILENAME__ = sizelimit
# Copyright (c) 2012 Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Request Body limiting middleware.

"""

from oslo.config import cfg
import webob.dec
import webob.exc

from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common.middleware import base


#default request size is 112k
max_req_body_size = cfg.IntOpt('max_request_body_size',
                               deprecated_name='osapi_max_request_body_size',
                               default=114688,
                               help='The maximum body size '
                                    'per request, in bytes')

CONF = cfg.CONF
CONF.register_opt(max_req_body_size)


class LimitingReader(object):
    """Reader to limit the size of an incoming request."""
    def __init__(self, data, limit):
        """Initiates LimitingReader object.

        :param data: Underlying data object
        :param limit: maximum number of bytes the reader should allow
        """
        self.data = data
        self.limit = limit
        self.bytes_read = 0

    def __iter__(self):
        for chunk in self.data:
            self.bytes_read += len(chunk)
            if self.bytes_read > self.limit:
                msg = _("Request is too large.")
                raise webob.exc.HTTPRequestEntityTooLarge(explanation=msg)
            else:
                yield chunk

    def read(self, i=None):
        result = self.data.read(i)
        self.bytes_read += len(result)
        if self.bytes_read > self.limit:
            msg = _("Request is too large.")
            raise webob.exc.HTTPRequestEntityTooLarge(explanation=msg)
        return result


class RequestBodySizeLimiter(base.Middleware):
    """Limit the size of incoming requests."""

    @webob.dec.wsgify
    def __call__(self, req):
        if req.content_length > CONF.max_request_body_size:
            msg = _("Request is too large.")
            raise webob.exc.HTTPRequestEntityTooLarge(explanation=msg)
        if req.content_length is None and req.is_body_readable:
            limiter = LimitingReader(req.body_file,
                                     CONF.max_request_body_size)
            req.body_file = limiter
        return self.application

########NEW FILE########
__FILENAME__ = network_utils
# Copyright 2012 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Network-related utilities and helper functions.
"""

# TODO(jd) Use six.moves once
# https://bitbucket.org/gutworth/six/pull-request/28
# is merged
try:
    import urllib.parse
    SplitResult = urllib.parse.SplitResult
except ImportError:
    import urlparse
    SplitResult = urlparse.SplitResult

from six.moves.urllib import parse


def parse_host_port(address, default_port=None):
    """Interpret a string as a host:port pair.

    An IPv6 address MUST be escaped if accompanied by a port,
    because otherwise ambiguity ensues: 2001:db8:85a3::8a2e:370:7334
    means both [2001:db8:85a3::8a2e:370:7334] and
    [2001:db8:85a3::8a2e:370]:7334.

    >>> parse_host_port('server01:80')
    ('server01', 80)
    >>> parse_host_port('server01')
    ('server01', None)
    >>> parse_host_port('server01', default_port=1234)
    ('server01', 1234)
    >>> parse_host_port('[::1]:80')
    ('::1', 80)
    >>> parse_host_port('[::1]')
    ('::1', None)
    >>> parse_host_port('[::1]', default_port=1234)
    ('::1', 1234)
    >>> parse_host_port('2001:db8:85a3::8a2e:370:7334', default_port=1234)
    ('2001:db8:85a3::8a2e:370:7334', 1234)

    """
    if address[0] == '[':
        # Escaped ipv6
        _host, _port = address[1:].split(']')
        host = _host
        if ':' in _port:
            port = _port.split(':')[1]
        else:
            port = default_port
    else:
        if address.count(':') == 1:
            host, port = address.split(':')
        else:
            # 0 means ipv4, >1 means ipv6.
            # We prohibit unescaped ipv6 addresses with port.
            host = address
            port = default_port

    return (host, None if port is None else int(port))


class ModifiedSplitResult(SplitResult):
    """Split results class for urlsplit."""

    # NOTE(dims): The functions below are needed for Python 2.6.x.
    # We can remove these when we drop support for 2.6.x.
    @property
    def hostname(self):
        netloc = self.netloc.split('@', 1)[-1]
        host, port = parse_host_port(netloc)
        return host

    @property
    def port(self):
        netloc = self.netloc.split('@', 1)[-1]
        host, port = parse_host_port(netloc)
        return port


def urlsplit(url, scheme='', allow_fragments=True):
    """Parse a URL using urlparse.urlsplit(), splitting query and fragments.
    This function papers over Python issue9374 when needed.

    The parameters are the same as urlparse.urlsplit.
    """
    scheme, netloc, path, query, fragment = parse.urlsplit(
        url, scheme, allow_fragments)
    if allow_fragments and '#' in path:
        path, fragment = path.split('#', 1)
    if '?' in path:
        path, query = path.split('?', 1)
    return ModifiedSplitResult(scheme, netloc,
                               path, query, fragment)

########NEW FILE########
__FILENAME__ = policy
# Copyright (c) 2012 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Common Policy Engine Implementation

Policies can be expressed in one of two forms: A list of lists, or a
string written in the new policy language.

In the list-of-lists representation, each check inside the innermost
list is combined as with an "and" conjunction--for that check to pass,
all the specified checks must pass.  These innermost lists are then
combined as with an "or" conjunction.  This is the original way of
expressing policies, but there now exists a new way: the policy
language.

In the policy language, each check is specified the same way as in the
list-of-lists representation: a simple "a:b" pair that is matched to
the correct code to perform that check.  However, conjunction
operators are available, allowing for more expressiveness in crafting
policies.

As an example, take the following rule, expressed in the list-of-lists
representation::

    [["role:admin"], ["project_id:%(project_id)s", "role:projectadmin"]]

In the policy language, this becomes::

    role:admin or (project_id:%(project_id)s and role:projectadmin)

The policy language also has the "not" operator, allowing a richer
policy rule::

    project_id:%(project_id)s and not role:dunce

It is possible to perform policy checks on the following user
attributes (obtained through the token): user_id, domain_id or
project_id::

    domain_id:<some_value>

Attributes sent along with API calls can be used by the policy engine
(on the right side of the expression), by using the following syntax::

    <some_value>:user.id

Contextual attributes of objects identified by their IDs are loaded
from the database. They are also available to the policy engine and
can be checked through the `target` keyword::

    <some_value>:target.role.name

All these attributes (related to users, API calls, and context) can be
checked against each other or against constants, be it literals (True,
<a_number>) or strings.

Finally, two special policy checks should be mentioned; the policy
check "@" will always accept an access, and the policy check "!" will
always reject an access.  (Note that if a rule is either the empty
list ("[]") or the empty string, this is equivalent to the "@" policy
check.)  Of these, the "!" policy check is probably the most useful,
as it allows particular rules to be explicitly disabled.
"""

import abc
import ast
import re

from oslo.config import cfg
import six
import six.moves.urllib.parse as urlparse
import six.moves.urllib.request as urlrequest

from ceilometer.openstack.common import fileutils
from ceilometer.openstack.common.gettextutils import _, _LE
from ceilometer.openstack.common import jsonutils
from ceilometer.openstack.common import log as logging


policy_opts = [
    cfg.StrOpt('policy_file',
               default='policy.json',
               help=_('JSON file containing policy')),
    cfg.StrOpt('policy_default_rule',
               default='default',
               help=_('Rule enforced when requested rule is not found')),
]

CONF = cfg.CONF
CONF.register_opts(policy_opts)

LOG = logging.getLogger(__name__)

_checks = {}


class PolicyNotAuthorized(Exception):

    def __init__(self, rule):
        msg = _("Policy doesn't allow %s to be performed.") % rule
        super(PolicyNotAuthorized, self).__init__(msg)


class Rules(dict):
    """A store for rules. Handles the default_rule setting directly."""

    @classmethod
    def load_json(cls, data, default_rule=None):
        """Allow loading of JSON rule data."""

        # Suck in the JSON data and parse the rules
        rules = dict((k, parse_rule(v)) for k, v in
                     jsonutils.loads(data).items())

        return cls(rules, default_rule)

    def __init__(self, rules=None, default_rule=None):
        """Initialize the Rules store."""

        super(Rules, self).__init__(rules or {})
        self.default_rule = default_rule

    def __missing__(self, key):
        """Implements the default rule handling."""

        if isinstance(self.default_rule, dict):
            raise KeyError(key)

        # If the default rule isn't actually defined, do something
        # reasonably intelligent
        if not self.default_rule:
            raise KeyError(key)

        if isinstance(self.default_rule, BaseCheck):
            return self.default_rule

        # We need to check this or we can get infinite recursion
        if self.default_rule not in self:
            raise KeyError(key)

        elif isinstance(self.default_rule, six.string_types):
            return self[self.default_rule]

    def __str__(self):
        """Dumps a string representation of the rules."""

        # Start by building the canonical strings for the rules
        out_rules = {}
        for key, value in self.items():
            # Use empty string for singleton TrueCheck instances
            if isinstance(value, TrueCheck):
                out_rules[key] = ''
            else:
                out_rules[key] = str(value)

        # Dump a pretty-printed JSON representation
        return jsonutils.dumps(out_rules, indent=4)


class Enforcer(object):
    """Responsible for loading and enforcing rules.

    :param policy_file: Custom policy file to use, if none is
                        specified, `CONF.policy_file` will be
                        used.
    :param rules: Default dictionary / Rules to use. It will be
                  considered just in the first instantiation. If
                  `load_rules(True)`, `clear()` or `set_rules(True)`
                  is called this will be overwritten.
    :param default_rule: Default rule to use, CONF.default_rule will
                         be used if none is specified.
    :param use_conf: Whether to load rules from cache or config file.
    """

    def __init__(self, policy_file=None, rules=None,
                 default_rule=None, use_conf=True):
        self.rules = Rules(rules, default_rule)
        self.default_rule = default_rule or CONF.policy_default_rule

        self.policy_path = None
        self.policy_file = policy_file or CONF.policy_file
        self.use_conf = use_conf

    def set_rules(self, rules, overwrite=True, use_conf=False):
        """Create a new Rules object based on the provided dict of rules.

        :param rules: New rules to use. It should be an instance of dict.
        :param overwrite: Whether to overwrite current rules or update them
                          with the new rules.
        :param use_conf: Whether to reload rules from cache or config file.
        """

        if not isinstance(rules, dict):
            raise TypeError(_("Rules must be an instance of dict or Rules, "
                            "got %s instead") % type(rules))
        self.use_conf = use_conf
        if overwrite:
            self.rules = Rules(rules, self.default_rule)
        else:
            self.rules.update(rules)

    def clear(self):
        """Clears Enforcer rules, policy's cache and policy's path."""
        self.set_rules({})
        self.default_rule = None
        self.policy_path = None

    def load_rules(self, force_reload=False):
        """Loads policy_path's rules.

        Policy file is cached and will be reloaded if modified.

        :param force_reload: Whether to overwrite current rules.
        """

        if force_reload:
            self.use_conf = force_reload

        if self.use_conf:
            if not self.policy_path:
                self.policy_path = self._get_policy_path()

            reloaded, data = fileutils.read_cached_file(
                self.policy_path, force_reload=force_reload)
            if reloaded or not self.rules:
                rules = Rules.load_json(data, self.default_rule)
                self.set_rules(rules)
                LOG.debug("Rules successfully reloaded")

    def _get_policy_path(self):
        """Locate the policy json data file.

        :param policy_file: Custom policy file to locate.

        :returns: The policy path

        :raises: ConfigFilesNotFoundError if the file couldn't
                 be located.
        """
        policy_file = CONF.find_file(self.policy_file)

        if policy_file:
            return policy_file

        raise cfg.ConfigFilesNotFoundError((self.policy_file,))

    def enforce(self, rule, target, creds, do_raise=False,
                exc=None, *args, **kwargs):
        """Checks authorization of a rule against the target and credentials.

        :param rule: A string or BaseCheck instance specifying the rule
                    to evaluate.
        :param target: As much information about the object being operated
                    on as possible, as a dictionary.
        :param creds: As much information about the user performing the
                    action as possible, as a dictionary.
        :param do_raise: Whether to raise an exception or not if check
                        fails.
        :param exc: Class of the exception to raise if the check fails.
                    Any remaining arguments passed to check() (both
                    positional and keyword arguments) will be passed to
                    the exception class. If not specified, PolicyNotAuthorized
                    will be used.

        :return: Returns False if the policy does not allow the action and
                exc is not provided; otherwise, returns a value that
                evaluates to True.  Note: for rules using the "case"
                expression, this True value will be the specified string
                from the expression.
        """

        # NOTE(flaper87): Not logging target or creds to avoid
        # potential security issues.
        LOG.debug("Rule %s will be now enforced" % rule)

        self.load_rules()

        # Allow the rule to be a Check tree
        if isinstance(rule, BaseCheck):
            result = rule(target, creds, self)
        elif not self.rules:
            # No rules to reference means we're going to fail closed
            result = False
        else:
            try:
                # Evaluate the rule
                result = self.rules[rule](target, creds, self)
            except KeyError:
                LOG.debug("Rule [%s] doesn't exist" % rule)
                # If the rule doesn't exist, fail closed
                result = False

        # If it is False, raise the exception if requested
        if do_raise and not result:
            if exc:
                raise exc(*args, **kwargs)

            raise PolicyNotAuthorized(rule)

        return result


@six.add_metaclass(abc.ABCMeta)
class BaseCheck(object):
    """Abstract base class for Check classes."""

    @abc.abstractmethod
    def __str__(self):
        """String representation of the Check tree rooted at this node."""

        pass

    @abc.abstractmethod
    def __call__(self, target, cred, enforcer):
        """Triggers if instance of the class is called.

        Performs the check. Returns False to reject the access or a
        true value (not necessary True) to accept the access.
        """

        pass


class FalseCheck(BaseCheck):
    """A policy check that always returns False (disallow)."""

    def __str__(self):
        """Return a string representation of this check."""

        return "!"

    def __call__(self, target, cred, enforcer):
        """Check the policy."""

        return False


class TrueCheck(BaseCheck):
    """A policy check that always returns True (allow)."""

    def __str__(self):
        """Return a string representation of this check."""

        return "@"

    def __call__(self, target, cred, enforcer):
        """Check the policy."""

        return True


class Check(BaseCheck):
    """A base class to allow for user-defined policy checks."""

    def __init__(self, kind, match):
        """Initiates Check instance.

        :param kind: The kind of the check, i.e., the field before the
                     ':'.
        :param match: The match of the check, i.e., the field after
                      the ':'.
        """

        self.kind = kind
        self.match = match

    def __str__(self):
        """Return a string representation of this check."""

        return "%s:%s" % (self.kind, self.match)


class NotCheck(BaseCheck):
    """Implements the "not" logical operator.

    A policy check that inverts the result of another policy check.
    """

    def __init__(self, rule):
        """Initialize the 'not' check.

        :param rule: The rule to negate.  Must be a Check.
        """

        self.rule = rule

    def __str__(self):
        """Return a string representation of this check."""

        return "not %s" % self.rule

    def __call__(self, target, cred, enforcer):
        """Check the policy.

        Returns the logical inverse of the wrapped check.
        """

        return not self.rule(target, cred, enforcer)


class AndCheck(BaseCheck):
    """Implements the "and" logical operator.

    A policy check that requires that a list of other checks all return True.
    """

    def __init__(self, rules):
        """Initialize the 'and' check.

        :param rules: A list of rules that will be tested.
        """

        self.rules = rules

    def __str__(self):
        """Return a string representation of this check."""

        return "(%s)" % ' and '.join(str(r) for r in self.rules)

    def __call__(self, target, cred, enforcer):
        """Check the policy.

        Requires that all rules accept in order to return True.
        """

        for rule in self.rules:
            if not rule(target, cred, enforcer):
                return False

        return True

    def add_check(self, rule):
        """Adds rule to be tested.

        Allows addition of another rule to the list of rules that will
        be tested.  Returns the AndCheck object for convenience.
        """

        self.rules.append(rule)
        return self


class OrCheck(BaseCheck):
    """Implements the "or" operator.

    A policy check that requires that at least one of a list of other
    checks returns True.
    """

    def __init__(self, rules):
        """Initialize the 'or' check.

        :param rules: A list of rules that will be tested.
        """

        self.rules = rules

    def __str__(self):
        """Return a string representation of this check."""

        return "(%s)" % ' or '.join(str(r) for r in self.rules)

    def __call__(self, target, cred, enforcer):
        """Check the policy.

        Requires that at least one rule accept in order to return True.
        """

        for rule in self.rules:
            if rule(target, cred, enforcer):
                return True
        return False

    def add_check(self, rule):
        """Adds rule to be tested.

        Allows addition of another rule to the list of rules that will
        be tested.  Returns the OrCheck object for convenience.
        """

        self.rules.append(rule)
        return self


def _parse_check(rule):
    """Parse a single base check rule into an appropriate Check object."""

    # Handle the special checks
    if rule == '!':
        return FalseCheck()
    elif rule == '@':
        return TrueCheck()

    try:
        kind, match = rule.split(':', 1)
    except Exception:
        LOG.exception(_LE("Failed to understand rule %s") % rule)
        # If the rule is invalid, we'll fail closed
        return FalseCheck()

    # Find what implements the check
    if kind in _checks:
        return _checks[kind](kind, match)
    elif None in _checks:
        return _checks[None](kind, match)
    else:
        LOG.error(_LE("No handler for matches of kind %s") % kind)
        return FalseCheck()


def _parse_list_rule(rule):
    """Translates the old list-of-lists syntax into a tree of Check objects.

    Provided for backwards compatibility.
    """

    # Empty rule defaults to True
    if not rule:
        return TrueCheck()

    # Outer list is joined by "or"; inner list by "and"
    or_list = []
    for inner_rule in rule:
        # Elide empty inner lists
        if not inner_rule:
            continue

        # Handle bare strings
        if isinstance(inner_rule, six.string_types):
            inner_rule = [inner_rule]

        # Parse the inner rules into Check objects
        and_list = [_parse_check(r) for r in inner_rule]

        # Append the appropriate check to the or_list
        if len(and_list) == 1:
            or_list.append(and_list[0])
        else:
            or_list.append(AndCheck(and_list))

    # If we have only one check, omit the "or"
    if not or_list:
        return FalseCheck()
    elif len(or_list) == 1:
        return or_list[0]

    return OrCheck(or_list)


# Used for tokenizing the policy language
_tokenize_re = re.compile(r'\s+')


def _parse_tokenize(rule):
    """Tokenizer for the policy language.

    Most of the single-character tokens are specified in the
    _tokenize_re; however, parentheses need to be handled specially,
    because they can appear inside a check string.  Thankfully, those
    parentheses that appear inside a check string can never occur at
    the very beginning or end ("%(variable)s" is the correct syntax).
    """

    for tok in _tokenize_re.split(rule):
        # Skip empty tokens
        if not tok or tok.isspace():
            continue

        # Handle leading parens on the token
        clean = tok.lstrip('(')
        for i in range(len(tok) - len(clean)):
            yield '(', '('

        # If it was only parentheses, continue
        if not clean:
            continue
        else:
            tok = clean

        # Handle trailing parens on the token
        clean = tok.rstrip(')')
        trail = len(tok) - len(clean)

        # Yield the cleaned token
        lowered = clean.lower()
        if lowered in ('and', 'or', 'not'):
            # Special tokens
            yield lowered, clean
        elif clean:
            # Not a special token, but not composed solely of ')'
            if len(tok) >= 2 and ((tok[0], tok[-1]) in
                                  [('"', '"'), ("'", "'")]):
                # It's a quoted string
                yield 'string', tok[1:-1]
            else:
                yield 'check', _parse_check(clean)

        # Yield the trailing parens
        for i in range(trail):
            yield ')', ')'


class ParseStateMeta(type):
    """Metaclass for the ParseState class.

    Facilitates identifying reduction methods.
    """

    def __new__(mcs, name, bases, cls_dict):
        """Create the class.

        Injects the 'reducers' list, a list of tuples matching token sequences
        to the names of the corresponding reduction methods.
        """

        reducers = []

        for key, value in cls_dict.items():
            if not hasattr(value, 'reducers'):
                continue
            for reduction in value.reducers:
                reducers.append((reduction, key))

        cls_dict['reducers'] = reducers

        return super(ParseStateMeta, mcs).__new__(mcs, name, bases, cls_dict)


def reducer(*tokens):
    """Decorator for reduction methods.

    Arguments are a sequence of tokens, in order, which should trigger running
    this reduction method.
    """

    def decorator(func):
        # Make sure we have a list of reducer sequences
        if not hasattr(func, 'reducers'):
            func.reducers = []

        # Add the tokens to the list of reducer sequences
        func.reducers.append(list(tokens))

        return func

    return decorator


@six.add_metaclass(ParseStateMeta)
class ParseState(object):
    """Implement the core of parsing the policy language.

    Uses a greedy reduction algorithm to reduce a sequence of tokens into
    a single terminal, the value of which will be the root of the Check tree.

    Note: error reporting is rather lacking.  The best we can get with
    this parser formulation is an overall "parse failed" error.
    Fortunately, the policy language is simple enough that this
    shouldn't be that big a problem.
    """

    def __init__(self):
        """Initialize the ParseState."""

        self.tokens = []
        self.values = []

    def reduce(self):
        """Perform a greedy reduction of the token stream.

        If a reducer method matches, it will be executed, then the
        reduce() method will be called recursively to search for any more
        possible reductions.
        """

        for reduction, methname in self.reducers:
            if (len(self.tokens) >= len(reduction) and
                    self.tokens[-len(reduction):] == reduction):
                # Get the reduction method
                meth = getattr(self, methname)

                # Reduce the token stream
                results = meth(*self.values[-len(reduction):])

                # Update the tokens and values
                self.tokens[-len(reduction):] = [r[0] for r in results]
                self.values[-len(reduction):] = [r[1] for r in results]

                # Check for any more reductions
                return self.reduce()

    def shift(self, tok, value):
        """Adds one more token to the state.  Calls reduce()."""

        self.tokens.append(tok)
        self.values.append(value)

        # Do a greedy reduce...
        self.reduce()

    @property
    def result(self):
        """Obtain the final result of the parse.

        Raises ValueError if the parse failed to reduce to a single result.
        """

        if len(self.values) != 1:
            raise ValueError("Could not parse rule")
        return self.values[0]

    @reducer('(', 'check', ')')
    @reducer('(', 'and_expr', ')')
    @reducer('(', 'or_expr', ')')
    def _wrap_check(self, _p1, check, _p2):
        """Turn parenthesized expressions into a 'check' token."""

        return [('check', check)]

    @reducer('check', 'and', 'check')
    def _make_and_expr(self, check1, _and, check2):
        """Create an 'and_expr'.

        Join two checks by the 'and' operator.
        """

        return [('and_expr', AndCheck([check1, check2]))]

    @reducer('and_expr', 'and', 'check')
    def _extend_and_expr(self, and_expr, _and, check):
        """Extend an 'and_expr' by adding one more check."""

        return [('and_expr', and_expr.add_check(check))]

    @reducer('check', 'or', 'check')
    def _make_or_expr(self, check1, _or, check2):
        """Create an 'or_expr'.

        Join two checks by the 'or' operator.
        """

        return [('or_expr', OrCheck([check1, check2]))]

    @reducer('or_expr', 'or', 'check')
    def _extend_or_expr(self, or_expr, _or, check):
        """Extend an 'or_expr' by adding one more check."""

        return [('or_expr', or_expr.add_check(check))]

    @reducer('not', 'check')
    def _make_not_expr(self, _not, check):
        """Invert the result of another check."""

        return [('check', NotCheck(check))]


def _parse_text_rule(rule):
    """Parses policy to the tree.

    Translates a policy written in the policy language into a tree of
    Check objects.
    """

    # Empty rule means always accept
    if not rule:
        return TrueCheck()

    # Parse the token stream
    state = ParseState()
    for tok, value in _parse_tokenize(rule):
        state.shift(tok, value)

    try:
        return state.result
    except ValueError:
        # Couldn't parse the rule
        LOG.exception(_LE("Failed to understand rule %r") % rule)

        # Fail closed
        return FalseCheck()


def parse_rule(rule):
    """Parses a policy rule into a tree of Check objects."""

    # If the rule is a string, it's in the policy language
    if isinstance(rule, six.string_types):
        return _parse_text_rule(rule)
    return _parse_list_rule(rule)


def register(name, func=None):
    """Register a function or Check class as a policy check.

    :param name: Gives the name of the check type, e.g., 'rule',
                 'role', etc.  If name is None, a default check type
                 will be registered.
    :param func: If given, provides the function or class to register.
                 If not given, returns a function taking one argument
                 to specify the function or class to register,
                 allowing use as a decorator.
    """

    # Perform the actual decoration by registering the function or
    # class.  Returns the function or class for compliance with the
    # decorator interface.
    def decorator(func):
        _checks[name] = func
        return func

    # If the function or class is given, do the registration
    if func:
        return decorator(func)

    return decorator


@register("rule")
class RuleCheck(Check):
    def __call__(self, target, creds, enforcer):
        """Recursively checks credentials based on the defined rules."""

        try:
            return enforcer.rules[self.match](target, creds, enforcer)
        except KeyError:
            # We don't have any matching rule; fail closed
            return False


@register("role")
class RoleCheck(Check):
    def __call__(self, target, creds, enforcer):
        """Check that there is a matching role in the cred dict."""

        return self.match.lower() in [x.lower() for x in creds['roles']]


@register('http')
class HttpCheck(Check):
    def __call__(self, target, creds, enforcer):
        """Check http: rules by calling to a remote server.

        This example implementation simply verifies that the response
        is exactly 'True'.
        """

        url = ('http:' + self.match) % target
        data = {'target': jsonutils.dumps(target),
                'credentials': jsonutils.dumps(creds)}
        post_data = urlparse.urlencode(data)
        f = urlrequest.urlopen(url, post_data)
        return f.read() == "True"


@register(None)
class GenericCheck(Check):
    def __call__(self, target, creds, enforcer):
        """Check an individual match.

        Matches look like:

            tenant:%(tenant_id)s
            role:compute:admin
            True:%(user.enabled)s
            'Member':%(role.name)s
        """

        # TODO(termie): do dict inspection via dot syntax
        try:
            match = self.match % target
        except KeyError:
            # While doing GenericCheck if key not
            # present in Target return false
            return False

        try:
            # Try to interpret self.kind as a literal
            leftval = ast.literal_eval(self.kind)
        except ValueError:
            try:
                leftval = creds[self.kind]
            except KeyError:
                return False
        return match == six.text_type(leftval)

########NEW FILE########
__FILENAME__ = service
# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# Copyright 2011 Justin Santa Barbara
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Generic Node base class for all workers that run on hosts."""

import errno
import logging as std_logging
import os
import random
import signal
import sys
import time

try:
    # Importing just the symbol here because the io module does not
    # exist in Python 2.6.
    from io import UnsupportedOperation  # noqa
except ImportError:
    # Python 2.6
    UnsupportedOperation = None

import eventlet
from eventlet import event
from oslo.config import cfg

from ceilometer.openstack.common import eventlet_backdoor
from ceilometer.openstack.common.gettextutils import _LE, _LI, _LW
from ceilometer.openstack.common import importutils
from ceilometer.openstack.common import log as logging
from ceilometer.openstack.common import systemd
from ceilometer.openstack.common import threadgroup


rpc = importutils.try_import('ceilometer.openstack.common.rpc')
CONF = cfg.CONF
LOG = logging.getLogger(__name__)


def _sighup_supported():
    return hasattr(signal, 'SIGHUP')


def _is_daemon():
    # The process group for a foreground process will match the
    # process group of the controlling terminal. If those values do
    # not match, or ioctl() fails on the stdout file handle, we assume
    # the process is running in the background as a daemon.
    # http://www.gnu.org/software/bash/manual/bashref.html#Job-Control-Basics
    try:
        is_daemon = os.getpgrp() != os.tcgetpgrp(sys.stdout.fileno())
    except OSError as err:
        if err.errno == errno.ENOTTY:
            # Assume we are a daemon because there is no terminal.
            is_daemon = True
        else:
            raise
    except UnsupportedOperation:
        # Could not get the fileno for stdout, so we must be a daemon.
        is_daemon = True
    return is_daemon


def _is_sighup_and_daemon(signo):
    if not (_sighup_supported() and signo == signal.SIGHUP):
        # Avoid checking if we are a daemon, because the signal isn't
        # SIGHUP.
        return False
    return _is_daemon()


def _signo_to_signame(signo):
    signals = {signal.SIGTERM: 'SIGTERM',
               signal.SIGINT: 'SIGINT'}
    if _sighup_supported():
        signals[signal.SIGHUP] = 'SIGHUP'
    return signals[signo]


def _set_signals_handler(handler):
    signal.signal(signal.SIGTERM, handler)
    signal.signal(signal.SIGINT, handler)
    if _sighup_supported():
        signal.signal(signal.SIGHUP, handler)


class Launcher(object):
    """Launch one or more services and wait for them to complete."""

    def __init__(self):
        """Initialize the service launcher.

        :returns: None

        """
        self.services = Services()
        self.backdoor_port = eventlet_backdoor.initialize_if_enabled()

    def launch_service(self, service):
        """Load and start the given service.

        :param service: The service you would like to start.
        :returns: None

        """
        service.backdoor_port = self.backdoor_port
        self.services.add(service)

    def stop(self):
        """Stop all services which are currently running.

        :returns: None

        """
        self.services.stop()

    def wait(self):
        """Waits until all services have been stopped, and then returns.

        :returns: None

        """
        self.services.wait()

    def restart(self):
        """Reload config files and restart service.

        :returns: None

        """
        cfg.CONF.reload_config_files()
        self.services.restart()


class SignalExit(SystemExit):
    def __init__(self, signo, exccode=1):
        super(SignalExit, self).__init__(exccode)
        self.signo = signo


class ServiceLauncher(Launcher):
    def _handle_signal(self, signo, frame):
        # Allow the process to be killed again and die from natural causes
        _set_signals_handler(signal.SIG_DFL)
        raise SignalExit(signo)

    def handle_signal(self):
        _set_signals_handler(self._handle_signal)

    def _wait_for_exit_or_signal(self, ready_callback=None):
        status = None
        signo = 0

        LOG.debug('Full set of CONF:')
        CONF.log_opt_values(LOG, std_logging.DEBUG)

        try:
            if ready_callback:
                ready_callback()
            super(ServiceLauncher, self).wait()
        except SignalExit as exc:
            signame = _signo_to_signame(exc.signo)
            LOG.info(_LI('Caught %s, exiting'), signame)
            status = exc.code
            signo = exc.signo
        except SystemExit as exc:
            status = exc.code
        finally:
            self.stop()
            if rpc:
                try:
                    rpc.cleanup()
                except Exception:
                    # We're shutting down, so it doesn't matter at this point.
                    LOG.exception(_LE('Exception during rpc cleanup.'))

        return status, signo

    def wait(self, ready_callback=None):
        while True:
            self.handle_signal()
            status, signo = self._wait_for_exit_or_signal(ready_callback)
            if not _is_sighup_and_daemon(signo):
                return status
            self.restart()


class ServiceWrapper(object):
    def __init__(self, service, workers):
        self.service = service
        self.workers = workers
        self.children = set()
        self.forktimes = []


class ProcessLauncher(object):
    def __init__(self, wait_interval=0.01):
        """Constructor.

        :param wait_interval: The interval to sleep for between checks
                              of child process exit.
        """
        self.children = {}
        self.sigcaught = None
        self.running = True
        self.wait_interval = wait_interval
        rfd, self.writepipe = os.pipe()
        self.readpipe = eventlet.greenio.GreenPipe(rfd, 'r')
        self.handle_signal()

    def handle_signal(self):
        _set_signals_handler(self._handle_signal)

    def _handle_signal(self, signo, frame):
        self.sigcaught = signo
        self.running = False

        # Allow the process to be killed again and die from natural causes
        _set_signals_handler(signal.SIG_DFL)

    def _pipe_watcher(self):
        # This will block until the write end is closed when the parent
        # dies unexpectedly
        self.readpipe.read()

        LOG.info(_LI('Parent process has died unexpectedly, exiting'))

        sys.exit(1)

    def _child_process_handle_signal(self):
        # Setup child signal handlers differently
        def _sigterm(*args):
            signal.signal(signal.SIGTERM, signal.SIG_DFL)
            raise SignalExit(signal.SIGTERM)

        def _sighup(*args):
            signal.signal(signal.SIGHUP, signal.SIG_DFL)
            raise SignalExit(signal.SIGHUP)

        signal.signal(signal.SIGTERM, _sigterm)
        if _sighup_supported():
            signal.signal(signal.SIGHUP, _sighup)
        # Block SIGINT and let the parent send us a SIGTERM
        signal.signal(signal.SIGINT, signal.SIG_IGN)

    def _child_wait_for_exit_or_signal(self, launcher):
        status = 0
        signo = 0

        # NOTE(johannes): All exceptions are caught to ensure this
        # doesn't fallback into the loop spawning children. It would
        # be bad for a child to spawn more children.
        try:
            launcher.wait()
        except SignalExit as exc:
            signame = _signo_to_signame(exc.signo)
            LOG.info(_LI('Caught %s, exiting'), signame)
            status = exc.code
            signo = exc.signo
        except SystemExit as exc:
            status = exc.code
        except BaseException:
            LOG.exception(_LE('Unhandled exception'))
            status = 2
        finally:
            launcher.stop()

        return status, signo

    def _child_process(self, service):
        self._child_process_handle_signal()

        # Reopen the eventlet hub to make sure we don't share an epoll
        # fd with parent and/or siblings, which would be bad
        eventlet.hubs.use_hub()

        # Close write to ensure only parent has it open
        os.close(self.writepipe)
        # Create greenthread to watch for parent to close pipe
        eventlet.spawn_n(self._pipe_watcher)

        # Reseed random number generator
        random.seed()

        launcher = Launcher()
        launcher.launch_service(service)
        return launcher

    def _start_child(self, wrap):
        if len(wrap.forktimes) > wrap.workers:
            # Limit ourselves to one process a second (over the period of
            # number of workers * 1 second). This will allow workers to
            # start up quickly but ensure we don't fork off children that
            # die instantly too quickly.
            if time.time() - wrap.forktimes[0] < wrap.workers:
                LOG.info(_LI('Forking too fast, sleeping'))
                time.sleep(1)

            wrap.forktimes.pop(0)

        wrap.forktimes.append(time.time())

        pid = os.fork()
        if pid == 0:
            launcher = self._child_process(wrap.service)
            while True:
                self._child_process_handle_signal()
                status, signo = self._child_wait_for_exit_or_signal(launcher)
                if not _is_sighup_and_daemon(signo):
                    break
                launcher.restart()

            os._exit(status)

        LOG.info(_LI('Started child %d'), pid)

        wrap.children.add(pid)
        self.children[pid] = wrap

        return pid

    def launch_service(self, service, workers=1):
        wrap = ServiceWrapper(service, workers)

        LOG.info(_LI('Starting %d workers'), wrap.workers)
        while self.running and len(wrap.children) < wrap.workers:
            self._start_child(wrap)

    def _wait_child(self):
        try:
            # Don't block if no child processes have exited
            pid, status = os.waitpid(0, os.WNOHANG)
            if not pid:
                return None
        except OSError as exc:
            if exc.errno not in (errno.EINTR, errno.ECHILD):
                raise
            return None

        if os.WIFSIGNALED(status):
            sig = os.WTERMSIG(status)
            LOG.info(_LI('Child %(pid)d killed by signal %(sig)d'),
                     dict(pid=pid, sig=sig))
        else:
            code = os.WEXITSTATUS(status)
            LOG.info(_LI('Child %(pid)s exited with status %(code)d'),
                     dict(pid=pid, code=code))

        if pid not in self.children:
            LOG.warning(_LW('pid %d not in child list'), pid)
            return None

        wrap = self.children.pop(pid)
        wrap.children.remove(pid)
        return wrap

    def _respawn_children(self):
        while self.running:
            wrap = self._wait_child()
            if not wrap:
                # Yield to other threads if no children have exited
                # Sleep for a short time to avoid excessive CPU usage
                # (see bug #1095346)
                eventlet.greenthread.sleep(self.wait_interval)
                continue
            while self.running and len(wrap.children) < wrap.workers:
                self._start_child(wrap)

    def wait(self):
        """Loop waiting on children to die and respawning as necessary."""

        LOG.debug('Full set of CONF:')
        CONF.log_opt_values(LOG, std_logging.DEBUG)

        try:
            while True:
                self.handle_signal()
                self._respawn_children()
                if self.sigcaught:
                    signame = _signo_to_signame(self.sigcaught)
                    LOG.info(_LI('Caught %s, stopping children'), signame)
                if not _is_sighup_and_daemon(self.sigcaught):
                    break

                for pid in self.children:
                    os.kill(pid, signal.SIGHUP)
                self.running = True
                self.sigcaught = None
        except eventlet.greenlet.GreenletExit:
            LOG.info(_LI("Wait called after thread killed.  Cleaning up."))

        for pid in self.children:
            try:
                os.kill(pid, signal.SIGTERM)
            except OSError as exc:
                if exc.errno != errno.ESRCH:
                    raise

        # Wait for children to die
        if self.children:
            LOG.info(_LI('Waiting on %d children to exit'), len(self.children))
            while self.children:
                self._wait_child()


class Service(object):
    """Service object for binaries running on hosts."""

    def __init__(self, threads=1000):
        self.tg = threadgroup.ThreadGroup(threads)

        # signal that the service is done shutting itself down:
        self._done = event.Event()

    def reset(self):
        # NOTE(Fengqian): docs for Event.reset() recommend against using it
        self._done = event.Event()

    def start(self):
        pass

    def stop(self):
        self.tg.stop()
        self.tg.wait()
        # Signal that service cleanup is done:
        if not self._done.ready():
            self._done.send()

    def wait(self):
        self._done.wait()


class Services(object):

    def __init__(self):
        self.services = []
        self.tg = threadgroup.ThreadGroup()
        self.done = event.Event()

    def add(self, service):
        self.services.append(service)
        self.tg.add_thread(self.run_service, service, self.done)

    def stop(self):
        # wait for graceful shutdown of services:
        for service in self.services:
            service.stop()
            service.wait()

        # Each service has performed cleanup, now signal that the run_service
        # wrapper threads can now die:
        if not self.done.ready():
            self.done.send()

        # reap threads:
        self.tg.stop()

    def wait(self):
        self.tg.wait()

    def restart(self):
        self.stop()
        self.done = event.Event()
        for restart_service in self.services:
            restart_service.reset()
            self.tg.add_thread(self.run_service, restart_service, self.done)

    @staticmethod
    def run_service(service, done):
        """Service start wrapper.

        :param service: service to run
        :param done: event to wait on until a shutdown is triggered
        :returns: None

        """
        service.start()
        systemd.notify_once()
        done.wait()


def launch(service, workers=1):
    if workers is None or workers == 1:
        launcher = ServiceLauncher()
        launcher.launch_service(service)
    else:
        launcher = ProcessLauncher()
        launcher.launch_service(service, workers=workers)

    return launcher

########NEW FILE########
__FILENAME__ = strutils
# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
System-level utilities and helper functions.
"""

import math
import re
import sys
import unicodedata

import six

from ceilometer.openstack.common.gettextutils import _


UNIT_PREFIX_EXPONENT = {
    'k': 1,
    'K': 1,
    'Ki': 1,
    'M': 2,
    'Mi': 2,
    'G': 3,
    'Gi': 3,
    'T': 4,
    'Ti': 4,
}
UNIT_SYSTEM_INFO = {
    'IEC': (1024, re.compile(r'(^[-+]?\d*\.?\d+)([KMGT]i?)?(b|bit|B)$')),
    'SI': (1000, re.compile(r'(^[-+]?\d*\.?\d+)([kMGT])?(b|bit|B)$')),
}

TRUE_STRINGS = ('1', 't', 'true', 'on', 'y', 'yes')
FALSE_STRINGS = ('0', 'f', 'false', 'off', 'n', 'no')

SLUGIFY_STRIP_RE = re.compile(r"[^\w\s-]")
SLUGIFY_HYPHENATE_RE = re.compile(r"[-\s]+")


def int_from_bool_as_string(subject):
    """Interpret a string as a boolean and return either 1 or 0.

    Any string value in:

        ('True', 'true', 'On', 'on', '1')

    is interpreted as a boolean True.

    Useful for JSON-decoded stuff and config file parsing
    """
    return bool_from_string(subject) and 1 or 0


def bool_from_string(subject, strict=False, default=False):
    """Interpret a string as a boolean.

    A case-insensitive match is performed such that strings matching 't',
    'true', 'on', 'y', 'yes', or '1' are considered True and, when
    `strict=False`, anything else returns the value specified by 'default'.

    Useful for JSON-decoded stuff and config file parsing.

    If `strict=True`, unrecognized values, including None, will raise a
    ValueError which is useful when parsing values passed in from an API call.
    Strings yielding False are 'f', 'false', 'off', 'n', 'no', or '0'.
    """
    if not isinstance(subject, six.string_types):
        subject = str(subject)

    lowered = subject.strip().lower()

    if lowered in TRUE_STRINGS:
        return True
    elif lowered in FALSE_STRINGS:
        return False
    elif strict:
        acceptable = ', '.join(
            "'%s'" % s for s in sorted(TRUE_STRINGS + FALSE_STRINGS))
        msg = _("Unrecognized value '%(val)s', acceptable values are:"
                " %(acceptable)s") % {'val': subject,
                                      'acceptable': acceptable}
        raise ValueError(msg)
    else:
        return default


def safe_decode(text, incoming=None, errors='strict'):
    """Decodes incoming text/bytes string using `incoming` if they're not
       already unicode.

    :param incoming: Text's current encoding
    :param errors: Errors handling policy. See here for valid
        values http://docs.python.org/2/library/codecs.html
    :returns: text or a unicode `incoming` encoded
                representation of it.
    :raises TypeError: If text is not an instance of str
    """
    if not isinstance(text, (six.string_types, six.binary_type)):
        raise TypeError("%s can't be decoded" % type(text))

    if isinstance(text, six.text_type):
        return text

    if not incoming:
        incoming = (sys.stdin.encoding or
                    sys.getdefaultencoding())

    try:
        return text.decode(incoming, errors)
    except UnicodeDecodeError:
        # Note(flaper87) If we get here, it means that
        # sys.stdin.encoding / sys.getdefaultencoding
        # didn't return a suitable encoding to decode
        # text. This happens mostly when global LANG
        # var is not set correctly and there's no
        # default encoding. In this case, most likely
        # python will use ASCII or ANSI encoders as
        # default encodings but they won't be capable
        # of decoding non-ASCII characters.
        #
        # Also, UTF-8 is being used since it's an ASCII
        # extension.
        return text.decode('utf-8', errors)


def safe_encode(text, incoming=None,
                encoding='utf-8', errors='strict'):
    """Encodes incoming text/bytes string using `encoding`.

    If incoming is not specified, text is expected to be encoded with
    current python's default encoding. (`sys.getdefaultencoding`)

    :param incoming: Text's current encoding
    :param encoding: Expected encoding for text (Default UTF-8)
    :param errors: Errors handling policy. See here for valid
        values http://docs.python.org/2/library/codecs.html
    :returns: text or a bytestring `encoding` encoded
                representation of it.
    :raises TypeError: If text is not an instance of str
    """
    if not isinstance(text, (six.string_types, six.binary_type)):
        raise TypeError("%s can't be encoded" % type(text))

    if not incoming:
        incoming = (sys.stdin.encoding or
                    sys.getdefaultencoding())

    if isinstance(text, six.text_type):
        if six.PY3:
            return text.encode(encoding, errors).decode(incoming)
        else:
            return text.encode(encoding, errors)
    elif text and encoding != incoming:
        # Decode text before encoding it with `encoding`
        text = safe_decode(text, incoming, errors)
        if six.PY3:
            return text.encode(encoding, errors).decode(incoming)
        else:
            return text.encode(encoding, errors)

    return text


def string_to_bytes(text, unit_system='IEC', return_int=False):
    """Converts a string into an float representation of bytes.

    The units supported for IEC ::

        Kb(it), Kib(it), Mb(it), Mib(it), Gb(it), Gib(it), Tb(it), Tib(it)
        KB, KiB, MB, MiB, GB, GiB, TB, TiB

    The units supported for SI ::

        kb(it), Mb(it), Gb(it), Tb(it)
        kB, MB, GB, TB

    Note that the SI unit system does not support capital letter 'K'

    :param text: String input for bytes size conversion.
    :param unit_system: Unit system for byte size conversion.
    :param return_int: If True, returns integer representation of text
                       in bytes. (default: decimal)
    :returns: Numerical representation of text in bytes.
    :raises ValueError: If text has an invalid value.

    """
    try:
        base, reg_ex = UNIT_SYSTEM_INFO[unit_system]
    except KeyError:
        msg = _('Invalid unit system: "%s"') % unit_system
        raise ValueError(msg)
    match = reg_ex.match(text)
    if match:
        magnitude = float(match.group(1))
        unit_prefix = match.group(2)
        if match.group(3) in ['b', 'bit']:
            magnitude /= 8
    else:
        msg = _('Invalid string format: %s') % text
        raise ValueError(msg)
    if not unit_prefix:
        res = magnitude
    else:
        res = magnitude * pow(base, UNIT_PREFIX_EXPONENT[unit_prefix])
    if return_int:
        return int(math.ceil(res))
    return res


def to_slug(value, incoming=None, errors="strict"):
    """Normalize string.

    Convert to lowercase, remove non-word characters, and convert spaces
    to hyphens.

    Inspired by Django's `slugify` filter.

    :param value: Text to slugify
    :param incoming: Text's current encoding
    :param errors: Errors handling policy. See here for valid
        values http://docs.python.org/2/library/codecs.html
    :returns: slugified unicode representation of `value`
    :raises TypeError: If text is not an instance of str
    """
    value = safe_decode(value, incoming, errors)
    # NOTE(aababilov): no need to use safe_(encode|decode) here:
    # encodings are always "ascii", error handling is always "ignore"
    # and types are always known (first: unicode; second: str)
    value = unicodedata.normalize("NFKD", value).encode(
        "ascii", "ignore").decode("ascii")
    value = SLUGIFY_STRIP_RE.sub("", value).strip().lower()
    return SLUGIFY_HYPHENATE_RE.sub("-", value)

########NEW FILE########
__FILENAME__ = systemd
# Copyright 2012-2014 Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Helper module for systemd service readiness notification.
"""

import os
import socket
import sys

from ceilometer.openstack.common import log as logging


LOG = logging.getLogger(__name__)


def _abstractify(socket_name):
    if socket_name.startswith('@'):
        # abstract namespace socket
        socket_name = '\0%s' % socket_name[1:]
    return socket_name


def _sd_notify(unset_env, msg):
    notify_socket = os.getenv('NOTIFY_SOCKET')
    if notify_socket:
        sock = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM)
        try:
            sock.connect(_abstractify(notify_socket))
            sock.sendall(msg)
            if unset_env:
                del os.environ['NOTIFY_SOCKET']
        except EnvironmentError:
            LOG.debug("Systemd notification failed", exc_info=True)
        finally:
            sock.close()


def notify():
    """Send notification to Systemd that service is ready.
    For details see
      http://www.freedesktop.org/software/systemd/man/sd_notify.html
    """
    _sd_notify(False, 'READY=1')


def notify_once():
    """Send notification once to Systemd that service is ready.
    Systemd sets NOTIFY_SOCKET environment variable with the name of the
    socket listening for notifications from services.
    This method removes the NOTIFY_SOCKET environment variable to ensure
    notification is sent only once.
    """
    _sd_notify(True, 'READY=1')


def onready(notify_socket, timeout):
    """Wait for systemd style notification on the socket.

    :param notify_socket: local socket address
    :type notify_socket:  string
    :param timeout:       socket timeout
    :type timeout:        float
    :returns:             0 service ready
                          1 service not ready
                          2 timeout occured
    """
    sock = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM)
    sock.settimeout(timeout)
    sock.bind(_abstractify(notify_socket))
    try:
        msg = sock.recv(512)
    except socket.timeout:
        return 2
    finally:
        sock.close()
    if 'READY=1' in msg:
        return 0
    else:
        return 1


if __name__ == '__main__':
    # simple CLI for testing
    if len(sys.argv) == 1:
        notify()
    elif len(sys.argv) >= 2:
        timeout = float(sys.argv[1])
        notify_socket = os.getenv('NOTIFY_SOCKET')
        if notify_socket:
            retval = onready(notify_socket, timeout)
            sys.exit(retval)

########NEW FILE########
__FILENAME__ = test
# Copyright (c) 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

##############################################################################
##############################################################################
##
## DO NOT MODIFY THIS FILE
##
## This file is being graduated to the ceilometertest library. Please make all
## changes there, and only backport critical fixes here. - dhellmann
##
##############################################################################
##############################################################################

"""Common utilities used in testing"""

import logging
import os
import tempfile

import fixtures
import testtools

_TRUE_VALUES = ('True', 'true', '1', 'yes')
_LOG_FORMAT = "%(levelname)8s [%(name)s] %(message)s"


class BaseTestCase(testtools.TestCase):

    def setUp(self):
        super(BaseTestCase, self).setUp()
        self._set_timeout()
        self._fake_output()
        self._fake_logs()
        self.useFixture(fixtures.NestedTempfile())
        self.useFixture(fixtures.TempHomeDir())
        self.tempdirs = []

    def _set_timeout(self):
        test_timeout = os.environ.get('OS_TEST_TIMEOUT', 0)
        try:
            test_timeout = int(test_timeout)
        except ValueError:
            # If timeout value is invalid do not set a timeout.
            test_timeout = 0
        if test_timeout > 0:
            self.useFixture(fixtures.Timeout(test_timeout, gentle=True))

    def _fake_output(self):
        if os.environ.get('OS_STDOUT_CAPTURE') in _TRUE_VALUES:
            stdout = self.useFixture(fixtures.StringStream('stdout')).stream
            self.useFixture(fixtures.MonkeyPatch('sys.stdout', stdout))
        if os.environ.get('OS_STDERR_CAPTURE') in _TRUE_VALUES:
            stderr = self.useFixture(fixtures.StringStream('stderr')).stream
            self.useFixture(fixtures.MonkeyPatch('sys.stderr', stderr))

    def _fake_logs(self):
        if os.environ.get('OS_DEBUG') in _TRUE_VALUES:
            level = logging.DEBUG
        else:
            level = logging.INFO
        capture_logs = os.environ.get('OS_LOG_CAPTURE') in _TRUE_VALUES
        if capture_logs:
            self.useFixture(
                fixtures.FakeLogger(
                    format=_LOG_FORMAT,
                    level=level,
                    nuke_handlers=capture_logs,
                )
            )
        else:
            logging.basicConfig(format=_LOG_FORMAT, level=level)

    def create_tempfiles(self, files, ext='.conf'):
        tempfiles = []
        for (basename, contents) in files:
            if not os.path.isabs(basename):
                (fd, path) = tempfile.mkstemp(prefix=basename, suffix=ext)
            else:
                path = basename + ext
                fd = os.open(path, os.O_CREAT | os.O_WRONLY)
            tempfiles.append(path)
            try:
                os.write(fd, contents)
            finally:
                os.close(fd)
        return tempfiles

########NEW FILE########
__FILENAME__ = threadgroup
# Copyright 2012 Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
import threading

import eventlet
from eventlet import greenpool

from ceilometer.openstack.common import log as logging
from ceilometer.openstack.common import loopingcall


LOG = logging.getLogger(__name__)


def _thread_done(gt, *args, **kwargs):
    """Callback function to be passed to GreenThread.link() when we spawn()
    Calls the :class:`ThreadGroup` to notify if.

    """
    kwargs['group'].thread_done(kwargs['thread'])


class Thread(object):
    """Wrapper around a greenthread, that holds a reference to the
    :class:`ThreadGroup`. The Thread will notify the :class:`ThreadGroup` when
    it has done so it can be removed from the threads list.
    """
    def __init__(self, thread, group):
        self.thread = thread
        self.thread.link(_thread_done, group=group, thread=self)

    def stop(self):
        self.thread.kill()

    def wait(self):
        return self.thread.wait()

    def link(self, func, *args, **kwargs):
        self.thread.link(func, *args, **kwargs)


class ThreadGroup(object):
    """The point of the ThreadGroup class is to:

    * keep track of timers and greenthreads (making it easier to stop them
      when need be).
    * provide an easy API to add timers.
    """
    def __init__(self, thread_pool_size=10):
        self.pool = greenpool.GreenPool(thread_pool_size)
        self.threads = []
        self.timers = []

    def add_dynamic_timer(self, callback, initial_delay=None,
                          periodic_interval_max=None, *args, **kwargs):
        timer = loopingcall.DynamicLoopingCall(callback, *args, **kwargs)
        timer.start(initial_delay=initial_delay,
                    periodic_interval_max=periodic_interval_max)
        self.timers.append(timer)

    def add_timer(self, interval, callback, initial_delay=None,
                  *args, **kwargs):
        pulse = loopingcall.FixedIntervalLoopingCall(callback, *args, **kwargs)
        pulse.start(interval=interval,
                    initial_delay=initial_delay)
        self.timers.append(pulse)

    def add_thread(self, callback, *args, **kwargs):
        gt = self.pool.spawn(callback, *args, **kwargs)
        th = Thread(gt, self)
        self.threads.append(th)
        return th

    def thread_done(self, thread):
        self.threads.remove(thread)

    def stop(self):
        current = threading.current_thread()

        # Iterate over a copy of self.threads so thread_done doesn't
        # modify the list while we're iterating
        for x in self.threads[:]:
            if x is current:
                # don't kill the current thread.
                continue
            try:
                x.stop()
            except Exception as ex:
                LOG.exception(ex)

        for x in self.timers:
            try:
                x.stop()
            except Exception as ex:
                LOG.exception(ex)
        self.timers = []

    def wait(self):
        for x in self.timers:
            try:
                x.wait()
            except eventlet.greenlet.GreenletExit:
                pass
            except Exception as ex:
                LOG.exception(ex)
        current = threading.current_thread()

        # Iterate over a copy of self.threads so thread_done doesn't
        # modify the list while we're iterating
        for x in self.threads[:]:
            if x is current:
                continue
            try:
                x.wait()
            except eventlet.greenlet.GreenletExit:
                pass
            except Exception as ex:
                LOG.exception(ex)

########NEW FILE########
__FILENAME__ = timeutils
# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Time related utilities and helper functions.
"""

import calendar
import datetime
import time

import iso8601
import six


# ISO 8601 extended time format with microseconds
_ISO8601_TIME_FORMAT_SUBSECOND = '%Y-%m-%dT%H:%M:%S.%f'
_ISO8601_TIME_FORMAT = '%Y-%m-%dT%H:%M:%S'
PERFECT_TIME_FORMAT = _ISO8601_TIME_FORMAT_SUBSECOND


def isotime(at=None, subsecond=False):
    """Stringify time in ISO 8601 format."""
    if not at:
        at = utcnow()
    st = at.strftime(_ISO8601_TIME_FORMAT
                     if not subsecond
                     else _ISO8601_TIME_FORMAT_SUBSECOND)
    tz = at.tzinfo.tzname(None) if at.tzinfo else 'UTC'
    st += ('Z' if tz == 'UTC' else tz)
    return st


def parse_isotime(timestr):
    """Parse time from ISO 8601 format."""
    try:
        return iso8601.parse_date(timestr)
    except iso8601.ParseError as e:
        raise ValueError(six.text_type(e))
    except TypeError as e:
        raise ValueError(six.text_type(e))


def strtime(at=None, fmt=PERFECT_TIME_FORMAT):
    """Returns formatted utcnow."""
    if not at:
        at = utcnow()
    return at.strftime(fmt)


def parse_strtime(timestr, fmt=PERFECT_TIME_FORMAT):
    """Turn a formatted time back into a datetime."""
    return datetime.datetime.strptime(timestr, fmt)


def normalize_time(timestamp):
    """Normalize time in arbitrary timezone to UTC naive object."""
    offset = timestamp.utcoffset()
    if offset is None:
        return timestamp
    return timestamp.replace(tzinfo=None) - offset


def is_older_than(before, seconds):
    """Return True if before is older than seconds."""
    if isinstance(before, six.string_types):
        before = parse_strtime(before).replace(tzinfo=None)
    else:
        before = before.replace(tzinfo=None)

    return utcnow() - before > datetime.timedelta(seconds=seconds)


def is_newer_than(after, seconds):
    """Return True if after is newer than seconds."""
    if isinstance(after, six.string_types):
        after = parse_strtime(after).replace(tzinfo=None)
    else:
        after = after.replace(tzinfo=None)

    return after - utcnow() > datetime.timedelta(seconds=seconds)


def utcnow_ts():
    """Timestamp version of our utcnow function."""
    if utcnow.override_time is None:
        # NOTE(kgriffs): This is several times faster
        # than going through calendar.timegm(...)
        return int(time.time())

    return calendar.timegm(utcnow().timetuple())


def utcnow():
    """Overridable version of utils.utcnow."""
    if utcnow.override_time:
        try:
            return utcnow.override_time.pop(0)
        except AttributeError:
            return utcnow.override_time
    return datetime.datetime.utcnow()


def iso8601_from_timestamp(timestamp):
    """Returns a iso8601 formatted date from timestamp."""
    return isotime(datetime.datetime.utcfromtimestamp(timestamp))


utcnow.override_time = None


def set_time_override(override_time=None):
    """Overrides utils.utcnow.

    Make it return a constant time or a list thereof, one at a time.

    :param override_time: datetime instance or list thereof. If not
                          given, defaults to the current UTC time.
    """
    utcnow.override_time = override_time or datetime.datetime.utcnow()


def advance_time_delta(timedelta):
    """Advance overridden time using a datetime.timedelta."""
    assert(not utcnow.override_time is None)
    try:
        for dt in utcnow.override_time:
            dt += timedelta
    except TypeError:
        utcnow.override_time += timedelta


def advance_time_seconds(seconds):
    """Advance overridden time by seconds."""
    advance_time_delta(datetime.timedelta(0, seconds))


def clear_time_override():
    """Remove the overridden time."""
    utcnow.override_time = None


def marshall_now(now=None):
    """Make an rpc-safe datetime with microseconds.

    Note: tzinfo is stripped, but not required for relative times.
    """
    if not now:
        now = utcnow()
    return dict(day=now.day, month=now.month, year=now.year, hour=now.hour,
                minute=now.minute, second=now.second,
                microsecond=now.microsecond)


def unmarshall_time(tyme):
    """Unmarshall a datetime dict."""
    return datetime.datetime(day=tyme['day'],
                             month=tyme['month'],
                             year=tyme['year'],
                             hour=tyme['hour'],
                             minute=tyme['minute'],
                             second=tyme['second'],
                             microsecond=tyme['microsecond'])


def delta_seconds(before, after):
    """Return the difference between two timing objects.

    Compute the difference in seconds between two date, time, or
    datetime objects (as a float, to microsecond resolution).
    """
    delta = after - before
    return total_seconds(delta)


def total_seconds(delta):
    """Return the total seconds of datetime.timedelta object.

    Compute total seconds of datetime.timedelta, datetime.timedelta
    doesn't have method total_seconds in Python2.6, calculate it manually.
    """
    try:
        return delta.total_seconds()
    except AttributeError:
        return ((delta.days * 24 * 3600) + delta.seconds +
                float(delta.microseconds) / (10 ** 6))


def is_soon(dt, window):
    """Determines if time is going to happen in the next window seconds.

    :param dt: the time
    :param window: minimum seconds to remain to consider the time not soon

    :return: True if expiration is within the given duration
    """
    soon = (utcnow() + datetime.timedelta(seconds=window))
    return normalize_time(dt) <= soon

########NEW FILE########
__FILENAME__ = units
# Copyright 2013 IBM Corp
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Unit constants
"""

#Binary unit constants.
Ki = 1024
Mi = 1024 ** 2
Gi = 1024 ** 3
Ti = 1024 ** 4
Pi = 1024 ** 5
Ei = 1024 ** 6
Zi = 1024 ** 7
Yi = 1024 ** 8

#Decimal unit constants.
k = 1000
M = 1000 ** 2
G = 1000 ** 3
T = 1000 ** 4
P = 1000 ** 5
E = 1000 ** 6
Z = 1000 ** 7
Y = 1000 ** 8

########NEW FILE########
__FILENAME__ = notifications
# Author: Swann Croiset <swann.croiset@bull.net>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Handler for producing orchestration metering from Heat notification
   events.
"""

from oslo.config import cfg
import oslo.messaging

from ceilometer import plugin
from ceilometer import sample


OPTS = [
    cfg.StrOpt('heat_control_exchange',
               default='heat',
               help="Exchange name for Heat notifications"),
]

cfg.CONF.register_opts(OPTS)
SERVICE = 'orchestration'


class StackCRUD(plugin.NotificationBase):

    resource_name = '%s.stack' % SERVICE

    @property
    def event_types(self):
        return [
            '%s.create.end' % (self.resource_name),
            '%s.update.end' % (self.resource_name),
            '%s.delete.end' % (self.resource_name),
            '%s.resume.end' % (self.resource_name),
            '%s.suspend.end' % (self.resource_name),
        ]

    @staticmethod
    def get_targets(conf):
        """Return a sequence of oslo.messaging.Target defining the exchange and
        topics to be connected for this plugin.
        """
        return [oslo.messaging.Target(topic=topic,
                                      exchange=conf.heat_control_exchange)
                for topic in conf.notification_topics]

    def process_notification(self, message):
        name = message['event_type']                \
            .replace(self.resource_name, 'stack')   \
            .replace('.end', '')

        project_id = message['payload']['tenant_id']

        # Trying to use the trustor_id if trusts is used by Heat,
        user_id = message.get('_context_trustor_user_id') or \
            message['_context_user_id']

        yield sample.Sample.from_notification(
            name=name,
            type=sample.TYPE_DELTA,
            unit='stack',
            volume=1,
            resource_id=message['payload']['stack_identity'],
            user_id=user_id,
            project_id=project_id,
            message=message)

########NEW FILE########
__FILENAME__ = pipeline
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Intel Corp.
# Copyright © 2014 Red Hat, Inc
#
# Authors: Yunhong Jiang <yunhong.jiang@intel.com>
#          Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import fnmatch
import itertools
import operator
import os

from oslo.config import cfg
import yaml

from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer import publisher
from ceilometer import transformer as xformer


OPTS = [
    cfg.StrOpt('pipeline_cfg_file',
               default="pipeline.yaml",
               help="Configuration file for pipeline definition."
               ),
]

cfg.CONF.register_opts(OPTS)

LOG = log.getLogger(__name__)


class PipelineException(Exception):
    def __init__(self, message, pipeline_cfg):
        self.msg = message
        self.pipeline_cfg = pipeline_cfg

    def __str__(self):
        return 'Pipeline %s: %s' % (self.pipeline_cfg, self.msg)


class PublishContext(object):

    def __init__(self, context, pipelines=None):
        pipelines = pipelines or []
        self.pipelines = set(pipelines)
        self.context = context

    def add_pipelines(self, pipelines):
        self.pipelines.update(pipelines)

    def __enter__(self):
        def p(samples):
            for p in self.pipelines:
                p.publish_samples(self.context,
                                  samples)
        return p

    def __exit__(self, exc_type, exc_value, traceback):
        for p in self.pipelines:
            p.flush(self.context)


class Source(object):
    """Represents a source of samples, in effect a set of pollsters
    and/or notification handlers emitting samples for a set of matching
    meters.

    Each source encapsulates meter name matching, polling interval
    determination, optional resource enumeration or discovery, and
    mapping to one or more sinks for publication.

    """

    def __init__(self, cfg):
        self.cfg = cfg

        try:
            self.name = cfg['name']
            try:
                self.interval = int(cfg['interval'])
            except ValueError:
                raise PipelineException("Invalid interval value", cfg)
            # Support 'counters' for backward compatibility
            self.meters = cfg.get('meters', cfg.get('counters'))
            self.sinks = cfg.get('sinks')
        except KeyError as err:
            raise PipelineException(
                "Required field %s not specified" % err.args[0], cfg)

        if self.interval <= 0:
            raise PipelineException("Interval value should > 0", cfg)

        self.resources = cfg.get('resources') or []
        if not isinstance(self.resources, list):
            raise PipelineException("Resources should be a list", cfg)

        self.discovery = cfg.get('discovery') or []
        if not isinstance(self.discovery, list):
            raise PipelineException("Discovery should be a list", cfg)

        self._check_meters()

    def __str__(self):
        return self.name

    def _check_meters(self):
        """Meter rules checking

        At least one meaningful meter exist
        Included type and excluded type meter can't co-exist at
        the same pipeline
        Included type meter and wildcard can't co-exist at same pipeline

        """
        meters = self.meters
        if not meters:
            raise PipelineException("No meter specified", self.cfg)

        if [x for x in meters if x[0] not in '!*'] and \
           [x for x in meters if x[0] == '!']:
            raise PipelineException(
                "Both included and excluded meters specified",
                cfg)

        if '*' in meters and [x for x in meters if x[0] not in '!*']:
            raise PipelineException(
                "Included meters specified with wildcard",
                self.cfg)

    # (yjiang5) To support meters like instance:m1.tiny,
    # which include variable part at the end starting with ':'.
    # Hope we will not add such meters in future.
    @staticmethod
    def _variable_meter_name(name):
        m = name.partition(':')
        if m[1] == ':':
            return m[1].join((m[0], '*'))
        else:
            return name

    def support_meter(self, meter_name):
        meter_name = self._variable_meter_name(meter_name)

        # Special case: if we only have negation, we suppose the default is
        # allow
        default = all(meter.startswith('!') for meter in self.meters)

        # Support wildcard like storage.* and !disk.*
        # Start with negation, we consider that the order is deny, allow
        if any(fnmatch.fnmatch(meter_name, meter[1:])
               for meter in self.meters
               if meter[0] == '!'):
            return False

        if any(fnmatch.fnmatch(meter_name, meter)
               for meter in self.meters
               if meter[0] != '!'):
            return True

        return default

    def check_sinks(self, sinks):
        if not self.sinks:
            raise PipelineException(
                "No sink defined in source %s" % self,
                self.cfg)
        for sink in self.sinks:
            if sink not in sinks:
                raise PipelineException(
                    "Dangling sink %s from source %s" % (sink, self),
                    self.cfg)


class Sink(object):
    """Represents a sink for the transformation and publication of
    samples emitted from a related source.

    Each sink config is concerned *only* with the transformation rules
    and publication conduits for samples.

    In effect, a sink describes a chain of handlers. The chain starts
    with zero or more transformers and ends with one or more publishers.

    The first transformer in the chain is passed samples from the
    corresponding source, takes some action such as deriving rate of
    change, performing unit conversion, or aggregating, before passing
    the modified sample to next step.

    The subsequent transformers, if any, handle the data similarly.

    At the end of the chain, publishers publish the data. The exact
    publishing method depends on publisher type, for example, pushing
    into data storage via the message bus providing guaranteed delivery,
    or for loss-tolerant samples UDP may be used.

    If no transformers are included in the chain, the publishers are
    passed samples directly from the sink which are published unchanged.

    """

    def __init__(self, cfg, transformer_manager):
        self.cfg = cfg

        try:
            self.name = cfg['name']
            # It's legal to have no transformer specified
            self.transformer_cfg = cfg['transformers'] or []
        except KeyError as err:
            raise PipelineException(
                "Required field %s not specified" % err.args[0], cfg)

        if not cfg.get('publishers'):
            raise PipelineException("No publisher specified", cfg)

        self.publishers = []
        for p in cfg['publishers']:
            if '://' not in p:
                # Support old format without URL
                p = p + "://"
            try:
                self.publishers.append(publisher.get_publisher(p))
            except Exception:
                LOG.exception(_("Unable to load publisher %s"), p)

        self.transformers = self._setup_transformers(cfg, transformer_manager)

    def __str__(self):
        return self.name

    def _setup_transformers(self, cfg, transformer_manager):
        transformer_cfg = cfg['transformers'] or []
        transformers = []
        for transformer in transformer_cfg:
            parameter = transformer['parameters'] or {}
            try:
                ext = transformer_manager.get_ext(transformer['name'])
            except KeyError:
                raise PipelineException(
                    "No transformer named %s loaded" % transformer['name'],
                    cfg)
            transformers.append(ext.plugin(**parameter))
            LOG.info(_(
                "Pipeline %(pipeline)s: Setup transformer instance %(name)s "
                "with parameter %(param)s") % ({'pipeline': self,
                                                'name': transformer['name'],
                                                'param': parameter}))

        return transformers

    def _transform_sample(self, start, ctxt, sample):
        try:
            for transformer in self.transformers[start:]:
                sample = transformer.handle_sample(ctxt, sample)
                if not sample:
                    LOG.debug(_(
                        "Pipeline %(pipeline)s: Sample dropped by "
                        "transformer %(trans)s") % ({'pipeline': self,
                                                     'trans': transformer}))
                    return
            return sample
        except Exception as err:
            LOG.warning(_("Pipeline %(pipeline)s: "
                          "Exit after error from transformer "
                          "%(trans)s for %(smp)s") % ({'pipeline': self,
                                                       'trans': transformer,
                                                       'smp': sample}))
            LOG.exception(err)

    def _publish_samples(self, start, ctxt, samples):
        """Push samples into pipeline for publishing.

        :param start: The first transformer that the sample will be injected.
                      This is mainly for flush() invocation that transformer
                      may emit samples.
        :param ctxt: Execution context from the manager or service.
        :param samples: Sample list.

        """

        transformed_samples = []
        for sample in samples:
            LOG.debug(_(
                "Pipeline %(pipeline)s: Transform sample "
                "%(smp)s from %(trans)s transformer") % ({'pipeline': self,
                                                          'smp': sample,
                                                          'trans': start}))
            sample = self._transform_sample(start, ctxt, sample)
            if sample:
                transformed_samples.append(sample)

        if transformed_samples:
            LOG.audit(_("Pipeline %s: Publishing samples"), self)
            for p in self.publishers:
                try:
                    p.publish_samples(ctxt, transformed_samples)
                except Exception:
                    LOG.exception(_(
                        "Pipeline %(pipeline)s: Continue after error "
                        "from publisher %(pub)s") % ({'pipeline': self,
                                                      'pub': p}))
            LOG.audit(_("Pipeline %s: Published samples") % self)

    def publish_samples(self, ctxt, samples):
        for meter_name, samples in itertools.groupby(
                sorted(samples, key=operator.attrgetter('name')),
                operator.attrgetter('name')):
            self._publish_samples(0, ctxt, samples)

    def flush(self, ctxt):
        """Flush data after all samples have been injected to pipeline."""

        for (i, transformer) in enumerate(self.transformers):
            try:
                self._publish_samples(i + 1, ctxt,
                                      list(transformer.flush(ctxt)))
            except Exception as err:
                LOG.warning(_(
                    "Pipeline %(pipeline)s: Error flushing "
                    "transformer %(trans)s") % ({'pipeline': self,
                                                 'trans': transformer}))
                LOG.exception(err)


class Pipeline(object):
    """Represents a coupling between a sink and a corresponding source.
    """

    def __init__(self, source, sink):
        self.source = source
        self.sink = sink
        self.name = str(self)

    def __str__(self):
        return (self.source.name if self.source.name == self.sink.name
                else '%s:%s' % (self.source.name, self.sink.name))

    def get_interval(self):
        return self.source.interval

    @property
    def resources(self):
        return self.source.resources

    @property
    def discovery(self):
        return self.source.discovery

    def support_meter(self, meter_name):
        return self.source.support_meter(meter_name)

    @property
    def publishers(self):
        return self.sink.publishers

    def publish_sample(self, ctxt, sample):
        self.publish_samples(ctxt, [sample])

    def publish_samples(self, ctxt, samples):
        supported = [s for s in samples if self.source.support_meter(s.name)]
        self.sink.publish_samples(ctxt, supported)

    def flush(self, ctxt):
        self.sink.flush(ctxt)


class PipelineManager(object):
    """Pipeline Manager

    Pipeline manager sets up pipelines according to config file

    Usually only one pipeline manager exists in the system.

    """

    def __init__(self, cfg, transformer_manager):
        """Setup the pipelines according to config.

        The configuration is supported in one of two forms:

        1. Deprecated: the source and sink configuration are conflated
           as a list of consolidated pipelines.

           The pipelines are defined as a list of dictionaries each
           specifying the target samples, the transformers involved,
           and the target publishers, for example:

           [{"name": pipeline_1,
             "interval": interval_time,
             "meters" : ["meter_1", "meter_2"],
             "resources": ["resource_uri1", "resource_uri2"],
             "transformers": [
                              {"name": "Transformer_1",
                               "parameters": {"p1": "value"}},

                              {"name": "Transformer_2",
                               "parameters": {"p1": "value"}},
                              ],
             "publishers": ["publisher_1", "publisher_2"]
            },
            {"name": pipeline_2,
             "interval": interval_time,
             "meters" : ["meter_3"],
             "publishers": ["publisher_3"]
            },
           ]

        2. Decoupled: the source and sink configuration are separately
           specified before being linked together. This allows source-
           specific configuration, such as resource discovery, to be
           kept focused only on the fine-grained source while avoiding
           the necessity for wide duplication of sink-related config.

           The configuration is provided in the form of separate lists
           of dictionaries defining sources and sinks, for example:

           {"sources": [{"name": source_1,
                         "interval": interval_time,
                         "meters" : ["meter_1", "meter_2"],
                         "resources": ["resource_uri1", "resource_uri2"],
                         "sinks" : ["sink_1", "sink_2"]
                        },
                        {"name": source_2,
                         "interval": interval_time,
                         "meters" : ["meter_3"],
                         "sinks" : ["sink_2"]
                        },
                       ],
            "sinks": [{"name": sink_1,
                       "transformers": [
                              {"name": "Transformer_1",
                               "parameters": {"p1": "value"}},

                              {"name": "Transformer_2",
                               "parameters": {"p1": "value"}},
                             ],
                        "publishers": ["publisher_1", "publisher_2"]
                       },
                       {"name": sink_2,
                        "publishers": ["publisher_3"]
                       },
                      ]
           }

        The semantics of the common individual configuration elements
        are identical in the deprecated and decoupled version.

        The interval determines the cadence of sample injection into
        the pipeline where samples are produced under the direct control
        of an agent, i.e. via a polling cycle as opposed to incoming
        notifications.

        Valid meter format is '*', '!meter_name', or 'meter_name'.
        '*' is wildcard symbol means any meters; '!meter_name' means
        "meter_name" will be excluded; 'meter_name' means 'meter_name'
        will be included.

        The 'meter_name" is Sample name field. For meter names with
        variable like "instance:m1.tiny", it's "instance:*".

        Valid meters definition is all "included meter names", all
        "excluded meter names", wildcard and "excluded meter names", or
        only wildcard.

        The resources is list of URI indicating the resources from where
        the meters should be polled. It's optional and it's up to the
        specific pollster to decide how to use it.

        Transformer's name is plugin name in setup.cfg.

        Publisher's name is plugin name in setup.cfg

        """
        self.pipelines = []
        if 'sources' in cfg or 'sinks' in cfg:
            if not ('sources' in cfg and 'sinks' in cfg):
                raise PipelineException("Both sources & sinks are required",
                                        cfg)
            LOG.info(_('detected decoupled pipeline config format'))
            sources = [Source(s) for s in cfg.get('sources', [])]
            sinks = dict((s['name'], Sink(s, transformer_manager))
                         for s in cfg.get('sinks', []))
            for source in sources:
                source.check_sinks(sinks)
                for target in source.sinks:
                    self.pipelines.append(Pipeline(source,
                                                   sinks[target]))
        else:
            LOG.warning(_('detected deprecated pipeline config format'))
            for pipedef in cfg:
                source = Source(pipedef)
                sink = Sink(pipedef, transformer_manager)
                self.pipelines.append(Pipeline(source, sink))

    def publisher(self, context):
        """Build a new Publisher for these manager pipelines.

        :param context: The context.
        """
        return PublishContext(context, self.pipelines)


def setup_pipeline(transformer_manager=None):
    """Setup pipeline manager according to yaml config file."""
    cfg_file = cfg.CONF.pipeline_cfg_file
    if not os.path.exists(cfg_file):
        cfg_file = cfg.CONF.find_file(cfg_file)

    LOG.debug(_("Pipeline config file: %s"), cfg_file)

    with open(cfg_file) as fap:
        data = fap.read()

    pipeline_cfg = yaml.safe_load(data)
    LOG.info(_("Pipeline config: %s"), pipeline_cfg)

    return PipelineManager(pipeline_cfg,
                           transformer_manager or
                           xformer.TransformerExtensionManager(
                               'ceilometer.transformer',
                           ))

########NEW FILE########
__FILENAME__ = plugin
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Base class for plugins.
"""

import abc
import collections
import fnmatch

import oslo.messaging
import six

from ceilometer import messaging
from ceilometer.openstack.common import context
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log

LOG = log.getLogger(__name__)

ExchangeTopics = collections.namedtuple('ExchangeTopics',
                                        ['exchange', 'topics'])


class PluginBase(object):
    """Base class for all plugins.
    """


@six.add_metaclass(abc.ABCMeta)
class NotificationBase(PluginBase):
    """Base class for plugins that support the notification API."""
    def __init__(self, pipeline_manager):
        super(NotificationBase, self).__init__()
        self.pipeline_manager = pipeline_manager

    @abc.abstractproperty
    def event_types(self):
        """Return a sequence of strings defining the event types to be
        given to this plugin.
        """

    def get_targets(self, conf):
        """Return a sequence of oslo.messaging.Target defining the exchange and
        topics to be connected for this plugin.

        :param conf: Configuration.
        """

        #TODO(sileht): Backwards compatibility, remove in J+1
        if hasattr(self, 'get_exchange_topics'):
            LOG.warn(_('get_exchange_topics API of NotificationPlugin is'
                       'deprecated, implements get_targets instead.'))

            targets = []
            for exchange, topics in self.get_exchange_topics(conf):
                targets.extend([oslo.messaging.Target(topic=topic,
                                                      exchange=exchange)
                                for topic in topics])
            return targets

    @abc.abstractmethod
    def process_notification(self, message):
        """Return a sequence of Counter instances for the given message.

        :param message: Message to process.
        """

    @staticmethod
    def _handle_event_type(event_type, event_type_to_handle):
        """Check whether event_type should be handled according to
        event_type_to_handle.

        """
        return any(map(lambda e: fnmatch.fnmatch(event_type, e),
                       event_type_to_handle))

    def info(self, ctxt, publisher_id, event_type, payload, metadata):
        """RPC endpoint for notification messages

        When another service sends a notification over the message
        bus, this method receives it.

        :param ctxt: oslo.messaging context
        :param publisher_id: publisher of the notification
        :param event_type: type of notification
        :param payload: notification payload
        :param metadata: metadata about the notification

        """
        notification = messaging.convert_to_old_notification_format(
            'info', ctxt, publisher_id, event_type, payload, metadata)
        self.to_samples_and_publish(context.get_admin_context(), notification)

    def to_samples_and_publish(self, context, notification):
        """Return samples produced by *process_notification* for the given
        notification.

        :param context: Execution context from the service or RPC call
        :param notification: The notification to process.

        """

        #TODO(sileht): this will be moved into oslo.messaging
        #see oslo.messaging bp notification-dispatcher-filter
        if not self._handle_event_type(notification['event_type'],
                                       self.event_types):
            return

        with self.pipeline_manager.publisher(context) as p:
            p(list(self.process_notification(notification)))


@six.add_metaclass(abc.ABCMeta)
class PollsterBase(PluginBase):
    """Base class for plugins that support the polling API."""

    @abc.abstractmethod
    def get_samples(self, manager, cache, resources=None):
        """Return a sequence of Counter instances from polling the resources.

        :param manager: The service manager class invoking the plugin.
        :param cache: A dictionary to allow pollsters to pass data
                      between themselves when recomputing it would be
                      expensive (e.g., asking another service for a
                      list of objects).
        :param resources: A list of the endpoints the pollster will get data
                          from. It's up to the specific pollster to decide
                          how to use it.

        """


@six.add_metaclass(abc.ABCMeta)
class DiscoveryBase(object):
    @abc.abstractmethod
    def discover(self, param=None):
        """Discover resources to monitor.
        :param param: an optional parameter to guide the discovery
        """

########NEW FILE########
__FILENAME__ = file
# -*- encoding: utf-8 -*-
#
# Copyright 2013 IBM Corp
#
# Author: Tong Li <litong01@us.ibm.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import logging
import logging.handlers
import six.moves.urllib.parse as urlparse

from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer import publisher

LOG = log.getLogger(__name__)


class FilePublisher(publisher.PublisherBase):
    """Publisher metering data to file.

    The publisher which records metering data into a file. The file name and
    location should be configured in ceilometer pipeline configuration file.
    If a file name and location is not specified, this File Publisher will not
    log any meters other than log a warning in Ceilometer log file.

    To enable this publisher, add the following section to file
    /etc/ceilometer/publisher.yaml or simply add it to an existing pipeline.

        -
            name: meter_file
            interval: 600
            counters:
                - "*"
            transformers:
            publishers:
                - file:///var/test?max_bytes=10000000&backup_count=5

    File path is required for this publisher to work properly. If max_bytes
    or backup_count is missing, FileHandler will be used to save the metering
    data. If max_bytes and backup_count are present, RotatingFileHandler will
    be used to save the metering data.
    """

    def __init__(self, parsed_url):
        super(FilePublisher, self).__init__(parsed_url)

        self.publisher_logger = None
        path = parsed_url.path
        if not path or path.lower() == 'file':
            LOG.error(_('The path for the file publisher is required'))
            return

        rfh = None
        max_bytes = 0
        backup_count = 0
        # Handling other configuration options in the query string
        if parsed_url.query:
            params = urlparse.parse_qs(parsed_url.query)
            if params.get('max_bytes') and params.get('backup_count'):
                try:
                    max_bytes = int(params.get('max_bytes')[0])
                    backup_count = int(params.get('backup_count')[0])
                except ValueError:
                    LOG.error(_('max_bytes and backup_count should be '
                              'numbers.'))
                    return
        # create rotating file handler
        rfh = logging.handlers.RotatingFileHandler(
            path, encoding='utf8', maxBytes=max_bytes,
            backupCount=backup_count)

        self.publisher_logger = logging.Logger('publisher.file')
        self.publisher_logger.propagate = False
        self.publisher_logger.setLevel(logging.INFO)
        rfh.setLevel(logging.INFO)
        self.publisher_logger.addHandler(rfh)

    def publish_samples(self, context, samples):
        """Send a metering message for publishing

        :param context: Execution context from the service or RPC call
        :param samples: Samples from pipeline after transformation
        """
        if self.publisher_logger:
            for sample in samples:
                self.publisher_logger.info(sample.as_dict())

########NEW FILE########
__FILENAME__ = rpc
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Publish a sample using the preferred RPC mechanism.
"""


import itertools
import operator
import six.moves.urllib.parse as urlparse

from oslo.config import cfg
import oslo.messaging
import oslo.messaging._drivers.common

from ceilometer import messaging
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer import publisher
from ceilometer.publisher import utils


LOG = log.getLogger(__name__)

METER_PUBLISH_OPTS = [
    cfg.StrOpt('metering_topic',
               default='metering',
               help='The topic that ceilometer uses for metering messages.',
               deprecated_group="DEFAULT",
               ),
]


def register_opts(config):
    """Register the options for publishing metering messages.
    """
    config.register_opts(METER_PUBLISH_OPTS, group="publisher_rpc")


register_opts(cfg.CONF)


def oslo_messaging_is_rabbit():
    kombu = ['ceilometer.openstack.common.rpc.impl_kombu',
             'oslo.messaging._drivers.impl_rabbit:RabbitDriver'
             'rabbit']
    return cfg.CONF.rpc_backend in kombu or (
        cfg.CONF.transport_url and
        cfg.CONF.transport_url.startswith('rabbit://'))


def override_backend_retry_config(value):
    """Override the retry config option native to the configured
       rpc backend (if such a native config option exists).

       :param value: the value to override
    """
    # TODO(sileht): ultimately we should add to olso a more generic concept
    # of retry config (i.e. not specific to an individual AMQP provider)
    # see: https://bugs.launchpad.net/ceilometer/+bug/1244698
    # and: https://bugs.launchpad.net/oslo.messaging/+bug/1282639
    if oslo_messaging_is_rabbit():
        if 'rabbit_max_retries' in cfg.CONF:
            cfg.CONF.set_override('rabbit_max_retries', value)


class RPCPublisher(publisher.PublisherBase):

    def __init__(self, parsed_url):
        options = urlparse.parse_qs(parsed_url.query)
        # the values of the option is a list of url params values
        # only take care of the latest one if the option
        # is provided more than once
        self.per_meter_topic = bool(int(
            options.get('per_meter_topic', [0])[-1]))

        self.target = options.get('target', ['record_metering_data'])[0]

        self.policy = options.get('policy', ['default'])[-1]
        self.max_queue_length = int(options.get(
            'max_queue_length', [1024])[-1])

        self.local_queue = []

        if self.policy in ['queue', 'drop']:
            LOG.info(_('Publishing policy set to %s, '
                       'override backend retry config to 1') % self.policy)
            override_backend_retry_config(1)
        elif self.policy == 'default':
            LOG.info(_('Publishing policy set to %s') % self.policy)
        else:
            LOG.warn(_('Publishing policy is unknown (%s) force to default')
                     % self.policy)
            self.policy = 'default'

        self.rpc_client = messaging.get_rpc_client(version='1.0')

    def publish_samples(self, context, samples):
        """Publish samples on RPC.

        :param context: Execution context from the service or RPC call.
        :param samples: Samples from pipeline after transformation.

        """

        meters = [
            utils.meter_message_from_counter(
                sample,
                cfg.CONF.publisher.metering_secret)
            for sample in samples
        ]

        topic = cfg.CONF.publisher_rpc.metering_topic
        LOG.audit(_('Publishing %(m)d samples on %(t)s') % (
            {'m': len(meters), 't': topic}))
        self.local_queue.append((context, topic, meters))

        if self.per_meter_topic:
            for meter_name, meter_list in itertools.groupby(
                    sorted(meters, key=operator.itemgetter('counter_name')),
                    operator.itemgetter('counter_name')):
                meter_list = list(meter_list)
                topic_name = topic + '.' + meter_name
                LOG.audit(_('Publishing %(m)d samples on %(n)s') % (
                          {'m': len(meter_list), 'n': topic_name}))
                self.local_queue.append((context, topic_name, meter_list))

        self.flush()

    def flush(self):
        # NOTE(sileht):
        # IO of the rpc stuff in handled by eventlet,
        # this is why the self.local_queue, is emptied before processing the
        # queue and the remaining messages in the queue are added to
        # self.local_queue after in case of a other call have already added
        # something in the self.local_queue
        queue = self.local_queue
        self.local_queue = []
        self.local_queue = self._process_queue(queue, self.policy) + \
            self.local_queue
        if self.policy == 'queue':
            self._check_queue_length()

    def _check_queue_length(self):
        queue_length = len(self.local_queue)
        if queue_length > self.max_queue_length > 0:
            count = queue_length - self.max_queue_length
            self.local_queue = self.local_queue[count:]
            LOG.warn(_("Publisher max local_queue length is exceeded, "
                     "dropping %d oldest samples") % count)

    def _process_queue(self, queue, policy):
        # NOTE(sileht):
        # the behavior of rpc.cast call depends of rabbit_max_retries
        # if rabbit_max_retries <= 0:
        #   it returns only if the msg has been sent on the amqp queue
        # if rabbit_max_retries > 0:
        #   it raises a exception if rabbitmq is unreachable
        #
        # the default policy just respect the rabbitmq configuration
        # nothing special is done if rabbit_max_retries <= 0
        # and exception is reraised if rabbit_max_retries > 0
        while queue:
            context, topic, meters = queue[0]
            try:
                self.rpc_client.prepare(topic=topic).cast(
                    context, self.target, data=meters)
            except oslo.messaging._drivers.common.RPCException:
                samples = sum([len(m) for __, __, m in queue])
                if policy == 'queue':
                    LOG.warn(_("Failed to publish %d samples, queue them"),
                             samples)
                    return queue
                elif policy == 'drop':
                    LOG.warn(_("Failed to publish %d samples, dropping them"),
                             samples)
                    return []
                # default, occur only if rabbit_max_retries > 0
                raise
            else:
                queue.pop(0)
        return []

########NEW FILE########
__FILENAME__ = test
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 eNovance
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Publish a sample in memory, useful for testing
"""

from ceilometer import publisher


class TestPublisher(publisher.PublisherBase):
    """Publisher used in unit testing."""

    def __init__(self, parsed_url):
        self.samples = []
        self.calls = 0

    def publish_samples(self, context, samples):
        """Send a metering message for publishing

        :param context: Execution context from the service or RPC call
        :param samples: Samples from pipeline after transformation
        """
        self.samples.extend(samples)
        self.calls += 1

########NEW FILE########
__FILENAME__ = udp
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 eNovance
#
# Author: Julien Danjou <julien@danjou.info>,
#         Tyaptin Ilya <ityaptin@mirantis.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Publish a sample using an UDP mechanism
"""

import socket

import msgpack
from oslo.config import cfg

from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer.openstack.common import network_utils
from ceilometer import publisher
from ceilometer.publisher import utils

cfg.CONF.import_opt('udp_port', 'ceilometer.collector',
                    group='collector')

LOG = log.getLogger(__name__)


class UDPPublisher(publisher.PublisherBase):
    def __init__(self, parsed_url):
        self.host, self.port = network_utils.parse_host_port(
            parsed_url.netloc,
            default_port=cfg.CONF.collector.udp_port)
        self.socket = socket.socket(socket.AF_INET,
                                    socket.SOCK_DGRAM)

    def publish_samples(self, context, samples):
        """Send a metering message for publishing

        :param context: Execution context from the service or RPC call
        :param samples: Samples from pipeline after transformation
        """

        for sample in samples:
            msg = utils.meter_message_from_counter(
                sample,
                cfg.CONF.publisher.metering_secret)
            host = self.host
            port = self.port
            LOG.debug(_("Publishing sample %(msg)s over UDP to "
                        "%(host)s:%(port)d") % {'msg': msg, 'host': host,
                                                'port': port})
            try:
                self.socket.sendto(msgpack.dumps(msg),
                                   (self.host, self.port))
            except Exception as e:
                LOG.warn(_("Unable to send sample over UDP"))
                LOG.exception(e)

########NEW FILE########
__FILENAME__ = utils
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#         Tyaptin Ilya <ityaptin@mirantis.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Utils for publishers
"""

import hashlib
import hmac

from oslo.config import cfg

from ceilometer import utils

METER_PUBLISH_OPTS = [
    cfg.StrOpt('metering_secret',
               secret=True,
               default='change this or be hacked',
               help='Secret value for signing metering messages.',
               deprecated_opts=[cfg.DeprecatedOpt("metering_secret",
                                                  "DEFAULT"),
                                cfg.DeprecatedOpt("metering_secret",
                                                  "publisher_rpc")]
               ),
]


def register_opts(config):
    """Register the options for publishing metering messages.
    """
    config.register_opts(METER_PUBLISH_OPTS, group="publisher")


register_opts(cfg.CONF)


def compute_signature(message, secret):
    """Return the signature for a message dictionary.
    """
    digest_maker = hmac.new(secret, '', hashlib.sha256)
    for name, value in utils.recursive_keypairs(message):
        if name == 'message_signature':
            # Skip any existing signature value, which would not have
            # been part of the original message.
            continue
        digest_maker.update(name)
        digest_maker.update(unicode(value).encode('utf-8'))
    return digest_maker.hexdigest()


def verify_signature(message, secret):
    """Check the signature in the message against the value computed
    from the rest of the contents.
    """
    old_sig = message.get('message_signature')
    new_sig = compute_signature(message, secret)
    return new_sig == old_sig


def meter_message_from_counter(sample, secret):
    """Make a metering message ready to be published or stored.

    Returns a dictionary containing a metering message
    for a notification message and a Sample instance.
    """
    msg = {'source': sample.source,
           'counter_name': sample.name,
           'counter_type': sample.type,
           'counter_unit': sample.unit,
           'counter_volume': sample.volume,
           'user_id': sample.user_id,
           'project_id': sample.project_id,
           'resource_id': sample.resource_id,
           'timestamp': sample.timestamp,
           'resource_metadata': sample.resource_metadata,
           'message_id': sample.id,
           }
    msg['message_signature'] = compute_signature(msg, secret)
    return msg

########NEW FILE########
__FILENAME__ = sample
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
# Copyright © 2013 eNovance
#
# Authors: Doug Hellmann <doug.hellmann@dreamhost.com>
#          Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Sample class for holding data about a metering event.

A Sample doesn't really do anything, but we need a way to
ensure that all of the appropriate fields have been filled
in by the plugins that create them.
"""

import copy
import uuid

from oslo.config import cfg


OPTS = [
    cfg.StrOpt('sample_source',
               default='openstack',
               deprecated_name='counter_source',
               help='Source for samples emitted on this instance.'),
]

cfg.CONF.register_opts(OPTS)


# Fields explanation:
#
# Source: the source of this sample
# Name: the name of the meter, must be unique
# Type: the type of the meter, must be either:
#       - cumulative: the value is incremented and never reset to 0
#       - delta: the value is reset to 0 each time it is sent
#       - gauge: the value is an absolute value and is not a counter
# Unit: the unit of the meter
# Volume: the sample value
# User ID: the user ID
# Project ID: the project ID
# Resource ID: the resource ID
# Timestamp: when the sample has been read
# Resource metadata: various metadata
class Sample(object):

    def __init__(self, name, type, unit, volume, user_id, project_id,
                 resource_id, timestamp, resource_metadata, source=None):
        self.name = name
        self.type = type
        self.unit = unit
        self.volume = volume
        self.user_id = user_id
        self.project_id = project_id
        self.resource_id = resource_id
        self.timestamp = timestamp
        self.resource_metadata = resource_metadata
        self.source = source or cfg.CONF.sample_source
        self.id = str(uuid.uuid1())

    def as_dict(self):
        return copy.copy(self.__dict__)

    @classmethod
    def from_notification(cls, name, type, volume, unit,
                          user_id, project_id, resource_id,
                          message, source=None):
        metadata = copy.copy(message['payload'])
        metadata['event_type'] = message['event_type']
        metadata['host'] = message['publisher_id']
        return cls(name=name,
                   type=type,
                   volume=volume,
                   unit=unit,
                   user_id=user_id,
                   project_id=project_id,
                   resource_id=resource_id,
                   timestamp=message['timestamp'],
                   resource_metadata=metadata,
                   source=source)

TYPE_GAUGE = 'gauge'
TYPE_DELTA = 'delta'
TYPE_CUMULATIVE = 'cumulative'

TYPES = (TYPE_GAUGE, TYPE_DELTA, TYPE_CUMULATIVE)

########NEW FILE########
__FILENAME__ = service
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
#
# Copyright © 2012-2014 eNovance <licensing@enovance.com>
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import os
import socket
import sys

from oslo.config import cfg

from ceilometer import messaging
from ceilometer.openstack.common import gettextutils
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer import utils


OPTS = [
    cfg.StrOpt('host',
               default=socket.gethostname(),
               help='Name of this node, which must be valid in an AMQP '
               'key. Can be an opaque identifier. For ZeroMQ only, must '
               'be a valid host name, FQDN, or IP address.'),
    cfg.IntOpt('collector_workers',
               default=1,
               help='Number of workers for collector service. A single '
               'collector is enabled by default.'),
    cfg.IntOpt('notification_workers',
               default=1,
               help='Number of workers for notification service. A single '
               'notification agent is enabled by default.'),
]
cfg.CONF.register_opts(OPTS)

CLI_OPTIONS = [
    cfg.StrOpt('os-username',
               deprecated_group="DEFAULT",
               default=os.environ.get('OS_USERNAME', 'ceilometer'),
               help='User name to use for OpenStack service access.'),
    cfg.StrOpt('os-password',
               deprecated_group="DEFAULT",
               secret=True,
               default=os.environ.get('OS_PASSWORD', 'admin'),
               help='Password to use for OpenStack service access.'),
    cfg.StrOpt('os-tenant-id',
               deprecated_group="DEFAULT",
               default=os.environ.get('OS_TENANT_ID', ''),
               help='Tenant ID to use for OpenStack service access.'),
    cfg.StrOpt('os-tenant-name',
               deprecated_group="DEFAULT",
               default=os.environ.get('OS_TENANT_NAME', 'admin'),
               help='Tenant name to use for OpenStack service access.'),
    cfg.StrOpt('os-cacert',
               default=os.environ.get('OS_CACERT'),
               help='Certificate chain for SSL validation.'),
    cfg.StrOpt('os-auth-url',
               deprecated_group="DEFAULT",
               default=os.environ.get('OS_AUTH_URL',
                                      'http://localhost:5000/v2.0'),
               help='Auth URL to use for OpenStack service access.'),
    cfg.StrOpt('os-region-name',
               deprecated_group="DEFAULT",
               default=os.environ.get('OS_REGION_NAME'),
               help='Region name to use for OpenStack service endpoints.'),
    cfg.StrOpt('os-endpoint-type',
               default=os.environ.get('OS_ENDPOINT_TYPE', 'publicURL'),
               help='Type of endpoint in Identity service catalog to use for '
                    'communication with OpenStack services.'),
    cfg.BoolOpt('insecure',
                default=False,
                help='Disables X.509 certificate validation when an '
                     'SSL connection to Identity Service is established.'),
]
cfg.CONF.register_cli_opts(CLI_OPTIONS, group="service_credentials")


LOG = log.getLogger(__name__)


class WorkerException(Exception):
    """Exception for errors relating to service workers
    """


def get_workers(name):
    workers = (cfg.CONF.get('%s_workers' % name) or
               utils.cpu_count())
    if workers and workers < 1:
        msg = (_("%(worker_name)s value of %(workers)s is invalid, "
                 "must be greater than 0") %
               {'worker_name': '%s_workers' % name, 'workers': str(workers)})
        raise WorkerException(msg)
    return workers


def prepare_service(argv=None):
    gettextutils.install('ceilometer', lazy=True)
    gettextutils.enable_lazy()
    cfg.set_defaults(log.log_opts,
                     default_log_levels=['amqplib=WARN',
                                         'qpid.messaging=INFO',
                                         'sqlalchemy=WARN',
                                         'keystoneclient=INFO',
                                         'stevedore=INFO',
                                         'eventlet.wsgi.server=WARN',
                                         'iso8601=WARN'
                                         ])
    if argv is None:
        argv = sys.argv
    cfg.CONF(argv[1:], project='ceilometer')
    log.setup('ceilometer')
    messaging.setup()

########NEW FILE########
__FILENAME__ = base
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Base classes for storage engines
"""

import datetime
import math

from six import moves

from ceilometer.openstack.common import timeutils


def iter_period(start, end, period):
    """Split a time from start to end in periods of a number of seconds. This
    function yield the (start, end) time for each period composing the time
    passed as argument.

    :param start: When the period set start.
    :param end: When the period end starts.
    :param period: The duration of the period.

    """
    period_start = start
    increment = datetime.timedelta(seconds=period)
    for i in moves.xrange(int(math.ceil(
            timeutils.delta_seconds(start, end)
            / float(period)))):
        next_start = period_start + increment
        yield (period_start, next_start)
        period_start = next_start


def _handle_sort_key(model_name, sort_key=None):
    """Generate sort keys according to the passed in sort key from user.

    :param model_name: Database model name be query.(alarm, meter, etc.)
    :param sort_key: sort key passed from user.
    return: sort keys list
    """
    sort_keys_extra = {'alarm': ['name', 'user_id', 'project_id'],
                       'meter': ['user_id', 'project_id'],
                       'resource': ['user_id', 'project_id', 'timestamp'],
                       }

    sort_keys = sort_keys_extra[model_name]
    if not sort_key:
        return sort_keys
    # NOTE(Fengqian): We need to put the sort key from user
    #in the first place of sort keys list.
    try:
        sort_keys.remove(sort_key)
    except ValueError:
        pass
    finally:
        sort_keys.insert(0, sort_key)
    return sort_keys


class MultipleResultsFound(Exception):
    pass


class NoResultFound(Exception):
    pass


class Pagination(object):
    """Class for pagination query."""

    def __init__(self, limit=None, primary_sort_dir='desc', sort_keys=None,
                 sort_dirs=None, marker_value=None):
        """This puts all parameters used for paginate query together.

        :param limit: Maximum number of items to return;
        :param primary_sort_dir: Sort direction of primary key.
        :param marker_value: Value of primary key to identify the last item of
                             the previous page.
        :param sort_keys: Array of attributes passed in by users to sort the
                            results besides the primary key.
        :param sort_dirs: Per-column array of sort_dirs, corresponding to
                            sort_keys.
        """
        self.limit = limit
        self.primary_sort_dir = primary_sort_dir
        self.marker_value = marker_value
        self.sort_keys = sort_keys or []
        self.sort_dirs = sort_dirs or []


class Connection(object):
    """Base class for storage system connections."""

    """A dictionary representing the capabilities of this driver.
    """
    CAPABILITIES = {
        'meters': {'pagination': False,
                   'query': {'simple': False,
                             'metadata': False,
                             'complex': False}},
        'resources': {'pagination': False,
                      'query': {'simple': False,
                                'metadata': False,
                                'complex': False}},
        'samples': {'pagination': False,
                    'groupby': False,
                    'query': {'simple': False,
                              'metadata': False,
                              'complex': False}},
        'statistics': {'pagination': False,
                       'groupby': False,
                       'query': {'simple': False,
                                 'metadata': False,
                                 'complex': False},
                       'aggregation': {'standard': False,
                                       'selectable': {
                                           'max': False,
                                           'min': False,
                                           'sum': False,
                                           'avg': False,
                                           'count': False,
                                           'stddev': False,
                                           'cardinality': False}}
                       },
        'alarms': {'query': {'simple': False,
                             'complex': False},
                   'history': {'query': {'simple': False,
                                         'complex': False}}},
        'events': {'query': {'simple': False}},
    }

    def __init__(self, url):
        """Constructor."""
        pass

    @staticmethod
    def upgrade():
        """Migrate the database to `version` or the most recent version."""

    @staticmethod
    def record_metering_data(data):
        """Write the data to the backend storage system.

        :param data: a dictionary such as returned by
                     ceilometer.meter.meter_message_from_counter

        All timestamps must be naive utc datetime object.
        """
        raise NotImplementedError('Projects not implemented')

    @staticmethod
    def clear_expired_metering_data(ttl):
        """Clear expired data from the backend storage system according to the
        time-to-live.

        :param ttl: Number of seconds to keep records for.

        """
        raise NotImplementedError('Clearing samples not implemented')

    @staticmethod
    def get_resources(user=None, project=None, source=None,
                      start_timestamp=None, start_timestamp_op=None,
                      end_timestamp=None, end_timestamp_op=None,
                      metaquery=None, resource=None, pagination=None):
        """Return an iterable of models.Resource instances containing
        resource information.

        :param user: Optional ID for user that owns the resource.
        :param project: Optional ID for project that owns the resource.
        :param source: Optional source filter.
        :param start_timestamp: Optional modified timestamp start range.
        :param start_timestamp_op: Optional timestamp start range operation.
        :param end_timestamp: Optional modified timestamp end range.
        :param end_timestamp_op: Optional timestamp end range operation.
        :param metaquery: Optional dict with metadata to match on.
        :param resource: Optional resource filter.
        :param pagination: Optional pagination query.
        """
        raise NotImplementedError('Resources not implemented')

    @staticmethod
    def get_meters(user=None, project=None, resource=None, source=None,
                   metaquery=None, pagination=None):
        """Return an iterable of model.Meter instances containing meter
        information.

        :param user: Optional ID for user that owns the resource.
        :param project: Optional ID for project that owns the resource.
        :param resource: Optional resource filter.
        :param source: Optional source filter.
        :param metaquery: Optional dict with metadata to match on.
        :param pagination: Optional pagination query.
        """
        raise NotImplementedError('Meters not implemented')

    @staticmethod
    def get_samples(sample_filter, limit=None):
        """Return an iterable of model.Sample instances.

        :param sample_filter: Filter.
        :param limit: Maximum number of results to return.
        """
        raise NotImplementedError('Samples not implemented')

    @staticmethod
    def get_meter_statistics(sample_filter, period=None, groupby=None,
                             aggregate=None):
        """Return an iterable of model.Statistics instances.

        The filter must have a meter value set.
        """
        raise NotImplementedError('Statistics not implemented')

    @staticmethod
    def get_alarms(name=None, user=None,
                   project=None, enabled=None, alarm_id=None, pagination=None):
        """Yields a lists of alarms that match filters."""
        raise NotImplementedError('Alarms not implemented')

    @staticmethod
    def create_alarm(alarm):
        """Create an alarm. Returns the alarm as created.

        :param alarm: The alarm to create.
        """
        raise NotImplementedError('Alarms not implemented')

    @staticmethod
    def update_alarm(alarm):
        """Update alarm."""
        raise NotImplementedError('Alarms not implemented')

    @staticmethod
    def delete_alarm(alarm_id):
        """Delete an alarm."""
        raise NotImplementedError('Alarms not implemented')

    @staticmethod
    def get_alarm_changes(alarm_id, on_behalf_of,
                          user=None, project=None, type=None,
                          start_timestamp=None, start_timestamp_op=None,
                          end_timestamp=None, end_timestamp_op=None):
        """Yields list of AlarmChanges describing alarm history

        Changes are always sorted in reverse order of occurrence, given
        the importance of currency.

        Segregation for non-administrative users is done on the basis
        of the on_behalf_of parameter. This allows such users to have
        visibility on both the changes initiated by themselves directly
        (generally creation, rule changes, or deletion) and also on those
        changes initiated on their behalf by the alarming service (state
        transitions after alarm thresholds are crossed).

        :param alarm_id: ID of alarm to return changes for
        :param on_behalf_of: ID of tenant to scope changes query (None for
                             administrative user, indicating all projects)
        :param user: Optional ID of user to return changes for
        :param project: Optional ID of project to return changes for
        :project type: Optional change type
        :param start_timestamp: Optional modified timestamp start range
        :param start_timestamp_op: Optional timestamp start range operation
        :param end_timestamp: Optional modified timestamp end range
        :param end_timestamp_op: Optional timestamp end range operation
        """
        raise NotImplementedError('Alarm history not implemented')

    @staticmethod
    def record_alarm_change(alarm_change):
        """Record alarm change event."""
        raise NotImplementedError('Alarm history not implemented')

    @staticmethod
    def clear():
        """Clear database."""

    @staticmethod
    def record_events(events):
        """Write the events to the backend storage system.

        :param events: a list of model.Event objects.
        """
        raise NotImplementedError('Events not implemented.')

    @staticmethod
    def get_events(event_filter):
        """Return an iterable of model.Event objects.
        """
        raise NotImplementedError('Events not implemented.')

    @staticmethod
    def get_event_types():
        """Return all event types as an iterable of strings.
        """
        raise NotImplementedError('Events not implemented.')

    @staticmethod
    def get_trait_types(event_type):
        """Return a dictionary containing the name and data type of
        the trait type. Only trait types for the provided event_type are
        returned.

        :param event_type: the type of the Event
        """
        raise NotImplementedError('Events not implemented.')

    @staticmethod
    def get_traits(event_type, trait_type=None):
        """Return all trait instances associated with an event_type. If
        trait_type is specified, only return instances of that trait type.

        :param event_type: the type of the Event to filter by
        :param trait_type: the name of the Trait to filter by
        """

        raise NotImplementedError('Events not implemented.')

    @staticmethod
    def query_samples(filter_expr=None, orderby=None, limit=None):
        """Return an iterable of model.Sample objects.

        :param filter_expr: Filter expression for query.
        :param orderby: List of field name and direction pairs for order by.
        :param limit: Maximum number of results to return.
        """

        raise NotImplementedError('Complex query for samples '
                                  'is not implemented.')

    @staticmethod
    def query_alarms(filter_expr=None, orderby=None, limit=None):
        """Return an iterable of model.Alarm objects.

        :param filter_expr: Filter expression for query.
        :param orderby: List of field name and direction pairs for order by.
        :param limit: Maximum number of results to return.
        """

        raise NotImplementedError('Complex query for alarms '
                                  'is not implemented.')

    @staticmethod
    def query_alarm_history(filter_expr=None, orderby=None, limit=None):
        """Return an iterable of model.AlarmChange objects.

        :param filter_expr: Filter expression for query.
        :param orderby: List of field name and direction pairs for order by.
        :param limit: Maximum number of results to return.
        """

        raise NotImplementedError('Complex query for alarms '
                                  'history is not implemented.')

    @classmethod
    def get_capabilities(cls):
        """Return an dictionary representing the capabilities of each driver.
        """
        return cls.CAPABILITIES

########NEW FILE########
__FILENAME__ = impl_db2
# -*- encoding: utf-8 -*-
# Copyright © 2012 New Dream Network, LLC (DreamHost)
# Copyright © 2013 eNovance
# Copyright © 2013 IBM Corp
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#         Julien Danjou <julien@danjou.info>
#         Tong Li <litong01@us.ibm.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""DB2 storage backend
"""

from __future__ import division
import copy
import datetime
import itertools
import sys

import bson.code
import bson.objectid
import pymongo

from ceilometer.openstack.common import log
from ceilometer.openstack.common import timeutils
from ceilometer import storage
from ceilometer.storage import base
from ceilometer.storage import models
from ceilometer.storage import pymongo_base
from ceilometer import utils

LOG = log.getLogger(__name__)


AVAILABLE_CAPABILITIES = {
    'resources': {'query': {'simple': True,
                            'metadata': True}},
    'statistics': {'groupby': True,
                   'query': {'simple': True,
                             'metadata': True},
                   'aggregation': {'standard': True}}
}


class Connection(pymongo_base.Connection):
    """The db2 storage for Ceilometer

    Collections::

        - meter
          - the raw incoming data
        - resource
          - the metadata for resources
          - { _id: uuid of resource,
              metadata: metadata dictionaries
              user_id: uuid
              project_id: uuid
              meter: [ array of {counter_name: string, counter_type: string,
                                 counter_unit: string} ]
            }
    """

    CAPABILITIES = utils.update_nested(pymongo_base.Connection.CAPABILITIES,
                                       AVAILABLE_CAPABILITIES)
    CONNECTION_POOL = pymongo_base.ConnectionPool()

    GROUP = {'_id': '$counter_name',
             'unit': {'$min': '$counter_unit'},
             'min': {'$min': '$counter_volume'},
             'max': {'$max': '$counter_volume'},
             'sum': {'$sum': '$counter_volume'},
             'count': {'$sum': 1},
             'duration_start': {'$min': '$timestamp'},
             'duration_end': {'$max': '$timestamp'},
             }

    PROJECT = {'_id': 0, 'unit': 1,
               'min': 1, 'max': 1, 'sum': 1, 'count': 1,
               'avg': {'$divide': ['$sum', '$count']},
               'duration_start': 1,
               'duration_end': 1,
               }

    SORT_OPERATION_MAP = {'desc': pymongo.DESCENDING, 'asc': pymongo.ASCENDING}

    SECONDS_IN_A_DAY = 86400

    def __init__(self, url):

        # Since we are using pymongo, even though we are connecting to DB2
        # we still have to make sure that the scheme which used to distinguish
        # db2 driver from mongodb driver be replaced so that pymongo will not
        # produce an exception on the scheme.
        url = url.replace('db2:', 'mongodb:', 1)
        self.conn = self.CONNECTION_POOL.connect(url)

        # Require MongoDB 2.2 to use aggregate(), since we are using mongodb
        # as backend for test, the following code is necessary to make sure
        # that the test wont try aggregate on older mongodb during the test.
        # For db2, the versionArray won't be part of the server_info, so there
        # will not be exception when real db2 gets used as backend.
        server_info = self.conn.server_info()
        if server_info.get('sysInfo'):
            self._using_mongodb = True
        else:
            self._using_mongodb = False

        if self._using_mongodb and server_info.get('versionArray') < [2, 2]:
            raise storage.StorageBadVersion("Need at least MongoDB 2.2")

        connection_options = pymongo.uri_parser.parse_uri(url)
        self.db = getattr(self.conn, connection_options['database'])
        if connection_options.get('username'):
            self.db.authenticate(connection_options['username'],
                                 connection_options['password'])

        self.upgrade()

    @classmethod
    def _build_sort_instructions(cls, sort_keys=None, sort_dir='desc'):
        """Returns a sort_instruction.

        Sort instructions are used in the query to determine what attributes
        to sort on and what direction to use.
        :param q: The query dict passed in.
        :param sort_keys: array of attributes by which results be sorted.
        :param sort_dir: direction in which results be sorted (asc, desc).
        :return: sort parameters
        """
        sort_keys = sort_keys or []
        sort_instructions = []
        _sort_dir = cls.SORT_OPERATION_MAP.get(
            sort_dir, cls.SORT_OPERATION_MAP['desc'])

        for _sort_key in sort_keys:
            _instruction = (_sort_key, _sort_dir)
            sort_instructions.append(_instruction)

        return sort_instructions

    def upgrade(self, version=None):
        # Establish indexes
        #
        # We need variations for user_id vs. project_id because of the
        # way the indexes are stored in b-trees. The user_id and
        # project_id values are usually mutually exclusive in the
        # queries, so the database won't take advantage of an index
        # including both.
        if self.db.resource.index_information() == {}:
            resource_id = str(bson.objectid.ObjectId())
            self.db.resource.insert({'_id': resource_id,
                                     'no_key': resource_id})
            meter_id = str(bson.objectid.ObjectId())
            self.db.meter.insert({'_id': meter_id,
                                  'no_key': meter_id})

            self.db.resource.ensure_index([
                ('user_id', pymongo.ASCENDING),
                ('project_id', pymongo.ASCENDING),
                ('source', pymongo.ASCENDING)], name='resource_idx')

            self.db.meter.ensure_index([
                ('resource_id', pymongo.ASCENDING),
                ('user_id', pymongo.ASCENDING),
                ('project_id', pymongo.ASCENDING),
                ('counter_name', pymongo.ASCENDING),
                ('timestamp', pymongo.ASCENDING),
                ('source', pymongo.ASCENDING)], name='meter_idx')

            self.db.meter.ensure_index([('timestamp',
                                         pymongo.DESCENDING)],
                                       name='timestamp_idx')

            self.db.resource.remove({'_id': resource_id})
            self.db.meter.remove({'_id': meter_id})

        # remove API v1 related table
        self.db.user.drop()
        self.db.project.drop()

    def clear(self):
        # db2 does not support drop_database, remove all collections
        for col in ['resource', 'meter']:
            self.db[col].drop()
        # drop_database command does nothing on db2 database since this has
        # not been implemented. However calling this method is important for
        # removal of all the empty dbs created during the test runs since
        # test run is against mongodb on Jenkins
        self.conn.drop_database(self.db)
        self.conn.close()

    def record_metering_data(self, data):
        """Write the data to the backend storage system.

        :param data: a dictionary such as returned by
                     ceilometer.meter.meter_message_from_counter
        """
        # Record the updated resource metadata
        self.db.resource.update(
            {'_id': data['resource_id']},
            {'$set': {'project_id': data['project_id'],
                      'user_id': data['user_id'] or 'null',
                      'metadata': data['resource_metadata'],
                      'source': data['source'],
                      },
             '$addToSet': {'meter': {'counter_name': data['counter_name'],
                                     'counter_type': data['counter_type'],
                                     'counter_unit': data['counter_unit'],
                                     },
                           },
             },
            upsert=True,
        )

        # Record the raw data for the meter. Use a copy so we do not
        # modify a data structure owned by our caller (the driver adds
        # a new key '_id').
        record = copy.copy(data)
        record['recorded_at'] = timeutils.utcnow()
        # Make sure that the data does have field _id which db2 wont add
        # automatically.
        if record.get('_id') is None:
            record['_id'] = str(bson.objectid.ObjectId())
        self.db.meter.insert(record)

    def get_resources(self, user=None, project=None, source=None,
                      start_timestamp=None, start_timestamp_op=None,
                      end_timestamp=None, end_timestamp_op=None,
                      metaquery=None, resource=None, pagination=None):
        """Return an iterable of models.Resource instances

        :param user: Optional ID for user that owns the resource.
        :param project: Optional ID for project that owns the resource.
        :param source: Optional source filter.
        :param start_timestamp: Optional modified timestamp start range.
        :param start_timestamp_op: Optional start time operator, like gt, ge.
        :param end_timestamp: Optional modified timestamp end range.
        :param end_timestamp_op: Optional end time operator, like lt, le.
        :param metaquery: Optional dict with metadata to match on.
        :param resource: Optional resource filter.
        :param pagination: Optional pagination query.
        """
        if pagination:
            raise NotImplementedError('Pagination not implemented')

        metaquery = metaquery or {}

        q = {}
        if user is not None:
            q['user_id'] = user
        if project is not None:
            q['project_id'] = project
        if source is not None:
            q['source'] = source
        if resource is not None:
            q['resource_id'] = resource
        # Add resource_ prefix so it matches the field in the db
        q.update(dict(('resource_' + k, v)
                      for (k, v) in metaquery.iteritems()))

        if start_timestamp or end_timestamp:
            # Look for resources matching the above criteria and with
            # samples in the time range we care about, then change the
            # resource query to return just those resources by id.
            ts_range = pymongo_base.make_timestamp_range(start_timestamp,
                                                         end_timestamp,
                                                         start_timestamp_op,
                                                         end_timestamp_op)
            if ts_range:
                q['timestamp'] = ts_range

        sort_keys = base._handle_sort_key('resource', 'timestamp')
        sort_keys.insert(0, 'resource_id')
        sort_instructions = self._build_sort_instructions(sort_keys=sort_keys,
                                                          sort_dir='desc')
        resource = lambda x: x['resource_id']
        meters = self.db.meter.find(q, sort=sort_instructions)
        for resource_id, r_meters in itertools.groupby(meters, key=resource):
            # Because we have to know first/last timestamp, and we need a full
            # list of references to the resource's meters, we need a tuple
            # here.
            r_meters = tuple(r_meters)
            latest_meter = r_meters[0]
            last_ts = latest_meter['timestamp']
            first_ts = r_meters[-1]['timestamp']

            yield models.Resource(resource_id=latest_meter['resource_id'],
                                  project_id=latest_meter['project_id'],
                                  first_sample_timestamp=first_ts,
                                  last_sample_timestamp=last_ts,
                                  source=latest_meter['source'],
                                  user_id=latest_meter['user_id'],
                                  metadata=latest_meter['resource_metadata'])

    def get_meter_statistics(self, sample_filter, period=None, groupby=None,
                             aggregate=None):
        """Return an iterable of models.Statistics instance containing meter
        statistics described by the query parameters.

        The filter must have a meter value set.
        """
        if (groupby and
                set(groupby) - set(['user_id', 'project_id',
                                    'resource_id', 'source'])):
            raise NotImplementedError("Unable to group by these fields")

        if aggregate:
            raise NotImplementedError('Selectable aggregates not implemented')

        q = pymongo_base.make_query_from_filter(sample_filter)

        if period:
            if sample_filter.start:
                period_start = sample_filter.start
            else:
                period_start = self.db.meter.find(
                    limit=1, sort=[('timestamp',
                                    pymongo.ASCENDING)])[0]['timestamp']

        if groupby:
            sort_keys = ['counter_name'] + groupby + ['timestamp']
        else:
            sort_keys = ['counter_name', 'timestamp']

        sort_instructions = self._build_sort_instructions(sort_keys=sort_keys,
                                                          sort_dir='asc')
        meters = self.db.meter.find(q, sort=sort_instructions)

        def _group_key(meter):
            # the method to define a key for groupby call
            key = {}
            for y in sort_keys:
                if y == 'timestamp' and period:
                    key[y] = (timeutils.delta_seconds(period_start,
                                                      meter[y]) // period)
                elif y != 'timestamp':
                    key[y] = meter[y]
            return key

        def _to_offset(periods):
            return {'days': (periods * period) // self.SECONDS_IN_A_DAY,
                    'seconds': (periods * period) % self.SECONDS_IN_A_DAY}

        for key, grouped_meters in itertools.groupby(meters, key=_group_key):
            stat = models.Statistics(unit=None,
                                     min=sys.maxint, max=-sys.maxint,
                                     avg=0, sum=0, count=0,
                                     period=0, period_start=0, period_end=0,
                                     duration=0, duration_start=0,
                                     duration_end=0, groupby=None)

            for meter in grouped_meters:
                stat.unit = meter.get('counter_unit', '')
                m_volume = meter.get('counter_volume')
                if stat.min > m_volume:
                    stat.min = m_volume
                if stat.max < m_volume:
                    stat.max = m_volume
                stat.sum += m_volume
                stat.count += 1
                if stat.duration_start == 0:
                    stat.duration_start = meter['timestamp']
                stat.duration_end = meter['timestamp']
                if groupby and not stat.groupby:
                    stat.groupby = {}
                    for group_key in groupby:
                        stat.groupby[group_key] = meter[group_key]

            stat.duration = timeutils.delta_seconds(stat.duration_start,
                                                    stat.duration_end)
            stat.avg = stat.sum / stat.count
            if period:
                stat.period = period
                periods = key.get('timestamp')
                stat.period_start = period_start + \
                    datetime.timedelta(**(_to_offset(periods)))
                stat.period_end = period_start + \
                    datetime.timedelta(**(_to_offset(periods + 1)))
            else:
                stat.period_start = stat.duration_start
                stat.period_end = stat.duration_end
            yield stat

########NEW FILE########
__FILENAME__ = impl_hbase
# -*- encoding: utf-8 -*-
#
# Copyright © 2012, 2013 Dell Inc.
#
# Author: Stas Maksimov <Stanislav_M@dell.com>
# Author: Shengjie Min <Shengjie_Min@dell.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""HBase storage backend
"""
import copy
import datetime
import hashlib
import itertools
import json
import os
import re
import six.moves.urllib.parse as urlparse

import bson.json_util
import happybase

from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer.openstack.common import network_utils
from ceilometer.openstack.common import timeutils
from ceilometer import storage
from ceilometer.storage import base
from ceilometer.storage import models
from ceilometer import utils

LOG = log.getLogger(__name__)


AVAILABLE_CAPABILITIES = {
    'meters': {'query': {'simple': True,
                         'metadata': True}},
    'resources': {'query': {'simple': True,
                            'metadata': True}},
    'samples': {'query': {'simple': True,
                          'metadata': True}},
    'statistics': {'query': {'simple': True,
                             'metadata': True},
                   'aggregation': {'standard': True}},
}


class Connection(base.Connection):
    """Put the data into a HBase database

    Collections:

    - meter (describes sample actually)
      - row-key: consists of reversed timestamp, meter and an md5 of
                 user+resource+project for purposes of uniqueness
      - Column Families:
          f: contains the following qualifiers:
               -counter_name : <name of counter>
               -counter_type : <type of counter>
               -counter_unit : <unit of counter>
               -counter_volume : <volume of counter>
               -message: <raw incoming data>
               -message_id: <id of message>
               -message_signature: <signature of message>
               -resource_metadata: raw metadata for corresponding resource
                of the meter
               -project_id: <id of project>
               -resource_id: <id of resource>
               -user_id: <id of user>
               -recorded_at: <datetime when sample has been recorded (utc.now)>
               -flattened metadata with prefix r_metadata. e.g.
                f:r_metadata.display_name or f:r_metadata.tag
               -rts: <reversed timestamp of entry>
               -timestamp: <meter's timestamp (came from message)>
               -source for meter with prefix 's'

    - resource
      - row_key: uuid of resource
      - Column Families:
          f: contains the following qualifiers:
               -resource_metadata: raw metadata for corresponding resource
               -project_id: <id of project>
               -resource_id: <id of resource>
               -user_id: <id of user>
               -flattened metadata with prefix r_metadata. e.g.
                f:r_metadata.display_name or f:r_metadata.tag
               -sources for all corresponding meters with prefix 's'
               -all meters for this resource in format
                "%s!%s!%s+%s" % (counter_name, counter_type, counter_unit,
                source)

    - alarm
      - row_key: uuid of alarm
      - Column Families:
          f: contains the raw incoming alarm data

    - alarm_h
      - row_key: uuid of alarm + "_" + reversed timestamp
      - Column Families:
          f: raw incoming alarm_history data. Timestamp becomes now()
             if not determined
    """

    CAPABILITIES = utils.update_nested(base.Connection.CAPABILITIES,
                                       AVAILABLE_CAPABILITIES)
    _memory_instance = None

    RESOURCE_TABLE = "resource"
    METER_TABLE = "meter"
    ALARM_TABLE = "alarm"
    ALARM_HISTORY_TABLE = "alarm_h"

    def __init__(self, url):
        """Hbase Connection Initialization."""
        opts = self._parse_connection_url(url)

        if opts['host'] == '__test__':
            url = os.environ.get('CEILOMETER_TEST_HBASE_URL')
            if url:
                # Reparse URL, but from the env variable now
                opts = self._parse_connection_url(url)
                self.conn_pool = self._get_connection_pool(opts)
            else:
                # This is a in-memory usage for unit tests
                if Connection._memory_instance is None:
                    LOG.debug(_('Creating a new in-memory HBase '
                              'Connection object'))
                    Connection._memory_instance = MConnectionPool()
                self.conn_pool = Connection._memory_instance
        else:
            self.conn_pool = self._get_connection_pool(opts)

    def upgrade(self):
        with self.conn_pool.connection() as conn:
            conn.create_table(self.RESOURCE_TABLE, {'f': dict(max_versions=1)})
            conn.create_table(self.METER_TABLE, {'f': dict(max_versions=1)})
            conn.create_table(self.ALARM_TABLE, {'f': dict()})
            conn.create_table(self.ALARM_HISTORY_TABLE, {'f': dict()})

    def clear(self):
        LOG.debug(_('Dropping HBase schema...'))
        with self.conn_pool.connection() as conn:
            for table in [self.RESOURCE_TABLE,
                          self.METER_TABLE,
                          self.ALARM_TABLE,
                          self.ALARM_HISTORY_TABLE]:
                try:
                    conn.disable_table(table)
                except Exception:
                    LOG.debug(_('Cannot disable table but ignoring error'))
                try:
                    conn.delete_table(table)
                except Exception:
                    LOG.debug(_('Cannot delete table but ignoring error'))

    @staticmethod
    def _get_connection_pool(conf):
        """Return a connection pool to the database.

        .. note::

          The tests use a subclass to override this and return an
          in-memory connection pool.
        """
        LOG.debug(_('connecting to HBase on %(host)s:%(port)s') % (
                  {'host': conf['host'], 'port': conf['port']}))
        return happybase.ConnectionPool(size=100, host=conf['host'],
                                        port=conf['port'],
                                        table_prefix=conf['table_prefix'])

    @staticmethod
    def _parse_connection_url(url):
        """Parse connection parameters from a database url.

        .. note::

        HBase Thrift does not support authentication and there is no
        database name, so we are not looking for these in the url.
        """
        opts = {}
        result = network_utils.urlsplit(url)
        opts['table_prefix'] = urlparse.parse_qs(
            result.query).get('table_prefix', [None])[0]
        opts['dbtype'] = result.scheme
        if ':' in result.netloc:
            opts['host'], port = result.netloc.split(':')
        else:
            opts['host'] = result.netloc
            port = 9090
        opts['port'] = port and int(port) or 9090
        return opts

    def update_alarm(self, alarm):
        """Create an alarm.
        :param alarm: The alarm to create. It is Alarm object, so we need to
        call as_dict()
        """
        _id = alarm.alarm_id
        alarm_to_store = serialize_entry(alarm.as_dict())
        with self.conn_pool.connection() as conn:
            alarm_table = conn.table(self.ALARM_TABLE)
            alarm_table.put(_id, alarm_to_store)
            stored_alarm = deserialize_entry(alarm_table.row(_id))[0]
        return models.Alarm(**stored_alarm)

    create_alarm = update_alarm

    def delete_alarm(self, alarm_id):
        with self.conn_pool.connection() as conn:
            alarm_table = conn.table(self.ALARM_TABLE)
            alarm_table.delete(alarm_id)

    def get_alarms(self, name=None, user=None,
                   project=None, enabled=None, alarm_id=None, pagination=None):

        if pagination:
            raise NotImplementedError('Pagination not implemented')

        q = make_query(alarm_id=alarm_id, name=name, enabled=enabled,
                       user_id=user, project_id=project)

        with self.conn_pool.connection() as conn:
            alarm_table = conn.table(self.ALARM_TABLE)
            gen = alarm_table.scan(filter=q)
            for ignored, data in gen:
                stored_alarm = deserialize_entry(data)[0]
                yield models.Alarm(**stored_alarm)

    def get_alarm_changes(self, alarm_id, on_behalf_of,
                          user=None, project=None, type=None,
                          start_timestamp=None, start_timestamp_op=None,
                          end_timestamp=None, end_timestamp_op=None):
        q = make_query(alarm_id=alarm_id, on_behalf_of=on_behalf_of, type=type,
                       user_id=user, project_id=project)
        start_row, end_row = make_timestamp_query(
            _make_general_rowkey_scan,
            start=start_timestamp, start_op=start_timestamp_op,
            end=end_timestamp, end_op=end_timestamp_op, bounds_only=True,
            some_id=alarm_id)
        with self.conn_pool.connection() as conn:
            alarm_history_table = conn.table(self.ALARM_HISTORY_TABLE)
            gen = alarm_history_table.scan(filter=q, row_start=start_row,
                                           row_stop=end_row)
            for ignored, data in gen:
                stored_entry = deserialize_entry(data)[0]
                yield models.AlarmChange(**stored_entry)

    def record_alarm_change(self, alarm_change):
        """Record alarm change event.
        """
        alarm_change_dict = serialize_entry(alarm_change)
        ts = alarm_change.get('timestamp') or datetime.datetime.now()
        rts = reverse_timestamp(ts)
        with self.conn_pool.connection() as conn:
            alarm_history_table = conn.table(self.ALARM_HISTORY_TABLE)
            alarm_history_table.put(alarm_change.get('alarm_id') + "_" +
                                    str(rts), alarm_change_dict)

    def record_metering_data(self, data):
        """Write the data to the backend storage system.

        :param data: a dictionary such as returned by
                     ceilometer.meter.meter_message_from_counter
        """
        with self.conn_pool.connection() as conn:
            resource_table = conn.table(self.RESOURCE_TABLE)
            meter_table = conn.table(self.METER_TABLE)

            resource_metadata = data.get('resource_metadata', {})
            # Determine the name of new meter
            new_meter = _format_meter_reference(
                data['counter_name'], data['counter_type'],
                data['counter_unit'], data['source'])
            #TODO(nprivalova): try not to store resource_id
            resource = serialize_entry(**{
                'source': data['source'], 'meter': new_meter,
                'resource_metadata': resource_metadata,
                'resource_id': data['resource_id'],
                'project_id': data['project_id'], 'user_id': data['user_id']})
            resource_table.put(data['resource_id'], resource)

            #TODO(nprivalova): improve uniqueness
            # Rowkey consists of reversed timestamp, meter and an md5 of
            # user+resource+project for purposes of uniqueness
            m = hashlib.md5()
            m.update("%s%s%s" % (data['user_id'], data['resource_id'],
                                 data['project_id']))

            # We use reverse timestamps in rowkeys as they are sorted
            # alphabetically.
            rts = reverse_timestamp(data['timestamp'])
            row = "%s_%d_%s" % (data['counter_name'], rts, m.hexdigest())
            record = serialize_entry(data, **{'source': data['source'],
                                              'rts': rts,
                                              'message': data,
                                              'recorded_at': timeutils.utcnow(
                                              )})
            meter_table.put(row, record)

    def get_resources(self, user=None, project=None, source=None,
                      start_timestamp=None, start_timestamp_op=None,
                      end_timestamp=None, end_timestamp_op=None,
                      metaquery=None, resource=None, pagination=None):
        """Return an iterable of models.Resource instances

        :param user: Optional ID for user that owns the resource.
        :param project: Optional ID for project that owns the resource.
        :param source: Optional source filter.
        :param start_timestamp: Optional modified timestamp start range.
        :param start_timestamp_op: Optional start time operator, like ge, gt.
        :param end_timestamp: Optional modified timestamp end range.
        :param end_timestamp_op: Optional end time operator, like lt, le.
        :param metaquery: Optional dict with metadata to match on.
        :param resource: Optional resource filter.
        :param pagination: Optional pagination query.
        """
        if pagination:
            raise NotImplementedError('Pagination not implemented')

        metaquery = metaquery or {}

        sample_filter = storage.SampleFilter(
            user=user, project=project,
            start=start_timestamp, start_timestamp_op=start_timestamp_op,
            end=end_timestamp, end_timestamp_op=end_timestamp_op,
            resource=resource, source=source, metaquery=metaquery)
        q, start_row, stop_row = make_sample_query_from_filter(
            sample_filter, require_meter=False)
        with self.conn_pool.connection() as conn:
            meter_table = conn.table(self.METER_TABLE)
            LOG.debug(_("Query Meter table: %s") % q)
            meters = meter_table.scan(filter=q, row_start=start_row,
                                      row_stop=stop_row)
            d_meters = []
            for i, m in meters:
                d_meters.append(deserialize_entry(m))

            # We have to sort on resource_id before we can group by it.
            # According to the itertools documentation a new group is
            # generated when the value of the key function changes
            # (it breaks there).
            meters = sorted(d_meters, key=_resource_id_from_record_tuple)
            for resource_id, r_meters in itertools.groupby(
                    meters, key=_resource_id_from_record_tuple):
                # We need deserialized entry(data[0]), sources (data[1]) and
                # metadata(data[3])
                meter_rows = [(data[0], data[1], data[3]) for data in sorted(
                    r_meters, key=_timestamp_from_record_tuple)]
                latest_data = meter_rows[-1]
                min_ts = meter_rows[0][0]['timestamp']
                max_ts = latest_data[0]['timestamp']
                yield models.Resource(
                    resource_id=resource_id,
                    first_sample_timestamp=min_ts,
                    last_sample_timestamp=max_ts,
                    project_id=latest_data[0]['project_id'],
                    source=latest_data[1][0],
                    user_id=latest_data[0]['user_id'],
                    metadata=latest_data[2],
                )

    def get_meters(self, user=None, project=None, resource=None, source=None,
                   metaquery=None, pagination=None):
        """Return an iterable of models.Meter instances

        :param user: Optional ID for user that owns the resource.
        :param project: Optional ID for project that owns the resource.
        :param resource: Optional resource filter.
        :param source: Optional source filter.
        :param metaquery: Optional dict with metadata to match on.
        :param pagination: Optional pagination query.
        """

        metaquery = metaquery or {}

        if pagination:
            raise NotImplementedError(_('Pagination not implemented'))
        with self.conn_pool.connection() as conn:
            resource_table = conn.table(self.RESOURCE_TABLE)
            q = make_query(metaquery=metaquery, user_id=user,
                           project_id=project, resource_id=resource,
                           source=source)
            LOG.debug(_("Query Resource table: %s") % q)

            gen = resource_table.scan(filter=q)
            # We need result set to be sure that user doesn't receive several
            # same meters. Please see bug
            # https://bugs.launchpad.net/ceilometer/+bug/1301371
            result = set()
            for ignored, data in gen:
                flatten_result, s, meters, md = deserialize_entry(data)
                for m in meters:
                    meter_raw, m_source = m.split("+")
                    name, type, unit = meter_raw.split('!')
                    meter_dict = {'name': name,
                                  'type': type,
                                  'unit': unit,
                                  'resource_id': flatten_result['resource_id'],
                                  'project_id': flatten_result['project_id'],
                                  'user_id': flatten_result['user_id']}
                    frozen_meter = frozenset(meter_dict.items())
                    if frozen_meter in result:
                        continue
                    result.add(frozen_meter)
                    meter_dict.update({'source':
                                       m_source if m_source else None})

                    yield models.Meter(**meter_dict)

    def get_samples(self, sample_filter, limit=None):
        """Return an iterable of models.Sample instances.

        :param sample_filter: Filter.
        :param limit: Maximum number of results to return.
        """
        with self.conn_pool.connection() as conn:
            meter_table = conn.table(self.METER_TABLE)

            q, start, stop = make_sample_query_from_filter(
                sample_filter, require_meter=False)
            LOG.debug(_("Query Meter Table: %s") % q)
            gen = meter_table.scan(filter=q, row_start=start, row_stop=stop)
            for ignored, meter in gen:
                if limit is not None:
                    if limit == 0:
                        break
                    else:
                        limit -= 1
                d_meter = deserialize_entry(meter)[0]
                d_meter['message']['recorded_at'] = d_meter['recorded_at']
                yield models.Sample(**d_meter['message'])

    @staticmethod
    def _update_meter_stats(stat, meter):
        """Do the stats calculation on a requested time bucket in stats dict

        :param stats: dict where aggregated stats are kept
        :param index: time bucket index in stats
        :param meter: meter record as returned from HBase
        :param start_time: query start time
        :param period: length of the time bucket
        """
        vol = meter['counter_volume']
        ts = meter['timestamp']
        stat.unit = meter['counter_unit']
        stat.min = min(vol, stat.min or vol)
        stat.max = max(vol, stat.max)
        stat.sum = vol + (stat.sum or 0)
        stat.count += 1
        stat.avg = (stat.sum / float(stat.count))
        stat.duration_start = min(ts, stat.duration_start or ts)
        stat.duration_end = max(ts, stat.duration_end or ts)
        stat.duration = \
            timeutils.delta_seconds(stat.duration_start,
                                    stat.duration_end)

    def get_meter_statistics(self, sample_filter, period=None, groupby=None,
                             aggregate=None):
        """Return an iterable of models.Statistics instances containing meter
        statistics described by the query parameters.

        The filter must have a meter value set.

        .. note::

           Due to HBase limitations the aggregations are implemented
           in the driver itself, therefore this method will be quite slow
           because of all the Thrift traffic it is going to create.

        """
        if groupby:
            raise NotImplementedError("Group by not implemented.")

        if aggregate:
            raise NotImplementedError('Selectable aggregates not implemented')

        with self.conn_pool.connection() as conn:
            meter_table = conn.table(self.METER_TABLE)
            q, start, stop = make_sample_query_from_filter(sample_filter)
            meters = map(deserialize_entry, list(meter for (ignored, meter) in
                         meter_table.scan(filter=q, row_start=start,
                                          row_stop=stop)))

        if sample_filter.start:
            start_time = sample_filter.start
        elif meters:
            start_time = meters[-1][0]['timestamp']
        else:
            start_time = None

        if sample_filter.end:
            end_time = sample_filter.end
        elif meters:
            end_time = meters[0][0]['timestamp']
        else:
            end_time = None

        results = []

        if not period:
            period = 0
            period_start = start_time
            period_end = end_time

        # As our HBase meters are stored as newest-first, we need to iterate
        # in the reverse order
        for meter in meters[::-1]:
            ts = meter[0]['timestamp']
            if period:
                offset = int(timeutils.delta_seconds(
                    start_time, ts) / period) * period
                period_start = start_time + datetime.timedelta(0, offset)

            if not results or not results[-1].period_start == \
                    period_start:
                if period:
                    period_end = period_start + datetime.timedelta(
                        0, period)
                results.append(
                    models.Statistics(unit='',
                                      count=0,
                                      min=0,
                                      max=0,
                                      avg=0,
                                      sum=0,
                                      period=period,
                                      period_start=period_start,
                                      period_end=period_end,
                                      duration=None,
                                      duration_start=None,
                                      duration_end=None,
                                      groupby=None)
                )
            self._update_meter_stats(results[-1], meter[0])
        return results


###############
# This is a very crude version of "in-memory HBase", which implements just
# enough functionality of HappyBase API to support testing of our driver.
#
class MTable(object):
    """HappyBase.Table mock
    """
    def __init__(self, name, families):
        self.name = name
        self.families = families
        self._rows = {}

    def row(self, key):
        return self._rows.get(key, {})

    def rows(self, keys):
        return ((k, self.row(k)) for k in keys)

    def put(self, key, data):
        if key not in self._rows:
            self._rows[key] = data
        else:
            self._rows[key].update(data)

    def delete(self, key):
        del self._rows[key]

    def scan(self, filter=None, columns=None, row_start=None, row_stop=None):
        columns = columns or []
        sorted_keys = sorted(self._rows)
        # copy data between row_start and row_stop into a dict
        rows = {}
        for row in sorted_keys:
            if row_start and row < row_start:
                continue
            if row_stop and row > row_stop:
                break
            rows[row] = copy.copy(self._rows[row])
        if columns:
            ret = {}
            for row in rows.keys():
                data = rows[row]
                for key in data:
                    if key in columns:
                        ret[row] = data
            rows = ret
        elif filter:
            # TODO(jdanjou): we should really parse this properly,
            # but at the moment we are only going to support AND here
            filters = filter.split('AND')
            for f in filters:
                # Extract filter name and its arguments
                g = re.search("(.*)\((.*),?\)", f)
                fname = g.group(1).strip()
                fargs = [s.strip().replace('\'', '')
                         for s in g.group(2).split(',')]
                m = getattr(self, fname)
                if callable(m):
                    # overwrite rows for filtering to take effect
                    # in case of multiple filters
                    rows = m(fargs, rows)
                else:
                    raise NotImplementedError("%s filter is not implemented, "
                                              "you may want to add it!")
        for k in sorted(rows):
            yield k, rows[k]

    @staticmethod
    def SingleColumnValueFilter(args, rows):
        """This method is called from scan() when 'SingleColumnValueFilter'
        is found in the 'filter' argument
        """
        op = args[2]
        column = "%s:%s" % (args[0], args[1])
        value = args[3]
        if value.startswith('binary:'):
            value = value[7:]
        r = {}
        for row in rows:
            data = rows[row]

            if op == '=':
                if column in data and data[column] == value:
                    r[row] = data
            elif op == '<=':
                if column in data and data[column] <= value:
                    r[row] = data
            elif op == '>=':
                if column in data and data[column] >= value:
                    r[row] = data
            else:
                raise NotImplementedError("In-memory "
                                          "SingleColumnValueFilter "
                                          "doesn't support the %s operation "
                                          "yet" % op)
        return r


class MConnectionPool(object):
    def __init__(self):
        self.conn = MConnection()

    def connection(self):
        return self.conn


class MConnection(object):
    """HappyBase.Connection mock
    """
    def __init__(self):
        self.tables = {}

    def __enter__(self, *args, **kwargs):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        pass

    def open(self):
        LOG.debug(_("Opening in-memory HBase connection"))

    def create_table(self, n, families=None):
        families = families or {}
        if n in self.tables:
            return self.tables[n]
        t = MTable(n, families)
        self.tables[n] = t
        return t

    def delete_table(self, name, use_prefix=True):
        del self.tables[name]

    def table(self, name):
        return self.create_table(name)


#################################################
# Here be various HBase helpers
def reverse_timestamp(dt):
    """Reverse timestamp so that newer timestamps are represented by smaller
    numbers than older ones.

    Reverse timestamps is a technique used in HBase rowkey design. When period
    queries are required the HBase rowkeys must include timestamps, but as
    rowkeys in HBase are ordered lexicographically, the timestamps must be
    reversed.
    """
    epoch = datetime.datetime(1970, 1, 1)
    td = dt - epoch
    ts = td.microseconds + td.seconds * 1000000 + td.days * 86400000000
    return 0x7fffffffffffffff - ts


def make_timestamp_query(func, start=None, start_op=None, end=None,
                         end_op=None, bounds_only=False, **kwargs):
    """Return a filter start and stop row for filtering and a query
    which based on the fact that CF-name is 'rts'
    :param start: Optional start timestamp
    :param start_op: Optional start timestamp operator, like gt, ge
    :param end: Optional end timestamp
    :param end_op: Optional end timestamp operator, like lt, le
    :param bounds_only: if True than query will not be returned
    :param func: a function that provide a format of row
    :param kwargs: kwargs for :param func
    """
    rts_start, rts_end = get_start_end_rts(start, start_op, end, end_op)
    start_row, end_row = func(rts_start, rts_end, **kwargs)

    if bounds_only:
        return start_row, end_row

    q = []
    # We dont need to dump here because get_start_end_rts returns strings
    if rts_start:
        q.append("SingleColumnValueFilter ('f', 'rts', <=, 'binary:%s')" %
                 rts_start)
    if rts_end:
        q.append("SingleColumnValueFilter ('f', 'rts', >=, 'binary:%s')" %
                 rts_end)

    res_q = None
    if len(q):
        res_q = " AND ".join(q)

    return start_row, end_row, res_q


def get_start_end_rts(start, start_op, end, end_op):

    rts_start = str(reverse_timestamp(start) + 1) if start else ""
    rts_end = str(reverse_timestamp(end) + 1) if end else ""

    #By default, we are using ge for lower bound and lt for upper bound
    if start_op == 'gt':
        rts_start = str(long(rts_start) - 2)
    if end_op == 'le':
        rts_end = str(long(rts_end) - 1)

    return rts_start, rts_end


def make_query(metaquery=None, **kwargs):
    """Return a filter query string based on the selected parameters.

    :param metaquery: optional metaquery dict
    :param kwargs: key-value pairs to filter on. Key should be a real
     column name in db
    """
    q = []
    # Note: we use extended constructor for SingleColumnValueFilter here.
    # It is explicitly specified that entry should not be returned if CF is not
    # found in table.
    for key, value in kwargs.items():
        if value is not None:
            if key == 'source':
                q.append("SingleColumnValueFilter "
                         "('f', 's_%s', =, 'binary:%s', true, true)" %
                         (value, dump('1')))
            else:
                q.append("SingleColumnValueFilter "
                         "('f', '%s', =, 'binary:%s', true, true)" %
                         (key, dump(value)))
    res_q = None
    if len(q):
        res_q = " AND ".join(q)

    if metaquery:
        meta_q = []
        for k, v in metaquery.items():
            meta_q.append(
                "SingleColumnValueFilter ('f', '%s', =, 'binary:%s', "
                "true, true)"
                % ('r_' + k, dump(v)))
        meta_q = " AND ".join(meta_q)
        # join query and metaquery
        if res_q is not None:
            res_q += " AND " + meta_q
        else:
            res_q = meta_q   # metaquery only

    return res_q


def make_sample_query_from_filter(sample_filter, require_meter=True):
    """Return a query dictionary based on the settings in the filter.

    :param sample_filter: SampleFilter instance
    :param require_meter: If true and the filter does not have a meter,
                          raise an error.
    """
    meter = sample_filter.meter
    if not meter and require_meter:
        raise RuntimeError('Missing required meter specifier')
    start_row, end_row, ts_query = make_timestamp_query(
        _make_general_rowkey_scan,
        start=sample_filter.start, start_op=sample_filter.start_timestamp_op,
        end=sample_filter.end, end_op=sample_filter.end_timestamp_op,
        some_id=meter)

    q = make_query(metaquery=sample_filter.metaquery,
                   user_id=sample_filter.user,
                   project_id=sample_filter.project,
                   counter_name=meter,
                   resource_id=sample_filter.resource,
                   source=sample_filter.source,
                   message_id=sample_filter.message_id)

    if q:
        ts_query = (" AND " + ts_query) if ts_query else ""
        res_q = q + ts_query if ts_query else q
    else:
        res_q = ts_query if ts_query else None
    return res_q, start_row, end_row


def _make_general_rowkey_scan(rts_start=None, rts_end=None, some_id=None):
    """If it's filter on some_id without start and end,
        start_row = some_id while end_row = some_id + MAX_BYTE
    """
    if some_id is None:
        return None, None
    if not rts_start:
        rts_start = chr(127)
    end_row = "%s_%s" % (some_id, rts_start)
    start_row = "%s_%s" % (some_id, rts_end)

    return start_row, end_row


def _format_meter_reference(counter_name, counter_type, counter_unit, source):
    """Format reference to meter data.
    """
    return "%s!%s!%s+%s" % (counter_name, counter_type, counter_unit, source)


def _timestamp_from_record_tuple(record):
    """Extract timestamp from HBase tuple record
    """
    return record[0]['timestamp']


def _resource_id_from_record_tuple(record):
    """Extract resource_id from HBase tuple record
    """
    return record[0]['resource_id']


def deserialize_entry(entry, get_raw_meta=True):
    """Return a list of flatten_result, sources, meters and metadata
    flatten_result contains a dict of simple structures such as 'resource_id':1
    sources/meters are the lists of sources and meters correspondingly.
    metadata is metadata dict. This dict may be returned as flattened if
    get_raw_meta is False.

    :param entry: entry from HBase, without row name and timestamp
    :param get_raw_meta: If true then raw metadata will be returned
                         If False metadata will be constructed from
                         'f:r_metadata.' fields
    """
    flatten_result = {}
    sources = []
    meters = []
    metadata_flattened = {}
    for k, v in entry.items():
        if k.startswith('f:s_'):
            sources.append(k[4:])
        elif k.startswith('f:r_metadata.'):
            metadata_flattened[k[len('f:r_metadata.'):]] = load(v)
        elif k.startswith('f:m_'):
            meters.append(k[4:])
        else:
            flatten_result[k[2:]] = load(v)
    if get_raw_meta:
        metadata = flatten_result.get('resource_metadata', {})
    else:
        metadata = metadata_flattened

    return flatten_result, sources, meters, metadata


def serialize_entry(data=None, **kwargs):
    """Return a dict that is ready to be stored to HBase

    :param data: dict to be serialized
    :param kwargs: additional args
    """
    data = data or {}
    entry_dict = copy.copy(data)
    entry_dict.update(**kwargs)

    result = {}
    for k, v in entry_dict.items():
        if k == 'source':
            # user, project and resource tables may contain several sources.
            # Besides, resource table may contain several meters.
            # To make insertion safe we need to store all meters and sources in
            # a separate cell. For this purpose s_ and m_ prefixes are
            # introduced.
                result['f:s_%s' % v] = dump('1')

        elif k == 'meter':
            result['f:m_%s' % v] = dump('1')
        elif k == 'resource_metadata':
            # keep raw metadata as well as flattened to provide
            # capability with API v2. It will be flattened in another
            # way on API level. But we need flattened too for quick filtering.
            flattened_meta = dump_metadata(v)
            for k, m in flattened_meta.items():
                result['f:r_metadata.' + k] = dump(m)
            result['f:resource_metadata'] = dump(v)
        else:
            result['f:' + k] = dump(v)
    return result


def dump_metadata(meta):
    resource_metadata = {}
    for key, v in utils.dict_to_keyval(meta):
        resource_metadata[key] = v
    return resource_metadata


def dump(data):
    return json.dumps(data, default=bson.json_util.default)


def load(data):
    return json.loads(data, object_hook=object_hook)


# We don't want to have tzinfo in decoded json.This object_hook is
# overwritten json_util.object_hook for $date
def object_hook(dct):
    if "$date" in dct:
        dt = bson.json_util.object_hook(dct)
        return dt.replace(tzinfo=None)
    return bson.json_util.object_hook(dct)

########NEW FILE########
__FILENAME__ = impl_log
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Simple logging storage backend.
"""

from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer.storage import base

LOG = log.getLogger(__name__)


class Connection(base.Connection):
    """Log the data.
    """

    def upgrade(self):
        pass

    def clear(self):
        pass

    def record_metering_data(self, data):
        """Write the data to the backend storage system.

        :param data: a dictionary such as returned by
                     ceilometer.meter.meter_message_from_counter
        """
        LOG.info(_('metering data %(counter_name)s for %(resource_id)s: '
                   '%(counter_volume)s')
                 % ({'counter_name': data['counter_name'],
                     'resource_id': data['resource_id'],
                     'counter_volume': data['counter_volume']}))

    def clear_expired_metering_data(self, ttl):
        """Clear expired data from the backend storage system according to the
        time-to-live.

        :param ttl: Number of seconds to keep records for.

        """
        LOG.info(_("Dropping data with TTL %d"), ttl)

    def get_resources(self, user=None, project=None, source=None,
                      start_timestamp=None, start_timestamp_op=None,
                      end_timestamp=None, end_timestamp_op=None,
                      metaquery=None, resource=None, pagination=None):
        """Return an iterable of dictionaries containing resource information.

        { 'resource_id': UUID of the resource,
          'project_id': UUID of project owning the resource,
          'user_id': UUID of user owning the resource,
          'timestamp': UTC datetime of last update to the resource,
          'metadata': most current metadata for the resource,
          'meter': list of the meters reporting data for the resource,
          }

        :param user: Optional ID for user that owns the resource.
        :param project: Optional ID for project that owns the resource.
        :param source: Optional source filter.
        :param start_timestamp: Optional modified timestamp start range.
        :param start_timestamp_op: Optional start time operator, like gt, ge.
        :param end_timestamp: Optional modified timestamp end range.
        :param end_timestamp_op: Optional end time operator, like lt, le.
        :param metaquery: Optional dict with metadata to match on.
        :param resource: Optional resource filter.
        :param pagination: Optional pagination query.
        """
        return []

    def get_meters(self, user=None, project=None, resource=None, source=None,
                   limit=None, metaquery=None, pagination=None):
        """Return an iterable of dictionaries containing meter information.

        { 'name': name of the meter,
          'type': type of the meter (gauge, delta, cumulative),
          'resource_id': UUID of the resource,
          'project_id': UUID of project owning the resource,
          'user_id': UUID of user owning the resource,
          }

        :param user: Optional ID for user that owns the resource.
        :param project: Optional ID for project that owns the resource.
        :param resource: Optional resource filter.
        :param source: Optional source filter.
        :param limit: Maximum number of results to return.
        :param metaquery: Optional dict with metadata to match on.
        :param pagination: Optional pagination query.
        """
        return []

    def get_samples(self, sample_filter):
        """Return an iterable of samples as created by
        :func:`ceilometer.meter.meter_message_from_counter`.
        """
        return []

    def get_meter_statistics(self, sample_filter, period=None, groupby=None,
                             aggregate=None):
        """Return a dictionary containing meter statistics.
        described by the query parameters.

        The filter must have a meter value set.

        { 'min':
          'max':
          'avg':
          'sum':
          'count':
          'period':
          'period_start':
          'period_end':
          'duration':
          'duration_start':
          'duration_end':
          }

        """
        return []

    def get_alarms(self, name=None, user=None,
                   project=None, enabled=None, alarm_id=None, pagination=None):
        """Yields a lists of alarms that match filters
        """
        return []

    def create_alarm(self, alarm):
        """Create alarm.
        """
        return alarm

    def update_alarm(self, alarm):
        """update alarm
        """
        return alarm

    def delete_alarm(self, alarm_id):
        """Delete an alarm."""

########NEW FILE########
__FILENAME__ = impl_mongodb
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
# Copyright © 2013 eNovance
# Copyright © 2014 Red Hat, Inc
#
# Authors: Doug Hellmann <doug.hellmann@dreamhost.com>
#          Julien Danjou <julien@danjou.info>
#          Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""MongoDB storage backend
"""

import calendar
import copy
import datetime
import json
import operator
import uuid

import bson.code
import bson.objectid
import pymongo

from oslo.config import cfg

from ceilometer.openstack.common import log
from ceilometer.openstack.common import timeutils
from ceilometer import storage
from ceilometer.storage import base
from ceilometer.storage import models
from ceilometer.storage import pymongo_base
from ceilometer import utils

cfg.CONF.import_opt('time_to_live', 'ceilometer.storage',
                    group="database")

LOG = log.getLogger(__name__)


AVAILABLE_CAPABILITIES = {
    'resources': {'query': {'simple': True,
                            'metadata': True}},
    'statistics': {'groupby': True,
                   'query': {'simple': True,
                             'metadata': True},
                   'aggregation': {'standard': True,
                                   'selectable': {'max': True,
                                                  'min': True,
                                                  'sum': True,
                                                  'avg': True,
                                                  'count': True,
                                                  'stddev': True,
                                                  'cardinality': True}}}
}


class Connection(pymongo_base.Connection):
    """Put the data into a MongoDB database

    Collections::

        - meter
          - the raw incoming data
        - resource
          - the metadata for resources
          - { _id: uuid of resource,
              metadata: metadata dictionaries
              user_id: uuid
              project_id: uuid
              meter: [ array of {counter_name: string, counter_type: string,
                                 counter_unit: string} ]
            }
    """

    CAPABILITIES = utils.update_nested(pymongo_base.Connection.CAPABILITIES,
                                       AVAILABLE_CAPABILITIES)
    CONNECTION_POOL = pymongo_base.ConnectionPool()

    REDUCE_GROUP_CLEAN = bson.code.Code("""
    function ( curr, result ) {
        if (result.resources.indexOf(curr.resource_id) < 0)
            result.resources.push(curr.resource_id);
    }
    """)

    STANDARD_AGGREGATES = dict(
        emit_initial=dict(
            sum='',
            count='',
            avg='',
            min='',
            max=''
        ),
        emit_body=dict(
            sum='sum: this.counter_volume,',
            count='count: NumberInt(1),',
            avg='acount: NumberInt(1), asum: this.counter_volume,',
            min='min: this.counter_volume,',
            max='max: this.counter_volume,'
        ),
        reduce_initial=dict(
            sum='',
            count='',
            avg='',
            min='',
            max=''
        ),
        reduce_body=dict(
            sum='sum: values[0].sum,',
            count='count: values[0].count,',
            avg='acount: values[0].acount, asum: values[0].asum,',
            min='min: values[0].min,',
            max='max: values[0].max,'
        ),
        reduce_computation=dict(
            sum='res.sum += values[i].sum;',
            count='res.count = NumberInt(res.count + values[i].count);',
            avg=('res.acount = NumberInt(res.acount + values[i].acount);'
                 'res.asum += values[i].asum;'),
            min='if ( values[i].min < res.min ) {res.min = values[i].min;}',
            max='if ( values[i].max > res.max ) {res.max = values[i].max;}'
        ),
        finalize=dict(
            sum='',
            count='',
            avg='value.avg = value.asum / value.acount;',
            min='',
            max=''
        ),
    )

    UNPARAMETERIZED_AGGREGATES = dict(
        emit_initial=dict(
            stddev=(
                ''
            )
        ),
        emit_body=dict(
            stddev='sdsum: this.counter_volume,'
                   'sdcount: 1,'
                   'weighted_distances: 0,'
                   'stddev: 0,'
        ),
        reduce_initial=dict(
            stddev=''
        ),
        reduce_body=dict(
            stddev='sdsum: values[0].sdsum,'
                   'sdcount: values[0].sdcount,'
                   'weighted_distances: values[0].weighted_distances,'
                   'stddev: values[0].stddev,'
        ),
        reduce_computation=dict(
            stddev=(
                'var deviance = (res.sdsum / res.sdcount) - values[i].sdsum;'
                'var weight = res.sdcount / ++res.sdcount;'
                'res.weighted_distances += (Math.pow(deviance, 2) * weight);'
                'res.sdsum += values[i].sdsum;'
            )
        ),
        finalize=dict(
            stddev=(
                'value.stddev = Math.sqrt(value.weighted_distances /'
                '  value.sdcount);'
            )
        ),
    )

    PARAMETERIZED_AGGREGATES = dict(
        validate=dict(
            cardinality=lambda p: p in ['resource_id', 'user_id', 'project_id',
                                        'source']
        ),
        emit_initial=dict(
            cardinality=(
                'aggregate["cardinality/%(aggregate_param)s"] = 1;'
                'var distinct_%(aggregate_param)s = {};'
                'distinct_%(aggregate_param)s[this["%(aggregate_param)s"]]'
                '   = true;'
            )
        ),
        emit_body=dict(
            cardinality=(
                'distinct_%(aggregate_param)s : distinct_%(aggregate_param)s,'
                '%(aggregate_param)s : this["%(aggregate_param)s"],'
            )
        ),
        reduce_initial=dict(
            cardinality=''
        ),
        reduce_body=dict(
            cardinality=(
                'aggregate : values[0].aggregate,'
                'distinct_%(aggregate_param)s:'
                '  values[0].distinct_%(aggregate_param)s,'
                '%(aggregate_param)s : values[0]["%(aggregate_param)s"],'
            )
        ),
        reduce_computation=dict(
            cardinality=(
                'if (!(values[i]["%(aggregate_param)s"] in'
                '      res.distinct_%(aggregate_param)s)) {'
                '  res.distinct_%(aggregate_param)s[values[i]'
                '    ["%(aggregate_param)s"]] = true;'
                '  res.aggregate["cardinality/%(aggregate_param)s"] += 1;}'
            )
        ),
        finalize=dict(
            cardinality=''
        ),
    )

    EMIT_STATS_COMMON = """
        var aggregate = {};
        %(aggregate_initial_placeholder)s
        emit(%(key_val)s, { unit: this.counter_unit,
                            aggregate : aggregate,
                            %(aggregate_body_placeholder)s
                            groupby : %(groupby_val)s,
                            duration_start : this.timestamp,
                            duration_end : this.timestamp,
                            period_start : %(period_start_val)s,
                            period_end : %(period_end_val)s} )
    """

    MAP_STATS_PERIOD_VAR = """
        var period = %(period)d * 1000;
        var period_first = %(period_first)d * 1000;
        var period_start = period_first
                           + (Math.floor(new Date(this.timestamp.getTime()
                                         - period_first) / period)
                              * period);
    """

    MAP_STATS_GROUPBY_VAR = """
        var groupby_fields = %(groupby_fields)s;
        var groupby = {};
        var groupby_key = {};

        for ( var i=0; i<groupby_fields.length; i++ ) {
            groupby[groupby_fields[i]] = this[groupby_fields[i]]
            groupby_key[groupby_fields[i]] = this[groupby_fields[i]]
        }
    """

    PARAMS_MAP_STATS = {
        'key_val': '\'statistics\'',
        'groupby_val': 'null',
        'period_start_val': 'this.timestamp',
        'period_end_val': 'this.timestamp',
        'aggregate_initial_placeholder': '%(aggregate_initial_val)s',
        'aggregate_body_placeholder': '%(aggregate_body_val)s'
    }

    MAP_STATS = bson.code.Code("function () {" +
                               EMIT_STATS_COMMON % PARAMS_MAP_STATS +
                               "}")

    PARAMS_MAP_STATS_PERIOD = {
        'key_val': 'period_start',
        'groupby_val': 'null',
        'period_start_val': 'new Date(period_start)',
        'period_end_val': 'new Date(period_start + period)',
        'aggregate_initial_placeholder': '%(aggregate_initial_val)s',
        'aggregate_body_placeholder': '%(aggregate_body_val)s'
    }

    MAP_STATS_PERIOD = bson.code.Code(
        "function () {" +
        MAP_STATS_PERIOD_VAR +
        EMIT_STATS_COMMON % PARAMS_MAP_STATS_PERIOD +
        "}")

    PARAMS_MAP_STATS_GROUPBY = {
        'key_val': 'groupby_key',
        'groupby_val': 'groupby',
        'period_start_val': 'this.timestamp',
        'period_end_val': 'this.timestamp',
        'aggregate_initial_placeholder': '%(aggregate_initial_val)s',
        'aggregate_body_placeholder': '%(aggregate_body_val)s'
    }

    MAP_STATS_GROUPBY = bson.code.Code(
        "function () {" +
        MAP_STATS_GROUPBY_VAR +
        EMIT_STATS_COMMON % PARAMS_MAP_STATS_GROUPBY +
        "}")

    PARAMS_MAP_STATS_PERIOD_GROUPBY = {
        'key_val': 'groupby_key',
        'groupby_val': 'groupby',
        'period_start_val': 'new Date(period_start)',
        'period_end_val': 'new Date(period_start + period)',
        'aggregate_initial_placeholder': '%(aggregate_initial_val)s',
        'aggregate_body_placeholder': '%(aggregate_body_val)s'
    }

    MAP_STATS_PERIOD_GROUPBY = bson.code.Code(
        "function () {" +
        MAP_STATS_PERIOD_VAR +
        MAP_STATS_GROUPBY_VAR +
        "    groupby_key['period_start'] = period_start\n" +
        EMIT_STATS_COMMON % PARAMS_MAP_STATS_PERIOD_GROUPBY +
        "}")

    REDUCE_STATS = bson.code.Code("""
    function (key, values) {
        %(aggregate_initial_val)s
        var res = { unit: values[0].unit,
                    aggregate: values[0].aggregate,
                    %(aggregate_body_val)s
                    groupby: values[0].groupby,
                    period_start: values[0].period_start,
                    period_end: values[0].period_end,
                    duration_start: values[0].duration_start,
                    duration_end: values[0].duration_end };
        for ( var i=1; i<values.length; i++ ) {
            %(aggregate_computation_val)s
            if ( values[i].duration_start < res.duration_start )
               res.duration_start = values[i].duration_start;
            if ( values[i].duration_end > res.duration_end )
               res.duration_end = values[i].duration_end;
        }
        return res;
    }
    """)

    FINALIZE_STATS = bson.code.Code("""
    function (key, value) {
        %(aggregate_val)s
        value.duration = (value.duration_end - value.duration_start) / 1000;
        value.period = NumberInt((value.period_end - value.period_start)
                                  / 1000);
        return value;
    }""")

    SORT_OPERATION_MAPPING = {'desc': (pymongo.DESCENDING, '$lt'),
                              'asc': (pymongo.ASCENDING, '$gt')}

    MAP_RESOURCES = bson.code.Code("""
    function () {
        emit(this.resource_id,
             {user_id: this.user_id,
              project_id: this.project_id,
              source: this.source,
              first_timestamp: this.timestamp,
              last_timestamp: this.timestamp,
              metadata: this.resource_metadata})
    }""")

    REDUCE_RESOURCES = bson.code.Code("""
    function (key, values) {
        var merge = {user_id: values[0].user_id,
                     project_id: values[0].project_id,
                     source: values[0].source,
                     first_timestamp: values[0].first_timestamp,
                     last_timestamp: values[0].last_timestamp,
                     metadata: values[0].metadata}
        values.forEach(function(value) {
            if (merge.first_timestamp - value.first_timestamp > 0) {
                merge.first_timestamp = value.first_timestamp;
                merge.user_id = value.user_id;
                merge.project_id = value.project_id;
                merge.source = value.source;
            } else if (merge.last_timestamp - value.last_timestamp <= 0) {
                merge.last_timestamp = value.last_timestamp;
                merge.metadata = value.metadata;
            }
        });
        return merge;
      }""")

    _GENESIS = datetime.datetime(year=datetime.MINYEAR, month=1, day=1)
    _APOCALYPSE = datetime.datetime(year=datetime.MAXYEAR, month=12, day=31,
                                    hour=23, minute=59, second=59)

    def __init__(self, url):

        # NOTE(jd) Use our own connection pooling on top of the Pymongo one.
        # We need that otherwise we overflow the MongoDB instance with new
        # connection since we instanciate a Pymongo client each time someone
        # requires a new storage connection.
        self.conn = self.CONNECTION_POOL.connect(url)

        # Require MongoDB 2.4 to use $setOnInsert
        if self.conn.server_info()['versionArray'] < [2, 4]:
            raise storage.StorageBadVersion("Need at least MongoDB 2.4")

        connection_options = pymongo.uri_parser.parse_uri(url)
        self.db = getattr(self.conn, connection_options['database'])
        if connection_options.get('username'):
            self.db.authenticate(connection_options['username'],
                                 connection_options['password'])

        # NOTE(jd) Upgrading is just about creating index, so let's do this
        # on connection to be sure at least the TTL is correcly updated if
        # needed.
        self.upgrade()

    def upgrade(self):
        # Establish indexes
        #
        # We need variations for user_id vs. project_id because of the
        # way the indexes are stored in b-trees. The user_id and
        # project_id values are usually mutually exclusive in the
        # queries, so the database won't take advantage of an index
        # including both.
        name_qualifier = dict(user_id='', project_id='project_')
        background = dict(user_id=False, project_id=True)
        for primary in ['user_id', 'project_id']:
            name = 'resource_%sidx' % name_qualifier[primary]
            self.db.resource.ensure_index([
                (primary, pymongo.ASCENDING),
                ('source', pymongo.ASCENDING),
            ], name=name, background=background[primary])

            name = 'meter_%sidx' % name_qualifier[primary]
            self.db.meter.ensure_index([
                ('resource_id', pymongo.ASCENDING),
                (primary, pymongo.ASCENDING),
                ('counter_name', pymongo.ASCENDING),
                ('timestamp', pymongo.ASCENDING),
                ('source', pymongo.ASCENDING),
            ], name=name, background=background[primary])

        self.db.resource.ensure_index([('last_sample_timestamp',
                                        pymongo.DESCENDING)],
                                      name='last_sample_timestamp_idx',
                                      sparse=True)
        self.db.meter.ensure_index([('timestamp', pymongo.DESCENDING)],
                                   name='timestamp_idx')
        # remove API v1 related table
        self.db.user.drop()
        self.db.project.drop()

        indexes = self.db.meter.index_information()

        ttl = cfg.CONF.database.time_to_live

        if ttl <= 0:
            if 'meter_ttl' in indexes:
                self.db.meter.drop_index('meter_ttl')
            return

        if 'meter_ttl' in indexes:
            # NOTE(sileht): manually check expireAfterSeconds because
            # ensure_index doesn't update index options if the index already
            # exists
            if ttl == indexes['meter_ttl'].get('expireAfterSeconds', -1):
                return

            self.db.meter.drop_index('meter_ttl')

        self.db.meter.create_index(
            [('timestamp', pymongo.ASCENDING)],
            expireAfterSeconds=ttl,
            name='meter_ttl'
        )

    def clear(self):
        self.conn.drop_database(self.db)
        # Connection will be reopened automatically if needed
        self.conn.close()

    def record_metering_data(self, data):
        """Write the data to the backend storage system.

        :param data: a dictionary such as returned by
                     ceilometer.meter.meter_message_from_counter
        """
        # Record the updated resource metadata - we use $setOnInsert to
        # unconditionally insert sample timestamps and resource metadata
        # (in the update case, this must be conditional on the sample not
        # being out-of-order)
        resource = self.db.resource.find_and_modify(
            {'_id': data['resource_id']},
            {'$set': {'project_id': data['project_id'],
                      'user_id': data['user_id'],
                      'source': data['source'],
                      },
             '$setOnInsert': {'metadata': data['resource_metadata'],
                              'first_sample_timestamp': data['timestamp'],
                              'last_sample_timestamp': data['timestamp'],
                              },
             '$addToSet': {'meter': {'counter_name': data['counter_name'],
                                     'counter_type': data['counter_type'],
                                     'counter_unit': data['counter_unit'],
                                     },
                           },
             },
            upsert=True,
            new=True,
        )

        # only update last sample timestamp if actually later (the usual
        # in-order case)
        last_sample_timestamp = resource.get('last_sample_timestamp')
        if (last_sample_timestamp is None or
                last_sample_timestamp <= data['timestamp']):
            self.db.resource.update(
                {'_id': data['resource_id']},
                {'$set': {'metadata': data['resource_metadata'],
                          'last_sample_timestamp': data['timestamp']}}
            )

        # only update first sample timestamp if actually earlier (the unusual
        # out-of-order case)
        # NOTE: a null first sample timestamp is not updated as this indicates
        # a pre-existing resource document dating from before we started
        # recording these timestamps in the resource collection
        first_sample_timestamp = resource.get('first_sample_timestamp')
        if (first_sample_timestamp is not None and
                first_sample_timestamp > data['timestamp']):
            self.db.resource.update(
                {'_id': data['resource_id']},
                {'$set': {'first_sample_timestamp': data['timestamp']}}
            )

        # Record the raw data for the meter. Use a copy so we do not
        # modify a data structure owned by our caller (the driver adds
        # a new key '_id').
        record = copy.copy(data)
        record['recorded_at'] = timeutils.utcnow()
        self.db.meter.insert(record)

    def clear_expired_metering_data(self, ttl):
        """Clear expired data from the backend storage system according to the
        time-to-live.

        :param ttl: Number of seconds to keep records for.

        """
        results = self.db.meter.group(
            key={},
            condition={},
            reduce=self.REDUCE_GROUP_CLEAN,
            initial={
                'resources': [],
            }
        )[0]

        self.db.resource.remove({'_id': {'$nin': results['resources']}})

    @staticmethod
    def _get_marker(db_collection, marker_pairs):
        """Return the mark document according to the attribute-value pairs.

        :param db_collection: Database collection that be query.
        :param maker_pairs: Attribute-value pairs filter.
        """
        if db_collection is None:
            return
        if not marker_pairs:
            return
        ret = db_collection.find(marker_pairs, limit=2)

        if ret.count() == 0:
            raise base.NoResultFound
        elif ret.count() > 1:
            raise base.MultipleResultsFound
        else:
            _ret = ret.__getitem__(0)
            return _ret

    @classmethod
    def _recurse_sort_keys(cls, sort_keys, marker, flag):
        _first = sort_keys[0]
        value = marker[_first]
        if len(sort_keys) == 1:
            return {_first: {flag: value}}
        else:
            criteria_equ = {_first: {'eq': value}}
            criteria_cmp = cls._recurse_sort_keys(sort_keys[1:], marker, flag)
        return dict(criteria_equ, ** criteria_cmp)

    @classmethod
    def _build_paginate_query(cls, marker, sort_keys=None, sort_dir='desc'):
        """Returns a query with sorting / pagination.

        Pagination works by requiring sort_key and sort_dir.
        We use the last item in previous page as the 'marker' for pagination.
        So we return values that follow the passed marker in the order.
        :param q: The query dict passed in.
        :param marker: the last item of the previous page; we return the next
                       results after this item.
        :param sort_keys: array of attributes by which results be sorted.
        :param sort_dir: direction in which results be sorted (asc, desc).
        :return: sort parameters, query to use
        """
        all_sort = []
        sort_keys = sort_keys or []
        all_sort, _op = cls._build_sort_instructions(sort_keys, sort_dir)

        if marker is not None:
            sort_criteria_list = []

            for i in range(len(sort_keys)):
                #NOTE(fengqian): Generate the query criteria recursively.
                #sort_keys=[k1, k2, k3], maker_value=[v1, v2, v3]
                #sort_flags = ['$lt', '$gt', 'lt'].
                #The query criteria should be
                #{'k3': {'$lt': 'v3'}, 'k2': {'eq': 'v2'}, 'k1': {'eq': 'v1'}},
                #{'k2': {'$gt': 'v2'}, 'k1': {'eq': 'v1'}},
                #{'k1': {'$lt': 'v1'}} with 'OR' operation.
                #Each recurse will generate one items of three.
                sort_criteria_list.append(cls._recurse_sort_keys(
                                          sort_keys[:(len(sort_keys) - i)],
                                          marker, _op))

            metaquery = {"$or": sort_criteria_list}
        else:
            metaquery = {}

        return all_sort, metaquery

    @classmethod
    def _build_sort_instructions(cls, sort_keys=None, sort_dir='desc'):
        """Returns a sort_instruction and paging operator.

        Sort instructions are used in the query to determine what attributes
        to sort on and what direction to use.
        :param q: The query dict passed in.
        :param sort_keys: array of attributes by which results be sorted.
        :param sort_dir: direction in which results be sorted (asc, desc).
        :return: sort instructions and paging operator
        """
        sort_keys = sort_keys or []
        sort_instructions = []
        _sort_dir, operation = cls.SORT_OPERATION_MAPPING.get(
            sort_dir, cls.SORT_OPERATION_MAPPING['desc'])

        for _sort_key in sort_keys:
            _instruction = (_sort_key, _sort_dir)
            sort_instructions.append(_instruction)

        return sort_instructions, operation

    @classmethod
    def paginate_query(cls, q, db_collection, limit=None, marker=None,
                       sort_keys=None, sort_dir='desc'):
        """Returns a query result with sorting / pagination.

        Pagination works by requiring sort_key and sort_dir.
        We use the last item in previous page as the 'marker' for pagination.
        So we return values that follow the passed marker in the order.
        :param q: the query dict passed in.
        :param db_collection: Database collection that be query.
        :param limit: maximum number of items to return.
        :param marker: the last item of the previous page; we return the next
                       results after this item.
        :param sort_keys: array of attributes by which results be sorted.
        :param sort_dir: direction in which results be sorted (asc, desc).
        return: The query with sorting/pagination added.
        """

        sort_keys = sort_keys or []
        all_sort, query = cls._build_paginate_query(marker,
                                                    sort_keys,
                                                    sort_dir)
        q.update(query)

        #NOTE(Fengqian):MongoDB collection.find can not handle limit
        #when it equals None, it will raise TypeError, so we treate
        #None as 0 for the value of limit.
        if limit is None:
            limit = 0
        return db_collection.find(q, limit=limit, sort=all_sort)

    def _get_time_constrained_resources(self, query,
                                        start_timestamp, start_timestamp_op,
                                        end_timestamp, end_timestamp_op,
                                        metaquery, resource):
        """Return an iterable of models.Resource instances constrained
           by sample timestamp.

        :param query: project/user/source query
        :param start_timestamp: modified timestamp start range.
        :param start_timestamp_op: start time operator, like gt, ge.
        :param end_timestamp: modified timestamp end range.
        :param end_timestamp_op: end time operator, like lt, le.
        :param metaquery: dict with metadata to match on.
        :param resource: resource filter.
        """
        if resource is not None:
            query['resource_id'] = resource

        # Add resource_ prefix so it matches the field in the db
        query.update(dict(('resource_' + k, v)
                          for (k, v) in metaquery.iteritems()))

        # FIXME(dhellmann): This may not perform very well,
        # but doing any better will require changing the database
        # schema and that will need more thought than I have time
        # to put into it today.
        # Look for resources matching the above criteria and with
        # samples in the time range we care about, then change the
        # resource query to return just those resources by id.
        ts_range = pymongo_base.make_timestamp_range(start_timestamp,
                                                     end_timestamp,
                                                     start_timestamp_op,
                                                     end_timestamp_op)
        if ts_range:
            query['timestamp'] = ts_range

        sort_keys = base._handle_sort_key('resource')
        sort_instructions = self._build_sort_instructions(sort_keys)[0]

        # use a unique collection name for the results collection,
        # as result post-sorting (as oppposed to reduce pre-sorting)
        # is not possible on an inline M-R
        out = 'resource_list_%s' % uuid.uuid4()
        self.db.meter.map_reduce(self.MAP_RESOURCES,
                                 self.REDUCE_RESOURCES,
                                 out=out,
                                 sort={'resource_id': 1},
                                 query=query)

        try:
            for r in self.db[out].find(sort=sort_instructions):
                resource = r['value']
                yield models.Resource(
                    resource_id=r['_id'],
                    user_id=resource['user_id'],
                    project_id=resource['project_id'],
                    first_sample_timestamp=resource['first_timestamp'],
                    last_sample_timestamp=resource['last_timestamp'],
                    source=resource['source'],
                    metadata=resource['metadata'])
        finally:
            self.db[out].drop()

    def _get_floating_resources(self, query, metaquery, resource):
        """Return an iterable of models.Resource instances unconstrained
           by timestamp.

        :param query: project/user/source query
        :param metaquery: dict with metadata to match on.
        :param resource: resource filter.
        """
        if resource is not None:
            query['_id'] = resource

        query.update(dict((k, v)
                          for (k, v) in metaquery.iteritems()))

        keys = base._handle_sort_key('resource')
        sort_keys = ['last_sample_timestamp' if i == 'timestamp' else i
                     for i in keys]
        sort_instructions = self._build_sort_instructions(sort_keys)[0]

        for r in self.db.resource.find(query, sort=sort_instructions):
            yield models.Resource(
                resource_id=r['_id'],
                user_id=r['user_id'],
                project_id=r['project_id'],
                first_sample_timestamp=r.get('first_sample_timestamp',
                                             self._GENESIS),
                last_sample_timestamp=r.get('last_sample_timestamp',
                                            self._APOCALYPSE),
                source=r['source'],
                metadata=r['metadata'])

    def get_resources(self, user=None, project=None, source=None,
                      start_timestamp=None, start_timestamp_op=None,
                      end_timestamp=None, end_timestamp_op=None,
                      metaquery=None, resource=None, pagination=None):
        """Return an iterable of models.Resource instances

        :param user: Optional ID for user that owns the resource.
        :param project: Optional ID for project that owns the resource.
        :param source: Optional source filter.
        :param start_timestamp: Optional modified timestamp start range.
        :param start_timestamp_op: Optional start time operator, like gt, ge.
        :param end_timestamp: Optional modified timestamp end range.
        :param end_timestamp_op: Optional end time operator, like lt, le.
        :param metaquery: Optional dict with metadata to match on.
        :param resource: Optional resource filter.
        :param pagination: Optional pagination query.
        """
        if pagination:
            raise NotImplementedError('Pagination not implemented')

        metaquery = metaquery or {}

        query = {}
        if user is not None:
            query['user_id'] = user
        if project is not None:
            query['project_id'] = project
        if source is not None:
            query['source'] = source

        if start_timestamp or end_timestamp:
            return self._get_time_constrained_resources(query,
                                                        start_timestamp,
                                                        start_timestamp_op,
                                                        end_timestamp,
                                                        end_timestamp_op,
                                                        metaquery, resource)
        else:
            return self._get_floating_resources(query, metaquery, resource)

    def _aggregate_param(self, fragment_key, aggregate):
        fragment_map = self.STANDARD_AGGREGATES[fragment_key]

        if not aggregate:
            return ''.join([f for f in fragment_map.values()])

        fragments = ''

        for a in aggregate:
            if a.func in self.STANDARD_AGGREGATES[fragment_key]:
                fragment_map = self.STANDARD_AGGREGATES[fragment_key]
                fragments += fragment_map[a.func]
            elif a.func in self.UNPARAMETERIZED_AGGREGATES[fragment_key]:
                fragment_map = self.UNPARAMETERIZED_AGGREGATES[fragment_key]
                fragments += fragment_map[a.func]
            elif a.func in self.PARAMETERIZED_AGGREGATES[fragment_key]:
                fragment_map = self.PARAMETERIZED_AGGREGATES[fragment_key]
                v = self.PARAMETERIZED_AGGREGATES['validate'].get(a.func)
                if not (v and v(a.param)):
                    raise storage.StorageBadAggregate('Bad aggregate: %s.%s'
                                                      % (a.func, a.param))
                params = dict(aggregate_param=a.param)
                fragments += (fragment_map[a.func] % params)
            else:
                raise NotImplementedError('Selectable aggregate function %s'
                                          ' is not supported' % a.func)

        return fragments

    def get_meter_statistics(self, sample_filter, period=None, groupby=None,
                             aggregate=None):
        """Return an iterable of models.Statistics instance containing meter
        statistics described by the query parameters.

        The filter must have a meter value set.

        """
        if (groupby and
                set(groupby) - set(['user_id', 'project_id',
                                    'resource_id', 'source'])):
            raise NotImplementedError("Unable to group by these fields")

        q = pymongo_base.make_query_from_filter(sample_filter)

        if period:
            if sample_filter.start:
                period_start = sample_filter.start
            else:
                period_start = self.db.meter.find(
                    limit=1, sort=[('timestamp',
                                    pymongo.ASCENDING)])[0]['timestamp']
            period_start = int(calendar.timegm(period_start.utctimetuple()))
            map_params = {'period': period,
                          'period_first': period_start,
                          'groupby_fields': json.dumps(groupby)}
            if groupby:
                map_fragment = self.MAP_STATS_PERIOD_GROUPBY
            else:
                map_fragment = self.MAP_STATS_PERIOD
        else:
            if groupby:
                map_params = {'groupby_fields': json.dumps(groupby)}
                map_fragment = self.MAP_STATS_GROUPBY
            else:
                map_params = dict()
                map_fragment = self.MAP_STATS

        sub = self._aggregate_param

        map_params['aggregate_initial_val'] = sub('emit_initial', aggregate)
        map_params['aggregate_body_val'] = sub('emit_body', aggregate)

        map_stats = map_fragment % map_params

        reduce_params = dict(
            aggregate_initial_val=sub('reduce_initial', aggregate),
            aggregate_body_val=sub('reduce_body', aggregate),
            aggregate_computation_val=sub('reduce_computation', aggregate)
        )
        reduce_stats = self.REDUCE_STATS % reduce_params

        finalize_params = dict(aggregate_val=sub('finalize', aggregate))
        finalize_stats = self.FINALIZE_STATS % finalize_params

        results = self.db.meter.map_reduce(
            map_stats,
            reduce_stats,
            {'inline': 1},
            finalize=finalize_stats,
            query=q,
        )

        # FIXME(terriyu) Fix get_meter_statistics() so we don't use sorted()
        # to return the results
        return sorted(
            (self._stats_result_to_model(r['value'], groupby, aggregate)
             for r in results['results']),
            key=operator.attrgetter('period_start'))

    @staticmethod
    def _stats_result_aggregates(result, aggregate):
        stats_args = {}
        for attr in ['count', 'min', 'max', 'sum', 'avg']:
            if attr in result:
                stats_args[attr] = result[attr]

        if aggregate:
            stats_args['aggregate'] = {}
            for a in aggregate:
                ak = '%s%s' % (a.func, '/%s' % a.param if a.param else '')
                if ak in result:
                    stats_args['aggregate'][ak] = result[ak]
                elif 'aggregate' in result:
                    stats_args['aggregate'][ak] = result['aggregate'].get(ak)
        return stats_args

    @staticmethod
    def _stats_result_to_model(result, groupby, aggregate):
        stats_args = Connection._stats_result_aggregates(result, aggregate)
        stats_args['unit'] = result['unit']
        stats_args['duration'] = result['duration']
        stats_args['duration_start'] = result['duration_start']
        stats_args['duration_end'] = result['duration_end']
        stats_args['period'] = result['period']
        stats_args['period_start'] = result['period_start']
        stats_args['period_end'] = result['period_end']
        stats_args['groupby'] = (dict(
            (g, result['groupby'][g]) for g in groupby) if groupby else None)
        return models.Statistics(**stats_args)

########NEW FILE########
__FILENAME__ = impl_sqlalchemy
# -*- encoding: utf-8 -*-
#
# Author: John Tran <jhtran@att.com>
#         Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

"""SQLAlchemy storage backend."""

from __future__ import absolute_import
import datetime
import operator
import os
import types

from oslo.config import cfg
from sqlalchemy import and_
from sqlalchemy import asc
from sqlalchemy import desc
from sqlalchemy import distinct
from sqlalchemy import func
from sqlalchemy import not_
from sqlalchemy import or_
from sqlalchemy.orm import aliased

from ceilometer.openstack.common.db import exception as dbexc
from ceilometer.openstack.common.db.sqlalchemy import migration
import ceilometer.openstack.common.db.sqlalchemy.session as sqlalchemy_session
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer.openstack.common import timeutils
from ceilometer import storage
from ceilometer.storage import base
from ceilometer.storage import models as api_models
from ceilometer.storage.sqlalchemy import models
from ceilometer import utils

LOG = log.getLogger(__name__)


META_TYPE_MAP = {bool: models.MetaBool,
                 str: models.MetaText,
                 unicode: models.MetaText,
                 types.NoneType: models.MetaText,
                 int: models.MetaBigInt,
                 long: models.MetaBigInt,
                 float: models.MetaFloat}

STANDARD_AGGREGATES = dict(
    avg=func.avg(models.Sample.volume).label('avg'),
    sum=func.sum(models.Sample.volume).label('sum'),
    min=func.min(models.Sample.volume).label('min'),
    max=func.max(models.Sample.volume).label('max'),
    count=func.count(models.Sample.volume).label('count')
)

UNPARAMETERIZED_AGGREGATES = dict(
    stddev=func.stddev_pop(models.Sample.volume).label('stddev')
)

PARAMETERIZED_AGGREGATES = dict(
    validate=dict(
        cardinality=lambda p: p in ['resource_id', 'user_id', 'project_id']
    ),
    compute=dict(
        cardinality=lambda p: func.count(
            distinct(getattr(models.Sample, p))
        ).label('cardinality/%s' % p)
    )
)

AVAILABLE_CAPABILITIES = {
    'meters': {'query': {'simple': True,
                         'metadata': True}},
    'resources': {'query': {'simple': True,
                            'metadata': True}},
    'samples': {'pagination': True,
                'groupby': True,
                'query': {'simple': True,
                          'metadata': True,
                          'complex': True}},
    'statistics': {'groupby': True,
                   'query': {'simple': True,
                             'metadata': True},
                   'aggregation': {'standard': True,
                                   'selectable': {
                                       'max': True,
                                       'min': True,
                                       'sum': True,
                                       'avg': True,
                                       'count': True,
                                       'stddev': True,
                                       'cardinality': True}}
                   },
    'alarms': {'query': {'simple': True,
                         'complex': True},
               'history': {'query': {'simple': True,
                                     'complex': True}}},
    'events': {'query': {'simple': True}},
}


def apply_metaquery_filter(session, query, metaquery):
    """Apply provided metaquery filter to existing query.

    :param session: session used for original query
    :param query: Query instance
    :param metaquery: dict with metadata to match on.
    """
    for k, value in metaquery.iteritems():
        key = k[9:]  # strip out 'metadata.' prefix
        try:
            _model = META_TYPE_MAP[type(value)]
        except KeyError:
            raise NotImplementedError('Query on %(key)s is of %(value)s '
                                      'type and is not supported' %
                                      {"key": k, "value": type(value)})
        else:
            meta_alias = aliased(_model)
            on_clause = and_(models.Sample.id == meta_alias.id,
                             meta_alias.meta_key == key)
            # outer join is needed to support metaquery
            # with or operator on non existent metadata field
            # see: test_query_non_existing_metadata_with_result
            # test case.
            query = query.outerjoin(meta_alias, on_clause)
            query = query.filter(meta_alias.value == value)

    return query


def make_query_from_filter(session, query, sample_filter, require_meter=True):
    """Return a query dictionary based on the settings in the filter.

    :param session: session used for original query
    :param query: Query instance
    :param sample_filter: SampleFilter instance
    :param require_meter: If true and the filter does not have a meter,
                          raise an error.
    """

    if sample_filter.meter:
        query = query.filter(models.Meter.name == sample_filter.meter)
    elif require_meter:
        raise RuntimeError('Missing required meter specifier')
    if sample_filter.source:
        query = query.filter(
            models.Sample.source_id == sample_filter.source)
    if sample_filter.start:
        ts_start = sample_filter.start
        if sample_filter.start_timestamp_op == 'gt':
            query = query.filter(models.Sample.timestamp > ts_start)
        else:
            query = query.filter(models.Sample.timestamp >= ts_start)
    if sample_filter.end:
        ts_end = sample_filter.end
        if sample_filter.end_timestamp_op == 'le':
            query = query.filter(models.Sample.timestamp <= ts_end)
        else:
            query = query.filter(models.Sample.timestamp < ts_end)
    if sample_filter.user:
        query = query.filter(models.Sample.user_id == sample_filter.user)
    if sample_filter.project:
        query = query.filter(
            models.Sample.project_id == sample_filter.project)
    if sample_filter.resource:
        query = query.filter(
            models.Sample.resource_id == sample_filter.resource)
    if sample_filter.message_id:
        query = query.filter(
            models.Sample.message_id == sample_filter.message_id)

    if sample_filter.metaquery:
        query = apply_metaquery_filter(session, query,
                                       sample_filter.metaquery)

    return query


class Connection(base.Connection):
    """Put the data into a SQLAlchemy database.

    Tables::

        - meter
          - meter definition
          - { id: meter def id
              name: meter name
              type: meter type
              unit: meter unit
              }
        - sample
          - the raw incoming data
          - { id: sample id
              meter_id: meter id            (->meter.id)
              user_id: user uuid
              project_id: project uuid
              resource_id: resource uuid
              source_id: source id
              resource_metadata: metadata dictionaries
              volume: sample volume
              timestamp: datetime
              message_signature: message signature
              message_id: message uuid
              }
    """
    CAPABILITIES = utils.update_nested(base.Connection.CAPABILITIES,
                                       AVAILABLE_CAPABILITIES)

    def __init__(self, url):
        self._engine_facade = sqlalchemy_session.EngineFacade.from_config(
            url,
            cfg.CONF  # TODO(Alexei_987) Remove access to global CONF object
        )

    def upgrade(self):
        path = os.path.join(os.path.abspath(os.path.dirname(__file__)),
                            'sqlalchemy', 'migrate_repo')
        migration.db_sync(self._engine_facade.get_engine(), path)

    def clear(self):
        engine = self._engine_facade.get_engine()
        for table in reversed(models.Base.metadata.sorted_tables):
            engine.execute(table.delete())
        self._engine_facade._session_maker.close_all()
        engine.dispose()

    @staticmethod
    def _create_meter(session, name, type, unit):
        try:
            nested = session.connection().dialect.name != 'sqlite'
            with session.begin(nested=nested,
                               subtransactions=not nested):
                obj = session.query(models.Meter)\
                    .filter(models.Meter.name == name)\
                    .filter(models.Meter.type == type)\
                    .filter(models.Meter.unit == unit).first()
                if obj is None:
                    obj = models.Meter(name=name, type=type, unit=unit)
                    session.add(obj)
        except dbexc.DBDuplicateEntry:
            # retry function to pick up duplicate committed object
            obj = Connection._create_meter(session, name, type, unit)

        return obj

    def record_metering_data(self, data):
        """Write the data to the backend storage system.

        :param data: a dictionary such as returned by
                     ceilometer.meter.meter_message_from_counter
        """
        session = self._engine_facade.get_session()
        with session.begin():
            # Record the raw data for the sample.
            rmetadata = data['resource_metadata']
            meter = self._create_meter(session,
                                       data['counter_name'],
                                       data['counter_type'],
                                       data['counter_unit'])
            sample = models.Sample(meter_id=meter.id)
            session.add(sample)
            sample.resource_id = data['resource_id']
            sample.project_id = data['project_id']
            sample.user_id = data['user_id']
            sample.timestamp = data['timestamp']
            sample.resource_metadata = rmetadata
            sample.volume = data['counter_volume']
            sample.message_signature = data['message_signature']
            sample.message_id = data['message_id']
            sample.source_id = data['source']
            session.flush()

            if rmetadata:
                if isinstance(rmetadata, dict):
                    for key, v in utils.dict_to_keyval(rmetadata):
                        try:
                            _model = META_TYPE_MAP[type(v)]
                        except KeyError:
                            LOG.warn(_("Unknown metadata type. Key (%s) will "
                                       "not be queryable."), key)
                        else:
                            session.add(_model(id=sample.id,
                                               meta_key=key,
                                               value=v))

    def clear_expired_metering_data(self, ttl):
        """Clear expired data from the backend storage system according to the
        time-to-live.

        :param ttl: Number of seconds to keep records for.

        """

        session = self._engine_facade.get_session()
        with session.begin():
            end = timeutils.utcnow() - datetime.timedelta(seconds=ttl)
            sample_query = session.query(models.Sample)\
                .filter(models.Sample.timestamp < end)
            for sample_obj in sample_query.all():
                session.delete(sample_obj)

    def get_resources(self, user=None, project=None, source=None,
                      start_timestamp=None, start_timestamp_op=None,
                      end_timestamp=None, end_timestamp_op=None,
                      metaquery=None, resource=None, pagination=None):
        """Return an iterable of api_models.Resource instances

        :param user: Optional ID for user that owns the resource.
        :param project: Optional ID for project that owns the resource.
        :param source: Optional source filter.
        :param start_timestamp: Optional modified timestamp start range.
        :param start_timestamp_op: Optional start time operator, like gt, ge.
        :param end_timestamp: Optional modified timestamp end range.
        :param end_timestamp_op: Optional end time operator, like lt, le.
        :param metaquery: Optional dict with metadata to match on.
        :param resource: Optional resource filter.
        :param pagination: Optional pagination query.
        """
        if pagination:
            raise NotImplementedError('Pagination not implemented')

        metaquery = metaquery or {}

        def _apply_filters(query):
            # TODO(gordc) this should be merged with make_query_from_filter
            for column, value in [(models.Sample.resource_id, resource),
                                  (models.Sample.user_id, user),
                                  (models.Sample.project_id, project),
                                  (models.Sample.source_id, source)]:
                if value:
                    query = query.filter(column == value)
            if metaquery:
                query = apply_metaquery_filter(session, query, metaquery)
            if start_timestamp:
                if start_timestamp_op == 'gt':
                    query = query.filter(
                        models.Sample.timestamp > start_timestamp)
                else:
                    query = query.filter(
                        models.Sample.timestamp >= start_timestamp)
            if end_timestamp:
                if end_timestamp_op == 'le':
                    query = query.filter(
                        models.Sample.timestamp <= end_timestamp)
                else:
                    query = query.filter(
                        models.Sample.timestamp < end_timestamp)
            return query

        session = self._engine_facade.get_session()
        # get list of resource_ids
        res_q = session.query(distinct(models.Sample.resource_id))
        res_q = _apply_filters(res_q)

        for res_id in res_q.all():
            # get latest Sample
            max_q = session.query(models.Sample)\
                .filter(models.Sample.resource_id == res_id[0])
            max_q = _apply_filters(max_q)
            max_q = max_q.order_by(models.Sample.timestamp.desc(),
                                   models.Sample.id.desc()).limit(1)

            # get the min timestamp value.
            min_q = session.query(models.Sample.timestamp)\
                .filter(models.Sample.resource_id == res_id[0])
            min_q = _apply_filters(min_q)
            min_q = min_q.order_by(models.Sample.timestamp.asc()).limit(1)

            sample = max_q.first()
            if sample:
                yield api_models.Resource(
                    resource_id=sample.resource_id,
                    project_id=sample.project_id,
                    first_sample_timestamp=min_q.first().timestamp,
                    last_sample_timestamp=sample.timestamp,
                    source=sample.source_id,
                    user_id=sample.user_id,
                    metadata=sample.resource_metadata
                )

    def get_meters(self, user=None, project=None, resource=None, source=None,
                   metaquery=None, pagination=None):
        """Return an iterable of api_models.Meter instances

        :param user: Optional ID for user that owns the resource.
        :param project: Optional ID for project that owns the resource.
        :param resource: Optional ID of the resource.
        :param source: Optional source filter.
        :param metaquery: Optional dict with metadata to match on.
        :param pagination: Optional pagination query.
        """

        if pagination:
            raise NotImplementedError('Pagination not implemented')

        metaquery = metaquery or {}

        def _apply_filters(query):
            # TODO(gordc) this should be merged with make_query_from_filter
            for column, value in [(models.Sample.resource_id, resource),
                                  (models.Sample.user_id, user),
                                  (models.Sample.project_id, project),
                                  (models.Sample.source_id, source)]:
                if value:
                    query = query.filter(column == value)
            if metaquery:
                query = apply_metaquery_filter(session, query, metaquery)
            return query

        session = self._engine_facade.get_session()

        # sample_subq is used to reduce sample records
        # by selecting a record for each (resource_id, meter_id).
        # max() is used to choice a sample record, so the latest record
        # is selected for each (resource_id, meter_id).
        sample_subq = session.query(
            func.max(models.Sample.id).label('id'))\
            .group_by(models.Sample.meter_id, models.Sample.resource_id)
        sample_subq = sample_subq.subquery()

        # SELECT sample.* FROM sample INNER JOIN
        #  (SELECT max(sample.id) AS id FROM sample
        #   GROUP BY sample.resource_id, sample.meter_id) AS anon_2
        # ON sample.id = anon_2.id
        query_sample = session.query(models.MeterSample).\
            join(sample_subq, models.MeterSample.id == sample_subq.c.id)
        query_sample = _apply_filters(query_sample)

        for sample in query_sample.all():
            yield api_models.Meter(
                name=sample.counter_name,
                type=sample.counter_type,
                unit=sample.counter_unit,
                resource_id=sample.resource_id,
                project_id=sample.project_id,
                source=sample.source_id,
                user_id=sample.user_id)

    def _retrieve_samples(self, query):
        samples = query.all()

        for s in samples:
            # Remove the id generated by the database when
            # the sample was inserted. It is an implementation
            # detail that should not leak outside of the driver.
            yield api_models.Sample(
                source=s.source_id,
                counter_name=s.counter_name,
                counter_type=s.counter_type,
                counter_unit=s.counter_unit,
                counter_volume=s.counter_volume,
                user_id=s.user_id,
                project_id=s.project_id,
                resource_id=s.resource_id,
                timestamp=s.timestamp,
                recorded_at=s.recorded_at,
                resource_metadata=s.resource_metadata,
                message_id=s.message_id,
                message_signature=s.message_signature,
            )

    def get_samples(self, sample_filter, limit=None):
        """Return an iterable of api_models.Samples.

        :param sample_filter: Filter.
        :param limit: Maximum number of results to return.
        """
        if limit == 0:
            return []

        table = models.MeterSample
        session = self._engine_facade.get_session()
        query = session.query(table)
        query = make_query_from_filter(session, query, sample_filter,
                                       require_meter=False)
        transformer = QueryTransformer(table, query)
        transformer.apply_options(None,
                                  limit)
        return self._retrieve_samples(transformer.get_query())

    def _retrieve_data(self, filter_expr, orderby, limit, table):
        if limit == 0:
            return []

        session = self._engine_facade.get_session()
        query = session.query(table)
        transformer = QueryTransformer(table, query)
        if filter_expr is not None:
            transformer.apply_filter(filter_expr)

        transformer.apply_options(orderby,
                                  limit)

        retrieve = {models.MeterSample: self._retrieve_samples,
                    models.Alarm: self._retrieve_alarms,
                    models.AlarmChange: self._retrieve_alarm_history}
        return retrieve[table](transformer.get_query())

    def query_samples(self, filter_expr=None, orderby=None, limit=None):
        return self._retrieve_data(filter_expr,
                                   orderby,
                                   limit,
                                   models.MeterSample)

    @staticmethod
    def _get_aggregate_functions(aggregate):
        if not aggregate:
            return [f for f in STANDARD_AGGREGATES.values()]

        functions = []

        for a in aggregate:
            if a.func in STANDARD_AGGREGATES:
                functions.append(STANDARD_AGGREGATES[a.func])
            elif a.func in UNPARAMETERIZED_AGGREGATES:
                functions.append(UNPARAMETERIZED_AGGREGATES[a.func])
            elif a.func in PARAMETERIZED_AGGREGATES['compute']:
                validate = PARAMETERIZED_AGGREGATES['validate'].get(a.func)
                if not (validate and validate(a.param)):
                    raise storage.StorageBadAggregate('Bad aggregate: %s.%s'
                                                      % (a.func, a.param))
                compute = PARAMETERIZED_AGGREGATES['compute'][a.func]
                functions.append(compute(a.param))
            else:
                raise NotImplementedError('Selectable aggregate function %s'
                                          ' is not supported' % a.func)

        return functions

    def _make_stats_query(self, sample_filter, groupby, aggregate):

        select = [
            models.Meter.unit,
            func.min(models.Sample.timestamp).label('tsmin'),
            func.max(models.Sample.timestamp).label('tsmax'),
        ]

        select.extend(self._get_aggregate_functions(aggregate))

        session = self._engine_facade.get_session()

        if groupby:
            group_attributes = [getattr(models.Sample, g) for g in groupby]
            select.extend(group_attributes)

        query = session.query(*select).filter(
            models.Meter.id == models.Sample.meter_id)\
            .group_by(models.Meter.unit)

        if groupby:
            query = query.group_by(*group_attributes)

        return make_query_from_filter(session, query, sample_filter)

    @staticmethod
    def _stats_result_aggregates(result, aggregate):
        stats_args = {}
        if isinstance(result.count, (int, long)):
            stats_args['count'] = result.count
        for attr in ['min', 'max', 'sum', 'avg']:
            if hasattr(result, attr):
                stats_args[attr] = getattr(result, attr)
        if aggregate:
            stats_args['aggregate'] = {}
            for a in aggregate:
                key = '%s%s' % (a.func, '/%s' % a.param if a.param else '')
                stats_args['aggregate'][key] = getattr(result, key)
        return stats_args

    @staticmethod
    def _stats_result_to_model(result, period, period_start,
                               period_end, groupby, aggregate):
        stats_args = Connection._stats_result_aggregates(result, aggregate)
        stats_args['unit'] = result.unit
        duration = (timeutils.delta_seconds(result.tsmin, result.tsmax)
                    if result.tsmin is not None and result.tsmax is not None
                    else None)
        stats_args['duration'] = duration
        stats_args['duration_start'] = result.tsmin
        stats_args['duration_end'] = result.tsmax
        stats_args['period'] = period
        stats_args['period_start'] = period_start
        stats_args['period_end'] = period_end
        stats_args['groupby'] = (dict(
            (g, getattr(result, g)) for g in groupby) if groupby else None)
        return api_models.Statistics(**stats_args)

    def get_meter_statistics(self, sample_filter, period=None, groupby=None,
                             aggregate=None):
        """Return an iterable of api_models.Statistics instances containing
        meter statistics described by the query parameters.

        The filter must have a meter value set.

        """
        if groupby:
            for group in groupby:
                if group not in ['user_id', 'project_id', 'resource_id']:
                    raise NotImplementedError('Unable to group by '
                                              'these fields')

        if not period:
            for res in self._make_stats_query(sample_filter,
                                              groupby,
                                              aggregate):
                if res.count:
                    yield self._stats_result_to_model(res, 0,
                                                      res.tsmin, res.tsmax,
                                                      groupby,
                                                      aggregate)
            return

        if not sample_filter.start or not sample_filter.end:
            res = self._make_stats_query(sample_filter,
                                         None,
                                         aggregate).first()
            if not res:
                # NOTE(liusheng):The 'res' may be NoneType, because no
                # sample has found with sample filter(s).
                return

        query = self._make_stats_query(sample_filter, groupby, aggregate)
        # HACK(jd) This is an awful method to compute stats by period, but
        # since we're trying to be SQL agnostic we have to write portable
        # code, so here it is, admire! We're going to do one request to get
        # stats by period. We would like to use GROUP BY, but there's no
        # portable way to manipulate timestamp in SQL, so we can't.
        for period_start, period_end in base.iter_period(
                sample_filter.start or res.tsmin,
                sample_filter.end or res.tsmax,
                period):
            q = query.filter(models.Sample.timestamp >= period_start)
            q = q.filter(models.Sample.timestamp < period_end)
            for r in q.all():
                if r.count:
                    yield self._stats_result_to_model(
                        result=r,
                        period=int(timeutils.delta_seconds(period_start,
                                                           period_end)),
                        period_start=period_start,
                        period_end=period_end,
                        groupby=groupby,
                        aggregate=aggregate
                    )

    @staticmethod
    def _row_to_alarm_model(row):
        return api_models.Alarm(alarm_id=row.alarm_id,
                                enabled=row.enabled,
                                type=row.type,
                                name=row.name,
                                description=row.description,
                                timestamp=row.timestamp,
                                user_id=row.user_id,
                                project_id=row.project_id,
                                state=row.state,
                                state_timestamp=row.state_timestamp,
                                ok_actions=row.ok_actions,
                                alarm_actions=row.alarm_actions,
                                insufficient_data_actions=
                                row.insufficient_data_actions,
                                rule=row.rule,
                                time_constraints=row.time_constraints,
                                repeat_actions=row.repeat_actions)

    def _retrieve_alarms(self, query):
        return (self._row_to_alarm_model(x) for x in query.all())

    def get_alarms(self, name=None, user=None,
                   project=None, enabled=None, alarm_id=None, pagination=None):
        """Yields a lists of alarms that match filters
        :param user: Optional ID for user that owns the resource.
        :param project: Optional ID for project that owns the resource.
        :param enabled: Optional boolean to list disable alarm.
        :param alarm_id: Optional alarm_id to return one alarm.
        :param pagination: Optional pagination query.
        """

        if pagination:
            raise NotImplementedError('Pagination not implemented')

        session = self._engine_facade.get_session()
        query = session.query(models.Alarm)
        if name is not None:
            query = query.filter(models.Alarm.name == name)
        if enabled is not None:
            query = query.filter(models.Alarm.enabled == enabled)
        if user is not None:
            query = query.filter(models.Alarm.user_id == user)
        if project is not None:
            query = query.filter(models.Alarm.project_id == project)
        if alarm_id is not None:
            query = query.filter(models.Alarm.alarm_id == alarm_id)

        return self._retrieve_alarms(query)

    def create_alarm(self, alarm):
        """Create an alarm.

        :param alarm: The alarm to create.
        """
        session = self._engine_facade.get_session()
        with session.begin():
            alarm_row = models.Alarm(alarm_id=alarm.alarm_id)
            alarm_row.update(alarm.as_dict())
            session.add(alarm_row)

        return self._row_to_alarm_model(alarm_row)

    def update_alarm(self, alarm):
        """Update an alarm.

        :param alarm: the new Alarm to update
        """
        session = self._engine_facade.get_session()
        with session.begin():
            alarm_row = session.merge(models.Alarm(alarm_id=alarm.alarm_id))
            alarm_row.update(alarm.as_dict())

        return self._row_to_alarm_model(alarm_row)

    def delete_alarm(self, alarm_id):
        """Delete an alarm

        :param alarm_id: ID of the alarm to delete
        """
        session = self._engine_facade.get_session()
        with session.begin():
            session.query(models.Alarm).filter(
                models.Alarm.alarm_id == alarm_id).delete()

    @staticmethod
    def _row_to_alarm_change_model(row):
        return api_models.AlarmChange(event_id=row.event_id,
                                      alarm_id=row.alarm_id,
                                      type=row.type,
                                      detail=row.detail,
                                      user_id=row.user_id,
                                      project_id=row.project_id,
                                      on_behalf_of=row.on_behalf_of,
                                      timestamp=row.timestamp)

    def query_alarms(self, filter_expr=None, orderby=None, limit=None):
        """Yields a lists of alarms that match filter
        """
        return self._retrieve_data(filter_expr, orderby, limit, models.Alarm)

    def _retrieve_alarm_history(self, query):
        return (self._row_to_alarm_change_model(x) for x in query.all())

    def query_alarm_history(self, filter_expr=None, orderby=None, limit=None):
        """Return an iterable of model.AlarmChange objects.
        """
        return self._retrieve_data(filter_expr,
                                   orderby,
                                   limit,
                                   models.AlarmChange)

    def get_alarm_changes(self, alarm_id, on_behalf_of,
                          user=None, project=None, type=None,
                          start_timestamp=None, start_timestamp_op=None,
                          end_timestamp=None, end_timestamp_op=None):
        """Yields list of AlarmChanges describing alarm history

        Changes are always sorted in reverse order of occurrence, given
        the importance of currency.

        Segregation for non-administrative users is done on the basis
        of the on_behalf_of parameter. This allows such users to have
        visibility on both the changes initiated by themselves directly
        (generally creation, rule changes, or deletion) and also on those
        changes initiated on their behalf by the alarming service (state
        transitions after alarm thresholds are crossed).

        :param alarm_id: ID of alarm to return changes for
        :param on_behalf_of: ID of tenant to scope changes query (None for
                             administrative user, indicating all projects)
        :param user: Optional ID of user to return changes for
        :param project: Optional ID of project to return changes for
        :project type: Optional change type
        :param start_timestamp: Optional modified timestamp start range
        :param start_timestamp_op: Optional timestamp start range operation
        :param end_timestamp: Optional modified timestamp end range
        :param end_timestamp_op: Optional timestamp end range operation
        """
        session = self._engine_facade.get_session()
        query = session.query(models.AlarmChange)
        query = query.filter(models.AlarmChange.alarm_id == alarm_id)

        if on_behalf_of is not None:
            query = query.filter(
                models.AlarmChange.on_behalf_of == on_behalf_of)
        if user is not None:
            query = query.filter(models.AlarmChange.user_id == user)
        if project is not None:
            query = query.filter(models.AlarmChange.project_id == project)
        if type is not None:
            query = query.filter(models.AlarmChange.type == type)
        if start_timestamp:
            if start_timestamp_op == 'gt':
                query = query.filter(
                    models.AlarmChange.timestamp > start_timestamp)
            else:
                query = query.filter(
                    models.AlarmChange.timestamp >= start_timestamp)
        if end_timestamp:
            if end_timestamp_op == 'le':
                query = query.filter(
                    models.AlarmChange.timestamp <= end_timestamp)
            else:
                query = query.filter(
                    models.AlarmChange.timestamp < end_timestamp)

        query = query.order_by(desc(models.AlarmChange.timestamp))
        return self._retrieve_alarm_history(query)

    def record_alarm_change(self, alarm_change):
        """Record alarm change event.
        """
        session = self._engine_facade.get_session()
        with session.begin():
            alarm_change_row = models.AlarmChange(
                event_id=alarm_change['event_id'])
            alarm_change_row.update(alarm_change)
            session.add(alarm_change_row)

    def _get_or_create_trait_type(self, trait_type, data_type, session=None):
        """Find if this trait already exists in the database, and
        if it does not, create a new entry in the trait type table.
        """
        if session is None:
            session = self._engine_facade.get_session()
        with session.begin(subtransactions=True):
            tt = session.query(models.TraitType).filter(
                models.TraitType.desc == trait_type,
                models.TraitType.data_type == data_type).first()
            if not tt:
                tt = models.TraitType(trait_type, data_type)
                session.add(tt)
        return tt

    def _make_trait(self, trait_model, event, session=None):
        """Make a new Trait from a Trait model.

        Doesn't flush or add to session.
        """
        trait_type = self._get_or_create_trait_type(trait_model.name,
                                                    trait_model.dtype,
                                                    session)
        value_map = models.Trait._value_map
        values = {'t_string': None, 't_float': None,
                  't_int': None, 't_datetime': None}
        value = trait_model.value
        values[value_map[trait_model.dtype]] = value
        return models.Trait(trait_type, event, **values)

    def _get_or_create_event_type(self, event_type, session=None):
        """Here, we check to see if an event type with the supplied
        name already exists. If not, we create it and return the record.

        This may result in a flush.
        """
        if session is None:
            session = self._engine_facade.get_session()
        with session.begin(subtransactions=True):
            et = session.query(models.EventType).filter(
                models.EventType.desc == event_type).first()
            if not et:
                et = models.EventType(event_type)
                session.add(et)
        return et

    def _record_event(self, session, event_model):
        """Store a single Event, including related Traits.
        """
        with session.begin(subtransactions=True):
            event_type = self._get_or_create_event_type(event_model.event_type,
                                                        session=session)

            event = models.Event(event_model.message_id, event_type,
                                 event_model.generated)
            session.add(event)

            new_traits = []
            if event_model.traits:
                for trait in event_model.traits:
                    t = self._make_trait(trait, event, session=session)
                    session.add(t)
                    new_traits.append(t)

        # Note: we don't flush here, explicitly (unless a new trait or event
        # does it). Otherwise, just wait until all the Events are staged.
        return (event, new_traits)

    def record_events(self, event_models):
        """Write the events to SQL database via sqlalchemy.

        :param event_models: a list of model.Event objects.

        Returns a list of events that could not be saved in a
        (reason, event) tuple. Reasons are enumerated in
        storage.model.Event

        Flush when they're all added, unless new EventTypes or
        TraitTypes are added along the way.
        """
        session = self._engine_facade.get_session()
        events = []
        problem_events = []
        for event_model in event_models:
            event = None
            try:
                with session.begin():
                    event = self._record_event(session, event_model)
            except dbexc.DBDuplicateEntry:
                problem_events.append((api_models.Event.DUPLICATE,
                                       event_model))
            except Exception as e:
                LOG.exception(_('Failed to record event: %s') % e)
                problem_events.append((api_models.Event.UNKNOWN_PROBLEM,
                                       event_model))
            events.append(event)
        return problem_events

    def get_events(self, event_filter):
        """Return an iterable of model.Event objects.

        :param event_filter: EventFilter instance
        """

        start = event_filter.start_time
        end = event_filter.end_time
        session = self._engine_facade.get_session()
        LOG.debug(_("Getting events that match filter: %s") % event_filter)
        with session.begin():
            event_query = session.query(models.Event)

            # Build up the join conditions
            event_join_conditions = [models.EventType.id ==
                                     models.Event.event_type_id]

            if event_filter.event_type:
                event_join_conditions\
                    .append(models.EventType.desc == event_filter.event_type)

            event_query = event_query.join(models.EventType,
                                           and_(*event_join_conditions))

            # Build up the where conditions
            event_filter_conditions = []
            if event_filter.message_id:
                event_filter_conditions\
                    .append(models.Event.message_id == event_filter.message_id)
            if start:
                event_filter_conditions.append(models.Event.generated >= start)
            if end:
                event_filter_conditions.append(models.Event.generated <= end)

            if event_filter_conditions:
                event_query = event_query\
                    .filter(and_(*event_filter_conditions))

            event_models_dict = {}
            if event_filter.traits_filter:
                for trait_filter in event_filter.traits_filter:

                    # Build a sub query that joins Trait to TraitType
                    # where the trait name matches
                    trait_name = trait_filter.pop('key')
                    conditions = [models.Trait.trait_type_id ==
                                  models.TraitType.id,
                                  models.TraitType.desc == trait_name]

                    for key, value in trait_filter.iteritems():
                        if key == 'string':
                            conditions.append(models.Trait.t_string == value)
                        elif key == 'integer':
                            conditions.append(models.Trait.t_int == value)
                        elif key == 'datetime':
                            conditions.append(models.Trait.t_datetime == value)
                        elif key == 'float':
                            conditions.append(models.Trait.t_float == value)

                    trait_query = session.query(models.Trait.event_id)\
                        .join(models.TraitType, and_(*conditions)).subquery()

                    event_query = event_query\
                        .join(trait_query,
                              models.Event.id == trait_query.c.event_id)
            else:
                # If there are no trait filters, grab the events from the db
                query = session.query(models.Event.id,
                                      models.Event.generated,
                                      models.Event.message_id,
                                      models.EventType.desc)\
                    .join(models.EventType,
                          and_(*event_join_conditions))
                if event_filter_conditions:
                    query = query.filter(and_(*event_filter_conditions))
                for (id, generated, message_id, desc) in query.all():
                    event_models_dict[id] = api_models.Event(message_id,
                                                             desc,
                                                             generated,
                                                             [])

            # Build event models for the events
            event_query = event_query.subquery()
            query = session.query(models.Trait)\
                .join(models.TraitType,
                      models.Trait.trait_type_id == models.TraitType.id)\
                .join(event_query, models.Trait.event_id == event_query.c.id)

            # Now convert the sqlalchemy objects back into Models ...
            for trait in query.all():
                event = event_models_dict.get(trait.event_id)
                if not event:
                    event = api_models.Event(
                        trait.event.message_id,
                        trait.event.event_type.desc,
                        trait.event.generated, [])
                    event_models_dict[trait.event_id] = event
                trait_model = api_models.Trait(trait.trait_type.desc,
                                               trait.trait_type.data_type,
                                               trait.get_value())
                event.append_trait(trait_model)

        event_models = event_models_dict.values()
        return sorted(event_models, key=operator.attrgetter('generated'))

    def get_event_types(self):
        """Return all event types as an iterable of strings.
        """

        session = self._engine_facade.get_session()
        with session.begin():
            query = session.query(models.EventType.desc)\
                .order_by(models.EventType.desc)
            for name in query.all():
                # The query returns a tuple with one element.
                yield name[0]

    def get_trait_types(self, event_type):
        """Return a dictionary containing the name and data type of
        the trait type. Only trait types for the provided event_type are
        returned.

        :param event_type: the type of the Event
        """
        session = self._engine_facade.get_session()

        LOG.debug(_("Get traits for %s") % event_type)
        with session.begin():
            query = (session.query(models.TraitType.desc,
                                   models.TraitType.data_type)
                     .join(models.Trait,
                           models.Trait.trait_type_id ==
                           models.TraitType.id)
                     .join(models.Event,
                           models.Event.id ==
                           models.Trait.event_id)
                     .join(models.EventType,
                           and_(models.EventType.id ==
                                models.Event.id,
                                models.EventType.desc ==
                                event_type))
                     .group_by(models.TraitType.desc,
                               models.TraitType.data_type)
                     .distinct())

            for desc, type in query.all():
                yield {'name': desc, 'data_type': type}

    def get_traits(self, event_type, trait_type=None):
        """Return all trait instances associated with an event_type. If
        trait_type is specified, only return instances of that trait type.

        :param event_type: the type of the Event to filter by
        :param trait_type: the name of the Trait to filter by
        """

        session = self._engine_facade.get_session()
        with session.begin():
            trait_type_filters = [models.TraitType.id ==
                                  models.Trait.trait_type_id]
            if trait_type:
                trait_type_filters.append(models.TraitType.desc == trait_type)

            query = (session.query(models.Trait)
                     .join(models.TraitType, and_(*trait_type_filters))
                     .join(models.Event,
                           models.Event.id == models.Trait.event_id)
                     .join(models.EventType,
                           and_(models.EventType.id ==
                                models.Event.event_type_id,
                                models.EventType.desc == event_type)))

            for trait in query.all():
                type = trait.trait_type
                yield api_models.Trait(name=type.desc,
                                       dtype=type.data_type,
                                       value=trait.get_value())


class QueryTransformer(object):
    operators = {"=": operator.eq,
                 "<": operator.lt,
                 ">": operator.gt,
                 "<=": operator.le,
                 "=<": operator.le,
                 ">=": operator.ge,
                 "=>": operator.ge,
                 "!=": operator.ne,
                 "in": lambda field_name, values: field_name.in_(values)}

    complex_operators = {"or": or_,
                         "and": and_,
                         "not": not_}

    ordering_functions = {"asc": asc,
                          "desc": desc}

    def __init__(self, table, query):
        self.table = table
        self.query = query

    def _handle_complex_op(self, complex_op, nodes):
        op = self.complex_operators[complex_op]
        if op == not_:
            nodes = [nodes]
        element_list = []
        for node in nodes:
            element = self._transform(node)
            element_list.append(element)
        return op(*element_list)

    def _handle_simple_op(self, simple_op, nodes):
        op = self.operators[simple_op]
        field_name = nodes.keys()[0]
        value = nodes.values()[0]
        if field_name.startswith('resource_metadata.'):
            return self._handle_metadata(op, field_name, value)
        else:
            return op(getattr(self.table, field_name), value)

    def _handle_metadata(self, op, field_name, value):
        if op == self.operators["in"]:
            raise NotImplementedError('Metadata query with in '
                                      'operator is not implemented')

        field_name = field_name[len('resource_metadata.'):]
        meta_table = META_TYPE_MAP[type(value)]
        meta_alias = aliased(meta_table)
        on_clause = and_(self.table.id == meta_alias.id,
                         meta_alias.meta_key == field_name)
        # outer join is needed to support metaquery
        # with or operator on non existent metadata field
        # see: test_query_non_existing_metadata_with_result
        # test case.
        self.query = self.query.outerjoin(meta_alias, on_clause)
        return op(meta_alias.value, value)

    def _transform(self, sub_tree):
        operator = sub_tree.keys()[0]
        nodes = sub_tree.values()[0]
        if operator in self.complex_operators:
            return self._handle_complex_op(operator, nodes)
        else:
            return self._handle_simple_op(operator, nodes)

    def apply_filter(self, expression_tree):
        condition = self._transform(expression_tree)
        self.query = self.query.filter(condition)

    def apply_options(self, orderby, limit):
        self._apply_order_by(orderby)
        if limit is not None:
            self.query = self.query.limit(limit)

    def _apply_order_by(self, orderby):
        if orderby is not None:
            for field in orderby:
                ordering_function = self.ordering_functions[field.values()[0]]
                self.query = self.query.order_by(ordering_function(
                    getattr(self.table, field.keys()[0])))
        else:
            self.query = self.query.order_by(desc(self.table.timestamp))

    def get_query(self):
        return self.query

########NEW FILE########
__FILENAME__ = models
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Model classes for use in the storage API.
"""
import inspect

from ceilometer.openstack.common import timeutils


class Model(object):
    """Base class for storage API models.
    """

    def __init__(self, **kwds):
        self.fields = list(kwds)
        for k, v in kwds.iteritems():
            setattr(self, k, v)

    def as_dict(self):
        d = {}
        for f in self.fields:
            v = getattr(self, f)
            if isinstance(v, Model):
                v = v.as_dict()
            elif isinstance(v, list) and v and isinstance(v[0], Model):
                v = [sub.as_dict() for sub in v]
            d[f] = v
        return d

    def __eq__(self, other):
        return self.as_dict() == other.as_dict()

    @classmethod
    def get_field_names(cls):
        fields = inspect.getargspec(cls.__init__)[0]
        return set(fields) - set(["self"])


class Event(Model):
    """A raw event from the source system. Events have Traits.

       Metrics will be derived from one or more Events.
    """

    DUPLICATE = 1
    UNKNOWN_PROBLEM = 2

    def __init__(self, message_id, event_type, generated, traits):
        """Create a new event.

        :param message_id:  Unique ID for the message this event
                            stemmed from. This is different than
                            the Event ID, which comes from the
                            underlying storage system.
        :param event_type:  The type of the event.
        :param generated:   UTC time for when the event occurred.
        :param traits:      list of Traits on this Event.
        """
        Model.__init__(self, message_id=message_id, event_type=event_type,
                       generated=generated, traits=traits)

    def append_trait(self, trait_model):
        self.traits.append(trait_model)

    def __repr__(self):
        trait_list = []
        if self.traits:
            trait_list = [str(trait) for trait in self.traits]
        return "<Event: %s, %s, %s, %s>" % \
            (self.message_id, self.event_type, self.generated,
             " ".join(trait_list))


class Trait(Model):
    """A Trait is a key/value pair of data on an Event. The value is variant
    record of basic data types (int, date, float, etc).
    """

    NONE_TYPE = 0
    TEXT_TYPE = 1
    INT_TYPE = 2
    FLOAT_TYPE = 3
    DATETIME_TYPE = 4

    type_names = {
        NONE_TYPE: "none",
        TEXT_TYPE: "string",
        INT_TYPE: "integer",
        FLOAT_TYPE: "float",
        DATETIME_TYPE: "datetime"
    }

    def __init__(self, name, dtype, value):
        if not dtype:
            dtype = Trait.NONE_TYPE
        Model.__init__(self, name=name, dtype=dtype, value=value)

    def __repr__(self):
        return "<Trait: %s %d %s>" % (self.name, self.dtype, self.value)

    def get_type_name(self):
        return self.get_name_by_type(self.dtype)

    @classmethod
    def get_type_by_name(cls, type_name):
        return getattr(cls, '%s_TYPE' % type_name.upper(), None)

    @classmethod
    def get_type_names(cls):
        return cls.type_names.values()

    @classmethod
    def get_name_by_type(cls, type_id):
        return cls.type_names.get(type_id, "none")

    @classmethod
    def convert_value(cls, trait_type, value):
        if trait_type is cls.INT_TYPE:
            return int(value)
        if trait_type is cls.FLOAT_TYPE:
            return float(value)
        if trait_type is cls.DATETIME_TYPE:
            return timeutils.normalize_time(timeutils.parse_isotime(value))
        return str(value)


class Resource(Model):
    """Something for which sample data has been collected.
    """

    def __init__(self, resource_id, project_id,
                 first_sample_timestamp,
                 last_sample_timestamp,
                 source, user_id, metadata):
        """Create a new resource.

        :param resource_id: UUID of the resource
        :param project_id:  UUID of project owning the resource
        :param first_sample_timestamp: first sample timestamp captured
        :param last_sample_timestamp: last sample timestamp captured
        :param source:      the identifier for the user/project id definition
        :param user_id:     UUID of user owning the resource
        :param metadata:    most current metadata for the resource (a dict)
        """
        Model.__init__(self,
                       resource_id=resource_id,
                       first_sample_timestamp=first_sample_timestamp,
                       last_sample_timestamp=last_sample_timestamp,
                       project_id=project_id,
                       source=source,
                       user_id=user_id,
                       metadata=metadata,
                       )


class Meter(Model):
    """Definition of a meter for which sample data has been collected.
    """

    def __init__(self, name, type, unit, resource_id, project_id, source,
                 user_id):
        """Create a new meter.

        :param name: name of the meter
        :param type: type of the meter (gauge, delta, cumulative)
        :param unit: unit of the meter
        :param resource_id: UUID of the resource
        :param project_id: UUID of project owning the resource
        :param source: the identifier for the user/project id definition
        :param user_id: UUID of user owning the resource
        """
        Model.__init__(self,
                       name=name,
                       type=type,
                       unit=unit,
                       resource_id=resource_id,
                       project_id=project_id,
                       source=source,
                       user_id=user_id,
                       )


class Sample(Model):
    """One collected data point.
    """
    def __init__(self,
                 source,
                 counter_name, counter_type, counter_unit, counter_volume,
                 user_id, project_id, resource_id,
                 timestamp, resource_metadata,
                 message_id,
                 message_signature,
                 recorded_at,
                 ):
        """Create a new sample.

        :param source: the identifier for the user/project id definition
        :param counter_name: the name of the measurement being taken
        :param counter_type: the type of the measurement
        :param counter_unit: the units for the measurement
        :param counter_volume: the measured value
        :param user_id: the user that triggered the measurement
        :param project_id: the project that owns the resource
        :param resource_id: the thing on which the measurement was taken
        :param timestamp: the time of the measurement
        :param resource_metadata: extra details about the resource
        :param message_id: a message identifier
        :param recorded_at: sample record timestamp
        :param message_signature: a hash created from the rest of the
                                  message data
        """
        Model.__init__(self,
                       source=source,
                       counter_name=counter_name,
                       counter_type=counter_type,
                       counter_unit=counter_unit,
                       counter_volume=counter_volume,
                       user_id=user_id,
                       project_id=project_id,
                       resource_id=resource_id,
                       timestamp=timestamp,
                       resource_metadata=resource_metadata,
                       message_id=message_id,
                       message_signature=message_signature,
                       recorded_at=recorded_at)


class Statistics(Model):
    """Computed statistics based on a set of sample data.
    """
    def __init__(self, unit,
                 period, period_start, period_end,
                 duration, duration_start, duration_end,
                 groupby, **data):
        """Create a new statistics object.

        :param unit: The unit type of the data set
        :param period: The length of the time range covered by these stats
        :param period_start: The timestamp for the start of the period
        :param period_end: The timestamp for the end of the period
        :param duration: The total time for the matching samples
        :param duration_start: The earliest time for the matching samples
        :param duration_end: The latest time for the matching samples
        :param groupby: The fields used to group the samples.
        :param data: some or all of the following aggregates
           min: The smallest volume found
           max: The largest volume found
           avg: The average of all volumes found
           sum: The total of all volumes found
           count: The number of samples found
           aggregate: name-value pairs for selectable aggregates
        """
        Model.__init__(self, unit=unit,
                       period=period, period_start=period_start,
                       period_end=period_end, duration=duration,
                       duration_start=duration_start,
                       duration_end=duration_end,
                       groupby=groupby,
                       **data)


class Alarm(Model):
    ALARM_INSUFFICIENT_DATA = 'insufficient data'
    ALARM_OK = 'ok'
    ALARM_ALARM = 'alarm'

    ALARM_ACTIONS_MAP = {
        ALARM_INSUFFICIENT_DATA: 'insufficient_data_actions',
        ALARM_OK: 'ok_actions',
        ALARM_ALARM: 'alarm_actions',
    }

    """
    An alarm to monitor.

    :param alarm_id: UUID of the alarm
    :param type: type of the alarm
    :param name: The Alarm name
    :param description: User friendly description of the alarm
    :param enabled: Is the alarm enabled
    :param state: Alarm state (ok/alarm/insufficient data)
    :param rule: A rule that defines when the alarm fires
    :param user_id: the owner/creator of the alarm
    :param project_id: the project_id of the creator
    :param evaluation_periods: the number of periods
    :param period: the time period in seconds
    :param time_constraints: the list of the alarm's time constraints, if any
    :param timestamp: the timestamp when the alarm was last updated
    :param state_timestamp: the timestamp of the last state change
    :param ok_actions: the list of webhooks to call when entering the ok state
    :param alarm_actions: the list of webhooks to call when entering the
                          alarm state
    :param insufficient_data_actions: the list of webhooks to call when
                                      entering the insufficient data state
    :param repeat_actions: Is the actions should be triggered on each
                           alarm evaluation.
    """
    def __init__(self, alarm_id, type, enabled, name, description,
                 timestamp, user_id, project_id, state, state_timestamp,
                 ok_actions, alarm_actions, insufficient_data_actions,
                 repeat_actions, rule, time_constraints):
        Model.__init__(
            self,
            alarm_id=alarm_id,
            type=type,
            enabled=enabled,
            name=name,
            description=description,
            timestamp=timestamp,
            user_id=user_id,
            project_id=project_id,
            state=state,
            state_timestamp=state_timestamp,
            ok_actions=ok_actions,
            alarm_actions=alarm_actions,
            insufficient_data_actions=
            insufficient_data_actions,
            repeat_actions=repeat_actions,
            rule=rule,
            time_constraints=time_constraints)


class AlarmChange(Model):
    """Record of an alarm change.

    :param event_id: UUID of the change event
    :param alarm_id: UUID of the alarm
    :param type: The type of change
    :param detail: JSON fragment describing change
    :param user_id: the user ID of the initiating identity
    :param project_id: the project ID of the initiating identity
    :param on_behalf_of: the tenant on behalf of which the change
                         is being made
    :param timestamp: the timestamp of the change
    """

    CREATION = 'creation'
    RULE_CHANGE = 'rule change'
    STATE_TRANSITION = 'state transition'
    DELETION = 'deletion'

    def __init__(self,
                 event_id,
                 alarm_id,
                 type,
                 detail,
                 user_id,
                 project_id,
                 on_behalf_of,
                 timestamp=None
                 ):
        Model.__init__(
            self,
            event_id=event_id,
            alarm_id=alarm_id,
            type=type,
            detail=detail,
            user_id=user_id,
            project_id=project_id,
            on_behalf_of=on_behalf_of,
            timestamp=timestamp)

########NEW FILE########
__FILENAME__ = pymongo_base
# -*- encoding: utf-8 -*-
#
# Copyright Ericsson AB 2013. All rights reserved
#
# Authors: Ildiko Vancsa <ildiko.vancsa@ericsson.com>
#          Balazs Gibizer <balazs.gibizer@ericsson.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Common functions for MongoDB and DB2 backends
"""

import pymongo
import weakref

from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer.openstack.common import network_utils
from ceilometer.storage import base
from ceilometer.storage import models
from ceilometer import utils

LOG = log.getLogger(__name__)


def make_timestamp_range(start, end,
                         start_timestamp_op=None, end_timestamp_op=None):

    """Given two possible datetimes and their operations, create the query
    document to find timestamps within that range.
    By default, using $gte for the lower bound and $lt for the
    upper bound.
    """
    ts_range = {}

    if start:
        if start_timestamp_op == 'gt':
            start_timestamp_op = '$gt'
        else:
            start_timestamp_op = '$gte'
        ts_range[start_timestamp_op] = start

    if end:
        if end_timestamp_op == 'le':
            end_timestamp_op = '$lte'
        else:
            end_timestamp_op = '$lt'
        ts_range[end_timestamp_op] = end
    return ts_range


def make_query_from_filter(sample_filter, require_meter=True):
    """Return a query dictionary based on the settings in the filter.

    :param filter: SampleFilter instance
    :param require_meter: If true and the filter does not have a meter,
                          raise an error.
    """
    q = {}

    if sample_filter.user:
        q['user_id'] = sample_filter.user
    if sample_filter.project:
        q['project_id'] = sample_filter.project

    if sample_filter.meter:
        q['counter_name'] = sample_filter.meter
    elif require_meter:
        raise RuntimeError('Missing required meter specifier')

    ts_range = make_timestamp_range(sample_filter.start,
                                    sample_filter.end,
                                    sample_filter.start_timestamp_op,
                                    sample_filter.end_timestamp_op)

    if ts_range:
        q['timestamp'] = ts_range

    if sample_filter.resource:
        q['resource_id'] = sample_filter.resource
    if sample_filter.source:
        q['source'] = sample_filter.source
    if sample_filter.message_id:
        q['message_id'] = sample_filter.message_id

    # so the samples call metadata resource_metadata, so we convert
    # to that.
    q.update(dict(('resource_%s' % k, v)
                  for (k, v) in sample_filter.metaquery.iteritems()))
    return q


class ConnectionPool(object):

    def __init__(self):
        self._pool = {}

    def connect(self, url):
        connection_options = pymongo.uri_parser.parse_uri(url)
        del connection_options['database']
        del connection_options['username']
        del connection_options['password']
        del connection_options['collection']
        pool_key = tuple(connection_options)

        if pool_key in self._pool:
            client = self._pool.get(pool_key)()
            if client:
                return client
        splitted_url = network_utils.urlsplit(url)
        log_data = {'db': splitted_url.scheme,
                    'nodelist': connection_options['nodelist']}
        LOG.info(_('Connecting to %(db)s on %(nodelist)s') % log_data)
        client = pymongo.MongoClient(
            url,
            safe=True)
        self._pool[pool_key] = weakref.ref(client)
        return client


COMMON_AVAILABLE_CAPABILITIES = {
    'meters': {'query': {'simple': True,
                         'metadata': True}},
    'samples': {'query': {'simple': True,
                          'metadata': True,
                          'complex': True}},
    'alarms': {'query': {'simple': True,
                         'complex': True},
               'history': {'query': {'simple': True,
                                     'complex': True}}},
}


class Connection(base.Connection):
    """Base Connection class for MongoDB and DB2 drivers.
    """
    CAPABILITIES = utils.update_nested(base.Connection.CAPABILITIES,
                                       COMMON_AVAILABLE_CAPABILITIES)

    def get_meters(self, user=None, project=None, resource=None, source=None,
                   metaquery=None, pagination=None):
        """Return an iterable of models.Meter instances

        :param user: Optional ID for user that owns the resource.
        :param project: Optional ID for project that owns the resource.
        :param resource: Optional resource filter.
        :param source: Optional source filter.
        :param metaquery: Optional dict with metadata to match on.
        :param pagination: Optional pagination query.
        """

        if pagination:
            raise NotImplementedError('Pagination not implemented')

        metaquery = metaquery or {}

        q = {}
        if user is not None:
            q['user_id'] = user
        if project is not None:
            q['project_id'] = project
        if resource is not None:
            q['_id'] = resource
        if source is not None:
            q['source'] = source
        q.update(metaquery)

        for r in self.db.resource.find(q):
            for r_meter in r['meter']:
                yield models.Meter(
                    name=r_meter['counter_name'],
                    type=r_meter['counter_type'],
                    # Return empty string if 'counter_unit' is not valid for
                    # backward compatibility.
                    unit=r_meter.get('counter_unit', ''),
                    resource_id=r['_id'],
                    project_id=r['project_id'],
                    source=r['source'],
                    user_id=r['user_id'],
                )

    def update_alarm(self, alarm):
        """update alarm
        """
        data = alarm.as_dict()

        self.db.alarm.update(
            {'alarm_id': alarm.alarm_id},
            {'$set': data},
            upsert=True)

        stored_alarm = self.db.alarm.find({'alarm_id': alarm.alarm_id})[0]
        del stored_alarm['_id']
        self._ensure_encapsulated_rule_format(stored_alarm)
        self._ensure_time_constraints(stored_alarm)
        return models.Alarm(**stored_alarm)

    create_alarm = update_alarm

    def delete_alarm(self, alarm_id):
        """Delete an alarm
        """
        self.db.alarm.remove({'alarm_id': alarm_id})

    def record_alarm_change(self, alarm_change):
        """Record alarm change event.
        """
        self.db.alarm_history.insert(alarm_change.copy())

    def get_samples(self, sample_filter, limit=None):
        """Return an iterable of model.Sample instances.

        :param sample_filter: Filter.
        :param limit: Maximum number of results to return.
        """
        if limit == 0:
            return []
        q = make_query_from_filter(sample_filter,
                                   require_meter=False)

        return self._retrieve_samples(q,
                                      [("timestamp", pymongo.DESCENDING)],
                                      limit)

    def get_alarms(self, name=None, user=None,
                   project=None, enabled=None, alarm_id=None, pagination=None):
        """Yields a lists of alarms that match filters
        :param name: The Alarm name.
        :param user: Optional ID for user that owns the resource.
        :param project: Optional ID for project that owns the resource.
        :param enabled: Optional boolean to list disable alarm.
        :param alarm_id: Optional alarm_id to return one alarm.
        :param pagination: Optional pagination query.
        """
        if pagination:
            raise NotImplementedError('Pagination not implemented')

        q = {}
        if user is not None:
            q['user_id'] = user
        if project is not None:
            q['project_id'] = project
        if name is not None:
            q['name'] = name
        if enabled is not None:
            q['enabled'] = enabled
        if alarm_id is not None:
            q['alarm_id'] = alarm_id

        return self._retrieve_alarms(q, [], None)

    def get_alarm_changes(self, alarm_id, on_behalf_of,
                          user=None, project=None, type=None,
                          start_timestamp=None, start_timestamp_op=None,
                          end_timestamp=None, end_timestamp_op=None):
        """Yields list of AlarmChanges describing alarm history

        Changes are always sorted in reverse order of occurrence, given
        the importance of currency.

        Segregation for non-administrative users is done on the basis
        of the on_behalf_of parameter. This allows such users to have
        visibility on both the changes initiated by themselves directly
        (generally creation, rule changes, or deletion) and also on those
        changes initiated on their behalf by the alarming service (state
        transitions after alarm thresholds are crossed).

        :param alarm_id: ID of alarm to return changes for
        :param on_behalf_of: ID of tenant to scope changes query (None for
                             administrative user, indicating all projects)
        :param user: Optional ID of user to return changes for
        :param project: Optional ID of project to return changes for
        :project type: Optional change type
        :param start_timestamp: Optional modified timestamp start range
        :param start_timestamp_op: Optional timestamp start range operation
        :param end_timestamp: Optional modified timestamp end range
        :param end_timestamp_op: Optional timestamp end range operation
        """
        q = dict(alarm_id=alarm_id)
        if on_behalf_of is not None:
            q['on_behalf_of'] = on_behalf_of
        if user is not None:
            q['user_id'] = user
        if project is not None:
            q['project_id'] = project
        if type is not None:
            q['type'] = type
        if start_timestamp or end_timestamp:
            ts_range = make_timestamp_range(start_timestamp,
                                            end_timestamp,
                                            start_timestamp_op,
                                            end_timestamp_op)
            if ts_range:
                q['timestamp'] = ts_range

        return self._retrieve_alarm_changes(q,
                                            [("timestamp",
                                              pymongo.DESCENDING)],
                                            None)

    def query_samples(self, filter_expr=None, orderby=None, limit=None):
        return self._retrieve_data(filter_expr, orderby, limit, models.Meter)

    def query_alarms(self, filter_expr=None, orderby=None, limit=None):
        """Return an iterable of model.Alarm objects.
        """
        return self._retrieve_data(filter_expr, orderby, limit, models.Alarm)

    def query_alarm_history(self, filter_expr=None, orderby=None, limit=None):
        """Return an iterable of model.AlarmChange objects.
        """
        return self._retrieve_data(filter_expr,
                                   orderby,
                                   limit,
                                   models.AlarmChange)

    def _retrieve_data(self, filter_expr, orderby, limit, model):
        if limit == 0:
            return []
        query_filter = {}
        orderby_filter = [("timestamp", pymongo.DESCENDING)]
        transformer = QueryTransformer()
        if orderby is not None:
            orderby_filter = transformer.transform_orderby(orderby)
        if filter_expr is not None:
            query_filter = transformer.transform_filter(filter_expr)

        retrieve = {models.Meter: self._retrieve_samples,
                    models.Alarm: self._retrieve_alarms,
                    models.AlarmChange: self._retrieve_alarm_changes}
        return retrieve[model](query_filter, orderby_filter, limit)

    def _retrieve_samples(self, query, orderby, limit):
        if limit is not None:
            samples = self.db.meter.find(query,
                                         limit=limit,
                                         sort=orderby)
        else:
            samples = self.db.meter.find(query,
                                         sort=orderby)

        for s in samples:
            # Remove the ObjectId generated by the database when
            # the sample was inserted. It is an implementation
            # detail that should not leak outside of the driver.
            del s['_id']
            # Backward compatibility for samples without units
            s['counter_unit'] = s.get('counter_unit', '')
            # Tolerate absence of recorded_at in older datapoints
            s['recorded_at'] = s.get('recorded_at')
            yield models.Sample(**s)

    def _retrieve_alarms(self, query_filter, orderby, limit):
        if limit is not None:
            alarms = self.db.alarm.find(query_filter,
                                        limit=limit,
                                        sort=orderby)
        else:
            alarms = self.db.alarm.find(query_filter, sort=orderby)

        for alarm in alarms:
            a = {}
            a.update(alarm)
            del a['_id']
            self._ensure_encapsulated_rule_format(a)
            self._ensure_time_constraints(a)
            yield models.Alarm(**a)

    def _retrieve_alarm_changes(self, query_filter, orderby, limit):
        if limit is not None:
            alarms_history = self.db.alarm_history.find(query_filter,
                                                        limit=limit,
                                                        sort=orderby)
        else:
            alarms_history = self.db.alarm_history.find(
                query_filter, sort=orderby)

        for alarm_history in alarms_history:
            ah = {}
            ah.update(alarm_history)
            del ah['_id']
            yield models.AlarmChange(**ah)

    @classmethod
    def _ensure_encapsulated_rule_format(cls, alarm):
        """This ensure the alarm returned by the storage have the correct
        format. The previous format looks like:
        {
            'alarm_id': '0ld-4l3rt',
            'enabled': True,
            'name': 'old-alert',
            'description': 'old-alert',
            'timestamp': None,
            'meter_name': 'cpu',
            'user_id': 'me',
            'project_id': 'and-da-boys',
            'comparison_operator': 'lt',
            'threshold': 36,
            'statistic': 'count',
            'evaluation_periods': 1,
            'period': 60,
            'state': "insufficient data",
            'state_timestamp': None,
            'ok_actions': [],
            'alarm_actions': ['http://nowhere/alarms'],
            'insufficient_data_actions': [],
            'repeat_actions': False,
            'matching_metadata': {'key': 'value'}
            # or 'matching_metadata': [{'key': 'key', 'value': 'value'}]
        }
        """

        if isinstance(alarm.get('rule'), dict):
            return

        alarm['type'] = 'threshold'
        alarm['rule'] = {}
        alarm['matching_metadata'] = cls._decode_matching_metadata(
            alarm['matching_metadata'])
        for field in ['period', 'evaluation_periods', 'threshold',
                      'statistic', 'comparison_operator', 'meter_name']:
            if field in alarm:
                alarm['rule'][field] = alarm[field]
                del alarm[field]

        query = []
        for key in alarm['matching_metadata']:
            query.append({'field': key,
                          'op': 'eq',
                          'value': alarm['matching_metadata'][key],
                          'type': 'string'})
        del alarm['matching_metadata']
        alarm['rule']['query'] = query

    @staticmethod
    def _decode_matching_metadata(matching_metadata):
        if isinstance(matching_metadata, dict):
            #note(sileht): keep compatibility with alarm
            #with matching_metadata as a dict
            return matching_metadata
        else:
            new_matching_metadata = {}
            for elem in matching_metadata:
                new_matching_metadata[elem['key']] = elem['value']
            return new_matching_metadata

    @staticmethod
    def _ensure_time_constraints(alarm):
        """Ensures the alarm has a time constraints field."""
        if 'time_constraints' not in alarm:
            alarm['time_constraints'] = []


class QueryTransformer(object):

    operators = {"<": "$lt",
                 ">": "$gt",
                 "<=": "$lte",
                 "=<": "$lte",
                 ">=": "$gte",
                 "=>": "$gte",
                 "!=": "$ne",
                 "in": "$in"}

    complex_operators = {"or": "$or",
                         "and": "$and"}

    ordering_functions = {"asc": pymongo.ASCENDING,
                          "desc": pymongo.DESCENDING}

    def transform_orderby(self, orderby):
        orderby_filter = []

        for field in orderby:
            field_name = field.keys()[0]
            ordering = self.ordering_functions[field.values()[0]]
            orderby_filter.append((field_name, ordering))
        return orderby_filter

    @staticmethod
    def _move_negation_to_leaf(condition):
        """Moves every not operator to the leafs by
        applying the De Morgan rules and anihilating
        double negations
        """
        def _apply_de_morgan(tree, negated_subtree, negated_op):
            if negated_op == "and":
                new_op = "or"
            else:
                new_op = "and"

            tree[new_op] = [{"not": child}
                            for child in negated_subtree[negated_op]]
            del tree["not"]

        def transform(subtree):
            op = subtree.keys()[0]
            if op in ["and", "or"]:
                [transform(child) for child in subtree[op]]
            elif op == "not":
                negated_tree = subtree[op]
                negated_op = negated_tree.keys()[0]
                if negated_op == "and":
                    _apply_de_morgan(subtree, negated_tree, negated_op)
                    transform(subtree)
                elif negated_op == "or":
                    _apply_de_morgan(subtree, negated_tree, negated_op)
                    transform(subtree)
                elif negated_op == "not":
                    # two consecutive not annihilates theirselves
                    new_op = negated_tree.values()[0].keys()[0]
                    subtree[new_op] = negated_tree[negated_op][new_op]
                    del subtree["not"]
                    transform(subtree)

        transform(condition)

    def transform_filter(self, condition):
        # in Mongo not operator can only be applied to
        # simple expressions so we have to move every
        # not operator to the leafs of the expression tree
        self._move_negation_to_leaf(condition)
        return self._process_json_tree(condition)

    def _handle_complex_op(self, complex_op, nodes):
        element_list = []
        for node in nodes:
            element = self._process_json_tree(node)
            element_list.append(element)
        complex_operator = self.complex_operators[complex_op]
        op = {complex_operator: element_list}
        return op

    def _handle_not_op(self, negated_tree):
        # assumes that not is moved to the leaf already
        # so we are next to a leaf
        negated_op = negated_tree.keys()[0]
        negated_field = negated_tree[negated_op].keys()[0]
        value = negated_tree[negated_op][negated_field]
        if negated_op == "=":
            return {negated_field: {"$ne": value}}
        elif negated_op == "!=":
            return {negated_field: value}
        else:
            return {negated_field: {"$not":
                                    {self.operators[negated_op]: value}}}

    def _handle_simple_op(self, simple_op, nodes):
        field_name = nodes.keys()[0]
        field_value = nodes.values()[0]

        # no operator for equal in Mongo
        if simple_op == "=":
            op = {field_name: field_value}
            return op

        operator = self.operators[simple_op]
        op = {field_name: {operator: field_value}}
        return op

    def _process_json_tree(self, condition_tree):
        operator_node = condition_tree.keys()[0]
        nodes = condition_tree.values()[0]

        if operator_node in self.complex_operators:
            return self._handle_complex_op(operator_node, nodes)

        if operator_node == "not":
            negated_tree = condition_tree[operator_node]
            return self._handle_not_op(negated_tree)

        return self._handle_simple_op(operator_node, nodes)

########NEW FILE########
__FILENAME__ = manage
#!/usr/bin/env python
from migrate.versioning.shell import main

if __name__ == '__main__':
    main(debug='False')

########NEW FILE########
__FILENAME__ = 001_add_meter_table
# -*- encoding: utf-8 -*-
#
# Author: John Tran <jhtran@att.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from sqlalchemy import Column
from sqlalchemy import DateTime
from sqlalchemy import Index
from sqlalchemy import Integer
from sqlalchemy import MetaData
from sqlalchemy import String
from sqlalchemy import Table
from sqlalchemy import UniqueConstraint


def upgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)

    meter = Table(
        'meter', meta,
        Column('id', Integer, primary_key=True, index=True),
        Column('counter_name', String(255)),
        Column('user_id', String(255), index=True),
        Column('project_id', String(255), index=True),
        Column('resource_id', String(255)),
        Column('resource_metadata', String(5000)),
        Column('counter_type', String(255)),
        Column('counter_volume', Integer),
        Column('counter_duration', Integer),
        Column('timestamp', DateTime(timezone=False), index=True),
        Column('message_signature', String(1000)),
        Column('message_id', String(1000)),
        mysql_engine='InnoDB',
        mysql_charset='utf8',
    )

    resource = Table(
        'resource', meta,
        Column('id', String(255), primary_key=True, index=True),
        Column('resource_metadata', String(5000)),
        Column('project_id', String(255), index=True),
        Column('received_timestamp', DateTime(timezone=False)),
        Column('timestamp', DateTime(timezone=False), index=True),
        Column('user_id', String(255), index=True),
        mysql_engine='InnoDB',
        mysql_charset='utf8',
    )

    user = Table(
        'user', meta,
        Column('id', String(255), primary_key=True, index=True),
        mysql_engine='InnoDB',
        mysql_charset='utf8',
    )

    project = Table(
        'project', meta,
        Column('id', String(255), primary_key=True, index=True),
        mysql_engine='InnoDB',
        mysql_charset='utf8',
    )

    sourceassoc = Table(
        'sourceassoc', meta,
        Column('source_id', String(255), index=True),
        Column('user_id', String(255)),
        Column('project_id', String(255)),
        Column('resource_id', String(255)),
        Column('meter_id', Integer),
        Index('idx_su', 'source_id', 'user_id'),
        Index('idx_sp', 'source_id', 'project_id'),
        Index('idx_sr', 'source_id', 'resource_id'),
        Index('idx_sm', 'source_id', 'meter_id'),
        mysql_engine='InnoDB',
        mysql_charset='utf8',
    )

    source = Table(
        'source', meta,
        Column('id', String(255), primary_key=True, index=True),
        UniqueConstraint('id'),
        mysql_engine='InnoDB',
        mysql_charset='utf8',
    )

    tables = [meter, project, resource, user, source, sourceassoc]
    for i in sorted(tables):
        i.create()


def downgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    for name in ['source', 'sourceassoc', 'project',
                 'user', 'resource', 'meter']:
        t = Table(name, meta, autoload=True)
        t.drop()

########NEW FILE########
__FILENAME__ = 002_remove_duration
# -*- encoding: utf-8 -*-
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from sqlalchemy import Column
from sqlalchemy import Integer
from sqlalchemy import MetaData
from sqlalchemy import Table


def upgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    meter = Table('meter', meta, autoload=True)
    duration = Column('counter_duration', Integer)
    meter.drop_column(duration)


def downgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    meter = Table('meter', meta, autoload=True)
    duration = Column('counter_duration', Integer)
    meter.create_column(duration)

########NEW FILE########
__FILENAME__ = 003_set_utf8_charset
# Copyright 2012 Canonical.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.


def upgrade(migrate_engine):

    if migrate_engine.name == "mysql":
        tables = ['meter', 'user', 'resource', 'project', 'source',
                  'sourceassoc']
        sql = "SET foreign_key_checks = 0;"

        for table in tables:
            sql += "ALTER TABLE %s CONVERT TO CHARACTER SET utf8;" % table
        sql += "SET foreign_key_checks = 1;"
        sql += "ALTER DATABASE %s DEFAULT CHARACTER SET utf8;" \
            % migrate_engine.url.database
        migrate_engine.execute(sql)


def downgrade(migrate_engine):
    # Operations to reverse the above upgrade go here.
    if migrate_engine.name == "mysql":
        tables = ['meter', 'user', 'resource', 'project', 'source',
                  'sourceassoc']
        sql = "SET foreign_key_checks = 0;"

        for table in tables:
            sql += "ALTER TABLE %s CONVERT TO CHARACTER SET latin1;" % table
        sql += "SET foreign_key_checks = 1;"
        sql += "ALTER DATABASE %s DEFAULT CHARACTER SET latin1;" \
            % migrate_engine.url.database
        migrate_engine.execute(sql)

########NEW FILE########
__FILENAME__ = 004_add_counter_unit
# -*- encoding: utf-8 -*-
#
# Author: Guillaume Pernot <gpernot@praksys.org>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from sqlalchemy import Column
from sqlalchemy import MetaData
from sqlalchemy import String
from sqlalchemy import Table


def upgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    meter = Table('meter', meta, autoload=True)
    unit = Column('counter_unit', String(255))
    meter.create_column(unit)


def downgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    meter = Table('meter', meta, autoload=True)
    unit = Column('counter_unit', String(255))
    meter.drop_column(unit)

########NEW FILE########
__FILENAME__ = 005_remove_resource_timestamp
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 eNovance
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from sqlalchemy import MetaData, Table, Column, DateTime

from ceilometer.openstack.common import timeutils


def upgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    resource = Table('resource', meta, autoload=True)
    timestamp = Column('timestamp', DateTime)
    resource.drop_column(timestamp)
    received_timestamp = Column('received_timestamp', DateTime)
    resource.drop_column(received_timestamp)


def downgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    resource = Table('resource', meta, autoload=True)
    timestamp = Column('timestamp', DateTime)
    resource.create_column(timestamp)
    received_timestamp = Column('received_timestamp', DateTime,
                                default=timeutils.utcnow)
    resource.create_column(received_timestamp)

########NEW FILE########
__FILENAME__ = 006_counter_volume_is_float
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 eNovance SAS <licensing@enovance.com>
# Author: François Charlier <francois.charlier@enovance.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from sqlalchemy import Float
from sqlalchemy import Integer
from sqlalchemy import MetaData
from sqlalchemy import Table


def upgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    meter = Table('meter', meta, autoload=True)
    meter.c.counter_volume.alter(type=Float(53))


def downgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    meter = Table('meter', meta, autoload=True)
    meter.c.counter_volume.alter(type=Integer)

########NEW FILE########
__FILENAME__ = 007_add_alarm_table
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 eNovance <licensing@enovance.com>
# Copyright © 2013 Red Hat, Inc.
#
# Author: Mehdi Abaakouk <mehdi.abaakouk@enovance.com>
#         Angus Salkeld <asalkeld@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from sqlalchemy import MetaData, Table, Column, Text
from sqlalchemy import Boolean, Integer, String, DateTime, Float


def upgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    alarm = Table(
        'alarm', meta,
        Column('id', String(255), primary_key=True, index=True),
        Column('enabled', Boolean),
        Column('name', Text()),
        Column('description', Text()),
        Column('timestamp', DateTime(timezone=False)),
        Column('counter_name', String(255), index=True),
        Column('user_id', String(255), index=True),
        Column('project_id', String(255), index=True),
        Column('comparison_operator', String(2)),
        Column('threshold', Float),
        Column('statistic', String(255)),
        Column('evaluation_periods', Integer),
        Column('period', Integer),
        Column('state', String(255)),
        Column('state_timestamp', DateTime(timezone=False)),
        Column('ok_actions', Text()),
        Column('alarm_actions', Text()),
        Column('insufficient_data_actions', Text()),
        Column('matching_metadata', Text()),
        mysql_engine='InnoDB',
        mysql_charset='utf8')
    alarm.create()


def downgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    alarm = Table('alarm', meta, autoload=True)
    alarm.drop()

########NEW FILE########
__FILENAME__ = 008_add_events
# -*- encoding: utf-8 -*-
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from sqlalchemy import Column
from sqlalchemy import Float
from sqlalchemy import ForeignKey
from sqlalchemy import Integer
from sqlalchemy import MetaData
from sqlalchemy import String
from sqlalchemy import Table


def upgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)

    unique_name = Table(
        'unique_name', meta,
        Column('id', Integer, primary_key=True),
        Column('key', String(32), index=True),
        mysql_engine='InnoDB',
        mysql_charset='utf8',
    )
    unique_name.create()

    event = Table(
        'event', meta,
        Column('id', Integer, primary_key=True),
        Column('generated', Float(asdecimal=True), index=True),
        Column('unique_name_id', Integer, ForeignKey('unique_name.id')),
        mysql_engine='InnoDB',
        mysql_charset='utf8',
    )
    event.create()

    trait = Table(
        'trait', meta,
        Column('id', Integer, primary_key=True),
        Column('name_id', Integer, ForeignKey('unique_name.id')),
        Column('t_type', Integer, index=True),
        Column('t_string', String(32), nullable=True, default=None,
               index=True),
        Column('t_float', Float, nullable=True, default=None, index=True),
        Column('t_int', Integer, nullable=True, default=None, index=True),
        Column('t_datetime', Float(asdecimal=True), nullable=True,
               default=None, index=True),
        Column('event_id', Integer, ForeignKey('event.id')),
        mysql_engine='InnoDB',
        mysql_charset='utf8',
    )
    trait.create()


def downgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    for name in ['trait', 'event', 'unique_name']:
        t = Table(name, meta, autoload=True)
        t.drop()

########NEW FILE########
__FILENAME__ = 009_event_strings
# -*- encoding: utf-8 -*-
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from sqlalchemy import MetaData
from sqlalchemy import Table
from sqlalchemy import VARCHAR


def upgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    name = Table('unique_name', meta, autoload=True)
    name.c.key.alter(type=VARCHAR(length=255))
    trait = Table('trait', meta, autoload=True)
    trait.c.t_string.alter(type=VARCHAR(length=255))


def downgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    name = Table('unique_name', meta, autoload=True)
    name.c.key.alter(type=VARCHAR(length=32))
    trait = Table('trait', meta, autoload=True)
    trait.c.t_string.alter(type=VARCHAR(length=32))

########NEW FILE########
__FILENAME__ = 010_add_index_to_meter
# -*- encoding: utf-8 -*-
#
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import sqlalchemy as sa


def upgrade(migrate_engine):
    meta = sa.MetaData(bind=migrate_engine)
    meter = sa.Table('meter', meta, autoload=True)
    index = sa.Index('idx_meter_rid_cname', meter.c.resource_id,
                     meter.c.counter_name)
    index.create(bind=migrate_engine)


def downgrade(migrate_engine):
    meta = sa.MetaData(bind=migrate_engine)
    meter = sa.Table('meter', meta, autoload=True)
    index = sa.Index('idx_meter_rid_cname', meter.c.resource_id,
                     meter.c.counter_name)
    index.drop(bind=migrate_engine)

########NEW FILE########
__FILENAME__ = 011_indexes_cleanup
# -*- encoding: utf-8 -*-
#
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from sqlalchemy import Index, MetaData, Table


INDEXES = {
    #`table_name`: ((`index_name`, `column`),)
    "user": (('ix_user_id', 'id'),),
    "source": (('ix_source_id', 'id'),),
    "project": (('ix_project_id', 'id'),),
    "meter": (('ix_meter_id', 'id'),),
    "alarm": (('ix_alarm_id', 'id'),),
    "resource": (('ix_resource_id', 'id'),)
}


def upgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    load_tables = dict((table_name, Table(table_name, meta, autoload=True))
                       for table_name in INDEXES.keys())
    for table_name, indexes in INDEXES.items():
        table = load_tables[table_name]
        for index_name, column in indexes:
            index = Index(index_name, table.c[column])
            index.drop()


def downgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    load_tables = dict((table_name, Table(table_name, meta, autoload=True))
                       for table_name in INDEXES.keys())
    for table_name, indexes in INDEXES.items():
        table = load_tables[table_name]
        for index_name, column in indexes:
            index = Index(index_name, table.c[column])
            index.create()

########NEW FILE########
__FILENAME__ = 012_add_missing_foreign_keys
# -*- encoding: utf-8 -*-
#
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from migrate import ForeignKeyConstraint
from sqlalchemy import MetaData, Table
from sqlalchemy.sql.expression import select

TABLES = ['resource', 'sourceassoc', 'user',
          'project', 'meter', 'source', 'alarm']

INDEXES = {
    "resource": (('user_id', 'user', 'id'),
                 ('project_id', 'project', 'id')),
    "sourceassoc": (('user_id', 'user', 'id'),
                    ('project_id', 'project', 'id'),
                    ('resource_id', 'resource', 'id'),
                    ('meter_id', 'meter', 'id'),
                    ('source_id', 'source', 'id')),
    "alarm": (('user_id', 'user', 'id'),
              ('project_id', 'project', 'id')),
    "meter": (('user_id', 'user', 'id'),
              ('project_id', 'project', 'id'),
              ('resource_id', 'resource', 'id'),)
}


def upgrade(migrate_engine):
    if migrate_engine.name == 'sqlite':
        return
    meta = MetaData(bind=migrate_engine)
    load_tables = dict((table_name, Table(table_name, meta, autoload=True))
                       for table_name in TABLES)
    for table_name, indexes in INDEXES.items():
        table = load_tables[table_name]
        for column, ref_table_name, ref_column_name in indexes:
            ref_table = load_tables[ref_table_name]
            subq = select([getattr(ref_table.c, ref_column_name)])
            sql_del = table.delete().where(
                ~ getattr(table.c, column).in_(subq))
            migrate_engine.execute(sql_del)

            params = {'columns': [table.c[column]],
                      'refcolumns': [ref_table.c[ref_column_name]]}
            if migrate_engine.name == 'mysql':
                params['name'] = "_".join(('fk', table_name, column))
            fkey = ForeignKeyConstraint(**params)
            fkey.create()


def downgrade(migrate_engine):
    if migrate_engine.name == 'sqlite':
        return
    meta = MetaData(bind=migrate_engine)
    load_tables = dict((table_name, Table(table_name, meta, autoload=True))
                       for table_name in TABLES)
    for table_name, indexes in INDEXES.items():
        table = load_tables[table_name]
        for column, ref_table_name, ref_column_name in indexes:
            ref_table = load_tables[ref_table_name]
            params = {'columns': [table.c[column]],
                      'refcolumns': [ref_table.c[ref_column_name]]}
            if migrate_engine.name == 'mysql':
                params['name'] = "_".join(('fk', table_name, column))
            with migrate_engine.begin():
                fkey = ForeignKeyConstraint(**params)
                fkey.drop()

########NEW FILE########
__FILENAME__ = 013_rename_counter_to_meter_alarm
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 eNovance <licensing@enovance.com>
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from sqlalchemy import MetaData, Table


def upgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine
    alarm = Table('alarm', meta, autoload=True)
    alarm.c.counter_name.alter(name='meter_name')


def downgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine
    alarm = Table('alarm', meta, autoload=True)
    alarm.c.meter_name.alter(name='counter_name')

########NEW FILE########
__FILENAME__ = 014_add_event_message_id
# -*- encoding: utf-8 -*-
#
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from migrate.changeset.constraint import UniqueConstraint
import sqlalchemy

from ceilometer.storage.sqlalchemy import models


def upgrade(migrate_engine):
    meta = sqlalchemy.MetaData(bind=migrate_engine)

    event = sqlalchemy.Table('event', meta, autoload=True)
    message_id = sqlalchemy.Column('message_id', sqlalchemy.String(50))
    event.create_column(message_id)

    cons = UniqueConstraint('message_id', table=event)
    cons.create()

    index = sqlalchemy.Index('idx_event_message_id', models.Event.message_id)
    index.create(bind=migrate_engine)

    # Populate the new column ...
    trait = sqlalchemy.Table('trait', meta, autoload=True)
    unique_name = sqlalchemy.Table('unique_name', meta, autoload=True)
    join = trait.join(unique_name, unique_name.c.id == trait.c.name_id)
    traits = sqlalchemy.select([trait.c.event_id, trait.c.t_string],
                               whereclause=(unique_name.c.key == 'message_id'),
                               from_obj=join)

    for event_id, value in traits.execute():
        event.update().\
            where(event.c.id == event_id).\
            values(message_id=value).\
            execute()

    # Leave the Trait, makes the rollback easier and won't really hurt anyone.


def downgrade(migrate_engine):
    meta = sqlalchemy.MetaData(bind=migrate_engine)
    event = sqlalchemy.Table('event', meta, autoload=True)
    message_id = sqlalchemy.Column('message_id', sqlalchemy.String(50))
    cons = UniqueConstraint('message_id', table=event)
    cons.drop()
    index = sqlalchemy.Index('idx_event_message_id', models.Event.message_id)
    index.drop(bind=migrate_engine)
    event.drop_column(message_id)

########NEW FILE########
__FILENAME__ = 015_add_alarm_history_table
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Red Hat, Inc.
#
# Author: Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from migrate import ForeignKeyConstraint
from sqlalchemy import MetaData, Table, Column, Index
from sqlalchemy import String, DateTime


def upgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine

    project = Table('project', meta, autoload=True)
    user = Table('user', meta, autoload=True)

    alarm_history = Table(
        'alarm_history', meta,
        Column('event_id', String(255), primary_key=True, index=True),
        Column('alarm_id', String(255)),
        Column('on_behalf_of', String(255)),
        Column('project_id', String(255)),
        Column('user_id', String(255)),
        Column('type', String(20)),
        Column('detail', String(255)),
        Column('timestamp', DateTime(timezone=False)),
        mysql_engine='InnoDB',
        mysql_charset='utf8')

    alarm_history.create()

    if migrate_engine.name in ['mysql', 'postgresql']:
        indices = [Index('ix_alarm_history_alarm_id',
                         alarm_history.c.alarm_id),
                   Index('ix_alarm_history_on_behalf_of',
                         alarm_history.c.on_behalf_of),
                   Index('ix_alarm_history_project_id',
                         alarm_history.c.project_id),
                   Index('ix_alarm_history_on_user_id',
                         alarm_history.c.user_id)]

        for index in indices:
            index.create(migrate_engine)

        fkeys = [ForeignKeyConstraint(columns=[alarm_history.c.on_behalf_of],
                                      refcolumns=[project.c.id]),
                 ForeignKeyConstraint(columns=[alarm_history.c.project_id],
                                      refcolumns=[project.c.id]),
                 ForeignKeyConstraint(columns=[alarm_history.c.user_id],
                                      refcolumns=[user.c.id])]
        for fkey in fkeys:
            fkey.create(engine=migrate_engine)


def downgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine
    alarm_history = Table('alarm_history', meta, autoload=True)
    alarm_history.drop()

########NEW FILE########
__FILENAME__ = 016_simpler_alarm
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 eNovance <licensing@enovance.com>
#
# Author: Mehdi Abaakouk <mehdi.abaakouk@enovance.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import json

from sqlalchemy import MetaData, Table, Column, Index
from sqlalchemy import String, Float, Integer, Text


def upgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine
    table = Table('alarm', meta, autoload=True)

    type = Column('type', String(50), default='threshold')
    type.create(table, populate_default=True)

    rule = Column('rule', Text())
    rule.create(table)

    for row in table.select().execute().fetchall():
        query = []
        if row.matching_metadata is not None:
            matching_metadata = json.loads(row.matching_metadata)
            for key in matching_metadata:
                query.append({'field': key,
                              'op': 'eq',
                              'value': matching_metadata[key]})
        rule = {
            'meter_name': row.meter_name,
            'comparison_operator': row.comparison_operator,
            'threshold': row.threshold,
            'statistic': row.statistic,
            'evaluation_periods': row.evaluation_periods,
            'period': row.period,
            'query': query
        }
        table.update().where(table.c.id == row.id).values(rule=rule).execute()

    index = Index('ix_alarm_counter_name', table.c.meter_name)
    index.drop(bind=migrate_engine)
    table.c.meter_name.drop()
    table.c.comparison_operator.drop()
    table.c.threshold.drop()
    table.c.statistic.drop()
    table.c.evaluation_periods.drop()
    table.c.period.drop()
    table.c.matching_metadata.drop()


def downgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine
    table = Table('alarm', meta, autoload=True)

    columns = [
        Column('meter_name', String(255)),
        Column('comparison_operator', String(2)),
        Column('threshold', Float),
        Column('statistic', String(255)),
        Column('evaluation_periods', Integer),
        Column('period', Integer),
        Column('matching_metadata', Text())
    ]
    for c in columns:
        c.create(table)

    for row in table.select().execute().fetchall():
        if row.type != 'threshold':
            #note: type insupported in previous version
            table.delete().where(table.c.id == row.id).execute()
        else:
            rule = json.loads(row.rule)
            values = {'comparison_operator': rule['comparison_operator'],
                      'threshold': float(rule['threshold']),
                      'statistic': rule['statistic'],
                      'evaluation_periods': int(rule['evaluation_periods']),
                      'period': int(rule['period']),
                      'meter_name': int(rule['mater_name']),
                      'matching_metadata': {}}

            #note: op are ignored because previous format don't support it
            for q in rule['query']:
                values['matching_metadata'][q['field']] = q['value']
            values['matching_metadata'] = json.dumps(
                values['matching_metadata'])
            table.update().where(table.c.id == row.id
                                 ).values(**values).execute()

    index = Index('ix_alarm_counter_name', table.c.meter_name)
    index.create(bind=migrate_engine)

    table.c.type.drop()
    table.c.rule.drop()

########NEW FILE########
__FILENAME__ = 017_convert_timestamp_as_datetime_to_decimal
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Rackspace Hosting
#
# Author: Thomas Maddox <thomas.maddox@rackspace.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import sqlalchemy as sa

from ceilometer.storage.sqlalchemy import migration
from ceilometer.storage.sqlalchemy import models

_col = 'timestamp'


def _convert_data_type(table, col, from_t, to_t, pk_attr='id', index=False):
    temp_col_n = 'convert_data_type_temp_col'
    # Override column we're going to convert with from_t, since the type we're
    # replacing could be custom and we need to tell SQLALchemy how to perform
    # CRUD operations with it.
    table = sa.Table(table.name, table.metadata, sa.Column(col, from_t),
                     extend_existing=True)
    sa.Column(temp_col_n, to_t).create(table)

    key_attr = getattr(table.c, pk_attr)
    orig_col = getattr(table.c, col)
    new_col = getattr(table.c, temp_col_n)

    query = sa.select([key_attr, orig_col])
    for key, value in migration.paged(query):
        table.update().where(key_attr == key)\
            .values({temp_col_n: value}).execute()

    orig_col.drop()
    new_col.alter(name=col)
    if index:
        sa.Index('ix_%s_%s' % (table.name, col), new_col).create()


def upgrade(migrate_engine):
    if migrate_engine.name == 'mysql':
        meta = sa.MetaData(bind=migrate_engine)
        meter = sa.Table('meter', meta, autoload=True)
        _convert_data_type(meter, _col, sa.DateTime(),
                           models.PreciseTimestamp(),
                           pk_attr='id', index=True)


def downgrade(migrate_engine):
    if migrate_engine.name == 'mysql':
        meta = sa.MetaData(bind=migrate_engine)
        meter = sa.Table('meter', meta, autoload=True)
        _convert_data_type(meter, _col, models.PreciseTimestamp(),
                           sa.DateTime(), pk_attr='id', index=True)

########NEW FILE########
__FILENAME__ = 018_resource_resource_metadata_is_text
# -*- encoding: utf-8 -*-

# Copyright 2013 OpenStack Foundation
# All Rights Reserved.
# Copyright 2013 IBM Corp.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from sqlalchemy import MetaData
from sqlalchemy import String
from sqlalchemy import Table
from sqlalchemy import Text


def upgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    resource = Table('resource', meta, autoload=True)
    resource.c.resource_metadata.alter(type=Text)


def downgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    resource = Table('resource', meta, autoload=True)
    resource.c.resource_metadata.alter(type=String(5000))

########NEW FILE########
__FILENAME__ = 019_alarm_history_detail_is_text
# -*- encoding: utf-8 -*-

# Copyright 2013 OpenStack Foundation
# All Rights Reserved.
# Copyright 2013 IBM Corp.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from sqlalchemy import MetaData
from sqlalchemy import String
from sqlalchemy import Table
from sqlalchemy import Text


def upgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    alm_hist = Table('alarm_history', meta, autoload=True)
    alm_hist.c.detail.alter(type=Text)


def downgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    alm_hist = Table('alarm_history', meta, autoload=True)
    alm_hist.c.detail.alter(type=String(255))

########NEW FILE########
__FILENAME__ = 020_add_metadata_tables
#
# Copyright 2013 OpenStack Foundation
# All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import json

from sqlalchemy import Boolean
from sqlalchemy import Column
from sqlalchemy import Float
from sqlalchemy import ForeignKey
from sqlalchemy import Integer
from sqlalchemy import MetaData
from sqlalchemy.sql import select
from sqlalchemy import String
from sqlalchemy import Table
from sqlalchemy import Text

from ceilometer import utils

tables = [('metadata_text', Text, True),
          ('metadata_bool', Boolean, False),
          ('metadata_int', Integer, False),
          ('metadata_float', Float, False)]


def upgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    meter = Table('meter', meta, autoload=True)
    meta_tables = {}
    for t_name, t_type, t_nullable in tables:
        meta_tables[t_name] = Table(
            t_name, meta,
            Column('id', Integer, ForeignKey('meter.id'), primary_key=True),
            Column('meta_key', String(255), index=True, primary_key=True),
            Column('value', t_type, nullable=t_nullable),
            mysql_engine='InnoDB',
            mysql_charset='utf8',
        )
        meta_tables[t_name].create()

    for row in select([meter]).execute():
        if row['resource_metadata']:
            meter_id = row['id']
            rmeta = json.loads(row['resource_metadata'])
            for key, v in utils.dict_to_keyval(rmeta):
                ins = None
                if isinstance(v, basestring) or v is None:
                    ins = meta_tables['metadata_text'].insert()
                elif isinstance(v, bool):
                    ins = meta_tables['metadata_bool'].insert()
                elif isinstance(v, (int, long)):
                    ins = meta_tables['metadata_int'].insert()
                elif isinstance(v, float):
                    ins = meta_tables['metadata_float'].insert()
                if ins is not None:
                    ins.values(id=meter_id, meta_key=key, value=v).execute()


def downgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    for t in tables:
        table = Table(t[0], meta, autoload=True)
        table.drop()

########NEW FILE########
__FILENAME__ = 021_add_event_types
# -*- encoding: utf-8 -*-
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from migrate import ForeignKeyConstraint
from sqlalchemy import Column
from sqlalchemy import Integer
from sqlalchemy import MetaData
from sqlalchemy import select
from sqlalchemy import String
from sqlalchemy import Table

from ceilometer.storage.sqlalchemy import migration


def upgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    event_type = Table(
        'event_type', meta,
        Column('id', Integer, primary_key=True),
        Column('desc', String(255), unique=True),
        mysql_engine='InnoDB',
        mysql_charset='utf8',
    )
    event_type.create()
    event = Table('event', meta, autoload=True)
    unique_name = Table('unique_name', meta, autoload=True)

    # Event type is a specialization of Unique name, so
    # we insert into the event_type table all the distinct
    # unique names from the event.unique_name field along
    # with the key from the unique_name table, and
    # then rename the event.unique_name field to event.event_type
    conn = migrate_engine.connect()
    sql = ("INSERT INTO event_type "
           "SELECT unique_name.id, unique_name.key FROM event "
           "INNER JOIN unique_name "
           "ON event.unique_name_id = unique_name.id "
           "GROUP BY unique_name.id")
    conn.execute(sql)
    conn.close()
    # Now we need to drop the foreign key constraint, rename
    # the event.unique_name column, and re-add a new foreign
    # key constraint
    params = {'columns': [event.c.unique_name_id],
              'refcolumns': [unique_name.c.id]}
    if migrate_engine.name == 'mysql':
        params['name'] = "event_ibfk_1"
    fkey = ForeignKeyConstraint(**params)
    fkey.drop()

    Column('event_type_id', Integer).create(event)

    # Move data from unique_name_id column into event_type_id column
    # and delete the entry from the unique_name table
    query = select([event.c.id, event.c.unique_name_id])
    for key, value in migration.paged(query):
        event.update().where(event.c.id == key)\
            .values({"event_type_id": value}).execute()
        unique_name.delete()\
            .where(unique_name.c.id == key).execute()

    params = {'columns': [event.c.event_type_id],
              'refcolumns': [event_type.c.id]}
    if migrate_engine.name == 'mysql':
        params['name'] = "_".join(('fk', 'event_type', 'id'))
    fkey = ForeignKeyConstraint(**params)
    fkey.create()

    event.c.unique_name_id.drop()


def downgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    event_type = Table('event_type', meta, autoload=True)
    event = Table('event', meta, autoload=True)
    unique_name = Table('unique_name', meta, autoload=True)
    # Re-insert the event type table records into the old
    # unique_name table.
    conn = migrate_engine.connect()
    sql = ("INSERT INTO unique_name "
           "SELECT event_type.id, event_type.desc FROM event_type")
    conn.execute(sql)
    conn.close()
    # Drop the foreign key constraint to event_type, drop the
    # event_type table, rename the event.event_type column to
    # event.unique_name, and re-add the old foreign
    # key constraint
    params = {'columns': [event.c.event_type_id],
              'refcolumns': [event_type.c.id]}
    if migrate_engine.name == 'mysql':
        params['name'] = "_".join(('fk', 'event_type', 'id'))
    fkey = ForeignKeyConstraint(**params)
    fkey.drop()

    event_type.drop()

    Column('unique_name_id', Integer).create(event)

    # Move data from event_type_id column to unique_name_id column
    query = select([event.c.id, event.c.event_type_id])
    for key, value in migration.paged(query):
        event.update().where(event.c.id == key)\
            .values({"unique_name_id": value}).execute()

    event.c.event_type_id.drop()
    params = {'columns': [event.c.unique_name_id],
              'refcolumns': [unique_name.c.id]}
    if migrate_engine.name == 'mysql':
        params['name'] = 'event_ibfk_1'
    fkey = ForeignKeyConstraint(**params)
    fkey.create()

########NEW FILE########
__FILENAME__ = 022_metadata_int_is_bigint
# -*- encoding: utf-8 -*-

# Copyright 2013 OpenStack Foundation
# All Rights Reserved.
# Copyright 2013 IBM Corp.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from sqlalchemy import BigInteger
from sqlalchemy import Integer
from sqlalchemy import MetaData
from sqlalchemy import Table


def upgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    resource = Table('metadata_int', meta, autoload=True)
    resource.c.value.alter(type=BigInteger)


def downgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    resource = Table('metadata_int', meta, autoload=True)
    resource.c.value.alter(type=Integer)

########NEW FILE########
__FILENAME__ = 023_add_trait_types
# -*- encoding: utf-8 -*-
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from migrate import ForeignKeyConstraint
from sqlalchemy import Column
from sqlalchemy import Integer
from sqlalchemy import MetaData
from sqlalchemy import select
from sqlalchemy import String
from sqlalchemy import Table
from sqlalchemy import UniqueConstraint

from ceilometer.storage.sqlalchemy import migration


def upgrade(migrate_engine):
    meta = MetaData(migrate_engine)
    trait_type = Table(
        'trait_type', meta,
        Column('id', Integer, primary_key=True),
        Column('desc', String(255)),
        Column('data_type', Integer),
        UniqueConstraint('desc', 'data_type', name="tt_unique"),
        mysql_engine='InnoDB',
        mysql_charset='utf8',
    )
    trait = Table('trait', meta, autoload=True)
    unique_name = Table('unique_name', meta, autoload=True)
    trait_type.create(migrate_engine)
    # Trait type extracts data from Trait and Unique name.
    # We take all trait names from Unique Name, and data types
    # from Trait. We then remove dtype and name from trait, and
    # remove the name field.

    conn = migrate_engine.connect()
    sql = ("INSERT INTO trait_type "
           "SELECT unique_name.id, unique_name.key, trait.t_type FROM trait "
           "INNER JOIN unique_name "
           "ON trait.name_id = unique_name.id "
           "GROUP BY unique_name.id, unique_name.key, trait.t_type")
    conn.execute(sql)
    conn.close()

    # Now we need to drop the foreign key constraint, rename
    # the trait.name column, and re-add a new foreign
    # key constraint
    params = {'columns': [trait.c.name_id],
              'refcolumns': [unique_name.c.id]}
    if migrate_engine.name == 'mysql':
        params['name'] = "trait_ibfk_1"  # foreign key to the unique name table
    fkey = ForeignKeyConstraint(**params)
    fkey.drop()

    Column('trait_type_id', Integer).create(trait)

    # Move data from name_id column into trait_type_id column
    query = select([trait.c.id, trait.c.name_id])
    for key, value in migration.paged(query):
        trait.update().where(trait.c.id == key)\
            .values({"trait_type_id": value}).execute()

    trait.c.name_id.drop()

    params = {'columns': [trait.c.trait_type_id],
              'refcolumns': [trait_type.c.id]}
    if migrate_engine.name == 'mysql':
        params['name'] = "_".join(('fk', 'trait_type', 'id'))

    fkey = ForeignKeyConstraint(**params)
    fkey.create()

    # Drop the t_type column to data_type.
    trait.c.t_type.drop()

    # Finally, drop the unique_name table - we don't need it
    # anymore.
    unique_name.drop()


def downgrade(migrate_engine):
    meta = MetaData(migrate_engine)
    unique_name = Table(
        'unique_name', meta,
        Column('id', Integer, primary_key=True),
        Column('key', String(255), unique=True),
        mysql_engine='InnoDB',
        mysql_charset='utf8',
    )

    trait_type = Table('trait_type', meta, autoload=True)
    trait = Table('trait', meta, autoload=True)

    # Create the UniqueName table, drop the foreign key constraint
    # to trait_type, drop the trait_type table, rename the
    # trait.trait_type column to traitname, re-add the dtype to
    # the trait table, and re-add the old foreign key constraint

    unique_name.create(migrate_engine)

    conn = migrate_engine.connect()
    sql = ("INSERT INTO unique_name "
           "SELECT trait_type.id, trait_type.desc "
           "FROM trait_type")

    conn.execute(sql)
    conn.close()
    params = {'columns': [trait.c.trait_type_id],
              'refcolumns': [trait_type.c.id]}

    if migrate_engine.name == 'mysql':
        params['name'] = "_".join(('fk', 'trait_type', 'id'))
    fkey = ForeignKeyConstraint(**params)
    fkey.drop()

    # Re-create the old columns in trait
    Column("name_id", Integer).create(trait)
    Column("t_type", Integer).create(trait)

    # copy data from trait_type.data_type into trait.t_type
    query = select([trait_type.c.id, trait_type.c.data_type])
    for key, value in migration.paged(query):
        trait.update().where(trait.c.trait_type_id == key)\
            .values({"t_type": value}).execute()

    # Move data from name_id column into trait_type_id column
    query = select([trait.c.id, trait.c.trait_type_id])
    for key, value in migration.paged(query):
        trait.update().where(trait.c.id == key)\
            .values({"name_id": value}).execute()

    # Add a foreign key to the unique_name table
    params = {'columns': [trait.c.name_id],
              'refcolumns': [unique_name.c.id]}
    if migrate_engine.name == 'mysql':
        params['name'] = 'trait_ibfk_1'
    fkey = ForeignKeyConstraint(**params)
    fkey.create()

    trait.c.trait_type_id.drop()

    # Drop the trait_type table. It isn't needed anymore
    trait_type.drop()

########NEW FILE########
__FILENAME__ = 024_event_use_floatingprecision
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 eNovance SAS <licensing@enovance.com>
#
# Author: Mehdi Abaakouk <mehdi.abaakouk@enovance.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import sqlalchemy as sa

from ceilometer.storage.sqlalchemy import migration
from ceilometer.storage.sqlalchemy import models


def _convert_data_type(table, col, from_t, to_t, pk_attr='id', index=False):
    temp_col_n = 'convert_data_type_temp_col'
    # Override column we're going to convert with from_t, since the type we're
    # replacing could be custom and we need to tell SQLALchemy how to perform
    # CRUD operations with it.
    table = sa.Table(table.name, table.metadata, sa.Column(col, from_t),
                     extend_existing=True)
    sa.Column(temp_col_n, to_t).create(table)

    key_attr = getattr(table.c, pk_attr)
    orig_col = getattr(table.c, col)
    new_col = getattr(table.c, temp_col_n)

    query = sa.select([key_attr, orig_col])
    for key, value in migration.paged(query):
        table.update().where(key_attr == key)\
            .values({temp_col_n: value}).execute()

    orig_col.drop()
    new_col.alter(name=col)
    if index:
        sa.Index('ix_%s_%s' % (table.name, col), new_col).create()


def upgrade(migrate_engine):
    if migrate_engine.name == 'mysql':
        meta = sa.MetaData(bind=migrate_engine)
        event = sa.Table('event', meta, autoload=True)
        _convert_data_type(event, 'generated', sa.Float(),
                           models.PreciseTimestamp(),
                           pk_attr='id', index=True)
        trait = sa.Table('trait', meta, autoload=True)
        _convert_data_type(trait, 't_datetime', sa.Float(),
                           models.PreciseTimestamp(),
                           pk_attr='id', index=True)


def downgrade(migrate_engine):
    if migrate_engine.name == 'mysql':
        meta = sa.MetaData(bind=migrate_engine)
        event = sa.Table('event', meta, autoload=True)
        _convert_data_type(event, 'generated', models.PreciseTimestamp(),
                           sa.Float(), pk_attr='id', index=True)
        trait = sa.Table('trait', meta, autoload=True)
        _convert_data_type(trait, 't_datetime', models.PreciseTimestamp(),
                           sa.Float(), pk_attr='id', index=True)

########NEW FILE########
__FILENAME__ = 025_alarm_use_floatingprecision
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 eNovance SAS <licensing@enovance.com>
#
# Author: Mehdi Abaakouk <mehdi.abaakouk@enovance.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import sqlalchemy as sa

from ceilometer.storage.sqlalchemy import migration
from ceilometer.storage.sqlalchemy import models


def _convert_data_type(table, col, from_t, to_t, pk_attr='id'):
    temp_col_n = 'convert_data_type_temp_col'
    # Override column we're going to convert with from_t, since the type we're
    # replacing could be custom and we need to tell SQLALchemy how to perform
    # CRUD operations with it.
    table = sa.Table(table.name, table.metadata, sa.Column(col, from_t),
                     extend_existing=True)
    sa.Column(temp_col_n, to_t).create(table)

    key_attr = getattr(table.c, pk_attr)
    orig_col = getattr(table.c, col)
    new_col = getattr(table.c, temp_col_n)

    query = sa.select([key_attr, orig_col])
    for key, value in migration.paged(query):
        table.update().where(key_attr == key)\
            .values({temp_col_n: value}).execute()

    orig_col.drop()
    new_col.alter(name=col)


to_convert = [
    ('alarm', 'timestamp', 'id'),
    ('alarm', 'state_timestamp', 'id'),
    ('alarm_history', 'timestamp', 'alarm_id'),
]


def upgrade(migrate_engine):
    if migrate_engine.name == 'mysql':
        meta = sa.MetaData(bind=migrate_engine)
        for table_name, col_name, pk_attr in to_convert:
            table = sa.Table(table_name, meta, autoload=True)
            _convert_data_type(table, col_name, sa.DateTime(),
                               models.PreciseTimestamp(),
                               pk_attr=pk_attr)


def downgrade(migrate_engine):
    if migrate_engine.name == 'mysql':
        meta = sa.MetaData(bind=migrate_engine)
        for table_name, col_name, pk_attr in to_convert:
            table = sa.Table(table_name, meta, autoload=True)
            _convert_data_type(table, col_name, models.PreciseTimestamp(),
                               sa.DateTime(),
                               pk_attr=pk_attr)

########NEW FILE########
__FILENAME__ = 026_float_size
# -*- encoding: utf-8 -*-
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from sqlalchemy import Float
from sqlalchemy import MetaData
from sqlalchemy import Table


def upgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    metadata_float = Table('metadata_float', meta, autoload=True)
    metadata_float.c.value.alter(type=Float(53))
    trait = Table('trait', meta, autoload=True)
    trait.c.t_float.alter(type=Float(53))


def downgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    metadata_float = Table('metadata_float', meta, autoload=True)
    metadata_float.c.value.alter(type=Float())
    trait = Table('trait', meta, autoload=True)
    trait.c.t_string.alter(type=Float())

########NEW FILE########
__FILENAME__ = 027_remove_alarm_fk_constraints
# -*- encoding: utf-8 -*-
#
# Copyright © 2014 Intel Crop.
#
# Author: Lianhao Lu <lianhao.lu@intel.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from migrate import ForeignKeyConstraint
from sqlalchemy import MetaData, Table
from sqlalchemy.sql.expression import select

TABLES = ['user', 'project', 'alarm']

INDEXES = {
    "alarm": (('user_id', 'user', 'id'),
              ('project_id', 'project', 'id')),
}


def upgrade(migrate_engine):
    if migrate_engine.name == 'sqlite':
        return
    meta = MetaData(bind=migrate_engine)
    load_tables = dict((table_name, Table(table_name, meta, autoload=True))
                       for table_name in TABLES)
    for table_name, indexes in INDEXES.items():
        table = load_tables[table_name]
        for column, ref_table_name, ref_column_name in indexes:
            ref_table = load_tables[ref_table_name]
            params = {'columns': [table.c[column]],
                      'refcolumns': [ref_table.c[ref_column_name]]}
            if migrate_engine.name == 'mysql':
                params['name'] = "_".join(('fk', table_name, column))
            fkey = ForeignKeyConstraint(**params)
            fkey.drop()


def downgrade(migrate_engine):
    if migrate_engine.name == 'sqlite':
        return
    meta = MetaData(bind=migrate_engine)
    load_tables = dict((table_name, Table(table_name, meta, autoload=True))
                       for table_name in TABLES)
    for table_name, indexes in INDEXES.items():
        table = load_tables[table_name]
        for column, ref_table_name, ref_column_name in indexes:
            ref_table = load_tables[ref_table_name]
            subq = select([getattr(ref_table.c, ref_column_name)])
            sql_del = table.delete().where(
                ~ getattr(table.c, column).in_(subq))
            migrate_engine.execute(sql_del)

            params = {'columns': [table.c[column]],
                      'refcolumns': [ref_table.c[ref_column_name]]}
            if migrate_engine.name == 'mysql':
                params['name'] = "_".join(('fk', table_name, column))
            fkey = ForeignKeyConstraint(**params)
            fkey.create()

########NEW FILE########
__FILENAME__ = 028_alembic_migrations
# -*- encoding: utf-8 -*-
#
# Copyright 2014 OpenStack Foundation
# All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import migrate
import sqlalchemy as sa


def get_alembic_version(meta):
    """Return Alembic version or None if no Alembic table exists."""
    try:
        a_ver = sa.Table(
            'alembic_version',
            meta,
            autoload=True)
        return sa.select([a_ver.c.version_num]).scalar()
    except sa.exc.NoSuchTableError:
        return None


def delete_alembic(meta):
    try:
        sa.Table(
            'alembic_version',
            meta,
            autoload=True).drop(checkfirst=True)
    except sa.exc.NoSuchTableError:
        pass


INDEXES = (
    # ([dialects], table_name, index_name, create/delete, uniq/not_uniq)
    (['mysql', 'sqlite', 'postgresql'],
     'resource',
     'resource_user_id_project_id_key',
     ('user_id', 'project_id'), True, False, True),
    (['mysql'], 'source', 'id', ('id',), False, True, False))


def index_cleanup(meta, table_name, uniq_name, columns,
                  create, unique, limited):
    table = sa.Table(table_name, meta, autoload=True)
    if create:
        if limited and meta.bind.engine.name == 'mysql':
            # For some versions of mysql we can get an error
            # "Specified key was too long; max key length is 1000 bytes".
            # We should create an index by hand in this case with limited
            # length of columns.
            columns_mysql = ",".join((c + "(100)" for c in columns))
            sql = ("create index %s ON %s (%s)" % (uniq_name, table,
                                                   columns_mysql))
            meta.bind.engine.execute(sql)
        else:
            cols = [table.c[col] for col in columns]
            sa.Index(uniq_name, *cols, unique=unique).create()
    else:
        if unique:
            migrate.UniqueConstraint(*columns, table=table,
                                     name=uniq_name).drop()
        else:
            cols = [table.c[col] for col in columns]
            sa.Index(uniq_name, *cols).drop()


def change_uniq(meta, downgrade=False):
    uniq_name = 'uniq_sourceassoc0meter_id0user_id'
    columns = ('meter_id', 'user_id')

    if meta.bind.engine.name == 'sqlite':
        return

    sourceassoc = sa.Table('sourceassoc', meta, autoload=True)
    meter = sa.Table('meter', meta, autoload=True)
    user = sa.Table('user', meta, autoload=True)
    if meta.bind.engine.name == 'mysql':
        # For mysql dialect all dependent FK should be removed
        #  before renaming of constraint.
        params = {'columns': [sourceassoc.c.meter_id],
                  'refcolumns': [meter.c.id],
                  'name': 'fk_sourceassoc_meter_id'}
        migrate.ForeignKeyConstraint(**params).drop()
        params = {'columns': [sourceassoc.c.user_id],
                  'refcolumns': [user.c.id],
                  'name': 'fk_sourceassoc_user_id'}
        migrate.ForeignKeyConstraint(**params).drop()
    if downgrade:
        migrate.UniqueConstraint(*columns, table=sourceassoc,
                                 name=uniq_name).drop()
    else:
        migrate.UniqueConstraint(*columns, table=sourceassoc,
                                 name=uniq_name).create()
    if meta.bind.engine.name == 'mysql':
        params = {'columns': [sourceassoc.c.meter_id],
                  'refcolumns': [meter.c.id],
                  'name': 'fk_sourceassoc_meter_id'}
        migrate.ForeignKeyConstraint(**params).create()
        params = {'columns': [sourceassoc.c.user_id],
                  'refcolumns': [user.c.id],
                  'name': 'fk_sourceassoc_user_id'}
        migrate.ForeignKeyConstraint(**params).create()


def upgrade(migrate_engine):
    meta = sa.MetaData(bind=migrate_engine)
    a_ver = get_alembic_version(meta)

    if not a_ver:
        alarm = sa.Table('alarm', meta, autoload=True)
        repeat_act = sa.Column('repeat_actions', sa.Boolean,
                               server_default=sa.sql.expression.false())
        alarm.create_column(repeat_act)
        a_ver = '43b1a023dfaa'

    if a_ver == '43b1a023dfaa':
        meter = sa.Table('meter', meta, autoload=True)
        meter.c.resource_metadata.alter(type=sa.Text)
        a_ver = '17738166b91'

    if a_ver == '17738166b91':
        for (engine_names, table_name, uniq_name,
             columns, create, uniq, limited) in INDEXES:
            if migrate_engine.name in engine_names:
                index_cleanup(meta, table_name, uniq_name,
                              columns, create, uniq, limited)
        a_ver = 'b6ae66d05e3'

    if a_ver == 'b6ae66d05e3':
        change_uniq(meta)

    delete_alembic(meta)


def downgrade(migrate_engine):
    meta = sa.MetaData(bind=migrate_engine)

    change_uniq(meta, downgrade=True)

    for (engine_names, table_name, uniq_name,
         columns, create, uniq, limited) in INDEXES:
        if migrate_engine.name in engine_names:
            index_cleanup(meta, table_name, uniq_name,
                          columns, not create, uniq, limited)

    meter = sa.Table('meter', meta, autoload=True)
    meter.c.resource_metadata.alter(type=sa.String(5000))

    alarm = sa.Table('alarm', meta, autoload=True)
    repeat_act = sa.Column('repeat_actions', sa.Boolean)
    alarm.drop_column(repeat_act)

########NEW FILE########
__FILENAME__ = 029_sample_recorded_at
# -*- encoding: utf-8 -*-
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import sqlalchemy

from ceilometer.openstack.common import timeutils
from ceilometer.storage.sqlalchemy import models


def upgrade(migrate_engine):
    meta = sqlalchemy.MetaData(bind=migrate_engine)
    meter = sqlalchemy.Table('meter', meta, autoload=True)
    c = sqlalchemy.Column('recorded_at', models.PreciseTimestamp(),
                          default=timeutils.utcnow)
    meter.create_column(c)


def downgrade(migrate_engine):
    meta = sqlalchemy.MetaData(bind=migrate_engine)
    meter = sqlalchemy.Table('meter', meta, autoload=True)
    meter.drop_column('recorded_at')

########NEW FILE########
__FILENAME__ = 030_rename_meter_table
# -*- encoding: utf-8 -*-
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import migrate
import sqlalchemy as sa


def _handle_meter_indices(meta, downgrade=False):
    if meta.bind.engine.name == 'sqlite':
        return

    resource = sa.Table('resource', meta, autoload=True)
    project = sa.Table('project', meta, autoload=True)
    user = sa.Table('user', meta, autoload=True)
    meter = sa.Table('meter', meta, autoload=True)

    indices = [(sa.Index('ix_meter_timestamp', meter.c.timestamp),
                sa.Index('ix_sample_timestamp', meter.c.timestamp)),
               (sa.Index('ix_meter_user_id', meter.c.user_id),
                sa.Index('ix_sample_user_id', meter.c.user_id)),
               (sa.Index('ix_meter_project_id', meter.c.project_id),
                sa.Index('ix_sample_project_id', meter.c.project_id)),
               (sa.Index('idx_meter_rid_cname', meter.c.resource_id,
                         meter.c.counter_name),
                sa.Index('idx_sample_rid_cname', meter.c.resource_id,
                         meter.c.counter_name))]

    fk_params = [({'columns': [meter.c.resource_id],
                   'refcolumns': [resource.c.id]},
                 'fk_meter_resource_id',
                 'fk_sample_resource_id'),
                 ({'columns': [meter.c.project_id],
                   'refcolumns': [project.c.id]},
                 'fk_meter_project_id',
                 'fk_sample_project_id'),
                 ({'columns': [meter.c.user_id],
                   'refcolumns': [user.c.id]},
                 'fk_meter_user_id',
                 'fk_sample_user_id')]

    for fk in fk_params:
        params = fk[0]
        if meta.bind.engine.name == 'mysql':
            params['name'] = fk[2] if downgrade else fk[1]
        migrate.ForeignKeyConstraint(**params).drop()

    for meter_ix, sample_ix in indices:
        meter_ix.create() if downgrade else meter_ix.drop()
        sample_ix.drop() if downgrade else sample_ix.create()

    for fk in fk_params:
        params = fk[0]
        if meta.bind.engine.name == 'mysql':
            params['name'] = fk[1] if downgrade else fk[2]
        migrate.ForeignKeyConstraint(**params).create()


def _alter_sourceassoc(meta, t_name, ix_name, post_action=False):
    if meta.bind.engine.name == 'sqlite':
        return

    sourceassoc = sa.Table('sourceassoc', meta, autoload=True)
    table = sa.Table(t_name, meta, autoload=True)
    user = sa.Table('user', meta, autoload=True)

    c_name = '%s_id' % t_name
    col = getattr(sourceassoc.c, c_name)
    uniq_name = 'uniq_sourceassoc0%s0user_id' % c_name

    uniq_cols = (c_name, 'user_id')
    param = {'columns': [col],
             'refcolumns': [table.c.id]}
    user_param = {'columns': [sourceassoc.c.user_id],
                  'refcolumns': [user.c.id]}
    if meta.bind.engine.name == 'mysql':
        param['name'] = 'fk_sourceassoc_%s' % c_name
        user_param['name'] = 'fk_sourceassoc_user_id'

    actions = [migrate.ForeignKeyConstraint(**user_param),
               migrate.ForeignKeyConstraint(**param),
               sa.Index(ix_name, sourceassoc.c.source_id, col),
               migrate.UniqueConstraint(*uniq_cols, table=sourceassoc,
                                        name=uniq_name)]
    for action in actions:
        action.create() if post_action else action.drop()


def upgrade(migrate_engine):
    meta = sa.MetaData(bind=migrate_engine)

    _handle_meter_indices(meta)
    meter = sa.Table('meter', meta, autoload=True)
    meter.rename('sample')

    _alter_sourceassoc(meta, 'meter', 'idx_sm')
    sourceassoc = sa.Table('sourceassoc', meta, autoload=True)
    sourceassoc.c.meter_id.alter(name='sample_id')
    #re-bind metadata to pick up alter name change
    meta = sa.MetaData(bind=migrate_engine)
    _alter_sourceassoc(meta, 'sample', 'idx_ss', True)


def downgrade(migrate_engine):
    meta = sa.MetaData(bind=migrate_engine)

    sample = sa.Table('sample', meta, autoload=True)
    sample.rename('meter')
    _handle_meter_indices(meta, True)

    _alter_sourceassoc(meta, 'sample', 'idx_ss')
    sourceassoc = sa.Table('sourceassoc', meta, autoload=True)
    sourceassoc.c.sample_id.alter(name='meter_id')
    meta = sa.MetaData(bind=migrate_engine)
    _alter_sourceassoc(meta, 'meter', 'idx_sm', True)

########NEW FILE########
__FILENAME__ = 031_add_new_meter_table
#
# Copyright 2013 OpenStack Foundation
# All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import migrate
import sqlalchemy as sa


def handle_rid_index(meta, downgrade=False):
    if meta.bind.engine.name == 'sqlite':
        return

    resource = sa.Table('resource', meta, autoload=True)
    sample = sa.Table('sample', meta, autoload=True)
    params = {'columns': [sample.c.resource_id],
              'refcolumns': [resource.c.id],
              'name': 'fk_sample_resource_id'}
    if meta.bind.engine.name == 'mysql':
        # For mysql dialect all dependent FK should be removed
        #  before index create/delete
        migrate.ForeignKeyConstraint(**params).drop()

    index = sa.Index('idx_sample_rid_cname', sample.c.resource_id,
                     sample.c.counter_name)
    index.create() if downgrade else index.drop()

    if meta.bind.engine.name == 'mysql':
        migrate.ForeignKeyConstraint(**params).create()


def upgrade(migrate_engine):
    meta = sa.MetaData(bind=migrate_engine)
    meter = sa.Table(
        'meter', meta,
        sa.Column('id', sa.Integer, primary_key=True),
        sa.Column('name', sa.String(255), nullable=False),
        sa.Column('type', sa.String(255)),
        sa.Column('unit', sa.String(255)),
        sa.UniqueConstraint('name', 'type', 'unit', name='def_unique'),
        mysql_engine='InnoDB',
        mysql_charset='utf8'
    )
    meter.create()
    sample = sa.Table('sample', meta, autoload=True)
    query = sa.select([sample.c.counter_name, sample.c.counter_type,
                       sample.c.counter_unit]).distinct()
    for row in query.execute():
        meter.insert().values(name=row['counter_name'],
                              type=row['counter_type'],
                              unit=row['counter_unit']).execute()

    meter_id = sa.Column('meter_id', sa.Integer)
    meter_id.create(sample)
    params = {'columns': [sample.c.meter_id],
              'refcolumns': [meter.c.id]}
    if migrate_engine.name == 'mysql':
        params['name'] = 'fk_sample_meter_id'
    if migrate_engine.name != 'sqlite':
        migrate.ForeignKeyConstraint(**params).create()

    index = sa.Index('ix_meter_name', meter.c.name)
    index.create(bind=migrate_engine)

    for row in sa.select([meter]).execute():
        sample.update()\
            .where(sa.and_(sample.c.counter_name == row['name'],
                           sample.c.counter_type == row['type'],
                           sample.c.counter_unit == row['unit']))\
            .values({sample.c.meter_id: row['id']}).execute()

    handle_rid_index(meta)

    sample.c.counter_name.drop()
    sample.c.counter_type.drop()
    sample.c.counter_unit.drop()
    sample.c.counter_volume.alter(name='volume')


def downgrade(migrate_engine):
    meta = sa.MetaData(bind=migrate_engine)
    sample = sa.Table('sample', meta, autoload=True)
    sample.c.volume.alter(name='counter_volume')
    sa.Column('counter_name', sa.String(255)).create(sample)
    sa.Column('counter_type', sa.String(255)).create(sample)
    sa.Column('counter_unit', sa.String(255)).create(sample)
    meter = sa.Table('meter', meta, autoload=True)
    for row in sa.select([meter]).execute():
        sample.update()\
            .where(sample.c.meter_id == row['id'])\
            .values({sample.c.counter_name: row['name'],
                     sample.c.counter_type: row['type'],
                     sample.c.counter_unit: row['unit']}).execute()

    params = {'columns': [sample.c.meter_id],
              'refcolumns': [meter.c.id]}
    if migrate_engine.name == 'mysql':
        params['name'] = 'fk_sample_meter_id'
    if migrate_engine.name != 'sqlite':
        migrate.ForeignKeyConstraint(**params).drop()

    handle_rid_index(meta, True)

    sample.c.meter_id.drop()
    meter.drop()

########NEW FILE########
__FILENAME__ = 032_add_alarm_time_constraints
# -*- encoding: utf-8 -*-
#
# Author: Nejc Saje <nejc.saje@xlab.si>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from sqlalchemy import Column
from sqlalchemy import MetaData
from sqlalchemy import Table
from sqlalchemy import Text


def upgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    alarm = Table('alarm', meta, autoload=True)
    time_constraints = Column('time_constraints', Text())
    alarm.create_column(time_constraints)


def downgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    alarm = Table('alarm', meta, autoload=True)
    time_constraints = Column('time_constraints', Text())
    alarm.drop_column(time_constraints)

########NEW FILE########
__FILENAME__ = 033_alarm_id_rename
# -*- encoding: utf-8 -*-
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from sqlalchemy import MetaData
from sqlalchemy import Table


def upgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    users = Table('alarm', meta, autoload=True)
    users.c.id.alter(name='alarm_id')


def downgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)
    users = Table('alarm', meta, autoload=True)
    users.c.alarm_id.alter(name='id')

########NEW FILE########
__FILENAME__ = 034_drop_dump_tables
# -*- encoding: utf-8 -*-
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import sqlalchemy as sa

TABLES_012 = ['resource', 'sourceassoc', 'user',
              'project', 'meter', 'source', 'alarm']
TABLES_027 = ['user', 'project', 'alarm']


def upgrade(migrate_engine):
    meta = sa.MetaData(bind=migrate_engine)
    for table_name in TABLES_027:
        try:
            sa.Table('dump027_' + table_name, meta, autoload=True)\
                .drop(checkfirst=True)
        except sa.exc.NoSuchTableError:
            pass
    for table_name in TABLES_012:
        try:
            sa.Table('dump_' + table_name, meta, autoload=True)\
                .drop(checkfirst=True)
        except sa.exc.NoSuchTableError:
            pass


def downgrade(migrate_engine):
    pass

########NEW FILE########
__FILENAME__ = 035_drop_user_project_tables
# -*- encoding: utf-8 -*-
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from migrate import ForeignKeyConstraint, UniqueConstraint
import sqlalchemy as sa
from sqlalchemy.sql.expression import select, Alias, not_, and_, exists

TABLES_DROP = ['user', 'project']
TABLES = ['user', 'project', 'sourceassoc', 'sample',
          'resource', 'alarm_history']

INDEXES = {
    "sample": (('user_id', 'user', 'id'),
               ('project_id', 'project', 'id')),
    "sourceassoc": (('user_id', 'user', 'id'),
                    ('project_id', 'project', 'id')),
    "resource": (('user_id', 'user', 'id'),
                 ('project_id', 'project', 'id')),
    "alarm_history": (('user_id', 'user', 'id'),
                      ('project_id', 'project', 'id'),
                      ('on_behalf_of', 'project', 'id')),
}


def upgrade(migrate_engine):
    meta = sa.MetaData(bind=migrate_engine)
    load_tables = dict((table_name, sa.Table(table_name, meta,
                                             autoload=True))
                       for table_name in TABLES)

    if migrate_engine.name != 'sqlite':
        for table_name, indexes in INDEXES.items():
            table = load_tables[table_name]
            for column, ref_table_name, ref_column_name in indexes:
                ref_table = load_tables[ref_table_name]
                params = {'columns': [table.c[column]],
                          'refcolumns': [ref_table.c[ref_column_name]]}

                if migrate_engine.name == "mysql" and \
                        table_name != 'alarm_history':
                    params['name'] = "_".join(('fk', table_name, column))
                elif migrate_engine.name == "postgresql" and \
                        table_name == "sample":
                    # The fk contains the old table name
                    params['name'] = "_".join(('meter', column, 'fkey'))

                fkey = ForeignKeyConstraint(**params)
                fkey.drop()

    sourceassoc = load_tables['sourceassoc']
    if migrate_engine.name != 'sqlite':
        idx = sa.Index('idx_su', sourceassoc.c.source_id,
                       sourceassoc.c.user_id)
        idx.drop(bind=migrate_engine)
        idx = sa.Index('idx_sp', sourceassoc.c.source_id,
                       sourceassoc.c.project_id)
        idx.drop(bind=migrate_engine)

        params = {}
        if migrate_engine.name == "mysql":
            params = {'name': 'uniq_sourceassoc0sample_id'}
        uc = UniqueConstraint('sample_id', table=sourceassoc, **params)
        uc.create()

        params = {}
        if migrate_engine.name == "mysql":
            params = {'name': 'uniq_sourceassoc0sample_id0user_id'}
        uc = UniqueConstraint('sample_id', 'user_id',
                              table=sourceassoc, **params)
        uc.drop()
    sourceassoc.c.user_id.drop()
    sourceassoc.c.project_id.drop()

    for table_name in TABLES_DROP:
        sa.Table(table_name, meta, autoload=True).drop()


def downgrade(migrate_engine):
    meta = sa.MetaData(bind=migrate_engine)
    user = sa.Table(
        'user', meta,
        sa.Column('id', sa.String(255), primary_key=True),
        mysql_engine='InnoDB',
        mysql_charset='utf8',
    )

    project = sa.Table(
        'project', meta,
        sa.Column('id', sa.String(255), primary_key=True),
        mysql_engine='InnoDB',
        mysql_charset='utf8',
    )

    tables = [project, user]
    for i in sorted(tables):
        i.create()

    load_tables = dict((table_name, sa.Table(table_name, meta, autoload=True))
                       for table_name in TABLES)

    # Restore the sourceassoc columns and constraints
    sourceassoc = load_tables['sourceassoc']
    user_id = sa.Column('user_id', sa.String(255))
    project_id = sa.Column('project_id', sa.String(255))
    sourceassoc.create_column(user_id)
    sourceassoc.create_column(project_id)

    if migrate_engine.name != 'sqlite':
        params = {}
        if migrate_engine.name == "mysql":
            params = {'name': 'uniq_sourceassoc0sample_id0user_id'}
        uc = UniqueConstraint('sample_id', 'user_id',
                              table=sourceassoc, **params)
        uc.create()

        params = {}
        if migrate_engine.name == "mysql":
            params = {'name': 'uniq_sourceassoc0sample_id'}
        uc = UniqueConstraint('sample_id', table=sourceassoc, **params)
        uc.drop()

        idx = sa.Index('idx_su', sourceassoc.c.source_id,
                       sourceassoc.c.user_id)
        idx.create(bind=migrate_engine)
        idx = sa.Index('idx_sp', sourceassoc.c.source_id,
                       sourceassoc.c.project_id)
        idx.create(bind=migrate_engine)

    # Restore the user/project columns and constraints in all tables
    for table_name, indexes in INDEXES.items():
        table = load_tables[table_name]
        for column, ref_table_name, ref_column_name in indexes:
            ref_table = load_tables[ref_table_name]
            c = getattr(Alias(table).c, column)
            except_q = exists([getattr(ref_table.c, ref_column_name)])
            q = select([c]).where(and_(c != sa.null(), not_(except_q)))
            q = q.distinct()

            # NOTE(sileht): workaround for
            # https://bitbucket.org/zzzeek/sqlalchemy/
            # issue/3044/insert-from-select-union_all
            q.select = lambda: q

            sql_ins = ref_table.insert().from_select(
                [getattr(ref_table.c, ref_column_name)], q)
            try:
                migrate_engine.execute(sql_ins)
            except TypeError:
                # from select is empty
                pass

            if migrate_engine.name != 'sqlite':
                params = {'columns': [table.c[column]],
                          'refcolumns': [ref_table.c[ref_column_name]]}

                if migrate_engine.name == "mysql" and \
                        table_name != 'alarm_history':
                    params['name'] = "_".join(('fk', table_name, column))
                elif migrate_engine.name == "postgresql" and \
                        table_name == "sample":
                    # The fk contains the old table name
                    params['name'] = "_".join(('meter', column, 'fkey'))

                fkey = ForeignKeyConstraint(**params)
                fkey.create()

########NEW FILE########
__FILENAME__ = 036_drop_sourceassoc_resource_tables
# -*- encoding: utf-8 -*-
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from migrate import ForeignKeyConstraint, UniqueConstraint
import sqlalchemy as sa

from ceilometer.storage.sqlalchemy import migration


TABLES = ['sample', 'resource', 'source', 'sourceassoc']
DROP_TABLES = ['resource', 'source', 'sourceassoc']

INDEXES = {
    "sample": (('resource_id', 'resource', 'id'),),
    "sourceassoc": (('sample_id', 'sample', 'id'),
                    ('resource_id', 'resource', 'id'),
                    ('source_id', 'source', 'id'))
}


def upgrade(migrate_engine):
    meta = sa.MetaData(bind=migrate_engine)
    load_tables = dict((table_name, sa.Table(table_name, meta,
                                             autoload=True))
                       for table_name in TABLES)

    # drop foreign keys
    if migrate_engine.name != 'sqlite':
        for table_name, indexes in INDEXES.items():
            table = load_tables[table_name]
            for column, ref_table_name, ref_column_name in indexes:
                ref_table = load_tables[ref_table_name]
                params = {'columns': [table.c[column]],
                          'refcolumns': [ref_table.c[ref_column_name]]}
                fk_table_name = table_name
                if migrate_engine.name == "mysql":
                    params['name'] = "_".join(('fk', fk_table_name, column))
                elif (migrate_engine.name == "postgresql" and
                      table_name == 'sample'):
                    # fk was not renamed in script 030
                    params['name'] = "_".join(('meter', column, 'fkey'))
                fkey = ForeignKeyConstraint(**params)
                fkey.drop()

    # create source field in sample
    sample = load_tables['sample']
    sample.create_column(sa.Column('source_id', sa.String(255)))

    # move source values to samples
    sourceassoc = load_tables['sourceassoc']
    query = sa.select([sourceassoc.c.sample_id, sourceassoc.c.source_id])\
        .where(sourceassoc.c.sample_id.isnot(None))
    for sample_id, source_id in migration.paged(query):
        sample.update().where(sample_id == sample.c.id)\
            .values({'source_id': source_id}).execute()

    # drop tables
    for table_name in DROP_TABLES:
        sa.Table(table_name, meta, autoload=True).drop()


def downgrade(migrate_engine):
    meta = sa.MetaData(bind=migrate_engine)
    sample = sa.Table('sample', meta, autoload=True)
    resource = sa.Table(
        'resource', meta,
        sa.Column('id', sa.String(255), primary_key=True),
        sa.Column('resource_metadata', sa.Text),
        sa.Column('user_id', sa.String(255)),
        sa.Column('project_id', sa.String(255)),
        sa.Index('ix_resource_project_id', 'project_id'),
        sa.Index('ix_resource_user_id', 'user_id'),
        sa.Index('resource_user_id_project_id_key', 'user_id', 'project_id'),
        mysql_engine='InnoDB',
        mysql_charset='utf8',
    )
    resource.create()

    source = sa.Table(
        'source', meta,
        sa.Column('id', sa.String(255), primary_key=True),
        mysql_engine='InnoDB',
        mysql_charset='utf8',
    )
    source.create()

    sourceassoc = sa.Table(
        'sourceassoc', meta,
        sa.Column('sample_id', sa.Integer),
        sa.Column('resource_id', sa.String(255)),
        sa.Column('source_id', sa.String(255)),
        sa.Index('idx_sr', 'source_id', 'resource_id'),
        sa.Index('idx_ss', 'source_id', 'sample_id'),
        mysql_engine='InnoDB',
        mysql_charset='utf8',
    )
    sourceassoc.create()

    params = {}
    if migrate_engine.name == "mysql":
        params = {'name': 'uniq_sourceassoc0sample_id'}
    uc = UniqueConstraint('sample_id', table=sourceassoc, **params)
    uc.create()

    # reload source/resource tables.
    # NOTE(gordc): fine to skip non-id attributes in table since
    # they're constantly updated and not used by api
    for table, col in [(source, 'source_id'), (resource, 'resource_id')]:
        q = sa.select([sample.c[col]]).distinct()
        # NOTE(sileht): workaround for
        # https://bitbucket.org/zzzeek/sqlalchemy/
        # issue/3044/insert-from-select-union_all
        q.select = lambda: q
        sql_ins = table.insert().from_select([table.c.id], q)
        try:
            migrate_engine.execute(sql_ins)
        except TypeError:
            # from select is empty
            pass

    # reload sourceassoc tables
    for ref_col, col in [('id', 'sample_id'), ('resource_id', 'resource_id')]:
        q = sa.select([sample.c.source_id, sample.c[ref_col]]).distinct()
        q.select = lambda: q
        sql_ins = sourceassoc.insert().from_select([sourceassoc.c.source_id,
                                                    sourceassoc.c[col]], q)
        try:
            migrate_engine.execute(sql_ins)
        except TypeError:
            # from select is empty
            pass

    sample.c.source_id.drop()

    load_tables = dict((table_name, sa.Table(table_name, meta,
                                             autoload=True))
                       for table_name in TABLES)

    # add foreign keys
    if migrate_engine.name != 'sqlite':
        for table_name, indexes in INDEXES.items():
            table = load_tables[table_name]
            for column, ref_table_name, ref_column_name in indexes:
                ref_table = load_tables[ref_table_name]
                params = {'columns': [table.c[column]],
                          'refcolumns': [ref_table.c[ref_column_name]]}
                fk_table_name = table_name
                if migrate_engine.name == "mysql":
                    params['name'] = "_".join(('fk', fk_table_name, column))
                elif (migrate_engine.name == "postgresql" and
                      table_name == 'sample'):
                    # fk was not renamed in script 030
                    params['name'] = "_".join(('meter', column, 'fkey'))
                fkey = ForeignKeyConstraint(**params)
                fkey.create()

########NEW FILE########
__FILENAME__ = migration
# -*- encoding: utf-8 -*-
#
# Author: John Tran <jhtran@att.com>
#         Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.


def paged(query, size=1000):
    """Page query results

    :param query: the SQLAlchemy query to execute
    :param size: the max page size
    return: generator with query data
    """
    offset = 0
    while True:
        page = query.offset(offset).limit(size).execute()
        if page.rowcount <= 0:
            # There are no more rows
            break
        for row in page:
            yield row
        offset += size

########NEW FILE########
__FILENAME__ = models
# -*- encoding: utf-8 -*-
#
# Author: John Tran <jhtran@att.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

"""
SQLAlchemy models for Ceilometer data.
"""

import json

from sqlalchemy import Column, Integer, String, ForeignKey, \
    Index, UniqueConstraint, BigInteger, join
from sqlalchemy import Float, Boolean, Text, DateTime
from sqlalchemy.dialects.mysql import DECIMAL
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import backref
from sqlalchemy.orm import column_property
from sqlalchemy.orm import relationship
from sqlalchemy.types import TypeDecorator

from ceilometer.openstack.common import timeutils
from ceilometer.storage import models as api_models
from ceilometer import utils


class JSONEncodedDict(TypeDecorator):
    "Represents an immutable structure as a json-encoded string."

    impl = String

    def process_bind_param(self, value, dialect):
        if value is not None:
            value = json.dumps(value)
        return value

    def process_result_value(self, value, dialect):
        if value is not None:
            value = json.loads(value)
        return value


class PreciseTimestamp(TypeDecorator):
    """Represents a timestamp precise to the microsecond."""

    impl = DateTime

    def load_dialect_impl(self, dialect):
        if dialect.name == 'mysql':
            return dialect.type_descriptor(DECIMAL(precision=20,
                                                   scale=6,
                                                   asdecimal=True))
        return self.impl

    def process_bind_param(self, value, dialect):
        if value is None:
            return value
        elif dialect.name == 'mysql':
            return utils.dt_to_decimal(value)
        return value

    def process_result_value(self, value, dialect):
        if value is None:
            return value
        elif dialect.name == 'mysql':
            return utils.decimal_to_dt(value)
        return value


class CeilometerBase(object):
    """Base class for Ceilometer Models."""
    __table_args__ = {'mysql_charset': "utf8",
                      'mysql_engine': "InnoDB"}
    __table_initialized__ = False

    def __setitem__(self, key, value):
        setattr(self, key, value)

    def __getitem__(self, key):
        return getattr(self, key)

    def update(self, values):
        """Make the model object behave like a dict."""
        for k, v in values.iteritems():
            setattr(self, k, v)


Base = declarative_base(cls=CeilometerBase)


class MetaText(Base):
    """Metering text metadata."""

    __tablename__ = 'metadata_text'
    __table_args__ = (
        Index('ix_meta_text_key', 'meta_key'),
    )
    id = Column(Integer, ForeignKey('sample.id'), primary_key=True)
    meta_key = Column(String(255), primary_key=True)
    value = Column(Text)


class MetaBool(Base):
    """Metering boolean metadata."""

    __tablename__ = 'metadata_bool'
    __table_args__ = (
        Index('ix_meta_bool_key', 'meta_key'),
    )
    id = Column(Integer, ForeignKey('sample.id'), primary_key=True)
    meta_key = Column(String(255), primary_key=True)
    value = Column(Boolean)


class MetaBigInt(Base):
    """Metering integer metadata."""

    __tablename__ = 'metadata_int'
    __table_args__ = (
        Index('ix_meta_int_key', 'meta_key'),
    )
    id = Column(Integer, ForeignKey('sample.id'), primary_key=True)
    meta_key = Column(String(255), primary_key=True)
    value = Column(BigInteger, default=False)


class MetaFloat(Base):
    """Metering float metadata."""

    __tablename__ = 'metadata_float'
    __table_args__ = (
        Index('ix_meta_float_key', 'meta_key'),
    )
    id = Column(Integer, ForeignKey('sample.id'), primary_key=True)
    meta_key = Column(String(255), primary_key=True)
    value = Column(Float(53), default=False)


class Meter(Base):
    """Meter definition data."""

    __tablename__ = 'meter'
    __table_args__ = (
        UniqueConstraint('name', 'type', 'unit', name='def_unique'),
        Index('ix_meter_name', 'name')
    )
    id = Column(Integer, primary_key=True)
    name = Column(String(255), nullable=False)
    type = Column(String(255))
    unit = Column(String(255))


class Sample(Base):
    """Metering data."""

    __tablename__ = 'sample'
    __table_args__ = (
        Index('ix_sample_timestamp', 'timestamp'),
        Index('ix_sample_user_id', 'user_id'),
        Index('ix_sample_project_id', 'project_id'),
    )
    id = Column(Integer, primary_key=True)
    meter_id = Column(Integer, ForeignKey('meter.id'))
    user_id = Column(String(255))
    project_id = Column(String(255))
    resource_id = Column(String(255))
    resource_metadata = Column(JSONEncodedDict())
    volume = Column(Float(53))
    timestamp = Column(PreciseTimestamp(), default=lambda: timeutils.utcnow())
    recorded_at = Column(PreciseTimestamp(),
                         default=lambda: timeutils.utcnow())
    message_signature = Column(String(1000))
    message_id = Column(String(1000))
    source_id = Column(String(255))
    meta_text = relationship("MetaText", backref="sample",
                             cascade="all, delete-orphan")
    meta_float = relationship("MetaFloat", backref="sample",
                              cascade="all, delete-orphan")
    meta_int = relationship("MetaBigInt", backref="sample",
                            cascade="all, delete-orphan")
    meta_bool = relationship("MetaBool", backref="sample",
                             cascade="all, delete-orphan")


class MeterSample(Base):
    """Helper model as many of the filters work against Sample data
    joined with Meter data.
    """
    meter = Meter.__table__
    sample = Sample.__table__
    __table__ = join(meter, sample)

    id = column_property(sample.c.id)
    meter_id = column_property(meter.c.id, sample.c.meter_id)
    counter_name = column_property(meter.c.name)
    counter_type = column_property(meter.c.type)
    counter_unit = column_property(meter.c.unit)
    counter_volume = column_property(sample.c.volume)


class Alarm(Base):
    """Define Alarm data."""
    __tablename__ = 'alarm'
    __table_args__ = (
        Index('ix_alarm_user_id', 'user_id'),
        Index('ix_alarm_project_id', 'project_id'),
    )
    alarm_id = Column(String(255), primary_key=True)
    enabled = Column(Boolean)
    name = Column(Text)
    type = Column(String(50))
    description = Column(Text)
    timestamp = Column(PreciseTimestamp, default=lambda: timeutils.utcnow())

    user_id = Column(String(255))
    project_id = Column(String(255))

    state = Column(String(255))
    state_timestamp = Column(PreciseTimestamp,
                             default=lambda: timeutils.utcnow())

    ok_actions = Column(JSONEncodedDict)
    alarm_actions = Column(JSONEncodedDict)
    insufficient_data_actions = Column(JSONEncodedDict)
    repeat_actions = Column(Boolean)

    rule = Column(JSONEncodedDict)
    time_constraints = Column(JSONEncodedDict)


class AlarmChange(Base):
    """Define AlarmChange data."""
    __tablename__ = 'alarm_history'
    __table_args__ = (
        Index('ix_alarm_history_alarm_id', 'alarm_id'),
    )
    event_id = Column(String(255), primary_key=True)
    alarm_id = Column(String(255))
    on_behalf_of = Column(String(255))
    project_id = Column(String(255))
    user_id = Column(String(255))
    type = Column(String(20))
    detail = Column(Text)
    timestamp = Column(PreciseTimestamp, default=lambda: timeutils.utcnow())


class EventType(Base):
    """Types of event records."""
    __tablename__ = 'event_type'

    id = Column(Integer, primary_key=True)
    desc = Column(String(255), unique=True)

    def __init__(self, event_type):
        self.desc = event_type

    def __repr__(self):
        return "<EventType: %s>" % self.desc


class Event(Base):
    __tablename__ = 'event'
    __table_args__ = (
        Index('ix_event_message_id', 'message_id'),
        Index('ix_event_type_id', 'event_type_id'),
        Index('ix_event_generated', 'generated')
    )
    id = Column(Integer, primary_key=True)
    message_id = Column(String(50), unique=True)
    generated = Column(PreciseTimestamp())

    event_type_id = Column(Integer, ForeignKey('event_type.id'))
    event_type = relationship("EventType", backref=backref('event_type'))

    def __init__(self, message_id, event_type, generated):
        self.message_id = message_id
        self.event_type = event_type
        self.generated = generated

    def __repr__(self):
        return "<Event %d('Event: %s %s, Generated: %s')>" % (self.id,
                                                              self.message_id,
                                                              self.event_type,
                                                              self.generated)


class TraitType(Base):
    """Types of event traits. A trait type includes a description
    and a data type. Uniqueness is enforced compositely on the
    data_type and desc fields. This is to accommodate cases, such as
    'generated', which, depending on the corresponding event,
    could be a date, a boolean, or a float.

    """
    __tablename__ = 'trait_type'
    __table_args__ = (
        UniqueConstraint('desc', 'data_type', name='tt_unique'),
        Index('ix_trait_type', 'desc')
    )

    id = Column(Integer, primary_key=True)
    desc = Column(String(255))
    data_type = Column(Integer)

    def __init__(self, desc, data_type):
        self.desc = desc
        self.data_type = data_type

    def __repr__(self):
        return "<TraitType: %s:%d>" % (self.desc, self.data_type)


class Trait(Base):
    __tablename__ = 'trait'
    __table_args__ = (
        Index('ix_trait_t_int', 't_int'),
        Index('ix_trait_t_string', 't_string'),
        Index('ix_trait_t_datetime', 't_datetime'),
        Index('ix_trait_t_float', 't_float'),
    )
    id = Column(Integer, primary_key=True)

    trait_type_id = Column(Integer, ForeignKey('trait_type.id'))
    trait_type = relationship("TraitType", backref=backref('trait_type'))

    t_string = Column(String(255), nullable=True, default=None)
    t_float = Column(Float(53), nullable=True, default=None)
    t_int = Column(Integer, nullable=True, default=None)
    t_datetime = Column(PreciseTimestamp(), nullable=True, default=None)

    event_id = Column(Integer, ForeignKey('event.id'))
    event = relationship("Event", backref=backref('event', order_by=id))

    _value_map = {api_models.Trait.TEXT_TYPE: 't_string',
                  api_models.Trait.FLOAT_TYPE: 't_float',
                  api_models.Trait.INT_TYPE: 't_int',
                  api_models.Trait.DATETIME_TYPE: 't_datetime'}

    def __init__(self, trait_type, event, t_string=None,
                 t_float=None, t_int=None, t_datetime=None):
        self.trait_type = trait_type
        self.t_string = t_string
        self.t_float = t_float
        self.t_int = t_int
        self.t_datetime = t_datetime
        self.event = event

    def get_value(self):
        if self.trait_type is None:
            dtype = None
        else:
            dtype = self.trait_type.data_type

        if dtype == api_models.Trait.INT_TYPE:
            return self.t_int
        if dtype == api_models.Trait.FLOAT_TYPE:
            return self.t_float
        if dtype == api_models.Trait.DATETIME_TYPE:
            return self.t_datetime
        if dtype == api_models.Trait.TEXT_TYPE:
            return self.t_string

        return None

    def __repr__(self):
        name = self.trait_type.name if self.trait_type else None
        data_type = self.trait_type.data_type if self.trait_type\
            else api_models.Trait.NONE_TYPE

        return "<Trait(%s) %d=%s/%s/%s/%s on %s>" % (name,
                                                     data_type,
                                                     self.t_string,
                                                     self.t_float,
                                                     self.t_int,
                                                     self.t_datetime,
                                                     self.event)

########NEW FILE########
__FILENAME__ = agentbase
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
# Copyright © 2013 Intel corp.
# Copyright © 2013 eNovance
# Copyright © 2014 Red Hat, Inc
#
# Authors: Yunhong Jiang <yunhong.jiang@intel.com>
#          Julien Danjou <julien@danjou.info>
#          Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import abc
import copy
import datetime

import mock
import six
from stevedore import extension

from ceilometer.openstack.common.fixture import config
from ceilometer.openstack.common.fixture import mockpatch
from ceilometer import pipeline
from ceilometer import plugin
from ceilometer import publisher
from ceilometer.publisher import test as test_publisher
from ceilometer import sample
from ceilometer.tests import base
from ceilometer import transformer


class TestSample(sample.Sample):
    def __init__(self, name, type, unit, volume, user_id, project_id,
                 resource_id, timestamp, resource_metadata, source=None):
        super(TestSample, self).__init__(name, type, unit, volume, user_id,
                                         project_id, resource_id, timestamp,
                                         resource_metadata, source)

    def __eq__(self, other):
        if isinstance(other, self.__class__):
            return self.__dict__ == other.__dict__
        return False

    def __ne__(self, other):
        return not self.__eq__(other)


default_test_data = TestSample(
    name='test',
    type=sample.TYPE_CUMULATIVE,
    unit='',
    volume=1,
    user_id='test',
    project_id='test',
    resource_id='test_run_tasks',
    timestamp=datetime.datetime.utcnow().isoformat(),
    resource_metadata={'name': 'Pollster'},
)


class TestPollster(plugin.PollsterBase):
    test_data = default_test_data

    def get_samples(self, manager, cache, resources=None):
        resources = resources or []
        self.samples.append((manager, resources))
        self.resources.extend(resources)
        c = copy.copy(self.test_data)
        c.resource_metadata['resources'] = resources
        return [c]


class TestPollsterException(TestPollster):
    def get_samples(self, manager, cache, resources=None):
        resources = resources or []
        self.samples.append((manager, resources))
        self.resources.extend(resources)
        raise Exception()


class TestDiscovery(plugin.DiscoveryBase):
    def discover(self, param=None):
        self.params.append(param)
        return self.resources


class TestDiscoveryException(plugin.DiscoveryBase):
    def discover(self, param=None):
        self.params.append(param)
        raise Exception()


@six.add_metaclass(abc.ABCMeta)
class BaseAgentManagerTestCase(base.BaseTestCase):

    class Pollster(TestPollster):
        samples = []
        resources = []
        test_data = default_test_data

    class PollsterAnother(TestPollster):
        samples = []
        resources = []
        test_data = TestSample(
            name='testanother',
            type=default_test_data.type,
            unit=default_test_data.unit,
            volume=default_test_data.volume,
            user_id=default_test_data.user_id,
            project_id=default_test_data.project_id,
            resource_id=default_test_data.resource_id,
            timestamp=default_test_data.timestamp,
            resource_metadata=default_test_data.resource_metadata)

    class PollsterException(TestPollsterException):
        samples = []
        resources = []
        test_data = TestSample(
            name='testexception',
            type=default_test_data.type,
            unit=default_test_data.unit,
            volume=default_test_data.volume,
            user_id=default_test_data.user_id,
            project_id=default_test_data.project_id,
            resource_id=default_test_data.resource_id,
            timestamp=default_test_data.timestamp,
            resource_metadata=default_test_data.resource_metadata)

    class PollsterExceptionAnother(TestPollsterException):
        samples = []
        resources = []
        test_data = TestSample(
            name='testexceptionanother',
            type=default_test_data.type,
            unit=default_test_data.unit,
            volume=default_test_data.volume,
            user_id=default_test_data.user_id,
            project_id=default_test_data.project_id,
            resource_id=default_test_data.resource_id,
            timestamp=default_test_data.timestamp,
            resource_metadata=default_test_data.resource_metadata)

    class Discovery(TestDiscovery):
        params = []
        resources = []

    class DiscoveryAnother(TestDiscovery):
        params = []
        resources = []

    class DiscoveryException(TestDiscoveryException):
        params = []

    def setup_pipeline(self):
        self.transformer_manager = transformer.TransformerExtensionManager(
            'ceilometer.transformer',
        )
        self.mgr.pipeline_manager = pipeline.PipelineManager(
            self.pipeline_cfg,
            self.transformer_manager)

    def create_pollster_manager(self):
        return extension.ExtensionManager.make_test_instance(
            [
                extension.Extension(
                    'test',
                    None,
                    None,
                    self.Pollster(), ),
                extension.Extension(
                    'testanother',
                    None,
                    None,
                    self.PollsterAnother(), ),
                extension.Extension(
                    'testexception',
                    None,
                    None,
                    self.PollsterException(), ),
                extension.Extension(
                    'testexceptionanother',
                    None,
                    None,
                    self.PollsterExceptionAnother(), ),
            ],
        )

    def create_discovery_manager(self):
        return extension.ExtensionManager.make_test_instance(
            [
                extension.Extension(
                    'testdiscovery',
                    None,
                    None,
                    self.Discovery(), ),
                extension.Extension(
                    'testdiscoveryanother',
                    None,
                    None,
                    self.DiscoveryAnother(), ),
                extension.Extension(
                    'testdiscoveryexception',
                    None,
                    None,
                    self.DiscoveryException(), ),
            ],
        )

    @abc.abstractmethod
    def create_manager(self):
        """Return subclass specific manager."""

    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    def setUp(self):
        super(BaseAgentManagerTestCase, self).setUp()
        self.mgr = self.create_manager()
        self.mgr.pollster_manager = self.create_pollster_manager()
        self.pipeline_cfg = [{
            'name': "test_pipeline",
            'interval': 60,
            'counters': ['test'],
            'resources': ['test://'] if self.source_resources else [],
            'transformers': [],
            'publishers': ["test"],
        }, ]
        self.setup_pipeline()
        self.CONF = self.useFixture(config.Config()).conf
        self.CONF.set_override(
            'pipeline_cfg_file',
            self.path_get('etc/ceilometer/pipeline.yaml')
        )
        self.useFixture(mockpatch.PatchObject(
            publisher, 'get_publisher', side_effect=self.get_publisher))

    def get_publisher(self, url, namespace=''):
        fake_drivers = {'test://': test_publisher.TestPublisher,
                        'new://': test_publisher.TestPublisher,
                        'rpc://': test_publisher.TestPublisher}
        return fake_drivers[url](url)

    def tearDown(self):
        self.Pollster.samples = []
        self.PollsterAnother.samples = []
        self.PollsterException.samples = []
        self.PollsterExceptionAnother.samples = []
        self.Pollster.resources = []
        self.PollsterAnother.resources = []
        self.PollsterException.resources = []
        self.PollsterExceptionAnother.resources = []
        self.Discovery.params = []
        self.DiscoveryAnother.params = []
        self.DiscoveryException.params = []
        self.Discovery.resources = []
        self.DiscoveryAnother.resources = []
        super(BaseAgentManagerTestCase, self).tearDown()

    def test_setup_polling_tasks(self):
        polling_tasks = self.mgr.setup_polling_tasks()
        self.assertEqual(1, len(polling_tasks))
        self.assertTrue(60 in polling_tasks.keys())
        per_task_resources = polling_tasks[60].resources
        self.assertEqual(1, len(per_task_resources))
        self.assertEqual(set(self.pipeline_cfg[0]['resources']),
                         set(per_task_resources['test'].resources))
        self.mgr.interval_task(polling_tasks.values()[0])
        pub = self.mgr.pipeline_manager.pipelines[0].publishers[0]
        del pub.samples[0].resource_metadata['resources']
        self.assertEqual(self.Pollster.test_data, pub.samples[0])

    def test_setup_polling_tasks_multiple_interval(self):
        self.pipeline_cfg.append({
            'name': "test_pipeline",
            'interval': 10,
            'counters': ['test'],
            'resources': ['test://'] if self.source_resources else [],
            'transformers': [],
            'publishers': ["test"],
        })
        self.setup_pipeline()
        polling_tasks = self.mgr.setup_polling_tasks()
        self.assertEqual(2, len(polling_tasks))
        self.assertTrue(60 in polling_tasks.keys())
        self.assertTrue(10 in polling_tasks.keys())

    def test_setup_polling_tasks_mismatch_counter(self):
        self.pipeline_cfg.append(
            {
                'name': "test_pipeline_1",
                'interval': 10,
                'counters': ['test_invalid'],
                'resources': ['invalid://'],
                'transformers': [],
                'publishers': ["test"],
            })
        polling_tasks = self.mgr.setup_polling_tasks()
        self.assertEqual(1, len(polling_tasks))
        self.assertTrue(60 in polling_tasks.keys())

    def test_setup_polling_task_same_interval(self):
        self.pipeline_cfg.append({
            'name': "test_pipeline",
            'interval': 60,
            'counters': ['testanother'],
            'resources': ['testanother://'] if self.source_resources else [],
            'transformers': [],
            'publishers': ["test"],
        })
        self.setup_pipeline()
        polling_tasks = self.mgr.setup_polling_tasks()
        self.assertEqual(1, len(polling_tasks))
        pollsters = polling_tasks.get(60).pollsters
        self.assertEqual(2, len(pollsters))
        per_task_resources = polling_tasks[60].resources
        self.assertEqual(2, len(per_task_resources))
        self.assertEqual(set(self.pipeline_cfg[0]['resources']),
                         set(per_task_resources['test'].resources))
        self.assertEqual(set(self.pipeline_cfg[1]['resources']),
                         set(per_task_resources['testanother'].resources))

    def test_interval_exception_isolation(self):
        self.pipeline_cfg = [
            {
                'name': "test_pipeline_1",
                'interval': 10,
                'counters': ['testexceptionanother'],
                'resources': ['test://'] if self.source_resources else [],
                'transformers': [],
                'publishers': ["test"],
            },
            {
                'name': "test_pipeline_2",
                'interval': 10,
                'counters': ['testexception'],
                'resources': ['test://'] if self.source_resources else [],
                'transformers': [],
                'publishers': ["test"],
            },
        ]
        self.mgr.pipeline_manager = pipeline.PipelineManager(
            self.pipeline_cfg,
            self.transformer_manager)

        polling_tasks = self.mgr.setup_polling_tasks()
        self.assertEqual(1, len(polling_tasks.keys()))
        polling_tasks.get(10)
        self.mgr.interval_task(polling_tasks.get(10))
        pub = self.mgr.pipeline_manager.pipelines[0].publishers[0]
        self.assertEqual(0, len(pub.samples))

    def test_agent_manager_start(self):
        mgr = self.create_manager()
        mgr.pollster_manager = self.mgr.pollster_manager
        mgr.create_polling_task = mock.MagicMock()
        mgr.tg = mock.MagicMock()
        mgr.start()
        self.assertTrue(mgr.tg.add_timer.called)

    def test_manager_exception_persistency(self):
        self.pipeline_cfg.append({
            'name': "test_pipeline",
            'interval': 60,
            'counters': ['testanother'],
            'transformers': [],
            'publishers': ["test"],
        })
        self.setup_pipeline()

    def _verify_discovery_params(self, expected):
        self.assertEqual(expected, self.Discovery.params)
        self.assertEqual(expected, self.DiscoveryAnother.params)
        self.assertEqual(expected, self.DiscoveryException.params)

    def _do_test_per_agent_discovery(self,
                                     discovered_resources,
                                     static_resources):
        self.mgr.discovery_manager = self.create_discovery_manager()
        if discovered_resources:
            self.mgr.default_discovery = [d.name
                                          for d in self.mgr.discovery_manager]
        self.Discovery.resources = discovered_resources
        self.DiscoveryAnother.resources = [d[::-1]
                                           for d in discovered_resources]
        self.pipeline_cfg[0]['resources'] = static_resources
        self.setup_pipeline()
        polling_tasks = self.mgr.setup_polling_tasks()
        self.mgr.interval_task(polling_tasks.get(60))
        self._verify_discovery_params([None] if discovered_resources else [])
        discovery = self.Discovery.resources + self.DiscoveryAnother.resources
        # compare resource lists modulo ordering
        self.assertEqual(set(static_resources or discovery),
                         set(self.Pollster.resources))

    def test_per_agent_discovery_discovered_only(self):
        self._do_test_per_agent_discovery(['discovered_1', 'discovered_2'],
                                          [])

    def test_per_agent_discovery_static_only(self):
        self._do_test_per_agent_discovery([],
                                          ['static_1', 'static_2'])

    def test_per_agent_discovery_discovered_overridden_by_static(self):
        self._do_test_per_agent_discovery(['discovered_1', 'discovered_2'],
                                          ['static_1', 'static_2'])

    def test_per_agent_discovery_overridden_by_per_pipeline_discovery(self):
        discovered_resources = ['discovered_1', 'discovered_2']
        self.mgr.discovery_manager = self.create_discovery_manager()
        self.Discovery.resources = discovered_resources
        self.DiscoveryAnother.resources = [d[::-1]
                                           for d in discovered_resources]
        self.pipeline_cfg[0]['discovery'] = ['testdiscoveryanother',
                                             'testdiscoverynonexistent',
                                             'testdiscoveryexception']
        self.pipeline_cfg[0]['resources'] = []
        self.setup_pipeline()
        polling_tasks = self.mgr.setup_polling_tasks()
        self.mgr.interval_task(polling_tasks.get(60))
        self.assertEqual(set(self.DiscoveryAnother.resources),
                         set(self.Pollster.resources))

    def _do_test_per_pipeline_discovery(self,
                                        discovered_resources,
                                        static_resources):
        self.mgr.discovery_manager = self.create_discovery_manager()
        self.Discovery.resources = discovered_resources
        self.DiscoveryAnother.resources = [d[::-1]
                                           for d in discovered_resources]
        self.pipeline_cfg[0]['discovery'] = ['testdiscovery',
                                             'testdiscoveryanother',
                                             'testdiscoverynonexistent',
                                             'testdiscoveryexception']
        self.pipeline_cfg[0]['resources'] = static_resources
        self.setup_pipeline()
        polling_tasks = self.mgr.setup_polling_tasks()
        self.mgr.interval_task(polling_tasks.get(60))
        discovery = self.Discovery.resources + self.DiscoveryAnother.resources
        # compare resource lists modulo ordering
        self.assertEqual(set(static_resources + discovery),
                         set(self.Pollster.resources))

    def test_per_pipeline_discovery_discovered_only(self):
        self._do_test_per_pipeline_discovery(['discovered_1', 'discovered_2'],
                                             [])

    def test_per_pipeline_discovery_static_only(self):
        self._do_test_per_pipeline_discovery([],
                                             ['static_1', 'static_2'])

    def test_per_pipeline_discovery_discovered_augmented_by_static(self):
        self._do_test_per_pipeline_discovery(['discovered_1', 'discovered_2'],
                                             ['static_1', 'static_2'])

    def test_multiple_pipelines_different_static_resources(self):
        # assert that the amalgation of all static resources for a set
        # of pipelines with a common interval is passed to individual
        # pollsters matching those pipelines
        self.pipeline_cfg[0]['resources'] = ['test://']
        self.pipeline_cfg.append({
            'name': "another_pipeline",
            'interval': 60,
            'counters': ['test'],
            'resources': ['another://'],
            'transformers': [],
            'publishers': ["new"],
        })
        self.mgr.discovery_manager = self.create_discovery_manager()
        self.Discovery.resources = []
        self.setup_pipeline()
        polling_tasks = self.mgr.setup_polling_tasks()
        self.assertEqual(1, len(polling_tasks))
        self.assertTrue(60 in polling_tasks.keys())
        self.mgr.interval_task(polling_tasks.get(60))
        self._verify_discovery_params([])
        self.assertEqual(1, len(self.Pollster.samples))
        amalgamated_resources = set(['test://', 'another://'])
        self.assertEqual(amalgamated_resources,
                         set(self.Pollster.samples[0][1]))
        for pipeline in self.mgr.pipeline_manager.pipelines:
            self.assertEqual(1, len(pipeline.publishers[0].samples))
            published = pipeline.publishers[0].samples[0]
            self.assertEqual(amalgamated_resources,
                             set(published.resource_metadata['resources']))

########NEW FILE########
__FILENAME__ = base
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 eNovance <licensing@enovance.com>
#
# Author: Mehdi Abaakouk <mehdi.abaakouk@enovance.com>
#         Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Base class for tests in ceilometer/alarm/evaluator/
"""
import mock

from ceilometer.openstack.common import test


class TestEvaluatorBase(test.BaseTestCase):
    def setUp(self):
        super(TestEvaluatorBase, self).setUp()
        self.api_client = mock.Mock()
        self.notifier = mock.MagicMock()
        self.evaluator = self.EVALUATOR(self.notifier)
        self.prepare_alarms()

    @staticmethod
    def prepare_alarms(self):
        self.alarms = []

    def _evaluate_all_alarms(self):
        for alarm in self.alarms:
            self.evaluator.evaluate(alarm)

    def _set_all_alarms(self, state):
        for alarm in self.alarms:
            alarm.state = state

    def _assert_all_alarms(self, state):
        for alarm in self.alarms:
            self.assertEqual(state, alarm.state)

########NEW FILE########
__FILENAME__ = test_base
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 IBM Corp
#
# Author: Tong Li <litong01@us.ibm.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""class for tests in ceilometer/alarm/evaluator/__init__.py
"""
import datetime
import mock
import pytz

from ceilometer.alarm import evaluator
from ceilometer.openstack.common import test
from ceilometer.openstack.common import timeutils


class TestEvaluatorBaseClass(test.BaseTestCase):
    def setUp(self):
        super(TestEvaluatorBaseClass, self).setUp()
        self.called = False

    def _notify(self, alarm, previous, reason, details):
        self.called = True
        raise Exception('Boom!')

    def test_base_refresh(self):
        notifier = mock.MagicMock()
        notifier.notify = self._notify

        class EvaluatorSub(evaluator.Evaluator):
            def evaluate(self, alarm):
                pass

        ev = EvaluatorSub(notifier)
        ev.api_client = mock.MagicMock()
        ev._refresh(mock.MagicMock(), mock.MagicMock(),
                    mock.MagicMock(), mock.MagicMock())
        self.assertTrue(self.called)

    @mock.patch.object(timeutils, 'utcnow')
    def test_base_time_constraints(self, mock_utcnow):
        alarm = mock.MagicMock()
        alarm.time_constraints = [
            {'name': 'test',
             'description': 'test',
             'start': '0 11 * * *',  # daily at 11:00
             'duration': 10800,  # 3 hours
             'timezone': ''},
            {'name': 'test2',
             'description': 'test',
             'start': '0 23 * * *',  # daily at 23:00
             'duration': 10800,  # 3 hours
             'timezone': ''},
        ]
        cls = evaluator.Evaluator
        mock_utcnow.return_value = datetime.datetime(2014, 1, 1, 12, 0, 0)
        self.assertTrue(cls.within_time_constraint(alarm))

        mock_utcnow.return_value = datetime.datetime(2014, 1, 2, 1, 0, 0)
        self.assertTrue(cls.within_time_constraint(alarm))

        mock_utcnow.return_value = datetime.datetime(2014, 1, 2, 5, 0, 0)
        self.assertFalse(cls.within_time_constraint(alarm))

    @mock.patch.object(timeutils, 'utcnow')
    def test_base_time_constraints_complex(self, mock_utcnow):
        alarm = mock.MagicMock()
        alarm.time_constraints = [
            {'name': 'test',
             'description': 'test',
             # Every consecutive 2 minutes (from the 3rd to the 57th) past
             # every consecutive 2 hours (between 3:00 and 12:59) on every day.
             'start': '3-57/2 3-12/2 * * *',
             'duration': 30,
             'timezone': ''}
        ]
        cls = evaluator.Evaluator

        # test minutes inside
        mock_utcnow.return_value = datetime.datetime(2014, 1, 5, 3, 3, 0)
        self.assertTrue(cls.within_time_constraint(alarm))
        mock_utcnow.return_value = datetime.datetime(2014, 1, 5, 3, 31, 0)
        self.assertTrue(cls.within_time_constraint(alarm))
        mock_utcnow.return_value = datetime.datetime(2014, 1, 5, 3, 57, 0)
        self.assertTrue(cls.within_time_constraint(alarm))

        # test minutes outside
        mock_utcnow.return_value = datetime.datetime(2014, 1, 5, 3, 2, 0)
        self.assertFalse(cls.within_time_constraint(alarm))
        mock_utcnow.return_value = datetime.datetime(2014, 1, 5, 3, 4, 0)
        self.assertFalse(cls.within_time_constraint(alarm))
        mock_utcnow.return_value = datetime.datetime(2014, 1, 5, 3, 58, 0)
        self.assertFalse(cls.within_time_constraint(alarm))

        # test hours inside
        mock_utcnow.return_value = datetime.datetime(2014, 1, 5, 3, 31, 0)
        self.assertTrue(cls.within_time_constraint(alarm))
        mock_utcnow.return_value = datetime.datetime(2014, 1, 5, 5, 31, 0)
        self.assertTrue(cls.within_time_constraint(alarm))
        mock_utcnow.return_value = datetime.datetime(2014, 1, 5, 11, 31, 0)
        self.assertTrue(cls.within_time_constraint(alarm))

        # test hours outside
        mock_utcnow.return_value = datetime.datetime(2014, 1, 5, 1, 31, 0)
        self.assertFalse(cls.within_time_constraint(alarm))
        mock_utcnow.return_value = datetime.datetime(2014, 1, 5, 4, 31, 0)
        self.assertFalse(cls.within_time_constraint(alarm))
        mock_utcnow.return_value = datetime.datetime(2014, 1, 5, 12, 31, 0)
        self.assertFalse(cls.within_time_constraint(alarm))

    @mock.patch.object(timeutils, 'utcnow')
    def test_base_time_constraints_timezone(self, mock_utcnow):
        alarm = mock.MagicMock()
        alarm.time_constraints = [
            {'name': 'test',
             'description': 'test',
             'start': '0 11 * * *',  # daily at 11:00
             'duration': 10800,  # 3 hours
             'timezone': 'Europe/Ljubljana'}
        ]
        cls = evaluator.Evaluator
        dt_eu = datetime.datetime(2014, 1, 1, 12, 0, 0,
                                  tzinfo=pytz.timezone('Europe/Ljubljana'))
        dt_us = datetime.datetime(2014, 1, 1, 12, 0, 0,
                                  tzinfo=pytz.timezone('US/Eastern'))
        mock_utcnow.return_value = dt_eu.astimezone(pytz.UTC)
        self.assertTrue(cls.within_time_constraint(alarm))

        mock_utcnow.return_value = dt_us.astimezone(pytz.UTC)
        self.assertFalse(cls.within_time_constraint(alarm))

########NEW FILE########
__FILENAME__ = test_combination
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 eNovance <licensing@enovance.com>
#
# Authors: Mehdi Abaakouk <mehdi.abaakouk@enovance.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests for ceilometer/alarm/threshold_evaluation.py
"""
import datetime
import mock
import pytz
import uuid

from ceilometer.alarm.evaluator import combination
from ceilometer.openstack.common import timeutils
from ceilometer.storage import models
from ceilometer.tests.alarm.evaluator import base
from ceilometerclient import exc
from ceilometerclient.v2 import alarms


class TestEvaluate(base.TestEvaluatorBase):
    EVALUATOR = combination.CombinationEvaluator

    def prepare_alarms(self):
        self.alarms = [
            models.Alarm(name='or-alarm',
                         description='the or alarm',
                         type='combination',
                         enabled=True,
                         user_id='foobar',
                         project_id='snafu',
                         alarm_id=str(uuid.uuid4()),
                         state='insufficient data',
                         state_timestamp=None,
                         timestamp=None,
                         insufficient_data_actions=[],
                         ok_actions=[],
                         alarm_actions=[],
                         repeat_actions=False,
                         time_constraints=[],
                         rule=dict(
                             alarm_ids=[
                                 '9cfc3e51-2ff1-4b1d-ac01-c1bd4c6d0d1e',
                                 '1d441595-d069-4e05-95ab-8693ba6a8302'],
                             operator='or',
                         )),
            models.Alarm(name='and-alarm',
                         description='the and alarm',
                         type='combination',
                         enabled=True,
                         user_id='foobar',
                         project_id='snafu',
                         alarm_id=str(uuid.uuid4()),
                         state='insufficient data',
                         state_timestamp=None,
                         timestamp=None,
                         insufficient_data_actions=[],
                         ok_actions=[],
                         alarm_actions=[],
                         repeat_actions=False,
                         time_constraints=[],
                         rule=dict(
                             alarm_ids=[
                                 'b82734f4-9d06-48f3-8a86-fa59a0c99dc8',
                                 '15a700e5-2fe8-4b3d-8c55-9e92831f6a2b'],
                             operator='and',
                         ))
        ]

    @staticmethod
    def _get_alarm(state):
        return alarms.Alarm(None, {'state': state})

    @staticmethod
    def _reason_data(alarm_ids):
        return {'type': 'combination', 'alarm_ids': alarm_ids}

    def _combination_transition_reason(self, state, alarm_ids1, alarm_ids2):
        return ([('Transition to %(state)s due to alarms %(alarm_ids)s'
                ' in state %(state)s')
                % {'state': state, 'alarm_ids': ",".join(alarm_ids1)},
                ('Transition to %(state)s due to alarms %(alarm_ids)s'
                ' in state %(state)s')
                % {'state': state, 'alarm_ids': ",".join(alarm_ids2)}],
                [self._reason_data(alarm_ids1), self._reason_data(alarm_ids2)])

    def _combination_remaining_reason(self, state, alarm_ids1, alarm_ids2):
        return ([('Remaining as %(state)s due to alarms %(alarm_ids)s'
                 ' in state %(state)s')
                % {'state': state, 'alarm_ids': ",".join(alarm_ids1)},
                ('Remaining as %(state)s due to alarms %(alarm_ids)s'
                 ' in state %(state)s')
                % {'state': state, 'alarm_ids': ",".join(alarm_ids2)}],
                [self._reason_data(alarm_ids1), self._reason_data(alarm_ids2)])

    def test_retry_transient_api_failure(self):
        with mock.patch('ceilometerclient.client.get_client',
                        return_value=self.api_client):
            broken = exc.CommunicationError(message='broken')
            self.api_client.alarms.get.side_effect = [
                broken,
                broken,
                broken,
                broken,
                self._get_alarm('ok'),
                self._get_alarm('ok'),
                self._get_alarm('ok'),
                self._get_alarm('ok'),
            ]
            self._evaluate_all_alarms()
            self._assert_all_alarms('insufficient data')
            self._evaluate_all_alarms()
            self._assert_all_alarms('ok')

    def test_simple_insufficient(self):
        self._set_all_alarms('ok')
        with mock.patch('ceilometerclient.client.get_client',
                        return_value=self.api_client):
            broken = exc.CommunicationError(message='broken')
            self.api_client.alarms.get.side_effect = broken
            self._evaluate_all_alarms()
            self._assert_all_alarms('insufficient data')
            expected = [mock.call(alarm.alarm_id, state='insufficient data')
                        for alarm in self.alarms]
            update_calls = self.api_client.alarms.set_state.call_args_list
            self.assertEqual(expected, update_calls)
            expected = [mock.call(
                alarm,
                'ok',
                ('Alarms %s are in unknown state' %
                 (",".join(alarm.rule['alarm_ids']))),
                self._reason_data(alarm.rule['alarm_ids']))
                for alarm in self.alarms]
            self.assertEqual(expected, self.notifier.notify.call_args_list)

    def test_to_ok_with_all_ok(self):
        self._set_all_alarms('insufficient data')
        with mock.patch('ceilometerclient.client.get_client',
                        return_value=self.api_client):
            self.api_client.alarms.get.side_effect = [
                self._get_alarm('ok'),
                self._get_alarm('ok'),
                self._get_alarm('ok'),
                self._get_alarm('ok'),
            ]
            self._evaluate_all_alarms()
            expected = [mock.call(alarm.alarm_id, state='ok')
                        for alarm in self.alarms]
            update_calls = self.api_client.alarms.set_state.call_args_list
            self.assertEqual(expected, update_calls)
            reasons, reason_datas = self._combination_transition_reason(
                'ok',
                self.alarms[0].rule['alarm_ids'],
                self.alarms[1].rule['alarm_ids'])
            expected = [mock.call(alarm, 'insufficient data',
                                  reason, reason_data)
                        for alarm, reason, reason_data
                        in zip(self.alarms, reasons, reason_datas)]
            self.assertEqual(expected, self.notifier.notify.call_args_list)

    def test_to_ok_with_one_alarm(self):
        self._set_all_alarms('alarm')
        with mock.patch('ceilometerclient.client.get_client',
                        return_value=self.api_client):
            self.api_client.alarms.get.side_effect = [
                self._get_alarm('ok'),
                self._get_alarm('ok'),
                self._get_alarm('alarm'),
                self._get_alarm('ok'),
            ]
            self._evaluate_all_alarms()
            expected = [mock.call(alarm.alarm_id, state='ok')
                        for alarm in self.alarms]
            update_calls = self.api_client.alarms.set_state.call_args_list
            self.assertEqual(expected, update_calls)
            reasons, reason_datas = self._combination_transition_reason(
                'ok',
                self.alarms[0].rule['alarm_ids'],
                [self.alarms[1].rule['alarm_ids'][1]])
            expected = [mock.call(alarm, 'alarm', reason, reason_data)
                        for alarm, reason, reason_data
                        in zip(self.alarms, reasons, reason_datas)]
            self.assertEqual(expected, self.notifier.notify.call_args_list)

    def test_to_alarm_with_all_alarm(self):
        self._set_all_alarms('ok')
        with mock.patch('ceilometerclient.client.get_client',
                        return_value=self.api_client):
            self.api_client.alarms.get.side_effect = [
                self._get_alarm('alarm'),
                self._get_alarm('alarm'),
                self._get_alarm('alarm'),
                self._get_alarm('alarm'),
            ]
            self._evaluate_all_alarms()
            expected = [mock.call(alarm.alarm_id, state='alarm')
                        for alarm in self.alarms]
            update_calls = self.api_client.alarms.set_state.call_args_list
            self.assertEqual(expected, update_calls)
            reasons, reason_datas = self._combination_transition_reason(
                'alarm',
                self.alarms[0].rule['alarm_ids'],
                self.alarms[1].rule['alarm_ids'])
            expected = [mock.call(alarm, 'ok', reason, reason_data)
                        for alarm, reason, reason_data
                        in zip(self.alarms, reasons, reason_datas)]
            self.assertEqual(expected, self.notifier.notify.call_args_list)

    def test_to_alarm_with_one_ok(self):
        self._set_all_alarms('ok')
        with mock.patch('ceilometerclient.client.get_client',
                        return_value=self.api_client):
            self.api_client.alarms.get.side_effect = [
                self._get_alarm('ok'),
                self._get_alarm('alarm'),
                self._get_alarm('alarm'),
                self._get_alarm('alarm'),
            ]
            self._evaluate_all_alarms()
            expected = [mock.call(alarm.alarm_id, state='alarm')
                        for alarm in self.alarms]
            update_calls = self.api_client.alarms.set_state.call_args_list
            self.assertEqual(expected, update_calls)
            reasons, reason_datas = self._combination_transition_reason(
                'alarm',
                [self.alarms[0].rule['alarm_ids'][1]],
                self.alarms[1].rule['alarm_ids'])
            expected = [mock.call(alarm, 'ok', reason, reason_data)
                        for alarm, reason, reason_data
                        in zip(self.alarms, reasons, reason_datas)]
            self.assertEqual(expected, self.notifier.notify.call_args_list)

    def test_to_unknown(self):
        self._set_all_alarms('ok')
        with mock.patch('ceilometerclient.client.get_client',
                        return_value=self.api_client):
            broken = exc.CommunicationError(message='broken')
            self.api_client.alarms.get.side_effect = [
                broken,
                self._get_alarm('ok'),
                self._get_alarm('insufficient data'),
                self._get_alarm('ok'),
            ]
            self._evaluate_all_alarms()
            expected = [mock.call(alarm.alarm_id, state='insufficient data')
                        for alarm in self.alarms]
            update_calls = self.api_client.alarms.set_state.call_args_list
            self.assertEqual(expected, update_calls)
            reasons = ['Alarms %s are in unknown state'
                       % self.alarms[0].rule['alarm_ids'][0],
                       'Alarms %s are in unknown state'
                       % self.alarms[1].rule['alarm_ids'][0]]
            reason_datas = [
                self._reason_data([self.alarms[0].rule['alarm_ids'][0]]),
                self._reason_data([self.alarms[1].rule['alarm_ids'][0]])]
            expected = [mock.call(alarm, 'ok', reason, reason_data)
                        for alarm, reason, reason_data
                        in zip(self.alarms, reasons, reason_datas)]
            self.assertEqual(expected, self.notifier.notify.call_args_list)

    def test_no_state_change(self):
        self._set_all_alarms('ok')
        with mock.patch('ceilometerclient.client.get_client',
                        return_value=self.api_client):
            self.api_client.alarms.get.side_effect = [
                self._get_alarm('ok'),
                self._get_alarm('ok'),
                self._get_alarm('ok'),
                self._get_alarm('ok'),
            ]
            self._evaluate_all_alarms()
            update_calls = self.api_client.alarms.set_state.call_args_list
            self.assertEqual([], update_calls)
            self.assertEqual([], self.notifier.notify.call_args_list)

    def test_no_state_change_and_repeat_actions(self):
        self.alarms[0].repeat_actions = True
        self.alarms[1].repeat_actions = True
        self._set_all_alarms('ok')
        with mock.patch('ceilometerclient.client.get_client',
                        return_value=self.api_client):
            self.api_client.alarms.get.side_effect = [
                self._get_alarm('ok'),
                self._get_alarm('ok'),
                self._get_alarm('ok'),
                self._get_alarm('ok'),
            ]
            self._evaluate_all_alarms()
            update_calls = self.api_client.alarms.set_state.call_args_list
            self.assertEqual([], update_calls)
            reasons, reason_datas = self._combination_remaining_reason(
                'ok',
                self.alarms[0].rule['alarm_ids'],
                self.alarms[1].rule['alarm_ids'])
            expected = [mock.call(alarm, 'ok', reason, reason_data)
                        for alarm, reason, reason_data
                        in zip(self.alarms, reasons, reason_datas)]

            self.assertEqual(expected, self.notifier.notify.call_args_list)

    @mock.patch.object(timeutils, 'utcnow')
    def test_state_change_inside_time_constraint(self, mock_utcnow):
        self._set_all_alarms('insufficient data')
        self.alarms[0].time_constraints = [
            {'name': 'test',
             'description': 'test',
             'start': '0 11 * * *',  # daily at 11:00
             'duration': 10800,  # 3 hours
             'timezone': 'Europe/Ljubljana'}
        ]
        self.alarms[1].time_constraints = self.alarms[0].time_constraints
        dt = datetime.datetime(2014, 1, 1, 12, 0, 0,
                               tzinfo=pytz.timezone('Europe/Ljubljana'))
        mock_utcnow.return_value = dt.astimezone(pytz.UTC)
        with mock.patch('ceilometerclient.client.get_client',
                        return_value=self.api_client):
            self.api_client.alarms.get.side_effect = [
                self._get_alarm('ok'),
                self._get_alarm('ok'),
                self._get_alarm('ok'),
                self._get_alarm('ok'),
            ]
            self._evaluate_all_alarms()
            expected = [mock.call(alarm.alarm_id, state='ok')
                        for alarm in self.alarms]
            update_calls = self.api_client.alarms.set_state.call_args_list
            self.assertEqual(expected, update_calls,
                             "Alarm should change state if the current "
                             "time is inside its time constraint.")
            reasons, reason_datas = self._combination_transition_reason(
                'ok',
                self.alarms[0].rule['alarm_ids'],
                self.alarms[1].rule['alarm_ids'])
            expected = [mock.call(alarm, 'insufficient data',
                                  reason, reason_data)
                        for alarm, reason, reason_data
                        in zip(self.alarms, reasons, reason_datas)]
            self.assertEqual(expected, self.notifier.notify.call_args_list)

    @mock.patch.object(timeutils, 'utcnow')
    def test_no_state_change_outside_time_constraint(self, mock_utcnow):
        self._set_all_alarms('insufficient data')
        self.alarms[0].time_constraints = [
            {'name': 'test',
             'description': 'test',
             'start': '0 11 * * *',  # daily at 11:00
             'duration': 10800,  # 3 hours
             'timezone': 'Europe/Ljubljana'}
        ]
        self.alarms[1].time_constraints = self.alarms[0].time_constraints
        dt = datetime.datetime(2014, 1, 1, 15, 0, 0,
                               tzinfo=pytz.timezone('Europe/Ljubljana'))
        mock_utcnow.return_value = dt.astimezone(pytz.UTC)
        with mock.patch('ceilometerclient.client.get_client',
                        return_value=self.api_client):
            self.api_client.alarms.get.side_effect = [
                self._get_alarm('ok'),
                self._get_alarm('ok'),
                self._get_alarm('ok'),
                self._get_alarm('ok'),
            ]
            self._evaluate_all_alarms()
            update_calls = self.api_client.alarms.set_state.call_args_list
            self.assertEqual([], update_calls,
                             "Alarm should not change state if the current "
                             " time is outside its time constraint.")
            self.assertEqual([], self.notifier.notify.call_args_list)

########NEW FILE########
__FILENAME__ = test_threshold
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Red Hat, Inc
#
# Author: Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests for ceilometer/alarm/evaluator/threshold.py
"""
import datetime
import mock
import pytz
import uuid

from six import moves

from ceilometer.alarm.evaluator import threshold
from ceilometer.openstack.common import timeutils
from ceilometer.storage import models
from ceilometer.tests.alarm.evaluator import base
from ceilometerclient import exc
from ceilometerclient.v2 import statistics
from oslo.config import cfg


class TestEvaluate(base.TestEvaluatorBase):
    EVALUATOR = threshold.ThresholdEvaluator

    def prepare_alarms(self):
        self.alarms = [
            models.Alarm(name='instance_running_hot',
                         description='instance_running_hot',
                         type='threshold',
                         enabled=True,
                         user_id='foobar',
                         project_id='snafu',
                         alarm_id=str(uuid.uuid4()),
                         state='insufficient data',
                         state_timestamp=None,
                         timestamp=None,
                         insufficient_data_actions=[],
                         ok_actions=[],
                         alarm_actions=[],
                         repeat_actions=False,
                         time_constraints=[],
                         rule=dict(
                             comparison_operator='gt',
                             threshold=80.0,
                             evaluation_periods=5,
                             statistic='avg',
                             period=60,
                             meter_name='cpu_util',
                             query=[{'field': 'meter',
                                     'op': 'eq',
                                     'value': 'cpu_util'},
                                    {'field': 'resource_id',
                                     'op': 'eq',
                                     'value': 'my_instance'}])
                         ),
            models.Alarm(name='group_running_idle',
                         description='group_running_idle',
                         type='threshold',
                         enabled=True,
                         user_id='foobar',
                         project_id='snafu',
                         state='insufficient data',
                         state_timestamp=None,
                         timestamp=None,
                         insufficient_data_actions=[],
                         ok_actions=[],
                         alarm_actions=[],
                         repeat_actions=False,
                         alarm_id=str(uuid.uuid4()),
                         time_constraints=[],
                         rule=dict(
                             comparison_operator='le',
                             threshold=10.0,
                             evaluation_periods=4,
                             statistic='max',
                             period=300,
                             meter_name='cpu_util',
                             query=[{'field': 'meter',
                                     'op': 'eq',
                                     'value': 'cpu_util'},
                                    {'field': 'metadata.user_metadata.AS',
                                     'op': 'eq',
                                     'value': 'my_group'}])
                         ),
        ]

    @staticmethod
    def _get_stat(attr, value, count=1):
        return statistics.Statistics(None, {attr: value, 'count': count})

    @staticmethod
    def _reason_data(disposition, count, most_recent):
        return {'type': 'threshold', 'disposition': disposition,
                'count': count, 'most_recent': most_recent}

    def _set_all_rules(self, field, value):
        for alarm in self.alarms:
            alarm.rule[field] = value

    def test_retry_transient_api_failure(self):
        with mock.patch('ceilometerclient.client.get_client',
                        return_value=self.api_client):
            broken = exc.CommunicationError(message='broken')
            avgs = [self._get_stat('avg', self.alarms[0].rule['threshold'] - v)
                    for v in moves.xrange(5)]
            maxs = [self._get_stat('max', self.alarms[1].rule['threshold'] + v)
                    for v in moves.xrange(1, 4)]
            self.api_client.statistics.list.side_effect = [broken,
                                                           broken,
                                                           avgs,
                                                           maxs]
            self._evaluate_all_alarms()
            self._assert_all_alarms('insufficient data')
            self._evaluate_all_alarms()
            self._assert_all_alarms('ok')

    def test_simple_insufficient(self):
        self._set_all_alarms('ok')
        with mock.patch('ceilometerclient.client.get_client',
                        return_value=self.api_client):
            self.api_client.statistics.list.return_value = []
            self._evaluate_all_alarms()
            self._assert_all_alarms('insufficient data')
            expected = [mock.call(alarm.alarm_id, state='insufficient data')
                        for alarm in self.alarms]
            update_calls = self.api_client.alarms.set_state.call_args_list
            self.assertEqual(update_calls, expected)
            expected = [mock.call(
                alarm,
                'ok',
                ('%d datapoints are unknown'
                 % alarm.rule['evaluation_periods']),
                self._reason_data('unknown',
                                  alarm.rule['evaluation_periods'],
                                  None))
                for alarm in self.alarms]
            self.assertEqual(expected, self.notifier.notify.call_args_list)

    def test_simple_alarm_trip(self):
        self._set_all_alarms('ok')
        with mock.patch('ceilometerclient.client.get_client',
                        return_value=self.api_client):
            avgs = [self._get_stat('avg', self.alarms[0].rule['threshold'] + v)
                    for v in moves.xrange(1, 6)]
            maxs = [self._get_stat('max', self.alarms[1].rule['threshold'] - v)
                    for v in moves.xrange(4)]
            self.api_client.statistics.list.side_effect = [avgs, maxs]
            self._evaluate_all_alarms()
            self._assert_all_alarms('alarm')
            expected = [mock.call(alarm.alarm_id, state='alarm')
                        for alarm in self.alarms]
            update_calls = self.api_client.alarms.set_state.call_args_list
            self.assertEqual(expected, update_calls)
            reasons = ['Transition to alarm due to 5 samples outside'
                       ' threshold, most recent: %s' % avgs[-1].avg,
                       'Transition to alarm due to 4 samples outside'
                       ' threshold, most recent: %s' % maxs[-1].max]
            reason_datas = [self._reason_data('outside', 5, avgs[-1].avg),
                            self._reason_data('outside', 4, maxs[-1].max)]
            expected = [mock.call(alarm, 'ok', reason, reason_data)
                        for alarm, reason, reason_data
                        in zip(self.alarms, reasons, reason_datas)]
            self.assertEqual(expected, self.notifier.notify.call_args_list)

    def test_simple_alarm_clear(self):
        self._set_all_alarms('alarm')
        with mock.patch('ceilometerclient.client.get_client',
                        return_value=self.api_client):
            avgs = [self._get_stat('avg', self.alarms[0].rule['threshold'] - v)
                    for v in moves.xrange(5)]
            maxs = [self._get_stat('max', self.alarms[1].rule['threshold'] + v)
                    for v in moves.xrange(1, 5)]
            self.api_client.statistics.list.side_effect = [avgs, maxs]
            self._evaluate_all_alarms()
            self._assert_all_alarms('ok')
            expected = [mock.call(alarm.alarm_id, state='ok')
                        for alarm in self.alarms]
            update_calls = self.api_client.alarms.set_state.call_args_list
            self.assertEqual(expected, update_calls)
            reasons = ['Transition to ok due to 5 samples inside'
                       ' threshold, most recent: %s' % avgs[-1].avg,
                       'Transition to ok due to 4 samples inside'
                       ' threshold, most recent: %s' % maxs[-1].max]
            reason_datas = [self._reason_data('inside', 5, avgs[-1].avg),
                            self._reason_data('inside', 4, maxs[-1].max)]
            expected = [mock.call(alarm, 'alarm', reason, reason_data)
                        for alarm, reason, reason_data
                        in zip(self.alarms, reasons, reason_datas)]
            self.assertEqual(expected, self.notifier.notify.call_args_list)

    def test_equivocal_from_known_state(self):
        self._set_all_alarms('ok')
        with mock.patch('ceilometerclient.client.get_client',
                        return_value=self.api_client):
            avgs = [self._get_stat('avg', self.alarms[0].rule['threshold'] + v)
                    for v in moves.xrange(5)]
            maxs = [self._get_stat('max', self.alarms[1].rule['threshold'] - v)
                    for v in moves.xrange(-1, 3)]
            self.api_client.statistics.list.side_effect = [avgs, maxs]
            self._evaluate_all_alarms()
            self._assert_all_alarms('ok')
            self.assertEqual(
                [],
                self.api_client.alarms.set_state.call_args_list)
            self.assertEqual([], self.notifier.notify.call_args_list)

    def test_equivocal_from_known_state_and_repeat_actions(self):
        self._set_all_alarms('ok')
        self.alarms[1].repeat_actions = True
        with mock.patch('ceilometerclient.client.get_client',
                        return_value=self.api_client):
            avgs = [self._get_stat('avg',
                                   self.alarms[0].rule['threshold'] + v)
                    for v in moves.xrange(5)]
            maxs = [self._get_stat('max',
                                   self.alarms[1].rule['threshold'] - v)
                    for v in moves.xrange(-1, 3)]
            self.api_client.statistics.list.side_effect = [avgs, maxs]
            self._evaluate_all_alarms()
            self._assert_all_alarms('ok')
            self.assertEqual([],
                             self.api_client.alarms.set_state.call_args_list)
            reason = 'Remaining as ok due to 4 samples inside' \
                     ' threshold, most recent: 8.0'
            reason_datas = self._reason_data('inside', 4, 8.0)
            expected = [mock.call(self.alarms[1], 'ok', reason, reason_datas)]
            self.assertEqual(expected, self.notifier.notify.call_args_list)

    def test_unequivocal_from_known_state_and_repeat_actions(self):
        self._set_all_alarms('alarm')
        self.alarms[1].repeat_actions = True
        with mock.patch('ceilometerclient.client.get_client',
                        return_value=self.api_client):
            avgs = [self._get_stat('avg',
                                   self.alarms[0].rule['threshold'] + v)
                    for v in moves.xrange(1, 6)]
            maxs = [self._get_stat('max',
                                   self.alarms[1].rule['threshold'] - v)
                    for v in moves.xrange(4)]
            self.api_client.statistics.list.side_effect = [avgs, maxs]
            self._evaluate_all_alarms()
            self._assert_all_alarms('alarm')
            self.assertEqual([],
                             self.api_client.alarms.set_state.call_args_list)
            reason = 'Remaining as alarm due to 4 samples outside' \
                     ' threshold, most recent: 7.0'
            reason_datas = self._reason_data('outside', 4, 7.0)
            expected = [mock.call(self.alarms[1], 'alarm',
                                  reason, reason_datas)]
            self.assertEqual(expected, self.notifier.notify.call_args_list)

    def test_state_change_and_repeat_actions(self):
        self._set_all_alarms('ok')
        self.alarms[0].repeat_actions = True
        self.alarms[1].repeat_actions = True
        with mock.patch('ceilometerclient.client.get_client',
                        return_value=self.api_client):
            avgs = [self._get_stat('avg', self.alarms[0].rule['threshold'] + v)
                    for v in moves.xrange(1, 6)]
            maxs = [self._get_stat('max', self.alarms[1].rule['threshold'] - v)
                    for v in moves.xrange(4)]
            self.api_client.statistics.list.side_effect = [avgs, maxs]
            self._evaluate_all_alarms()
            self._assert_all_alarms('alarm')
            expected = [mock.call(alarm.alarm_id, state='alarm')
                        for alarm in self.alarms]
            update_calls = self.api_client.alarms.set_state.call_args_list
            self.assertEqual(expected, update_calls)
            reasons = ['Transition to alarm due to 5 samples outside'
                       ' threshold, most recent: %s' % avgs[-1].avg,
                       'Transition to alarm due to 4 samples outside'
                       ' threshold, most recent: %s' % maxs[-1].max]
            reason_datas = [self._reason_data('outside', 5, avgs[-1].avg),
                            self._reason_data('outside', 4, maxs[-1].max)]
            expected = [mock.call(alarm, 'ok', reason, reason_data)
                        for alarm, reason, reason_data
                        in zip(self.alarms, reasons, reason_datas)]
            self.assertEqual(expected, self.notifier.notify.call_args_list)

    def test_equivocal_from_unknown(self):
        self._set_all_alarms('insufficient data')
        with mock.patch('ceilometerclient.client.get_client',
                        return_value=self.api_client):
            avgs = [self._get_stat('avg', self.alarms[0].rule['threshold'] + v)
                    for v in moves.xrange(1, 6)]
            maxs = [self._get_stat('max', self.alarms[1].rule['threshold'] - v)
                    for v in moves.xrange(4)]
            self.api_client.statistics.list.side_effect = [avgs, maxs]
            self._evaluate_all_alarms()
            self._assert_all_alarms('alarm')
            expected = [mock.call(alarm.alarm_id, state='alarm')
                        for alarm in self.alarms]
            update_calls = self.api_client.alarms.set_state.call_args_list
            self.assertEqual(expected, update_calls)
            reasons = ['Transition to alarm due to 5 samples outside'
                       ' threshold, most recent: %s' % avgs[-1].avg,
                       'Transition to alarm due to 4 samples outside'
                       ' threshold, most recent: %s' % maxs[-1].max]
            reason_datas = [self._reason_data('outside', 5, avgs[-1].avg),
                            self._reason_data('outside', 4, maxs[-1].max)]
            expected = [mock.call(alarm, 'insufficient data',
                                  reason, reason_data)
                        for alarm, reason, reason_data
                        in zip(self.alarms, reasons, reason_datas)]
            self.assertEqual(expected, self.notifier.notify.call_args_list)

    def _do_test_bound_duration(self, start, exclude_outliers=None):
        alarm = self.alarms[0]
        if exclude_outliers is not None:
            alarm.rule['exclude_outliers'] = exclude_outliers
        with mock.patch.object(timeutils, 'utcnow') as mock_utcnow:
            mock_utcnow.return_value = datetime.datetime(2012, 7, 2, 10, 45)
            constraint = self.evaluator._bound_duration(alarm, [])
            self.assertEqual([
                {'field': 'timestamp',
                 'op': 'le',
                 'value': timeutils.utcnow().isoformat()},
                {'field': 'timestamp',
                 'op': 'ge',
                 'value': start},
            ], constraint)

    def test_bound_duration_outlier_exclusion_defaulted(self):
        self._do_test_bound_duration('2012-07-02T10:39:00')

    def test_bound_duration_outlier_exclusion_clear(self):
        self._do_test_bound_duration('2012-07-02T10:39:00', False)

    def test_bound_duration_outlier_exclusion_set(self):
        self._do_test_bound_duration('2012-07-02T10:35:00', True)

    def test_threshold_endpoint_types(self):
        endpoint_types = ["internalURL", "publicURL"]
        for endpoint_type in endpoint_types:
            cfg.CONF.set_override('os_endpoint_type',
                                  endpoint_type,
                                  group='service_credentials')
            with mock.patch('ceilometerclient.client.get_client') as client:
                self.evaluator.api_client = None
                self._evaluate_all_alarms()
                conf = cfg.CONF.service_credentials
                expected = [mock.call(2,
                                      os_auth_url=conf.os_auth_url,
                                      os_region_name=conf.os_region_name,
                                      os_tenant_name=conf.os_tenant_name,
                                      os_password=conf.os_password,
                                      os_username=conf.os_username,
                                      os_cacert=conf.os_cacert,
                                      os_endpoint_type=conf.os_endpoint_type,
                                      insecure=conf.insecure)]
                actual = client.call_args_list
                self.assertEqual(expected, actual)

    def _do_test_simple_alarm_trip_outlier_exclusion(self, exclude_outliers):
        self._set_all_rules('exclude_outliers', exclude_outliers)
        self._set_all_alarms('ok')
        with mock.patch('ceilometerclient.client.get_client',
                        return_value=self.api_client):
            # most recent datapoints inside threshold but with
            # anomalously low sample count
            threshold = self.alarms[0].rule['threshold']
            avgs = [self._get_stat('avg',
                                   threshold + (v if v < 10 else -v),
                                   count=20 if v < 10 else 1)
                    for v in xrange(1, 11)]
            threshold = self.alarms[1].rule['threshold']
            maxs = [self._get_stat('max',
                                   threshold - (v if v < 7 else -v),
                                   count=20 if v < 7 else 1)
                    for v in xrange(8)]
            self.api_client.statistics.list.side_effect = [avgs, maxs]
            self._evaluate_all_alarms()
            self._assert_all_alarms('alarm' if exclude_outliers else 'ok')
            if exclude_outliers:
                expected = [mock.call(alarm.alarm_id, state='alarm')
                            for alarm in self.alarms]
                update_calls = self.api_client.alarms.set_state.call_args_list
                self.assertEqual(expected, update_calls)
                reasons = ['Transition to alarm due to 5 samples outside'
                           ' threshold, most recent: %s' % avgs[-2].avg,
                           'Transition to alarm due to 4 samples outside'
                           ' threshold, most recent: %s' % maxs[-2].max]
                reason_datas = [self._reason_data('outside', 5, avgs[-2].avg),
                                self._reason_data('outside', 4, maxs[-2].max)]
                expected = [mock.call(alarm, 'ok', reason, reason_data)
                            for alarm, reason, reason_data
                            in zip(self.alarms, reasons, reason_datas)]
                self.assertEqual(expected, self.notifier.notify.call_args_list)

    def test_simple_alarm_trip_with_outlier_exclusion(self):
        self. _do_test_simple_alarm_trip_outlier_exclusion(True)

    def test_simple_alarm_no_trip_without_outlier_exclusion(self):
        self. _do_test_simple_alarm_trip_outlier_exclusion(False)

    def _do_test_simple_alarm_clear_outlier_exclusion(self, exclude_outliers):
        self._set_all_rules('exclude_outliers', exclude_outliers)
        self._set_all_alarms('alarm')
        with mock.patch('ceilometerclient.client.get_client',
                        return_value=self.api_client):
            # most recent datapoints outside threshold but with
            # anomalously low sample count
            threshold = self.alarms[0].rule['threshold']
            avgs = [self._get_stat('avg',
                                   threshold - (v if v < 9 else -v),
                                   count=20 if v < 9 else 1)
                    for v in xrange(10)]
            threshold = self.alarms[1].rule['threshold']
            maxs = [self._get_stat('max',
                                   threshold + (v if v < 8 else -v),
                                   count=20 if v < 8 else 1)
                    for v in xrange(1, 9)]
            self.api_client.statistics.list.side_effect = [avgs, maxs]
            self._evaluate_all_alarms()
            self._assert_all_alarms('ok' if exclude_outliers else 'alarm')
            if exclude_outliers:
                expected = [mock.call(alarm.alarm_id, state='ok')
                            for alarm in self.alarms]
                update_calls = self.api_client.alarms.set_state.call_args_list
                self.assertEqual(expected, update_calls)
                reasons = ['Transition to ok due to 5 samples inside'
                           ' threshold, most recent: %s' % avgs[-2].avg,
                           'Transition to ok due to 4 samples inside'
                           ' threshold, most recent: %s' % maxs[-2].max]
                reason_datas = [self._reason_data('inside', 5, avgs[-2].avg),
                                self._reason_data('inside', 4, maxs[-2].max)]
                expected = [mock.call(alarm, 'alarm', reason, reason_data)
                            for alarm, reason, reason_data
                            in zip(self.alarms, reasons, reason_datas)]
                self.assertEqual(expected, self.notifier.notify.call_args_list)

    def test_simple_alarm_clear_with_outlier_exclusion(self):
        self. _do_test_simple_alarm_clear_outlier_exclusion(True)

    def test_simple_alarm_no_clear_without_outlier_exclusion(self):
        self. _do_test_simple_alarm_clear_outlier_exclusion(False)

    @mock.patch.object(timeutils, 'utcnow')
    def test_state_change_inside_time_constraint(self, mock_utcnow):
        self._set_all_alarms('ok')
        self.alarms[0].time_constraints = [
            {'name': 'test',
             'description': 'test',
             'start': '0 11 * * *',  # daily at 11:00
             'duration': 10800,  # 3 hours
             'timezone': 'Europe/Ljubljana'}
        ]
        self.alarms[1].time_constraints = self.alarms[0].time_constraints
        dt = datetime.datetime(2014, 1, 1, 12, 0, 0,
                               tzinfo=pytz.timezone('Europe/Ljubljana'))
        mock_utcnow.return_value = dt.astimezone(pytz.UTC)
        with mock.patch('ceilometerclient.client.get_client',
                        return_value=self.api_client):
            # the following part based on test_simple_insufficient
            self.api_client.statistics.list.return_value = []
            self._evaluate_all_alarms()
            self._assert_all_alarms('insufficient data')
            expected = [mock.call(alarm.alarm_id,
                                  state='insufficient data')
                        for alarm in self.alarms]
            update_calls = self.api_client.alarms.set_state.call_args_list
            self.assertEqual(expected, update_calls,
                             "Alarm should change state if the current "
                             "time is inside its time constraint.")
            expected = [mock.call(
                alarm,
                'ok',
                ('%d datapoints are unknown'
                 % alarm.rule['evaluation_periods']),
                self._reason_data('unknown',
                                  alarm.rule['evaluation_periods'],
                                  None))
                for alarm in self.alarms]
            self.assertEqual(expected, self.notifier.notify.call_args_list)

    @mock.patch.object(timeutils, 'utcnow')
    def test_no_state_change_outside_time_constraint(self, mock_utcnow):
        self._set_all_alarms('ok')
        self.alarms[0].time_constraints = [
            {'name': 'test',
             'description': 'test',
             'start': '0 11 * * *',  # daily at 11:00
             'duration': 10800,  # 3 hours
             'timezone': 'Europe/Ljubljana'}
        ]
        self.alarms[1].time_constraints = self.alarms[0].time_constraints
        dt = datetime.datetime(2014, 1, 1, 15, 0, 0,
                               tzinfo=pytz.timezone('Europe/Ljubljana'))
        mock_utcnow.return_value = dt.astimezone(pytz.UTC)
        with mock.patch('ceilometerclient.client.get_client',
                        return_value=self.api_client):
            self.api_client.statistics.list.return_value = []
            self._evaluate_all_alarms()
            self._assert_all_alarms('ok')
            update_calls = self.api_client.alarms.set_state.call_args_list
            self.assertEqual([], update_calls,
                             "Alarm should not change state if the current "
                             " time is outside its time constraint.")
            self.assertEqual([], self.notifier.notify.call_args_list)

########NEW FILE########
__FILENAME__ = test_coordination
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Red Hat, Inc
#
# Author: Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests for ceilometer/alarm/partition/coordination.py
"""
import datetime
import logging
import six
import uuid

import mock
from six import moves

from ceilometer.alarm.partition import coordination
from ceilometer import messaging
from ceilometer.openstack.common.fixture import config
from ceilometer.openstack.common import test
from ceilometer.openstack.common import timeutils
from ceilometer.storage import models


class TestCoordinate(test.BaseTestCase):
    def setUp(self):
        super(TestCoordinate, self).setUp()
        self.CONF = self.useFixture(config.Config()).conf
        messaging.setup('fake://')
        self.addCleanup(messaging.cleanup)

        self.test_interval = 120
        self.CONF.set_override('evaluation_interval',
                               self.test_interval,
                               group='alarm')
        self.api_client = mock.Mock()
        self.override_start = datetime.datetime(2012, 7, 2, 10, 45)
        patcher = mock.patch.object(timeutils, 'utcnow')
        self.addCleanup(patcher.stop)
        self.mock_utcnow = patcher.start()
        self.mock_utcnow.return_value = self.override_start
        self.partition_coordinator = coordination.PartitionCoordinator()
        self.partition_coordinator.coordination_rpc = mock.Mock()
        #add extra logger to check exception conditions and logged content
        self.output = six.moves.StringIO()
        self.str_handler = logging.StreamHandler(self.output)
        coordination.LOG.logger.addHandler(self.str_handler)

    def tearDown(self):
        super(TestCoordinate, self).tearDown()
        # clean up the logger
        coordination.LOG.logger.removeHandler(self.str_handler)
        self.output.close()

    def _no_alarms(self):
        self.api_client.alarms.list.return_value = []

    def _some_alarms(self, count):
        alarm_ids = [str(uuid.uuid4()) for _ in moves.xrange(count)]
        alarms = [self._make_alarm(aid) for aid in alarm_ids]
        self.api_client.alarms.list.return_value = alarms
        return alarm_ids

    def _current_alarms(self):
        return self.api_client.alarms.list.return_value

    def _dump_alarms(self, shave):
        alarms = self.api_client.alarms.list.return_value
        alarms = alarms[:shave]
        alarm_ids = [a.alarm_id for a in alarms]
        self.api_client.alarms.list.return_value = alarms
        return alarm_ids

    def _add_alarms(self, boost):
        new_alarm_ids = [str(uuid.uuid4()) for _ in moves.xrange(boost)]
        alarms = self.api_client.alarms.list.return_value
        for aid in new_alarm_ids:
            alarms.append(self._make_alarm(aid))
        self.api_client.alarms.list.return_value = alarms
        return new_alarm_ids

    @staticmethod
    def _make_alarm(uuid):
        return models.Alarm(name='instance_running_hot',
                            type='threshold',
                            user_id='foobar',
                            project_id='snafu',
                            enabled=True,
                            description='',
                            repeat_actions=False,
                            state='insufficient data',
                            state_timestamp=None,
                            timestamp=None,
                            ok_actions=[],
                            alarm_actions=[],
                            insufficient_data_actions=[],
                            alarm_id=uuid,
                            time_constraints=[],
                            rule=dict(
                                statistic='avg',
                                comparison_operator='gt',
                                threshold=80.0,
                                evaluation_periods=5,
                                period=60,
                                query=[],
                            ))

    def _advance_time(self, factor):
        delta = datetime.timedelta(seconds=self.test_interval * factor)
        self.mock_utcnow.return_value = timeutils.utcnow() + delta

    def _younger_by(self, offset):
        return self.partition_coordinator.this.priority + offset

    def _older_by(self, offset):
        return self.partition_coordinator.this.priority - offset

    def _check_mastership(self, expected):
        self.partition_coordinator.check_mastership(self.test_interval,
                                                    self.api_client)
        self.assertEqual(expected, self.partition_coordinator.is_master)

    def _new_partition(self, offset):
        younger = self._younger_by(offset)
        pid = uuid.uuid4()
        self.partition_coordinator.presence(pid, younger)
        return (pid, younger)

    def _check_assignments(self, others, alarm_ids, per_worker,
                           expect_uneffected=[]):
        rpc = self.partition_coordinator.coordination_rpc
        calls = rpc.assign.call_args_list
        return self._check_distribution(others, alarm_ids, per_worker, calls,
                                        expect_uneffected)

    def _check_allocation(self, others, alarm_ids, per_worker):
        rpc = self.partition_coordinator.coordination_rpc
        calls = rpc.allocate.call_args_list
        return self._check_distribution(others, alarm_ids, per_worker, calls)

    def _check_distribution(self, others, alarm_ids, per_worker, calls,
                            expect_uneffected=[]):
        uneffected = [pid for pid, _ in others]
        uneffected.extend(expect_uneffected)
        remainder = list(alarm_ids)
        for call in calls:
            args, _ = call
            target, alarms = args
            self.assertTrue(target in uneffected)
            uneffected.remove(target)
            self.assertEqual(per_worker, len(alarms))
            for aid in alarms:
                self.assertTrue(aid in remainder)
                remainder.remove(aid)
        self.assertEqual(set(expect_uneffected), set(uneffected))
        return remainder

    def _forget_assignments(self, expected_assignments):
        rpc = self.partition_coordinator.coordination_rpc
        self.assertEqual(expected_assignments, len(rpc.assign.call_args_list))
        rpc.reset_mock()

    def test_mastership_not_assumed_during_warmup(self):
        self._no_alarms()

        for _ in moves.xrange(7):
            # still warming up
            self._advance_time(0.25)
            self._check_mastership(False)

        # now warmed up
        self._advance_time(0.25)
        self._check_mastership(True)

    def test_uncontested_mastership_assumed(self):
        self._no_alarms()

        self._advance_time(3)

        self._check_mastership(True)

    def test_contested_mastership_assumed(self):
        self._no_alarms()

        self._advance_time(3)

        for offset in moves.xrange(1, 5):
            younger = self._younger_by(offset)
            self.partition_coordinator.presence(uuid.uuid4(), younger)

        self._check_mastership(True)

    def test_bested_mastership_relinquished(self):
        self._no_alarms()

        self._advance_time(3)

        self._check_mastership(True)

        older = self._older_by(1)
        self.partition_coordinator.presence(uuid.uuid4(), older)

        self._check_mastership(False)

    def _do_test_tie_broken_mastership(self, seed, expect_mastership):
        self._no_alarms()
        self.partition_coordinator.this.uuid = uuid.UUID(int=1)

        self._advance_time(3)

        self._check_mastership(True)

        tied = self.partition_coordinator.this.priority
        self.partition_coordinator.presence(uuid.UUID(int=seed), tied)

        self._check_mastership(expect_mastership)

    def test_tie_broken_mastership_assumed(self):
        self._do_test_tie_broken_mastership(2, True)

    def test_tie_broken_mastership_relinquished(self):
        self._do_test_tie_broken_mastership(0, False)

    def test_fair_distribution(self):
        alarm_ids = self._some_alarms(49)

        self._advance_time(3)

        others = [self._new_partition(i) for i in moves.xrange(1, 5)]

        self._check_mastership(True)

        remainder = self._check_assignments(others, alarm_ids, 10)
        self.assertEqual(set(self.partition_coordinator.assignment),
                         set(remainder))

    def test_rebalance_on_partition_startup(self):
        alarm_ids = self._some_alarms(49)

        self._advance_time(3)

        others = [self._new_partition(i) for i in moves.xrange(1, 5)]

        self._check_mastership(True)

        self. _forget_assignments(4)

        others.append(self._new_partition(5))
        self._check_mastership(True)

        remainder = self._check_assignments(others, alarm_ids, 9)
        self.assertEqual(set(self.partition_coordinator.assignment),
                         set(remainder))

    def test_rebalance_on_partition_staleness(self):
        alarm_ids = self._some_alarms(49)

        self._advance_time(3)

        others = [self._new_partition(i) for i in moves.xrange(1, 5)]

        self._check_mastership(True)

        self. _forget_assignments(4)

        self._advance_time(4)

        stale, _ = others.pop()
        for pid, younger in others:
            self.partition_coordinator.presence(pid, younger)

        self._check_mastership(True)

        remainder = self._check_assignments(others, alarm_ids, 13, [stale])
        self.assertEqual(set(self.partition_coordinator.assignment),
                         set(remainder))

    def test_rebalance_on_sufficient_deletion(self):
        alarm_ids = self._some_alarms(49)

        self._advance_time(3)

        others = [self._new_partition(i) for i in moves.xrange(1, 5)]

        self._check_mastership(True)

        self._forget_assignments(4)

        alarm_ids = self._dump_alarms(len(alarm_ids) / 2)

        self._check_mastership(True)

        remainder = self._check_assignments(others, alarm_ids, 5)
        self.assertEqual(set(self.partition_coordinator.assignment),
                         set(remainder))

    def test_no_rebalance_on_insufficient_deletion(self):
        alarm_ids = self._some_alarms(49)

        self._advance_time(3)

        others = [self._new_partition(i) for i in moves.xrange(1, 5)]

        self._check_mastership(True)

        self._forget_assignments(4)

        alarm_ids = self._dump_alarms(45)

        self._check_mastership(True)

        expect_uneffected = [pid for pid, _ in others]
        self._check_assignments(others, alarm_ids, 10, expect_uneffected)

    def test_no_rebalance_on_creation(self):
        self._some_alarms(49)

        self._advance_time(3)

        others = [self._new_partition(i) for i in moves.xrange(1, 5)]

        self._check_mastership(True)

        self._forget_assignments(4)

        new_alarm_ids = self._add_alarms(8)

        master_assignment = set(self.partition_coordinator.assignment)
        self._check_mastership(True)

        remainder = self._check_allocation(others, new_alarm_ids, 2)
        self.assertEqual(0, len(remainder))
        self.assertEqual(set(self.partition_coordinator.assignment),
                         master_assignment)

    def test_bail_when_overtaken_in_distribution(self):
        self._some_alarms(49)

        self._advance_time(3)

        for i in moves.xrange(1, 5):
            self._new_partition(i)

        def overtake(*args):
            self._new_partition(-1)

        rpc = self.partition_coordinator.coordination_rpc
        rpc.assign.side_effect = overtake

        self._check_mastership(False)

        self.assertEqual(1, len(rpc.assign.call_args_list))

    def test_assigned_alarms_no_assignment(self):
        alarms = self.partition_coordinator.assigned_alarms(self.api_client)
        self.assertEqual(0, len(alarms))

    def test_assigned_alarms_assignment(self):
        alarm_ids = self._some_alarms(6)

        uuid = self.partition_coordinator.this.uuid
        self.partition_coordinator.assign(uuid, alarm_ids)

        alarms = self.partition_coordinator.assigned_alarms(self.api_client)
        self.assertEqual(self._current_alarms(), alarms)

    def test_assigned_alarms_allocation(self):
        alarm_ids = self._some_alarms(6)

        uuid = self.partition_coordinator.this.uuid
        self.partition_coordinator.assign(uuid, alarm_ids)

        new_alarm_ids = self._add_alarms(2)
        self.partition_coordinator.allocate(uuid, new_alarm_ids)

        alarms = self.partition_coordinator.assigned_alarms(self.api_client)
        self.assertEqual(self._current_alarms(), alarms)

    def test_assigned_alarms_deleted_assignment(self):
        alarm_ids = self._some_alarms(6)

        uuid = self.partition_coordinator.this.uuid
        self.partition_coordinator.assign(uuid, alarm_ids)

        self._dump_alarms(len(alarm_ids) / 2)

        alarms = self.partition_coordinator.assigned_alarms(self.api_client)
        self.assertEqual(self._current_alarms(), alarms)

    def test__record_oldest(self):
        # Test when the partition to be recorded is the same as the oldest.
        self.partition_coordinator._record_oldest(
            self.partition_coordinator.oldest, True)
        self.assertIsNone(self.partition_coordinator.oldest)

    def test_check_mastership(self):
        # Test the method exception condition.
        self.partition_coordinator._is_master = mock.Mock(
            side_effect=Exception('Boom!'))
        self.partition_coordinator.check_mastership(10, None)
        self.assertTrue('mastership check failed' in self.output.getvalue())

    def test_report_presence(self):
        self.partition_coordinator.coordination_rpc.presence = mock.Mock(
            side_effect=Exception('Boom!'))
        self.partition_coordinator.report_presence()
        self.assertTrue('presence reporting failed' in self.output.getvalue())

    def test_assigned_alarms(self):
        api_client = mock.MagicMock()
        api_client.alarms = mock.Mock(side_effect=Exception('Boom!'))
        self.partition_coordinator.assignment = ['something']
        self.partition_coordinator.assigned_alarms(api_client)
        self.assertTrue('assignment retrieval failed' in
                        self.output.getvalue())


class TestPartitionIdentity(test.BaseTestCase):
    def setUp(self):
        super(TestPartitionIdentity, self).setUp()
        self.id_1st = coordination.PartitionIdentity(str(uuid.uuid4()), 1)
        self.id_2nd = coordination.PartitionIdentity(str(uuid.uuid4()), 2)

    def test_identity_ops(self):
        self.assertNotEqual(self.id_1st, 'Nothing')
        self.assertNotEqual(self.id_1st, self.id_2nd)
        self.assertTrue(self.id_1st < None)
        self.assertFalse(self.id_1st < 'Nothing')
        self.assertTrue(self.id_2nd > self.id_1st)

########NEW FILE########
__FILENAME__ = test_notifier
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 eNovance
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import six.moves.urllib.parse as urlparse

import mock
import requests

from ceilometer.alarm import service
from ceilometer import messaging
from ceilometer.openstack.common import context
from ceilometer.openstack.common.fixture import config
from ceilometer.openstack.common.fixture import mockpatch
from ceilometer.openstack.common import test


DATA_JSON = ('{"current": "ALARM", "alarm_id": "foobar",'
             ' "reason": "what ?", "reason_data": {"test": "test"},'
             ' "previous": "OK"}')
NOTIFICATION = dict(alarm_id='foobar',
                    condition=dict(threshold=42),
                    reason='what ?',
                    reason_data={'test': 'test'},
                    previous='OK',
                    current='ALARM')


class TestAlarmNotifier(test.BaseTestCase):

    def setUp(self):
        super(TestAlarmNotifier, self).setUp()
        messaging.setup('fake://')
        self.addCleanup(messaging.cleanup)

        self.CONF = self.useFixture(config.Config()).conf
        self.service = service.AlarmNotifierService()

    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    def test_init_host(self):
        # If we try to create a real RPC connection, init_host() never
        # returns. Mock it out so we can establish the service
        # configuration.
        with mock.patch.object(self.service.rpc_server, 'start'):
            self.service.start()

    def test_notify_alarm(self):
        data = {
            'actions': ['test://'],
            'alarm_id': 'foobar',
            'previous': 'OK',
            'current': 'ALARM',
            'reason': 'Everything is on fire',
            'reason_data': {'fire': 'everywhere'}
        }
        self.service.notify_alarm(context.get_admin_context(), data)
        notifications = self.service.notifiers['test'].obj.notifications
        self.assertEqual(1, len(notifications))
        self.assertEqual((urlparse.urlsplit(data['actions'][0]),
                          data['alarm_id'],
                          data['previous'],
                          data['current'],
                          data['reason'],
                          data['reason_data']),
                         notifications[0])

    def test_notify_alarm_no_action(self):
        self.service.notify_alarm(context.get_admin_context(), {})

    def test_notify_alarm_log_action(self):
        self.service.notify_alarm(context.get_admin_context(),
                                  {
                                      'actions': ['log://'],
                                      'alarm_id': 'foobar',
                                      'condition': {'threshold': 42},
                                  })

    @staticmethod
    def _fake_spawn_n(func, *args, **kwargs):
        func(*args, **kwargs)

    @staticmethod
    def _notification(action):
        notification = {}
        notification.update(NOTIFICATION)
        notification['actions'] = [action]
        return notification

    def test_notify_alarm_rest_action_ok(self):
        action = 'http://host/action'

        with mock.patch('eventlet.spawn_n', self._fake_spawn_n):
            with mock.patch.object(requests, 'post') as poster:
                self.service.notify_alarm(context.get_admin_context(),
                                          self._notification(action))
                poster.assert_called_with(action, data=DATA_JSON)

    def test_notify_alarm_rest_action_with_ssl_client_cert(self):
        action = 'https://host/action'
        certificate = "/etc/ssl/cert/whatever.pem"

        self.CONF.set_override("rest_notifier_certificate_file", certificate,
                               group='alarm')

        with mock.patch('eventlet.spawn_n', self._fake_spawn_n):
            with mock.patch.object(requests, 'post') as poster:
                self.service.notify_alarm(context.get_admin_context(),
                                          self._notification(action))
                poster.assert_called_with(action, data=DATA_JSON,
                                          cert=certificate, verify=True)

    def test_notify_alarm_rest_action_with_ssl_client_cert_and_key(self):
        action = 'https://host/action'
        certificate = "/etc/ssl/cert/whatever.pem"
        key = "/etc/ssl/cert/whatever.key"

        self.CONF.set_override("rest_notifier_certificate_file", certificate,
                               group='alarm')
        self.CONF.set_override("rest_notifier_certificate_key", key,
                               group='alarm')

        with mock.patch('eventlet.spawn_n', self._fake_spawn_n):
            with mock.patch.object(requests, 'post') as poster:
                self.service.notify_alarm(context.get_admin_context(),
                                          self._notification(action))
                poster.assert_called_with(action, data=DATA_JSON,
                                          cert=(certificate, key), verify=True)

    def test_notify_alarm_rest_action_with_ssl_verify_disable_by_cfg(self):
        action = 'https://host/action'

        self.CONF.set_override("rest_notifier_ssl_verify", False,
                               group='alarm')

        with mock.patch('eventlet.spawn_n', self._fake_spawn_n):
            with mock.patch.object(requests, 'post') as poster:
                self.service.notify_alarm(context.get_admin_context(),
                                          self._notification(action))
                poster.assert_called_with(action, data=DATA_JSON,
                                          verify=False)

    def test_notify_alarm_rest_action_with_ssl_verify_disable(self):
        action = 'https://host/action?ceilometer-alarm-ssl-verify=0'

        with mock.patch('eventlet.spawn_n', self._fake_spawn_n):
            with mock.patch.object(requests, 'post') as poster:
                self.service.notify_alarm(context.get_admin_context(),
                                          self._notification(action))
                poster.assert_called_with(action, data=DATA_JSON,
                                          verify=False)

    def test_notify_alarm_rest_action_with_ssl_verify_enable_by_user(self):
        action = 'https://host/action?ceilometer-alarm-ssl-verify=1'

        self.CONF.set_override("rest_notifier_ssl_verify", False,
                               group='alarm')

        with mock.patch('eventlet.spawn_n', self._fake_spawn_n):
            with mock.patch.object(requests, 'post') as poster:
                self.service.notify_alarm(context.get_admin_context(),
                                          self._notification(action))
                poster.assert_called_with(action, data=DATA_JSON,
                                          verify=True)

    @staticmethod
    def _fake_urlsplit(*args, **kwargs):
        raise Exception("Evil urlsplit!")

    def test_notify_alarm_invalid_url(self):
        with mock.patch('ceilometer.openstack.common.network_utils.urlsplit',
                        self._fake_urlsplit):
            LOG = mock.MagicMock()
            with mock.patch('ceilometer.alarm.service.LOG', LOG):
                self.service.notify_alarm(
                    context.get_admin_context(),
                    {
                        'actions': ['no-such-action-i-am-sure'],
                        'alarm_id': 'foobar',
                        'condition': {'threshold': 42},
                    })
                self.assertTrue(LOG.error.called)

    def test_notify_alarm_invalid_action(self):
        LOG = mock.MagicMock()
        with mock.patch('ceilometer.alarm.service.LOG', LOG):
            self.service.notify_alarm(
                context.get_admin_context(),
                {
                    'actions': ['no-such-action-i-am-sure://'],
                    'alarm_id': 'foobar',
                    'condition': {'threshold': 42},
                })
            self.assertTrue(LOG.error.called)

    def test_notify_alarm_trust_action(self):
        action = 'trust+http://trust-1234@host/action'
        url = 'http://host/action'

        client = mock.MagicMock()
        client.auth_token = 'token_1234'

        self.useFixture(mockpatch.Patch('keystoneclient.v3.client.Client',
                                        lambda **kwargs: client))

        with mock.patch('eventlet.spawn_n', self._fake_spawn_n):
            with mock.patch.object(requests, 'post') as poster:
                self.service.notify_alarm(context.get_admin_context(),
                                          self._notification(action))
                poster.assert_called_with(
                    url, data=DATA_JSON,
                    headers={'X-Auth-Token': 'token_1234'})

########NEW FILE########
__FILENAME__ = test_partitioned_alarm_svc
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Red Hat, Inc
#
# Author: Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests for ceilometer.alarm.service.PartitionedAlarmService.
"""
import contextlib
import mock
from stevedore import extension

from ceilometer.alarm import service
from ceilometer import messaging
from ceilometer.openstack.common.fixture import config
from ceilometer.openstack.common import test


class TestPartitionedAlarmService(test.BaseTestCase):
    def setUp(self):
        super(TestPartitionedAlarmService, self).setUp()
        messaging.setup('fake://')
        self.addCleanup(messaging.cleanup)

        self.threshold_eval = mock.Mock()
        self.api_client = mock.MagicMock()
        self.CONF = self.useFixture(config.Config()).conf

        self.CONF.set_override('host',
                               'fake_host')
        self.CONF.set_override('partition_rpc_topic',
                               'fake_topic',
                               group='alarm')
        self.partitioned = service.PartitionedAlarmService()
        self.partitioned.tg = mock.Mock()
        self.partitioned.partition_coordinator = mock.Mock()
        self.extension_mgr = extension.ExtensionManager.make_test_instance(
            [
                extension.Extension(
                    'threshold',
                    None,
                    None,
                    self.threshold_eval, ),
            ]
        )
        self.partitioned.extension_manager = self.extension_mgr

    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    def test_lifecycle(self):
        test_interval = 120
        self.CONF.set_override('evaluation_interval',
                               test_interval,
                               group='alarm')
        get_client = 'ceilometerclient.client.get_client'
        with contextlib.nested(
                mock.patch(get_client, return_value=self.api_client),
                mock.patch.object(self.partitioned.rpc_server, 'start')):
            self.partitioned.start()
            pc = self.partitioned.partition_coordinator
            expected = [
                mock.call(test_interval / 4,
                          pc.report_presence,
                          0),
                mock.call(test_interval / 2,
                          pc.check_mastership,
                          test_interval,
                          test_interval,
                          self.api_client),
                mock.call(test_interval,
                          self.partitioned._evaluate_assigned_alarms,
                          test_interval),
                mock.call(604800, mock.ANY),
            ]
            actual = self.partitioned.tg.add_timer.call_args_list
            self.assertEqual(expected, actual)
            self.partitioned.stop()

    def test_presence_reporting(self):
        priority = 42
        self.partitioned.presence(mock.Mock(),
                                  dict(uuid='uuid', priority=priority))
        pc = self.partitioned.partition_coordinator
        pc.presence.assert_called_once_with('uuid', priority)

    def test_alarm_assignment(self):
        alarms = [mock.Mock()]
        self.partitioned.assign(mock.Mock(),
                                dict(uuid='uuid', alarms=alarms))
        pc = self.partitioned.partition_coordinator
        pc.assign.assert_called_once_with('uuid', alarms)

    def test_alarm_allocation(self):
        alarms = [mock.Mock()]
        self.partitioned.allocate(mock.Mock(),
                                  dict(uuid='uuid', alarms=alarms))
        pc = self.partitioned.partition_coordinator
        pc.allocate.assert_called_once_with('uuid', alarms)

########NEW FILE########
__FILENAME__ = test_rpc
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 eNovance <licensing@enovance.com>
#
# Authors: Mehdi Abaakouk <mehdi.abaakouk@enovance.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import uuid

from ceilometerclient.v2 import alarms
import eventlet

from ceilometer.alarm import rpc as rpc_alarm
from ceilometer import messaging
from ceilometer.openstack.common import test
from ceilometer.openstack.common import timeutils
from ceilometer.storage import models


class FakeNotifier(object):
    def __init__(self):
        self.rpc = messaging.get_rpc_server("alarm_notifier", self)
        self.notified = []

    def start(self, expected_length):
        self.expected_length = expected_length
        self.rpc.start()

    def notify_alarm(self, context, data):
        self.notified.append(data)
        if len(self.notified) == self.expected_length:
            self.rpc.stop()


class TestRPCAlarmNotifier(test.BaseTestCase):
    def setUp(self):
        super(TestRPCAlarmNotifier, self).setUp()
        messaging.setup('fake://')
        self.addCleanup(messaging.cleanup)

        self.notifier_server = FakeNotifier()
        self.notifier = rpc_alarm.RPCAlarmNotifier()
        self.alarms = [
            alarms.Alarm(None, info={
                'name': 'instance_running_hot',
                'meter_name': 'cpu_util',
                'comparison_operator': 'gt',
                'threshold': 80.0,
                'evaluation_periods': 5,
                'statistic': 'avg',
                'state': 'ok',
                'ok_actions': ['http://host:8080/path'],
                'user_id': 'foobar',
                'project_id': 'snafu',
                'period': 60,
                'alarm_id': str(uuid.uuid4()),
                'matching_metadata':{'resource_id':
                                     'my_instance'}
            }),
            alarms.Alarm(None, info={
                'name': 'group_running_idle',
                'meter_name': 'cpu_util',
                'comparison_operator': 'le',
                'threshold': 10.0,
                'statistic': 'max',
                'evaluation_periods': 4,
                'state': 'insufficient data',
                'insufficient_data_actions': ['http://other_host/path'],
                'user_id': 'foobar',
                'project_id': 'snafu',
                'period': 300,
                'alarm_id': str(uuid.uuid4()),
                'matching_metadata':{'metadata.user_metadata.AS':
                                     'my_group'}
            }),
        ]

    def test_rpc_target(self):
        topic = self.notifier.client.target.topic
        self.assertEqual('alarm_notifier', topic)

    def test_notify_alarm(self):
        self.notifier_server.start(2)

        previous = ['alarm', 'ok']
        for i, a in enumerate(self.alarms):
            self.notifier.notify(a, previous[i], "what? %d" % i,
                                 {'fire': '%d' % i})

        self.notifier_server.rpc.wait()

        self.assertEqual(2, len(self.notifier_server.notified))
        for i, a in enumerate(self.alarms):
            actions = getattr(a, models.Alarm.ALARM_ACTIONS_MAP[a.state])
            self.assertEqual(self.alarms[i].alarm_id,
                             self.notifier_server.notified[i]["alarm_id"])
            self.assertEqual(actions,
                             self.notifier_server.notified[i]["actions"])
            self.assertEqual(previous[i],
                             self.notifier_server.notified[i]["previous"])
            self.assertEqual(self.alarms[i].state,
                             self.notifier_server.notified[i]["current"])
            self.assertEqual("what? %d" % i,
                             self.notifier_server.notified[i]["reason"])
            self.assertEqual({'fire': '%d' % i},
                             self.notifier_server.notified[i]["reason_data"])

    def test_notify_non_string_reason(self):
        self.notifier_server.start(1)
        self.notifier.notify(self.alarms[0], 'ok', 42, {})
        self.notifier_server.rpc.wait()
        reason = self.notifier_server.notified[0]['reason']
        self.assertIsInstance(reason, basestring)

    def test_notify_no_actions(self):
        alarm = alarms.Alarm(None, info={
            'name': 'instance_running_hot',
            'meter_name': 'cpu_util',
            'comparison_operator': 'gt',
            'threshold': 80.0,
            'evaluation_periods': 5,
            'statistic': 'avg',
            'state': 'ok',
            'user_id': 'foobar',
            'project_id': 'snafu',
            'period': 60,
            'ok_actions': [],
            'alarm_id': str(uuid.uuid4()),
            'matching_metadata': {'resource_id':
                                  'my_instance'}
        })
        self.notifier.notify(alarm, 'alarm', "what?", {})
        self.assertEqual(0, len(self.notifier_server.notified))


class FakeCoordinator(object):
    def __init__(self):
        self.rpc = messaging.get_rpc_server(
            "alarm_partition_coordination", self)
        self.notified = []

    def presence(self, context, data):
        self._record('presence', data)

    def allocate(self, context, data):
        self._record('allocate', data)

    def assign(self, context, data):
        self._record('assign', data)

    def _record(self, method, data):
        self.notified.append((method, data))
        self.rpc.stop()


class TestRPCAlarmPartitionCoordination(test.BaseTestCase):
    def setUp(self):
        super(TestRPCAlarmPartitionCoordination, self).setUp()
        messaging.setup('fake://')
        self.addCleanup(messaging.cleanup)

        self.coordinator_server = FakeCoordinator()
        self.coordinator_server.rpc.start()
        eventlet.sleep()  # must be sure that fanout queue is created

        self.ordination = rpc_alarm.RPCAlarmPartitionCoordination()
        self.alarms = [
            alarms.Alarm(None, info={
                'name': 'instance_running_hot',
                'meter_name': 'cpu_util',
                'comparison_operator': 'gt',
                'threshold': 80.0,
                'evaluation_periods': 5,
                'statistic': 'avg',
                'state': 'ok',
                'ok_actions': ['http://host:8080/path'],
                'user_id': 'foobar',
                'project_id': 'snafu',
                'period': 60,
                'alarm_id': str(uuid.uuid4()),
                'matching_metadata':{'resource_id':
                                     'my_instance'}
            }),
            alarms.Alarm(None, info={
                'name': 'group_running_idle',
                'meter_name': 'cpu_util',
                'comparison_operator': 'le',
                'threshold': 10.0,
                'statistic': 'max',
                'evaluation_periods': 4,
                'state': 'insufficient data',
                'insufficient_data_actions': ['http://other_host/path'],
                'user_id': 'foobar',
                'project_id': 'snafu',
                'period': 300,
                'alarm_id': str(uuid.uuid4()),
                'matching_metadata':{'metadata.user_metadata.AS':
                                     'my_group'}
            }),
        ]

    def test_ordination_presence(self):
        id = str(uuid.uuid4())
        priority = float(timeutils.utcnow().strftime('%s.%f'))
        self.ordination.presence(id, priority)
        self.coordinator_server.rpc.wait()
        method, args = self.coordinator_server.notified[0]
        self.assertEqual(id, args['uuid'])
        self.assertEqual(priority, args['priority'])
        self.assertEqual('presence', method)

    def test_ordination_assign(self):
        id = str(uuid.uuid4())
        self.ordination.assign(id, self.alarms)
        self.coordinator_server.rpc.wait()
        method, args = self.coordinator_server.notified[0]
        self.assertEqual(id, args['uuid'])
        self.assertEqual(2, len(args['alarms']))
        self.assertEqual('assign', method)

    def test_ordination_allocate(self):
        id = str(uuid.uuid4())
        self.ordination.allocate(id, self.alarms)
        self.coordinator_server.rpc.wait()
        method, args = self.coordinator_server.notified[0]
        self.assertEqual(id, args['uuid'])
        self.assertEqual(2, len(args['alarms']))
        self.assertEqual('allocate', method)

########NEW FILE########
__FILENAME__ = test_singleton_alarm_svc
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Red Hat, Inc
#
# Author: Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests for ceilometer.alarm.service.SingletonAlarmService.
"""
import mock

from oslo.config import cfg

from stevedore import extension

from ceilometer.alarm import service
from ceilometer import messaging
from ceilometer.openstack.common import test


class TestSingletonAlarmService(test.BaseTestCase):
    def setUp(self):
        super(TestSingletonAlarmService, self).setUp()
        messaging.setup('fake://')
        self.addCleanup(messaging.cleanup)

        self.threshold_eval = mock.Mock()
        self.evaluators = extension.ExtensionManager.make_test_instance(
            [
                extension.Extension(
                    'threshold',
                    None,
                    None,
                    self.threshold_eval),
            ]
        )
        self.api_client = mock.MagicMock()
        self.singleton = service.SingletonAlarmService()
        self.singleton.tg = mock.Mock()
        self.singleton.evaluators = self.evaluators
        self.singleton.supported_evaluators = ['threshold']

    def test_start(self):
        test_interval = 120
        cfg.CONF.set_override('evaluation_interval',
                              test_interval,
                              group='alarm')
        with mock.patch('ceilometerclient.client.get_client',
                        return_value=self.api_client):
            self.singleton.start()
            expected = [
                mock.call(test_interval,
                          self.singleton._evaluate_assigned_alarms,
                          0),
                mock.call(604800, mock.ANY),
            ]
            actual = self.singleton.tg.add_timer.call_args_list
            self.assertEqual(expected, actual)

    def test_evaluation_cycle(self):
        alarm = mock.Mock(type='threshold')
        self.api_client.alarms.list.return_value = [alarm]
        with mock.patch('ceilometerclient.client.get_client',
                        return_value=self.api_client):
            self.singleton._evaluate_assigned_alarms()
            self.threshold_eval.evaluate.assert_called_once_with(alarm)

    def test_unknown_extension_skipped(self):
        alarms = [
            mock.Mock(type='not_existing_type'),
            mock.Mock(type='threshold')
        ]

        self.api_client.alarms.list.return_value = alarms
        with mock.patch('ceilometerclient.client.get_client',
                        return_value=self.api_client):
            self.singleton.start()
            self.singleton._evaluate_assigned_alarms()
            self.threshold_eval.evaluate.assert_called_once_with(alarms[1])

    def test_singleton_endpoint_types(self):
        endpoint_types = ["internalURL", "publicURL"]
        for endpoint_type in endpoint_types:
            cfg.CONF.set_override('os_endpoint_type',
                                  endpoint_type,
                                  group='service_credentials')
            with mock.patch('ceilometerclient.client.get_client') as client:
                self.singleton.api_client = None
                self.singleton._evaluate_assigned_alarms()
                conf = cfg.CONF.service_credentials
                expected = [mock.call(2,
                                      os_auth_url=conf.os_auth_url,
                                      os_region_name=conf.os_region_name,
                                      os_tenant_name=conf.os_tenant_name,
                                      os_password=conf.os_password,
                                      os_username=conf.os_username,
                                      os_cacert=conf.os_cacert,
                                      os_endpoint_type=conf.os_endpoint_type,
                                      insecure=conf.insecure)]
                actual = client.call_args_list
                self.assertEqual(expected, actual)

########NEW FILE########
__FILENAME__ = test_app
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2014 IBM Corp.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import socket

from oslo.config import cfg

from ceilometer.api import app
from ceilometer.openstack.common.fixture import config
from ceilometer.tests import base


class TestApp(base.BaseTestCase):

    def setUp(self):
        super(TestApp, self).setUp()
        self.CONF = self.useFixture(config.Config()).conf

    def test_WSGI_address_family(self):
        self.CONF.set_override('host', '::', group='api')
        server_cls = app.get_server_cls(cfg.CONF.api.host)
        self.assertEqual(server_cls.address_family, socket.AF_INET6)

        self.CONF.set_override('host', '127.0.0.1', group='api')
        server_cls = app.get_server_cls(cfg.CONF.api.host)
        self.assertEqual(server_cls.address_family, socket.AF_INET)

        self.CONF.set_override('host', 'ddddd', group='api')
        server_cls = app.get_server_cls(cfg.CONF.api.host)
        self.assertEqual(server_cls.address_family, socket.AF_INET)

########NEW FILE########
__FILENAME__ = test_acl_scenarios
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Test ACL."""

import datetime
import json

import webtest

from ceilometer.api import app
from ceilometer.api.controllers import v2 as v2_api
from ceilometer.openstack.common import timeutils
from ceilometer.publisher import utils
from ceilometer import sample
from ceilometer.tests import api as acl
from ceilometer.tests.api.v2 import FunctionalTest
from ceilometer.tests import db as tests_db

VALID_TOKEN = '4562138218392831'
VALID_TOKEN2 = '4562138218392832'


class FakeMemcache(object):
    @staticmethod
    def get(key):
        if key == "tokens/%s" % VALID_TOKEN:
            dt = timeutils.utcnow() + datetime.timedelta(minutes=5)
            return json.dumps(({'access': {
                'token': {'id': VALID_TOKEN},
                'user': {
                    'id': 'user_id1',
                    'name': 'user_name1',
                    'tenantId': '123i2910',
                    'tenantName': 'mytenant',
                    'roles': [
                        {'name': 'admin'},
                    ]},
            }}, timeutils.isotime(dt)))
        if key == "tokens/%s" % VALID_TOKEN2:
            dt = timeutils.utcnow() + datetime.timedelta(minutes=5)
            return json.dumps(({'access': {
                'token': {'id': VALID_TOKEN2},
                'user': {
                    'id': 'user_id2',
                    'name': 'user-good',
                    'tenantId': 'project-good',
                    'tenantName': 'goodies',
                    'roles': [
                        {'name': 'Member'},
                    ]},
            }}, timeutils.isotime(dt)))

    @staticmethod
    def set(key, value, **kwargs):
        pass


class TestAPIACL(FunctionalTest,
                 tests_db.MixinTestsWithBackendScenarios):

    def setUp(self):
        super(TestAPIACL, self).setUp()
        self.environ = {'fake.cache': FakeMemcache()}

        for cnt in [
                sample.Sample(
                    'meter.test',
                    'cumulative',
                    '',
                    1,
                    'user-good',
                    'project-good',
                    'resource-good',
                    timestamp=datetime.datetime(2012, 7, 2, 10, 40),
                    resource_metadata={'display_name': 'test-server',
                                       'tag': 'self.sample'},
                    source='test_source'),
                sample.Sample(
                    'meter.mine',
                    'gauge',
                    '',
                    1,
                    'user-fred',
                    'project-good',
                    'resource-56',
                    timestamp=datetime.datetime(2012, 7, 2, 10, 43),
                    resource_metadata={'display_name': 'test-server',
                                       'tag': 'self.sample4'},
                    source='test_source')]:
            msg = utils.meter_message_from_counter(
                cnt,
                self.CONF.publisher.metering_secret)
            self.conn.record_metering_data(msg)

    def get_json(self, path, expect_errors=False, headers=None,
                 q=None, **params):
        return super(TestAPIACL, self).get_json(path,
                                                expect_errors=expect_errors,
                                                headers=headers,
                                                q=q or [],
                                                extra_environ=self.environ,
                                                **params)

    def _make_app(self):
        self.CONF.set_override("cache", "fake.cache", group=acl.OPT_GROUP_NAME)
        file_name = self.path_get('etc/ceilometer/api_paste.ini')
        self.CONF.set_override("api_paste_config", file_name)
        return webtest.TestApp(app.load_app())

    def test_non_authenticated(self):
        response = self.get_json('/meters', expect_errors=True)
        self.assertEqual(401, response.status_int)

    def test_authenticated_wrong_role(self):
        response = self.get_json('/meters',
                                 expect_errors=True,
                                 headers={
                                     "X-Roles": "Member",
                                     "X-Tenant-Name": "admin",
                                     "X-Project-Id":
                                     "bc23a9d531064583ace8f67dad60f6bb",
                                 })
        self.assertEqual(401, response.status_int)

    # FIXME(dhellmann): This test is not properly looking at the tenant
    # info. We do not correctly detect the improper tenant. That's
    # really something the keystone middleware would have to do using
    # the incoming token, which we aren't providing.
    #
    # def test_authenticated_wrong_tenant(self):
    #     response = self.get_json('/meters',
    #                              expect_errors=True,
    #                              headers={
    #             "X-Roles": "admin",
    #             "X-Tenant-Name": "achoo",
    #             "X-Project-Id": "bc23a9d531064583ace8f67dad60f6bb",
    #             })
    #     self.assertEqual(401, response.status_int)

    def test_authenticated(self):
        data = self.get_json('/meters',
                             headers={"X-Auth-Token": VALID_TOKEN,
                                      "X-Roles": "admin",
                                      "X-Tenant-Name": "admin",
                                      "X-Project-Id":
                                      "bc23a9d531064583ace8f67dad60f6bb",
                                      })
        ids = set(r['resource_id'] for r in data)
        self.assertEqual(set(['resource-good', 'resource-56']), ids)

    def test_with_non_admin_missing_project_query(self):
        data = self.get_json('/meters',
                             headers={"X-Roles": "Member",
                                      "X-Auth-Token": VALID_TOKEN2,
                                      "X-Project-Id": "project-good"})
        ids = set(r['resource_id'] for r in data)
        self.assertEqual(set(['resource-good', 'resource-56']), ids)

    def test_with_non_admin(self):
        data = self.get_json('/meters',
                             headers={"X-Roles": "Member",
                                      "X-Auth-Token": VALID_TOKEN2,
                                      "X-Project-Id": "project-good"},
                             q=[{'field': 'project_id',
                                 'value': 'project-good',
                                 }])
        ids = set(r['resource_id'] for r in data)
        self.assertEqual(set(['resource-good', 'resource-56']), ids)

    def test_non_admin_wrong_project(self):
        data = self.get_json('/meters',
                             expect_errors=True,
                             headers={"X-Roles": "Member",
                                      "X-Auth-Token": VALID_TOKEN2,
                                      "X-Project-Id": "project-good"},
                             q=[{'field': 'project_id',
                                 'value': 'project-wrong',
                                 }])
        self.assertEqual(401, data.status_int)

    def test_non_admin_two_projects(self):
        data = self.get_json('/meters',
                             expect_errors=True,
                             headers={"X-Roles": "Member",
                                      "X-Auth-Token": VALID_TOKEN2,
                                      "X-Project-Id": "project-good"},
                             q=[{'field': 'project_id',
                                 'value': 'project-good',
                                 },
                                {'field': 'project_id',
                                 'value': 'project-naughty',
                                 }])
        self.assertEqual(401, data.status_int)

    def test_non_admin_get_events(self):

        # NOTE(herndon): wsme does not handle the  error that is being
        # raised in by requires_admin dues to the decorator ordering. wsme
        # does not play nice with other decorators, and so requires_admin
        # must call wsme.wsexpose, and not the other way arou. The
        # implication is that I can't look at the status code in the
        # return value. Work around is to catch the exception here and
        # verify that the status code is correct.

        try:
            # Intentionally *not* using assertRaises here so I can look
            # at the status code of the exception.
            self.get_json('/event_types', expect_errors=True,
                          headers={"X-Roles": "Member",
                                   "X-Auth-Token": VALID_TOKEN2,
                                   "X-Project-Id": "project-good"})
        except v2_api.ClientSideError as ex:
            self.assertEqual(401, ex.code)
        else:
            self.fail()

########NEW FILE########
__FILENAME__ = test_alarm_scenarios
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 eNovance <licensing@enovance.com>
#
# Author: Mehdi Abaakouk <mehdi.abaakouk@enovance.com>
#         Angus Salkeld <asalkeld@redhat.com>
#         Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
'''Tests alarm operation
'''

import datetime
import json as jsonutils
import logging
import uuid

import mock
import oslo.messaging.conffixture
from six import moves

from ceilometer import messaging
from ceilometer.storage import models
from ceilometer.tests.api.v2 import FunctionalTest
from ceilometer.tests import db as tests_db


LOG = logging.getLogger(__name__)


class TestListEmptyAlarms(FunctionalTest,
                          tests_db.MixinTestsWithBackendScenarios):

    def test_empty(self):
        data = self.get_json('/alarms')
        self.assertEqual([], data)


class TestAlarms(FunctionalTest,
                 tests_db.MixinTestsWithBackendScenarios):

    def setUp(self):
        super(TestAlarms, self).setUp()
        self.auth_headers = {'X-User-Id': str(uuid.uuid4()),
                             'X-Project-Id': str(uuid.uuid4())}
        for alarm in [
            models.Alarm(name='name1',
                         type='threshold',
                         enabled=True,
                         alarm_id='a',
                         description='a',
                         state='insufficient data',
                         state_timestamp=None,
                         timestamp=None,
                         ok_actions=[],
                         insufficient_data_actions=[],
                         alarm_actions=[],
                         repeat_actions=True,
                         user_id=self.auth_headers['X-User-Id'],
                         project_id=self.auth_headers['X-Project-Id'],
                         time_constraints=[dict(name='testcons',
                                                start='0 11 * * *',
                                                duration=300)],
                         rule=dict(comparison_operator='gt',
                                   threshold=2.0,
                                   statistic='avg',
                                   evaluation_periods=60,
                                   period=1,
                                   meter_name='meter.test',
                                   query=[{'field': 'project_id',
                                           'op': 'eq', 'value':
                                           self.auth_headers['X-Project-Id']}
                                          ])
                         ),
            models.Alarm(name='name2',
                         type='threshold',
                         enabled=True,
                         alarm_id='b',
                         description='b',
                         state='insufficient data',
                         state_timestamp=None,
                         timestamp=None,
                         ok_actions=[],
                         insufficient_data_actions=[],
                         alarm_actions=[],
                         repeat_actions=False,
                         user_id=self.auth_headers['X-User-Id'],
                         project_id=self.auth_headers['X-Project-Id'],
                         time_constraints=[],
                         rule=dict(comparison_operator='gt',
                                   threshold=4.0,
                                   statistic='avg',
                                   evaluation_periods=60,
                                   period=1,
                                   meter_name='meter.test',
                                   query=[{'field': 'project_id',
                                           'op': 'eq', 'value':
                                           self.auth_headers['X-Project-Id']}
                                          ])
                         ),
            models.Alarm(name='name3',
                         type='threshold',
                         enabled=True,
                         alarm_id='c',
                         description='c',
                         state='insufficient data',
                         state_timestamp=None,
                         timestamp=None,
                         ok_actions=[],
                         insufficient_data_actions=[],
                         alarm_actions=[],
                         repeat_actions=False,
                         user_id=self.auth_headers['X-User-Id'],
                         project_id=self.auth_headers['X-Project-Id'],
                         time_constraints=[],
                         rule=dict(comparison_operator='gt',
                                   threshold=3.0,
                                   statistic='avg',
                                   evaluation_periods=60,
                                   period=1,
                                   meter_name='meter.mine',
                                   query=[{'field': 'project_id',
                                           'op': 'eq', 'value':
                                           self.auth_headers['X-Project-Id']}
                                          ])
                         ),
            models.Alarm(name='name4',
                         type='combination',
                         enabled=True,
                         alarm_id='d',
                         description='d',
                         state='insufficient data',
                         state_timestamp=None,
                         timestamp=None,
                         ok_actions=[],
                         insufficient_data_actions=[],
                         alarm_actions=[],
                         repeat_actions=False,
                         user_id=self.auth_headers['X-User-Id'],
                         project_id=self.auth_headers['X-Project-Id'],
                         time_constraints=[],
                         rule=dict(alarm_ids=['a', 'b'],
                                   operator='or')
                         )]:
            self.conn.update_alarm(alarm)

    @staticmethod
    def _add_default_threshold_rule(alarm):
        if 'exclude_outliers' not in alarm['threshold_rule']:
            alarm['threshold_rule']['exclude_outliers'] = False

    def _verify_alarm(self, json, alarm, expected_name=None):
        if expected_name and alarm.name != expected_name:
            self.fail("Alarm not found")
        self._add_default_threshold_rule(json)
        for key in json:
            if key.endswith('_rule'):
                storage_key = 'rule'
            else:
                storage_key = key
            self.assertEqual(json[key], getattr(alarm, storage_key))

    def test_list_alarms(self):
        data = self.get_json('/alarms')
        self.assertEqual(4, len(data))
        self.assertEqual(set(['name1', 'name2', 'name3', 'name4']),
                         set(r['name'] for r in data))
        self.assertEqual(set(['meter.test', 'meter.mine']),
                         set(r['threshold_rule']['meter_name']
                             for r in data if 'threshold_rule' in r))
        self.assertEqual(set(['or']),
                         set(r['combination_rule']['operator']
                             for r in data if 'combination_rule' in r))

    def test_alarms_query_with_timestamp(self):
        date_time = datetime.datetime(2012, 7, 2, 10, 41)
        isotime = date_time.isoformat()
        resp = self.get_json('/alarms',
                             q=[{'field': 'timestamp',
                                 'op': 'gt',
                                 'value': isotime}],
                             expect_errors=True)
        self.assertEqual(resp.status_code, 400)
        self.assertEqual(jsonutils.loads(resp.body)['error_message']
                         ['faultstring'],
                         'Unknown argument: "timestamp": '
                         'not valid for this resource')

    def test_get_not_existing_alarm(self):
        resp = self.get_json('/alarms/alarm-id-3', expect_errors=True)
        self.assertEqual(404, resp.status_code)
        self.assertEqual("Alarm alarm-id-3 Not Found",
                         jsonutils.loads(resp.body)['error_message']
                         ['faultstring'])

    def test_get_alarm(self):
        alarms = self.get_json('/alarms',
                               q=[{'field': 'name',
                                   'value': 'name1',
                                   }])
        self.assertEqual('name1', alarms[0]['name'])
        self.assertEqual('meter.test',
                         alarms[0]['threshold_rule']['meter_name'])

        one = self.get_json('/alarms/%s' % alarms[0]['alarm_id'])
        self.assertEqual('name1', one['name'])
        self.assertEqual('meter.test', one['threshold_rule']['meter_name'])
        self.assertEqual(alarms[0]['alarm_id'], one['alarm_id'])
        self.assertEqual(alarms[0]['repeat_actions'], one['repeat_actions'])
        self.assertEqual(alarms[0]['time_constraints'],
                         one['time_constraints'])

    def test_get_alarm_disabled(self):
        alarm = models.Alarm(name='disabled',
                             type='combination',
                             enabled=False,
                             alarm_id='d',
                             description='d',
                             state='insufficient data',
                             state_timestamp=None,
                             timestamp=None,
                             ok_actions=[],
                             insufficient_data_actions=[],
                             alarm_actions=[],
                             repeat_actions=False,
                             user_id=self.auth_headers['X-User-Id'],
                             project_id=self.auth_headers['X-Project-Id'],
                             time_constraints=[],
                             rule=dict(alarm_ids=['a', 'b'], operator='or'))
        self.conn.update_alarm(alarm)

        alarms = self.get_json('/alarms',
                               q=[{'field': 'enabled',
                                   'value': 'False'}])
        self.assertEqual(1, len(alarms))
        self.assertEqual('disabled', alarms[0]['name'])

        one = self.get_json('/alarms/%s' % alarms[0]['alarm_id'])
        self.assertEqual('disabled', one['name'])

    def test_get_alarm_combination(self):
        alarms = self.get_json('/alarms',
                               q=[{'field': 'name',
                                   'value': 'name4',
                                   }])
        self.assertEqual('name4', alarms[0]['name'])
        self.assertEqual(['a', 'b'],
                         alarms[0]['combination_rule']['alarm_ids'])
        self.assertEqual('or', alarms[0]['combination_rule']['operator'])

        one = self.get_json('/alarms/%s' % alarms[0]['alarm_id'])
        self.assertEqual('name4', one['name'])
        self.assertEqual(['a', 'b'],
                         alarms[0]['combination_rule']['alarm_ids'])
        self.assertEqual('or', alarms[0]['combination_rule']['operator'])
        self.assertEqual(alarms[0]['alarm_id'], one['alarm_id'])
        self.assertEqual(alarms[0]['repeat_actions'], one['repeat_actions'])

    def test_post_alarm_wsme_workaround(self):
        jsons = {
            'type': {
                'name': 'missing type',
                'threshold_rule': {
                    'meter_name': 'ameter',
                    'threshold': 2.0,
                }
            },
            'name': {
                'type': 'threshold',
                'threshold_rule': {
                    'meter_name': 'ameter',
                    'threshold': 2.0,
                }
            },
            'threshold_rule/meter_name': {
                'name': 'missing meter_name',
                'type': 'threshold',
                'threshold_rule': {
                    'threshold': 2.0,
                }
            },
            'threshold_rule/threshold': {
                'name': 'missing threshold',
                'type': 'threshold',
                'threshold_rule': {
                    'meter_name': 'ameter',
                }
            },
            'combination_rule/alarm_ids': {
                'name': 'missing alarm_ids',
                'type': 'combination',
                'combination_rule': {}
            }
        }
        for field, json in jsons.iteritems():
            resp = self.post_json('/alarms', params=json, expect_errors=True,
                                  status=400, headers=self.auth_headers)
            self.assertEqual("Invalid input for field/attribute %s."
                             " Value: \'None\'. Mandatory field missing."
                             % field.split('/', 1)[-1],
                             resp.json['error_message']['faultstring'])
        alarms = list(self.conn.get_alarms())
        self.assertEqual(4, len(alarms))

    def test_post_invalid_alarm_time_constraint_start(self):
        json = {
            'name': 'added_alarm_invalid_constraint_duration',
            'type': 'threshold',
            'time_constraints': [
                {
                    'name': 'testcons',
                    'start': '11:00am',
                    'duration': 10
                }
            ],
            'threshold_rule': {
                'meter_name': 'ameter',
                'threshold': 300.0
            }
        }
        self.post_json('/alarms', params=json, expect_errors=True, status=400,
                       headers=self.auth_headers)
        alarms = list(self.conn.get_alarms())
        self.assertEqual(4, len(alarms))

    def test_post_duplicate_time_constraint_name(self):
        json = {
            'name': 'added_alarm_duplicate_constraint_name',
            'type': 'threshold',
            'time_constraints': [
                {
                    'name': 'testcons',
                    'start': '* 11 * * *',
                    'duration': 10
                },
                {
                    'name': 'testcons',
                    'start': '* * * * *',
                    'duration': 20
                }
            ],
            'threshold_rule': {
                'meter_name': 'ameter',
                'threshold': 300.0
            }
        }
        resp = self.post_json('/alarms', params=json, expect_errors=True,
                              status=400, headers=self.auth_headers)
        self.assertEqual(
            "Time constraint names must be unique for a given alarm.",
            resp.json['error_message']['faultstring'])
        alarms = list(self.conn.get_alarms())
        self.assertEqual(4, len(alarms))

    def test_post_invalid_alarm_time_constraint_duration(self):
        json = {
            'name': 'added_alarm_invalid_constraint_duration',
            'type': 'threshold',
            'time_constraints': [
                {
                    'name': 'testcons',
                    'start': '* 11 * * *',
                    'duration': -1,
                }
            ],
            'threshold_rule': {
                'meter_name': 'ameter',
                'threshold': 300.0
            }
        }
        self.post_json('/alarms', params=json, expect_errors=True, status=400,
                       headers=self.auth_headers)
        alarms = list(self.conn.get_alarms())
        self.assertEqual(4, len(alarms))

    def test_post_invalid_alarm_time_constraint_timezone(self):
        json = {
            'name': 'added_alarm_invalid_constraint_timezone',
            'type': 'threshold',
            'time_constraints': [
                {
                    'name': 'testcons',
                    'start': '* 11 * * *',
                    'duration': 10,
                    'timezone': 'aaaa'
                }
            ],
            'threshold_rule': {
                'meter_name': 'ameter',
                'threshold': 300.0
            }
        }
        self.post_json('/alarms', params=json, expect_errors=True, status=400,
                       headers=self.auth_headers)
        alarms = list(self.conn.get_alarms())
        self.assertEqual(4, len(alarms))

    def test_post_invalid_alarm_period(self):
        json = {
            'name': 'added_alarm_invalid_period',
            'type': 'threshold',
            'threshold_rule': {
                'meter_name': 'ameter',
                'comparison_operator': 'gt',
                'threshold': 2.0,
                'statistic': 'avg',
                'period': -1,
            }

        }
        self.post_json('/alarms', params=json, expect_errors=True, status=400,
                       headers=self.auth_headers)
        alarms = list(self.conn.get_alarms())
        self.assertEqual(4, len(alarms))

    def test_post_null_threshold_rule(self):
        json = {
            'name': 'added_alarm_invalid_threshold_rule',
            'type': 'threshold',
            'threshold_rule': None,
            'combination_rule': None,
        }
        resp = self.post_json('/alarms', params=json, expect_errors=True,
                              status=400, headers=self.auth_headers)
        self.assertEqual(
            "threshold_rule must be set for threshold type alarm",
            resp.json['error_message']['faultstring'])

    def test_post_invalid_alarm_statistic(self):
        json = {
            'name': 'added_alarm',
            'type': 'threshold',
            'threshold_rule': {
                'meter_name': 'ameter',
                'comparison_operator': 'gt',
                'threshold': 2.0,
                'statistic': 'magic',
            }
        }
        self.post_json('/alarms', params=json, expect_errors=True, status=400,
                       headers=self.auth_headers)
        alarms = list(self.conn.get_alarms())
        self.assertEqual(4, len(alarms))

    def test_post_invalid_alarm_query(self):
        json = {
            'name': 'added_alarm',
            'type': 'threshold',
            'threshold_rule': {
                'meter_name': 'ameter',
                'query': [{'field': 'metadata.invalid',
                           'field': 'gt',
                           'value': 'value'}],
                'comparison_operator': 'gt',
                'threshold': 2.0,
                'statistic': 'avg',
            }
        }
        self.post_json('/alarms', params=json, expect_errors=True, status=400,
                       headers=self.auth_headers)
        alarms = list(self.conn.get_alarms())
        self.assertEqual(4, len(alarms))

    def test_post_invalid_alarm_query_field_type(self):
        json = {
            'name': 'added_alarm',
            'type': 'threshold',
            'threshold_rule': {
                'meter_name': 'ameter',
                'query': [{'field': 'metadata.valid',
                           'op': 'eq',
                           'value': 'value',
                           'type': 'blob'}],
                'comparison_operator': 'gt',
                'threshold': 2.0,
                'statistic': 'avg',
            }
        }
        resp = self.post_json('/alarms', params=json, expect_errors=True,
                              status=400, headers=self.auth_headers)
        expected_error_message = 'The data type blob is not supported.'
        resp_string = jsonutils.loads(resp.body)
        fault_string = resp_string['error_message']['faultstring']
        self.assertTrue(fault_string.startswith(expected_error_message))
        alarms = list(self.conn.get_alarms())
        self.assertEqual(4, len(alarms))

    def test_post_invalid_alarm_have_multiple_rules(self):
        json = {
            'name': 'added_alarm',
            'type': 'threshold',
            'threshold_rule': {
                'meter_name': 'ameter',
                'query': [{'field': 'meter',
                           'value': 'ameter'}],
                'comparison_operator': 'gt',
                'threshold': 2.0,
            },
            'combination_rule': {
                'alarm_ids': ['a', 'b'],

            }
        }
        resp = self.post_json('/alarms', params=json, expect_errors=True,
                              status=400, headers=self.auth_headers)
        alarms = list(self.conn.get_alarms())
        self.assertEqual(4, len(alarms))
        self.assertEqual('threshold_rule and combination_rule cannot '
                         'be set at the same time',
                         resp.json['error_message']['faultstring'])

    def test_post_invalid_alarm_timestamp_in_threshold_rule(self):
        date_time = datetime.datetime(2012, 7, 2, 10, 41)
        isotime = date_time.isoformat()

        json = {
            'name': 'invalid_alarm',
            'type': 'threshold',
            'threshold_rule': {
                'meter_name': 'ameter',
                'query': [{'field': 'timestamp',
                           'op': 'gt',
                           'value': isotime}],
                'comparison_operator': 'gt',
                'threshold': 2.0,
            }
        }
        resp = self.post_json('/alarms', params=json, expect_errors=True,
                              status=400, headers=self.auth_headers)
        alarms = list(self.conn.get_alarms())
        self.assertEqual(4, len(alarms))
        self.assertEqual(
            'Unknown argument: "timestamp": '
            'not valid for this resource',
            resp.json['error_message']['faultstring'])

    def test_post_alarm_defaults(self):
        to_check = {
            'enabled': True,
            'name': 'added_alarm_defaults',
            'state': 'insufficient data',
            'description': ('Alarm when ameter is eq a avg of '
                            '300.0 over 60 seconds'),
            'type': 'threshold',
            'ok_actions': [],
            'alarm_actions': [],
            'insufficient_data_actions': [],
            'repeat_actions': False,
            'threshold_rule': {
                'meter_name': 'ameter',
                'query': [{'field': 'project_id',
                           'op': 'eq',
                           'value': self.auth_headers['X-Project-Id']}],
                'threshold': 300.0,
                'comparison_operator': 'eq',
                'statistic': 'avg',
                'evaluation_periods': 1,
                'period': 60,
            }

        }
        self._add_default_threshold_rule(to_check)

        json = {
            'name': 'added_alarm_defaults',
            'type': 'threshold',
            'threshold_rule': {
                'meter_name': 'ameter',
                'threshold': 300.0
            }
        }
        self.post_json('/alarms', params=json, status=201,
                       headers=self.auth_headers)
        alarms = list(self.conn.get_alarms())
        self.assertEqual(5, len(alarms))
        for alarm in alarms:
            if alarm.name == 'added_alarm_defaults':
                for key in to_check:
                    if key.endswith('_rule'):
                        storage_key = 'rule'
                    else:
                        storage_key = key
                    self.assertEqual(to_check[key],
                                     getattr(alarm, storage_key))
                break
        else:
            self.fail("Alarm not found")

    def test_post_conflict(self):
        json = {
            'enabled': False,
            'name': 'added_alarm',
            'state': 'ok',
            'type': 'threshold',
            'ok_actions': ['http://something/ok'],
            'alarm_actions': ['http://something/alarm'],
            'insufficient_data_actions': ['http://something/no'],
            'repeat_actions': True,
            'threshold_rule': {
                'meter_name': 'ameter',
                'query': [{'field': 'metadata.field',
                           'op': 'eq',
                           'value': '5',
                           'type': 'string'}],
                'comparison_operator': 'le',
                'statistic': 'count',
                'threshold': 50,
                'evaluation_periods': '3',
                'period': '180',
            }
        }

        self.post_json('/alarms', params=json, status=201,
                       headers=self.auth_headers)
        self.post_json('/alarms', params=json, status=409,
                       headers=self.auth_headers)

    def _do_test_post_alarm(self, exclude_outliers=None):
        json = {
            'enabled': False,
            'name': 'added_alarm',
            'state': 'ok',
            'type': 'threshold',
            'ok_actions': ['http://something/ok'],
            'alarm_actions': ['http://something/alarm'],
            'insufficient_data_actions': ['http://something/no'],
            'repeat_actions': True,
            'threshold_rule': {
                'meter_name': 'ameter',
                'query': [{'field': 'metadata.field',
                           'op': 'eq',
                           'value': '5',
                           'type': 'string'}],
                'comparison_operator': 'le',
                'statistic': 'count',
                'threshold': 50,
                'evaluation_periods': '3',
                'period': '180',
            }
        }
        if exclude_outliers is not None:
            json['threshold_rule']['exclude_outliers'] = exclude_outliers

        self.post_json('/alarms', params=json, status=201,
                       headers=self.auth_headers)
        alarms = list(self.conn.get_alarms(enabled=False))
        self.assertEqual(1, len(alarms))
        json['threshold_rule']['query'].append({
            'field': 'project_id', 'op': 'eq',
            'value': self.auth_headers['X-Project-Id']})
        # to check to IntegerType type conversion
        json['threshold_rule']['evaluation_periods'] = 3
        json['threshold_rule']['period'] = 180
        self._verify_alarm(json, alarms[0], 'added_alarm')

    def test_post_alarm_outlier_exclusion_set(self):
        self._do_test_post_alarm(True)

    def test_post_alarm_outlier_exclusion_clear(self):
        self._do_test_post_alarm(False)

    def test_post_alarm_outlier_exclusion_defaulted(self):
        self._do_test_post_alarm()

    def test_post_alarm_noauth(self):
        json = {
            'enabled': False,
            'name': 'added_alarm',
            'state': 'ok',
            'type': 'threshold',
            'ok_actions': ['http://something/ok'],
            'alarm_actions': ['http://something/alarm'],
            'insufficient_data_actions': ['http://something/no'],
            'repeat_actions': True,
            'threshold_rule': {
                'meter_name': 'ameter',
                'query': [{'field': 'metadata.field',
                           'op': 'eq',
                           'value': '5',
                           'type': 'string'}],
                'comparison_operator': 'le',
                'statistic': 'count',
                'threshold': 50,
                'evaluation_periods': '3',
                'exclude_outliers': False,
                'period': '180',
            }
        }
        self.post_json('/alarms', params=json, status=201)
        alarms = list(self.conn.get_alarms(enabled=False))
        self.assertEqual(1, len(alarms))
        # to check to BoundedInt type conversion
        json['threshold_rule']['evaluation_periods'] = 3
        json['threshold_rule']['period'] = 180
        if alarms[0].name == 'added_alarm':
            for key in json:
                if key.endswith('_rule'):
                    storage_key = 'rule'
                else:
                    storage_key = key
                self.assertEqual(getattr(alarms[0], storage_key),
                                 json[key])
        else:
            self.fail("Alarm not found")

    def _do_test_post_alarm_as_admin(self, explicit_project_constraint):
        """Test the creation of an alarm as admin for another project."""
        json = {
            'enabled': False,
            'name': 'added_alarm',
            'state': 'ok',
            'type': 'threshold',
            'user_id': 'auseridthatisnotmine',
            'project_id': 'aprojectidthatisnotmine',
            'threshold_rule': {
                'meter_name': 'ameter',
                'query': [{'field': 'metadata.field',
                           'op': 'eq',
                           'value': '5',
                           'type': 'string'}],
                'comparison_operator': 'le',
                'statistic': 'count',
                'threshold': 50,
                'evaluation_periods': 3,
                'period': 180,
            }
        }
        if explicit_project_constraint:
            project_constraint = {'field': 'project_id', 'op': 'eq',
                                  'value': 'aprojectidthatisnotmine'}
            json['threshold_rule']['query'].append(project_constraint)
        headers = {}
        headers.update(self.auth_headers)
        headers['X-Roles'] = 'admin'
        self.post_json('/alarms', params=json, status=201,
                       headers=headers)
        alarms = list(self.conn.get_alarms(enabled=False))
        self.assertEqual(1, len(alarms))
        self.assertEqual('auseridthatisnotmine', alarms[0].user_id)
        self.assertEqual('aprojectidthatisnotmine', alarms[0].project_id)
        self._add_default_threshold_rule(json)
        if alarms[0].name == 'added_alarm':
            for key in json:
                if key.endswith('_rule'):
                    storage_key = 'rule'
                    if explicit_project_constraint:
                        self.assertEqual(json[key],
                                         getattr(alarms[0], storage_key))
                    else:
                        query = getattr(alarms[0], storage_key).get('query')
                        self.assertEqual(2, len(query))
                        implicit_constraint = {
                            u'field': u'project_id',
                            u'value': u'aprojectidthatisnotmine',
                            u'op': u'eq'
                        }
                        self.assertEqual(implicit_constraint, query[1])
                else:
                    self.assertEqual(json[key], getattr(alarms[0], key))
        else:
            self.fail("Alarm not found")

    def test_post_alarm_as_admin_explicit_project_constraint(self):
        """Test the creation of an alarm as admin for another project,
        with an explicit query constraint on the owner's project ID.
        """
        self._do_test_post_alarm_as_admin(True)

    def test_post_alarm_as_admin_implicit_project_constraint(self):
        """Test the creation of an alarm as admin for another project,
        without an explicit query constraint on the owner's project ID.
        """
        self._do_test_post_alarm_as_admin(False)

    def test_post_alarm_as_admin_no_user(self):
        """Test the creation of an alarm as admin for another project but
        forgetting to set the values.
        """
        json = {
            'enabled': False,
            'name': 'added_alarm',
            'state': 'ok',
            'type': 'threshold',
            'project_id': 'aprojectidthatisnotmine',
            'threshold_rule': {
                'meter_name': 'ameter',
                'query': [{'field': 'metadata.field',
                           'op': 'eq',
                           'value': '5',
                           'type': 'string'},
                          {'field': 'project_id', 'op': 'eq',
                           'value': 'aprojectidthatisnotmine'}],
                'comparison_operator': 'le',
                'statistic': 'count',
                'threshold': 50,
                'evaluation_periods': 3,
                'period': 180,
            }
        }
        headers = {}
        headers.update(self.auth_headers)
        headers['X-Roles'] = 'admin'
        self.post_json('/alarms', params=json, status=201,
                       headers=headers)
        alarms = list(self.conn.get_alarms(enabled=False))
        self.assertEqual(1, len(alarms))
        self.assertEqual(self.auth_headers['X-User-Id'], alarms[0].user_id)
        self.assertEqual('aprojectidthatisnotmine', alarms[0].project_id)
        self._verify_alarm(json, alarms[0], 'added_alarm')

    def test_post_alarm_as_admin_no_project(self):
        """Test the creation of an alarm as admin for another project but
        forgetting to set the values.
        """
        json = {
            'enabled': False,
            'name': 'added_alarm',
            'state': 'ok',
            'type': 'threshold',
            'user_id': 'auseridthatisnotmine',
            'threshold_rule': {
                'meter_name': 'ameter',
                'query': [{'field': 'metadata.field',
                           'op': 'eq',
                           'value': '5',
                           'type': 'string'},
                          {'field': 'project_id', 'op': 'eq',
                           'value': 'aprojectidthatisnotmine'}],
                'comparison_operator': 'le',
                'statistic': 'count',
                'threshold': 50,
                'evaluation_periods': 3,
                'period': 180,
            }
        }
        headers = {}
        headers.update(self.auth_headers)
        headers['X-Roles'] = 'admin'
        self.post_json('/alarms', params=json, status=201,
                       headers=headers)
        alarms = list(self.conn.get_alarms(enabled=False))
        self.assertEqual(1, len(alarms))
        self.assertEqual('auseridthatisnotmine', alarms[0].user_id)
        self.assertEqual(self.auth_headers['X-Project-Id'],
                         alarms[0].project_id)
        self._verify_alarm(json, alarms[0], 'added_alarm')

    @staticmethod
    def _alarm_representation_owned_by(identifiers):
        json = {
            'name': 'added_alarm',
            'enabled': False,
            'type': 'threshold',
            'ok_actions': ['http://something/ok'],
            'threshold_rule': {
                'meter_name': 'ameter',
                'query': [{'field': 'metadata.field',
                           'op': 'eq',
                           'value': '5',
                           'type': 'string'}],
                'comparison_operator': 'le',
                'statistic': 'count',
                'threshold': 50,
                'evaluation_periods': 3,
                'period': 180,
            }
        }
        for aspect, id in identifiers.iteritems():
            json['%s_id' % aspect] = id
        return json

    def _do_test_post_alarm_as_nonadmin_on_behalf_of_another(self,
                                                             identifiers):
        """Test that posting an alarm as non-admin on behalf of another
        user/project fails with an explicit 401 instead of reverting
        to the requestor's identity.
        """
        json = self._alarm_representation_owned_by(identifiers)
        headers = {}
        headers.update(self.auth_headers)
        headers['X-Roles'] = 'demo'
        resp = self.post_json('/alarms', params=json, status=401,
                              headers=headers)
        aspect = 'user' if 'user' in identifiers else 'project'
        params = dict(aspect=aspect, id=identifiers[aspect])
        self.assertEqual("Not Authorized to access %(aspect)s %(id)s" % params,
                         jsonutils.loads(resp.body)['error_message']
                         ['faultstring'])

    def test_post_alarm_as_nonadmin_on_behalf_of_another_user(self):
        identifiers = dict(user='auseridthatisnotmine')
        self._do_test_post_alarm_as_nonadmin_on_behalf_of_another(identifiers)

    def test_post_alarm_as_nonadmin_on_behalf_of_another_project(self):
        identifiers = dict(project='aprojectidthatisnotmine')
        self._do_test_post_alarm_as_nonadmin_on_behalf_of_another(identifiers)

    def test_post_alarm_as_nonadmin_on_behalf_of_another_creds(self):
        identifiers = dict(user='auseridthatisnotmine',
                           project='aprojectidthatisnotmine')
        self._do_test_post_alarm_as_nonadmin_on_behalf_of_another(identifiers)

    def _do_test_post_alarm_as_nonadmin_on_behalf_of_self(self, identifiers):
        """Test posting an alarm as non-admin on behalf of own user/project
        creates alarm associated with the requestor's identity.
        """
        json = self._alarm_representation_owned_by(identifiers)
        headers = {}
        headers.update(self.auth_headers)
        headers['X-Roles'] = 'demo'
        self.post_json('/alarms', params=json, status=201, headers=headers)
        alarms = list(self.conn.get_alarms(enabled=False))
        self.assertEqual(1, len(alarms))
        self.assertEqual(alarms[0].user_id,
                         self.auth_headers['X-User-Id'])
        self.assertEqual(alarms[0].project_id,
                         self.auth_headers['X-Project-Id'])

    def test_post_alarm_as_nonadmin_on_behalf_of_own_user(self):
        identifiers = dict(user=self.auth_headers['X-User-Id'])
        self._do_test_post_alarm_as_nonadmin_on_behalf_of_self(identifiers)

    def test_post_alarm_as_nonadmin_on_behalf_of_own_project(self):
        identifiers = dict(project=self.auth_headers['X-Project-Id'])
        self._do_test_post_alarm_as_nonadmin_on_behalf_of_self(identifiers)

    def test_post_alarm_as_nonadmin_on_behalf_of_own_creds(self):
        identifiers = dict(user=self.auth_headers['X-User-Id'],
                           project=self.auth_headers['X-Project-Id'])
        self._do_test_post_alarm_as_nonadmin_on_behalf_of_self(identifiers)

    def test_post_alarm_combination(self):
        json = {
            'enabled': False,
            'name': 'added_alarm',
            'state': 'ok',
            'type': 'combination',
            'ok_actions': ['http://something/ok'],
            'alarm_actions': ['http://something/alarm'],
            'insufficient_data_actions': ['http://something/no'],
            'repeat_actions': True,
            'combination_rule': {
                'alarm_ids': ['a',
                              'b'],
                'operator': 'and',
            }
        }
        self.post_json('/alarms', params=json, status=201,
                       headers=self.auth_headers)
        alarms = list(self.conn.get_alarms(enabled=False))
        self.assertEqual(1, len(alarms))
        if alarms[0].name == 'added_alarm':
            for key in json:
                if key.endswith('_rule'):
                    storage_key = 'rule'
                else:
                    storage_key = key
                self.assertEqual(json[key], getattr(alarms[0], storage_key))
        else:
            self.fail("Alarm not found")

    def test_post_combination_alarm_as_user_with_unauthorized_alarm(self):
        """Test that post a combination alarm as normal user/project
        with an alarm_id unauthorized for this project/user
        """
        json = {
            'enabled': False,
            'name': 'added_alarm',
            'state': 'ok',
            'type': 'combination',
            'ok_actions': ['http://something/ok'],
            'alarm_actions': ['http://something/alarm'],
            'insufficient_data_actions': ['http://something/no'],
            'repeat_actions': True,
            'combination_rule': {
                'alarm_ids': ['a',
                              'b'],
                'operator': 'and',
            }
        }
        an_other_user_auth = {'X-User-Id': str(uuid.uuid4()),
                              'X-Project-Id': str(uuid.uuid4())}
        resp = self.post_json('/alarms', params=json, status=404,
                              headers=an_other_user_auth)
        self.assertEqual("Alarm a Not Found",
                         jsonutils.loads(resp.body)['error_message']
                         ['faultstring'])

    def test_post_combination_alarm_as_admin_on_behalf_of_an_other_user(self):
        """Test that post a combination alarm as admin on behalf of an other
        user/project with an alarm_id unauthorized for this project/user
        """
        json = {
            'enabled': False,
            'name': 'added_alarm',
            'state': 'ok',
            'user_id': 'auseridthatisnotmine',
            'project_id': 'aprojectidthatisnotmine',
            'type': 'combination',
            'ok_actions': ['http://something/ok'],
            'alarm_actions': ['http://something/alarm'],
            'insufficient_data_actions': ['http://something/no'],
            'repeat_actions': True,
            'combination_rule': {
                'alarm_ids': ['a',
                              'b'],
                'operator': 'and',
            }
        }

        headers = {}
        headers.update(self.auth_headers)
        headers['X-Roles'] = 'admin'
        resp = self.post_json('/alarms', params=json, status=404,
                              headers=headers)
        self.assertEqual("Alarm a Not Found",
                         jsonutils.loads(resp.body)['error_message']
                         ['faultstring'])

    def test_post_combination_alarm_with_reasonable_description(self):
        """Test that post a combination alarm with two blanks around the
        operator in alarm description.
        """
        json = {
            'enabled': False,
            'name': 'added_alarm',
            'state': 'ok',
            'type': 'combination',
            'ok_actions': ['http://something/ok'],
            'alarm_actions': ['http://something/alarm'],
            'insufficient_data_actions': ['http://something/no'],
            'repeat_actions': True,
            'combination_rule': {
                'alarm_ids': ['a',
                              'b'],
                'operator': 'and',
            }
        }
        self.post_json('/alarms', params=json, status=201,
                       headers=self.auth_headers)
        alarms = list(self.conn.get_alarms(enabled=False))
        self.assertEqual(1, len(alarms))
        self.assertEqual(u'Combined state of alarms a and b',
                         alarms[0].description)

    def test_post_combination_alarm_as_admin_success_owner_unset(self):
        self._do_post_combination_alarm_as_admin_success(False)

    def test_post_combination_alarm_as_admin_success_owner_set(self):
        self._do_post_combination_alarm_as_admin_success(True)

    def test_post_combination_alarm_with_threshold_rule(self):
        """Test the creation of an combination alarm with threshold rule.
        """
        json = {
            'enabled': False,
            'name': 'added_alarm',
            'state': 'ok',
            'type': 'combination',
            'ok_actions': ['http://something/ok'],
            'alarm_actions': ['http://something/alarm'],
            'insufficient_data_actions': ['http://something/no'],
            'repeat_actions': True,
            'threshold_rule': {
                'meter_name': 'ameter',
                'query': [{'field': 'metadata.field',
                           'op': 'eq',
                           'value': '5',
                           'type': 'string'}],
                'comparison_operator': 'le',
                'statistic': 'count',
                'threshold': 50,
                'evaluation_periods': '3',
                'period': '180',
            }
        }
        resp = self.post_json('/alarms', params=json,
                              expect_errors=True, status=400,
                              headers=self.auth_headers)
        self.assertEqual(
            "combination_rule must be set for combination type alarm",
            resp.json['error_message']['faultstring'])

    def test_post_threshold_alarm_with_combination_rule(self):
        """Test the creation of an threshold alarm with combination rule.
        """
        json = {
            'enabled': False,
            'name': 'added_alarm',
            'state': 'ok',
            'type': 'threshold',
            'ok_actions': ['http://something/ok'],
            'alarm_actions': ['http://something/alarm'],
            'insufficient_data_actions': ['http://something/no'],
            'repeat_actions': True,
            'combination_rule': {
                'alarm_ids': ['a',
                              'b'],
                'operator': 'and',
            }
        }
        resp = self.post_json('/alarms', params=json,
                              expect_errors=True, status=400,
                              headers=self.auth_headers)
        self.assertEqual(
            "threshold_rule must be set for threshold type alarm",
            resp.json['error_message']['faultstring'])

    def _do_post_combination_alarm_as_admin_success(self, owner_is_set):
        """Test that post a combination alarm as admin on behalf of nobody
        with an alarm_id of someone else, with owner set or not
        """
        json = {
            'enabled': False,
            'name': 'added_alarm',
            'state': 'ok',
            'type': 'combination',
            'ok_actions': ['http://something/ok'],
            'alarm_actions': ['http://something/alarm'],
            'insufficient_data_actions': ['http://something/no'],
            'repeat_actions': True,
            'combination_rule': {
                'alarm_ids': ['a',
                              'b'],
                'operator': 'and',
            }
        }
        an_other_admin_auth = {'X-User-Id': str(uuid.uuid4()),
                               'X-Project-Id': str(uuid.uuid4()),
                               'X-Roles': 'admin'}
        if owner_is_set:
            json['project_id'] = an_other_admin_auth['X-Project-Id']
            json['user_id'] = an_other_admin_auth['X-User-Id']

        self.post_json('/alarms', params=json, status=201,
                       headers=an_other_admin_auth)
        alarms = list(self.conn.get_alarms(enabled=False))
        if alarms[0].name == 'added_alarm':
            for key in json:
                if key.endswith('_rule'):
                    storage_key = 'rule'
                else:
                    storage_key = key
                self.assertEqual(json[key], getattr(alarms[0], storage_key))
        else:
            self.fail("Alarm not found")

    def test_post_invalid_alarm_combination(self):
        """Test that post a combination alarm with a not existing alarm id
        """
        json = {
            'enabled': False,
            'name': 'added_alarm',
            'state': 'ok',
            'type': 'combination',
            'ok_actions': ['http://something/ok'],
            'alarm_actions': ['http://something/alarm'],
            'insufficient_data_actions': ['http://something/no'],
            'repeat_actions': True,
            'combination_rule': {
                'alarm_ids': ['not_exists',
                              'b'],
                'operator': 'and',
            }
        }
        self.post_json('/alarms', params=json, status=404,
                       headers=self.auth_headers)
        alarms = list(self.conn.get_alarms(enabled=False))
        self.assertEqual(0, len(alarms))

    def test_post_alarm_combination_duplicate_alarm_ids(self):
        """Test combination alarm doesn't allow duplicate alarm ids."""
        json_body = {
            'name': 'dup_alarm_id',
            'type': 'combination',
            'combination_rule': {
                'alarm_ids': ['a', 'a', 'd', 'a', 'c', 'c', 'b'],
            }
        }
        self.post_json('/alarms', params=json_body, status=201,
                       headers=self.auth_headers)
        alarms = list(self.conn.get_alarms(name='dup_alarm_id'))
        self.assertEqual(1, len(alarms))
        self.assertEqual(['a', 'd', 'c', 'b'],
                         alarms[0].rule.get('alarm_ids'))

    def _test_post_alarm_combination_rule_less_than_two_alarms(self,
                                                               alarm_ids=[]):
        json_body = {
            'name': 'one_alarm_in_combination_rule',
            'type': 'combination',
            'combination_rule': {
                'alarm_ids': alarm_ids
            }
        }

        resp = self.post_json('/alarms', params=json_body,
                              expect_errors=True, status=400,
                              headers=self.auth_headers)
        self.assertEqual(
            'Alarm combination rule should contain at'
            ' least two different alarm ids.',
            resp.json['error_message']['faultstring'])

    def test_post_alarm_combination_rule_with_no_alarm(self):
        self._test_post_alarm_combination_rule_less_than_two_alarms()

    def test_post_alarm_combination_rule_with_one_alarm(self):
        self._test_post_alarm_combination_rule_less_than_two_alarms(['a'])

    def test_post_alarm_combination_rule_with_two_same_alarms(self):
        self._test_post_alarm_combination_rule_less_than_two_alarms(['a',
                                                                     'a'])

    def test_put_alarm(self):
        json = {
            'enabled': False,
            'name': 'name_put',
            'state': 'ok',
            'type': 'threshold',
            'ok_actions': ['http://something/ok'],
            'alarm_actions': ['http://something/alarm'],
            'insufficient_data_actions': ['http://something/no'],
            'repeat_actions': True,
            'threshold_rule': {
                'meter_name': 'ameter',
                'query': [{'field': 'metadata.field',
                           'op': 'eq',
                           'value': '5',
                           'type': 'string'}],
                'comparison_operator': 'le',
                'statistic': 'count',
                'threshold': 50,
                'evaluation_periods': 3,
                'period': 180,
            }
        }
        data = self.get_json('/alarms',
                             q=[{'field': 'name',
                                 'value': 'name1',
                                 }])
        self.assertEqual(1, len(data))
        alarm_id = data[0]['alarm_id']

        self.put_json('/alarms/%s' % alarm_id,
                      params=json,
                      headers=self.auth_headers)
        alarm = list(self.conn.get_alarms(alarm_id=alarm_id, enabled=False))[0]
        json['threshold_rule']['query'].append({
            'field': 'project_id', 'op': 'eq',
            'value': self.auth_headers['X-Project-Id']})
        self._verify_alarm(json, alarm)

    def test_put_alarm_as_admin(self):
        json = {
            'user_id': 'myuserid',
            'project_id': 'myprojectid',
            'enabled': False,
            'name': 'name_put',
            'state': 'ok',
            'type': 'threshold',
            'ok_actions': ['http://something/ok'],
            'alarm_actions': ['http://something/alarm'],
            'insufficient_data_actions': ['http://something/no'],
            'repeat_actions': True,
            'threshold_rule': {
                'meter_name': 'ameter',
                'query': [{'field': 'metadata.field',
                           'op': 'eq',
                           'value': '5',
                           'type': 'string'},
                          {'field': 'project_id', 'op': 'eq',
                           'value': 'myprojectid'}],
                'comparison_operator': 'le',
                'statistic': 'count',
                'threshold': 50,
                'evaluation_periods': 3,
                'period': 180,
            }
        }
        headers = {}
        headers.update(self.auth_headers)
        headers['X-Roles'] = 'admin'

        data = self.get_json('/alarms',
                             headers=headers,
                             q=[{'field': 'name',
                                 'value': 'name1',
                                 }])
        self.assertEqual(1, len(data))
        alarm_id = data[0]['alarm_id']

        self.put_json('/alarms/%s' % alarm_id,
                      params=json,
                      headers=headers)
        alarm = list(self.conn.get_alarms(alarm_id=alarm_id, enabled=False))[0]
        self.assertEqual('myuserid', alarm.user_id)
        self.assertEqual('myprojectid', alarm.project_id)
        self._verify_alarm(json, alarm)

    def test_put_alarm_wrong_field(self):
        # Note: wsme will ignore unknown fields so will just not appear in
        # the Alarm.
        json = {
            'this_can_not_be_correct': 'ha',
            'enabled': False,
            'name': 'name1',
            'state': 'ok',
            'type': 'threshold',
            'ok_actions': ['http://something/ok'],
            'alarm_actions': ['http://something/alarm'],
            'insufficient_data_actions': ['http://something/no'],
            'repeat_actions': True,
            'threshold_rule': {
                'meter_name': 'ameter',
                'query': [{'field': 'metadata.field',
                           'op': 'eq',
                           'value': '5',
                           'type': 'string'}],
                'comparison_operator': 'le',
                'statistic': 'count',
                'threshold': 50,
                'evaluation_periods': 3,
                'period': 180,
            }
        }
        data = self.get_json('/alarms',
                             q=[{'field': 'name',
                                 'value': 'name1',
                                 }])
        self.assertEqual(1, len(data))
        alarm_id = data[0]['alarm_id']

        resp = self.put_json('/alarms/%s' % alarm_id,
                             params=json,
                             headers=self.auth_headers)
        self.assertEqual(200, resp.status_code)

    def test_put_alarm_with_existing_name(self):
        """Test that update a threshold alarm with an existing name.
        """
        json = {
            'enabled': False,
            'name': 'name1',
            'state': 'ok',
            'type': 'threshold',
            'ok_actions': ['http://something/ok'],
            'alarm_actions': ['http://something/alarm'],
            'insufficient_data_actions': ['http://something/no'],
            'repeat_actions': True,
            'threshold_rule': {
                'meter_name': 'ameter',
                'query': [{'field': 'metadata.field',
                           'op': 'eq',
                           'value': '5',
                           'type': 'string'}],
                'comparison_operator': 'le',
                'statistic': 'count',
                'threshold': 50,
                'evaluation_periods': 3,
                'period': 180,
            }
        }
        data = self.get_json('/alarms',
                             q=[{'field': 'name',
                                 'value': 'name2',
                                 }])
        self.assertEqual(1, len(data))
        alarm_id = data[0]['alarm_id']

        resp = self.put_json('/alarms/%s' % alarm_id,
                             expect_errors=True, status=409,
                             params=json,
                             headers=self.auth_headers)
        self.assertEqual(
            'Alarm with name=name1 exists',
            resp.json['error_message']['faultstring'])

    def test_put_alarm_combination_cannot_specify_itself(self):
        json = {
            'name': 'name4',
            'type': 'combination',
            'combination_rule': {
                'alarm_ids': ['d', 'a'],
            }
        }

        data = self.get_json('/alarms',
                             q=[{'field': 'name',
                                 'value': 'name4',
                                 }])
        self.assertEqual(1, len(data))
        alarm_id = data[0]['alarm_id']

        resp = self.put_json('/alarms/%s' % alarm_id,
                             expect_errors=True, status=400,
                             params=json,
                             headers=self.auth_headers)

        msg = 'Cannot specify alarm %s itself in combination rule' % alarm_id
        self.assertEqual(msg, resp.json['error_message']['faultstring'])

    def _test_put_alarm_combination_rule_less_than_two_alarms(self,
                                                              alarm_ids=[]):
        json_body = {
            'name': 'name4',
            'type': 'combination',
            'combination_rule': {
                'alarm_ids': alarm_ids
            }
        }

        data = self.get_json('/alarms',
                             q=[{'field': 'name',
                                 'value': 'name4',
                                 }])
        self.assertEqual(1, len(data))
        alarm_id = data[0]['alarm_id']

        resp = self.put_json('/alarms/%s' % alarm_id, params=json_body,
                             expect_errors=True, status=400,
                             headers=self.auth_headers)
        self.assertEqual(
            'Alarm combination rule should contain at'
            ' least two different alarm ids.',
            resp.json['error_message']['faultstring'])

    def test_put_alarm_combination_rule_with_no_alarm(self):
        self._test_put_alarm_combination_rule_less_than_two_alarms()

    def test_put_alarm_combination_rule_with_one_alarm(self):
        self._test_put_alarm_combination_rule_less_than_two_alarms(['a'])

    def test_put_alarm_combination_rule_with_two_same_alarm_itself(self):
        self._test_put_alarm_combination_rule_less_than_two_alarms(['d',
                                                                    'd'])

    def test_put_combination_alarm_with_duplicate_ids(self):
        """Test combination alarm doesn't allow duplicate alarm ids."""
        alarms = self.get_json('/alarms',
                               q=[{'field': 'name',
                                   'value': 'name4',
                                   }])
        self.assertEqual(1, len(alarms))
        alarm_id = alarms[0]['alarm_id']

        json_body = {
            'name': 'name4',
            'type': 'combination',
            'combination_rule': {
                'alarm_ids': ['c', 'a', 'b', 'a', 'c', 'b'],
            }
        }
        self.put_json('/alarms/%s' % alarm_id,
                      params=json_body, status=200,
                      headers=self.auth_headers)

        alarms = list(self.conn.get_alarms(alarm_id=alarm_id))
        self.assertEqual(1, len(alarms))
        self.assertEqual(['c', 'a', 'b'], alarms[0].rule.get('alarm_ids'))

    def test_delete_alarm(self):
        data = self.get_json('/alarms')
        self.assertEqual(4, len(data))

        resp = self.delete('/alarms/%s' % data[0]['alarm_id'],
                           headers=self.auth_headers,
                           status=204)
        self.assertEqual('', resp.body)
        alarms = list(self.conn.get_alarms())
        self.assertEqual(3, len(alarms))

    def test_get_state_alarm(self):
        data = self.get_json('/alarms')
        self.assertEqual(4, len(data))

        resp = self.get_json('/alarms/%s/state' % data[0]['alarm_id'],
                             headers=self.auth_headers)
        self.assertEqual(resp, data[0]['state'])

    def test_set_state_alarm(self):
        data = self.get_json('/alarms')
        self.assertEqual(4, len(data))

        resp = self.put_json('/alarms/%s/state' % data[0]['alarm_id'],
                             headers=self.auth_headers,
                             params='alarm')
        alarms = list(self.conn.get_alarms(alarm_id=data[0]['alarm_id']))
        self.assertEqual(1, len(alarms))
        self.assertEqual('alarm', alarms[0].state)
        self.assertEqual('alarm', resp.json)

    def test_set_invalid_state_alarm(self):
        data = self.get_json('/alarms')
        self.assertEqual(4, len(data))

        self.put_json('/alarms/%s/state' % data[0]['alarm_id'],
                      headers=self.auth_headers,
                      params='not valid',
                      status=400)

    def _get_alarm(self, id):
        data = self.get_json('/alarms')
        match = [a for a in data if a['alarm_id'] == id]
        self.assertEqual(1, len(match), 'alarm %s not found' % id)
        return match[0]

    def _get_alarm_history(self, alarm, auth_headers=None, query=None,
                           expect_errors=False, status=200):
        url = '/alarms/%s/history' % alarm['alarm_id']
        if query:
            url += '?q.op=%(op)s&q.value=%(value)s&q.field=%(field)s' % query
        resp = self.get_json(url,
                             headers=auth_headers or self.auth_headers,
                             expect_errors=expect_errors)
        if expect_errors:
            self.assertEqual(status, resp.status_code)
        return resp

    def _update_alarm(self, alarm, updated_data, auth_headers=None):
        data = self._get_alarm(alarm['alarm_id'])
        data.update(updated_data)
        self.put_json('/alarms/%s' % alarm['alarm_id'],
                      params=data,
                      headers=auth_headers or self.auth_headers)

    def _delete_alarm(self, alarm, auth_headers=None):
        self.delete('/alarms/%s' % alarm['alarm_id'],
                    headers=auth_headers or self.auth_headers,
                    status=204)

    def _assert_is_subset(self, expected, actual):
        for k, v in expected.iteritems():
            self.assertEqual(v, actual.get(k), 'mismatched field: %s' % k)
        self.assertIsNotNone(actual['event_id'])

    def _assert_in_json(self, expected, actual):
        actual = jsonutils.dumps(jsonutils.loads(actual), sort_keys=True)
        for k, v in expected.iteritems():
            fragment = jsonutils.dumps({k: v}, sort_keys=True)[1:-1]
            self.assertTrue(fragment in actual,
                            '%s not in %s' % (fragment, actual))

    def test_record_alarm_history_config(self):
        self.CONF.set_override('record_history', False, group='alarm')
        alarm = self._get_alarm('a')
        history = self._get_alarm_history(alarm)
        self.assertEqual([], history)
        self._update_alarm(alarm, dict(name='renamed'))
        history = self._get_alarm_history(alarm)
        self.assertEqual([], history)
        self.CONF.set_override('record_history', True, group='alarm')
        self._update_alarm(alarm, dict(name='foobar'))
        history = self._get_alarm_history(alarm)
        self.assertEqual(1, len(history))

    def test_get_recorded_alarm_history_on_create(self):
        new_alarm = {
            'name': 'new_alarm',
            'type': 'threshold',
            'threshold_rule': {
                'meter_name': 'ameter',
                'query': [],
                'comparison_operator': 'le',
                'statistic': 'max',
                'threshold': 42.0,
                'period': 60,
                'evaluation_periods': 1,
            }
        }
        self.post_json('/alarms', params=new_alarm, status=201,
                       headers=self.auth_headers)

        alarms = self.get_json('/alarms',
                               q=[{'field': 'name',
                                   'value': 'new_alarm',
                                   }])
        self.assertEqual(1, len(alarms))
        alarm = alarms[0]

        history = self._get_alarm_history(alarm)
        self.assertEqual(1, len(history))
        self._assert_is_subset(dict(alarm_id=alarm['alarm_id'],
                                    on_behalf_of=alarm['project_id'],
                                    project_id=alarm['project_id'],
                                    type='creation',
                                    user_id=alarm['user_id']),
                               history[0])
        self._add_default_threshold_rule(new_alarm)
        new_alarm['rule'] = new_alarm['threshold_rule']
        del new_alarm['threshold_rule']
        new_alarm['rule']['query'].append({
            'field': 'project_id', 'op': 'eq',
            'value': self.auth_headers['X-Project-Id']})
        self._assert_in_json(new_alarm, history[0]['detail'])

    def _do_test_get_recorded_alarm_history_on_update(self,
                                                      data,
                                                      type,
                                                      detail,
                                                      auth=None):
        alarm = self._get_alarm('a')
        history = self._get_alarm_history(alarm)
        self.assertEqual([], history)
        self._update_alarm(alarm, data, auth)
        history = self._get_alarm_history(alarm)
        self.assertEqual(1, len(history))
        project_id = auth['X-Project-Id'] if auth else alarm['project_id']
        user_id = auth['X-User-Id'] if auth else alarm['user_id']
        self._assert_is_subset(dict(alarm_id=alarm['alarm_id'],
                                    detail=detail,
                                    on_behalf_of=alarm['project_id'],
                                    project_id=project_id,
                                    type=type,
                                    user_id=user_id),
                               history[0])

    def test_get_recorded_alarm_history_rule_change(self):
        data = dict(name='renamed')
        detail = '{"name": "renamed"}'
        self._do_test_get_recorded_alarm_history_on_update(data,
                                                           'rule change',
                                                           detail)

    def test_get_recorded_alarm_history_state_transition_on_behalf_of(self):
        # credentials for new non-admin user, on who's behalf the alarm
        # is created
        member_user = str(uuid.uuid4())
        member_project = str(uuid.uuid4())
        member_auth = {'X-Roles': 'member',
                       'X-User-Id': member_user,
                       'X-Project-Id': member_project}
        new_alarm = {
            'name': 'new_alarm',
            'type': 'threshold',
            'state': 'ok',
            'threshold_rule': {
                'meter_name': 'other_meter',
                'query': [{'field': 'project_id',
                           'op': 'eq',
                           'value': member_project}],
                'comparison_operator': 'le',
                'statistic': 'max',
                'threshold': 42.0,
                'evaluation_periods': 1,
                'period': 60
            }
        }
        self.post_json('/alarms', params=new_alarm, status=201,
                       headers=member_auth)
        alarm = self.get_json('/alarms', headers=member_auth)[0]

        # effect a state transition as a new administrative user
        admin_user = str(uuid.uuid4())
        admin_project = str(uuid.uuid4())
        admin_auth = {'X-Roles': 'admin',
                      'X-User-Id': admin_user,
                      'X-Project-Id': admin_project}
        data = dict(state='alarm')
        self._update_alarm(alarm, data, auth_headers=admin_auth)

        self._add_default_threshold_rule(new_alarm)
        new_alarm['rule'] = new_alarm['threshold_rule']
        del new_alarm['threshold_rule']

        # ensure that both the creation event and state transition
        # are visible to the non-admin alarm owner and admin user alike
        for auth in [member_auth, admin_auth]:
            history = self._get_alarm_history(alarm, auth_headers=auth)
            self.assertEqual(2, len(history), 'hist: %s' % history)
            self._assert_is_subset(dict(alarm_id=alarm['alarm_id'],
                                        detail='{"state": "alarm"}',
                                        on_behalf_of=alarm['project_id'],
                                        project_id=admin_project,
                                        type='rule change',
                                        user_id=admin_user),
                                   history[0])
            self._assert_is_subset(dict(alarm_id=alarm['alarm_id'],
                                        on_behalf_of=alarm['project_id'],
                                        project_id=member_project,
                                        type='creation',
                                        user_id=member_user),
                                   history[1])
            self._assert_in_json(new_alarm, history[1]['detail'])

            # ensure on_behalf_of cannot be constrained in an API call
            query = dict(field='on_behalf_of',
                         op='eq',
                         value=alarm['project_id'])
            self._get_alarm_history(alarm, auth_headers=auth, query=query,
                                    expect_errors=True, status=400)

    def test_get_recorded_alarm_history_segregation(self):
        data = dict(name='renamed')
        detail = '{"name": "renamed"}'
        self._do_test_get_recorded_alarm_history_on_update(data,
                                                           'rule change',
                                                           detail)
        auth = {'X-Roles': 'member',
                'X-User-Id': str(uuid.uuid4()),
                'X-Project-Id': str(uuid.uuid4())}
        history = self._get_alarm_history(self._get_alarm('a'), auth)
        self.assertEqual([], history)

    def test_get_recorded_alarm_history_preserved_after_deletion(self):
        alarm = self._get_alarm('a')
        history = self._get_alarm_history(alarm)
        self.assertEqual([], history)
        self._update_alarm(alarm, dict(name='renamed'))
        history = self._get_alarm_history(alarm)
        self.assertEqual(1, len(history))
        alarm = self._get_alarm('a')
        self.delete('/alarms/%s' % alarm['alarm_id'],
                    headers=self.auth_headers,
                    status=204)
        history = self._get_alarm_history(alarm)
        self.assertEqual(2, len(history))
        self._assert_is_subset(dict(alarm_id=alarm['alarm_id'],
                                    on_behalf_of=alarm['project_id'],
                                    project_id=alarm['project_id'],
                                    type='deletion',
                                    user_id=alarm['user_id']),
                               history[0])
        alarm['rule'] = alarm['threshold_rule']
        del alarm['threshold_rule']
        self._assert_in_json(alarm, history[0]['detail'])
        detail = '{"name": "renamed"}'
        self._assert_is_subset(dict(alarm_id=alarm['alarm_id'],
                                    detail=detail,
                                    on_behalf_of=alarm['project_id'],
                                    project_id=alarm['project_id'],
                                    type='rule change',
                                    user_id=alarm['user_id']),
                               history[1])

    def test_get_alarm_history_ordered_by_recentness(self):
        alarm = self._get_alarm('a')
        for i in moves.xrange(10):
            self._update_alarm(alarm, dict(name='%s' % i))
        alarm = self._get_alarm('a')
        self._delete_alarm(alarm)
        history = self._get_alarm_history(alarm)
        self.assertEqual(11, len(history), 'hist: %s' % history)
        self._assert_is_subset(dict(alarm_id=alarm['alarm_id'],
                                    type='deletion'),
                               history[0])
        alarm['rule'] = alarm['threshold_rule']
        del alarm['threshold_rule']
        self._assert_in_json(alarm, history[0]['detail'])
        for i in moves.xrange(1, 10):
            detail = '{"name": "%s"}' % (10 - i)
            self._assert_is_subset(dict(alarm_id=alarm['alarm_id'],
                                        detail=detail,
                                        type='rule change'),
                                   history[i])

    def test_get_alarm_history_constrained_by_timestamp(self):
        alarm = self._get_alarm('a')
        self._update_alarm(alarm, dict(name='renamed'))
        after = datetime.datetime.utcnow().isoformat()
        query = dict(field='timestamp', op='gt', value=after)
        history = self._get_alarm_history(alarm, query=query)
        self.assertEqual(0, len(history))
        query['op'] = 'le'
        history = self._get_alarm_history(alarm, query=query)
        self.assertEqual(1, len(history))
        detail = '{"name": "renamed"}'
        self._assert_is_subset(dict(alarm_id=alarm['alarm_id'],
                                    detail=detail,
                                    on_behalf_of=alarm['project_id'],
                                    project_id=alarm['project_id'],
                                    type='rule change',
                                    user_id=alarm['user_id']),
                               history[0])

    def test_get_alarm_history_constrained_by_type(self):
        alarm = self._get_alarm('a')
        self._delete_alarm(alarm)
        query = dict(field='type', op='eq', value='deletion')
        history = self._get_alarm_history(alarm, query=query)
        self.assertEqual(1, len(history))
        self._assert_is_subset(dict(alarm_id=alarm['alarm_id'],
                                    on_behalf_of=alarm['project_id'],
                                    project_id=alarm['project_id'],
                                    type='deletion',
                                    user_id=alarm['user_id']),
                               history[0])
        alarm['rule'] = alarm['threshold_rule']
        del alarm['threshold_rule']
        self._assert_in_json(alarm, history[0]['detail'])

    def test_get_alarm_history_constrained_by_alarm_id_failed(self):
        alarm = self._get_alarm('b')
        query = dict(field='alarm_id', op='eq', value='b')
        resp = self._get_alarm_history(alarm, query=query,
                                       expect_errors=True, status=400)
        self.assertEqual('Unknown argument: "alarm_id": unrecognized'
                         ' field in query: [<Query u\'alarm_id\' eq'
                         ' u\'b\' Unset>], valid keys: set('
                         '[\'start_timestamp\', \'end_timestamp_op\','
                         ' \'project\', \'user\', \'start_timestamp_op\''
                         ', \'type\', \'end_timestamp\'])',
                         resp.json['error_message']['faultstring'])

    def test_get_alarm_history_constrained_by_not_supported_rule(self):
        alarm = self._get_alarm('b')
        query = dict(field='abcd', op='eq', value='abcd')
        resp = self._get_alarm_history(alarm, query=query,
                                       expect_errors=True, status=400)
        self.assertEqual('Unknown argument: "abcd": unrecognized'
                         ' field in query: [<Query u\'abcd\' eq'
                         ' u\'abcd\' Unset>], valid keys: set('
                         '[\'start_timestamp\', \'end_timestamp_op\','
                         ' \'project\', \'user\', \'start_timestamp_op\''
                         ', \'type\', \'end_timestamp\'])',
                         resp.json['error_message']['faultstring'])

    def test_get_nonexistent_alarm_history(self):
        # the existence of alarm history is independent of the
        # continued existence of the alarm itself
        history = self._get_alarm_history(dict(alarm_id='foobar'))
        self.assertEqual([], history)

    def test_alarms_sends_notification(self):
        # Hit the AlarmsController ...
        json = {
            'name': 'sent_notification',
            'type': 'threshold',
            'threshold_rule': {
                'meter_name': 'ameter',
                'comparison_operator': 'gt',
                'threshold': 2.0,
                'statistic': 'avg',
            }

        }
        endpoint = mock.MagicMock()
        target = oslo.messaging.Target(topic="notifications",
                                       exchange="ceilometer")
        listener = messaging.get_notification_listener([target],
                                                       [endpoint])
        listener.start()
        endpoint.info.side_effect = lambda *args: listener.stop()
        self.post_json('/alarms', params=json, headers=self.auth_headers)
        listener.wait()

        class PayloadMatcher(object):
            def __eq__(self, payload):
                return payload['detail']['name'] == 'sent_notification' and \
                    payload['type'] == 'creation' and \
                    payload['detail']['rule']['meter_name'] == 'ameter' and \
                    set(['alarm_id', 'detail', 'event_id', 'on_behalf_of',
                         'project_id', 'timestamp',
                         'user_id']).issubset(payload.keys())

        endpoint.info.assert_called_once_with(
            {'instance_uuid': None,
             'domain': None,
             'project_domain': None,
             'auth_token': None,
             'is_admin': False,
             'user': None,
             'tenant': None,
             'read_only': False,
             'show_deleted': False,
             'user_identity': '- - - - -',
             'request_id': mock.ANY,
             'user_domain': None},
            'ceilometer.api', 'alarm.creation',
            PayloadMatcher(), mock.ANY)

    def test_alarm_sends_notification(self):
        # Hit the AlarmController (with alarm_id supplied) ...
        data = self.get_json('/alarms')
        with mock.patch.object(messaging, 'get_notifier') as get_notifier:
            notifier = get_notifier.return_value

            self.delete('/alarms/%s' % data[0]['alarm_id'],
                        headers=self.auth_headers, status=204)
            get_notifier.assert_called_once_with(publisher_id='ceilometer.api')

        calls = notifier.info.call_args_list
        self.assertEqual(1, len(calls))
        args, _ = calls[0]
        context, event_type, payload = args
        self.assertEqual('alarm.deletion', event_type)
        self.assertEqual('name1', payload['detail']['name'])
        self.assertTrue(set(['alarm_id', 'detail', 'event_id', 'on_behalf_of',
                             'project_id', 'timestamp', 'type',
                             'user_id']).issubset(payload.keys()))

########NEW FILE########
__FILENAME__ = test_app
# -*- encoding: utf-8 -*-
#
# Copyright 2013 IBM Corp.
# Copyright © 2013 Julien Danjou
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Test basic ceilometer-api app
"""
import json
import os

import mock
import wsme

from ceilometer.api import app
from ceilometer.openstack.common import fileutils
from ceilometer.openstack.common.fixture import config
from ceilometer.openstack.common import gettextutils
from ceilometer import service
from ceilometer.tests import api as acl
from ceilometer.tests.api.v2 import FunctionalTest
from ceilometer.tests import base
from ceilometer.tests import db as tests_db


class TestApp(base.BaseTestCase):

    def setUp(self):
        super(TestApp, self).setUp()
        self.CONF = self.useFixture(config.Config()).conf

    def test_keystone_middleware_conf(self):
        self.CONF.set_override("auth_protocol", "file",
                               group=acl.OPT_GROUP_NAME)
        self.CONF.set_override("auth_version", "v2.0",
                               group=acl.OPT_GROUP_NAME)
        self.CONF.set_override("pipeline_cfg_file",
                               self.path_get("etc/ceilometer/pipeline.yaml"))
        self.CONF.set_override('connection', "log://", group="database")
        self.CONF.set_override("auth_uri", None, group=acl.OPT_GROUP_NAME)
        file_name = self.path_get('etc/ceilometer/api_paste.ini')
        self.CONF.set_override("api_paste_config", file_name)

        api_app = app.load_app()
        self.assertTrue(api_app.auth_uri.startswith('file'))

    def test_keystone_middleware_parse_conffile(self):
        pipeline_conf = self.path_get("etc/ceilometer/pipeline.yaml")
        api_conf = self.path_get('etc/ceilometer/api_paste.ini')
        content = "[DEFAULT]\n"\
                  "rpc_backend = fake\n"\
                  "pipeline_cfg_file = {0}\n"\
                  "api_paste_config = {1}\n"\
                  "[{2}]\n"\
                  "auth_protocol = file\n"\
                  "auth_version = v2.0\n".format(pipeline_conf,
                                                 api_conf,
                                                 acl.OPT_GROUP_NAME)

        tmpfile = fileutils.write_to_tempfile(content=content,
                                              prefix='ceilometer',
                                              suffix='.conf')
        service.prepare_service(['ceilometer-api',
                                 '--config-file=%s' % tmpfile])
        self.CONF.set_override('connection', "log://", group="database")
        api_app = app.load_app()
        self.assertTrue(api_app.auth_uri.startswith('file'))
        os.unlink(tmpfile)


class TestPecanApp(FunctionalTest):

    db_manager = tests_db.MongoDbManager()

    def test_pecan_extension_guessing_unset(self):
        # check Pecan does not assume .jpg is an extension
        response = self.app.get(self.PATH_PREFIX + '/meters/meter.jpg')
        self.assertEqual('application/json', response.content_type)


class TestApiMiddleware(FunctionalTest):

    db_manager = tests_db.MongoDbManager()

    no_lang_translated_error = 'No lang translated error'
    en_US_translated_error = 'en-US translated error'

    def _fake_translate(self, message, user_locale):
        if user_locale is None:
            return self.no_lang_translated_error
        else:
            return self.en_US_translated_error

    def test_json_parsable_error_middleware_404(self):
        response = self.get_json('/invalid_path',
                                 expect_errors=True,
                                 headers={"Accept":
                                          "application/json"}
                                 )
        self.assertEqual(404, response.status_int)
        self.assertEqual("application/json", response.content_type)
        self.assertTrue(response.json['error_message'])
        response = self.get_json('/invalid_path',
                                 expect_errors=True,
                                 headers={"Accept":
                                          "application/json,application/xml"}
                                 )
        self.assertEqual(404, response.status_int)
        self.assertEqual("application/json", response.content_type)
        self.assertTrue(response.json['error_message'])
        response = self.get_json('/invalid_path',
                                 expect_errors=True,
                                 headers={"Accept":
                                          "application/xml;q=0.8, \
                                          application/json"}
                                 )
        self.assertEqual(404, response.status_int)
        self.assertEqual("application/json", response.content_type)
        self.assertTrue(response.json['error_message'])
        response = self.get_json('/invalid_path',
                                 expect_errors=True
                                 )
        self.assertEqual(404, response.status_int)
        self.assertEqual("application/json", response.content_type)
        self.assertTrue(response.json['error_message'])
        response = self.get_json('/invalid_path',
                                 expect_errors=True,
                                 headers={"Accept":
                                          "text/html,*/*"}
                                 )
        self.assertEqual(404, response.status_int)
        self.assertEqual("application/json", response.content_type)
        self.assertTrue(response.json['error_message'])

    def test_json_parsable_error_middleware_translation_400(self):
        # Ensure translated messages get placed properly into json faults
        with mock.patch.object(gettextutils, 'translate',
                               side_effect=self._fake_translate):
            response = self.post_json('/alarms', params={'name': 'foobar',
                                                         'type': 'threshold'},
                                      expect_errors=True,
                                      headers={"Accept":
                                               "application/json"}
                                      )
        self.assertEqual(400, response.status_int)
        self.assertEqual("application/json", response.content_type)
        self.assertTrue(response.json['error_message'])
        self.assertEqual(self.no_lang_translated_error,
                         response.json['error_message']['faultstring'])

    def test_xml_parsable_error_middleware_404(self):
        response = self.get_json('/invalid_path',
                                 expect_errors=True,
                                 headers={"Accept":
                                          "application/xml,*/*"}
                                 )
        self.assertEqual(404, response.status_int)
        self.assertEqual("application/xml", response.content_type)
        self.assertEqual('error_message', response.xml.tag)
        response = self.get_json('/invalid_path',
                                 expect_errors=True,
                                 headers={"Accept":
                                          "application/json;q=0.8 \
                                          ,application/xml"}
                                 )
        self.assertEqual(404, response.status_int)
        self.assertEqual("application/xml", response.content_type)
        self.assertEqual('error_message', response.xml.tag)

    def test_xml_parsable_error_middleware_translation_400(self):
        # Ensure translated messages get placed properly into xml faults
        with mock.patch.object(gettextutils, 'translate',
                               side_effect=self._fake_translate):
            response = self.post_json('/alarms', params={'name': 'foobar',
                                                         'type': 'threshold'},
                                      expect_errors=True,
                                      headers={"Accept":
                                               "application/xml,*/*"}
                                      )
        self.assertEqual(400, response.status_int)
        self.assertEqual("application/xml", response.content_type)
        self.assertEqual('error_message', response.xml.tag)
        fault = response.xml.findall('./error/faultstring')
        for fault_string in fault:
            self.assertEqual(self.no_lang_translated_error, fault_string.text)

    def test_best_match_language(self):
        # Ensure that we are actually invoking language negotiation
        with mock.patch.object(gettextutils, 'translate',
                               side_effect=self._fake_translate):
            response = self.post_json('/alarms', params={'name': 'foobar',
                                                         'type': 'threshold'},
                                      expect_errors=True,
                                      headers={"Accept":
                                               "application/xml,*/*",
                                               "Accept-Language":
                                               "en-US"}
                                      )

        self.assertEqual(400, response.status_int)
        self.assertEqual("application/xml", response.content_type)
        self.assertEqual('error_message', response.xml.tag)
        fault = response.xml.findall('./error/faultstring')
        for fault_string in fault:
            self.assertEqual(self.en_US_translated_error, fault_string.text)

    def test_translated_then_untranslated_error(self):
        resp = self.get_json('/alarms/alarm-id-3', expect_errors=True)
        self.assertEqual(404, resp.status_code)
        self.assertEqual("Alarm alarm-id-3 Not Found",
                         json.loads(resp.body)['error_message']
                         ['faultstring'])

        with mock.patch('ceilometer.api.controllers.v2.EntityNotFound') \
                as CustomErrorClass:
            CustomErrorClass.return_value = wsme.exc.ClientSideError(
                "untranslated_error", status_code=404)
            resp = self.get_json('/alarms/alarm-id-5', expect_errors=True)

        self.assertEqual(404, resp.status_code)
        self.assertEqual("untranslated_error",
                         json.loads(resp.body)['error_message']
                         ['faultstring'])

########NEW FILE########
__FILENAME__ = test_capabilities
# -*- encoding: utf-8 -*-
#
# Copyright Ericsson AB 2014. All rights reserved
#
# Authors: Ildiko Vancsa <ildiko.vancsa@ericsson.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import testscenarios

from ceilometer.tests.api import v2 as tests_api
from ceilometer.tests import db as tests_db

load_tests = testscenarios.load_tests_apply_scenarios


class TestCapabilitiesController(tests_api.FunctionalTest,
                                 tests_db.MixinTestsWithBackendScenarios):

    def setUp(self):
            super(TestCapabilitiesController, self).setUp()
            self.url = '/capabilities'

    def test_capabilities(self):
        data = self.get_json(self.url)
        self.assertIsNotNone(data)
        self.assertNotEqual({}, data)

########NEW FILE########
__FILENAME__ = test_complex_query
# -*- encoding: utf-8 -*-
#
# Copyright Ericsson AB 2013. All rights reserved
#
# Authors: Ildiko Vancsa <ildiko.vancsa@ericsson.com>
#          Balazs Gibizer <balazs.gibizer@ericsson.com>
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
"""Test the methods related to complex query."""
import datetime
import fixtures
import jsonschema
import mock
import wsme

from ceilometer.api.controllers import v2 as api
from ceilometer.openstack.common import test
from ceilometer import storage as storage


class FakeComplexQuery(api.ValidatedComplexQuery):
    def __init__(self, db_model, additional_name_mapping=None, metadata=False):
        super(FakeComplexQuery, self).__init__(query=None,
                                               db_model=db_model,
                                               additional_name_mapping=
                                               additional_name_mapping or {},
                                               metadata_allowed=metadata)


sample_name_mapping = {"resource": "resource_id",
                       "meter": "counter_name",
                       "type": "counter_type",
                       "unit": "counter_unit",
                       "volume": "counter_volume"}


class TestComplexQuery(test.BaseTestCase):
    def setUp(self):
        super(TestComplexQuery, self).setUp()
        self.useFixture(fixtures.MonkeyPatch(
            'pecan.response', mock.MagicMock()))
        self.query = FakeComplexQuery(storage.models.Sample,
                                      sample_name_mapping,
                                      True)
        self.query_alarm = FakeComplexQuery(storage.models.Alarm)
        self.query_alarmchange = FakeComplexQuery(
            storage.models.AlarmChange)

    def test_replace_isotime_utc(self):
        filter_expr = {"=": {"timestamp": "2013-12-05T19:38:29Z"}}
        self.query._replace_isotime_with_datetime(filter_expr)
        self.assertEqual(datetime.datetime(2013, 12, 5, 19, 38, 29),
                         filter_expr["="]["timestamp"])

    def test_replace_isotime_timezone_removed(self):
        filter_expr = {"=": {"timestamp": "2013-12-05T20:38:29+01:00"}}
        self.query._replace_isotime_with_datetime(filter_expr)
        self.assertEqual(datetime.datetime(2013, 12, 5, 20, 38, 29),
                         filter_expr["="]["timestamp"])

    def test_replace_isotime_wrong_syntax(self):
        filter_expr = {"=": {"timestamp": "not a valid isotime string"}}
        self.assertRaises(wsme.exc.ClientSideError,
                          self.query._replace_isotime_with_datetime,
                          filter_expr)

    def test_replace_isotime_in_complex_filter(self):
        filter_expr = {"and": [{"=": {"timestamp": "2013-12-05T19:38:29Z"}},
                               {"=": {"timestamp": "2013-12-06T19:38:29Z"}}]}
        self.query._replace_isotime_with_datetime(filter_expr)
        self.assertEqual(datetime.datetime(2013, 12, 5, 19, 38, 29),
                         filter_expr["and"][0]["="]["timestamp"])
        self.assertEqual(datetime.datetime(2013, 12, 6, 19, 38, 29),
                         filter_expr["and"][1]["="]["timestamp"])

    def test_replace_isotime_in_complex_filter_with_unbalanced_tree(self):
        subfilter = {"and": [{"=": {"project_id": 42}},
                             {"=": {"timestamp": "2013-12-06T19:38:29Z"}}]}

        filter_expr = {"or": [{"=": {"timestamp": "2013-12-05T19:38:29Z"}},
                              subfilter]}

        self.query._replace_isotime_with_datetime(filter_expr)
        self.assertEqual(datetime.datetime(2013, 12, 5, 19, 38, 29),
                         filter_expr["or"][0]["="]["timestamp"])
        self.assertEqual(datetime.datetime(2013, 12, 6, 19, 38, 29),
                         filter_expr["or"][1]["and"][1]["="]["timestamp"])

    def test_convert_operator_to_lower_case(self):
        filter_expr = {"AND": [{"=": {"project_id": 42}},
                               {"=": {"project_id": 44}}]}
        self.query._convert_operator_to_lower_case(filter_expr)
        self.assertEqual("and", filter_expr.keys()[0])

        filter_expr = {"Or": [{"=": {"project_id": 43}},
                              {"anD": [{"=": {"project_id": 44}},
                                       {"=": {"project_id": 42}}]}]}
        self.query._convert_operator_to_lower_case(filter_expr)
        self.assertEqual("or", filter_expr.keys()[0])
        self.assertEqual("and", filter_expr["or"][1].keys()[0])

    def test_invalid_filter_misstyped_field_name_samples(self):
        filter = {"=": {"project_id11": 42}}
        self.assertRaises(jsonschema.ValidationError,
                          self.query._validate_filter,
                          filter)

    def test_invalid_filter_misstyped_field_name_alarms(self):
        filter = {"=": {"enabbled": True}}
        self.assertRaises(jsonschema.ValidationError,
                          self.query_alarm._validate_filter,
                          filter)

    def test_invalid_filter_misstyped_field_name_alarmchange(self):
        filter = {"=": {"tpe": "rule change"}}
        self.assertRaises(jsonschema.ValidationError,
                          self.query_alarmchange._validate_filter,
                          filter)

    def test_invalid_complex_filter_wrong_field_names(self):
        filter = {"and":
                  [{"=": {"non_existing_field": 42}},
                   {"=": {"project_id": 42}}]}
        self.assertRaises(jsonschema.ValidationError,
                          self.query._validate_filter,
                          filter)

        filter = {"and":
                  [{"=": {"project_id": 42}},
                   {"=": {"non_existing_field": 42}}]}
        self.assertRaises(jsonschema.ValidationError,
                          self.query_alarm._validate_filter,
                          filter)

        filter = {"and":
                  [{"=": {"project_id11": 42}},
                   {"=": {"project_id": 42}}]}
        self.assertRaises(jsonschema.ValidationError,
                          self.query_alarmchange._validate_filter,
                          filter)

        filter = {"or":
                  [{"=": {"non_existing_field": 42}},
                   {"and":
                    [{"=": {"project_id": 44}},
                     {"=": {"project_id": 42}}]}]}
        self.assertRaises(jsonschema.ValidationError,
                          self.query._validate_filter,
                          filter)

        filter = {"or":
                  [{"=": {"project_id": 43}},
                   {"and":
                    [{"=": {"project_id": 44}},
                     {"=": {"non_existing_field": 42}}]}]}
        self.assertRaises(jsonschema.ValidationError,
                          self.query_alarm._validate_filter,
                          filter)

    def test_convert_orderby(self):
        orderby = []
        self.query._convert_orderby_to_lower_case(orderby)
        self.assertEqual([], orderby)

        orderby = [{"project_id": "DESC"}]
        self.query._convert_orderby_to_lower_case(orderby)
        self.assertEqual([{"project_id": "desc"}], orderby)

        orderby = [{"project_id": "ASC"}, {"resource_id": "DESC"}]
        self.query._convert_orderby_to_lower_case(orderby)
        self.assertEqual([{"project_id": "asc"}, {"resource_id": "desc"}],
                         orderby)

    def test_validate_orderby_empty_direction(self):
        orderby = [{"project_id": ""}]
        self.assertRaises(jsonschema.ValidationError,
                          self.query._validate_orderby,
                          orderby)
        orderby = [{"project_id": "asc"}, {"resource_id": ""}]
        self.assertRaises(jsonschema.ValidationError,
                          self.query._validate_orderby,
                          orderby)

    def test_validate_orderby_wrong_order_string(self):
        orderby = [{"project_id": "not a valid order"}]
        self.assertRaises(jsonschema.ValidationError,
                          self.query._validate_orderby,
                          orderby)

    def test_validate_orderby_wrong_multiple_item_order_string(self):
        orderby = [{"project_id": "not a valid order"}, {"resource_id": "ASC"}]
        self.assertRaises(jsonschema.ValidationError,
                          self.query._validate_orderby,
                          orderby)

    def test_validate_orderby_empty_field_name(self):
        orderby = [{"": "ASC"}]
        self.assertRaises(jsonschema.ValidationError,
                          self.query._validate_orderby,
                          orderby)
        orderby = [{"project_id": "asc"}, {"": "desc"}]
        self.assertRaises(jsonschema.ValidationError,
                          self.query._validate_orderby,
                          orderby)

    def test_validate_orderby_wrong_field_name(self):
        orderby = [{"project_id11": "ASC"}]
        self.assertRaises(jsonschema.ValidationError,
                          self.query._validate_orderby,
                          orderby)

    def test_validate_orderby_wrong_field_name_multiple_item_orderby(self):
        orderby = [{"project_id": "asc"}, {"resource_id11": "ASC"}]
        self.assertRaises(jsonschema.ValidationError,
                          self.query._validate_orderby,
                          orderby)

    def test_validate_orderby_metadata_is_not_allowed(self):
        orderby = [{"metadata.display_name": "asc"}]
        self.assertRaises(jsonschema.ValidationError,
                          self.query._validate_orderby,
                          orderby)


class TestFilterSyntaxValidation(test.BaseTestCase):
    def setUp(self):
        super(TestFilterSyntaxValidation, self).setUp()
        self.query = FakeComplexQuery(storage.models.Sample,
                                      sample_name_mapping,
                                      True)

    def test_simple_operator(self):
        filter = {"=": {"project_id": "string_value"}}
        self.query._validate_filter(filter)

        filter = {"=>": {"project_id": "string_value"}}
        self.query._validate_filter(filter)

    def test_valid_value_types(self):
        filter = {"=": {"project_id": "string_value"}}
        self.query._validate_filter(filter)

        filter = {"=": {"project_id": 42}}
        self.query._validate_filter(filter)

        filter = {"=": {"project_id": 3.14}}
        self.query._validate_filter(filter)

        filter = {"=": {"project_id": True}}
        self.query._validate_filter(filter)

        filter = {"=": {"project_id": False}}
        self.query._validate_filter(filter)

    def test_invalid_simple_operator(self):
        filter = {"==": {"project_id": "string_value"}}
        self.assertRaises(jsonschema.ValidationError,
                          self.query._validate_filter,
                          filter)

        filter = {"": {"project_id": "string_value"}}
        self.assertRaises(jsonschema.ValidationError,
                          self.query._validate_filter,
                          filter)

    def test_more_than_one_operator_is_invalid(self):
        filter = {"=": {"project_id": "string_value"},
                  "<": {"": ""}}
        self.assertRaises(jsonschema.ValidationError,
                          self.query._validate_filter,
                          filter)

    def test_empty_expression_is_invalid(self):
        filter = {}
        self.assertRaises(jsonschema.ValidationError,
                          self.query._validate_filter,
                          filter)

    def test_invalid_field_name(self):
        filter = {"=": {"": "value"}}
        self.assertRaises(jsonschema.ValidationError,
                          self.query._validate_filter,
                          filter)

        filter = {"=": {" ": "value"}}
        self.assertRaises(jsonschema.ValidationError,
                          self.query._validate_filter,
                          filter)

        filter = {"=": {"\t": "value"}}
        self.assertRaises(jsonschema.ValidationError,
                          self.query._validate_filter,
                          filter)

    def test_more_than_one_field_is_invalid(self):
        filter = {"=": {"project_id": "value", "resource_id": "value"}}
        self.assertRaises(jsonschema.ValidationError,
                          self.query._validate_filter,
                          filter)

    def test_missing_field_after_simple_op_is_invalid(self):
        filter = {"=": {}}
        self.assertRaises(jsonschema.ValidationError,
                          self.query._validate_filter,
                          filter)

    def test_and_or(self):
        filter = {"and": [{"=": {"project_id": "string_value"}},
                          {"=": {"resource_id": "value"}}]}
        self.query._validate_filter(filter)

        filter = {"or": [{"and": [{"=": {"project_id": "string_value"}},
                                  {"=": {"resource_id": "value"}}]},
                         {"=": {"counter_name": "value"}}]}
        self.query._validate_filter(filter)

        filter = {"or": [{"and": [{"=": {"project_id": "string_value"}},
                                  {"=": {"resource_id": "value"}},
                                  {"<": {"counter_name": 42}}]},
                         {"=": {"counter_name": "value"}}]}
        self.query._validate_filter(filter)

    def test_complex_operator_with_in(self):
        filter = {"and": [{"<": {"counter_volume": 42}},
                          {">=": {"counter_volume": 36}},
                          {"in": {"project_id": ["project_id1",
                                                 "project_id2",
                                                 "project_id3"]}}]}
        self.query._validate_filter(filter)

    def test_invalid_complex_operator(self):
        filter = {"xor": [{"=": {"project_id": "string_value"}},
                          {"=": {"resource_id": "value"}}]}
        self.assertRaises(jsonschema.ValidationError,
                          self.query._validate_filter,
                          filter)

    def test_and_or_with_one_child_is_invalid(self):
        filter = {"or": [{"=": {"project_id": "string_value"}}]}
        self.assertRaises(jsonschema.ValidationError,
                          self.query._validate_filter,
                          filter)

    def test_complex_operator_with_zero_child_is_invalid(self):
        filter = {"or": []}
        self.assertRaises(jsonschema.ValidationError,
                          self.query._validate_filter,
                          filter)

    def test_more_than_one_complex_operator_is_invalid(self):
        filter = {"and": [{"=": {"project_id": "string_value"}},
                          {"=": {"resource_id": "value"}}],
                  "or": [{"=": {"project_id": "string_value"}},
                         {"=": {"resource_id": "value"}}]}
        self.assertRaises(jsonschema.ValidationError,
                          self.query._validate_filter,
                          filter)

    def test_not(self):
        filter = {"not": {"=": {"project_id": "value"}}}
        self.query._validate_filter(filter)

        filter = {
            "not":
            {"or":
             [{"and":
               [{"=": {"project_id": "string_value"}},
                {"=": {"resource_id": "value"}},
                {"<": {"counter_name": 42}}]},
              {"=": {"counter_name": "value"}}]}}
        self.query._validate_filter(filter)

    def test_not_with_zero_child_is_invalid(self):
        filter = {"not": {}}
        self.assertRaises(jsonschema.ValidationError,
                          self.query._validate_filter,
                          filter)

    def test_not_with_more_than_one_child_is_invalid(self):
        filter = {"not": {"=": {"project_id": "value"},
                          "!=": {"resource_id": "value"}}}
        self.assertRaises(jsonschema.ValidationError,
                          self.query._validate_filter,
                          filter)

    def test_empty_in_query_not_passing(self):
        filter = {"in": {"resource_id": []}}
        self.assertRaises(jsonschema.ValidationError,
                          self.query._validate_filter,
                          filter)

########NEW FILE########
__FILENAME__ = test_complex_query_scenarios
# -*- encoding: utf-8 -*-
#
# Copyright Ericsson AB 2013. All rights reserved
#
# Authors: Ildiko Vancsa <ildiko.vancsa@ericsson.com>
#          Balazs Gibizer <balazs.gibizer@ericsson.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests complex queries for samples
"""

import datetime
import logging

from ceilometer.openstack.common import timeutils
from ceilometer.publisher import utils
from ceilometer import sample
from ceilometer.storage import models
from ceilometer.tests.api import v2 as tests_api
from ceilometer.tests import db as tests_db

LOG = logging.getLogger(__name__)

admin_header = {"X-Roles": "admin",
                "X-Project-Id":
                "project-id1"}
non_admin_header = {"X-Roles": "Member",
                    "X-Project-Id":
                    "project-id1"}


class TestQueryMetersController(tests_api.FunctionalTest,
                                tests_db.MixinTestsWithBackendScenarios):

    def setUp(self):
        super(TestQueryMetersController, self).setUp()
        self.url = '/query/samples'

        for cnt in [
            sample.Sample('meter.test',
                          'cumulative',
                          '',
                          1,
                          'user-id1',
                          'project-id1',
                          'resource-id1',
                          timestamp=datetime.datetime(2012, 7, 2, 10, 40),
                          resource_metadata={'display_name': 'test-server1',
                                             'tag': 'self.sample',
                                             'size': 456,
                                             'util': 0.25,
                                             'is_public': True},
                          source='test_source'),
            sample.Sample('meter.test',
                          'cumulative',
                          '',
                          2,
                          'user-id2',
                          'project-id2',
                          'resource-id2',
                          timestamp=datetime.datetime(2012, 7, 2, 10, 41),
                          resource_metadata={'display_name': 'test-server2',
                                             'tag': 'self.sample',
                                             'size': 123,
                                             'util': 0.75,
                                             'is_public': True},
                          source='test_source'),
            sample.Sample('meter.test',
                          'cumulative',
                          '',
                          3,
                          'user-id3',
                          'project-id3',
                          'resource-id3',
                          timestamp=datetime.datetime(2012, 7, 2, 10, 42),
                          resource_metadata={'display_name': 'test-server3',
                                             'tag': 'self.sample',
                                             'size': 789,
                                             'util': 0.95,
                                             'is_public': True},
                          source='test_source')]:

            msg = utils.meter_message_from_counter(
                cnt,
                self.CONF.publisher.metering_secret)
            self.conn.record_metering_data(msg)

    def test_query_fields_are_optional(self):
        data = self.post_json(self.url, params={})
        self.assertEqual(3, len(data.json))

    def test_query_with_isotime(self):
        date_time = datetime.datetime(2012, 7, 2, 10, 41)
        isotime = date_time.isoformat()

        data = self.post_json(self.url,
                              params={"filter":
                                      '{">=": {"timestamp": "'
                                      + isotime + '"}}'})

        self.assertEqual(2, len(data.json))
        for sample in data.json:
            result_time = timeutils.parse_isotime(sample['timestamp'])
            result_time = result_time.replace(tzinfo=None)
            self.assertTrue(result_time >= date_time)

    def test_non_admin_tenant_sees_only_its_own_project(self):
        data = self.post_json(self.url,
                              params={},
                              headers=non_admin_header)
        for sample in data.json:
            self.assertEqual("project-id1", sample['project_id'])

    def test_non_admin_tenant_cannot_query_others_project(self):
        data = self.post_json(self.url,
                              params={"filter":
                                      '{"=": {"project_id": "project-id2"}}'},
                              expect_errors=True,
                              headers=non_admin_header)

        self.assertEqual(401, data.status_int)
        self.assertIn("Not Authorized to access project project-id2",
                      data.body)

    def test_non_admin_tenant_can_explicitly_filter_for_own_project(self):
        data = self.post_json(self.url,
                              params={"filter":
                                      '{"=": {"project_id": "project-id1"}}'},
                              headers=non_admin_header)

        for sample in data.json:
            self.assertEqual("project-id1", sample['project_id'])

    def test_admin_tenant_sees_every_project(self):
        data = self.post_json(self.url,
                              params={},
                              headers=admin_header)

        self.assertEqual(3, len(data.json))
        for sample in data.json:
            self.assertIn(sample['project_id'],
                          (["project-id1", "project-id2", "project-id3"]))

    def test_admin_tenant_sees_every_project_with_complex_filter(self):
        filter = ('{"OR": ' +
                  '[{"=": {"project_id": "project-id1"}}, ' +
                  '{"=": {"project_id": "project-id2"}}]}')
        data = self.post_json(self.url,
                              params={"filter": filter},
                              headers=admin_header)

        self.assertEqual(2, len(data.json))
        for sample in data.json:
            self.assertIn(sample['project_id'],
                          (["project-id1", "project-id2"]))

    def test_admin_tenant_sees_every_project_with_in_filter(self):
        filter = ('{"In": ' +
                  '{"project_id": ["project-id1", "project-id2"]}}')
        data = self.post_json(self.url,
                              params={"filter": filter},
                              headers=admin_header)

        self.assertEqual(2, len(data.json))
        for sample in data.json:
            self.assertIn(sample['project_id'],
                          (["project-id1", "project-id2"]))

    def test_admin_tenant_can_query_any_project(self):
        data = self.post_json(self.url,
                              params={"filter":
                                      '{"=": {"project_id": "project-id2"}}'},
                              headers=admin_header)

        self.assertEqual(1, len(data.json))
        for sample in data.json:
            self.assertIn(sample['project_id'], set(["project-id2"]))

    def test_query_with_orderby(self):
        data = self.post_json(self.url,
                              params={"orderby": '[{"project_id": "DESC"}]'})

        self.assertEqual(3, len(data.json))
        self.assertEqual(["project-id3", "project-id2", "project-id1"],
                         [s["project_id"] for s in data.json])

    def test_query_with_field_name_project(self):
        data = self.post_json(self.url,
                              params={"filter":
                                      '{"=": {"project": "project-id2"}}'})

        self.assertEqual(1, len(data.json))
        for sample in data.json:
            self.assertIn(sample['project_id'], set(["project-id2"]))

    def test_query_with_field_name_resource(self):
        data = self.post_json(self.url,
                              params={"filter":
                                      '{"=": {"resource": "resource-id2"}}'})

        self.assertEqual(1, len(data.json))
        for sample in data.json:
            self.assertIn(sample['resource_id'], set(["resource-id2"]))

    def test_query_with_field_name_user(self):
        data = self.post_json(self.url,
                              params={"filter":
                                      '{"=": {"user": "user-id2"}}'})

        self.assertEqual(1, len(data.json))
        for sample in data.json:
            self.assertIn(sample['user_id'], set(["user-id2"]))

    def test_query_with_field_name_meter(self):
        data = self.post_json(self.url,
                              params={"filter":
                                      '{"=": {"meter": "meter.test"}}'})

        self.assertEqual(3, len(data.json))
        for sample in data.json:
            self.assertIn(sample['meter'], set(["meter.test"]))

    def test_query_with_lower_and_upper_case_orderby(self):
        data = self.post_json(self.url,
                              params={"orderby": '[{"project_id": "DeSc"}]'})

        self.assertEqual(3, len(data.json))
        self.assertEqual(["project-id3", "project-id2", "project-id1"],
                         [s["project_id"] for s in data.json])

    def test_query_with_user_field_name_orderby(self):
        data = self.post_json(self.url,
                              params={"orderby": '[{"user": "aSc"}]'})

        self.assertEqual(3, len(data.json))
        self.assertEqual(["user-id1", "user-id2", "user-id3"],
                         [s["user_id"] for s in data.json])

    def test_query_with_volume_field_name_orderby(self):
        data = self.post_json(self.url,
                              params={"orderby": '[{"volume": "deSc"}]'})

        self.assertEqual(3, len(data.json))
        self.assertEqual([3, 2, 1],
                         [s["volume"] for s in data.json])

    def test_query_with_missing_order_in_orderby(self):
        data = self.post_json(self.url,
                              params={"orderby": '[{"project_id": ""}]'},
                              expect_errors=True)

        self.assertEqual(500, data.status_int)

    def test_filter_with_metadata(self):
        data = self.post_json(self.url,
                              params={"filter":
                                      '{">=": {"metadata.util": 0.5}}'})

        self.assertEqual(2, len(data.json))
        for sample in data.json:
            self.assertTrue(sample["metadata"]["util"] >= 0.5)

    def test_filter_with_negation(self):
        filter_expr = '{"not": {">=": {"metadata.util": 0.5}}}'
        data = self.post_json(self.url,
                              params={"filter": filter_expr})

        self.assertEqual(1, len(data.json))
        for sample in data.json:
            self.assertTrue(float(sample["metadata"]["util"]) < 0.5)

    def test_limit_should_be_positive(self):
        data = self.post_json(self.url,
                              params={"limit": 0},
                              expect_errors=True)

        self.assertEqual(400, data.status_int)
        self.assertIn("Limit should be positive", data.body)


class TestQueryAlarmsController(tests_api.FunctionalTest,
                                tests_db.MixinTestsWithBackendScenarios):

    def setUp(self):
        super(TestQueryAlarmsController, self).setUp()
        self.alarm_url = '/query/alarms'

        for state in ['ok', 'alarm', 'insufficient data']:
            for date in [datetime.datetime(2013, 1, 1),
                         datetime.datetime(2013, 2, 2)]:
                for id in [1, 2]:
                    alarm_id = "-".join([state, date.isoformat(), str(id)])
                    project_id = "project-id%d" % id
                    alarm = models.Alarm(name=alarm_id,
                                         type='threshold',
                                         enabled=True,
                                         alarm_id=alarm_id,
                                         description='a',
                                         state=state,
                                         state_timestamp=date,
                                         timestamp=date,
                                         ok_actions=[],
                                         insufficient_data_actions=[],
                                         alarm_actions=[],
                                         repeat_actions=True,
                                         user_id="user-id%d" % id,
                                         project_id=project_id,
                                         time_constraints=[],
                                         rule=dict(comparison_operator='gt',
                                                   threshold=2.0,
                                                   statistic='avg',
                                                   evaluation_periods=60,
                                                   period=1,
                                                   meter_name='meter.test',
                                                   query=[{'field':
                                                           'project_id',
                                                           'op': 'eq',
                                                           'value':
                                                           project_id}]))
                    self.conn.update_alarm(alarm)

    def test_query_all(self):
        data = self.post_json(self.alarm_url,
                              params={})

        self.assertEqual(12, len(data.json))

    def test_filter_with_isotime_timestamp(self):
        date_time = datetime.datetime(2013, 1, 1)
        isotime = date_time.isoformat()

        data = self.post_json(self.alarm_url,
                              params={"filter":
                                      '{">": {"timestamp": "'
                                      + isotime + '"}}'})

        self.assertEqual(6, len(data.json))
        for alarm in data.json:
            result_time = timeutils.parse_isotime(alarm['timestamp'])
            result_time = result_time.replace(tzinfo=None)
            self.assertTrue(result_time > date_time)

    def test_filter_with_isotime_state_timestamp(self):
        date_time = datetime.datetime(2013, 1, 1)
        isotime = date_time.isoformat()

        data = self.post_json(self.alarm_url,
                              params={"filter":
                                      '{">": {"state_timestamp": "'
                                      + isotime + '"}}'})

        self.assertEqual(6, len(data.json))
        for alarm in data.json:
            result_time = timeutils.parse_isotime(alarm['state_timestamp'])
            result_time = result_time.replace(tzinfo=None)
            self.assertTrue(result_time > date_time)

    def test_non_admin_tenant_sees_only_its_own_project(self):
        data = self.post_json(self.alarm_url,
                              params={},
                              headers=non_admin_header)
        for alarm in data.json:
            self.assertEqual("project-id1", alarm['project_id'])

    def test_non_admin_tenant_cannot_query_others_project(self):
        data = self.post_json(self.alarm_url,
                              params={"filter":
                                      '{"=": {"project_id": "project-id2"}}'},
                              expect_errors=True,
                              headers=non_admin_header)

        self.assertEqual(401, data.status_int)
        self.assertIn("Not Authorized to access project project-id2",
                      data.body)

    def test_non_admin_tenant_can_explicitly_filter_for_own_project(self):
        data = self.post_json(self.alarm_url,
                              params={"filter":
                                      '{"=": {"project_id": "project-id1"}}'},
                              headers=non_admin_header)

        for alarm in data.json:
            self.assertEqual("project-id1", alarm['project_id'])

    def test_admin_tenant_sees_every_project(self):
        data = self.post_json(self.alarm_url,
                              params={},
                              headers=admin_header)

        self.assertEqual(12, len(data.json))
        for alarm in data.json:
            self.assertIn(alarm['project_id'],
                          (["project-id1", "project-id2"]))

    def test_admin_tenant_can_query_any_project(self):
        data = self.post_json(self.alarm_url,
                              params={"filter":
                                      '{"=": {"project_id": "project-id2"}}'},
                              headers=admin_header)

        self.assertEqual(6, len(data.json))
        for alarm in data.json:
            self.assertIn(alarm['project_id'], set(["project-id2"]))

    def test_query_with_field_project(self):
        data = self.post_json(self.alarm_url,
                              params={"filter":
                                      '{"=": {"project": "project-id2"}}'})

        self.assertEqual(6, len(data.json))
        for sample in data.json:
            self.assertIn(sample['project_id'], set(["project-id2"]))

    def test_query_with_field_user_in_orderby(self):
        data = self.post_json(self.alarm_url,
                              params={"filter": '{"=": {"state": "alarm"}}',
                                      "orderby": '[{"user": "DESC"}]'})

        self.assertEqual(4, len(data.json))
        self.assertEqual(["user-id2", "user-id2", "user-id1", "user-id1"],
                         [s["user_id"] for s in data.json])

    def test_query_with_filter_orderby_and_limit(self):
        orderby = '[{"state_timestamp": "DESC"}]'
        data = self.post_json(self.alarm_url,
                              params={"filter": '{"=": {"state": "alarm"}}',
                                      "orderby": orderby,
                                      "limit": 3})

        self.assertEqual(3, len(data.json))
        self.assertEqual(["2013-02-02T00:00:00",
                          "2013-02-02T00:00:00",
                          "2013-01-01T00:00:00"],
                         [a["state_timestamp"] for a in data.json])
        for alarm in data.json:
            self.assertEqual("alarm", alarm["state"])

    def test_limit_should_be_positive(self):
        data = self.post_json(self.alarm_url,
                              params={"limit": 0},
                              expect_errors=True)

        self.assertEqual(400, data.status_int)
        self.assertIn("Limit should be positive", data.body)


class TestQueryAlarmsHistoryController(
        tests_api.FunctionalTest, tests_db.MixinTestsWithBackendScenarios):

    def setUp(self):
        super(TestQueryAlarmsHistoryController, self).setUp()
        self.url = '/query/alarms/history'
        for id in [1, 2]:
            for type in ["creation", "state transition"]:
                for date in [datetime.datetime(2013, 1, 1),
                             datetime.datetime(2013, 2, 2)]:
                    event_id = "-".join([str(id), type, date.isoformat()])
                    alarm_change = {"event_id": event_id,
                                    "alarm_id": "alarm-id%d" % id,
                                    "type": type,
                                    "detail": "",
                                    "user_id": "user-id%d" % id,
                                    "project_id": "project-id%d" % id,
                                    "on_behalf_of": "project-id%d" % id,
                                    "timestamp": date}

                    self.conn.record_alarm_change(alarm_change)

    def test_query_all(self):
        data = self.post_json(self.url,
                              params={})

        self.assertEqual(8, len(data.json))

    def test_filter_with_isotime(self):
        date_time = datetime.datetime(2013, 1, 1)
        isotime = date_time.isoformat()

        data = self.post_json(self.url,
                              params={"filter":
                                      '{">": {"timestamp":"'
                                      + isotime + '"}}'})

        self.assertEqual(4, len(data.json))
        for history in data.json:
            result_time = timeutils.parse_isotime(history['timestamp'])
            result_time = result_time.replace(tzinfo=None)
            self.assertTrue(result_time > date_time)

    def test_non_admin_tenant_sees_only_its_own_project(self):
        data = self.post_json(self.url,
                              params={},
                              headers=non_admin_header)
        for history in data.json:
            self.assertEqual("project-id1", history['on_behalf_of'])

    def test_non_admin_tenant_cannot_query_others_project(self):
        data = self.post_json(self.url,
                              params={"filter":
                                      '{"=": {"on_behalf_of":'
                                      + ' "project-id2"}}'},
                              expect_errors=True,
                              headers=non_admin_header)

        self.assertEqual(401, data.status_int)
        self.assertIn("Not Authorized to access project project-id2",
                      data.body)

    def test_non_admin_tenant_can_explicitly_filter_for_own_project(self):
        data = self.post_json(self.url,
                              params={"filter":
                                      '{"=": {"on_behalf_of":'
                                      + ' "project-id1"}}'},
                              headers=non_admin_header)

        for history in data.json:
            self.assertEqual("project-id1", history['on_behalf_of'])

    def test_admin_tenant_sees_every_project(self):
        data = self.post_json(self.url,
                              params={},
                              headers=admin_header)

        self.assertEqual(8, len(data.json))
        for history in data.json:
            self.assertIn(history['on_behalf_of'],
                          (["project-id1", "project-id2"]))

    def test_query_with_filter_for_project_orderby_with_user(self):
        data = self.post_json(self.url,
                              params={"filter":
                                      '{"=": {"project": "project-id1"}}',
                                      "orderby": '[{"user": "DESC"}]',
                                      "limit": 3})

        self.assertEqual(3, len(data.json))
        self.assertEqual(["user-id1",
                          "user-id1",
                          "user-id1"],
                         [h["user_id"] for h in data.json])
        for history in data.json:
            self.assertEqual("project-id1", history['project_id'])

    def test_query_with_filter_orderby_and_limit(self):
        data = self.post_json(self.url,
                              params={"filter": '{"=": {"type": "creation"}}',
                                      "orderby": '[{"timestamp": "DESC"}]',
                                      "limit": 3})

        self.assertEqual(3, len(data.json))
        self.assertEqual(["2013-02-02T00:00:00",
                          "2013-02-02T00:00:00",
                          "2013-01-01T00:00:00"],
                         [h["timestamp"] for h in data.json])
        for history in data.json:
            self.assertEqual("creation", history['type'])

    def test_limit_should_be_positive(self):
        data = self.post_json(self.url,
                              params={"limit": 0},
                              expect_errors=True)

        self.assertEqual(400, data.status_int)
        self.assertIn("Limit should be positive", data.body)

########NEW FILE########
__FILENAME__ = test_compute_duration_by_resource_scenarios
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Test listing raw events.
"""

import datetime
import logging

import mock

from ceilometer.openstack.common import timeutils
from ceilometer.storage import models
from ceilometer.tests.api.v2 import FunctionalTest
from ceilometer.tests import db as tests_db

LOG = logging.getLogger(__name__)


class TestComputeDurationByResource(FunctionalTest,
                                    tests_db.MixinTestsWithBackendScenarios):

    def setUp(self):
        super(TestComputeDurationByResource, self).setUp()
        # Create events relative to the range and pretend
        # that the intervening events exist.

        self.early1 = datetime.datetime(2012, 8, 27, 7, 0)
        self.early2 = datetime.datetime(2012, 8, 27, 17, 0)

        self.start = datetime.datetime(2012, 8, 28, 0, 0)

        self.middle1 = datetime.datetime(2012, 8, 28, 8, 0)
        self.middle2 = datetime.datetime(2012, 8, 28, 18, 0)

        self.end = datetime.datetime(2012, 8, 28, 23, 59)

        self.late1 = datetime.datetime(2012, 8, 29, 9, 0)
        self.late2 = datetime.datetime(2012, 8, 29, 19, 0)

    def _patch_get_interval(self, start, end):
        def get_interval(event_filter, period, groupby, aggregate):
            self.assertIsNotNone(event_filter.start)
            self.assertIsNotNone(event_filter.end)
            if (event_filter.start > end or event_filter.end < start):
                return []
            duration_start = max(event_filter.start, start)
            duration_end = min(event_filter.end, end)
            duration = timeutils.delta_seconds(duration_start, duration_end)
            return [
                models.Statistics(
                    unit='',
                    min=0,
                    max=0,
                    avg=0,
                    sum=0,
                    count=0,
                    period=None,
                    period_start=None,
                    period_end=None,
                    duration=duration,
                    duration_start=duration_start,
                    duration_end=duration_end,
                    groupby=None,
                )
            ]
        return mock.patch.object(type(self.conn), 'get_meter_statistics',
                                 side_effect=get_interval)

    def _invoke_api(self):
        return self.get_json('/meters/instance:m1.tiny/statistics',
                             q=[{'field': 'timestamp',
                                 'op': 'ge',
                                 'value': self.start.isoformat()},
                                {'field': 'timestamp',
                                 'op': 'le',
                                 'value': self.end.isoformat()},
                                {'field': 'search_offset',
                                 'value': 10}])

    def test_before_range(self):
        with self._patch_get_interval(self.early1, self.early2):
            data = self._invoke_api()
        self.assertEqual([], data)

    def _assert_times_match(self, actual, expected):
        if actual:
            actual = timeutils.parse_isotime(actual)
        actual = actual.replace(tzinfo=None)
        self.assertEqual(expected, actual)

    def test_overlap_range_start(self):
        with self._patch_get_interval(self.early1, self.middle1):
            data = self._invoke_api()
        self._assert_times_match(data[0]['duration_start'], self.start)
        self._assert_times_match(data[0]['duration_end'], self.middle1)
        self.assertEqual(8 * 60 * 60, data[0]['duration'])

    def test_within_range(self):
        with self._patch_get_interval(self.middle1, self.middle2):
            data = self._invoke_api()
        self._assert_times_match(data[0]['duration_start'], self.middle1)
        self._assert_times_match(data[0]['duration_end'], self.middle2)
        self.assertEqual(10 * 60 * 60, data[0]['duration'])

    def test_within_range_zero_duration(self):
        with self._patch_get_interval(self.middle1, self.middle1):
            data = self._invoke_api()
        self._assert_times_match(data[0]['duration_start'], self.middle1)
        self._assert_times_match(data[0]['duration_end'], self.middle1)
        self.assertEqual(0, data[0]['duration'])

    def test_overlap_range_end(self):
        with self._patch_get_interval(self.middle2, self.late1):
            data = self._invoke_api()
        self._assert_times_match(data[0]['duration_start'], self.middle2)
        self._assert_times_match(data[0]['duration_end'], self.end)
        self.assertEqual(((6 * 60) - 1) * 60, data[0]['duration'])

    def test_after_range(self):
        with self._patch_get_interval(self.late1, self.late2):
            data = self._invoke_api()
        self.assertEqual([], data)

    def test_without_end_timestamp(self):
        statistics = [
            models.Statistics(
                unit=None,
                count=0,
                min=None,
                max=None,
                avg=None,
                duration=None,
                duration_start=self.late1,
                duration_end=self.late2,
                sum=0,
                period=None,
                period_start=None,
                period_end=None,
                groupby=None,
            )
        ]
        with mock.patch.object(type(self.conn), 'get_meter_statistics',
                               return_value=statistics):
            data = self.get_json('/meters/instance:m1.tiny/statistics',
                                 q=[{'field': 'timestamp',
                                     'op': 'ge',
                                     'value': self.late1.isoformat()},
                                    {'field': 'resource_id',
                                     'value': 'resource-id'},
                                    {'field': 'search_offset',
                                     'value': 10}])
        self._assert_times_match(data[0]['duration_start'], self.late1)
        self._assert_times_match(data[0]['duration_end'], self.late2)

    def test_without_start_timestamp(self):
        statistics = [
            models.Statistics(
                unit=None,
                count=0,
                min=None,
                max=None,
                avg=None,
                duration=None,
                duration_start=self.early1,
                duration_end=self.early2,
                sum=0,
                period=None,
                period_start=None,
                period_end=None,
                groupby=None,
            )
        ]

        with mock.patch.object(type(self.conn), 'get_meter_statistics',
                               return_value=statistics):
            data = self.get_json('/meters/instance:m1.tiny/statistics',
                                 q=[{'field': 'timestamp',
                                     'op': 'le',
                                     'value': self.early2.isoformat()},
                                    {'field': 'resource_id',
                                     'value': 'resource-id'},
                                    {'field': 'search_offset',
                                     'value': 10}])
        self._assert_times_match(data[0]['duration_start'], self.early1)
        self._assert_times_match(data[0]['duration_end'], self.early2)

########NEW FILE########
__FILENAME__ = test_event_scenarios
# -*- encoding: utf-8 -*-
#
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Test event, event_type and trait retrieval."""

import datetime

from ceilometer.openstack.common import timeutils
from ceilometer.storage import models
from ceilometer.tests.api.v2 import FunctionalTest
from ceilometer.tests import db as tests_db

headers = {"X-Roles": "admin"}


class EventTestBase(FunctionalTest,
                    tests_db.MixinTestsWithBackendScenarios):

    def setUp(self):
        super(EventTestBase, self).setUp()
        self._generate_models()

    def _generate_models(self):
        event_models = []
        base = 0
        self.trait_time = datetime.datetime(2013, 12, 31, 5, 0)
        for event_type in ['Foo', 'Bar', 'Zoo']:
            trait_models = \
                [models.Trait(name, type, value)
                    for name, type, value in [
                        ('trait_A', models.Trait.TEXT_TYPE,
                            "my_%s_text" % event_type),
                        ('trait_B', models.Trait.INT_TYPE,
                            base + 1),
                        ('trait_C', models.Trait.FLOAT_TYPE,
                            float(base) + 0.123456),
                        ('trait_D', models.Trait.DATETIME_TYPE,
                            self.trait_time)]]

            # Message ID for test will be 'base'. So, message ID for the first
            # event will be '0', the second '100', and so on.
            event_models.append(
                models.Event(message_id=str(base),
                             event_type=event_type,
                             generated=self.trait_time,
                             traits=trait_models))
            base += 100

        self.conn.record_events(event_models)


class TestEventTypeAPI(EventTestBase):

    PATH = '/event_types'

    def test_event_types(self):
        data = self.get_json(self.PATH, headers=headers)
        for event_type in ['Foo', 'Bar', 'Zoo']:
            self.assertTrue(event_type in data)


class TestTraitAPI(EventTestBase):

    PATH = '/event_types/%s/traits'

    def test_get_traits_for_event(self):
        path = self.PATH % "Foo"
        data = self.get_json(path, headers=headers)

        self.assertEqual(4, len(data))

    def test_get_event_invalid_path(self):
        data = self.get_json('/event_types/trait_A/', headers=headers,
                             expect_errors=True)
        self.assertEqual(404, data.status_int)

    def test_get_traits_for_non_existent_event(self):
        path = self.PATH % "NO_SUCH_EVENT_TYPE"
        data = self.get_json(path, headers=headers)

        self.assertEqual([], data)

    def test_get_trait_data_for_event(self):
        path = (self.PATH % "Foo") + "/trait_A"
        data = self.get_json(path, headers=headers)

        self.assertEqual(1, len(data))

        trait = data[0]
        self.assertEqual("trait_A", trait['name'])

    def test_get_trait_data_for_non_existent_event(self):
        path = (self.PATH % "NO_SUCH_EVENT") + "/trait_A"
        data = self.get_json(path, headers=headers)

        self.assertEqual([], data)

    def test_get_trait_data_for_non_existent_trait(self):
        path = (self.PATH % "Foo") + "/no_such_trait"
        data = self.get_json(path, headers=headers)

        self.assertEqual([], data)


class TestEventAPI(EventTestBase):

    PATH = '/events'

    def test_get_events(self):
        data = self.get_json(self.PATH, headers=headers)
        self.assertEqual(3, len(data))
        # We expect to get native UTC generated time back
        expected_generated = timeutils.strtime(
            at=timeutils.normalize_time(self.trait_time),
            fmt=timeutils._ISO8601_TIME_FORMAT)
        for event in data:
            self.assertTrue(event['event_type'] in ['Foo', 'Bar', 'Zoo'])
            self.assertEqual(4, len(event['traits']))
            self.assertEqual(expected_generated, event['generated'])
            for trait_name in ['trait_A', 'trait_B',
                               'trait_C', 'trait_D']:
                self.assertTrue(trait_name in map(lambda x: x['name'],
                                                  event['traits']))

    def test_get_event_by_message_id(self):
        event = self.get_json(self.PATH + "/100", headers=headers)
        expected_traits = [{'name': 'trait_A',
                            'type': 'string',
                            'value': 'my_Bar_text'},
                           {'name': 'trait_B',
                            'type': 'integer',
                            'value': '101'},
                           {'name': 'trait_C',
                            'type': 'float',
                            'value': '100.123456'},
                           {'name': 'trait_D',
                            'type': 'datetime',
                            'value': '2013-12-31T05:00:00'}]
        self.assertEqual('100', event['message_id'])
        self.assertEqual('Bar', event['event_type'])
        self.assertEqual('2013-12-31T05:00:00', event['generated'])
        self.assertEqual(expected_traits, event['traits'])

    def test_get_event_by_message_id_no_such_id(self):
        data = self.get_json(self.PATH + "/DNE", headers=headers,
                             expect_errors=True)
        self.assertEqual(404, data.status_int)

    def test_get_events_filter_event_type(self):
        data = self.get_json(self.PATH, headers=headers,
                             q=[{'field': 'event_type',
                                 'value': 'Foo'}])
        self.assertEqual(1, len(data))

    def test_get_events_filter_text_trait(self):
        data = self.get_json(self.PATH, headers=headers,
                             q=[{'field': 'trait_A',
                                 'value': 'my_Foo_text',
                                 'type': 'string'}])
        self.assertEqual(1, len(data))
        self.assertEqual('Foo', data[0]['event_type'])

    def test_get_events_filter_int_trait(self):
        data = self.get_json(self.PATH, headers=headers,
                             q=[{'field': 'trait_B',
                                 'value': '101',
                                 'type': 'integer'}])
        self.assertEqual(1, len(data))
        self.assertEqual('Bar', data[0]['event_type'])

        traits = filter(lambda x: x['name'] == 'trait_B', data[0]['traits'])
        self.assertEqual(1, len(traits))
        self.assertEqual('integer', traits[0]['type'])
        self.assertEqual('101', traits[0]['value'])

    def test_get_events_filter_float_trait(self):
        data = self.get_json(self.PATH, headers=headers,
                             q=[{'field': 'trait_C',
                                 'value': '200.123456',
                                 'type': 'float'}])
        self.assertEqual(1, len(data))
        self.assertEqual('Zoo', data[0]['event_type'])

        traits = filter(lambda x: x['name'] == 'trait_C', data[0]['traits'])
        self.assertEqual(1, len(traits))
        self.assertEqual('float', traits[0]['type'])
        self.assertEqual('200.123456', traits[0]['value'])

    def test_get_events_filter_datetime_trait(self):
        data = self.get_json(self.PATH, headers=headers,
                             q=[{'field': 'trait_D',
                                 'value': self.trait_time.isoformat(),
                                 'type': 'datetime'}])
        self.assertEqual(3, len(data))
        traits = filter(lambda x: x['name'] == 'trait_D', data[0]['traits'])
        self.assertEqual(1, len(traits))
        self.assertEqual('datetime', traits[0]['type'])
        self.assertEqual(self.trait_time.isoformat(), traits[0]['value'])

    def test_get_events_multiple_filters(self):
        data = self.get_json(self.PATH, headers=headers,
                             q=[{'field': 'trait_B',
                                 'value': '1',
                                 'type': 'integer'},
                                {'field': 'trait_A',
                                 'value': 'my_Foo_text',
                                 'type': 'string'}])
        self.assertEqual(1, len(data))
        self.assertEqual('Foo', data[0]['event_type'])

    def test_get_events_multiple_filters_no_matches(self):
        data = self.get_json(self.PATH, headers=headers,
                             q=[{'field': 'trait_B',
                                 'value': '101',
                                 'type': 'integer'},
                                {'field': 'trait_A',
                                 'value': 'my_Foo_text',
                                 'type': 'string'}])

        self.assertEqual(0, len(data))

    def test_get_events_not_filters(self):
        data = self.get_json(self.PATH, headers=headers,
                             q=[])
        self.assertEqual(3, len(data))

########NEW FILE########
__FILENAME__ = test_list_events_scenarios
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Test listing raw events.
"""

import datetime
import logging

import mock
import webtest.app

from ceilometer.openstack.common import timeutils
from ceilometer.publisher import utils
from ceilometer import sample
from ceilometer.tests.api.v2 import FunctionalTest
from ceilometer.tests import db as tests_db


LOG = logging.getLogger(__name__)


class TestListEvents(FunctionalTest,
                     tests_db.MixinTestsWithBackendScenarios):

    def setUp(self):
        super(TestListEvents, self).setUp()
        patcher = mock.patch.object(timeutils, 'utcnow')
        self.addCleanup(patcher.stop)
        self.mock_utcnow = patcher.start()
        self.mock_utcnow.return_value = datetime.datetime(2014, 2, 11, 16, 42)
        self.sample1 = sample.Sample(
            'instance',
            'cumulative',
            '',
            1,
            'user-id',
            'project1',
            'resource-id',
            timestamp=datetime.datetime(2012, 7, 2, 10, 40),
            resource_metadata={'display_name': 'test-server',
                               'tag': 'self.sample',
                               'dict_properties': {'key': 'value'},
                               'not_ignored_list': ['returned'],
                               },
            source='test_source',
        )
        msg = utils.meter_message_from_counter(
            self.sample1,
            self.CONF.publisher.metering_secret,
        )
        self.conn.record_metering_data(msg)

        self.sample2 = sample.Sample(
            'instance',
            'cumulative',
            '',
            1,
            'user-id2',
            'project2',
            'resource-id-alternate',
            timestamp=datetime.datetime(2012, 7, 2, 10, 41),
            resource_metadata={'display_name': 'test-server',
                               'tag': 'self.sample2',
                               },
            source='source2',
        )
        msg2 = utils.meter_message_from_counter(
            self.sample2,
            self.CONF.publisher.metering_secret,
        )
        self.conn.record_metering_data(msg2)

    def test_all(self):
        data = self.get_json('/meters/instance')
        self.assertEqual(2, len(data))
        for s in data:
            self.assertEqual(timeutils.utcnow().isoformat(), s['recorded_at'])

    def test_all_trailing_slash(self):
        data = self.get_json('/meters/instance/')
        self.assertEqual(2, len(data))

    def test_all_limit(self):
        data = self.get_json('/meters/instance?limit=1')
        self.assertEqual(1, len(data))

    def test_all_limit_negative(self):
        self.assertRaises(webtest.app.AppError,
                          self.get_json,
                          '/meters/instance?limit=-2')

    def test_all_limit_bigger(self):
        data = self.get_json('/meters/instance?limit=42')
        self.assertEqual(2, len(data))

    def test_empty_project(self):
        data = self.get_json('/meters/instance',
                             q=[{'field': 'project_id',
                                 'value': 'no-such-project',
                                 }])
        self.assertEqual([], data)

    def test_by_project(self):
        data = self.get_json('/meters/instance',
                             q=[{'field': 'project_id',
                                 'value': 'project1',
                                 }])
        self.assertEqual(1, len(data))

    def test_empty_resource(self):
        data = self.get_json('/meters/instance',
                             q=[{'field': 'resource_id',
                                 'value': 'no-such-resource',
                                 }])
        self.assertEqual([], data)

    def test_by_resource(self):
        data = self.get_json('/meters/instance',
                             q=[{'field': 'resource_id',
                                 'value': 'resource-id',
                                 }])
        self.assertEqual(1, len(data))

    def test_empty_source(self):
        data = self.get_json('/meters/instance',
                             q=[{'field': 'source',
                                 'value': 'no-such-source',
                                 }])
        self.assertEqual(0, len(data))

    def test_by_source(self):
        data = self.get_json('/meters/instance',
                             q=[{'field': 'source',
                                 'value': 'test_source',
                                 }])
        self.assertEqual(1, len(data))

    def test_empty_user(self):
        data = self.get_json('/meters/instance',
                             q=[{'field': 'user_id',
                                 'value': 'no-such-user',
                                 }])
        self.assertEqual([], data)

    def test_by_user(self):
        data = self.get_json('/meters/instance',
                             q=[{'field': 'user_id',
                                 'value': 'user-id',
                                 }])
        self.assertEqual(1, len(data))

    def test_metadata(self):
        data = self.get_json('/meters/instance',
                             q=[{'field': 'resource_id',
                                 'value': 'resource-id',
                                 }])
        sample = data[0]
        self.assertIn('resource_metadata', sample)
        self.assertEqual(
            [('dict_properties.key', 'value'),
             ('display_name', 'test-server'),
             ('not_ignored_list', "['returned']"),
             ('tag', 'self.sample'),
             ],
            list(sorted(sample['resource_metadata'].iteritems())))

########NEW FILE########
__FILENAME__ = test_list_meters_scenarios
# -*- encoding: utf-8 -*-
#
# Copyright 2012 Red Hat, Inc.
# Copyright 2013 IBM Corp.
#
# Author: Angus Salkeld <asalkeld@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Test listing meters.
"""

import base64
import datetime
import json as jsonutils
import logging
import webtest.app

from ceilometer.publisher import utils
from ceilometer import sample
from ceilometer.tests.api.v2 import FunctionalTest
from ceilometer.tests import db as tests_db

LOG = logging.getLogger(__name__)


class TestListEmptyMeters(FunctionalTest,
                          tests_db.MixinTestsWithBackendScenarios):

    def test_empty(self):
        data = self.get_json('/meters')
        self.assertEqual([], data)


class TestValidateUserInput(FunctionalTest,
                            tests_db.MixinTestsWithBackendScenarios):

    def test_list_meters_query_float_metadata(self):
        self.assertRaises(webtest.app.AppError, self.get_json,
                          '/meters/meter.test',
                          q=[{'field': 'metadata.util',
                          'op': 'eq',
                          'value': '0.7.5',
                          'type': 'float'}])
        self.assertRaises(webtest.app.AppError, self.get_json,
                          '/meters/meter.test',
                          q=[{'field': 'metadata.util',
                          'op': 'eq',
                          'value': 'abacaba',
                          'type': 'boolean'}])
        self.assertRaises(webtest.app.AppError, self.get_json,
                          '/meters/meter.test',
                          q=[{'field': 'metadata.util',
                          'op': 'eq',
                          'value': '45.765',
                          'type': 'integer'}])


class TestListMeters(FunctionalTest,
                     tests_db.MixinTestsWithBackendScenarios):

    def setUp(self):
        super(TestListMeters, self).setUp()
        self.messages = []
        for cnt in [
                sample.Sample(
                    'meter.test',
                    'cumulative',
                    '',
                    1,
                    'user-id',
                    'project-id',
                    'resource-id',
                    timestamp=datetime.datetime(2012, 7, 2, 10, 40),
                    resource_metadata={'display_name': 'test-server',
                                       'tag': 'self.sample',
                                       'size': 123,
                                       'util': 0.75,
                                       'is_public': True},
                    source='test_source'),
                sample.Sample(
                    'meter.test',
                    'cumulative',
                    '',
                    3,
                    'user-id',
                    'project-id',
                    'resource-id',
                    timestamp=datetime.datetime(2012, 7, 2, 11, 40),
                    resource_metadata={'display_name': 'test-server',
                                       'tag': 'self.sample1',
                                       'size': 0,
                                       'util': 0.47,
                                       'is_public': False},
                    source='test_source'),
                sample.Sample(
                    'meter.mine',
                    'gauge',
                    '',
                    1,
                    'user-id',
                    'project-id',
                    'resource-id2',
                    timestamp=datetime.datetime(2012, 7, 2, 10, 41),
                    resource_metadata={'display_name': 'test-server',
                                       'tag': 'self.sample2',
                                       'size': 456,
                                       'util': 0.64,
                                       'is_public': False},
                    source='test_source'),
                sample.Sample(
                    'meter.test',
                    'cumulative',
                    '',
                    1,
                    'user-id2',
                    'project-id2',
                    'resource-id3',
                    timestamp=datetime.datetime(2012, 7, 2, 10, 42),
                    resource_metadata={'display_name': 'test-server',
                                       'tag': 'self.sample3',
                                       'size': 0,
                                       'util': 0.75,
                                       'is_public': False},
                    source='test_source'),
                sample.Sample(
                    'meter.test.new',
                    'cumulative',
                    '',
                    1,
                    'user-id',
                    'project-id',
                    'resource-id',
                    timestamp=datetime.datetime(2012, 7, 2, 10, 40),
                    resource_metadata={'display_name': 'test-server',
                                       'tag': 'self.sample3',
                                       'size': 0,
                                       'util': 0.75,
                                       'is_public': False},
                    source='test_source'),

                sample.Sample(
                    'meter.mine',
                    'gauge',
                    '',
                    1,
                    'user-id4',
                    'project-id2',
                    'resource-id4',
                    timestamp=datetime.datetime(2012, 7, 2, 10, 43),
                    resource_metadata={'display_name': 'test-server',
                                       'tag': 'self.sample4',
                                       'properties': {
                                           'prop_1': 'prop_value',
                                           'prop_2': {'sub_prop_1':
                                                      'sub_prop_value'}
                                       },
                                       'size': 0,
                                       'util': 0.58,
                                       'is_public': True},
                    source='test_source1')]:
            msg = utils.meter_message_from_counter(
                cnt,
                self.CONF.publisher.metering_secret)
            self.messages.append(msg)
            self.conn.record_metering_data(msg)

    def test_list_meters(self):
        data = self.get_json('/meters')
        self.assertEqual(5, len(data))
        self.assertEqual(set(['resource-id',
                              'resource-id2',
                              'resource-id3',
                              'resource-id4']),
                         set(r['resource_id'] for r in data))
        self.assertEqual(set(['meter.test', 'meter.mine', 'meter.test.new']),
                         set(r['name'] for r in data))
        self.assertEqual(set(['test_source', 'test_source1']),
                         set(r['source'] for r in data))

    def test_meters_query_with_timestamp(self):
        date_time = datetime.datetime(2012, 7, 2, 10, 41)
        isotime = date_time.isoformat()
        resp = self.get_json('/meters',
                             q=[{'field': 'timestamp',
                                 'op': 'gt',
                                 'value': isotime}],
                             expect_errors=True)
        self.assertEqual(400, resp.status_code)
        self.assertEqual('Unknown argument: "timestamp": '
                         'not valid for this resource',
                         jsonutils.loads(resp.body)['error_message']
                         ['faultstring'])

    def test_list_samples(self):
        data = self.get_json('/samples')
        self.assertEqual(6, len(data))

    def test_query_samples_with_invalid_field_name_and_non_eq_operator(self):
        resp = self.get_json('/samples',
                             q=[{'field': 'non_valid_field_name',
                                 'op': 'gt',
                                 'value': 3}],
                             expect_errors=True)
        resp_string = jsonutils.loads(resp.body)
        fault_string = resp_string['error_message']['faultstring']
        expected_error_message = ('Unknown argument: "non_valid_field_name"'
                                  ': unrecognized field in query: '
                                  '[<Query u\'non_valid_field_name\' '
                                  'gt u\'3\' ')
        self.assertEqual(400, resp.status_code)
        self.assertTrue(fault_string.startswith(expected_error_message))

    def test_query_samples_with_invalid_field_name_and_eq_operator(self):
        resp = self.get_json('/samples',
                             q=[{'field': 'non_valid_field_name',
                                 'op': 'eq',
                                 'value': 3}],
                             expect_errors=True)
        resp_string = jsonutils.loads(resp.body)
        fault_string = resp_string['error_message']['faultstring']
        expected_error_message = ('Unknown argument: "non_valid_field_name"'
                                  ': unrecognized field in query: '
                                  '[<Query u\'non_valid_field_name\' '
                                  'eq u\'3\' ')
        self.assertEqual(400, resp.status_code)
        self.assertTrue(fault_string.startswith(expected_error_message))

    def test_query_samples_with_invalid_operator_and_valid_field_name(self):
        resp = self.get_json('/samples',
                             q=[{'field': 'project_id',
                                 'op': 'lt',
                                 'value': '3'}],
                             expect_errors=True)
        resp_string = jsonutils.loads(resp.body)
        fault_string = resp_string['error_message']['faultstring']
        expected_error_message = ("Invalid input for field/attribute op. " +
                                  "Value: 'lt'. unimplemented operator for" +
                                  " project_id")
        self.assertEqual(400, resp.status_code)
        self.assertEqual(fault_string, expected_error_message)

    def test_list_meters_query_wrong_type_metadata(self):
        resp = self.get_json('/meters/meter.test',
                             q=[{'field': 'metadata.size',
                             'op': 'eq',
                             'value': '0',
                             'type': 'blob'}],
                             expect_errors=True
                             )
        expected_error_message = 'The data type blob is not supported.'
        resp_string = jsonutils.loads(resp.body)
        fault_string = resp_string['error_message']['faultstring']
        self.assertTrue(fault_string.startswith(expected_error_message))

    def test_query_samples_with_search_offset(self):
        resp = self.get_json('/samples',
                             q=[{'field': 'search_offset',
                                 'op': 'eq',
                                 'value': 42}],
                             expect_errors=True)
        self.assertEqual(400, resp.status_code)
        self.assertEqual("Invalid input for field/attribute field. "
                         "Value: 'search_offset'. "
                         "search_offset cannot be used without timestamp",
                         jsonutils.loads(resp.body)['error_message']
                         ['faultstring'])

    def test_list_meters_with_dict_metadata(self):
        data = self.get_json('/meters/meter.mine',
                             q=[{'field':
                                 'metadata.properties.prop_2.sub_prop_1',
                                 'op': 'eq',
                                 'value': 'sub_prop_value',
                                 }])
        self.assertEqual(1, len(data))
        self.assertEqual('resource-id4', data[0]['resource_id'])
        metadata = data[0]['resource_metadata']
        self.assertIsNotNone(metadata)
        self.assertEqual('self.sample4', metadata['tag'])
        self.assertEqual('prop_value', metadata['properties.prop_1'])

    def test_get_one_sample(self):
        sample_id = self.messages[1]['message_id']
        data = self.get_json('/samples/%s' % sample_id)
        self.assertIn('id', data)
        del data['recorded_at']
        self.assertEqual({
            u'id': sample_id,
            u'metadata': {u'display_name': u'test-server',
                          u'is_public': u'False',
                          u'size': u'0',
                          u'tag': u'self.sample1',
                          u'util': u'0.47'},
            u'meter': u'meter.test',
            u'project_id': u'project-id',
            u'resource_id': u'resource-id',
            u'timestamp': u'2012-07-02T11:40:00',
            u'type': u'cumulative',
            u'unit': u'',
            u'source': 'test_source',
            u'user_id': u'user-id',
            u'volume': 3.0}, data)

    def test_get_not_existing_sample(self):
        resp = self.get_json('/samples/not_exists', expect_errors=True,
                             status=404)
        self.assertEqual("Sample not_exists Not Found",
                         jsonutils.loads(resp.body)['error_message']
                         ['faultstring'])

    def test_list_samples_with_dict_metadata(self):
        data = self.get_json('/samples',
                             q=[{'field':
                                 'metadata.properties.prop_2.sub_prop_1',
                                 'op': 'eq',
                                 'value': 'sub_prop_value',
                                 }])
        self.assertIn('id', data[0])
        del data[0]['id']  # Randomly generated
        del data[0]['recorded_at']
        self.assertEqual([{
            u'user_id': u'user-id4',
            u'resource_id': u'resource-id4',
            u'timestamp': u'2012-07-02T10:43:00',
            u'meter': u'meter.mine',
            u'volume': 1.0,
            u'project_id': u'project-id2',
            u'type': u'gauge',
            u'unit': u'',
            u'source': u'test_source1',
            u'metadata': {u'display_name': u'test-server',
                          u'properties.prop_2:sub_prop_1': u'sub_prop_value',
                          u'util': u'0.58',
                          u'tag': u'self.sample4',
                          u'properties.prop_1': u'prop_value',
                          u'is_public': u'True',
                          u'size': u'0'}
        }], data)

    def test_list_meters_metadata_query(self):
        data = self.get_json('/meters/meter.test',
                             q=[{'field': 'metadata.tag',
                                 'op': 'eq',
                                 'value': 'self.sample1',
                                 }],)
        self.assertEqual(1, len(data))
        self.assertEqual(set(['resource-id']),
                         set(r['resource_id'] for r in data))
        self.assertEqual(set(['meter.test']),
                         set(r['counter_name'] for r in data))

    def test_list_meters_resource_metadata_query(self):
        # NOTE(jd) Same test as above, but with the alias resource_metadata
        # as query field
        data = self.get_json('/meters/meter.test',
                             q=[{'field': 'resource_metadata.tag',
                                 'op': 'eq',
                                 'value': 'self.sample1',
                                 }],)
        self.assertEqual(1, len(data))
        self.assertEqual(set(['resource-id']),
                         set(r['resource_id'] for r in data))
        self.assertEqual(set(['meter.test']),
                         set(r['counter_name'] for r in data))

    def test_list_meters_multi_metadata_query(self):
        data = self.get_json('/meters/meter.test',
                             q=[{'field': 'metadata.tag',
                                 'op': 'eq',
                                 'value': 'self.sample1',
                                 },
                                {'field': 'metadata.display_name',
                                 'op': 'eq',
                                 'value': 'test-server',
                                 }],)
        self.assertEqual(1, len(data))
        self.assertEqual(set(['resource-id']),
                         set(r['resource_id'] for r in data))
        self.assertEqual(set(['meter.test']),
                         set(r['counter_name'] for r in data))

    def test_list_meters_query_integer_metadata(self):
        data = self.get_json('/meters/meter.test',
                             q=[{'field': 'metadata.size',
                             'op': 'eq',
                             'value': '0',
                             'type': 'integer'}]
                             )
        self.assertEqual(2, len(data))
        self.assertEqual(set(['resource-id',
                              'resource-id3']),
                         set(r['resource_id'] for r in data))
        self.assertEqual(set(['meter.test']),
                         set(r['counter_name'] for r in data))
        self.assertEqual(set(['0']),
                         set(r['resource_metadata']['size'] for r in data))

    def test_list_meters_query_float_metadata(self):
        data = self.get_json('/meters/meter.test',
                             q=[{'field': 'metadata.util',
                             'op': 'eq',
                             'value': '0.75',
                             'type': 'float'}]
                             )
        self.assertEqual(2, len(data))
        self.assertEqual(set(['resource-id',
                              'resource-id3']),
                         set(r['resource_id'] for r in data))
        self.assertEqual(set(['meter.test']),
                         set(r['counter_name'] for r in data))
        self.assertEqual(set(['0.75']),
                         set(r['resource_metadata']['util'] for r in data))

    def test_list_meters_query_boolean_metadata(self):
        data = self.get_json('/meters/meter.mine',
                             q=[{'field': 'metadata.is_public',
                             'op': 'eq',
                             'value': 'False',
                             'type': 'boolean'}]
                             )
        self.assertEqual(1, len(data))
        self.assertEqual(set(['resource-id2']),
                         set(r['resource_id'] for r in data))
        self.assertEqual(set(['meter.mine']),
                         set(r['counter_name'] for r in data))
        self.assertEqual(set(['False']),
                         set(r['resource_metadata']['is_public']
                             for r in data))

    def test_list_meters_query_string_metadata(self):
        data = self.get_json('/meters/meter.test',
                             q=[{'field': 'metadata.tag',
                             'op': 'eq',
                             'value': 'self.sample'}]
                             )
        self.assertEqual(1, len(data))
        self.assertEqual(set(['resource-id']),
                         set(r['resource_id'] for r in data))
        self.assertEqual(set(['meter.test']),
                         set(r['counter_name'] for r in data))
        self.assertEqual(set(['self.sample']),
                         set(r['resource_metadata']['tag'] for r in data))

    def test_list_meters_query_integer_float_metadata_without_type(self):
        data = self.get_json('/meters/meter.test',
                             q=[{'field': 'metadata.size',
                                 'op': 'eq',
                                 'value': '0'},
                                {'field': 'metadata.util',
                                 'op': 'eq',
                                 'value': '0.75'}]
                             )
        self.assertEqual(1, len(data))
        self.assertEqual(set(['resource-id3']),
                         set(r['resource_id'] for r in data))
        self.assertEqual(set(['meter.test']),
                         set(r['counter_name'] for r in data))
        self.assertEqual(set(['0']),
                         set(r['resource_metadata']['size'] for r in data))
        self.assertEqual(set(['0.75']),
                         set(r['resource_metadata']['util'] for r in data))

    def test_with_resource(self):
        data = self.get_json('/meters', q=[{'field': 'resource_id',
                                            'value': 'resource-id',
                                            }])
        nids = set(r['name'] for r in data)
        self.assertEqual(set(['meter.test', 'meter.test.new']), nids)

        sids = set(r['source'] for r in data)
        self.assertEqual(set(['test_source']), sids)

    def test_with_resource_and_source(self):
        data = self.get_json('/meters', q=[{'field': 'resource_id',
                                            'value': 'resource-id4',
                                            },
                                           {'field': 'source',
                                            'value': 'test_source1',
                                            }])
        nids = set(r['name'] for r in data)
        self.assertEqual(set(['meter.mine']), nids)

        sids = set(r['source'] for r in data)
        self.assertEqual(set(['test_source1']), sids)

    def test_with_resource_and_metadata_query(self):
        data = self.get_json('/meters/meter.mine',
                             q=[{'field': 'resource_id',
                                 'op': 'eq',
                                 'value': 'resource-id2',
                                 },
                                {'field': 'metadata.tag',
                                 'op': 'eq',
                                 'value': 'self.sample2',
                                 }])
        self.assertEqual(1, len(data))
        self.assertEqual(set(['resource-id2']),
                         set(r['resource_id'] for r in data))
        self.assertEqual(set(['meter.mine']),
                         set(r['counter_name'] for r in data))

    def test_with_source(self):
        data = self.get_json('/meters', q=[{'field': 'source',
                                            'value': 'test_source',
                                            }])
        rids = set(r['resource_id'] for r in data)
        self.assertEqual(set(['resource-id',
                              'resource-id2',
                              'resource-id3']), rids)

        sids = set(r['source'] for r in data)
        self.assertEqual(set(['test_source']), sids)

    def test_with_source_and_metadata_query(self):
        data = self.get_json('/meters/meter.mine',
                             q=[{'field': 'source',
                                 'op': 'eq',
                                 'value': 'test_source',
                                 },
                                {'field': 'metadata.tag',
                                 'op': 'eq',
                                 'value': 'self.sample2',
                                 }])
        self.assertEqual(1, len(data))
        self.assertEqual(set(['test_source']),
                         set(r['source'] for r in data))
        self.assertEqual(set(['meter.mine']),
                         set(r['counter_name'] for r in data))

    def test_with_source_non_existent(self):
        data = self.get_json('/meters',
                             q=[{'field': 'source',
                                 'value': 'test_source_doesnt_exist',
                                 }],
                             )
        self.assertIsEmpty(data)

    def test_with_user(self):
        data = self.get_json('/meters',
                             q=[{'field': 'user_id',
                                 'value': 'user-id',
                                 }],
                             )

        uids = set(r['user_id'] for r in data)
        self.assertEqual(set(['user-id']), uids)

        nids = set(r['name'] for r in data)
        self.assertEqual(set(['meter.mine', 'meter.test', 'meter.test.new']),
                         nids)

        rids = set(r['resource_id'] for r in data)
        self.assertEqual(set(['resource-id', 'resource-id2']), rids)

        sids = set(r['source'] for r in data)
        self.assertEqual(set(['test_source']), sids)

    def test_with_user_and_source(self):
        data = self.get_json('/meters',
                             q=[{'field': 'user_id',
                                 'value': 'user-id4',
                                 },
                                {'field': 'source',
                                 'value': 'test_source1',
                                 }],
                             )

        uids = set(r['user_id'] for r in data)
        self.assertEqual(set(['user-id4']), uids)

        sids = set(r['source'] for r in data)
        self.assertEqual(set(['test_source1']), sids)

    def test_with_user_and_metadata_query(self):
        data = self.get_json('/meters/meter.test',
                             q=[{'field': 'user_id',
                                 'op': 'eq',
                                 'value': 'user-id',
                                 },
                                {'field': 'metadata.tag',
                                 'op': 'eq',
                                 'value': 'self.sample1',
                                 }])
        self.assertEqual(1, len(data))
        self.assertEqual(set(['user-id']), set(r['user_id'] for r in data))
        self.assertEqual(set(['meter.test']),
                         set(r['counter_name'] for r in data))

    def test_with_user_non_existent(self):
        data = self.get_json('/meters',
                             q=[{'field': 'user_id',
                                 'value': 'user-id-foobar123',
                                 }],
                             )
        self.assertEqual([], data)

    def test_with_project(self):
        data = self.get_json('/meters',
                             q=[{'field': 'project_id',
                                 'value': 'project-id2',
                                 }],
                             )
        rids = set(r['resource_id'] for r in data)
        self.assertEqual(set(['resource-id3', 'resource-id4']), rids)

        sids = set(r['source'] for r in data)
        self.assertEqual(set(['test_source', 'test_source1']), sids)

    def test_with_project_and_source(self):
        data = self.get_json('/meters',
                             q=[{'field': 'project_id',
                                 'value': 'project-id2',
                                 },
                                {'field': 'source',
                                 'value': 'test_source1',
                                 }],
                             )
        rids = set(r['resource_id'] for r in data)
        self.assertEqual(set(['resource-id4']), rids)

        sids = set(r['source'] for r in data)
        self.assertEqual(set(['test_source1']), sids)

    def test_with_project_and_metadata_query(self):
        data = self.get_json('/meters/meter.test',
                             q=[{'field': 'project_id',
                                 'op': 'eq',
                                 'value': 'project-id',
                                 },
                                {'field': 'metadata.tag',
                                 'op': 'eq',
                                 'value': 'self.sample1',
                                 }])
        self.assertEqual(1, len(data))
        self.assertEqual(set(['project-id']),
                         set(r['project_id'] for r in data))
        self.assertEqual(set(['meter.test']),
                         set(r['counter_name'] for r in data))

    def test_with_project_non_existent(self):
        data = self.get_json('/meters',
                             q=[{'field': 'project_id',
                                 'value': 'jd-was-here',
                                 }],
                             )
        self.assertEqual([], data)

    def test_list_meters_meter_id(self):
        data = self.get_json('/meters')
        for i in data:
            expected = base64.encodestring('%s+%s' % (i['resource_id'],
                                                      i['name']))
            self.assertEqual(expected, i['meter_id'])

########NEW FILE########
__FILENAME__ = test_list_resources_scenarios
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Test listing resources.
"""

import datetime
import json
import logging

from ceilometer.openstack.common import timeutils
from ceilometer.publisher import utils
from ceilometer import sample
from ceilometer.tests.api.v2 import FunctionalTest
from ceilometer.tests import db as tests_db

LOG = logging.getLogger(__name__)


class TestListResources(FunctionalTest,
                        tests_db.MixinTestsWithBackendScenarios):

    def test_empty(self):
        data = self.get_json('/resources')
        self.assertEqual([], data)

    @staticmethod
    def _isotime(timestamp):
        # drop TZ specifier
        return unicode(timeutils.isotime(timestamp))[:-1]

    def _verify_sample_timestamps(self, res, first, last):
        self.assertTrue('first_sample_timestamp' in res)
        self.assertEqual(self._isotime(first), res['first_sample_timestamp'])
        self.assertTrue('last_sample_timestamp' in res)
        self.assertEqual(self._isotime(last), res['last_sample_timestamp'])

    def test_instance_no_metadata(self):
        timestamp = datetime.datetime(2012, 7, 2, 10, 40)
        sample1 = sample.Sample(
            'instance',
            'cumulative',
            '',
            1,
            'user-id',
            'project-id',
            'resource-id',
            timestamp=timestamp,
            resource_metadata=None,
            source='test',
        )
        msg = utils.meter_message_from_counter(
            sample1,
            self.CONF.publisher.metering_secret,
        )
        self.conn.record_metering_data(msg)

        data = self.get_json('/resources')
        self.assertEqual(1, len(data))
        self._verify_sample_timestamps(data[0], timestamp, timestamp)

    def test_instances(self):
        timestamps = {
            'resource-id': datetime.datetime(2012, 7, 2, 10, 40),
            'resource-id-alternate': datetime.datetime(2012, 7, 2, 10, 41),
        }
        sample1 = sample.Sample(
            'instance',
            'cumulative',
            '',
            1,
            'user-id',
            'project-id',
            'resource-id',
            timestamp=timestamps['resource-id'],
            resource_metadata={'display_name': 'test-server',
                               'tag': 'self.sample',
                               },
            source='test',
        )
        msg = utils.meter_message_from_counter(
            sample1,
            self.CONF.publisher.metering_secret,
        )
        self.conn.record_metering_data(msg)

        sample2 = sample.Sample(
            'instance',
            'cumulative',
            '',
            1,
            'user-id',
            'project-id',
            'resource-id-alternate',
            timestamp=timestamps['resource-id-alternate'],
            resource_metadata={'display_name': 'test-server',
                               'tag': 'self.sample2',
                               },
            source='test',
        )
        msg2 = utils.meter_message_from_counter(
            sample2,
            self.CONF.publisher.metering_secret,
        )
        self.conn.record_metering_data(msg2)

        data = self.get_json('/resources')
        self.assertEqual(2, len(data))
        for res in data:
            timestamp = timestamps.get(res['resource_id'])
            self._verify_sample_timestamps(res, timestamp, timestamp)

    def test_instance_multiple_samples(self):
        timestamps = [
            datetime.datetime(2012, 7, 2, 10, 41),
            datetime.datetime(2012, 7, 2, 10, 42),
            datetime.datetime(2012, 7, 2, 10, 40),
        ]
        for timestamp in timestamps:
            datapoint = sample.Sample(
                'instance',
                'cumulative',
                '',
                1,
                'user-id',
                'project-id',
                'resource-id',
                timestamp=timestamp,
                resource_metadata={'display_name': 'test-server',
                                   'tag': 'self.sample-%s' % timestamp,
                                   },
                source='test',
            )
            msg = utils.meter_message_from_counter(
                datapoint,
                self.CONF.publisher.metering_secret,
            )
            self.conn.record_metering_data(msg)

        data = self.get_json('/resources')
        self.assertEqual(1, len(data))
        self._verify_sample_timestamps(data[0], timestamps[-1], timestamps[1])

    def test_instances_one(self):
        sample1 = sample.Sample(
            'instance',
            'cumulative',
            '',
            1,
            'user-id',
            'project-id',
            'resource-id',
            timestamp=datetime.datetime(2012, 7, 2, 10, 40),
            resource_metadata={'display_name': 'test-server',
                               'tag': 'self.sample',
                               },
            source='test',
        )
        msg = utils.meter_message_from_counter(
            sample1,
            self.CONF.publisher.metering_secret,
        )
        self.conn.record_metering_data(msg)

        sample2 = sample.Sample(
            'instance',
            'cumulative',
            '',
            1,
            'user-id',
            'project-id',
            'resource-id-alternate',
            timestamp=datetime.datetime(2012, 7, 2, 10, 41),
            resource_metadata={'display_name': 'test-server',
                               'tag': 'self.sample2',
                               },
            source='test',
        )
        msg2 = utils.meter_message_from_counter(
            sample2,
            self.CONF.publisher.metering_secret,
        )
        self.conn.record_metering_data(msg2)

        data = self.get_json('/resources/resource-id')
        self.assertEqual('resource-id', data['resource_id'])

    def test_with_source(self):
        sample1 = sample.Sample(
            'instance',
            'cumulative',
            '',
            1,
            'user-id',
            'project-id',
            'resource-id',
            timestamp=datetime.datetime(2012, 7, 2, 10, 40),
            resource_metadata={'display_name': 'test-server',
                               'tag': 'self.sample',
                               },
            source='test_list_resources',
        )
        msg = utils.meter_message_from_counter(
            sample1,
            self.CONF.publisher.metering_secret,
        )
        self.conn.record_metering_data(msg)

        sample2 = sample.Sample(
            'instance',
            'cumulative',
            '',
            1,
            'user-id2',
            'project-id',
            'resource-id-alternate',
            timestamp=datetime.datetime(2012, 7, 2, 10, 41),
            resource_metadata={'display_name': 'test-server',
                               'tag': 'self.sample2',
                               },
            source='not-test',
        )
        msg2 = utils.meter_message_from_counter(
            sample2,
            self.CONF.publisher.metering_secret,
        )
        self.conn.record_metering_data(msg2)

        data = self.get_json('/resources', q=[{'field': 'source',
                                               'value': 'test_list_resources',
                                               }])
        ids = [r['resource_id'] for r in data]
        self.assertEqual(['resource-id'], ids)
        sources = [r['source'] for r in data]
        self.assertEqual(['test_list_resources'], sources)

    def test_with_invalid_resource_id(self):
        sample1 = sample.Sample(
            'instance',
            'cumulative',
            '',
            1,
            'user-id',
            'project-id',
            'resource-id-1',
            timestamp=datetime.datetime(2012, 7, 2, 10, 40),
            resource_metadata={'display_name': 'test-server',
                               'tag': 'self.sample',
                               },
            source='test_list_resources',
        )
        msg = utils.meter_message_from_counter(
            sample1,
            self.CONF.publisher.metering_secret,
        )
        self.conn.record_metering_data(msg)

        sample2 = sample.Sample(
            'instance',
            'cumulative',
            '',
            1,
            'user-id2',
            'project-id',
            'resource-id-2',
            timestamp=datetime.datetime(2012, 7, 2, 10, 41),
            resource_metadata={'display_name': 'test-server',
                               'tag': 'self.sample2',
                               },
            source='test_list_resources',
        )
        msg2 = utils.meter_message_from_counter(
            sample2,
            self.CONF.publisher.metering_secret,
        )
        self.conn.record_metering_data(msg2)

        resp1 = self.get_json('/resources/resource-id-1')
        self.assertEqual("resource-id-1", resp1["resource_id"])

        resp2 = self.get_json('/resources/resource-id-2')
        self.assertEqual("resource-id-2", resp2["resource_id"])

        resp3 = self.get_json('/resources/resource-id-3', expect_errors=True)
        self.assertEqual(404, resp3.status_code)
        self.assertEqual("Resource resource-id-3 Not Found",
                         json.loads(resp3.body)['error_message']
                         ['faultstring'])

    def test_with_user(self):
        sample1 = sample.Sample(
            'instance',
            'cumulative',
            '',
            1,
            'user-id',
            'project-id',
            'resource-id',
            timestamp=datetime.datetime(2012, 7, 2, 10, 40),
            resource_metadata={'display_name': 'test-server',
                               'tag': 'self.sample',
                               },
            source='test_list_resources',
        )
        msg = utils.meter_message_from_counter(
            sample1,
            self.CONF.publisher.metering_secret,
        )
        self.conn.record_metering_data(msg)

        sample2 = sample.Sample(
            'instance',
            'cumulative',
            '',
            1,
            'user-id2',
            'project-id',
            'resource-id-alternate',
            timestamp=datetime.datetime(2012, 7, 2, 10, 41),
            resource_metadata={'display_name': 'test-server',
                               'tag': 'self.sample2',
                               },
            source='not-test',
        )
        msg2 = utils.meter_message_from_counter(
            sample2,
            self.CONF.publisher.metering_secret,
        )
        self.conn.record_metering_data(msg2)

        data = self.get_json('/resources', q=[{'field': 'user_id',
                                               'value': 'user-id',
                                               }])
        ids = [r['resource_id'] for r in data]
        self.assertEqual(['resource-id'], ids)

    def test_with_project(self):
        sample1 = sample.Sample(
            'instance',
            'cumulative',
            '',
            1,
            'user-id',
            'project-id',
            'resource-id',
            timestamp=datetime.datetime(2012, 7, 2, 10, 40),
            resource_metadata={'display_name': 'test-server',
                               'tag': 'self.sample',
                               },
            source='test_list_resources',
        )
        msg = utils.meter_message_from_counter(
            sample1,
            self.CONF.publisher.metering_secret,
        )
        self.conn.record_metering_data(msg)

        sample2 = sample.Sample(
            'instance',
            'cumulative',
            '',
            1,
            'user-id2',
            'project-id2',
            'resource-id-alternate',
            timestamp=datetime.datetime(2012, 7, 2, 10, 41),
            resource_metadata={'display_name': 'test-server',
                               'tag': 'self.sample2',
                               },
            source='not-test',
        )
        msg2 = utils.meter_message_from_counter(
            sample2,
            self.CONF.publisher.metering_secret,
        )
        self.conn.record_metering_data(msg2)

        data = self.get_json('/resources', q=[{'field': 'project_id',
                                               'value': 'project-id',
                                               }])
        ids = [r['resource_id'] for r in data]
        self.assertEqual(['resource-id'], ids)

    def test_with_user_non_admin(self):
        sample1 = sample.Sample(
            'instance',
            'cumulative',
            '',
            1,
            'user-id2',
            'project-id2',
            'resource-id-alternate',
            timestamp=datetime.datetime(2012, 7, 2, 10, 41),
            resource_metadata={'display_name': 'test-server',
                               'tag': 'self.sample1',
                               },
            source='not-test',
        )
        msg2 = utils.meter_message_from_counter(
            sample1,
            self.CONF.publisher.metering_secret,
        )
        self.conn.record_metering_data(msg2)

        data = self.get_json('/resources',
                             headers={"X-Roles": "Member",
                                      "X-Project-Id": "project-id2"})
        ids = set(r['resource_id'] for r in data)
        self.assertEqual(set(['resource-id-alternate']), ids)

    def test_with_user_wrong_tenant(self):
        sample1 = sample.Sample(
            'instance',
            'cumulative',
            '',
            1,
            'user-id2',
            'project-id2',
            'resource-id-alternate',
            timestamp=datetime.datetime(2012, 7, 2, 10, 41),
            resource_metadata={'display_name': 'test-server',
                               'tag': 'self.sample1',
                               },
            source='not-test',
        )
        msg2 = utils.meter_message_from_counter(
            sample1,
            self.CONF.publisher.metering_secret,
        )
        self.conn.record_metering_data(msg2)

        data = self.get_json('/resources',
                             headers={"X-Roles": "Member",
                                      "X-Project-Id": "project-wrong"})
        ids = set(r['resource_id'] for r in data)
        self.assertEqual(set(), ids)

    def test_metadata(self):
        sample1 = sample.Sample(
            'instance',
            'cumulative',
            '',
            1,
            'user-id',
            'project-id',
            'resource-id',
            timestamp=datetime.datetime(2012, 7, 2, 10, 40),
            resource_metadata={'display_name': 'test-server',
                               'tag': 'self.sample',
                               'dict_properties': {'key': 'value'},
                               'not_ignored_list': ['returned'],
                               },
            source='test',
        )
        msg = utils.meter_message_from_counter(
            sample1,
            self.CONF.publisher.metering_secret,
        )
        self.conn.record_metering_data(msg)

        data = self.get_json('/resources')
        metadata = data[0]['metadata']
        self.assertEqual([(u'dict_properties.key', u'value'),
                          (u'display_name', u'test-server'),
                          (u'not_ignored_list', u"['returned']"),
                          (u'tag', u'self.sample')],
                         list(sorted(metadata.iteritems())))

    def test_resource_meter_links(self):
        sample1 = sample.Sample(
            'instance',
            'cumulative',
            '',
            1,
            'user-id',
            'project-id',
            'resource-id',
            timestamp=datetime.datetime(2012, 7, 2, 10, 40),
            resource_metadata={'display_name': 'test-server',
                               'tag': 'self.sample',
                               },
            source='test_list_resources',
        )
        msg = utils.meter_message_from_counter(
            sample1,
            self.CONF.publisher.metering_secret,
        )
        self.conn.record_metering_data(msg)

        data = self.get_json('/resources')
        links = data[0]['links']
        self.assertEqual(2, len(links))
        self.assertEqual('self', links[0]['rel'])
        self.assertTrue((self.PATH_PREFIX + '/resources/resource-id')
                        in links[0]['href'])
        self.assertEqual('instance', links[1]['rel'])
        self.assertTrue((self.PATH_PREFIX + '/meters/instance?'
                         'q.field=resource_id&q.value=resource-id')
                        in links[1]['href'])

    def test_resource_skip_meter_links(self):
        sample1 = sample.Sample(
            'instance',
            'cumulative',
            '',
            1,
            'user-id',
            'project-id',
            'resource-id',
            timestamp=datetime.datetime(2012, 7, 2, 10, 40),
            resource_metadata={'display_name': 'test-server',
                               'tag': 'self.sample',
                               },
            source='test_list_resources',
        )
        msg = utils.meter_message_from_counter(
            sample1,
            self.CONF.publisher.metering_secret,
        )
        self.conn.record_metering_data(msg)

        data = self.get_json('/resources?meter_links=0')
        links = data[0]['links']
        self.assertEqual(len(links), 1)
        self.assertEqual(links[0]['rel'], 'self')
        self.assertTrue((self.PATH_PREFIX + '/resources/resource-id')
                        in links[0]['href'])

########NEW FILE########
__FILENAME__ = test_post_samples_scenarios
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Red Hat, Inc
#
# Author: Angus Salkeld <asalkeld@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Test listing raw events.
"""

import copy
import datetime

import mock

from ceilometer.openstack.common.fixture import mockpatch
from ceilometer.openstack.common import timeutils
from ceilometer.tests.api.v2 import FunctionalTest
from ceilometer.tests import db as tests_db


class TestPostSamples(FunctionalTest,
                      tests_db.MixinTestsWithBackendScenarios):
    def fake_cast(self, ctxt, target, data):
        for m in data:
            del m['message_signature']
        self.published.append(data)

    def patch_publishers(self):
        cast_ctxt = mock.Mock()
        cast_ctxt.cast.side_effect = self.fake_cast
        found = False
        pipeline_hook = self.app.app.application.app.hooks[2]
        for pipeline in pipeline_hook.pipeline_manager.pipelines:
            for publisher in pipeline.publishers:
                if hasattr(publisher, 'rpc_client'):
                    self.useFixture(mockpatch.PatchObject(
                        publisher.rpc_client, 'prepare',
                        return_value=cast_ctxt))
                    found = True
        if not found:
            raise Exception('fail to patch the rpc publisher')

    def setUp(self):
        super(TestPostSamples, self).setUp()
        self.published = []
        self.patch_publishers()

    def test_one(self):
        s1 = [{'counter_name': 'apples',
               'counter_type': 'gauge',
               'counter_unit': 'instance',
               'counter_volume': 1,
               'resource_id': 'bd9431c1-8d69-4ad3-803a-8d4a6b89fd36',
               'project_id': '35b17138-b364-4e6a-a131-8f3099c5be68',
               'user_id': 'efd87807-12d2-4b38-9c70-5f5c2ac427ff',
               'resource_metadata': {'name1': 'value1',
                                     'name2': 'value2'}}]
        data = self.post_json('/meters/apples/', s1)

        # timestamp not given so it is generated.
        s1[0]['timestamp'] = data.json[0]['timestamp']
        # Ignore message id that is randomly generated
        s1[0]['message_id'] = data.json[0]['message_id']
        # source is generated if not provided.
        s1[0]['source'] = '%s:openstack' % s1[0]['project_id']

        self.assertEqual(s1, data.json)
        self.assertEqual(s1[0], self.published[0][0])

    def test_nested_metadata(self):
        s1 = [{'counter_name': 'apples',
               'counter_type': 'gauge',
               'counter_unit': 'instance',
               'counter_volume': 1,
               'resource_id': 'bd9431c1-8d69-4ad3-803a-8d4a6b89fd36',
               'project_id': '35b17138-b364-4e6a-a131-8f3099c5be68',
               'user_id': 'efd87807-12d2-4b38-9c70-5f5c2ac427ff',
               'resource_metadata': {'nest.name1': 'value1',
                                     'name2': 'value2',
                                     'nest.name2': 'value3'}}]

        data = self.post_json('/meters/apples/', s1)

        # timestamp not given so it is generated.
        s1[0]['timestamp'] = data.json[0]['timestamp']
        # Ignore message id that is randomly generated
        s1[0]['message_id'] = data.json[0]['message_id']
        # source is generated if not provided.
        s1[0]['source'] = '%s:openstack' % s1[0]['project_id']

        unwound = copy.copy(s1[0])
        unwound['resource_metadata'] = {'nest': {'name1': 'value1',
                                                 'name2': 'value3'},
                                        'name2': 'value2'}
        # only the published sample should be unwound, not the representation
        # in the API response
        self.assertEqual(s1[0], data.json[0])
        self.assertEqual(unwound, self.published[0][0])

    def test_invalid_counter_type(self):
        s1 = [{'counter_name': 'my_counter_name',
               'counter_type': 'INVALID_TYPE',
               'counter_unit': 'instance',
               'counter_volume': 1,
               'source': 'closedstack',
               'resource_id': 'bd9431c1-8d69-4ad3-803a-8d4a6b89fd36',
               'project_id': '35b17138-b364-4e6a-a131-8f3099c5be68',
               'user_id': 'efd87807-12d2-4b38-9c70-5f5c2ac427ff',
               'resource_metadata': {'name1': 'value1',
                                     'name2': 'value2'}}]

        data = self.post_json('/meters/my_counter_name/', s1,
                              expect_errors=True)

        self.assertEqual(400, data.status_int)
        self.assertEqual(0, len(self.published))

    def test_messsage_id_provided(self):
        """Do not accept sample with message_id."""
        s1 = [{'counter_name': 'my_counter_name',
               'counter_type': 'gauge',
               'counter_unit': 'instance',
               'counter_volume': 1,
               'message_id': 'evil',
               'source': 'closedstack',
               'resource_id': 'bd9431c1-8d69-4ad3-803a-8d4a6b89fd36',
               'project_id': '35b17138-b364-4e6a-a131-8f3099c5be68',
               'user_id': 'efd87807-12d2-4b38-9c70-5f5c2ac427ff',
               'resource_metadata': {'name1': 'value1',
                                     'name2': 'value2'}}]

        data = self.post_json('/meters/my_counter_name/', s1,
                              expect_errors=True)

        self.assertEqual(400, data.status_int)
        self.assertEqual(0, len(self.published))

    def test_wrong_project_id(self):
        """Do not accept cross posting samples to different projects."""
        s1 = [{'counter_name': 'my_counter_name',
               'counter_type': 'gauge',
               'counter_unit': 'instance',
               'counter_volume': 1,
               'source': 'closedstack',
               'resource_id': 'bd9431c1-8d69-4ad3-803a-8d4a6b89fd36',
               'project_id': '35b17138-b364-4e6a-a131-8f3099c5be68',
               'user_id': 'efd87807-12d2-4b38-9c70-5f5c2ac427ff',
               'resource_metadata': {'name1': 'value1',
                                     'name2': 'value2'}}]

        data = self.post_json('/meters/my_counter_name/', s1,
                              expect_errors=True,
                              headers={
                                  "X-Roles": "Member",
                                  "X-Tenant-Name": "lu-tenant",
                                  "X-Project-Id":
                                  "bc23a9d531064583ace8f67dad60f6bb",
                              })

        self.assertEqual(400, data.status_int)
        self.assertEqual(0, len(self.published))

    def test_multiple_samples(self):
        """Send multiple samples.
        The usecase here is to reduce the chatter and send the counters
        at a slower cadence.
        """
        samples = []
        for x in range(6):
            dt = datetime.datetime(2012, 8, 27, x, 0, tzinfo=None)
            s = {'counter_name': 'apples',
                 'counter_type': 'gauge',
                 'counter_unit': 'instance',
                 'counter_volume': float(x * 3),
                 'source': 'evil',
                 'timestamp': dt.isoformat(),
                 'resource_id': 'bd9431c1-8d69-4ad3-803a-8d4a6b89fd36',
                 'project_id': '35b17138-b364-4e6a-a131-8f3099c5be68',
                 'user_id': 'efd87807-12d2-4b38-9c70-5f5c2ac427ff',
                 'resource_metadata': {'name1': str(x),
                                       'name2': str(x + 4)}}
            samples.append(s)

        data = self.post_json('/meters/apples/', samples)

        for x, s in enumerate(samples):
            # source is modified to include the project_id.
            s['source'] = '%s:%s' % (s['project_id'],
                                     s['source'])
            # Ignore message id that is randomly generated
            s['message_id'] = data.json[x]['message_id']

            # remove tzinfo to compare generated timestamp
            # with the provided one
            c = data.json[x]
            timestamp = timeutils.parse_isotime(c['timestamp'])
            c['timestamp'] = timestamp.replace(tzinfo=None).isoformat()

            # do the same on the pipeline
            msg = self.published[0][x]
            timestamp = timeutils.parse_isotime(msg['timestamp'])
            msg['timestamp'] = timestamp.replace(tzinfo=None).isoformat()

            self.assertEqual(s, c)
            self.assertEqual(s, self.published[0][x])

    def test_missing_mandatory_fields(self):
        """Do not accept posting samples with missing mandatory fields."""
        s1 = [{'counter_name': 'my_counter_name',
               'counter_type': 'gauge',
               'counter_unit': 'instance',
               'counter_volume': 1,
               'source': 'closedstack',
               'resource_id': 'bd9431c1-8d69-4ad3-803a-8d4a6b89fd36',
               'project_id': '35b17138-b364-4e6a-a131-8f3099c5be68',
               'user_id': 'efd87807-12d2-4b38-9c70-5f5c2ac427ff',
               'resource_metadata': {'name1': 'value1',
                                     'name2': 'value2'}}]

        # one by one try posting without a mandatory field.
        for m in ['counter_volume', 'counter_unit', 'counter_type',
                  'resource_id', 'counter_name']:
            s_broke = copy.copy(s1)
            del s_broke[0][m]
            print('posting without %s' % m)
            data = self.post_json('/meters/my_counter_name', s_broke,
                                  expect_errors=True)
            self.assertEqual(400, data.status_int)

    def test_multiple_project_id_and_admin(self):
        """Allow admin is allowed to set multiple project_id."""
        s1 = [{'counter_name': 'my_counter_name',
               'counter_type': 'gauge',
               'counter_unit': 'instance',
               'counter_volume': 1,
               'source': 'closedstack',
               'project_id': '35b17138-b364-4e6a-a131-8f3099c5be68',
               'user_id': 'efd87807-12d2-4b38-9c70-5f5c2ac427ff',
               'resource_id': 'bd9431c1-8d69-4ad3-803a-8d4a6b89fd36',
               },
              {'counter_name': 'my_counter_name',
               'counter_type': 'gauge',
               'counter_unit': 'instance',
               'counter_volume': 2,
               'source': 'closedstack',
               'project_id': '4af38dca-f6fc-11e2-94f5-14dae9283f29',
               'user_id': 'efd87807-12d2-4b38-9c70-5f5c2ac427ff',
               'resource_id': 'bd9431c1-8d69-4ad3-803a-8d4a6b89fd36',
               'resource_metadata': {'name1': 'value1',
                                     'name2': 'value2'}}]
        data = self.post_json('/meters/my_counter_name/', s1,
                              headers={"X-Roles": "admin"})

        self.assertEqual(200, data.status_int)
        for x, s in enumerate(s1):
            # source is modified to include the project_id.
            s['source'] = '%s:%s' % (s['project_id'],
                                     'closedstack')
            # Ignore message id that is randomly generated
            s['message_id'] = data.json[x]['message_id']
            # timestamp not given so it is generated.
            s['timestamp'] = data.json[x]['timestamp']
            s.setdefault('resource_metadata', dict())
            self.assertEqual(s, data.json[x])
            self.assertEqual(s, self.published[0][x])

    def test_multiple_samples_multiple_sources(self):
        """Do accept a single post with some multiples sources
        with some of them null
        """
        s1 = [{'counter_name': 'my_counter_name',
               'counter_type': 'gauge',
               'counter_unit': 'instance',
               'counter_volume': 1,
               'source': 'paperstack',
               'project_id': '35b17138-b364-4e6a-a131-8f3099c5be68',
               'user_id': 'efd87807-12d2-4b38-9c70-5f5c2ac427ff',
               'resource_id': 'bd9431c1-8d69-4ad3-803a-8d4a6b89fd36',
               },
              {'counter_name': 'my_counter_name',
               'counter_type': 'gauge',
               'counter_unit': 'instance',
               'counter_volume': 5,
               'source': 'waterstack',
               'project_id': '35b17138-b364-4e6a-a131-8f3099c5be68',
               'user_id': 'efd87807-12d2-4b38-9c70-5f5c2ac427ff',
               'resource_id': 'bd9431c1-8d69-4ad3-803a-8d4a6b89fd36',
               },
              {'counter_name': 'my_counter_name',
               'counter_type': 'gauge',
               'counter_unit': 'instance',
               'counter_volume': 2,
               'project_id': '35b17138-b364-4e6a-a131-8f3099c5be68',
               'user_id': 'efd87807-12d2-4b38-9c70-5f5c2ac427ff',
               'resource_id': 'bd9431c1-8d69-4ad3-803a-8d4a6b89fd36',
               'resource_metadata': {'name1': 'value1',
                                     'name2': 'value2'}}]
        data = self.post_json('/meters/my_counter_name/', s1,
                              expect_errors=True)
        self.assertEqual(200, data.status_int)
        for x, s in enumerate(s1):
            # source is modified to include the project_id.
            s['source'] = '%s:%s' % (
                s['project_id'],
                s.get('source', self.CONF.sample_source)
            )
            # Ignore message id that is randomly generated
            s['message_id'] = data.json[x]['message_id']
            # timestamp not given so it is generated.
            s['timestamp'] = data.json[x]['timestamp']
            s.setdefault('resource_metadata', dict())
            self.assertEqual(s, data.json[x])
            self.assertEqual(s, self.published[0][x])

    def test_missing_project_user_id(self):
        """Ensure missing project & user IDs are defaulted appropriately.
        """
        s1 = [{'counter_name': 'my_counter_name',
               'counter_type': 'gauge',
               'counter_unit': 'instance',
               'counter_volume': 1,
               'source': 'closedstack',
               'resource_id': 'bd9431c1-8d69-4ad3-803a-8d4a6b89fd36',
               'resource_metadata': {'name1': 'value1',
                                     'name2': 'value2'}}]

        project_id = 'bc23a9d531064583ace8f67dad60f6bb'
        user_id = 'fd87807-12d2-4b38-9c70-5f5c2ac427ff'
        data = self.post_json('/meters/my_counter_name/', s1,
                              expect_errors=True,
                              headers={
                                  'X-Roles': 'chief-bottle-washer',
                                  'X-Project-Id': project_id,
                                  'X-User-Id': user_id,
                              })

        self.assertEqual(200, data.status_int)
        for x, s in enumerate(s1):
            # source is modified to include the project_id.
            s['source'] = '%s:%s' % (project_id,
                                     s['source'])
            # Ignore message id that is randomly generated
            s['message_id'] = data.json[x]['message_id']
            # timestamp not given so it is generated.
            s['timestamp'] = data.json[x]['timestamp']
            s['user_id'] = user_id
            s['project_id'] = project_id

            self.assertEqual(s, data.json[x])
            self.assertEqual(s, self.published[0][x])

########NEW FILE########
__FILENAME__ = test_query
# Copyright 2013 OpenStack Foundation.
# All Rights Reserved.
# Copyright 2013 IBM Corp.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
"""Test the methods related to query."""
import datetime

import fixtures
import mock
import wsme

from ceilometer.api.controllers import v2 as api
from ceilometer.openstack.common.fixture.mockpatch import PatchObject
from ceilometer.openstack.common import test
from ceilometer.openstack.common import timeutils
from ceilometer import storage
from ceilometer.tests import base as tests_base


class TestQuery(test.BaseTestCase):
    def setUp(self):
        super(TestQuery, self).setUp()
        self.useFixture(fixtures.MonkeyPatch(
            'pecan.response', mock.MagicMock()))

    def test_get_value_as_type_with_integer(self):
        query = api.Query(field='metadata.size',
                          op='eq',
                          value='123',
                          type='integer')
        expected = 123
        self.assertEqual(expected, query._get_value_as_type())

    def test_get_value_as_type_with_float(self):
        query = api.Query(field='metadata.size',
                          op='eq',
                          value='123.456',
                          type='float')
        expected = 123.456
        self.assertEqual(expected, query._get_value_as_type())

    def test_get_value_as_type_with_boolean(self):
        query = api.Query(field='metadata.is_public',
                          op='eq',
                          value='True',
                          type='boolean')
        expected = True
        self.assertEqual(expected, query._get_value_as_type())

    def test_get_value_as_type_with_string(self):
        query = api.Query(field='metadata.name',
                          op='eq',
                          value='linux',
                          type='string')
        expected = 'linux'
        self.assertEqual(expected, query._get_value_as_type())

    def test_get_value_as_type_with_integer_without_type(self):
        query = api.Query(field='metadata.size',
                          op='eq',
                          value='123')
        expected = 123
        self.assertEqual(expected, query._get_value_as_type())

    def test_get_value_as_type_with_float_without_type(self):
        query = api.Query(field='metadata.size',
                          op='eq',
                          value='123.456')
        expected = 123.456
        self.assertEqual(expected, query._get_value_as_type())

    def test_get_value_as_type_with_boolean_without_type(self):
        query = api.Query(field='metadata.is_public',
                          op='eq',
                          value='True')
        expected = True
        self.assertEqual(expected, query._get_value_as_type())

    def test_get_value_as_type_with_string_without_type(self):
        query = api.Query(field='metadata.name',
                          op='eq',
                          value='linux')
        expected = 'linux'
        self.assertEqual(expected, query._get_value_as_type())

    def test_get_value_as_type_with_bad_type(self):
        query = api.Query(field='metadata.size',
                          op='eq',
                          value='123.456',
                          type='blob')
        self.assertRaises(wsme.exc.ClientSideError, query._get_value_as_type)

    def test_get_value_as_type_with_bad_value(self):
        query = api.Query(field='metadata.size',
                          op='eq',
                          value='fake',
                          type='integer')
        self.assertRaises(wsme.exc.ClientSideError, query._get_value_as_type)

    def test_get_value_as_type_integer_expression_without_type(self):
        # bug 1221736
        query = api.Query(field='should_be_a_string',
                          op='eq',
                          value='123-1')
        expected = '123-1'
        self.assertEqual(expected, query._get_value_as_type())

    def test_get_value_as_type_boolean_expression_without_type(self):
        # bug 1221736
        query = api.Query(field='should_be_a_string',
                          op='eq',
                          value='True or False')
        expected = 'True or False'
        self.assertEqual(expected, query._get_value_as_type())

    def test_get_value_as_type_with_syntax_error(self):
        # bug 1221736
        value = 'WWW-Layer-4a80714f-0232-4580-aa5e-81494d1a4147-uolhh25p5xxm'
        query = api.Query(field='group_id',
                          op='eq',
                          value=value)
        expected = value
        self.assertEqual(expected, query._get_value_as_type())

    def test_get_value_as_type_with_syntax_error_colons(self):
        # bug 1221736
        value = 'Ref::StackId'
        query = api.Query(field='field_name',
                          op='eq',
                          value=value)
        expected = value
        self.assertEqual(expected, query._get_value_as_type())


class TestValidateGroupByFields(test.BaseTestCase):

    def test_valid_field(self):
        result = api._validate_groupby_fields(['user_id'])
        self.assertEqual(['user_id'], result)

    def test_valid_fields_multiple(self):
        result = set(
            api._validate_groupby_fields(['user_id', 'project_id', 'source'])
        )
        self.assertEqual(set(['user_id', 'project_id', 'source']), result)

    def test_invalid_field(self):
        self.assertRaises(wsme.exc.UnknownArgument,
                          api._validate_groupby_fields,
                          ['wtf'])

    def test_invalid_field_multiple(self):
        self.assertRaises(wsme.exc.UnknownArgument,
                          api._validate_groupby_fields,
                          ['user_id', 'wtf', 'project_id', 'source'])

    def test_duplicate_fields(self):
        result = set(
            api._validate_groupby_fields(['user_id', 'source', 'user_id'])
        )
        self.assertEqual(set(['user_id', 'source']), result)


class TestQueryToKwArgs(tests_base.BaseTestCase):
    def setUp(self):
        super(TestQueryToKwArgs, self).setUp()
        self.useFixture(PatchObject(api, '_sanitize_query',
                        side_effect=lambda x, y, **z: x))
        self.useFixture(PatchObject(api, '_verify_query_segregation',
                        side_effect=lambda x, **z: x))

    def test_sample_filter_single(self):
        q = [api.Query(field='user_id',
                       op='eq',
                       value='uid')]
        kwargs = api._query_to_kwargs(q, storage.SampleFilter.__init__)
        self.assertIn('user', kwargs)
        self.assertEqual(1, len(kwargs))
        self.assertEqual('uid', kwargs['user'])

    def test_sample_filter_multi(self):
        q = [api.Query(field='user_id',
                       op='eq',
                       value='uid'),
             api.Query(field='project_id',
                       op='eq',
                       value='pid'),
             api.Query(field='resource_id',
                       op='eq',
                       value='rid'),
             api.Query(field='source',
                       op='eq',
                       value='source_name'),
             api.Query(field='meter',
                       op='eq',
                       value='meter_name')]
        kwargs = api._query_to_kwargs(q, storage.SampleFilter.__init__)
        self.assertEqual(5, len(kwargs))
        self.assertEqual('uid', kwargs['user'])
        self.assertEqual('pid', kwargs['project'])
        self.assertEqual('rid', kwargs['resource'])
        self.assertEqual('source_name', kwargs['source'])
        self.assertEqual('meter_name', kwargs['meter'])

    def test_sample_filter_timestamp(self):
        ts_start = timeutils.utcnow()
        ts_end = ts_start + datetime.timedelta(minutes=5)
        q = [api.Query(field='timestamp',
                       op='lt',
                       value=str(ts_end)),
             api.Query(field='timestamp',
                       op='gt',
                       value=str(ts_start))]
        kwargs = api._query_to_kwargs(q, storage.SampleFilter.__init__)
        self.assertEqual(4, len(kwargs))
        self.assertTimestampEqual(kwargs['start'], ts_start)
        self.assertTimestampEqual(kwargs['end'], ts_end)
        self.assertEqual('gt', kwargs['start_timestamp_op'])
        self.assertEqual('lt', kwargs['end_timestamp_op'])

    def test_sample_filter_meta(self):
        q = [api.Query(field='metadata.size',
                       op='eq',
                       value='20'),
             api.Query(field='resource_metadata.id',
                       op='eq',
                       value='meta_id')]
        kwargs = api._query_to_kwargs(q, storage.SampleFilter.__init__)
        self.assertEqual(1, len(kwargs))
        self.assertEqual(2, len(kwargs['metaquery']))
        self.assertEqual(20, kwargs['metaquery']['metadata.size'])
        self.assertEqual('meta_id', kwargs['metaquery']['metadata.id'])

    def test_sample_filter_non_equality_on_metadata(self):
        queries = [api.Query(field='resource_metadata.image_id',
                             op='gt',
                             value='image',
                             type='string'),
                   api.Query(field='metadata.ramdisk_id',
                             op='le',
                             value='ramdisk',
                             type='string')]
        with mock.patch('pecan.request') as request:
            request.headers.return_value = {'X-ProjectId': 'foobar'}
            self.assertRaises(
                wsme.exc.InvalidInput,
                api._query_to_kwargs,
                queries,
                storage.SampleFilter.__init__)

    def test_sample_filter_invalid_field(self):
        q = [api.Query(field='invalid',
                       op='eq',
                       value='20')]
        self.assertRaises(
            wsme.exc.UnknownArgument,
            api._query_to_kwargs, q, storage.SampleFilter.__init__)

    def test_sample_filter_invalid_op(self):
        q = [api.Query(field='user_id',
                       op='lt',
                       value='20')]
        self.assertRaises(
            wsme.exc.InvalidInput,
            api._query_to_kwargs, q, storage.SampleFilter.__init__)

    def test_sample_filter_timestamp_invalid_op(self):
        ts_start = timeutils.utcnow()
        q = [api.Query(field='timestamp',
                       op='eq',
                       value=str(ts_start))]
        self.assertRaises(
            wsme.exc.InvalidInput,
            api._query_to_kwargs, q, storage.SampleFilter.__init__)

    def test_sample_filter_exclude_internal(self):
        queries = [api.Query(field=f,
                             op='eq',
                             value='fake',
                             type='string')
                   for f in ['y', 'on_behalf_of', 'x']]
        with mock.patch('pecan.request') as request:
            request.headers.return_value = {'X-ProjectId': 'foobar'}
            self.assertRaises(wsme.exc.ClientSideError,
                              api._query_to_kwargs,
                              queries,
                              storage.SampleFilter.__init__,
                              internal_keys=['on_behalf_of'])

    def test_sample_filter_self_always_excluded(self):
        queries = [api.Query(field='user_id',
                             op='eq',
                             value='20')]
        with mock.patch('pecan.request') as request:
            request.headers.return_value = {'X-ProjectId': 'foobar'}
            kwargs = api._query_to_kwargs(queries,
                                          storage.SampleFilter.__init__)
            self.assertFalse('self' in kwargs)

    def test_sample_filter_translation(self):
        queries = [api.Query(field=f,
                             op='eq',
                             value='fake_%s' % f,
                             type='string') for f in ['user_id',
                                                      'project_id',
                                                      'resource_id']]
        with mock.patch('pecan.request') as request:
            request.headers.return_value = {'X-ProjectId': 'foobar'}
            kwargs = api._query_to_kwargs(queries,
                                          storage.SampleFilter.__init__)
            for o in ['user', 'project', 'resource']:
                self.assertEqual('fake_%s_id' % o, kwargs.get(o))

########NEW FILE########
__FILENAME__ = test_statistics
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Test statistics objects."""

import datetime

from ceilometer.api.controllers import v2
from ceilometer.openstack.common import test


class TestStatisticsDuration(test.BaseTestCase):

    def setUp(self):
        super(TestStatisticsDuration, self).setUp()

        # Create events relative to the range and pretend
        # that the intervening events exist.

        self.early1 = datetime.datetime(2012, 8, 27, 7, 0)
        self.early2 = datetime.datetime(2012, 8, 27, 17, 0)

        self.start = datetime.datetime(2012, 8, 28, 0, 0)

        self.middle1 = datetime.datetime(2012, 8, 28, 8, 0)
        self.middle2 = datetime.datetime(2012, 8, 28, 18, 0)

        self.end = datetime.datetime(2012, 8, 28, 23, 59)

        self.late1 = datetime.datetime(2012, 8, 29, 9, 0)
        self.late2 = datetime.datetime(2012, 8, 29, 19, 0)

    def test_nulls(self):
        s = v2.Statistics(duration_start=None,
                          duration_end=None,
                          start_timestamp=None,
                          end_timestamp=None,
                          )
        self.assertIsNone(s.duration_start)
        self.assertIsNone(s.duration_end)
        self.assertIsNone(s.duration)

    def test_overlap_range_start(self):
        s = v2.Statistics(duration_start=self.early1,
                          duration_end=self.middle1,
                          start_timestamp=self.start,
                          end_timestamp=self.end,
                          )
        self.assertEqual(self.start, s.duration_start)
        self.assertEqual(self.middle1, s.duration_end)
        self.assertEqual(8 * 60 * 60, s.duration)

    def test_within_range(self):
        s = v2.Statistics(duration_start=self.middle1,
                          duration_end=self.middle2,
                          start_timestamp=self.start,
                          end_timestamp=self.end,
                          )
        self.assertEqual(self.middle1, s.duration_start)
        self.assertEqual(self.middle2, s.duration_end)
        self.assertEqual(10 * 60 * 60, s.duration)

    def test_within_range_zero_duration(self):
        s = v2.Statistics(duration_start=self.middle1,
                          duration_end=self.middle1,
                          start_timestamp=self.start,
                          end_timestamp=self.end,
                          )
        self.assertEqual(self.middle1, s.duration_start)
        self.assertEqual(self.middle1, s.duration_end)
        self.assertEqual(0, s.duration)

    def test_overlap_range_end(self):
        s = v2.Statistics(duration_start=self.middle2,
                          duration_end=self.late1,
                          start_timestamp=self.start,
                          end_timestamp=self.end,
                          )
        self.assertEqual(self.middle2, s.duration_start)
        self.assertEqual(self.end, s.duration_end)
        self.assertEqual(((6 * 60) - 1) * 60, s.duration)

    def test_after_range(self):
        s = v2.Statistics(duration_start=self.late1,
                          duration_end=self.late2,
                          start_timestamp=self.start,
                          end_timestamp=self.end,
                          )
        self.assertIsNone(s.duration_start)
        self.assertIsNone(s.duration_end)
        self.assertIsNone(s.duration)

    def test_without_timestamp(self):
        s = v2.Statistics(duration_start=self.late1,
                          duration_end=self.late2,
                          start_timestamp=None,
                          end_timestamp=None,
                          )
        self.assertEqual(self.late1, s.duration_start)
        self.assertEqual(self.late2, s.duration_end)

########NEW FILE########
__FILENAME__ = test_statistics_scenarios
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Test events statistics retrieval."""

import datetime

from ceilometer.publisher import utils
from ceilometer import sample
from ceilometer.tests.api.v2 import FunctionalTest
from ceilometer.tests import db as tests_db


class TestMaxProjectVolume(FunctionalTest,
                           tests_db.MixinTestsWithBackendScenarios):

    PATH = '/meters/volume.size/statistics'

    def setUp(self):
        super(TestMaxProjectVolume, self).setUp()
        for i in range(3):
            s = sample.Sample(
                'volume.size',
                'gauge',
                'GiB',
                5 + i,
                'user-id',
                'project1',
                'resource-id-%s' % i,
                timestamp=datetime.datetime(2012, 9, 25, 10 + i, 30 + i),
                resource_metadata={'display_name': 'test-volume',
                                   'tag': 'self.sample',
                                   },
                source='source1',
            )
            msg = utils.meter_message_from_counter(
                s,
                self.CONF.publisher.metering_secret,
            )
            self.conn.record_metering_data(msg)

    def test_no_time_bounds(self):
        data = self.get_json(self.PATH, q=[{'field': 'project_id',
                                            'value': 'project1',
                                            }])
        self.assertEqual(7, data[0]['max'])
        self.assertEqual(3, data[0]['count'])

    def test_start_timestamp(self):
        data = self.get_json(self.PATH, q=[{'field': 'project_id',
                                            'value': 'project1',
                                            },
                                           {'field': 'timestamp',
                                            'op': 'ge',
                                            'value': '2012-09-25T11:30:00',
                                            },
                                           ])
        self.assertEqual(7, data[0]['max'])
        self.assertEqual(2, data[0]['count'])

    def test_start_timestamp_after(self):
        data = self.get_json(self.PATH, q=[{'field': 'project_id',
                                            'value': 'project1',
                                            },
                                           {'field': 'timestamp',
                                            'op': 'ge',
                                            'value': '2012-09-25T12:34:00',
                                            },
                                           ])
        self.assertEqual([], data)

    def test_end_timestamp(self):
        data = self.get_json(self.PATH, q=[{'field': 'project_id',
                                            'value': 'project1',
                                            },
                                           {'field': 'timestamp',
                                            'op': 'le',
                                            'value': '2012-09-25T11:30:00',
                                            },
                                           ])
        self.assertEqual(5, data[0]['max'])
        self.assertEqual(1, data[0]['count'])

    def test_end_timestamp_before(self):
        data = self.get_json(self.PATH, q=[{'field': 'project_id',
                                            'value': 'project1',
                                            },
                                           {'field': 'timestamp',
                                            'op': 'le',
                                            'value': '2012-09-25T09:54:00',
                                            },
                                           ])
        self.assertEqual([], data)

    def test_start_end_timestamp(self):
        data = self.get_json(self.PATH, q=[{'field': 'project_id',
                                            'value': 'project1',
                                            },
                                           {'field': 'timestamp',
                                            'op': 'ge',
                                            'value': '2012-09-25T11:30:00',
                                            },
                                           {'field': 'timestamp',
                                            'op': 'le',
                                            'value': '2012-09-25T11:32:00',
                                            },
                                           ])
        self.assertEqual(6, data[0]['max'])
        self.assertEqual(1, data[0]['count'])


class TestMaxResourceVolume(FunctionalTest,
                            tests_db.MixinTestsWithBackendScenarios):

    PATH = '/meters/volume.size/statistics'

    def setUp(self):
        super(TestMaxResourceVolume, self).setUp()
        for i in range(3):
            s = sample.Sample(
                'volume.size',
                'gauge',
                'GiB',
                5 + i,
                'user-id',
                'project1',
                'resource-id',
                timestamp=datetime.datetime(2012, 9, 25, 10 + i, 30 + i),
                resource_metadata={'display_name': 'test-volume',
                                   'tag': 'self.sample',
                                   },
                source='source1',
            )
            msg = utils.meter_message_from_counter(
                s,
                self.CONF.publisher.metering_secret,
            )
            self.conn.record_metering_data(msg)

    def test_no_time_bounds(self):
        data = self.get_json(self.PATH, q=[{'field': 'resource_id',
                                            'value': 'resource-id',
                                            }])
        self.assertEqual(7, data[0]['max'])
        self.assertEqual(3, data[0]['count'])

    def test_no_time_bounds_with_period(self):
        data = self.get_json(self.PATH,
                             q=[{'field': 'resource_id',
                                 'value': 'resource-id'}],
                             period=3600)
        self.assertEqual(3, len(data))
        self.assertEqual(set([u'2012-09-25T10:30:00',
                              u'2012-09-25T12:32:00',
                              u'2012-09-25T11:31:00']),
                         set(x['duration_start'] for x in data))
        self.assertEqual(3600, data[0]['period'])
        self.assertEqual(set([u'2012-09-25T10:30:00',
                              u'2012-09-25T11:30:00',
                              u'2012-09-25T12:30:00']),
                         set(x['period_start'] for x in data))

    def test_period_with_negative_value(self):
        resp = self.get_json(self.PATH, expect_errors=True,
                             q=[{'field': 'resource_id',
                                 'value': 'resource-id'}],
                             period=-1)
        self.assertEqual(400, resp.status_code)

    def test_start_timestamp(self):
        data = self.get_json(self.PATH, q=[{'field': 'resource_id',
                                            'value': 'resource-id',
                                            },
                                           {'field': 'timestamp',
                                            'op': 'ge',
                                            'value': '2012-09-25T11:30:00',
                                            },
                                           ])
        self.assertEqual(7, data[0]['max'])
        self.assertEqual(2, data[0]['count'])

    def test_start_timestamp_after(self):
        data = self.get_json(self.PATH, q=[{'field': 'resource_id',
                                            'value': 'resource-id',
                                            },
                                           {'field': 'timestamp',
                                            'op': 'ge',
                                            'value': '2012-09-25T12:34:00',
                                            },
                                           ])
        self.assertEqual([], data)

    def test_end_timestamp(self):
        data = self.get_json(self.PATH, q=[{'field': 'resource_id',
                                            'value': 'resource-id',
                                            },
                                           {'field': 'timestamp',
                                            'op': 'le',
                                            'value': '2012-09-25T11:30:00',
                                            },
                                           ])
        self.assertEqual(5, data[0]['max'])
        self.assertEqual(1, data[0]['count'])

    def test_end_timestamp_before(self):
        data = self.get_json(self.PATH, q=[{'field': 'resource_id',
                                            'value': 'resource-id',
                                            },
                                           {'field': 'timestamp',
                                            'op': 'le',
                                            'value': '2012-09-25T09:54:00',
                                            },
                                           ])
        self.assertEqual([], data)

    def test_start_end_timestamp(self):
        data = self.get_json(self.PATH, q=[{'field': 'resource_id',
                                            'value': 'resource-id',
                                            },
                                           {'field': 'timestamp',
                                            'op': 'ge',
                                            'value': '2012-09-25T11:30:00',
                                            },
                                           {'field': 'timestamp',
                                            'op': 'le',
                                            'value': '2012-09-25T11:32:00',
                                            },
                                           ])
        self.assertEqual(6, data[0]['max'])
        self.assertEqual(1, data[0]['count'])


class TestSumProjectVolume(FunctionalTest,
                           tests_db.MixinTestsWithBackendScenarios):

    PATH = '/meters/volume.size/statistics'

    def setUp(self):
        super(TestSumProjectVolume, self).setUp()
        for i in range(3):
            s = sample.Sample(
                'volume.size',
                'gauge',
                'GiB',
                5 + i,
                'user-id',
                'project1',
                'resource-id-%s' % i,
                timestamp=datetime.datetime(2012, 9, 25, 10 + i, 30 + i),
                resource_metadata={'display_name': 'test-volume',
                                   'tag': 'self.sample',
                                   },
                source='source1',
            )
            msg = utils.meter_message_from_counter(
                s,
                self.CONF.publisher.metering_secret,
            )
            self.conn.record_metering_data(msg)

    def test_no_time_bounds(self):
        data = self.get_json(self.PATH, q=[{'field': 'project_id',
                                            'value': 'project1',
                                            }])
        expected = 5 + 6 + 7
        self.assertEqual(expected, data[0]['sum'])
        self.assertEqual(3, data[0]['count'])

    def test_start_timestamp(self):
        data = self.get_json(self.PATH, q=[{'field': 'project_id',
                                            'value': 'project1',
                                            },
                                           {'field': 'timestamp',
                                            'op': 'ge',
                                            'value': '2012-09-25T11:30:00',
                                            },
                                           ])
        expected = 6 + 7
        self.assertEqual(expected, data[0]['sum'])
        self.assertEqual(2, data[0]['count'])

    def test_start_timestamp_after(self):
        data = self.get_json(self.PATH, q=[{'field': 'project_id',
                                            'value': 'project1',
                                            },
                                           {'field': 'timestamp',
                                            'op': 'ge',
                                            'value': '2012-09-25T12:34:00',
                                            },
                                           ])
        self.assertEqual([], data)

    def test_end_timestamp(self):
        data = self.get_json(self.PATH, q=[{'field': 'project_id',
                                            'value': 'project1',
                                            },
                                           {'field': 'timestamp',
                                            'op': 'le',
                                            'value': '2012-09-25T11:30:00',
                                            },
                                           ])
        self.assertEqual(5, data[0]['sum'])
        self.assertEqual(1, data[0]['count'])

    def test_end_timestamp_before(self):
        data = self.get_json(self.PATH, q=[{'field': 'project_id',
                                            'value': 'project1',
                                            },
                                           {'field': 'timestamp',
                                            'op': 'le',
                                            'value': '2012-09-25T09:54:00',
                                            },
                                           ])
        self.assertEqual([], data)

    def test_start_end_timestamp(self):
        data = self.get_json(self.PATH, q=[{'field': 'project_id',
                                            'value': 'project1',
                                            },
                                           {'field': 'timestamp',
                                            'op': 'ge',
                                            'value': '2012-09-25T11:30:00',
                                            },
                                           {'field': 'timestamp',
                                            'op': 'le',
                                            'value': '2012-09-25T11:32:00',
                                            },
                                           ])
        self.assertEqual(6, data[0]['sum'])
        self.assertEqual(1, data[0]['count'])


class TestSumResourceVolume(FunctionalTest,
                            tests_db.MixinTestsWithBackendScenarios):

    PATH = '/meters/volume.size/statistics'

    def setUp(self):
        super(TestSumResourceVolume, self).setUp()
        for i in range(3):
            s = sample.Sample(
                'volume.size',
                'gauge',
                'GiB',
                5 + i,
                'user-id',
                'project1',
                'resource-id',
                timestamp=datetime.datetime(2012, 9, 25, 10 + i, 30 + i),
                resource_metadata={'display_name': 'test-volume',
                                   'tag': 'self.sample',
                                   },
                source='source1',
            )
            msg = utils.meter_message_from_counter(
                s,
                self.CONF.publisher.metering_secret,
            )
            self.conn.record_metering_data(msg)

    def test_no_time_bounds(self):
        data = self.get_json(self.PATH, q=[{'field': 'resource_id',
                                            'value': 'resource-id',
                                            }])
        self.assertEqual(5 + 6 + 7, data[0]['sum'])
        self.assertEqual(3, data[0]['count'])

    def test_no_time_bounds_with_period(self):
        data = self.get_json(self.PATH,
                             q=[{'field': 'resource_id',
                                 'value': 'resource-id'}],
                             period=1800)
        self.assertEqual(3, len(data))
        self.assertEqual(set([u'2012-09-25T10:30:00',
                              u'2012-09-25T12:32:00',
                              u'2012-09-25T11:31:00']),
                         set(x['duration_start'] for x in data))
        self.assertEqual(1800, data[0]['period'])
        self.assertEqual(set([u'2012-09-25T10:30:00',
                              u'2012-09-25T11:30:00',
                              u'2012-09-25T12:30:00']),
                         set(x['period_start'] for x in data))

    def test_start_timestamp(self):
        data = self.get_json(self.PATH, q=[{'field': 'resource_id',
                                            'value': 'resource-id',
                                            },
                                           {'field': 'timestamp',
                                            'op': 'ge',
                                            'value': '2012-09-25T11:30:00',
                                            }])
        self.assertEqual(6 + 7, data[0]['sum'])
        self.assertEqual(2, data[0]['count'])

    def test_start_timestamp_with_period(self):
        data = self.get_json(self.PATH,
                             q=[{'field': 'resource_id',
                                 'value': 'resource-id'},
                                {'field': 'timestamp',
                                 'op': 'ge',
                                 'value': '2012-09-25T10:15:00'}],
                             period=7200)
        self.assertEqual(2, len(data))
        self.assertEqual(set([u'2012-09-25T10:30:00',
                              u'2012-09-25T12:32:00']),
                         set(x['duration_start'] for x in data))
        self.assertEqual(7200, data[0]['period'])
        self.assertEqual(set([u'2012-09-25T10:15:00',
                              u'2012-09-25T12:15:00']),
                         set(x['period_start'] for x in data))

    def test_start_timestamp_after(self):
        data = self.get_json(self.PATH, q=[{'field': 'resource_id',
                                            'value': 'resource-id',
                                            },
                                           {'field': 'timestamp',
                                            'op': 'ge',
                                            'value': '2012-09-25T12:34:00',
                                            }])
        self.assertEqual([], data)

    def test_end_timestamp(self):
        data = self.get_json(self.PATH, q=[{'field': 'resource_id',
                                            'value': 'resource-id',
                                            },
                                           {'field': 'timestamp',
                                            'op': 'le',
                                            'value': '2012-09-25T11:30:00',
                                            }])
        self.assertEqual(5, data[0]['sum'])
        self.assertEqual(1, data[0]['count'])

    def test_end_timestamp_before(self):
        data = self.get_json(self.PATH, q=[{'field': 'resource_id',
                                            'value': 'resource-id',
                                            },
                                           {'field': 'timestamp',
                                            'op': 'le',
                                            'value': '2012-09-25T09:54:00',
                                            }])
        self.assertEqual([], data)

    def test_start_end_timestamp(self):
        data = self.get_json(self.PATH, q=[{'field': 'resource_id',
                                            'value': 'resource-id',
                                            },
                                           {'field': 'timestamp',
                                            'op': 'ge',
                                            'value': '2012-09-25T11:30:00',
                                            },
                                           {'field': 'timestamp',
                                            'op': 'lt',
                                            'value': '2012-09-25T11:32:00',
                                            }])
        self.assertEqual(6, data[0]['sum'])
        self.assertEqual(1, data[0]['count'])


class TestGroupByInstance(FunctionalTest,
                          tests_db.MixinTestsWithBackendScenarios):

    PATH = '/meters/instance/statistics'

    def setUp(self):
        super(TestGroupByInstance, self).setUp()

        test_sample_data = (
            {'volume': 2, 'user': 'user-1', 'project': 'project-1',
             'resource': 'resource-1', 'timestamp': (2013, 8, 1, 16, 10),
             'metadata_flavor': 'm1.tiny', 'metadata_event': 'event-1',
             'source': 'source-2'},
            {'volume': 2, 'user': 'user-1', 'project': 'project-2',
             'resource': 'resource-1', 'timestamp': (2013, 8, 1, 15, 37),
             'metadata_flavor': 'm1.large', 'metadata_event': 'event-1',
             'source': 'source-2'},
            {'volume': 1, 'user': 'user-2', 'project': 'project-1',
             'resource': 'resource-2', 'timestamp': (2013, 8, 1, 10, 11),
             'metadata_flavor': 'm1.tiny', 'metadata_event': 'event-2',
             'source': 'source-1'},
            {'volume': 1, 'user': 'user-2', 'project': 'project-1',
             'resource': 'resource-2', 'timestamp': (2013, 8, 1, 10, 40),
             'metadata_flavor': 'm1.large', 'metadata_event': 'event-2',
             'source': 'source-1'},
            {'volume': 2, 'user': 'user-2', 'project': 'project-1',
             'resource': 'resource-1', 'timestamp': (2013, 8, 1, 14, 59),
             'metadata_flavor': 'm1.large', 'metadata_event': 'event-2',
             'source': 'source-1'},
            {'volume': 4, 'user': 'user-2', 'project': 'project-2',
             'resource': 'resource-2', 'timestamp': (2013, 8, 1, 17, 28),
             'metadata_flavor': 'm1.large', 'metadata_event': 'event-2',
             'source': 'source-1'},
            {'volume': 4, 'user': 'user-3', 'project': 'project-1',
             'resource': 'resource-3', 'timestamp': (2013, 8, 1, 11, 22),
             'metadata_flavor': 'm1.tiny', 'metadata_event': 'event-2',
             'source': 'source-3'},
        )

        for test_sample in test_sample_data:
            c = sample.Sample(
                'instance',
                sample.TYPE_CUMULATIVE,
                unit='s',
                volume=test_sample['volume'],
                user_id=test_sample['user'],
                project_id=test_sample['project'],
                resource_id=test_sample['resource'],
                timestamp=datetime.datetime(*test_sample['timestamp']),
                resource_metadata={'flavor': test_sample['metadata_flavor'],
                                   'event': test_sample['metadata_event'], },
                source=test_sample['source'],
            )
            msg = utils.meter_message_from_counter(
                c,
                self.CONF.publisher.metering_secret,
            )
            self.conn.record_metering_data(msg)

    def test_group_by_user(self):
        data = self.get_json(self.PATH, groupby=['user_id'])
        groupby_keys_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].keys())
        groupby_vals_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].values())
        self.assertEqual(set(['user_id']), groupby_keys_set)
        self.assertEqual(set(['user-1', 'user-2', 'user-3']), groupby_vals_set)

        for r in data:
            grp = r['groupby']
            if grp == {'user_id': 'user-1'}:
                self.assertEqual(2, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(2, r['min'])
                self.assertEqual(2, r['max'])
                self.assertEqual(4, r['sum'])
                self.assertEqual(2, r['avg'])
            elif grp == {'user_id': 'user-2'}:
                self.assertEqual(4, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(1, r['min'])
                self.assertEqual(4, r['max'])
                self.assertEqual(8, r['sum'])
                self.assertEqual(2, r['avg'])
            elif grp == {'user_id': 'user-3'}:
                self.assertEqual(1, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(4, r['min'])
                self.assertEqual(4, r['max'])
                self.assertEqual(4, r['sum'])
                self.assertEqual(4, r['avg'])

    def test_group_by_resource(self):
        data = self.get_json(self.PATH, groupby=['resource_id'])
        groupby_keys_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].keys())
        groupby_vals_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].values())
        self.assertEqual(set(['resource_id']), groupby_keys_set)
        self.assertEqual(set(['resource-1', 'resource-2', 'resource-3']),
                         groupby_vals_set)

        for r in data:
            grp = r['groupby']
            if grp == {'resource_id': 'resource-1'}:
                self.assertEqual(3, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(2, r['min'])
                self.assertEqual(2, r['max'])
                self.assertEqual(6, r['sum'])
                self.assertEqual(2, r['avg'])
            elif grp == {'resource_id': 'resource-2'}:
                self.assertEqual(3, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(1, r['min'])
                self.assertEqual(4, r['max'])
                self.assertEqual(6, r['sum'])
                self.assertEqual(2, r['avg'])
            elif grp == {'resource_id': 'resource-3'}:
                self.assertEqual(1, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(4, r['min'])
                self.assertEqual(4, r['max'])
                self.assertEqual(4, r['sum'])
                self.assertEqual(4, r['avg'])

    def test_group_by_project(self):
        data = self.get_json(self.PATH, groupby=['project_id'])
        groupby_keys_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].keys())
        groupby_vals_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].values())
        self.assertEqual(set(['project_id']), groupby_keys_set)
        self.assertEqual(set(['project-1', 'project-2']), groupby_vals_set)

        for r in data:
            grp = r['groupby']
            if grp == {'project_id': 'project-1'}:
                self.assertEqual(5, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(1, r['min'])
                self.assertEqual(4, r['max'])
                self.assertEqual(10, r['sum'])
                self.assertEqual(2, r['avg'])
            elif grp == {'project_id': 'project-2'}:
                self.assertEqual(2, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(2, r['min'])
                self.assertEqual(4, r['max'])
                self.assertEqual(6, r['sum'])
                self.assertEqual(3, r['avg'])

    def test_group_by_unknown_field(self):
        response = self.get_json(self.PATH,
                                 expect_errors=True,
                                 groupby=['wtf'])
        self.assertEqual(400, response.status_code)

    def test_group_by_multiple_regular(self):
        data = self.get_json(self.PATH, groupby=['user_id', 'resource_id'])
        groupby_keys_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].keys())
        groupby_vals_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].values())
        self.assertEqual(set(['user_id', 'resource_id']), groupby_keys_set)
        self.assertEqual(set(['user-1', 'user-2', 'user-3', 'resource-1',
                              'resource-2', 'resource-3']),
                         groupby_vals_set)

        for r in data:
            grp = r['groupby']
            if grp == {'user_id': 'user-1',
                                  'resource_id': 'resource-1'}:
                self.assertEqual(2, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(2, r['min'])
                self.assertEqual(2, r['max'])
                self.assertEqual(4, r['sum'])
                self.assertEqual(2, r['avg'])
            elif grp == {'user_id': 'user-2',
                         'resource_id': 'resource-1'}:
                self.assertEqual(1, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(2, r['min'])
                self.assertEqual(2, r['max'])
                self.assertEqual(2, r['sum'])
                self.assertEqual(2, r['avg'])
            elif grp == {'user_id': 'user-2',
                         'resource_id': 'resource-2'}:
                self.assertEqual(3, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(1, r['min'])
                self.assertEqual(4, r['max'])
                self.assertEqual(6, r['sum'])
                self.assertEqual(2, r['avg'])
            elif grp == {'user_id': 'user-3',
                         'resource_id': 'resource-3'}:
                self.assertEqual(1, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(4, r['min'])
                self.assertEqual(4, r['max'])
                self.assertEqual(4, r['sum'])
                self.assertEqual(4, r['avg'])
            else:
                self.assertNotEqual(grp, {'user_id': 'user-1',
                                          'resource_id': 'resource-2'})
                self.assertNotEqual(grp, {'user_id': 'user-1',
                                          'resource_id': 'resource-3'})
                self.assertNotEqual(grp, {'user_id': 'user-2',
                                          'resource_id': 'resource-3'})
                self.assertNotEqual(grp, {'user_id': 'user-3',
                                          'resource_id': 'resource-1'})
                self.assertNotEqual(grp, {'user_id': 'user-3',
                                          'resource_id': 'resource-2'})

    def test_group_by_with_query_filter(self):
        data = self.get_json(self.PATH,
                             q=[{'field': 'project_id',
                                 'op': 'eq',
                                 'value': 'project-1'}],
                             groupby=['resource_id'])
        groupby_keys_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].keys())
        groupby_vals_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].values())
        self.assertEqual(set(['resource_id']), groupby_keys_set)
        self.assertEqual(set(['resource-1', 'resource-2', 'resource-3']),
                         groupby_vals_set)

        for r in data:
            grp = r['groupby']
            if grp == {'resource_id': 'resource-1'}:
                self.assertEqual(2, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(2, r['min'])
                self.assertEqual(2, r['max'])
                self.assertEqual(4, r['sum'])
                self.assertEqual(2, r['avg'])
            elif grp == {'resource_id': 'resource-2'}:
                self.assertEqual(2, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(1, r['min'])
                self.assertEqual(1, r['max'])
                self.assertEqual(2, r['sum'])
                self.assertEqual(1, r['avg'])
            elif grp == {'resource_id': 'resource-3'}:
                self.assertEqual(1, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(4, r['min'])
                self.assertEqual(4, r['max'])
                self.assertEqual(4, r['sum'])
                self.assertEqual(4, r['avg'])

    def test_group_by_with_query_filter_multiple(self):
        data = self.get_json(self.PATH,
                             q=[{'field': 'user_id',
                                 'op': 'eq',
                                 'value': 'user-2'},
                                {'field': 'source',
                                 'op': 'eq',
                                 'value': 'source-1'}],
                             groupby=['project_id', 'resource_id'])
        groupby_keys_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].keys())
        groupby_vals_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].values())
        self.assertEqual(set(['project_id', 'resource_id']), groupby_keys_set)
        self.assertEqual(set(['project-1', 'project-2',
                              'resource-1', 'resource-2']),
                         groupby_vals_set)

        for r in data:
            grp = r['groupby']
            if grp == {'project_id': 'project-1',
                       'resource_id': 'resource-1'}:
                self.assertEqual(1, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(2, r['min'])
                self.assertEqual(2, r['max'])
                self.assertEqual(2, r['sum'])
                self.assertEqual(2, r['avg'])
            elif grp == {'project_id': 'project-1',
                         'resource_id': 'resource-2'}:
                self.assertEqual(2, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(1, r['min'])
                self.assertEqual(1, r['max'])
                self.assertEqual(2, r['sum'])
                self.assertEqual(1, r['avg'])
            elif grp == {'project_id': 'project-2',
                         'resource_id': 'resource-2'}:
                self.assertEqual(1, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(4, r['min'])
                self.assertEqual(4, r['max'])
                self.assertEqual(4, r['sum'])
                self.assertEqual(4, r['avg'])
            else:
                self.assertNotEqual(grp, {'project_id': 'project-2',
                                          'resource_id': 'resource-1'})

    def test_group_by_with_period(self):
        data = self.get_json(self.PATH,
                             groupby=['project_id'],
                             period=7200)
        groupby_keys_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].keys())
        groupby_vals_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].values())
        self.assertEqual(set(['project_id']), groupby_keys_set)
        self.assertEqual(set(['project-1', 'project-2']), groupby_vals_set)
        period_start_set = set(sub_dict['period_start'] for sub_dict in data)
        period_start_valid = set([u'2013-08-01T10:11:00',
                                  u'2013-08-01T14:11:00',
                                  u'2013-08-01T16:11:00'])
        self.assertEqual(period_start_valid, period_start_set)

        for r in data:
            grp = r['groupby']
            period_start = r['period_start']
            if (grp == {'project_id': 'project-1'} and
                    period_start == u'2013-08-01T10:11:00'):
                self.assertEqual(3, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(1, r['min'])
                self.assertEqual(4, r['max'])
                self.assertEqual(6, r['sum'])
                self.assertEqual(2, r['avg'])
                self.assertEqual(4260, r['duration'])
                self.assertEqual(u'2013-08-01T10:11:00', r['duration_start'])
                self.assertEqual(u'2013-08-01T11:22:00', r['duration_end'])
                self.assertEqual(7200, r['period'])
                self.assertEqual(u'2013-08-01T12:11:00', r['period_end'])
            elif (grp == {'project_id': 'project-1'} and
                    period_start == u'2013-08-01T14:11:00'):
                self.assertEqual(2, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(2, r['min'])
                self.assertEqual(2, r['max'])
                self.assertEqual(4, r['sum'])
                self.assertEqual(2, r['avg'])
                self.assertEqual(4260, r['duration'])
                self.assertEqual(u'2013-08-01T14:59:00', r['duration_start'])
                self.assertEqual(u'2013-08-01T16:10:00', r['duration_end'])
                self.assertEqual(7200, r['period'])
                self.assertEqual(u'2013-08-01T16:11:00', r['period_end'])
            elif (grp == {'project_id': 'project-2'} and
                    period_start == u'2013-08-01T14:11:00'):
                self.assertEqual(1, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(2, r['min'])
                self.assertEqual(2, r['max'])
                self.assertEqual(2, r['sum'])
                self.assertEqual(2, r['avg'])
                self.assertEqual(0, r['duration'])
                self.assertEqual(u'2013-08-01T15:37:00', r['duration_start'])
                self.assertEqual(u'2013-08-01T15:37:00', r['duration_end'])
                self.assertEqual(7200, r['period'])
                self.assertEqual(u'2013-08-01T16:11:00', r['period_end'])
            elif (grp == {'project_id': 'project-2'} and
                    period_start == u'2013-08-01T16:11:00'):
                self.assertEqual(1, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(4, r['min'])
                self.assertEqual(4, r['max'])
                self.assertEqual(4, r['sum'])
                self.assertEqual(4, r['avg'])
                self.assertEqual(0, r['duration'])
                self.assertEqual(u'2013-08-01T17:28:00', r['duration_start'])
                self.assertEqual(u'2013-08-01T17:28:00', r['duration_end'])
                self.assertEqual(7200, r['period'])
                self.assertEqual(u'2013-08-01T18:11:00', r['period_end'])
            else:
                self.assertNotEqual([grp, period_start],
                                    [{'project_id': 'project-1'},
                                     u'2013-08-01T16:11:00'])
                self.assertNotEqual([grp, period_start],
                                    [{'project_id': 'project-2'},
                                     u'2013-08-01T10:11:00'])

    def test_group_by_with_query_filter_and_period(self):
        data = self.get_json(self.PATH,
                             q=[{'field': 'source',
                                 'op': 'eq',
                                 'value': 'source-1'}],
                             groupby=['project_id'],
                             period=7200)
        groupby_keys_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].keys())
        groupby_vals_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].values())
        self.assertEqual(set(['project_id']), groupby_keys_set)
        self.assertEqual(set(['project-1', 'project-2']), groupby_vals_set)
        period_start_set = set(sub_dict['period_start'] for sub_dict in data)
        period_start_valid = set([u'2013-08-01T10:11:00',
                                  u'2013-08-01T14:11:00',
                                  u'2013-08-01T16:11:00'])
        self.assertEqual(period_start_valid, period_start_set)

        for r in data:
            grp = r['groupby']
            period_start = r['period_start']
            if (grp == {'project_id': 'project-1'} and
                    period_start == u'2013-08-01T10:11:00'):
                self.assertEqual(2, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(1, r['min'])
                self.assertEqual(1, r['max'])
                self.assertEqual(2, r['sum'])
                self.assertEqual(1, r['avg'])
                self.assertEqual(1740, r['duration'])
                self.assertEqual(u'2013-08-01T10:11:00', r['duration_start'])
                self.assertEqual(u'2013-08-01T10:40:00', r['duration_end'])
                self.assertEqual(7200, r['period'])
                self.assertEqual(u'2013-08-01T12:11:00', r['period_end'])
            elif (grp == {'project_id': 'project-1'} and
                    period_start == u'2013-08-01T14:11:00'):
                self.assertEqual(1, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(2, r['min'])
                self.assertEqual(2, r['max'])
                self.assertEqual(2, r['sum'])
                self.assertEqual(2, r['avg'])
                self.assertEqual(0, r['duration'])
                self.assertEqual(u'2013-08-01T14:59:00', r['duration_start'])
                self.assertEqual(u'2013-08-01T14:59:00', r['duration_end'])
                self.assertEqual(7200, r['period'])
                self.assertEqual(u'2013-08-01T16:11:00', r['period_end'])
            elif (grp == {'project_id': 'project-2'} and
                    period_start == u'2013-08-01T16:11:00'):
                self.assertEqual(1, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(4, r['min'])
                self.assertEqual(4, r['max'])
                self.assertEqual(4, r['sum'])
                self.assertEqual(4, r['avg'])
                self.assertEqual(0, r['duration'])
                self.assertEqual(u'2013-08-01T17:28:00', r['duration_start'])
                self.assertEqual(u'2013-08-01T17:28:00', r['duration_end'])
                self.assertEqual(7200, r['period'])
                self.assertEqual(u'2013-08-01T18:11:00', r['period_end'])
            else:
                self.assertNotEqual([grp, period_start],
                                    [{'project_id': 'project-1'},
                                     u'2013-08-01T16:11:00'])
                self.assertNotEqual([grp, period_start],
                                    [{'project_id': 'project-2'},
                                     u'2013-08-01T10:11:00'])
                self.assertNotEqual([grp, period_start],
                                    [{'project_id': 'project-2'},
                                     u'2013-08-01T14:11:00'])

    def test_group_by_start_timestamp_after(self):
        data = self.get_json(self.PATH,
                             q=[{'field': 'timestamp',
                                 'op': 'ge',
                                 'value': '2013-08-01T17:28:01'}],
                             groupby=['project_id'])
        self.assertEqual([], data)

    def test_group_by_end_timestamp_before(self):
        data = self.get_json(self.PATH,
                             q=[{'field': 'timestamp',
                                 'op': 'le',
                                 'value': '2013-08-01T10:10:59'}],
                             groupby=['project_id'])
        self.assertEqual([], data)

    def test_group_by_start_timestamp(self):
        data = self.get_json(self.PATH,
                             q=[{'field': 'timestamp',
                                 'op': 'ge',
                                 'value': '2013-08-01T14:58:00'}],
                             groupby=['project_id'])
        groupby_keys_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].keys())
        groupby_vals_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].values())
        self.assertEqual(set(['project_id']), groupby_keys_set)
        self.assertEqual(set(['project-1', 'project-2']), groupby_vals_set)

        for r in data:
            grp = r['groupby']
            if grp == {'project_id': 'project-1'}:
                self.assertEqual(2, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(2, r['min'])
                self.assertEqual(2, r['max'])
                self.assertEqual(4, r['sum'])
                self.assertEqual(2, r['avg'])
            elif grp == {'project_id': 'project-2'}:
                self.assertEqual(2, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(2, r['min'])
                self.assertEqual(4, r['max'])
                self.assertEqual(6, r['sum'])
                self.assertEqual(3, r['avg'])

    def test_group_by_end_timestamp(self):
        data = self.get_json(self.PATH,
                             q=[{'field': 'timestamp',
                                 'op': 'le',
                                 'value': '2013-08-01T11:45:00'}],
                             groupby=['project_id'])
        groupby_keys_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].keys())
        groupby_vals_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].values())
        self.assertEqual(set(['project_id']), groupby_keys_set)
        self.assertEqual(set(['project-1']), groupby_vals_set)

        for r in data:
            grp = r['groupby']
            if grp == {'project_id': 'project-1'}:
                self.assertEqual(3, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(1, r['min'])
                self.assertEqual(4, r['max'])
                self.assertEqual(6, r['sum'])
                self.assertEqual(2, r['avg'])

    def test_group_by_start_end_timestamp(self):
        data = self.get_json(self.PATH,
                             q=[{'field': 'timestamp',
                                 'op': 'ge',
                                 'value': '2013-08-01T08:17:03'},
                                {'field': 'timestamp',
                                 'op': 'le',
                                 'value': '2013-08-01T23:59:59'}],
                             groupby=['project_id'])
        groupby_keys_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].keys())
        groupby_vals_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].values())
        self.assertEqual(set(['project_id']), groupby_keys_set)
        self.assertEqual(set(['project-1', 'project-2']), groupby_vals_set)

        for r in data:
            grp = r['groupby']
            if grp == {'project_id': 'project-1'}:
                self.assertEqual(5, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(1, r['min'])
                self.assertEqual(4, r['max'])
                self.assertEqual(10, r['sum'])
                self.assertEqual(2, r['avg'])
            elif grp == {'project_id': 'project-2'}:
                self.assertEqual(2, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(2, r['min'])
                self.assertEqual(4, r['max'])
                self.assertEqual(6, r['sum'])
                self.assertEqual(3, r['avg'])

    def test_group_by_start_end_timestamp_with_query_filter(self):
        data = self.get_json(self.PATH,
                             q=[{'field': 'project_id',
                                 'op': 'eq',
                                 'value': 'project-1'},
                                {'field': 'timestamp',
                                 'op': 'ge',
                                 'value': '2013-08-01T11:01:00'},
                                {'field': 'timestamp',
                                 'op': 'le',
                                 'value': '2013-08-01T20:00:00'}],
                             groupby=['resource_id'])
        groupby_keys_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].keys())
        groupby_vals_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].values())
        self.assertEqual(set(['resource_id']), groupby_keys_set)
        self.assertEqual(set(['resource-1', 'resource-3']), groupby_vals_set)

        for r in data:
            grp = r['groupby']
            if grp == {'resource_id': 'resource-1'}:
                self.assertEqual(2, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(2, r['min'])
                self.assertEqual(2, r['max'])
                self.assertEqual(4, r['sum'])
                self.assertEqual(2, r['avg'])
            elif grp == {'resource_id': 'resource-3'}:
                self.assertEqual(1, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(4, r['min'])
                self.assertEqual(4, r['max'])
                self.assertEqual(4, r['sum'])
                self.assertEqual(4, r['avg'])

    def test_group_by_start_end_timestamp_with_period(self):
        data = self.get_json(self.PATH,
                             q=[{'field': 'timestamp',
                                 'op': 'ge',
                                 'value': '2013-08-01T14:00:00'},
                                {'field': 'timestamp',
                                 'op': 'le',
                                 'value': '2013-08-01T17:00:00'}],
                             groupby=['project_id'],
                             period=3600)
        groupby_keys_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].keys())
        groupby_vals_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].values())
        self.assertEqual(set(['project_id']), groupby_keys_set)
        self.assertEqual(set(['project-1', 'project-2']), groupby_vals_set)
        period_start_set = set(sub_dict['period_start'] for sub_dict in data)
        period_start_valid = set([u'2013-08-01T14:00:00',
                                  u'2013-08-01T15:00:00',
                                  u'2013-08-01T16:00:00'])
        self.assertEqual(period_start_valid, period_start_set)

        for r in data:
            grp = r['groupby']
            period_start = r['period_start']
            if (grp == {'project_id': 'project-1'} and
                    period_start == u'2013-08-01T14:00:00'):
                self.assertEqual(1, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(2, r['min'])
                self.assertEqual(2, r['max'])
                self.assertEqual(2, r['sum'])
                self.assertEqual(2, r['avg'])
                self.assertEqual(0, r['duration'])
                self.assertEqual(u'2013-08-01T14:59:00', r['duration_start'])
                self.assertEqual(u'2013-08-01T14:59:00', r['duration_end'])
                self.assertEqual(3600, r['period'])
                self.assertEqual(u'2013-08-01T15:00:00', r['period_end'])
            elif (grp == {'project_id': 'project-1'} and
                    period_start == u'2013-08-01T16:00:00'):
                self.assertEqual(1, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(2, r['min'])
                self.assertEqual(2, r['max'])
                self.assertEqual(2, r['sum'])
                self.assertEqual(2, r['avg'])
                self.assertEqual(0, r['duration'])
                self.assertEqual(u'2013-08-01T16:10:00', r['duration_start'])
                self.assertEqual(u'2013-08-01T16:10:00', r['duration_end'])
                self.assertEqual(3600, r['period'])
                self.assertEqual(u'2013-08-01T17:00:00', r['period_end'])
            elif (grp == {'project_id': 'project-2'} and
                    period_start == u'2013-08-01T15:00:00'):
                self.assertEqual(1, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(2, r['min'])
                self.assertEqual(2, r['max'])
                self.assertEqual(2, r['sum'])
                self.assertEqual(2, r['avg'])
                self.assertEqual(0, r['duration'])
                self.assertEqual(u'2013-08-01T15:37:00', r['duration_start'])
                self.assertEqual(u'2013-08-01T15:37:00', r['duration_end'])
                self.assertEqual(3600, r['period'])
                self.assertEqual(u'2013-08-01T16:00:00', r['period_end'])
            else:
                self.assertNotEqual([grp, period_start],
                                    [{'project_id': 'project-1'},
                                     u'2013-08-01T15:00:00'])
                self.assertNotEqual([grp, period_start],
                                    [{'project_id': 'project-2'},
                                     u'2013-08-01T14:00:00'])
                self.assertNotEqual([grp, period_start],
                                    [{'project_id': 'project-2'},
                                     u'2013-08-01T16:00:00'])

    def test_group_by_start_end_timestamp_with_query_filter_and_period(self):
        data = self.get_json(self.PATH,
                             q=[{'field': 'source',
                                 'op': 'eq',
                                 'value': 'source-1'},
                                {'field': 'timestamp',
                                 'op': 'ge',
                                 'value': '2013-08-01T10:00:00'},
                                {'field': 'timestamp',
                                 'op': 'le',
                                 'value': '2013-08-01T18:00:00'}],
                             groupby=['project_id'],
                             period=7200)
        groupby_keys_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].keys())
        groupby_vals_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].values())
        self.assertEqual(set(['project_id']), groupby_keys_set)
        self.assertEqual(set(['project-1', 'project-2']), groupby_vals_set)
        period_start_set = set(sub_dict['period_start'] for sub_dict in data)
        period_start_valid = set([u'2013-08-01T10:00:00',
                                  u'2013-08-01T14:00:00',
                                  u'2013-08-01T16:00:00'])
        self.assertEqual(period_start_valid, period_start_set)

        for r in data:
            grp = r['groupby']
            period_start = r['period_start']
            if (grp == {'project_id': 'project-1'} and
                    period_start == u'2013-08-01T10:00:00'):
                self.assertEqual(2, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(1, r['min'])
                self.assertEqual(1, r['max'])
                self.assertEqual(2, r['sum'])
                self.assertEqual(1, r['avg'])
                self.assertEqual(1740, r['duration'])
                self.assertEqual(u'2013-08-01T10:11:00', r['duration_start'])
                self.assertEqual(u'2013-08-01T10:40:00', r['duration_end'])
                self.assertEqual(7200, r['period'])
                self.assertEqual(u'2013-08-01T12:00:00', r['period_end'])
            elif (grp == {'project_id': 'project-1'} and
                    period_start == u'2013-08-01T14:00:00'):
                self.assertEqual(1, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(2, r['min'])
                self.assertEqual(2, r['max'])
                self.assertEqual(2, r['sum'])
                self.assertEqual(2, r['avg'])
                self.assertEqual(0, r['duration'])
                self.assertEqual(u'2013-08-01T14:59:00', r['duration_start'])
                self.assertEqual(u'2013-08-01T14:59:00', r['duration_end'])
                self.assertEqual(7200, r['period'])
                self.assertEqual(u'2013-08-01T16:00:00', r['period_end'])
            elif (grp == {'project_id': 'project-2'} and
                    period_start == u'2013-08-01T16:00:00'):
                self.assertEqual(1, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(4, r['min'])
                self.assertEqual(4, r['max'])
                self.assertEqual(4, r['sum'])
                self.assertEqual(4, r['avg'])
                self.assertEqual(0, r['duration'])
                self.assertEqual(u'2013-08-01T17:28:00', r['duration_start'])
                self.assertEqual(u'2013-08-01T17:28:00', r['duration_end'])
                self.assertEqual(7200, r['period'])
                self.assertEqual(u'2013-08-01T18:00:00', r['period_end'])
            else:
                self.assertNotEqual([grp, period_start],
                                    [{'project_id': 'project-1'},
                                     u'2013-08-01T16:00:00'])
                self.assertNotEqual([grp, period_start],
                                    [{'project_id': 'project-2'},
                                     u'2013-08-01T10:00:00'])
                self.assertNotEqual([grp, period_start],
                                    [{'project_id': 'project-2'},
                                     u'2013-08-01T14:00:00'])


class TestGroupBySource(FunctionalTest,
                        tests_db.MixinTestsWithBackendScenarios):

    # FIXME(terriyu): We have to put test_group_by_source in its own class
    # because SQLAlchemy currently doesn't support group by source statistics.
    # When group by source is supported in SQLAlchemy, this test should be
    # moved to TestGroupByInstance with all the other group by statistics
    # tests.

    scenarios = [
        ('mongodb', {'db_manager': tests_db.MongoDbManager()}),
        ('hbase', {'db_manager': tests_db.HBaseManager()}),
        ('db2', {'db_manager': tests_db.DB2Manager()}),
    ]

    PATH = '/meters/instance/statistics'

    def setUp(self):
        super(TestGroupBySource, self).setUp()

        test_sample_data = (
            {'volume': 2, 'user': 'user-1', 'project': 'project-1',
             'resource': 'resource-1', 'timestamp': (2013, 8, 1, 16, 10),
             'metadata_flavor': 'm1.tiny', 'metadata_event': 'event-1',
             'source': 'source-2'},
            {'volume': 2, 'user': 'user-1', 'project': 'project-2',
             'resource': 'resource-1', 'timestamp': (2013, 8, 1, 15, 37),
             'metadata_flavor': 'm1.large', 'metadata_event': 'event-1',
             'source': 'source-2'},
            {'volume': 1, 'user': 'user-2', 'project': 'project-1',
             'resource': 'resource-2', 'timestamp': (2013, 8, 1, 10, 11),
             'metadata_flavor': 'm1.tiny', 'metadata_event': 'event-2',
             'source': 'source-1'},
            {'volume': 1, 'user': 'user-2', 'project': 'project-1',
             'resource': 'resource-2', 'timestamp': (2013, 8, 1, 10, 40),
             'metadata_flavor': 'm1.large', 'metadata_event': 'event-2',
             'source': 'source-1'},
            {'volume': 2, 'user': 'user-2', 'project': 'project-1',
             'resource': 'resource-1', 'timestamp': (2013, 8, 1, 14, 59),
             'metadata_flavor': 'm1.large', 'metadata_event': 'event-2',
             'source': 'source-1'},
            {'volume': 4, 'user': 'user-2', 'project': 'project-2',
             'resource': 'resource-2', 'timestamp': (2013, 8, 1, 17, 28),
             'metadata_flavor': 'm1.large', 'metadata_event': 'event-2',
             'source': 'source-1'},
            {'volume': 4, 'user': 'user-3', 'project': 'project-1',
             'resource': 'resource-3', 'timestamp': (2013, 8, 1, 11, 22),
             'metadata_flavor': 'm1.tiny', 'metadata_event': 'event-2',
             'source': 'source-3'},
        )

        for test_sample in test_sample_data:
            c = sample.Sample(
                'instance',
                sample.TYPE_CUMULATIVE,
                unit='s',
                volume=test_sample['volume'],
                user_id=test_sample['user'],
                project_id=test_sample['project'],
                resource_id=test_sample['resource'],
                timestamp=datetime.datetime(*test_sample['timestamp']),
                resource_metadata={'flavor': test_sample['metadata_flavor'],
                                   'event': test_sample['metadata_event'], },
                source=test_sample['source'],
            )
            msg = utils.meter_message_from_counter(
                c,
                self.CONF.publisher.metering_secret,
            )
            self.conn.record_metering_data(msg)

    def tearDown(self):
        self.conn.clear()
        super(TestGroupBySource, self).tearDown()

    def test_group_by_source(self):
        data = self.get_json(self.PATH, groupby=['source'])
        groupby_keys_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].keys())
        groupby_vals_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].values())
        self.assertEqual(set(['source']), groupby_keys_set)
        self.assertEqual(set(['source-1', 'source-2', 'source-3']),
                         groupby_vals_set)

        for r in data:
            grp = r['groupby']
            if grp == {'source': 'source-1'}:
                self.assertEqual(4, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(1, r['min'])
                self.assertEqual(4, r['max'])
                self.assertEqual(8, r['sum'])
                self.assertEqual(2, r['avg'])
            elif grp == {'source': 'source-2'}:
                self.assertEqual(2, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(2, r['min'])
                self.assertEqual(2, r['max'])
                self.assertEqual(4, r['sum'])
                self.assertEqual(2, r['avg'])
            elif grp == {'source': 'source-3'}:
                self.assertEqual(1, r['count'])
                self.assertEqual('s', r['unit'])
                self.assertEqual(4, r['min'])
                self.assertEqual(4, r['max'])
                self.assertEqual(4, r['sum'])
                self.assertEqual(4, r['avg'])


class TestSelectableAggregates(FunctionalTest,
                               tests_db.MixinTestsWithBackendScenarios):

    PATH = '/meters/instance/statistics'

    def setUp(self):
        super(TestSelectableAggregates, self).setUp()

        test_sample_data = (
            {'volume': 2, 'user': 'user-1', 'project': 'project-1',
             'resource': 'resource-1', 'timestamp': (2013, 8, 1, 16, 10),
             'metadata_flavor': 'm1.tiny', 'metadata_event': 'event-1',
             'source': 'source'},
            {'volume': 2, 'user': 'user-2', 'project': 'project-2',
             'resource': 'resource-3', 'timestamp': (2013, 8, 1, 15, 37),
             'metadata_flavor': 'm1.large', 'metadata_event': 'event-1',
             'source': 'source'},
            {'volume': 1, 'user': 'user-2', 'project': 'project-2',
             'resource': 'resource-5', 'timestamp': (2013, 8, 1, 10, 11),
             'metadata_flavor': 'm1.medium', 'metadata_event': 'event-2',
             'source': 'source'},
            {'volume': 2, 'user': 'user-1', 'project': 'project-1',
             'resource': 'resource-2', 'timestamp': (2013, 8, 1, 10, 40),
             'metadata_flavor': 'm1.large', 'metadata_event': 'event-2',
             'source': 'source'},
            {'volume': 2, 'user': 'user-2', 'project': 'project-2',
             'resource': 'resource-4', 'timestamp': (2013, 8, 1, 14, 59),
             'metadata_flavor': 'm1.large', 'metadata_event': 'event-2',
             'source': 'source'},
            {'volume': 5, 'user': 'user-1', 'project': 'project-1',
             'resource': 'resource-2', 'timestamp': (2013, 8, 1, 17, 28),
             'metadata_flavor': 'm1.large', 'metadata_event': 'event-2',
             'source': 'source'},
            {'volume': 4, 'user': 'user-2', 'project': 'project-2',
             'resource': 'resource-3', 'timestamp': (2013, 8, 1, 11, 22),
             'metadata_flavor': 'm1.large', 'metadata_event': 'event-2',
             'source': 'source'},
            {'volume': 9, 'user': 'user-3', 'project': 'project-3',
             'resource': 'resource-4', 'timestamp': (2013, 8, 1, 11, 59),
             'metadata_flavor': 'm1.large', 'metadata_event': 'event-3',
             'source': 'source'},
        )

        for test_sample in test_sample_data:
            c = sample.Sample(
                'instance',
                sample.TYPE_GAUGE,
                unit='instance',
                volume=test_sample['volume'],
                user_id=test_sample['user'],
                project_id=test_sample['project'],
                resource_id=test_sample['resource'],
                timestamp=datetime.datetime(*test_sample['timestamp']),
                resource_metadata={'flavor': test_sample['metadata_flavor'],
                                   'event': test_sample['metadata_event'], },
                source=test_sample['source'],
            )
            msg = utils.meter_message_from_counter(
                c,
                self.CONF.publisher.metering_secret,
            )
            self.conn.record_metering_data(msg)

    def _do_test_per_tenant_selectable_standard_aggregate(self,
                                                          aggregate,
                                                          expected_values):
        agg_args = {'aggregate.func': aggregate}
        data = self.get_json(self.PATH, groupby=['project_id'], **agg_args)
        groupby_keys_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].keys())
        groupby_vals_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].values())
        self.assertEqual(set(['project_id']), groupby_keys_set)
        projects = ['project-1', 'project-2', 'project-3']
        self.assertEqual(set(projects), groupby_vals_set)

        standard_aggregates = set(['count', 'min', 'max', 'sum', 'avg'])
        for r in data:
            grp = r['groupby']
            for project in projects:
                if grp == {'project_id': project}:
                    expected = expected_values[projects.index(project)]
                    self.assertEqual('instance', r['unit'])
                    self.assertAlmostEqual(r[aggregate], expected)
                    self.assertIn('aggregate', r)
                    self.assertIn(aggregate, r['aggregate'])
                    self.assertAlmostEqual(r['aggregate'][aggregate], expected)
                    for a in standard_aggregates - set([aggregate]):
                        self.assertNotIn(a, r)

    def test_per_tenant_selectable_max(self):
        self._do_test_per_tenant_selectable_standard_aggregate('max',
                                                               [5, 4, 9])

    def test_per_tenant_selectable_min(self):
        self._do_test_per_tenant_selectable_standard_aggregate('min',
                                                               [2, 1, 9])

    def test_per_tenant_selectable_sum(self):
        self._do_test_per_tenant_selectable_standard_aggregate('sum',
                                                               [9, 9, 9])

    def test_per_tenant_selectable_avg(self):
        self._do_test_per_tenant_selectable_standard_aggregate('avg',
                                                               [3, 2.25, 9])

    def test_per_tenant_selectable_count(self):
        self._do_test_per_tenant_selectable_standard_aggregate('count',
                                                               [3, 4, 1])

    def test_per_tenant_selectable_parameterized_aggregate(self):
        agg_args = {'aggregate.func': 'cardinality',
                    'aggregate.param': 'resource_id'}
        data = self.get_json(self.PATH, groupby=['project_id'], **agg_args)
        groupby_keys_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].keys())
        groupby_vals_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].values())
        self.assertEqual(set(['project_id']), groupby_keys_set)
        projects = ['project-1', 'project-2', 'project-3']
        self.assertEqual(set(projects), groupby_vals_set)

        aggregate = 'cardinality/resource_id'
        expected_values = [2.0, 3.0, 1.0]
        standard_aggregates = set(['count', 'min', 'max', 'sum', 'avg'])
        for r in data:
            grp = r['groupby']
            for project in projects:
                if grp == {'project_id': project}:
                    expected = expected_values[projects.index(project)]
                    self.assertEqual('instance', r['unit'])
                    self.assertNotIn(aggregate, r)
                    self.assertIn('aggregate', r)
                    self.assertIn(aggregate, r['aggregate'])
                    self.assertEqual(expected, r['aggregate'][aggregate])
                    for a in standard_aggregates:
                        self.assertNotIn(a, r)

    def test_large_quantum_selectable_parameterized_aggregate(self):
        # add a large number of datapoints that won't impact on cardinality
        # if the computation logic is tolerant of different DB behavior on
        # larger numbers of samples per-period
        for i in xrange(200):
            s = sample.Sample(
                'instance',
                sample.TYPE_GAUGE,
                unit='instance',
                volume=i * 1.0,
                user_id='user-1',
                project_id='project-1',
                resource_id='resource-1',
                timestamp=datetime.datetime(2013, 8, 1, 11, i % 60),
                resource_metadata={'flavor': 'm1.tiny',
                                   'event': 'event-1', },
                source='source',
            )
            msg = utils.meter_message_from_counter(
                s,
                self.CONF.publisher.metering_secret,
            )
            self.conn.record_metering_data(msg)

        agg_args = {'aggregate.func': 'cardinality',
                    'aggregate.param': 'resource_id'}
        data = self.get_json(self.PATH, **agg_args)

        aggregate = 'cardinality/resource_id'
        expected_value = 5.0
        standard_aggregates = set(['count', 'min', 'max', 'sum', 'avg'])
        r = data[0]
        self.assertNotIn(aggregate, r)
        self.assertIn('aggregate', r)
        self.assertIn(aggregate, r['aggregate'])
        self.assertEqual(expected_value, r['aggregate'][aggregate])
        for a in standard_aggregates:
            self.assertNotIn(a, r)

    def test_repeated_unparameterized_aggregate(self):
        agg_params = 'aggregate.func=count&aggregate.func=count'
        data = self.get_json(self.PATH, override_params=agg_params)

        aggregate = 'count'
        expected_value = 8.0
        standard_aggregates = set(['min', 'max', 'sum', 'avg'])
        r = data[0]
        self.assertIn(aggregate, r)
        self.assertEqual(expected_value, r[aggregate])
        self.assertIn('aggregate', r)
        self.assertIn(aggregate, r['aggregate'])
        self.assertEqual(expected_value, r['aggregate'][aggregate])
        for a in standard_aggregates:
            self.assertNotIn(a, r)

    def test_fully_repeated_parameterized_aggregate(self):
        agg_params = ('aggregate.func=cardinality&'
                      'aggregate.param=resource_id&'
                      'aggregate.func=cardinality&'
                      'aggregate.param=resource_id&')
        data = self.get_json(self.PATH, override_params=agg_params)

        aggregate = 'cardinality/resource_id'
        expected_value = 5.0
        standard_aggregates = set(['count', 'min', 'max', 'sum', 'avg'])
        r = data[0]
        self.assertIn('aggregate', r)
        self.assertNotIn(aggregate, r)
        self.assertIn(aggregate, r['aggregate'])
        self.assertEqual(expected_value, r['aggregate'][aggregate])
        for a in standard_aggregates:
            self.assertNotIn(a, r)

    def test_partially_repeated_parameterized_aggregate(self):
        agg_params = ('aggregate.func=cardinality&'
                      'aggregate.param=resource_id&'
                      'aggregate.func=cardinality&'
                      'aggregate.param=project_id&')
        data = self.get_json(self.PATH, override_params=agg_params)

        expected_values = {'cardinality/resource_id': 5.0,
                           'cardinality/project_id': 3.0}
        standard_aggregates = set(['count', 'min', 'max', 'sum', 'avg'])
        r = data[0]
        self.assertIn('aggregate', r)
        for aggregate in expected_values.keys():
            self.assertNotIn(aggregate, r)
            self.assertIn(aggregate, r['aggregate'])
            self.assertEqual(expected_values[aggregate],
                             r['aggregate'][aggregate])
        for a in standard_aggregates:
            self.assertNotIn(a, r)

    def test_bad_selectable_parameterized_aggregate(self):
        agg_args = {'aggregate.func': 'cardinality',
                    'aggregate.param': 'injection_attack'}
        resp = self.get_json(self.PATH, status=[400],
                             groupby=['project_id'], **agg_args)
        self.assertTrue('error_message' in resp)
        self.assertEqual(resp['error_message'].get('faultcode'),
                         'Client')
        self.assertEqual(resp['error_message'].get('faultstring'),
                         'Bad aggregate: cardinality.injection_attack')


class TestUnparameterizedAggregates(FunctionalTest,
                                    tests_db.MixinTestsWithBackendScenarios):

    # We put the stddev test case in a separate class so that we
    # can easily exclude the sqlalchemy scenario, as sqlite doesn't
    # support the stddev_pop function and fails ungracefully with
    # OperationalError when it is used. However we still want to
    # test the corresponding functionality in the mongo driver.
    # For hbase & db2, the skip on NotImplementedError logic works
    # in the usual way.

    scenarios = [
        ('mongodb', {'db_manager': tests_db.MongoDbManager()}),
        ('hbase', {'db_manager': tests_db.HBaseManager()}),
        ('db2', {'db_manager': tests_db.DB2Manager()}),
    ]

    PATH = '/meters/instance/statistics'

    def setUp(self):
        super(TestUnparameterizedAggregates, self).setUp()

        test_sample_data = (
            {'volume': 2, 'user': 'user-1', 'project': 'project-1',
             'resource': 'resource-1', 'timestamp': (2013, 8, 1, 16, 10),
             'metadata_flavor': 'm1.tiny', 'metadata_event': 'event-1',
             'source': 'source'},
            {'volume': 2, 'user': 'user-2', 'project': 'project-2',
             'resource': 'resource-3', 'timestamp': (2013, 8, 1, 15, 37),
             'metadata_flavor': 'm1.large', 'metadata_event': 'event-1',
             'source': 'source'},
            {'volume': 1, 'user': 'user-2', 'project': 'project-2',
             'resource': 'resource-5', 'timestamp': (2013, 8, 1, 10, 11),
             'metadata_flavor': 'm1.medium', 'metadata_event': 'event-2',
             'source': 'source'},
            {'volume': 2, 'user': 'user-1', 'project': 'project-1',
             'resource': 'resource-2', 'timestamp': (2013, 8, 1, 10, 40),
             'metadata_flavor': 'm1.large', 'metadata_event': 'event-2',
             'source': 'source'},
            {'volume': 2, 'user': 'user-2', 'project': 'project-2',
             'resource': 'resource-4', 'timestamp': (2013, 8, 1, 14, 59),
             'metadata_flavor': 'm1.large', 'metadata_event': 'event-2',
             'source': 'source'},
            {'volume': 5, 'user': 'user-1', 'project': 'project-1',
             'resource': 'resource-2', 'timestamp': (2013, 8, 1, 17, 28),
             'metadata_flavor': 'm1.large', 'metadata_event': 'event-2',
             'source': 'source'},
            {'volume': 4, 'user': 'user-2', 'project': 'project-2',
             'resource': 'resource-3', 'timestamp': (2013, 8, 1, 11, 22),
             'metadata_flavor': 'm1.large', 'metadata_event': 'event-2',
             'source': 'source'},
            {'volume': 9, 'user': 'user-3', 'project': 'project-3',
             'resource': 'resource-4', 'timestamp': (2013, 8, 1, 11, 59),
             'metadata_flavor': 'm1.large', 'metadata_event': 'event-3',
             'source': 'source'},
        )

        for test_sample in test_sample_data:
            c = sample.Sample(
                'instance',
                sample.TYPE_GAUGE,
                unit='instance',
                volume=test_sample['volume'],
                user_id=test_sample['user'],
                project_id=test_sample['project'],
                resource_id=test_sample['resource'],
                timestamp=datetime.datetime(*test_sample['timestamp']),
                resource_metadata={'flavor': test_sample['metadata_flavor'],
                                   'event': test_sample['metadata_event'], },
                source=test_sample['source'],
            )
            msg = utils.meter_message_from_counter(
                c,
                self.CONF.publisher.metering_secret,
            )
            self.conn.record_metering_data(msg)

    def test_per_tenant_selectable_unparameterized_aggregate(self):
        agg_args = {'aggregate.func': 'stddev'}
        data = self.get_json(self.PATH, groupby=['project_id'], **agg_args)
        groupby_keys_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].keys())
        groupby_vals_set = set(x for sub_dict in data
                               for x in sub_dict['groupby'].values())
        self.assertEqual(set(['project_id']), groupby_keys_set)
        projects = ['project-1', 'project-2', 'project-3']
        self.assertEqual(set(projects), groupby_vals_set)

        aggregate = 'stddev'
        expected_values = [1.4142, 1.0897, 0.0]
        standard_aggregates = set(['count', 'min', 'max', 'sum', 'avg'])
        for r in data:
            grp = r['groupby']
            for project in projects:
                if grp == {'project_id': project}:
                    expected = expected_values[projects.index(project)]
                    self.assertEqual('instance', r['unit'])
                    self.assertNotIn(aggregate, r)
                    self.assertIn('aggregate', r)
                    self.assertIn(aggregate, r['aggregate'])
                    self.assertAlmostEqual(r['aggregate'][aggregate],
                                           expected,
                                           places=4)
                    for a in standard_aggregates:
                        self.assertNotIn(a, r)

########NEW FILE########
__FILENAME__ = test_wsme_custom_type
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 eNovance <licensing@enovance.com>
#
# Author: Mehdi Abaakouk <mehdi.abaakouk@enovance.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import wsme

from ceilometer.api.controllers import v2
from ceilometer.openstack.common import test


class TestWsmeCustomType(test.BaseTestCase):

    def test_advenum_default(self):
        class dummybase(wsme.types.Base):
            ae = v2.AdvEnum("name", str, "one", "other", default="other")

        obj = dummybase()
        self.assertEqual("other", obj.ae)

        obj = dummybase(ae="one")
        self.assertEqual("one", obj.ae)

        self.assertRaises(ValueError, dummybase, ae="not exists")

########NEW FILE########
__FILENAME__ = base
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Test base classes.
"""
import functools
import os.path
import six

from testtools import testcase

from ceilometer.openstack.common import test
from ceilometer.openstack.common import timeutils


class BaseTestCase(test.BaseTestCase):
    def assertTimestampEqual(self, first, second, msg=None):
        """Checks that two timestamps are equals.

        This relies on assertAlmostEqual to avoid rounding problem, and only
        checks up the first microsecond values.

        """
        return self.assertAlmostEqual(
            timeutils.delta_seconds(first, second),
            0.0,
            places=5)

    def assertIsEmpty(self, obj):
        try:
            if len(obj) != 0:
                self.fail("%s is not empty" % type(obj))
        except (TypeError, AttributeError):
            self.fail("%s doesn't have length" % type(obj))

    def assertIsNotEmpty(self, obj):
        try:
            if len(obj) == 0:
                self.fail("%s is empty" % type(obj))
        except (TypeError, AttributeError):
            self.fail("%s doesn't have length" % type(obj))

    @staticmethod
    def path_get(project_file=None):
        root = os.path.abspath(os.path.join(os.path.dirname(__file__),
                                            '..',
                                            '..',
                                            )
                               )
        if project_file:
            return os.path.join(root, project_file)
        else:
            return root


def _skip_decorator(func):
    @functools.wraps(func)
    def skip_if_not_implemented(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except AssertionError:
            raise
        except NotImplementedError as e:
            raise testcase.TestSkipped(six.text_type(e))
        except Exception as e:
            if 'not implemented' in six.text_type(e):
                raise testcase.TestSkipped(six.text_type(e))
            raise
    return skip_if_not_implemented


class SkipNotImplementedMeta(type):
    def __new__(cls, name, bases, local):
        for attr in local:
            value = local[attr]
            if callable(value) and (
                    attr.startswith('test_') or attr == 'setUp'):
                local[attr] = _skip_decorator(value)
        return type.__new__(cls, name, bases, local)

########NEW FILE########
__FILENAME__ = test_manager
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Intel Corp.
#
# Author: Lianhao Lu <lianhao.lu@intel.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests for ceilometer/central/manager.py
"""

import mock

from ceilometer.central import manager
from ceilometer.openstack.common.fixture import mockpatch
from ceilometer.openstack.common import test
from ceilometer.tests import agentbase


class TestManager(test.BaseTestCase):

    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    def test_load_plugins(self):
        mgr = manager.AgentManager()
        self.assertIsNotNone(list(mgr.pollster_manager))


class TestRunTasks(agentbase.BaseAgentManagerTestCase):
    @staticmethod
    def create_manager():
        return manager.AgentManager()

    def setUp(self):
        self.source_resources = True
        super(TestRunTasks, self).setUp()
        self.useFixture(mockpatch.Patch(
            'keystoneclient.v2_0.client.Client',
            return_value=None))

    def test_get_sample_resources(self):
        polling_tasks = self.mgr.setup_polling_tasks()
        self.mgr.interval_task(polling_tasks.values()[0])
        self.assertTrue(self.Pollster.resources)

    def test_skip_task_when_keystone_fail(self):
        """Test for https://bugs.launchpad.net/ceilometer/+bug/1287613."""
        self.useFixture(mockpatch.Patch(
            'keystoneclient.v2_0.client.Client',
            side_effect=Exception))
        polling_tasks = self.mgr.setup_polling_tasks()
        self.mgr.interval_task(polling_tasks.values()[0])
        self.assertFalse(self.Pollster.samples)

########NEW FILE########
__FILENAME__ = test_cpu
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Intel
#
# Author: Shuangtai Tian <Shuangtai.tian@intel.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests for converters for producing compute counter messages from
notification events.
"""

import copy

from ceilometer.compute.notifications import cpu
from ceilometer.openstack.common import test


METRICS_UPDATE = {
    u'_context_request_id': u'req-a8bfa89b-d28b-4b95-9e4b-7d7875275650',
    u'_context_quota_class': None,
    u'event_type': u'compute.metrics.update',
    u'_context_service_catalog': [],
    u'_context_auth_token': None,
    u'_context_user_id': None,
    u'payload': {
        u'metrics': [
            {'timestamp': u'2013-07-29T06:51:34.472416',
             'name': 'cpu.frequency', 'value': 1600,
             'source': 'libvirt.LibvirtDriver'},
            {'timestamp': u'2013-07-29T06:51:34.472416',
             'name': 'cpu.user.time', 'value': 17421440000000L,
             'source': 'libvirt.LibvirtDriver'},
            {'timestamp': u'2013-07-29T06:51:34.472416',
             'name': 'cpu.kernel.time', 'value': 7852600000000L,
             'source': 'libvirt.LibvirtDriver'},
            {'timestamp': u'2013-07-29T06:51:34.472416',
             'name': 'cpu.idle.time', 'value': 1307374400000000L,
             'source': 'libvirt.LibvirtDriver'},
            {'timestamp': u'2013-07-29T06:51:34.472416',
             'name': 'cpu.iowait.time', 'value': 11697470000000L,
             'source': 'libvirt.LibvirtDriver'},
            {'timestamp': u'2013-07-29T06:51:34.472416',
             'name': 'cpu.user.percent', 'value': 0.012959045637294348,
             'source': 'libvirt.LibvirtDriver'},
            {'timestamp': u'2013-07-29T06:51:34.472416',
             'name': 'cpu.kernel.percent', 'value': 0.005841204961898534,
             'source': 'libvirt.LibvirtDriver'},
            {'timestamp': u'2013-07-29T06:51:34.472416',
             'name': 'cpu.idle.percent', 'value': 0.9724985141658965,
             'source': 'libvirt.LibvirtDriver'},
            {'timestamp': u'2013-07-29T06:51:34.472416',
             'name': 'cpu.iowait.percent', 'value': 0.008701235234910634,
             'source': 'libvirt.LibvirtDriver'},
            {'timestamp': u'2013-07-29T06:51:34.472416',
             'name': 'cpu.percent', 'value': 0.027501485834103515,
             'source': 'libvirt.LibvirtDriver'}],
        u'nodename': u'tianst.sh.intel.com',
        u'host': u'tianst',
        u'host_id': u'10.0.1.1'},
    u'priority': u'INFO',
    u'_context_is_admin': True,
    u'_context_user': None,
    u'publisher_id': u'compute.tianst.sh.intel.com',
    u'message_id': u'6eccedba-120e-4db8-9735-2ad5f061e5ee',
    u'_context_remote_address': None,
    u'_context_roles': [],
    u'timestamp': u'2013-07-29 06:51:34.474815',
    u'_context_timestamp': u'2013-07-29T06:51:34.348091',
    u'_unique_id': u'0ee26117077648e18d88ac76e28a72e2',
    u'_context_project_name': None,
    u'_context_read_deleted': u'no',
    u'_context_tenant': None,
    u'_context_instance_lock_checked': False,
    u'_context_project_id': None,
    u'_context_user_name': None
}

RES_ID = '%s_%s' % (METRICS_UPDATE['payload']['host'],
                    METRICS_UPDATE['payload']['nodename'])


class TestMetricsNotifications(test.BaseTestCase):
    def _process_notification(self, ic):
        self.assertIn(METRICS_UPDATE['event_type'],
                      ic.event_types)
        samples = list(ic.process_notification(METRICS_UPDATE))
        self.assertEqual(RES_ID, samples[0].resource_id)
        return samples[0]

    def test_compute_metrics(self):
        ERROR_METRICS = copy.copy(METRICS_UPDATE)
        ERROR_METRICS['payload'] = {"metric_err": []}
        ic = cpu.CpuFrequency(None)
        info = ic._get_sample(METRICS_UPDATE, 'cpu.frequency')
        info_none = ic._get_sample(METRICS_UPDATE, 'abc.efg')
        info_error = ic._get_sample(ERROR_METRICS, 'cpu.frequency')
        self.assertEqual('cpu.frequency', info['payload']['name'])
        self.assertIsNone(info_none)
        self.assertIsNone(info_error)

    def test_compute_cpu_frequency(self):
        c = self._process_notification(cpu.CpuFrequency(None))
        self.assertEqual('compute.node.cpu.frequency', c.name)
        self.assertEqual(1600, c.volume)

    def test_compute_cpu_user_time(self):
        c = self._process_notification(cpu.CpuUserTime(None))
        self.assertEqual('compute.node.cpu.user.time', c.name)
        self.assertEqual(17421440000000L, c.volume)

    def test_compute_cpu_kernel_time(self):
        c = self._process_notification(cpu.CpuKernelTime(None))
        self.assertEqual('compute.node.cpu.kernel.time', c.name)
        self.assertEqual(7852600000000L, c.volume)

    def test_compute_cpu_idle_time(self):
        c = self._process_notification(cpu.CpuIdleTime(None))
        self.assertEqual('compute.node.cpu.idle.time', c.name)
        self.assertEqual(1307374400000000L, c.volume)

    def test_compute_cpu_iowait_time(self):
        c = self._process_notification(cpu.CpuIowaitTime(None))
        self.assertEqual('compute.node.cpu.iowait.time', c.name)
        self.assertEqual(11697470000000L, c.volume)

    def test_compute_cpu_kernel_percent(self):
        c = self._process_notification(cpu.CpuKernelPercent(None))
        self.assertEqual('compute.node.cpu.kernel.percent', c.name)
        self.assertEqual(0.5841204961898534, c.volume)

    def test_compute_cpu_idle_percent(self):
        c = self._process_notification(cpu.CpuIdlePercent(None))
        self.assertEqual('compute.node.cpu.idle.percent', c.name)
        self.assertEqual(97.24985141658965, c.volume)

    def test_compute_cpu_user_percent(self):
        c = self._process_notification(cpu.CpuUserPercent(None))
        self.assertEqual('compute.node.cpu.user.percent', c.name)
        self.assertEqual(1.2959045637294348, c.volume)

    def test_compute_cpu_iowait_percent(self):
        c = self._process_notification(cpu.CpuIowaitPercent(None))
        self.assertEqual('compute.node.cpu.iowait.percent', c.name)
        self.assertEqual(0.8701235234910634, c.volume)

    def test_compute_cpu_percent(self):
        c = self._process_notification(cpu.CpuPercent(None))
        self.assertEqual('compute.node.cpu.percent', c.name)
        self.assertEqual(2.7501485834103515, c.volume)

########NEW FILE########
__FILENAME__ = test_instance
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
# Copyright © 2013 eNovance
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#         Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests for converters for producing compute counter messages from
notification events.
"""

from ceilometer.compute.notifications import instance
from ceilometer.openstack.common import test
from ceilometer import sample


INSTANCE_CREATE_END = {
    u'_context_auth_token': u'3d8b13de1b7d499587dfc69b77dc09c2',
    u'_context_is_admin': True,
    u'_context_project_id': u'7c150a59fe714e6f9263774af9688f0e',
    u'_context_quota_class': None,
    u'_context_read_deleted': u'no',
    u'_context_remote_address': u'10.0.2.15',
    u'_context_request_id': u'req-d68b36e0-9233-467f-9afb-d81435d64d66',
    u'_context_roles': [u'admin'],
    u'_context_timestamp': u'2012-05-08T20:23:41.425105',
    u'_context_user_id': u'1e3ce043029547f1a61c1996d1a531a2',
    u'event_type': u'compute.instance.create.end',
    u'message_id': u'dae6f69c-00e0-41c0-b371-41ec3b7f4451',
    u'payload': {u'created_at': u'2012-05-08 20:23:41',
                 u'deleted_at': u'',
                 u'disk_gb': 0,
                 u'display_name': u'testme',
                 u'fixed_ips': [{u'address': u'10.0.0.2',
                                 u'floating_ips': [],
                                 u'meta': {},
                                 u'type': u'fixed',
                                 u'version': 4}],
                 u'image_ref_url': u'http://10.0.2.15:9292/images/UUID',
                 u'instance_id': u'9f9d01b9-4a58-4271-9e27-398b21ab20d1',
                 u'instance_type': u'm1.tiny',
                 u'instance_type_id': 2,
                 u'launched_at': u'2012-05-08 20:23:47.985999',
                 u'memory_mb': 512,
                 u'state': u'active',
                 u'state_description': u'',
                 u'tenant_id': u'7c150a59fe714e6f9263774af9688f0e',
                 u'user_id': u'1e3ce043029547f1a61c1996d1a531a2',
                 u'reservation_id': u'1e3ce043029547f1a61c1996d1a531a3',
                 u'vcpus': 1,
                 u'root_gb': 0,
                 u'ephemeral_gb': 0,
                 u'host': u'compute-host-name',
                 u'availability_zone': u'1e3ce043029547f1a61c1996d1a531a4',
                 u'os_type': u'linux?',
                 u'architecture': u'x86',
                 u'image_ref': u'UUID',
                 u'kernel_id': u'1e3ce043029547f1a61c1996d1a531a5',
                 u'ramdisk_id': u'1e3ce043029547f1a61c1996d1a531a6',
                 },
    u'priority': u'INFO',
    u'publisher_id': u'compute.vagrant-precise',
    u'timestamp': u'2012-05-08 20:23:48.028195',
}

INSTANCE_DELETE_START = {
    u'_context_auth_token': u'3d8b13de1b7d499587dfc69b77dc09c2',
    u'_context_is_admin': True,
    u'_context_project_id': u'7c150a59fe714e6f9263774af9688f0e',
    u'_context_quota_class': None,
    u'_context_read_deleted': u'no',
    u'_context_remote_address': u'10.0.2.15',
    u'_context_request_id': u'req-fb3c4546-a2e5-49b7-9fd2-a63bd658bc39',
    u'_context_roles': [u'admin'],
    u'_context_timestamp': u'2012-05-08T20:24:14.547374',
    u'_context_user_id': u'1e3ce043029547f1a61c1996d1a531a2',
    u'event_type': u'compute.instance.delete.start',
    u'message_id': u'a15b94ee-cb8e-4c71-9abe-14aa80055fb4',
    u'payload': {u'created_at': u'2012-05-08 20:23:41',
                 u'deleted_at': u'',
                 u'disk_gb': 0,
                 u'display_name': u'testme',
                 u'image_ref_url': u'http://10.0.2.15:9292/images/UUID',
                 u'instance_id': u'9f9d01b9-4a58-4271-9e27-398b21ab20d1',
                 u'instance_type': u'm1.tiny',
                 u'instance_type_id': 2,
                 u'launched_at': u'2012-05-08 20:23:47',
                 u'memory_mb': 512,
                 u'state': u'active',
                 u'state_description': u'deleting',
                 u'tenant_id': u'7c150a59fe714e6f9263774af9688f0e',
                 u'user_id': u'1e3ce043029547f1a61c1996d1a531a2',
                 u'reservation_id': u'1e3ce043029547f1a61c1996d1a531a3',
                 u'vcpus': 1,
                 u'root_gb': 0,
                 u'ephemeral_gb': 0,
                 u'host': u'compute-host-name',
                 u'availability_zone': u'1e3ce043029547f1a61c1996d1a531a4',
                 u'os_type': u'linux?',
                 u'architecture': u'x86',
                 u'image_ref': u'UUID',
                 u'kernel_id': u'1e3ce043029547f1a61c1996d1a531a5',
                 u'ramdisk_id': u'1e3ce043029547f1a61c1996d1a531a6',
                 },
    u'priority': u'INFO',
    u'publisher_id': u'compute.vagrant-precise',
    u'timestamp': u'2012-05-08 20:24:14.824743',
}

INSTANCE_EXISTS = {
    u'_context_auth_token': None,
    u'_context_is_admin': True,
    u'_context_project_id': None,
    u'_context_quota_class': None,
    u'_context_read_deleted': u'no',
    u'_context_remote_address': None,
    u'_context_request_id': u'req-659a8eb2-4372-4c01-9028-ad6e40b0ed22',
    u'_context_roles': [u'admin'],
    u'_context_timestamp': u'2012-05-08T16:03:43.760204',
    u'_context_user_id': None,
    u'event_type': u'compute.instance.exists',
    u'message_id': u'4b884c03-756d-4c06-8b42-80b6def9d302',
    u'payload': {u'audit_period_beginning': u'2012-05-08 15:00:00',
                 u'audit_period_ending': u'2012-05-08 16:00:00',
                 u'bandwidth': {},
                 u'created_at': u'2012-05-07 22:16:18',
                 u'deleted_at': u'',
                 u'disk_gb': 0,
                 u'display_name': u'testme',
                 u'image_ref_url': u'http://10.0.2.15:9292/images/UUID',
                 u'instance_id': u'3a513875-95c9-4012-a3e7-f90c678854e5',
                 u'instance_type': u'm1.tiny',
                 u'instance_type_id': 2,
                 u'launched_at': u'2012-05-07 23:01:27',
                 u'memory_mb': 512,
                 u'state': u'active',
                 u'state_description': u'',
                 u'tenant_id': u'7c150a59fe714e6f9263774af9688f0e',
                 u'user_id': u'1e3ce043029547f1a61c1996d1a531a2',
                 u'reservation_id': u'1e3ce043029547f1a61c1996d1a531a3',
                 u'vcpus': 1,
                 u'root_gb': 0,
                 u'ephemeral_gb': 0,
                 u'host': u'compute-host-name',
                 u'availability_zone': u'1e3ce043029547f1a61c1996d1a531a4',
                 u'os_type': u'linux?',
                 u'architecture': u'x86',
                 u'image_ref': u'UUID',
                 u'kernel_id': u'1e3ce043029547f1a61c1996d1a531a5',
                 u'ramdisk_id': u'1e3ce043029547f1a61c1996d1a531a6',
                 },
    u'priority': u'INFO',
    u'publisher_id': u'compute.vagrant-precise',
    u'timestamp': u'2012-05-08 16:03:44.122481',
}

INSTANCE_EXISTS_METADATA_LIST = {
    u'_context_auth_token': None,
    u'_context_is_admin': True,
    u'_context_project_id': None,
    u'_context_quota_class': None,
    u'_context_read_deleted': u'no',
    u'_context_remote_address': None,
    u'_context_request_id': u'req-659a8eb2-4372-4c01-9028-ad6e40b0ed22',
    u'_context_roles': [u'admin'],
    u'_context_timestamp': u'2012-05-08T16:03:43.760204',
    u'_context_user_id': None,
    u'event_type': u'compute.instance.exists',
    u'message_id': u'4b884c03-756d-4c06-8b42-80b6def9d302',
    u'payload': {u'audit_period_beginning': u'2012-05-08 15:00:00',
                 u'audit_period_ending': u'2012-05-08 16:00:00',
                 u'bandwidth': {},
                 u'created_at': u'2012-05-07 22:16:18',
                 u'deleted_at': u'',
                 u'disk_gb': 0,
                 u'display_name': u'testme',
                 u'image_ref_url': u'http://10.0.2.15:9292/images/UUID',
                 u'instance_id': u'3a513875-95c9-4012-a3e7-f90c678854e5',
                 u'instance_type': u'm1.tiny',
                 u'instance_type_id': 2,
                 u'launched_at': u'2012-05-07 23:01:27',
                 u'memory_mb': 512,
                 u'state': u'active',
                 u'state_description': u'',
                 u'tenant_id': u'7c150a59fe714e6f9263774af9688f0e',
                 u'user_id': u'1e3ce043029547f1a61c1996d1a531a2',
                 u'reservation_id': u'1e3ce043029547f1a61c1996d1a531a3',
                 u'vcpus': 1,
                 u'root_gb': 0,
                 u'metadata': [],
                 u'ephemeral_gb': 0,
                 u'host': u'compute-host-name',
                 u'availability_zone': u'1e3ce043029547f1a61c1996d1a531a4',
                 u'os_type': u'linux?',
                 u'architecture': u'x86',
                 u'image_ref': u'UUID',
                 u'kernel_id': u'1e3ce043029547f1a61c1996d1a531a5',
                 u'ramdisk_id': u'1e3ce043029547f1a61c1996d1a531a6',
                 },
    u'priority': u'INFO',
    u'publisher_id': u'compute.vagrant-precise',
    u'timestamp': u'2012-05-08 16:03:44.122481',
}


INSTANCE_FINISH_RESIZE_END = {
    u'_context_roles': [u'admin'],
    u'_context_request_id': u'req-e3f71bb9-e9b9-418b-a9db-a5950c851b25',
    u'_context_quota_class': None,
    u'event_type': u'compute.instance.finish_resize.end',
    u'_context_user_name': u'admin',
    u'_context_project_name': u'admin',
    u'timestamp': u'2013-01-04 15:10:17.436974',
    u'_context_is_admin': True,
    u'message_id': u'a2f7770d-b85d-4797-ab10-41407a44368e',
    u'_context_auth_token': None,
    u'_context_instance_lock_checked': False,
    u'_context_project_id': u'cea4b25edb484e5392727181b7721d29',
    u'_context_timestamp': u'2013-01-04T15:08:39.162612',
    u'_context_read_deleted': u'no',
    u'_context_user_id': u'01b83a5e23f24a6fb6cd073c0aee6eed',
    u'_context_remote_address': u'10.147.132.184',
    u'publisher_id': u'compute.ip-10-147-132-184.ec2.internal',
    u'payload': {u'state_description': u'',
                 u'availability_zone': None,
                 u'ephemeral_gb': 0,
                 u'instance_type_id': 5,
                 u'deleted_at': u'',
                 u'fixed_ips': [{u'floating_ips': [],
                                 u'label': u'private',
                                 u'version': 4,
                                 u'meta': {},
                                 u'address': u'10.0.0.3',
                                 u'type': u'fixed'}],
                 u'memory_mb': 2048,
                 u'user_id': u'01b83a5e23f24a6fb6cd073c0aee6eed',
                 u'reservation_id': u'r-u3fvim06',
                 u'hostname': u's1',
                 u'state': u'resized',
                 u'launched_at': u'2013-01-04T15:10:14.923939',
                 u'metadata': {u'metering.server_group': u'Group_A',
                               u'AutoScalingGroupName': u'tyky-Group_Awste7',
                               u'metering.foo.bar': u'true'},
                 u'ramdisk_id': u'5f23128e-5525-46d8-bc66-9c30cd87141a',
                 u'access_ip_v6': None,
                 u'disk_gb': 20,
                 u'access_ip_v4': None,
                 u'kernel_id': u'571478e0-d5e7-4c2e-95a5-2bc79443c28a',
                 u'host': u'ip-10-147-132-184.ec2.internal',
                 u'display_name': u's1',
                 u'image_ref_url': u'http://10.147.132.184:9292/images/'
                 'a130b9d9-e00e-436e-9782-836ccef06e8a',
                 u'root_gb': 20,
                 u'tenant_id': u'cea4b25edb484e5392727181b7721d29',
                 u'created_at': u'2013-01-04T11:21:48.000000',
                 u'instance_id': u'648e8963-6886-4c3c-98f9-4511c292f86b',
                 u'instance_type': u'm1.small',
                 u'vcpus': 1,
                 u'image_meta': {u'kernel_id':
                                 u'571478e0-d5e7-4c2e-95a5-2bc79443c28a',
                                 u'ramdisk_id':
                                 u'5f23128e-5525-46d8-bc66-9c30cd87141a',
                                 u'base_image_ref':
                                 u'a130b9d9-e00e-436e-9782-836ccef06e8a'},
                 u'architecture': None,
                 u'os_type': None
                 },
    u'priority': u'INFO'
}

INSTANCE_RESIZE_REVERT_END = {
    u'_context_roles': [u'admin'],
    u'_context_request_id': u'req-9da1d714-dabe-42fd-8baa-583e57cd4f1a',
    u'_context_quota_class': None,
    u'event_type': u'compute.instance.resize.revert.end',
    u'_context_user_name': u'admin',
    u'_context_project_name': u'admin',
    u'timestamp': u'2013-01-04 15:20:32.009532',
    u'_context_is_admin': True,
    u'message_id': u'c48deeba-d0c3-4154-b3db-47480b52267a',
    u'_context_auth_token': None,
    u'_context_instance_lock_checked': False,
    u'_context_project_id': u'cea4b25edb484e5392727181b7721d29',
    u'_context_timestamp': u'2013-01-04T15:19:51.018218',
    u'_context_read_deleted': u'no',
    u'_context_user_id': u'01b83a5e23f24a6fb6cd073c0aee6eed',
    u'_context_remote_address': u'10.147.132.184',
    u'publisher_id': u'compute.ip-10-147-132-184.ec2.internal',
    u'payload': {u'state_description': u'resize_reverting',
                 u'availability_zone': None,
                 u'ephemeral_gb': 0,
                 u'instance_type_id': 2,
                 u'deleted_at': u'',
                 u'reservation_id': u'r-u3fvim06',
                 u'memory_mb': 512,
                 u'user_id': u'01b83a5e23f24a6fb6cd073c0aee6eed',
                 u'hostname': u's1',
                 u'state': u'resized',
                 u'launched_at': u'2013-01-04T15:10:14.000000',
                 u'metadata': {u'metering.server_group': u'Group_A',
                               u'AutoScalingGroupName': u'tyky-Group_A-wste7',
                               u'metering.foo.bar': u'true'},
                 u'ramdisk_id': u'5f23128e-5525-46d8-bc66-9c30cd87141a',
                 u'access_ip_v6': None,
                 u'disk_gb': 0,
                 u'access_ip_v4': None,
                 u'kernel_id': u'571478e0-d5e7-4c2e-95a5-2bc79443c28a',
                 u'host': u'ip-10-147-132-184.ec2.internal',
                 u'display_name': u's1',
                 u'image_ref_url': u'http://10.147.132.184:9292/images/'
                 'a130b9d9-e00e-436e-9782-836ccef06e8a',
                 u'root_gb': 0,
                 u'tenant_id': u'cea4b25edb484e5392727181b7721d29',
                 u'created_at': u'2013-01-04T11:21:48.000000',
                 u'instance_id': u'648e8963-6886-4c3c-98f9-4511c292f86b',
                 u'instance_type': u'm1.tiny',
                 u'vcpus': 1,
                 u'image_meta': {u'kernel_id':
                                 u'571478e0-d5e7-4c2e-95a5-2bc79443c28a',
                                 u'ramdisk_id':
                                 u'5f23128e-5525-46d8-bc66-9c30cd87141a',
                                 u'base_image_ref':
                                 u'a130b9d9-e00e-436e-9782-836ccef06e8a'},
                 u'architecture': None,
                 u'os_type': None
                 },
    u'priority': u'INFO'
}

INSTANCE_DELETE_SAMPLES = {
    u'_context_roles': [u'admin'],
    u'_context_request_id': u'req-9da1d714-dabe-42fd-8baa-583e57cd4f1a',
    u'_context_quota_class': None,
    u'event_type': u'compute.instance.delete.samples',
    u'_context_user_name': u'admin',
    u'_context_project_name': u'admin',
    u'timestamp': u'2013-01-04 15:20:32.009532',
    u'_context_is_admin': True,
    u'message_id': u'c48deeba-d0c3-4154-b3db-47480b52267a',
    u'_context_auth_token': None,
    u'_context_instance_lock_checked': False,
    u'_context_project_id': u'cea4b25edb484e5392727181b7721d29',
    u'_context_timestamp': u'2013-01-04T15:19:51.018218',
    u'_context_read_deleted': u'no',
    u'_context_user_id': u'01b83a5e23f24a6fb6cd073c0aee6eed',
    u'_context_remote_address': u'10.147.132.184',
    u'publisher_id': u'compute.ip-10-147-132-184.ec2.internal',
    u'payload': {u'state_description': u'resize_reverting',
                 u'availability_zone': None,
                 u'ephemeral_gb': 0,
                 u'instance_type_id': 2,
                 u'deleted_at': u'',
                 u'reservation_id': u'r-u3fvim06',
                 u'memory_mb': 512,
                 u'user_id': u'01b83a5e23f24a6fb6cd073c0aee6eed',
                 u'hostname': u's1',
                 u'state': u'resized',
                 u'launched_at': u'2013-01-04T15:10:14.000000',
                 u'metadata': {u'metering.server_group': u'Group_A',
                               u'AutoScalingGroupName': u'tyky-Group_A-wste7',
                               u'metering.foo.bar': u'true'},
                 u'ramdisk_id': u'5f23128e-5525-46d8-bc66-9c30cd87141a',
                 u'access_ip_v6': None,
                 u'disk_gb': 0,
                 u'access_ip_v4': None,
                 u'kernel_id': u'571478e0-d5e7-4c2e-95a5-2bc79443c28a',
                 u'host': u'ip-10-147-132-184.ec2.internal',
                 u'display_name': u's1',
                 u'image_ref_url': u'http://10.147.132.184:9292/images/'
                 'a130b9d9-e00e-436e-9782-836ccef06e8a',
                 u'root_gb': 0,
                 u'tenant_id': u'cea4b25edb484e5392727181b7721d29',
                 u'created_at': u'2013-01-04T11:21:48.000000',
                 u'instance_id': u'648e8963-6886-4c3c-98f9-4511c292f86b',
                 u'instance_type': u'm1.tiny',
                 u'vcpus': 1,
                 u'image_meta': {u'kernel_id':
                                 u'571478e0-d5e7-4c2e-95a5-2bc79443c28a',
                                 u'ramdisk_id':
                                 u'5f23128e-5525-46d8-bc66-9c30cd87141a',
                                 u'base_image_ref':
                                 u'a130b9d9-e00e-436e-9782-836ccef06e8a'},
                 u'architecture': None,
                 u'os_type': None,
                 u'samples': [{u'name': u'sample-name1',
                               u'type': u'sample-type1',
                               u'unit': u'sample-units1',
                               u'volume': 1},
                              {u'name': u'sample-name2',
                               u'type': u'sample-type2',
                               u'unit': u'sample-units2',
                               u'volume': 2},
                              ],
                 },
    u'priority': u'INFO'
}

INSTANCE_SCHEDULED = {
    u'_context_request_id': u'req-f28a836a-32bf-4cc3-940a-3515878c181f',
    u'_context_quota_class': None,
    u'event_type': u'scheduler.run_instance.scheduled',
    u'_context_service_catalog': [{
        u'endpoints': [{
            u'adminURL':
            u'http://172.16.12.21:8776/v1/2bd766a095b44486bf07cf7f666997eb',
            u'region': u'RegionOne',
            u'internalURL':
            u'http://172.16.12.21:8776/v1/2bd766a095b44486bf07cf7f666997eb',
            u'id': u'30cb904fdc294eea9b225e06b2d0d4eb',
            u'publicURL':
            u'http://172.16.12.21:8776/v1/2bd766a095b44486bf07cf7f666997eb'}],
        u'endpoints_links': [],
        u'type': u'volume',
        u'name': u'cinder'}],
    u'_context_auth_token': u'TOK',
    u'_context_user_id': u'0a757cd896b64b65ba3784afef564116',
    u'payload': {
        'instance_id': 'fake-uuid1-1',
        u'weighted_host': {u'host': u'eglynn-f19-devstack3', u'weight': 1.0},
        u'request_spec': {
            u'num_instances': 1,
            u'block_device_mapping': [{
                u'instance_uuid': u'9206baae-c3b6-41bc-96f2-2c0726ff51c8',
                u'guest_format': None,
                u'boot_index': 0,
                u'no_device': None,
                u'connection_info': None,
                u'volume_id': None,
                u'volume_size': None,
                u'device_name': None,
                u'disk_bus': None,
                u'image_id': u'0560ac3f-3bcd-434d-b012-8dd7a212b73b',
                u'source_type': u'image',
                u'device_type': u'disk',
                u'snapshot_id': None,
                u'destination_type': u'local',
                u'delete_on_termination': True}],
            u'image': {
                u'status': u'active',
                u'name': u'cirros-0.3.1-x86_64-uec',
                u'deleted': False,
                u'container_format': u'ami',
                u'created_at': u'2014-02-18T13:16:26.000000',
                u'disk_format': u'ami',
                u'updated_at': u'2014-02-18T13:16:27.000000',
                u'properties': {
                    u'kernel_id': u'c8794c1a-4158-42cc-9f97-d0d250c9c6a4',
                    u'ramdisk_id': u'4999726c-545c-4a9e-bfc0-917459784275'},
                u'min_disk': 0,
                u'min_ram': 0,
                u'checksum': u'f8a2eeee2dc65b3d9b6e63678955bd83',
                u'owner': u'2bd766a095b44486bf07cf7f666997eb',
                u'is_public': True,
                u'deleted_at': None,
                u'id': u'0560ac3f-3bcd-434d-b012-8dd7a212b73b',
                u'size': 25165824},
            u'instance_type': {
                u'root_gb': 1,
                u'name': u'm1.tiny',
                u'ephemeral_gb': 0,
                u'memory_mb': 512,
                u'vcpus': 1,
                u'extra_specs': {},
                u'swap': 0,
                u'rxtx_factor': 1.0,
                u'flavorid': u'1',
                u'vcpu_weight': None,
                u'id': 2},
            u'instance_properties': {
                u'vm_state': u'building',
                u'availability_zone': None,
                u'terminated_at': None,
                u'ephemeral_gb': 0,
                u'instance_type_id': 2,
                u'user_data': None,
                u'cleaned': False,
                u'vm_mode': None,
                u'deleted_at': None,
                u'reservation_id': u'r-ven5q6om',
                u'id': 15,
                u'security_groups': [{
                    u'deleted_at': None,
                    u'user_id': u'0a757cd896b64b65ba3784afef564116',
                    u'description': u'default',
                    u'deleted': False,
                    u'created_at': u'2014-02-19T11:02:31.000000',
                    u'updated_at': None,
                    u'project_id': u'2bd766a095b44486bf07cf7f666997eb',
                    u'id': 1,
                    u'name': u'default'}],
                u'disable_terminate': False,
                u'root_device_name': None,
                u'display_name': u'new',
                u'uuid': u'9206baae-c3b6-41bc-96f2-2c0726ff51c8',
                u'default_swap_device': None,
                u'info_cache': {
                    u'instance_uuid': u'9206baae-c3b6-41bc-96f2-2c0726ff51c8',
                    u'deleted': False,
                    u'created_at': u'2014-03-05T12:44:00.000000',
                    u'updated_at': None,
                    u'network_info': [],
                    u'deleted_at': None},
                u'hostname': u'new',
                u'launched_on': None,
                u'display_description': u'new',
                u'key_data': None,
                u'deleted': False,
                u'config_drive': u'',
                u'power_state': 0,
                u'default_ephemeral_device': None,
                u'progress': 0,
                u'project_id': u'2bd766a095b44486bf07cf7f666997eb',
                u'launched_at': None,
                u'scheduled_at': None,
                u'node': None,
                u'ramdisk_id': u'4999726c-545c-4a9e-bfc0-917459784275',
                u'access_ip_v6': None,
                u'access_ip_v4': None,
                u'kernel_id': u'c8794c1a-4158-42cc-9f97-d0d250c9c6a4',
                u'key_name': None,
                u'updated_at': None,
                u'host': None,
                u'root_gb': 1,
                u'user_id': u'0a757cd896b64b65ba3784afef564116',
                u'system_metadata': {
                    u'image_kernel_id':
                    u'c8794c1a-4158-42cc-9f97-d0d250c9c6a4',
                    u'image_min_disk': u'1',
                    u'instance_type_memory_mb': u'512',
                    u'instance_type_swap': u'0',
                    u'instance_type_vcpu_weight': None,
                    u'instance_type_root_gb': u'1',
                    u'instance_type_name': u'm1.tiny',
                    u'image_ramdisk_id':
                    u'4999726c-545c-4a9e-bfc0-917459784275',
                    u'instance_type_id': u'2',
                    u'instance_type_ephemeral_gb': u'0',
                    u'instance_type_rxtx_factor': u'1.0',
                    u'instance_type_flavorid': u'1',
                    u'instance_type_vcpus': u'1',
                    u'image_container_format': u'ami',
                    u'image_min_ram': u'0',
                    u'image_disk_format': u'ami',
                    u'image_base_image_ref':
                    u'0560ac3f-3bcd-434d-b012-8dd7a212b73b'},
                u'task_state': u'scheduling',
                u'shutdown_terminate': False,
                u'cell_name': None,
                u'ephemeral_key_uuid': None,
                u'locked': False,
                u'name': u'instance-0000000f',
                u'created_at': u'2014-03-05T12:44:00.000000',
                u'locked_by': None,
                u'launch_index': 0,
                u'memory_mb': 512,
                u'vcpus': 1,
                u'image_ref': u'0560ac3f-3bcd-434d-b012-8dd7a212b73b',
                u'architecture': None,
                u'auto_disk_config': False,
                u'os_type': None,
                u'metadata': {u'metering.server_group': u'Group_A',
                              u'AutoScalingGroupName': u'tyky-Group_Awste7',
                              u'metering.foo.bar': u'true'}},
                u'security_group': [u'default'],
                u'instance_uuids': [u'9206baae-c3b6-41bc-96f2-2c0726ff51c8']}},
    u'priority': u'INFO',
    u'_context_is_admin': True,
    u'_context_timestamp': u'2014-03-05T12:44:00.135674',
    u'publisher_id': u'scheduler.eglynn-f19-devstack3',
    u'message_id': u'd6c1ae63-a26b-47c7-8397-8794216e09dd',
    u'_context_remote_address': u'172.16.12.21',
    u'_context_roles': [u'_member_', u'admin'],
    u'timestamp': u'2014-03-05 12:44:00.733758',
    u'_context_user': u'0a757cd896b64b65ba3784afef564116',
    u'_unique_id': u'2af47cbdde604ff794bb046f3f9db1e2',
    u'_context_project_name': u'admin',
    u'_context_read_deleted': u'no',
    u'_context_tenant': u'2bd766a095b44486bf07cf7f666997eb',
    u'_context_instance_lock_checked': False,
    u'_context_project_id': u'2bd766a095b44486bf07cf7f666997eb',
    u'_context_user_name': u'admin'
}


class TestNotifications(test.BaseTestCase):

    def test_process_notification(self):
        info = list(instance.Instance(None).process_notification(
            INSTANCE_CREATE_END
        ))[0]
        for name, actual, expected in [
                ('counter_name', info.name, 'instance'),
                ('counter_type', info.type, sample.TYPE_GAUGE),
                ('counter_volume', info.volume, 1),
                ('timestamp', info.timestamp,
                 INSTANCE_CREATE_END['timestamp']),
                ('resource_id', info.resource_id,
                 INSTANCE_CREATE_END['payload']['instance_id']),
                ('instance_type_id',
                 info.resource_metadata['instance_type_id'],
                 INSTANCE_CREATE_END['payload']['instance_type_id']),
                ('host', info.resource_metadata['host'],
                 INSTANCE_CREATE_END['publisher_id']),
        ]:
            self.assertEqual(expected, actual, name)

    @staticmethod
    def _find_counter(counters, name):
        return filter(lambda counter: counter.name == name, counters)[0]

    def _verify_user_metadata(self, metadata):
        self.assertIn('user_metadata', metadata)
        user_meta = metadata['user_metadata']
        self.assertEqual(user_meta.get('server_group'), 'Group_A')
        self.assertNotIn('AutoScalingGroupName', user_meta)
        self.assertIn('foo_bar', user_meta)
        self.assertNotIn('foo.bar', user_meta)

    def test_instance_create_instance(self):
        ic = instance.Instance(None)
        counters = list(ic.process_notification(INSTANCE_CREATE_END))
        self.assertEqual(1, len(counters))
        c = counters[0]
        self.assertEqual(1, c.volume)

    def test_instance_create_flavor(self):
        ic = instance.InstanceFlavor(None)
        counters = list(ic.process_notification(INSTANCE_CREATE_END))
        self.assertEqual(1, len(counters))
        c = counters[0]
        self.assertEqual(1, c.volume)

    def test_instance_create_memory(self):
        ic = instance.Memory(None)
        counters = list(ic.process_notification(INSTANCE_CREATE_END))
        self.assertEqual(1, len(counters))
        c = counters[0]
        self.assertEqual(INSTANCE_CREATE_END['payload']['memory_mb'], c.volume)

    def test_instance_create_vcpus(self):
        ic = instance.VCpus(None)
        counters = list(ic.process_notification(INSTANCE_CREATE_END))
        self.assertEqual(1, len(counters))
        c = counters[0]
        self.assertEqual(INSTANCE_CREATE_END['payload']['vcpus'], c.volume)

    def test_instance_create_root_disk_size(self):
        ic = instance.RootDiskSize(None)
        counters = list(ic.process_notification(INSTANCE_CREATE_END))
        self.assertEqual(1, len(counters))
        c = counters[0]
        self.assertEqual(INSTANCE_CREATE_END['payload']['root_gb'], c.volume)

    def test_instance_create_ephemeral_disk_size(self):
        ic = instance.EphemeralDiskSize(None)
        counters = list(ic.process_notification(INSTANCE_CREATE_END))
        self.assertEqual(1, len(counters))
        c = counters[0]
        self.assertEqual(INSTANCE_CREATE_END['payload']['ephemeral_gb'],
                         c.volume)

    def test_instance_exists_instance(self):
        ic = instance.Instance(None)
        counters = list(ic.process_notification(INSTANCE_EXISTS))
        self.assertEqual(1, len(counters))

    def test_instance_exists_metadata_list(self):
        ic = instance.Instance(None)
        counters = list(ic.process_notification(INSTANCE_EXISTS_METADATA_LIST))
        self.assertEqual(1, len(counters))

    def test_instance_exists_flavor(self):
        ic = instance.Instance(None)
        counters = list(ic.process_notification(INSTANCE_EXISTS))
        self.assertEqual(1, len(counters))

    def test_instance_delete_instance(self):
        ic = instance.Instance(None)
        counters = list(ic.process_notification(INSTANCE_DELETE_START))
        self.assertEqual(1, len(counters))

    def test_instance_delete_flavor(self):
        ic = instance.Instance(None)
        counters = list(ic.process_notification(INSTANCE_DELETE_START))
        self.assertEqual(1, len(counters))

    def test_instance_finish_resize_instance(self):
        ic = instance.Instance(None)
        counters = list(ic.process_notification(INSTANCE_FINISH_RESIZE_END))
        self.assertEqual(1, len(counters))
        c = counters[0]
        self.assertEqual(1, c.volume)
        self._verify_user_metadata(c.resource_metadata)

    def test_instance_finish_resize_flavor(self):
        ic = instance.InstanceFlavor(None)
        counters = list(ic.process_notification(INSTANCE_FINISH_RESIZE_END))
        self.assertEqual(1, len(counters))
        c = counters[0]
        self.assertEqual(1, c.volume)
        self.assertEqual('instance:m1.small', c.name)
        self._verify_user_metadata(c.resource_metadata)

    def test_instance_finish_resize_memory(self):
        ic = instance.Memory(None)
        counters = list(ic.process_notification(INSTANCE_FINISH_RESIZE_END))
        self.assertEqual(1, len(counters))
        c = counters[0]
        self.assertEqual(INSTANCE_FINISH_RESIZE_END['payload']['memory_mb'],
                         c.volume)
        self._verify_user_metadata(c.resource_metadata)

    def test_instance_finish_resize_vcpus(self):
        ic = instance.VCpus(None)
        counters = list(ic.process_notification(INSTANCE_FINISH_RESIZE_END))
        self.assertEqual(1, len(counters))
        c = counters[0]
        self.assertEqual(INSTANCE_FINISH_RESIZE_END['payload']['vcpus'],
                         c.volume)
        self._verify_user_metadata(c.resource_metadata)

    def test_instance_resize_finish_instance(self):
        ic = instance.Instance(None)
        counters = list(ic.process_notification(INSTANCE_FINISH_RESIZE_END))
        self.assertEqual(1, len(counters))
        c = counters[0]
        self.assertEqual(1, c.volume)
        self._verify_user_metadata(c.resource_metadata)

    def test_instance_resize_finish_flavor(self):
        ic = instance.InstanceFlavor(None)
        counters = list(ic.process_notification(INSTANCE_RESIZE_REVERT_END))
        self.assertEqual(1, len(counters))
        c = counters[0]
        self.assertEqual(1, c.volume)
        self.assertEqual('instance:m1.tiny', c.name)
        self._verify_user_metadata(c.resource_metadata)

    def test_instance_resize_finish_memory(self):
        ic = instance.Memory(None)
        counters = list(ic.process_notification(INSTANCE_RESIZE_REVERT_END))
        self.assertEqual(1, len(counters))
        c = counters[0]
        self.assertEqual(INSTANCE_RESIZE_REVERT_END['payload']['memory_mb'],
                         c.volume)
        self._verify_user_metadata(c.resource_metadata)

    def test_instance_resize_finish_vcpus(self):
        ic = instance.VCpus(None)
        counters = list(ic.process_notification(INSTANCE_RESIZE_REVERT_END))
        self.assertEqual(1, len(counters))
        c = counters[0]
        self.assertEqual(INSTANCE_RESIZE_REVERT_END['payload']['vcpus'],
                         c.volume)
        self._verify_user_metadata(c.resource_metadata)

    def test_instance_delete_samples(self):
        ic = instance.InstanceDelete(None)
        counters = list(ic.process_notification(INSTANCE_DELETE_SAMPLES))
        self.assertEqual(2, len(counters))
        names = [c.name for c in counters]
        self.assertEqual(['sample-name1', 'sample-name2'], names)
        c = counters[0]
        self._verify_user_metadata(c.resource_metadata)

    def test_instance_scheduled(self):
        ic = instance.InstanceScheduled(None)

        self.assertIn(INSTANCE_SCHEDULED['event_type'],
                      ic.event_types)

        counters = list(ic.process_notification(INSTANCE_SCHEDULED))
        self.assertEqual(1, len(counters))
        names = [c.name for c in counters]
        self.assertEqual(['instance.scheduled'], names)
        rid = [c.resource_id for c in counters]
        self.assertEqual(['fake-uuid1-1'], rid)

########NEW FILE########
__FILENAME__ = base
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 eNovance <licensing@enovance.com>
# Copyright © 2012 Red Hat, Inc
#
# Author: Julien Danjou <julien@danjou.info>
# Author: Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import mock

import ceilometer.tests.base as base


class TestPollsterBase(base.BaseTestCase):

    def setUp(self):
        super(TestPollsterBase, self).setUp()

        self.addCleanup(mock.patch.stopall)

        self.inspector = mock.Mock()
        self.instance = mock.MagicMock()
        self.instance.name = 'instance-00000001'
        setattr(self.instance, 'OS-EXT-SRV-ATTR:instance_name',
                self.instance.name)
        self.instance.id = 1
        self.instance.flavor = {'name': 'm1.small', 'id': 2, 'vcpus': 1,
                                'ram': 512, 'disk': 20, 'ephemeral': 0}
        self.instance.status = 'active'

        patch_virt = mock.patch('ceilometer.compute.virt.inspector'
                                '.get_hypervisor_inspector',
                                mock.Mock(return_value=self.inspector))
        patch_virt.start()

########NEW FILE########
__FILENAME__ = test_cpu
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 eNovance <licensing@enovance.com>
# Copyright © 2012 Red Hat, Inc
#
# Author: Julien Danjou <julien@danjou.info>
# Author: Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import time

import mock
import six

from ceilometer.compute import manager
from ceilometer.compute.pollsters import cpu
from ceilometer.compute.virt import inspector as virt_inspector
from ceilometer.tests.compute.pollsters import base


class TestCPUPollster(base.TestPollsterBase):

    def setUp(self):
        super(TestCPUPollster, self).setUp()

    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    def test_get_samples(self):
        next_value = iter((
            virt_inspector.CPUStats(time=1 * (10 ** 6), number=2),
            virt_inspector.CPUStats(time=3 * (10 ** 6), number=2),
            # cpu_time resets on instance restart
            virt_inspector.CPUStats(time=2 * (10 ** 6), number=2),
        ))

        def inspect_cpus(name):
            return six.next(next_value)

        self.inspector.inspect_cpus = mock.Mock(side_effect=inspect_cpus)

        mgr = manager.AgentManager()
        pollster = cpu.CPUPollster()

        def _verify_cpu_metering(expected_time):
            cache = {}
            samples = list(pollster.get_samples(mgr, cache, [self.instance]))
            self.assertEqual(1, len(samples))
            self.assertEqual(set(['cpu']), set([s.name for s in samples]))
            self.assertEqual(expected_time, samples[0].volume)
            self.assertEqual(2, samples[0].resource_metadata.get('cpu_number'))
            # ensure elapsed time between polling cycles is non-zero
            time.sleep(0.001)

        _verify_cpu_metering(1 * (10 ** 6))
        _verify_cpu_metering(3 * (10 ** 6))
        _verify_cpu_metering(2 * (10 ** 6))

    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    def test_get_samples_no_caching(self):
        cpu_stats = virt_inspector.CPUStats(time=1 * (10 ** 6), number=2)
        self.inspector.inspect_cpus = mock.Mock(return_value=cpu_stats)

        mgr = manager.AgentManager()
        pollster = cpu.CPUPollster()

        cache = {}
        samples = list(pollster.get_samples(mgr, cache, [self.instance]))
        self.assertEqual(1, len(samples))
        self.assertEqual(10 ** 6, samples[0].volume)
        self.assertEqual(0, len(cache))


class TestCPUUtilPollster(base.TestPollsterBase):

    def setUp(self):
        super(TestCPUUtilPollster, self).setUp()

    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    def test_get_samples(self):
        next_value = iter((
            virt_inspector.CPUUtilStats(util=40),
            virt_inspector.CPUUtilStats(util=60),
        ))

        def inspect_cpu_util(name, duration):
            return six.next(next_value)

        self.inspector.inspect_cpu_util = \
            mock.Mock(side_effect=inspect_cpu_util)

        mgr = manager.AgentManager()
        pollster = cpu.CPUUtilPollster()

        def _verify_cpu_util_metering(expected_util):
            cache = {}
            samples = list(pollster.get_samples(mgr, cache, [self.instance]))
            self.assertEqual(1, len(samples))
            self.assertEqual(set(['cpu_util']),
                             set([s.name for s in samples]))
            self.assertEqual(expected_util, samples[0].volume)

        _verify_cpu_util_metering(40)
        _verify_cpu_util_metering(60)

########NEW FILE########
__FILENAME__ = test_diskio
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 eNovance <licensing@enovance.com>
# Copyright © 2012 Red Hat, Inc
#
# Author: Julien Danjou <julien@danjou.info>
# Author: Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import mock

from ceilometer.compute import manager
from ceilometer.compute.pollsters import disk
from ceilometer.compute.virt import inspector as virt_inspector
from ceilometer.tests.compute.pollsters import base


class TestDiskPollsters(base.TestPollsterBase):

    DISKS = [
        (virt_inspector.Disk(device='vda'),
         virt_inspector.DiskStats(read_bytes=1L, read_requests=2L,
                                  write_bytes=3L, write_requests=4L,
                                  errors=-1L))
    ]

    def setUp(self):
        super(TestDiskPollsters, self).setUp()
        self.inspector.inspect_disks = mock.Mock(return_value=self.DISKS)

    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    def _check_get_samples(self, factory, name, expected_volume):
        pollster = factory()

        mgr = manager.AgentManager()
        cache = {}
        samples = list(pollster.get_samples(mgr, cache, [self.instance]))
        self.assertIsNotEmpty(samples)
        self.assertIn(pollster.CACHE_KEY_DISK, cache)
        self.assertIn(self.instance.name, cache[pollster.CACHE_KEY_DISK])

        self.assertEqual(set([name]), set([s.name for s in samples]))

        match = [s for s in samples if s.name == name]
        self.assertEqual(len(match), 1, 'missing counter %s' % name)
        self.assertEqual(expected_volume, match[0].volume)
        self.assertEqual('cumulative', match[0].type)

    def test_disk_read_requests(self):
        self._check_get_samples(disk.ReadRequestsPollster,
                                'disk.read.requests', 2L)

    def test_disk_read_bytes(self):
        self._check_get_samples(disk.ReadBytesPollster,
                                'disk.read.bytes', 1L)

    def test_disk_write_requests(self):
        self._check_get_samples(disk.WriteRequestsPollster,
                                'disk.write.requests', 4L)

    def test_disk_write_bytes(self):
        self._check_get_samples(disk.WriteBytesPollster,
                                'disk.write.bytes', 3L)


class TestDiskRatePollsters(base.TestPollsterBase):

    DISKS = [
        (virt_inspector.Disk(device='disk1'),
         virt_inspector.DiskRateStats(1024, 300, 5120, 700)),

        (virt_inspector.Disk(device='disk2'),
         virt_inspector.DiskRateStats(2048, 400, 6144, 800))
    ]

    def setUp(self):
        super(TestDiskRatePollsters, self).setUp()
        self.inspector.inspect_disk_rates = \
            mock.Mock(return_value=self.DISKS)

    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    def _check_get_samples(self, factory, sample_name, expected_volume):
        pollster = factory()

        mgr = manager.AgentManager()
        cache = {}
        samples = list(pollster.get_samples(mgr, cache, [self.instance]))
        self.assertIsNotEmpty(samples)
        self.assertIsNotNone(samples)
        self.assertIn(pollster.CACHE_KEY_DISK_RATE, cache)
        self.assertIn(self.instance.id, cache[pollster.CACHE_KEY_DISK_RATE])

        self.assertEqual(set([sample_name]), set([s.name for s in samples]))

        match = [s for s in samples if s.name == sample_name]
        self.assertEqual(1, len(match), 'missing counter %s' % sample_name)
        self.assertEqual(expected_volume, match[0].volume)
        self.assertEqual('gauge', match[0].type)

    def test_disk_read_bytes_rate(self):
        self._check_get_samples(disk.ReadBytesRatePollster,
                                'disk.read.bytes.rate', 3072L)

    def test_disk_read_requests_rate(self):
        self._check_get_samples(disk.ReadRequestsRatePollster,
                                'disk.read.requests.rate', 700L)

    def test_disk_write_bytes_rate(self):
        self._check_get_samples(disk.WriteBytesRatePollster,
                                'disk.write.bytes.rate', 11264L)

    def test_disk_write_requests_rate(self):
        self._check_get_samples(disk.WriteRequestsRatePollster,
                                'disk.write.requests.rate', 1500L)

########NEW FILE########
__FILENAME__ = test_instance
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 eNovance <licensing@enovance.com>
# Copyright © 2012 Red Hat, Inc
#
# Author: Julien Danjou <julien@danjou.info>
# Author: Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import mock

from ceilometer.compute import manager
from ceilometer.compute.pollsters import instance as pollsters_instance
from ceilometer.tests.compute.pollsters import base


class TestInstancePollster(base.TestPollsterBase):

    def setUp(self):
        super(TestInstancePollster, self).setUp()

    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    def test_get_samples_instance(self):
        mgr = manager.AgentManager()
        pollster = pollsters_instance.InstancePollster()
        samples = list(pollster.get_samples(mgr, {}, [self.instance]))
        self.assertEqual(1, len(samples))
        self.assertEqual('instance', samples[0].name)
        self.assertEqual(1, samples[0].resource_metadata['vcpus'])
        self.assertEqual(512, samples[0].resource_metadata['memory_mb'])
        self.assertEqual(20, samples[0].resource_metadata['disk_gb'])
        self.assertEqual(20, samples[0].resource_metadata['root_gb'])
        self.assertEqual(0, samples[0].resource_metadata['ephemeral_gb'])
        self.assertEqual('active', samples[0].resource_metadata['status'])

    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    def test_get_samples_instance_flavor(self):
        mgr = manager.AgentManager()
        pollster = pollsters_instance.InstanceFlavorPollster()
        samples = list(pollster.get_samples(mgr, {}, [self.instance]))
        self.assertEqual(1, len(samples))
        self.assertEqual('instance:m1.small', samples[0].name)

########NEW FILE########
__FILENAME__ = test_location_metadata
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 eNovance <licensing@enovance.com>
# Copyright © 2012 Red Hat, Inc
#
# Author: Julien Danjou <julien@danjou.info>
# Author: Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests for the compute pollsters.
"""

import mock

from ceilometer.compute import manager
from ceilometer.compute.pollsters import util
from ceilometer.openstack.common import test


class FauxInstance(object):

    def __init__(self, **kwds):
        for name, value in kwds.items():
            setattr(self, name, value)

    def __getitem__(self, key):
        return getattr(self, key)

    def get(self, key, default):
        try:
            return getattr(self, key)
        except AttributeError:
            return default


class TestLocationMetadata(test.BaseTestCase):

    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    def setUp(self):
        self.manager = manager.AgentManager()
        super(TestLocationMetadata, self).setUp()

        # Mimics an instance returned from nova api call
        self.INSTANCE_PROPERTIES = {'name': 'display name',
                                    'OS-EXT-SRV-ATTR:instance_name':
                                    'instance-000001',
                                    'OS-EXT-AZ:availability_zone':
                                    'foo-zone',
                                    'reservation_id': 'reservation id',
                                    'architecture': 'x86_64',
                                    'kernel_id': 'kernel id',
                                    'os_type': 'linux',
                                    'ramdisk_id': 'ramdisk id',
                                    'status': 'active',
                                    'ephemeral_gb': 0,
                                    'root_gb': 20,
                                    'disk_gb': 20,
                                    'image': {'id': 1,
                                              'links': [{"rel": "bookmark",
                                                         'href': 2}]},
                                    'hostId': '1234-5678',
                                    'flavor': {'id': 1,
                                               'disk': 20,
                                               'ram': 512,
                                               'vcpus': 2,
                                               'ephemeral': 0},
                                    'metadata': {'metering.autoscale.group':
                                                 'X' * 512,
                                                 'metering.ephemeral_gb': 42}}

        self.instance = FauxInstance(**self.INSTANCE_PROPERTIES)

    def test_metadata(self):
        md = util._get_metadata_from_object(self.instance)
        for prop, value in self.INSTANCE_PROPERTIES.iteritems():
            if prop not in ("metadata"):
                # Special cases
                if prop == 'name':
                    prop = 'display_name'
                elif prop == 'hostId':
                    prop = "host"
                elif prop == 'OS-EXT-SRV-ATTR:instance_name':
                    prop = 'name'
                self.assertEqual(value, md[prop])
        user_metadata = md['user_metadata']
        expected = self.INSTANCE_PROPERTIES[
            'metadata']['metering.autoscale.group'][:256]
        self.assertEqual(expected, user_metadata['autoscale_group'])
        self.assertEqual(1, len(user_metadata))

    def test_metadata_empty_image(self):
        self.INSTANCE_PROPERTIES['image'] = None
        self.instance = FauxInstance(**self.INSTANCE_PROPERTIES)
        md = util._get_metadata_from_object(self.instance)
        self.assertIsNone(md['image'])
        self.assertIsNone(md['image_ref'])
        self.assertIsNone(md['image_ref_url'])

    def test_metadata_image_through_conductor(self):
        # There should be no links here, should default to None
        self.INSTANCE_PROPERTIES['image'] = {'id': 1}
        self.instance = FauxInstance(**self.INSTANCE_PROPERTIES)
        md = util._get_metadata_from_object(self.instance)
        self.assertEqual(1, md['image_ref'])
        self.assertIsNone(md['image_ref_url'])

########NEW FILE########
__FILENAME__ = test_memory
# Copyright (c) 2014 VMware, Inc.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import mock
import six

from ceilometer.compute import manager
from ceilometer.compute.pollsters import memory
from ceilometer.compute.virt import inspector as virt_inspector
from ceilometer.tests.compute.pollsters import base


class TestMemoryPollster(base.TestPollsterBase):

    def setUp(self):
        super(TestMemoryPollster, self).setUp()

    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    def test_get_samples(self):
        next_value = iter((
            virt_inspector.MemoryUsageStats(usage=1.0),
            virt_inspector.MemoryUsageStats(usage=2.0),
        ))

        def inspect_memory_usage(instance, duration):
            return six.next(next_value)

        self.inspector.inspect_memory_usage = \
            mock.Mock(side_effect=inspect_memory_usage)

        mgr = manager.AgentManager()
        pollster = memory.MemoryUsagePollster()

        def _verify_memory_metering(expected_memory_mb):
            cache = {}
            samples = list(pollster.get_samples(mgr, cache, [self.instance]))
            self.assertEqual(1, len(samples))
            self.assertEqual(set(['memory.usage']),
                             set([s.name for s in samples]))
            self.assertEqual(expected_memory_mb, samples[0].volume)

        _verify_memory_metering(1.0)
        _verify_memory_metering(2.0)

########NEW FILE########
__FILENAME__ = test_net
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 eNovance <licensing@enovance.com>
# Copyright © 2012 Red Hat, Inc
#
# Author: Julien Danjou <julien@danjou.info>
# Author: Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import mock

from ceilometer.compute import manager
from ceilometer.compute.pollsters import net
from ceilometer.compute.virt import inspector as virt_inspector
from ceilometer.tests.compute.pollsters import base


class TestNetPollster(base.TestPollsterBase):

    def setUp(self):
        super(TestNetPollster, self).setUp()
        self.vnic0 = virt_inspector.Interface(
            name='vnet0',
            fref='fa163e71ec6e',
            mac='fa:16:3e:71:ec:6d',
            parameters=dict(ip='10.0.0.2',
                            projmask='255.255.255.0',
                            projnet='proj1',
                            dhcp_server='10.0.0.1'))
        stats0 = virt_inspector.InterfaceStats(rx_bytes=1L, rx_packets=2L,
                                               tx_bytes=3L, tx_packets=4L)
        self.vnic1 = virt_inspector.Interface(
            name='vnet1',
            fref='fa163e71ec6f',
            mac='fa:16:3e:71:ec:6e',
            parameters=dict(ip='192.168.0.3',
                            projmask='255.255.255.0',
                            projnet='proj2',
                            dhcp_server='10.0.0.2'))
        stats1 = virt_inspector.InterfaceStats(rx_bytes=5L, rx_packets=6L,
                                               tx_bytes=7L, tx_packets=8L)
        self.vnic2 = virt_inspector.Interface(
            name='vnet2',
            fref=None,
            mac='fa:18:4e:72:fc:7e',
            parameters=dict(ip='192.168.0.4',
                            projmask='255.255.255.0',
                            projnet='proj3',
                            dhcp_server='10.0.0.3'))
        stats2 = virt_inspector.InterfaceStats(rx_bytes=9L, rx_packets=10L,
                                               tx_bytes=11L, tx_packets=12L)

        vnics = [
            (self.vnic0, stats0),
            (self.vnic1, stats1),
            (self.vnic2, stats2),
        ]
        self.inspector.inspect_vnics = mock.Mock(return_value=vnics)

    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    def _check_get_samples(self, factory, expected):
        mgr = manager.AgentManager()
        pollster = factory()
        samples = list(pollster.get_samples(mgr, {}, [self.instance]))
        self.assertEqual(3, len(samples))  # one for each nic
        self.assertEqual(set([samples[0].name]),
                         set([s.name for s in samples]))

        def _verify_vnic_metering(ip, expected_volume, expected_rid):
            match = [s for s in samples
                     if s.resource_metadata['parameters']['ip'] == ip
                     ]
            self.assertEqual(len(match), 1, 'missing ip %s' % ip)
            self.assertEqual(expected_volume, match[0].volume)
            self.assertEqual('cumulative', match[0].type)
            self.assertEqual(expected_rid, match[0].resource_id)

        for ip, volume, rid in expected:
            _verify_vnic_metering(ip, volume, rid)

    def test_incoming_bytes(self):
        instance_name_id = "%s-%s" % (self.instance.name, self.instance.id)
        self._check_get_samples(
            net.IncomingBytesPollster,
            [('10.0.0.2', 1L, self.vnic0.fref),
             ('192.168.0.3', 5L, self.vnic1.fref),
             ('192.168.0.4', 9L,
              "%s-%s" % (instance_name_id, self.vnic2.name)),
             ],
        )

    def test_outgoing_bytes(self):
        instance_name_id = "%s-%s" % (self.instance.name, self.instance.id)
        self._check_get_samples(
            net.OutgoingBytesPollster,
            [('10.0.0.2', 3L, self.vnic0.fref),
             ('192.168.0.3', 7L, self.vnic1.fref),
             ('192.168.0.4', 11L,
              "%s-%s" % (instance_name_id, self.vnic2.name)),
             ],
        )

    def test_incoming_packets(self):
        instance_name_id = "%s-%s" % (self.instance.name, self.instance.id)
        self._check_get_samples(
            net.IncomingPacketsPollster,
            [('10.0.0.2', 2L, self.vnic0.fref),
             ('192.168.0.3', 6L, self.vnic1.fref),
             ('192.168.0.4', 10L,
              "%s-%s" % (instance_name_id, self.vnic2.name)),
             ],
        )

    def test_outgoing_packets(self):
        instance_name_id = "%s-%s" % (self.instance.name, self.instance.id)
        self._check_get_samples(
            net.OutgoingPacketsPollster,
            [('10.0.0.2', 4L, self.vnic0.fref),
             ('192.168.0.3', 8L, self.vnic1.fref),
             ('192.168.0.4', 12L,
              "%s-%s" % (instance_name_id, self.vnic2.name)),
             ],
        )


class TestNetPollsterCache(base.TestPollsterBase):

    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    def _check_get_samples_cache(self, factory):
        vnic0 = virt_inspector.Interface(
            name='vnet0',
            fref='fa163e71ec6e',
            mac='fa:16:3e:71:ec:6d',
            parameters=dict(ip='10.0.0.2',
                            projmask='255.255.255.0',
                            projnet='proj1',
                            dhcp_server='10.0.0.1'))
        stats0 = virt_inspector.InterfaceStats(rx_bytes=1L, rx_packets=2L,
                                               tx_bytes=3L, tx_packets=4L)
        vnics = [(vnic0, stats0)]

        mgr = manager.AgentManager()
        pollster = factory()
        cache = {
            pollster.CACHE_KEY_VNIC: {
                self.instance.name: vnics,
            },
        }
        samples = list(pollster.get_samples(mgr, cache, [self.instance]))
        self.assertEqual(1, len(samples))

    def test_incoming_bytes(self):
        self._check_get_samples_cache(net.IncomingBytesPollster)

    def test_outgoing_bytes(self):
        self._check_get_samples_cache(net.OutgoingBytesPollster)

    def test_incoming_packets(self):
        self._check_get_samples_cache(net.IncomingPacketsPollster)

    def test_outgoing_packets(self):
        self._check_get_samples_cache(net.OutgoingPacketsPollster)


class TestNetRatesPollster(base.TestPollsterBase):

    def setUp(self):
        super(TestNetRatesPollster, self).setUp()
        self.vnic0 = virt_inspector.Interface(
            name='vnet0',
            fref='fa163e71ec6e',
            mac='fa:16:3e:71:ec:6d',
            parameters=dict(ip='10.0.0.2',
                            projmask='255.255.255.0',
                            projnet='proj1',
                            dhcp_server='10.0.0.1'))
        stats0 = virt_inspector.InterfaceRateStats(rx_bytes_rate=1L,
                                                   tx_bytes_rate=2L)
        self.vnic1 = virt_inspector.Interface(
            name='vnet1',
            fref='fa163e71ec6f',
            mac='fa:16:3e:71:ec:6e',
            parameters=dict(ip='192.168.0.3',
                            projmask='255.255.255.0',
                            projnet='proj2',
                            dhcp_server='10.0.0.2'))
        stats1 = virt_inspector.InterfaceRateStats(rx_bytes_rate=3L,
                                                   tx_bytes_rate=4L)
        self.vnic2 = virt_inspector.Interface(
            name='vnet2',
            fref=None,
            mac='fa:18:4e:72:fc:7e',
            parameters=dict(ip='192.168.0.4',
                            projmask='255.255.255.0',
                            projnet='proj3',
                            dhcp_server='10.0.0.3'))
        stats2 = virt_inspector.InterfaceRateStats(rx_bytes_rate=5L,
                                                   tx_bytes_rate=6L)

        vnics = [
            (self.vnic0, stats0),
            (self.vnic1, stats1),
            (self.vnic2, stats2),
        ]
        self.inspector.inspect_vnic_rates = mock.Mock(return_value=vnics)

    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    def _check_get_samples(self, factory, expected):
        mgr = manager.AgentManager()
        pollster = factory()
        samples = list(pollster.get_samples(mgr, {}, [self.instance]))
        self.assertEqual(3, len(samples))  # one for each nic
        self.assertEqual(set([samples[0].name]),
                         set([s.name for s in samples]))

        def _verify_vnic_metering(ip, expected_volume, expected_rid):
            match = [s for s in samples
                     if s.resource_metadata['parameters']['ip'] == ip
                     ]
            self.assertEqual(1, len(match), 'missing ip %s' % ip)
            self.assertEqual(expected_volume, match[0].volume)
            self.assertEqual('gauge', match[0].type)
            self.assertEqual(expected_rid, match[0].resource_id)

        for ip, volume, rid in expected:
            _verify_vnic_metering(ip, volume, rid)

    def test_incoming_bytes_rate(self):
        instance_name_id = "%s-%s" % (self.instance.name, self.instance.id)
        self._check_get_samples(
            net.IncomingBytesRatePollster,
            [('10.0.0.2', 1L, self.vnic0.fref),
             ('192.168.0.3', 3L, self.vnic1.fref),
             ('192.168.0.4', 5L,
              "%s-%s" % (instance_name_id, self.vnic2.name)),
             ],
        )

    def test_outgoing_bytes(self):
        instance_name_id = "%s-%s" % (self.instance.name, self.instance.id)
        self._check_get_samples(
            net.OutgoingBytesRatePollster,
            [('10.0.0.2', 2L, self.vnic0.fref),
             ('192.168.0.3', 4L, self.vnic1.fref),
             ('192.168.0.4', 6L,
              "%s-%s" % (instance_name_id, self.vnic2.name)),
             ],
        )

########NEW FILE########
__FILENAME__ = test_manager
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests for ceilometer/agent/manager.py
"""
import mock

from ceilometer import agent
from ceilometer.compute import manager
from ceilometer import nova_client
from ceilometer.openstack.common.fixture import mockpatch
from ceilometer.openstack.common import test
from ceilometer.tests import agentbase


class TestManager(test.BaseTestCase):

    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    def test_load_plugins(self):
        mgr = manager.AgentManager()
        self.assertIsNotNone(list(mgr.pollster_manager))


class TestRunTasks(agentbase.BaseAgentManagerTestCase):

    def _fake_instance(self, name, state):
        instance = mock.MagicMock()
        instance.name = name
        setattr(instance, 'OS-EXT-STS:vm_state', state)
        return instance

    def _raise_exception(self):
        raise Exception

    @staticmethod
    def create_manager():
        return manager.AgentManager()

    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    def setUp(self):
        self.source_resources = False
        super(TestRunTasks, self).setUp()

        # Set up a fake instance value to be returned by
        # instance_get_all_by_host() so when the manager gets the list
        # of instances to poll we can control the results.
        self.instances = [self._fake_instance('doing', 'active'),
                          self._fake_instance('resting', 'paused')]
        stillborn_instance = self._fake_instance('stillborn', 'error')

        self.useFixture(mockpatch.PatchObject(
            nova_client.Client,
            'instance_get_all_by_host',
            side_effect=lambda *x: self.instances + [stillborn_instance]))

    def test_setup_polling_tasks(self):
        super(TestRunTasks, self).test_setup_polling_tasks()
        self.assertEqual(self.Pollster.samples[0][1], self.instances)

    def test_interval_exception_isolation(self):
        super(TestRunTasks, self).test_interval_exception_isolation()
        self.assertEqual(1, len(self.PollsterException.samples))
        self.assertEqual(1, len(self.PollsterExceptionAnother.samples))

    def test_manager_exception_persistency(self):
        super(TestRunTasks, self).test_manager_exception_persistency()
        with mock.patch.object(nova_client.Client, 'instance_get_all_by_host',
                               side_effect=lambda *x: self._raise_exception()):
            mgr = manager.AgentManager()
            polling_task = agent.PollingTask(mgr)
            polling_task.poll_and_publish()

    def self_local_instances_default_agent_discovery(self):
        self.setup_pipeline()
        self.assertEqual(self.mgr.default_discovery, ['local_instances'])
        polling_tasks = self.mgr.setup_polling_tasks()
        self.mgr.interval_task(polling_tasks.get(60))
        self._verify_discovery_params([None])
        self.assertEqual(set(self.Pollster.resources),
                         set(self.instances))

########NEW FILE########
__FILENAME__ = test_inspector
# Copyright 2013 Cloudbase Solutions Srl
#
# Author: Alessandro Pilotti <apilotti@cloudbasesolutions.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""
Tests for Hyper-V inspector.
"""

import mock

from ceilometer.compute.virt.hyperv import inspector as hyperv_inspector
from ceilometer.openstack.common import test
from ceilometer.openstack.common import units


class TestHyperVInspection(test.BaseTestCase):

    def setUp(self):
        self._inspector = hyperv_inspector.HyperVInspector()
        self._inspector._utils = mock.MagicMock()

        super(TestHyperVInspection, self).setUp()

    def test_inspect_instances(self):
        fake_name = 'fake_name'
        fake_uuid = 'fake_uuid'
        fake_instances = [(fake_name, fake_uuid)]
        self._inspector._utils.get_all_vms.return_value = fake_instances

        inspected_instances = list(self._inspector.inspect_instances())

        self.assertEqual(1, len(inspected_instances))
        self.assertEqual(fake_name, inspected_instances[0].name)
        self.assertEqual(fake_uuid, inspected_instances[0].UUID)

    def test_inspect_cpus(self):
        fake_instance_name = 'fake_instance_name'
        fake_host_cpu_clock = 1000
        fake_host_cpu_count = 2
        fake_cpu_clock_used = 2000
        fake_cpu_count = 3000
        fake_uptime = 4000

        fake_cpu_percent_used = (fake_cpu_clock_used /
                                 float(fake_host_cpu_clock * fake_cpu_count))
        fake_cpu_time = (long(fake_uptime * fake_cpu_percent_used) *
                         1000)

        self._inspector._utils.get_host_cpu_info.return_value = (
            fake_host_cpu_clock, fake_host_cpu_count)

        self._inspector._utils.get_cpu_metrics.return_value = (
            fake_cpu_clock_used, fake_cpu_count, fake_uptime)

        cpu_stats = self._inspector.inspect_cpus(fake_instance_name)

        self.assertEqual(fake_cpu_count, cpu_stats.number)
        self.assertEqual(fake_cpu_time, cpu_stats.time)

    def test_inspect_vnics(self):
        fake_instance_name = 'fake_instance_name'
        fake_rx_mb = 1000
        fake_tx_mb = 2000
        fake_element_name = 'fake_element_name'
        fake_address = 'fake_address'

        self._inspector._utils.get_vnic_metrics.return_value = [{
            'rx_mb': fake_rx_mb,
            'tx_mb': fake_tx_mb,
            'element_name': fake_element_name,
            'address': fake_address}]

        inspected_vnics = list(self._inspector.inspect_vnics(
            fake_instance_name))

        self.assertEqual(1, len(inspected_vnics))
        self.assertEqual(2, len(inspected_vnics[0]))

        inspected_vnic, inspected_stats = inspected_vnics[0]

        self.assertEqual(fake_element_name, inspected_vnic.name)
        self.assertEqual(fake_address, inspected_vnic.mac)

        self.assertEqual(fake_rx_mb * units.Mi, inspected_stats.rx_bytes)
        self.assertEqual(fake_tx_mb * units.Mi, inspected_stats.tx_bytes)

    def test_inspect_disks(self):
        fake_instance_name = 'fake_instance_name'
        fake_read_mb = 1000
        fake_write_mb = 2000
        fake_instance_id = "fake_fake_instance_id"
        fake_host_resource = "fake_host_resource"

        fake_device = {"instance_id": fake_instance_id,
                       "host_resource": fake_host_resource}

        self._inspector._utils.get_disk_metrics.return_value = [{
            'read_mb': fake_read_mb,
            'write_mb': fake_write_mb,
            'instance_id': fake_instance_id,
            'host_resource': fake_host_resource}]

        inspected_disks = list(self._inspector.inspect_disks(
            fake_instance_name))

        self.assertEqual(1, len(inspected_disks))
        self.assertEqual(2, len(inspected_disks[0]))

        inspected_disk, inspected_stats = inspected_disks[0]

        self.assertEqual(fake_device, inspected_disk.device)

        self.assertEqual(fake_read_mb * units.Mi, inspected_stats.read_bytes)
        self.assertEqual(fake_write_mb * units.Mi, inspected_stats.write_bytes)

########NEW FILE########
__FILENAME__ = test_utilsv2
# Copyright 2013 Cloudbase Solutions Srl
#
# Author: Alessandro Pilotti <apilotti@cloudbasesolutions.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""
Tests for Hyper-V utilsv2.
"""

import mock

from ceilometer.compute.virt.hyperv import utilsv2 as utilsv2
from ceilometer.compute.virt import inspector
from ceilometer.openstack.common import test


class TestUtilsV2(test.BaseTestCase):

    _FAKE_RETURN_CLASS = 'fake_return_class'

    def setUp(self):
        self._utils = utilsv2.UtilsV2()
        self._utils._conn = mock.MagicMock()
        self._utils._conn_cimv2 = mock.MagicMock()

        super(TestUtilsV2, self).setUp()

    def test_get_host_cpu_info(self):
        _fake_clock_speed = 1000
        _fake_cpu_count = 2

        mock_cpu = mock.MagicMock()
        mock_cpu.MaxClockSpeed = _fake_clock_speed

        self._utils._conn_cimv2.Win32_Processor.return_value = [mock_cpu,
                                                                mock_cpu]
        cpu_info = self._utils.get_host_cpu_info()

        self.assertEqual(_fake_clock_speed, cpu_info[0])
        self.assertEqual(_fake_cpu_count, cpu_info[1])

    def test_get_all_vms(self):
        fake_vm_element_name = "fake_vm_element_name"
        fake_vm_name = "fake_vm_name"

        mock_vm = mock.MagicMock()
        mock_vm.ElementName = fake_vm_element_name
        mock_vm.Name = fake_vm_name
        self._utils._conn.Msvm_ComputerSystem.return_value = [mock_vm]

        vms = self._utils.get_all_vms()

        self.assertEqual((fake_vm_element_name, fake_vm_name), vms[0])

    def test_get_cpu_metrics(self):
        fake_vm_element_name = "fake_vm_element_name"
        fake_cpu_count = 2
        fake_uptime = 1000
        fake_cpu_metric_val = 2000

        self._utils._lookup_vm = mock.MagicMock()
        self._utils._lookup_vm().OnTimeInMilliseconds = fake_uptime

        self._utils._get_vm_resources = mock.MagicMock()
        mock_res = self._utils._get_vm_resources()[0]
        mock_res.VirtualQuantity = fake_cpu_count

        self._utils._get_metrics = mock.MagicMock()
        self._utils._get_metrics()[0].MetricValue = fake_cpu_metric_val

        cpu_metrics = self._utils.get_cpu_metrics(fake_vm_element_name)

        self.assertEqual(3, len(cpu_metrics))
        self.assertEqual(fake_cpu_metric_val, cpu_metrics[0])
        self.assertEqual(fake_cpu_count, cpu_metrics[1])
        self.assertEqual(fake_uptime, cpu_metrics[2])

    @mock.patch('ceilometer.compute.virt.hyperv.utilsv2.UtilsV2'
                '._sum_metric_values_by_defs')
    @mock.patch('ceilometer.compute.virt.hyperv.utilsv2.UtilsV2'
                '._get_metric_value_instances')
    def test_get_vnic_metrics(self, mock_get_instances, mock_get_by_defs):
        fake_vm_element_name = "fake_vm_element_name"
        fake_vnic_element_name = "fake_vnic_name"
        fake_vnic_address = "fake_vnic_address"
        fake_vnic_path = "fake_vnic_path"
        fake_rx_mb = 1000
        fake_tx_mb = 2000

        self._utils._lookup_vm = mock.MagicMock()
        self._utils._get_vm_resources = mock.MagicMock()

        mock_port = mock.MagicMock()
        mock_port.Parent = fake_vnic_path

        mock_vnic = mock.MagicMock()
        mock_vnic.path_.return_value = fake_vnic_path
        mock_vnic.ElementName = fake_vnic_element_name
        mock_vnic.Address = fake_vnic_address

        self._utils._get_vm_resources.side_effect = [[mock_port], [mock_vnic]]

        self._utils._get_metric_def = mock.MagicMock()

        mock_get_by_defs.return_value = [fake_rx_mb, fake_tx_mb]

        vnic_metrics = list(self._utils.get_vnic_metrics(fake_vm_element_name))

        self.assertEqual(1, len(vnic_metrics))
        self.assertEqual(fake_rx_mb, vnic_metrics[0]['rx_mb'])
        self.assertEqual(fake_tx_mb, vnic_metrics[0]['tx_mb'])
        self.assertEqual(fake_vnic_element_name,
                         vnic_metrics[0]['element_name'])
        self.assertEqual(fake_vnic_address, vnic_metrics[0]['address'])

    def test_get_disk_metrics(self):
        fake_vm_element_name = "fake_vm_element_name"
        fake_host_resource = "fake_host_resource"
        fake_instance_id = "fake_instance_id"
        fake_read_mb = 1000
        fake_write_mb = 2000

        self._utils._lookup_vm = mock.MagicMock()

        mock_disk = mock.MagicMock()
        mock_disk.HostResource = [fake_host_resource]
        mock_disk.InstanceID = fake_instance_id
        self._utils._get_vm_resources = mock.MagicMock(
            return_value=[mock_disk])

        self._utils._get_metric_def = mock.MagicMock()

        self._utils._get_metric_values = mock.MagicMock()
        self._utils._get_metric_values.return_value = [fake_read_mb,
                                                       fake_write_mb]

        disk_metrics = list(self._utils.get_disk_metrics(fake_vm_element_name))

        self.assertEqual(1, len(disk_metrics))
        self.assertEqual(fake_read_mb, disk_metrics[0]['read_mb'])
        self.assertEqual(fake_write_mb, disk_metrics[0]['write_mb'])
        self.assertEqual(fake_instance_id, disk_metrics[0]['instance_id'])
        self.assertEqual(fake_host_resource, disk_metrics[0]['host_resource'])

    def test_get_metric_value_instances(self):
        mock_el1 = mock.MagicMock()
        mock_associator = mock.MagicMock()
        mock_el1.associators.return_value = [mock_associator]

        mock_el2 = mock.MagicMock()
        mock_el2.associators.return_value = []

        returned = self._utils._get_metric_value_instances(
            [mock_el1, mock_el2], self._FAKE_RETURN_CLASS)

        self.assertEqual([mock_associator], returned)

    def test_lookup_vm(self):
        fake_vm_element_name = "fake_vm_element_name"
        fake_vm = "fake_vm"
        self._utils._conn.Msvm_ComputerSystem.return_value = [fake_vm]

        vm = self._utils._lookup_vm(fake_vm_element_name)

        self.assertEqual(fake_vm, vm)

    def test_lookup_vm_not_found(self):
        fake_vm_element_name = "fake_vm_element_name"
        self._utils._conn.Msvm_ComputerSystem.return_value = []

        self.assertRaises(inspector.InstanceNotFoundException,
                          self._utils._lookup_vm, fake_vm_element_name)

    def test_lookup_vm_duplicate_found(self):
        fake_vm_element_name = "fake_vm_element_name"
        fake_vm = "fake_vm"
        self._utils._conn.Msvm_ComputerSystem.return_value = [fake_vm, fake_vm]

        self.assertRaises(utilsv2.HyperVException,
                          self._utils._lookup_vm, fake_vm_element_name)

    def test_get_metric_values(self):
        fake_metric_def_id = "fake_metric_def_id"
        fake_metric_value = "1000"

        mock_metric = mock.MagicMock()
        mock_metric.MetricDefinitionId = fake_metric_def_id
        mock_metric.MetricValue = fake_metric_value

        mock_element = mock.MagicMock()
        mock_element.associators.return_value = [mock_metric]

        mock_metric_def = mock.MagicMock()
        mock_metric_def.Id = fake_metric_def_id

        metric_values = self._utils._get_metric_values(mock_element,
                                                       [mock_metric_def])

        self.assertEqual(1, len(metric_values))
        self.assertEqual(long(fake_metric_value), metric_values[0])

    def test_get_vm_setting_data(self):
        mock_vm_s = mock.MagicMock()
        mock_vm_s.VirtualSystemType = self._utils._VIRTUAL_SYSTEM_TYPE_REALIZED

        mock_vm = mock.MagicMock()
        mock_vm.associators.return_value = [mock_vm_s]

        vm_setting_data = self._utils._get_vm_setting_data(mock_vm)

        self.assertEqual(mock_vm_s, vm_setting_data)

########NEW FILE########
__FILENAME__ = test_inspector
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 Red Hat, Inc
#
# Author: Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests for libvirt inspector.
"""

import contextlib

import fixtures
import mock

from ceilometer.compute.virt import inspector as virt_inspector
from ceilometer.compute.virt.libvirt import inspector as libvirt_inspector
from ceilometer.openstack.common import test


class TestLibvirtInspection(test.BaseTestCase):

    def setUp(self):
        super(TestLibvirtInspection, self).setUp()
        self.instance_name = 'instance-00000001'
        self.inspector = libvirt_inspector.LibvirtInspector()
        self.inspector.connection = mock.Mock()
        libvirt_inspector.libvirt = mock.Mock()
        libvirt_inspector.libvirt.VIR_DOMAIN_SHUTOFF = 5
        self.domain = mock.Mock()
        self.addCleanup(mock.patch.stopall)

    def test_inspect_instances(self):
        class FakeDomain(object):
            def name(self):
                return 'fake_name'

            def UUIDString(self):
                return 'uuid'

        fake_domain = FakeDomain()
        connection = self.inspector.connection
        with contextlib.nested(mock.patch.object(connection, 'numOfDomains',
                                                 return_value=1),
                               mock.patch.object(connection, 'listDomainsID',
                                                 return_value=[42]),
                               mock.patch.object(connection, 'lookupByID',
                                                 return_value=fake_domain)):
            inspected_instances = list(self.inspector.inspect_instances())
            self.assertEqual(1, len(inspected_instances))
            inspected_instance = inspected_instances[0]
            self.assertEqual('fake_name', inspected_instance.name)
            self.assertEqual('uuid', inspected_instance.UUID)

    def test_inspect_cpus(self):
        with contextlib.nested(mock.patch.object(self.inspector.connection,
                                                 'lookupByName',
                                                 return_value=self.domain),
                               mock.patch.object(self.domain, 'info',
                                                 return_value=(0L, 0L, 0L,
                                                               2L, 999999L))):
                cpu_info = self.inspector.inspect_cpus(self.instance_name)
                self.assertEqual(2L, cpu_info.number)
                self.assertEqual(999999L, cpu_info.time)

    def test_inspect_vnics(self):
        dom_xml = """
             <domain type='kvm'>
                 <devices>
                    <!-- NOTE(dprince): interface with no target -->
                    <interface type='bridge'>
                       <mac address='fa:16:3e:93:31:5a'/>
                       <source bridge='br100'/>
                       <model type='virtio'/>
                       <address type='pci' domain='0x0000' bus='0x00' \
                       slot='0x03' function='0x0'/>
                    </interface>
                    <!-- NOTE(dprince): interface with no mac -->
                    <interface type='bridge'>
                       <source bridge='br100'/>
                       <target dev='foo'/>
                       <model type='virtio'/>
                       <address type='pci' domain='0x0000' bus='0x00' \
                       slot='0x03' function='0x0'/>
                    </interface>
                    <interface type='bridge'>
                       <mac address='fa:16:3e:71:ec:6d'/>
                       <source bridge='br100'/>
                       <target dev='vnet0'/>
                       <filterref filter=
                        'nova-instance-00000001-fa163e71ec6d'>
                         <parameter name='DHCPSERVER' value='10.0.0.1'/>
                         <parameter name='IP' value='10.0.0.2'/>
                         <parameter name='PROJMASK' value='255.255.255.0'/>
                         <parameter name='PROJNET' value='10.0.0.0'/>
                       </filterref>
                       <alias name='net0'/>
                     </interface>
                     <interface type='bridge'>
                       <mac address='fa:16:3e:71:ec:6e'/>
                       <source bridge='br100'/>
                       <target dev='vnet1'/>
                       <filterref filter=
                        'nova-instance-00000001-fa163e71ec6e'>
                         <parameter name='DHCPSERVER' value='192.168.0.1'/>
                         <parameter name='IP' value='192.168.0.2'/>
                         <parameter name='PROJMASK' value='255.255.255.0'/>
                         <parameter name='PROJNET' value='192.168.0.0'/>
                       </filterref>
                       <alias name='net1'/>
                     </interface>
                     <interface type='bridge'>
                       <mac address='fa:16:3e:96:33:f0'/>
                       <source bridge='qbr420008b3-7c'/>
                       <target dev='vnet2'/>
                       <model type='virtio'/>
                       <address type='pci' domain='0x0000' bus='0x00' \
                       slot='0x03' function='0x0'/>
                    </interface>
                 </devices>
             </domain>
        """

        interface_stats = {
            'vnet0': (1L, 2L, 0L, 0L, 3L, 4L, 0L, 0L),
            'vnet1': (5L, 6L, 0L, 0L, 7L, 8L, 0L, 0L),
            'vnet2': (9L, 10L, 0L, 0L, 11L, 12L, 0L, 0L),
        }
        interfaceStats = interface_stats.__getitem__

        connection = self.inspector.connection
        with contextlib.nested(mock.patch.object(connection, 'lookupByName',
                                                 return_value=self.domain),
                               mock.patch.object(self.domain, 'XMLDesc',
                                                 return_value=dom_xml),
                               mock.patch.object(self.domain,
                                                 'interfaceStats',
                                                 side_effect=interfaceStats),
                               mock.patch.object(self.domain, 'info',
                                                 return_value=(0L, 0L, 0L,
                                                 2L, 999999L))):
            interfaces = list(self.inspector.inspect_vnics(self.instance_name))

            self.assertEqual(3, len(interfaces))
            vnic0, info0 = interfaces[0]
            self.assertEqual('vnet0', vnic0.name)
            self.assertEqual('fa:16:3e:71:ec:6d', vnic0.mac)
            self.assertEqual('nova-instance-00000001-fa163e71ec6d', vnic0.fref)
            self.assertEqual('255.255.255.0', vnic0.parameters.get('projmask'))
            self.assertEqual('10.0.0.2', vnic0.parameters.get('ip'))
            self.assertEqual('10.0.0.0', vnic0.parameters.get('projnet'))
            self.assertEqual('10.0.0.1', vnic0.parameters.get('dhcpserver'))
            self.assertEqual(1L, info0.rx_bytes)
            self.assertEqual(2L, info0.rx_packets)
            self.assertEqual(3L, info0.tx_bytes)
            self.assertEqual(4L, info0.tx_packets)

            vnic1, info1 = interfaces[1]
            self.assertEqual('vnet1', vnic1.name)
            self.assertEqual('fa:16:3e:71:ec:6e', vnic1.mac)
            self.assertEqual('nova-instance-00000001-fa163e71ec6e', vnic1.fref)
            self.assertEqual('255.255.255.0', vnic1.parameters.get('projmask'))
            self.assertEqual('192.168.0.2', vnic1.parameters.get('ip'))
            self.assertEqual('192.168.0.0', vnic1.parameters.get('projnet'))
            self.assertEqual('192.168.0.1', vnic1.parameters.get('dhcpserver'))
            self.assertEqual(5L, info1.rx_bytes)
            self.assertEqual(6L, info1.rx_packets)
            self.assertEqual(7L, info1.tx_bytes)
            self.assertEqual(8L, info1.tx_packets)

            vnic2, info2 = interfaces[2]
            self.assertEqual('vnet2', vnic2.name)
            self.assertEqual('fa:16:3e:96:33:f0', vnic2.mac)
            self.assertIsNone(vnic2.fref)
            self.assertEqual(dict(), vnic2.parameters)
            self.assertEqual(9L, info2.rx_bytes)
            self.assertEqual(10L, info2.rx_packets)
            self.assertEqual(11L, info2.tx_bytes)
            self.assertEqual(12L, info2.tx_packets)

    def test_inspect_vnics_with_domain_shutoff(self):
        connection = self.inspector.connection
        with contextlib.nested(mock.patch.object(connection, 'lookupByName',
                                                 return_value=self.domain),
                               mock.patch.object(self.domain, 'info',
                                                 return_value=(5L, 0L, 0L,
                                                 2L, 999999L))):
            interfaces = list(self.inspector.inspect_vnics(self.instance_name))
            self.assertEqual(interfaces, [])

    def test_inspect_disks(self):
        dom_xml = """
             <domain type='kvm'>
                 <devices>
                     <disk type='file' device='disk'>
                         <driver name='qemu' type='qcow2' cache='none'/>
                         <source file='/path/instance-00000001/disk'/>
                         <target dev='vda' bus='virtio'/>
                         <alias name='virtio-disk0'/>
                         <address type='pci' domain='0x0000' bus='0x00'
                                  slot='0x04' function='0x0'/>
                     </disk>
                 </devices>
             </domain>
        """

        with contextlib.nested(mock.patch.object(self.inspector.connection,
                                                 'lookupByName',
                                                 return_value=self.domain),
                               mock.patch.object(self.domain, 'XMLDesc',
                                                 return_value=dom_xml),
                               mock.patch.object(self.domain, 'blockStats',
                                                 return_value=(1L, 2L, 3L,
                                                              4L, -1)),
                               mock.patch.object(self.domain, 'info',
                                                 return_value=(0L, 0L, 0L,
                                                 2L, 999999L))):
                disks = list(self.inspector.inspect_disks(self.instance_name))

                self.assertEqual(1, len(disks))
                disk0, info0 = disks[0]
                self.assertEqual('vda', disk0.device)
                self.assertEqual(1L, info0.read_requests)
                self.assertEqual(2L, info0.read_bytes)
                self.assertEqual(3L, info0.write_requests)
                self.assertEqual(4L, info0.write_bytes)

    def test_inspect_disks_with_domain_shutoff(self):
        connection = self.inspector.connection
        with contextlib.nested(mock.patch.object(connection, 'lookupByName',
                                                 return_value=self.domain),
                               mock.patch.object(self.domain, 'info',
                                                 return_value=(5L, 0L, 0L,
                                                 2L, 999999L))):
            disks = list(self.inspector.inspect_disks(self.instance_name))
            self.assertEqual(disks, [])


class TestLibvirtInspectionWithError(test.BaseTestCase):

    class fakeLibvirtError(Exception):
        pass

    def setUp(self):
        super(TestLibvirtInspectionWithError, self).setUp()
        self.inspector = libvirt_inspector.LibvirtInspector()
        self.useFixture(fixtures.MonkeyPatch(
            'ceilometer.compute.virt.libvirt.inspector.'
            'LibvirtInspector._get_connection',
            self._dummy_get_connection))
        libvirt_inspector.libvirt = mock.Mock()
        libvirt_inspector.libvirt.libvirtError = self.fakeLibvirtError

    def _dummy_get_connection(*args, **kwargs):
        raise Exception('dummy')

    def test_inspect_unknown_error(self):
        self.assertRaises(virt_inspector.InspectorException,
                          self.inspector.inspect_cpus, 'foo')

########NEW FILE########
__FILENAME__ = test_inspector
# Copyright (c) 2014 VMware, Inc.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
"""
Tests for VMware Vsphere inspector.
"""

import mock

from oslo.vmware import api

from ceilometer.compute.virt import inspector as virt_inspector
from ceilometer.compute.virt.vmware import inspector as vsphere_inspector
from ceilometer.openstack.common import test


class TestVsphereInspection(test.BaseTestCase):

    def setUp(self):
        api_session = api.VMwareAPISession("test_server", "test_user",
                                           "test_password", 0, None,
                                           create_session=False)
        api_session._vim = mock.MagicMock()
        vsphere_inspector.get_api_session = mock.Mock(
            return_value=api_session)
        self._inspector = vsphere_inspector.VsphereInspector()
        self._inspector._ops = mock.MagicMock()

        super(TestVsphereInspection, self).setUp()

    def test_inspect_memory_usage(self):
        fake_instance_moid = 'fake_instance_moid'
        fake_instance_id = 'fake_instance_id'
        fake_perf_counter_id = 'fake_perf_counter_id'
        fake_memory_value = 1024.0
        fake_stat = virt_inspector.MemoryUsageStats(usage=1.0)

        def construct_mock_instance_object(fake_instance_id):
            instance_object = mock.MagicMock()
            instance_object.id = fake_instance_id
            return instance_object

        fake_instance = construct_mock_instance_object(fake_instance_id)
        self._inspector._ops.get_vm_moid.return_value = fake_instance_moid
        self._inspector._ops.get_perf_counter_id.return_value = \
            fake_perf_counter_id
        self._inspector._ops.query_vm_aggregate_stats.return_value = \
            fake_memory_value
        memory_stat = self._inspector.inspect_memory_usage(fake_instance)
        self.assertEqual(fake_stat, memory_stat)

    def test_inspect_cpu_util(self):
        fake_instance_moid = 'fake_instance_moid'
        fake_instance_id = 'fake_instance_id'
        fake_perf_counter_id = 'fake_perf_counter_id'
        fake_cpu_util_value = 60
        fake_stat = virt_inspector.CPUUtilStats(util=60)

        def construct_mock_instance_object(fake_instance_id):
            instance_object = mock.MagicMock()
            instance_object.id = fake_instance_id
            return instance_object

        fake_instance = construct_mock_instance_object(fake_instance_id)
        self._inspector._ops.get_vm_moid.return_value = fake_instance_moid
        self._inspector._ops.get_perf_counter_id.return_value = \
            fake_perf_counter_id
        self._inspector._ops.query_vm_aggregate_stats.return_value = \
            fake_cpu_util_value * 100
        cpu_util_stat = self._inspector.inspect_cpu_util(fake_instance)
        self.assertEqual(fake_stat, cpu_util_stat)

    def test_inspect_vnic_rates(self):

        # construct test data
        test_vm_moid = "vm-21"
        vnic1 = "vnic-1"
        vnic2 = "vnic-2"
        counter_name_to_id_map = {
            vsphere_inspector.VC_NETWORK_RX_COUNTER: 1,
            vsphere_inspector.VC_NETWORK_TX_COUNTER: 2
        }
        counter_id_to_stats_map = {
            1: {vnic1: 1, vnic2: 3},
            2: {vnic1: 2, vnic2: 4},
        }

        def get_counter_id_side_effect(counter_full_name):
            return counter_name_to_id_map[counter_full_name]

        def query_stat_side_effect(vm_moid, counter_id, duration):
            # assert inputs
            self.assertEqual(test_vm_moid, vm_moid)
            self.assertTrue(counter_id in counter_id_to_stats_map)
            return counter_id_to_stats_map[counter_id]

        # configure vsphere operations mock with the test data
        ops_mock = self._inspector._ops
        ops_mock.get_vm_moid.return_value = test_vm_moid
        ops_mock.get_perf_counter_id.side_effect = get_counter_id_side_effect
        ops_mock.query_vm_device_stats.side_effect = \
            query_stat_side_effect
        result = self._inspector.inspect_vnic_rates(mock.MagicMock())

        # validate result
        expected_stats = {
            vnic1: virt_inspector.InterfaceRateStats(1024, 2048),
            vnic2: virt_inspector.InterfaceRateStats(3072, 4096)
        }

        for vnic, rates_info in result:
            self.assertEqual(expected_stats[vnic.name], rates_info)

    def test_inspect_disk_rates(self):

        # construct test data
        test_vm_moid = "vm-21"
        disk1 = "disk-1"
        disk2 = "disk-2"
        counter_name_to_id_map = {
            vsphere_inspector.VC_DISK_READ_RATE_CNTR: 1,
            vsphere_inspector.VC_DISK_READ_REQUESTS_RATE_CNTR: 2,
            vsphere_inspector.VC_DISK_WRITE_RATE_CNTR: 3,
            vsphere_inspector.VC_DISK_WRITE_REQUESTS_RATE_CNTR: 4
        }
        counter_id_to_stats_map = {
            1: {disk1: 1, disk2: 2},
            2: {disk1: 300, disk2: 400},
            3: {disk1: 5, disk2: 6},
            4: {disk1: 700},
        }

        def get_counter_id_side_effect(counter_full_name):
            return counter_name_to_id_map[counter_full_name]

        def query_stat_side_effect(vm_moid, counter_id, duration):
            # assert inputs
            self.assertEqual(test_vm_moid, vm_moid)
            self.assertTrue(counter_id in counter_id_to_stats_map)
            return counter_id_to_stats_map[counter_id]

        # configure vsphere operations mock with the test data
        ops_mock = self._inspector._ops
        ops_mock.get_vm_moid.return_value = test_vm_moid
        ops_mock.get_perf_counter_id.side_effect = get_counter_id_side_effect
        ops_mock.query_vm_device_stats.side_effect = query_stat_side_effect

        result = self._inspector.inspect_disk_rates(mock.MagicMock())

        # validate result
        expected_stats = {
            disk1: virt_inspector.DiskRateStats(1024, 300, 5120, 700),
            disk2: virt_inspector.DiskRateStats(2048, 400, 6144, 0)
        }

        actual_stats = dict((disk.device, rates) for (disk, rates) in result)
        self.assertEqual(expected_stats, actual_stats)

########NEW FILE########
__FILENAME__ = test_vsphere_operations
# Copyright (c) 2014 VMware, Inc.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import mock
from oslo.vmware import api

from ceilometer.compute.virt.vmware import vsphere_operations
from ceilometer.openstack.common import test


class VsphereOperationsTest(test.BaseTestCase):

    def setUp(self):
        api_session = api.VMwareAPISession("test_server", "test_user",
                                           "test_password", 0, None,
                                           create_session=False)
        api_session._vim = mock.MagicMock()
        self._vsphere_ops = vsphere_operations.VsphereOperations(api_session,
                                                                 1000)
        super(VsphereOperationsTest, self).setUp()

    def test_get_vm_moid(self):

        vm1_moid = "vm-1"
        vm2_moid = "vm-2"
        vm1_instance = "0a651a71-142c-4813-aaa6-42e5d5c80d85"
        vm2_instance = "db1d2533-6bef-4cb2-aef3-920e109f5693"

        def construct_mock_vm_object(vm_moid, vm_instance):
            vm_object = mock.MagicMock()
            vm_object.obj.value = vm_moid
            vm_object.propSet[0].val = vm_instance
            return vm_object

        def retrieve_props_side_effect(pc, specSet, options):
            # assert inputs
            self.assertEqual(self._vsphere_ops._max_objects,
                             options.maxObjects)
            self.assertEqual(vsphere_operations.VM_INSTANCE_ID_PROPERTY,
                             specSet[0].pathSet[0])

            # mock return result
            vm1 = construct_mock_vm_object(vm1_moid, vm1_instance)
            vm2 = construct_mock_vm_object(vm2_moid, vm2_instance)
            result = mock.MagicMock()
            result.objects.__iter__.return_value = [vm1, vm2]
            return result

        vim_mock = self._vsphere_ops._api_session._vim
        vim_mock.RetrievePropertiesEx.side_effect = retrieve_props_side_effect
        vim_mock.ContinueRetrievePropertiesEx.return_value = None

        vm_moid = self._vsphere_ops.get_vm_moid(vm1_instance)
        self.assertEqual(vm1_moid, vm_moid)

        vm_moid = self._vsphere_ops.get_vm_moid(vm2_instance)
        self.assertEqual(vm2_moid, vm_moid)

    def test_query_vm_property(self):

        vm_moid = "vm-21"
        vm_property_name = "runtime.powerState"
        vm_property_val = "poweredON"

        def retrieve_props_side_effect(pc, specSet, options):
            # assert inputs
            self.assertEqual(vm_moid, specSet[0].obj.value)
            self.assertEqual(vm_property_name, specSet[0].pathSet[0])

            # mock return result
            result = mock.MagicMock()
            result.objects[0].propSet[0].val = vm_property_val
            return result

        vim_mock = self._vsphere_ops._api_session._vim
        vim_mock.RetrievePropertiesEx.side_effect = retrieve_props_side_effect

        actual_val = self._vsphere_ops.query_vm_property(vm_moid,
                                                         vm_property_name)
        self.assertEqual(vm_property_val, actual_val)

    def test_get_perf_counter_id(self):

        def construct_mock_counter_info(group_name, counter_name, rollup_type,
                                        counter_id):
            counter_info = mock.MagicMock()
            counter_info.groupInfo.key = group_name
            counter_info.nameInfo.key = counter_name
            counter_info.rollupType = rollup_type
            counter_info.key = counter_id
            return counter_info

        def retrieve_props_side_effect(pc, specSet, options):
            # assert inputs
            self.assertEqual(vsphere_operations.PERF_COUNTER_PROPERTY,
                             specSet[0].pathSet[0])

            # mock return result
            counter_info1 = construct_mock_counter_info("a", "b", "c", 1)
            counter_info2 = construct_mock_counter_info("x", "y", "z", 2)
            result = mock.MagicMock()
            result.objects[0].propSet[0].val.PerfCounterInfo.__iter__. \
                return_value = [counter_info1, counter_info2]
            return result

        vim_mock = self._vsphere_ops._api_session._vim
        vim_mock.RetrievePropertiesEx.side_effect = retrieve_props_side_effect

        counter_id = self._vsphere_ops.get_perf_counter_id("a:b:c")
        self.assertEqual(1, counter_id)

        counter_id = self._vsphere_ops.get_perf_counter_id("x:y:z")
        self.assertEqual(2, counter_id)

    def test_query_vm_stats(self):

        vm_moid = "vm-21"
        device1 = "device-1"
        device2 = "device-2"
        device3 = "device-3"
        counter_id = 5

        def construct_mock_metric_series(device_name, stat_values):
            metric_series = mock.MagicMock()
            metric_series.value = stat_values
            metric_series.id.instance = device_name
            return metric_series

        def vim_query_perf_side_effect(perf_manager, querySpec):
            # assert inputs
            self.assertEqual(vm_moid, querySpec[0].entity.value)
            self.assertEqual(counter_id, querySpec[0].metricId[0].counterId)
            self.assertEqual(vsphere_operations.VC_REAL_TIME_SAMPLING_INTERVAL,
                             querySpec[0].intervalId)

            # mock return result
            perf_stats = mock.MagicMock()
            perf_stats[0].sampleInfo = ["s1", "s2", "s3"]
            perf_stats[0].value.__iter__.return_value = [
                construct_mock_metric_series(None, [111, 222, 333]),
                construct_mock_metric_series(device1, [100, 200, 300]),
                construct_mock_metric_series(device2, [10, 20, 30]),
                construct_mock_metric_series(device3, [1, 2, 3])
            ]
            return perf_stats

        vim_mock = self._vsphere_ops._api_session._vim
        vim_mock.QueryPerf.side_effect = vim_query_perf_side_effect
        ops = self._vsphere_ops

        # test aggregate stat
        stat_val = ops.query_vm_aggregate_stats(vm_moid, counter_id, 60)
        self.assertEqual(222, stat_val)

        # test per-device(non-aggregate) stats
        expected_device_stats = {
            device1: 200,
            device2: 20,
            device3: 2
        }
        stats = ops.query_vm_device_stats(vm_moid, counter_id, 60)
        self.assertEqual(expected_device_stats, stats)

########NEW FILE########
__FILENAME__ = db
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
# Copyright © 2013 eNovance
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#         Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

"""Base classes for API tests."""
import fixtures
import os
import uuid
import warnings

import six
import testscenarios.testcase

from ceilometer.openstack.common.fixture import config
import ceilometer.openstack.common.fixture.mockpatch as oslo_mock
from ceilometer import storage
from ceilometer.tests import base as test_base


class TestBase(testscenarios.testcase.WithScenarios, test_base.BaseTestCase):
    def setUp(self):
        super(TestBase, self).setUp()

        self.useFixture(self.db_manager)

        self.CONF = self.useFixture(config.Config()).conf

        with warnings.catch_warnings():
            warnings.filterwarnings(
                action='ignore',
                message='.*you must provide a username and password.*')
            try:
                self.conn = storage.get_connection(self.db_manager.connection)
            except storage.StorageBadVersion as e:
                self.skipTest(six.text_type(e))
        self.conn.upgrade()

        self.useFixture(oslo_mock.Patch('ceilometer.storage.get_connection',
                                        return_value=self.conn))

        self.CONF([], project='ceilometer')

        # Set a default location for the pipeline config file so the
        # tests work even if ceilometer is not installed globally on
        # the system.
        self.CONF.set_override(
            'pipeline_cfg_file',
            self.path_get('etc/ceilometer/pipeline.yaml')
        )

    def tearDown(self):
        self.conn.clear()
        self.conn = None
        super(TestBase, self).tearDown()


class MongoDbManager(fixtures.Fixture):

    def __init__(self):
        self.url = os.environ.get('CEILOMETER_TEST_MONGODB_URL')
        if not self.url:
            raise RuntimeError(
                "No MongoDB test URL set,"
                "export CEILOMETER_TEST_MONGODB_URL environment variable")

    def setUp(self):
        super(MongoDbManager, self).setUp()
        self.connection = '%(url)s_%(db)s' % {
            'url': self.url,
            'db': uuid.uuid4().hex
        }


class DB2Manager(MongoDbManager):
    def __init__(self):
        self.url = (os.environ.get('CEILOMETER_TEST_DB2_URL') or
                    os.environ.get('CEILOMETER_TEST_MONGODB_URL'))
        if not self.url:
            raise RuntimeError(
                "No DB2 test URL set, "
                "export CEILOMETER_TEST_DB2_URL environment variable")
        else:
            # This is to make sure that the db2 driver is used when
            # CEILOMETER_TEST_DB2_URL was not set
            self.url = self.url.replace('mongodb:', 'db2:', 1)


class HBaseManager(fixtures.Fixture):
    def __init__(self):
        self.url = os.environ.get('CEILOMETER_TEST_HBASE_URL')
        if not self.url:
            self.url = 'hbase://__test__'

    def setUp(self):
        super(HBaseManager, self).setUp()
        self.connection = '%s?table_prefix=%s' % (
            self.url,
            uuid.uuid4().hex)


class SQLiteManager(fixtures.Fixture):

    def setUp(self):
        super(SQLiteManager, self).setUp()
        self.connection = 'sqlite://'


@six.add_metaclass(test_base.SkipNotImplementedMeta)
class MixinTestsWithBackendScenarios(object):

    scenarios = [
        ('sqlite', {'db_manager': SQLiteManager()}),
        ('mongodb', {'db_manager': MongoDbManager()}),
        ('hbase', {'db_manager': HBaseManager()}),
        ('db2', {'db_manager': DB2Manager()})
    ]

########NEW FILE########
__FILENAME__ = test_db
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 IBM Corp
#
# Author: Tong Li <litong01@us.ibm.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import datetime

import mock

from ceilometer.dispatcher import database
from ceilometer.openstack.common.fixture import config
from ceilometer.openstack.common import test
from ceilometer.publisher import utils


class TestDispatcherDB(test.BaseTestCase):

    def setUp(self):
        super(TestDispatcherDB, self).setUp()
        self.CONF = self.useFixture(config.Config()).conf
        self.CONF.set_override('connection', 'sqlite://', group='database')
        self.dispatcher = database.DatabaseDispatcher(self.CONF)
        self.ctx = None

    def test_valid_message(self):
        msg = {'counter_name': 'test',
               'resource_id': self.id(),
               'counter_volume': 1,
               }
        msg['message_signature'] = utils.compute_signature(
            msg,
            self.CONF.publisher.metering_secret,
        )

        with mock.patch.object(self.dispatcher.storage_conn,
                               'record_metering_data') as record_metering_data:
            self.dispatcher.record_metering_data(msg)

        record_metering_data.assert_called_once_with(msg)

    def test_invalid_message(self):
        msg = {'counter_name': 'test',
               'resource_id': self.id(),
               'counter_volume': 1,
               }
        msg['message_signature'] = 'invalid-signature'

        class ErrorConnection:

            called = False

            def record_metering_data(self, data):
                self.called = True

        self.dispatcher.storage_conn = ErrorConnection()

        self.dispatcher.record_metering_data(msg)

        if self.dispatcher.storage_conn.called:
            self.fail('Should not have called the storage connection')

    def test_timestamp_conversion(self):
        msg = {'counter_name': 'test',
               'resource_id': self.id(),
               'counter_volume': 1,
               'timestamp': '2012-07-02T13:53:40Z',
               }
        msg['message_signature'] = utils.compute_signature(
            msg,
            self.CONF.publisher.metering_secret,
        )

        expected = msg.copy()
        expected['timestamp'] = datetime.datetime(2012, 7, 2, 13, 53, 40)

        with mock.patch.object(self.dispatcher.storage_conn,
                               'record_metering_data') as record_metering_data:
            self.dispatcher.record_metering_data(msg)

        record_metering_data.assert_called_once_with(expected)

    def test_timestamp_tzinfo_conversion(self):
        msg = {'counter_name': 'test',
               'resource_id': self.id(),
               'counter_volume': 1,
               'timestamp': '2012-09-30T15:31:50.262-08:00',
               }
        msg['message_signature'] = utils.compute_signature(
            msg,
            self.CONF.publisher.metering_secret,
        )

        expected = msg.copy()
        expected['timestamp'] = datetime.datetime(2012, 9, 30, 23,
                                                  31, 50, 262000)

        with mock.patch.object(self.dispatcher.storage_conn,
                               'record_metering_data') as record_metering_data:
            self.dispatcher.record_metering_data(msg)

        record_metering_data.assert_called_once_with(expected)

########NEW FILE########
__FILENAME__ = test_file
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 IBM Corp
#
# Author: Tong Li <litong01@us.ibm.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import logging.handlers
import os
import tempfile

from ceilometer.dispatcher import file
from ceilometer.openstack.common.fixture import config
from ceilometer.openstack.common import test
from ceilometer.publisher import utils


class TestDispatcherFile(test.BaseTestCase):

    def setUp(self):
        super(TestDispatcherFile, self).setUp()
        self.CONF = self.useFixture(config.Config()).conf

    def test_file_dispatcher_with_all_config(self):
        # Create a temporaryFile to get a file name
        tf = tempfile.NamedTemporaryFile('r')
        filename = tf.name
        tf.close()

        self.CONF.dispatcher_file.file_path = filename
        self.CONF.dispatcher_file.max_bytes = 50
        self.CONF.dispatcher_file.backup_count = 5
        dispatcher = file.FileDispatcher(self.CONF)

        # The number of the handlers should be 1
        self.assertEqual(1, len(dispatcher.log.handlers))
        # The handler should be RotatingFileHandler
        handler = dispatcher.log.handlers[0]
        self.assertIsInstance(handler,
                              logging.handlers.RotatingFileHandler)

        msg = {'counter_name': 'test',
               'resource_id': self.id(),
               'counter_volume': 1,
               }
        msg['message_signature'] = utils.compute_signature(
            msg,
            self.CONF.publisher.metering_secret,
        )

        # The record_metering_data method should exist and not produce errors.
        dispatcher.record_metering_data(msg)
        # After the method call above, the file should have been created.
        self.assertTrue(os.path.exists(handler.baseFilename))

    def test_file_dispatcher_with_path_only(self):
        # Create a temporaryFile to get a file name
        tf = tempfile.NamedTemporaryFile('r')
        filename = tf.name
        tf.close()

        self.CONF.dispatcher_file.file_path = filename
        self.CONF.dispatcher_file.max_bytes = None
        self.CONF.dispatcher_file.backup_count = None
        dispatcher = file.FileDispatcher(self.CONF)

        # The number of the handlers should be 1
        self.assertEqual(1, len(dispatcher.log.handlers))
        # The handler should be RotatingFileHandler
        handler = dispatcher.log.handlers[0]
        self.assertIsInstance(handler,
                              logging.FileHandler)

        msg = {'counter_name': 'test',
               'resource_id': self.id(),
               'counter_volume': 1,
               }
        msg['message_signature'] = utils.compute_signature(
            msg,
            self.CONF.publisher.metering_secret,
        )

        # The record_metering_data method should exist and not produce errors.
        dispatcher.record_metering_data(msg)
        # After the method call above, the file should have been created.
        self.assertTrue(os.path.exists(handler.baseFilename))

    def test_file_dispatcher_with_no_path(self):
        self.CONF.dispatcher_file.file_path = None
        dispatcher = file.FileDispatcher(self.CONF)

        # The log should be None
        self.assertIsNone(dispatcher.log)

########NEW FILE########
__FILENAME__ = test_kwapi
# -*- coding: utf-8 -*-
#
# Author: François Rossigneux <francois.rossigneux@inria.fr>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import datetime

from keystoneclient import exceptions
import mock

from ceilometer.central import manager
from ceilometer.energy import kwapi
from ceilometer.openstack.common import context
from ceilometer.openstack.common.fixture.mockpatch import PatchObject
from ceilometer.openstack.common import test


PROBE_DICT = {
    "probes": {
        "A": {
            "timestamp": 1357730232.68754,
            "w": 107.3,
            "kwh": 0.001058255421506034
        },
        "B": {
            "timestamp": 1357730232.048158,
            "w": 15.0,
            "kwh": 0.029019045026169896
        },
        "C": {
            "timestamp": 1357730232.223375,
            "w": 95.0,
            "kwh": 0.17361822634312918
        }
    }
}


class TestManager(manager.AgentManager):

    def __init__(self):
        super(TestManager, self).__init__()
        self.keystone = None


class TestKwapi(test.BaseTestCase):

    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    def setUp(self):
        super(TestKwapi, self).setUp()
        self.context = context.get_admin_context()
        self.manager = TestManager()

    @staticmethod
    def fake_get_kwapi_client(ksclient):
        raise exceptions.EndpointNotFound("fake keystone exception")

    def test_endpoint_not_exist(self):
        with PatchObject(kwapi._Base, 'get_kwapi_client',
                         side_effect=self.fake_get_kwapi_client):
            pollster = kwapi.EnergyPollster()
            samples = list(pollster.get_samples(self.manager, {}))

        self.assertEqual(0, len(samples))


class TestEnergyPollster(test.BaseTestCase):

    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    def setUp(self):
        super(TestEnergyPollster, self).setUp()
        self.context = context.get_admin_context()
        self.manager = TestManager()
        self.useFixture(PatchObject(kwapi._Base, '_iter_probes',
                                    side_effect=self.fake_iter_probes))

    @staticmethod
    def fake_iter_probes(ksclient, cache):
        probes = PROBE_DICT['probes']
        for key, value in probes.iteritems():
            probe_dict = value
            probe_dict['id'] = key
            yield probe_dict

    def test_sample(self):
        cache = {}
        samples = list(kwapi.EnergyPollster().get_samples(
            self.manager,
            cache,
        ))
        self.assertEqual(3, len(samples))
        samples_by_name = dict((s.resource_id, s) for s in samples)
        for name, probe in PROBE_DICT['probes'].items():
            sample = samples_by_name[name]
            expected = datetime.datetime.fromtimestamp(
                probe['timestamp']
            ).isoformat()
            self.assertEqual(expected, sample.timestamp)
            self.assertEqual(probe['kwh'], sample.volume)
            # self.assert_(
            #     any(map(lambda sample: sample.volume == probe['w'],
            #             power_samples)))


class TestEnergyPollsterCache(test.BaseTestCase):

    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    def setUp(self):
        super(TestEnergyPollsterCache, self).setUp()
        self.context = context.get_admin_context()
        self.manager = TestManager()

    def test_get_samples_cached(self):
        probe = {'id': 'A'}
        probe.update(PROBE_DICT['probes']['A'])
        cache = {
            kwapi.EnergyPollster.CACHE_KEY_PROBE: [probe],
        }
        self.manager.keystone = mock.Mock()
        pollster = kwapi.EnergyPollster()
        with mock.patch.object(pollster, '_get_probes') as do_not_call:
            do_not_call.side_effect = AssertionError('should not be called')
            samples = list(pollster.get_samples(self.manager, cache))
        self.assertEqual(1, len(samples))


class TestPowerPollster(test.BaseTestCase):

    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    def setUp(self):
        super(TestPowerPollster, self).setUp()
        self.context = context.get_admin_context()
        self.manager = TestManager()
        self.useFixture(PatchObject(kwapi._Base, '_iter_probes',
                                    side_effect=self.fake_iter_probes))

    @staticmethod
    def fake_iter_probes(ksclient, cache):
        probes = PROBE_DICT['probes']
        for key, value in probes.iteritems():
            probe_dict = value
            probe_dict['id'] = key
            yield probe_dict

    def test_sample(self):
        cache = {}
        samples = list(kwapi.PowerPollster().get_samples(
            self.manager,
            cache,
        ))
        self.assertEqual(3, len(samples))
        samples_by_name = dict((s.resource_id, s) for s in samples)
        for name, probe in PROBE_DICT['probes'].items():
            sample = samples_by_name[name]
            expected = datetime.datetime.fromtimestamp(
                probe['timestamp']
            ).isoformat()
            self.assertEqual(expected, sample.timestamp)
            self.assertEqual(probe['w'], sample.volume)


class TestPowerPollsterCache(test.BaseTestCase):

    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    def setUp(self):
        super(TestPowerPollsterCache, self).setUp()
        self.context = context.get_admin_context()
        self.manager = TestManager()

    def test_get_samples_cached(self):
        probe = {'id': 'A'}
        probe.update(PROBE_DICT['probes']['A'])
        cache = {
            kwapi.PowerPollster.CACHE_KEY_PROBE: [probe],
        }
        self.manager.keystone = mock.Mock()
        pollster = kwapi.PowerPollster()
        with mock.patch.object(pollster, '_get_probes') as do_not_call:
            do_not_call.side_effect = AssertionError('should not be called')
            samples = list(pollster.get_samples(self.manager, cache))
        self.assertEqual(1, len(samples))

########NEW FILE########
__FILENAME__ = test_converter
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Rackspace Hosting.
#
# Author: Monsyne Dragon <mdragon@rackspace.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import datetime

import jsonpath_rw
import mock
from oslo.config import cfg as oslo_cfg
import six

from ceilometer.event import converter
from ceilometer.storage import models
from ceilometer.tests import base


class ConverterBase(base.BaseTestCase):
    def _create_test_notification(self, event_type, message_id, **kw):
        return dict(event_type=event_type,
                    message_id=message_id,
                    priority="INFO",
                    publisher_id="compute.host-1-2-3",
                    timestamp="2013-08-08 21:06:37.803826",
                    payload=kw,
                    )

    def assertIsValidEvent(self, event, notification):
        self.assertIsNot(
            None, event,
            "Notification dropped unexpectedly:"
            " %s" % str(notification))
        self.assertIsInstance(event, models.Event)

    def assertIsNotValidEvent(self, event, notification):
        self.assertIs(
            None, event,
            "Notification NOT dropped when expected to be dropped:"
            " %s" % str(notification))

    def assertHasTrait(self, event, name, value=None, dtype=None):
        traits = [trait for trait in event.traits if trait.name == name]
        self.assertTrue(
            len(traits) > 0,
            "Trait %s not found in event %s" % (name, event))
        trait = traits[0]
        if value is not None:
            self.assertEqual(value, trait.value)
        if dtype is not None:
            self.assertEqual(dtype, trait.dtype)
            if dtype == models.Trait.INT_TYPE:
                self.assertIsInstance(trait.value, int)
            elif dtype == models.Trait.FLOAT_TYPE:
                self.assertIsInstance(trait.value, float)
            elif dtype == models.Trait.DATETIME_TYPE:
                self.assertIsInstance(trait.value, datetime.datetime)
            elif dtype == models.Trait.TEXT_TYPE:
                self.assertIsInstance(trait.value, six.string_types)

    def assertDoesNotHaveTrait(self, event, name):
        traits = [trait for trait in event.traits if trait.name == name]
        self.assertEqual(
            len(traits), 0,
            "Extra Trait %s found in event %s" % (name, event))

    def assertHasDefaultTraits(self, event):
        text = models.Trait.TEXT_TYPE
        self.assertHasTrait(event, 'service', dtype=text)

    def _cmp_tree(self, this, other):
        if hasattr(this, 'right') and hasattr(other, 'right'):
            return (self._cmp_tree(this.right, other.right) and
                    self._cmp_tree(this.left, other.left))
        if not hasattr(this, 'right') and not hasattr(other, 'right'):
            return this == other
        return False

    def assertPathsEqual(self, path1, path2):
        self.assertTrue(self._cmp_tree(path1, path2),
                        'JSONPaths not equivalent %s %s' % (path1, path2))


class TestTraitDefinition(ConverterBase):

    def setUp(self):
        super(TestTraitDefinition, self).setUp()
        self.n1 = self._create_test_notification(
            "test.thing",
            "uuid-for-notif-0001",
            instance_uuid="uuid-for-instance-0001",
            instance_id="id-for-instance-0001",
            instance_uuid2=None,
            instance_id2=None,
            host='host-1-2-3',
            bogus_date='',
            image_meta=dict(
                        disk_gb='20',
                        thing='whatzit'),
            foobar=50)

        self.ext1 = mock.MagicMock(name='mock_test_plugin')
        self.test_plugin_class = self.ext1.plugin
        self.test_plugin = self.test_plugin_class()
        self.test_plugin.trait_value.return_value = 'foobar'
        self.ext1.reset_mock()

        self.ext2 = mock.MagicMock(name='mock_nothing_plugin')
        self.nothing_plugin_class = self.ext2.plugin
        self.nothing_plugin = self.nothing_plugin_class()
        self.nothing_plugin.trait_value.return_value = None
        self.ext2.reset_mock()

        self.fake_plugin_mgr = dict(test=self.ext1, nothing=self.ext2)

    def test_to_trait_with_plugin(self):
        cfg = dict(type='text',
                   fields=['payload.instance_id', 'payload.instance_uuid'],
                   plugin=dict(name='test'))

        tdef = converter.TraitDefinition('test_trait', cfg,
                                         self.fake_plugin_mgr)
        t = tdef.to_trait(self.n1)
        self.assertIsInstance(t, models.Trait)
        self.assertEqual('test_trait', t.name)
        self.assertEqual(models.Trait.TEXT_TYPE, t.dtype)
        self.assertEqual('foobar', t.value)
        self.test_plugin_class.assert_called_once_with()
        self.test_plugin.trait_value.assert_called_once_with([
            ('payload.instance_id', 'id-for-instance-0001'),
            ('payload.instance_uuid', 'uuid-for-instance-0001')])

    def test_to_trait_null_match_with_plugin(self):
        cfg = dict(type='text',
                   fields=['payload.nothere', 'payload.bogus'],
                   plugin=dict(name='test'))

        tdef = converter.TraitDefinition('test_trait', cfg,
                                         self.fake_plugin_mgr)
        t = tdef.to_trait(self.n1)
        self.assertIsInstance(t, models.Trait)
        self.assertEqual('test_trait', t.name)
        self.assertEqual(models.Trait.TEXT_TYPE, t.dtype)
        self.assertEqual('foobar', t.value)
        self.test_plugin_class.assert_called_once_with()
        self.test_plugin.trait_value.assert_called_once_with([])

    def test_to_trait_with_plugin_null(self):
        cfg = dict(type='text',
                   fields=['payload.instance_id', 'payload.instance_uuid'],
                   plugin=dict(name='nothing'))

        tdef = converter.TraitDefinition('test_trait', cfg,
                                         self.fake_plugin_mgr)
        t = tdef.to_trait(self.n1)
        self.assertIs(None, t)
        self.nothing_plugin_class.assert_called_once_with()
        self.nothing_plugin.trait_value.assert_called_once_with([
            ('payload.instance_id', 'id-for-instance-0001'),
            ('payload.instance_uuid', 'uuid-for-instance-0001')])

    def test_to_trait_with_plugin_with_parameters(self):
        cfg = dict(type='text',
                   fields=['payload.instance_id', 'payload.instance_uuid'],
                   plugin=dict(name='test', parameters=dict(a=1, b='foo')))

        tdef = converter.TraitDefinition('test_trait', cfg,
                                         self.fake_plugin_mgr)
        t = tdef.to_trait(self.n1)
        self.assertIsInstance(t, models.Trait)
        self.assertEqual('test_trait', t.name)
        self.assertEqual(models.Trait.TEXT_TYPE, t.dtype)
        self.assertEqual('foobar', t.value)
        self.test_plugin_class.assert_called_once_with(a=1, b='foo')
        self.test_plugin.trait_value.assert_called_once_with([
            ('payload.instance_id', 'id-for-instance-0001'),
            ('payload.instance_uuid', 'uuid-for-instance-0001')])

    def test_to_trait(self):
        cfg = dict(type='text', fields='payload.instance_id')
        tdef = converter.TraitDefinition('test_trait', cfg,
                                         self.fake_plugin_mgr)
        t = tdef.to_trait(self.n1)
        self.assertIsInstance(t, models.Trait)
        self.assertEqual('test_trait', t.name)
        self.assertEqual(models.Trait.TEXT_TYPE, t.dtype)
        self.assertEqual('id-for-instance-0001', t.value)

        cfg = dict(type='int', fields='payload.image_meta.disk_gb')
        tdef = converter.TraitDefinition('test_trait', cfg,
                                         self.fake_plugin_mgr)
        t = tdef.to_trait(self.n1)
        self.assertIsInstance(t, models.Trait)
        self.assertEqual('test_trait', t.name)
        self.assertEqual(models.Trait.INT_TYPE, t.dtype)
        self.assertEqual(20, t.value)

    def test_to_trait_multiple(self):
        cfg = dict(type='text', fields=['payload.instance_id',
                                        'payload.instance_uuid'])
        tdef = converter.TraitDefinition('test_trait', cfg,
                                         self.fake_plugin_mgr)
        t = tdef.to_trait(self.n1)
        self.assertIsInstance(t, models.Trait)
        self.assertEqual('id-for-instance-0001', t.value)

        cfg = dict(type='text', fields=['payload.instance_uuid',
                                        'payload.instance_id'])
        tdef = converter.TraitDefinition('test_trait', cfg,
                                         self.fake_plugin_mgr)
        t = tdef.to_trait(self.n1)
        self.assertIsInstance(t, models.Trait)
        self.assertEqual('uuid-for-instance-0001', t.value)

    def test_to_trait_multiple_different_nesting(self):
        cfg = dict(type='int', fields=['payload.foobar',
                   'payload.image_meta.disk_gb'])
        tdef = converter.TraitDefinition('test_trait', cfg,
                                         self.fake_plugin_mgr)
        t = tdef.to_trait(self.n1)
        self.assertIsInstance(t, models.Trait)
        self.assertEqual(50, t.value)

        cfg = dict(type='int', fields=['payload.image_meta.disk_gb',
                   'payload.foobar'])
        tdef = converter.TraitDefinition('test_trait', cfg,
                                         self.fake_plugin_mgr)
        t = tdef.to_trait(self.n1)
        self.assertIsInstance(t, models.Trait)
        self.assertEqual(20, t.value)

    def test_to_trait_some_null_multiple(self):
        cfg = dict(type='text', fields=['payload.instance_id2',
                                        'payload.instance_uuid'])
        tdef = converter.TraitDefinition('test_trait', cfg,
                                         self.fake_plugin_mgr)
        t = tdef.to_trait(self.n1)
        self.assertIsInstance(t, models.Trait)
        self.assertEqual('uuid-for-instance-0001', t.value)

    def test_to_trait_some_missing_multiple(self):
        cfg = dict(type='text', fields=['payload.not_here_boss',
                                        'payload.instance_uuid'])
        tdef = converter.TraitDefinition('test_trait', cfg,
                                         self.fake_plugin_mgr)
        t = tdef.to_trait(self.n1)
        self.assertIsInstance(t, models.Trait)
        self.assertEqual('uuid-for-instance-0001', t.value)

    def test_to_trait_missing(self):
        cfg = dict(type='text', fields='payload.not_here_boss')
        tdef = converter.TraitDefinition('test_trait', cfg,
                                         self.fake_plugin_mgr)
        t = tdef.to_trait(self.n1)
        self.assertIs(None, t)

    def test_to_trait_null(self):
        cfg = dict(type='text', fields='payload.instance_id2')
        tdef = converter.TraitDefinition('test_trait', cfg,
                                         self.fake_plugin_mgr)
        t = tdef.to_trait(self.n1)
        self.assertIs(None, t)

    def test_to_trait_empty_nontext(self):
        cfg = dict(type='datetime', fields='payload.bogus_date')
        tdef = converter.TraitDefinition('test_trait', cfg,
                                         self.fake_plugin_mgr)
        t = tdef.to_trait(self.n1)
        self.assertIs(None, t)

    def test_to_trait_multiple_null_missing(self):
        cfg = dict(type='text', fields=['payload.not_here_boss',
                                        'payload.instance_id2'])
        tdef = converter.TraitDefinition('test_trait', cfg,
                                         self.fake_plugin_mgr)
        t = tdef.to_trait(self.n1)
        self.assertIs(None, t)

    def test_missing_fields_config(self):
        self.assertRaises(converter.EventDefinitionException,
                          converter.TraitDefinition,
                          'bogus_trait',
                          dict(),
                          self.fake_plugin_mgr)

    def test_string_fields_config(self):
        cfg = dict(fields='payload.test')
        t = converter.TraitDefinition('test_trait', cfg, self.fake_plugin_mgr)
        self.assertPathsEqual(t.fields, jsonpath_rw.parse('payload.test'))

    def test_list_fields_config(self):
        cfg = dict(fields=['payload.test', 'payload.other'])
        t = converter.TraitDefinition('test_trait', cfg, self.fake_plugin_mgr)
        self.assertPathsEqual(
            t.fields,
            jsonpath_rw.parse('(payload.test)|(payload.other)'))

    def test_invalid_path_config(self):
        #test invalid jsonpath...
        cfg = dict(fields='payload.bogus(')
        self.assertRaises(converter.EventDefinitionException,
                          converter.TraitDefinition,
                          'bogus_trait',
                          cfg,
                          self.fake_plugin_mgr)

    def test_invalid_plugin_config(self):
        #test invalid jsonpath...
        cfg = dict(fields='payload.test', plugin=dict(bogus="true"))
        self.assertRaises(converter.EventDefinitionException,
                          converter.TraitDefinition,
                          'test_trait',
                          cfg,
                          self.fake_plugin_mgr)

    def test_unknown_plugin(self):
        #test invalid jsonpath...
        cfg = dict(fields='payload.test', plugin=dict(name='bogus'))
        self.assertRaises(converter.EventDefinitionException,
                          converter.TraitDefinition,
                          'test_trait',
                          cfg,
                          self.fake_plugin_mgr)

    def test_type_config(self):
        cfg = dict(type='text', fields='payload.test')
        t = converter.TraitDefinition('test_trait', cfg, self.fake_plugin_mgr)
        self.assertEqual(models.Trait.TEXT_TYPE, t.trait_type)

        cfg = dict(type='int', fields='payload.test')
        t = converter.TraitDefinition('test_trait', cfg, self.fake_plugin_mgr)
        self.assertEqual(models.Trait.INT_TYPE, t.trait_type)

        cfg = dict(type='float', fields='payload.test')
        t = converter.TraitDefinition('test_trait', cfg, self.fake_plugin_mgr)
        self.assertEqual(models.Trait.FLOAT_TYPE, t.trait_type)

        cfg = dict(type='datetime', fields='payload.test')
        t = converter.TraitDefinition('test_trait', cfg, self.fake_plugin_mgr)
        self.assertEqual(models.Trait.DATETIME_TYPE, t.trait_type)

    def test_invalid_type_config(self):
        #test invalid jsonpath...
        cfg = dict(type='bogus', fields='payload.test')
        self.assertRaises(converter.EventDefinitionException,
                          converter.TraitDefinition,
                          'bogus_trait',
                          cfg,
                          self.fake_plugin_mgr)


class TestEventDefinition(ConverterBase):

    def setUp(self):
        super(TestEventDefinition, self).setUp()

        self.traits_cfg = {
            'instance_id': {
                'type': 'text',
                'fields': ['payload.instance_uuid',
                           'payload.instance_id'],
            },
            'host': {
                'type': 'text',
                'fields': 'payload.host',
            },
        }

        self.test_notification1 = self._create_test_notification(
            "test.thing",
            "uuid-for-notif-0001",
            instance_id="uuid-for-instance-0001",
            host='host-1-2-3')

        self.test_notification2 = self._create_test_notification(
            "test.thing",
            "uuid-for-notif-0002",
            instance_id="uuid-for-instance-0002")

        self.test_notification3 = self._create_test_notification(
            "test.thing",
            "uuid-for-notif-0003",
            instance_id="uuid-for-instance-0003",
            host=None)
        self.fake_plugin_mgr = {}

    def test_to_event(self):
        dtype = models.Trait.TEXT_TYPE
        cfg = dict(event_type='test.thing', traits=self.traits_cfg)
        edef = converter.EventDefinition(cfg, self.fake_plugin_mgr)

        e = edef.to_event(self.test_notification1)
        self.assertEqual('test.thing', e.event_type)
        self.assertEqual(datetime.datetime(2013, 8, 8, 21, 6, 37, 803826),
                         e.generated)

        self.assertHasDefaultTraits(e)
        self.assertHasTrait(e, 'host', value='host-1-2-3', dtype=dtype)
        self.assertHasTrait(e, 'instance_id',
                            value='uuid-for-instance-0001',
                            dtype=dtype)

    def test_to_event_missing_trait(self):
        dtype = models.Trait.TEXT_TYPE
        cfg = dict(event_type='test.thing', traits=self.traits_cfg)
        edef = converter.EventDefinition(cfg, self.fake_plugin_mgr)

        e = edef.to_event(self.test_notification2)

        self.assertHasDefaultTraits(e)
        self.assertHasTrait(e, 'instance_id',
                            value='uuid-for-instance-0002',
                            dtype=dtype)
        self.assertDoesNotHaveTrait(e, 'host')

    def test_to_event_null_trait(self):
        dtype = models.Trait.TEXT_TYPE
        cfg = dict(event_type='test.thing', traits=self.traits_cfg)
        edef = converter.EventDefinition(cfg, self.fake_plugin_mgr)

        e = edef.to_event(self.test_notification3)

        self.assertHasDefaultTraits(e)
        self.assertHasTrait(e, 'instance_id',
                            value='uuid-for-instance-0003',
                            dtype=dtype)
        self.assertDoesNotHaveTrait(e, 'host')

    def test_bogus_cfg_no_traits(self):
        bogus = dict(event_type='test.foo')
        self.assertRaises(converter.EventDefinitionException,
                          converter.EventDefinition,
                          bogus,
                          self.fake_plugin_mgr)

    def test_bogus_cfg_no_type(self):
        bogus = dict(traits=self.traits_cfg)
        self.assertRaises(converter.EventDefinitionException,
                          converter.EventDefinition,
                          bogus,
                          self.fake_plugin_mgr)

    def test_included_type_string(self):
        cfg = dict(event_type='test.thing', traits=self.traits_cfg)
        edef = converter.EventDefinition(cfg, self.fake_plugin_mgr)
        self.assertEqual(1, len(edef._included_types))
        self.assertEqual('test.thing', edef._included_types[0])
        self.assertEqual(0, len(edef._excluded_types))
        self.assertTrue(edef.included_type('test.thing'))
        self.assertFalse(edef.excluded_type('test.thing'))
        self.assertTrue(edef.match_type('test.thing'))
        self.assertFalse(edef.match_type('random.thing'))

    def test_included_type_list(self):
        cfg = dict(event_type=['test.thing', 'other.thing'],
                   traits=self.traits_cfg)
        edef = converter.EventDefinition(cfg, self.fake_plugin_mgr)
        self.assertEqual(2, len(edef._included_types))
        self.assertEqual(0, len(edef._excluded_types))
        self.assertTrue(edef.included_type('test.thing'))
        self.assertTrue(edef.included_type('other.thing'))
        self.assertFalse(edef.excluded_type('test.thing'))
        self.assertTrue(edef.match_type('test.thing'))
        self.assertTrue(edef.match_type('other.thing'))
        self.assertFalse(edef.match_type('random.thing'))

    def test_excluded_type_string(self):
        cfg = dict(event_type='!test.thing', traits=self.traits_cfg)
        edef = converter.EventDefinition(cfg, self.fake_plugin_mgr)
        self.assertEqual(1, len(edef._included_types))
        self.assertEqual('*', edef._included_types[0])
        self.assertEqual('test.thing', edef._excluded_types[0])
        self.assertEqual(1, len(edef._excluded_types))
        self.assertEqual('test.thing', edef._excluded_types[0])
        self.assertTrue(edef.excluded_type('test.thing'))
        self.assertTrue(edef.included_type('random.thing'))
        self.assertFalse(edef.match_type('test.thing'))
        self.assertTrue(edef.match_type('random.thing'))

    def test_excluded_type_list(self):
        cfg = dict(event_type=['!test.thing', '!other.thing'],
                   traits=self.traits_cfg)
        edef = converter.EventDefinition(cfg, self.fake_plugin_mgr)
        self.assertEqual(1, len(edef._included_types))
        self.assertEqual(2, len(edef._excluded_types))
        self.assertTrue(edef.excluded_type('test.thing'))
        self.assertTrue(edef.excluded_type('other.thing'))
        self.assertFalse(edef.excluded_type('random.thing'))
        self.assertFalse(edef.match_type('test.thing'))
        self.assertFalse(edef.match_type('other.thing'))
        self.assertTrue(edef.match_type('random.thing'))

    def test_mixed_type_list(self):
        cfg = dict(event_type=['*.thing', '!test.thing', '!other.thing'],
                   traits=self.traits_cfg)
        edef = converter.EventDefinition(cfg, self.fake_plugin_mgr)
        self.assertEqual(1, len(edef._included_types))
        self.assertEqual(2, len(edef._excluded_types))
        self.assertTrue(edef.excluded_type('test.thing'))
        self.assertTrue(edef.excluded_type('other.thing'))
        self.assertFalse(edef.excluded_type('random.thing'))
        self.assertFalse(edef.match_type('test.thing'))
        self.assertFalse(edef.match_type('other.thing'))
        self.assertFalse(edef.match_type('random.whatzit'))
        self.assertTrue(edef.match_type('random.thing'))

    def test_catchall(self):
        cfg = dict(event_type=['*.thing', '!test.thing', '!other.thing'],
                   traits=self.traits_cfg)
        edef = converter.EventDefinition(cfg, self.fake_plugin_mgr)
        self.assertFalse(edef.is_catchall)

        cfg = dict(event_type=['!other.thing'],
                   traits=self.traits_cfg)
        edef = converter.EventDefinition(cfg, self.fake_plugin_mgr)
        self.assertFalse(edef.is_catchall)

        cfg = dict(event_type=['other.thing'],
                   traits=self.traits_cfg)
        edef = converter.EventDefinition(cfg, self.fake_plugin_mgr)
        self.assertFalse(edef.is_catchall)

        cfg = dict(event_type=['*', '!other.thing'],
                   traits=self.traits_cfg)
        edef = converter.EventDefinition(cfg, self.fake_plugin_mgr)
        self.assertFalse(edef.is_catchall)

        cfg = dict(event_type=['*'],
                   traits=self.traits_cfg)
        edef = converter.EventDefinition(cfg, self.fake_plugin_mgr)
        self.assertTrue(edef.is_catchall)

        cfg = dict(event_type=['*', 'foo'],
                   traits=self.traits_cfg)
        edef = converter.EventDefinition(cfg, self.fake_plugin_mgr)
        self.assertTrue(edef.is_catchall)

    @mock.patch('ceilometer.openstack.common.timeutils.utcnow')
    def test_extract_when(self, mock_utcnow):
        now = datetime.datetime.utcnow()
        modified = now + datetime.timedelta(minutes=1)
        mock_utcnow.return_value = now

        body = {"timestamp": str(modified)}
        when = converter.EventDefinition._extract_when(body)
        self.assertTimestampEqual(modified, when)

        body = {"_context_timestamp": str(modified)}
        when = converter.EventDefinition._extract_when(body)
        self.assertTimestampEqual(modified, when)

        then = now + datetime.timedelta(hours=1)
        body = {"timestamp": str(modified), "_context_timestamp": str(then)}
        when = converter.EventDefinition._extract_when(body)
        self.assertTimestampEqual(modified, when)

        when = converter.EventDefinition._extract_when({})
        self.assertTimestampEqual(now, when)

    def test_default_traits(self):
        cfg = dict(event_type='test.thing', traits={})
        edef = converter.EventDefinition(cfg, self.fake_plugin_mgr)
        default_traits = converter.EventDefinition.DEFAULT_TRAITS.keys()
        traits = set(edef.traits.keys())
        for dt in default_traits:
            self.assertIn(dt, traits)
        self.assertEqual(len(converter.EventDefinition.DEFAULT_TRAITS),
                         len(edef.traits))

    def test_traits(self):
        cfg = dict(event_type='test.thing', traits=self.traits_cfg)
        edef = converter.EventDefinition(cfg, self.fake_plugin_mgr)
        default_traits = converter.EventDefinition.DEFAULT_TRAITS.keys()
        traits = set(edef.traits.keys())
        for dt in default_traits:
            self.assertIn(dt, traits)
        self.assertIn('host', traits)
        self.assertIn('instance_id', traits)
        self.assertEqual(len(converter.EventDefinition.DEFAULT_TRAITS) + 2,
                         len(edef.traits))


class TestNotificationConverter(ConverterBase):

    def setUp(self):
        super(TestNotificationConverter, self).setUp()

        self.valid_event_def1 = [{
            'event_type': 'compute.instance.create.*',
            'traits': {
                'instance_id': {
                    'type': 'text',
                    'fields': ['payload.instance_uuid',
                               'payload.instance_id'],
                },
                'host': {
                    'type': 'text',
                    'fields': 'payload.host',
                },
            },
        }]

        self.test_notification1 = self._create_test_notification(
            "compute.instance.create.start",
            "uuid-for-notif-0001",
            instance_id="uuid-for-instance-0001",
            host='host-1-2-3')
        self.test_notification2 = self._create_test_notification(
            "bogus.notification.from.mars",
            "uuid-for-notif-0002",
            weird='true',
            host='cydonia')
        self.fake_plugin_mgr = {}

    @mock.patch('ceilometer.openstack.common.timeutils.utcnow')
    def test_converter_missing_keys(self, mock_utcnow):
        # test a malformed notification
        now = datetime.datetime.utcnow()
        mock_utcnow.return_value = now
        c = converter.NotificationEventsConverter(
            [],
            self.fake_plugin_mgr,
            add_catchall=True)
        message = {'event_type': "foo",
                   'message_id': "abc",
                   'publisher_id': "1"}
        e = c.to_event(message)
        self.assertIsValidEvent(e, message)
        self.assertEqual(1, len(e.traits))
        self.assertEqual("foo", e.event_type)
        self.assertEqual(now, e.generated)

    def test_converter_with_catchall(self):
        c = converter.NotificationEventsConverter(
            self.valid_event_def1,
            self.fake_plugin_mgr,
            add_catchall=True)
        self.assertEqual(2, len(c.definitions))
        e = c.to_event(self.test_notification1)
        self.assertIsValidEvent(e, self.test_notification1)
        self.assertEqual(3, len(e.traits))
        self.assertHasDefaultTraits(e)
        self.assertHasTrait(e, 'instance_id')
        self.assertHasTrait(e, 'host')

        e = c.to_event(self.test_notification2)
        self.assertIsValidEvent(e, self.test_notification2)
        self.assertEqual(1, len(e.traits))
        self.assertHasDefaultTraits(e)
        self.assertDoesNotHaveTrait(e, 'instance_id')
        self.assertDoesNotHaveTrait(e, 'host')

    def test_converter_without_catchall(self):
        c = converter.NotificationEventsConverter(
            self.valid_event_def1,
            self.fake_plugin_mgr,
            add_catchall=False)
        self.assertEqual(1, len(c.definitions))
        e = c.to_event(self.test_notification1)
        self.assertIsValidEvent(e, self.test_notification1)
        self.assertEqual(3, len(e.traits))
        self.assertHasDefaultTraits(e)
        self.assertHasTrait(e, 'instance_id')
        self.assertHasTrait(e, 'host')

        e = c.to_event(self.test_notification2)
        self.assertIsNotValidEvent(e, self.test_notification2)

    def test_converter_empty_cfg_with_catchall(self):
        c = converter.NotificationEventsConverter(
            [],
            self.fake_plugin_mgr,
            add_catchall=True)
        self.assertEqual(1, len(c.definitions))
        e = c.to_event(self.test_notification1)
        self.assertIsValidEvent(e, self.test_notification1)
        self.assertEqual(1, len(e.traits))
        self.assertHasDefaultTraits(e)

        e = c.to_event(self.test_notification2)
        self.assertIsValidEvent(e, self.test_notification2)
        self.assertEqual(1, len(e.traits))
        self.assertHasDefaultTraits(e)

    def test_converter_empty_cfg_without_catchall(self):
        c = converter.NotificationEventsConverter(
            [],
            self.fake_plugin_mgr,
            add_catchall=False)
        self.assertEqual(0, len(c.definitions))
        e = c.to_event(self.test_notification1)
        self.assertIsNotValidEvent(e, self.test_notification1)

        e = c.to_event(self.test_notification2)
        self.assertIsNotValidEvent(e, self.test_notification2)

    def test_setup_events_default_config(self):

        def mock_exists(path):
            return False

        def mock_get_config_file():
            return None

        with mock.patch('ceilometer.event.converter.get_config_file',
                        mock_get_config_file):

            oslo_cfg.CONF.set_override('drop_unmatched_notifications',
                                       False, group='event')

            with mock.patch('os.path.exists', mock_exists):
                c = converter.setup_events(self.fake_plugin_mgr)
            self.assertIsInstance(c, converter.NotificationEventsConverter)
            self.assertEqual(1, len(c.definitions))
            self.assertTrue(c.definitions[0].is_catchall)

            oslo_cfg.CONF.set_override('drop_unmatched_notifications',
                                       True, group='event')

            with mock.patch('os.path.exists', mock_exists):
                c = converter.setup_events(self.fake_plugin_mgr)
            self.assertIsInstance(c, converter.NotificationEventsConverter)
            self.assertEqual(0, len(c.definitions))

########NEW FILE########
__FILENAME__ = test_endpoint
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests for Ceilometer notify daemon."""

import mock

import oslo.messaging
from stevedore import extension

from ceilometer.event import endpoint as event_endpoint
from ceilometer import messaging
from ceilometer.openstack.common.fixture import config
from ceilometer.storage import models
from ceilometer.tests import base as tests_base

TEST_NOTICE_CTXT = {
    u'auth_token': u'3d8b13de1b7d499587dfc69b77dc09c2',
    u'is_admin': True,
    u'project_id': u'7c150a59fe714e6f9263774af9688f0e',
    u'quota_class': None,
    u'read_deleted': u'no',
    u'remote_address': u'10.0.2.15',
    u'request_id': u'req-d68b36e0-9233-467f-9afb-d81435d64d66',
    u'roles': [u'admin'],
    u'timestamp': u'2012-05-08T20:23:41.425105',
    u'user_id': u'1e3ce043029547f1a61c1996d1a531a2',
}

TEST_NOTICE_METADATA = {
    u'message_id': u'dae6f69c-00e0-41c0-b371-41ec3b7f4451',
    u'timestamp': u'2012-05-08 20:23:48.028195',
}

TEST_NOTICE_PAYLOAD = {
    u'created_at': u'2012-05-08 20:23:41',
    u'deleted_at': u'',
    u'disk_gb': 0,
    u'display_name': u'testme',
    u'fixed_ips': [{u'address': u'10.0.0.2',
                    u'floating_ips': [],
                    u'meta': {},
                    u'type': u'fixed',
                    u'version': 4}],
    u'image_ref_url': u'http://10.0.2.15:9292/images/UUID',
    u'instance_id': u'9f9d01b9-4a58-4271-9e27-398b21ab20d1',
    u'instance_type': u'm1.tiny',
    u'instance_type_id': 2,
    u'launched_at': u'2012-05-08 20:23:47.985999',
    u'memory_mb': 512,
    u'state': u'active',
    u'state_description': u'',
    u'tenant_id': u'7c150a59fe714e6f9263774af9688f0e',
    u'user_id': u'1e3ce043029547f1a61c1996d1a531a2',
    u'reservation_id': u'1e3ce043029547f1a61c1996d1a531a3',
    u'vcpus': 1,
    u'root_gb': 0,
    u'ephemeral_gb': 0,
    u'host': u'compute-host-name',
    u'availability_zone': u'1e3ce043029547f1a61c1996d1a531a4',
    u'os_type': u'linux?',
    u'architecture': u'x86',
    u'image_ref': u'UUID',
    u'kernel_id': u'1e3ce043029547f1a61c1996d1a531a5',
    u'ramdisk_id': u'1e3ce043029547f1a61c1996d1a531a6',
}


class TestEventEndpoint(tests_base.BaseTestCase):

    def setUp(self):
        super(TestEventEndpoint, self).setUp()
        self.CONF = self.useFixture(config.Config()).conf
        self.CONF([])
        messaging.setup('fake://')
        self.addCleanup(messaging.cleanup)
        self.CONF.set_override("connection", "log://", group='database')
        self.CONF.set_override("store_events", True, group="notification")

        self.mock_dispatcher = mock.MagicMock()
        self.endpoint = event_endpoint.EventsNotificationEndpoint()
        self.endpoint.dispatcher_manager = \
            extension.ExtensionManager.make_test_instance([
                extension.Extension('test', None, None, self.mock_dispatcher)
            ])
        self.endpoint.event_converter = mock.MagicMock()
        self.endpoint.event_converter.to_event.return_value = mock.MagicMock(
            event_type='test.test')

    def test_message_to_event(self):
        self.endpoint.info(TEST_NOTICE_CTXT, 'compute.vagrant-precise',
                           'compute.instance.create.end',
                           TEST_NOTICE_PAYLOAD, TEST_NOTICE_METADATA)

    def test_message_to_event_duplicate(self):
        self.mock_dispatcher.record_events.return_value = [
            (models.Event.DUPLICATE, object())]
        message = {'event_type': "foo", 'message_id': "abc"}
        self.endpoint.process_notification(message)  # Should return silently.

    def test_message_to_event_bad_event(self):
        self.CONF.set_override("ack_on_event_error", False,
                               group="notification")
        self.mock_dispatcher.record_events.return_value = [
            (models.Event.UNKNOWN_PROBLEM, object())]
        message = {'event_type': "foo", 'message_id': "abc"}
        ret = self.endpoint.process_notification(message)
        self.assertEqual(oslo.messaging.NotificationResult.REQUEUE, ret)

########NEW FILE########
__FILENAME__ = test_trait_plugins
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Rackspace Hosting.
#
# Author: Monsyne Dragon <mdragon@rackspace.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ceilometer.event import trait_plugins
from ceilometer.openstack.common import test


class TestSplitterPlugin(test.BaseTestCase):

    def setUp(self):
        super(TestSplitterPlugin, self).setUp()
        self.pclass = trait_plugins.SplitterTraitPlugin

    def test_split(self):
        param = dict(separator='-', segment=0)
        plugin = self.pclass(**param)
        match_list = [('test.thing', 'test-foobar-baz')]
        value = plugin.trait_value(match_list)
        self.assertEqual('test', value)

        param = dict(separator='-', segment=1)
        plugin = self.pclass(**param)
        match_list = [('test.thing', 'test-foobar-baz')]
        value = plugin.trait_value(match_list)
        self.assertEqual('foobar', value)

        param = dict(separator='-', segment=1, max_split=1)
        plugin = self.pclass(**param)
        match_list = [('test.thing', 'test-foobar-baz')]
        value = plugin.trait_value(match_list)
        self.assertEqual('foobar-baz', value)

    def test_no_sep(self):
        param = dict(separator='-', segment=0)
        plugin = self.pclass(**param)
        match_list = [('test.thing', 'test.foobar.baz')]
        value = plugin.trait_value(match_list)
        self.assertEqual('test.foobar.baz', value)

    def test_no_segment(self):
        param = dict(separator='-', segment=5)
        plugin = self.pclass(**param)
        match_list = [('test.thing', 'test-foobar-baz')]
        value = plugin.trait_value(match_list)
        self.assertIs(None, value)

    def test_no_match(self):
        param = dict(separator='-', segment=0)
        plugin = self.pclass(**param)
        match_list = []
        value = plugin.trait_value(match_list)
        self.assertIs(None, value)


class TestBitfieldPlugin(test.BaseTestCase):

    def setUp(self):
        super(TestBitfieldPlugin, self).setUp()
        self.pclass = trait_plugins.BitfieldTraitPlugin
        self.init = 0
        self.params = dict(initial_bitfield=self.init,
                           flags=[dict(path='payload.foo', bit=0, value=42),
                                  dict(path='payload.foo', bit=1, value=12),
                                  dict(path='payload.thud', bit=1, value=23),
                                  dict(path='thingy.boink', bit=4),
                                  dict(path='thingy.quux', bit=6,
                                       value="wokka"),
                                  dict(path='payload.bar', bit=10,
                                       value='test')])

    def test_bitfield(self):
        match_list = [('payload.foo', 12),
                      ('payload.bar', 'test'),
                      ('thingy.boink', 'testagain')]

        plugin = self.pclass(**self.params)
        value = plugin.trait_value(match_list)
        self.assertEqual(0x412, value)

    def test_initial(self):
        match_list = [('payload.foo', 12),
                      ('payload.bar', 'test'),
                      ('thingy.boink', 'testagain')]
        self.params['initial_bitfield'] = 0x2000
        plugin = self.pclass(**self.params)
        value = plugin.trait_value(match_list)
        self.assertEqual(0x2412, value)

    def test_no_match(self):
        match_list = []
        plugin = self.pclass(**self.params)
        value = plugin.trait_value(match_list)
        self.assertEqual(self.init, value)

    def test_multi(self):
        match_list = [('payload.foo', 12),
                      ('payload.thud', 23),
                      ('payload.bar', 'test'),
                      ('thingy.boink', 'testagain')]

        plugin = self.pclass(**self.params)
        value = plugin.trait_value(match_list)
        self.assertEqual(0x412, value)

########NEW FILE########
__FILENAME__ = base
# -*- encoding: utf-8 -*-
#
# Copyright © 2014 Intel Corp
#
# Authors: Lianhao Lu <lianhao.lu@intel.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ceilometer.hardware.inspector import base


class InspectorBaseTest(object):
    """Subclass must set self.inspector and self.host in
    self.setUp()
    """

    cpu = [base.CPUStats(cpu_1_min=0.1,
                         cpu_5_min=0.2,
                         cpu_15_min=0.3),
           ]

    network = [(base.Interface(name='eth0',
                               mac='112233445566',
                               ip='10.0.0.1'),
                base.InterfaceStats(bandwidth=1250000 / 8,
                                    rx_bytes=1000,
                                    tx_bytes=2000,
                                    error=1)),
               ]
    diskspace = [(base.Disk(device='/dev/sda1', path='/'),
                  base.DiskStats(size=1000, used=500),
                  ),
                 (base.Disk(device='/dev/sda2', path='/home'),
                  base.DiskStats(size=2000, used=1000),
                  ),
                 ]
    memory = [base.MemoryStats(total=1000, used=500)]

    def test_inspect_cpu(self):
        self.assertEqual(list(self.inspector.inspect_cpu(self.host)),
                         self.cpu)

    def test_inspect_network(self):
        self.assertEqual(list(self.inspector.inspect_network(self.host)),
                         self.network)

    def test_inspect_disk(self):
        self.assertEqual(list(self.inspector.inspect_disk(self.host)),
                         self.diskspace)

    def test_inspect_memory(self):
        self.assertEqual(list(self.inspector.inspect_memory(self.host)),
                         self.memory)

########NEW FILE########
__FILENAME__ = test_inspector
# -*- encoding: utf-8 -*-
#
# Copyright © 2014 Intel Corp
#
# Authors: Lianhao Lu <lianhao.lu@intel.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ceilometer.hardware import inspector
from ceilometer.openstack.common import network_utils
from ceilometer.tests import base


class TestHardwareInspector(base.BaseTestCase):
    def test_get_inspector(self):
        url = network_utils.urlsplit("snmp://")
        driver = inspector.get_inspector(url)
        self.assertTrue(driver)

    def test_get_inspector_illegal(self):
        url = network_utils.urlsplit("illegal://")
        self.assertRaises(RuntimeError,
                          inspector.get_inspector,
                          url)

########NEW FILE########
__FILENAME__ = test_snmp
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Intel Corp
#
# Authors: Lianhao Lu <lianhao.lu@intel.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests for ceilometer/hardware/inspector/snmp/inspector.py
"""

from ceilometer.hardware.inspector import snmp
from ceilometer.openstack.common.fixture import mockpatch
from ceilometer.openstack.common import network_utils
from ceilometer.tests import base as test_base
from ceilometer.tests.hardware.inspector import base

Base = base.InspectorBaseTest


class FakeMac(object):
    def __init__(self):
        self.val = "0x%s" % Base.network[0][0].mac

    def prettyPrint(self):
        return str(self.val)

ins = snmp.SNMPInspector
GETCMD_MAP = {
    ins._cpu_1_min_load_oid: (None,
                              None,
                              0,
                              [('',
                                Base.cpu[0].cpu_1_min,
                                )],
                              ),
    ins._cpu_5_min_load_oid: (None,
                              None,
                              0,
                              [('',
                                Base.cpu[0].cpu_5_min,
                                )],
                              ),
    ins._cpu_15_min_load_oid: (None,
                               None,
                               0,
                               [('',
                                 Base.cpu[0].cpu_15_min,
                                 )],
                               ),
    ins._memory_total_oid: (None,
                            None,
                            0,
                            [('',
                              Base.memory[0].total,
                              )],
                            ),
    ins._memory_used_oid: (None,
                           None,
                           0,
                           [('',
                             Base.memory[0].used,
                             )],
                           ),
    ins._disk_path_oid + '.1': (None,
                                None,
                                0,
                                [('',
                                  Base.diskspace[0][0].path,
                                  )],
                                ),
    ins._disk_device_oid + '.1': (None,
                                  None,
                                  0,
                                  [('',
                                    Base.diskspace[0][0].device,
                                    )],
                                  ),
    ins._disk_size_oid + '.1': (None,
                                None,
                                0,
                                [('',
                                  Base.diskspace[0][1].size,
                                  )],
                                ),
    ins._disk_used_oid + '.1': (None,
                                None,
                                0,
                                [('',
                                  Base.diskspace[0][1].used,
                                  )],
                                ),
    ins._disk_path_oid + '.2': (None,
                                None,
                                0,
                                [('',
                                  Base.diskspace[1][0].path,
                                  )],
                                ),
    ins._disk_device_oid + '.2': (None,
                                  None,
                                  0,
                                  [('',
                                    Base.diskspace[1][0].device,
                                    )],
                                  ),
    ins._disk_size_oid + '.2': (None,
                                None,
                                0,
                                [('',
                                  Base.diskspace[1][1].size,
                                  )],
                                ),
    ins._disk_used_oid + '.2': (None,
                                None,
                                0,
                                [('',
                                  Base.diskspace[1][1].used,
                                  )],
                                ),
    ins._interface_name_oid + '.1': (None,
                                     None,
                                     0,
                                     [('',
                                       Base.network[0][0].name,
                                       )],
                                     ),
    ins._interface_mac_oid + '.1': (None,
                                    None,
                                    0,
                                    [('',
                                      FakeMac(),
                                      )],
                                    ),
    ins._interface_bandwidth_oid + '.1': (None,
                                          None,
                                          0,
                                          [('',
                                            Base.network[0][1].bandwidth * 8,
                                            )],
                                          ),
    ins._interface_received_oid + '.1': (None,
                                         None,
                                         0,
                                         [('',
                                           Base.network[0][1].rx_bytes,
                                           )],
                                         ),
    ins._interface_transmitted_oid + '.1': (None,
                                            None,
                                            0,
                                            [('',
                                              Base.network[0][1].tx_bytes,
                                              )],
                                            ),
    ins._interface_error_oid + '.1': (None,
                                      None,
                                      0,
                                      [('',
                                        Base.network[0][1].error,
                                        )],
                                      ),
}

NEXTCMD_MAP = {
    ins._disk_index_oid: (None,
                          None,
                          0,
                          [[('1.3.6.1.4.1.2021.9.1.1.1', 1)],
                           [('1.3.6.1.4.1.2021.9.1.1.2', 2)]]),
    ins._interface_index_oid: (None,
                               None,
                               0,
                               [[('1.3.6.1.2.1.2.2.1.1.1', 1)],
                                ]),
    ins._interface_ip_oid: (None,
                            None,
                            0,
                            [[('1.3.6.1.2.1.4.20.1.2.10.0.0.1',
                               1)],
                             ]),
}


def faux_getCmd(authData, transportTarget, oid):
    try:
        return GETCMD_MAP[oid]
    except KeyError:
        return ("faux_getCmd Error", None, 0, [])


def faux_nextCmd(authData, transportTarget, oid):
    try:
        return NEXTCMD_MAP[oid]
    except KeyError:
        return ("faux_nextCmd Error", None, 0, [])


class TestSNMPInspector(Base, test_base.BaseTestCase):
    def setUp(self):
        super(TestSNMPInspector, self).setUp()
        self.inspector = snmp.SNMPInspector()
        self.host = network_utils.urlsplit("snmp://localhost")
        self.useFixture(mockpatch.PatchObject(
            self.inspector._cmdGen, 'getCmd', new=faux_getCmd))
        self.useFixture(mockpatch.PatchObject(
            self.inspector._cmdGen, 'nextCmd', new=faux_nextCmd))

    def test_get_security_name(self):
        self.assertEqual(self.inspector._get_security_name(self.host),
                         self.inspector._security_name)
        host2 = network_utils.urlsplit("snmp://foo:80?security_name=fake")
        self.assertEqual(self.inspector._get_security_name(host2),
                         'fake')

    def test_get_cmd_error(self):
        self.useFixture(mockpatch.PatchObject(
            self.inspector, '_memory_total_oid', new='failure'))

        def get_list(func, *args, **kwargs):
            return list(func(*args, **kwargs))

        self.assertRaises(snmp.SNMPException,
                          get_list,
                          self.inspector.inspect_memory,
                          self.host)

########NEW FILE########
__FILENAME__ = base
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Intel Corp
#
# Authors: Lianhao Lu <lianhao.lu@intel.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import fixtures
import mock

from ceilometer.central import manager
from ceilometer.hardware.inspector import base as inspector_base
from ceilometer.tests import base as test_base


class FakeInspector(inspector_base.Inspector):
    CPU = inspector_base.CPUStats(cpu_1_min=0.99,
                                  cpu_5_min=0.77,
                                  cpu_15_min=0.55)
    DISK = (inspector_base.Disk(device='/dev/sda1', path='/'),
            inspector_base.DiskStats(size=1000, used=90))
    MEMORY = inspector_base.MemoryStats(total=1000, used=90)
    NET = (inspector_base.Interface(name='test.teest',
                                    mac='001122334455',
                                    ip='10.0.0.2'),
           inspector_base.InterfaceStats(bandwidth=1000,
                                         rx_bytes=90,
                                         tx_bytes=80,
                                         error=1))

    def inspect_cpu(self, host):
        yield self.CPU

    def inspect_disk(self, host):
        yield self.DISK

    def inspect_memory(self, host):
        yield self.MEMORY

    def inspect_network(self, host):
        yield self.NET


class TestPollsterBase(test_base.BaseTestCase):
    def faux_get_inspector(url, namespace=None):
        return FakeInspector()

    def setUp(self):
        super(TestPollsterBase, self).setUp()
        self.hosts = ["test://test", "test://test2"]
        self.useFixture(fixtures.MonkeyPatch(
            'ceilometer.hardware.inspector.get_inspector',
            self.faux_get_inspector))

    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    def _check_get_samples(self, factory, name,
                           expected_value, expected_type, expected_unit=None):
        mgr = manager.AgentManager()
        pollster = factory()
        cache = {}
        samples = list(pollster.get_samples(mgr, cache, self.hosts))
        self.assertTrue(samples)
        self.assertIn(pollster.CACHE_KEY, cache)
        for host in self.hosts:
            self.assertIn(host, cache[pollster.CACHE_KEY])

        self.assertEqual(set([name]),
                         set([s.name for s in samples]))
        match = [s for s in samples if s.name == name]
        self.assertEqual(expected_value, match[0].volume)
        self.assertEqual(expected_type, match[0].type)
        if expected_unit:
            self.assertEqual(expected_unit, match[0].unit)

########NEW FILE########
__FILENAME__ = test_cpu
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Intel Corp
#
# Authors: Lianhao Lu <lianhao.lu@intel.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ceilometer.hardware.pollsters import cpu
from ceilometer import sample
from ceilometer.tests.hardware.pollsters import base


class TestCPUPollsters(base.TestPollsterBase):
    def test_1min(self):
        self._check_get_samples(cpu.CPULoad1MinPollster,
                                'hardware.cpu.load.1min',
                                0.99, sample.TYPE_GAUGE,
                                expected_unit='process')

    def test_5min(self):
        self._check_get_samples(cpu.CPULoad5MinPollster,
                                'hardware.cpu.load.5min',
                                0.77, sample.TYPE_GAUGE,
                                expected_unit='process')

    def test_15min(self):
        self._check_get_samples(cpu.CPULoad15MinPollster,
                                'hardware.cpu.load.15min',
                                0.55, sample.TYPE_GAUGE,
                                expected_unit='process')

########NEW FILE########
__FILENAME__ = test_disk
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Intel Corp
#
# Authors: Lianhao Lu <lianhao.lu@intel.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ceilometer.hardware.pollsters import disk
from ceilometer import sample
from ceilometer.tests.hardware.pollsters import base


class TestDiskPollsters(base.TestPollsterBase):
    def test_disk_size_total(self):
        self._check_get_samples(disk.DiskTotalPollster,
                                'hardware.disk.size.total',
                                1000, sample.TYPE_GAUGE)

    def test_disk_size_used(self):
        self._check_get_samples(disk.DiskUsedPollster,
                                'hardware.disk.size.used',
                                90, sample.TYPE_GAUGE)

########NEW FILE########
__FILENAME__ = test_memory
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Intel Corp
#
# Authors: Lianhao Lu <lianhao.lu@intel.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ceilometer.hardware.pollsters import memory
from ceilometer import sample
from ceilometer.tests.hardware.pollsters import base


class TestMemoryPollsters(base.TestPollsterBase):
    def test_memory_size_total(self):
        self._check_get_samples(memory.MemoryTotalPollster,
                                'hardware.memory.total',
                                1000, sample.TYPE_GAUGE)

    def test_memory_size_used(self):
        self._check_get_samples(memory.MemoryUsedPollster,
                                'hardware.memory.used',
                                90, sample.TYPE_GAUGE)

########NEW FILE########
__FILENAME__ = test_net
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Intel Corp
#
# Authors: Lianhao Lu <lianhao.lu@intel.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ceilometer.hardware.pollsters import net
from ceilometer import sample
from ceilometer.tests.hardware.pollsters import base


class TestNetPollsters(base.TestPollsterBase):
    def test_bandwidth(self):
        self._check_get_samples(net.BandwidthBytesPollster,
                                'hardware.network.bandwidth.bytes',
                                1000, sample.TYPE_CUMULATIVE)

    def test_incoming(self):
        self._check_get_samples(net.IncomingBytesPollster,
                                'hardware.network.incoming.bytes',
                                90, sample.TYPE_CUMULATIVE)

    def test_outgoing(self):
        self._check_get_samples(net.OutgoingBytesPollster,
                                'hardware.network.outgoing.bytes',
                                80, sample.TYPE_CUMULATIVE)

    def test_error(self):
        self._check_get_samples(net.OutgoingErrorsPollster,
                                'hardware.network.outgoing.errors',
                                1, sample.TYPE_CUMULATIVE)

########NEW FILE########
__FILENAME__ = test_glance
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import mock

from ceilometer.central import manager
from ceilometer.image import glance
from ceilometer.openstack.common import context
from ceilometer.openstack.common.fixture import mockpatch
from ceilometer.openstack.common import test

IMAGE_LIST = [
    type('Image', (object,),
         {u'status': u'queued',
          u'name': "some name",
          u'deleted': False,
          u'container_format': None,
          u'created_at': u'2012-09-18T16:29:46',
          u'disk_format': None,
          u'updated_at': u'2012-09-18T16:29:46',
          u'properties': {},
          u'min_disk': 0,
          u'protected': False,
          u'id': u'1d21a8d0-25f4-4e0a-b4ec-85f40237676b',
          u'location': None,
          u'checksum': None,
          u'owner': u'4c8364fc20184ed7971b76602aa96184',
          u'is_public': True,
          u'deleted_at': None,
          u'min_ram': 0,
          u'size': 2048}),
    type('Image', (object,),
         {u'status': u'active',
          u'name': "hello world",
          u'deleted': False,
          u'container_format': None,
          u'created_at': u'2012-09-18T16:27:41',
          u'disk_format': None,
          u'updated_at': u'2012-09-18T16:27:41',
          u'properties': {},
          u'min_disk': 0,
          u'protected': False,
          u'id': u'22be9f90-864d-494c-aa74-8035fd535989',
          u'location': None,
          u'checksum': None,
          u'owner': u'9e4f98287a0246daa42eaf4025db99d4',
          u'is_public': True,
          u'deleted_at': None,
          u'min_ram': 0,
          u'size': 0}),
    type('Image', (object,),
         {u'status': u'queued',
          u'name': None,
          u'deleted': False,
          u'container_format': None,
          u'created_at': u'2012-09-18T16:23:27',
          u'disk_format': "raw",
          u'updated_at': u'2012-09-18T16:23:27',
          u'properties': {},
          u'min_disk': 0,
          u'protected': False,
          u'id': u'8d133f6c-38a8-403c-b02c-7071b69b432d',
          u'location': None,
          u'checksum': None,
          u'owner': u'5f8806a76aa34ee8b8fc8397bd154319',
          u'is_public': True,
          u'deleted_at': None,
          u'min_ram': 0,
          u'size': 1024}),
    # Make one duplicate private image to test the iter_images method.
    type('Image', (object,),
         {u'status': u'queued',
          u'name': "some name",
          u'deleted': False,
          u'container_format': None,
          u'created_at': u'2012-09-18T16:29:46',
          u'disk_format': None,
          u'updated_at': u'2012-09-18T16:29:46',
          u'properties': {},
          u'min_disk': 0,
          u'protected': False,
          u'id': u'1d21a8d0-25f4-4e0a-b4ec-85f40237676b',
          u'location': None,
          u'checksum': None,
          u'owner': u'4c8364fc20184ed7971b76602aa96184',
          u'is_public': True,
          u'deleted_at': None,
          u'min_ram': 0,
          u'size': 2048}),
]


class _BaseObject(object):
    pass


class TestManager(manager.AgentManager):

    def __init__(self):
        super(TestManager, self).__init__()
        self.keystone = None


class TestImagePollster(test.BaseTestCase):

    def fake_get_glance_client(self, ksclient):
        glanceclient = _BaseObject()
        setattr(glanceclient, "images", _BaseObject())
        setattr(glanceclient.images,
                "list", lambda *args, **kwargs: iter(IMAGE_LIST))
        return glanceclient

    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    def setUp(self):
        super(TestImagePollster, self).setUp()
        self.context = context.get_admin_context()
        self.manager = TestManager()
        self.useFixture(mockpatch.PatchObject(
            glance._Base, 'get_glance_client',
            side_effect=self.fake_get_glance_client))

    def test_iter_images(self):
        # Tests whether the iter_images method returns an unique image
        # list when there is nothing in the cache
        images = list(glance.ImagePollster().
                      _iter_images(self.manager.keystone, {}))
        self.assertEqual(len(set(image.id for image in images)), len(images))

    def test_iter_images_cached(self):
        # Tests whether the iter_images method returns the values from
        # the cache
        cache = {'images': []}
        images = list(glance.ImagePollster().
                      _iter_images(self.manager.keystone, cache))
        self.assertEqual([], images)

    def test_image(self):
        samples = list(glance.ImagePollster().get_samples(self.manager, {}))
        self.assertEqual(3, len(samples))
        for sample in samples:
            self.assertEqual(1, sample.volume)

    def test_image_size(self):
        samples = list(glance.ImageSizePollster().get_samples(self.manager,
                                                              {}))
        self.assertEqual(3, len(samples))
        for image in IMAGE_LIST:
            self.assertTrue(
                any(map(lambda sample: sample.volume == image.size,
                        samples)))

    def test_image_get_sample_names(self):
        samples = list(glance.ImagePollster().get_samples(self.manager, {}))
        self.assertEqual(set(['image']), set([s.name for s in samples]))

    def test_image_size_get_sample_names(self):
        samples = list(glance.ImageSizePollster().get_samples(self.manager,
                                                              {}))
        self.assertEqual(set(['image.size']), set([s.name for s in samples]))

########NEW FILE########
__FILENAME__ = test_notifications
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 Red Hat Inc.
#
# Author: Eoghan Glynn <eglynn@redhat.com>
# Author: Julien danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import datetime

import mock

from ceilometer.image import notifications
from ceilometer.openstack.common import test
from ceilometer import sample


def fake_uuid(x):
    return '%s-%s-%s-%s' % (x * 8, x * 4, x * 4, x * 12)


NOW = datetime.datetime.isoformat(datetime.datetime.utcnow())

NOTIFICATION_SEND = {
    u'event_type': u'image.send',
    u'timestamp': NOW,
    u'message_id': fake_uuid('a'),
    u'priority': u'INFO',
    u'publisher_id': u'images.example.com',
    u'payload': {u'receiver_tenant_id': fake_uuid('b'),
                 u'destination_ip': u'1.2.3.4',
                 u'bytes_sent': 42,
                 u'image_id': fake_uuid('c'),
                 u'receiver_user_id': fake_uuid('d'),
                 u'owner_id': fake_uuid('e')}
}

IMAGE_META = {u'status': u'saving',
              u'name': u'fake image #3',
              u'deleted': False,
              u'container_format': u'ovf',
              u'created_at': u'2012-09-18T10:13:44.571370',
              u'disk_format': u'vhd',
              u'updated_at': u'2012-09-18T10:13:44.623120',
              u'properties': {u'key2': u'value2',
                              u'key1': u'value1'},
              u'min_disk': 0,
              u'protected': False,
              u'id': fake_uuid('c'),
              u'location': None,
              u'checksum': u'd990432ef91afef3ad9dbf4a975d3365',
              u'owner': "fake",
              u'is_public': False,
              u'deleted_at': None,
              u'min_ram': 0,
              u'size': 19}


NOTIFICATION_UPDATE = {"message_id": "0c65cb9c-018c-11e2-bc91-5453ed1bbb5f",
                       "publisher_id": "images.example.com",
                       "event_type": "image.update",
                       "priority": "info",
                       "payload": IMAGE_META,
                       "timestamp": NOW}


NOTIFICATION_UPLOAD = {"message_id": "0c65cb9c-018c-11e2-bc91-5453ed1bbb5f",
                       "publisher_id": "images.example.com",
                       "event_type": "image.upload",
                       "priority": "info",
                       "payload": IMAGE_META,
                       "timestamp": NOW}


NOTIFICATION_DELETE = {"message_id": "0c65cb9c-018c-11e2-bc91-5453ed1bbb5f",
                       "publisher_id": "images.example.com",
                       "event_type": "image.delete",
                       "priority": "info",
                       "payload": IMAGE_META,
                       "timestamp": NOW}


class TestNotification(test.BaseTestCase):

    def _verify_common_counter(self, c, name, volume):
        self.assertIsNotNone(c)
        self.assertEqual(c.name, name)
        self.assertEqual(fake_uuid('c'), c.resource_id)
        self.assertEqual(NOW, c.timestamp)
        self.assertEqual(volume, c.volume)
        metadata = c.resource_metadata
        self.assertEqual(u'images.example.com', metadata.get('host'))

    def test_image_download(self):
        handler = notifications.ImageDownload(mock.Mock())
        counters = list(handler.process_notification(NOTIFICATION_SEND))
        self.assertEqual(1, len(counters))
        download = counters[0]
        self._verify_common_counter(download, 'image.download', 42)
        self.assertEqual(fake_uuid('d'), download.user_id)
        self.assertEqual(fake_uuid('b'), download.project_id)
        self.assertEqual(sample.TYPE_DELTA, download.type)

    def test_image_serve(self):
        handler = notifications.ImageServe(mock.Mock())
        counters = list(handler.process_notification(NOTIFICATION_SEND))
        self.assertEqual(1, len(counters))
        serve = counters[0]
        self._verify_common_counter(serve, 'image.serve', 42)
        self.assertEqual(fake_uuid('e'), serve.project_id)
        self.assertEqual(fake_uuid('d'),
                         serve.resource_metadata.get('receiver_user_id'))
        self.assertEqual(fake_uuid('b'),
                         serve.resource_metadata.get('receiver_tenant_id'))
        self.assertEqual(sample.TYPE_DELTA, serve.type)

    def test_image_crud_on_update(self):
        handler = notifications.ImageCRUD(mock.Mock())
        counters = list(handler.process_notification(NOTIFICATION_UPDATE))
        self.assertEqual(1, len(counters))
        update = counters[0]
        self._verify_common_counter(update, 'image.update', 1)
        self.assertEqual(sample.TYPE_DELTA, update.type)

    def test_image_on_update(self):
        handler = notifications.Image(mock.Mock())
        counters = list(handler.process_notification(NOTIFICATION_UPDATE))
        self.assertEqual(1, len(counters))
        update = counters[0]
        self._verify_common_counter(update, 'image', 1)
        self.assertEqual(sample.TYPE_GAUGE, update.type)

    def test_image_size_on_update(self):
        handler = notifications.ImageSize(mock.Mock())
        counters = list(handler.process_notification(NOTIFICATION_UPDATE))
        self.assertEqual(1, len(counters))
        update = counters[0]
        self._verify_common_counter(update, 'image.size',
                                    IMAGE_META['size'])
        self.assertEqual(sample.TYPE_GAUGE, update.type)

    def test_image_crud_on_upload(self):
        handler = notifications.ImageCRUD(mock.Mock())
        counters = list(handler.process_notification(NOTIFICATION_UPLOAD))
        self.assertEqual(1, len(counters))
        upload = counters[0]
        self._verify_common_counter(upload, 'image.upload', 1)
        self.assertEqual(sample.TYPE_DELTA, upload.type)

    def test_image_on_upload(self):
        handler = notifications.Image(mock.Mock())
        counters = list(handler.process_notification(NOTIFICATION_UPLOAD))
        self.assertEqual(1, len(counters))
        upload = counters[0]
        self._verify_common_counter(upload, 'image', 1)
        self.assertEqual(sample.TYPE_GAUGE, upload.type)

    def test_image_size_on_upload(self):
        handler = notifications.ImageSize(mock.Mock())
        counters = list(handler.process_notification(NOTIFICATION_UPLOAD))
        self.assertEqual(1, len(counters))
        upload = counters[0]
        self._verify_common_counter(upload, 'image.size',
                                    IMAGE_META['size'])
        self.assertEqual(sample.TYPE_GAUGE, upload.type)

    def test_image_crud_on_delete(self):
        handler = notifications.ImageCRUD(mock.Mock())
        counters = list(handler.process_notification(NOTIFICATION_DELETE))
        self.assertEqual(1, len(counters))
        delete = counters[0]
        self._verify_common_counter(delete, 'image.delete', 1)
        self.assertEqual(sample.TYPE_DELTA, delete.type)

    def test_image_on_delete(self):
        handler = notifications.Image(mock.Mock())
        counters = list(handler.process_notification(NOTIFICATION_DELETE))
        self.assertEqual(1, len(counters))
        delete = counters[0]
        self._verify_common_counter(delete, 'image', 1)
        self.assertEqual(sample.TYPE_GAUGE, delete.type)

    def test_image_size_on_delete(self):
        handler = notifications.ImageSize(mock.Mock())
        counters = list(handler.process_notification(NOTIFICATION_DELETE))
        self.assertEqual(1, len(counters))
        delete = counters[0]
        self._verify_common_counter(delete, 'image.size',
                                    IMAGE_META['size'])
        self.assertEqual(sample.TYPE_GAUGE, delete.type)

########NEW FILE########
__FILENAME__ = test_client
# Copyright (C) 2014 eNovance SAS <licensing@enovance.com>
#
# Author: Sylvain Afchain <sylvain.afchain@enovance.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import mock

from ceilometer.network.statistics.opencontrail import client
from ceilometer.openstack.common import test


class TestOpencontrailClient(test.BaseTestCase):

    def setUp(self):
        super(TestOpencontrailClient, self).setUp()
        self.client = client.Client('http://127.0.0.1:8143',
                                    'admin', 'admin', None, False)

        self.post_resp = mock.MagicMock()
        self.post = mock.patch('requests.post',
                               return_value=self.post_resp).start()

        self.post_resp.raw.version = 1.1
        self.post_resp.status_code = 302
        self.post_resp.reason = 'Moved'
        self.post_resp.headers = {}
        self.post_resp.cookies = {'connect.sid': 'aaa'}
        self.post_resp.content = 'dummy'

        self.get_resp = mock.MagicMock()
        self.get = mock.patch('requests.get',
                              return_value=self.get_resp).start()
        self.get_resp.raw_version = 1.1
        self.get_resp.status_code = 200
        self.post_resp.content = 'dqs'

    def test_port_statistics(self):
        uuid = 'bbb'
        self.client.networks.get_port_statistics(uuid)

        call_args = self.post.call_args_list[0][0]
        call_kwargs = self.post.call_args_list[0][1]

        expected_url = 'http://127.0.0.1:8143/authenticate'
        self.assertEqual(expected_url, call_args[0])

        data = call_kwargs.get('data')
        expected_data = {'domain': None, 'password': 'admin',
                         'username': 'admin'}
        self.assertEqual(expected_data, data)

        call_args = self.get.call_args_list[0][0]
        call_kwargs = self.get.call_args_list[0][1]

        expected_url = ('http://127.0.0.1:8143/api/tenant/'
                        'networking/virtual-machines/details')
        self.assertEqual(expected_url, call_args[0])

        data = call_kwargs.get('data')
        cookies = call_kwargs.get('cookies')

        expected_data = {'fqnUUID': 'bbb', 'type': 'vn'}
        expected_cookies = {'connect.sid': 'aaa'}
        self.assertEqual(expected_data, data)
        self.assertEqual(expected_cookies, cookies)

########NEW FILE########
__FILENAME__ = test_driver
# Copyright (C) 2014 eNovance SAS <licensing@enovance.com>
#
# Author: Sylvain Afchain <sylvain.afchain@enovance.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import mock
from six.moves.urllib import parse as url_parse

from ceilometer.network.statistics.opencontrail import driver
from ceilometer.openstack.common import test


class TestOpencontrailDriver(test.BaseTestCase):

    def setUp(self):
        super(TestOpencontrailDriver, self).setUp()

        self.nc_ports = mock.patch('ceilometer.neutron_client'
                                   '.Client.port_get_all',
                                   return_value=self.fake_ports())
        self.nc_ports.start()

        self.nc_networks = mock.patch('ceilometer.neutron_client'
                                      '.Client.network_get_all',
                                      return_value=self.fake_networks())
        self.nc_networks.start()

        self.driver = driver.OpencontrailDriver()
        self.parse_url = url_parse.ParseResult('opencontrail',
                                               '127.0.0.1:8143',
                                               '/', None, None, None)
        self.params = {'password': ['admin'],
                       'scheme': ['http'],
                       'username': ['admin'],
                       'verify_ssl': ['false']}

    @staticmethod
    def fake_ports():
        return [{'admin_state_up': True,
                 'device_owner': 'compute:None',
                 'device_id': '674e553b-8df9-4321-87d9-93ba05b93558',
                 'extra_dhcp_opts': [],
                 'id': '96d49cc3-4e01-40ce-9cac-c0e32642a442',
                 'mac_address': 'fa:16:3e:c5:35:93',
                 'name': '',
                 'network_id': '298a3088-a446-4d5a-bad8-f92ecacd786b',
                 'status': 'ACTIVE',
                 'tenant_id': '89271fa581ab4380bf172f868c3615f9'}]

    @staticmethod
    def fake_networks():
        return [{'admin_state_up': True,
                 'id': '298a3088-a446-4d5a-bad8-f92ecacd786b',
                 'name': 'public',
                 'provider:network_type': 'gre',
                 'provider:physical_network': None,
                 'provider:segmentation_id': 2,
                 'router:external': True,
                 'shared': False,
                 'status': 'ACTIVE',
                 'subnets': [u'c4b6f5b8-3508-4896-b238-a441f25fb492'],
                 'tenant_id': '62d6f08bbd3a44f6ad6f00ca15cce4e5'}]

    @staticmethod
    def fake_port_stats():
        return {"value": [{
            "name": "c588ebb7-ae52-485a-9f0c-b2791c5da196",
            "value": {
                "UveVirtualMachineAgent": {
                    "if_stats_list": [{
                        "out_bytes": 22,
                        "in_bandwidth_usage": 0,
                        "in_bytes": 23,
                        "out_bandwidth_usage": 0,
                        "out_pkts": 5,
                        "in_pkts": 6,
                        "name": ("674e553b-8df9-4321-87d9-93ba05b93558:"
                                 "96d49cc3-4e01-40ce-9cac-c0e32642a442")
                    }]}}}]}

    def _test_meter(self, meter_name, expected):
        with mock.patch('ceilometer.network.'
                        'statistics.opencontrail.'
                        'client.NetworksAPIClient.'
                        'get_port_statistics',
                        return_value=self.fake_port_stats()) as port_stats:

            samples = self.driver.get_sample_data(meter_name, self.parse_url,
                                                  self.params, {})

            self.assertEqual(expected, [s for s in samples])

            net_id = '298a3088-a446-4d5a-bad8-f92ecacd786b'
            port_stats.assert_called_with(net_id)

    def test_switch_port_receive_packets(self):
        expected = [
            (6,
             '96d49cc3-4e01-40ce-9cac-c0e32642a442',
             {'device_owner_id': '674e553b-8df9-4321-87d9-93ba05b93558',
              'network_id': '298a3088-a446-4d5a-bad8-f92ecacd786b',
              'tenant_id': '89271fa581ab4380bf172f868c3615f9'},
             mock.ANY)]
        self._test_meter('switch.port.receive.packets', expected)

    def test_switch_port_transmit_packets(self):
        expected = [
            (5,
             '96d49cc3-4e01-40ce-9cac-c0e32642a442',
             {'device_owner_id': '674e553b-8df9-4321-87d9-93ba05b93558',
              'network_id': '298a3088-a446-4d5a-bad8-f92ecacd786b',
              'tenant_id': '89271fa581ab4380bf172f868c3615f9'},
             mock.ANY)]
        self._test_meter('switch.port.transmit.packets', expected)

    def test_switch_port_receive_bytes(self):
        expected = [
            (23,
             '96d49cc3-4e01-40ce-9cac-c0e32642a442',
             {'device_owner_id': '674e553b-8df9-4321-87d9-93ba05b93558',
              'network_id': '298a3088-a446-4d5a-bad8-f92ecacd786b',
              'tenant_id': '89271fa581ab4380bf172f868c3615f9'},
             mock.ANY)]
        self._test_meter('switch.port.receive.bytes', expected)

    def test_switch_port_transmit_bytes(self):
        expected = [
            (22,
             '96d49cc3-4e01-40ce-9cac-c0e32642a442',
             {'device_owner_id': '674e553b-8df9-4321-87d9-93ba05b93558',
              'network_id': '298a3088-a446-4d5a-bad8-f92ecacd786b',
              'tenant_id': '89271fa581ab4380bf172f868c3615f9'},
             mock.ANY)]
        self._test_meter('switch.port.transmit.bytes', expected)

########NEW FILE########
__FILENAME__ = test_client
#
# Copyright 2013 NEC Corporation.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import mock

from requests import auth as req_auth
import six
from six.moves.urllib import parse as url_parse

from ceilometer.network.statistics.opendaylight import client
from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import test


class TestClientHTTPBasicAuth(test.BaseTestCase):

    auth_way = 'basic'
    scheme = 'http'

    def setUp(self):
        super(TestClientHTTPBasicAuth, self).setUp()
        self.parsed_url = url_parse.urlparse(
            'http://127.0.0.1:8080/controller/nb/v2?container_name=default&'
            'container_name=egg&auth=%s&user=admin&password=admin_pass&'
            'scheme=%s' % (self.auth_way, self.scheme))
        self.params = url_parse.parse_qs(self.parsed_url.query)
        self.endpoint = url_parse.urlunparse(
            url_parse.ParseResult(self.scheme,
                                  self.parsed_url.netloc,
                                  self.parsed_url.path,
                                  None, None, None))
        odl_params = {}
        odl_params['auth'] = self.params.get('auth')[0]
        odl_params['user'] = self.params.get('user')[0]
        odl_params['password'] = self.params.get('password')[0]
        self.client = client.Client(self.endpoint, odl_params)

        self.resp = mock.MagicMock()
        self.get = mock.patch('requests.get',
                              return_value=self.resp).start()

        self.resp.raw.version = 1.1
        self.resp.status_code = 200
        self.resp.reason = 'OK'
        self.resp.headers = {}
        self.resp.content = 'dummy'

    def _test_request(self, method, url):
        data = method('default')

        call_args = self.get.call_args_list[0][0]
        call_kwargs = self.get.call_args_list[0][1]

        # check url
        real_url = url % {'container_name': 'default',
                          'scheme': self.scheme}
        self.assertEqual(real_url, call_args[0])

        # check auth parameters
        auth = call_kwargs.get('auth')
        if self.auth_way == 'digest':
            self.assertIsInstance(auth, req_auth.HTTPDigestAuth)
        else:
            self.assertIsInstance(auth, req_auth.HTTPBasicAuth)
        self.assertEqual('admin', auth.username)
        self.assertEqual('admin_pass', auth.password)

        # check header
        self.assertEqual(
            {'Accept': 'application/json'},
            call_kwargs['headers'])

        # check return value
        self.assertEqual(self.get().json(), data)

    def test_flow_statistics(self):
        self._test_request(
            self.client.statistics.get_flow_statistics,
            '%(scheme)s://127.0.0.1:8080/controller/nb/v2'
            '/statistics/%(container_name)s/flow')

    def test_port_statistics(self):
        self._test_request(
            self.client.statistics.get_port_statistics,
            '%(scheme)s://127.0.0.1:8080/controller/nb/v2'
            '/statistics/%(container_name)s/port')

    def test_table_statistics(self):
        self._test_request(
            self.client.statistics.get_table_statistics,
            '%(scheme)s://127.0.0.1:8080/controller/nb/v2'
            '/statistics/%(container_name)s/table')

    def test_topology(self):
        self._test_request(
            self.client.topology.get_topology,
            '%(scheme)s://127.0.0.1:8080/controller/nb/v2'
            '/topology/%(container_name)s')

    def test_user_links(self):
        self._test_request(
            self.client.topology.get_user_links,
            '%(scheme)s://127.0.0.1:8080/controller/nb/v2'
            '/topology/%(container_name)s/userLinks')

    def test_switch(self):
        self._test_request(
            self.client.switch_manager.get_nodes,
            '%(scheme)s://127.0.0.1:8080/controller/nb/v2'
            '/switchmanager/%(container_name)s/nodes')

    def test_active_hosts(self):
        self._test_request(
            self.client.host_tracker.get_active_hosts,
            '%(scheme)s://127.0.0.1:8080/controller/nb/v2'
            '/hosttracker/%(container_name)s/hosts/active')

    def test_inactive_hosts(self):
        self._test_request(
            self.client.host_tracker.get_inactive_hosts,
            '%(scheme)s://127.0.0.1:8080/controller/nb/v2'
            '/hosttracker/%(container_name)s/hosts/inactive')

    def test_http_error(self):
        self.resp.status_code = 404
        self.resp.reason = 'Not Found'

        try:
            self.client.statistics.get_flow_statistics('default')
            self.fail('')
        except client.OpenDaylightRESTAPIFailed as e:
            self.assertEqual(
                _('OpenDaylitght API returned %(status)s %(reason)s') %
                {'status': self.resp.status_code,
                 'reason': self.resp.reason},
                six.text_type(e))

    def test_other_error(self):

        class _Exception(Exception):
            pass

        self.get = mock.patch('requests.get',
                              side_effect=_Exception).start()

        self.assertRaises(_Exception,
                          self.client.statistics.get_flow_statistics,
                          'default')


class TestClientHTTPDigestAuth(TestClientHTTPBasicAuth):

    auth_way = 'digest'


class TestClientHTTPSBasicAuth(TestClientHTTPBasicAuth):

    scheme = 'https'


class TestClientHTTPSDigestAuth(TestClientHTTPDigestAuth):

    scheme = 'https'

########NEW FILE########
__FILENAME__ = test_driver
#
# Copyright 2013 NEC Corporation.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import abc
import mock

import six
from six import moves
from six.moves.urllib import parse as url_parse

from ceilometer.network.statistics.opendaylight import driver
from ceilometer.openstack.common import test


@six.add_metaclass(abc.ABCMeta)
class _Base(test.BaseTestCase):

    @abc.abstractproperty
    def flow_data(self):
        pass

    @abc.abstractproperty
    def port_data(self):
        pass

    @abc.abstractproperty
    def table_data(self):
        pass

    @abc.abstractproperty
    def topology_data(self):
        pass

    @abc.abstractproperty
    def switch_data(self):
        pass

    @abc.abstractproperty
    def user_links_data(self):
        pass

    @abc.abstractproperty
    def active_hosts_data(self):
        pass

    @abc.abstractproperty
    def inactive_hosts_data(self):
        pass

    fake_odl_url = url_parse.ParseResult('opendaylight',
                                         'localhost:8080',
                                         'controller/nb/v2',
                                         None,
                                         None,
                                         None)

    fake_params = url_parse.parse_qs('user=admin&password=admin&scheme=http&'
                                     'container_name=default&auth=basic')

    fake_params_multi_container = \
        url_parse.parse_qs('user=admin&password=admin&scheme=http&'
                           'container_name=first&container_name=second&'
                           'auth=basic')

    def setUp(self):
        super(_Base, self).setUp()
        self.addCleanup(mock.patch.stopall)

        self.driver = driver.OpenDayLightDriver()

        self.get_flow_statistics = mock.patch(
            'ceilometer.network.statistics.opendaylight.client.'
            'StatisticsAPIClient.get_flow_statistics',
            return_value=self.flow_data).start()

        mock.patch('ceilometer.network.statistics.opendaylight.client.'
                   'StatisticsAPIClient.get_table_statistics',
                   return_value=self.table_data).start()

        mock.patch('ceilometer.network.statistics.opendaylight.client.'
                   'StatisticsAPIClient.get_port_statistics',
                   return_value=self.port_data).start()

        mock.patch('ceilometer.network.statistics.opendaylight.client.'
                   'TopologyAPIClient.get_topology',
                   return_value=self.topology_data).start()

        mock.patch('ceilometer.network.statistics.opendaylight.client.'
                   'TopologyAPIClient.get_user_links',
                   return_value=self.user_links_data).start()

        mock.patch('ceilometer.network.statistics.opendaylight.client.'
                   'SwitchManagerAPIClient.get_nodes',
                   return_value=self.switch_data).start()

        mock.patch('ceilometer.network.statistics.opendaylight.client.'
                   'HostTrackerAPIClient.get_active_hosts',
                   return_value=self.active_hosts_data).start()

        mock.patch('ceilometer.network.statistics.opendaylight.client.'
                   'HostTrackerAPIClient.get_inactive_hosts',
                   return_value=self.inactive_hosts_data).start()

    def _test_for_meter(self, meter_name, expected_data):
        sample_data = self.driver.get_sample_data(meter_name,
                                                  self.fake_odl_url,
                                                  self.fake_params,
                                                  {})

        for sample, expected in moves.zip(sample_data, expected_data):
            self.assertEqual(expected[0], sample[0])  # check volume
            self.assertEqual(expected[1], sample[1])  # check resource id
            self.assertEqual(expected[2], sample[2])  # check resource metadata
            self.assertIsNotNone(sample[3])  # timestamp


class TestOpenDayLightDriverSpecial(_Base):

    flow_data = {"flowStatistics": []}
    port_data = {"portStatistics": []}
    table_data = {"tableStatistics": []}
    topology_data = {"edgeProperties": []}
    switch_data = {"nodeProperties": []}
    user_links_data = {"userLinks": []}
    active_hosts_data = {"hostConfig": []}
    inactive_hosts_data = {"hostConfig": []}

    def test_not_implemented_meter(self):
        sample_data = self.driver.get_sample_data('egg',
                                                  self.fake_odl_url,
                                                  self.fake_params,
                                                  {})
        self.assertIsNone(sample_data)

        sample_data = self.driver.get_sample_data('switch.table.egg',
                                                  self.fake_odl_url,
                                                  self.fake_params,
                                                  {})
        self.assertIsNone(sample_data)

    def test_cache(self):
        cache = {}
        self.driver.get_sample_data('switch',
                                    self.fake_odl_url,
                                    self.fake_params,
                                    cache)
        self.driver.get_sample_data('switch',
                                    self.fake_odl_url,
                                    self.fake_params,
                                    cache)
        self.assertEqual(self.get_flow_statistics.call_count, 1)

        cache = {}
        self.driver.get_sample_data('switch',
                                    self.fake_odl_url,
                                    self.fake_params,
                                    cache)
        self.assertEqual(self.get_flow_statistics.call_count, 2)

    def test_multi_container(self):
        cache = {}
        self.driver.get_sample_data('switch',
                                    self.fake_odl_url,
                                    self.fake_params_multi_container,
                                    cache)
        self.assertEqual(self.get_flow_statistics.call_count, 2)

        self.assertIn('network.statistics.opendaylight', cache)

        odl_data = cache['network.statistics.opendaylight']

        self.assertIn('first', odl_data)
        self.assertIn('second', odl_data)

    def test_http_error(self):

        mock.patch('ceilometer.network.statistics.opendaylight.client.'
                   'StatisticsAPIClient.get_flow_statistics',
                   side_effect=Exception()).start()

        sample_data = self.driver.get_sample_data('switch',
                                                  self.fake_odl_url,
                                                  self.fake_params,
                                                  {})

        self.assertEqual(0, len(sample_data))

        mock.patch('ceilometer.network.statistics.opendaylight.client.'
                   'StatisticsAPIClient.get_flow_statistics',
                   side_effect=[Exception(), self.flow_data]).start()
        cache = {}
        self.driver.get_sample_data('switch',
                                    self.fake_odl_url,
                                    self.fake_params_multi_container,
                                    cache)

        self.assertIn('network.statistics.opendaylight', cache)

        odl_data = cache['network.statistics.opendaylight']

        self.assertIn('second', odl_data)


class TestOpenDayLightDriverSimple(_Base):

    flow_data = {
        "flowStatistics": [
            {
                "node": {
                    "id": "00:00:00:00:00:00:00:02",
                    "type": "OF"
                },
                "flowStatistic": [
                    {
                        "flow": {
                            "match": {
                                "matchField": [
                                    {
                                        "type": "DL_TYPE",
                                        "value": "2048"
                                    },
                                    {
                                        "mask": "255.255.255.255",
                                        "type": "NW_DST",
                                        "value": "1.1.1.1"
                                    }
                                ]
                            },
                            "actions": {
                                "@type": "output",
                                "port": {
                                    "id": "3",
                                    "node": {
                                        "id": "00:00:00:00:00:00:00:02",
                                        "type": "OF"
                                    },
                                    "type": "OF"
                                }
                            },
                            "hardTimeout": "0",
                            "id": "0",
                            "idleTimeout": "0",
                            "priority": "1"
                        },
                        "byteCount": "0",
                        "durationNanoseconds": "397000000",
                        "durationSeconds": "1828",
                        "packetCount": "0",
                        "tableId": "0"
                    },
                ]
            }
        ]
    }
    port_data = {
        "portStatistics": [
            {
                "node": {
                    "id": "00:00:00:00:00:00:00:02",
                    "type": "OF"
                },
                "portStatistic": [
                    {
                        "nodeConnector": {
                            "id": "4",
                            "node": {
                                "id": "00:00:00:00:00:00:00:02",
                                "type": "OF"
                            },
                            "type": "OF"
                        },
                        "collisionCount": "0",
                        "receiveBytes": "0",
                        "receiveCrcError": "0",
                        "receiveDrops": "0",
                        "receiveErrors": "0",
                        "receiveFrameError": "0",
                        "receiveOverRunError": "0",
                        "receivePackets": "0",
                        "transmitBytes": "0",
                        "transmitDrops": "0",
                        "transmitErrors": "0",
                        "transmitPackets": "0"
                    },
                ]
            }
        ]
    }
    table_data = {
        "tableStatistics": [
            {
                "node": {
                    "id": "00:00:00:00:00:00:00:02",
                    "type": "OF"
                },
                "tableStatistic": [
                    {
                        "activeCount": "11",
                        "lookupCount": "816",
                        "matchedCount": "220",
                        "nodeTable": {
                            "id": "0",
                            "node": {
                                "id": "00:00:00:00:00:00:00:02",
                                "type": "OF"
                            }
                        }
                    },
                ]
            }
        ]
    }
    topology_data = {"edgeProperties": []}
    switch_data = {
        "nodeProperties": [
            {
                "node": {
                    "id": "00:00:00:00:00:00:00:02",
                    "type": "OF"
                },
                "properties": {
                    "actions": {
                        "value": "4095"
                    },
                    "timeStamp": {
                        "name": "connectedSince",
                        "value": "1377291227877"
                    }
                }
            },
        ]
    }
    user_links_data = {"userLinks": []}
    active_hosts_data = {"hostConfig": []}
    inactive_hosts_data = {"hostConfig": []}

    def test_meter_switch(self):
        expected_data = [
            (1, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                "properties_actions": "4095",
                "properties_timeStamp_connectedSince": "1377291227877"
            }),
        ]

        self._test_for_meter('switch', expected_data)

    def test_meter_switch_port(self):
        expected_data = [
            (1, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '4',
            }),
        ]
        self._test_for_meter('switch.port', expected_data)

    def test_meter_switch_port_receive_packets(self):
        expected_data = [
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '4'}),
        ]
        self._test_for_meter('switch.port.receive.packets', expected_data)

    def test_meter_switch_port_transmit_packets(self):
        expected_data = [
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '4'}),
        ]
        self._test_for_meter('switch.port.transmit.packets', expected_data)

    def test_meter_switch_port_receive_bytes(self):
        expected_data = [
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '4'}),
        ]
        self._test_for_meter('switch.port.receive.bytes', expected_data)

    def test_meter_switch_port_transmit_bytes(self):
        expected_data = [
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '4'}),
        ]
        self._test_for_meter('switch.port.transmit.bytes', expected_data)

    def test_meter_switch_port_receive_drops(self):
        expected_data = [
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '4'}),
        ]
        self._test_for_meter('switch.port.receive.drops', expected_data)

    def test_meter_switch_port_transmit_drops(self):
        expected_data = [
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '4'}),
        ]
        self._test_for_meter('switch.port.transmit.drops', expected_data)

    def test_meter_switch_port_receive_errors(self):
        expected_data = [
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '4'}),
        ]
        self._test_for_meter('switch.port.receive.errors', expected_data)

    def test_meter_switch_port_transmit_errors(self):
        expected_data = [
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '4'}),
        ]
        self._test_for_meter('switch.port.transmit.errors', expected_data)

    def test_meter_switch_port_receive_frame_error(self):
        expected_data = [
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '4'}),
        ]
        self._test_for_meter('switch.port.receive.frame_error', expected_data)

    def test_meter_switch_port_receive_overrun_error(self):
        expected_data = [
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '4'}),
        ]
        self._test_for_meter('switch.port.receive.overrun_error',
                             expected_data)

    def test_meter_switch_port_receive_crc_error(self):
        expected_data = [
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '4'}),
        ]
        self._test_for_meter('switch.port.receive.crc_error', expected_data)

    def test_meter_switch_port_collision_count(self):
        expected_data = [
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '4'}),
        ]
        self._test_for_meter('switch.port.collision.count', expected_data)

    def test_meter_switch_table(self):
        expected_data = [
            (1, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'table_id': '0'}),
        ]
        self._test_for_meter('switch.table', expected_data)

    def test_meter_switch_table_active_entries(self):
        expected_data = [
            (11, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'table_id': '0'}),
        ]
        self._test_for_meter('switch.table.active.entries', expected_data)

    def test_meter_switch_table_lookup_packets(self):
        expected_data = [
            (816, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'table_id': '0'}),
        ]
        self._test_for_meter('switch.table.lookup.packets', expected_data)

    def test_meter_switch_table_matched_packets(self):
        expected_data = [
            (220, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'table_id': '0'}),
        ]
        self._test_for_meter('switch.table.matched.packets', expected_data)

    def test_meter_switch_flow(self):
        expected_data = [
            (1, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'table_id': '0',
                'flow_id': '0',
                "flow_match_matchField[0]_type": "DL_TYPE",
                "flow_match_matchField[0]_value": "2048",
                "flow_match_matchField[1]_mask": "255.255.255.255",
                "flow_match_matchField[1]_type": "NW_DST",
                "flow_match_matchField[1]_value": "1.1.1.1",
                "flow_actions_@type": "output",
                "flow_actions_port_id": "3",
                "flow_actions_port_node_id": "00:00:00:00:00:00:00:02",
                "flow_actions_port_node_type": "OF",
                "flow_actions_port_type": "OF",
                "flow_hardTimeout": "0",
                "flow_idleTimeout": "0",
                "flow_priority": "1"
            }),
        ]
        self._test_for_meter('switch.flow', expected_data)

    def test_meter_switch_flow_duration_seconds(self):
        expected_data = [
            (1828, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'table_id': '0',
                'flow_id': '0',
                "flow_match_matchField[0]_type": "DL_TYPE",
                "flow_match_matchField[0]_value": "2048",
                "flow_match_matchField[1]_mask": "255.255.255.255",
                "flow_match_matchField[1]_type": "NW_DST",
                "flow_match_matchField[1]_value": "1.1.1.1",
                "flow_actions_@type": "output",
                "flow_actions_port_id": "3",
                "flow_actions_port_node_id": "00:00:00:00:00:00:00:02",
                "flow_actions_port_node_type": "OF",
                "flow_actions_port_type": "OF",
                "flow_hardTimeout": "0",
                "flow_idleTimeout": "0",
                "flow_priority": "1"}),
        ]
        self._test_for_meter('switch.flow.duration_seconds', expected_data)

    def test_meter_switch_flow_duration_nanoseconds(self):
        expected_data = [
            (397000000, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'table_id': '0',
                'flow_id': '0',
                "flow_match_matchField[0]_type": "DL_TYPE",
                "flow_match_matchField[0]_value": "2048",
                "flow_match_matchField[1]_mask": "255.255.255.255",
                "flow_match_matchField[1]_type": "NW_DST",
                "flow_match_matchField[1]_value": "1.1.1.1",
                "flow_actions_@type": "output",
                "flow_actions_port_id": "3",
                "flow_actions_port_node_id": "00:00:00:00:00:00:00:02",
                "flow_actions_port_node_type": "OF",
                "flow_actions_port_type": "OF",
                "flow_hardTimeout": "0",
                "flow_idleTimeout": "0",
                "flow_priority": "1"}),
        ]
        self._test_for_meter('switch.flow.duration_nanoseconds', expected_data)

    def test_meter_switch_flow_packets(self):
        expected_data = [
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'table_id': '0',
                'flow_id': '0',
                "flow_match_matchField[0]_type": "DL_TYPE",
                "flow_match_matchField[0]_value": "2048",
                "flow_match_matchField[1]_mask": "255.255.255.255",
                "flow_match_matchField[1]_type": "NW_DST",
                "flow_match_matchField[1]_value": "1.1.1.1",
                "flow_actions_@type": "output",
                "flow_actions_port_id": "3",
                "flow_actions_port_node_id": "00:00:00:00:00:00:00:02",
                "flow_actions_port_node_type": "OF",
                "flow_actions_port_type": "OF",
                "flow_hardTimeout": "0",
                "flow_idleTimeout": "0",
                "flow_priority": "1"}),
        ]
        self._test_for_meter('switch.flow.packets', expected_data)

    def test_meter_switch_flow_bytes(self):
        expected_data = [
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'table_id': '0',
                'flow_id': '0',
                "flow_match_matchField[0]_type": "DL_TYPE",
                "flow_match_matchField[0]_value": "2048",
                "flow_match_matchField[1]_mask": "255.255.255.255",
                "flow_match_matchField[1]_type": "NW_DST",
                "flow_match_matchField[1]_value": "1.1.1.1",
                "flow_actions_@type": "output",
                "flow_actions_port_id": "3",
                "flow_actions_port_node_id": "00:00:00:00:00:00:00:02",
                "flow_actions_port_node_type": "OF",
                "flow_actions_port_type": "OF",
                "flow_hardTimeout": "0",
                "flow_idleTimeout": "0",
                "flow_priority": "1"}),
        ]
        self._test_for_meter('switch.flow.bytes', expected_data)


class TestOpenDayLightDriverComplex(_Base):

    flow_data = {
        "flowStatistics": [
            {
                "node": {
                    "id": "00:00:00:00:00:00:00:02",
                    "type": "OF"
                },
                "flowStatistic": [
                    {
                        "flow": {
                            "match": {
                                "matchField": [
                                    {
                                        "type": "DL_TYPE",
                                        "value": "2048"
                                    },
                                    {
                                        "mask": "255.255.255.255",
                                        "type": "NW_DST",
                                        "value": "1.1.1.1"
                                    }
                                ]
                            },
                            "actions": {
                                "@type": "output",
                                "port": {
                                    "id": "3",
                                    "node": {
                                        "id": "00:00:00:00:00:00:00:02",
                                        "type": "OF"
                                    },
                                    "type": "OF"
                                }
                            },
                            "hardTimeout": "0",
                            "id": "0",
                            "idleTimeout": "0",
                            "priority": "1"
                        },
                        "byteCount": "0",
                        "durationNanoseconds": "397000000",
                        "durationSeconds": "1828",
                        "packetCount": "0",
                        "tableId": "0"
                    },
                    {
                        "flow": {
                            "match": {
                                "matchField": [
                                    {
                                        "type": "DL_TYPE",
                                        "value": "2048"
                                    },
                                    {
                                        "mask": "255.255.255.255",
                                        "type": "NW_DST",
                                        "value": "1.1.1.2"
                                    }
                                ]
                            },
                            "actions": {
                                "@type": "output",
                                "port": {
                                    "id": "4",
                                    "node": {
                                        "id": "00:00:00:00:00:00:00:03",
                                        "type": "OF"
                                    },
                                    "type": "OF"
                                }
                            },
                            "hardTimeout": "0",
                            "id": "0",
                            "idleTimeout": "0",
                            "priority": "1"
                        },
                        "byteCount": "89",
                        "durationNanoseconds": "200000",
                        "durationSeconds": "5648",
                        "packetCount": "30",
                        "tableId": "1"
                    }
                ]
            }
        ]
    }
    port_data = {
        "portStatistics": [
            {
                "node": {
                    "id": "00:00:00:00:00:00:00:02",
                    "type": "OF"
                },
                "portStatistic": [
                    {
                        "nodeConnector": {
                            "id": "4",
                            "node": {
                                "id": "00:00:00:00:00:00:00:02",
                                "type": "OF"
                            },
                            "type": "OF"
                        },
                        "collisionCount": "0",
                        "receiveBytes": "0",
                        "receiveCrcError": "0",
                        "receiveDrops": "0",
                        "receiveErrors": "0",
                        "receiveFrameError": "0",
                        "receiveOverRunError": "0",
                        "receivePackets": "0",
                        "transmitBytes": "0",
                        "transmitDrops": "0",
                        "transmitErrors": "0",
                        "transmitPackets": "0"
                    },
                    {
                        "nodeConnector": {
                            "id": "3",
                            "node": {
                                "id": "00:00:00:00:00:00:00:02",
                                "type": "OF"
                            },
                            "type": "OF"
                        },
                        "collisionCount": "0",
                        "receiveBytes": "12740",
                        "receiveCrcError": "0",
                        "receiveDrops": "0",
                        "receiveErrors": "0",
                        "receiveFrameError": "0",
                        "receiveOverRunError": "0",
                        "receivePackets": "182",
                        "transmitBytes": "12110",
                        "transmitDrops": "0",
                        "transmitErrors": "0",
                        "transmitPackets": "173"
                    },
                    {
                        "nodeConnector": {
                            "id": "2",
                            "node": {
                                "id": "00:00:00:00:00:00:00:02",
                                "type": "OF"
                            },
                            "type": "OF"
                        },
                        "collisionCount": "0",
                        "receiveBytes": "12180",
                        "receiveCrcError": "0",
                        "receiveDrops": "0",
                        "receiveErrors": "0",
                        "receiveFrameError": "0",
                        "receiveOverRunError": "0",
                        "receivePackets": "174",
                        "transmitBytes": "12670",
                        "transmitDrops": "0",
                        "transmitErrors": "0",
                        "transmitPackets": "181"
                    },
                    {
                        "nodeConnector": {
                            "id": "1",
                            "node": {
                                "id": "00:00:00:00:00:00:00:02",
                                "type": "OF"
                            },
                            "type": "OF"
                        },
                        "collisionCount": "0",
                        "receiveBytes": "0",
                        "receiveCrcError": "0",
                        "receiveDrops": "0",
                        "receiveErrors": "0",
                        "receiveFrameError": "0",
                        "receiveOverRunError": "0",
                        "receivePackets": "0",
                        "transmitBytes": "0",
                        "transmitDrops": "0",
                        "transmitErrors": "0",
                        "transmitPackets": "0"
                    },
                    {
                        "nodeConnector": {
                            "id": "0",
                            "node": {
                                "id": "00:00:00:00:00:00:00:02",
                                "type": "OF"
                            },
                            "type": "OF"
                        },
                        "collisionCount": "0",
                        "receiveBytes": "0",
                        "receiveCrcError": "0",
                        "receiveDrops": "0",
                        "receiveErrors": "0",
                        "receiveFrameError": "0",
                        "receiveOverRunError": "0",
                        "receivePackets": "0",
                        "transmitBytes": "0",
                        "transmitDrops": "0",
                        "transmitErrors": "0",
                        "transmitPackets": "0"
                    }
                ]
            }
        ]
    }
    table_data = {
        "tableStatistics": [
            {
                "node": {
                    "id": "00:00:00:00:00:00:00:02",
                    "type": "OF"
                },
                "tableStatistic": [
                    {
                        "activeCount": "11",
                        "lookupCount": "816",
                        "matchedCount": "220",
                        "nodeTable": {
                            "id": "0",
                            "node": {
                                "id": "00:00:00:00:00:00:00:02",
                                "type": "OF"
                            }
                        }
                    },
                    {
                        "activeCount": "20",
                        "lookupCount": "10",
                        "matchedCount": "5",
                        "nodeTable": {
                            "id": "1",
                            "node": {
                                "id": "00:00:00:00:00:00:00:02",
                                "type": "OF"
                            }
                        }
                    }
                ]
            }
        ]
    }
    topology_data = {
        "edgeProperties": [
            {
                "edge": {
                    "headNodeConnector": {
                        "id": "2",
                        "node": {
                            "id": "00:00:00:00:00:00:00:03",
                            "type": "OF"
                        },
                        "type": "OF"
                    },
                    "tailNodeConnector": {
                        "id": "2",
                        "node": {
                            "id": "00:00:00:00:00:00:00:02",
                            "type": "OF"
                        },
                        "type": "OF"
                    }
                },
                "properties": {
                    "bandwidth": {
                        "value": 10000000000
                    },
                    "config": {
                        "value": 1
                    },
                    "name": {
                        "value": "s2-eth3"
                    },
                    "state": {
                        "value": 1
                    },
                    "timeStamp": {
                        "name": "creation",
                        "value": 1379527162648
                    }
                }
            },
            {
                "edge": {
                    "headNodeConnector": {
                        "id": "5",
                        "node": {
                            "id": "00:00:00:00:00:00:00:02",
                            "type": "OF"
                        },
                        "type": "OF"
                    },
                    "tailNodeConnector": {
                        "id": "2",
                        "node": {
                            "id": "00:00:00:00:00:00:00:04",
                            "type": "OF"
                        },
                        "type": "OF"
                    }
                },
                "properties": {
                    "timeStamp": {
                        "name": "creation",
                        "value": 1379527162648
                    }
                }
            }
        ]
    }
    switch_data = {
        "nodeProperties": [
            {
                "node": {
                    "id": "00:00:00:00:00:00:00:02",
                    "type": "OF"
                },
                "properties": {
                    "actions": {
                        "value": "4095"
                    },
                    "buffers": {
                        "value": "256"
                    },
                    "capabilities": {
                        "value": "199"
                    },
                    "description": {
                        "value": "None"
                    },
                    "macAddress": {
                        "value": "00:00:00:00:00:02"
                    },
                    "tables": {
                        "value": "-1"
                    },
                    "timeStamp": {
                        "name": "connectedSince",
                        "value": "1377291227877"
                    }
                }
            },
            {
                "node": {
                    "id": "00:00:00:00:00:00:00:03",
                    "type": "OF"
                },
                "properties": {
                    "actions": {
                        "value": "1024"
                    },
                    "buffers": {
                        "value": "512"
                    },
                    "capabilities": {
                        "value": "1000"
                    },
                    "description": {
                        "value": "Foo Bar"
                    },
                    "macAddress": {
                        "value": "00:00:00:00:00:03"
                    },
                    "tables": {
                        "value": "10"
                    },
                    "timeStamp": {
                        "name": "connectedSince",
                        "value": "1377291228000"
                    }
                }
            }
        ]
    }
    user_links_data = {
        "userLinks": [
            {
                "dstNodeConnector": "OF|5@OF|00:00:00:00:00:00:00:05",
                "name": "link1",
                "srcNodeConnector": "OF|3@OF|00:00:00:00:00:00:00:02",
                "status": "Success"
            }
        ]
    }
    active_hosts_data = {
        "hostConfig": [
            {
                "dataLayerAddress": "00:00:00:00:01:01",
                "networkAddress": "1.1.1.1",
                "nodeConnectorId": "9",
                "nodeConnectorType": "OF",
                "nodeId": "00:00:00:00:00:00:00:01",
                "nodeType": "OF",
                "staticHost": "false",
                "vlan": "0"
            },
            {
                "dataLayerAddress": "00:00:00:00:02:02",
                "networkAddress": "2.2.2.2",
                "nodeConnectorId": "1",
                "nodeConnectorType": "OF",
                "nodeId": "00:00:00:00:00:00:00:02",
                "nodeType": "OF",
                "staticHost": "true",
                "vlan": "0"
            }
        ]
    }
    inactive_hosts_data = {
        "hostConfig": [
            {
                "dataLayerAddress": "00:00:00:01:01:01",
                "networkAddress": "1.1.1.3",
                "nodeConnectorId": "8",
                "nodeConnectorType": "OF",
                "nodeId": "00:00:00:00:00:00:00:01",
                "nodeType": "OF",
                "staticHost": "false",
                "vlan": "0"
            },
            {
                "dataLayerAddress": "00:00:00:01:02:02",
                "networkAddress": "2.2.2.4",
                "nodeConnectorId": "0",
                "nodeConnectorType": "OF",
                "nodeId": "00:00:00:00:00:00:00:02",
                "nodeType": "OF",
                "staticHost": "false",
                "vlan": "1"
            }
        ]
    }

    def test_meter_switch(self):
        expected_data = [
            (1, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                "properties_actions": "4095",
                "properties_buffers": "256",
                "properties_capabilities": "199",
                "properties_description": "None",
                "properties_macAddress": "00:00:00:00:00:02",
                "properties_tables": "-1",
                "properties_timeStamp_connectedSince": "1377291227877"
            }),
            (1, "00:00:00:00:00:00:00:03", {
                'controller': 'OpenDaylight',
                'container': 'default',
                "properties_actions": "1024",
                "properties_buffers": "512",
                "properties_capabilities": "1000",
                "properties_description": "Foo Bar",
                "properties_macAddress": "00:00:00:00:00:03",
                "properties_tables": "10",
                "properties_timeStamp_connectedSince": "1377291228000"
            }),
        ]

        self._test_for_meter('switch', expected_data)

    def test_meter_switch_port(self):
        expected_data = [
            (1, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '4',
            }),
            (1, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '3',
                'user_link_node_id': '00:00:00:00:00:00:00:05',
                'user_link_node_port': '5',
                'user_link_status': 'Success',
                'user_link_name': 'link1',
            }),
            (1, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '2',
                'topology_node_id': '00:00:00:00:00:00:00:03',
                'topology_node_port': '2',
                "topology_bandwidth": 10000000000,
                "topology_config": 1,
                "topology_name": "s2-eth3",
                "topology_state": 1,
                "topology_timeStamp_creation": 1379527162648
            }),
            (1, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '1',
                'host_status': 'active',
                'host_dataLayerAddress': '00:00:00:00:02:02',
                'host_networkAddress': '2.2.2.2',
                'host_staticHost': 'true',
                'host_vlan': '0',
            }),
            (1, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '0',
                'host_status': 'inactive',
                'host_dataLayerAddress': '00:00:00:01:02:02',
                'host_networkAddress': '2.2.2.4',
                'host_staticHost': 'false',
                'host_vlan': '1',
            }),
        ]
        self._test_for_meter('switch.port', expected_data)

    def test_meter_switch_port_receive_packets(self):
        expected_data = [
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '4'}),
            (182, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '3'}),
            (174, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '2'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '1'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '0'}),
        ]
        self._test_for_meter('switch.port.receive.packets', expected_data)

    def test_meter_switch_port_transmit_packets(self):
        expected_data = [
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '4'}),
            (173, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '3'}),
            (181, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '2'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '1'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '0'}),
        ]
        self._test_for_meter('switch.port.transmit.packets', expected_data)

    def test_meter_switch_port_receive_bytes(self):
        expected_data = [
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '4'}),
            (12740, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '3'}),
            (12180, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '2'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '1'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '0'}),
        ]
        self._test_for_meter('switch.port.receive.bytes', expected_data)

    def test_meter_switch_port_transmit_bytes(self):
        expected_data = [
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '4'}),
            (12110, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '3'}),
            (12670, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '2'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '1'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '0'}),
        ]
        self._test_for_meter('switch.port.transmit.bytes', expected_data)

    def test_meter_switch_port_receive_drops(self):
        expected_data = [
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '4'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '3'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '2'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '1'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '0'}),
        ]
        self._test_for_meter('switch.port.receive.drops', expected_data)

    def test_meter_switch_port_transmit_drops(self):
        expected_data = [
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '4'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '3'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '2'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '1'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '0'}),
        ]
        self._test_for_meter('switch.port.transmit.drops', expected_data)

    def test_meter_switch_port_receive_errors(self):
        expected_data = [
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '4'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '3'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '2'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '1'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '0'}),
        ]
        self._test_for_meter('switch.port.receive.errors', expected_data)

    def test_meter_switch_port_transmit_errors(self):
        expected_data = [
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '4'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '3'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '2'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '1'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '0'}),
        ]
        self._test_for_meter('switch.port.transmit.errors', expected_data)

    def test_meter_switch_port_receive_frame_error(self):
        expected_data = [
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '4'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '3'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '2'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '1'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '0'}),
        ]
        self._test_for_meter('switch.port.receive.frame_error', expected_data)

    def test_meter_switch_port_receive_overrun_error(self):
        expected_data = [
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '4'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '3'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '2'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '1'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '0'}),
        ]
        self._test_for_meter('switch.port.receive.overrun_error',
                             expected_data)

    def test_meter_switch_port_receive_crc_error(self):
        expected_data = [
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '4'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '3'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '2'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '1'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '0'}),
        ]
        self._test_for_meter('switch.port.receive.crc_error', expected_data)

    def test_meter_switch_port_collision_count(self):
        expected_data = [
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '4'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '3'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '2'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '1'}),
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'port': '0'}),
        ]
        self._test_for_meter('switch.port.collision.count', expected_data)

    def test_meter_switch_table(self):
        expected_data = [
            (1, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'table_id': '0'}),
            (1, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'table_id': '1'}),
        ]
        self._test_for_meter('switch.table', expected_data)

    def test_meter_switch_table_active_entries(self):
        expected_data = [
            (11, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'table_id': '0'}),
            (20, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'table_id': '1'}),
        ]
        self._test_for_meter('switch.table.active.entries', expected_data)

    def test_meter_switch_table_lookup_packets(self):
        expected_data = [
            (816, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'table_id': '0'}),
            (10, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'table_id': '1'}),
        ]
        self._test_for_meter('switch.table.lookup.packets', expected_data)

    def test_meter_switch_table_matched_packets(self):
        expected_data = [
            (220, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'table_id': '0'}),
            (5, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'table_id': '1'}),
        ]
        self._test_for_meter('switch.table.matched.packets', expected_data)

    def test_meter_switch_flow(self):
        expected_data = [
            (1, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'table_id': '0',
                'flow_id': '0',
                "flow_match_matchField[0]_type": "DL_TYPE",
                "flow_match_matchField[0]_value": "2048",
                "flow_match_matchField[1]_mask": "255.255.255.255",
                "flow_match_matchField[1]_type": "NW_DST",
                "flow_match_matchField[1]_value": "1.1.1.1",
                "flow_actions_@type": "output",
                "flow_actions_port_id": "3",
                "flow_actions_port_node_id": "00:00:00:00:00:00:00:02",
                "flow_actions_port_node_type": "OF",
                "flow_actions_port_type": "OF",
                "flow_hardTimeout": "0",
                "flow_idleTimeout": "0",
                "flow_priority": "1"
            }),
            (1, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'table_id': '1',
                'flow_id': '0',
                "flow_match_matchField[0]_type": "DL_TYPE",
                "flow_match_matchField[0]_value": "2048",
                "flow_match_matchField[1]_mask": "255.255.255.255",
                "flow_match_matchField[1]_type": "NW_DST",
                "flow_match_matchField[1]_value": "1.1.1.2",
                "flow_actions_@type": "output",
                "flow_actions_port_id": "4",
                "flow_actions_port_node_id": "00:00:00:00:00:00:00:03",
                "flow_actions_port_node_type": "OF",
                "flow_actions_port_type": "OF",
                "flow_hardTimeout": "0",
                "flow_idleTimeout": "0",
                "flow_priority": "1"
            }),
        ]
        self._test_for_meter('switch.flow', expected_data)

    def test_meter_switch_flow_duration_seconds(self):
        expected_data = [
            (1828, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'table_id': '0',
                'flow_id': '0',
                "flow_match_matchField[0]_type": "DL_TYPE",
                "flow_match_matchField[0]_value": "2048",
                "flow_match_matchField[1]_mask": "255.255.255.255",
                "flow_match_matchField[1]_type": "NW_DST",
                "flow_match_matchField[1]_value": "1.1.1.1",
                "flow_actions_@type": "output",
                "flow_actions_port_id": "3",
                "flow_actions_port_node_id": "00:00:00:00:00:00:00:02",
                "flow_actions_port_node_type": "OF",
                "flow_actions_port_type": "OF",
                "flow_hardTimeout": "0",
                "flow_idleTimeout": "0",
                "flow_priority": "1"}),
            (5648, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'table_id': '1',
                'flow_id': '0',
                "flow_match_matchField[0]_type": "DL_TYPE",
                "flow_match_matchField[0]_value": "2048",
                "flow_match_matchField[1]_mask": "255.255.255.255",
                "flow_match_matchField[1]_type": "NW_DST",
                "flow_match_matchField[1]_value": "1.1.1.2",
                "flow_actions_@type": "output",
                "flow_actions_port_id": "4",
                "flow_actions_port_node_id": "00:00:00:00:00:00:00:03",
                "flow_actions_port_node_type": "OF",
                "flow_actions_port_type": "OF",
                "flow_hardTimeout": "0",
                "flow_idleTimeout": "0",
                "flow_priority": "1"}),
        ]
        self._test_for_meter('switch.flow.duration_seconds', expected_data)

    def test_meter_switch_flow_duration_nanoseconds(self):
        expected_data = [
            (397000000, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'table_id': '0',
                'flow_id': '0',
                "flow_match_matchField[0]_type": "DL_TYPE",
                "flow_match_matchField[0]_value": "2048",
                "flow_match_matchField[1]_mask": "255.255.255.255",
                "flow_match_matchField[1]_type": "NW_DST",
                "flow_match_matchField[1]_value": "1.1.1.1",
                "flow_actions_@type": "output",
                "flow_actions_port_id": "3",
                "flow_actions_port_node_id": "00:00:00:00:00:00:00:02",
                "flow_actions_port_node_type": "OF",
                "flow_actions_port_type": "OF",
                "flow_hardTimeout": "0",
                "flow_idleTimeout": "0",
                "flow_priority": "1"}),
            (200000, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'table_id': '1',
                'flow_id': '0',
                "flow_match_matchField[0]_type": "DL_TYPE",
                "flow_match_matchField[0]_value": "2048",
                "flow_match_matchField[1]_mask": "255.255.255.255",
                "flow_match_matchField[1]_type": "NW_DST",
                "flow_match_matchField[1]_value": "1.1.1.2",
                "flow_actions_@type": "output",
                "flow_actions_port_id": "4",
                "flow_actions_port_node_id": "00:00:00:00:00:00:00:03",
                "flow_actions_port_node_type": "OF",
                "flow_actions_port_type": "OF",
                "flow_hardTimeout": "0",
                "flow_idleTimeout": "0",
                "flow_priority": "1"}),
        ]
        self._test_for_meter('switch.flow.duration_nanoseconds', expected_data)

    def test_meter_switch_flow_packets(self):
        expected_data = [
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'table_id': '0',
                'flow_id': '0',
                "flow_match_matchField[0]_type": "DL_TYPE",
                "flow_match_matchField[0]_value": "2048",
                "flow_match_matchField[1]_mask": "255.255.255.255",
                "flow_match_matchField[1]_type": "NW_DST",
                "flow_match_matchField[1]_value": "1.1.1.1",
                "flow_actions_@type": "output",
                "flow_actions_port_id": "3",
                "flow_actions_port_node_id": "00:00:00:00:00:00:00:02",
                "flow_actions_port_node_type": "OF",
                "flow_actions_port_type": "OF",
                "flow_hardTimeout": "0",
                "flow_idleTimeout": "0",
                "flow_priority": "1"}),
            (30, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'table_id': '1',
                'flow_id': '0',
                "flow_match_matchField[0]_type": "DL_TYPE",
                "flow_match_matchField[0]_value": "2048",
                "flow_match_matchField[1]_mask": "255.255.255.255",
                "flow_match_matchField[1]_type": "NW_DST",
                "flow_match_matchField[1]_value": "1.1.1.2",
                "flow_actions_@type": "output",
                "flow_actions_port_id": "4",
                "flow_actions_port_node_id": "00:00:00:00:00:00:00:03",
                "flow_actions_port_node_type": "OF",
                "flow_actions_port_type": "OF",
                "flow_hardTimeout": "0",
                "flow_idleTimeout": "0",
                "flow_priority": "1"}),
        ]
        self._test_for_meter('switch.flow.packets', expected_data)

    def test_meter_switch_flow_bytes(self):
        expected_data = [
            (0, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'table_id': '0',
                'flow_id': '0',
                "flow_match_matchField[0]_type": "DL_TYPE",
                "flow_match_matchField[0]_value": "2048",
                "flow_match_matchField[1]_mask": "255.255.255.255",
                "flow_match_matchField[1]_type": "NW_DST",
                "flow_match_matchField[1]_value": "1.1.1.1",
                "flow_actions_@type": "output",
                "flow_actions_port_id": "3",
                "flow_actions_port_node_id": "00:00:00:00:00:00:00:02",
                "flow_actions_port_node_type": "OF",
                "flow_actions_port_type": "OF",
                "flow_hardTimeout": "0",
                "flow_idleTimeout": "0",
                "flow_priority": "1"}),
            (89, "00:00:00:00:00:00:00:02", {
                'controller': 'OpenDaylight',
                'container': 'default',
                'table_id': '1',
                'flow_id': '0',
                "flow_match_matchField[0]_type": "DL_TYPE",
                "flow_match_matchField[0]_value": "2048",
                "flow_match_matchField[1]_mask": "255.255.255.255",
                "flow_match_matchField[1]_type": "NW_DST",
                "flow_match_matchField[1]_value": "1.1.1.2",
                "flow_actions_@type": "output",
                "flow_actions_port_id": "4",
                "flow_actions_port_node_id": "00:00:00:00:00:00:00:03",
                "flow_actions_port_node_type": "OF",
                "flow_actions_port_type": "OF",
                "flow_hardTimeout": "0",
                "flow_idleTimeout": "0",
                "flow_priority": "1"}),
        ]
        self._test_for_meter('switch.flow.bytes', expected_data)

########NEW FILE########
__FILENAME__ = test_driver
#
# Copyright 2014 NEC Corporation.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ceilometer.network.statistics import driver
from ceilometer.openstack.common import test


class TestDriver(test.BaseTestCase):

    def test_driver_ok(self):

        class OkDriver(driver.Driver):

            def get_sample_data(self, meter_name, resources, cache):
                pass

        OkDriver()

    def test_driver_ng(self):

        class NgDriver(driver.Driver):
            """get_sample_data method is lost."""

        self.assertRaises(TypeError, NgDriver)

########NEW FILE########
__FILENAME__ = test_flow
#
# Copyright 2014 NEC Corporation.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ceilometer.network.statistics import flow
from ceilometer import sample
from ceilometer.tests.network import statistics


class TestFlowPollsters(statistics._PollsterTestBase):

    def test_flow_pollster(self):
        self._test_pollster(
            flow.FlowPollster,
            'switch.flow',
            sample.TYPE_GAUGE,
            'flow')

    def test_flow_pollster_duration_seconds(self):
        self._test_pollster(
            flow.FlowPollsterDurationSeconds,
            'switch.flow.duration_seconds',
            sample.TYPE_GAUGE,
            's')

    def test_flow_pollster_duration_nanoseconds(self):
        self._test_pollster(
            flow.FlowPollsterDurationNanoseconds,
            'switch.flow.duration_nanoseconds',
            sample.TYPE_GAUGE,
            'ns')

    def test_flow_pollster_packets(self):
        self._test_pollster(
            flow.FlowPollsterPackets,
            'switch.flow.packets',
            sample.TYPE_CUMULATIVE,
            'packet')

    def test_flow_pollster_bytes(self):
        self._test_pollster(
            flow.FlowPollsterBytes,
            'switch.flow.bytes',
            sample.TYPE_CUMULATIVE,
            'B')

########NEW FILE########
__FILENAME__ = test_port
#
# Copyright 2014 NEC Corporation.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ceilometer.network.statistics import port
from ceilometer import sample
from ceilometer.tests.network import statistics


class TestPortPollsters(statistics._PollsterTestBase):

    def test_port_pollster(self):
        self._test_pollster(
            port.PortPollster,
            'switch.port',
            sample.TYPE_GAUGE,
            'port')

    def test_port_pollster_receive_packets(self):
        self._test_pollster(
            port.PortPollsterReceivePackets,
            'switch.port.receive.packets',
            sample.TYPE_CUMULATIVE,
            'packet')

    def test_port_pollster_transmit_packets(self):
        self._test_pollster(
            port.PortPollsterTransmitPackets,
            'switch.port.transmit.packets',
            sample.TYPE_CUMULATIVE,
            'packet')

    def test_port_pollster_receive_bytes(self):
        self._test_pollster(
            port.PortPollsterReceiveBytes,
            'switch.port.receive.bytes',
            sample.TYPE_CUMULATIVE,
            'B')

    def test_port_pollster_transmit_bytes(self):
        self._test_pollster(
            port.PortPollsterTransmitBytes,
            'switch.port.transmit.bytes',
            sample.TYPE_CUMULATIVE,
            'B')

    def test_port_pollster_receive_drops(self):
        self._test_pollster(
            port.PortPollsterReceiveDrops,
            'switch.port.receive.drops',
            sample.TYPE_CUMULATIVE,
            'packet')

    def test_port_pollster_transmit_drops(self):
        self._test_pollster(
            port.PortPollsterTransmitDrops,
            'switch.port.transmit.drops',
            sample.TYPE_CUMULATIVE,
            'packet')

    def test_port_pollster_receive_errors(self):
        self._test_pollster(
            port.PortPollsterReceiveErrors,
            'switch.port.receive.errors',
            sample.TYPE_CUMULATIVE,
            'packet')

    def test_port_pollster_transmit_errors(self):
        self._test_pollster(
            port.PortPollsterTransmitErrors,
            'switch.port.transmit.errors',
            sample.TYPE_CUMULATIVE,
            'packet')

    def test_port_pollster_receive_frame_errors(self):
        self._test_pollster(
            port.PortPollsterReceiveFrameErrors,
            'switch.port.receive.frame_error',
            sample.TYPE_CUMULATIVE,
            'packet')

    def test_port_pollster_receive_overrun_errors(self):
        self._test_pollster(
            port.PortPollsterReceiveOverrunErrors,
            'switch.port.receive.overrun_error',
            sample.TYPE_CUMULATIVE,
            'packet')

    def test_port_pollster_receive_crc_errors(self):
        self._test_pollster(
            port.PortPollsterReceiveCRCErrors,
            'switch.port.receive.crc_error',
            sample.TYPE_CUMULATIVE,
            'packet')

    def test_port_pollster_collision_count(self):
        self._test_pollster(
            port.PortPollsterCollisionCount,
            'switch.port.collision.count',
            sample.TYPE_CUMULATIVE,
            'packet')

########NEW FILE########
__FILENAME__ = test_statistics
#
# Copyright 2014 NEC Corporation.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ceilometer.network import statistics
from ceilometer.network.statistics import driver
from ceilometer.openstack.common import test
from ceilometer.openstack.common import timeutils
from ceilometer import sample


class TestBase(test.BaseTestCase):

    def test_subclass_ok(self):

        class OkSubclass(statistics._Base):

            meter_name = 'foo'
            meter_type = sample.TYPE_GAUGE
            meter_unit = 'B'

        OkSubclass()

    def test_subclass_ng(self):

        class NgSubclass1(statistics._Base):
            '''meter_name is lost.'''

            meter_type = sample.TYPE_GAUGE
            meter_unit = 'B'

        class NgSubclass2(statistics._Base):
            '''meter_type is lost.'''

            meter_name = 'foo'
            meter_unit = 'B'

        class NgSubclass3(statistics._Base):
            '''meter_unit is lost.'''

            meter_name = 'foo'
            meter_type = sample.TYPE_GAUGE

        self.assertRaises(TypeError, NgSubclass1)
        self.assertRaises(TypeError, NgSubclass2)
        self.assertRaises(TypeError, NgSubclass3)


class TestBaseGetSamples(test.BaseTestCase):

    def setUp(self):
        super(TestBaseGetSamples, self).setUp()

        class FakePollster(statistics._Base):
            meter_name = 'foo'
            meter_type = sample.TYPE_CUMULATIVE
            meter_unit = 'bar'

        self.pollster = FakePollster()

    def tearDown(self):
        statistics._Base.drivers = {}
        super(TestBaseGetSamples, self).tearDown()

    def _setup_ext_mgr(self, **drivers):
        statistics._Base.drivers = drivers

    def _make_fake_driver(self, *return_values):
        class FakeDriver(driver.Driver):

            def __init__(self):
                self.index = 0

            def get_sample_data(self, meter_name, parse_url, params, cache):
                if self.index >= len(return_values):
                    yield None
                retval = return_values[self.index]
                self.index += 1
                yield retval
        return FakeDriver

    def _make_timestamps(self, count):
        return [timeutils.isotime() for i in range(count)]

    def _get_samples(self, *resources):

        return [v for v in self.pollster.get_samples(self, {}, resources)]

    def _assert_sample(self, s, volume, resource_id, resource_metadata,
                       timestamp):
            self.assertEqual(s.name, 'foo')
            self.assertEqual(s.type, sample.TYPE_CUMULATIVE)
            self.assertEqual(s.unit, 'bar')
            self.assertEqual(s.volume, volume)
            self.assertIsNone(s.user_id)
            self.assertIsNone(s.project_id)
            self.assertEqual(s.resource_id, resource_id)
            self.assertEqual(s.timestamp, timestamp)
            self.assertEqual(s.resource_metadata, resource_metadata)

    def test_get_samples_one_driver_one_resource(self):
        times = self._make_timestamps(2)
        fake_driver = self._make_fake_driver((1, 'a', {'spam': 'egg'},
                                              times[0]),
                                             (2, 'b', None, times[1]))

        self._setup_ext_mgr(http=fake_driver())

        samples = self._get_samples('http://foo')

        self.assertEqual(len(samples), 1)
        self._assert_sample(samples[0], 1, 'a', {'spam': 'egg'}, times[0])

    def test_get_samples_one_driver_two_resource(self):
        times = self._make_timestamps(3)
        fake_driver = self._make_fake_driver((1, 'a', {'spam': 'egg'},
                                              times[0]),
                                             (2, 'b', None, times[1]),
                                             (3, 'c', None, times[2]))

        self._setup_ext_mgr(http=fake_driver())

        samples = self._get_samples('http://foo', 'http://bar')

        self.assertEqual(len(samples), 2)
        self._assert_sample(samples[0], 1, 'a', {'spam': 'egg'}, times[2])
        self._assert_sample(samples[1], 2, 'b', None, times[1])

    def test_get_samples_two_driver_one_resource(self):
        times = self._make_timestamps(4)
        fake_driver1 = self._make_fake_driver((1, 'a', {'spam': 'egg'},
                                               times[0]),
                                              (2, 'b', None), times[1])

        fake_driver2 = self._make_fake_driver((11, 'A', None, times[2]),
                                              (12, 'B', None, times[3]))

        self._setup_ext_mgr(http=fake_driver1(), https=fake_driver2())

        samples = self._get_samples('http://foo')

        self.assertEqual(len(samples), 1)
        self._assert_sample(samples[0], 1, 'a', {'spam': 'egg'}, times[0])

    def test_get_samples_multi_samples(self):
        times = self._make_timestamps(2)
        fake_driver = self._make_fake_driver([(1, 'a', {'spam': 'egg'},
                                               times[0]),
                                              (2, 'b', None, times[1])])

        self._setup_ext_mgr(http=fake_driver())

        samples = self._get_samples('http://foo')

        self.assertEqual(len(samples), 2)
        self._assert_sample(samples[0], 1, 'a', {'spam': 'egg'}, times[0])
        self._assert_sample(samples[1], 2, 'b', None, times[1])

    def test_get_samples_return_none(self):
        fake_driver = self._make_fake_driver(None)

        self._setup_ext_mgr(http=fake_driver())

        samples = self._get_samples('http://foo')

        self.assertEqual(len(samples), 0)

    def test_get_samples_return_no_generator(self):
        class NoneFakeDriver(driver.Driver):

            def get_sample_data(self, meter_name, parse_url, params, cache):
                return None

        self._setup_ext_mgr(http=NoneFakeDriver())
        samples = self._get_samples('http://foo')
        self.assertFalse(samples)

########NEW FILE########
__FILENAME__ = test_switch
#
# Copyright 2014 NEC Corporation.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ceilometer.network.statistics import switch
from ceilometer import sample
from ceilometer.tests.network import statistics


class TestSwitchPollster(statistics._PollsterTestBase):

    def test_table_pollster(self):
        self._test_pollster(
            switch.SWPollster,
            'switch',
            sample.TYPE_GAUGE,
            'switch')

########NEW FILE########
__FILENAME__ = test_table
#
# Copyright 2014 NEC Corporation.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ceilometer.network.statistics import table
from ceilometer import sample
from ceilometer.tests.network import statistics


class TestTablePollsters(statistics._PollsterTestBase):

    def test_table_pollster(self):
        self._test_pollster(
            table.TablePollster,
            'switch.table',
            sample.TYPE_GAUGE,
            'table')

    def test_table_pollster_active_entries(self):
        self._test_pollster(
            table.TablePollsterActiveEntries,
            'switch.table.active.entries',
            sample.TYPE_GAUGE,
            'entry')

    def test_table_pollster_lookup_packets(self):
        self._test_pollster(
            table.TablePollsterLookupPackets,
            'switch.table.lookup.packets',
            sample.TYPE_GAUGE,
            'packet')

    def test_table_pollster_matched_packets(self):
        self._test_pollster(
            table.TablePollsterMatchedPackets,
            'switch.table.matched.packets',
            sample.TYPE_GAUGE,
            'packet')

########NEW FILE########
__FILENAME__ = test_floatingip
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 eNovance <licensing@enovance.com>
#
# Copyright 2013 IBM Corp
# All Rights Reserved.
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import mock

from ceilometer.central import manager
from ceilometer.network import floatingip
from ceilometer.openstack.common import context
from ceilometer.openstack.common import test


class TestFloatingIPPollster(test.BaseTestCase):

    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    def setUp(self):
        super(TestFloatingIPPollster, self).setUp()
        self.addCleanup(mock.patch.stopall)
        self.context = context.get_admin_context()
        self.manager = manager.AgentManager()
        self.pollster = floatingip.FloatingIPPollster()
        fake_ips = self.fake_get_ips()
        patch_virt = mock.patch('ceilometer.nova_client.Client.'
                                'floating_ip_get_all',
                                return_value=fake_ips)
        patch_virt.start()

    @staticmethod
    def fake_get_ips():
        ips = []
        for i in range(1, 4):
            ip = mock.MagicMock()
            ip.id = i
            ip.ip = '1.1.1.%d' % i
            ip.pool = 'public'
            ips.append(ip)
        return ips

    # FIXME(dhellmann): Is there a useful way to define this
    # test without a database?
    #
    # def test_get_samples_none_defined(self):
    #     try:
    #         list(self.pollster.get_samples(self.manager,
    #                                         self.context)
    #              )
    #     except exception.NoFloatingIpsDefined:
    #         pass
    #     else:
    #         assert False, 'Should have seen an error'

    def test_get_samples_not_empty(self):
        samples = list(self.pollster.get_samples(self.manager, {}))
        self.assertEqual(3, len(samples))
        # It's necessary to verify all the attributes extracted by Nova
        # API /os-floating-ips to make sure they're available and correct.
        self.assertEqual(1, samples[0].resource_id)
        self.assertEqual("1.1.1.1", samples[0].resource_metadata["address"])
        self.assertEqual("public", samples[0].resource_metadata["pool"])

        self.assertEqual(2, samples[1].resource_id)
        self.assertEqual("1.1.1.2", samples[1].resource_metadata["address"])
        self.assertEqual("public", samples[1].resource_metadata["pool"])

        self.assertEqual(3, samples[2].resource_id)
        self.assertEqual("1.1.1.3", samples[2].resource_metadata["address"])
        self.assertEqual("public", samples[2].resource_metadata["pool"])

    def test_get_meter_names(self):
        samples = list(self.pollster.get_samples(self.manager, {}))
        self.assertEqual(set(['ip.floating']), set([s.name for s in samples]))

    def test_get_samples_cached(self):
        cache = {}
        cache['floating_ips'] = self.fake_get_ips()[:2]
        samples = list(self.pollster.get_samples(self.manager, cache))
        self.assertEqual(2, len(samples))

########NEW FILE########
__FILENAME__ = test_notifications
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests for ceilometer.network.notifications
"""

import mock

from ceilometer.network import notifications
from ceilometer.tests import base as test

NOTIFICATION_NETWORK_CREATE = {
    u'_context_roles': [u'anotherrole',
                        u'Member'],
    u'_context_read_deleted': u'no',
    u'event_type': u'network.create.end',
    u'timestamp': u'2012-09-27 14:11:27.086575',
    u'_context_tenant_id': u'82ed0c40ebe64d0bb3310027039c8ed2',
    u'payload': {u'network':
                 {u'status': u'ACTIVE',
                  u'subnets': [],
                  u'name': u'abcedf',
                  u'router:external': False,
                  u'tenant_id': u'82ed0c40ebe64d0bb3310027039c8ed2',
                  u'admin_state_up': True,
                  u'shared': False,
                  u'id': u'7fd4eb2f-a38e-4c25-8490-71ca8800c9be'}},
    u'priority': u'INFO',
    u'_context_is_admin': False,
    u'_context_timestamp': u'2012-09-27 14:11:26.924779',
    u'_context_user_id': u'b44b7ce67fc84414a5c1660a92a1b862',
    u'publisher_id': u'network.ubuntu-VirtualBox',
    u'message_id': u'9e839576-cc47-4c60-a7d8-5743681213b1'}

NOTIFICATION_BULK_NETWORK_CREATE = {
    '_context_roles': [u'_member_',
                       u'heat_stack_owner',
                       u'admin'],
    u'_context_request_id': u'req-a2dfdefd-b773-4400-9d52-5e146e119950',
    u'_context_read_deleted': u'no',
    u'event_type': u'network.create.end',
    u'_context_user_name': u'admin',
    u'_context_project_name': u'admin',
    u'timestamp': u'2014-05-1510: 24: 56.335612',
    u'_context_tenant_id': u'980ec4870033453ead65c0470a78b8a8',
    u'_context_tenant_name': u'admin',
    u'_context_tenant': u'980ec4870033453ead65c0470a78b8a8',
    u'message_id': u'914eb601-9390-4a72-8629-f013a4c84467',
    u'priority': 'info',
    u'_context_is_admin': True,
    u'_context_project_id': u'980ec4870033453ead65c0470a78b8a8',
    u'_context_timestamp': u'2014-05-1510: 24: 56.285975',
    u'_context_user': u'7520940056d54cceb25cbce888300bea',
    u'_context_user_id': u'7520940056d54cceb25cbce888300bea',
    u'publisher_id': u'network.devstack',
    u'payload': {
        u'networks': [{u'status': u'ACTIVE',
                       u'subnets': [],
                       u'name': u'test2',
                       u'provider: physical_network': None,
                       u'admin_state_up': True,
                       u'tenant_id': u'980ec4870033453ead65c0470a78b8a8',
                       u'provider: network_type': u'local',
                       u'shared': False,
                       u'id': u'7cbc7a66-bbd0-41fc-a186-81c3da5c9843',
                       u'provider: segmentation_id': None},
                      {u'status': u'ACTIVE',
                       u'subnets': [],
                       u'name': u'test3',
                       u'provider: physical_network': None,
                       u'admin_state_up': True,
                       u'tenant_id': u'980ec4870033453ead65c0470a78b8a8',
                       u'provider: network_type': u'local',
                       u'shared': False,
                       u'id': u'5a7cb86f-1638-4cc1-8dcc-8bbbc8c7510d',
                       u'provider: segmentation_id': None}]
    }
}

NOTIFICATION_SUBNET_CREATE = {
    u'_context_roles': [u'anotherrole',
                        u'Member'],
    u'_context_read_deleted': u'no',
    u'event_type': u'subnet.create.end',
    u'timestamp': u'2012-09-27 14:11:27.426620',
    u'_context_tenant_id': u'82ed0c40ebe64d0bb3310027039c8ed2',
    u'payload': {
        u'subnet': {
            u'name': u'mysubnet',
            u'enable_dhcp': True,
            u'network_id': u'7fd4eb2f-a38e-4c25-8490-71ca8800c9be',
            u'tenant_id': u'82ed0c40ebe64d0bb3310027039c8ed2',
            u'dns_nameservers': [],
            u'allocation_pools': [{u'start': u'192.168.42.2',
                                   u'end': u'192.168.42.254'}],
            u'host_routes': [],
            u'ip_version': 4,
            u'gateway_ip': u'192.168.42.1',
            u'cidr': u'192.168.42.0/24',
            u'id': u'1a3a170d-d7ce-4cc9-b1db-621da15a25f5'}},
    u'priority': u'INFO',
    u'_context_is_admin': False,
    u'_context_timestamp': u'2012-09-27 14:11:27.214490',
    u'_context_user_id': u'b44b7ce67fc84414a5c1660a92a1b862',
    u'publisher_id': u'network.ubuntu-VirtualBox',
    u'message_id': u'd86dfc66-d3c3-4aea-b06d-bf37253e6116'}

NOTIFICATION_BULK_SUBNET_CREATE = {
    '_context_roles': [u'_member_',
                       u'heat_stack_owner',
                       u'admin'],
    u'_context_request_id': u'req-b77e278a-0cce-4987-9f82-15957b234768',
    u'_context_read_deleted': u'no',
    u'event_type': u'subnet.create.end',
    u'_context_user_name': u'admin',
    u'_context_project_name': u'admin',
    u'timestamp': u'2014-05-1510: 47: 08.133888',
    u'_context_tenant_id': u'980ec4870033453ead65c0470a78b8a8',
    u'_context_tenant_name': u'admin',
    u'_context_tenant': u'980ec4870033453ead65c0470a78b8a8',
    u'message_id': u'c7e6f9fd-ead2-415f-8493-b95bedf72e43',
    u'priority': u'info',
    u'_context_is_admin': True,
    u'_context_project_id': u'980ec4870033453ead65c0470a78b8a8',
    u'_context_timestamp': u'2014-05-1510: 47: 07.970043',
    u'_context_user': u'7520940056d54cceb25cbce888300bea',
    u'_context_user_id': u'7520940056d54cceb25cbce888300bea',
    u'publisher_id': u'network.devstack',
    u'payload': {
        u'subnets': [{u'name': u'',
                      u'enable_dhcp': True,
                      u'network_id': u'3ddfe60b-34b4-4e9d-9440-43c904b1c58e',
                      u'tenant_id': u'980ec4870033453ead65c0470a78b8a8',
                      u'dns_nameservers': [],
                      u'ipv6_ra_mode': None,
                      u'allocation_pools': [{u'start': u'10.0.4.2',
                                             u'end': u'10.0.4.254'}],
                      u'host_routes': [],
                      u'ipv6_address_mode': None,
                      u'ip_version': 4,
                      u'gateway_ip': u'10.0.4.1',
                      u'cidr': u'10.0.4.0/24',
                      u'id': u'14020d7b-6dd7-4349-bb8e-8f954c919022'},
                     {u'name': u'',
                      u'enable_dhcp': True,
                      u'network_id': u'3ddfe60b-34b4-4e9d-9440-43c904b1c58e',
                      u'tenant_id': u'980ec4870033453ead65c0470a78b8a8',
                      u'dns_nameservers': [],
                      u'ipv6_ra_mode': None,
                      u'allocation_pools': [{u'start': u'10.0.5.2',
                                             u'end': u'10.0.5.254'}],
                      u'host_routes': [],
                      u'ipv6_address_mode': None,
                      u'ip_version': 4,
                      u'gateway_ip': u'10.0.5.1',
                      u'cidr': u'10.0.5.0/24',
                      u'id': u'a080991b-a32a-4bf7-a558-96c4b77d075c'}]
    }
}

NOTIFICATION_PORT_CREATE = {
    u'_context_roles': [u'anotherrole',
                        u'Member'],
    u'_context_read_deleted': u'no',
    u'event_type': u'port.create.end',
    u'timestamp': u'2012-09-27 14:28:31.536370',
    u'_context_tenant_id': u'82ed0c40ebe64d0bb3310027039c8ed2',
    u'payload': {
        u'port': {
            u'status': u'ACTIVE',
            u'name': u'',
            u'admin_state_up': True,
            u'network_id': u'7fd4eb2f-a38e-4c25-8490-71ca8800c9be',
            u'tenant_id': u'82ed0c40ebe64d0bb3310027039c8ed2',
            u'device_owner': u'',
            u'mac_address': u'fa:16:3e:75:0c:49',
            u'fixed_ips': [{
                u'subnet_id': u'1a3a170d-d7ce-4cc9-b1db-621da15a25f5',
                u'ip_address': u'192.168.42.3'}],
            u'id': u'9cdfeb92-9391-4da7-95a1-ca214831cfdb',
            u'device_id': u''}},
    u'priority': u'INFO',
    u'_context_is_admin': False,
    u'_context_timestamp': u'2012-09-27 14:28:31.438919',
    u'_context_user_id': u'b44b7ce67fc84414a5c1660a92a1b862',
    u'publisher_id': u'network.ubuntu-VirtualBox',
    u'message_id': u'7135b8ab-e13c-4ac8-bc31-75e7f756622a'}

NOTIFICATION_BULK_PORT_CREATE = {
    u'_context_roles': [u'_member_',
                        u'SwiftOperator'],
    u'_context_request_id': u'req-678be9ad-c399-475a-b3e8-8da0c06375aa',
    u'_context_read_deleted': u'no',
    u'event_type': u'port.create.end',
    u'_context_project_name': u'demo',
    u'timestamp': u'2014-05-0909: 19: 58.317548',
    u'_context_tenant_id': u'133087d90fc149528b501dd8b75ea965',
    u'_context_timestamp': u'2014-05-0909: 19: 58.160011',
    u'_context_tenant': u'133087d90fc149528b501dd8b75ea965',
    u'payload': {
        u'ports': [{u'status': u'DOWN',
                    u'name': u'port--1501135095',
                    u'allowed_address_pairs': [],
                    u'admin_state_up': True,
                    u'network_id': u'acf63fdc-b43b-475d-8cca-9429b843d5e8',
                    u'tenant_id': u'133087d90fc149528b501dd8b75ea965',
                    u'binding: vnic_type': u'normal',
                    u'device_owner': u'',
                    u'mac_address': u'fa: 16: 3e: 37: 10: 39',
                    u'fixed_ips': [],
                    u'id': u'296c2c9f-14e9-48da-979d-78b213454c59',
                    u'security_groups': [
                        u'a06f7c9d-9e5a-46b0-9f6c-ce812aa2e5ff'],
                    u'device_id': u''},
                   {u'status': u'DOWN',
                    u'name': u'',
                    u'allowed_address_pairs': [],
                    u'admin_state_up': False,
                    u'network_id': u'0a8eea59-0146-425c-b470-e9ddfa99ec61',
                    u'tenant_id': u'133087d90fc149528b501dd8b75ea965',
                    u'binding: vnic_type': u'normal',
                    u'device_owner': u'',
                    u'mac_address': u'fa: 16: 3e: 8e: 6e: 53',
                    u'fixed_ips': [],
                    u'id': u'd8bb667f-5cd3-4eca-a984-268e25b1b7a5',
                    u'security_groups': [
                        u'a06f7c9d-9e5a-46b0-9f6c-ce812aa2e5ff'],
                    u'device_id': u''}]
    },
    u'_unique_id': u'60b1650f17fc4fa59492f447321fb26c',
    u'_context_is_admin': False,
    u'_context_project_id': u'133087d90fc149528b501dd8b75ea965',
    u'_context_tenant_name': u'demo',
    u'_context_user': u'b1eb48f9c54741f4adc1b4ea512d400c',
    u'_context_user_name': u'demo',
    u'publisher_id': u'network.os-ci-test12',
    u'message_id': u'04aa45e1-3c30-4c69-8638-e7ff8621e9bc',
    u'_context_user_id': u'b1eb48f9c54741f4adc1b4ea512d400c',
    u'priority': u'INFO'
}

NOTIFICATION_PORT_UPDATE = {
    u'_context_roles': [u'anotherrole',
                        u'Member'],
    u'_context_read_deleted': u'no',
    u'event_type': u'port.update.end',
    u'timestamp': u'2012-09-27 14:35:09.514052',
    u'_context_tenant_id': u'82ed0c40ebe64d0bb3310027039c8ed2',
    u'payload': {
        u'port': {
            u'status': u'ACTIVE',
            u'name': u'bonjour',
            u'admin_state_up': True,
            u'network_id': u'7fd4eb2f-a38e-4c25-8490-71ca8800c9be',
            u'tenant_id': u'82ed0c40ebe64d0bb3310027039c8ed2',
            u'device_owner': u'',
            u'mac_address': u'fa:16:3e:75:0c:49',
            u'fixed_ips': [{
                u'subnet_id': u'1a3a170d-d7ce-4cc9-b1db-621da15a25f5',
                u'ip_address': u'192.168.42.3'}],
            u'id': u'9cdfeb92-9391-4da7-95a1-ca214831cfdb',
            u'device_id': u''}},
    u'priority': u'INFO',
    u'_context_is_admin': False,
    u'_context_timestamp': u'2012-09-27 14:35:09.447682',
    u'_context_user_id': u'b44b7ce67fc84414a5c1660a92a1b862',
    u'publisher_id': u'network.ubuntu-VirtualBox',
    u'message_id': u'07b0a3a1-c0b5-40ab-a09c-28dee6bf48f4'}


NOTIFICATION_NETWORK_EXISTS = {
    u'_context_roles': [u'anotherrole',
                        u'Member'],
    u'_context_read_deleted': u'no',
    u'event_type': u'network.exists',
    u'timestamp': u'2012-09-27 14:11:27.086575',
    u'_context_tenant_id': u'82ed0c40ebe64d0bb3310027039c8ed2',
    u'payload': {u'network':
                 {u'status': u'ACTIVE',
                  u'subnets': [],
                  u'name': u'abcedf',
                  u'router:external': False,
                  u'tenant_id': u'82ed0c40ebe64d0bb3310027039c8ed2',
                  u'admin_state_up': True,
                  u'shared': False,
                  u'id': u'7fd4eb2f-a38e-4c25-8490-71ca8800c9be'}},
    u'priority': u'INFO',
    u'_context_is_admin': False,
    u'_context_timestamp': u'2012-09-27 14:11:26.924779',
    u'_context_user_id': u'b44b7ce67fc84414a5c1660a92a1b862',
    u'publisher_id': u'network.ubuntu-VirtualBox',
    u'message_id': u'9e839576-cc47-4c60-a7d8-5743681213b1'}


NOTIFICATION_ROUTER_EXISTS = {
    u'_context_roles': [u'anotherrole',
                        u'Member'],
    u'_context_read_deleted': u'no',
    u'event_type': u'router.exists',
    u'timestamp': u'2012-09-27 14:11:27.086575',
    u'_context_tenant_id': u'82ed0c40ebe64d0bb3310027039c8ed2',
    u'payload': {u'router':
                 {'status': u'ACTIVE',
                  'external_gateway_info':
                  {'network_id': u'89d55642-4dec-43a4-a617-6cec051393b5'},
                  'name': u'router1',
                  'admin_state_up': True,
                  'tenant_id': u'bb04a2b769c94917b57ba49df7783cfd',
                  'id': u'ab8bb3ed-df23-4ca0-8f03-b887abcd5c23'}},
    u'priority': u'INFO',
    u'_context_is_admin': False,
    u'_context_timestamp': u'2012-09-27 14:11:26.924779',
    u'_context_user_id': u'b44b7ce67fc84414a5c1660a92a1b862',
    u'publisher_id': u'network.ubuntu-VirtualBox',
    u'message_id': u'9e839576-cc47-4c60-a7d8-5743681213b1'}


NOTIFICATION_FLOATINGIP_EXISTS = {
    u'_context_roles': [u'anotherrole',
                        u'Member'],
    u'_context_read_deleted': u'no',
    u'event_type': u'floatingip.exists',
    u'timestamp': u'2012-09-27 14:11:27.086575',
    u'_context_tenant_id': u'82ed0c40ebe64d0bb3310027039c8ed2',
    u'payload': {u'floatingip':
                 {'router_id': None,
                  'tenant_id': u'6e5f9df9b3a249ab834f25fe1b1b81fd',
                  'floating_network_id':
                  u'001400f7-1710-4245-98c3-39ba131cc39a',
                  'fixed_ip_address': None,
                  'floating_ip_address': u'172.24.4.227',
                  'port_id': None,
                  'id': u'2b7cc28c-6f78-4735-9246-257168405de6'}},
    u'priority': u'INFO',
    u'_context_is_admin': False,
    u'_context_timestamp': u'2012-09-27 14:11:26.924779',
    u'_context_user_id': u'b44b7ce67fc84414a5c1660a92a1b862',
    u'publisher_id': u'network.ubuntu-VirtualBox',
    u'message_id': u'9e839576-cc47-4c60-a7d8-5743681213b1'}


NOTIFICATION_FLOATINGIP_UPDATE = {
    u'_context_roles': [u'anotherrole',
                        u'Member'],
    u'_context_read_deleted': u'no',
    u'event_type': u'floatingip.update.start',
    u'timestamp': u'2012-09-27 14:11:27.086575',
    u'_context_tenant_id': u'82ed0c40ebe64d0bb3310027039c8ed2',
    u'payload': {u'floatingip':
                   {u'fixed_ip_address': u'172.24.4.227',
                    u'id': u'a68c9390-829e-4732-bad4-e0a978498cc5',
                    u'port_id': u'e12150f2-885b-45bc-a248-af1c23787d55'}},
    u'priority': u'INFO',
    u'_unique_id': u'e483db017b2341fd9ec314dcda88d3e9',
    u'_context_is_admin': False,
    u'_context_project_id': u'82ed0c40ebe64d0bb3310027039c8ed2',
    u'_context_timestamp': u'2012-09-27 14:11:26.924779',
    u'_context_user_id': u'b44b7ce67fc84414a5c1660a92a1b862',
    u'publisher_id': u'network.ubuntu-VirtualBox',
    u'message_id': u'9e839576-cc47-4c60-a7d8-5743681213b1'}


NOTIFICATION_L3_METER = {
    u'_context_roles': [u'admin'],
    u'_context_read_deleted': u'no',
    u'event_type': u'l3.meter',
    u'timestamp': u'2013-08-22 13:14:06.880304',
    u'_context_tenant_id': None,
    u'payload': {u'first_update': 1377176476,
                 u'bytes': 0,
                 u'label_id': u'383244a7-e99b-433a-b4a1-d37cf5b17d15',
                 u'last_update': 1377177246,
                 u'host': u'precise64',
                 u'tenant_id': u'admin',
                 u'time': 30,
                 u'pkts': 0},
    u'priority': u'INFO',
    u'_context_is_admin': True,
    u'_context_timestamp': u'2013-08-22 13:01:06.614635',
    u'_context_user_id': None,
    u'publisher_id': u'metering.precise64',
    u'message_id': u'd7aee6e8-c7eb-4d47-9338-f60920d708e4',
    u'_unique_id': u'd5a3bdacdcc24644b84e67a4c10e886a',
    u'_context_project_id': None}


class TestNotifications(test.BaseTestCase):
    def test_network_create(self):
        v = notifications.Network(mock.Mock())
        samples = list(v.process_notification(NOTIFICATION_NETWORK_CREATE))
        self.assertEqual(2, len(samples))
        self.assertEqual("network.create", samples[1].name)

    def test_bulk_network_create(self):
        v = notifications.Network(mock.Mock())
        samples = list(v.process_notification(
            NOTIFICATION_BULK_NETWORK_CREATE))
        self.assertEqual(4, len(samples))
        self.assertEqual("network", samples[0].name)
        self.assertEqual("network.create", samples[1].name)
        self.assertEqual("network", samples[2].name)
        self.assertEqual("network.create", samples[3].name)

    def test_subnet_create(self):
        v = notifications.Subnet(mock.Mock())
        samples = list(v.process_notification(NOTIFICATION_SUBNET_CREATE))
        self.assertEqual(2, len(samples))
        self.assertEqual("subnet.create", samples[1].name)

    def test_bulk_subnet_create(self):
        v = notifications.Subnet(mock.Mock())
        samples = list(v.process_notification(NOTIFICATION_BULK_SUBNET_CREATE))
        self.assertEqual(4, len(samples))
        self.assertEqual("subnet", samples[0].name)
        self.assertEqual("subnet.create", samples[1].name)
        self.assertEqual("subnet", samples[2].name)
        self.assertEqual("subnet.create", samples[3].name)

    def test_port_create(self):
        v = notifications.Port(mock.Mock())
        samples = list(v.process_notification(NOTIFICATION_PORT_CREATE))
        self.assertEqual(2, len(samples))
        self.assertEqual("port.create", samples[1].name)

    def test_bulk_port_create(self):
        v = notifications.Port(mock.Mock())
        samples = list(v.process_notification(NOTIFICATION_BULK_PORT_CREATE))
        self.assertEqual(4, len(samples))
        self.assertEqual("port", samples[0].name)
        self.assertEqual("port.create", samples[1].name)
        self.assertEqual("port", samples[2].name)
        self.assertEqual("port.create", samples[3].name)

    def test_port_update(self):
        v = notifications.Port(mock.Mock())
        samples = list(v.process_notification(NOTIFICATION_PORT_UPDATE))
        self.assertEqual(2, len(samples))
        self.assertEqual("port.update", samples[1].name)

    def test_network_exists(self):
        v = notifications.Network(mock.Mock())
        samples = v.process_notification(NOTIFICATION_NETWORK_EXISTS)
        self.assertEqual(1, len(list(samples)))

    def test_router_exists(self):
        v = notifications.Router(mock.Mock())
        samples = v.process_notification(NOTIFICATION_ROUTER_EXISTS)
        self.assertEqual(1, len(list(samples)))

    def test_floatingip_exists(self):
        v = notifications.FloatingIP(mock.Mock())
        samples = list(v.process_notification(NOTIFICATION_FLOATINGIP_EXISTS))
        self.assertEqual(1, len(samples))
        self.assertEqual("ip.floating", samples[0].name)

    def test_floatingip_update(self):
        v = notifications.FloatingIP(mock.Mock())
        samples = list(v.process_notification(NOTIFICATION_FLOATINGIP_UPDATE))
        self.assertEqual(len(samples), 2)
        self.assertEqual(samples[0].name, "ip.floating")

    def test_metering_report(self):
        v = notifications.Bandwidth(mock.Mock())
        samples = list(v.process_notification(NOTIFICATION_L3_METER))
        self.assertEqual(1, len(samples))
        self.assertEqual("bandwidth", samples[0].name)


class TestEventTypes(test.BaseTestCase):

    def test_network(self):
        v = notifications.Network(mock.Mock())
        events = v.event_types
        self.assertIsNotEmpty(events)

    def test_subnet(self):
        v = notifications.Subnet(mock.Mock())
        events = v.event_types
        self.assertIsNotEmpty(events)

    def test_port(self):
        v = notifications.Port(mock.Mock())
        events = v.event_types
        self.assertIsNotEmpty(events)

    def test_router(self):
        self.assertTrue(notifications.Router(mock.Mock()).event_types)

    def test_floatingip(self):
        self.assertTrue(notifications.FloatingIP(mock.Mock()).event_types)

    def test_bandwidth(self):
        self.assertTrue(notifications.Bandwidth(mock.Mock()).event_types)

########NEW FILE########
__FILENAME__ = test_swift
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 eNovance <licensing@enovance.com>
#
# Author: Guillaume Pernot <gpernot@praksys.org>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import collections

from keystoneclient import exceptions
import mock
from swiftclient import client as swift_client
import testscenarios.testcase

from ceilometer.central import manager
from ceilometer.objectstore import swift
from ceilometer.openstack.common.fixture.mockpatch import PatchObject
from ceilometer.openstack.common import test

HEAD_ACCOUNTS = [('tenant-000', {'x-account-object-count': 12,
                                 'x-account-bytes-used': 321321321,
                                 'x-account-container-count': 7,
                                 }),
                 ('tenant-001', {'x-account-object-count': 34,
                                 'x-account-bytes-used': 9898989898,
                                 'x-account-container-count': 17,
                                 })]

GET_ACCOUNTS = [('tenant-002', ({'x-account-object-count': 10,
                                 'x-account-bytes-used': 123123,
                                 'x-account-container-count': 2,
                                 },
                                [{'count': 10,
                                  'bytes': 123123,
                                  'name': 'my_container'},
                                 {'count': 0,
                                  'bytes': 0,
                                  'name': 'new_container'
                                  }])),
                ('tenant-003', ({'x-account-object-count': 0,
                                 'x-account-bytes-used': 0,
                                 'x-account-container-count': 0,
                                 }, [])), ]


class TestManager(manager.AgentManager):

    def __init__(self):
        super(TestManager, self).__init__()
        self.keystone = mock.MagicMock()


class TestSwiftPollster(testscenarios.testcase.WithScenarios,
                        test.BaseTestCase):

    # Define scenarios to run all of the tests against all of the
    # pollsters.
    scenarios = [
        ('storage.objects',
         {'factory': swift.ObjectsPollster}),
        ('storage.objects.size',
         {'factory': swift.ObjectsSizePollster}),
        ('storage.objects.containers',
         {'factory': swift.ObjectsContainersPollster}),
        ('storage.containers.objects',
         {'factory': swift.ContainersObjectsPollster}),
        ('storage.containers.objects.size',
         {'factory': swift.ContainersSizePollster}),
    ]

    @staticmethod
    def fake_ks_service_catalog_url_for(*args, **kwargs):
        raise exceptions.EndpointNotFound("Fake keystone exception")

    def fake_iter_accounts(self, ksclient, cache):
        for i in self.ACCOUNTS:
            yield i

    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    def setUp(self):
        super(TestSwiftPollster, self).setUp()
        self.pollster = self.factory()
        self.manager = TestManager()

        if self.pollster.CACHE_KEY_METHOD == 'swift.head_account':
            self.ACCOUNTS = HEAD_ACCOUNTS
        else:
            self.ACCOUNTS = GET_ACCOUNTS

    def test_iter_accounts_no_cache(self):
        cache = {}
        with PatchObject(self.factory, '_get_account_info',
                         return_value=[]):
            data = list(self.pollster._iter_accounts(mock.Mock(), cache))

        self.assertTrue(self.pollster.CACHE_KEY_TENANT in cache)
        self.assertTrue(self.pollster.CACHE_KEY_METHOD in cache)
        self.assertEqual([], data)

    def test_iter_accounts_tenants_cached(self):
        # Verify that if there are tenants pre-cached then the account
        # info loop iterates over those instead of asking for the list
        # again.
        ksclient = mock.Mock()
        ksclient.tenants.list.side_effect = AssertionError(
            'should not be called',
        )

        api_method = '%s_account' % self.pollster.METHOD
        with PatchObject(swift_client, api_method, new=ksclient):
            with PatchObject(self.factory, '_neaten_url'):
                Tenant = collections.namedtuple('Tenant', 'id')
                cache = {
                    self.pollster.CACHE_KEY_TENANT: [
                        Tenant(self.ACCOUNTS[0][0])
                    ],
                }
                data = list(self.pollster._iter_accounts(mock.Mock(), cache))
        self.assertTrue(self.pollster.CACHE_KEY_METHOD in cache)
        self.assertEqual(self.ACCOUNTS[0][0], data[0][0])

    def test_neaten_url(self):
        test_endpoint = 'http://127.0.0.1:8080'
        test_tenant_id = 'a7fd1695fa154486a647e44aa99a1b9b'
        standard_url = test_endpoint + '/v1/' + 'AUTH_' + test_tenant_id

        self.assertEqual(standard_url,
                         swift._Base._neaten_url(test_endpoint,
                                                 test_tenant_id))
        self.assertEqual(standard_url,
                         swift._Base._neaten_url(test_endpoint + '/',
                                                 test_tenant_id))
        self.assertEqual(standard_url,
                         swift._Base._neaten_url(test_endpoint + '/v1',
                                                 test_tenant_id))
        self.assertEqual(standard_url,
                         swift._Base._neaten_url(standard_url,
                                                 test_tenant_id))

    def test_metering(self):
        with PatchObject(self.factory, '_iter_accounts',
                         side_effect=self.fake_iter_accounts):
            samples = list(self.pollster.get_samples(self.manager, {}))

        self.assertEqual(2, len(samples))

    def test_get_meter_names(self):
        with PatchObject(self.factory, '_iter_accounts',
                         side_effect=self.fake_iter_accounts):
            samples = list(self.pollster.get_samples(self.manager, {}))

        self.assertEqual(set([samples[0].name]),
                         set([s.name for s in samples]))

    def test_endpoint_notfound(self):
        with PatchObject(self.manager.keystone.service_catalog, 'url_for',
                         side_effect=self.fake_ks_service_catalog_url_for):
            samples = list(self.pollster.get_samples(self.manager, {}))

        self.assertEqual(0, len(samples))

########NEW FILE########
__FILENAME__ = test_swift_middleware
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 eNovance <licensing@enovance.com>
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import six

import mock
try:
    # Swift >= 1.7.5
    import swift.common.swob
    REQUEST = swift.common.swob
except ImportError:
    import webob
    REQUEST = webob

from ceilometer import messaging
from ceilometer.objectstore import swift_middleware
from ceilometer.openstack.common.fixture import config
from ceilometer.openstack.common.fixture.mockpatch import PatchObject
from ceilometer.openstack.common import test
from ceilometer import pipeline


class FakeApp(object):
    def __init__(self, body=['This string is 28 bytes long']):
        self.body = body

    def __call__(self, env, start_response):
        start_response('200 OK', [
            ('Content-Type', 'text/plain'),
            ('Content-Length', str(sum(map(len, self.body))))
        ])
        while env['wsgi.input'].read(5):
            pass
        return self.body


class TestSwiftMiddleware(test.BaseTestCase):

    class _faux_pipeline_manager(pipeline.PipelineManager):
        class _faux_pipeline(object):
            def __init__(self, pipeline_manager):
                self.pipeline_manager = pipeline_manager
                self.samples = []

            def publish_samples(self, ctxt, samples):
                self.samples.extend(samples)

            def flush(self, context):
                pass

        def __init__(self):
            self.pipelines = [self._faux_pipeline(self)]

    def _fake_setup_pipeline(self, transformer_manager=None):
        return self.pipeline_manager

    def setUp(self):
        super(TestSwiftMiddleware, self).setUp()
        self.pipeline_manager = self._faux_pipeline_manager()
        self.useFixture(PatchObject(pipeline, 'setup_pipeline',
                                    side_effect=self._fake_setup_pipeline))
        messaging.setup('fake://')
        self.addCleanup(messaging.cleanup)
        self.CONF = self.useFixture(config.Config()).conf

    @staticmethod
    def start_response(*args):
            pass

    def test_rpc_setup(self):
        swift_middleware.CeilometerMiddleware(FakeApp(), {})
        self.assertEqual('ceilometer', self.CONF.control_exchange)

    def test_get(self):
        app = swift_middleware.CeilometerMiddleware(FakeApp(), {})
        req = REQUEST.Request.blank('/1.0/account/container/obj',
                                    environ={'REQUEST_METHOD': 'GET'})
        resp = app(req.environ, self.start_response)
        self.assertEqual(["This string is 28 bytes long"], list(resp))
        samples = self.pipeline_manager.pipelines[0].samples
        self.assertEqual(2, len(samples))
        data = samples[0]
        self.assertEqual(28, data.volume)
        self.assertEqual('1.0', data.resource_metadata['version'])
        self.assertEqual('container', data.resource_metadata['container'])
        self.assertEqual('obj', data.resource_metadata['object'])

        # test the # of request and the request method
        data = samples[1]
        self.assertEqual('storage.api.request', data.name)
        self.assertEqual(1, data.volume)
        self.assertEqual('get', data.resource_metadata['method'])

    def test_put(self):
        app = swift_middleware.CeilometerMiddleware(FakeApp(body=['']), {})
        req = REQUEST.Request.blank(
            '/1.0/account/container/obj',
            environ={'REQUEST_METHOD': 'PUT',
                     'wsgi.input':
                     six.moves.cStringIO('some stuff')})
        list(app(req.environ, self.start_response))
        samples = self.pipeline_manager.pipelines[0].samples
        self.assertEqual(2, len(samples))
        data = samples[0]
        self.assertEqual(10, data.volume)
        self.assertEqual('1.0', data.resource_metadata['version'])
        self.assertEqual('container', data.resource_metadata['container'])
        self.assertEqual('obj', data.resource_metadata['object'])

        # test the # of request and the request method
        data = samples[1]
        self.assertEqual('storage.api.request', data.name)
        self.assertEqual(1, data.volume)
        self.assertEqual('put', data.resource_metadata['method'])

    def test_post(self):
        app = swift_middleware.CeilometerMiddleware(FakeApp(body=['']), {})
        req = REQUEST.Request.blank(
            '/1.0/account/container/obj',
            environ={'REQUEST_METHOD': 'POST',
                     'wsgi.input': six.moves.cStringIO('some other stuff')})
        list(app(req.environ, self.start_response))
        samples = self.pipeline_manager.pipelines[0].samples
        self.assertEqual(2, len(samples))
        data = samples[0]
        self.assertEqual(16, data.volume)
        self.assertEqual('1.0', data.resource_metadata['version'])
        self.assertEqual('container', data.resource_metadata['container'])
        self.assertEqual('obj', data.resource_metadata['object'])

        # test the # of request and the request method
        data = samples[1]
        self.assertEqual('storage.api.request', data.name)
        self.assertEqual(1, data.volume)
        self.assertEqual('post', data.resource_metadata['method'])

    def test_head(self):
        app = swift_middleware.CeilometerMiddleware(FakeApp(body=['']), {})
        req = REQUEST.Request.blank('/1.0/account/container/obj',
                                    environ={'REQUEST_METHOD': 'HEAD'})
        list(app(req.environ, self.start_response))
        samples = self.pipeline_manager.pipelines[0].samples
        self.assertEqual(1, len(samples))
        data = samples[0]
        self.assertEqual('1.0', data.resource_metadata['version'])
        self.assertEqual('container', data.resource_metadata['container'])
        self.assertEqual('obj', data.resource_metadata['object'])
        self.assertEqual('head', data.resource_metadata['method'])

        self.assertEqual('storage.api.request', data.name)
        self.assertEqual(1, data.volume)

    def test_bogus_request(self):
        """Test even for arbitrary request method, this will still work."""
        app = swift_middleware.CeilometerMiddleware(FakeApp(body=['']), {})
        req = REQUEST.Request.blank('/1.0/account/container/obj',
                                    environ={'REQUEST_METHOD': 'BOGUS'})
        list(app(req.environ, self.start_response))
        samples = self.pipeline_manager.pipelines[0].samples

        self.assertEqual(1, len(samples))
        data = samples[0]
        self.assertEqual('1.0', data.resource_metadata['version'])
        self.assertEqual('container', data.resource_metadata['container'])
        self.assertEqual('obj', data.resource_metadata['object'])
        self.assertEqual('bogus', data.resource_metadata['method'])

        self.assertEqual('storage.api.request', data.name)
        self.assertEqual(1, data.volume)

    def test_get_container(self):
        app = swift_middleware.CeilometerMiddleware(FakeApp(), {})
        req = REQUEST.Request.blank('/1.0/account/container',
                                    environ={'REQUEST_METHOD': 'GET'})
        list(app(req.environ, self.start_response))
        samples = self.pipeline_manager.pipelines[0].samples
        self.assertEqual(2, len(samples))
        data = samples[0]
        self.assertEqual(28, data.volume)
        self.assertEqual('1.0', data.resource_metadata['version'])
        self.assertEqual('container', data.resource_metadata['container'])
        self.assertIsNone(data.resource_metadata['object'])

    def test_no_metadata_headers(self):
        app = swift_middleware.CeilometerMiddleware(FakeApp(), {})
        req = REQUEST.Request.blank('/1.0/account/container',
                                    environ={'REQUEST_METHOD': 'GET'})
        list(app(req.environ, self.start_response))
        samples = self.pipeline_manager.pipelines[0].samples
        self.assertEqual(2, len(samples))
        data = samples[0]
        http_headers = [k for k in data.resource_metadata.keys()
                        if k.startswith('http_header_')]
        self.assertEqual(0, len(http_headers))
        self.assertEqual('1.0', data.resource_metadata['version'])
        self.assertEqual('container', data.resource_metadata['container'])
        self.assertIsNone(data.resource_metadata['object'])

    def test_metadata_headers(self):
        app = swift_middleware.CeilometerMiddleware(FakeApp(), {
            'metadata_headers': 'X_VAR1, x-var2, x-var3'
        })
        req = REQUEST.Request.blank('/1.0/account/container',
                                    environ={'REQUEST_METHOD': 'GET'},
                                    headers={'X_VAR1': 'value1',
                                             'X_VAR2': 'value2'})
        list(app(req.environ, self.start_response))
        samples = self.pipeline_manager.pipelines[0].samples
        self.assertEqual(2, len(samples))
        data = samples[0]
        http_headers = [k for k in data.resource_metadata.keys()
                        if k.startswith('http_header_')]
        self.assertEqual(2, len(http_headers))
        self.assertEqual('1.0', data.resource_metadata['version'])
        self.assertEqual('container', data.resource_metadata['container'])
        self.assertIsNone(data.resource_metadata['object'])
        self.assertEqual('value1',
                         data.resource_metadata['http_header_x_var1'])
        self.assertEqual('value2',
                         data.resource_metadata['http_header_x_var2'])
        self.assertFalse('http_header_x_var3' in data.resource_metadata)

    def test_metadata_headers_on_not_existing_header(self):
        app = swift_middleware.CeilometerMiddleware(FakeApp(), {
            'metadata_headers': 'x-var3'
        })
        req = REQUEST.Request.blank('/1.0/account/container',
                                    environ={'REQUEST_METHOD': 'GET'})
        list(app(req.environ, self.start_response))
        samples = self.pipeline_manager.pipelines[0].samples
        self.assertEqual(2, len(samples))
        data = samples[0]
        http_headers = [k for k in data.resource_metadata.keys()
                        if k.startswith('http_header_')]
        self.assertEqual(0, len(http_headers))
        self.assertEqual('1.0', data.resource_metadata['version'])
        self.assertEqual('container', data.resource_metadata['container'])
        self.assertIsNone(data.resource_metadata['object'])

    def test_bogus_path(self):
        app = swift_middleware.CeilometerMiddleware(FakeApp(), {})
        req = REQUEST.Request.blank('/5.0//',
                                    environ={'REQUEST_METHOD': 'GET'})
        list(app(req.environ, self.start_response))
        samples = self.pipeline_manager.pipelines[0].samples
        self.assertEqual(0, len(samples))

    def test_missing_resource_id(self):
        app = swift_middleware.CeilometerMiddleware(FakeApp(), {})
        req = REQUEST.Request.blank('/v1/', environ={'REQUEST_METHOD': 'GET'})
        list(app(req.environ, self.start_response))
        samples = self.pipeline_manager.pipelines[0].samples
        self.assertEqual(0, len(samples))

    @mock.patch.object(swift_middleware.CeilometerMiddleware,
                       'publish_sample')
    def test_publish_sample_fail(self, mocked_publish_sample):
        mocked_publish_sample.side_effect = Exception("a exception")
        app = swift_middleware.CeilometerMiddleware(FakeApp(body=["test"]), {})
        req = REQUEST.Request.blank('/1.0/account/container',
                                    environ={'REQUEST_METHOD': 'GET'})
        resp = list(app(req.environ, self.start_response))
        samples = self.pipeline_manager.pipelines[0].samples
        self.assertEqual(0, len(samples))
        self.assertEqual(["test"], resp)
        mocked_publish_sample.assert_called_once_with(mock.ANY, 0, 4)

    def test_reseller_prefix(self):
        # No reseller prefix set: ensure middleware uses AUTH_
        app = swift_middleware.CeilometerMiddleware(FakeApp(), {})
        req = REQUEST.Request.blank('/1.0/AUTH_account/container/obj',
                                    environ={'REQUEST_METHOD': 'GET'})
        list(app(req.environ, self.start_response))
        samples = self.pipeline_manager.pipelines[0].samples[0]
        self.assertEqual("account", samples.resource_id)

        # Custom reseller prefix set
        app = swift_middleware.CeilometerMiddleware(
            FakeApp(), {'reseller_prefix': 'CUSTOM_'})
        req = REQUEST.Request.blank('/1.0/CUSTOM_account/container/obj',
                                    environ={'REQUEST_METHOD': 'GET'})
        list(app(req.environ, self.start_response))
        samples = self.pipeline_manager.pipelines[0].samples[0]
        self.assertEqual("account", samples.resource_id)

    def test_invalid_reseller_prefix(self):
        # Custom reseller prefix set, but without trailing underscore
        app = swift_middleware.CeilometerMiddleware(
            FakeApp(), {'reseller_prefix': 'CUSTOM'})
        req = REQUEST.Request.blank('/1.0/CUSTOM_account/container/obj',
                                    environ={'REQUEST_METHOD': 'GET'})
        list(app(req.environ, self.start_response))
        samples = self.pipeline_manager.pipelines[0].samples[0]
        self.assertEqual("account", samples.resource_id)

########NEW FILE########
__FILENAME__ = test_notifications
# Author: Swann Croiset <swann.croiset@bull.net>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import datetime

import mock
from oslo.config import cfg

from ceilometer.openstack.common import test
from ceilometer.orchestration import notifications
from ceilometer import sample

NOW = datetime.datetime.isoformat(datetime.datetime.utcnow())

TENANT_ID = u'4c35985848bf4419b3f3d52c22e5792d'
STACK_NAME = u'AS1-ASGroup-53sqbo7sor7i'
STACK_ID = u'cb4a6fd1-1f5d-4002-ae91-9b91573cfb03'
USER_NAME = u'demo'
USER_ID = u'2e61f25ec63a4f6c954a6245421448a4'
TRUSTOR_ID = u'foo-Trustor-Id'

STACK_ARN = u'arn:openstack:heat::%s:stacks/%s/%s' % (TENANT_ID,
                                                      STACK_NAME,
                                                      STACK_ID)


CONF = cfg.CONF
CONF.set_override('use_stderr', True)

from ceilometer.openstack.common import log
LOG = log.getLogger(__name__)


def stack_notification_for(operation, use_trust=None):

    if use_trust:
        trust_id = 'footrust'
        trustor_id = TRUSTOR_ID
    else:
        trust_id = None
        trustor_id = None

    return {
        u'event_type': '%s.stack.%s.end' % (notifications.SERVICE,
                                            operation),
        u'_context_roles': [
            u'Member',
        ],
        u'_context_request_id': u'req-cf24cf30-af35-4a47-ae29-e74d75ebc6de',
        u'_context_auth_url': u'http://0.1.0.1:1010/v2.0',
        u'timestamp': NOW,
        u'_unique_id': u'1afb4283660f410c802af4d5992a39f2',
        u'_context_tenant_id': TENANT_ID,
        u'payload': {
            u'state_reason': u'Stack create completed successfully',
            u'user_id': USER_NAME,
            u'stack_identity': STACK_ARN,
            u'stack_name': STACK_NAME,
            u'tenant_id': TENANT_ID,
            u'create_at': u'2014-01-27T13:13:19Z',
            u'state': u'CREATE_COMPLETE'
        },
        u'_context_username': USER_NAME,
        u'_context_auth_token': u'MIISAwYJKoZIhvcNAQcCoII...',
        u'_context_password': u'password',
        u'_context_user_id': USER_ID,
        u'_context_trustor_user_id': trustor_id,
        u'_context_aws_creds': None,
        u'_context_show_deleted': False,
        u'_context_tenant': USER_NAME,
        u'_context_trust_id': trust_id,
        u'priority': u'INFO',
        u'_context_is_admin': False,
        u'_context_user': USER_NAME,
        u'publisher_id': u'orchestration.node-n5x66lxdy67d',
        u'message_id': u'ef921faa-7f7b-4854-8b86-a424ab93c96e',
    }


class TestNotification(test.BaseTestCase):

    def _verify_common_sample(self, s, name, volume):
        self.assertIsNotNone(s)
        self.assertEqual(s.name, 'stack.%s' % name)
        self.assertEqual(s.timestamp, NOW)
        self.assertEqual(s.type, sample.TYPE_DELTA)
        self.assertEqual(s.project_id, TENANT_ID)
        self.assertEqual(s.resource_id, STACK_ARN)
        metadata = s.resource_metadata
        self.assertEqual(metadata.get('host'),
                         u'orchestration.node-n5x66lxdy67d')

    def _test_operation(self, operation, trust=None):
        notif = stack_notification_for(operation, trust)
        handler = notifications.StackCRUD(mock.Mock())
        data = list(handler.process_notification(notif))
        self.assertEqual(len(data), 1)
        if trust:
            self.assertEqual(data[0].user_id, TRUSTOR_ID)
        else:
            self.assertEqual(data[0].user_id, USER_ID)
        self._verify_common_sample(data[0], operation, 1)

    def test_create(self):
        self._test_operation('create')

    def test_create_trust(self):
        self._test_operation('create', trust=True)

    def test_update(self):
        self._test_operation('update')

    def test_delete(self):
        self._test_operation('delete')

    def test_resume(self):
        self._test_operation('resume')

    def test_suspend(self):
        self._test_operation('suspend')

########NEW FILE########
__FILENAME__ = pipeline_base
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Intel Corp.
#
# Authors: Yunhong Jiang <yunhong.jiang@intel.com>
#          Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import abc
import datetime

import mock
import six
from stevedore import extension

from ceilometer.openstack.common.fixture import mockpatch
from ceilometer.openstack.common import test
from ceilometer.openstack.common import timeutils
from ceilometer import pipeline
from ceilometer import publisher
from ceilometer.publisher import test as test_publisher
from ceilometer import sample
from ceilometer import transformer
from ceilometer.transformer import accumulator
from ceilometer.transformer import conversions


@six.add_metaclass(abc.ABCMeta)
class BasePipelineTestCase(test.BaseTestCase):
    def fake_tem_init(self):
        """Fake a transformerManager for pipeline
           The faked entry point setting is below:
           update: TransformerClass
           except: TransformerClassException
           drop:   TransformerClassDrop
        """
        pass

    def fake_tem_get_ext(self, name):
        class_name_ext = {
            'update': self.TransformerClass,
            'except': self.TransformerClassException,
            'drop': self.TransformerClassDrop,
            'cache': accumulator.TransformerAccumulator,
            'aggregator': conversions.AggregatorTransformer,
            'unit_conversion': conversions.ScalingTransformer,
            'rate_of_change': conversions.RateOfChangeTransformer,
        }

        if name in class_name_ext:
            return extension.Extension(name, None,
                                       class_name_ext[name],
                                       None,
                                       )

        raise KeyError(name)

    def get_publisher(self, url, namespace=''):
        fake_drivers = {'test://': test_publisher.TestPublisher,
                        'new://': test_publisher.TestPublisher,
                        'except://': self.PublisherClassException}
        return fake_drivers[url](url)

    class PublisherClassException(publisher.PublisherBase):
        def publish_samples(self, ctxt, counters):
            raise Exception()

    class TransformerClass(transformer.TransformerBase):
        samples = []

        def __init__(self, append_name='_update'):
            self.__class__.samples = []
            self.append_name = append_name

        def flush(self, ctxt):
            return []

        def handle_sample(self, ctxt, counter):
            self.__class__.samples.append(counter)
            newname = getattr(counter, 'name') + self.append_name
            return sample.Sample(
                name=newname,
                type=counter.type,
                volume=counter.volume,
                unit=counter.unit,
                user_id=counter.user_id,
                project_id=counter.project_id,
                resource_id=counter.resource_id,
                timestamp=counter.timestamp,
                resource_metadata=counter.resource_metadata,
            )

    class TransformerClassDrop(transformer.TransformerBase):
        samples = []

        def __init__(self):
            self.__class__.samples = []

        def handle_sample(self, ctxt, counter):
            self.__class__.samples.append(counter)

    class TransformerClassException(object):
        def handle_sample(self, ctxt, counter):
            raise Exception()

    def setUp(self):
        super(BasePipelineTestCase, self).setUp()

        self.test_counter = sample.Sample(
            name='a',
            type=sample.TYPE_GAUGE,
            volume=1,
            unit='B',
            user_id="test_user",
            project_id="test_proj",
            resource_id="test_resource",
            timestamp=timeutils.utcnow().isoformat(),
            resource_metadata={}
        )

        self.useFixture(mockpatch.PatchObject(
            transformer.TransformerExtensionManager, "__init__",
            side_effect=self.fake_tem_init))

        self.useFixture(mockpatch.PatchObject(
            transformer.TransformerExtensionManager, "get_ext",
            side_effect=self.fake_tem_get_ext))

        self.useFixture(mockpatch.PatchObject(
            publisher, 'get_publisher', side_effect=self.get_publisher))

        self.transformer_manager = transformer.TransformerExtensionManager()

        self._setup_pipeline_cfg()

    @abc.abstractmethod
    def _setup_pipeline_cfg(self):
        """Setup the appropriate form of pipeline config."""

    @abc.abstractmethod
    def _augment_pipeline_cfg(self):
        """Augment the pipeline config with an additional element."""

    @abc.abstractmethod
    def _break_pipeline_cfg(self):
        """Break the pipeline config with a malformed element."""

    @abc.abstractmethod
    def _set_pipeline_cfg(self, field, value):
        """Set a field to a value in the pipeline config."""

    @abc.abstractmethod
    def _extend_pipeline_cfg(self, field, value):
        """Extend an existing field in the pipeline config with a value."""

    @abc.abstractmethod
    def _unset_pipeline_cfg(self, field):
        """Clear an existing field in the pipeline config."""

    def _exception_create_pipelinemanager(self):
        self.assertRaises(pipeline.PipelineException,
                          pipeline.PipelineManager,
                          self.pipeline_cfg,
                          self.transformer_manager)

    def test_no_counters(self):
        self._unset_pipeline_cfg('counters')
        self._exception_create_pipelinemanager()

    def test_no_transformers(self):
        self._unset_pipeline_cfg('transformers')
        self._exception_create_pipelinemanager()

    def test_no_name(self):
        self._unset_pipeline_cfg('name')
        self._exception_create_pipelinemanager()

    def test_no_interval(self):
        self._unset_pipeline_cfg('interval')
        self._exception_create_pipelinemanager()

    def test_no_publishers(self):
        self._unset_pipeline_cfg('publishers')
        self._exception_create_pipelinemanager()

    def test_invalid_resources(self):
        invalid_resource = {'invalid': 1}
        self._set_pipeline_cfg('resources', invalid_resource)
        self._exception_create_pipelinemanager()

    def test_check_counters_include_exclude_same(self):
        counter_cfg = ['a', '!a']
        self._set_pipeline_cfg('counters', counter_cfg)
        self._exception_create_pipelinemanager()

    def test_check_counters_include_exclude(self):
        counter_cfg = ['a', '!b']
        self._set_pipeline_cfg('counters', counter_cfg)
        self._exception_create_pipelinemanager()

    def test_check_counters_wildcard_included(self):
        counter_cfg = ['a', '*']
        self._set_pipeline_cfg('counters', counter_cfg)
        self._exception_create_pipelinemanager()

    def test_check_publishers_invalid_publisher(self):
        publisher_cfg = ['test_invalid']
        self._set_pipeline_cfg('publishers', publisher_cfg)

    def test_invalid_string_interval(self):
        self._set_pipeline_cfg('interval', 'string')
        self._exception_create_pipelinemanager()

    def test_check_transformer_invalid_transformer(self):
        transformer_cfg = [
            {'name': "test_invalid",
             'parameters': {}}
        ]
        self._set_pipeline_cfg('transformers', transformer_cfg)
        self._exception_create_pipelinemanager()

    def test_get_interval(self):
        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)

        pipe = pipeline_manager.pipelines[0]
        self.assertEqual(5, pipe.get_interval())

    def test_publisher_transformer_invoked(self):
        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)

        with pipeline_manager.publisher(None) as p:
            p([self.test_counter])

        publisher = pipeline_manager.pipelines[0].publishers[0]
        self.assertEqual(1, len(publisher.samples))
        self.assertEqual(1, len(self.TransformerClass.samples))
        self.assertEqual('a_update', getattr(publisher.samples[0], "name"))
        self.assertEqual('a',
                         getattr(self.TransformerClass.samples[0], "name"))

    def test_multiple_included_counters(self):
        counter_cfg = ['a', 'b']
        self._set_pipeline_cfg('counters', counter_cfg)
        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)

        with pipeline_manager.publisher(None) as p:
            p([self.test_counter])

        publisher = pipeline_manager.pipelines[0].publishers[0]
        self.assertEqual(1, len(publisher.samples))

        self.test_counter = sample.Sample(
            name='b',
            type=self.test_counter.type,
            volume=self.test_counter.volume,
            unit=self.test_counter.unit,
            user_id=self.test_counter.user_id,
            project_id=self.test_counter.project_id,
            resource_id=self.test_counter.resource_id,
            timestamp=self.test_counter.timestamp,
            resource_metadata=self.test_counter.resource_metadata,
        )

        with pipeline_manager.publisher(None) as p:
            p([self.test_counter])

        self.assertEqual(2, len(publisher.samples))
        self.assertEqual(2, len(self.TransformerClass.samples))
        self.assertEqual('a_update', getattr(publisher.samples[0], "name"))
        self.assertEqual('b_update', getattr(publisher.samples[1], "name"))

    def test_counter_dont_match(self):
        counter_cfg = ['nomatch']
        self._set_pipeline_cfg('counters', counter_cfg)
        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        with pipeline_manager.publisher(None) as p:
            p([self.test_counter])

        publisher = pipeline_manager.pipelines[0].publishers[0]
        self.assertEqual(0, len(publisher.samples))
        self.assertEqual(0, publisher.calls)

    def test_wildcard_counter(self):
        counter_cfg = ['*']
        self._set_pipeline_cfg('counters', counter_cfg)
        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        with pipeline_manager.publisher(None) as p:
            p([self.test_counter])

        publisher = pipeline_manager.pipelines[0].publishers[0]
        self.assertEqual(1, len(publisher.samples))
        self.assertEqual(1, len(self.TransformerClass.samples))
        self.assertEqual('a_update', getattr(publisher.samples[0], "name"))

    def test_wildcard_excluded_counters(self):
        counter_cfg = ['*', '!a']
        self._set_pipeline_cfg('counters', counter_cfg)
        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        self.assertFalse(pipeline_manager.pipelines[0].support_meter('a'))

    def test_wildcard_excluded_counters_not_excluded(self):
        counter_cfg = ['*', '!b']
        self._set_pipeline_cfg('counters', counter_cfg)
        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        with pipeline_manager.publisher(None) as p:
            p([self.test_counter])
        publisher = pipeline_manager.pipelines[0].publishers[0]
        self.assertEqual(1, len(publisher.samples))
        self.assertEqual(1, len(self.TransformerClass.samples))
        self.assertEqual('a_update', getattr(publisher.samples[0], "name"))

    def test_all_excluded_counters_not_excluded(self):
        counter_cfg = ['!b', '!c']
        self._set_pipeline_cfg('counters', counter_cfg)
        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        with pipeline_manager.publisher(None) as p:
            p([self.test_counter])

        publisher = pipeline_manager.pipelines[0].publishers[0]
        self.assertEqual(1, len(publisher.samples))
        self.assertEqual(1, len(self.TransformerClass.samples))
        self.assertEqual('a_update', getattr(publisher.samples[0], "name"))
        self.assertEqual('a',
                         getattr(self.TransformerClass.samples[0], "name"))

    def test_all_excluded_counters_is_excluded(self):
        counter_cfg = ['!a', '!c']
        self._set_pipeline_cfg('counters', counter_cfg)
        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        self.assertFalse(pipeline_manager.pipelines[0].support_meter('a'))
        self.assertTrue(pipeline_manager.pipelines[0].support_meter('b'))
        self.assertFalse(pipeline_manager.pipelines[0].support_meter('c'))

    def test_wildcard_and_excluded_wildcard_counters(self):
        counter_cfg = ['*', '!disk.*']
        self._set_pipeline_cfg('counters', counter_cfg)
        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        self.assertFalse(pipeline_manager.pipelines[0].
                         support_meter('disk.read.bytes'))
        self.assertTrue(pipeline_manager.pipelines[0].support_meter('cpu'))

    def test_included_counter_and_wildcard_counters(self):
        counter_cfg = ['cpu', 'disk.*']
        self._set_pipeline_cfg('counters', counter_cfg)
        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        self.assertTrue(pipeline_manager.pipelines[0].
                        support_meter('disk.read.bytes'))
        self.assertTrue(pipeline_manager.pipelines[0].support_meter('cpu'))
        self.assertFalse(pipeline_manager.pipelines[0].
                         support_meter('instance'))

    def test_excluded_counter_and_excluded_wildcard_counters(self):
        counter_cfg = ['!cpu', '!disk.*']
        self._set_pipeline_cfg('counters', counter_cfg)
        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        self.assertFalse(pipeline_manager.pipelines[0].
                         support_meter('disk.read.bytes'))
        self.assertFalse(pipeline_manager.pipelines[0].support_meter('cpu'))
        self.assertTrue(pipeline_manager.pipelines[0].
                        support_meter('instance'))

    def test_multiple_pipeline(self):
        self._augment_pipeline_cfg()

        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        with pipeline_manager.publisher(None) as p:
            p([self.test_counter])

        self.test_counter = sample.Sample(
            name='b',
            type=self.test_counter.type,
            volume=self.test_counter.volume,
            unit=self.test_counter.unit,
            user_id=self.test_counter.user_id,
            project_id=self.test_counter.project_id,
            resource_id=self.test_counter.resource_id,
            timestamp=self.test_counter.timestamp,
            resource_metadata=self.test_counter.resource_metadata,
        )

        with pipeline_manager.publisher(None) as p:
            p([self.test_counter])

        publisher = pipeline_manager.pipelines[0].publishers[0]
        self.assertEqual(1, len(publisher.samples))
        self.assertEqual(1, publisher.calls)
        self.assertEqual('a_update', getattr(publisher.samples[0], "name"))
        new_publisher = pipeline_manager.pipelines[1].publishers[0]
        self.assertEqual(1, len(new_publisher.samples))
        self.assertEqual(1, new_publisher.calls)
        self.assertEqual('b_new', getattr(new_publisher.samples[0], "name"))
        self.assertEqual(2, len(self.TransformerClass.samples))
        self.assertEqual('a',
                         getattr(self.TransformerClass.samples[0], "name"))
        self.assertEqual('b',
                         getattr(self.TransformerClass.samples[1], "name"))

    def test_multiple_pipeline_exception(self):
        self._break_pipeline_cfg()
        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)

        with pipeline_manager.publisher(None) as p:
            p([self.test_counter])

        self.test_counter = sample.Sample(
            name='b',
            type=self.test_counter.type,
            volume=self.test_counter.volume,
            unit=self.test_counter.unit,
            user_id=self.test_counter.user_id,
            project_id=self.test_counter.project_id,
            resource_id=self.test_counter.resource_id,
            timestamp=self.test_counter.timestamp,
            resource_metadata=self.test_counter.resource_metadata,
        )

        with pipeline_manager.publisher(None) as p:
            p([self.test_counter])

        publisher = pipeline_manager.pipelines[0].publishers[0]
        self.assertEqual(1, publisher.calls)
        self.assertEqual(1, len(publisher.samples))
        self.assertEqual('a_update', getattr(publisher.samples[0], "name"))
        self.assertEqual(2, len(self.TransformerClass.samples))
        self.assertEqual('a',
                         getattr(self.TransformerClass.samples[0], "name"))
        self.assertEqual('b',
                         getattr(self.TransformerClass.samples[1], "name"))

    def test_none_transformer_pipeline(self):
        self._set_pipeline_cfg('transformers', None)
        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        with pipeline_manager.publisher(None) as p:
            p([self.test_counter])
        publisher = pipeline_manager.pipelines[0].publishers[0]
        self.assertEqual(1, len(publisher.samples))
        self.assertEqual(1, publisher.calls)
        self.assertEqual('a', getattr(publisher.samples[0], 'name'))

    def test_empty_transformer_pipeline(self):
        self._set_pipeline_cfg('transformers', [])
        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        with pipeline_manager.publisher(None) as p:
            p([self.test_counter])
        publisher = pipeline_manager.pipelines[0].publishers[0]
        self.assertEqual(1, len(publisher.samples))
        self.assertEqual(1, publisher.calls)
        self.assertEqual('a', getattr(publisher.samples[0], 'name'))

    def test_multiple_transformer_same_class(self):
        transformer_cfg = [
            {
                'name': 'update',
                'parameters': {}
            },
            {
                'name': 'update',
                'parameters': {}
            },
        ]
        self._set_pipeline_cfg('transformers', transformer_cfg)
        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)

        with pipeline_manager.publisher(None) as p:
            p([self.test_counter])

        publisher = pipeline_manager.pipelines[0].publishers[0]
        self.assertEqual(1, publisher.calls)
        self.assertEqual(1, len(publisher.samples))
        self.assertEqual('a_update_update',
                         getattr(publisher.samples[0], 'name'))
        self.assertEqual(2, len(self.TransformerClass.samples))
        self.assertEqual('a',
                         getattr(self.TransformerClass.samples[0], 'name'))
        self.assertEqual('a_update',
                         getattr(self.TransformerClass.samples[1], 'name'))

    def test_multiple_transformer_same_class_different_parameter(self):
        transformer_cfg = [
            {
                'name': 'update',
                'parameters':
                {
                    "append_name": "_update",
                }
            },
            {
                'name': 'update',
                'parameters':
                {
                    "append_name": "_new",
                }
            },
        ]
        self._set_pipeline_cfg('transformers', transformer_cfg)
        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        with pipeline_manager.publisher(None) as p:
            p([self.test_counter])

        self.assertEqual(2, len(self.TransformerClass.samples))
        self.assertEqual('a',
                         getattr(self.TransformerClass.samples[0], 'name'))
        self.assertEqual('a_update',
                         getattr(self.TransformerClass.samples[1], 'name'))
        publisher = pipeline_manager.pipelines[0].publishers[0]
        self.assertEqual(1,
                         len(publisher.samples))
        self.assertEqual('a_update_new',
                         getattr(publisher.samples[0], 'name'))

    def test_multiple_transformer_drop_transformer(self):
        transformer_cfg = [
            {
                'name': 'update',
                'parameters':
                {
                    "append_name": "_update",
                }
            },
            {
                'name': 'drop',
                'parameters': {}
            },
            {
                'name': 'update',
                'parameters':
                {
                    "append_name": "_new",
                }
            },
        ]
        self._set_pipeline_cfg('transformers', transformer_cfg)
        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        with pipeline_manager.publisher(None) as p:
            p([self.test_counter])

        publisher = pipeline_manager.pipelines[0].publishers[0]
        self.assertEqual(0, len(publisher.samples))
        self.assertEqual(1, len(self.TransformerClass.samples))
        self.assertEqual('a',
                         getattr(self.TransformerClass.samples[0], 'name'))
        self.assertEqual(1,
                         len(self.TransformerClassDrop.samples))
        self.assertEqual('a_update',
                         getattr(self.TransformerClassDrop.samples[0], 'name'))

    def test_multiple_publisher(self):
        self._set_pipeline_cfg('publishers', ['test://', 'new://'])
        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)

        with pipeline_manager.publisher(None) as p:
            p([self.test_counter])

        publisher = pipeline_manager.pipelines[0].publishers[0]
        new_publisher = pipeline_manager.pipelines[0].publishers[1]
        self.assertEqual(1, len(publisher.samples))
        self.assertEqual(1, len(new_publisher.samples))
        self.assertEqual('a_update',
                         getattr(new_publisher.samples[0], 'name'))
        self.assertEqual('a_update',
                         getattr(publisher.samples[0], 'name'))

    def test_multiple_publisher_isolation(self):
        self._set_pipeline_cfg('publishers', ['except://', 'new://'])
        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        with pipeline_manager.publisher(None) as p:
            p([self.test_counter])

        new_publisher = pipeline_manager.pipelines[0].publishers[1]
        self.assertEqual(1, len(new_publisher.samples))
        self.assertEqual('a_update',
                         getattr(new_publisher.samples[0], 'name'))

    def test_multiple_counter_pipeline(self):
        self._set_pipeline_cfg('counters', ['a', 'b'])
        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        with pipeline_manager.publisher(None) as p:
            p([self.test_counter,
               sample.Sample(
                   name='b',
                   type=self.test_counter.type,
                   volume=self.test_counter.volume,
                   unit=self.test_counter.unit,
                   user_id=self.test_counter.user_id,
                   project_id=self.test_counter.project_id,
                   resource_id=self.test_counter.resource_id,
                   timestamp=self.test_counter.timestamp,
                   resource_metadata=self.test_counter.resource_metadata,
               )])

        publisher = pipeline_manager.pipelines[0].publishers[0]
        self.assertEqual(2, len(publisher.samples))
        self.assertEqual('a_update', getattr(publisher.samples[0], 'name'))
        self.assertEqual('b_update', getattr(publisher.samples[1], 'name'))

    def test_flush_pipeline_cache(self):
        CACHE_SIZE = 10
        extra_transformer_cfg = [
            {
                'name': 'cache',
                'parameters': {
                    'size': CACHE_SIZE,
                }
            },
            {
                'name': 'update',
                'parameters':
                {
                    'append_name': '_new'
                }
            },
        ]
        self._extend_pipeline_cfg('transformers', extra_transformer_cfg)
        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        pipe = pipeline_manager.pipelines[0]

        pipe.publish_sample(None, self.test_counter)
        publisher = pipeline_manager.pipelines[0].publishers[0]
        self.assertEqual(0, len(publisher.samples))
        pipe.flush(None)
        self.assertEqual(0, len(publisher.samples))
        pipe.publish_sample(None, self.test_counter)
        pipe.flush(None)
        self.assertEqual(0, len(publisher.samples))
        for i in range(CACHE_SIZE - 2):
            pipe.publish_sample(None, self.test_counter)
        pipe.flush(None)
        self.assertEqual(CACHE_SIZE, len(publisher.samples))
        self.assertEqual('a_update_new', getattr(publisher.samples[0], 'name'))

    def test_flush_pipeline_cache_multiple_counter(self):
        CACHE_SIZE = 3
        extra_transformer_cfg = [
            {
                'name': 'cache',
                'parameters': {
                    'size': CACHE_SIZE
                }
            },
            {
                'name': 'update',
                'parameters':
                {
                    'append_name': '_new'
                }
            },
        ]
        self._extend_pipeline_cfg('transformers', extra_transformer_cfg)
        self._set_pipeline_cfg('counters', ['a', 'b'])
        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        with pipeline_manager.publisher(None) as p:
            p([self.test_counter,
               sample.Sample(
                   name='b',
                   type=self.test_counter.type,
                   volume=self.test_counter.volume,
                   unit=self.test_counter.unit,
                   user_id=self.test_counter.user_id,
                   project_id=self.test_counter.project_id,
                   resource_id=self.test_counter.resource_id,
                   timestamp=self.test_counter.timestamp,
                   resource_metadata=self.test_counter.resource_metadata,
               )])

        publisher = pipeline_manager.pipelines[0].publishers[0]
        self.assertEqual(0, len(publisher.samples))

        with pipeline_manager.publisher(None) as p:
            p([self.test_counter])

        self.assertEqual(CACHE_SIZE, len(publisher.samples))
        self.assertEqual('a_update_new',
                         getattr(publisher.samples[0], 'name'))
        self.assertEqual('b_update_new',
                         getattr(publisher.samples[1], 'name'))

    def test_flush_pipeline_cache_before_publisher(self):
        extra_transformer_cfg = [{
            'name': 'cache',
            'parameters': {}
        }]
        self._extend_pipeline_cfg('transformers', extra_transformer_cfg)
        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        pipe = pipeline_manager.pipelines[0]

        publisher = pipe.publishers[0]
        pipe.publish_sample(None, self.test_counter)
        self.assertEqual(0, len(publisher.samples))
        pipe.flush(None)
        self.assertEqual(1, len(publisher.samples))
        self.assertEqual('a_update',
                         getattr(publisher.samples[0], 'name'))

    def test_variable_counter(self):
        self.pipeline_cfg = [{
            'name': "test_pipeline",
            'interval': 5,
            'counters': ['a:*'],
            'transformers': [
                {'name': "update",
                 'parameters': {}}
            ],
            'publishers': ["test://"],
        }, ]
        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)

        self.test_counter = sample.Sample(
            name='a:b',
            type=self.test_counter.type,
            volume=self.test_counter.volume,
            unit=self.test_counter.unit,
            user_id=self.test_counter.user_id,
            project_id=self.test_counter.project_id,
            resource_id=self.test_counter.resource_id,
            timestamp=self.test_counter.timestamp,
            resource_metadata=self.test_counter.resource_metadata,
        )

        with pipeline_manager.publisher(None) as p:
            p([self.test_counter])

        publisher = pipeline_manager.pipelines[0].publishers[0]
        self.assertEqual(1, len(publisher.samples))
        self.assertEqual(1, len(self.TransformerClass.samples))
        self.assertEqual('a:b_update',
                         getattr(publisher.samples[0], "name"))
        self.assertEqual('a:b',
                         getattr(self.TransformerClass.samples[0], "name"))

    def test_global_unit_conversion(self):
        scale = 'volume / ((10**6) * 60)'
        transformer_cfg = [
            {
                'name': 'unit_conversion',
                'parameters': {
                    'source': {},
                    'target': {'name': 'cpu_mins',
                               'unit': 'min',
                               'scale': scale},
                }
            },
        ]
        self._set_pipeline_cfg('transformers', transformer_cfg)
        self._set_pipeline_cfg('counters', ['cpu'])
        counters = [
            sample.Sample(
                name='cpu',
                type=sample.TYPE_CUMULATIVE,
                volume=1200000000,
                unit='ns',
                user_id='test_user',
                project_id='test_proj',
                resource_id='test_resource',
                timestamp=timeutils.utcnow().isoformat(),
                resource_metadata={}
            ),
        ]

        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        pipe = pipeline_manager.pipelines[0]

        pipe.publish_samples(None, counters)
        publisher = pipeline_manager.pipelines[0].publishers[0]
        self.assertEqual(1, len(publisher.samples))
        pipe.flush(None)
        self.assertEqual(1, len(publisher.samples))
        cpu_mins = publisher.samples[-1]
        self.assertEqual('cpu_mins', getattr(cpu_mins, 'name'))
        self.assertEqual('min', getattr(cpu_mins, 'unit'))
        self.assertEqual(sample.TYPE_CUMULATIVE, getattr(cpu_mins, 'type'))
        self.assertEqual(20, getattr(cpu_mins, 'volume'))

    def test_unit_identified_source_unit_conversion(self):
        transformer_cfg = [
            {
                'name': 'unit_conversion',
                'parameters': {
                    'source': {'unit': '°C'},
                    'target': {'unit': '°F',
                               'scale': '(volume * 1.8) + 32'},
                }
            },
        ]
        self._set_pipeline_cfg('transformers', transformer_cfg)
        self._set_pipeline_cfg('counters', ['core_temperature',
                                            'ambient_temperature'])
        counters = [
            sample.Sample(
                name='core_temperature',
                type=sample.TYPE_GAUGE,
                volume=36.0,
                unit='°C',
                user_id='test_user',
                project_id='test_proj',
                resource_id='test_resource',
                timestamp=timeutils.utcnow().isoformat(),
                resource_metadata={}
            ),
            sample.Sample(
                name='ambient_temperature',
                type=sample.TYPE_GAUGE,
                volume=88.8,
                unit='°F',
                user_id='test_user',
                project_id='test_proj',
                resource_id='test_resource',
                timestamp=timeutils.utcnow().isoformat(),
                resource_metadata={}
            ),
        ]

        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        pipe = pipeline_manager.pipelines[0]

        pipe.publish_samples(None, counters)
        publisher = pipeline_manager.pipelines[0].publishers[0]
        self.assertEqual(2, len(publisher.samples))
        core_temp = publisher.samples[1]
        self.assertEqual('core_temperature', getattr(core_temp, 'name'))
        self.assertEqual('°F', getattr(core_temp, 'unit'))
        self.assertEqual(96.8, getattr(core_temp, 'volume'))
        amb_temp = publisher.samples[0]
        self.assertEqual('ambient_temperature', getattr(amb_temp, 'name'))
        self.assertEqual('°F', getattr(amb_temp, 'unit'))
        self.assertEqual(88.8, getattr(amb_temp, 'volume'))
        self.assertEqual(96.8, getattr(core_temp, 'volume'))

    def _do_test_rate_of_change_conversion(self, prev, curr, type, expected,
                                           offset=1, weight=None):
        s = "(resource_metadata.user_metadata.autoscaling_weight or 1.0)" \
            "* (resource_metadata.non.existent or 1.0)" \
            "* (100.0 / (10**9 * (resource_metadata.cpu_number or 1)))"
        transformer_cfg = [
            {
                'name': 'rate_of_change',
                'parameters': {
                    'source': {},
                    'target': {'name': 'cpu_util',
                               'unit': '%',
                               'type': sample.TYPE_GAUGE,
                               'scale': s},
                }
            },
        ]
        self._set_pipeline_cfg('transformers', transformer_cfg)
        self._set_pipeline_cfg('counters', ['cpu'])
        now = timeutils.utcnow()
        later = now + datetime.timedelta(minutes=offset)
        um = {'autoscaling_weight': weight} if weight else {}
        counters = [
            sample.Sample(
                name='cpu',
                type=type,
                volume=prev,
                unit='ns',
                user_id='test_user',
                project_id='test_proj',
                resource_id='test_resource',
                timestamp=now.isoformat(),
                resource_metadata={'cpu_number': 4,
                                   'user_metadata': um},
            ),
            sample.Sample(
                name='cpu',
                type=type,
                volume=prev,
                unit='ns',
                user_id='test_user',
                project_id='test_proj',
                resource_id='test_resource2',
                timestamp=now.isoformat(),
                resource_metadata={'cpu_number': 2,
                                   'user_metadata': um},
            ),
            sample.Sample(
                name='cpu',
                type=type,
                volume=curr,
                unit='ns',
                user_id='test_user',
                project_id='test_proj',
                resource_id='test_resource',
                timestamp=later.isoformat(),
                resource_metadata={'cpu_number': 4,
                                   'user_metadata': um},
            ),
            sample.Sample(
                name='cpu',
                type=type,
                volume=curr,
                unit='ns',
                user_id='test_user',
                project_id='test_proj',
                resource_id='test_resource2',
                timestamp=later.isoformat(),
                resource_metadata={'cpu_number': 2,
                                   'user_metadata': um},
            ),
        ]

        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        pipe = pipeline_manager.pipelines[0]

        pipe.publish_samples(None, counters)
        publisher = pipeline_manager.pipelines[0].publishers[0]
        self.assertEqual(2, len(publisher.samples))
        pipe.flush(None)
        self.assertEqual(2, len(publisher.samples))
        cpu_util = publisher.samples[0]
        self.assertEqual('cpu_util', getattr(cpu_util, 'name'))
        self.assertEqual('test_resource', getattr(cpu_util, 'resource_id'))
        self.assertEqual('%', getattr(cpu_util, 'unit'))
        self.assertEqual(sample.TYPE_GAUGE, getattr(cpu_util, 'type'))
        self.assertEqual(expected, getattr(cpu_util, 'volume'))
        cpu_util = publisher.samples[1]
        self.assertEqual('cpu_util', getattr(cpu_util, 'name'))
        self.assertEqual('test_resource2', getattr(cpu_util, 'resource_id'))
        self.assertEqual('%', getattr(cpu_util, 'unit'))
        self.assertEqual(sample.TYPE_GAUGE, getattr(cpu_util, 'type'))
        self.assertEqual(expected * 2, getattr(cpu_util, 'volume'))

    def test_rate_of_change_conversion(self):
        self._do_test_rate_of_change_conversion(120000000000,
                                                180000000000,
                                                sample.TYPE_CUMULATIVE,
                                                25.0)

    def test_rate_of_change_conversion_weight(self):
        self._do_test_rate_of_change_conversion(120000000000,
                                                180000000000,
                                                sample.TYPE_CUMULATIVE,
                                                27.5,
                                                weight=1.1)

    def test_rate_of_change_conversion_negative_cumulative_delta(self):
        self._do_test_rate_of_change_conversion(180000000000,
                                                120000000000,
                                                sample.TYPE_CUMULATIVE,
                                                50.0)

    def test_rate_of_change_conversion_negative_gauge_delta(self):
        self._do_test_rate_of_change_conversion(180000000000,
                                                120000000000,
                                                sample.TYPE_GAUGE,
                                                -25.0)

    def test_rate_of_change_conversion_zero_delay(self):
        self._do_test_rate_of_change_conversion(120000000000,
                                                120000000000,
                                                sample.TYPE_CUMULATIVE,
                                                0.0,
                                                offset=0)

    def test_rate_of_change_no_predecessor(self):
        s = "100.0 / (10**9 * resource_metadata.get('cpu_number', 1))"
        transformer_cfg = [
            {
                'name': 'rate_of_change',
                'parameters': {
                    'source': {},
                    'target': {'name': 'cpu_util',
                               'unit': '%',
                               'type': sample.TYPE_GAUGE,
                               'scale': s}
                }
            },
        ]
        self._set_pipeline_cfg('transformers', transformer_cfg)
        self._set_pipeline_cfg('counters', ['cpu'])
        now = timeutils.utcnow()
        counters = [
            sample.Sample(
                name='cpu',
                type=sample.TYPE_CUMULATIVE,
                volume=120000000000,
                unit='ns',
                user_id='test_user',
                project_id='test_proj',
                resource_id='test_resource',
                timestamp=now.isoformat(),
                resource_metadata={'cpu_number': 4}
            ),
        ]

        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        pipe = pipeline_manager.pipelines[0]

        pipe.publish_samples(None, counters)
        publisher = pipeline_manager.pipelines[0].publishers[0]
        self.assertEqual(0, len(publisher.samples))
        pipe.flush(None)
        self.assertEqual(0, len(publisher.samples))

    def test_resources(self):
        resources = ['test1://', 'test2://']
        self._set_pipeline_cfg('resources', resources)
        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        self.assertEqual(resources,
                         pipeline_manager.pipelines[0].resources)

    def test_no_resources(self):
        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        self.assertEqual(0, len(pipeline_manager.pipelines[0].resources))

    def _do_test_rate_of_change_mapping(self, pipe, meters, units):
        now = timeutils.utcnow()
        base = 1000
        offset = 7
        rate = 42
        later = now + datetime.timedelta(minutes=offset)
        counters = []
        for v, ts in [(base, now.isoformat()),
                      (base + (offset * 60 * rate), later.isoformat())]:
            for n, u, r in [(meters[0], units[0], 'resource1'),
                            (meters[1], units[1], 'resource2')]:
                s = sample.Sample(
                    name=n,
                    type=sample.TYPE_CUMULATIVE,
                    volume=v,
                    unit=u,
                    user_id='test_user',
                    project_id='test_proj',
                    resource_id=r,
                    timestamp=ts,
                    resource_metadata={},
                )
                counters.append(s)

        pipe.publish_samples(None, counters)
        publisher = pipe.publishers[0]
        self.assertEqual(2, len(publisher.samples))
        pipe.flush(None)
        self.assertEqual(2, len(publisher.samples))
        bps = publisher.samples[0]
        self.assertEqual('%s.rate' % meters[0], getattr(bps, 'name'))
        self.assertEqual('resource1', getattr(bps, 'resource_id'))
        self.assertEqual('%s/s' % units[0], getattr(bps, 'unit'))
        self.assertEqual(sample.TYPE_GAUGE, getattr(bps, 'type'))
        self.assertEqual(rate, getattr(bps, 'volume'))
        rps = publisher.samples[1]
        self.assertEqual('%s.rate' % meters[1], getattr(rps, 'name'))
        self.assertEqual('resource2', getattr(rps, 'resource_id'))
        self.assertEqual('%s/s' % units[1], getattr(rps, 'unit'))
        self.assertEqual(sample.TYPE_GAUGE, getattr(rps, 'type'))
        self.assertEqual(rate, getattr(rps, 'volume'))

    def test_rate_of_change_mapping(self):
        map_from = {'name': 'disk\\.(read|write)\\.(bytes|requests)',
                    'unit': '(B|request)'}
        map_to = {'name': 'disk.\\1.\\2.rate',
                  'unit': '\\1/s'}
        transformer_cfg = [
            {
                'name': 'rate_of_change',
                'parameters': {
                    'source': {
                        'map_from': map_from
                    },
                    'target': {
                        'map_to': map_to,
                        'type': sample.TYPE_GAUGE
                    },
                },
            },
        ]
        self._set_pipeline_cfg('transformers', transformer_cfg)
        self._set_pipeline_cfg('counters', ['disk.read.bytes',
                                            'disk.write.requests'])
        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        pipe = pipeline_manager.pipelines[0]
        meters = ('disk.read.bytes', 'disk.write.requests')
        units = ('B', 'request')
        self._do_test_rate_of_change_mapping(pipe, meters, units)

    def _do_test_aggregator(self, parameters, expected_length):
        transformer_cfg = [
            {
                'name': 'aggregator',
                'parameters': parameters,
            },
        ]
        self._set_pipeline_cfg('transformers', transformer_cfg)
        self._set_pipeline_cfg('counters', ['storage.objects.incoming.bytes'])
        counters = [
            sample.Sample(
                name='storage.objects.incoming.bytes',
                type=sample.TYPE_DELTA,
                volume=26,
                unit='B',
                user_id='test_user',
                project_id='test_proj',
                resource_id='test_resource',
                timestamp=timeutils.utcnow().isoformat(),
                resource_metadata={'version': '1.0'}
            ),
            sample.Sample(
                name='storage.objects.incoming.bytes',
                type=sample.TYPE_DELTA,
                volume=16,
                unit='B',
                user_id='test_user',
                project_id='test_proj',
                resource_id='test_resource',
                timestamp=timeutils.utcnow().isoformat(),
                resource_metadata={'version': '2.0'}
            ),
            sample.Sample(
                name='storage.objects.incoming.bytes',
                type=sample.TYPE_DELTA,
                volume=53,
                unit='B',
                user_id='test_user_bis',
                project_id='test_proj_bis',
                resource_id='test_resource',
                timestamp=timeutils.utcnow().isoformat(),
                resource_metadata={'version': '1.0'}
            ),
            sample.Sample(
                name='storage.objects.incoming.bytes',
                type=sample.TYPE_DELTA,
                volume=42,
                unit='B',
                user_id='test_user_bis',
                project_id='test_proj_bis',
                resource_id='test_resource',
                timestamp=timeutils.utcnow().isoformat(),
                resource_metadata={'version': '2.0'}
            ),
            sample.Sample(
                name='storage.objects.incoming.bytes',
                type=sample.TYPE_DELTA,
                volume=15,
                unit='B',
                user_id='test_user',
                project_id='test_proj_bis',
                resource_id='test_resource',
                timestamp=timeutils.utcnow().isoformat(),
                resource_metadata={'version': '2.0'}
            ),
            sample.Sample(
                name='storage.objects.incoming.bytes',
                type=sample.TYPE_DELTA,
                volume=2,
                unit='B',
                user_id='test_user_bis',
                project_id='test_proj',
                resource_id='test_resource',
                timestamp=timeutils.utcnow().isoformat(),
                resource_metadata={'version': '3.0'}
            ),
        ]

        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        pipe = pipeline_manager.pipelines[0]

        pipe.publish_samples(None, counters)
        pipe.flush(None)
        publisher = pipeline_manager.pipelines[0].publishers[0]
        self.assertEqual(expected_length, len(publisher.samples))
        return sorted(publisher.samples, key=lambda s: s.volume)

    def test_aggregator_metadata(self):
        for conf, expected_version in [('last', '2.0'), ('first', '1.0')]:
            samples = self._do_test_aggregator({
                'resource_metadata': conf,
                'target': {'name': 'aggregated-bytes'}
            }, expected_length=4)
            s = samples[0]
            self.assertEqual('aggregated-bytes', s.name)
            self.assertEqual(2, s.volume)
            self.assertEqual('test_user_bis', s.user_id)
            self.assertEqual('test_proj', s.project_id)
            self.assertEqual({'version': '3.0'},
                             s.resource_metadata)
            s = samples[1]
            self.assertEqual('aggregated-bytes', s.name)
            self.assertEqual(15, s.volume)
            self.assertEqual('test_user', s.user_id)
            self.assertEqual('test_proj_bis', s.project_id)
            self.assertEqual({'version': '2.0'},
                             s.resource_metadata)
            s = samples[2]
            self.assertEqual('aggregated-bytes', s.name)
            self.assertEqual(42, s.volume)
            self.assertEqual('test_user', s.user_id)
            self.assertEqual('test_proj', s.project_id)
            self.assertEqual({'version': expected_version},
                             s.resource_metadata)
            s = samples[3]
            self.assertEqual('aggregated-bytes', s.name)
            self.assertEqual(95, s.volume)
            self.assertEqual('test_user_bis', s.user_id)
            self.assertEqual('test_proj_bis', s.project_id)
            self.assertEqual({'version': expected_version},
                             s.resource_metadata)

    def test_aggregator_user_last_and_metadata_last(self):
        samples = self._do_test_aggregator({
            'resource_metadata': 'last',
            'user_id': 'last',
            'target': {'name': 'aggregated-bytes'}
        }, expected_length=2)
        s = samples[0]
        self.assertEqual('aggregated-bytes', s.name)
        self.assertEqual(44, s.volume)
        self.assertEqual('test_user_bis', s.user_id)
        self.assertEqual('test_proj', s.project_id)
        self.assertEqual({'version': '3.0'},
                         s.resource_metadata)
        s = samples[1]
        self.assertEqual('aggregated-bytes', s.name)
        self.assertEqual(110, s.volume)
        self.assertEqual('test_user', s.user_id)
        self.assertEqual('test_proj_bis', s.project_id)
        self.assertEqual({'version': '2.0'},
                         s.resource_metadata)

    def test_aggregator_user_first_and_metadata_last(self):
        samples = self._do_test_aggregator({
            'resource_metadata': 'last',
            'user_id': 'first',
            'target': {'name': 'aggregated-bytes'}
        }, expected_length=2)
        s = samples[0]
        self.assertEqual('aggregated-bytes', s.name)
        self.assertEqual(44, s.volume)
        self.assertEqual('test_user', s.user_id)
        self.assertEqual('test_proj', s.project_id)
        self.assertEqual({'version': '3.0'},
                         s.resource_metadata)
        s = samples[1]
        self.assertEqual('aggregated-bytes', s.name)
        self.assertEqual(110, s.volume)
        self.assertEqual('test_user_bis', s.user_id)
        self.assertEqual('test_proj_bis', s.project_id)
        self.assertEqual({'version': '2.0'},
                         s.resource_metadata)

    def test_aggregator_all_first(self):
        samples = self._do_test_aggregator({
            'resource_metadata': 'first',
            'user_id': 'first',
            'project_id': 'first',
            'target': {'name': 'aggregated-bytes'}
        }, expected_length=1)
        s = samples[0]
        self.assertEqual('aggregated-bytes', s.name)
        self.assertEqual(154, s.volume)
        self.assertEqual('test_user', s.user_id)
        self.assertEqual('test_proj', s.project_id)
        self.assertEqual({'version': '1.0'},
                         s.resource_metadata)

    def test_aggregator_all_last(self):
        samples = self._do_test_aggregator({
            'resource_metadata': 'last',
            'user_id': 'last',
            'project_id': 'last',
            'target': {'name': 'aggregated-bytes'}
        }, expected_length=1)
        s = samples[0]
        self.assertEqual('aggregated-bytes', s.name)
        self.assertEqual(154, s.volume)
        self.assertEqual('test_user_bis', s.user_id)
        self.assertEqual('test_proj', s.project_id)
        self.assertEqual({'version': '3.0'},
                         s.resource_metadata)

    def test_aggregator_all_mixed(self):
        samples = self._do_test_aggregator({
            'resource_metadata': 'drop',
            'user_id': 'first',
            'project_id': 'last',
            'target': {'name': 'aggregated-bytes'}
        }, expected_length=1)
        s = samples[0]
        self.assertEqual('aggregated-bytes', s.name)
        self.assertEqual(154, s.volume)
        self.assertEqual('test_user', s.user_id)
        self.assertEqual('test_proj', s.project_id)
        self.assertEqual({}, s.resource_metadata)

    def test_aggregator_metadata_default(self):
        samples = self._do_test_aggregator({
            'user_id': 'last',
            'project_id': 'last',
            'target': {'name': 'aggregated-bytes'}
        }, expected_length=1)
        s = samples[0]
        self.assertEqual('aggregated-bytes', s.name)
        self.assertEqual(154, s.volume)
        self.assertEqual('test_user_bis', s.user_id)
        self.assertEqual('test_proj', s.project_id)
        self.assertEqual({'version': '3.0'},
                         s.resource_metadata)

    @mock.patch('ceilometer.transformer.conversions.LOG')
    def test_aggregator_metadata_invalid(self, mylog):
        samples = self._do_test_aggregator({
            'resource_metadata': 'invalid',
            'user_id': 'last',
            'project_id': 'last',
            'target': {'name': 'aggregated-bytes'}
        }, expected_length=1)
        s = samples[0]
        self.assertTrue(mylog.warn.called)
        self.assertEqual('aggregated-bytes', s.name)
        self.assertEqual(154, s.volume)
        self.assertEqual('test_user_bis', s.user_id)
        self.assertEqual('test_proj', s.project_id)
        self.assertEqual({'version': '3.0'},
                         s.resource_metadata)

    def test_aggregator_sized_flush(self):
        transformer_cfg = [
            {
                'name': 'aggregator',
                'parameters': {'size': 2},
            },
        ]
        self._set_pipeline_cfg('transformers', transformer_cfg)
        self._set_pipeline_cfg('counters', ['storage.objects.incoming.bytes'])
        counters = [
            sample.Sample(
                name='storage.objects.incoming.bytes',
                type=sample.TYPE_DELTA,
                volume=26,
                unit='B',
                user_id='test_user',
                project_id='test_proj',
                resource_id='test_resource',
                timestamp=timeutils.utcnow().isoformat(),
                resource_metadata={'version': '1.0'}
            ),
            sample.Sample(
                name='storage.objects.incoming.bytes',
                type=sample.TYPE_DELTA,
                volume=16,
                unit='B',
                user_id='test_user_bis',
                project_id='test_proj_bis',
                resource_id='test_resource',
                timestamp=timeutils.utcnow().isoformat(),
                resource_metadata={'version': '2.0'}
            )
        ]

        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        pipe = pipeline_manager.pipelines[0]

        pipe.publish_samples(None, [counters[0]])
        pipe.flush(None)
        publisher = pipe.publishers[0]
        self.assertEqual(0, len(publisher.samples))

        pipe.publish_samples(None, [counters[1]])
        pipe.flush(None)
        publisher = pipe.publishers[0]
        self.assertEqual(2, len(publisher.samples))

    def test_aggregator_timed_flush(self):
        timeutils.set_time_override()
        transformer_cfg = [
            {
                'name': 'aggregator',
                'parameters': {'size': 900, 'retention_time': 60},
            },
        ]
        self._set_pipeline_cfg('transformers', transformer_cfg)
        self._set_pipeline_cfg('counters', ['storage.objects.incoming.bytes'])
        counters = [
            sample.Sample(
                name='storage.objects.incoming.bytes',
                type=sample.TYPE_DELTA,
                volume=26,
                unit='B',
                user_id='test_user',
                project_id='test_proj',
                resource_id='test_resource',
                timestamp=timeutils.utcnow().isoformat(),
                resource_metadata={'version': '1.0'}
            ),
        ]

        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        pipe = pipeline_manager.pipelines[0]

        pipe.publish_samples(None, counters)
        pipe.flush(None)
        publisher = pipeline_manager.pipelines[0].publishers[0]
        self.assertEqual(0, len(publisher.samples))

        timeutils.advance_time_seconds(120)
        pipe.flush(None)
        publisher = pipeline_manager.pipelines[0].publishers[0]
        self.assertEqual(1, len(publisher.samples))

########NEW FILE########
__FILENAME__ = test_file
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 eNovance
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests for ceilometer/publisher/file.py
"""

import datetime
import logging
import logging.handlers
import os
import tempfile

from ceilometer.openstack.common import network_utils as utils
from ceilometer.openstack.common import test
from ceilometer.publisher import file
from ceilometer import sample


class TestFilePublisher(test.BaseTestCase):

    test_data = [
        sample.Sample(
            name='test',
            type=sample.TYPE_CUMULATIVE,
            unit='',
            volume=1,
            user_id='test',
            project_id='test',
            resource_id='test_run_tasks',
            timestamp=datetime.datetime.utcnow().isoformat(),
            resource_metadata={'name': 'TestPublish'},
        ),
        sample.Sample(
            name='test2',
            type=sample.TYPE_CUMULATIVE,
            unit='',
            volume=1,
            user_id='test',
            project_id='test',
            resource_id='test_run_tasks',
            timestamp=datetime.datetime.utcnow().isoformat(),
            resource_metadata={'name': 'TestPublish'},
        ),
        sample.Sample(
            name='test2',
            type=sample.TYPE_CUMULATIVE,
            unit='',
            volume=1,
            user_id='test',
            project_id='test',
            resource_id='test_run_tasks',
            timestamp=datetime.datetime.utcnow().isoformat(),
            resource_metadata={'name': 'TestPublish'},
        ),
    ]

    def test_file_publisher_maxbytes(self):
        # Test valid configurations
        tempdir = tempfile.mkdtemp()
        name = '%s/log_file' % tempdir
        parsed_url = utils.urlsplit('file://%s?max_bytes=50&backup_count=3'
                                    % name)
        publisher = file.FilePublisher(parsed_url)
        publisher.publish_samples(None,
                                  self.test_data)

        handler = publisher.publisher_logger.handlers[0]
        self.assertIsInstance(handler,
                              logging.handlers.RotatingFileHandler)
        self.assertEqual([50, name, 3], [handler.maxBytes,
                                         handler.baseFilename,
                                         handler.backupCount])
        # The rotating file gets created since only allow 50 bytes.
        self.assertTrue(os.path.exists('%s.1' % name))

    def test_file_publisher(self):
        # Test missing max bytes, backup count configurations
        tempdir = tempfile.mkdtemp()
        name = '%s/log_file_plain' % tempdir
        parsed_url = utils.urlsplit('file://%s' % name)
        publisher = file.FilePublisher(parsed_url)
        publisher.publish_samples(None,
                                  self.test_data)

        handler = publisher.publisher_logger.handlers[0]
        self.assertIsInstance(handler,
                              logging.handlers.RotatingFileHandler)
        self.assertEqual([0, name, 0], [handler.maxBytes,
                                        handler.baseFilename,
                                        handler.backupCount])
        # Test the content is corrected saved in the file
        self.assertTrue(os.path.exists(name))
        with open(name, 'r') as f:
            content = f.read()
        for sample in self.test_data:
            self.assertTrue(sample.id in content)
            self.assertTrue(sample.timestamp in content)

    def test_file_publisher_invalid(self):
        # Test invalid max bytes, backup count configurations
        tempdir = tempfile.mkdtemp()
        parsed_url = utils.urlsplit(
            'file://%s/log_file_bad'
            '?max_bytes=yus&backup_count=5y' % tempdir)
        publisher = file.FilePublisher(parsed_url)
        publisher.publish_samples(None,
                                  self.test_data)

        self.assertIsNone(publisher.publisher_logger)

########NEW FILE########
__FILENAME__ = test_rpc_publisher
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#         Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests for ceilometer/publisher/rpc.py
"""
import datetime

import eventlet
import mock
import oslo.messaging
import oslo.messaging._drivers.common

from ceilometer import messaging
from ceilometer.openstack.common import context
from ceilometer.openstack.common.fixture import config
from ceilometer.openstack.common import network_utils
from ceilometer.openstack.common import test
from ceilometer.publisher import rpc
from ceilometer import sample


class TestPublish(test.BaseTestCase):
    test_data = [
        sample.Sample(
            name='test',
            type=sample.TYPE_CUMULATIVE,
            unit='',
            volume=1,
            user_id='test',
            project_id='test',
            resource_id='test_run_tasks',
            timestamp=datetime.datetime.utcnow().isoformat(),
            resource_metadata={'name': 'TestPublish'},
        ),
        sample.Sample(
            name='test',
            type=sample.TYPE_CUMULATIVE,
            unit='',
            volume=1,
            user_id='test',
            project_id='test',
            resource_id='test_run_tasks',
            timestamp=datetime.datetime.utcnow().isoformat(),
            resource_metadata={'name': 'TestPublish'},
        ),
        sample.Sample(
            name='test2',
            type=sample.TYPE_CUMULATIVE,
            unit='',
            volume=1,
            user_id='test',
            project_id='test',
            resource_id='test_run_tasks',
            timestamp=datetime.datetime.utcnow().isoformat(),
            resource_metadata={'name': 'TestPublish'},
        ),
        sample.Sample(
            name='test2',
            type=sample.TYPE_CUMULATIVE,
            unit='',
            volume=1,
            user_id='test',
            project_id='test',
            resource_id='test_run_tasks',
            timestamp=datetime.datetime.utcnow().isoformat(),
            resource_metadata={'name': 'TestPublish'},
        ),
        sample.Sample(
            name='test3',
            type=sample.TYPE_CUMULATIVE,
            unit='',
            volume=1,
            user_id='test',
            project_id='test',
            resource_id='test_run_tasks',
            timestamp=datetime.datetime.utcnow().isoformat(),
            resource_metadata={'name': 'TestPublish'},
        ),
    ]

    def setUp(self):
        super(TestPublish, self).setUp()
        self.CONF = self.useFixture(config.Config()).conf
        messaging.setup('fake://')
        self.addCleanup(messaging.cleanup)
        self.published = []

    def test_published_no_mock(self):
        publisher = rpc.RPCPublisher(
            network_utils.urlsplit('rpc://'))

        endpoint = mock.MagicMock(['record_metering_data'])
        collector = messaging.get_rpc_server(
            self.CONF.publisher_rpc.metering_topic, endpoint)
        endpoint.record_metering_data.side_effect = \
            lambda *args, **kwds: collector.stop()

        collector.start()
        eventlet.sleep()
        publisher.publish_samples(context.RequestContext(),
                                  self.test_data)
        collector.wait()

        class Matcher(object):
            @staticmethod
            def __eq__(data):
                for i, sample in enumerate(data):
                    if sample['counter_name'] != self.test_data[i].name:
                        return False
                return True

        endpoint.record_metering_data.assert_called_once_with(
            mock.ANY, data=Matcher())

    def test_publish_target(self):
        publisher = rpc.RPCPublisher(
            network_utils.urlsplit('rpc://?target=custom_procedure_call'))
        cast_context = mock.MagicMock()
        with mock.patch.object(publisher.rpc_client, 'prepare') as prepare:
            prepare.return_value = cast_context
            publisher.publish_samples(mock.MagicMock(),
                                      self.test_data)

        prepare.assert_called_once_with(
            topic=self.CONF.publisher_rpc.metering_topic)
        cast_context.cast.assert_called_once_with(
            mock.ANY, 'custom_procedure_call', data=mock.ANY)

    def test_published_with_per_meter_topic(self):
        publisher = rpc.RPCPublisher(
            network_utils.urlsplit('rpc://?per_meter_topic=1'))
        with mock.patch.object(publisher.rpc_client, 'prepare') as prepare:
            publisher.publish_samples(mock.MagicMock(),
                                      self.test_data)

            class MeterGroupMatcher(object):
                def __eq__(self, meters):
                    return len(set(meter['counter_name']
                                   for meter in meters)) == 1

            topic = self.CONF.publisher_rpc.metering_topic
            expected = [mock.call(topic=topic),
                        mock.call().cast(mock.ANY, 'record_metering_data',
                                         data=mock.ANY),
                        mock.call(topic=topic + '.test'),
                        mock.call().cast(mock.ANY, 'record_metering_data',
                                         data=MeterGroupMatcher()),
                        mock.call(topic=topic + '.test2'),
                        mock.call().cast(mock.ANY, 'record_metering_data',
                                         data=MeterGroupMatcher()),
                        mock.call(topic=topic + '.test3'),
                        mock.call().cast(mock.ANY, 'record_metering_data',
                                         data=MeterGroupMatcher())]
            self.assertEqual(expected, prepare.mock_calls)

    def test_published_concurrency(self):
        """This test the concurrent access to the local queue
        of the rpc publisher
        """

        publisher = rpc.RPCPublisher(network_utils.urlsplit('rpc://'))
        cast_context = mock.MagicMock()

        with mock.patch.object(publisher.rpc_client, 'prepare') as prepare:
            def fake_prepare_go(topic):
                return cast_context

            def fake_prepare_wait(topic):
                prepare.side_effect = fake_prepare_go
                # Sleep to simulate concurrency and allow other threads to work
                eventlet.sleep(0)
                return cast_context

            prepare.side_effect = fake_prepare_wait

            job1 = eventlet.spawn(publisher.publish_samples,
                                  mock.MagicMock(), self.test_data)
            job2 = eventlet.spawn(publisher.publish_samples,
                                  mock.MagicMock(), self.test_data)

            job1.wait()
            job2.wait()

        self.assertEqual('default', publisher.policy)
        self.assertEqual(2, len(cast_context.cast.mock_calls))
        self.assertEqual(0, len(publisher.local_queue))

    @mock.patch('ceilometer.publisher.rpc.LOG')
    def test_published_with_no_policy(self, mylog):
        publisher = rpc.RPCPublisher(
            network_utils.urlsplit('rpc://'))
        side_effect = oslo.messaging._drivers.common.RPCException()
        with mock.patch.object(publisher.rpc_client, 'prepare') as prepare:
            prepare.side_effect = side_effect

            self.assertRaises(
                oslo.messaging._drivers.common.RPCException,
                publisher.publish_samples,
                mock.MagicMock(), self.test_data)
            self.assertTrue(mylog.info.called)
            self.assertEqual('default', publisher.policy)
            self.assertEqual(0, len(publisher.local_queue))
            prepare.assert_called_once_with(
                topic=self.CONF.publisher_rpc.metering_topic)

    @mock.patch('ceilometer.publisher.rpc.LOG')
    def test_published_with_policy_block(self, mylog):
        publisher = rpc.RPCPublisher(
            network_utils.urlsplit('rpc://?policy=default'))
        side_effect = oslo.messaging._drivers.common.RPCException()
        with mock.patch.object(publisher.rpc_client, 'prepare') as prepare:
            prepare.side_effect = side_effect
            self.assertRaises(
                oslo.messaging._drivers.common.RPCException,
                publisher.publish_samples,
                mock.MagicMock(), self.test_data)
            self.assertTrue(mylog.info.called)
            self.assertEqual(0, len(publisher.local_queue))
            prepare.assert_called_once_with(
                topic=self.CONF.publisher_rpc.metering_topic)

    @mock.patch('ceilometer.publisher.rpc.LOG')
    def test_published_with_policy_incorrect(self, mylog):
        publisher = rpc.RPCPublisher(
            network_utils.urlsplit('rpc://?policy=notexist'))
        side_effect = oslo.messaging._drivers.common.RPCException()
        with mock.patch.object(publisher.rpc_client, 'prepare') as prepare:
            prepare.side_effect = side_effect
            self.assertRaises(
                oslo.messaging._drivers.common.RPCException,
                publisher.publish_samples,
                mock.MagicMock(), self.test_data)
            self.assertTrue(mylog.warn.called)
            self.assertEqual('default', publisher.policy)
            self.assertEqual(0, len(publisher.local_queue))
            prepare.assert_called_once_with(
                topic=self.CONF.publisher_rpc.metering_topic)

    def test_published_with_policy_drop_and_rpc_down(self):
        publisher = rpc.RPCPublisher(
            network_utils.urlsplit('rpc://?policy=drop'))
        side_effect = oslo.messaging._drivers.common.RPCException()
        with mock.patch.object(publisher.rpc_client, 'prepare') as prepare:
            prepare.side_effect = side_effect
            publisher.publish_samples(mock.MagicMock(),
                                      self.test_data)
            self.assertEqual(0, len(publisher.local_queue))
            prepare.assert_called_once_with(
                topic=self.CONF.publisher_rpc.metering_topic)

    def test_published_with_policy_queue_and_rpc_down(self):
        publisher = rpc.RPCPublisher(
            network_utils.urlsplit('rpc://?policy=queue'))
        side_effect = oslo.messaging._drivers.common.RPCException()
        with mock.patch.object(publisher.rpc_client, 'prepare') as prepare:
            prepare.side_effect = side_effect

            publisher.publish_samples(mock.MagicMock(),
                                      self.test_data)
            self.assertEqual(1, len(publisher.local_queue))
            prepare.assert_called_once_with(
                topic=self.CONF.publisher_rpc.metering_topic)

    def test_published_with_policy_queue_and_rpc_down_up(self):
        self.rpc_unreachable = True
        publisher = rpc.RPCPublisher(
            network_utils.urlsplit('rpc://?policy=queue'))

        side_effect = oslo.messaging._drivers.common.RPCException()
        with mock.patch.object(publisher.rpc_client, 'prepare') as prepare:
            prepare.side_effect = side_effect
            publisher.publish_samples(mock.MagicMock(),
                                      self.test_data)

            self.assertEqual(1, len(publisher.local_queue))

            prepare.side_effect = mock.MagicMock()
            publisher.publish_samples(mock.MagicMock(),
                                      self.test_data)

            self.assertEqual(0, len(publisher.local_queue))

            topic = self.CONF.publisher_rpc.metering_topic
            expected = [mock.call(topic=topic),
                        mock.call(topic=topic),
                        mock.call(topic=topic)]
            self.assertEqual(expected, prepare.mock_calls)

    def test_published_with_policy_sized_queue_and_rpc_down(self):
        publisher = rpc.RPCPublisher(
            network_utils.urlsplit('rpc://?policy=queue&max_queue_length=3'))

        side_effect = oslo.messaging._drivers.common.RPCException()
        with mock.patch.object(publisher.rpc_client, 'prepare') as prepare:
            prepare.side_effect = side_effect
            for i in range(0, 5):
                for s in self.test_data:
                    s.source = 'test-%d' % i
                publisher.publish_samples(mock.MagicMock(),
                                          self.test_data)

        self.assertEqual(3, len(publisher.local_queue))
        self.assertEqual(
            'test-2',
            publisher.local_queue[0][2][0]['source']
        )
        self.assertEqual(
            'test-3',
            publisher.local_queue[1][2][0]['source']
        )
        self.assertEqual(
            'test-4',
            publisher.local_queue[2][2][0]['source']
        )

    def test_published_with_policy_default_sized_queue_and_rpc_down(self):
        publisher = rpc.RPCPublisher(
            network_utils.urlsplit('rpc://?policy=queue'))

        side_effect = oslo.messaging._drivers.common.RPCException()
        with mock.patch.object(publisher.rpc_client, 'prepare') as prepare:
            prepare.side_effect = side_effect
            for i in range(0, 2000):
                for s in self.test_data:
                    s.source = 'test-%d' % i
                publisher.publish_samples(mock.MagicMock(),
                                          self.test_data)

        self.assertEqual(1024, len(publisher.local_queue))
        self.assertEqual(
            'test-976',
            publisher.local_queue[0][2][0]['source']
        )
        self.assertEqual(
            'test-1999',
            publisher.local_queue[1023][2][0]['source']
        )

########NEW FILE########
__FILENAME__ = test_udp
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 eNovance
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests for ceilometer/publisher/udp.py
"""

import datetime

import mock
import msgpack

from ceilometer.openstack.common.fixture import config
from ceilometer.openstack.common import network_utils
from ceilometer.openstack.common import test
from ceilometer.publisher import udp
from ceilometer.publisher import utils
from ceilometer import sample


COUNTER_SOURCE = 'testsource'


class TestUDPPublisher(test.BaseTestCase):
    test_data = [
        sample.Sample(
            name='test',
            type=sample.TYPE_CUMULATIVE,
            unit='',
            volume=1,
            user_id='test',
            project_id='test',
            resource_id='test_run_tasks',
            timestamp=datetime.datetime.utcnow().isoformat(),
            resource_metadata={'name': 'TestPublish'},
            source=COUNTER_SOURCE,
        ),
        sample.Sample(
            name='test',
            type=sample.TYPE_CUMULATIVE,
            unit='',
            volume=1,
            user_id='test',
            project_id='test',
            resource_id='test_run_tasks',
            timestamp=datetime.datetime.utcnow().isoformat(),
            resource_metadata={'name': 'TestPublish'},
            source=COUNTER_SOURCE,
        ),
        sample.Sample(
            name='test2',
            type=sample.TYPE_CUMULATIVE,
            unit='',
            volume=1,
            user_id='test',
            project_id='test',
            resource_id='test_run_tasks',
            timestamp=datetime.datetime.utcnow().isoformat(),
            resource_metadata={'name': 'TestPublish'},
            source=COUNTER_SOURCE,
        ),
        sample.Sample(
            name='test2',
            type=sample.TYPE_CUMULATIVE,
            unit='',
            volume=1,
            user_id='test',
            project_id='test',
            resource_id='test_run_tasks',
            timestamp=datetime.datetime.utcnow().isoformat(),
            resource_metadata={'name': 'TestPublish'},
            source=COUNTER_SOURCE,
        ),
        sample.Sample(
            name='test3',
            type=sample.TYPE_CUMULATIVE,
            unit='',
            volume=1,
            user_id='test',
            project_id='test',
            resource_id='test_run_tasks',
            timestamp=datetime.datetime.utcnow().isoformat(),
            resource_metadata={'name': 'TestPublish'},
            source=COUNTER_SOURCE,
        ),
    ]

    def _make_fake_socket(self, published):
        def _fake_socket_socket(family, type):
            def record_data(msg, dest):
                published.append((msg, dest))

            udp_socket = mock.Mock()
            udp_socket.sendto = record_data
            return udp_socket

        return _fake_socket_socket

    def setUp(self):
        super(TestUDPPublisher, self).setUp()
        self.CONF = self.useFixture(config.Config()).conf
        self.CONF.publisher.metering_secret = 'not-so-secret'

    def test_published(self):
        self.data_sent = []
        with mock.patch('socket.socket',
                        self._make_fake_socket(self.data_sent)):
            publisher = udp.UDPPublisher(
                network_utils.urlsplit('udp://somehost'))
        publisher.publish_samples(None,
                                  self.test_data)

        self.assertEqual(5, len(self.data_sent))

        sent_counters = []

        for data, dest in self.data_sent:
            counter = msgpack.loads(data)
            sent_counters.append(counter)

            # Check destination
            self.assertEqual(('somehost',
                              self.CONF.collector.udp_port), dest)

        # Check that counters are equal
        self.assertEqual(sorted(
            [utils.meter_message_from_counter(d, "not-so-secret")
             for d in self.test_data]), sorted(sent_counters))

    @staticmethod
    def _raise_ioerror(*args):
        raise IOError

    def _make_broken_socket(self, family, type):
        udp_socket = mock.Mock()
        udp_socket.sendto = self._raise_ioerror
        return udp_socket

    def test_publish_error(self):
        with mock.patch('socket.socket',
                        self._make_broken_socket):
            publisher = udp.UDPPublisher(
                network_utils.urlsplit('udp://localhost'))
        publisher.publish_samples(None,
                                  self.test_data)

########NEW FILE########
__FILENAME__ = test_utils
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#         Julien Danjou <julien@danjou.info>
#         Tyaptin Ilya <ityaptin@mirantis.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests for ceilometer/publisher/utils.py
"""

from ceilometer.openstack.common import jsonutils
from ceilometer.openstack.common import test
from ceilometer.publisher import utils


class TestSignature(test.BaseTestCase):
    def test_compute_signature_change_key(self):
        sig1 = utils.compute_signature({'a': 'A', 'b': 'B'},
                                       'not-so-secret')
        sig2 = utils.compute_signature({'A': 'A', 'b': 'B'},
                                       'not-so-secret')
        self.assertNotEqual(sig1, sig2)

    def test_compute_signature_change_value(self):
        sig1 = utils.compute_signature({'a': 'A', 'b': 'B'},
                                       'not-so-secret')
        sig2 = utils.compute_signature({'a': 'a', 'b': 'B'},
                                       'not-so-secret')
        self.assertNotEqual(sig1, sig2)

    def test_compute_signature_same(self):
        sig1 = utils.compute_signature({'a': 'A', 'b': 'B'},
                                       'not-so-secret')
        sig2 = utils.compute_signature({'a': 'A', 'b': 'B'},
                                       'not-so-secret')
        self.assertEqual(sig1, sig2)

    def test_compute_signature_signed(self):
        data = {'a': 'A', 'b': 'B'}
        sig1 = utils.compute_signature(data, 'not-so-secret')
        data['message_signature'] = sig1
        sig2 = utils.compute_signature(data, 'not-so-secret')
        self.assertEqual(sig1, sig2)

    def test_compute_signature_use_configured_secret(self):
        data = {'a': 'A', 'b': 'B'}
        sig1 = utils.compute_signature(data, 'not-so-secret')
        sig2 = utils.compute_signature(data, 'different-value')
        self.assertNotEqual(sig1, sig2)

    def test_verify_signature_signed(self):
        data = {'a': 'A', 'b': 'B'}
        sig1 = utils.compute_signature(data, 'not-so-secret')
        data['message_signature'] = sig1
        self.assertTrue(utils.verify_signature(data, 'not-so-secret'))

    def test_verify_signature_unsigned(self):
        data = {'a': 'A', 'b': 'B'}
        self.assertFalse(utils.verify_signature(data, 'not-so-secret'))

    def test_verify_signature_incorrect(self):
        data = {'a': 'A', 'b': 'B',
                'message_signature': 'Not the same'}
        self.assertFalse(utils.verify_signature(data, 'not-so-secret'))

    def test_verify_signature_nested(self):
        data = {'a': 'A',
                'b': 'B',
                'nested': {'a': 'A',
                           'b': 'B',
                           },
                }
        data['message_signature'] = utils.compute_signature(
            data,
            'not-so-secret')
        self.assertTrue(utils.verify_signature(data, 'not-so-secret'))

    def test_verify_signature_nested_json(self):
        data = {'a': 'A',
                'b': 'B',
                'nested': {'a': 'A',
                           'b': 'B',
                           'c': ('c',),
                           'd': ['d']
                           },
                }
        data['message_signature'] = utils.compute_signature(
            data,
            'not-so-secret')
        jsondata = jsonutils.loads(jsonutils.dumps(data))
        self.assertTrue(utils.verify_signature(jsondata, 'not-so-secret'))

########NEW FILE########
__FILENAME__ = test_models
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Rackspace Hosting
#
# Author: Thomas Maddox <thomas.maddox@rackspace.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import datetime

import mock
import sqlalchemy
from sqlalchemy.dialects.mysql import DECIMAL
from sqlalchemy.types import NUMERIC

from ceilometer.openstack.common import test
from ceilometer.storage.sqlalchemy import models
from ceilometer import utils


class PreciseTimestampTest(test.BaseTestCase):

    @staticmethod
    def fake_dialect(name):
        def _type_descriptor_mock(desc):
            if type(desc) == DECIMAL:
                return NUMERIC(precision=desc.precision, scale=desc.scale)
        dialect = mock.MagicMock()
        dialect.name = name
        dialect.type_descriptor = _type_descriptor_mock
        return dialect

    def setUp(self):
        super(PreciseTimestampTest, self).setUp()
        self._mysql_dialect = self.fake_dialect('mysql')
        self._postgres_dialect = self.fake_dialect('postgres')
        self._type = models.PreciseTimestamp()
        self._date = datetime.datetime(2012, 7, 2, 10, 44)

    def test_load_dialect_impl_mysql(self):
        result = self._type.load_dialect_impl(self._mysql_dialect)
        self.assertEqual(NUMERIC, type(result))
        self.assertEqual(20, result.precision)
        self.assertEqual(6, result.scale)
        self.assertTrue(result.asdecimal)

    def test_load_dialect_impl_postgres(self):
        result = self._type.load_dialect_impl(self._postgres_dialect)
        self.assertEqual(sqlalchemy.DateTime, type(result))

    def test_process_bind_param_store_decimal_mysql(self):
        expected = utils.dt_to_decimal(self._date)
        result = self._type.process_bind_param(self._date, self._mysql_dialect)
        self.assertEqual(expected, result)

    def test_process_bind_param_store_datetime_postgres(self):
        result = self._type.process_bind_param(self._date,
                                               self._postgres_dialect)
        self.assertEqual(self._date, result)

    def test_process_bind_param_store_none_mysql(self):
        result = self._type.process_bind_param(None, self._mysql_dialect)
        self.assertIsNone(result)

    def test_process_bind_param_store_none_postgres(self):
        result = self._type.process_bind_param(None,
                                               self._postgres_dialect)
        self.assertIsNone(result)

    def test_process_result_value_datetime_mysql(self):
        dec_value = utils.dt_to_decimal(self._date)
        result = self._type.process_result_value(dec_value,
                                                 self._mysql_dialect)
        self.assertEqual(self._date, result)

    def test_process_result_value_datetime_postgres(self):
        result = self._type.process_result_value(self._date,
                                                 self._postgres_dialect)
        self.assertEqual(self._date, result)

    def test_process_result_value_none_mysql(self):
        result = self._type.process_result_value(None,
                                                 self._mysql_dialect)
        self.assertIsNone(result)

    def test_process_result_value_none_postgres(self):
        result = self._type.process_result_value(None,
                                                 self._postgres_dialect)
        self.assertIsNone(result)

########NEW FILE########
__FILENAME__ = test_base
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 eNovance
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import datetime
import math

from ceilometer.openstack.common import test
from ceilometer.storage import base


class BaseTest(test.BaseTestCase):

    def test_iter_period(self):
        times = list(base.iter_period(
            datetime.datetime(2013, 1, 1, 12, 0),
            datetime.datetime(2013, 1, 1, 13, 0),
            60))
        self.assertEqual(60, len(times))
        self.assertEqual((datetime.datetime(2013, 1, 1, 12, 10),
                          datetime.datetime(2013, 1, 1, 12, 11)), times[10])
        self.assertEqual((datetime.datetime(2013, 1, 1, 12, 21),
                          datetime.datetime(2013, 1, 1, 12, 22)), times[21])

    def test_iter_period_bis(self):
        times = list(base.iter_period(
            datetime.datetime(2013, 1, 2, 13, 0),
            datetime.datetime(2013, 1, 2, 14, 0),
            55))
        self.assertEqual(math.ceil(3600 / 55.0), len(times))
        self.assertEqual((datetime.datetime(2013, 1, 2, 13, 9, 10),
                          datetime.datetime(2013, 1, 2, 13, 10, 5)),
                         times[10])
        self.assertEqual((datetime.datetime(2013, 1, 2, 13, 19, 15),
                          datetime.datetime(2013, 1, 2, 13, 20, 10)),
                         times[21])

    def test_handle_sort_key(self):
        sort_keys_alarm = base._handle_sort_key('alarm')
        self.assertEqual(['name', 'user_id', 'project_id'], sort_keys_alarm)

        sort_keys_meter = base._handle_sort_key('meter', 'foo')
        self.assertEqual(['foo', 'user_id', 'project_id'], sort_keys_meter)

        sort_keys_resource = base._handle_sort_key('resource', 'project_id')
        self.assertEqual(['project_id', 'user_id', 'timestamp'],
                         sort_keys_resource)

########NEW FILE########
__FILENAME__ = test_get_connection
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests for ceilometer/storage/
"""

from ceilometer.openstack.common import test
from ceilometer import storage
from ceilometer.storage import impl_log


class EngineTest(test.BaseTestCase):

    def test_get_connection(self):
        engine = storage.get_connection('log://localhost')
        self.assertIsInstance(engine, impl_log.Connection)

    def test_get_connection_no_such_engine(self):
        try:
            storage.get_connection('no-such-engine://localhost')
        except RuntimeError as err:
            self.assertIn('no-such-engine', unicode(err))

########NEW FILE########
__FILENAME__ = test_impl_db2
# -*- encoding: utf-8 -*-
#
# Copyright Ericsson AB 2014. All rights reserved
#
# Authors: Ildiko Vancsa <ildiko.vancsa@ericsson.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests for ceilometer/storage/impl_db2.py

.. note::
  In order to run the tests against another MongoDB server set the
  environment variable CEILOMETER_TEST_DB2_URL to point to a DB2
  server before running the tests.

"""

from ceilometer.storage import impl_db2
from ceilometer.tests import base as test_base


class CapabilitiesTest(test_base.BaseTestCase):
    # Check the returned capabilities list, which is specific to each DB
    # driver

    def test_capabilities(self):
        expected_capabilities = {
            'meters': {'pagination': False,
                       'query': {'simple': True,
                                 'metadata': True,
                                 'complex': False}},
            'resources': {'pagination': False,
                          'query': {'simple': True,
                                    'metadata': True,
                                    'complex': False}},
            'samples': {'pagination': False,
                        'groupby': False,
                        'query': {'simple': True,
                                  'metadata': True,
                                  'complex': True}},
            'statistics': {'pagination': False,
                           'groupby': True,
                           'query': {'simple': True,
                                     'metadata': True,
                                     'complex': False},
                           'aggregation': {'standard': True,
                                           'selectable': {
                                               'max': False,
                                               'min': False,
                                               'sum': False,
                                               'avg': False,
                                               'count': False,
                                               'stddev': False,
                                               'cardinality': False}}
                           },
            'alarms': {'query': {'simple': True,
                                 'complex': True},
                       'history': {'query': {'simple': True,
                                             'complex': True}}},
            'events': {'query': {'simple': False}}
        }

        actual_capabilities = impl_db2.Connection.get_capabilities()
        self.assertEqual(expected_capabilities, actual_capabilities)

########NEW FILE########
__FILENAME__ = test_impl_hbase
# -*- encoding: utf-8 -*-
#
# Copyright © 2012, 2013 Dell Inc.
#
# Author: Stas Maksimov <Stanislav_M@dell.com>
# Author: Shengjie Min <Shengjie_Min@dell.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests for ceilometer/storage/impl_hbase.py

.. note::
  In order to run the tests against real HBase server set the environment
  variable CEILOMETER_TEST_HBASE_URL to point to that HBase instance before
  running the tests. Make sure the Thrift server is running on that server.

"""
from mock import patch

from ceilometer.storage import impl_hbase as hbase
from ceilometer.tests import base as test_base
from ceilometer.tests import db as tests_db


class HBaseEngineTestBase(tests_db.TestBase):
    db_manager = tests_db.HBaseManager()


class ConnectionTest(HBaseEngineTestBase):

    def test_hbase_connection(self):
        conn = hbase.Connection(self.db_manager.connection)
        self.assertIsInstance(conn.conn_pool.connection(), hbase.MConnection)

        class TestConn(object):
            def __init__(self, host, port):
                self.netloc = '%s:%s' % (host, port)

            def open(self):
                pass

        def get_connection_pool(conf):
            return TestConn(conf['host'], conf['port'])

        with patch.object(hbase.Connection, '_get_connection_pool',
                          side_effect=get_connection_pool):
            conn = hbase.Connection('hbase://test_hbase:9090')
        self.assertIsInstance(conn.conn_pool, TestConn)


class CapabilitiesTest(test_base.BaseTestCase):
    # Check the returned capabilities list, which is specific to each DB
    # driver

    def test_capabilities(self):
        expected_capabilities = {
            'meters': {'pagination': False,
                       'query': {'simple': True,
                                 'metadata': True,
                                 'complex': False}},
            'resources': {'pagination': False,
                          'query': {'simple': True,
                                    'metadata': True,
                                    'complex': False}},
            'samples': {'pagination': False,
                        'groupby': False,
                        'query': {'simple': True,
                                  'metadata': True,
                                  'complex': False}},
            'statistics': {'pagination': False,
                           'groupby': False,
                           'query': {'simple': True,
                                     'metadata': True,
                                     'complex': False},
                           'aggregation': {'standard': True,
                                           'selectable': {
                                               'max': False,
                                               'min': False,
                                               'sum': False,
                                               'avg': False,
                                               'count': False,
                                               'stddev': False,
                                               'cardinality': False}}
                           },
            'alarms': {'query': {'simple': False,
                                 'complex': False},
                       'history': {'query': {'simple': False,
                                             'complex': False}}},
            'events': {'query': {'simple': False}}
        }

        actual_capabilities = hbase.Connection.get_capabilities()
        self.assertEqual(expected_capabilities, actual_capabilities)

########NEW FILE########
__FILENAME__ = test_impl_log
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests for ceilometer/storage/impl_log.py
"""

from ceilometer.openstack.common import test
from ceilometer.storage import impl_log


class ConnectionTest(test.BaseTestCase):
    def test_get_connection(self):
        conn = impl_log.Connection(None)
        conn.record_metering_data({'counter_name': 'test',
                                   'resource_id': __name__,
                                   'counter_volume': 1,
                                   })

########NEW FILE########
__FILENAME__ = test_impl_mongodb
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests for ceilometer/storage/impl_mongodb.py

.. note::
  In order to run the tests against another MongoDB server set the
  environment variable CEILOMETER_TEST_MONGODB_URL to point to a MongoDB
  server before running the tests.

"""

from ceilometer.storage import base
from ceilometer.storage import impl_mongodb
from ceilometer.tests import base as test_base
from ceilometer.tests import db as tests_db
from ceilometer.tests.storage import test_storage_scenarios


class MongoDBEngineTestBase(tests_db.TestBase):
    db_manager = tests_db.MongoDbManager()


class MongoDBConnection(MongoDBEngineTestBase):
    def test_connection_pooling(self):
        test_conn = impl_mongodb.Connection(self.db_manager.connection)
        self.assertEqual(self.conn.conn, test_conn.conn)

    def test_replica_set(self):
        url = self.db_manager.connection + '?replicaSet=foobar'
        conn = impl_mongodb.Connection(url)
        self.assertTrue(conn.conn)

    def test_recurse_sort_keys(self):
        sort_keys = ['k1', 'k2', 'k3']
        marker = {'k1': 'v1', 'k2': 'v2', 'k3': 'v3'}
        flag = '$lt'
        ret = impl_mongodb.Connection._recurse_sort_keys(sort_keys=sort_keys,
                                                         marker=marker,
                                                         flag=flag)
        expect = {'k3': {'$lt': 'v3'}, 'k2': {'eq': 'v2'}, 'k1': {'eq': 'v1'}}
        self.assertEqual(expect, ret)


class MongoDBTestMarkerBase(test_storage_scenarios.DBTestBase,
                            MongoDBEngineTestBase):
    #NOTE(Fengqian): All these three test case are the same for resource
    #and meter collection. As to alarm, we will set up in AlarmTestPagination.
    def test_get_marker(self):
        marker_pairs = {'user_id': 'user-id-4'}
        ret = impl_mongodb.Connection._get_marker(self.conn.db.resource,
                                                  marker_pairs)
        self.assertEqual('project-id-4', ret['project_id'])

    def test_get_marker_None(self):
        marker_pairs = {'user_id': 'user-id-foo'}
        try:
            ret = impl_mongodb.Connection._get_marker(self.conn.db.resource,
                                                      marker_pairs)
            self.assertEqual('project-id-foo', ret['project_id'])
        except base.NoResultFound:
            self.assertTrue(True)

    def test_get_marker_multiple(self):
        try:
            marker_pairs = {'project_id': 'project-id'}
            ret = impl_mongodb.Connection._get_marker(self.conn.db.resource,
                                                      marker_pairs)
            self.assertEqual('project-id-foo', ret['project_id'])
        except base.MultipleResultsFound:
            self.assertTrue(True)


class IndexTest(MongoDBEngineTestBase):
    def test_meter_ttl_index_absent(self):
        # create a fake index and check it is deleted
        self.conn.db.meter.ensure_index('foo', name='meter_ttl')
        self.CONF.set_override('time_to_live', -1, group='database')
        self.conn.upgrade()
        self.assertTrue(self.conn.db.meter.ensure_index('foo',
                                                        name='meter_ttl'))
        self.CONF.set_override('time_to_live', 456789, group='database')
        self.conn.upgrade()
        self.assertFalse(self.conn.db.meter.ensure_index('foo',
                                                         name='meter_ttl'))

    def test_meter_ttl_index_present(self):
        self.CONF.set_override('time_to_live', 456789, group='database')
        self.conn.upgrade()
        self.assertFalse(self.conn.db.meter.ensure_index('foo',
                                                         name='meter_ttl'))
        self.assertEqual(456789,
                         self.conn.db.meter.index_information()
                         ['meter_ttl']['expireAfterSeconds'])

        self.CONF.set_override('time_to_live', -1, group='database')
        self.conn.upgrade()
        self.assertTrue(self.conn.db.meter.ensure_index('foo',
                                                        name='meter_ttl'))


class AlarmTestPagination(test_storage_scenarios.AlarmTestBase,
                          MongoDBEngineTestBase):
    def test_alarm_get_marker(self):
        self.add_some_alarms()
        marker_pairs = {'name': 'red-alert'}
        ret = impl_mongodb.Connection._get_marker(self.conn.db.alarm,
                                                  marker_pairs=marker_pairs)
        self.assertEqual('test.one', ret['rule']['meter_name'])

    def test_alarm_get_marker_None(self):
        self.add_some_alarms()
        try:
            marker_pairs = {'name': 'user-id-foo'}
            ret = impl_mongodb.Connection._get_marker(self.conn.db.alarm,
                                                      marker_pairs)
            self.assertEqual('meter_name-foo', ret['rule']['meter_name'])
        except base.NoResultFound:
            self.assertTrue(True)

    def test_alarm_get_marker_multiple(self):
        self.add_some_alarms()
        try:
            marker_pairs = {'user_id': 'me'}
            ret = impl_mongodb.Connection._get_marker(self.conn.db.alarm,
                                                      marker_pairs)
            self.assertEqual('counter-name-foo', ret['rule']['meter_name'])
        except base.MultipleResultsFound:
            self.assertTrue(True)


class CapabilitiesTest(test_base.BaseTestCase):
    # Check the returned capabilities list, which is specific to each DB
    # driver

    def test_capabilities(self):
        expected_capabilities = {
            'meters': {'pagination': False,
                       'query': {'simple': True,
                                 'metadata': True,
                                 'complex': False}},
            'resources': {'pagination': False,
                          'query': {'simple': True,
                                    'metadata': True,
                                    'complex': False}},
            'samples': {'pagination': False,
                        'groupby': False,
                        'query': {'simple': True,
                                  'metadata': True,
                                  'complex': True}},
            'statistics': {'pagination': False,
                           'groupby': True,
                           'query': {'simple': True,
                                     'metadata': True,
                                     'complex': False},
                           'aggregation': {'standard': True,
                                           'selectable': {
                                               'max': True,
                                               'min': True,
                                               'sum': True,
                                               'avg': True,
                                               'count': True,
                                               'stddev': True,
                                               'cardinality': True}}
                           },
            'alarms': {'query': {'simple': True,
                                 'complex': True},
                       'history': {'query': {'simple': True,
                                             'complex': True}}},
            'events': {'query': {'simple': False}}
        }

        actual_capabilities = impl_mongodb.Connection.get_capabilities()
        self.assertEqual(expected_capabilities, actual_capabilities)

########NEW FILE########
__FILENAME__ = test_impl_sqlalchemy
# -*- encoding: utf-8 -*-
#
# Author: John Tran <jhtran@att.com>
#         Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests for ceilometer/storage/impl_sqlalchemy.py

.. note::
  In order to run the tests against real SQL server set the environment
  variable CEILOMETER_TEST_SQL_URL to point to a SQL server before running
  the tests.

"""

import datetime
import repr

from mock import patch

from ceilometer.openstack.common import timeutils
from ceilometer.storage import impl_sqlalchemy
from ceilometer.storage import models
from ceilometer.storage.sqlalchemy import models as sql_models

from ceilometer.tests import base as test_base
from ceilometer.tests import db as tests_db
from ceilometer.tests.storage import test_storage_scenarios as scenarios


class EventTestBase(tests_db.TestBase):
    # Note: Do not derive from SQLAlchemyEngineTestBase, since we
    # don't want to automatically inherit all the Meter setup.
    db_manager = tests_db.SQLiteManager()


class CeilometerBaseTest(EventTestBase):
    def test_ceilometer_base(self):
        base = sql_models.CeilometerBase()
        base['key'] = 'value'
        self.assertEqual('value', base['key'])


class TraitTypeTest(EventTestBase):
    # TraitType is a construct specific to sqlalchemy.
    # Not applicable to other drivers.

    def test_trait_type_exists(self):
        tt1 = self.conn._get_or_create_trait_type("foo", 0)
        self.assertTrue(tt1.id >= 0)
        tt2 = self.conn._get_or_create_trait_type("foo", 0)
        self.assertEqual(tt2.id, tt1.id)
        self.assertEqual(tt2.desc, tt1.desc)
        self.assertEqual(tt2.data_type, tt1.data_type)

    def test_new_trait_type(self):
        tt1 = self.conn._get_or_create_trait_type("foo", 0)
        self.assertTrue(tt1.id >= 0)
        tt2 = self.conn._get_or_create_trait_type("blah", 0)
        self.assertNotEqual(tt1.id, tt2.id)
        self.assertNotEqual(tt1.desc, tt2.desc)
        # Test the method __repr__ returns a string
        self.assertTrue(repr.repr(tt2))

    def test_trait_different_data_type(self):
        tt1 = self.conn._get_or_create_trait_type("foo", 0)
        self.assertTrue(tt1.id >= 0)
        tt2 = self.conn._get_or_create_trait_type("foo", 1)
        self.assertNotEqual(tt1.id, tt2.id)
        self.assertEqual(tt2.desc, tt1.desc)
        self.assertNotEqual(tt1.data_type, tt2.data_type)
        # Test the method __repr__ returns a string
        self.assertTrue(repr.repr(tt2))


class EventTypeTest(EventTestBase):
    # EventType is a construct specific to sqlalchemy
    # Not applicable to other drivers.

    def test_event_type_exists(self):
        et1 = self.conn._get_or_create_event_type("foo")
        self.assertTrue(et1.id >= 0)
        et2 = self.conn._get_or_create_event_type("foo")
        self.assertEqual(et2.id, et1.id)
        self.assertEqual(et2.desc, et1.desc)

    def test_event_type_unique(self):
        et1 = self.conn._get_or_create_event_type("foo")
        self.assertTrue(et1.id >= 0)
        et2 = self.conn._get_or_create_event_type("blah")
        self.assertNotEqual(et1.id, et2.id)
        self.assertNotEqual(et1.desc, et2.desc)
        # Test the method __repr__ returns a string
        self.assertTrue(repr.repr(et2))


class MyException(Exception):
    pass


class EventTest(EventTestBase):
    def test_string_traits(self):
        model = models.Trait("Foo", models.Trait.TEXT_TYPE, "my_text")
        trait = self.conn._make_trait(model, None)
        self.assertEqual(models.Trait.TEXT_TYPE, trait.trait_type.data_type)
        self.assertIsNone(trait.t_float)
        self.assertIsNone(trait.t_int)
        self.assertIsNone(trait.t_datetime)
        self.assertEqual("my_text", trait.t_string)
        self.assertIsNotNone(trait.trait_type.desc)

    def test_int_traits(self):
        model = models.Trait("Foo", models.Trait.INT_TYPE, 100)
        trait = self.conn._make_trait(model, None)
        self.assertEqual(models.Trait.INT_TYPE, trait.trait_type.data_type)
        self.assertIsNone(trait.t_float)
        self.assertIsNone(trait.t_string)
        self.assertIsNone(trait.t_datetime)
        self.assertEqual(100, trait.t_int)
        self.assertIsNotNone(trait.trait_type.desc)

    def test_float_traits(self):
        model = models.Trait("Foo", models.Trait.FLOAT_TYPE, 123.456)
        trait = self.conn._make_trait(model, None)
        self.assertEqual(models.Trait.FLOAT_TYPE, trait.trait_type.data_type)
        self.assertIsNone(trait.t_int)
        self.assertIsNone(trait.t_string)
        self.assertIsNone(trait.t_datetime)
        self.assertEqual(123.456, trait.t_float)
        self.assertIsNotNone(trait.trait_type.desc)

    def test_datetime_traits(self):
        now = datetime.datetime.utcnow()
        model = models.Trait("Foo", models.Trait.DATETIME_TYPE, now)
        trait = self.conn._make_trait(model, None)
        self.assertEqual(models.Trait.DATETIME_TYPE,
                         trait.trait_type.data_type)
        self.assertIsNone(trait.t_int)
        self.assertIsNone(trait.t_string)
        self.assertIsNone(trait.t_float)
        self.assertEqual(now, trait.t_datetime)
        self.assertIsNotNone(trait.trait_type.desc)

    def test_bad_event(self):
        now = datetime.datetime.utcnow()
        m = [models.Event("1", "Foo", now, []),
             models.Event("2", "Zoo", now, [])]

        with patch.object(self.conn, "_record_event") as mock_save:
            mock_save.side_effect = MyException("Boom")
            problem_events = self.conn.record_events(m)
        self.assertEqual(2, len(problem_events))
        for bad, event in problem_events:
            self.assertEqual(bad, models.Event.UNKNOWN_PROBLEM)

    def test_get_none_value_traits(self):
        model = sql_models.Trait(None, None, 5)
        self.assertIsNone(model.get_value())
        self.assertTrue(repr.repr(model))

    def test_event_repr(self):
        ev = sql_models.Event('msg_id', None, False)
        ev.id = 100
        self.assertTrue(repr.repr(ev))


class RelationshipTest(scenarios.DBTestBase):
    # Note: Do not derive from SQLAlchemyEngineTestBase, since we
    # don't want to automatically inherit all the Meter setup.
    db_manager = tests_db.SQLiteManager()

    @patch.object(timeutils, 'utcnow')
    def test_clear_metering_data_meta_tables(self, mock_utcnow):
        mock_utcnow.return_value = datetime.datetime(2012, 7, 2, 10, 45)
        self.conn.clear_expired_metering_data(3 * 60)

        session = self.conn._engine_facade.get_session()
        meta_tables = [sql_models.MetaText, sql_models.MetaFloat,
                       sql_models.MetaBigInt, sql_models.MetaBool]
        for table in meta_tables:
            self.assertEqual(0, session.query(table)
                .filter(~table.id.in_(
                    session.query(sql_models.Sample.id)
                        .group_by(sql_models.Sample.id)
                        )).count())


class CapabilitiesTest(test_base.BaseTestCase):
    # Check the returned capabilities list, which is specific to each DB
    # driver

    def test_capabilities(self):
        expected_capabilities = {
            'meters': {'pagination': False,
                       'query': {'simple': True,
                                 'metadata': True,
                                 'complex': False}},
            'resources': {'pagination': False,
                          'query': {'simple': True,
                                    'metadata': True,
                                    'complex': False}},
            'samples': {'pagination': True,
                        'groupby': True,
                        'query': {'simple': True,
                                  'metadata': True,
                                  'complex': True}},
            'statistics': {'pagination': False,
                           'groupby': True,
                           'query': {'simple': True,
                                     'metadata': True,
                                     'complex': False},
                           'aggregation': {'standard': True,
                                           'selectable': {
                                               'max': True,
                                               'min': True,
                                               'sum': True,
                                               'avg': True,
                                               'count': True,
                                               'stddev': True,
                                               'cardinality': True}}
                           },
            'alarms': {'query': {'simple': True,
                                 'complex': True},
                       'history': {'query': {'simple': True,
                                             'complex': True}}},
            'events': {'query': {'simple': True}}
        }

        actual_capabilities = impl_sqlalchemy.Connection.get_capabilities()
        self.assertEqual(expected_capabilities, actual_capabilities)

########NEW FILE########
__FILENAME__ = test_models
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import datetime

from ceilometer.openstack.common import test
from ceilometer.storage import models


class FakeModel(models.Model):
    def __init__(self, arg1, arg2):
        models.Model.__init__(self, arg1=arg1, arg2=arg2)


class ModelTest(test.BaseTestCase):

    def test_create_attributes(self):
        m = FakeModel(1, 2)
        self.assertEqual(1, m.arg1)
        self.assertEqual(2, m.arg2)

    def test_as_dict(self):
        m = FakeModel(1, 2)
        d = m.as_dict()
        self.assertEqual({'arg1': 1, 'arg2': 2}, d)

    def test_as_dict_recursive(self):
        m = FakeModel(1, FakeModel('a', 'b'))
        d = m.as_dict()
        self.assertEqual({'arg1': 1,
                          'arg2': {'arg1': 'a',
                                   'arg2': 'b'}},
                         d)

    def test_as_dict_recursive_list(self):
        m = FakeModel(1, [FakeModel('a', 'b')])
        d = m.as_dict()
        self.assertEqual({'arg1': 1,
                          'arg2': [{'arg1': 'a',
                                    'arg2': 'b'}]},
                         d)

    def test_event_repr_no_traits(self):
        x = models.Event("1", "name", "now", None)
        self.assertEqual("<Event: 1, name, now, >", repr(x))

    def test_get_field_names_of_sample(self):
        sample_fields = ["source", "counter_name", "counter_type",
                         "counter_unit", "counter_volume", "user_id",
                         "project_id", "resource_id", "timestamp",
                         "resource_metadata", "message_id",
                         "message_signature", "recorded_at"]

        self.assertEqual(set(sample_fields),
                         set(models.Sample.get_field_names()))

    def test_get_field_names_of_alarm(self):
        alarm_fields = ["alarm_id", "type", "enabled", "name", "description",
                        "timestamp", "user_id", "project_id", "state",
                        "state_timestamp", "ok_actions", "alarm_actions",
                        "insufficient_data_actions", "repeat_actions", "rule",
                        "time_constraints"]

        self.assertEqual(set(alarm_fields),
                         set(models.Alarm.get_field_names()))

    def test_get_field_names_of_alarmchange(self):
        alarmchange_fields = ["event_id", "alarm_id", "type", "detail",
                              "user_id", "project_id", "on_behalf_of",
                              "timestamp"]

        self.assertEqual(set(alarmchange_fields),
                         set(models.AlarmChange.get_field_names()))


class TestTraitModel(test.BaseTestCase):

    def test_convert_value(self):
        v = models.Trait.convert_value(
            models.Trait.INT_TYPE, '10')
        self.assertEqual(10, v)
        self.assertIsInstance(v, int)
        v = models.Trait.convert_value(
            models.Trait.FLOAT_TYPE, '10')
        self.assertEqual(10.0, v)
        self.assertIsInstance(v, float)

        v = models.Trait.convert_value(
            models.Trait.DATETIME_TYPE, '2013-08-08 21:05:37.123456')
        self.assertEqual(datetime.datetime(2013, 8, 8, 21, 5, 37, 123456), v)
        self.assertIsInstance(v, datetime.datetime)

        v = models.Trait.convert_value(
            models.Trait.TEXT_TYPE, 10)
        self.assertEqual("10", v)
        self.assertIsInstance(v, str)

########NEW FILE########
__FILENAME__ = test_pymongo_base
# -*- encoding: utf-8 -*-
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests the mongodb and db2 common functionality
"""

import copy
import datetime

from mock import patch
import testscenarios

from ceilometer.publisher import utils
from ceilometer import sample
from ceilometer.tests import db as tests_db
from ceilometer.tests.storage import test_storage_scenarios

load_tests = testscenarios.load_tests_apply_scenarios


class CompatibilityTest(test_storage_scenarios.DBTestBase,
                        tests_db.MixinTestsWithBackendScenarios):

    scenarios = [
        ('mongodb', {'db_manager': tests_db.MongoDbManager()}),
        ('db2', {'db_manager': tests_db.DB2Manager()}),
    ]

    def prepare_data(self):
        def old_record_metering_data(self, data):
            self.db.user.update(
                {'_id': data['user_id']},
                {'$addToSet': {'source': data['source'],
                               },
                 },
                upsert=True,
            )
            self.db.project.update(
                {'_id': data['project_id']},
                {'$addToSet': {'source': data['source'],
                               },
                 },
                upsert=True,
            )
            received_timestamp = datetime.datetime.utcnow()
            self.db.resource.update(
                {'_id': data['resource_id']},
                {'$set': {'project_id': data['project_id'],
                          'user_id': data['user_id'],
                          # Current metadata being used and when it was
                          # last updated.
                          'timestamp': data['timestamp'],
                          'received_timestamp': received_timestamp,
                          'metadata': data['resource_metadata'],
                          'source': data['source'],
                          },
                 '$addToSet': {'meter': {'counter_name': data['counter_name'],
                                         'counter_type': data['counter_type'],
                                         },
                               },
                 },
                upsert=True,
            )

            record = copy.copy(data)
            self.db.meter.insert(record)

        # Stubout with the old version DB schema, the one w/o 'counter_unit'
        with patch.object(self.conn, 'record_metering_data',
                          side_effect=old_record_metering_data):
            self.counters = []
            c = sample.Sample(
                'volume.size',
                'gauge',
                'GiB',
                5,
                'user-id',
                'project1',
                'resource-id',
                timestamp=datetime.datetime(2012, 9, 25, 10, 30),
                resource_metadata={'display_name': 'test-volume',
                                   'tag': 'self.counter',
                                   },
                source='test',
            )
            self.counters.append(c)
            msg = utils.meter_message_from_counter(
                c,
                secret='not-so-secret')
            self.conn.record_metering_data(self.conn, msg)

        # Create the old format alarm with a dict instead of a
        # array for matching_metadata
        alarm = dict(alarm_id='0ld-4l3rt',
                     enabled=True,
                     name='old-alert',
                     description='old-alert',
                     timestamp=None,
                     meter_name='cpu',
                     user_id='me',
                     project_id='and-da-boys',
                     comparison_operator='lt',
                     threshold=36,
                     statistic='count',
                     evaluation_periods=1,
                     period=60,
                     state="insufficient data",
                     state_timestamp=None,
                     ok_actions=[],
                     alarm_actions=['http://nowhere/alarms'],
                     insufficient_data_actions=[],
                     repeat_actions=False,
                     matching_metadata={'key': 'value'})

        self.conn.db.alarm.update(
            {'alarm_id': alarm['alarm_id']},
            {'$set': alarm},
            upsert=True)

        alarm['alarm_id'] = 'other-kind-of-0ld-4l3rt'
        alarm['name'] = 'other-old-alaert'
        alarm['matching_metadata'] = [{'key': 'key1', 'value': 'value1'},
                                      {'key': 'key2', 'value': 'value2'}]
        self.conn.db.alarm.update(
            {'alarm_id': alarm['alarm_id']},
            {'$set': alarm},
            upsert=True)

    def test_alarm_get_old_format_matching_metadata_dict(self):
        old = list(self.conn.get_alarms(name='old-alert'))[0]
        self.assertEqual('threshold', old.type)
        self.assertEqual([{'field': 'key',
                           'op': 'eq',
                           'value': 'value',
                           'type': 'string'}],
                         old.rule['query'])
        self.assertEqual(60, old.rule['period'])
        self.assertEqual('cpu', old.rule['meter_name'])
        self.assertEqual(1, old.rule['evaluation_periods'])
        self.assertEqual('count', old.rule['statistic'])
        self.assertEqual('lt', old.rule['comparison_operator'])
        self.assertEqual(36, old.rule['threshold'])

    def test_alarm_get_old_format_matching_metadata_array(self):
        old = list(self.conn.get_alarms(name='other-old-alaert'))[0]
        self.assertEqual('threshold', old.type)
        self.assertEqual(sorted([{'field': 'key1',
                                  'op': 'eq',
                                  'value': 'value1',
                                  'type': 'string'},
                                 {'field': 'key2',
                                  'op': 'eq',
                                  'value': 'value2',
                                  'type': 'string'}]),
                         sorted(old.rule['query']),)
        self.assertEqual('cpu', old.rule['meter_name'])
        self.assertEqual(60, old.rule['period'])
        self.assertEqual(1, old.rule['evaluation_periods'])
        self.assertEqual('count', old.rule['statistic'])
        self.assertEqual('lt', old.rule['comparison_operator'])
        self.assertEqual(36, old.rule['threshold'])

    def test_counter_unit(self):
        meters = list(self.conn.get_meters())
        self.assertEqual(1, len(meters))

########NEW FILE########
__FILENAME__ = test_storage_scenarios
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Intel Corp.
#
# Author: Lianhao Lu <lianhao.lu@intel.com>
#         Shane Wang <shane.wang@intel.com>
#         Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
""" Base classes for DB backend implementation test
"""

import datetime

import mock

from ceilometer.openstack.common import timeutils
from ceilometer.publisher import utils
from ceilometer import sample
from ceilometer import storage
from ceilometer.storage import base
from ceilometer.storage import impl_mongodb as mongodb
from ceilometer.storage import models
from ceilometer.tests import db as tests_db


class DBTestBase(tests_db.TestBase):
    def create_and_store_sample(self, timestamp=datetime.datetime.utcnow(),
                                metadata={
                                    'display_name': 'test-server',
                                    'tag': 'self.counter'
                                },
                                name='instance',
                                sample_type=sample.TYPE_CUMULATIVE, unit='',
                                volume=1, user_id='user-id',
                                project_id='project-id',
                                resource_id='resource-id', source=None):
        s = sample.Sample(
            name, sample_type, unit=unit, volume=volume, user_id=user_id,
            project_id=project_id, resource_id=resource_id,
            timestamp=timestamp,
            resource_metadata=metadata, source=source
        )
        msg = utils.meter_message_from_counter(
            s, self.CONF.publisher.metering_secret
        )
        self.conn.record_metering_data(msg)
        return msg

    def setUp(self):
        super(DBTestBase, self).setUp()
        patcher = mock.patch.object(timeutils, 'utcnow')
        self.addCleanup(patcher.stop)
        self.mock_utcnow = patcher.start()
        self.mock_utcnow.return_value = datetime.datetime(2015, 7, 2, 10, 39)
        self.prepare_data()

    def prepare_data(self):
        original_timestamps = [(2012, 7, 2, 10, 40), (2012, 7, 2, 10, 41),
                               (2012, 7, 2, 10, 41), (2012, 7, 2, 10, 42),
                               (2012, 7, 2, 10, 43)]

        timestamps_for_test_samples_default_order = [(2012, 7, 2, 10, 44),
                                                     (2011, 5, 30, 18, 3),
                                                     (2012, 12, 1, 1, 25),
                                                     (2012, 2, 29, 6, 59),
                                                     (2013, 5, 31, 23, 7)]
        timestamp_list = (original_timestamps +
                          timestamps_for_test_samples_default_order)

        self.msgs = []

        self.msgs.append(self.create_and_store_sample(
            timestamp=datetime.datetime(2012, 7, 2, 10, 39),
            source='test-1')
        )
        self.msgs.append(self.create_and_store_sample(
            timestamp=datetime.datetime(*timestamp_list[0]),
            source='test-1')
        )
        self.msgs.append(self.create_and_store_sample(
            timestamp=datetime.datetime(*timestamp_list[1]),
            resource_id='resource-id-alternate',
            metadata={'display_name': 'test-server', 'tag': 'self.counter2'},
            source='test-2')
        )
        self.msgs.append(self.create_and_store_sample(
            timestamp=datetime.datetime(*timestamp_list[2]),
            resource_id='resource-id-alternate',
            user_id='user-id-alternate',
            metadata={'display_name': 'test-server', 'tag': 'self.counter3'},
            source='test-3')
        )

        start_idx = 3
        end_idx = len(timestamp_list)

        for i, ts in zip(range(start_idx - 1, end_idx - 1),
                         timestamp_list[start_idx:end_idx]):
            self.msgs.append(
                self.create_and_store_sample(
                    timestamp=datetime.datetime(*ts),
                    user_id='user-id-%s' % i,
                    project_id='project-id-%s' % i,
                    resource_id='resource-id-%s' % i,
                    metadata={
                        'display_name': 'test-server',
                        'tag': 'counter-%s' % i
                    },
                    source='test')
            )


class ResourceTest(DBTestBase,
                   tests_db.MixinTestsWithBackendScenarios):

    def test_get_resources(self):
        expected_first_sample_timestamp = datetime.datetime(2012, 7, 2, 10, 39)
        expected_last_sample_timestamp = datetime.datetime(2012, 7, 2, 10, 40)
        msgs_sources = [msg['source'] for msg in self.msgs]
        resources = list(self.conn.get_resources())
        self.assertEqual(len(resources), 9)
        for resource in resources:
            if resource.resource_id != 'resource-id':
                continue
            self.assertEqual(resource.first_sample_timestamp,
                             expected_first_sample_timestamp)
            self.assertEqual(resource.last_sample_timestamp,
                             expected_last_sample_timestamp)
            self.assertEqual(resource.resource_id, 'resource-id')
            self.assertEqual(resource.project_id, 'project-id')
            self.assertIn(resource.source, msgs_sources)
            self.assertEqual(resource.user_id, 'user-id')
            self.assertEqual(resource.metadata['display_name'], 'test-server')
            break
        else:
            self.fail('Never found resource-id')

    def test_get_resources_start_timestamp(self):
        timestamp = datetime.datetime(2012, 7, 2, 10, 42)
        expected = set(['resource-id-2', 'resource-id-3', 'resource-id-4',
                        'resource-id-6', 'resource-id-8'])

        resources = list(self.conn.get_resources(start_timestamp=timestamp))
        resource_ids = [r.resource_id for r in resources]
        self.assertEqual(set(resource_ids), expected)

        resources = list(self.conn.get_resources(start_timestamp=timestamp,
                                                 start_timestamp_op='ge'))
        resource_ids = [r.resource_id for r in resources]
        self.assertEqual(set(resource_ids), expected)

        resources = list(self.conn.get_resources(start_timestamp=timestamp,
                                                 start_timestamp_op='gt'))
        resource_ids = [r.resource_id for r in resources]
        expected.remove('resource-id-2')
        self.assertEqual(set(resource_ids), expected)

    def test_get_resources_end_timestamp(self):
        timestamp = datetime.datetime(2012, 7, 2, 10, 42)
        expected = set(['resource-id', 'resource-id-alternate',
                        'resource-id-5', 'resource-id-7'])

        resources = list(self.conn.get_resources(end_timestamp=timestamp))
        resource_ids = [r.resource_id for r in resources]
        self.assertEqual(set(resource_ids), expected)

        resources = list(self.conn.get_resources(end_timestamp=timestamp,
                                                 end_timestamp_op='lt'))
        resource_ids = [r.resource_id for r in resources]
        self.assertEqual(set(resource_ids), expected)

        resources = list(self.conn.get_resources(end_timestamp=timestamp,
                                                 end_timestamp_op='le'))
        resource_ids = [r.resource_id for r in resources]
        expected.add('resource-id-2')
        self.assertEqual(set(resource_ids), expected)

    def test_get_resources_both_timestamps(self):
        start_ts = datetime.datetime(2012, 7, 2, 10, 42)
        end_ts = datetime.datetime(2012, 7, 2, 10, 43)

        resources = list(self.conn.get_resources(start_timestamp=start_ts,
                                                 end_timestamp=end_ts))
        resource_ids = [r.resource_id for r in resources]
        self.assertEqual(set(resource_ids), set(['resource-id-2']))

        resources = list(self.conn.get_resources(start_timestamp=start_ts,
                                                 end_timestamp=end_ts,
                                                 start_timestamp_op='ge',
                                                 end_timestamp_op='lt'))
        resource_ids = [r.resource_id for r in resources]
        self.assertEqual(set(resource_ids), set(['resource-id-2']))

        resources = list(self.conn.get_resources(start_timestamp=start_ts,
                                                 end_timestamp=end_ts,
                                                 start_timestamp_op='gt',
                                                 end_timestamp_op='lt'))
        resource_ids = [r.resource_id for r in resources]
        self.assertEqual(len(resource_ids), 0)

        resources = list(self.conn.get_resources(start_timestamp=start_ts,
                                                 end_timestamp=end_ts,
                                                 start_timestamp_op='gt',
                                                 end_timestamp_op='le'))
        resource_ids = [r.resource_id for r in resources]
        self.assertEqual(set(resource_ids), set(['resource-id-3']))

        resources = list(self.conn.get_resources(start_timestamp=start_ts,
                                                 end_timestamp=end_ts,
                                                 start_timestamp_op='ge',
                                                 end_timestamp_op='le'))
        resource_ids = [r.resource_id for r in resources]
        self.assertEqual(set(resource_ids),
                         set(['resource-id-2', 'resource-id-3']))

    def test_get_resources_by_source(self):
        resources = list(self.conn.get_resources(source='test-1'))
        self.assertEqual(len(resources), 1)
        ids = set(r.resource_id for r in resources)
        self.assertEqual(ids, set(['resource-id']))

    def test_get_resources_by_user(self):
        resources = list(self.conn.get_resources(user='user-id'))
        self.assertTrue(len(resources) == 2 or len(resources) == 1)
        ids = set(r.resource_id for r in resources)
        # tolerate storage driver only reporting latest owner of resource
        resources_ever_owned_by = set(['resource-id',
                                       'resource-id-alternate'])
        resources_now_owned_by = set(['resource-id'])
        self.assertTrue(ids == resources_ever_owned_by or
                        ids == resources_now_owned_by,
                        'unexpected resources: %s' % ids)

    def test_get_resources_by_alternate_user(self):
        resources = list(self.conn.get_resources(user='user-id-alternate'))
        self.assertEqual(1, len(resources))
        # only a single resource owned by this user ever
        self.assertEqual('resource-id-alternate', resources[0].resource_id)

    def test_get_resources_by_project(self):
        resources = list(self.conn.get_resources(project='project-id'))
        self.assertEqual(len(resources), 2)
        ids = set(r.resource_id for r in resources)
        self.assertEqual(ids, set(['resource-id', 'resource-id-alternate']))

    def test_get_resources_by_metaquery(self):
        q = {'metadata.display_name': 'test-server'}
        resources = list(self.conn.get_resources(metaquery=q))
        self.assertEqual(len(resources), 9)

    def test_get_resources_by_empty_metaquery(self):
        resources = list(self.conn.get_resources(metaquery={}))
        self.assertEqual(len(resources), 9)

    def test_get_resources_most_recent_metadata_all(self):
        resources = self.conn.get_resources()
        expected_tags = ['self.counter', 'self.counter3', 'counter-2',
                         'counter-3', 'counter-4', 'counter-5', 'counter-6',
                         'counter-7', 'counter-8']

        for resource in resources:
            self.assertIn(resource.metadata['tag'], expected_tags)

    def test_get_resources_most_recent_metadata_single(self):
        resource = list(
            self.conn.get_resources(resource='resource-id-alternate')
        )[0]
        expected_tag = 'self.counter3'
        self.assertEqual(resource.metadata['tag'], expected_tag)


class ResourceTestPagination(DBTestBase,
                             tests_db.MixinTestsWithBackendScenarios):

    def test_get_resource_all_limit(self):
        pagination = base.Pagination(limit=8)
        results = list(self.conn.get_resources(pagination=pagination))
        self.assertEqual(len(results), 8)

        pagination = base.Pagination(limit=5)
        results = list(self.conn.get_resources(pagination=pagination))
        self.assertEqual(len(results), 5)

    def test_get_resources_all_marker(self):
        pagination = base.Pagination(primary_sort_dir='asc',
                                     sort_keys=['user_id'],
                                     sort_dirs=['asc'],
                                     marker_value='resource-id-4')
        results = list(self.conn.get_resources(pagination=pagination))
        self.assertEqual(len(results), 5)

    def test_get_resources_paginate(self):
        pagination = base.Pagination(limit=3, primary_sort_dir='asc',
                                     sort_keys=['user_id'], sort_dirs=['asc'],
                                     marker_value='resource-id-4')
        results = self.conn.get_resources(pagination=pagination)
        self.assertEqual(['user-id-5', 'user-id-6', 'user-id-7'],
                         [i.user_id for i in results])

        pagination = base.Pagination(limit=2, primary_sort_dir='desc',
                                     sort_keys=['user_id'], sort_dirs=['asc'],
                                     marker_value='resource-id-4')
        results = list(self.conn.get_resources(pagination=pagination))
        self.assertEqual(['user-id-3', 'user-id-2'],
                         [i.user_id for i in results])

        pagination = base.Pagination(limit=3, primary_sort_dir='asc',
                                     sort_keys=['user_id'], sort_dirs=['asc'],
                                     marker_value='resource-id-5')
        results = list(self.conn.get_resources(pagination=pagination))
        self.assertEqual(['resource-id-6', 'resource-id-7', 'resource-id-8'],
                         [i.resource_id for i in results])


class ResourceTestOrdering(DBTestBase,
                           tests_db.MixinTestsWithBackendScenarios):
    def prepare_data(self):
        sample_timings = [('resource-id-1', [(2013, 8, 10, 10, 43),
                                             (2013, 8, 10, 10, 44),
                                             (2013, 8, 10, 10, 42),
                                             (2013, 8, 10, 10, 49),
                                             (2013, 8, 10, 10, 47)]),
                          ('resource-id-2', [(2013, 8, 10, 10, 43),
                                             (2013, 8, 10, 10, 48),
                                             (2013, 8, 10, 10, 42),
                                             (2013, 8, 10, 10, 48),
                                             (2013, 8, 10, 10, 47)]),
                          ('resource-id-3', [(2013, 8, 10, 10, 43),
                                             (2013, 8, 10, 10, 44),
                                             (2013, 8, 10, 10, 50),
                                             (2013, 8, 10, 10, 49),
                                             (2013, 8, 10, 10, 47)])]

        counter = 0
        for resource, timestamps in sample_timings:
            for timestamp in timestamps:
                self.create_and_store_sample(
                    timestamp=datetime.datetime(*timestamp),
                    resource_id=resource,
                    user_id=str(counter % 2),
                    project_id=str(counter % 3),
                    metadata={
                        'display_name': 'test-server',
                        'tag': 'sample-%s' % counter
                    },
                    source='test'
                )
                counter += 1

    def test_get_resources_ordering_all(self):
        resources = list(self.conn.get_resources())
        expected = set([
            ('resource-id-1', 'sample-3'),
            ('resource-id-2', 'sample-8'),
            ('resource-id-3', 'sample-12')
        ])
        received = set([(r.resource_id, r.metadata['tag']) for r in resources])
        self.assertEqual(received, expected)

    def test_get_resources_ordering_single(self):
        resource = list(self.conn.get_resources(resource='resource-id-2'))[0]
        self.assertEqual(resource.resource_id, 'resource-id-2')
        self.assertEqual(resource.metadata['tag'], 'sample-8')


class MeterTest(DBTestBase,
                tests_db.MixinTestsWithBackendScenarios):

    def test_get_meters(self):
        msgs_sources = [msg['source'] for msg in self.msgs]
        results = list(self.conn.get_meters())
        self.assertEqual(len(results), 9)
        for meter in results:
            self.assertIn(meter.source, msgs_sources)

    def test_get_meters_by_user(self):
        results = list(self.conn.get_meters(user='user-id'))
        self.assertEqual(len(results), 1)

    def test_get_meters_by_project(self):
        results = list(self.conn.get_meters(project='project-id'))
        self.assertEqual(len(results), 2)

    def test_get_meters_by_metaquery(self):
        q = {'metadata.display_name': 'test-server'}
        results = list(self.conn.get_meters(metaquery=q))
        self.assertIsNotEmpty(results)
        self.assertEqual(len(results), 9)

    def test_get_meters_by_empty_metaquery(self):
        results = list(self.conn.get_meters(metaquery={}))
        self.assertEqual(len(results), 9)


class MeterTestPagination(DBTestBase,
                          tests_db.MixinTestsWithBackendScenarios):

    def tet_get_meters_all_limit(self):
        pagination = base.Pagination(limit=8)
        results = list(self.conn.get_meters(pagination=pagination))
        self.assertEqual(len(results), 8)

        pagination = base.Pagination(limit=5)
        results = list(self.conn.get_meters(pagination=pagination))
        self.assertEqual(len(results), 5)

    def test_get_meters_all_marker(self):
        pagination = base.Pagination(limit=3, primary_sort_dir='desc',
                                     sort_keys=['user_id'],
                                     sort_dirs=['desc'],
                                     marker_value='resource-id-5')

        results = list(self.conn.get_meters(pagination=pagination))
        self.assertEqual(len(results), 8)

    def test_get_meters_paginate(self):
        pagination = base.Pagination(limit=3, primary_sort_dir='desc',
                                     sort_keys=['user_id'], sort_dirs=['desc'],
                                     marker_value='resource-id-5')
        results = self.conn.get_meters(pagination=pagination)
        self.assertEqual(['user-id-8', 'user-id-7', 'user-id-6'],
                         [i.user_id for i in results])

        pagination = base.Pagination(limit=3, primary_sort_dir='asc',
                                     sort_keys=['user_id'], sort_dirs=['desc'],
                                     marker_value='resource-id-5')
        results = self.conn.get_meters(pagination=pagination)
        self.assertEqual(['user-id-5', 'user-id-6', 'user-id-7'],
                         [i.user_id for i in results])

        pagination = base.Pagination(limit=2, primary_sort_dir='desc',
                                     sort_keys=['user_id'], sort_dirs=['desc'],
                                     marker_value='resource-id-5')
        results = list(self.conn.get_meters(pagination=pagination))
        self.assertEqual(['user-id-3', 'user-id-2'],
                         [i.user_id for i in results])

        pagination = base.Pagination(limit=3, primary_sort_dir='desc',
                                     sort_keys=['user_id'], sort_dirs=['desc'],
                                     marker_value='resource-id-5')
        results = self.conn.get_meters(pagination=pagination)
        self.assertEqual([], [i.user_id for i in results])


class RawSampleTest(DBTestBase,
                    tests_db.MixinTestsWithBackendScenarios):

    def test_get_samples_limit_zero(self):
        f = storage.SampleFilter()
        results = list(self.conn.get_samples(f, limit=0))
        self.assertEqual(len(results), 0)

    def test_get_samples_limit(self):
        f = storage.SampleFilter()
        results = list(self.conn.get_samples(f, limit=3))
        self.assertEqual(len(results), 3)
        for result in results:
            self.assertTimestampEqual(result.recorded_at,
                                      timeutils.utcnow())

    def test_get_samples_in_default_order(self):
        f = storage.SampleFilter()
        prev_timestamp = None
        for sample in self.conn.get_samples(f):
            if prev_timestamp is not None:
                self.assertTrue(prev_timestamp >= sample.timestamp)
            prev_timestamp = sample.timestamp

    def test_get_samples_by_user(self):
        f = storage.SampleFilter(user='user-id')
        results = list(self.conn.get_samples(f))
        self.assertEqual(len(results), 3)
        for meter in results:
            d = meter.as_dict()
            self.assertTimestampEqual(d['recorded_at'],
                                      timeutils.utcnow())
            del d['recorded_at']
            self.assertIn(d, self.msgs[:3])

    def test_get_samples_by_user_limit(self):
        f = storage.SampleFilter(user='user-id')
        results = list(self.conn.get_samples(f, limit=1))
        self.assertEqual(len(results), 1)

    def test_get_samples_by_user_limit_bigger(self):
        f = storage.SampleFilter(user='user-id')
        results = list(self.conn.get_samples(f, limit=42))
        self.assertEqual(len(results), 3)

    def test_get_samples_by_project(self):
        f = storage.SampleFilter(project='project-id')
        results = list(self.conn.get_samples(f))
        self.assertIsNotNone(results)
        for meter in results:
            d = meter.as_dict()
            self.assertTimestampEqual(d['recorded_at'],
                                      timeutils.utcnow())
            del d['recorded_at']
            self.assertIn(d, self.msgs[:4])

    def test_get_samples_by_resource(self):
        f = storage.SampleFilter(user='user-id', resource='resource-id')
        results = list(self.conn.get_samples(f))
        self.assertIsNotEmpty(results)
        meter = results[1]
        d = meter.as_dict()
        self.assertEqual(d['recorded_at'], timeutils.utcnow())
        del d['recorded_at']
        self.assertEqual(d, self.msgs[0])

    def test_get_samples_by_metaquery(self):
        q = {'metadata.display_name': 'test-server'}
        f = storage.SampleFilter(metaquery=q)
        results = list(self.conn.get_samples(f))
        self.assertIsNotNone(results)
        for meter in results:
            d = meter.as_dict()
            self.assertTimestampEqual(d['recorded_at'],
                                      timeutils.utcnow())
            del d['recorded_at']
            self.assertIn(d, self.msgs)

    def test_get_samples_by_start_time(self):
        timestamp = datetime.datetime(2012, 7, 2, 10, 41)
        f = storage.SampleFilter(
            user='user-id',
            start=timestamp,
        )

        results = list(self.conn.get_samples(f))
        self.assertEqual(len(results), 1)
        self.assertEqual(results[0].timestamp, timestamp)

        f.start_timestamp_op = 'ge'
        results = list(self.conn.get_samples(f))
        self.assertEqual(len(results), 1)
        self.assertEqual(results[0].timestamp, timestamp)

        f.start_timestamp_op = 'gt'
        results = list(self.conn.get_samples(f))
        self.assertEqual(len(results), 0)

    def test_get_samples_by_end_time(self):
        timestamp = datetime.datetime(2012, 7, 2, 10, 40)
        f = storage.SampleFilter(
            user='user-id',
            end=timestamp,
        )

        results = list(self.conn.get_samples(f))
        self.assertEqual(len(results), 1)

        f.end_timestamp_op = 'lt'
        results = list(self.conn.get_samples(f))
        self.assertEqual(len(results), 1)

        f.end_timestamp_op = 'le'
        results = list(self.conn.get_samples(f))
        self.assertEqual(len(results), 2)
        self.assertEqual(results[1].timestamp,
                         datetime.datetime(2012, 7, 2, 10, 39))

    def test_get_samples_by_both_times(self):
        start_ts = datetime.datetime(2012, 7, 2, 10, 42)
        end_ts = datetime.datetime(2012, 7, 2, 10, 43)
        f = storage.SampleFilter(
            start=start_ts,
            end=end_ts,
        )

        results = list(self.conn.get_samples(f))
        self.assertEqual(len(results), 1)
        self.assertEqual(results[0].timestamp, start_ts)

        f.start_timestamp_op = 'gt'
        f.end_timestamp_op = 'lt'
        results = list(self.conn.get_samples(f))
        self.assertEqual(len(results), 0)

        f.start_timestamp_op = 'ge'
        f.end_timestamp_op = 'lt'
        results = list(self.conn.get_samples(f))
        self.assertEqual(len(results), 1)
        self.assertEqual(results[0].timestamp, start_ts)

        f.start_timestamp_op = 'gt'
        f.end_timestamp_op = 'le'
        results = list(self.conn.get_samples(f))
        self.assertEqual(len(results), 1)
        self.assertEqual(results[0].timestamp, end_ts)

        f.start_timestamp_op = 'ge'
        f.end_timestamp_op = 'le'
        results = list(self.conn.get_samples(f))
        self.assertEqual(len(results), 2)
        self.assertEqual(results[0].timestamp, end_ts)
        self.assertEqual(results[1].timestamp, start_ts)

    def test_get_samples_by_name(self):
        f = storage.SampleFilter(user='user-id', meter='no-such-meter')
        results = list(self.conn.get_samples(f))
        self.assertIsEmpty(results)

    def test_get_samples_by_name2(self):
        f = storage.SampleFilter(user='user-id', meter='instance')
        results = list(self.conn.get_samples(f))
        self.assertIsNotEmpty(results)

    def test_get_samples_by_source(self):
        f = storage.SampleFilter(source='test-1')
        results = list(self.conn.get_samples(f))
        self.assertEqual(len(results), 2)

    def test_clear_metering_data(self):
        # NOTE(jd) Override this test in MongoDB because our code doesn't clear
        # the collections, this is handled by MongoDB TTL feature.
        if isinstance(self.conn, mongodb.Connection):
            return

        self.mock_utcnow.return_value = datetime.datetime(2012, 7, 2, 10, 45)
        self.conn.clear_expired_metering_data(3 * 60)
        f = storage.SampleFilter(meter='instance')
        results = list(self.conn.get_samples(f))
        self.assertEqual(len(results), 5)
        results = list(self.conn.get_resources())
        self.assertEqual(len(results), 5)

    def test_clear_metering_data_no_data_to_remove(self):
        # NOTE(jd) Override this test in MongoDB because our code doesn't clear
        # the collections, this is handled by MongoDB TTL feature.
        if isinstance(self.conn, mongodb.Connection):
            return

        self.mock_utcnow.return_value = datetime.datetime(2010, 7, 2, 10, 45)
        self.conn.clear_expired_metering_data(3 * 60)
        f = storage.SampleFilter(meter='instance')
        results = list(self.conn.get_samples(f))
        self.assertEqual(len(results), 11)
        results = list(self.conn.get_resources())
        self.assertEqual(len(results), 9)

    def test_clear_metering_data_with_alarms(self):
        # NOTE(jd) Override this test in MongoDB because our code doesn't clear
        # the collections, this is handled by MongoDB TTL feature.
        if isinstance(self.conn, mongodb.Connection):
            return

        alarm = models.Alarm(alarm_id='r3d',
                             enabled=True,
                             type='threshold',
                             name='red-alert',
                             description='my red-alert',
                             timestamp=None,
                             user_id='user-id',
                             project_id='project-id',
                             state="insufficient data",
                             state_timestamp=None,
                             ok_actions=[],
                             alarm_actions=['http://nowhere/alarms'],
                             insufficient_data_actions=[],
                             repeat_actions=False,
                             time_constraints=[],
                             rule=dict(comparison_operator='eq',
                                       threshold=36,
                                       statistic='count',
                                       evaluation_periods=1,
                                       period=60,
                                       meter_name='test.one',
                                       query=[{'field': 'key',
                                               'op': 'eq',
                                               'value': 'value',
                                              'type': 'string'}]),
                             )

        self.conn.create_alarm(alarm)
        self.mock_utcnow.return_value = datetime.datetime(2012, 7, 2, 10, 45)
        self.conn.clear_expired_metering_data(5)
        f = storage.SampleFilter(meter='instance')
        results = list(self.conn.get_samples(f))
        self.assertEqual(len(results), 2)
        results = list(self.conn.get_resources())
        self.assertEqual(len(results), 2)


class ComplexSampleQueryTest(DBTestBase,
                             tests_db.MixinTestsWithBackendScenarios):
    def setUp(self):
        super(ComplexSampleQueryTest, self).setUp()
        self.complex_filter = {
            "and":
            [{"or":
              [{"=": {"resource_id": "resource-id-42"}},
               {"=": {"resource_id": "resource-id-44"}}]},
             {"and":
              [{"=": {"counter_name": "cpu_util"}},
               {"and":
                [{">": {"counter_volume": 0.4}},
                 {"not": {">": {"counter_volume": 0.8}}}]}]}]}
        or_expression = [{"=": {"resource_id": "resource-id-42"}},
                         {"=": {"resource_id": "resource-id-43"}},
                         {"=": {"resource_id": "resource-id-44"}}]
        and_expression = [{">": {"counter_volume": 0.4}},
                          {"not": {">": {"counter_volume": 0.8}}}]
        self.complex_filter_list = {"and":
                                    [{"or": or_expression},
                                     {"and":
                                      [{"=": {"counter_name": "cpu_util"}},
                                       {"and": and_expression}]}]}
        in_expression = {"in": {"resource_id": ["resource-id-42",
                                                "resource-id-43",
                                                "resource-id-44"]}}
        self.complex_filter_in = {"and":
                                  [in_expression,
                                   {"and":
                                    [{"=": {"counter_name": "cpu_util"}},
                                     {"and": and_expression}]}]}

    def _create_samples(self):
        for resource in range(42, 45):
            for volume in [0.79, 0.41, 0.4, 0.8, 0.39, 0.81]:
                metadata = {'a_string_key': "meta-value" + str(volume),
                            'a_float_key': volume,
                            'an_int_key': resource,
                            'a_bool_key': (resource == 43)}

                self.create_and_store_sample(resource_id="resource-id-%s"
                                                         % resource,
                                             metadata=metadata,
                                             name="cpu_util",
                                             volume=volume)

    def test_no_filter(self):
        results = list(self.conn.query_samples())
        self.assertEqual(len(results), len(self.msgs))
        for sample in results:
            d = sample.as_dict()
            del d['recorded_at']
            self.assertIn(d, self.msgs)

    def test_no_filter_with_zero_limit(self):
        limit = 0
        results = list(self.conn.query_samples(limit=limit))
        self.assertEqual(len(results), limit)

    def test_no_filter_with_limit(self):
        limit = 3
        results = list(self.conn.query_samples(limit=limit))
        self.assertEqual(len(results), limit)

    def test_query_simple_filter(self):
        simple_filter = {"=": {"resource_id": "resource-id-8"}}
        results = list(self.conn.query_samples(filter_expr=simple_filter))
        self.assertEqual(len(results), 1)
        for sample in results:
            self.assertEqual(sample.resource_id, "resource-id-8")

    def test_query_simple_filter_with_not_equal_relation(self):
        simple_filter = {"!=": {"resource_id": "resource-id-8"}}
        results = list(self.conn.query_samples(filter_expr=simple_filter))
        self.assertEqual(len(results), len(self.msgs) - 1)
        for sample in results:
            self.assertNotEqual(sample.resource_id, "resource-id-8")

    def test_query_complex_filter(self):
        self._create_samples()
        results = list(self.conn.query_samples(filter_expr=
                                               self.complex_filter))
        self.assertEqual(len(results), 6)
        for sample in results:
            self.assertIn(sample.resource_id,
                          set(["resource-id-42", "resource-id-44"]))
            self.assertEqual(sample.counter_name,
                             "cpu_util")
            self.assertTrue(sample.counter_volume > 0.4)
            self.assertTrue(sample.counter_volume <= 0.8)

    def test_query_complex_filter_with_limit(self):
        self._create_samples()
        limit = 3
        results = list(self.conn.query_samples(filter_expr=self.complex_filter,
                                               limit=limit))
        self.assertEqual(len(results), limit)

    def test_query_complex_filter_with_simple_orderby(self):
        self._create_samples()
        expected_volume_order = [0.41, 0.41, 0.79, 0.79, 0.8, 0.8]
        orderby = [{"counter_volume": "asc"}]
        results = list(self.conn.query_samples(filter_expr=self.complex_filter,
                                               orderby=orderby))
        self.assertEqual(expected_volume_order,
                         [s.counter_volume for s in results])

    def test_query_complex_filter_with_complex_orderby(self):
        self._create_samples()
        expected_volume_order = [0.41, 0.41, 0.79, 0.79, 0.8, 0.8]
        expected_resource_id_order = ["resource-id-44", "resource-id-42",
                                      "resource-id-44", "resource-id-42",
                                      "resource-id-44", "resource-id-42"]

        orderby = [{"counter_volume": "asc"}, {"resource_id": "desc"}]

        results = list(self.conn.query_samples(filter_expr=self.complex_filter,
                       orderby=orderby))

        self.assertEqual(expected_volume_order,
                         [s.counter_volume for s in results])
        self.assertEqual(expected_resource_id_order,
                         [s.resource_id for s in results])

    def test_query_complex_filter_with_list(self):
        self._create_samples()
        results = list(
            self.conn.query_samples(filter_expr=self.complex_filter_list))
        self.assertEqual(len(results), 9)
        for sample in results:
            self.assertIn(sample.resource_id,
                          set(["resource-id-42",
                               "resource-id-43",
                               "resource-id-44"]))
            self.assertEqual(sample.counter_name,
                             "cpu_util")
            self.assertTrue(sample.counter_volume > 0.4)
            self.assertTrue(sample.counter_volume <= 0.8)

    def test_query_complex_filter_with_list_with_limit(self):
        self._create_samples()
        limit = 3
        results = list(
            self.conn.query_samples(filter_expr=self.complex_filter_list,
                                    limit=limit))
        self.assertEqual(len(results), limit)

    def test_query_complex_filter_with_list_with_simple_orderby(self):
        self._create_samples()
        expected_volume_order = [0.41, 0.41, 0.41, 0.79, 0.79,
                                 0.79, 0.8, 0.8, 0.8]
        orderby = [{"counter_volume": "asc"}]
        results = list(
            self.conn.query_samples(filter_expr=self.complex_filter_list,
                                    orderby=orderby))
        self.assertEqual(expected_volume_order,
                         [s.counter_volume for s in results])

    def test_query_complex_filterwith_list_with_complex_orderby(self):
        self._create_samples()
        expected_volume_order = [0.41, 0.41, 0.41, 0.79, 0.79,
                                 0.79, 0.8, 0.8, 0.8]
        expected_resource_id_order = ["resource-id-44", "resource-id-43",
                                      "resource-id-42", "resource-id-44",
                                      "resource-id-43", "resource-id-42",
                                      "resource-id-44", "resource-id-43",
                                      "resource-id-42"]

        orderby = [{"counter_volume": "asc"}, {"resource_id": "desc"}]

        results = list(
            self.conn.query_samples(filter_expr=self.complex_filter_list,
                                    orderby=orderby))

        self.assertEqual(expected_volume_order,
                         [s.counter_volume for s in results])
        self.assertEqual(expected_resource_id_order,
                         [s.resource_id for s in results])

    def test_query_complex_filter_with_wrong_order_in_orderby(self):
        self._create_samples()

        orderby = [{"counter_volume": "not valid order"},
                   {"resource_id": "desc"}]

        query = lambda: list(self.conn.query_samples(filter_expr=
                                                     self.complex_filter,
                                                     orderby=orderby))
        self.assertRaises(KeyError, query)

    def test_query_complex_filter_with_in(self):
        self._create_samples()
        results = list(
            self.conn.query_samples(filter_expr=self.complex_filter_in))
        self.assertEqual(len(results), 9)
        for sample in results:
            self.assertIn(sample.resource_id,
                          set(["resource-id-42",
                               "resource-id-43",
                               "resource-id-44"]))
            self.assertEqual(sample.counter_name,
                             "cpu_util")
            self.assertTrue(sample.counter_volume > 0.4)
            self.assertTrue(sample.counter_volume <= 0.8)

    def test_query_simple_metadata_filter(self):
        self._create_samples()

        filter_expr = {"=": {"resource_metadata.a_bool_key": True}}

        results = list(self.conn.query_samples(filter_expr=filter_expr))

        self.assertEqual(len(results), 6)
        for sample in results:
            self.assertTrue(sample.resource_metadata["a_bool_key"])

    def test_query_simple_metadata_with_in_op(self):
        self._create_samples()

        filter_expr = {"in": {"resource_metadata.an_int_key": [42, 43]}}

        results = list(self.conn.query_samples(filter_expr=filter_expr))

        self.assertEqual(len(results), 12)
        for sample in results:
            self.assertIn(sample.resource_metadata["an_int_key"], [42, 43])

    def test_query_complex_metadata_filter(self):
        self._create_samples()
        subfilter = {"or": [{"=": {"resource_metadata.a_string_key":
                                   "meta-value0.81"}},
                            {"<=": {"resource_metadata.a_float_key": 0.41}}]}
        filter_expr = {"and": [{">": {"resource_metadata.an_int_key": 42}},
                               subfilter]}

        results = list(self.conn.query_samples(filter_expr=filter_expr))

        self.assertEqual(len(results), 8)
        for sample in results:
            self.assertTrue((sample.resource_metadata["a_string_key"] ==
                            "meta-value0.81" or
                            sample.resource_metadata["a_float_key"] <= 0.41))
            self.assertTrue(sample.resource_metadata["an_int_key"] > 42)

    def test_query_mixed_data_and_metadata_filter(self):
        self._create_samples()
        subfilter = {"or": [{"=": {"resource_metadata.a_string_key":
                                   "meta-value0.81"}},
                            {"<=": {"resource_metadata.a_float_key": 0.41}}]}

        filter_expr = {"and": [{"=": {"resource_id": "resource-id-42"}},
                               subfilter]}

        results = list(self.conn.query_samples(filter_expr=filter_expr))

        self.assertEqual(len(results), 4)
        for sample in results:
            self.assertTrue((sample.resource_metadata["a_string_key"] ==
                            "meta-value0.81" or
                            sample.resource_metadata["a_float_key"] <= 0.41))
            self.assertEqual(sample.resource_id, "resource-id-42")

    def test_query_non_existing_metadata_with_result(self):
        self._create_samples()

        filter_expr = {
            "or": [{"=": {"resource_metadata.a_string_key":
                          "meta-value0.81"}},
                   {"<=": {"resource_metadata.key_not_exists": 0.41}}]}

        results = list(self.conn.query_samples(filter_expr=filter_expr))

        self.assertEqual(len(results), 3)
        for sample in results:
            self.assertEqual(sample.resource_metadata["a_string_key"],
                             "meta-value0.81")

    def test_query_non_existing_metadata_without_result(self):
        self._create_samples()

        filter_expr = {
            "or": [{"=": {"resource_metadata.key_not_exists":
                          "meta-value0.81"}},
                   {"<=": {"resource_metadata.key_not_exists": 0.41}}]}

        results = list(self.conn.query_samples(filter_expr=filter_expr))
        self.assertEqual(len(results), 0)

    def test_query_negated_metadata(self):
        self._create_samples()

        filter_expr = {
            "and": [{"=": {"resource_id": "resource-id-42"}},
                    {"not": {"or": [{">": {"resource_metadata.an_int_key":
                                           43}},
                                    {"<=": {"resource_metadata.a_float_key":
                                            0.41}}]}}]}

        results = list(self.conn.query_samples(filter_expr=filter_expr))

        self.assertEqual(len(results), 3)
        for sample in results:
            self.assertEqual(sample.resource_id, "resource-id-42")
            self.assertTrue(sample.resource_metadata["an_int_key"] <= 43)
            self.assertTrue(sample.resource_metadata["a_float_key"] > 0.41)

    def test_query_negated_complex_expression(self):
        self._create_samples()
        filter_expr = {
            "and":
            [{"=": {"counter_name": "cpu_util"}},
             {"not":
              {"or":
               [{"or":
                 [{"=": {"resource_id": "resource-id-42"}},
                  {"=": {"resource_id": "resource-id-44"}}]},
                {"and":
                 [{">": {"counter_volume": 0.4}},
                  {"<": {"counter_volume": 0.8}}]}]}}]}

        results = list(self.conn.query_samples(filter_expr=filter_expr))

        self.assertEqual(len(results), 4)
        for sample in results:
            self.assertEqual(sample.resource_id,
                             "resource-id-43")
            self.assertIn(sample.counter_volume, [0.39, 0.4, 0.8, 0.81])
            self.assertEqual(sample.counter_name,
                             "cpu_util")

    def test_query_with_double_negation(self):
        self._create_samples()
        filter_expr = {
            "and":
            [{"=": {"counter_name": "cpu_util"}},
             {"not":
              {"or":
               [{"or":
                 [{"=": {"resource_id": "resource-id-42"}},
                  {"=": {"resource_id": "resource-id-44"}}]},
                {"and": [{"not": {"<=": {"counter_volume": 0.4}}},
                         {"<": {"counter_volume": 0.8}}]}]}}]}

        results = list(self.conn.query_samples(filter_expr=filter_expr))

        self.assertEqual(len(results), 4)
        for sample in results:
            self.assertEqual(sample.resource_id,
                             "resource-id-43")
            self.assertIn(sample.counter_volume, [0.39, 0.4, 0.8, 0.81])
            self.assertEqual(sample.counter_name,
                             "cpu_util")

    def test_query_negate_not_equal(self):
        self._create_samples()
        filter_expr = {"not": {"!=": {"resource_id": "resource-id-43"}}}

        results = list(self.conn.query_samples(filter_expr=filter_expr))

        self.assertEqual(len(results), 6)
        for sample in results:
            self.assertEqual(sample.resource_id,
                             "resource-id-43")

    def test_query_negated_in_op(self):
        self._create_samples()
        filter_expr = {
            "and": [{"not": {"in": {"counter_volume": [0.39, 0.4, 0.79]}}},
                    {"=": {"resource_id": "resource-id-42"}}]}

        results = list(self.conn.query_samples(filter_expr=filter_expr))

        self.assertEqual(len(results), 3)
        for sample in results:
            self.assertIn(sample.counter_volume,
                          [0.41, 0.8, 0.81])


class StatisticsTest(DBTestBase,
                     tests_db.MixinTestsWithBackendScenarios):

    def prepare_data(self):
        for i in range(3):
            c = sample.Sample(
                'volume.size',
                'gauge',
                'GiB',
                5 + i,
                'user-id',
                'project1',
                'resource-id',
                timestamp=datetime.datetime(2012, 9, 25, 10 + i, 30 + i),
                resource_metadata={'display_name': 'test-volume',
                                   'tag': 'self.counter',
                                   },
                source='test',
            )
            msg = utils.meter_message_from_counter(
                c,
                secret='not-so-secret',
            )
            self.conn.record_metering_data(msg)
        for i in range(3):
            c = sample.Sample(
                'volume.size',
                'gauge',
                'GiB',
                8 + i,
                'user-5',
                'project2',
                'resource-6',
                timestamp=datetime.datetime(2012, 9, 25, 10 + i, 30 + i),
                resource_metadata={'display_name': 'test-volume',
                                   'tag': 'self.counter',
                                   },
                source='test',
            )
            msg = utils.meter_message_from_counter(
                c,
                secret='not-so-secret',
            )
            self.conn.record_metering_data(msg)
        for i in range(3):
            c = sample.Sample(
                'memory',
                'gauge',
                'MB',
                8 + i,
                'user-5',
                'project2',
                'resource-6',
                timestamp=datetime.datetime(2012, 9, 25, 10 + i, 30 + i),
                resource_metadata={},
                source='test',
            )
            msg = utils.meter_message_from_counter(
                c,
                secret='not-so-secret',
            )
            self.conn.record_metering_data(msg)

    def test_by_meter(self):
        f = storage.SampleFilter(
            meter='memory'
        )
        results = list(self.conn.get_meter_statistics(f))[0]
        self.assertEqual(results.duration,
                         (datetime.datetime(2012, 9, 25, 12, 32)
                          - datetime.datetime(2012, 9, 25, 10, 30)).seconds)
        self.assertEqual(results.count, 3)
        self.assertEqual(results.unit, 'MB')
        self.assertEqual(results.min, 8)
        self.assertEqual(results.max, 10)
        self.assertEqual(results.sum, 27)
        self.assertEqual(results.avg, 9)

    def test_by_user(self):
        f = storage.SampleFilter(
            user='user-5',
            meter='volume.size',
        )
        results = list(self.conn.get_meter_statistics(f))[0]
        self.assertEqual(results.duration,
                         (datetime.datetime(2012, 9, 25, 12, 32)
                          - datetime.datetime(2012, 9, 25, 10, 30)).seconds)
        self.assertEqual(results.count, 3)
        self.assertEqual(results.unit, 'GiB')
        self.assertEqual(results.min, 8)
        self.assertEqual(results.max, 10)
        self.assertEqual(results.sum, 27)
        self.assertEqual(results.avg, 9)

    def test_no_period_in_query(self):
        f = storage.SampleFilter(
            user='user-5',
            meter='volume.size',
        )
        results = list(self.conn.get_meter_statistics(f))[0]
        self.assertEqual(results.period, 0)

    def test_period_is_int(self):
        f = storage.SampleFilter(
            meter='volume.size',
        )
        results = list(self.conn.get_meter_statistics(f))[0]
        self.assertIs(type(results.period), int)
        self.assertEqual(results.count, 6)

    def test_by_user_period(self):
        f = storage.SampleFilter(
            user='user-5',
            meter='volume.size',
            start='2012-09-25T10:28:00',
        )
        results = list(self.conn.get_meter_statistics(f, period=7200))
        self.assertEqual(len(results), 2)
        self.assertEqual(set(r.period_start for r in results),
                         set([datetime.datetime(2012, 9, 25, 10, 28),
                              datetime.datetime(2012, 9, 25, 12, 28)]))
        self.assertEqual(set(r.period_end for r in results),
                         set([datetime.datetime(2012, 9, 25, 12, 28),
                              datetime.datetime(2012, 9, 25, 14, 28)]))
        r = results[0]
        self.assertEqual(r.period_start,
                         datetime.datetime(2012, 9, 25, 10, 28))
        self.assertEqual(r.count, 2)
        self.assertEqual(r.unit, 'GiB')
        self.assertEqual(r.avg, 8.5)
        self.assertEqual(r.min, 8)
        self.assertEqual(r.max, 9)
        self.assertEqual(r.sum, 17)
        self.assertEqual(r.period, 7200)
        self.assertIsInstance(r.period, int)
        expected_end = r.period_start + datetime.timedelta(seconds=7200)
        self.assertEqual(r.period_end, expected_end)
        self.assertEqual(r.duration, 3660)
        self.assertEqual(r.duration_start,
                         datetime.datetime(2012, 9, 25, 10, 30))
        self.assertEqual(r.duration_end,
                         datetime.datetime(2012, 9, 25, 11, 31))

    def test_by_user_period_with_timezone(self):
        dates = [
            '2012-09-25T00:28:00-10:00',
            '2012-09-25T01:28:00-09:00',
            '2012-09-25T02:28:00-08:00',
            '2012-09-25T03:28:00-07:00',
            '2012-09-25T04:28:00-06:00',
            '2012-09-25T05:28:00-05:00',
            '2012-09-25T06:28:00-04:00',
            '2012-09-25T07:28:00-03:00',
            '2012-09-25T08:28:00-02:00',
            '2012-09-25T09:28:00-01:00',
            '2012-09-25T10:28:00Z',
            '2012-09-25T11:28:00+01:00',
            '2012-09-25T12:28:00+02:00',
            '2012-09-25T13:28:00+03:00',
            '2012-09-25T14:28:00+04:00',
            '2012-09-25T15:28:00+05:00',
            '2012-09-25T16:28:00+06:00',
            '2012-09-25T17:28:00+07:00',
            '2012-09-25T18:28:00+08:00',
            '2012-09-25T19:28:00+09:00',
            '2012-09-25T20:28:00+10:00',
            '2012-09-25T21:28:00+11:00',
            '2012-09-25T22:28:00+12:00',
        ]
        for date in dates:
            f = storage.SampleFilter(
                user='user-5',
                meter='volume.size',
                start=date
            )
            results = list(self.conn.get_meter_statistics(f, period=7200))
            self.assertEqual(len(results), 2)
            self.assertEqual(set(r.period_start for r in results),
                             set([datetime.datetime(2012, 9, 25, 10, 28),
                                  datetime.datetime(2012, 9, 25, 12, 28)]))
            self.assertEqual(set(r.period_end for r in results),
                             set([datetime.datetime(2012, 9, 25, 12, 28),
                                  datetime.datetime(2012, 9, 25, 14, 28)]))

    def test_by_user_period_start_end(self):
        f = storage.SampleFilter(
            user='user-5',
            meter='volume.size',
            start='2012-09-25T10:28:00',
            end='2012-09-25T11:28:00',
        )
        results = list(self.conn.get_meter_statistics(f, period=1800))
        self.assertEqual(len(results), 1)
        r = results[0]
        self.assertEqual(r.period_start,
                         datetime.datetime(2012, 9, 25, 10, 28))
        self.assertEqual(r.count, 1)
        self.assertEqual(r.unit, 'GiB')
        self.assertEqual(r.avg, 8)
        self.assertEqual(r.min, 8)
        self.assertEqual(r.max, 8)
        self.assertEqual(r.sum, 8)
        self.assertEqual(r.period, 1800)
        self.assertEqual(r.period_end,
                         r.period_start + datetime.timedelta(seconds=1800))
        self.assertEqual(r.duration, 0)
        self.assertEqual(r.duration_start,
                         datetime.datetime(2012, 9, 25, 10, 30))
        self.assertEqual(r.duration_end,
                         datetime.datetime(2012, 9, 25, 10, 30))

    def test_by_project(self):
        f = storage.SampleFilter(
            meter='volume.size',
            resource='resource-id',
            start='2012-09-25T11:30:00',
            end='2012-09-25T11:32:00',
        )
        results = list(self.conn.get_meter_statistics(f))[0]
        self.assertEqual(results.duration, 0)
        self.assertEqual(results.count, 1)
        self.assertEqual(results.unit, 'GiB')
        self.assertEqual(results.min, 6)
        self.assertEqual(results.max, 6)
        self.assertEqual(results.sum, 6)
        self.assertEqual(results.avg, 6)

    def test_one_resource(self):
        f = storage.SampleFilter(
            user='user-id',
            meter='volume.size',
        )
        results = list(self.conn.get_meter_statistics(f))[0]
        self.assertEqual(results.duration,
                         (datetime.datetime(2012, 9, 25, 12, 32)
                          - datetime.datetime(2012, 9, 25, 10, 30)).seconds)
        self.assertEqual(results.count, 3)
        self.assertEqual(results.unit, 'GiB')
        self.assertEqual(results.min, 5)
        self.assertEqual(results.max, 7)
        self.assertEqual(results.sum, 18)
        self.assertEqual(results.avg, 6)

    def test_with_no_sample(self):
        f = storage.SampleFilter(
            user='user-not-exists',
            meter='volume.size',
        )
        results = list(self.conn.get_meter_statistics(f, period=1800))
        self.assertEqual([], results)


class StatisticsGroupByTest(DBTestBase,
                            tests_db.MixinTestsWithBackendScenarios):

    def prepare_data(self):
        test_sample_data = (
            {'volume': 2, 'user': 'user-1', 'project': 'project-1',
             'resource': 'resource-1', 'timestamp': (2013, 8, 1, 16, 10),
             'metadata_flavor': 'm1.tiny', 'metadata_event': 'event-1',
             'source': 'source-2'},
            {'volume': 2, 'user': 'user-1', 'project': 'project-2',
             'resource': 'resource-1', 'timestamp': (2013, 8, 1, 15, 37),
             'metadata_flavor': 'm1.large', 'metadata_event': 'event-1',
             'source': 'source-2'},
            {'volume': 1, 'user': 'user-2', 'project': 'project-1',
             'resource': 'resource-2', 'timestamp': (2013, 8, 1, 10, 11),
             'metadata_flavor': 'm1.tiny', 'metadata_event': 'event-2',
             'source': 'source-1'},
            {'volume': 1, 'user': 'user-2', 'project': 'project-1',
             'resource': 'resource-2', 'timestamp': (2013, 8, 1, 10, 40),
             'metadata_flavor': 'm1.large', 'metadata_event': 'event-2',
             'source': 'source-1'},
            {'volume': 2, 'user': 'user-2', 'project': 'project-1',
             'resource': 'resource-1', 'timestamp': (2013, 8, 1, 14, 59),
             'metadata_flavor': 'm1.large', 'metadata_event': 'event-2',
             'source': 'source-1'},
            {'volume': 4, 'user': 'user-2', 'project': 'project-2',
             'resource': 'resource-2', 'timestamp': (2013, 8, 1, 17, 28),
             'metadata_flavor': 'm1.large', 'metadata_event': 'event-2',
             'source': 'source-1'},
            {'volume': 4, 'user': 'user-3', 'project': 'project-1',
             'resource': 'resource-3', 'timestamp': (2013, 8, 1, 11, 22),
             'metadata_flavor': 'm1.tiny', 'metadata_event': 'event-2',
             'source': 'source-3'},
        )

        for test_sample in test_sample_data:
            c = sample.Sample(
                'instance',
                sample.TYPE_CUMULATIVE,
                unit='s',
                volume=test_sample['volume'],
                user_id=test_sample['user'],
                project_id=test_sample['project'],
                resource_id=test_sample['resource'],
                timestamp=datetime.datetime(*test_sample['timestamp']),
                resource_metadata={'flavor': test_sample['metadata_flavor'],
                                   'event': test_sample['metadata_event'], },
                source=test_sample['source'],
            )
            msg = utils.meter_message_from_counter(
                c,
                self.CONF.publisher.metering_secret,
            )
            self.conn.record_metering_data(msg)

    def test_group_by_user(self):
        f = storage.SampleFilter(
            meter='instance',
        )
        results = list(self.conn.get_meter_statistics(f, groupby=['user_id']))
        self.assertEqual(len(results), 3)
        groupby_list = [r.groupby for r in results]
        groupby_keys_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.keys())
        groupby_vals_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.values())
        self.assertEqual(groupby_keys_set, set(['user_id']))
        self.assertEqual(groupby_vals_set, set(['user-1', 'user-2', 'user-3']))

        for r in results:
            if r.groupby == {'user_id': 'user-1'}:
                self.assertEqual(r.count, 2)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 2)
                self.assertEqual(r.max, 2)
                self.assertEqual(r.sum, 4)
                self.assertEqual(r.avg, 2)
            elif r.groupby == {'user_id': 'user-2'}:
                self.assertEqual(r.count, 4)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 1)
                self.assertEqual(r.max, 4)
                self.assertEqual(r.sum, 8)
                self.assertEqual(r.avg, 2)
            elif r.groupby == {'user_id': 'user-3'}:
                self.assertEqual(r.count, 1)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 4)
                self.assertEqual(r.max, 4)
                self.assertEqual(r.sum, 4)
                self.assertEqual(r.avg, 4)

    def test_group_by_resource(self):
        f = storage.SampleFilter(
            meter='instance',
        )
        results = list(self.conn.get_meter_statistics(f,
                                                      groupby=['resource_id']))
        self.assertEqual(len(results), 3)
        groupby_list = [r.groupby for r in results]
        groupby_keys_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.keys())
        groupby_vals_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.values())
        self.assertEqual(groupby_keys_set, set(['resource_id']))
        self.assertEqual(groupby_vals_set, set(['resource-1',
                                                'resource-2',
                                                'resource-3']))
        for r in results:
            if r.groupby == {'resource_id': 'resource-1'}:
                self.assertEqual(r.count, 3)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 2)
                self.assertEqual(r.max, 2)
                self.assertEqual(r.sum, 6)
                self.assertEqual(r.avg, 2)
            elif r.groupby == {'resource_id': 'resource-2'}:
                self.assertEqual(r.count, 3)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 1)
                self.assertEqual(r.max, 4)
                self.assertEqual(r.sum, 6)
                self.assertEqual(r.avg, 2)
            elif r.groupby == {'resource_id': 'resource-3'}:
                self.assertEqual(r.count, 1)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 4)
                self.assertEqual(r.max, 4)
                self.assertEqual(r.sum, 4)
                self.assertEqual(r.avg, 4)

    def test_group_by_project(self):
        f = storage.SampleFilter(
            meter='instance',
        )
        results = list(self.conn.get_meter_statistics(f,
                                                      groupby=['project_id']))
        self.assertEqual(len(results), 2)
        groupby_list = [r.groupby for r in results]
        groupby_keys_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.keys())
        groupby_vals_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.values())
        self.assertEqual(groupby_keys_set, set(['project_id']))
        self.assertEqual(groupby_vals_set, set(['project-1', 'project-2']))

        for r in results:
            if r.groupby == {'project_id': 'project-1'}:
                self.assertEqual(r.count, 5)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 1)
                self.assertEqual(r.max, 4)
                self.assertEqual(r.sum, 10)
                self.assertEqual(r.avg, 2)
            elif r.groupby == {'project_id': 'project-2'}:
                self.assertEqual(r.count, 2)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 2)
                self.assertEqual(r.max, 4)
                self.assertEqual(r.sum, 6)
                self.assertEqual(r.avg, 3)

    def test_group_by_source(self):
        f = storage.SampleFilter(
            meter='instance',
        )
        results = list(self.conn.get_meter_statistics(f, groupby=['source']))
        self.assertEqual(len(results), 3)
        groupby_list = [r.groupby for r in results]
        groupby_keys_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.keys())
        groupby_vals_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.values())
        self.assertEqual(groupby_keys_set, set(['source']))
        self.assertEqual(groupby_vals_set, set(['source-1',
                                                'source-2',
                                                'source-3']))

        for r in results:
            if r.groupby == {'source': 'source-1'}:
                self.assertEqual(r.count, 4)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 1)
                self.assertEqual(r.max, 4)
                self.assertEqual(r.sum, 8)
                self.assertEqual(r.avg, 2)
            elif r.groupby == {'source': 'source-2'}:
                self.assertEqual(r.count, 2)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 2)
                self.assertEqual(r.max, 2)
                self.assertEqual(r.sum, 4)
                self.assertEqual(r.avg, 2)
            elif r.groupby == {'source': 'source-3'}:
                self.assertEqual(r.count, 1)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 4)
                self.assertEqual(r.max, 4)
                self.assertEqual(r.sum, 4)
                self.assertEqual(r.avg, 4)

    def test_group_by_unknown_field(self):
        f = storage.SampleFilter(
            meter='instance',
        )
        # NOTE(terriyu): The MongoDB get_meter_statistics() returns a list
        # whereas the SQLAlchemy get_meter_statistics() returns a generator.
        # You have to apply list() to the SQLAlchemy generator to get it to
        # throw an error. The MongoDB get_meter_statistics() will throw an
        # error before list() is called. By using lambda, we can cover both
        # MongoDB and SQLAlchemy in a single test.
        self.assertRaises(
            NotImplementedError,
            lambda: list(self.conn.get_meter_statistics(f, groupby=['wtf']))
        )

    def test_group_by_metadata(self):
        # TODO(terriyu): test_group_by_metadata needs to be implemented.
        # This test should check grouping by a single metadata field.
        pass

    def test_group_by_multiple_regular(self):
        f = storage.SampleFilter(
            meter='instance',
        )
        results = list(self.conn.get_meter_statistics(f,
                                                      groupby=['user_id',
                                                               'resource_id']))
        self.assertEqual(len(results), 4)
        groupby_list = [r.groupby for r in results]
        groupby_keys_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.keys())
        groupby_vals_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.values())
        self.assertEqual(groupby_keys_set, set(['user_id', 'resource_id']))
        self.assertEqual(groupby_vals_set, set(['user-1', 'user-2',
                                                'user-3', 'resource-1',
                                                'resource-2', 'resource-3']))

        for r in results:
            if r.groupby == {'user_id': 'user-1', 'resource_id': 'resource-1'}:
                self.assertEqual(r.count, 2)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 2)
                self.assertEqual(r.max, 2)
                self.assertEqual(r.sum, 4)
                self.assertEqual(r.avg, 2)
            elif r.groupby == {'user_id': 'user-2',
                               'resource_id': 'resource-1'}:
                self.assertEqual(r.count, 1)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 2)
                self.assertEqual(r.max, 2)
                self.assertEqual(r.sum, 2)
                self.assertEqual(r.avg, 2)
            elif r.groupby == {'user_id': 'user-2',
                               'resource_id': 'resource-2'}:
                self.assertEqual(r.count, 3)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 1)
                self.assertEqual(r.max, 4)
                self.assertEqual(r.sum, 6)
                self.assertEqual(r.avg, 2)
            elif r.groupby == {'user_id': 'user-3',
                               'resource_id': 'resource-3'}:
                self.assertEqual(r.count, 1)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 4)
                self.assertEqual(r.max, 4)
                self.assertEqual(r.sum, 4)
                self.assertEqual(r.avg, 4)
            else:
                self.assertNotEqual(r.groupby, {'user_id': 'user-1',
                                                'resource_id': 'resource-2'})
                self.assertNotEqual(r.groupby, {'user_id': 'user-1',
                                                'resource_id': 'resource-3'})
                self.assertNotEqual(r.groupby, {'user_id': 'user-2',
                                                'resource_id': 'resource-3'})
                self.assertNotEqual(r.groupby, {'user_id': 'user-3',
                                                'resource_id': 'resource-1'})
                self.assertNotEqual(r.groupby, {'user_id': 'user-3',
                                                'resource_id': 'resource-2'})

    def test_group_by_multiple_metadata(self):
        # TODO(terriyu): test_group_by_multiple_metadata needs to be
        # implemented.
        # This test should check grouping by multiple metadata fields.
        pass

    def test_group_by_multiple_regular_metadata(self):
        # TODO(terriyu): test_group_by_multiple_regular_metadata needs to be
        # implemented.
        # This test should check grouping by a combination of regular and
        # metadata fields.
        pass

    def test_group_by_with_query_filter(self):
        f = storage.SampleFilter(
            meter='instance',
            project='project-1',
        )
        results = list(self.conn.get_meter_statistics(
            f,
            groupby=['resource_id']))
        self.assertEqual(len(results), 3)
        groupby_list = [r.groupby for r in results]
        groupby_keys_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.keys())
        groupby_vals_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.values())
        self.assertEqual(groupby_keys_set, set(['resource_id']))
        self.assertEqual(groupby_vals_set, set(['resource-1',
                                                'resource-2',
                                                'resource-3']))

        for r in results:
            if r.groupby == {'resource_id': 'resource-1'}:
                self.assertEqual(r.count, 2)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 2)
                self.assertEqual(r.max, 2)
                self.assertEqual(r.sum, 4)
                self.assertEqual(r.avg, 2)
            elif r.groupby == {'resource_id': 'resource-2'}:
                self.assertEqual(r.count, 2)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 1)
                self.assertEqual(r.max, 1)
                self.assertEqual(r.sum, 2)
                self.assertEqual(r.avg, 1)
            elif r.groupby == {'resource_id': 'resource-3'}:
                self.assertEqual(r.count, 1)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 4)
                self.assertEqual(r.max, 4)
                self.assertEqual(r.sum, 4)
                self.assertEqual(r.avg, 4)

    def test_group_by_metadata_with_query_filter(self):
        # TODO(terriyu): test_group_by_metadata_with_query_filter needs to be
        # implemented.
        # This test should check grouping by a metadata field in combination
        # with a query filter.
        pass

    def test_group_by_with_query_filter_multiple(self):
        f = storage.SampleFilter(
            meter='instance',
            user='user-2',
            source='source-1',
        )
        results = list(self.conn.get_meter_statistics(
            f,
            groupby=['project_id', 'resource_id']))
        self.assertEqual(len(results), 3)
        groupby_list = [r.groupby for r in results]
        groupby_keys_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.keys())
        groupby_vals_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.values())
        self.assertEqual(groupby_keys_set, set(['project_id', 'resource_id']))
        self.assertEqual(groupby_vals_set, set(['project-1', 'project-2',
                                                'resource-1', 'resource-2']))

        for r in results:
            if r.groupby == {'project_id': 'project-1',
                             'resource_id': 'resource-1'}:
                self.assertEqual(r.count, 1)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 2)
                self.assertEqual(r.max, 2)
                self.assertEqual(r.sum, 2)
                self.assertEqual(r.avg, 2)
            elif r.groupby == {'project_id': 'project-1',
                               'resource_id': 'resource-2'}:
                self.assertEqual(r.count, 2)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 1)
                self.assertEqual(r.max, 1)
                self.assertEqual(r.sum, 2)
                self.assertEqual(r.avg, 1)
            elif r.groupby == {'project_id': 'project-2',
                               'resource_id': 'resource-2'}:
                self.assertEqual(r.count, 1)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 4)
                self.assertEqual(r.max, 4)
                self.assertEqual(r.sum, 4)
                self.assertEqual(r.avg, 4)
            else:
                self.assertNotEqual(r.groupby, {'project_id': 'project-2',
                                                'resource_id': 'resource-1'})

    def test_group_by_metadata_with_query_filter_multiple(self):
        # TODO(terriyu): test_group_by_metadata_with_query_filter_multiple
        # needs to be implemented.
        # This test should check grouping by multiple metadata fields in
        # combination with a query filter.
        pass

    def test_group_by_with_period(self):
        f = storage.SampleFilter(
            meter='instance',
        )
        results = list(self.conn.get_meter_statistics(f,
                                                      period=7200,
                                                      groupby=['project_id']))
        self.assertEqual(len(results), 4)
        groupby_list = [r.groupby for r in results]
        groupby_keys_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.keys())
        groupby_vals_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.values())
        self.assertEqual(groupby_keys_set, set(['project_id']))
        self.assertEqual(groupby_vals_set, set(['project-1', 'project-2']))
        period_start_set = set([r.period_start for r in results])
        period_start_valid = set([datetime.datetime(2013, 8, 1, 10, 11),
                                  datetime.datetime(2013, 8, 1, 14, 11),
                                  datetime.datetime(2013, 8, 1, 16, 11)])
        self.assertEqual(period_start_set, period_start_valid)

        for r in results:
            if (r.groupby == {'project_id': 'project-1'} and
                    r.period_start == datetime.datetime(2013, 8, 1, 10, 11)):
                self.assertEqual(r.count, 3)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 1)
                self.assertEqual(r.max, 4)
                self.assertEqual(r.sum, 6)
                self.assertEqual(r.avg, 2)
                self.assertEqual(r.duration, 4260)
                self.assertEqual(r.duration_start,
                                 datetime.datetime(2013, 8, 1, 10, 11))
                self.assertEqual(r.duration_end,
                                 datetime.datetime(2013, 8, 1, 11, 22))
                self.assertEqual(r.period, 7200)
                self.assertEqual(r.period_end,
                                 datetime.datetime(2013, 8, 1, 12, 11))
            elif (r.groupby == {'project_id': 'project-1'} and
                    r.period_start == datetime.datetime(2013, 8, 1, 14, 11)):
                self.assertEqual(r.count, 2)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 2)
                self.assertEqual(r.max, 2)
                self.assertEqual(r.sum, 4)
                self.assertEqual(r.avg, 2)
                self.assertEqual(r.duration, 4260)
                self.assertEqual(r.duration_start,
                                 datetime.datetime(2013, 8, 1, 14, 59))
                self.assertEqual(r.duration_end,
                                 datetime.datetime(2013, 8, 1, 16, 10))
                self.assertEqual(r.period, 7200)
                self.assertEqual(r.period_end,
                                 datetime.datetime(2013, 8, 1, 16, 11))
            elif (r.groupby == {'project_id': 'project-2'} and
                    r.period_start == datetime.datetime(2013, 8, 1, 14, 11)):
                self.assertEqual(r.count, 1)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 2)
                self.assertEqual(r.max, 2)
                self.assertEqual(r.sum, 2)
                self.assertEqual(r.avg, 2)
                self.assertEqual(r.duration, 0)
                self.assertEqual(r.duration_start,
                                 datetime.datetime(2013, 8, 1, 15, 37))
                self.assertEqual(r.duration_end,
                                 datetime.datetime(2013, 8, 1, 15, 37))
                self.assertEqual(r.period, 7200)
                self.assertEqual(r.period_end,
                                 datetime.datetime(2013, 8, 1, 16, 11))
            elif (r.groupby == {'project_id': 'project-2'} and
                    r.period_start == datetime.datetime(2013, 8, 1, 16, 11)):
                self.assertEqual(r.count, 1)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 4)
                self.assertEqual(r.max, 4)
                self.assertEqual(r.sum, 4)
                self.assertEqual(r.avg, 4)
                self.assertEqual(r.duration, 0)
                self.assertEqual(r.duration_start,
                                 datetime.datetime(2013, 8, 1, 17, 28))
                self.assertEqual(r.duration_end,
                                 datetime.datetime(2013, 8, 1, 17, 28))
                self.assertEqual(r.period, 7200)
                self.assertEqual(r.period_end,
                                 datetime.datetime(2013, 8, 1, 18, 11))
            else:
                self.assertNotEqual([r.groupby, r.period_start],
                                    [{'project_id': 'project-1'},
                                     datetime.datetime(2013, 8, 1, 16, 11)])
                self.assertNotEqual([r.groupby, r.period_start],
                                    [{'project_id': 'project-2'},
                                     datetime.datetime(2013, 8, 1, 10, 11)])

    def test_group_by_metadata_with_period(self):
        # TODO(terriyu): test_group_by_metadata_with_period needs to be
        # implemented.
        # This test should check grouping by metadata fields in combination
        # with period grouping.
        pass

    def test_group_by_with_query_filter_and_period(self):
        f = storage.SampleFilter(
            meter='instance',
            source='source-1',
        )
        results = list(self.conn.get_meter_statistics(f,
                                                      period=7200,
                                                      groupby=['project_id']))
        self.assertEqual(len(results), 3)
        groupby_list = [r.groupby for r in results]
        groupby_keys_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.keys())
        groupby_vals_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.values())
        self.assertEqual(groupby_keys_set, set(['project_id']))
        self.assertEqual(groupby_vals_set, set(['project-1', 'project-2']))
        period_start_set = set([r.period_start for r in results])
        period_start_valid = set([datetime.datetime(2013, 8, 1, 10, 11),
                                  datetime.datetime(2013, 8, 1, 14, 11),
                                  datetime.datetime(2013, 8, 1, 16, 11)])
        self.assertEqual(period_start_set, period_start_valid)

        for r in results:
            if (r.groupby == {'project_id': 'project-1'} and
                    r.period_start == datetime.datetime(2013, 8, 1, 10, 11)):
                self.assertEqual(r.count, 2)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 1)
                self.assertEqual(r.max, 1)
                self.assertEqual(r.sum, 2)
                self.assertEqual(r.avg, 1)
                self.assertEqual(r.duration, 1740)
                self.assertEqual(r.duration_start,
                                 datetime.datetime(2013, 8, 1, 10, 11))
                self.assertEqual(r.duration_end,
                                 datetime.datetime(2013, 8, 1, 10, 40))
                self.assertEqual(r.period, 7200)
                self.assertEqual(r.period_end,
                                 datetime.datetime(2013, 8, 1, 12, 11))
            elif (r.groupby == {'project_id': 'project-1'} and
                    r.period_start == datetime.datetime(2013, 8, 1, 14, 11)):
                self.assertEqual(r.count, 1)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 2)
                self.assertEqual(r.max, 2)
                self.assertEqual(r.sum, 2)
                self.assertEqual(r.avg, 2)
                self.assertEqual(r.duration, 0)
                self.assertEqual(r.duration_start,
                                 datetime.datetime(2013, 8, 1, 14, 59))
                self.assertEqual(r.duration_end,
                                 datetime.datetime(2013, 8, 1, 14, 59))
                self.assertEqual(r.period, 7200)
                self.assertEqual(r.period_end,
                                 datetime.datetime(2013, 8, 1, 16, 11))
            elif (r.groupby == {'project_id': 'project-2'} and
                    r.period_start == datetime.datetime(2013, 8, 1, 16, 11)):
                self.assertEqual(r.count, 1)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 4)
                self.assertEqual(r.max, 4)
                self.assertEqual(r.sum, 4)
                self.assertEqual(r.avg, 4)
                self.assertEqual(r.duration, 0)
                self.assertEqual(r.duration_start,
                                 datetime.datetime(2013, 8, 1, 17, 28))
                self.assertEqual(r.duration_end,
                                 datetime.datetime(2013, 8, 1, 17, 28))
                self.assertEqual(r.period, 7200)
                self.assertEqual(r.period_end,
                                 datetime.datetime(2013, 8, 1, 18, 11))
            else:
                self.assertNotEqual([r.groupby, r.period_start],
                                    [{'project_id': 'project-1'},
                                     datetime.datetime(2013, 8, 1, 16, 11)])
                self.assertNotEqual([r.groupby, r.period_start],
                                    [{'project_id': 'project-2'},
                                     datetime.datetime(2013, 8, 1, 10, 11)])

    def test_group_by_metadata_with_query_filter_and_period(self):
        # TODO(terriyu): test_group_by_metadata_with_query_filter_and_period
        # needs to be implemented.
        # This test should check grouping with metadata fields in combination
        # with a query filter and period grouping.
        pass

    def test_group_by_start_timestamp_after(self):
        f = storage.SampleFilter(
            meter='instance',
            start=datetime.datetime(2013, 8, 1, 17, 28, 1),
        )
        results = list(self.conn.get_meter_statistics(f,
                                                      groupby=['project_id']))

        self.assertEqual(results, [])

    def test_group_by_end_timestamp_before(self):
        f = storage.SampleFilter(
            meter='instance',
            end=datetime.datetime(2013, 8, 1, 10, 10, 59),
        )
        results = list(self.conn.get_meter_statistics(f,
                                                      groupby=['project_id']))

        self.assertEqual(results, [])

    def test_group_by_start_timestamp(self):
        f = storage.SampleFilter(
            meter='instance',
            start=datetime.datetime(2013, 8, 1, 14, 58),
        )
        results = list(self.conn.get_meter_statistics(f,
                                                      groupby=['project_id']))
        self.assertEqual(len(results), 2)
        groupby_list = [r.groupby for r in results]
        groupby_keys_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.keys())
        groupby_vals_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.values())
        self.assertEqual(groupby_keys_set, set(['project_id']))
        self.assertEqual(groupby_vals_set, set(['project-1', 'project-2']))

        for r in results:
            if r.groupby == {'project_id': 'project-1'}:
                self.assertEqual(r.count, 2)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 2)
                self.assertEqual(r.max, 2)
                self.assertEqual(r.sum, 4)
                self.assertEqual(r.avg, 2)
            elif r.groupby == {'project_id': 'project-2'}:
                self.assertEqual(r.count, 2)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 2)
                self.assertEqual(r.max, 4)
                self.assertEqual(r.sum, 6)
                self.assertEqual(r.avg, 3)

    def test_group_by_end_timestamp(self):
        f = storage.SampleFilter(
            meter='instance',
            end=datetime.datetime(2013, 8, 1, 11, 45),
        )
        results = list(self.conn.get_meter_statistics(f,
                                                      groupby=['project_id']))
        self.assertEqual(len(results), 1)
        groupby_list = [r.groupby for r in results]
        groupby_keys_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.keys())
        groupby_vals_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.values())
        self.assertEqual(groupby_keys_set, set(['project_id']))
        self.assertEqual(groupby_vals_set, set(['project-1']))

        for r in results:
            if r.groupby == {'project_id': 'project-1'}:
                self.assertEqual(r.count, 3)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 1)
                self.assertEqual(r.max, 4)
                self.assertEqual(r.sum, 6)
                self.assertEqual(r.avg, 2)

    def test_group_by_start_end_timestamp(self):
        f = storage.SampleFilter(
            meter='instance',
            start=datetime.datetime(2013, 8, 1, 8, 17, 3),
            end=datetime.datetime(2013, 8, 1, 23, 59, 59),
        )
        results = list(self.conn.get_meter_statistics(f,
                                                      groupby=['project_id']))
        self.assertEqual(len(results), 2)
        groupby_list = [r.groupby for r in results]
        groupby_keys_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.keys())
        groupby_vals_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.values())
        self.assertEqual(groupby_keys_set, set(['project_id']))
        self.assertEqual(groupby_vals_set, set(['project-1', 'project-2']))

        for r in results:
            if r.groupby == {'project_id': 'project-1'}:
                self.assertEqual(r.count, 5)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 1)
                self.assertEqual(r.max, 4)
                self.assertEqual(r.sum, 10)
                self.assertEqual(r.avg, 2)
            elif r.groupby == {'project_id': 'project-2'}:
                self.assertEqual(r.count, 2)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 2)
                self.assertEqual(r.max, 4)
                self.assertEqual(r.sum, 6)
                self.assertEqual(r.avg, 3)

    def test_group_by_start_end_timestamp_with_query_filter(self):
        f = storage.SampleFilter(
            meter='instance',
            project='project-1',
            start=datetime.datetime(2013, 8, 1, 11, 1),
            end=datetime.datetime(2013, 8, 1, 20, 0),
        )
        results = list(self.conn.get_meter_statistics(f,
                                                      groupby=['resource_id']))
        groupby_list = [r.groupby for r in results]
        groupby_keys_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.keys())
        groupby_vals_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.values())
        self.assertEqual(groupby_keys_set, set(['resource_id']))
        self.assertEqual(groupby_vals_set, set(['resource-1', 'resource-3']))

        for r in results:
            if r.groupby == {'resource_id': 'resource-1'}:
                self.assertEqual(r.count, 2)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 2)
                self.assertEqual(r.max, 2)
                self.assertEqual(r.sum, 4)
                self.assertEqual(r.avg, 2)
            elif r.groupby == {'resource_id': 'resource-3'}:
                self.assertEqual(r.count, 1)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 4)
                self.assertEqual(r.max, 4)
                self.assertEqual(r.sum, 4)
                self.assertEqual(r.avg, 4)

    def test_group_by_start_end_timestamp_with_period(self):
        f = storage.SampleFilter(
            meter='instance',
            start=datetime.datetime(2013, 8, 1, 14, 0),
            end=datetime.datetime(2013, 8, 1, 17, 0),
        )
        results = list(self.conn.get_meter_statistics(f,
                                                      period=3600,
                                                      groupby=['project_id']))
        self.assertEqual(len(results), 3)
        groupby_list = [r.groupby for r in results]
        groupby_keys_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.keys())
        groupby_vals_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.values())
        self.assertEqual(groupby_keys_set, set(['project_id']))
        self.assertEqual(groupby_vals_set, set(['project-1', 'project-2']))
        period_start_set = set([r.period_start for r in results])
        period_start_valid = set([datetime.datetime(2013, 8, 1, 14, 0),
                                  datetime.datetime(2013, 8, 1, 15, 0),
                                  datetime.datetime(2013, 8, 1, 16, 0)])
        self.assertEqual(period_start_set, period_start_valid)

        for r in results:
            if (r.groupby == {'project_id': 'project-1'} and
                    r.period_start == datetime.datetime(2013, 8, 1, 14, 0)):
                self.assertEqual(r.count, 1)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 2)
                self.assertEqual(r.max, 2)
                self.assertEqual(r.sum, 2)
                self.assertEqual(r.avg, 2)
                self.assertEqual(r.duration, 0)
                self.assertEqual(r.duration_start,
                                 datetime.datetime(2013, 8, 1, 14, 59))
                self.assertEqual(r.duration_end,
                                 datetime.datetime(2013, 8, 1, 14, 59))
                self.assertEqual(r.period, 3600)
                self.assertEqual(r.period_end,
                                 datetime.datetime(2013, 8, 1, 15, 0))
            elif (r.groupby == {'project_id': 'project-1'} and
                    r.period_start == datetime.datetime(2013, 8, 1, 16, 0)):
                self.assertEqual(r.count, 1)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 2)
                self.assertEqual(r.max, 2)
                self.assertEqual(r.sum, 2)
                self.assertEqual(r.avg, 2)
                self.assertEqual(r.duration, 0)
                self.assertEqual(r.duration_start,
                                 datetime.datetime(2013, 8, 1, 16, 10))
                self.assertEqual(r.duration_end,
                                 datetime.datetime(2013, 8, 1, 16, 10))
                self.assertEqual(r.period, 3600)
                self.assertEqual(r.period_end,
                                 datetime.datetime(2013, 8, 1, 17, 0))
            elif (r.groupby == {'project_id': 'project-2'} and
                    r.period_start == datetime.datetime(2013, 8, 1, 15, 0)):
                self.assertEqual(r.count, 1)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 2)
                self.assertEqual(r.max, 2)
                self.assertEqual(r.sum, 2)
                self.assertEqual(r.avg, 2)
                self.assertEqual(r.duration, 0)
                self.assertEqual(r.duration_start,
                                 datetime.datetime(2013, 8, 1, 15, 37))
                self.assertEqual(r.duration_end,
                                 datetime.datetime(2013, 8, 1, 15, 37))
                self.assertEqual(r.period, 3600)
                self.assertEqual(r.period_end,
                                 datetime.datetime(2013, 8, 1, 16, 0))
            else:
                self.assertNotEqual([r.groupby, r.period_start],
                                    [{'project_id': 'project-1'},
                                     datetime.datetime(2013, 8, 1, 15, 0)])
                self.assertNotEqual([r.groupby, r.period_start],
                                    [{'project_id': 'project-2'},
                                     datetime.datetime(2013, 8, 1, 14, 0)])
                self.assertNotEqual([r.groupby, r.period_start],
                                    [{'project_id': 'project-2'},
                                     datetime.datetime(2013, 8, 1, 16, 0)])

    def test_group_by_start_end_timestamp_with_query_filter_and_period(self):
        f = storage.SampleFilter(
            meter='instance',
            source='source-1',
            start=datetime.datetime(2013, 8, 1, 10, 0),
            end=datetime.datetime(2013, 8, 1, 18, 0),
        )
        results = list(self.conn.get_meter_statistics(f,
                                                      period=7200,
                                                      groupby=['project_id']))
        self.assertEqual(len(results), 3)
        groupby_list = [r.groupby for r in results]
        groupby_keys_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.keys())
        groupby_vals_set = set(x for sub_dict in groupby_list
                               for x in sub_dict.values())
        self.assertEqual(groupby_keys_set, set(['project_id']))
        self.assertEqual(groupby_vals_set, set(['project-1', 'project-2']))
        period_start_set = set([r.period_start for r in results])
        period_start_valid = set([datetime.datetime(2013, 8, 1, 10, 0),
                                  datetime.datetime(2013, 8, 1, 14, 0),
                                  datetime.datetime(2013, 8, 1, 16, 0)])
        self.assertEqual(period_start_set, period_start_valid)

        for r in results:
            if (r.groupby == {'project_id': 'project-1'} and
                    r.period_start == datetime.datetime(2013, 8, 1, 10, 0)):
                self.assertEqual(r.count, 2)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 1)
                self.assertEqual(r.max, 1)
                self.assertEqual(r.sum, 2)
                self.assertEqual(r.avg, 1)
                self.assertEqual(r.duration, 1740)
                self.assertEqual(r.duration_start,
                                 datetime.datetime(2013, 8, 1, 10, 11))
                self.assertEqual(r.duration_end,
                                 datetime.datetime(2013, 8, 1, 10, 40))
                self.assertEqual(r.period, 7200)
                self.assertEqual(r.period_end,
                                 datetime.datetime(2013, 8, 1, 12, 0))
            elif (r.groupby == {'project_id': 'project-1'} and
                    r.period_start == datetime.datetime(2013, 8, 1, 14, 0)):
                self.assertEqual(r.count, 1)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 2)
                self.assertEqual(r.max, 2)
                self.assertEqual(r.sum, 2)
                self.assertEqual(r.avg, 2)
                self.assertEqual(r.duration, 0)
                self.assertEqual(r.duration_start,
                                 datetime.datetime(2013, 8, 1, 14, 59))
                self.assertEqual(r.duration_end,
                                 datetime.datetime(2013, 8, 1, 14, 59))
                self.assertEqual(r.period, 7200)
                self.assertEqual(r.period_end,
                                 datetime.datetime(2013, 8, 1, 16, 0))
            elif (r.groupby == {'project_id': 'project-2'} and
                    r.period_start == datetime.datetime(2013, 8, 1, 16, 0)):
                self.assertEqual(r.count, 1)
                self.assertEqual(r.unit, 's')
                self.assertEqual(r.min, 4)
                self.assertEqual(r.max, 4)
                self.assertEqual(r.sum, 4)
                self.assertEqual(r.avg, 4)
                self.assertEqual(r.duration, 0)
                self.assertEqual(r.duration_start,
                                 datetime.datetime(2013, 8, 1, 17, 28))
                self.assertEqual(r.duration_end,
                                 datetime.datetime(2013, 8, 1, 17, 28))
                self.assertEqual(r.period, 7200)
                self.assertEqual(r.period_end,
                                 datetime.datetime(2013, 8, 1, 18, 0))
            else:
                self.assertNotEqual([r.groupby, r.period_start],
                                    [{'project_id': 'project-1'},
                                     datetime.datetime(2013, 8, 1, 16, 0)])
                self.assertNotEqual([r.groupby, r.period_start],
                                    [{'project_id': 'project-2'},
                                     datetime.datetime(2013, 8, 1, 10, 0)])
                self.assertNotEqual([r.groupby, r.period_start],
                                    [{'project_id': 'project-2'},
                                     datetime.datetime(2013, 8, 1, 14, 0)])


class CounterDataTypeTest(DBTestBase,
                          tests_db.MixinTestsWithBackendScenarios):
    def prepare_data(self):
        c = sample.Sample(
            'dummyBigCounter',
            sample.TYPE_CUMULATIVE,
            unit='',
            volume=3372036854775807,
            user_id='user-id',
            project_id='project-id',
            resource_id='resource-id',
            timestamp=datetime.datetime(2012, 7, 2, 10, 40),
            resource_metadata={},
            source='test-1',
        )
        msg = utils.meter_message_from_counter(
            c,
            self.CONF.publisher.metering_secret,
        )

        self.conn.record_metering_data(msg)

        c = sample.Sample(
            'dummySmallCounter',
            sample.TYPE_CUMULATIVE,
            unit='',
            volume=-3372036854775807,
            user_id='user-id',
            project_id='project-id',
            resource_id='resource-id',
            timestamp=datetime.datetime(2012, 7, 2, 10, 40),
            resource_metadata={},
            source='test-1',
        )
        msg = utils.meter_message_from_counter(
            c,
            self.CONF.publisher.metering_secret,
        )
        self.conn.record_metering_data(msg)

        c = sample.Sample(
            'floatCounter',
            sample.TYPE_CUMULATIVE,
            unit='',
            volume=1938495037.53697,
            user_id='user-id',
            project_id='project-id',
            resource_id='resource-id',
            timestamp=datetime.datetime(2012, 7, 2, 10, 40),
            resource_metadata={},
            source='test-1',
        )
        msg = utils.meter_message_from_counter(
            c,
            self.CONF.publisher.metering_secret,
        )
        self.conn.record_metering_data(msg)

    def test_storage_can_handle_large_values(self):
        f = storage.SampleFilter(
            meter='dummyBigCounter',
        )
        results = list(self.conn.get_samples(f))
        self.assertEqual(results[0].counter_volume, 3372036854775807)

        f = storage.SampleFilter(
            meter='dummySmallCounter',
        )
        results = list(self.conn.get_samples(f))
        self.assertEqual(results[0].counter_volume, -3372036854775807)

    def test_storage_can_handle_float_values(self):
        f = storage.SampleFilter(
            meter='floatCounter',
        )
        results = list(self.conn.get_samples(f))
        self.assertEqual(results[0].counter_volume, 1938495037.53697)


class AlarmTestBase(DBTestBase):
    def add_some_alarms(self):
        alarms = [models.Alarm(alarm_id='r3d',
                               enabled=True,
                               type='threshold',
                               name='red-alert',
                               description='my red-alert',
                               timestamp=None,
                               user_id='me',
                               project_id='and-da-boys',
                               state="insufficient data",
                               state_timestamp=None,
                               ok_actions=[],
                               alarm_actions=['http://nowhere/alarms'],
                               insufficient_data_actions=[],
                               repeat_actions=False,
                               time_constraints=[dict(name='testcons',
                                                      start='0 11 * * *',
                                                      duration=300)],
                               rule=dict(comparison_operator='eq',
                                         threshold=36,
                                         statistic='count',
                                         evaluation_periods=1,
                                         period=60,
                                         meter_name='test.one',
                                         query=[{'field': 'key',
                                                 'op': 'eq',
                                                 'value': 'value',
                                                 'type': 'string'}]),
                               ),
                  models.Alarm(alarm_id='0r4ng3',
                               enabled=True,
                               type='threshold',
                               name='orange-alert',
                               description='a orange',
                               timestamp=None,
                               user_id='me',
                               project_id='and-da-boys',
                               state="insufficient data",
                               state_timestamp=None,
                               ok_actions=[],
                               alarm_actions=['http://nowhere/alarms'],
                               insufficient_data_actions=[],
                               repeat_actions=False,
                               time_constraints=[],
                               rule=dict(comparison_operator='gt',
                                         threshold=75,
                                         statistic='avg',
                                         evaluation_periods=1,
                                         period=60,
                                         meter_name='test.fourty',
                                         query=[{'field': 'key2',
                                                 'op': 'eq',
                                                 'value': 'value2',
                                                 'type': 'string'}]),
                               ),
                  models.Alarm(alarm_id='y3ll0w',
                               enabled=False,
                               type='threshold',
                               name='yellow-alert',
                               description='yellow',
                               timestamp=None,
                               user_id='me',
                               project_id='and-da-boys',
                               state="insufficient data",
                               state_timestamp=None,
                               ok_actions=[],
                               alarm_actions=['http://nowhere/alarms'],
                               insufficient_data_actions=[],
                               repeat_actions=False,
                               time_constraints=[],
                               rule=dict(comparison_operator='lt',
                                         threshold=10,
                                         statistic='min',
                                         evaluation_periods=1,
                                         period=60,
                                         meter_name='test.five',
                                         query=[{'field': 'key2',
                                                 'op': 'eq',
                                                 'value': 'value2',
                                                 'type': 'string'},
                                                {'field':
                                                 'user_metadata.key3',
                                                 'op': 'eq',
                                                 'value': 'value3',
                                                 'type': 'string'}]),
                               )]

        for a in alarms:
            self.conn.create_alarm(a)


class AlarmTest(AlarmTestBase,
                tests_db.MixinTestsWithBackendScenarios):

    def test_empty(self):
        alarms = list(self.conn.get_alarms())
        self.assertEqual([], alarms)

    def test_list(self):
        self.add_some_alarms()
        alarms = list(self.conn.get_alarms())
        self.assertEqual(len(alarms), 3)

    def test_list_enabled(self):
        self.add_some_alarms()
        alarms = list(self.conn.get_alarms(enabled=True))
        self.assertEqual(len(alarms), 2)

    def test_list_disabled(self):
        self.add_some_alarms()
        alarms = list(self.conn.get_alarms(enabled=False))
        self.assertEqual(len(alarms), 1)

    def test_add(self):
        self.add_some_alarms()
        alarms = list(self.conn.get_alarms())
        self.assertEqual(len(alarms), 3)

        meter_names = sorted([a.rule['meter_name'] for a in alarms])
        self.assertEqual(meter_names,
                         ['test.five', 'test.fourty', 'test.one'])

    def test_update(self):
        self.add_some_alarms()
        orange = list(self.conn.get_alarms(name='orange-alert'))[0]
        orange.enabled = False
        orange.state = models.Alarm.ALARM_INSUFFICIENT_DATA
        query = [{'field': 'metadata.group',
                  'op': 'eq',
                  'value': 'test.updated',
                  'type': 'string'}]
        orange.rule['query'] = query
        orange.rule['meter_name'] = 'new_meter_name'
        updated = self.conn.update_alarm(orange)
        self.assertEqual(updated.enabled, False)
        self.assertEqual(updated.state, models.Alarm.ALARM_INSUFFICIENT_DATA)
        self.assertEqual(updated.rule['query'], query)
        self.assertEqual(updated.rule['meter_name'], 'new_meter_name')

    def test_update_llu(self):
        llu = models.Alarm(alarm_id='llu',
                           enabled=True,
                           type='threshold',
                           name='llu',
                           description='llu',
                           timestamp=None,
                           user_id='bla',
                           project_id='ffo',
                           state="insufficient data",
                           state_timestamp=None,
                           ok_actions=[],
                           alarm_actions=[],
                           insufficient_data_actions=[],
                           repeat_actions=False,
                           time_constraints=[],
                           rule=dict(comparison_operator='lt',
                                     threshold=34,
                                     statistic='max',
                                     evaluation_periods=1,
                                     period=60,
                                     meter_name='llt',
                                     query=[])
                           )
        updated = self.conn.update_alarm(llu)
        updated.state = models.Alarm.ALARM_OK
        updated.description = ':)'
        self.conn.update_alarm(updated)

        all = list(self.conn.get_alarms())
        self.assertEqual(len(all), 1)

    def test_delete(self):
        self.add_some_alarms()
        victim = list(self.conn.get_alarms(name='orange-alert'))[0]
        self.conn.delete_alarm(victim.alarm_id)
        survivors = list(self.conn.get_alarms())
        self.assertEqual(len(survivors), 2)
        for s in survivors:
            self.assertNotEqual(victim.name, s.name)


class AlarmTestPagination(AlarmTestBase,
                          tests_db.MixinTestsWithBackendScenarios):

    def test_get_alarm_all_limit(self):
        self.add_some_alarms()
        pagination = base.Pagination(limit=2)
        alarms = list(self.conn.get_alarms(pagination=pagination))
        self.assertEqual(len(alarms), 2)

        pagination = base.Pagination(limit=1)
        alarms = list(self.conn.get_alarms(pagination=pagination))
        self.assertEqual(len(alarms), 1)

    def test_get_alarm_all_marker(self):
        self.add_some_alarms()

        pagination = base.Pagination(marker_value='orange-alert')
        alarms = list(self.conn.get_alarms(pagination=pagination))
        self.assertEqual(len(alarms), 0)

        pagination = base.Pagination(marker_value='red-alert')
        alarms = list(self.conn.get_alarms(pagination=pagination))
        self.assertEqual(len(alarms), 1)

        pagination = base.Pagination(marker_value='yellow-alert')
        alarms = list(self.conn.get_alarms(pagination=pagination))
        self.assertEqual(len(alarms), 2)

    def test_get_alarm_paginate(self):

        self.add_some_alarms()

        pagination = base.Pagination(limit=4, marker_value='yellow-alert')
        page = list(self.conn.get_alarms(pagination=pagination))
        self.assertEqual(['red-alert', 'orange-alert'], [i.name for i in page])

        pagination = base.Pagination(limit=2, marker_value='orange-alert',
                                     primary_sort_dir='asc')
        page1 = list(self.conn.get_alarms(pagination=pagination))
        self.assertEqual(['red-alert', 'yellow-alert'],
                         [i.name for i in page1])


class ComplexAlarmQueryTest(AlarmTestBase,
                            tests_db.MixinTestsWithBackendScenarios):

    def test_no_filter(self):
        self.add_some_alarms()
        result = list(self.conn.query_alarms())
        self.assertEqual(3, len(result))

    def test_no_filter_with_limit(self):
        self.add_some_alarms()
        result = list(self.conn.query_alarms(limit=2))
        self.assertEqual(2, len(result))

    def test_filter(self):
        self.add_some_alarms()
        filter_expr = {"and":
                       [{"or":
                        [{"=": {"name": "yellow-alert"}},
                         {"=": {"name": "red-alert"}}]},
                       {"=": {"enabled": True}}]}

        result = list(self.conn.query_alarms(filter_expr=filter_expr))

        self.assertEqual(1, len(result))
        for a in result:
            self.assertIn(a.name, set(["yellow-alert", "red-alert"]))
            self.assertTrue(a.enabled)

    def test_filter_for_alarm_id(self):
        self.add_some_alarms()
        filter_expr = {"=": {"alarm_id": "0r4ng3"}}

        result = list(self.conn.query_alarms(filter_expr=filter_expr))

        self.assertEqual(1, len(result))
        for a in result:
            self.assertEqual(a.alarm_id, "0r4ng3")

    def test_filter_and_orderby(self):
        self.add_some_alarms()
        result = list(self.conn.query_alarms(filter_expr={"=":
                                                          {"enabled":
                                                          True}},
                                             orderby=[{"name": "asc"}]))
        self.assertEqual(2, len(result))
        self.assertEqual(["orange-alert", "red-alert"],
                         [a.name for a in result])
        for a in result:
            self.assertTrue(a.enabled)


class ComplexAlarmHistoryQueryTest(AlarmTestBase,
                                   tests_db.MixinTestsWithBackendScenarios):
    def setUp(self):
        super(DBTestBase, self).setUp()
        self.filter_expr = {"and":
                            [{"or":
                              [{"=": {"type": "rule change"}},
                               {"=": {"type": "state transition"}}]},
                             {"=": {"alarm_id": "0r4ng3"}}]}
        self.add_some_alarms()
        self.prepare_alarm_history()

    def prepare_alarm_history(self):
        alarms = list(self.conn.get_alarms())
        for alarm in alarms:
            i = alarms.index(alarm)
            alarm_change = dict(event_id=
                                "16fd2706-8baf-433b-82eb-8c7fada847c%s" % i,
                                alarm_id=alarm.alarm_id,
                                type=models.AlarmChange.CREATION,
                                detail="detail %s" % alarm.name,
                                user_id=alarm.user_id,
                                project_id=alarm.project_id,
                                on_behalf_of=alarm.project_id,
                                timestamp=datetime.datetime(2012, 9, 24,
                                                            7 + i,
                                                            30 + i))
            self.conn.record_alarm_change(alarm_change=alarm_change)

            alarm_change2 = dict(event_id=
                                 "16fd2706-8baf-433b-82eb-8c7fada847d%s" % i,
                                 alarm_id=alarm.alarm_id,
                                 type=models.AlarmChange.RULE_CHANGE,
                                 detail="detail %s" % i,
                                 user_id=alarm.user_id,
                                 project_id=alarm.project_id,
                                 on_behalf_of=alarm.project_id,
                                 timestamp=datetime.datetime(2012, 9, 25,
                                                             10 + i,
                                                             30 + i))
            self.conn.record_alarm_change(alarm_change=alarm_change2)

            alarm_change3 = dict(event_id=
                                 "16fd2706-8baf-433b-82eb-8c7fada847e%s"
                                 % i,
                                 alarm_id=alarm.alarm_id,
                                 type=models.AlarmChange.STATE_TRANSITION,
                                 detail="detail %s" % (i + 1),
                                 user_id=alarm.user_id,
                                 project_id=alarm.project_id,
                                 on_behalf_of=alarm.project_id,
                                 timestamp=datetime.datetime(2012, 9, 26,
                                                             10 + i,
                                                             30 + i))

            if alarm.name == "red-alert":
                alarm_change3['on_behalf_of'] = 'and-da-girls'

            self.conn.record_alarm_change(alarm_change=alarm_change3)

            if alarm.name in ["red-alert", "yellow-alert"]:
                alarm_change4 = dict(event_id=
                                     "16fd2706-8baf-433b-82eb-8c7fada847f%s"
                                     % i,
                                     alarm_id=alarm.alarm_id,
                                     type=models.AlarmChange.DELETION,
                                     detail="detail %s" % (i + 2),
                                     user_id=alarm.user_id,
                                     project_id=alarm.project_id,
                                     on_behalf_of=alarm.project_id,
                                     timestamp=datetime.datetime(2012, 9, 27,
                                                                 10 + i,
                                                                 30 + i))
                self.conn.record_alarm_change(alarm_change=alarm_change4)

    def test_alarm_history_with_no_filter(self):
        history = list(self.conn.query_alarm_history())
        self.assertEqual(11, len(history))

    def test_alarm_history_with_no_filter_and_limit(self):
        history = list(self.conn.query_alarm_history(limit=3))
        self.assertEqual(3, len(history))

    def test_alarm_history_with_filter(self):
        history = list(
            self.conn.query_alarm_history(filter_expr=self.filter_expr))
        self.assertEqual(2, len(history))

    def test_alarm_history_with_filter_and_orderby(self):
        history = list(
            self.conn.query_alarm_history(filter_expr=self.filter_expr,
                                          orderby=[{"timestamp":
                                                   "asc"}]))
        self.assertEqual([models.AlarmChange.RULE_CHANGE,
                          models.AlarmChange.STATE_TRANSITION],
                         [h.type for h in history])

    def test_alarm_history_with_filter_and_orderby_and_limit(self):
        history = list(
            self.conn.query_alarm_history(filter_expr=self.filter_expr,
                                          orderby=[{"timestamp":
                                                    "asc"}],
                                          limit=1))
        self.assertEqual(models.AlarmChange.RULE_CHANGE, history[0].type)

    def test_alarm_history_with_on_behalf_of_filter(self):
        filter_expr = {"=": {"on_behalf_of": "and-da-girls"}}
        history = list(self.conn.query_alarm_history(filter_expr=filter_expr))
        self.assertEqual(1, len(history))
        self.assertEqual("16fd2706-8baf-433b-82eb-8c7fada847e0",
                         history[0].event_id)

    def test_alarm_history_with_alarm_id_as_filter(self):
        filter_expr = {"=": {"alarm_id": "r3d"}}
        history = list(self.conn.query_alarm_history(filter_expr=filter_expr,
                                                     orderby=[{"timestamp":
                                                               "asc"}]))
        self.assertEqual(4, len(history))
        self.assertEqual([models.AlarmChange.CREATION,
                          models.AlarmChange.RULE_CHANGE,
                          models.AlarmChange.STATE_TRANSITION,
                          models.AlarmChange.DELETION],
                         [h.type for h in history])


class EventTestBase(tests_db.TestBase,
                    tests_db.MixinTestsWithBackendScenarios):
    """Separate test base class because we don't want to
    inherit all the Meter stuff.
    """

    def setUp(self):
        super(EventTestBase, self).setUp()
        self.prepare_data()

    def prepare_data(self):
        # Add some data ...
        pass


class EventTest(EventTestBase):
    def test_duplicate_message_id(self):
        now = datetime.datetime.utcnow()
        m = [models.Event("1", "Foo", now, None),
             models.Event("1", "Zoo", now, [])]
        problem_events = self.conn.record_events(m)
        self.assertEqual(1, len(problem_events))
        bad = problem_events[0]
        self.assertEqual(models.Event.DUPLICATE, bad[0])


class GetEventTest(EventTestBase):
    def prepare_data(self):
        self.event_models = []
        base = 0
        self.start = datetime.datetime(2013, 12, 31, 5, 0)
        now = self.start
        for event_type in ['Foo', 'Bar', 'Zoo', 'Foo', 'Bar', 'Zoo']:
            trait_models = \
                [models.Trait(name, dtype, value)
                    for name, dtype, value in [
                        ('trait_A', models.Trait.TEXT_TYPE,
                            "my_%s_text" % event_type),
                        ('trait_B', models.Trait.INT_TYPE,
                            base + 1),
                        ('trait_C', models.Trait.FLOAT_TYPE,
                            float(base) + 0.123456),
                        ('trait_D', models.Trait.DATETIME_TYPE, now)]]
            self.event_models.append(
                models.Event("id_%s_%d" % (event_type, base),
                             event_type, now, trait_models))
            base += 100
            now = now + datetime.timedelta(hours=1)
        self.end = now

        self.conn.record_events(self.event_models)

    def test_generated_is_datetime(self):
        event_filter = storage.EventFilter(self.start, self.end)
        events = self.conn.get_events(event_filter)
        self.assertEqual(6, len(events))
        for i, event in enumerate(events):
            self.assertIsInstance(event.generated, datetime.datetime)
            self.assertEqual(event.generated,
                             self.event_models[i].generated)
            model_traits = self.event_models[i].traits
            for j, trait in enumerate(event.traits):
                if trait.dtype == models.Trait.DATETIME_TYPE:
                    self.assertIsInstance(trait.value, datetime.datetime)
                    self.assertEqual(trait.value, model_traits[j].value)

    def test_simple_get(self):
        event_filter = storage.EventFilter(self.start, self.end)
        events = self.conn.get_events(event_filter)
        self.assertEqual(6, len(events))
        start_time = None
        for i, type in enumerate(['Foo', 'Bar', 'Zoo']):
            self.assertEqual(events[i].event_type, type)
            self.assertEqual(4, len(events[i].traits))
            # Ensure sorted results ...
            if start_time is not None:
                # Python 2.6 has no assertLess :(
                self.assertTrue(start_time < events[i].generated)
            start_time = events[i].generated

    def test_simple_get_event_type(self):
        expected_trait_values = {
            'id_Bar_100': {
                'trait_A': 'my_Bar_text',
                'trait_B': 101,
                'trait_C': 100.123456,
                'trait_D': self.start + datetime.timedelta(hours=1)
            },
            'id_Bar_400': {
                'trait_A': 'my_Bar_text',
                'trait_B': 401,
                'trait_C': 400.123456,
                'trait_D': self.start + datetime.timedelta(hours=4)
            }
        }

        event_filter = storage.EventFilter(self.start, self.end, "Bar")
        events = self.conn.get_events(event_filter)
        self.assertEqual(2, len(events))
        self.assertEqual(events[0].event_type, "Bar")
        self.assertEqual(events[1].event_type, "Bar")
        self.assertEqual(4, len(events[0].traits))
        self.assertEqual(4, len(events[1].traits))
        for event in events:
            trait_values = expected_trait_values.get(event.message_id,
                                                     None)
            if not trait_values:
                self.fail("Unexpected event ID returned:" % event.message_id)

            for trait in event.traits:
                expected_val = trait_values.get(trait.name)
                if not expected_val:
                    self.fail("Unexpected trait type: %s" % trait.dtype)
                self.assertEqual(expected_val, trait.value)

    def test_get_event_trait_filter(self):
        trait_filters = [{'key': 'trait_B', 'integer': 101}]
        event_filter = storage.EventFilter(self.start, self.end,
                                           traits_filter=trait_filters)
        events = self.conn.get_events(event_filter)
        self.assertEqual(1, len(events))
        self.assertEqual(events[0].event_type, "Bar")
        self.assertEqual(4, len(events[0].traits))

    def test_get_event_multiple_trait_filter(self):
        trait_filters = [{'key': 'trait_B', 'integer': 1},
                         {'key': 'trait_A', 'string': 'my_Foo_text'}]
        event_filter = storage.EventFilter(self.start, self.end,
                                           traits_filter=trait_filters)
        events = self.conn.get_events(event_filter)
        self.assertEqual(1, len(events))
        self.assertEqual(events[0].event_type, "Foo")
        self.assertEqual(4, len(events[0].traits))

    def test_get_event_multiple_trait_filter_expect_none(self):
        trait_filters = [{'key': 'trait_B', 'integer': 1},
                         {'key': 'trait_A', 'string': 'my_Zoo_text'}]
        event_filter = storage.EventFilter(self.start, self.end,
                                           traits_filter=trait_filters)
        events = self.conn.get_events(event_filter)
        self.assertEqual(0, len(events))

    def test_get_event_types(self):
        event_types = [e for e in
                       self.conn.get_event_types()]

        self.assertEqual(3, len(event_types))
        self.assertTrue("Bar" in event_types)
        self.assertTrue("Foo" in event_types)
        self.assertTrue("Zoo" in event_types)

    def test_get_trait_types(self):
        trait_types = [tt for tt in
                       self.conn.get_trait_types("Foo")]
        self.assertEqual(4, len(trait_types))
        trait_type_names = map(lambda x: x['name'], trait_types)
        self.assertIn("trait_A", trait_type_names)
        self.assertIn("trait_B", trait_type_names)
        self.assertIn("trait_C", trait_type_names)
        self.assertIn("trait_D", trait_type_names)

    def test_get_trait_types_unknown_event(self):
        trait_types = [tt for tt in
                       self.conn.get_trait_types("Moo")]
        self.assertEqual(0, len(trait_types))

    def test_get_traits(self):
        traits = self.conn.get_traits("Bar")
        #format results in a way that makes them easier to
        #work with
        trait_dict = {}
        for trait in traits:
            trait_dict[trait.name] = trait.dtype

        self.assertTrue("trait_A" in trait_dict)
        self.assertEqual(models.Trait.TEXT_TYPE, trait_dict["trait_A"])
        self.assertTrue("trait_B" in trait_dict)
        self.assertEqual(models.Trait.INT_TYPE, trait_dict["trait_B"])
        self.assertTrue("trait_C" in trait_dict)
        self.assertEqual(models.Trait.FLOAT_TYPE, trait_dict["trait_C"])
        self.assertTrue("trait_D" in trait_dict)
        self.assertEqual(models.Trait.DATETIME_TYPE,
                         trait_dict["trait_D"])

    def test_get_all_traits(self):
        traits = self.conn.\
            get_traits("Foo")
        traits = [t for t in traits]
        self.assertEqual(8, len(traits))

        trait = traits[0]
        self.assertEqual("trait_A", trait.name)
        self.assertEqual(models.Trait.TEXT_TYPE, trait.dtype)

    def test_simple_get_event_no_traits(self):
        new_events = [models.Event("id_notraits", "NoTraits", self.start, [])]
        bad_events = self.conn.record_events(new_events)
        event_filter = storage.EventFilter(self.start, self.end, "NoTraits")
        events = self.conn.get_events(event_filter)
        self.assertEqual(0, len(bad_events))
        self.assertEqual(1, len(events))
        self.assertEqual(events[0].message_id, "id_notraits")
        self.assertEqual(events[0].event_type, "NoTraits")
        self.assertEqual(0, len(events[0].traits))

    def test_simple_get_no_filters(self):
        event_filter = storage.EventFilter(None, None, None)
        events = self.conn.get_events(event_filter)
        self.assertEqual(6, len(events))

    def test_get_by_message_id(self):
        new_events = [models.Event("id_testid",
                                   "MessageIDTest",
                                   self.start,
                                   [])]

        bad_events = self.conn.record_events(new_events)
        event_filter = storage.EventFilter(message_id="id_testid")
        events = self.conn.get_events(event_filter)
        self.assertEqual(0, len(bad_events))
        self.assertEqual(1, len(events))
        event = events[0]
        self.assertEqual("id_testid", event.message_id)


class BigIntegerTest(tests_db.TestBase,
                     tests_db.MixinTestsWithBackendScenarios):
    def test_metadata_bigint(self):
        metadata = {'bigint': 99999999999999}
        s = sample.Sample(name='name',
                          type=sample.TYPE_GAUGE,
                          unit='B',
                          volume=1,
                          user_id='user-id',
                          project_id='project-id',
                          resource_id='resource-id',
                          timestamp=datetime.datetime.utcnow(),
                          resource_metadata=metadata)
        msg = utils.meter_message_from_counter(
            s, self.CONF.publisher.metering_secret)
        self.conn.record_metering_data(msg)

########NEW FILE########
__FILENAME__ = test_bin
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 eNovance <licensing@enovance.com>
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import json
import os
import random
import socket
import subprocess
import time

import httplib2

from ceilometer.openstack.common import fileutils
from ceilometer.tests import base


class BinTestCase(base.BaseTestCase):
    def setUp(self):
        super(BinTestCase, self).setUp()
        content = ("[DEFAULT]\n"
                   "rpc_backend=fake\n"
                   "[database]\n"
                   "connection=log://localhost\n")
        self.tempfile = fileutils.write_to_tempfile(content=content,
                                                    prefix='ceilometer',
                                                    suffix='.conf')

    def tearDown(self):
        super(BinTestCase, self).tearDown()
        os.remove(self.tempfile)

    def test_dbsync_run(self):
        subp = subprocess.Popen(['ceilometer-dbsync',
                                 "--config-file=%s" % self.tempfile])
        self.assertEqual(0, subp.wait())

    def test_run_expirer_ttl_disabled(self):
        subp = subprocess.Popen(['ceilometer-expirer',
                                 '-d',
                                 "--config-file=%s" % self.tempfile],
                                stderr=subprocess.PIPE)
        __, err = subp.communicate()
        self.assertEqual(0, subp.poll())
        self.assertIn("Nothing to clean", err)

    def test_run_expirer_ttl_enabled(self):
        content = ("[DEFAULT]\n"
                   "rpc_backend=fake\n"
                   "[database]\n"
                   "time_to_live=1\n"
                   "connection=log://localhost\n")
        self.tempfile = fileutils.write_to_tempfile(content=content,
                                                    prefix='ceilometer',
                                                    suffix='.conf')
        subp = subprocess.Popen(['ceilometer-expirer',
                                 '-d',
                                 "--config-file=%s" % self.tempfile],
                                stderr=subprocess.PIPE)
        __, err = subp.communicate()
        self.assertEqual(0, subp.poll())
        self.assertIn("Dropping data with TTL 1", err)


class BinSendSampleTestCase(base.BaseTestCase):
    def setUp(self):
        super(BinSendSampleTestCase, self).setUp()
        pipeline_cfg_file = self.path_get('etc/ceilometer/pipeline.yaml')
        content = "[DEFAULT]\n"\
                  "rpc_backend=fake\n"\
                  "pipeline_cfg_file={0}\n".format(pipeline_cfg_file)

        self.tempfile = fileutils.write_to_tempfile(content=content,
                                                    prefix='ceilometer',
                                                    suffix='.conf')

    def tearDown(self):
        super(BinSendSampleTestCase, self).tearDown()
        os.remove(self.tempfile)

    def test_send_counter_run(self):
        subp = subprocess.Popen(['ceilometer-send-sample',
                                 "--config-file=%s" % self.tempfile,
                                 "--sample-resource=someuuid",
                                 "--sample-name=mycounter"])
        self.assertEqual(0, subp.wait())


class BinApiTestCase(base.BaseTestCase):

    def setUp(self):
        super(BinApiTestCase, self).setUp()
        # create api_paste.ini file without authentication
        content = \
            "[pipeline:main]\n"\
            "pipeline = api-server\n"\
            "[app:api-server]\n"\
            "paste.app_factory = ceilometer.api.app:app_factory\n"
        self.paste = fileutils.write_to_tempfile(content=content,
                                                 prefix='api_paste',
                                                 suffix='.ini')

        # create ceilometer.conf file
        self.api_port = random.randint(10000, 11000)
        self.http = httplib2.Http()
        pipeline_cfg_file = self.path_get('etc/ceilometer/pipeline.yaml')
        policy_file = self.path_get('etc/ceilometer/policy.json')
        content = "[DEFAULT]\n"\
                  "rpc_backend=fake\n"\
                  "auth_strategy=noauth\n"\
                  "debug=true\n"\
                  "pipeline_cfg_file={0}\n"\
                  "policy_file={1}\n"\
                  "api_paste_config={2}\n"\
                  "[api]\n"\
                  "port={3}\n"\
                  "[database]\n"\
                  "connection=log://localhost\n".format(pipeline_cfg_file,
                                                        policy_file,
                                                        self.paste,
                                                        self.api_port)

        self.tempfile = fileutils.write_to_tempfile(content=content,
                                                    prefix='ceilometer',
                                                    suffix='.conf')
        self.subp = subprocess.Popen(['ceilometer-api',
                                      "--config-file=%s" % self.tempfile])

    def tearDown(self):
        super(BinApiTestCase, self).tearDown()
        self.subp.kill()
        self.subp.wait()
        os.remove(self.tempfile)

    def get_response(self, path):
        url = 'http://%s:%d/%s' % ('127.0.0.1', self.api_port, path)

        for x in range(10):
            try:
                r, c = self.http.request(url, 'GET')
            except socket.error:
                time.sleep(.5)
                self.assertIsNone(self.subp.poll())
            else:
                return r, c
        return (None, None)

    def test_v2(self):
        response, content = self.get_response('v2/meters')
        self.assertEqual(200, response.status)
        self.assertEqual([], json.loads(content))

########NEW FILE########
__FILENAME__ = test_collector
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
import contextlib
import socket

import mock
import msgpack
import oslo.messaging
from stevedore import extension

from ceilometer import collector
from ceilometer import messaging
from ceilometer.openstack.common import context
from ceilometer.openstack.common.fixture import config
from ceilometer.openstack.common import timeutils
from ceilometer.publisher import utils
from ceilometer import sample
from ceilometer.tests import base as tests_base


class FakeConnection():
    def create_worker(self, topic, proxy, pool_name):
        pass


class TestCollector(tests_base.BaseTestCase):
    def setUp(self):
        super(TestCollector, self).setUp()
        messaging.setup('fake://')
        self.addCleanup(messaging.cleanup)

        self.CONF = self.useFixture(config.Config()).conf
        self.CONF.set_override("connection", "log://", group='database')
        self.CONF.set_override('metering_secret', 'not-so-secret',
                               group='publisher')
        self.counter = sample.Sample(
            name='foobar',
            type='bad',
            unit='F',
            volume=1,
            user_id='jd',
            project_id='ceilometer',
            resource_id='cat',
            timestamp=timeutils.utcnow().isoformat(),
            resource_metadata={},
        ).as_dict()

        self.utf8_msg = utils.meter_message_from_counter(
            sample.Sample(
                name=u'test',
                type=sample.TYPE_CUMULATIVE,
                unit=u'',
                volume=1,
                user_id=u'test',
                project_id=u'test',
                resource_id=u'test_run_tasks',
                timestamp=timeutils.utcnow().isoformat(),
                resource_metadata={u'name': [([u'TestPublish'])]},
                source=u'testsource',
            ),
            'not-so-secret')

        self.srv = collector.CollectorService()

    def _make_test_manager(self, plugin):
        return extension.ExtensionManager.make_test_instance([
            extension.Extension(
                'test',
                None,
                None,
                plugin,
            ),
        ])

    def _make_fake_socket(self, sample):
        def recvfrom(size):
            # Make the loop stop
            self.srv.stop()
            return msgpack.dumps(sample), ('127.0.0.1', 12345)

        sock = mock.Mock()
        sock.recvfrom = recvfrom
        return sock

    def _verify_udp_socket(self, udp_socket):
        conf = self.CONF.collector
        udp_socket.setsockopt.assert_called_once_with(socket.SOL_SOCKET,
                                                      socket.SO_REUSEADDR, 1)
        udp_socket.bind.assert_called_once_with((conf.udp_address,
                                                 conf.udp_port))

    def test_record_metering_data(self):
        mock_dispatcher = mock.MagicMock()
        self.srv.dispatcher_manager = self._make_test_manager(mock_dispatcher)
        self.srv.record_metering_data(None, self.counter)
        mock_dispatcher.record_metering_data.assert_called_once_with(
            data=self.counter)

    def test_udp_receive(self):
        self.CONF.set_override('rpc_backend', '')
        mock_dispatcher = mock.MagicMock()
        self.srv.dispatcher_manager = self._make_test_manager(mock_dispatcher)
        self.counter['source'] = 'mysource'
        self.counter['counter_name'] = self.counter['name']
        self.counter['counter_volume'] = self.counter['volume']
        self.counter['counter_type'] = self.counter['type']
        self.counter['counter_unit'] = self.counter['unit']

        udp_socket = self._make_fake_socket(self.counter)
        with mock.patch('socket.socket', return_value=udp_socket):
            self.srv.start_udp()

        self._verify_udp_socket(udp_socket)

        mock_dispatcher.record_metering_data.assert_called_once_with(
            self.counter)

    def test_udp_receive_storage_error(self):
        self.CONF.set_override('rpc_backend', '')
        mock_dispatcher = mock.MagicMock()
        self.srv.dispatcher_manager = self._make_test_manager(mock_dispatcher)
        mock_dispatcher.record_metering_data.side_effect = self._raise_error

        self.counter['source'] = 'mysource'
        self.counter['counter_name'] = self.counter['name']
        self.counter['counter_volume'] = self.counter['volume']
        self.counter['counter_type'] = self.counter['type']
        self.counter['counter_unit'] = self.counter['unit']

        udp_socket = self._make_fake_socket(self.counter)
        with mock.patch('socket.socket', return_value=udp_socket):
            self.srv.start_udp()

        self._verify_udp_socket(udp_socket)

        mock_dispatcher.record_metering_data.assert_called_once_with(
            self.counter)

    @staticmethod
    def _raise_error():
        raise Exception

    def test_udp_receive_bad_decoding(self):
        self.CONF.set_override('rpc_backend', '')
        udp_socket = self._make_fake_socket(self.counter)
        with mock.patch('socket.socket', return_value=udp_socket):
            with mock.patch('msgpack.loads', self._raise_error):
                self.srv.start_udp()

        self._verify_udp_socket(udp_socket)

    @staticmethod
    def _dummy_thread_group_add_thread(method):
        method()

    @mock.patch.object(oslo.messaging.MessageHandlingServer, 'start')
    @mock.patch.object(collector.CollectorService, 'start_udp')
    def test_only_udp(self, udp_start, rpc_start):
        """Check that only UDP is started if rpc_backend is empty."""
        self.CONF.set_override('rpc_backend', '')
        udp_socket = self._make_fake_socket(self.counter)
        with contextlib.nested(
                mock.patch.object(
                    self.srv.tg, 'add_thread',
                    side_effect=self._dummy_thread_group_add_thread),
                mock.patch('socket.socket', return_value=udp_socket)):
            self.srv.start()
            self.assertEqual(0, rpc_start.call_count)
            self.assertEqual(1, udp_start.call_count)

    @mock.patch.object(oslo.messaging.MessageHandlingServer, 'start')
    @mock.patch.object(collector.CollectorService, 'start_udp')
    def test_only_rpc(self, udp_start, rpc_start):
        """Check that only RPC is started if udp_address is empty."""
        self.CONF.set_override('udp_address', '', group='collector')
        with mock.patch.object(
                self.srv.tg, 'add_thread',
                side_effect=self._dummy_thread_group_add_thread):
            self.srv.start()
            self.assertEqual(1, rpc_start.call_count)
            self.assertEqual(0, udp_start.call_count)

    def test_udp_receive_valid_encoding(self):
        self.data_sent = []
        with mock.patch('socket.socket',
                        return_value=self._make_fake_socket(self.utf8_msg)):
            self.srv.rpc_server = mock.MagicMock()
            mock_dispatcher = mock.MagicMock()
            self.srv.dispatcher_manager = \
                self._make_test_manager(mock_dispatcher)
            self.srv.start_udp()
            self.assertTrue(utils.verify_signature(
                mock_dispatcher.method_calls[0][1][0],
                "not-so-secret"))

    @mock.patch('ceilometer.storage.impl_log.LOG')
    def test_collector_no_mock(self, mylog):
        self.CONF.set_override('udp_address', '', group='collector')
        self.srv.start()
        mylog.info.side_effect = lambda *args: self.srv.stop()

        client = messaging.get_rpc_client(version='1.0')
        cclient = client.prepare(topic='metering')
        cclient.cast(context.RequestContext(),
                     'record_metering_data', data=[self.utf8_msg])

        self.srv.rpc_server.wait()
        mylog.info.assert_called_once_with(
            'metering data test for test_run_tasks: 1')

########NEW FILE########
__FILENAME__ = test_decoupled_pipeline
# -*- encoding: utf-8 -*-
#
# Copyright © 2014 Red Hat, Inc
#
# Author: Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import yaml

from ceilometer import pipeline
from ceilometer import sample
from ceilometer.tests import pipeline_base


class TestDecoupledPipeline(pipeline_base.BasePipelineTestCase):
    def _setup_pipeline_cfg(self):
        source = {'name': 'test_source',
                  'interval': 5,
                  'counters': ['a'],
                  'resources': [],
                  'sinks': ['test_sink']}
        sink = {'name': 'test_sink',
                'transformers': [{'name': 'update', 'parameters': {}}],
                'publishers': ['test://']}
        self.pipeline_cfg = {'sources': [source], 'sinks': [sink]}

    def _augment_pipeline_cfg(self):
        self.pipeline_cfg['sources'].append({
            'name': 'second_source',
            'interval': 5,
            'counters': ['b'],
            'resources': [],
            'sinks': ['second_sink']
        })
        self.pipeline_cfg['sinks'].append({
            'name': 'second_sink',
            'transformers': [{
                'name': 'update',
                'parameters':
                {
                    'append_name': '_new',
                }
            }],
            'publishers': ['new'],
        })

    def _break_pipeline_cfg(self):
        self.pipeline_cfg['sources'].append({
            'name': 'second_source',
            'interval': 5,
            'counters': ['b'],
            'resources': [],
            'sinks': ['second_sink']
        })
        self.pipeline_cfg['sinks'].append({
            'name': 'second_sink',
            'transformers': [{
                'name': 'update',
                'parameters':
                {
                    'append_name': '_new',
                }
            }],
            'publishers': ['except'],
        })

    def _set_pipeline_cfg(self, field, value):
        if field in self.pipeline_cfg['sources'][0]:
            self.pipeline_cfg['sources'][0][field] = value
        else:
            self.pipeline_cfg['sinks'][0][field] = value

    def _extend_pipeline_cfg(self, field, value):
        if field in self.pipeline_cfg['sources'][0]:
            self.pipeline_cfg['sources'][0][field].extend(value)
        else:
            self.pipeline_cfg['sinks'][0][field].extend(value)

    def _unset_pipeline_cfg(self, field):
        if field in self.pipeline_cfg['sources'][0]:
            del self.pipeline_cfg['sources'][0][field]
        else:
            del self.pipeline_cfg['sinks'][0][field]

    def test_source_no_sink(self):
        del self.pipeline_cfg['sinks']
        self._exception_create_pipelinemanager()

    def test_source_dangling_sink(self):
        self.pipeline_cfg['sources'].append({
            'name': 'second_source',
            'interval': 5,
            'counters': ['b'],
            'resources': [],
            'sinks': ['second_sink']
        })
        self._exception_create_pipelinemanager()

    def test_sink_no_source(self):
        del self.pipeline_cfg['sources']
        self._exception_create_pipelinemanager()

    def test_source_with_multiple_sinks(self):
        counter_cfg = ['a', 'b']
        self._set_pipeline_cfg('counters', counter_cfg)
        self.pipeline_cfg['sinks'].append({
            'name': 'second_sink',
            'transformers': [{
                'name': 'update',
                'parameters':
                {
                    'append_name': '_new',
                }
            }],
            'publishers': ['new'],
        })
        self.pipeline_cfg['sources'][0]['sinks'].append('second_sink')

        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        with pipeline_manager.publisher(None) as p:
            p([self.test_counter])

        self.test_counter = sample.Sample(
            name='b',
            type=self.test_counter.type,
            volume=self.test_counter.volume,
            unit=self.test_counter.unit,
            user_id=self.test_counter.user_id,
            project_id=self.test_counter.project_id,
            resource_id=self.test_counter.resource_id,
            timestamp=self.test_counter.timestamp,
            resource_metadata=self.test_counter.resource_metadata,
        )

        with pipeline_manager.publisher(None) as p:
            p([self.test_counter])

        self.assertEqual(len(pipeline_manager.pipelines), 2)
        self.assertEqual(str(pipeline_manager.pipelines[0]),
                         'test_source:test_sink')
        self.assertEqual(str(pipeline_manager.pipelines[1]),
                         'test_source:second_sink')
        test_publisher = pipeline_manager.pipelines[0].publishers[0]
        new_publisher = pipeline_manager.pipelines[1].publishers[0]
        for publisher, sfx in [(test_publisher, '_update'),
                               (new_publisher, '_new')]:
            self.assertEqual(len(publisher.samples), 2)
            self.assertEqual(publisher.calls, 2)
            self.assertEqual(getattr(publisher.samples[0], "name"), 'a' + sfx)
            self.assertEqual(getattr(publisher.samples[1], "name"), 'b' + sfx)

    def test_multiple_sources_with_single_sink(self):
        self.pipeline_cfg['sources'].append({
            'name': 'second_source',
            'interval': 5,
            'counters': ['b'],
            'resources': [],
            'sinks': ['test_sink']
        })

        pipeline_manager = pipeline.PipelineManager(self.pipeline_cfg,
                                                    self.transformer_manager)
        with pipeline_manager.publisher(None) as p:
            p([self.test_counter])

        self.test_counter = sample.Sample(
            name='b',
            type=self.test_counter.type,
            volume=self.test_counter.volume,
            unit=self.test_counter.unit,
            user_id=self.test_counter.user_id,
            project_id=self.test_counter.project_id,
            resource_id=self.test_counter.resource_id,
            timestamp=self.test_counter.timestamp,
            resource_metadata=self.test_counter.resource_metadata,
        )

        with pipeline_manager.publisher(None) as p:
            p([self.test_counter])

        self.assertEqual(len(pipeline_manager.pipelines), 2)
        self.assertEqual(str(pipeline_manager.pipelines[0]),
                         'test_source:test_sink')
        self.assertEqual(str(pipeline_manager.pipelines[1]),
                         'second_source:test_sink')
        test_publisher = pipeline_manager.pipelines[0].publishers[0]
        another_publisher = pipeline_manager.pipelines[1].publishers[0]
        for publisher in [test_publisher, another_publisher]:
            self.assertEqual(len(publisher.samples), 2)
            self.assertEqual(publisher.calls, 2)
            self.assertEqual(getattr(publisher.samples[0], "name"), 'a_update')
            self.assertEqual(getattr(publisher.samples[1], "name"), 'b_update')

        transformed_samples = self.TransformerClass.samples
        self.assertEqual(len(transformed_samples), 2)
        self.assertEqual([getattr(s, 'name') for s in transformed_samples],
                         ['a', 'b'])

    def _do_test_rate_of_change_in_boilerplate_pipeline_cfg(self, index,
                                                            meters, units):
        with open('etc/ceilometer/pipeline.yaml') as fap:
            data = fap.read()
        pipeline_cfg = yaml.safe_load(data)
        for s in pipeline_cfg['sinks']:
            s['publishers'] = ['test://']
        pipeline_manager = pipeline.PipelineManager(pipeline_cfg,
                                                    self.transformer_manager)
        pipe = pipeline_manager.pipelines[index]
        self._do_test_rate_of_change_mapping(pipe, meters, units)

    def test_rate_of_change_boilerplate_disk_read_cfg(self):
        meters = ('disk.read.bytes', 'disk.read.requests')
        units = ('B', 'request')
        self._do_test_rate_of_change_in_boilerplate_pipeline_cfg(2,
                                                                 meters,
                                                                 units)

    def test_rate_of_change_boilerplate_disk_write_cfg(self):
        meters = ('disk.write.bytes', 'disk.write.requests')
        units = ('B', 'request')
        self._do_test_rate_of_change_in_boilerplate_pipeline_cfg(2,
                                                                 meters,
                                                                 units)

    def test_rate_of_change_boilerplate_network_incoming_cfg(self):
        meters = ('network.incoming.bytes', 'network.incoming.packets')
        units = ('B', 'packet')
        self._do_test_rate_of_change_in_boilerplate_pipeline_cfg(3,
                                                                 meters,
                                                                 units)

    def test_rate_of_change_boilerplate_network_outgoing_cfg(self):
        meters = ('network.outgoing.bytes', 'network.outgoing.packets')
        units = ('B', 'packet')
        self._do_test_rate_of_change_in_boilerplate_pipeline_cfg(3,
                                                                 meters,
                                                                 units)

########NEW FILE########
__FILENAME__ = test_deprecated_pipeline
# -*- encoding: utf-8 -*-
#
# Copyright © 2014 Red Hat, Inc
#
# Author: Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import yaml

from ceilometer import pipeline
from ceilometer.tests import pipeline_base


class TestDeprecatedPipeline(pipeline_base.BasePipelineTestCase):
    def _setup_pipeline_cfg(self):
        self.pipeline_cfg = [{
            'name': 'test_pipeline',
            'interval': 5,
            'counters': ['a'],
            'transformers': [
                {'name': 'update',
                 'parameters': {}}
            ],
            'publishers': ['test://'],
        }, ]

    def _augment_pipeline_cfg(self):
        self.pipeline_cfg.append({
            'name': 'second_pipeline',
            'interval': 5,
            'counters': ['b'],
            'transformers': [{
                'name': 'update',
                'parameters':
                {
                    'append_name': '_new',
                }
            }],
            'publishers': ['new'],
        })

    def _break_pipeline_cfg(self):
        self.pipeline_cfg.append({
            'name': 'second_pipeline',
            'interval': 5,
            'counters': ['b'],
            'transformers': [{
                'name': 'update',
                'parameters':
                {
                    'append_name': '_new',
                }
            }],
            'publishers': ['except'],
        })

    def _set_pipeline_cfg(self, field, value):
        self.pipeline_cfg[0][field] = value

    def _extend_pipeline_cfg(self, field, value):
        self.pipeline_cfg[0][field].extend(value)

    def _unset_pipeline_cfg(self, field):
        del self.pipeline_cfg[0][field]

    def _do_test_rate_of_change_in_boilerplate_pipeline_cfg(self, index,
                                                            meters, units):
        with open('etc/ceilometer/deprecated_pipeline.yaml') as fap:
            data = fap.read()
        pipeline_cfg = yaml.safe_load(data)
        for p in pipeline_cfg:
            p['publishers'] = ['test://']
        pipeline_manager = pipeline.PipelineManager(pipeline_cfg,
                                                    self.transformer_manager)
        pipe = pipeline_manager.pipelines[index]
        self._do_test_rate_of_change_mapping(pipe, meters, units)

    def test_rate_of_change_boilerplate_disk_read_cfg(self):
        meters = ('disk.read.bytes', 'disk.read.requests')
        units = ('B', 'request')
        self._do_test_rate_of_change_in_boilerplate_pipeline_cfg(2,
                                                                 meters,
                                                                 units)

    def test_rate_of_change_boilerplate_disk_write_cfg(self):
        meters = ('disk.write.bytes', 'disk.write.requests')
        units = ('B', 'request')
        self._do_test_rate_of_change_in_boilerplate_pipeline_cfg(2,
                                                                 meters,
                                                                 units)

    def test_rate_of_change_boilerplate_network_incoming_cfg(self):
        meters = ('network.incoming.bytes', 'network.incoming.packets')
        units = ('B', 'packet')
        self._do_test_rate_of_change_in_boilerplate_pipeline_cfg(3,
                                                                 meters,
                                                                 units)

    def test_rate_of_change_boilerplate_network_outgoing_cfg(self):
        meters = ('network.outgoing.bytes', 'network.outgoing.packets')
        units = ('B', 'packet')
        self._do_test_rate_of_change_in_boilerplate_pipeline_cfg(3,
                                                                 meters,
                                                                 units)

########NEW FILE########
__FILENAME__ = test_middleware
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 eNovance
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import mock

from ceilometer import middleware
from ceilometer.openstack.common.fixture import config
from ceilometer.openstack.common import test


HTTP_REQUEST = {
    u'_context_auth_token': u'3d8b13de1b7d499587dfc69b77dc09c2',
    u'_context_is_admin': True,
    u'_context_project_id': u'7c150a59fe714e6f9263774af9688f0e',
    u'_context_quota_class': None,
    u'_context_read_deleted': u'no',
    u'_context_remote_address': u'10.0.2.15',
    u'_context_request_id': u'req-d68b36e0-9233-467f-9afb-d81435d64d66',
    u'_context_roles': [u'admin'],
    u'_context_timestamp': u'2012-05-08T20:23:41.425105',
    u'_context_user_id': u'1e3ce043029547f1a61c1996d1a531a2',
    u'event_type': u'http.request',
    u'message_id': u'dae6f69c-00e0-41c0-b371-41ec3b7f4451',
    u'payload': {u'request': {'HTTP_X_FOOBAR': 'foobaz',
                              'HTTP_X_USER_ID': 'jd-x32',
                              'HTTP_X_PROJECT_ID': 'project-id',
                              'HTTP_X_SERVICE_NAME': 'nova'}},
    u'priority': u'INFO',
    u'publisher_id': u'compute.vagrant-precise',
    u'timestamp': u'2012-05-08 20:23:48.028195',
}

HTTP_RESPONSE = {
    u'_context_auth_token': u'3d8b13de1b7d499587dfc69b77dc09c2',
    u'_context_is_admin': True,
    u'_context_project_id': u'7c150a59fe714e6f9263774af9688f0e',
    u'_context_quota_class': None,
    u'_context_read_deleted': u'no',
    u'_context_remote_address': u'10.0.2.15',
    u'_context_request_id': u'req-d68b36e0-9233-467f-9afb-d81435d64d66',
    u'_context_roles': [u'admin'],
    u'_context_timestamp': u'2012-05-08T20:23:41.425105',
    u'_context_user_id': u'1e3ce043029547f1a61c1996d1a531a2',
    u'event_type': u'http.response',
    u'message_id': u'dae6f69c-00e0-41c0-b371-41ec3b7f4451',
    u'payload': {u'request': {'HTTP_X_FOOBAR': 'foobaz',
                              'HTTP_X_USER_ID': 'jd-x32',
                              'HTTP_X_PROJECT_ID': 'project-id',
                              'HTTP_X_SERVICE_NAME': 'nova'},
                 u'response': {'status': '200 OK'}},
    u'priority': u'INFO',
    u'publisher_id': u'compute.vagrant-precise',
    u'timestamp': u'2012-05-08 20:23:48.028195',
}


class TestNotifications(test.BaseTestCase):

    def setUp(self):
        super(TestNotifications, self).setUp()
        self.CONF = self.useFixture(config.Config()).conf

    def test_process_request_notification(self):
        sample = list(middleware.HTTPRequest(mock.Mock()).process_notification(
            HTTP_REQUEST
        ))[0]
        self.assertEqual(HTTP_REQUEST['payload']['request']['HTTP_X_USER_ID'],
                         sample.user_id)
        self.assertEqual(HTTP_REQUEST['payload']['request']
                         ['HTTP_X_PROJECT_ID'], sample.project_id)
        self.assertEqual(HTTP_REQUEST['payload']['request']
                         ['HTTP_X_SERVICE_NAME'], sample.resource_id)
        self.assertEqual(1, sample.volume)

    def test_process_response_notification(self):
        sample = list(middleware.HTTPResponse(
            mock.Mock()).process_notification(HTTP_RESPONSE))[0]
        self.assertEqual(HTTP_RESPONSE['payload']['request']['HTTP_X_USER_ID'],
                         sample.user_id)
        self.assertEqual(HTTP_RESPONSE['payload']['request']
                         ['HTTP_X_PROJECT_ID'], sample.project_id)
        self.assertEqual(HTTP_RESPONSE['payload']['request']
                         ['HTTP_X_SERVICE_NAME'], sample.resource_id)
        self.assertEqual(1, sample.volume)

    def test_targets(self):
        targets = middleware.HTTPRequest(mock.Mock()).get_targets(self.CONF)
        self.assertEqual(4, len(targets))

########NEW FILE########
__FILENAME__ = test_neutronclient
# Copyright (C) 2014 eNovance SAS <licensing@enovance.com>
#
# Author: Sylvain Afchain <sylvain.afchain@enovance.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from mock import patch

from ceilometer import neutron_client
from ceilometer.openstack.common import test


class TestNeutronClient(test.BaseTestCase):

    def setUp(self):
        super(TestNeutronClient, self).setUp()
        self.nc = neutron_client.Client()

    @staticmethod
    def fake_ports_list():
        return {'ports':
                [{'admin_state_up': True,
                  'device_id': '674e553b-8df9-4321-87d9-93ba05b93558',
                  'device_owner': 'network:router_gateway',
                  'extra_dhcp_opts': [],
                  'id': '96d49cc3-4e01-40ce-9cac-c0e32642a442',
                  'mac_address': 'fa:16:3e:c5:35:93',
                  'name': '',
                  'network_id': '298a3088-a446-4d5a-bad8-f92ecacd786b',
                  'status': 'ACTIVE',
                  'tenant_id': '89271fa581ab4380bf172f868c3615f9'}]}

    def test_port_get_all(self):
        with patch.object(self.nc.client, 'list_ports',
                          side_effect=self.fake_ports_list):
            ports = self.nc.port_get_all()

        self.assertEqual(1, len(ports))
        self.assertEqual('96d49cc3-4e01-40ce-9cac-c0e32642a442',
                         ports[0]['id'])

    @staticmethod
    def fake_networks_list():
        return {'networks':
                [{'admin_state_up': True,
                  'id': '298a3088-a446-4d5a-bad8-f92ecacd786b',
                  'name': 'public',
                  'provider:network_type': 'gre',
                  'provider:physical_network': None,
                  'provider:segmentation_id': 2,
                  'router:external': True,
                  'shared': False,
                  'status': 'ACTIVE',
                  'subnets': [u'c4b6f5b8-3508-4896-b238-a441f25fb492'],
                  'tenant_id': '62d6f08bbd3a44f6ad6f00ca15cce4e5'}]}

    def test_network_get_all(self):
        with patch.object(self.nc.client, 'list_networks',
                          side_effect=self.fake_networks_list):
            networks = self.nc.network_get_all()

        self.assertEqual(1, len(networks))
        self.assertEqual('298a3088-a446-4d5a-bad8-f92ecacd786b',
                         networks[0]['id'])

########NEW FILE########
__FILENAME__ = test_notification
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests for Ceilometer notify daemon."""

import mock

import oslo.messaging
import oslo.messaging.conffixture
from stevedore import extension
import yaml

from ceilometer.compute.notifications import instance
from ceilometer import messaging
from ceilometer import notification
from ceilometer.openstack.common import context
from ceilometer.openstack.common import fileutils
from ceilometer.openstack.common.fixture import config
from ceilometer.tests import base as tests_base

TEST_NOTICE_CTXT = {
    u'auth_token': u'3d8b13de1b7d499587dfc69b77dc09c2',
    u'is_admin': True,
    u'project_id': u'7c150a59fe714e6f9263774af9688f0e',
    u'quota_class': None,
    u'read_deleted': u'no',
    u'remote_address': u'10.0.2.15',
    u'request_id': u'req-d68b36e0-9233-467f-9afb-d81435d64d66',
    u'roles': [u'admin'],
    u'timestamp': u'2012-05-08T20:23:41.425105',
    u'user_id': u'1e3ce043029547f1a61c1996d1a531a2',
}

TEST_NOTICE_METADATA = {
    u'message_id': u'dae6f69c-00e0-41c0-b371-41ec3b7f4451',
    u'timestamp': u'2012-05-08 20:23:48.028195',
}

TEST_NOTICE_PAYLOAD = {
    u'created_at': u'2012-05-08 20:23:41',
    u'deleted_at': u'',
    u'disk_gb': 0,
    u'display_name': u'testme',
    u'fixed_ips': [{u'address': u'10.0.0.2',
                    u'floating_ips': [],
                    u'meta': {},
                    u'type': u'fixed',
                    u'version': 4}],
    u'image_ref_url': u'http://10.0.2.15:9292/images/UUID',
    u'instance_id': u'9f9d01b9-4a58-4271-9e27-398b21ab20d1',
    u'instance_type': u'm1.tiny',
    u'instance_type_id': 2,
    u'launched_at': u'2012-05-08 20:23:47.985999',
    u'memory_mb': 512,
    u'state': u'active',
    u'state_description': u'',
    u'tenant_id': u'7c150a59fe714e6f9263774af9688f0e',
    u'user_id': u'1e3ce043029547f1a61c1996d1a531a2',
    u'reservation_id': u'1e3ce043029547f1a61c1996d1a531a3',
    u'vcpus': 1,
    u'root_gb': 0,
    u'ephemeral_gb': 0,
    u'host': u'compute-host-name',
    u'availability_zone': u'1e3ce043029547f1a61c1996d1a531a4',
    u'os_type': u'linux?',
    u'architecture': u'x86',
    u'image_ref': u'UUID',
    u'kernel_id': u'1e3ce043029547f1a61c1996d1a531a5',
    u'ramdisk_id': u'1e3ce043029547f1a61c1996d1a531a6',
}


class TestNotification(tests_base.BaseTestCase):

    def setUp(self):
        super(TestNotification, self).setUp()
        self.CONF = self.useFixture(config.Config()).conf
        messaging.setup('fake://')
        self.addCleanup(messaging.cleanup)
        self.CONF.set_override("connection", "log://", group='database')
        self.CONF.set_override("store_events", False, group="notification")
        self.srv = notification.NotificationService()

    def fake_get_notifications_manager(self, pm):
        self.plugin = instance.Instance(pm)
        return extension.ExtensionManager.make_test_instance(
            [
                extension.Extension('test',
                                    None,
                                    None,
                                    self.plugin)
            ]
        )

    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    @mock.patch.object(oslo.messaging.MessageHandlingServer, 'start',
                       mock.MagicMock())
    @mock.patch('ceilometer.event.endpoint.EventsNotificationEndpoint')
    def _do_process_notification_manager_start(self,
                                               fake_event_endpoint_class):
        with mock.patch.object(self.srv, '_get_notifications_manager') \
                as get_nm:
            get_nm.side_effect = self.fake_get_notifications_manager
            self.srv.start()
        self.fake_event_endpoint = fake_event_endpoint_class.return_value

    def test_start_multiple_listeners(self):
        urls = ["fake://vhost1", "fake://vhost2"]
        self.CONF.set_override("messaging_urls", urls, group="notification")
        self._do_process_notification_manager_start()
        self.assertEqual(2, len(self.srv.listeners))

    def test_process_notification(self):
        self._do_process_notification_manager_start()
        self.srv.pipeline_manager.pipelines[0] = mock.MagicMock()

        self.plugin.info(TEST_NOTICE_CTXT, 'compute.vagrant-precise',
                         'compute.instance.create.end',
                         TEST_NOTICE_PAYLOAD, TEST_NOTICE_METADATA)

        self.assertEqual(1, len(self.srv.listeners[0].dispatcher.endpoints))
        self.assertTrue(self.srv.pipeline_manager.publisher.called)

    def test_process_notification_no_events(self):
        self._do_process_notification_manager_start()
        self.assertEqual(1, len(self.srv.listeners[0].dispatcher.endpoints))
        self.assertNotEqual(self.fake_event_endpoint,
                            self.srv.listeners[0].dispatcher.endpoints[0])

    def test_process_notification_with_events(self):
        self.CONF.set_override("store_events", True, group="notification")
        self._do_process_notification_manager_start()
        self.assertEqual(2, len(self.srv.listeners[0].dispatcher.endpoints))
        self.assertEqual(self.fake_event_endpoint,
                         self.srv.listeners[0].dispatcher.endpoints[0])

    @mock.patch('ceilometer.event.converter.get_config_file',
                mock.MagicMock(return_value=None))
    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    @mock.patch.object(oslo.messaging.MessageHandlingServer, 'start',
                       mock.MagicMock())
    def test_event_dispatcher_loaded(self):
        self.CONF.set_override("store_events", True, group="notification")
        with mock.patch.object(self.srv, '_get_notifications_manager') \
                as get_nm:
            get_nm.side_effect = self.fake_get_notifications_manager
            self.srv.start()
        self.assertEqual(2, len(self.srv.listeners[0].dispatcher.endpoints))
        event_endpoint = self.srv.listeners[0].dispatcher.endpoints[0]
        self.assertEqual(1, len(list(event_endpoint.dispatcher_manager)))


class TestRealNotification(tests_base.BaseTestCase):
    def setUp(self):
        super(TestRealNotification, self).setUp()
        self.CONF = self.useFixture(config.Config()).conf
        self.useFixture(oslo.messaging.conffixture.ConfFixture(self.CONF))

        pipeline = yaml.dump([{
            'name': 'test_pipeline',
            'interval': 5,
            'counters': ['*'],
            'transformers': [],
            'publishers': ['test://'],
        }])
        pipeline_cfg_file = fileutils.write_to_tempfile(content=pipeline,
                                                        prefix="pipeline",
                                                        suffix="yaml")
        self.CONF.set_override("pipeline_cfg_file", pipeline_cfg_file)
        self.CONF.set_override("notification_driver", "messaging")
        self.CONF.set_override("control_exchange", "nova")
        messaging.setup('fake://')
        self.addCleanup(messaging.cleanup)

        self.srv = notification.NotificationService()

    @mock.patch('ceilometer.publisher.test.TestPublisher')
    def test_notification_service(self, fake_publisher_cls):
        self.srv.start()

        fake_publisher = fake_publisher_cls.return_value
        fake_publisher.publish_samples.side_effect = \
            lambda *args: self.srv.stop()

        notifier = messaging.get_notifier("compute.vagrant-precise")
        notifier.info(context.RequestContext(), 'compute.instance.create.end',
                      TEST_NOTICE_PAYLOAD)

        self.srv.listeners[0].wait()

        class SamplesMatcher(object):
            def __eq__(self, samples):
                for s in samples:
                    if s.resource_id != "9f9d01b9-4a58-4271-9e27-398b21ab20d1":
                        return False
                return True

        fake_publisher.publish_samples.assert_has_calls([
            mock.call(mock.ANY, SamplesMatcher()),
            mock.call(mock.ANY, SamplesMatcher()),
            mock.call(mock.ANY, SamplesMatcher()),
            mock.call(mock.ANY, SamplesMatcher()),
            mock.call(mock.ANY, SamplesMatcher()),
        ])

########NEW FILE########
__FILENAME__ = test_notifier
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 eNovance
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests for ceilometer/notifier.py
"""

from ceilometer import notifier
from ceilometer.openstack.common import test
from ceilometer import pipeline
from ceilometer import transformer


MESSAGE = {
    u'event_type': u'compute.instance.create.end',
    u'message_id': u'dae6f69c-00e0-41c0-b371-41ec3b7f4451',
    u'payload': {u'created_at': u'2012-05-08 20:23:41',
                 u'deleted_at': u'',
                 u'disk_gb': 0,
                 u'display_name': u'testme',
                 u'fixed_ips': [{u'address': u'10.0.0.2',
                                 u'floating_ips': [],
                                 u'meta': {},
                                 u'type': u'fixed',
                                 u'version': 4}],
                 u'image_ref_url': u'http://10.0.2.15:9292/images/UUID',
                 u'instance_id': u'9f9d01b9-4a58-4271-9e27-398b21ab20d1',
                 u'instance_type': u'm1.tiny',
                 u'instance_type_id': 2,
                 u'launched_at': u'2012-05-08 20:23:47.985999',
                 u'memory_mb': 512,
                 u'state': u'active',
                 u'state_description': u'',
                 u'tenant_id': u'7c150a59fe714e6f9263774af9688f0e',
                 u'user_id': u'1e3ce043029547f1a61c1996d1a531a2',
                 u'reservation_id': u'1e3ce043029547f1a61c1996d1a531a3',
                 u'vcpus': 1,
                 u'root_gb': 0,
                 u'ephemeral_gb': 0,
                 u'host': u'compute-host-name',
                 u'availability_zone': u'1e3ce043029547f1a61c1996d1a531a4',
                 u'os_type': u'linux?',
                 u'architecture': u'x86',
                 u'image_ref': u'UUID',
                 u'kernel_id': u'1e3ce043029547f1a61c1996d1a531a5',
                 u'ramdisk_id': u'1e3ce043029547f1a61c1996d1a531a6',
                 },
    u'priority': u'INFO',
    u'publisher_id': u'compute.vagrant-precise',
    u'timestamp': u'2012-05-08 20:23:48.028195',
}


class TestNotifier(test.BaseTestCase):

    def test_process_notification(self):
        transformer_manager = transformer.TransformerExtensionManager(
            'ceilometer.transformer',
        )
        notifier._pipeline_manager = pipeline.PipelineManager(
            [{
                'name': "test_pipeline",
                'interval': 60,
                'counters': ['*'],
                'transformers': [],
                'publishers': ["test"],
            }],
            transformer_manager)

        pub = notifier._pipeline_manager.pipelines[0].publishers[0]
        self.assertEqual(0, len(pub.samples))
        notifier.notify(None, MESSAGE)
        self.assertTrue(len(pub.samples) > 0)
        self.assertIn('disk.ephemeral.size',
                      [c.name for c in pub.samples])

########NEW FILE########
__FILENAME__ = test_novaclient
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 eNovance <licensing@enovance.com>
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import mock
from mock import patch
import novaclient

from ceilometer import nova_client
from ceilometer.openstack.common.fixture import mockpatch
from ceilometer.openstack.common import test


class TestNovaClient(test.BaseTestCase):

    def setUp(self):
        super(TestNovaClient, self).setUp()
        self.nv = nova_client.Client()
        self.useFixture(mockpatch.PatchObject(
            self.nv.nova_client.flavors, 'get',
            side_effect=self.fake_flavors_get))
        self.useFixture(mockpatch.PatchObject(
            self.nv.nova_client.images, 'get',
            side_effect=self.fake_images_get))

    @staticmethod
    def fake_flavors_get(*args, **kwargs):
        a = mock.MagicMock()
        a.id = args[0]
        if a.id == 1:
            a.name = 'm1.tiny'
        elif a.id == 2:
            a.name = 'm1.large'
        else:
            raise novaclient.exceptions.NotFound('foobar')
        return a

    @staticmethod
    def fake_images_get(*args, **kwargs):
        a = mock.MagicMock()
        a.id = args[0]
        image_details = {
            1: ('ubuntu-12.04-x86', dict(kernel_id=11, ramdisk_id=21)),
            2: ('centos-5.4-x64', dict(kernel_id=12, ramdisk_id=22)),
            3: ('rhel-6-x64', None),
            4: ('rhel-6-x64', dict()),
            5: ('rhel-6-x64', dict(kernel_id=11)),
            6: ('rhel-6-x64', dict(ramdisk_id=21))
        }

        if a.id in image_details:
            a.name = image_details[a.id][0]
            a.metadata = image_details[a.id][1]
        else:
            raise novaclient.exceptions.NotFound('foobar')

        return a

    @staticmethod
    def fake_flavors_list():
        a = mock.MagicMock()
        a.id = 1
        a.name = 'm1.tiny'
        b = mock.MagicMock()
        b.id = 2
        b.name = 'm1.large'
        return [a, b]

    @staticmethod
    def fake_servers_list(*args, **kwargs):
        a = mock.MagicMock()
        a.id = 42
        a.flavor = {'id': 1}
        a.image = {'id': 1}
        return [a]

    def test_instance_get_all_by_host(self):
        with patch.object(self.nv.nova_client.servers, 'list',
                          side_effect=self.fake_servers_list):
            instances = self.nv.instance_get_all_by_host('foobar')

        self.assertEqual(1, len(instances))
        self.assertEqual('m1.tiny', instances[0].flavor['name'])
        self.assertEqual('ubuntu-12.04-x86', instances[0].image['name'])
        self.assertEqual(11, instances[0].kernel_id)
        self.assertEqual(21, instances[0].ramdisk_id)

    @staticmethod
    def fake_servers_list_unknown_flavor(*args, **kwargs):
        a = mock.MagicMock()
        a.id = 42
        a.flavor = {'id': 666}
        a.image = {'id': 1}
        return [a]

    def test_instance_get_all_by_host_unknown_flavor(self):
        with patch.object(self.nv.nova_client.servers, 'list',
                          side_effect=self.fake_servers_list_unknown_flavor):
            instances = self.nv.instance_get_all_by_host('foobar')

        self.assertEqual(1, len(instances))
        self.assertEqual('unknown-id-666', instances[0].flavor['name'])

    @staticmethod
    def fake_servers_list_unknown_image(*args, **kwargs):
        a = mock.MagicMock()
        a.id = 42
        a.flavor = {'id': 1}
        a.image = {'id': 666}
        return [a]

    @staticmethod
    def fake_servers_list_image_missing_metadata(*args, **kwargs):
        a = mock.MagicMock()
        a.id = 42
        a.flavor = {'id': 1}
        a.image = {'id': args[0]}
        return [a]

    @staticmethod
    def fake_instance_image_missing(*args, **kwargs):
        a = mock.MagicMock()
        a.id = 42
        a.flavor = {'id': 666}
        a.image = None
        return [a]

    def test_instance_get_all_by_host_unknown_image(self):
        with patch.object(self.nv.nova_client.servers, 'list',
                          side_effect=self.fake_servers_list_unknown_image):
            instances = self.nv.instance_get_all_by_host('foobar')

        self.assertEqual(1, len(instances))
        self.assertEqual('unknown-id-666', instances[0].image['name'])

    def test_with_flavor_and_image(self):
        results = self.nv._with_flavor_and_image(self.fake_servers_list())
        instance = results[0]
        self.assertEqual('ubuntu-12.04-x86', instance.image['name'])
        self.assertEqual('m1.tiny', instance.flavor['name'])
        self.assertEqual(11, instance.kernel_id)
        self.assertEqual(21, instance.ramdisk_id)

    def test_with_flavor_and_image_unknown_image(self):
        instances = self.fake_servers_list_unknown_image()
        results = self.nv._with_flavor_and_image(instances)
        instance = results[0]
        self.assertEqual('unknown-id-666', instance.image['name'])
        self.assertNotEqual(instance.flavor['name'], 'unknown-id-666')
        self.assertIsNone(instance.kernel_id)
        self.assertIsNone(instance.ramdisk_id)

    def test_with_flavor_and_image_unknown_flavor(self):
        instances = self.fake_servers_list_unknown_flavor()
        results = self.nv._with_flavor_and_image(instances)
        instance = results[0]
        self.assertEqual('unknown-id-666', instance.flavor['name'])
        self.assertEqual(0, instance.flavor['vcpus'])
        self.assertEqual(0, instance.flavor['ram'])
        self.assertEqual(0, instance.flavor['disk'])
        self.assertNotEqual(instance.image['name'], 'unknown-id-666')
        self.assertEqual(11, instance.kernel_id)
        self.assertEqual(21, instance.ramdisk_id)

    def test_with_flavor_and_image_none_metadata(self):
        instances = self.fake_servers_list_image_missing_metadata(3)
        results = self.nv._with_flavor_and_image(instances)
        instance = results[0]
        self.assertIsNone(instance.kernel_id)
        self.assertIsNone(instance.ramdisk_id)

    def test_with_flavor_and_image_missing_metadata(self):
        instances = self.fake_servers_list_image_missing_metadata(4)
        results = self.nv._with_flavor_and_image(instances)
        instance = results[0]
        self.assertIsNone(instance.kernel_id)
        self.assertIsNone(instance.ramdisk_id)

    def test_with_flavor_and_image_missing_ramdisk(self):
        instances = self.fake_servers_list_image_missing_metadata(5)
        results = self.nv._with_flavor_and_image(instances)
        instance = results[0]
        self.assertEqual(11, instance.kernel_id)
        self.assertIsNone(instance.ramdisk_id)

    def test_with_flavor_and_image_missing_kernel(self):
        instances = self.fake_servers_list_image_missing_metadata(6)
        results = self.nv._with_flavor_and_image(instances)
        instance = results[0]
        self.assertIsNone(instance.kernel_id)
        self.assertEqual(21, instance.ramdisk_id)

    def test_with_missing_image_instance(self):
        instances = self.fake_instance_image_missing()
        results = self.nv._with_flavor_and_image(instances)
        instance = results[0]
        self.assertIsNone(instance.kernel_id)
        self.assertIsNone(instance.image)
        self.assertIsNone(instance.ramdisk_id)

########NEW FILE########
__FILENAME__ = test_plugin
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 eNovance <licensing@enovance.com>
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import mock

from ceilometer.openstack.common.fixture import config
from ceilometer.openstack.common import test
from ceilometer import plugin


TEST_NOTIFICATION = {
    u'_context_auth_token': u'3d8b13de1b7d499587dfc69b77dc09c2',
    u'_context_is_admin': True,
    u'_context_project_id': u'7c150a59fe714e6f9263774af9688f0e',
    u'_context_quota_class': None,
    u'_context_read_deleted': u'no',
    u'_context_remote_address': u'10.0.2.15',
    u'_context_request_id': u'req-d68b36e0-9233-467f-9afb-d81435d64d66',
    u'_context_roles': [u'admin'],
    u'_context_timestamp': u'2012-05-08T20:23:41.425105',
    u'_context_user_id': u'1e3ce043029547f1a61c1996d1a531a2',
    u'event_type': u'compute.instance.create.end',
    u'message_id': u'dae6f69c-00e0-41c0-b371-41ec3b7f4451',
    u'payload': {u'created_at': u'2012-05-08 20:23:41',
                 u'deleted_at': u'',
                 u'disk_gb': 0,
                 u'display_name': u'testme',
                 u'fixed_ips': [{u'address': u'10.0.0.2',
                                 u'floating_ips': [],
                                 u'meta': {},
                                 u'type': u'fixed',
                                 u'version': 4}],
                 u'image_ref_url': u'http://10.0.2.15:9292/images/UUID',
                 u'instance_id': u'9f9d01b9-4a58-4271-9e27-398b21ab20d1',
                 u'instance_type': u'm1.tiny',
                 u'instance_type_id': 2,
                 u'launched_at': u'2012-05-08 20:23:47.985999',
                 u'memory_mb': 512,
                 u'state': u'active',
                 u'state_description': u'',
                 u'tenant_id': u'7c150a59fe714e6f9263774af9688f0e',
                 u'user_id': u'1e3ce043029547f1a61c1996d1a531a2',
                 u'reservation_id': u'1e3ce043029547f1a61c1996d1a531a3',
                 u'vcpus': 1,
                 u'root_gb': 0,
                 u'ephemeral_gb': 0,
                 u'host': u'compute-host-name',
                 u'availability_zone': u'1e3ce043029547f1a61c1996d1a531a4',
                 u'os_type': u'linux?',
                 u'architecture': u'x86',
                 u'image_ref': u'UUID',
                 u'kernel_id': u'1e3ce043029547f1a61c1996d1a531a5',
                 u'ramdisk_id': u'1e3ce043029547f1a61c1996d1a531a6',
                 },
    u'priority': u'INFO',
    u'publisher_id': u'compute.vagrant-precise',
    u'timestamp': u'2012-05-08 20:23:48.028195',
}


class NotificationBaseTestCase(test.BaseTestCase):
    def setUp(self):
        super(NotificationBaseTestCase, self).setUp()
        self.CONF = self.useFixture(config.Config()).conf

    def test_handle_event_type(self):
        self.assertFalse(plugin.NotificationBase._handle_event_type(
            'compute.instance.start', ['compute']))
        self.assertFalse(plugin.NotificationBase._handle_event_type(
            'compute.instance.start', ['compute.*.foobar']))
        self.assertFalse(plugin.NotificationBase._handle_event_type(
            'compute.instance.start', ['compute.*.*.foobar']))
        self.assertTrue(plugin.NotificationBase._handle_event_type(
            'compute.instance.start', ['compute.*']))
        self.assertTrue(plugin.NotificationBase._handle_event_type(
            'compute.instance.start', ['*']))
        self.assertTrue(plugin.NotificationBase._handle_event_type(
            'compute.instance.start', ['compute.*.start']))
        self.assertTrue(plugin.NotificationBase._handle_event_type(
            'compute.instance.start', ['*.start']))
        self.assertTrue(plugin.NotificationBase._handle_event_type(
            'compute.instance.start', ['compute.*.*.foobar', 'compute.*']))

    class FakePlugin(plugin.NotificationBase):
        def get_exchange_topics(self, conf):
            return [plugin.ExchangeTopics(exchange="exchange1",
                                          topics=["t1", "t2"]),
                    plugin.ExchangeTopics(exchange="exchange2",
                                          topics=['t3'])]

        def process_notification(self, message):
            return message

    class FakeComputePlugin(FakePlugin):
        event_types = ['compute.*']

    class FakeNetworkPlugin(FakePlugin):
        event_types = ['network.*']

    def _do_test_to_samples(self, plugin_class, match):
        pm = mock.MagicMock()
        plug = plugin_class(pm)
        publish = pm.publisher.return_value.__enter__.return_value

        plug.to_samples_and_publish(mock.Mock(), TEST_NOTIFICATION)

        if match:
            publish.assert_called_once_with(list(TEST_NOTIFICATION))
        else:
            self.assertEqual(0, publish.call_count)

    def test_to_samples_match(self):
        self._do_test_to_samples(self.FakeComputePlugin, True)

    def test_to_samples_no_match(self):
        self._do_test_to_samples(self.FakeNetworkPlugin, False)

    def test_get_targets_compat(self):
        targets = self.FakeComputePlugin(mock.Mock()).get_targets(self.CONF)
        self.assertEqual(3, len(targets))
        self.assertEqual('t1', targets[0].topic)
        self.assertEqual('exchange1', targets[0].exchange)
        self.assertEqual('t2', targets[1].topic)
        self.assertEqual('exchange1', targets[1].exchange)
        self.assertEqual('t3', targets[2].topic)
        self.assertEqual('exchange2', targets[2].exchange)

########NEW FILE########
__FILENAME__ = test_utils
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
# Copyright (c) 2013 OpenStack Foundation
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
# All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests for ceilometer/utils.py
"""
import datetime
import decimal

from ceilometer.openstack.common import test
from ceilometer import utils


class TestUtils(test.BaseTestCase):

    def test_datetime_to_decimal(self):
        expected = 1356093296.12
        utc_datetime = datetime.datetime.utcfromtimestamp(expected)
        actual = utils.dt_to_decimal(utc_datetime)
        self.assertEqual(expected, float(actual))

    def test_decimal_to_datetime(self):
        expected = 1356093296.12
        dexpected = decimal.Decimal(str(expected))  # Python 2.6 wants str()
        expected_datetime = datetime.datetime.utcfromtimestamp(expected)
        actual_datetime = utils.decimal_to_dt(dexpected)
        self.assertEqual(expected_datetime, actual_datetime)

    def test_recursive_keypairs(self):
        data = {'a': 'A', 'b': 'B',
                'nested': {'a': 'A', 'b': 'B'}}
        pairs = list(utils.recursive_keypairs(data))
        self.assertEqual([('a', 'A'), ('b', 'B'),
                          ('nested:a', 'A'), ('nested:b', 'B')],
                         pairs)

    def test_recursive_keypairs_with_separator(self):
        data = {'a': 'A',
                'b': 'B',
                'nested': {'a': 'A',
                           'b': 'B',
                           },
                }
        separator = '.'
        pairs = list(utils.recursive_keypairs(data, separator))
        self.assertEqual([('a', 'A'),
                          ('b', 'B'),
                          ('nested.a', 'A'),
                          ('nested.b', 'B')],
                         pairs)

    def test_recursive_keypairs_with_list_of_dict(self):
        small = 1
        big = 1 << 64
        expected = [('a', 'A'),
                    ('b', 'B'),
                    ('nested:list', ['{%d: 99, %dL: 42}' % (small, big)])]
        # the keys 1 and 1<<64 cause a hash collision on 64bit platforms
        for nested in [{small: 99, big: 42}, {big: 42, small: 99}]:
            data = {'a': 'A',
                    'b': 'B',
                    'nested': {'list': [nested]}}
            pairs = list(utils.recursive_keypairs(data))
            self.assertEqual(expected, pairs)

    def test_restore_nesting_unested(self):
        metadata = {'a': 'A', 'b': 'B'}
        unwound = utils.restore_nesting(metadata)
        self.assertIs(metadata, unwound)

    def test_restore_nesting(self):
        metadata = {'a': 'A', 'b': 'B',
                    'nested:a': 'A',
                    'nested:b': 'B',
                    'nested:twice:c': 'C',
                    'nested:twice:d': 'D',
                    'embedded:e': 'E'}
        unwound = utils.restore_nesting(metadata)
        expected = {'a': 'A', 'b': 'B',
                    'nested': {'a': 'A', 'b': 'B',
                               'twice': {'c': 'C', 'd': 'D'}},
                    'embedded': {'e': 'E'}}
        self.assertEqual(expected, unwound)
        self.assertIsNot(metadata, unwound)

    def test_restore_nesting_with_separator(self):
        metadata = {'a': 'A', 'b': 'B',
                    'nested.a': 'A',
                    'nested.b': 'B',
                    'nested.twice.c': 'C',
                    'nested.twice.d': 'D',
                    'embedded.e': 'E'}
        unwound = utils.restore_nesting(metadata, separator='.')
        expected = {'a': 'A', 'b': 'B',
                    'nested': {'a': 'A', 'b': 'B',
                               'twice': {'c': 'C', 'd': 'D'}},
                    'embedded': {'e': 'E'}}
        self.assertEqual(expected, unwound)
        self.assertIsNot(metadata, unwound)

    def test_decimal_to_dt_with_none_parameter(self):
        self.assertIsNone(utils.decimal_to_dt(None))

    def test_dict_to_kv(self):
        data = {'a': 'A',
                'b': 'B',
                'nested': {'a': 'A',
                           'b': 'B',
                           },
                'nested2': [{'c': 'A'}, {'c': 'B'}]
                }
        pairs = list(utils.dict_to_keyval(data))
        self.assertEqual([('a', 'A'), ('b', 'B'),
                         ('nested2[0].c', 'A'),
                         ('nested2[1].c', 'B'),
                         ('nested.a', 'A'),
                         ('nested.b', 'B')],
                         pairs)

########NEW FILE########
__FILENAME__ = test_notifications
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import mock

from ceilometer.openstack.common import test
from ceilometer.volume import notifications

NOTIFICATION_VOLUME_EXISTS = {
    u'_context_roles': [u'admin'],
    u'_context_request_id': u'req-7ef29a5d-adeb-48a8-b104-59c05361aa27',
    u'_context_quota_class': None,
    u'event_type': u'volume.exists',
    u'timestamp': u'2012-09-21 09:29:10.620731',
    u'message_id': u'e0e6a5ad-2fc9-453c-b3fb-03fe504538dc',
    u'_context_auth_token': None,
    u'_context_is_admin': True,
    u'_context_project_id': None,
    u'_context_timestamp': u'2012-09-21T09:29:10.266928',
    u'_context_read_deleted': u'no',
    u'_context_user_id': None,
    u'_context_remote_address': None,
    u'publisher_id': u'volume.ubuntu-VirtualBox',
    u'payload': {u'status': u'available',
                 u'audit_period_beginning': u'2012-09-20 00:00:00',
                 u'display_name': u'volume1',
                 u'tenant_id': u'6c97f1ecf17047eab696786d56a0bff5',
                 u'created_at': u'2012-09-20 15:05:16',
                 u'snapshot_id': None,
                 u'volume_type': None,
                 u'volume_id': u'84c363b9-9854-48dc-b949-fe04263f4cf0',
                 u'audit_period_ending': u'2012-09-21 00:00:00',
                 u'user_id': u'4d2fa4b76a4a4ecab8c468c8dea42f89',
                 u'launched_at': u'2012-09-20 15:05:23',
                 u'size': 2},
    u'priority': u'INFO'
}

NOTIFICATION_VOLUME_DELETE = {
    u'_context_roles': [u'Member', u'admin'],
    u'_context_request_id': u'req-6ba8ccb4-1093-4a39-b029-adfaa3fc7ceb',
    u'_context_quota_class': None,
    u'event_type': u'volume.delete.start',
    u'timestamp': u'2012-09-21 10:24:13.168630',
    u'message_id': u'f6e6bc1f-fcd5-41e1-9a86-da7d024f03d9',
    u'_context_auth_token': u'277c6899de8a4b3d999f3e2e4c0915ff',
    u'_context_is_admin': True,
    u'_context_project_id': u'6c97f1ecf17047eab696786d56a0bff5',
    u'_context_timestamp': u'2012-09-21T10:23:54.741228',
    u'_context_read_deleted': u'no',
    u'_context_user_id': u'4d2fa4b76a4a4ecab8c468c8dea42f89',
    u'_context_remote_address': u'192.168.22.101',
    u'publisher_id': u'volume.ubuntu-VirtualBox',
    u'payload': {u'status': u'deleting',
                 u'volume_type_id': None,
                 u'display_name': u'abc',
                 u'tenant_id': u'6c97f1ecf17047eab696786d56a0bff5',
                 u'created_at': u'2012-09-21 10:10:47',
                 u'snapshot_id': None,
                 u'volume_id': u'3b761164-84b4-4eb3-8fcb-1974c641d6ef',
                 u'user_id': u'4d2fa4b76a4a4ecab8c468c8dea42f89',
                 u'launched_at': u'2012-09-21 10:10:50',
                 u'size': 3},
    u'priority': u'INFO'}


NOTIFICATION_VOLUME_RESIZE = {
    u'_context_roles': [u'Member', u'admin'],
    u'_context_request_id': u'req-6ba8ccb4-1093-4a39-b029-adfaa3fc7ceb',
    u'_context_quota_class': None,
    u'event_type': u'volume.resize.end',
    u'timestamp': u'2012-09-21 10:24:13.168630',
    u'message_id': u'b5814258-3425-4eb7-b6b7-bf4811203e58',
    u'_context_auth_token': u'277c6899de8a4b3d999f3e2e4c0915ff',
    u'_context_is_admin': True,
    u'_context_project_id': u'6c97f1ecf17047eab696786d56a0bff5',
    u'_context_timestamp': u'2012-09-21T10:02:27.134211',
    u'_context_read_deleted': u'no',
    u'_context_user_id': u'4d2fa4b76a4a4ecab8c468c8dea42f89',
    u'_context_remote_address': u'192.168.22.101',
    u'publisher_id': u'volume.ubuntu-VirtualBox',
    u'payload': {u'status': u'extending',
                 u'volume_type_id': None,
                 u'display_name': u'abc',
                 u'tenant_id': u'6c97f1ecf17047eab696786d56a0bff5',
                 u'created_at': u'2012-09-21 10:10:47',
                 u'snapshot_id': None,
                 u'volume_id': u'3b761164-84b4-4eb3-8fcb-1974c641d6ef',
                 u'user_id': u'4d2fa4b76a4a4ecab8c468c8dea42f89',
                 u'launched_at': u'2012-09-21 10:10:50',
                 u'size': 3},
    u'priority': u'INFO'}


NOTIFICATION_SNAPSHOT_EXISTS = {
    u'_context_roles': [u'admin'],
    u'_context_request_id': u'req-7ef29a5d-adeb-48a8-b104-59c05361aa27',
    u'_context_quota_class': None,
    u'event_type': u'snapshot.exists',
    u'timestamp': u'2012-09-21 09:29:10.620731',
    u'message_id': u'e0e6a5ad-2fc9-453c-b3fb-03fe504538dc',
    u'_context_auth_token': None,
    u'_context_is_admin': True,
    u'_context_project_id': None,
    u'_context_timestamp': u'2012-09-21T09:29:10.266928',
    u'_context_read_deleted': u'no',
    u'_context_user_id': None,
    u'_context_remote_address': None,
    u'publisher_id': u'volume.ubuntu-VirtualBox',
    u"payload": {u"audit_period_beginning": u"2014-05-06 11:00:00",
                 u"audit_period_ending": u"2014-05-06 12:00:00",
                 u"availability_zone": u"left",
                 u"created_at": u"2014-05-06 09:33:43",
                 u"deleted": u"",
                 u"display_name": "lil snapshot",
                 u"snapshot_id": u"dd163129-9476-4cf5-9311-dd425324d8d8",
                 u"status": u"available",
                 u"tenant_id": u"compliance",
                 u"user_id": u"e0271f64847b49429bb304c775c7427a",
                 u"volume_id": u"b96e026e-c9bf-4418-8d6f-4ba493bbb7d6",
                 u"volume_size": 3},
    u'priority': u'INFO'}


class TestNotifications(test.BaseTestCase):

    def _verify_common_sample_volume(self, s, name, notification):
        self.assertIsNotNone(s)
        self.assertEqual(s.name, name)
        self.assertEqual(notification['payload']['volume_id'], s.resource_id)
        self.assertEqual(notification['timestamp'], s.timestamp)
        metadata = s.resource_metadata
        self.assertEqual(notification['publisher_id'], metadata.get('host'))

    def test_volume_exists(self):
        v = notifications.Volume(mock.Mock())
        samples = list(v.process_notification(NOTIFICATION_VOLUME_EXISTS))
        self.assertEqual(1, len(samples))
        s = samples[0]
        self._verify_common_sample_volume(
            s, 'volume', NOTIFICATION_VOLUME_EXISTS)
        self.assertEqual(1, s.volume)

    def test_volume_size_exists(self):
        v = notifications.VolumeSize(mock.Mock())
        samples = list(v.process_notification(NOTIFICATION_VOLUME_EXISTS))
        self.assertEqual(1, len(samples))
        s = samples[0]
        self._verify_common_sample_volume(s, 'volume.size',
                                          NOTIFICATION_VOLUME_EXISTS)
        self.assertEqual(NOTIFICATION_VOLUME_EXISTS['payload']['size'],
                         s.volume)

    def test_volume_delete(self):
        v = notifications.Volume(mock.Mock())
        samples = list(v.process_notification(NOTIFICATION_VOLUME_DELETE))
        self.assertEqual(1, len(samples))
        s = samples[0]
        self._verify_common_sample_volume(
            s, 'volume', NOTIFICATION_VOLUME_DELETE)
        self.assertEqual(1, s.volume)

    def test_volume_size_delete(self):
        v = notifications.VolumeSize(mock.Mock())
        samples = list(v.process_notification(NOTIFICATION_VOLUME_DELETE))
        self.assertEqual(1, len(samples))
        s = samples[0]
        self._verify_common_sample_volume(s, 'volume.size',
                                          NOTIFICATION_VOLUME_DELETE)
        self.assertEqual(NOTIFICATION_VOLUME_DELETE['payload']['size'],
                         s.volume)

    def test_volume_resize(self):
        v = notifications.Volume(mock.Mock())
        samples = list(v.process_notification(NOTIFICATION_VOLUME_RESIZE))
        self.assertEqual(1, len(samples))
        s = samples[0]
        self._verify_common_sample_volume(
            s, 'volume', NOTIFICATION_VOLUME_RESIZE)
        self.assertEqual(1, s.volume)

    def test_volume_size_resize(self):
        v = notifications.VolumeSize(mock.Mock())
        samples = list(v.process_notification(NOTIFICATION_VOLUME_RESIZE))
        self.assertEqual(1, len(samples))
        s = samples[0]
        self._verify_common_sample_volume(s, 'volume.size',
                                          NOTIFICATION_VOLUME_RESIZE)
        self.assertEqual(NOTIFICATION_VOLUME_RESIZE['payload']['size'],
                         s.volume)

    def _verify_common_sample_snapshot(self, s, name, notification):
        self.assertIsNotNone(s)
        self.assertEqual(name, s.name)
        self.assertEqual(notification['payload']['snapshot_id'], s.resource_id)
        self.assertEqual(notification['timestamp'], s.timestamp)
        metadata = s.resource_metadata
        self.assertEqual(notification['publisher_id'], metadata.get('host'))

    def test_snapshot_exists(self):
        v = notifications.Snapshot(mock.Mock())
        samples = list(v.process_notification(NOTIFICATION_SNAPSHOT_EXISTS))
        self.assertEqual(1, len(samples))
        s = samples[0]
        self._verify_common_sample_snapshot(s, 'snapshot',
                                            NOTIFICATION_SNAPSHOT_EXISTS)
        self.assertEqual(1, s.volume)

    def test_snapshot_size_exists(self):
        v = notifications.SnapshotSize(mock.Mock())
        samples = list(v.process_notification(NOTIFICATION_SNAPSHOT_EXISTS))
        self.assertEqual(1, len(samples))
        s = samples[0]
        self._verify_common_sample_snapshot(s, 'snapshot.size',
                                            NOTIFICATION_SNAPSHOT_EXISTS)
        volume_size = NOTIFICATION_SNAPSHOT_EXISTS['payload']['volume_size']
        self.assertEqual(volume_size, s.volume)

########NEW FILE########
__FILENAME__ = accumulator
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Julien Danjou
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ceilometer import transformer


class TransformerAccumulator(transformer.TransformerBase):
    """Transformer that accumulates sample until a threshold, and then flush
    them out in the wild.

    """

    def __init__(self, size=1, **kwargs):
        if size >= 1:
            self.samples = []
        self.size = size
        super(TransformerAccumulator, self).__init__(**kwargs)

    def handle_sample(self, context, sample):
        if self.size >= 1:
            self.samples.append(sample)
        else:
            return sample

    def flush(self, context):
        if len(self.samples) >= self.size:
            x = self.samples
            self.samples = []
            return x
        return []

########NEW FILE########
__FILENAME__ = conversions
# -*- encoding: utf-8 -*-
#
# Copyright © 2013 Red Hat, Inc
#
# Author: Eoghan Glynn <eglynn@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import collections
import re

from ceilometer.openstack.common.gettextutils import _
from ceilometer.openstack.common import log
from ceilometer.openstack.common import timeutils
from ceilometer import sample
from ceilometer import transformer

LOG = log.getLogger(__name__)


class Namespace(object):
    """Encapsulates the namespace wrapping the evaluation of the
       configured scale factor. This allows nested dicts to be
       accessed in the attribute style, and missing attributes
       to yield false when used in a boolean expression.
    """
    def __init__(self, seed):
        self.__dict__ = collections.defaultdict(lambda: Namespace({}))
        self.__dict__.update(seed)
        for k, v in self.__dict__.iteritems():
            if isinstance(v, dict):
                self.__dict__[k] = Namespace(v)

    def __getattr__(self, attr):
        return self.__dict__[attr]

    def __getitem__(self, key):
        return self.__dict__[key]

    def __nonzero__(self):
        return len(self.__dict__) > 0


class ScalingTransformer(transformer.TransformerBase):
    """Transformer to apply a scaling conversion.
    """

    def __init__(self, source=None, target=None, **kwargs):
        """Initialize transformer with configured parameters.

        :param source: dict containing source sample unit
        :param target: dict containing target sample name, type,
                       unit and scaling factor (a missing value
                       connotes no change)
        """
        source = source or {}
        target = target or {}
        self.source = source
        self.target = target
        self.scale = target.get('scale')
        LOG.debug(_('scaling conversion transformer with source:'
                    ' %(source)s target: %(target)s:')
                  % {'source': source,
                     'target': target})
        super(ScalingTransformer, self).__init__(**kwargs)

    def _scale(self, s):
        """Apply the scaling factor (either a straight multiplicative
           factor or else a string to be eval'd).
        """
        ns = Namespace(s.as_dict())

        scale = self.scale
        return ((eval(scale, {}, ns) if isinstance(scale, basestring)
                 else s.volume * scale) if scale else s.volume)

    def _map(self, s, attr):
        """Apply the name or unit mapping if configured.
        """
        mapped = None
        from_ = self.source.get('map_from')
        to_ = self.target.get('map_to')
        if from_ and to_:
            if from_.get(attr) and to_.get(attr):
                try:
                    mapped = re.sub(from_[attr], to_[attr], getattr(s, attr))
                except Exception:
                    pass
        return mapped or self.target.get(attr, getattr(s, attr))

    def _convert(self, s, growth=1):
        """Transform the appropriate sample fields.
        """
        return sample.Sample(
            name=self._map(s, 'name'),
            unit=self._map(s, 'unit'),
            type=self.target.get('type', s.type),
            volume=self._scale(s) * growth,
            user_id=s.user_id,
            project_id=s.project_id,
            resource_id=s.resource_id,
            timestamp=s.timestamp,
            resource_metadata=s.resource_metadata
        )

    def handle_sample(self, context, s):
        """Handle a sample, converting if necessary."""
        LOG.debug(_('handling sample %s'), (s,))
        if (self.source.get('unit', s.unit) == s.unit):
            s = self._convert(s)
            LOG.debug(_('converted to: %s'), (s,))
        return s


class RateOfChangeTransformer(ScalingTransformer):
    """Transformer based on the rate of change of a sample volume,
       for example taking the current and previous volumes of a
       cumulative sample and producing a gauge value based on the
       proportion of some maximum used.
    """

    def __init__(self, **kwargs):
        """Initialize transformer with configured parameters.
        """
        super(RateOfChangeTransformer, self).__init__(**kwargs)
        self.cache = {}
        self.scale = self.scale or '1'

    def handle_sample(self, context, s):
        """Handle a sample, converting if necessary."""
        LOG.debug(_('handling sample %s'), (s,))
        key = s.name + s.resource_id
        prev = self.cache.get(key)
        timestamp = timeutils.parse_isotime(s.timestamp)
        self.cache[key] = (s.volume, timestamp)

        if prev:
            prev_volume = prev[0]
            prev_timestamp = prev[1]
            time_delta = timeutils.delta_seconds(prev_timestamp, timestamp)
            # we only allow negative deltas for noncumulative samples, whereas
            # for cumulative we assume that a reset has occurred in the interim
            # so that the current volume gives a lower bound on growth
            volume_delta = (s.volume - prev_volume
                            if (prev_volume <= s.volume or
                                s.type != sample.TYPE_CUMULATIVE)
                            else s.volume)
            rate_of_change = ((1.0 * volume_delta / time_delta)
                              if time_delta else 0.0)

            s = self._convert(s, rate_of_change)
            LOG.debug(_('converted to: %s'), (s,))
        else:
            LOG.warn(_('dropping sample with no predecessor: %s'),
                     (s,))
            s = None
        return s


class AggregatorTransformer(ScalingTransformer):
    """Transformer that aggregate sample until a threshold or/and a
    retention_time, and then flush them out in the wild.

    Example:
      To aggregate sample by resource_metadata and keep the
      resource_metadata of the latest received sample;

        AggregatorTransformer(retention_time=60, resource_metadata='last')

      To aggregate sample by user_id and resource_metadata and keep the
      user_id of the first received sample and drop the resource_metadata.

        AggregatorTransformer(size=15, user_id='first',
                              resource_metadata='drop')

    """

    def __init__(self, size=1, retention_time=None,
                 project_id=None, user_id=None, resource_metadata="last",
                 **kwargs):
        super(AggregatorTransformer, self).__init__(**kwargs)
        self.samples = {}
        self.size = size
        self.retention_time = retention_time
        self.initial_timestamp = None
        self.aggregated_samples = 0

        self.key_attributes = []
        self.merged_attribute_policy = {}

        self._init_attribute('project_id', project_id)
        self._init_attribute('user_id', user_id)
        self._init_attribute('resource_metadata', resource_metadata,
                             is_droppable=True, mandatory=True)

    def _init_attribute(self, name, value, is_droppable=False,
                        mandatory=False):
        drop = ['drop'] if is_droppable else []
        if value or mandatory:
            if value not in ['last', 'first'] + drop:
                LOG.warn('%s is unknown (%s), using last' % (name, value))
                value = 'last'
            self.merged_attribute_policy[name] = value
        else:
            self.key_attributes.append(name)

    def _get_unique_key(self, s):
        non_aggregated_keys = "-".join([getattr(s, field)
                                        for field in self.key_attributes])
        #NOTE(sileht): it assumes, a meter always have the same unit/type
        return "%s-%s-%s" % (s.name, s.resource_id, non_aggregated_keys)

    def handle_sample(self, context, sample):
        if not self.initial_timestamp:
            self.initial_timestamp = timeutils.parse_strtime(
                sample.timestamp)

        self.aggregated_samples += 1
        key = self._get_unique_key(sample)
        if key not in self.samples:
            self.samples[key] = self._convert(sample)
            if self.merged_attribute_policy[
                    'resource_metadata'] == 'drop':
                self.samples[key].resource_metadata = {}
        else:
            self.samples[key].volume += self._scale(sample)
            for field in self.merged_attribute_policy:
                if self.merged_attribute_policy[field] == 'last':
                    setattr(self.samples[key], field,
                            getattr(sample, field))

    def flush(self, context):
        expired = self.retention_time and \
            timeutils.is_older_than(self.initial_timestamp,
                                    self.retention_time)
        full = self.aggregated_samples >= self.size
        if full or expired:
            x = self.samples.values()
            self.samples = {}
            self.aggregated_samples = 0
            self.initial_timestamp = None
            return x
        return []

########NEW FILE########
__FILENAME__ = utils
# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# Copyright 2011 Justin Santa Barbara

# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Utilities and helper functions."""

import calendar
import copy
import datetime
import decimal
import multiprocessing

from ceilometer.openstack.common import timeutils
from ceilometer.openstack.common import units


def recursive_keypairs(d, separator=':'):
    """Generator that produces sequence of keypairs for nested dictionaries.
    """
    for name, value in sorted(d.iteritems()):
        if isinstance(value, dict):
            for subname, subvalue in recursive_keypairs(value, separator):
                yield ('%s%s%s' % (name, separator, subname), subvalue)
        elif isinstance(value, (tuple, list)):
            # When doing a pair of JSON encode/decode operations to the tuple,
            # the tuple would become list. So we have to generate the value as
            # list here.

            # in the special case of the list item itself being a dict,
            # create an equivalent dict with a predictable insertion order
            # to avoid inconsistencies in the message signature computation
            # for equivalent payloads modulo ordering
            first = lambda i: i[0]
            m = map(lambda x: unicode(dict(sorted(x.items(), key=first))
                                      if isinstance(x, dict)
                                      else x).encode('utf-8'),
                    value)
            yield name, list(m)
        else:
            yield name, value


def restore_nesting(d, separator=':'):
    """Unwinds a flattened dict to restore nesting.
    """
    d = copy.copy(d) if any([separator in k for k in d.keys()]) else d
    for k, v in d.items():
        if separator in k:
            top, rem = k.split(separator, 1)
            nest = d[top] if isinstance(d.get(top), dict) else {}
            nest[rem] = v
            d[top] = restore_nesting(nest, separator)
            del d[k]
    return d


def dt_to_decimal(utc):
    """Datetime to Decimal.

    Some databases don't store microseconds in datetime
    so we always store as Decimal unixtime.
    """
    if utc is None:
        return None

    decimal.getcontext().prec = 30
    return decimal.Decimal(str(calendar.timegm(utc.utctimetuple()))) + \
        (decimal.Decimal(str(utc.microsecond)) /
         decimal.Decimal("1000000.0"))


def decimal_to_dt(dec):
    """Return a datetime from Decimal unixtime format.
    """
    if dec is None:
        return None

    integer = int(dec)
    micro = (dec - decimal.Decimal(integer)) * decimal.Decimal(units.M)
    daittyme = datetime.datetime.utcfromtimestamp(integer)
    return daittyme.replace(microsecond=int(round(micro)))


def sanitize_timestamp(timestamp):
    """Return a naive utc datetime object."""
    if not timestamp:
        return timestamp
    if not isinstance(timestamp, datetime.datetime):
        timestamp = timeutils.parse_isotime(timestamp)
    return timeutils.normalize_time(timestamp)


def stringify_timestamps(data):
    """Stringify any datetimes in given dict."""
    isa_timestamp = lambda v: isinstance(v, datetime.datetime)
    return dict((k, v.isoformat() if isa_timestamp(v) else v)
                for (k, v) in data.iteritems())


def dict_to_keyval(value, key_base=None):
    """Expand a given dict to its corresponding key-value pairs.

    Generated keys are fully qualified, delimited using dot notation.
    ie. key = 'key.child_key.grandchild_key[0]'
    """
    val_iter, key_func = None, None
    if isinstance(value, dict):
        val_iter = value.iteritems()
        key_func = lambda k: key_base + '.' + k if key_base else k
    elif isinstance(value, (tuple, list)):
        val_iter = enumerate(value)
        key_func = lambda k: key_base + '[%d]' % k

    if val_iter:
        for k, v in val_iter:
            key_gen = key_func(k)
            if isinstance(v, dict) or isinstance(v, (tuple, list)):
                for key_gen, v in dict_to_keyval(v, key_gen):
                    yield key_gen, v
            else:
                yield key_gen, v


def lowercase_keys(mapping):
    """Converts the values of the keys in mapping to lowercase."""
    items = mapping.items()
    for key, value in items:
        del mapping[key]
        mapping[key.lower()] = value


def lowercase_values(mapping):
    """Converts the values in the mapping dict to lowercase."""
    items = mapping.items()
    for key, value in items:
        mapping[key] = value.lower()


def update_nested(original_dict, updates):
    """Updates the leaf nodes in a nest dict, without replacing
       entire sub-dicts.
    """
    dict_to_update = copy.deepcopy(original_dict)
    for key, value in updates.iteritems():
        if isinstance(value, dict):
            sub_dict = update_nested(dict_to_update.get(key, {}), value)
            dict_to_update[key] = sub_dict
        else:
            dict_to_update[key] = updates[key]
    return dict_to_update


def cpu_count():
    try:
        return multiprocessing.cpu_count() or 1
    except NotImplementedError:
        return 1


def uniq(dupes, attrs):
    """Exclude elements of dupes with a duplicated set of attribute values."""
    key = lambda d: '/'.join([getattr(d, a) or '' for a in attrs])
    keys = []
    deduped = []
    for d in dupes:
        if key(d) not in keys:
            deduped.append(d)
            keys.append(key(d))
    return deduped

########NEW FILE########
__FILENAME__ = notifications
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Julien Danjou <julien@danjou.info>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Converters for producing volume counter messages from cinder notification
events.
"""

from oslo.config import cfg
import oslo.messaging

from ceilometer import plugin
from ceilometer import sample


OPTS = [
    cfg.StrOpt('cinder_control_exchange',
               default='cinder',
               help="Exchange name for Cinder notifications."),
]


cfg.CONF.register_opts(OPTS)


class _Base(plugin.NotificationBase):
    """Convert volume/snapshot notification into Counters."""

    @staticmethod
    def get_targets(conf):
        """Return a sequence of oslo.messaging.Target defining the exchange and
        topics to be connected for this plugin.
        """
        return [oslo.messaging.Target(topic=topic,
                                      exchange=conf.cinder_control_exchange)
                for topic in conf.notification_topics]


class _VolumeBase(_Base):
    """Convert volume notifications into Counters."""

    event_types = [
        'volume.exists',
        'volume.create.*',
        'volume.delete.*',
        'volume.resize.*',
    ]


class Volume(_VolumeBase):
    def process_notification(self, message):
        yield sample.Sample.from_notification(
            name='volume',
            type=sample.TYPE_GAUGE,
            unit='volume',
            volume=1,
            user_id=message['payload']['user_id'],
            project_id=message['payload']['tenant_id'],
            resource_id=message['payload']['volume_id'],
            message=message)


class VolumeSize(_VolumeBase):
    def process_notification(self, message):
        yield sample.Sample.from_notification(
            name='volume.size',
            type=sample.TYPE_GAUGE,
            unit='GB',
            volume=message['payload']['size'],
            user_id=message['payload']['user_id'],
            project_id=message['payload']['tenant_id'],
            resource_id=message['payload']['volume_id'],
            message=message)


class _SnapshotBase(_Base):
    """Convert snapshot notifications into Counters."""

    event_types = [
        'snapshot.exists',
        'snapshot.create.*',
        'snapshot.delete.*',
        'snapshot.resize.*',
    ]


class Snapshot(_SnapshotBase):
    def process_notification(self, message):
        yield sample.Sample.from_notification(
            name='snapshot',
            type=sample.TYPE_GAUGE,
            unit='snapshot',
            volume=1,
            user_id=message['payload']['user_id'],
            project_id=message['payload']['tenant_id'],
            resource_id=message['payload']['snapshot_id'],
            message=message)


class SnapshotSize(_SnapshotBase):
    def process_notification(self, message):
        yield sample.Sample.from_notification(
            name='snapshot.size',
            type=sample.TYPE_GAUGE,
            unit='GB',
            volume=message['payload']['volume_size'],
            user_id=message['payload']['user_id'],
            project_id=message['payload']['tenant_id'],
            resource_id=message['payload']['snapshot_id'],
            message=message)

########NEW FILE########
__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# Ceilometer documentation build configuration file, created by
# sphinx-quickstart on Thu Oct 27 11:38:59 2011.
#
# This file is execfile()d with the current directory set to its
# containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.
from __future__ import print_function

import sys
import os

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT = os.path.abspath(os.path.join(BASE_DIR, "..", ".."))

sys.path.insert(0, ROOT)
sys.path.insert(0, BASE_DIR)

# This is required for ReadTheDocs.org, but isn't a bad idea anyway.
os.environ['DJANGO_SETTINGS_MODULE'] = 'openstack_dashboard.settings'


def write_autodoc_index():

    def find_autodoc_modules(module_name, sourcedir):
        """Return a list of modules in the SOURCE directory."""
        modlist = []
        os.chdir(os.path.join(sourcedir, module_name))
        print("SEARCHING %s" % sourcedir)
        for root, dirs, files in os.walk("."):
            for filename in files:
                if filename.endswith(".py"):
                    # remove the pieces of the root
                    elements = root.split(os.path.sep)
                    # replace the leading "." with the module name
                    elements[0] = module_name
                    # and get the base module name
                    base, extension = os.path.splitext(filename)
                    if not (base == "__init__"):
                        elements.append(base)
                    result = ".".join(elements)
                    #print result
                    modlist.append(result)
        return modlist

    RSTDIR = os.path.abspath(os.path.join(BASE_DIR, "sourcecode"))
    SRCS = {'ceilometer': ROOT}

    EXCLUDED_MODULES = ('ceilometer.tests')
    CURRENT_SOURCES = {}

    if not(os.path.exists(RSTDIR)):
        os.mkdir(RSTDIR)
    CURRENT_SOURCES[RSTDIR] = ['autoindex.rst']

    INDEXOUT = open(os.path.join(RSTDIR, "autoindex.rst"), "w")
    INDEXOUT.write("=================\n")
    INDEXOUT.write("Source Code Index\n")
    INDEXOUT.write("=================\n")

    for modulename, path in SRCS.items():
        sys.stdout.write("Generating source documentation for %s\n" %
                         modulename)
        INDEXOUT.write("\n%s\n" % modulename.capitalize())
        INDEXOUT.write("%s\n" % ("=" * len(modulename),))
        INDEXOUT.write(".. toctree::\n")
        INDEXOUT.write("   :maxdepth: 1\n")
        INDEXOUT.write("\n")

        MOD_DIR = os.path.join(RSTDIR, modulename)
        CURRENT_SOURCES[MOD_DIR] = []
        if not(os.path.exists(MOD_DIR)):
            os.mkdir(MOD_DIR)
        for module in find_autodoc_modules(modulename, path):
            if any([module.startswith(exclude)
                    for exclude
                    in EXCLUDED_MODULES]):
                print("Excluded module %s." % module)
                continue
            mod_path = os.path.join(path, *module.split("."))
            generated_file = os.path.join(MOD_DIR, "%s.rst" % module)

            INDEXOUT.write("   %s/%s\n" % (modulename, module))

            # Find the __init__.py module if this is a directory
            if os.path.isdir(mod_path):
                source_file = ".".join((os.path.join(mod_path, "__init__"),
                                        "py",))
            else:
                source_file = ".".join((os.path.join(mod_path), "py"))

            CURRENT_SOURCES[MOD_DIR].append("%s.rst" % module)
            # Only generate a new file if the source has changed or we don't
            # have a doc file to begin with.
            if not os.access(generated_file, os.F_OK) or \
                    os.stat(generated_file).st_mtime < \
                    os.stat(source_file).st_mtime:
                print("Module %s updated, generating new documentation." \
                      % module)
                FILEOUT = open(generated_file, "w")
                header = "The :mod:`%s` Module" % module
                FILEOUT.write("%s\n" % ("=" * len(header),))
                FILEOUT.write("%s\n" % header)
                FILEOUT.write("%s\n" % ("=" * len(header),))
                FILEOUT.write(".. automodule:: %s\n" % module)
                FILEOUT.write("  :members:\n")
                FILEOUT.write("  :undoc-members:\n")
                FILEOUT.write("  :show-inheritance:\n")
                FILEOUT.write("  :noindex:\n")
                FILEOUT.close()

    INDEXOUT.close()

    # Delete auto-generated .rst files for sources which no longer exist
    for directory, subdirs, files in list(os.walk(RSTDIR)):
        for old_file in files:
            if old_file not in CURRENT_SOURCES.get(directory, []):
                print("Removing outdated file for %s" % old_file)
                os.remove(os.path.join(directory, old_file))


write_autodoc_index()

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

# -- General configuration ----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings.
# They can be extensions coming with Sphinx (named 'sphinx.ext.*')
# or your custom ones.
extensions = [
    'sphinx.ext.autodoc',
    'sphinx.ext.intersphinx',
    'sphinx.ext.todo',
    'sphinxcontrib.autohttp.flask',
    'wsmeext.sphinxext',
    'sphinx.ext.coverage',
    'sphinx.ext.pngmath',
    'sphinx.ext.viewcode',
    'sphinxcontrib.pecanwsme.rest',
    'oslosphinx',
    'sphinxcontrib.docbookrestapi.setup'
]

wsme_protocols = ['restjson', 'restxml']

todo_include_todos = True

# Add any paths that contain templates here, relative to this directory.
if os.getenv('HUDSON_PUBLISH_DOCS'):
    templates_path = ['_ga', '_templates']
else:
    templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'Ceilometer'
copyright = u'2012-2014, OpenStack Foundation'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['**/#*', '**~', '**/#*#']

# The reST default role (used for this markup: `text`)
# to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []

primary_domain = 'py'
nitpicky = False


# -- Options for HTML output --------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
# html_theme_path = ['.']
# html_theme = '_theme'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
html_theme_options = {
    "nosidebar": "false"
}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'
git_cmd = "git log --pretty=format:'%ad, commit %h' --date=local -n1"
html_last_updated_fmt = os.popen(git_cmd).read()

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'Ceilometerdoc'


# -- Options for LaTeX output -------------------------------------------------

latex_elements = {
    # The paper size ('letterpaper' or 'a4paper').
    #'papersize': 'letterpaper',

    # The font size ('10pt', '11pt' or '12pt').
    #'pointsize': '10pt',

    # Additional stuff for the LaTeX preamble.
    #'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass
# [howto/manual]).
latex_documents = [
    ('index', 'Ceilometer.tex', u'Ceilometer Documentation',
     u'OpenStack Foundation', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output -------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'ceilometer', u'Ceilometer Documentation',
     [u'OpenStack'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output -----------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
    ('index', 'Ceilometer', u'Ceilometer Documentation', u'OpenStack',
     'Ceilometer', 'One line description of project.', 'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'


# -- Options for Epub output --------------------------------------------------

# Bibliographic Dublin Core info.
epub_title = u'Ceilometer'
epub_author = u'OpenStack'
epub_publisher = u'OpenStack'
epub_copyright = u'2012-2014, OpenStack'

# The language of the text. It defaults to the language option
# or en if the language is not set.
#epub_language = ''

# The scheme of the identifier. Typical schemes are ISBN or URL.
#epub_scheme = ''

# The unique identifier of the text. This can be an ISBN number
# or the project homepage.
#epub_identifier = ''

# A unique identification for the text.
#epub_uid = ''

# A tuple containing the cover image and cover page html template filenames.
#epub_cover = ()

# HTML files that should be inserted before the pages created by sphinx.
# The format is a list of tuples containing the path and title.
#epub_pre_files = []

# HTML files shat should be inserted after the pages created by sphinx.
# The format is a list of tuples containing the path and title.
#epub_post_files = []

# A list of files that should not be packed into the epub file.
#epub_exclude_files = []

# The depth of the table of contents in toc.ncx.
#epub_tocdepth = 3

# Allow duplicate toc entries.
#epub_tocdup = True

########NEW FILE########
__FILENAME__ = test_notifier
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Julien Danjou <julien@danjou.info>
#         Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
"""Tests for ceilometer.compute.nova_notifier
"""

import contextlib
import datetime

import mock

from stevedore import extension

## NOTE(dhellmann): These imports are not in the generally approved
## alphabetical order, but they are in the order that actually
## works. Please don't change them.
from nova.tests import fake_network
from nova.compute import vm_states
try:
    from nova.compute import flavors
except ImportError:
    from nova.compute import instance_types as flavors

from nova.objects import instance as nova_instance
from nova import config
from nova import context
from nova import db
from nova.openstack.common import importutils
from nova.openstack.common import log as logging

# This option is used in the nova_notifier module, so make
# sure it is defined.
config.cfg.CONF.import_opt('compute_manager', 'nova.service')

# HACK(jd) Import this first because of the second HACK below, and because
# of Nova not having these module yet as of this writing
from ceilometer.openstack.common import test
from ceilometer.openstack.common.fixture import config
from ceilometer.openstack.common.fixture import moxstubout

# HACK(dhellmann): Import this before any other ceilometer code
# because the notifier module messes with the import path to force
# nova's version of oslo to be used instead of ceilometer's.
from ceilometer.compute import nova_notifier

from ceilometer import sample
from ceilometer.compute.pollsters import util

LOG = logging.getLogger(__name__)
nova_CONF = config.cfg.CONF


class TestNovaNotifier(test.BaseTestCase):

    class Pollster(object):
        instances = []
        test_data_1 = sample.Sample(
            name='test1',
            type=sample.TYPE_CUMULATIVE,
            unit='units-go-here',
            volume=1,
            user_id='test',
            project_id='test',
            resource_id='test_run_tasks',
            timestamp=datetime.datetime.utcnow().isoformat(),
            resource_metadata={'name': 'Pollster',
                               },
        )

        def get_samples(self, manager, cache, instance):
            self.instances.append((manager, instance))
            test_data_2 = util.make_sample_from_instance(
                instance,
                name='test2',
                type=sample.TYPE_CUMULATIVE,
                unit='units-go-here',
                volume=1,
            )
            return [self.test_data_1, test_data_2]

    @mock.patch('ceilometer.pipeline.setup_pipeline', mock.MagicMock())
    def setUp(self):
        super(TestNovaNotifier, self).setUp()
        nova_CONF.compute_driver = 'nova.virt.fake.FakeDriver'
        nova_CONF.notification_driver = [
            nova_notifier.__name__,
            'nova.openstack.common.notifier.rpc_notifier',
        ]
        nova_CONF.rpc_backend = 'nova.openstack.common.rpc.impl_fake'
        nova_CONF.vnc_enabled = False
        nova_CONF.spice.enabled = False
        self.compute = importutils.import_object(nova_CONF.compute_manager)
        self.context = context.get_admin_context()
        self.stubs = self.useFixture(moxstubout.MoxStubout()).stubs
        fake_network.set_stub_network_methods(self.stubs)

        self.instance_data = {"display_name": "instance-1",
                              "id": 1,
                              "image_ref": "FAKE",
                              "user_id": "FAKE",
                              "project_id": "FAKE",
                              "display_name": "FAKE NAME",
                              "hostname": "abcdef",
                              "reservation_id": "FAKE RID",
                              "instance_type_id": 1,
                              "architecture": "x86",
                              "memory_mb": "1024",
                              "root_gb": "20",
                              "ephemeral_gb": "0",
                              "vcpus": 1,
                              'node': "fakenode",
                              "host": "fakehost",
                              "availability_zone":
                              "1e3ce043029547f1a61c1996d1a531a4",
                              "created_at": '2012-05-08 20:23:41',
                              "launched_at": '2012-05-08 20:25:45',
                              "terminated_at": '2012-05-09 20:23:41',
                              "os_type": "linux",
                              "kernel_id": "kernelid",
                              "ramdisk_id": "ramdiskid",
                              "vm_state": vm_states.ACTIVE,
                              "task_state": None,
                              "access_ip_v4": "192.168.5.4",
                              "access_ip_v6": "2001:DB8::0",
                              "metadata": {},
                              "uuid": "144e08f4-00cb-11e2-888e-5453ed1bbb5f",
                              "system_metadata": {},
                              "user_data": None,
                              "cleaned": 0,
                              "deleted": None,
                              "vm_mode": None,
                              "deleted_at": None,
                              "disable_terminate": False,
                              "root_device_name": None,
                              "default_swap_device": None,
                              "launched_on": None,
                              "display_description": None,
                              "key_data": None,
                              "key_name": None,
                              "config_drive": None,
                              "power_state": None,
                              "default_ephemeral_device": None,
                              "progress": 0,
                              "scheduled_at": None,
                              "updated_at": None,
                              "shutdown_terminate": False,
                              "cell_name": 'cell',
                              "locked": False,
                              "locked_by": None,
                              "launch_index": 0,
                              "auto_disk_config": False,
                              "ephemeral_key_uuid": None
                              }

        self.instance = nova_instance.Instance()
        self.instance = nova_instance.Instance._from_db_object(
                context, self.instance, self.instance_data,
                expected_attrs=['metadata', 'system_metadata'])

        self.stubs.Set(db, 'instance_info_cache_delete', self.do_nothing)
        self.stubs.Set(db, 'instance_destroy', self.do_nothing)
        self.stubs.Set(db, 'instance_system_metadata_get',
                       self.fake_db_instance_system_metadata_get)
        self.stubs.Set(db, 'block_device_mapping_get_all_by_instance',
                       lambda context, instance: {})
        self.stubs.Set(db, 'instance_update_and_get_original',
                       lambda *args, **kwargs: (self.instance, self.instance))
        self.stubs.Set(flavors, 'extract_flavor', self.fake_extract_flavor)

        # Set up to capture the notification messages generated by the
        # plugin and to invoke our notifier plugin.
        self.notifications = []

        ext_mgr = extension.ExtensionManager.make_test_instance(
            extensions=[
                extension.Extension('test',
                                    None,
                                    None,
                                    self.Pollster(),
                                ),
            ],
        )
        self.ext_mgr = ext_mgr
        self.gatherer = nova_notifier.DeletedInstanceStatsGatherer(ext_mgr)
        # Initialize the global _gatherer in nova_notifier to use the
        # gatherer in this test instead of the gatherer in nova_notifier.
        nova_notifier.initialize_gatherer(self.gatherer)

        # Terminate the instance to trigger the notification.
        with contextlib.nested(
            # Under Grizzly, Nova has moved to no-db access on the
            # compute node. The compute manager uses RPC to talk to
            # the conductor. We need to disable communication between
            # the nova manager and the remote system since we can't
            # expect the message bus to be available, or the remote
            # controller to be there if the message bus is online.
            mock.patch.object(self.compute, 'conductor_api'),
            # The code that looks up the instance uses a global
            # reference to the API, so we also have to patch that to
            # return our fake data.
            mock.patch.object(nova_notifier.conductor_api,
                              'instance_get_by_uuid',
                              self.fake_instance_ref_get),
            mock.patch('nova.openstack.common.notifier.rpc_notifier.notify',
                       self.notify)
        ):
            with mock.patch.object(self.compute.conductor_api,
                                   'instance_destroy',
                                   return_value=self.instance):
                self.compute.terminate_instance(self.context,
                                                instance=self.instance,
                                                bdms=[],
                                                reservations=[])

    def tearDown(self):
        self.Pollster.instances = []
        super(TestNovaNotifier, self).tearDown()
        nova_notifier._gatherer = None

    # The instance returned by conductor API is a dictionary actually,
    # and it will be transformed to an nova_notifier.Instance object
    # that looks like what the novaclient gives them.
    def fake_instance_ref_get(self, context, id_):
        return self.instance_data

    @staticmethod
    def fake_extract_flavor(instance_ref):
        return {'ephemeral_gb': 0,
                'flavorid': '1',
                'id': 2,
                'memory_mb': 512,
                'name': 'm1.tiny',
                'root_gb': 1,
                'rxtx_factor': 1.0,
                'swap': 0,
                'vcpu_weight': None,
                'vcpus': 1}

    @staticmethod
    def do_nothing(*args, **kwargs):
        pass

    @staticmethod
    def fake_db_instance_system_metadata_get(context, uuid):
        return dict(meta_a=123, meta_b="foobar")

    def notify(self, context, message):
        self.notifications.append(message)

    def test_pollster_called(self):
        self.assertEqual(len(self.Pollster.instances), 1)

    def test_correct_instance(self):
        for i, (gatherer, inst) in enumerate(self.Pollster.instances):
            self.assertEqual((i, inst.uuid), (i, self.instance.uuid))

    def test_correct_gatherer(self):
        for i, (gatherer, inst) in enumerate(self.Pollster.instances):
            self.assertEqual((i, gatherer), (i, self.gatherer))

    def test_instance_flavor(self):
        inst = nova_notifier.Instance(context, self.instance)
        self.assertEqual(inst.flavor['name'], 'm1.tiny')
        self.assertEqual(inst.flavor['flavor_id'], '1')

    def test_samples(self):
        # Ensure that the outgoing notification looks like what we expect
        for message in self.notifications:
            event = message['event_type']
            if event != 'compute.instance.delete.samples':
                continue
            payload = message['payload']
            samples = payload['samples']

            # Because the playload's samples doesn't include instance
            # metadata, we can't check the metadata field directly.
            # But if we make a mistake in the instance attributes, such
            # as missing instance.name or instance.flavor['name'], it
            # will raise AttributeError, which results the number of
            # the samples doesn't equal to 2.
            self.assertEqual(len(samples), 2)
            s1 = payload['samples'][0]
            self.assertEqual(s1, {'name': 'test1',
                                  'type': sample.TYPE_CUMULATIVE,
                                  'unit': 'units-go-here',
                                  'volume': 1,
                                  })
            s2 = payload['samples'][1]
            self.assertEqual(s2, {'name': 'test2',
                                  'type': sample.TYPE_CUMULATIVE,
                                  'unit': 'units-go-here',
                                  'volume': 1,
                                  })

            break
        else:
            assert False, 'Did not find expected event'

########NEW FILE########
__FILENAME__ = make_test_data
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network, LLC (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

"""Command line tool for creating test data for ceilometer.
"""
from __future__ import print_function

import argparse
import datetime
import logging
import sys

from oslo.config import cfg

from ceilometer.publisher import utils
from ceilometer import sample
from ceilometer import storage
from ceilometer.openstack.common import timeutils


def main():
    cfg.CONF([], project='ceilometer')

    parser = argparse.ArgumentParser(
        description='generate metering data',
    )
    parser.add_argument(
        '--interval',
        default=10,
        type=int,
        help='The period between events, in minutes.',
    )
    parser.add_argument(
        '--start',
        default=31,
        help='The number of days in the past to start timestamps.',
    )
    parser.add_argument(
        '--end',
        default=2,
        help='The number of days into the future to continue timestamps.',
    )
    parser.add_argument(
        '--type',
        choices=('gauge', 'cumulative'),
        default='gauge',
        help='Counter type.',
    )
    parser.add_argument(
        '--unit',
        default=None,
        help='Counter unit.',
    )
    parser.add_argument(
        '--project',
        help='Project id of owner.',
    )
    parser.add_argument(
        '--user',
        help='User id of owner.',
    )
    parser.add_argument(
        'resource',
        help='The resource id for the meter data.',
    )
    parser.add_argument(
        'counter',
        help='The counter name for the meter data.',
    )
    parser.add_argument(
        'volume',
        help='The amount to attach to the meter.',
        type=int,
        default=1,
    )
    args = parser.parse_args()

    # Set up logging to use the console
    console = logging.StreamHandler(sys.stderr)
    console.setLevel(logging.DEBUG)
    formatter = logging.Formatter('%(message)s')
    console.setFormatter(formatter)
    root_logger = logging.getLogger('')
    root_logger.addHandler(console)
    root_logger.setLevel(logging.DEBUG)

    # Connect to the metering database
    conn = storage.get_connection(cfg.CONF)

    # Find the user and/or project for a real resource
    if not (args.user or args.project):
        for r in conn.get_resources():
            if r['resource_id'] == args.resource:
                args.user = r['user_id']
                args.project = r['project_id']
                break

    # Compute start and end timestamps for the
    # new data.
    timestamp = timeutils.parse_isotime(args.start)
    end = timeutils.parse_isotime(args.end)
    increment = datetime.timedelta(minutes=args.interval)

    # Generate events
    n = 0
    while timestamp <= end:
        c = sample.Sample(name=args.counter,
                            type=args.type,
                            unit=args.unit,
                            volume=args.volume,
                            user_id=args.user,
                            project_id=args.project,
                            resource_id=args.resource,
                            timestamp=timestamp,
                            resource_metadata={},
                            source='artificial',
                            )
        data = utils.meter_message_from_counter(
            c,
            cfg.CONF.publisher.metering_secret)
        conn.record_metering_data(data)
        n += 1
        timestamp = timestamp + increment

    print('Added %d new events' % n)

    return 0

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = release-bugs
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 Graham Binns for Canonical
#
# Author: Graham Binns <graham.binns@gmail.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

"""Command line tool for releasing Ceilometer bugs."""

from __future__ import print_function

import argparse
import sys

try:
    from launchpadlib.launchpad import Launchpad
    from launchpadlib.uris import LPNET_SERVICE_ROOT as SERVICE_ROOT
except ImportError:
    print("Can't import launchpadlib.")
    sys.exit(1)


PROJECT_NAME = "ceilometer"
MESSAGE_TEMPLATE = "Released with milestone {milestone_title}."
PRE_RELEASE_STATUS = "Fix Released"
RELEASE_PROMPT = (
    "Found {bug_count} '{pre_release_status}' bugs for milestone "
    "{milestone_title}. Mark them 'Fix Released'? [y/n]: "
)


def main():
    parser = argparse.ArgumentParser(
        description="Release Ceilometer bugs for a milestone.")
    parser.add_argument(
        '--milestone', help="The name of the milestone to release for.",
        required=True)
    args = parser.parse_args()

    lp = Launchpad.login_with(
        "ceilometer-bug-release-script", SERVICE_ROOT)
    the_project = lp.projects[PROJECT_NAME]
    milestone = lp.load(
        "%s/+milestone/%s" % (the_project.self_link, args.milestone))
    bugs_for_milestone = the_project.searchTasks(
        status=PRE_RELEASE_STATUS, milestone=milestone)
    bug_count = len(bugs_for_milestone)
    if bug_count == 0:
        print("No bugs to release for milestone %s" % milestone.name)
        sys.exit(0)
    mark_released = raw_input(RELEASE_PROMPT.format(
        bug_count=bug_count,
        pre_release_status=PRE_RELEASE_STATUS,
        milestone_title=milestone.name))
    if mark_released.lower() != "y":
        print("Not releasing bugs.")
        sys.exit(0)
    for bug_task in bugs_for_milestone:
        # We re-load the bugtask to avoid having bug 369293 bite us.
        bug_task = lp.load(bug_task.self_link)
        sys.stdout.write("Updating %s..." % bug_task.bug.id)
        bug_task.status = "Fix Released"
        bug_task.lp_save()
        bug_task.bug.newMessage(
            MESSAGE_TEMPLATE.format(milestone_title=milestone.title))
        sys.stdout.write("DONE\n")


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = show_data
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
#
# Copyright © 2012 New Dream Network (DreamHost)
#
# Author: Doug Hellmann <doug.hellmann@dreamhost.com>
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
from __future__ import print_function

import sys

from oslo.config import cfg

from ceilometer import storage


def show_users(db, args):
    for u in sorted(db.get_users()):
        print(u)


def show_resources(db, args):
    if args:
        users = args
    else:
        users = sorted(db.get_users())
    for u in users:
        print(u)
        for resource in db.get_resources(user=u):
            print('  %(resource_id)s %(timestamp)s' % resource)
            for k, v in sorted(resource['metadata'].iteritems()):
                print('      %-10s : %s' % (k, v))
            for meter in resource['meter']:
                totals = db.get_statistics(storage.SampleFilter(
                    user=u,
                    meter=meter['counter_name'],
                    resource=resource['resource_id'],
                ))
                # FIXME(dhellmann): Need a way to tell whether to use
                # max() or sum() by meter name without hard-coding.
                if meter['counter_name'] in ['cpu', 'disk']:
                    value = totals[0]['max']
                else:
                    value = totals[0]['sum']
                print('    %s (%s): %s' % \
                     (meter['counter_name'], meter['counter_type'],
                      value))


def show_total_resources(db, args):
    if args:
        users = args
    else:
        users = sorted(db.get_users())
    for u in users:
        print(u)
        for meter in ['disk', 'cpu', 'instance']:
            stats = db.get_statistics(storage.SampleFilter(
                user=u,
                meter=meter,
            ))
            if meter in ['cpu', 'disk']:
                total = stats['max']
            else:
                total = stats['sum']
            print('  ', meter, total)


def show_raw(db, args):
    fmt = '    %(timestamp)s %(counter_name)10s %(counter_volume)s'
    for u in sorted(db.get_users()):
        print(u)
        for resource in db.get_resources(user=u):
            print('  ', resource['resource_id'])
            for sample in db.get_samples(storage.SampleFilter(
                    user=u,
                    resource=resource['resource_id'],
            )):
                print(fmt % sample)


def show_help(db, args):
    print('COMMANDS:')
    for name in sorted(COMMANDS.keys()):
        print(name)


def show_projects(db, args):
    for u in sorted(db.get_projects()):
        print(u)


COMMANDS = {
    'users': show_users,
    'projects': show_projects,
    'help': show_help,
    'resources': show_resources,
    'total_resources': show_total_resources,
    'raw': show_raw,
}


def main(argv):
    extra_args = cfg.CONF(
        sys.argv[1:],
        # NOTE(dhellmann): Read the configuration file(s) for the
        #ceilometer collector by default.
        default_config_files=['/etc/ceilometer/ceilometer.conf'],
    )
    db = storage.get_connection(cfg.CONF)
    command = extra_args[0] if extra_args else 'help'
    COMMANDS[command](db, extra_args[1:])


if __name__ == '__main__':
    main(sys.argv)

########NEW FILE########
