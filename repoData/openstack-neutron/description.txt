README

This docbkx-example folder is provided for those who want to use the maven mojo supplied with the project to build their own documents to PDF and HTML (webhelp) format. It's intended to be a template and model. 

You can edit the src/docbkx/example.xml file using vi, emacs, or another DocBook editor. At Rackspace we use Oxygen. Both Oxygen and XML Mind offer free licenses to those working on open source project documentation.

To build the output, install Apache Maven (https://maven.apache.org/) and then run:

mvn clean generate-sources 

in the directory containing the pom.xml file. 

Feel free to ask questions of the openstack-docs team at https://launchpad.net/~openstack-doc. 


Certificates in this folder will be used to
verify signatures for any controllers the plugin
connects to.

Certificates in this folder must match the name
of the controller they should be used to authenticate
with a .pem extension.

For example, the certificate for the controller
"192.168.0.1" should be named "192.168.0.1.pem".

This directory contains the migration scripts for the Neutron project.  Please
see the README in neutron/db/migration on how to use and generate new
migrations.



# Copyright 2012 New Dream Network, LLC (DreamHost)
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
# @author Mark McClain (DreamHost)

The migrations in the alembic/versions contain the changes needed to migrate
from older Neutron releases to newer versions. A migration occurs by executing
a script that details the changes needed to upgrade/downgrade the database. The
migration scripts are ordered so that multiple scripts can run sequentially to
update the database. The scripts are executed by Neutron's migration wrapper
which uses the Alembic library to manage the migration.  Neutron supports
migration from Folsom or later.


If you are a deployer or developer and want to migrate from Folsom to Grizzly
or later you must first add version tracking to the database:

$ neutron-db-manage --config-file /path/to/neutron.conf \
 --config-file /path/to/plugin/config.ini stamp folsom

You can then upgrade to the latest database version via:
$ neutron-db-manage --config-file /path/to/neutron.conf \
 --config-file /path/to/plugin/config.ini upgrade head

To check the current database version:
$ neutron-db-manage --config-file /path/to/neutron.conf \
 --config-file /path/to/plugin/config.ini current

To create a script to run the migration offline:
$ neutron-db-manage --config-file /path/to/neutron.conf \
 --config-file /path/to/plugin/config.ini upgrade head --sql

To run the offline migration between specific migration versions:
$ neutron-db-manage --config-file /path/to/neutron.conf \
--config-file /path/to/plugin/config.ini upgrade \
<start version>:<end version> --sql

Upgrade the database incrementally:
$ neutron-db-manage --config-file /path/to/neutron.conf \
--config-file /path/to/plugin/config.ini upgrade --delta <# of revs>

Downgrade the database by a certain number of revisions:
$ neutron-db-manage --config-file /path/to/neutron.conf \
--config-file /path/to/plugin/config.ini downgrade --delta <# of revs>


DEVELOPERS:
A database migration script is required when you submit a change to Neutron
that alters the database model definition.  The migration script is a special
python file that includes code to update/downgrade the database to match the
changes in the model definition. Alembic will execute these scripts in order to
provide a linear migration path between revision. The neutron-db-manage command
can be used to generate migration template for you to complete.  The operations
in the template are those supported by the Alembic migration library.

$ neutron-db-manage --config-file /path/to/neutron.conf \
--config-file /path/to/plugin/config.ini revision \
-m "description of revision" \
--autogenerate

This generates a prepopulated template with the changes needed to match the
database state with the models.  You should inspect the autogenerated template
to ensure that the proper models have been altered.

In rare circumstances, you may want to start with an empty migration template
and manually author the changes necessary for an upgrade/downgrade.  You can
create a blank file via:

$ neutron-db-manage --config-file /path/to/neutron.conf \
--config-file /path/to/plugin/config.ini revision \
-m "description of revision"

The migration timeline should remain linear so that there is a clear path when
upgrading/downgrading.  To verify that the timeline does branch, you can run
this command:
$ neutron-db-manage --config-file /path/to/neutron.conf \
--config-file /path/to/plugin/config.ini check_migration

If the migration path does branch, you can find the branch point via:
$ neutron-db-manage --config-file /path/to/neutron.conf \
--config-file /path/to/plugin/config.ini history

Debug Helper Script for Neutron

- Configure
export NEUTRON_TEST_CONFIG_FILE=/etc/neutron/debug.ini
or
export NEUTRON_TEST_CONFIG_FILE=/etc/neutron/l3_agent.ini

you can also specify config file by --config-file option

- Usage
neutron-debug commands

probe-create <net-id>
  Create probe port - create port and interface, then plug it in.
  This commands returns a port id of a probe port. A probe port is a port which is used to test.
  The port id is probe id.
  We can have multiple probe probes in a network, in order to check connectivity between ports.

  neutron-debug probe-exec probe_id_1 'nc -l 192.168.100.3 22'
  neutron-debug probe-exec probe_id_2 'nc -vz 192.168.100.4 22'

  Note: You should use a user and a tenant who has permission to
   modify network and subnet if you want to probe. For example, you need to be admin user if you
   want to probe external network.

probe-delete <port-id>  Delete probe - delete port then uplug
probe-exec <port-id> 'command'    Exec commands on the namespace of the probe
`probe-exec <port-id>` 'interactive command' Exec interactive command (eg, ssh)

probe-list     List probes
probe-clear    Clear All probes

ping-all --id <network_id> --timeout 1 (optional)
         ping-all is all-in-one command to ping all fixed ip's in all network or a specified network.
         In the command probe is automatically created if needed.

neutron-debug extends the shell of neutronclient,  so you can use all the commands of neutron


# Neuron REST Proxy Plug-in for Big Switch and FloodLight Controllers

This module provides a generic neutron plugin 'NeutronRestProxy' that
translates neutron function calls to authenticated REST requests (JSON supported)
to a set of redundant external network controllers.

It also keeps a local persistent store of neutron state that has been
setup using that API.

Currently the FloodLight Openflow Controller or the Big Switch Networks Controller
can be configured as external network controllers for this plugin.

For more details on this plugin, please refer to the following link:
http://www.openflowhub.org/display/floodlightcontroller/Neutron+REST+Proxy+Plugin

Brocade Openstack Neutron Plugin
================================

* up-to-date version of these instructions are located at:
  http://wiki.openstack.org/brocade-neutron-plugin

* N.B.: Please see Prerequisites section  regarding ncclient (netconf client library)

* Supports VCS (Virtual Cluster of Switches)


Openstack Brocade Neutron Plugin implements the Neutron v2.0 API.

This plugin is meant to orchestrate Brocade VCS switches running NOS, examples of these are:

   1. VDX 67xx series of switches
   2. VDX 87xx series of switches

Brocade Neutron plugin implements the Neutron v2.0 API. It uses NETCONF at the backend
to configure the Brocade switch.

             +------------+        +------------+          +-------------+
             |            |        |            |          |             |
             |            |        |            |          |   Brocade   |
             | Openstack  |  v2.0  |  Brocade   |  NETCONF |  VCS Switch |
             | Neutron    +--------+  Neutron   +----------+             |
             |            |        |  Plugin    |          |  VDX 67xx   |
             |            |        |            |          |  VDX 87xx   |
             |            |        |            |          |             |
             |            |        |            |          |             |
             +------------+        +------------+          +-------------+


Directory Structure
===================

Normally you will have your Openstack directory structure as follows:

         /opt/stack/nova/
         /opt/stack/horizon/
         ...
         /opt/stack/neutron/neutron/plugins/

Within this structure, Brocade plugin resides at:

         /opt/stack/neutron/neutron/plugins/brocade


Prerequsites
============

This plugin requires installation of the python netconf client (ncclient) library:

ncclient v0.3.1 - Python library for NETCONF clients available at http://github.com/brocade/ncclient

  % git clone https://www.github.com/brocade/ncclient
  % cd ncclient; sudo python ./setup.py install


Configuration
=============

1. Specify to Neutron that you will be using the Brocade Plugin - this is done
by setting the parameter core_plugin in Neutron:

        core_plugin = neutron.plugins.brocade.NeutronPlugin.BrocadePluginV2

2. Physical switch configuration parameters and Brocade specific database configuration is specified in
the configuration file specified in the brocade.ini files:

        % cat /etc/neutron/plugins/brocade/brocade.ini
        [SWITCH]
        username = admin
        password = password
        address  = <switch mgmt ip address>
        ostype   = NOS

        [database]
        connection = mysql://root:pass@localhost/brocade_neutron?charset=utf8

        (please see list of more configuration parameters in the brocade.ini file)

Running Setup.py
================

Running setup.py with appropriate permissions will copy the default configuration
file to /etc/neutron/plugins/brocade/brocade.ini. This file MUST be edited to
suit your setup/environment.

      % cd /opt/stack/neutron/neutron/plugins/brocade
      % python setup.py


Devstack
========

Please see special notes for devstack at:
http://wiki.openstack.org/brocade-neutron-plugin

In order to use Brocade Neutron Plugin, add the following lines in localrc, if localrc file doe
 not exist create one:

ENABLED_SERVICES=g-api,g-reg,key,n-api,n-crt,n-obj,n-cpu,n-net,n-cond,cinder,c-sch,c-api,c-vol,n-sch,n-novnc,n-xvnc,n-cauth,horizon,rabbit,neutron,q-svc,q-agt
Q_PLUGIN=brocade

As part of running devstack/stack.sh, the configuration files is copied as:

  % cp /opt/stack/neutron/etc/neutron/plugins/brocade/brocade.ini /etc/neutron/plugins/brocade/brocade.ini

(hence it is important to make any changes to the configuration in:
/opt/stack/neutron/etc/neutron/plugins/brocade/brocade.ini)


Start the neutron-server with IP address of switch configured in brocade.ini:
(for configuration instruction please see README.md in the above directory)

nostest.py:
This tests two things:
     1. Creates port-profile on the physical switch when a neutron 'network' is created
     2. Associates the MAC address with the created port-profile

noscli.py:
    CLI interface to create/delete/associate MAC/dissociate MAC
    Commands:
        % noscli.py create <network>
          (after running check that PP is created on the switch)

        % noscli.py delete <network>
          (after running check that PP is deleted from the switch)

        % noscli.py associate <network> <mac>
          (after running check that MAC is associated with PP)

        % noscli.py dissociate <network> <mac>
          (after running check that MAC is dissociated from the PP)



Cisco Neutron Virtual Network Plugin

This plugin implements Neutron v2 APIs and helps configure
topologies consisting of virtual and physical switches.

For more details on use please refer to:
http://wiki.openstack.org/cisco-neutron

Embrane Neutron Plugin

This plugin interfaces OpenStack Neutron with Embrane's heleos platform, which
provides layer 3-7 network services for cloud environments.

L2 connectivity is leveraged by one of the supported existing plugins.

For more details on use, configuration and implementation please refer to:
http://wiki.openstack.org/wiki/Neutron/EmbraneNeutronPlugin
IBM SDN-VE Neutron Plugin

This plugin implements Neutron v2 APIs.

For more details on how to use it please refer to the following page:
http://wiki.openstack.org/wiki/IBM-Neutron

# -- Background

The Neutron Linux Bridge plugin is a plugin that allows you to manage
connectivity between VMs on hosts that are capable of running a Linux Bridge.

The Neutron Linux Bridge plugin consists of three components:

1) The plugin itself: The plugin uses a database backend (mysql for
   now) to store configuration and mappings that are used by the
   agent.  The mysql server runs on a central server (often the same
   host as nova itself).

2) The neutron service host which will be running neutron.  This can
   be run on the server running nova.

3) An agent which runs on the host and communicates with the host operating
   system. The agent gathers the configuration and mappings from
   the mysql database running on the neutron host.

The sections below describe how to configure and run the neutron
service with the Linux Bridge plugin.

# -- Python library dependencies

   Make sure you have the following package(s) installedi on neutron server
   host as well as any hosts which run the agent:
   python-configobj
   bridge-utils
   python-mysqldb
   sqlite3

# -- Nova configuration (controller node)

1) Ensure that the neutron network manager is configured in the
   nova.conf on the node that will be running nova-network.

network_manager=nova.network.neutron.manager.NeutronManager

# -- Nova configuration (compute node(s))

1) Configure the vif driver, and libvirt/vif type

connection_type=libvirt
libvirt_type=qemu
libvirt_vif_type=ethernet
libvirt_vif_driver=nova.virt.libvirt.vif.NeutronLinuxBridgeVIFDriver
linuxnet_interface_driver=nova.network.linux_net.NeutronLinuxBridgeInterfaceDriver

2) If you want a DHCP server to be run for the VMs to acquire IPs,
   add the following flag to your nova.conf file:

neutron_use_dhcp=true

(Note: For more details on how to work with Neutron using Nova, i.e. how to create networks and such,
 please refer to the top level Neutron README which points to the relevant documentation.)

# -- Neutron configuration

Make the Linux Bridge plugin the current neutron plugin

- edit neutron.conf and change the core_plugin

core_plugin = neutron.plugins.linuxbridge.lb_neutron_plugin.LinuxBridgePluginV2

# -- Database config.

(Note: The plugin ships with a default SQLite in-memory database configuration,
 and can be used to run tests without performing the suggested DB config below.)

The Linux Bridge neutron plugin requires access to a mysql database in order
to store configuration and mappings that will be used by the agent.  Here is
how to set up the database on the host that you will be running the neutron
service on.

MySQL should be installed on the host, and all plugins and clients
must be configured with access to the database.

To prep mysql, run:

$ mysql -u root -p -e "create database neutron_linux_bridge"

# log in to mysql service
$ mysql -u root -p
# The Linux Bridge Neutron agent running on each compute node must be able to
# make a mysql connection back to the main database server.
mysql> GRANT USAGE ON *.* to root@'yourremotehost' IDENTIFIED BY 'newpassword';
# force update of authorization changes
mysql> FLUSH PRIVILEGES;

(Note: If the remote connection fails to MySQL, you might need to add the IP address,
 and/or fully-qualified hostname, and/or unqualified hostname in the above GRANT sql
 command. Also, you might need to specify "ALL" instead of "USAGE".)

# -- Plugin configuration

- Edit the configuration file:
  etc/neutron/plugins/linuxbridge/linuxbridge_conf.ini
  Make sure it matches your mysql configuration.  This file must be updated
  with the addresses and credentials to access the database.

  Note: debug and logging information should be updated in etc/neutron.conf

  Note: When running the tests, set the connection type to sqlite, and when
  actually running the server set it to mysql. At any given time, only one
  of these should be active in the conf file (you can comment out the other).

- On the neutron server, network_vlan_ranges must be configured in
  linuxbridge_conf.ini to specify the names of the physical networks
  managed by the linuxbridge plugin, along with the ranges of VLAN IDs
  available on each physical network for allocation to virtual
  networks. An entry of the form
  "<physical_network>:<vlan_min>:<vlan_max>" specifies a VLAN range on
  the named physical network. An entry of the form
  "<physical_network>" specifies a named network without making a
  range of VLANs available for allocation. Networks specified using
  either form are available for adminstrators to create provider flat
  networks and provider VLANs. Multiple VLAN ranges can be specified
  for the same physical network.

  The following example linuxbridge_conf.ini entry shows three
  physical networks that can be used to create provider networks, with
  ranges of VLANs available for allocation on two of them:

  [VLANS]
  network_vlan_ranges = physnet1:1000:2999,physnet1:3000:3999,physnet2,physnet3:1:4094


# -- Agent configuration

- Edit the configuration file:
  etc/neutron/plugins/linuxbridge/linuxbridge_conf.ini

- Copy neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py
  and etc/neutron/plugins/linuxbridge/linuxbridge_conf.ini
  to the compute node.

- Copy the neutron.conf file to the compute node

  Note: debug and logging information should be updated in etc/neutron.conf

- On each compute node, the network_interface_mappings must be
  configured in linuxbridge_conf.ini to map each physical network name
  to the physical interface connecting the node to that physical
  network. Entries are of the form
  "<physical_network>:<physical_interface>". For example, one compute
  node may use the following physical_inteface_mappings entries:

  [LINUX_BRIDGE]
  physical_interface_mappings = physnet1:eth1,physnet2:eth2,physnet3:eth3

  while another might use:

  [LINUX_BRIDGE]
  physical_interface_mappings = physnet1:em3,physnet2:em2,physnet3:em1


$ Run the following:
  python linuxbridge_neutron_agent.py --config-file neutron.conf
                                      --config-file linuxbridge_conf.ini

  Note that the the user running the agent must have sudo priviliges
  to run various networking commands. Also, the agent can be
  configured to use neutron-rootwrap, limiting what commands it can
  run via sudo. See http://wiki.openstack.org/Packager/Rootwrap for
  details on rootwrap.

  As an alternative to coping the agent python file, if neutron is
  installed on the compute node, the agent can be run as
  bin/neutron-linuxbridge-agent.

# -- Background

This plugin supports multiple plugin at same time. This plugin is for L3 connectivility
between networks which are realized by different plugins.This plugin adds new attributes 'flavor:network' and 'flavor:router".
flavor:network corresponds to specific l2 plugin ( flavor-plugin mapping could be configurable by plugin_list config.
flavor:router corresponds to specific l3 plugin ( flavor-plugin mapping could be configurable by l3_plugin_list config. Note that Metaplugin can provide l3 functionaliteis for l2 plugin which didn't support l3 extension yet.
This plugin also support extensions. We can map extension to plugin by using extension_map config.

[database]
# This line MUST be changed to actually run the plugin.
# Example:
# connection = mysql://root:nova@127.0.0.1:3306/ovs_neutron
# Replace 127.0.0.1 above with the IP address of the database used by the
# main neutron server. (Leave it as is if the database runs on this host.)
connection = mysql://root:password@localhost/neutron_metaplugin?charset=utf8

# Database reconnection retry times - in event connectivity is lost
# set to -1 implgies an infinite retry count
# max_retries = 10
# Database reconnection interval in seconds - in event connectivity is lost
retry_interval = 2

[meta]
## This is list of flavor:neutron_plugins
# extension method is used in the order of this list
plugin_list= 'openvswitch:neutron.plugins.openvswitch.ovs_neutron_plugin.OVSneutronPluginV2,linuxbridge:neutron.plugins.linuxbridge.lb_neutron_plugin.LinuxBridgePluginV2'
# plugin for l3
l3_plugin_list= 'openvswitch:neutron.plugins.openvswitch.ovs_neutron_plugin.OVSneutronPluginV2,linuxbridge:neutron.plugins.linuxbridge.lb_neutron_plugin.LinuxBridgePluginV2'

# Default value of flavor
default_flavor = 'openvswitch'
# Default value for l3
default_l3_flavor = 'openvswitch'

# supported extensions
supported_extension_aliases = 'providernet'
# specific method map for each flavor to extensions
extension_map = 'get_port_stats:nvp'

# -- BridgeDriver Configration
# In order to use metaplugin, you should use MetaDriver. Following configation is needed.

[DEFAULT]
# Meta Plugin
# Mapping between flavor and driver
meta_flavor_driver_mappings = openvswitch:neutron.agent.linux.interface.OVSInterfaceDriver, linuxbridge:neutron.agent.linux.interface.BridgeInterfaceDriver
# interface driver for MetaPlugin
interface_driver = neutron.agent.linux.interface.MetaInterfaceDriver

[proxy]
auth_url = http://10.0.0.1:35357/v2.0
auth_region = RegionOne
admin_tenant_name = service
admin_user =      neutron
admin_password = password


# -- Agent
Agents for Metaplugin are in neutron/plugins/metaplugin/agent
linuxbridge_neutron_agent and ovs_neutron_agent is available.

# -- Extensions

- flavor
MetaPlugin supports flavor and provider net extension.
Metaplugin select plugin_list using flavor.
One plugin may use multiple flavor value. If the plugin support flavor, it may provide
multiple flavor of network.

- Attribute extension
Each plugin can use attribute extension such as provider_net, if you specify that in supported_extension_aliases.

- providernet
Vlan ID range of each plugin should be different, since Metaplugin dose not manage that.

#- limitations

Basically, All plugin should inherit NeutronDbPluginV2.
Metaplugin assumes all plugin share same Database especially for IPAM part in NeutronV2 API.
You can use another plugin if you use ProxyPluginV2, which proxies request to the another neutron server.

Example flavor configration for ProxyPluginV2

meta_flavor_driver_mappings = "openvswitch:neutron.agent.linux.interface.OVSInterfaceDriver,proxy:neutron.plugins.metaplugin.proxy_neutron_plugin.ProxyPluginV2"

- Limited L3 support
In folsom version, l3 is an extension. There is no way to extend exntension attributes.
so you can set flavor:router value but you can't get flavor:router value in API output.
L3 agent dont's support flavor:router.




Brocade ML2 Mechanism driver from ML2 plugin
============================================

* up-to-date version of these instructions are located at:
  http://50.56.236.34/docs/brocade-ml2-mechanism.txt
* N.B.: Please see Prerequisites section  regarding ncclient (netconf client library)
* Supports VCS (Virtual Cluster of Switches)
* Issues/Questions/Bugs: sharis@brocade.com



   1. VDX 67xx series of switches
   2. VDX 87xx series of switches

ML2 plugin requires mechanism driver to support configuring of hardware switches.
Brocade Mechanism for ML2 uses NETCONF at the backend to configure the Brocade switch.
Currently the mechanism drivers support VLANs only.

             +------------+        +------------+          +-------------+
             |            |        |            |          |             |
   Neutron   |            |        |            |          |   Brocade   |
     v2.0    | Openstack  |        |  Brocade   |  NETCONF |  VCS Switch |
         ----+ Neutron    +--------+  Mechanism +----------+             |
             | ML2        |        |  Driver    |          |  VDX 67xx   |
             | Plugin     |        |            |          |  VDX 87xx   |
             |            |        |            |          |             |
             |            |        |            |          |             |
             +------------+        +------------+          +-------------+


Configuration

In order to use this mechnism the brocade configuration file needs to be edited with the appropriate
configuration information:

        % cat /etc/neutron/plugins/ml2/ml2_conf_brocade.ini
        [switch]
        username = admin
        password = password
        address  = <switch mgmt ip address>
        ostype   = NOS
        physical_networks = phys1

Additionally the brocade mechanism driver needs to be enabled from the ml2 config file:

       % cat /etc/neutron/plugins/ml2/ml2_conf.ini

       [ml2]
       tenant_network_types = vlan
       type_drivers = local,flat,vlan,gre,vxlan
       mechanism_drivers = openvswitch,brocade
       # OR mechanism_drivers = openvswitch,linuxbridge,hyperv,brocade
       ...
       ...
       ...


Required L2 Agent

This mechanism driver works in conjunction with an L2 Agent. The agent should be loaded as well in order for it to configure the virtual network int the host machine. Please see the configuration above. Atleast one of linuxbridge or openvswitch must be specified.

Neutron ML2 Cisco Nexus Mechanism Driver README


Notes:

The initial version of this driver supports only a single physical
network.

For provider networks, extended configuration options are not
currently supported.

This driver's database may have duplicate entries also found in the
core ML2 database. Since the Cisco Nexus DB code is a port from the
plugins/cisco implementation this duplication will remain until the
plugins/cisco code is deprecated.


For more details on using Cisco Nexus switches under ML2 please refer to:
http://wiki.openstack.org/wiki/Neutron/ML2/MechCiscoNexus

Neutron ML2 l2 population Mechanism Drivers

l2 population (l2pop) mechanism drivers implements the ML2 driver to improve
open source plugins overlay implementations (VXLAN with Linux bridge and
GRE/VXLAN with OVS). This mechanism driver is implemented in ML2 to propagate
the forwarding information among agents using a common RPC API.

More informations could be found on the wiki page [1].

VXLAN Linux kernel:
-------------------
The VXLAN Linux kernel module provide all necessary functionalities to populate
the forwarding table and local ARP responder tables. This module appears on
release 3.7 of the vanilla Linux kernel in experimental:
- 3.8: first stable release, no edge replication (multicast necessary),
- 3.9: edge replication only for the broadcasted packets,
- 3.11: edge replication for broadcast, multicast and unknown packets.

Note: Some distributions (like RHEL) have backported this module on precedent
      kernel version.

OpenvSwitch:
------------
The OVS OpenFlow tables provide all of the necessary functionality to populate
the forwarding table and local ARP responder tables.
A wiki page describe how the flow tables did evolve on OVS agents:
- [2] without local ARP responder
- [3] with local ARP responder. /!\ This functionality is only available since
                                    the development branch 2.1. It's possible
                                    to disable (enable by default) it through
                                    the flag 'arp_responder'. /!\


Note: A difference persists between the LB and OVS agents when they are used
      with the l2-pop mechanism driver (and local ARP responder available). The
      LB agent will drop unknown unicast (VXLAN bridge mode), whereas the OVS
      agent will flood it.

[1] https://wiki.openstack.org/wiki/L2population_blueprint
[2] https://wiki.openstack.org/wiki/Ovs-flow-logic#OVS_flows_logic
[3] https://wiki.openstack.org/wiki/Ovs-flow-logic#OVS_flows_logic_with_local_ARP_responder

Arista Neutron ML2 Mechanism Driver

This mechanism driver implements ML2 Driver API and is used to manage the virtual and physical networks using Arista Hardware.

Note: Initial version of this driver support VLANs only.

For more details on use please refer to:
https://wiki.openstack.org/wiki/Arista-neutron-ml2-driver

OpenDaylight ML2 MechanismDriver
================================
OpenDaylight is an Open Source SDN Controller developed by a plethora of
companies and hosted by the Linux Foundation. The OpenDaylight website
contains more information on the capabilities OpenDaylight provides:

    http://www.opendaylight.org

Theory of operation
===================
The OpenStack Neutron integration with OpenDaylight consists of the ML2
MechanismDriver which acts as a REST proxy and passess all Neutron API
calls into OpenDaylight. OpenDaylight contains a NB REST service (called
the NeutronAPIService) which caches data from these proxied API calls and
makes it available to other services inside of OpenDaylight. One current
user of the SB side of the NeutronAPIService is the OVSDB code in
OpenDaylight. OVSDB uses the neutron information to isolate tenant networks
using GRE or VXLAN tunnels.

How to use the OpenDaylight ML2 MechanismDriver
===============================================
To use the ML2 MechanismDriver, you need to ensure you have it configured
as one of the "mechanism_drivers" in ML2:

    mechanism_drivers=opendaylight

The next step is to setup the "[ml2_odl]" section in either the ml2_conf.ini
file or in a separate ml2_conf_odl.ini file. An example is shown below:

    [ml2_odl]
    password = admin
    username = admin
    url = http://192.168.100.1:8080/controller/nb/v2/neutron

When starting OpenDaylight, ensure you have the SimpleForwarding application
disabled or remove the .jar file from the plugins directory. Also ensure you
start OpenDaylight before you start OpenStack Neutron.

There is devstack support for this which will automatically pull down OpenDaylight
and start it as part of devstack as well. The patch for this will likely merge
around the same time as this patch merges.

The Modular Layer 2 (ML2) plugin is a framework allowing OpenStack
Networking to simultaneously utilize the variety of layer 2 networking
technologies found in complex real-world data centers. It supports the
Open vSwitch, Linux bridge, and Hyper-V L2 agents, replacing and
deprecating the monolithic plugins previously associated with those
agents, and can also support hardware devices and SDN controllers. The
ML2 framework is intended to greatly simplify adding support for new
L2 networking technologies, requiring much less initial and ongoing
effort than would be required for an additional monolithic core
plugin. It is also intended to foster innovation through its
organization as optional driver modules.

The ML2 plugin supports all the non-vendor-specific neutron API
extensions, and works with the standard neutron DHCP agent. It
utilizes the service plugin interface to implement the L3 router
abstraction, allowing use of either the standard neutron L3 agent or
alternative L3 solutions. Additional service plugins can also be used
with the ML2 core plugin.

Drivers within ML2 implement separately extensible sets of network
types and of mechanisms for accessing networks of those types. Unlike
with the metaplugin, multiple mechanisms can be used simultaneously to
access different ports of the same virtual network. Mechanisms can
utilize L2 agents via RPC and/or interact with external devices or
controllers. By utilizing the multiprovidernet extension, virtual
networks can be composed of multiple segments of the same or different
types. Type and mechanism drivers are loaded as python entrypoints
using the stevedore library.

Each available network type is managed by an ML2 type driver.  Type
drivers maintain any needed type-specific network state, and perform
provider network validation and tenant network allocation. As of the
havana release, drivers for the local, flat, vlan, gre, and vxlan
network types are included.

Each available networking mechanism is managed by an ML2 mechanism
driver. All registered mechanism drivers are called twice when
networks, subnets, and ports are created, updated, or deleted. They
are first called as part of the DB transaction, where they can
maintain any needed driver-specific state. Once the transaction has
been committed, they are called again, at which point they can
interact with external devices and controllers. Mechanism drivers are
also called as part of the port binding process, to determine whether
the associated mechanism can provide connectivity for the network, and
if so, the network segment and VIF driver to be used. The havana
release includes mechanism drivers for the Open vSwitch, Linux bridge,
and Hyper-V L2 agents, for Arista and Cisco switches, and for the
Tail-f NCS. It also includes an L2 Population mechanism driver that
can help optimize tunneled virtual network traffic.

For additional information regarding the ML2 plugin and its collection
of type and mechanism drivers, see the OpenStack manuals and
http://wiki.openstack.org/wiki/Neutron/ML2.

Mellanox Neutron Plugin

This plugin implements Neutron v2 APIs with support for
Mellanox embedded switch functionality as part of the
VPI (Ethernet/InfiniBand) HCA.

For more details on the plugin, please refer to the following link:
https://wiki.openstack.org/wiki/Mellanox-Quantum

Quantum NEC OpenFlow Plugin


# -- What's this?

https://wiki.openstack.org/wiki/Neutron/NEC_OpenFlow_Plugin


# -- Installation

Use QuickStart Script for this plugin.  This provides you auto installation and
configuration of Nova, Neutron and Trema.
https://github.com/nec-openstack/quantum-openflow-plugin/tree/folsom

This directory includes agent for OpenFlow Agent mechanism driver.

# -- Installation

For how to install/set up ML2 mechanism driver for OpenFlow Agent, please refer to
https://github.com/osrg/ryu/wiki/OpenStack

# -- Ryu General

For general Ryu stuff, please refer to
http://www.osrg.net/ryu/

Ryu is available at github
git://github.com/osrg/ryu.git
https://github.com/osrg/ryu

The mailing is at
ryu-devel@lists.sourceforge.net
https://lists.sourceforge.net/lists/listinfo/ryu-devel

Enjoy!

One Convergence Neutron Plugin to implement the Neutron v2.0 API. The plugin
works with One Convergence NVSD controller to provide network virtualization
functionality.

The plugin is enabled with the following configuration line in neutron.conf:

core_plugin = neutron.plugins.oneconvergence.plugin.OneConvergencePluginV2

The configuration parameters required for the plugin are specified in the file
etc/neutron/plugins/oneconvergence/nvsdplugin.ini. The configuration file contains
description of the different parameters.

To enable One Convergence Neutron Plugin with devstack and configure the required
parameters, use the following lines in localrc:

Q_PLUGIN=oneconvergence

disable_service n-net
enable_service q-agt
enable_service q-dhcp
enable_service q-svc
enable_service q-l3
enable_service q-meta
enable_service neutron

NVSD_IP=
NVSD_PORT=
NVSD_USER=
NVSD_PASSWD=

The NVSD controller configuration should be specified in nvsdplugin.ini before
invoking stack.sh.

This directory contains files that are required for the XenAPI support.
They should be installed in the XenServer / Xen Cloud Platform dom0.

If you install them manually, you will need to ensure that the newly
added files are executable. You can do this by running the following
command (from dom0):

    chmod a+x /etc/xapi.d/plugins/*

Otherwise, you can build an rpm by running the following command:

    ./contrib/build-rpm.sh

and install the rpm by running the following command (from dom0):

    rpm -i openstack-neutron-xen-plugins.rpm

The Open vSwitch (OVS) Neutron plugin is a simple plugin to manage OVS
features using a local agent running on each hypervisor.

For details on how to configure and use the plugin, see:

http://openvswitch.org/openstack/documentation/

PLUMgrid Neutron Plugin for Virtual Network Infrastructure (VNI)

This plugin implements Neutron v2 APIs and helps configure
L2/L3 virtual networks consisting of PLUMgrid Platform.
Implements External Networks and Port Binding Extension

For more details on use please refer to:
http://wiki.openstack.org/PLUMgrid-Neutron

Neutron plugin for Ryu Network Operating System
This directory includes neutron plugin for Ryu Network Operating System.

# -- Installation

For how to install/set up this plugin with Ryu and OpenStack, please refer to
https://github.com/osrg/ryu/wiki/OpenStack

# -- Ryu General

For general Ryu stuff, please refer to
http://www.osrg.net/ryu/

Ryu is available at github
git://github.com/osrg/ryu.git
https://github.com/osrg/ryu

The mailing is at
ryu-devel@lists.sourceforge.net
https://lists.sourceforge.net/lists/listinfo/ryu-devel

Enjoy!

This service plugin implements the L3 routing functionality (resources router
and floatingip) that in earlier releases before Havana was provided by core
plugins (openvswitch, linuxbridge, ... etc).

Core plugins can now choose not to implement L3 routing functionality and
instead delegate that to the L3 routing service plugin.

The required changes to a core plugin are in that case:
- Do not inherit 'l3_db.L3_NAT_db_mixin' (or its descendants like extraroute)
  anymore.
- Remove "router" from 'supported_extension_aliases'.
- Modify any 'self' references to members in L3_NAT_db_mixin to instead use
  'manager.NeutronManager.get_service_plugins().get(constants.L3_ROUTER_NAT)'
  For example,
     self.prevent_l3_port_deletion(...)
  becomes something like
     plugin = manager.NeutronManager.get_service_plugins().get(
         constants.L3_ROUTER_NAT)
     if plugin:
         plugin.prevent_l3_port_deletion(...)

If the core plugin has relied on the L3Agent the following must also be changed:
- Do not inherit 'l3_rpc_base.L3RpcCallbackMixin' in any '*RpcCallbacks' class.
- Do not be a consumer of the topics.L3PLUGIN topic for RPC.

To use the L3 routing service plugin, add
'neutron.services.l3_router.l3_router_plugin.L3RouterPlugin'
to 'service_plugins' in '/etc/neutron/neutron.conf'.
That is,
service_plugins = neutron.services.l3_router.l3_router_plugin.L3RouterPlugin

Embrane LBaaS Driver

This DRIVER interfaces OpenStack Neutron with Embrane's heleos platform,
Load Balancing appliances for cloud environments.

L2 connectivity is leveraged by one of the supported existing plugins.

For more details on use, configuration and implementation please refer to:
https://wiki.openstack.org/wiki/Neutron/LBaaS/EmbraneDriver
ca_certs directory for SSL unit tests
No files will be generated here, but it should exist for the tests

combined certificates directory for SSL unit tests
No files will be created here, but it should exist for the tests

host_certs directory for SSL unit tests
No files will be created here, but it should exist for the tests

# -- Welcome!

  You have come across a cloud computing network fabric controller.  It has
  identified itself as "Neutron."  It aims to tame your (cloud) networking!

# -- External Resources:

 The homepage for Neutron is: http://launchpad.net/neutron .  Use this
 site for asking for help, and filing bugs. Code is available on github at
 <http://github.com/openstack/neutron>.

 The latest and most in-depth documentation on how to use Neutron is
 available at: <http://docs.openstack.org>.  This includes:

 Neutron Administrator Guide
 http://docs.openstack.org/trunk/openstack-network/admin/content/

 Neutron API Reference:
 http://docs.openstack.org/api/openstack-network/2.0/content/

 The start of some developer documentation is available at:
 http://wiki.openstack.org/NeutronDevelopment

 For help using or hacking on Neutron, you can send mail to
 <mailto:openstack-dev@lists.openstack.org>.

