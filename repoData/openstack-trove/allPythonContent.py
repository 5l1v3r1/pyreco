__FILENAME__ = conf
# -*- coding: utf-8 -*-
#

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

# -- General configuration ----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.intersphinx',
              'sphinx.ext.todo',
              'sphinx.ext.viewcode',
              'oslosphinx']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'Trove'
copyright = u'2013, OpenStack Foundation'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
from trove import version as trove_version
# The full version, including alpha/beta/rc tags.
release = trove_version.version_string_with_vcs()
# The short X.Y version.
version = trove_version.canonical_version_string()

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
modindex_common_prefix = ['trove.']

# If true, keep warnings as "system message" paragraphs in the built documents.
#keep_warnings = False

# -- Options for HTML output --------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
# html_theme_path = ["."]
# html_theme = '_theme'
# html_static_path = ['_static']

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
# html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = '%sdoc' % project


# -- Options for LaTeX output -------------------------------------------------

latex_elements = {
    # The paper size ('letterpaper' or 'a4paper').
    #'papersize': 'letterpaper',

    # The font size ('10pt', '11pt' or '12pt').
    #'pointsize': '10pt',

    # Additional stuff for the LaTeX preamble.
    #'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual])
latex_documents = [
    (
        'index',
        '%s.tex' % project,
        u'%s Documentation' % project,
        u'OpenStack Foundation',
        'manual'
    ),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output -------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
# man_pages = [
#     (
#         'index',
#         '%s' % project,
#         u'%s Documentation' % project,
#         u'OpenStack Foundation',
#         1
#     ),
# ]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output -----------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
    (
        'index',
        '%s' % project,
        u'%s Documentation' % project,
        u'OpenStack Foundation',
        '%s' % project,
        'Database as a service.',
        'Miscellaneous'
        'manual'
    ),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'

# If true, do not generate a @detailmenu in the "Top" node's menu.
#texinfo_no_detailmenu = False


# -- Options for Epub output --------------------------------------------------

# Bibliographic Dublin Core info.
epub_title = u'%s' % project
epub_author = u'OpenStack Foundation'
epub_publisher = u'OpenStack Foundation'
epub_copyright = u'2013, OpenStack Foundation'

# The language of the text. It defaults to the language option
# or en if the language is not set.
#epub_language = ''

# The scheme of the identifier. Typical schemes are ISBN or URL.
#epub_scheme = ''

# The unique identifier of the text. This can be a ISBN number
# or the project homepage.
#epub_identifier = ''

# A unique identification for the text.
#epub_uid = ''

# A tuple containing the cover image and cover page html template filenames.
#epub_cover = ()

# A sequence of (type, uri, title) tuples for the guide element of content.opf.
#epub_guide = ()

# HTML files that should be inserted before the pages created by sphinx.
# The format is a list of tuples containing the path and title.
#epub_pre_files = []

# HTML files shat should be inserted after the pages created by sphinx.
# The format is a list of tuples containing the path and title.
#epub_post_files = []

# A list of files that should not be packed into the epub file.
#epub_exclude_files = []

# The depth of the table of contents in toc.ncx.
#epub_tocdepth = 3

# Allow duplicate toc entries.
#epub_tocdup = True

# Fix unsupported image types using the PIL.
#epub_fix_images = False

# Scale large images.
#epub_max_image_width = 0

# If 'no', URL addresses will not be shown.
#epub_show_urls = 'inline'

# If false, no index is generated.
#epub_use_index = True


# Example configuration for intersphinx: refer to the Python standard library.
intersphinx_mapping = {'http://docs.python.org/': None}

########NEW FILE########
__FILENAME__ = dns_client
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
DNS Client interface. Child of OpenStack client to handle auth issues.
We have to duplicate a lot of code from the OpenStack client since so much
is different here.
"""

from trove.openstack.common import log as logging

import exceptions

try:
    import json
except ImportError:
    import simplejson as json


from novaclient.client import HTTPClient
from novaclient.v1_1.client import Client

LOG = logging.getLogger('rsdns.client.dns_client')


class DNSaasClient(HTTPClient):

    def __init__(self, accountId, user, apikey, auth_url, management_base_url):
        tenant = "dbaas"
        super(DNSaasClient, self).__init__(user, apikey, tenant, auth_url)
        self.accountId = accountId
        self.management_base_url = management_base_url
        self.api_key = apikey
        self.disable_ssl_certificate_validation = True
        self.service = "cloudDNS"

    def authenticate(self):
        """Set the management url and auth token"""
        req_body = {'credentials': {'username': self.user,
                                    'key': self.api_key}}
        resp, body = self.request(self.auth_url, "POST", body=req_body)
        if 'access' in body:
            if not self.management_url:
                # Assume the new Keystone lite:
                catalog = body['access']['serviceCatalog']
                for service in catalog:
                    if service['name'] == self.service:
                        self.management_url = service['adminURL']
            self.auth_token = body['access']['token']['id']
        else:
            # Assume pre-Keystone Light:
            try:
                if not self.management_url:
                    keys = ['auth',
                            'serviceCatalog',
                            self.service,
                            0,
                            'publicURL']
                    url = body
                    for key in keys:
                        url = url[key]
                    self.management_url = url
                self.auth_token = body['auth']['token']['id']
            except KeyError:
                raise NotImplementedError("Service: %s is not available"
                % self.service)

    def request(self, *args, **kwargs):
        kwargs.setdefault('headers', kwargs.get('headers', {}))
        kwargs['headers']['User-Agent'] = self.USER_AGENT
        kwargs['headers']['Accept'] = 'application/json'
        if 'body' in kwargs:
            kwargs['headers']['Content-Type'] = 'application/json'
            kwargs['body'] = json.dumps(kwargs['body'])
            LOG.debug("REQ HEADERS:" + str(kwargs['headers']))
            LOG.debug("REQ BODY:" + str(kwargs['body']))

        resp, body = super(HTTPClient, self).request(*args, **kwargs)

        self.http_log(args, kwargs, resp, body)

        if body:
            try:
                body = json.loads(body)
            except ValueError:
                pass
        else:
            body = None

        if resp.status in (400, 401, 403, 404, 408, 409, 413, 500, 501):
            raise exceptions.from_response(resp, body)

        return resp, body


class DNSaas(Client):
    """
    Top-level object to access the DNSaas service
    """

    def __init__(self, accountId, username, apikey,
                 auth_url='https://auth.api.rackspacecloud.com/v1.0',
                 management_base_url=None):
        from rsdns.client.dns_client import DNSaasClient
        from rsdns.client.domains import DomainsManager
        from rsdns.client.records import RecordsManager

        super(DNSaas, self).__init__(self, accountId, username, apikey,
                                     auth_url, management_base_url)
        self.client = DNSaasClient(accountId, username, apikey, auth_url,
                                    management_base_url)
        self.domains = DomainsManager(self)
        self.records = RecordsManager(self)

########NEW FILE########
__FILENAME__ = domains
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Domains interface.
"""

from novaclient import base

import os
from rsdns.client.future import FutureResource


class Domain(base.Resource):
    """
    A Domain has a name and stores records.  In the API they are id'd by ints.
    """

    def response_list_name(self):
        return "domains"


class FutureDomain(FutureResource):

    def convert_callback(self, resp, body):
        return Domain(self.manager, body)

    def response_list_name(self):
        return "domains"


class DomainsManager(base.ManagerWithFind):
    """
    Manage :class:`Domain` resources.
    """
    resource_class = Domain

    def create(self, name):
        """Not implemented / needed yet."""
        if os.environ.get("ADD_DOMAINS", "False") == 'True':
            accountId = self.api.client.accountId
            data = {"domains":
                        [
                            {"name": name,
                             "ttl":"5600",
                             "emailAddress":"dbaas_dns@rackspace.com",
                            }
                        ]
                    }
            resp, body = self.api.client.post("/domains", body=data)
            if resp.status == 202:
                return FutureDomain(self, **body)
            raise RuntimeError("Did not expect response " + str(resp.status))
        else:
            raise NotImplementedError("No need for create.")

    def create_from_list(self, list):
        return [self.resource_class(self, res) for res in list]

    def delete(self, *args, **kwargs):
        """Not implemented / needed yet."""
        raise NotImplementedError("No need for create.")

    def list(self, name=None):
        """
        Get a list of all domains.

        :rtype: list of :class:`Domain`
        """
        url = "/domains"
        if name:
            url += "?name=" + name
        resp, body = self.api.client.get(url)
        try:
            list = body['domains']
        except KeyError:
            raise RuntimeError('Body was missing "domains" key.')
        return self.create_from_list(list)

########NEW FILE########
__FILENAME__ = exceptions
#    Copyright 2011 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from novaclient import exceptions


class UnprocessableEntity(exceptions.ClientException):
    """
    HTTP 422 - Unprocessable Entity: The request cannot be processed.
    """
    http_status = 422
    message = "Unprocessable Entity"


_code_map = dict((c.http_status, c) for c in [UnprocessableEntity])


def from_response(response, body):
    """
    Return an instance of an ClientException or subclass
    based on an httplib2 response.

    Usage::

        resp, body = http.request(...)
        if resp.status != 200:
            raise exception_from_response(resp, body)
    """
    cls = _code_map.get(response.status, None)
    if not cls:
        cls = exceptions._code_map.get(response.status,
                                       exceptions.ClientException)
    if body:
        message = "n/a"
        details = "n/a"
        if hasattr(body, 'keys'):
            error = body[body.keys()[0]]
            message = error.get('message', None)
            details = error.get('details', None)
        return cls(code=response.status, message=message, details=details)
    else:
        return cls(code=response.status)

########NEW FILE########
__FILENAME__ = future


class RsDnsError(RuntimeError):

    def __init__(self, error):
        self.error_msg = ""
        try:
            for message in error['validationErrors']['messages']:
                self.error_msg += message
        except KeyError:
            self.error_msg += "... (did not understand the RsDNS response)."
        super(RsDnsError, self).__init__(self.error_msg)

    def __str__(self):
        return self.message


class FutureResource(object):
    """Polls a callback url to return a resource."""

    def __init__(self, manager, jobId, callbackUrl, status, **kwargs):
        self.manager = manager
        self.jobId = jobId
        self.callbackUrl = unicode(callbackUrl)
        self.result = None
        management_url = unicode(self.manager.api.client.management_url)
        if self.callbackUrl.startswith(management_url):
            self.callbackUrl = self.callbackUrl[len(management_url):]

    def call_callback(self):
        return self.manager.api.client.get(self.callbackUrl +
                                           "?showDetails=true")

    def poll(self):
        if not self.result:
            resp, body = self.call_callback()
            if resp.status == 202:
                return None
            if resp.status == 200:
                if body['status'] == 'ERROR':
                    raise RsDnsError(body['error'])
                elif body['status'] != 'COMPLETED':
                    return None
                resp_list = body['response'][self.response_list_name()]
                self.result = self.manager.create_from_list(resp_list)
        return self.result

    @property
    def ready(self):
        return (self.result or self.poll()) is not None

    @property
    def resource(self):
        return self.result or self.poll()

########NEW FILE########
__FILENAME__ = records
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Records interface.
"""

import six.moves.urllib.parse as urlparse

from novaclient import base
from rsdns.client.future import FutureResource


class FutureRecord(FutureResource):

    def convert_callback(self, resp, body):
        try:
            list = body['records']
        except NameError:
            raise RuntimeError('Body was missing "records" or "record" key.')
        if len(list) != 1:
            raise RuntimeError('Return result had ' + str(len(list)) +
                               'records, not 1.')
        return Record(self, list[0])

    def response_list_name(self):
        return "records"


class Record(base.Resource):
    """
    A Record is a individual dns record (Cname, A, MX, etc..)
    """

    pass


class RecordsManager(base.ManagerWithFind):
    """
    Manage :class:`Record` resources.
    """
    resource_class = Record

    def create(self, domain, record_name, record_data, record_type,
               record_ttl):
        """
        Create a new Record on the given domain

        :param domain: The ID of the :class:`Domain` to get.
        :param record: The ID of the :class:`Record` to get.
        :rtype: :class:`Record`
        """
        data = {"records": [{"type": record_type, "name": record_name,
                            "data": record_data, "ttl": record_ttl}]}
        resp, body = self.api.client.post("/domains/%s/records" % \
                                          base.getid(domain), body=data)
        if resp.status == 202:
            return FutureRecord(self, **body)
        raise RuntimeError("Did not expect response when creating a DNS "
                            "record %s" % str(resp.status))

    def create_from_list(self, list):
        return [self.resource_class(self, res) for res in list]

    def delete(self, domain_id, record_id):
        self._delete("/domains/%s/records/%s" % (domain_id, record_id))

    def match_record(self, record, name=None, address=None, type=None):
        assert(isinstance(record, Record))
        return (not name or record.name == name) and \
               (not address or record.data == address) and \
               (not type or record.type == type)

    def get(self, domain_id, record_id):
        """
        Get a single record by id.

        :rtype: Single instance of :class:`Record`
        """
        url = "/domains/%s/records" % domain_id
        if record_id:
            url += ("/%s" % record_id)
        resp, body = self.api.client.get(url)
        try:
            item = body
        except IndexError:
            raise RuntimeError('Body was missing record element.')
        return self.resource_class(self, item)

    def list(self, domain_id, record_id=None, record_name=None,
             record_address=None, record_type=None):
        """
        Get a list of all records under a domain.

        :rtype: list of :class:`Record`
        """
        url = "/domains/%s/records" % domain_id
        if record_id:
            url += ("/%s" % record_id)
        offset = 0
        list = []
        while offset is not None:
            next_url = "%s?offset=%d" % (url, offset)
            partial_list, offset = self.page_list(next_url)
            list += partial_list
        all_records = self.create_from_list(list)
        return [record for record in all_records
                if self.match_record(record, record_name, record_address,
                                     record_type)]

    def page_list(self, url):
        """
        Given a URL and an offset, returns a tuple containing a list and the
        next URL.
        """
        resp, body = self.api.client.get(url)
        try:
            list = body['records']
        except NameError:
            raise RuntimeError('Body was missing "records" or "record" key.')
        next_offset = None
        links = body.get('links', [])
        for link in links:
            if link['rel'] == 'next':
                next = link['href']
                params = urlparse.parse_qs(urlparse.urlparse(next).query)
                offset_list = params.get('offset', [])
                if len(offset_list) == 1:
                    next_offset = int(offset_list[0])
                elif len(offset_list) == 0:
                    next_offset = None
                else:
                    raise RuntimeError("Next href had multiple offset params!")
        return (list, next_offset)

########NEW FILE########
__FILENAME__ = run_tests
# Copyright 2013 OpenStack Foundation
# Copyright 2013 Rackspace Hosting
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

import gettext
import os
import urllib
import sys
import traceback

from trove.common import cfg
from trove.openstack.common import log as logging
from trove.tests.config import CONFIG
from wsgi_intercept.httplib2_intercept import install as wsgi_install
import proboscis
import wsgi_intercept
from trove.openstack.common.rpc import service as rpc_service

import eventlet
eventlet.monkey_patch(thread=False)

CONF = cfg.CONF


def add_support_for_localization():
    """Adds support for localization in the logging.

    If ../nova/__init__.py exists, add ../ to Python search path, so that
    it will override what happens to be installed in
    /usr/(local/)lib/python...

    """
    path = os.path.join(os.path.abspath(sys.argv[0]), os.pardir, os.pardir)
    possible_topdir = os.path.normpath(path)
    if os.path.exists(os.path.join(possible_topdir, 'nova', '__init__.py')):
        sys.path.insert(0, possible_topdir)

    gettext.install('nova', unicode=1)


def initialize_trove(config_file):
    from trove.openstack.common import pastedeploy

    cfg.CONF(args=[],
             project='trove',
             default_config_files=[config_file])
    logging.setup(None)
    topic = CONF.taskmanager_queue

    from trove.taskmanager import manager
    manager_impl = manager.Manager()
    taskman_service = rpc_service.Service(None, topic=topic,
                                          manager=manager_impl)
    taskman_service.start()

    return pastedeploy.paste_deploy_app(config_file, 'trove', {})


def datastore_init():
    # Adds the datastore for mysql (needed to make most calls work).
    from trove.datastore import models

    models.DBDatastore.create(id=CONFIG.dbaas_datastore_id,
                              name=CONFIG.dbaas_datastore,
                              default_version_id=
                              CONFIG.dbaas_datastore_version_id)
    models.DBDatastore.create(id=CONFIG.dbaas_datastore_id_no_versions,
                              name='Test_Datastore_1',
                              default_version_id=None)

    models.DBDatastoreVersion.create(id=CONFIG.dbaas_datastore_version_id,
                                     datastore_id=
                                     CONFIG.dbaas_datastore_id,
                                     name=CONFIG.dbaas_datastore_version,
                                     manager="mysql",
                                     image_id=
                                     'c00000c0-00c0-0c00-00c0-000c000000cc',
                                     packages='test packages',
                                     active=1)
    models.DBDatastoreVersion.create(id="d00000d0-00d0-0d00-00d0-000d000000dd",
                                     datastore_id=
                                     CONFIG.dbaas_datastore_id,
                                     name='mysql_inactive_version',
                                     manager="mysql",
                                     image_id=
                                     'c00000c0-00c0-0c00-00c0-000c000000cc',
                                     packages=None, active=0)


def initialize_database():
    from trove.db import get_db_api
    from trove.db.sqlalchemy import session
    db_api = get_db_api()
    db_api.drop_db(CONF)  # Destroys the database, if it exists.
    db_api.db_sync(CONF)
    session.configure_db(CONF)
    datastore_init()
    db_api.configure_db(CONF)


def initialize_fakes(app):
    # Set up WSGI interceptor. This sets up a fake host that responds each
    # time httplib tries to communicate to localhost, port 8779.
    def wsgi_interceptor(*args, **kwargs):

        def call_back(env, start_response):
            path_info = env.get('PATH_INFO')
            if path_info:
                env['PATH_INFO'] = urllib.unquote(path_info)
            #print("%s %s" % (args, kwargs))
            return app.__call__(env, start_response)

        return call_back

    wsgi_intercept.add_wsgi_intercept('localhost',
                                      CONF.bind_port,
                                      wsgi_interceptor)
    from trove.tests.util import event_simulator
    event_simulator.monkey_patch()


def parse_args_for_test_config():
    for index in range(len(sys.argv)):
        arg = sys.argv[index]
        print(arg)
        if arg[:14] == "--test-config=":
            del sys.argv[index]
            return arg[14:]
    return 'etc/tests/localhost.test.conf'

if __name__ == "__main__":
    try:
        wsgi_install()
        add_support_for_localization()
        # Load Trove app
        # Paste file needs absolute path
        config_file = os.path.realpath('etc/trove/trove.conf.test')
        # 'etc/trove/test-api-paste.ini'
        app = initialize_trove(config_file)
        # Initialize sqlite database.
        initialize_database()
        # Swap out WSGI, httplib, and several sleep functions
        # with test doubles.
        initialize_fakes(app)

        # Initialize the test configuration.
        test_config_file = parse_args_for_test_config()
        CONFIG.load_from_file(test_config_file)

        # F401 unused imports needed for tox tests
        from trove.tests.api import backups  # noqa
        from trove.tests.api import header  # noqa
        from trove.tests.api import limits  # noqa
        from trove.tests.api import flavors  # noqa
        from trove.tests.api import versions  # noqa
        from trove.tests.api import instances as rd_instances  # noqa
        from trove.tests.api import instances_actions as rd_actions  # noqa
        from trove.tests.api import instances_delete  # noqa
        from trove.tests.api import instances_mysql_down  # noqa
        from trove.tests.api import instances_resize  # noqa
        from trove.tests.api import databases  # noqa
        from trove.tests.api import datastores  # noqa
        from trove.tests.api import root  # noqa
        from trove.tests.api import root_on_create  # noqa
        from trove.tests.api import users  # noqa
        from trove.tests.api import user_access  # noqa
        from trove.tests.api.mgmt import accounts  # noqa
        from trove.tests.api.mgmt import admin_required  # noqa
        from trove.tests.api.mgmt import hosts  # noqa
        from trove.tests.api.mgmt import instances as mgmt_instances  # noqa
        from trove.tests.api.mgmt import instances_actions as mgmt_actions  # noqa
        from trove.tests.api.mgmt import storage  # noqa
        from trove.tests.api.mgmt import malformed_json  # noqa
    except Exception as e:
        print("Run tests failed: %s" % e)
        traceback.print_exc()
        raise

    proboscis.TestProgram().run_and_exit()

########NEW FILE########
__FILENAME__ = install_venv
#!/usr/bin/env python

# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
#
# Copyright 2010 OpenStack, LLC
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Installation script for Trove's development virtualenv
"""

import os
import subprocess
import sys


ROOT = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
VENV = os.path.join(ROOT, '.venv')
PIP_REQUIRES = os.path.join(ROOT, 'requirements.txt')
TEST_REQUIRES = os.path.join(ROOT, 'test-requirements.txt')
PY_VERSION = "python%s.%s" % (sys.version_info[0], sys.version_info[1])


def die(message, *args):
    print >> sys.stderr, message % args
    sys.exit(1)


def check_python_version():
    if sys.version_info < (2, 6):
        die("Need Python Version >= 2.6")


def run_command(cmd, redirect_output=True, check_exit_code=True):
    """
    Runs a command in an out-of-process shell, returning the
    output of that command.  Working directory is ROOT.
    """
    if redirect_output:
        stdout = subprocess.PIPE
    else:
        stdout = None

    proc = subprocess.Popen(cmd, cwd=ROOT, stdout=stdout)
    output = proc.communicate()[0]
    if check_exit_code and proc.returncode != 0:
        die('Command "%s" failed.\n%s', ' '.join(cmd), output)
    return output


HAS_EASY_INSTALL = bool(run_command(['which', 'easy_install'],
    check_exit_code=False).strip())
HAS_VIRTUALENV = bool(run_command(['which', 'virtualenv'],
    check_exit_code=False).strip())


def check_dependencies():
    """Make sure virtualenv is in the path."""

    if not HAS_VIRTUALENV:
        print 'not found.'
        # Try installing it via easy_install...
        if HAS_EASY_INSTALL:
            print 'Installing virtualenv via easy_install...',
            if not (run_command(['which', 'easy_install']) and
                    run_command(['easy_install', 'virtualenv'])):
                die('ERROR: virtualenv not found.\n\Trove development'
                    ' requires virtualenv, please install it using your'
                    ' favorite package management tool')
            print 'done.'
    print 'done.'


def create_virtualenv(venv=VENV):
    """Creates the virtual environment and installs PIP only into the
    virtual environment
    """
    print 'Creating venv...',
    run_command(['virtualenv', '-q', '--no-site-packages', VENV])
    print 'done.'
    print 'Installing pip in virtualenv...',
    if not run_command(['tools/with_venv.sh', 'easy_install', 'pip']).strip():
        die("Failed to install pip.")
    print 'done.'


def install_dependencies(venv=VENV):
    print 'Installing dependencies with pip (this can take a while)...'
    # Install greenlet by hand - just listing it in the requires file does not
    # get it in stalled in the right order
    run_command(['tools/with_venv.sh', '-E', venv, 'pip', 'install',
                 'greenlet'], redirect_output=False)
    for requires in (PIP_REQUIRES, TEST_REQUIRES):
        run_command(['tools/with_venv.sh', '-E', venv, 'pip', 'install', '-r',
                     requires], redirect_output=False)

    # Tell the virtual env how to "import trove"
    pthfile = os.path.join(venv, "lib", PY_VERSION, "site-packages",
        "trove.pth")
    f = open(pthfile, 'w')
    f.write("%s\n" % ROOT)


def print_help():
    help = """
    Trove development environment setup is complete.

    Trove development uses virtualenv to track and manage Python
    dependencies while in development and testing.

    To activate the Trove virtualenv for the extent of your current shell
    session you can run:

    $ source .venv/bin/activate

    Or, if you prefer, you can run commands in the virtualenv on a case by case
    basis by running:

    $ tools/with_venv.sh <your command>

    Also, make test will automatically use the virtualenv.
    """
    print help


def main(argv):
    check_python_version()
    check_dependencies()
    create_virtualenv()
    install_dependencies()
    print_help()

if __name__ == '__main__':
    main(sys.argv)

########NEW FILE########
__FILENAME__ = models
#Copyright [2013] Hewlett-Packard Development Company, L.P.

#Licensed under the Apache License, Version 2.0 (the "License");
#you may not use this file except in compliance with the License.
#You may obtain a copy of the License at
#
#http://www.apache.org/licenses/LICENSE-2.0
#
#Unless required by applicable law or agreed to in writing, software
#distributed under the License is distributed on an "AS IS" BASIS,
#WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#See the License for the specific language governing permissions and
#limitations under the License.

"""Model classes that form the core of snapshots functionality."""

from sqlalchemy import desc
from swiftclient.client import ClientException

from trove.common import cfg
from trove.common import exception
from trove.db.models import DatabaseModelBase
from trove.openstack.common import log as logging
from trove.taskmanager import api
from trove.common.remote import create_swift_client
from trove.common import utils
from trove.quota.quota import run_with_quotas

CONF = cfg.CONF
LOG = logging.getLogger(__name__)


class BackupState(object):
    NEW = "NEW"
    BUILDING = "BUILDING"
    SAVING = "SAVING"
    COMPLETED = "COMPLETED"
    FAILED = "FAILED"
    DELETE_FAILED = "DELETE_FAILED"
    RUNNING_STATES = [NEW, BUILDING, SAVING]
    END_STATES = [COMPLETED, FAILED, DELETE_FAILED]


class Backup(object):

    @classmethod
    def validate_can_perform_action(cls, instance, operation):
        """
        Raises exception if backup strategy is not supported
        """
        datastore_cfg = CONF.get(instance.datastore_version.manager)
        if not datastore_cfg or not (
                datastore_cfg.get('backup_strategy', None)):
            raise exception.DatastoreOperationNotSupported(
                operation=operation, datastore=instance.datastore.name)

    @classmethod
    def create(cls, context, instance, name, description=None, parent_id=None):
        """
        create db record for Backup
        :param cls:
        :param context: tenant_id included
        :param instance:
        :param name:
        :param description:
        :return:
        """

        def _create_resources():
            # parse the ID from the Ref
            instance_id = utils.get_id_from_href(instance)

            # verify that the instance exists and can perform actions
            from trove.instance.models import Instance
            instance_model = Instance.load(context, instance_id)
            instance_model.validate_can_perform_action()
            cls.validate_can_perform_action(
                instance_model, 'backup_create')
            cls.verify_swift_auth_token(context)

            parent = None
            if parent_id:
                # Look up the parent info or fail early if not found or if
                # the user does not have access to the parent.
                _parent = cls.get_by_id(context, parent_id)
                parent = {
                    'location': _parent.location,
                    'checksum': _parent.checksum,
                }
            try:
                db_info = DBBackup.create(name=name,
                                          description=description,
                                          tenant_id=context.tenant,
                                          state=BackupState.NEW,
                                          instance_id=instance_id,
                                          parent_id=parent_id,
                                          deleted=False)
            except exception.InvalidModelError as ex:
                LOG.exception("Unable to create Backup record:")
                raise exception.BackupCreationError(str(ex))

            backup_info = {'id': db_info.id,
                           'name': name,
                           'description': description,
                           'instance_id': instance_id,
                           'backup_type': db_info.backup_type,
                           'checksum': db_info.checksum,
                           'parent': parent,
                           }
            api.API(context).create_backup(backup_info, instance_id)
            return db_info

        return run_with_quotas(context.tenant,
                               {'backups': 1},
                               _create_resources)

    @classmethod
    def running(cls, instance_id, exclude=None):
        """
        Returns the first running backup for instance_id
        :param instance_id: Id of the instance
        :param exclude: Backup ID to exclude from the query (any other running)
        """
        query = DBBackup.query()
        query = query.filter(DBBackup.instance_id == instance_id,
                             DBBackup.state.in_(BackupState.RUNNING_STATES))
        # filter out deleted backups, PEP8 does not like field == False!
        query = query.filter_by(deleted=False)
        if exclude:
            query = query.filter(DBBackup.id != exclude)
        return query.first()

    @classmethod
    def get_by_id(cls, context, backup_id, deleted=False):
        """
        get the backup for that id
        :param cls:
        :param backup_id: Id of the backup to return
        :param deleted: Return deleted backups
        :return:
        """
        try:
            db_info = DBBackup.find_by(context=context,
                                       id=backup_id,
                                       deleted=deleted)
            return db_info
        except exception.NotFound:
            raise exception.NotFound(uuid=backup_id)

    @classmethod
    def _paginate(cls, context, query):
        """Paginate the results of the base query.
        We use limit/offset as the results need to be ordered by date
        and not the primary key.
        """
        marker = int(context.marker or 0)
        limit = int(context.limit or CONF.backups_page_size)
        # order by 'updated DESC' to show the most recent backups first
        query = query.order_by(desc(DBBackup.updated))
        # Apply limit/offset
        query = query.limit(limit)
        query = query.offset(marker)
        # check if we need to send a marker for the next page
        if query.count() < limit:
            marker = None
        else:
            marker += limit
        return query.all(), marker

    @classmethod
    def list(cls, context):
        """
        list all live Backups belong to given tenant
        :param cls:
        :param context: tenant_id included
        :return:
        """
        query = DBBackup.query()
        query = query.filter_by(tenant_id=context.tenant,
                                deleted=False)
        return cls._paginate(context, query)

    @classmethod
    def list_for_instance(cls, context, instance_id):
        """
        list all live Backups associated with given instance
        :param cls:
        :param instance_id:
        :return:
        """
        query = DBBackup.query()
        query = query.filter_by(instance_id=instance_id,
                                deleted=False)
        return cls._paginate(context, query)

    @classmethod
    def fail_for_instance(cls, instance_id):
        query = DBBackup.query()
        query = query.filter(DBBackup.instance_id == instance_id,
                             DBBackup.state.in_(BackupState.RUNNING_STATES))
        query = query.filter_by(deleted=False)
        for backup in query.all():
            backup.state = BackupState.FAILED
            backup.save()

    @classmethod
    def delete(cls, context, backup_id):
        """
        update Backup table on deleted flag for given Backup
        :param cls:
        :param context: context containing the tenant id and token
        :param backup_id: Backup uuid
        :return:
        """

        # Recursively delete all children and grandchildren of this backup.
        query = DBBackup.query()
        query = query.filter_by(parent_id=backup_id, deleted=False)
        for child in query.all():
            cls.delete(context, child.id)

        def _delete_resources():
            backup = cls.get_by_id(context, backup_id)
            if backup.is_running:
                msg = ("Backup %s cannot be delete because it is running." %
                       backup_id)
                raise exception.UnprocessableEntity(msg)
            cls.verify_swift_auth_token(context)
            api.API(context).delete_backup(backup_id)

        return run_with_quotas(context.tenant,
                               {'backups': -1},
                               _delete_resources)

    @classmethod
    def verify_swift_auth_token(cls, context):
        try:
            client = create_swift_client(context)
            client.get_account()
        except ClientException:
            raise exception.SwiftAuthError(tenant_id=context.tenant)


def persisted_models():
    return {'backups': DBBackup}


class DBBackup(DatabaseModelBase):
    """A table for Backup records"""
    _data_fields = ['id', 'name', 'description', 'location', 'backup_type',
                    'size', 'tenant_id', 'state', 'instance_id',
                    'checksum', 'backup_timestamp', 'deleted', 'created',
                    'updated', 'deleted_at', 'parent_id']
    preserve_on_delete = True

    @property
    def is_running(self):
        return self.state in BackupState.RUNNING_STATES

    @property
    def is_done(self):
        return self.state in BackupState.END_STATES

    @property
    def filename(self):
        if self.location:
            last_slash = self.location.rfind("/")
            if last_slash < 0:
                raise ValueError("Bad location for backup object.")
            return self.location[last_slash + 1:]
        else:
            return None

    def check_swift_object_exist(self, context, verify_checksum=False):
        try:
            parts = self.location.split('/')
            obj = parts[-1]
            container = parts[-2]
            client = create_swift_client(context)
            LOG.info(_("Checking if backup exist in '%s'") % self.location)
            resp = client.head_object(container, obj)
            if verify_checksum:
                LOG.info(_("Checking if backup checksum matches swift."))
                # swift returns etag in double quotes
                # e.g. '"dc3b0827f276d8d78312992cc60c2c3f"'
                swift_checksum = resp['etag'].strip('"')
                if self.checksum != swift_checksum:
                    raise exception.RestoreBackupIntegrityError(
                        backup_id=self.id)
            return True
        except ClientException as e:
            if e.http_status == 404:
                return False
            else:
                raise exception.SwiftAuthError(tenant_id=context.tenant)

########NEW FILE########
__FILENAME__ = service
# Copyright 2013 OpenStack Foundation
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from trove.backup import views
from trove.backup.models import Backup
from trove.common import apischema
from trove.common import cfg
from trove.common import pagination
from trove.common import wsgi
from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _

CONF = cfg.CONF
LOG = logging.getLogger(__name__)


class BackupController(wsgi.Controller):
    """
    Controller for accessing backups in the OpenStack API.
    """
    schemas = apischema.backup

    def index(self, req, tenant_id):
        """
        Return all backups information for a tenant ID.
        """
        LOG.debug("Listing Backups for tenant '%s'" % tenant_id)
        context = req.environ[wsgi.CONTEXT_KEY]
        backups, marker = Backup.list(context)
        view = views.BackupViews(backups)
        paged = pagination.SimplePaginatedDataView(req.url, 'backups', view,
                                                   marker)
        return wsgi.Result(paged.data(), 200)

    def show(self, req, tenant_id, id):
        """Return a single backup."""
        LOG.info(_("Showing a backup for tenant '%s'") % tenant_id)
        LOG.info(_("id : '%s'\n\n") % id)
        context = req.environ[wsgi.CONTEXT_KEY]
        backup = Backup.get_by_id(context, id)
        return wsgi.Result(views.BackupView(backup).data(), 200)

    def create(self, req, body, tenant_id):
        LOG.debug("Creating a Backup for tenant '%s'" % tenant_id)
        context = req.environ[wsgi.CONTEXT_KEY]
        data = body['backup']
        instance = data['instance']
        name = data['name']
        desc = data.get('description')
        parent = data.get('parent_id')
        backup = Backup.create(context, instance, name, desc, parent_id=parent)
        return wsgi.Result(views.BackupView(backup).data(), 202)

    def delete(self, req, tenant_id, id):
        LOG.debug("Delete Backup for tenant: %s, ID: %s" % (tenant_id, id))
        context = req.environ[wsgi.CONTEXT_KEY]
        Backup.delete(context, id)
        return wsgi.Result(None, 202)

########NEW FILE########
__FILENAME__ = views
# Copyright 2013 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


class BackupView(object):

    def __init__(self, backup):
        self.backup = backup

    def data(self):
        return {"backup": {
            "id": self.backup.id,
            "name": self.backup.name,
            "description": self.backup.description,
            "locationRef": self.backup.location,
            "instance_id": self.backup.instance_id,
            "created": self.backup.created,
            "updated": self.backup.updated,
            "size": self.backup.size,
            "status": self.backup.state,
            "parent_id": self.backup.parent_id,
        }
        }


class BackupViews(object):

    def __init__(self, backups):
        self.backups = backups

    def data(self):
        backups = []

        for b in self.backups:
            backups.append(BackupView(b).data()["backup"])
        return {"backups": backups}

########NEW FILE########
__FILENAME__ = api
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from trove.cmd.common import with_initialize


@with_initialize
def main(CONF):
    from trove.common import wsgi
    conf_file = CONF.find_file(CONF.api_paste_config)
    launcher = wsgi.launch('trove', CONF.bind_port or 8779, conf_file,
                           workers=CONF.trove_api_workers)
    launcher.wait()

########NEW FILE########
__FILENAME__ = common
# Copyright 2014 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


def initialize(extra_opts=None, pre_logging=None):
    # Initialize localization support (the underscore character).
    import gettext
    gettext.install('trove', unicode=1)

    # Apply whole eventlet.monkey_patch excluding 'thread' module.
    # Decision for 'thread' module patching will be made
    # after debug_utils is set up.
    import eventlet
    eventlet.monkey_patch(all=True, thread=False)

    # Import only the modules necessary to initialize logging and determine if
    # debug_utils are enabled.
    import sys
    from trove.common import cfg
    from trove.common import debug_utils
    from trove.openstack.common import log as logging

    conf = cfg.CONF
    if extra_opts:
        conf.register_cli_opts(extra_opts)

    cfg.parse_args(sys.argv)
    if pre_logging:
        pre_logging(conf)

    logging.setup(None)
    debug_utils.setup()

    # Patch 'thread' module if debug is disabled
    if not debug_utils.enabled():
        eventlet.monkey_patch(thread=True)

    # Initialize Trove database.
    from trove.db import get_db_api
    get_db_api().configure_db(conf)

    return conf  # May be used by other scripts


def with_initialize(main_function=None, **kwargs):
    """
    Decorates a script main function to make sure that dependency imports and
    initialization happens correctly.
    """
    def apply(main_function):
        def run():
            conf = initialize(**kwargs)
            return main_function(conf)

        return run

    if main_function:
        return apply(main_function)
    else:
        return apply

########NEW FILE########
__FILENAME__ = conductor
# Copyright 2013 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from trove.cmd.common import with_initialize


@with_initialize
def main(conf):
    from trove.common.rpc import service as rpc_service
    from trove.openstack.common import service as openstack_service

    manager = 'trove.conductor.manager.Manager'
    topic = conf.conductor_queue
    server = rpc_service.RpcService(manager=manager, topic=topic)
    launcher = openstack_service.launch(server,
                                        workers=conf.trove_conductor_workers)
    launcher.wait()

########NEW FILE########
__FILENAME__ = fakemode
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from oslo.config import cfg as openstack_cfg
from trove.cmd.common import with_initialize


opts = [
    openstack_cfg.BoolOpt('fork', short='f', default=False, dest='fork'),
    openstack_cfg.StrOpt('pid-file', default='.pid'),
    openstack_cfg.StrOpt('override-logfile', default=None),
]


def setup_logging(conf):
    if conf.override_logfile:
        conf.use_stderr = False
        conf.log_file = conf.override_logfile


@with_initialize(extra_opts=opts, pre_logging=setup_logging)
def main(conf):
    if conf.fork:
        pid = os.fork()
        if pid == 0:
            start_server(conf)
        else:
            print("Starting server:%s" % pid)
            pid_file = CONF.pid_file
            with open(pid_file, 'w') as f:
                f.write(str(pid))
    else:
        start_server(conf)


def start_fake_taskmanager(conf):
    topic = conf.taskmanager_queue
    from trove.openstack.common.rpc import service as rpc_service
    from trove.taskmanager import manager
    manager_impl = manager.Manager()
    taskman_service = rpc_service.Service(None, topic=topic,
                                          manager=manager_impl)
    taskman_service.start()


def start_server(conf):
    from trove.common import wsgi
    conf_file = conf.find_file(conf.api_paste_config)
    launcher = wsgi.launch('trove', conf.bind_port or 8779, conf_file,
                           workers=conf.trove_api_workers)
    start_fake_taskmanager(conf)
    launcher.wait()

########NEW FILE########
__FILENAME__ = guest
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import eventlet

import gettext
import sys


gettext.install('trove', unicode=1)


from trove.common import cfg
from trove.common import debug_utils
from trove.common.rpc import service as rpc_service
from oslo.config import cfg as openstack_cfg
from trove.openstack.common import log as logging
from trove.openstack.common import service as openstack_service

# Apply whole eventlet.monkey_patch excluding 'thread' module.
# Decision for 'thread' module patching will be made
# after debug_utils setting up
eventlet.monkey_patch(all=True, thread=False)

CONF = cfg.CONF
CONF.register_opts([openstack_cfg.StrOpt('guest_id')])


def main():
    cfg.parse_args(sys.argv)
    from trove.guestagent import dbaas
    logging.setup(None)

    debug_utils.setup()

    # Patch 'thread' module if debug is disabled
    if not debug_utils.enabled():
        eventlet.monkey_patch(thread=True)

    manager = dbaas.datastore_registry().get(CONF.datastore_manager)
    if not manager:
        msg = ("Manager class not registered for datastore manager %s" %
               CONF.datastore_manager)
        raise RuntimeError(msg)
    server = rpc_service.RpcService(manager=manager, host=CONF.guest_id)
    launcher = openstack_service.launch(server)
    launcher.wait()

########NEW FILE########
__FILENAME__ = manage
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import gettext
import inspect
import sys


gettext.install('trove', unicode=1)


from trove.common import cfg
from trove.common import exception
from trove.common import utils
from trove.db import get_db_api
from trove.openstack.common import log as logging
from trove.datastore import models as datastore_models


CONF = cfg.CONF


class Commands(object):

    def __init__(self):
        self.db_api = get_db_api()

    def db_sync(self, repo_path=None):
        self.db_api.db_sync(CONF, repo_path=repo_path)

    def db_upgrade(self, version=None, repo_path=None):
        self.db_api.db_upgrade(CONF, version, repo_path=None)

    def db_downgrade(self, version, repo_path=None):
        self.db_api.db_downgrade(CONF, version, repo_path=None)

    def execute(self):
        exec_method = getattr(self, CONF.action.name)
        args = inspect.getargspec(exec_method)
        args.args.remove('self')
        kwargs = {}
        for arg in args.args:
            kwargs[arg] = getattr(CONF.action, arg)
        exec_method(**kwargs)

    def datastore_update(self, datastore_name, default_version):
        try:
            datastore_models.update_datastore(datastore_name,
                                              default_version)
            print("Datastore '%s' updated." % datastore_name)
        except exception.DatastoreVersionNotFound as e:
            print(e)

    def datastore_version_update(self, datastore, version_name, manager,
                                 image_id, packages, active):
        try:
            datastore_models.update_datastore_version(datastore,
                                                      version_name,
                                                      manager,
                                                      image_id,
                                                      packages, active)
            print("Datastore version '%s' updated." % version_name)
        except exception.DatastoreNotFound as e:
            print(e)

    def db_recreate(self, repo_path):
        """Drops the database and recreates it."""
        self.db_api.drop_db(CONF)
        self.db_sync(repo_path)

    def params_of(self, command_name):
        if Commands.has(command_name):
            return utils.MethodInspector(getattr(self, command_name))


def main():

    def actions(subparser):
        parser = subparser.add_parser('db_sync')
        parser.add_argument('--repo_path')

        parser = subparser.add_parser('db_upgrade')
        parser.add_argument('--version')
        parser.add_argument('--repo_path')

        parser = subparser.add_parser('db_downgrade')
        parser.add_argument('version')
        parser.add_argument('--repo_path')

        parser = subparser.add_parser('datastore_update')
        parser.add_argument('datastore_name')
        parser.add_argument('default_version')

        parser = subparser.add_parser('datastore_version_update')
        parser.add_argument('datastore')
        parser.add_argument('version_name')
        parser.add_argument('manager')
        parser.add_argument('image_id')
        parser.add_argument('packages')
        parser.add_argument('active')

        parser = subparser.add_parser(
            'db_recreate', description='Drop the database and recreate it.')
        parser.add_argument(
            '--repo_path', help='SQLAlchemy Migrate repository path.')
        parser = subparser.add_parser('db_recreate')
        parser.add_argument('repo_path')

    cfg.custom_parser('action', actions)
    cfg.parse_args(sys.argv)

    try:
        logging.setup(None)

        Commands().execute()
        sys.exit(0)
    except TypeError as e:
        print(_("Possible wrong number of arguments supplied %s") % e)
        sys.exit(2)
    except Exception:
        print(_("Command failed, please check log for more info."))
        raise

########NEW FILE########
__FILENAME__ = taskmanager
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from oslo.config import cfg as openstack_cfg
from trove.cmd.common import with_initialize


extra_opts = [openstack_cfg.StrOpt('taskmanager_manager')]


def startup(conf, topic):
    from trove.common.rpc import service as rpc_service
    from trove.openstack.common import service as openstack_service

    server = rpc_service.RpcService(manager=conf.taskmanager_manager,
                                    topic=topic)
    launcher = openstack_service.launch(server)
    launcher.wait()


@with_initialize(extra_opts=extra_opts)
def main(conf):
    startup(conf, None)


@with_initialize(extra_opts=extra_opts)
def mgmt_main():
    startup(conf, "mgmt-taskmanager")

########NEW FILE########
__FILENAME__ = api
#    Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import routes

from trove.common import wsgi
from trove.configuration.service import ConfigurationsController
from trove.configuration.service import ParametersController
from trove.flavor.service import FlavorController
from trove.instance.service import InstanceController
from trove.limits.service import LimitsController
from trove.backup.service import BackupController
from trove.versions import VersionsController
from trove.datastore.service import DatastoreController


class API(wsgi.Router):
    """API"""
    def __init__(self):
        mapper = routes.Mapper()
        super(API, self).__init__(mapper)
        self._instance_router(mapper)
        self._datastore_router(mapper)
        self._flavor_router(mapper)
        self._versions_router(mapper)
        self._limits_router(mapper)
        self._backups_router(mapper)
        self._configurations_router(mapper)

    def _versions_router(self, mapper):
        versions_resource = VersionsController().create_resource()
        mapper.connect("/",
                       controller=versions_resource,
                       action="show",
                       conditions={'method': ['GET']})

    def _datastore_router(self, mapper):
        datastore_resource = DatastoreController().create_resource()
        mapper.resource("datastore", "/{tenant_id}/datastores",
                        controller=datastore_resource)
        mapper.connect("/{tenant_id}/datastores/{datastore}/versions",
                       controller=datastore_resource,
                       action="version_index")
        mapper.connect("/{tenant_id}/datastores/{datastore}/versions/{id}",
                       controller=datastore_resource,
                       action="version_show")
        mapper.connect("/{tenant_id}/datastores/versions/{uuid}",
                       controller=datastore_resource,
                       action="version_show_by_uuid")

    def _instance_router(self, mapper):
        instance_resource = InstanceController().create_resource()
        mapper.connect("/{tenant_id}/instances",
                       controller=instance_resource,
                       action="index",
                       conditions={'method': ['GET']})
        mapper.connect("/{tenant_id}/instances",
                       controller=instance_resource,
                       action="create",
                       conditions={'method': ['POST']})
        mapper.connect("/{tenant_id}/instances/{id}",
                       controller=instance_resource,
                       action="show",
                       conditions={'method': ['GET']})
        mapper.connect("/{tenant_id}/instances/{id}/action",
                       controller=instance_resource,
                       action="action",
                       conditions={'method': ['POST']})
        mapper.connect("/{tenant_id}/instances/{id}",
                       controller=instance_resource,
                       action="update",
                       conditions={'method': ['PUT']})
        mapper.connect("/{tenant_id}/instances/{id}",
                       controller=instance_resource,
                       action="delete",
                       conditions={'method': ['DELETE']})
        mapper.connect("/{tenant_id}/instances/{id}/backups",
                       controller=instance_resource,
                       action="backups",
                       conditions={'method': ['GET']})
        mapper.connect("/{tenant_id}/instances/{id}/configuration",
                       controller=instance_resource,
                       action="configuration",
                       conditions={'method': ['GET']})

    def _flavor_router(self, mapper):
        flavor_resource = FlavorController().create_resource()
        mapper.connect("/{tenant_id}/flavors",
                       controller=flavor_resource,
                       action="index",
                       conditions={'method': ['GET']})
        mapper.connect("/{tenant_id}/flavors/{id}",
                       controller=flavor_resource,
                       action="show",
                       conditions={'method': ['GET']})

    def _limits_router(self, mapper):
        limits_resource = LimitsController().create_resource()
        mapper.connect("/{tenant_id}/limits",
                       controller=limits_resource,
                       action="index",
                       conditions={'method': ['GET']})

    def _backups_router(self, mapper):
        backups_resource = BackupController().create_resource()
        mapper.connect("/{tenant_id}/backups",
                       controller=backups_resource,
                       action="index",
                       conditions={'method': ['GET']})
        mapper.connect("/{tenant_id}/backups",
                       controller=backups_resource,
                       action="create",
                       conditions={'method': ['POST']})
        mapper.connect("/{tenant_id}/backups/{id}",
                       controller=backups_resource,
                       action="show",
                       conditions={'method': ['GET']})
        mapper.connect("/{tenant_id}/backups/{id}",
                       controller=backups_resource,
                       action="action",
                       conditions={'method': ['POST']})
        mapper.connect("/{tenant_id}/backups/{id}",
                       controller=backups_resource,
                       action="delete",
                       conditions={'method': ['DELETE']})

    def _configurations_router(self, mapper):
        parameters_resource = ParametersController().create_resource()
        path = '/{tenant_id}/datastores/versions/{version}/parameters'
        mapper.connect(path,
                       controller=parameters_resource,
                       action='index_by_version',
                       conditions={'method': ['GET']})
        path = '/{tenant_id}/datastores/versions/{version}/parameters/{name}'
        mapper.connect(path,
                       controller=parameters_resource,
                       action='show_by_version',
                       conditions={'method': ['GET']})

        path = '/{tenant_id}/datastores/{datastore}/versions/{id}'
        mapper.connect(path + '/parameters',
                       controller=parameters_resource,
                       action='index',
                       conditions={'method': ['GET']})
        mapper.connect(path + '/parameters/{name}',
                       controller=parameters_resource,
                       action='show',
                       conditions={'method': ['GET']})

        configuration_resource = ConfigurationsController().create_resource()
        mapper.connect('/{tenant_id}/configurations',
                       controller=configuration_resource,
                       action='index',
                       conditions={'method': ['GET']})
        mapper.connect('/{tenant_id}/configurations',
                       controller=configuration_resource,
                       action='create',
                       conditions={'method': ['POST']})
        mapper.connect('/{tenant_id}/configurations/{id}',
                       controller=configuration_resource,
                       action='show',
                       conditions={'method': ['GET']})
        mapper.connect('/{tenant_id}/configurations/{id}/instances',
                       controller=configuration_resource,
                       action='instances',
                       conditions={'method': ['GET']})
        mapper.connect('/{tenant_id}/configurations/{id}',
                       controller=configuration_resource,
                       action='edit',
                       conditions={'method': ['PATCH']})
        mapper.connect('/{tenant_id}/configurations/{id}',
                       controller=configuration_resource,
                       action='update',
                       conditions={'method': ['PUT']})
        mapper.connect('/{tenant_id}/configurations/{id}',
                       controller=configuration_resource,
                       action='delete',
                       conditions={'method': ['DELETE']})


def app_factory(global_conf, **local_conf):
    return API()

########NEW FILE########
__FILENAME__ = apischema
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
from trove.common import cfg

CONF = cfg.CONF

url_ref = {
    "type": "string",
    "minLength": 8,
    "pattern": 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]'
               '|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
}

flavorref = {
    'oneOf': [
        url_ref,
        {
            "type": "string",
            "maxLength": 5,
            "pattern": "[0-9]+"
        },
        {
            "type": "integer"
        }]
}

volume_size = {
    "oneOf": [
        {
            "type": "integer",
            "minimum": 0
        },
        {
            "type": "string",
            "minLength": 1,
            "pattern": "[0-9]+"
        }]
}

non_empty_string = {
    "type": "string",
    "minLength": 1,
    "maxLength": 255,
    "pattern": "^.*[0-9a-zA-Z]+.*$"
}

host_string = {
    "type": "string",
    "minLength": 1,
    "pattern": "^[%]?[\w(-).]*[%]?$"
}

name_string = {
    "type": "string",
    "minLength": 1,
    "maxLength": 16,
    "pattern": "^.*[0-9a-zA-Z]+.*$"
}

uuid = {
    "type": "string",
    "minLength": 1,
    "maxLength": 64,
    "pattern": "^([0-9a-fA-F]){8}-([0-9a-fA-F]){4}-([0-9a-fA-F]){4}"
               "-([0-9a-fA-F]){4}-([0-9a-fA-F]){12}$"
}

volume = {
    "type": "object",
    "required": ["size"],
    "properties": {
        "size": volume_size,
        "required": True
    }
}

nics = {
    "type": "array",
    "items": {
        "type": "object",
    }
}

databases_ref_list = {
    "type": "array",
    "minItems": 0,
    "uniqueItems": True,
    "items": {
        "type": "object",
        "required": ["name"],
        "additionalProperties": True,
        "properties": {
            "name": non_empty_string
        }
    }
}

databases_ref_list_required = {
    "type": "array",
    "minItems": 0,
    "uniqueItems": True,
    "items": {
        "type": "object",
        "required": ["name"],
        "additionalProperties": True,
        "properties": {
            "name": non_empty_string
        }
    }
}

databases_ref = {
    "type": "object",
    "required": ["databases"],
    "additionalProperties": True,
    "properties": {
        "databases": databases_ref_list_required
    }
}

databases_def = {
    "type": "array",
    "minItems": 0,
    "items": {
        "type": "object",
        "required": ["name"],
        "additionalProperties": True,
        "properties": {
            "name": non_empty_string,
            "character_set": non_empty_string,
            "collate": non_empty_string
        }
    }
}

user_attributes = {
    "type": "object",
    "additionalProperties": True,
    "minProperties": 1,
    "properties": {
        "name": name_string,
        "password": non_empty_string,
        "host": host_string
    }
}


users_list = {
    "type": "array",
    "minItems": 0,
    "items": {
        "type": "object",
        "required": ["name", "password"],
        "additionalProperties": True,
        "properties": {
            "name": name_string,
            "password": non_empty_string,
            "host": host_string,
            "databases": databases_ref_list
        }
    }
}

configuration_id = {
    'oneOf': [
        uuid
    ]
}

instance = {
    "create": {
        "type": "object",
        "required": ["instance"],
        "additionalProperties": True,
        "properties": {
            "instance": {
                "type": "object",
                "required": ["name", "flavorRef",
                             "volume" if CONF.trove_volume_support else None],
                "additionalProperties": True,
                "properties": {
                    "name": non_empty_string,
                    "configuration_id": configuration_id,
                    "flavorRef": flavorref,
                    "volume": volume,
                    "databases": databases_def,
                    "users": users_list,
                    "restorePoint": {
                        "type": "object",
                        "required": ["backupRef"],
                        "additionalProperties": True,
                        "properties": {
                            "backupRef": uuid
                        }
                    },
                    "availability_zone": non_empty_string,
                    "datastore": {
                        "type": "object",
                        "additionalProperties": True,
                        "properties": {
                            "type": non_empty_string,
                            "version": non_empty_string
                        }
                    },
                    "nics": nics
                }
            }
        }
    },
    "action": {
        "resize": {
            "volume": {
                "type": "object",
                "required": ["resize"],
                "additionalProperties": True,
                "properties": {
                    "resize": {
                        "type": "object",
                        "required": ["volume"],
                        "additionalProperties": True,
                        "properties": {
                            "volume": volume
                        }
                    }
                }
            },
            'flavorRef': {
                "type": "object",
                "required": ["resize"],
                "additionalProperties": True,
                "properties": {
                    "resize": {
                        "type": "object",
                        "required": ["flavorRef"],
                        "additionalProperties": True,
                        "properties": {
                            "flavorRef": flavorref
                        }
                    }
                }
            }
        },
        "restart": {
            "type": "object",
            "required": ["restart"],
            "additionalProperties": True,
            "properties": {
                "restart": {
                    "type": "object"
                }
            }
        }
    }
}

mgmt_instance = {
    "action": {
        'migrate': {
            "type": "object",
            "required": ["migrate"],
            "additionalProperties": True,
            "properties": {
                "migrate": {
                    "type": "object"
                }
            }
        },
        "reboot": {
            "type": "object",
            "required": ["reboot"],
            "additionalProperties": True,
            "properties": {
                "reboot": {
                    "type": "object"
                }
            }
        },
        "stop": {
            "type": "object",
            "required": ["stop"],
            "additionalProperties": True,
            "properties": {
                "stop": {
                    "type": "object"
                }
            }
        }
    }
}

user = {
    "create": {
        "name": "users:create",
        "type": "object",
        "required": ["users"],
        "properties": {
            "users": users_list
        }
    },
    "update_all": {
        "users": {
            "type": "object",
            "required": ["users"],
            "additionalProperties": True,
            "properties": {
                "users": users_list
            }
        },
        "databases": databases_ref
    },
    "update": {
        "type": "object",
        "required": ["user"],
        "additionalProperties": True,
        "properties": {
            "user": user_attributes
        }
    }
}

dbschema = {
    "create": {
        "type": "object",
        "required": ["databases"],
        "additionalProperties": True,
        "properties": {
            "databases": databases_def
        }
    }
}

backup = {
    "create": {
        "name": "backup:create",
        "type": "object",
        "required": ["backup"],
        "properties": {
            "backup": {
                "type": "object",
                "required": ["instance", "name"],
                "properties": {
                    "description": non_empty_string,
                    "instance": uuid,
                    "name": non_empty_string,
                    "parent_id": uuid
                }
            }
        }
    }
}

configuration = {
    "create": {
        "name": "configuration:create",
        "type": "object",
        "required": ["configuration"],
        "properties": {
            "configuration": {
                "type": "object",
                "required": ["values", "name"],
                "properties": {
                    "description": non_empty_string,
                    "values": {
                        "type": "object",
                    },
                    "name": non_empty_string,
                    "datastore": {
                        "type": "object",
                        "additionalProperties": True,
                        "properties": {
                            "type": non_empty_string,
                            "version": non_empty_string
                        }
                    }
                }
            }
        }
    },
    "update": {
        "name": "configuration:update",
        "type": "object",
        "required": ["configuration"],
        "properties": {
            "configuration": {
                "type": "object",
                "required": [],
                "properties": {
                    "description": non_empty_string,
                    "values": {
                        "type": "object",
                    },
                    "name": non_empty_string
                }
            }
        }
    },
    "edit": {
        "name": "configuration:edit",
        "type": "object",
        "required": ["configuration"],
        "properties": {
            "configuration": {
                "type": "object",
                "required": [],
                "properties": {
                    "values": {
                        "type": "object",
                    }
                }
            }
        }
    }
}

account = {
    'create': {
        "type": "object",
        "name": "users",
        "required": ["users"],
        "additionalProperties": True,
        "properties": {
            "users": users_list
        }
    }
}

########NEW FILE########
__FILENAME__ = auth
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import re
import webob.exc
import wsgi

from trove.common import exception
from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _

LOG = logging.getLogger(__name__)


class AuthorizationMiddleware(wsgi.Middleware):

    def __init__(self, application, auth_providers, **local_config):
        self.auth_providers = auth_providers
        LOG.debug("Auth middleware providers: %s" % auth_providers)
        super(AuthorizationMiddleware, self).__init__(application,
                                                      **local_config)

    def process_request(self, request):
        roles = request.headers.get('X_ROLE', '').split(',')
        LOG.debug("Processing auth request with roles: %s" % roles)
        tenant_id = request.headers.get('X-Tenant-Id', None)
        LOG.debug("Processing auth request with tenant_id: %s" % tenant_id)
        for provider in self.auth_providers:
            provider.authorize(request, tenant_id, roles)

    @classmethod
    def factory(cls, global_config, **local_config):
        def _factory(app):
            LOG.debug("Created auth middleware with config: %s" %
                      local_config)
            return cls(app, [TenantBasedAuth()], **local_config)
        return _factory


class TenantBasedAuth(object):

    # The paths differ from melange, so the regex must differ as well,
    # trove starts with a tenant_id
    tenant_scoped_url = re.compile("/(?P<tenant_id>.*?)/.*")

    def authorize(self, request, tenant_id, roles):
        match_for_tenant = self.tenant_scoped_url.match(request.path_info)
        if (match_for_tenant and
                tenant_id == match_for_tenant.group('tenant_id')):
            LOG.debug(logging.mask_password(
                      _("Authorized tenant '%(tenant_id)s' request: "
                        "%(request)s") %
                      {'tenant_id': tenant_id, 'request': request}))
            return True
        msg = _("User with tenant id %s cannot access this resource")
        LOG.debug(msg % tenant_id)
        raise webob.exc.HTTPForbidden(msg)


def admin_context(f):
    """
    Verify that the current context has administrative access,
    or throw an exception. Trove API functions typically take the form
    function(self, req), or function(self, req, id).
    """
    def wrapper(*args, **kwargs):
        try:
            req = args[1]
            context = req.environ.get('trove.context')
        except Exception:
            raise exception.TroveError("Cannot load request context.")
        if not context.is_admin:
            raise exception.Forbidden("User does not have admin privileges.")
        return f(*args, **kwargs)
    return wrapper

########NEW FILE########
__FILENAME__ = cfg
# Copyright 2011 OpenStack Foundation
# Copyright 2014 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
"""Routines for configuring Trove."""

from oslo.config import cfg
from trove.openstack.common import log as logging

import os.path

UNKNOWN_SERVICE_ID = 'unknown-service-id-error'

path_opts = [
    cfg.StrOpt('pybasedir',
               default=os.path.abspath(os.path.join(os.path.dirname(__file__),
                                                    '../')),
               help='Directory where the trove python module is installed.'),
]

LOG = logging.getLogger(__name__)

common_opts = [
    cfg.StrOpt('sql_connection',
               default='sqlite:///trove_test.sqlite',
               help='SQL Connection.',
               secret=True),
    cfg.IntOpt('sql_idle_timeout', default=3600),
    cfg.BoolOpt('sql_query_log', default=False),
    cfg.IntOpt('bind_port', default=8779),
    cfg.StrOpt('api_extensions_path', default='trove/extensions/routes',
               help='Path to extensions.'),
    cfg.StrOpt('api_paste_config',
               default="api-paste.ini",
               help='File name for the paste.deploy config for trove-api.'),
    cfg.BoolOpt('trove_volume_support',
                default=True,
                help='Whether to provision a cinder volume for datadir.'),
    cfg.ListOpt('admin_roles', default=['admin']),
    cfg.BoolOpt('update_status_on_fail', default=False,
                help='If instance fails to become active, '
                     'taskmanager updates statuses, '
                     'service status = FAILED_TIMEOUT_GUESTAGENT, '
                     'instance task status = BUILDING_ERROR_TIMEOUT_GA.'),
    cfg.StrOpt('os_region_name',
               help='Region name of this node. Used when searching catalog.'),
    cfg.StrOpt('nova_compute_url', help='URL without the tenant segment.'),
    cfg.StrOpt('nova_compute_service_type', default='compute',
               help='Service type to use when searching catalog.'),
    cfg.StrOpt('cinder_url', help='URL without the tenant segment.'),
    cfg.StrOpt('cinder_service_type', default='volumev2',
               help='Service type to use when searching catalog.'),
    cfg.StrOpt('heat_url', help='URL without the tenant segment.'),
    cfg.StrOpt('heat_service_type', default='orchestration',
               help='Service type to use when searching catalog.'),
    cfg.StrOpt('swift_url', help='URL ending in AUTH_.'),
    cfg.StrOpt('swift_service_type', default='object-store',
               help='Service type to use when searching catalog.'),
    cfg.StrOpt('trove_auth_url', default='http://0.0.0.0:5000/v2.0'),
    cfg.StrOpt('host', default='0.0.0.0'),
    cfg.IntOpt('report_interval', default=10,
               help='The interval in seconds which periodic tasks are run.'),
    cfg.IntOpt('periodic_interval', default=60),
    cfg.BoolOpt('trove_dns_support', default=False),
    cfg.StrOpt('db_api_implementation', default='trove.db.sqlalchemy.api'),
    cfg.StrOpt('dns_driver', default='trove.dns.driver.DnsDriver'),
    cfg.StrOpt('dns_instance_entry_factory',
               default='trove.dns.driver.DnsInstanceEntryFactory'),
    cfg.StrOpt('dns_hostname', default=""),
    cfg.StrOpt('dns_account_id', default=""),
    cfg.StrOpt('dns_endpoint_url', default="0.0.0.0"),
    cfg.StrOpt('dns_service_type', default=""),
    cfg.StrOpt('dns_region', default=""),
    cfg.StrOpt('dns_auth_url', default=""),
    cfg.StrOpt('dns_domain_name', default=""),
    cfg.StrOpt('dns_username', default="", secret=True),
    cfg.StrOpt('dns_passkey', default="", secret=True),
    cfg.StrOpt('dns_management_base_url', default=""),
    cfg.IntOpt('dns_ttl', default=300),
    cfg.StrOpt('dns_domain_id', default=""),
    cfg.IntOpt('users_page_size', default=20),
    cfg.IntOpt('databases_page_size', default=20),
    cfg.IntOpt('instances_page_size', default=20),
    cfg.IntOpt('backups_page_size', default=20),
    cfg.IntOpt('configurations_page_size', default=20),
    cfg.ListOpt('ignore_users', default=['os_admin', 'root']),
    cfg.ListOpt('ignore_dbs', default=['lost+found',
                                       'mysql',
                                       'information_schema']),
    cfg.IntOpt('agent_call_low_timeout', default=5),
    cfg.IntOpt('agent_call_high_timeout', default=60),
    cfg.StrOpt('guest_id', default=None),
    cfg.IntOpt('state_change_wait_time', default=3 * 60),
    cfg.IntOpt('agent_heartbeat_time', default=10),
    cfg.IntOpt('num_tries', default=3),
    cfg.StrOpt('volume_fstype', default='ext3'),
    cfg.StrOpt('format_options', default='-m 5'),
    cfg.IntOpt('volume_format_timeout', default=120),
    cfg.StrOpt('mount_options', default='defaults,noatime'),
    cfg.IntOpt('max_instances_per_user', default=5,
               help='Default maximum number of instances per tenant.'),
    cfg.IntOpt('max_accepted_volume_size', default=5,
               help='Default maximum volume size for an instance.'),
    cfg.IntOpt('max_volumes_per_user', default=20,
               help='Default maximum volume capacity (in GB) spanning across '
                    'all trove volumes per tenant.'),
    cfg.IntOpt('max_backups_per_user', default=50,
               help='Default maximum number of backups created by a tenant.'),
    cfg.StrOpt('quota_driver',
               default='trove.quota.quota.DbQuotaDriver',
               help='Default driver to use for quota checks.'),
    cfg.StrOpt('taskmanager_queue', default='taskmanager'),
    cfg.StrOpt('conductor_queue', default='trove-conductor'),
    cfg.IntOpt('trove_conductor_workers', default=1),
    cfg.BoolOpt('use_nova_server_volume', default=False),
    cfg.BoolOpt('use_heat', default=False),
    cfg.StrOpt('device_path', default='/dev/vdb'),
    cfg.StrOpt('default_datastore', default=None,
               help="The default datastore id or name to use if one is not "
               "provided by the user. If the default value is None, the field "
               "becomes required in the instance-create request."),
    cfg.StrOpt('datastore_manager', default=None,
               help='Manager class in guestagent, setup by taskmanager on '
               'instance provision.'),
    cfg.StrOpt('block_device_mapping', default='vdb'),
    cfg.IntOpt('server_delete_time_out', default=60),
    cfg.IntOpt('volume_time_out', default=60),
    cfg.IntOpt('heat_time_out', default=60),
    cfg.IntOpt('reboot_time_out', default=60 * 2),
    cfg.IntOpt('dns_time_out', default=60 * 2),
    cfg.IntOpt('resize_time_out', default=60 * 10),
    cfg.IntOpt('revert_time_out', default=60 * 10),
    cfg.ListOpt('root_grant', default=['ALL']),
    cfg.BoolOpt('root_grant_option', default=True),
    cfg.IntOpt('default_password_length', default=36),
    cfg.IntOpt('http_get_rate', default=200),
    cfg.IntOpt('http_post_rate', default=200),
    cfg.IntOpt('http_delete_rate', default=200),
    cfg.IntOpt('http_put_rate', default=200),
    cfg.IntOpt('http_mgmt_post_rate', default=200),
    cfg.BoolOpt('hostname_require_ipv4', default=True,
                help="Require user hostnames to be IPv4 addresses."),
    cfg.BoolOpt('trove_security_groups_support', default=True),
    cfg.StrOpt('trove_security_group_name_prefix', default='SecGroup'),
    cfg.StrOpt('trove_security_group_rule_cidr', default='0.0.0.0/0'),
    cfg.IntOpt('trove_api_workers', default=None),
    cfg.IntOpt('usage_sleep_time', default=5,
               help='Time to sleep during the check active guest.'),
    cfg.StrOpt('region', default='LOCAL_DEV',
               help='The region this service is located.'),
    cfg.StrOpt('backup_runner',
               default='trove.guestagent.backup.backup_types.InnoBackupEx'),
    cfg.DictOpt('backup_runner_options', default={},
                help='Additional options to be passed to the backup runner.'),
    cfg.StrOpt('backup_strategy', default='InnoBackupEx',
               help='Default strategy to perform backups.'),
    cfg.StrOpt('backup_namespace',
               default='trove.guestagent.strategies.backup.mysql_impl',
               help='Namespace to load backup strategies from.'),
    cfg.StrOpt('restore_namespace',
               default='trove.guestagent.strategies.restore.mysql_impl',
               help='Namespace to load restore strategies from.'),
    cfg.DictOpt('backup_incremental_strategy',
                default={'InnoBackupEx': 'InnoBackupExIncremental'},
                help='Incremental Backup Runner based on the default'
                ' strategy. For strategies that do not implement an'
                ' incremental, the runner will use the default full backup.'),
    cfg.BoolOpt('verify_swift_checksum_on_restore', default=True,
                help='Enable verification of swift checksum before starting '
                'restore; makes sure the checksum of original backup matches '
                'checksum of the swift backup file.'),
    cfg.StrOpt('storage_strategy', default='SwiftStorage',
               help="Default strategy to store backups."),
    cfg.StrOpt('storage_namespace',
               default='trove.guestagent.strategies.storage.swift',
               help='Namespace to load the default storage strategy from.'),
    cfg.StrOpt('backup_swift_container', default='database_backups'),
    cfg.BoolOpt('backup_use_gzip_compression', default=True,
                help='Compress backups using gzip.'),
    cfg.BoolOpt('backup_use_openssl_encryption', default=True,
                help='Encrypt backups using OpenSSL.'),
    cfg.StrOpt('backup_aes_cbc_key', default='default_aes_cbc_key',
               help='Default OpenSSL aes_cbc key.'),
    cfg.BoolOpt('backup_use_snet', default=False,
                help='Send backup files over snet.'),
    cfg.IntOpt('backup_chunk_size', default=2 ** 16,
               help='Chunk size to stream to swift container.'
               ' This should be in multiples of 128 bytes, since this is the'
               ' size of an md5 digest block allowing the process to update'
               ' the file checksum during streaming.'
               ' See: http://stackoverflow.com/questions/1131220/'),
    cfg.IntOpt('backup_segment_max_size', default=2 * (1024 ** 3),
               help="Maximum size of each segment of the backup file."),
    cfg.StrOpt('remote_dns_client',
               default='trove.common.remote.dns_client'),
    cfg.StrOpt('remote_guest_client',
               default='trove.common.remote.guest_client'),
    cfg.StrOpt('remote_nova_client',
               default='trove.common.remote.nova_client'),
    cfg.StrOpt('remote_cinder_client',
               default='trove.common.remote.cinder_client'),
    cfg.StrOpt('remote_heat_client',
               default='trove.common.remote.heat_client'),
    cfg.StrOpt('remote_swift_client',
               default='trove.common.remote.swift_client'),
    cfg.StrOpt('exists_notification_transformer',
               help='Transformer for exists notifications.'),
    cfg.IntOpt('exists_notification_ticks', default=360,
               help='Number of report_intervals to wait between pushing '
                    'events (see report_interval).'),
    cfg.DictOpt('notification_service_id',
                default={'mysql': '2f3ff068-2bfb-4f70-9a9d-a6bb65bc084b',
                         'redis': 'b216ffc5-1947-456c-a4cf-70f94c05f7d0',
                         'cassandra': '459a230d-4e97-4344-9067-2a54a310b0ed',
                         'couchbase': 'fa62fe68-74d9-4779-a24e-36f19602c415',
                         'mongodb': 'c8c907af-7375-456f-b929-b637ff9209ee'},
                help='Unique ID to tag notification events.'),
    cfg.StrOpt('nova_proxy_admin_user', default='',
               help="Admin username used to connect to nova.", secret=True),
    cfg.StrOpt('nova_proxy_admin_pass', default='',
               help="Admin password used to connect to nova,", secret=True),
    cfg.StrOpt('nova_proxy_admin_tenant_name', default='',
               help="Admin tenant used to connect to nova.", secret=True),
    cfg.StrOpt('network_label_regex', default='^private$'),
    cfg.StrOpt('ip_regex', default=None),
    cfg.StrOpt('cloudinit_location', default='/etc/trove/cloudinit',
               help="Path to folder with cloudinit scripts."),
    cfg.StrOpt('guest_config',
               default='$pybasedir/etc/trove/trove-guestagent.conf.sample',
               help="Path to guestagent config file."),
    cfg.DictOpt('datastore_registry_ext', default=dict(),
                help='Extension for default datastore managers.'
                     ' Allows to use custom managers for each of'
                     ' datastore supported in trove.'),
    cfg.StrOpt('template_path',
               default='/etc/trove/templates/',
               help='Path which leads to datastore templates.'),
    cfg.BoolOpt('sql_query_logging', default=False,
                help='Allow insecure logging while '
                     'executing queries through SQLAlchemy.'),
    cfg.ListOpt('expected_filetype_suffixes',
                default=['json'],
                help='Filetype endings not to be reattached to an ID '
                     'by the utils method correct_id_with_req.'),
    cfg.ListOpt('default_neutron_networks',
                default=[],
                help='List of network IDs which should be attached'
                     ' to instance when networks are not specified'
                     ' in API call.'),
    cfg.IntOpt('max_header_line', default=16384,
               help='Maximum line size of message headers to be accepted. '
                    'max_header_line may need to be increased when using '
                    'large tokens (typically those generated by the '
                    'Keystone v3 API with big service catalogs).'),
]

# Datastore specific option groups

# Mysql
mysql_group = cfg.OptGroup(
    'mysql', title='MySQL options',
    help="Oslo option group designed for MySQL datastore")
mysql_opts = [
    cfg.ListOpt('tcp_ports', default=["3306"],
                help='List of TCP ports and/or port ranges to open'
                     ' in the security group (only applicable '
                     'if trove_security_groups_support is True).'),
    cfg.ListOpt('udp_ports', default=[],
                help='List of UDP ports and/or port ranges to open'
                     ' in the security group (only applicable '
                     'if trove_security_groups_support is True).'),
    cfg.StrOpt('backup_strategy', default='InnoBackupEx',
               help='Default strategy to perform backups.'),
    cfg.StrOpt('mount_point', default='/var/lib/mysql',
               help="Filesystem path for mounting "
                    "volumes if volume support is enabled."),
    cfg.BoolOpt('root_on_create', default=False,
                help='Enable the automatic creation of the root user for the '
                'service during instance-create. The generated password for '
                'the root user is immediately returned in the response of '
                "instance-create as the 'password' field."),
    cfg.IntOpt('usage_timeout', default=400,
               help='Timeout to wait for a guest to become active.'),
]

# Percona
percona_group = cfg.OptGroup(
    'percona', title='Percona options',
    help="Oslo option group designed for Percona datastore")
percona_opts = [
    cfg.ListOpt('tcp_ports', default=["3306"],
                help='List of TCP ports and/or port ranges to open'
                     ' in the security group (only applicable '
                     'if trove_security_groups_support is True).'),
    cfg.ListOpt('udp_ports', default=[],
                help='List of UDP ports and/or port ranges to open'
                     ' in the security group (only applicable '
                     'if trove_security_groups_support is True).'),
    cfg.StrOpt('backup_strategy', default='InnoBackupEx',
               help='Default strategy to perform backups.'),
    cfg.StrOpt('mount_point', default='/var/lib/mysql',
               help="Filesystem path for mounting "
                    "volumes if volume support is enabled."),
    cfg.BoolOpt('root_on_create', default=False,
                help='Enable the automatic creation of the root user for the '
                'service during instance-create. The generated password for '
                'the root user is immediately returned in the response of '
                "instance-create as the 'password' field."),
    cfg.IntOpt('usage_timeout', default=450,
               help='Timeout to wait for a guest to become active.'),
]

# Redis
redis_group = cfg.OptGroup(
    'redis', title='Redis options',
    help="Oslo option group designed for Redis datastore")
redis_opts = [
    cfg.ListOpt('tcp_ports', default=["6379"],
                help='List of TCP ports and/or port ranges to open'
                     ' in the security group (only applicable '
                     'if trove_security_groups_support is True).'),
    cfg.ListOpt('udp_ports', default=[],
                help='List of UDP ports and/or port ranges to open'
                     ' in the security group (only applicable '
                     'if trove_security_groups_support is True).'),
    cfg.StrOpt('backup_strategy', default=None,
               help='Default strategy to perform backups.'),
    cfg.StrOpt('mount_point', default='/var/lib/redis',
               help="Filesystem path for mounting "
               "volumes if volume support is enabled."),
    cfg.IntOpt('usage_timeout', default=450,
               help='Timeout to wait for a guest to become active.'),
]

# Cassandra
cassandra_group = cfg.OptGroup(
    'cassandra', title='Cassandra options',
    help="Oslo option group designed for Cassandra datastore")
cassandra_opts = [
    cfg.ListOpt('tcp_ports', default=["7000", "7001", "9042", "9160"],
                help='List of TCP ports and/or port ranges to open'
                     ' in the security group (only applicable '
                     'if trove_security_groups_support is True).'),
    cfg.ListOpt('udp_ports', default=[],
                help='List of UDP ports and/or port ranges to open'
                     ' in the security group (only applicable '
                     'if trove_security_groups_support is True).'),
    cfg.StrOpt('backup_strategy', default=None,
               help='Default strategy to perform backups.'),
    cfg.StrOpt('mount_point', default='/var/lib/cassandra',
               help="Filesystem path for mounting "
               "volumes if volume support is enabled."),
    cfg.IntOpt('usage_timeout', default=600,
               help='Timeout to wait for a guest to become active.'),
]

#Couchbase
couchbase_group = cfg.OptGroup(
    'couchbase', title='Couchbase options',
    help="Oslo option group designed for Couchbase datastore")
couchbase_opts = [
    cfg.ListOpt('tcp_ports',
                default=["8091", "8092", "4369", "11209-11211",
                         "21100-21199"],
                help='List of TCP ports and/or port ranges to open'
                     ' in the security group (only applicable '
                     'if trove_security_groups_support is True).'),
    cfg.ListOpt('udp_ports', default=[],
                help='List of UDP ports and/or port ranges to open'
                     ' in the security group (only applicable '
                     'if trove_security_groups_support is True).'),
    cfg.StrOpt('backup_strategy', default=None,
               help='Default strategy to perform backups.'),
    cfg.StrOpt('mount_point', default='/var/lib/couchbase',
               help="Filesystem path for mounting "
               "volumes if volume support is enabled."),
    cfg.IntOpt('usage_timeout', default=450,
               help='Timeout to wait for a guest to become active.'),
    cfg.BoolOpt('root_on_create', default=True,
                help='Enable the automatic creation of the root user for the '
                'service during instance-create. The generated password for '
                'the root user is immediately returned in the response of '
                "instance-create as the 'password' field."),
]

# MongoDB
mongodb_group = cfg.OptGroup(
    'mongodb', title='MongoDB options',
    help="Oslo option group designed for MongoDB datastore")
mongodb_opts = [
    cfg.ListOpt('tcp_ports', default=["2500", "27017"],
                help='List of TCP ports and/or port ranges to open'
                     ' in the security group (only applicable '
                     'if trove_security_groups_support is True).'),
    cfg.ListOpt('udp_ports', default=[],
                help='List of UPD ports and/or port ranges to open'
                     ' in the security group (only applicable '
                     'if trove_security_groups_support is True).'),
    cfg.StrOpt('backup_strategy', default=None,
               help='Default strategy to perform backups.'),
    cfg.StrOpt('mount_point', default='/var/lib/mongodb',
               help="Filesystem path for mounting "
               "volumes if volume support is enabled."),
    cfg.IntOpt('usage_timeout', default=450,
               help='Timeout to wait for a guest to become active.'),
]

CONF = cfg.CONF

CONF.register_opts(path_opts)
CONF.register_opts(common_opts)

CONF.register_group(mysql_group)
CONF.register_group(percona_group)
CONF.register_group(redis_group)
CONF.register_group(cassandra_group)
CONF.register_group(couchbase_group)
CONF.register_group(mongodb_group)

CONF.register_opts(mysql_opts, mysql_group)
CONF.register_opts(percona_opts, percona_group)
CONF.register_opts(redis_opts, redis_group)
CONF.register_opts(cassandra_opts, cassandra_group)
CONF.register_opts(couchbase_opts, couchbase_group)
CONF.register_opts(mongodb_opts, mongodb_group)


def custom_parser(parsername, parser):
    CONF.register_cli_opt(cfg.SubCommandOpt(parsername, handler=parser))


def parse_args(argv, default_config_files=None):
    cfg.CONF(args=argv[1:],
             project='trove',
             default_config_files=default_config_files)

########NEW FILE########
__FILENAME__ = configurations
#    Copyright 2014 Rackspace Hosting
#    All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import io
import json
from trove.common import cfg
from trove.common import exception
from trove.common import utils
from trove.openstack.common import log as logging
from six.moves import configparser


LOG = logging.getLogger(__name__)
CONF = cfg.CONF
ENV = utils.ENV


def _get_item(key, dictList):
    for item in dictList:
        if key == item.get('name'):
            return item


def do_configs_require_restart(overrides, datastore_manager='mysql'):
    rules = get_validation_rules(datastore_manager=datastore_manager)
    LOG.debug("overrides: %s" % overrides)
    LOG.debug("rules?: %s" % rules)
    for key in overrides.keys():
        rule = _get_item(key, rules['configuration-parameters'])
        LOG.debug("checking the rule: %s" % rule)
        if rule.get('restart_required'):
            return True
    return False


def get_validation_rules(datastore_manager='mysql'):
    try:
        config_location = ("%s/validation-rules.json" % datastore_manager)
        template = ENV.get_template(config_location)
        return json.loads(template.render())
    except Exception:
        msg = "This operation is not supported for this datastore at this time"
        LOG.exception(msg)
        raise exception.UnprocessableEntity(message=msg)


class MySQLConfParser(object):
    """MySQLConfParser"""
    def __init__(self, config):
        self.config = config

    def parse(self):
        good_cfg = self._remove_commented_lines(str(self.config))
        cfg_parser = configparser.ConfigParser()
        cfg_parser.readfp(io.BytesIO(str(good_cfg)))
        return cfg_parser.items("mysqld")

    def _remove_commented_lines(self, config_str):
        ret = []
        for line in config_str.splitlines():
            line_clean = line.strip()
            if line_clean.startswith('#'):
                continue
            elif line_clean.startswith('!'):
                continue
            elif line_clean.startswith(';'):
                continue
            # python 2.6 configparser doesnt like params without values
            elif line_clean.startswith('[') and line_clean.endswith(']'):
                ret.append(line_clean)
            elif line_clean and "=" not in line_clean:
                LOG.debug("fixing line without '=' in it: %s" % line_clean)
                ret.append(line_clean + " = 1")
            else:
                ret.append(line_clean)
        rendered = "\n".join(ret)
        return rendered

########NEW FILE########
__FILENAME__ = context
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Simple class that stores security context information in the web request.

Projects should subclass this class if they wish to enhance the request
context or provide additional information in their specific WSGI pipeline.
"""

from trove.openstack.common import context
from trove.openstack.common import local


class TroveContext(context.RequestContext):
    """
    Stores information about the security context under which the user
    accesses the system, as well as additional request information.
    """
    def __init__(self, **kwargs):
        self.limit = kwargs.pop('limit', None)
        self.marker = kwargs.pop('marker', None)
        self.service_catalog = kwargs.pop('service_catalog', None)
        super(TroveContext, self).__init__(**kwargs)

        if not hasattr(local.store, 'context'):
            self.update_store()

    def to_dict(self):
        parent_dict = super(TroveContext, self).to_dict()
        parent_dict.update({'limit': self.limit,
                            'marker': self.marker,
                            'service_catalog': self.service_catalog
                            })
        return parent_dict

    def update_store(self):
        local.store.context = self

    @classmethod
    def from_dict(cls, values):
        return cls(**values)

########NEW FILE########
__FILENAME__ = debug_utils
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.
#

"""Help utilities for debugging"""

import sys

from oslo.config import cfg

from trove.openstack.common import log


LOG = log.getLogger(__name__)
CONF = cfg.CONF

__debug_state = None

pydev_debug_opts = [
    cfg.StrOpt("pydev_debug",
               choices=("disabled", "enabled", "auto"),
               default="disabled",
               help="Enable or disable pydev remote debugging. "
                    "If value is 'auto' tries to connect to remote "
                    "debugger server, but in case of error "
                    "continues running with debugging disabled."),

    cfg.StrOpt("pydev_debug_host",
               help="Pydev debug server host (localhost by default)."),

    cfg.IntOpt("pydev_debug_port",
               help="Pydev debug server port (5678 by default)."),

    cfg.StrOpt("pydev_path",
               help="Set path to pydevd library, used if pydevd is "
                    "not found in python sys.path.")
]

CONF.register_opts(pydev_debug_opts)


def setup():
    """
    Analyze configuration for pydev remote debugging and establish
    connection to remote debugger service if needed

    @return: True if remote debugging was enabled successfully,
        otherwise - False
    """

    global __debug_state

    if CONF.pydev_debug == "enabled":
        __debug_state = __setup_remote_pydev_debug(
            pydev_debug_host=CONF.pydev_debug_host,
            pydev_debug_port=CONF.pydev_debug_port,
            pydev_path=CONF.pydev_path)
    elif CONF.pydev_debug == "auto":
        __debug_state = __setup_remote_pydev_debug_safe(
            pydev_debug_host=CONF.pydev_debug_host,
            pydev_debug_port=CONF.pydev_debug_port,
            pydev_path=CONF.pydev_path)
    else:
        __debug_state = False


def enabled():
    """
    @return: True if connection to remote debugger established, otherwise False
    """
    assert __debug_state is not None, ("debug_utils are not initialized. "
                                       "Please call setup() method first")
    return __debug_state


def __setup_remote_pydev_debug_safe(pydev_debug_host=None,
                                    pydev_debug_port=5678, pydev_path=None):
    """
    Safe version of __setup_remote_pydev_debug method. In error case returns
    False as result instead of Exception raising

    @see: __setup_remote_pydev_debug
    """

    try:
        return __setup_remote_pydev_debug(
            pydev_debug_host=pydev_debug_host,
            pydev_debug_port=pydev_debug_port,
            pydev_path=pydev_path)
    except Exception as e:
        LOG.info("Cann't connect to remote debug server. Continue working in "
                 "standard mode. Error: %s", e)
        return False


def __setup_remote_pydev_debug(pydev_debug_host=None, pydev_debug_port=None,
                               pydev_path=None):
    """
    Method connects to remote debug server, and attach current thread trace
    to debugger. Also thread.start_new_thread thread.start_new are patched to
    enable debugging of new threads

    @param pydev_debug_host: remote debug server host hame, 'localhost'
        if not specified or None
    @param pydev_debug_port: remote debug server port, 5678
        if not specified or None
    @param pydev_path: optional path to pydevd library, used it pydevd is not
        found in python sys.path
    @return: True if debugging initialized,
        otherwise exception should be raised
    """

    if pydev_debug_port is None:
        pydev_debug_port = 5678
    try:
        import pydevd
        LOG.debug("pydevd module was imported from system path")
    except ImportError:
        LOG.debug("Cann't load pydevd module from system path. Try load it "
                  "from pydev_path: %s", pydev_path)
        assert pydev_path, "pydev_path is not set"
        if pydev_path not in sys.path:
            sys.path.append(pydev_path)
        import pydevd
        LOG.debug("pydevd module was imported from pydev_path: %s", pydev_path)
    pydevd.settrace(
        host=pydev_debug_host,
        port=pydev_debug_port,
        stdoutToServer=True,
        stderrToServer=True,
        trace_only_current_thread=False,
        suspend=False,
    )
    return True

########NEW FILE########
__FILENAME__ = exception
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
"""I totally stole most of this from melange, thx guys!!!"""


import re

from trove.openstack.common import log as logging
from trove.openstack.common import exception as openstack_exception
from trove.openstack.common import processutils
from trove.openstack.common.gettextutils import _


ClientConnectionError = openstack_exception.ClientConnectionError
ProcessExecutionError = processutils.ProcessExecutionError
DatabaseMigrationError = openstack_exception.DatabaseMigrationError
LOG = logging.getLogger(__name__)
wrap_exception = openstack_exception.wrap_exception


def safe_fmt_string(text):
    return re.sub(r'%([0-9]+)', r'\1', text)


class TroveError(openstack_exception.OpenstackException):
    """Base exception that all custom trove app exceptions inherit from."""
    internal_message = None

    def __init__(self, message=None, **kwargs):
        if message is not None:
            self.message = message
        if self.internal_message is not None:
            try:
                LOG.error(safe_fmt_string(self.internal_message) % kwargs)
            except Exception:
                LOG.error(self.internal_message)
        self.message = safe_fmt_string(self.message)
        super(TroveError, self).__init__(**kwargs)


class DBConstraintError(TroveError):

    message = _("Failed to save %(model_name)s because: %(error)s")


class InvalidRPCConnectionReuse(TroveError):

    message = _("Invalid RPC Connection Reuse")


class NotFound(TroveError):

    message = _("Resource %(uuid)s cannot be found")


class FlavorNotFound(TroveError):

    message = _("Resource %(uuid)s cannot be found")


class UserNotFound(NotFound):

    message = _("User %(uuid)s cannot be found on the instance.")


class DatabaseNotFound(NotFound):

    message = _("Database %(uuid)s cannot be found on the instance.")


class ComputeInstanceNotFound(NotFound):

    internal_message = _("Cannot find compute instance %(server_id)s for "
                         "instance %(instance_id)s.")

    message = _("Resource %(instance_id)s can not be retrieved.")


class DnsRecordNotFound(NotFound):

    message = _("DnsRecord with name= %(name)s not found.")


class DatastoreNotFound(NotFound):

    message = _("Datastore '%(datastore)s' cannot be found.")


class DatastoreVersionNotFound(NotFound):

    message = _("Datastore version '%(version)s' cannot be found.")


class DatastoresNotFound(NotFound):

    message = _("Datastores cannot be found.")


class DatastoreNoVersion(TroveError):

    message = _("Datastore '%(datastore)s' has no version '%(version)s'.")


class DatastoreVersionInactive(TroveError):

    message = _("Datastore version '%(version)s' is not active.")


class DatastoreDefaultDatastoreNotFound(TroveError):

    message = _("Please specify datastore.")


class DatastoreDefaultVersionNotFound(TroveError):

    message = _("Default version for datastore '%(datastore)s' not found.")


class DatastoreOperationNotSupported(TroveError):

    message = _("The '%(operation)s' operation is not supported for "
                "the '%(datastore)s' datastore.")


class NoUniqueMatch(TroveError):

    message = _("Multiple matches found for '%(name)s', i"
                "use an UUID to be more specific.")


class OverLimit(TroveError):

    internal_message = _("The server rejected the request due to its size or "
                         "rate.")


class QuotaExceeded(TroveError):

    message = _("Quota exceeded for resources: %(overs)s")


class VolumeQuotaExceeded(QuotaExceeded):

    message = _("Instance volume quota exceeded.")


class GuestError(TroveError):

    message = _("An error occurred communicating with the guest: "
                "%(original_message)s.")


class GuestTimeout(TroveError):

    message = _("Timeout trying to connect to the Guest Agent.")


class BadRequest(TroveError):

    message = _("The server could not comply with the request since it is "
                "either malformed or otherwise incorrect.")


class MissingKey(BadRequest):

    message = _("Required element/key - %(key)s was not specified")


class DatabaseAlreadyExists(BadRequest):

    message = _('A database with the name "%(name)s" already exists.')


class UserAlreadyExists(BadRequest):

    message = _('A user with the name "%(name)s" already exists.')


class InstanceAssignedToConfiguration(BadRequest):

    message = _('A configuration group cannot be deleted if it is '
                'associated with one or more non-terminated instances. '
                'Detach the configuration group from all non-terminated '
                'instances and please try again.')


class UnprocessableEntity(TroveError):

    message = _("Unable to process the contained request")


class CannotResizeToSameSize(TroveError):

    message = _("When resizing, instances must change size!")


class VolumeAttachmentsNotFound(NotFound):

    message = _("Cannot find the volumes attached to compute "
                "instance %(server_id)")


class VolumeCreationFailure(TroveError):

    message = _("Failed to create a volume in Nova.")


class VolumeSizeNotSpecified(BadRequest):

    message = _("Volume size was not specified.")


class LocalStorageNotSpecified(BadRequest):

    message = _("Local storage not specified in flavor ID: %(flavor)s.")


class LocalStorageNotSupported(TroveError):

    message = _("Local storage support is not enabled.")


class VolumeNotSupported(TroveError):

    message = _("Volume support is not enabled.")


class TaskManagerError(TroveError):

    message = _("An error occurred communicating with the task manager: "
                "%(original_message)s.")


class BadValue(TroveError):

    message = _("Value could not be converted: %(msg)s")


class PollTimeOut(TroveError):

    message = _("Polling request timed out.")


class Forbidden(TroveError):

    message = _("User does not have admin privileges.")


class InvalidModelError(TroveError):

    message = _("The following values are invalid: %(errors)s")


class ModelNotFoundError(NotFound):

    message = _("Not Found")


class UpdateGuestError(TroveError):

    message = _("Failed to update instances")


class ConfigNotFound(NotFound):

    message = _("Config file not found")


class PasteAppNotFound(NotFound):

    message = _("Paste app not found.")


class QuotaNotFound(NotFound):
    message = _("Quota could not be found")


class TenantQuotaNotFound(QuotaNotFound):
    message = _("Quota for tenant %(tenant_id)s could not be found.")


class QuotaResourceUnknown(QuotaNotFound):
    message = _("Unknown quota resources %(unknown)s.")


class BackupUploadError(TroveError):
    message = _("Unable to upload Backup onto swift")


class BackupDownloadError(TroveError):
    message = _("Unable to download Backup from swift")


class BackupCreationError(TroveError):
    message = _("Unable to create Backup")


class BackupUpdateError(TroveError):
    message = _("Unable to update Backup table in db")


class SecurityGroupCreationError(TroveError):

    message = _("Failed to create Security Group.")


class SecurityGroupDeletionError(TroveError):

    message = _("Failed to delete Security Group.")


class SecurityGroupRuleCreationError(TroveError):

    message = _("Failed to create Security Group Rule.")


class SecurityGroupRuleDeletionError(TroveError):

    message = _("Failed to delete Security Group Rule.")


class MalformedSecurityGroupRuleError(TroveError):

    message = _("Error creating security group rules."
                " Malformed port(s). Port(s) is not integer."
                " FromPort = %(from)s greater than ToPort = %(to)s")


class BackupNotCompleteError(TroveError):

    message = _("Unable to create instance because backup %(backup_id)s is "
                "not completed")


class BackupFileNotFound(NotFound):
    message = _("Backup file in %(location)s was not found in the object "
                "storage.")


class BackupDatastoreVersionMismatchError(TroveError):
    message = _("The datastore-version from which the backup was"
                " taken, %(version1)s, does not match the destination"
                " datastore-version of %(version2)s")


class SwiftAuthError(TroveError):
    message = _("Swift account not accessible for tenant %(tenant_id)s.")


class DatabaseForUserNotInDatabaseListError(TroveError):
    message = _("The request indicates that user %(user)s should have access "
                "to database %(database)s, but database %(database)s is not "
                "included in the initial databases list.")


class DatabaseInitialDatabaseDuplicateError(TroveError):
    message = _("Two or more databases share the same name in the initial "
                "databases list. Please correct the names or remove the "
                "duplicate entries.")


class DatabaseInitialUserDuplicateError(TroveError):
    message = _("Two or more users share the same name and host in the "
                "initial users list. Please correct the names or remove the "
                "duplicate entries.")


class RestoreBackupIntegrityError(TroveError):
    message = _("Current Swift object checksum does not match original "
                "checksum for backup %(backup_id)s.")


class ConfigKeyNotFound(NotFound):
    message = _("%(key)s is not a supported configuration parameter")


class NoConfigParserFound(NotFound):
    message = _("No configuration parser found for datastore "
                "%(datastore_manager)s")


class ConfigurationDatastoreNotMatchInstance(TroveError):
    message = _("Datastore Version on Configuration "
                "%(config_datastore_version)s does not "
                "match the Datastore Version on the instance "
                "%(instance_datastore_version)s.")


class ConfigurationParameterDeleted(object):
    message = _("%(parameter_name)s parameter can no longer be "
                " set as of %(parameter_deleted_at)s")


class ConfigurationAlreadyAttached(TroveError):
    message = _("Instance %(instance_id)s already has a "
                "Configuration Group attached: %(configuration_id)s.")


class InvalidInstanceState(TroveError):
    message = _("The operation you have requested cannot be executed because "
                "the instance status is currently: %(status)s")


class RegionAmbiguity(TroveError):
    """Found more than one matching endpoint in Service Catalog."""
    message = _("Multiple matches for service_type=%(service_type)s and "
                "endpoint_region=%(endpoint_region)s. This generally means "
                "that a region is required and you have not supplied one.")


class NoServiceEndpoint(TroveError):
    """Could not find requested endpoint in Service Catalog."""
    message = ("Endpoint not found for service_type=%(service_type)s, "
               "endpoint_type=%(endpoint_type)s, "
               "endpoint_region=%(endpoint_region)s")


class EmptyCatalog(NoServiceEndpoint):
    """The service catalog is empty."""
    message = 'Empty catalog'

########NEW FILE########
__FILENAME__ = extensions
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import routes
from trove.openstack.common import log as logging

from trove.openstack.common import extensions
from trove.common import cfg
from trove.common import wsgi

LOG = logging.getLogger(__name__)

ExtensionsDescriptor = extensions.ExtensionDescriptor

CONF = cfg.CONF


class ResourceExtension(extensions.ResourceExtension):
    def __init__(self, collection, controller, parent=None,
                 collection_actions=None, member_actions=None,
                 deserializer=None, serializer=None):
        super(ResourceExtension, self).__init__(
            collection, controller,
            parent=parent,
            collection_actions=collection_actions,
            member_actions=member_actions,
            deserializer=wsgi.RequestDeserializer(),
            serializer=wsgi.TroveResponseSerializer())


class TroveExtensionMiddleware(extensions.ExtensionMiddleware):

    def __init__(self, application, ext_mgr=None):
        ext_mgr = (ext_mgr or
                   ExtensionManager(CONF.api_extensions_path))
        mapper = routes.Mapper()

        # extended resources
        for resource_ext in ext_mgr.get_resources():
            LOG.debug('Extended resource: %s', resource_ext.collection)
            # The only difference here is that we are using our common
            # wsgi.Resource instead of the openstack common wsgi.Resource
            exception_map = None
            if hasattr(resource_ext.controller, 'exception_map'):
                exception_map = resource_ext.controller.exception_map
            controller_resource = wsgi.Resource(resource_ext.controller,
                                                resource_ext.deserializer,
                                                resource_ext.serializer,
                                                exception_map)

            self._map_custom_collection_actions(resource_ext, mapper,
                                                controller_resource)
            kargs = dict(controller=controller_resource,
                         collection=resource_ext.collection_actions,
                         member=resource_ext.member_actions)
            if resource_ext.parent:
                kargs['parent_resource'] = resource_ext.parent
            mapper.resource(resource_ext.collection,
                            resource_ext.collection, **kargs)

        # extended actions
        action_resources = self._action_ext_resources(application, ext_mgr,
                                                      mapper)
        for action in ext_mgr.get_actions():
            LOG.debug('Extended action: %s', action.action_name)
            resource = action_resources[action.collection]
            resource.add_action(action.action_name, action.handler)

        # extended requests
        req_controllers = self._request_ext_resources(application, ext_mgr,
                                                      mapper)
        for request_ext in ext_mgr.get_request_extensions():
            LOG.debug('Extended request: %s', request_ext.key)
            controller = req_controllers[request_ext.key]
            controller.add_handler(request_ext.handler)

        self._router = routes.middleware.RoutesMiddleware(self._dispatch,
                                                          mapper)

        super(extensions.ExtensionMiddleware, self).__init__(application)


def factory(global_config, **local_config):
    """Paste factory."""
    def _factory(app):
        extensions.DEFAULT_XMLNS = "http://docs.openstack.org/trove"
        ext_mgr = extensions.ExtensionManager(CONF.api_extensions_path)
        return TroveExtensionMiddleware(app, ext_mgr)
    return _factory

########NEW FILE########
__FILENAME__ = instance
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


class ServiceStatus(object):
    """Represents the status of the app and in some rare cases the agent.

    Code and description are what is stored in the database. "api_status"
    refers to the status which comes back from the REST API.
    """
    _lookup = {}

    def __init__(self, code, description, api_status):
        self._code = code
        self._description = description
        self._api_status = api_status
        ServiceStatus._lookup[code] = self

    @property
    def action_is_allowed(self):
        allowed_statuses = [
            ServiceStatuses.RUNNING._code,
            ServiceStatuses.SHUTDOWN._code,
            ServiceStatuses.CRASHED._code,
            ServiceStatuses.BLOCKED._code,
        ]
        return self._code in allowed_statuses

    @property
    def api_status(self):
        return self._api_status

    @property
    def code(self):
        return self._code

    @property
    def description(self):
        return self._description

    def __eq__(self, other):
        if not isinstance(other, ServiceStatus):
            return False
        return self.code == other.code

    @staticmethod
    def from_code(code):
        if code not in ServiceStatus._lookup:
            msg = 'Status code %s is not a valid ServiceStatus integer code.'
            raise ValueError(msg % code)
        return ServiceStatus._lookup[code]

    @staticmethod
    def from_description(desc):
        all_items = ServiceStatus._lookup.items()
        status_codes = [code for (code, status) in all_items
                        if status.description == desc]
        if not status_codes:
            msg = 'Status description %s is not a valid ServiceStatus.'
            raise ValueError(msg % desc)
        return ServiceStatus._lookup[status_codes[0]]

    @staticmethod
    def is_valid_code(code):
        return code in ServiceStatus._lookup

    def __str__(self):
        return self._description

    def __repr__(self):
        return self._api_status


class ServiceStatuses(object):
    RUNNING = ServiceStatus(0x01, 'running', 'ACTIVE')
    BLOCKED = ServiceStatus(0x02, 'blocked', 'BLOCKED')
    PAUSED = ServiceStatus(0x03, 'paused', 'SHUTDOWN')
    SHUTDOWN = ServiceStatus(0x04, 'shutdown', 'SHUTDOWN')
    CRASHED = ServiceStatus(0x06, 'crashed', 'SHUTDOWN')
    FAILED = ServiceStatus(0x08, 'failed to spawn', 'FAILED')
    BUILDING = ServiceStatus(0x09, 'building', 'BUILD')
    UNKNOWN = ServiceStatus(0x16, 'unknown', 'ERROR')
    NEW = ServiceStatus(0x17, 'new', 'NEW')
    DELETED = ServiceStatus(0x05, 'deleted', 'DELETED')
    FAILED_TIMEOUT_GUESTAGENT = ServiceStatus(0x18, 'guestagent error',
                                              'ERROR')

# Dissuade further additions at run-time.
ServiceStatus.__init__ = None

########NEW FILE########
__FILENAME__ = limits
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Module dedicated functions/classes dealing with rate limiting requests.
"""

import collections
import copy
import httplib
import math
import re
import time
import webob.dec
import webob.exc

from trove.common import cfg
from trove.common import wsgi as base_wsgi
from trove.openstack.common import importutils
from trove.openstack.common import jsonutils
from trove.openstack.common import wsgi
from trove.openstack.common.gettextutils import _


CONF = cfg.CONF

# Convenience constants for the limits dictionary passed to Limiter().
PER_SECOND = 1
PER_MINUTE = 60
PER_HOUR = 60 * 60
PER_DAY = 60 * 60 * 24


class Limit(object):
    """
    Stores information about a limit for HTTP requests.
    """

    UNITS = {
        1: "SECOND",
        60: "MINUTE",
        60 * 60: "HOUR",
        60 * 60 * 24: "DAY",
    }

    UNIT_MAP = dict([(v, k) for k, v in UNITS.items()])

    def __init__(self, verb, uri, regex, value, unit):
        """
        Initialize a new `Limit`.

        @param verb: HTTP verb (POST, PUT, etc.)
        @param uri: Human-readable URI
        @param regex: Regular expression format for this limit
        @param value: Integer number of requests which can be made
        @param unit: Unit of measure for the value parameter
        """
        self.verb = verb
        self.uri = uri
        self.regex = regex
        self.value = int(value)
        self.unit = unit
        self.unit_string = self.display_unit().lower()
        self.remaining = int(value)

        if value <= 0:
            raise ValueError("Limit value must be > 0")

        self.last_request = None
        self.next_request = None

        self.water_level = 0
        self.capacity = self.unit
        self.request_value = float(self.capacity) / float(self.value)
        msg = _("Only %(value)s %(verb)s request(s) can be "
                "made to %(uri)s every %(unit_string)s.")
        self.error_message = msg % self.__dict__

    def __call__(self, verb, url):
        """
        Represents a call to this limit from a relevant request.

        @param verb: string http verb (POST, GET, etc.)
        @param url: string URL
        """
        if self.verb != verb or not re.match(self.regex, url):
            return

        now = self._get_time()

        if self.last_request is None:
            self.last_request = now

        leak_value = now - self.last_request

        self.water_level -= leak_value
        self.water_level = max(self.water_level, 0)
        self.water_level += self.request_value

        difference = self.water_level - self.capacity

        self.last_request = now

        if difference > 0:
            self.water_level -= self.request_value
            self.next_request = now + difference
            return difference

        cap = self.capacity
        water = self.water_level
        val = self.value

        self.remaining = math.floor(((cap - water) / cap) * val)
        self.next_request = now

    def _get_time(self):
        """Retrieve the current time. Broken out for testability."""
        return time.time()

    def display_unit(self):
        """Display the string name of the unit."""
        return self.UNITS.get(self.unit, "UNKNOWN")

    def display(self):
        """Return a useful representation of this class."""
        return {
            "verb": self.verb,
            "URI": self.uri,
            "regex": self.regex,
            "value": self.value,
            "remaining": int(self.remaining),
            "unit": self.display_unit(),
            "resetTime": int(self.next_request or self._get_time()),
        }

# "Limit" format is a dictionary with the HTTP verb, human-readable URI,
# a regular-expression to match, value and unit of measure (PER_DAY, etc.)
DEFAULT_LIMITS = [
    Limit("POST", "*", ".*", CONF.http_post_rate, PER_MINUTE),
    Limit("PUT", "*", ".*", CONF.http_put_rate, PER_MINUTE),
    Limit("DELETE", "*", ".*", CONF.http_delete_rate, PER_MINUTE),
    Limit("GET", "*", ".*", CONF.http_get_rate, PER_MINUTE),
    Limit("POST", "*/mgmt", "^/mgmt", CONF.http_mgmt_post_rate, PER_MINUTE),
]


class RateLimitingMiddleware(base_wsgi.TroveMiddleware):
    """
    Rate-limits requests passing through this middleware. All limit information
    is stored in memory for this implementation.
    """

    def __init__(self, application, limits=None, limiter=None, **kwargs):
        """
        Initialize new `RateLimitingMiddleware`, which wraps the given WSGI
        application and sets up the given limits.

        @param application: WSGI application to wrap
        @param limits: String describing limits
        @param limiter: String identifying class for representing limits

        Other parameters are passed to the constructor for the limiter.
        """
        base_wsgi.Middleware.__init__(self, application)

        # Select the limiter class
        if limiter is None:
            limiter = Limiter
        else:
            limiter = importutils.import_class(limiter)

        # Parse the limits, if any are provided
        if limits is not None:
            limits = limiter.parse_limits(limits)

        self._limiter = limiter(limits or DEFAULT_LIMITS, **kwargs)

    @webob.dec.wsgify(RequestClass=wsgi.Request)
    def __call__(self, req):
        """
        Represents a single call through this middleware. We should record the
        request if we have a limit relevant to it. If no limit is relevant to
        the request, ignore it.

        If the request should be rate limited, return a fault telling the user
        they are over the limit and need to retry later.
        """
        verb = req.method
        url = req.url
        context = req.environ.get(base_wsgi.CONTEXT_KEY)

        tenant_id = None
        if context:
            tenant_id = context.tenant

        delay, error = self._limiter.check_for_delay(verb, url, tenant_id)

        if delay:
            msg = _("This request was rate-limited.")
            retry = time.time() + delay
            return base_wsgi.OverLimitFault(msg, error, retry)

        req.environ["trove.limits"] = self._limiter.get_limits(tenant_id)

        return self.application


class Limiter(object):
    """
    Rate-limit checking class which handles limits in memory.
    """

    def __init__(self, limits, **kwargs):
        """
        Initialize the new `Limiter`.

        @param limits: List of `Limit` objects
        """
        self.limits = copy.deepcopy(limits)
        self.levels = collections.defaultdict(lambda: copy.deepcopy(limits))

        # Pick up any per-user limit information
        for key, value in kwargs.items():
            if key.startswith('user:'):
                username = key[5:]
                self.levels[username] = self.parse_limits(value)

    def get_limits(self, username=None):
        """
        Return the limits for a given user.
        """
        return [limit.display() for limit in self.levels[username]]

    def check_for_delay(self, verb, url, username=None):
        """
        Check the given verb/user/user triplet for limit.

        @return: Tuple of delay (in seconds) and error message (or None, None)
        """
        delays = []

        for limit in self.levels[username]:
            delay = limit(verb, url)
            if delay:
                delays.append((delay, limit.error_message))

        if delays:
            delays.sort()
            return delays[0]

        return None, None

    # This was ported from nova.
    # Keeping it as a static method for the sake of consistency
    #
    # Note: This method gets called before the class is instantiated,
    # so this must be either a static method or a class method.  It is
    # used to develop a list of limits to feed to the constructor.  We
    # put this in the class so that subclasses can override the
    # default limit parsing.
    @staticmethod
    def parse_limits(limits):
        """
        Convert a string into a list of Limit instances.  This
        implementation expects a semicolon-separated sequence of
        parenthesized groups, where each group contains a
        comma-separated sequence consisting of HTTP method,
        user-readable URI, a URI reg-exp, an integer number of
        requests which can be made, and a unit of measure.  Valid
        values for the latter are "SECOND", "MINUTE", "HOUR", and
        "DAY".

        @return: List of Limit instances.
        """

        # Handle empty limit strings
        limits = limits.strip()
        if not limits:
            return []

        # Split up the limits by semicolon
        result = []
        for group in limits.split(';'):
            group = group.strip()
            if group[:1] != '(' or group[-1:] != ')':
                raise ValueError("Limit rules must be surrounded by "
                                 "parentheses")
            group = group[1:-1]

            # Extract the Limit arguments
            args = [a.strip() for a in group.split(',')]
            if len(args) != 5:
                raise ValueError("Limit rules must contain the following "
                                 "arguments: verb, uri, regex, value, unit")

            # Pull out the arguments
            verb, uri, regex, value, unit = args

            # Upper-case the verb
            verb = verb.upper()

            # Convert value--raises ValueError if it's not integer
            value = int(value)

            # Convert unit
            unit = unit.upper()
            if unit not in Limit.UNIT_MAP:
                raise ValueError("Invalid units specified")
            unit = Limit.UNIT_MAP[unit]

            # Build a limit
            result.append(Limit(verb, uri, regex, value, unit))

        return result


class WsgiLimiter(object):
    """
    Rate-limit checking from a WSGI application. Uses an in-memory `Limiter`.

    To use, POST ``/<username>`` with JSON data such as::

        {
            "verb" : GET,
            "path" : "/servers"
        }

    and receive a 204 No Content, or a 403 Forbidden with an X-Wait-Seconds
    header containing the number of seconds to wait before the action would
    succeed.
    """

    def __init__(self, limits=None):
        """
        Initialize the new `WsgiLimiter`.

        @param limits: List of `Limit` objects
        """
        self._limiter = Limiter(limits or DEFAULT_LIMITS)

    @webob.dec.wsgify(RequestClass=wsgi.Request)
    def __call__(self, request):
        """
        Handles a call to this application. Returns 204 if the request is
        acceptable to the limiter, else a 403 is returned with a relevant
        header indicating when the request *will* succeed.
        """
        if request.method != "POST":
            raise webob.exc.HTTPMethodNotAllowed()

        try:
            info = dict(jsonutils.loads(request.body))
        except ValueError:
            raise webob.exc.HTTPBadRequest()

        username = request.path_info_pop()
        verb = info.get("verb")
        path = info.get("path")

        delay, error = self._limiter.check_for_delay(verb, path, username)

        if delay:
            headers = {"X-Wait-Seconds": "%.2f" % delay}
            return webob.exc.HTTPForbidden(headers=headers, explanation=error)
        else:
            return webob.exc.HTTPNoContent()


class WsgiLimiterProxy(object):
    """
    Rate-limit requests based on answers from a remote source.
    """

    def __init__(self, limiter_address):
        """
        Initialize the new `WsgiLimiterProxy`.

        @param limiter_address: IP/port combination of where to request limit
        """
        self.limiter_address = limiter_address

    def check_for_delay(self, verb, path, username=None):
        body = jsonutils.dumps({"verb": verb, "path": path})
        headers = {"Content-Type": "application/json"}

        conn = httplib.HTTPConnection(self.limiter_address)

        if username:
            conn.request("POST", "/%s" % (username), body, headers)
        else:
            conn.request("POST", "/", body, headers)

        resp = conn.getresponse()

        if 200 >= resp.status < 300:
            return None, None

        return resp.getheader("X-Wait-Seconds"), resp.read() or None

    # This was ported from nova.
    # Keeping it as a static method for the sake of consistency
    #
    # Note: This method gets called before the class is instantiated,
    # so this must be either a static method or a class method.  It is
    # used to develop a list of limits to feed to the constructor.
    # This implementation returns an empty list, since all limit
    # decisions are made by a remote server.
    @staticmethod
    def parse_limits(limits):
        """
        Ignore a limits string--simply doesn't apply for the limit
        proxy.

        @return: Empty list.
        """

        return []

########NEW FILE########
__FILENAME__ = models
# Copyright 2010-2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Model classes that form the core of instances functionality."""

from trove.openstack.common import log as logging
from trove.common import remote


LOG = logging.getLogger(__name__)


class ModelBase(object):
    """
    An object which can be stored in the database.
    """

    _data_fields = []
    _auto_generated_attrs = []

    def _validate(self, errors):
        """Subclasses override this to offer additional validation.

        For each validation error a key with the field name and an error
        message is added to the dict.

        """
        pass

    def data(self, **options):
        """Called to serialize object to a dictionary."""
        data_fields = self._data_fields + self._auto_generated_attrs
        return dict([(field, self[field]) for field in data_fields])

    def is_valid(self):
        """Called when persisting data to ensure the format is correct."""
        self.errors = {}
        self._validate(self.errors)
#        self._validate_columns_type()
#        self._before_validate()
#        self._validate()
        return self.errors == {}

    def __setitem__(self, key, value):
        """Overloaded to cause this object to look like a data entity."""
        setattr(self, key, value)

    def __getitem__(self, key):
        """Overloaded to cause this object to look like a data entity."""
        return getattr(self, key)

    def __eq__(self, other):
        """Overloaded to cause this object to look like a data entity."""
        if not hasattr(other, 'id'):
            return False
        return type(other) == type(self) and other.id == self.id

    def __ne__(self, other):
        """Overloaded to cause this object to look like a data entity."""
        return not self == other

    def __hash__(self):
        """Overloaded to cause this object to look like a data entity."""
        return self.id.__hash__()


class RemoteModelBase(ModelBase):

    # This should be set by the remote model during init time
    # The data() method will be using this
    _data_object = None

    def _data_item(self, data_object):
        data_fields = self._data_fields + self._auto_generated_attrs
        return dict([(field, getattr(data_object, field))
                     for field in data_fields])

    # data magic that will allow for a list of _data_object or a single item
    # if the object is a list, it will turn it into a list of hash's again
    def data(self, **options):
        if self._data_object is None:
            raise LookupError("data object is None")
        if isinstance(self._data_object, list):
            return [self._data_item(item) for item in self._data_object]
        else:
            return self._data_item(self._data_object)


class NovaRemoteModelBase(RemoteModelBase):

    @classmethod
    def get_client(cls, context):
        return remote.create_nova_client(context)


class SwiftRemoteModelBase(RemoteModelBase):

    @classmethod
    def get_client(cls, context):
        return remote.create_swift_client(context)

########NEW FILE########
__FILENAME__ = pagination
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import urllib
import six.moves.urllib.parse as urlparse


def url_quote(s):
    if s is None:
        return s
    return urllib.quote(str(s))


class PaginatedDataView(object):

    def __init__(self, collection_type, collection, current_page_url,
                 next_page_marker=None):
        self.collection_type = collection_type
        self.collection = collection
        self.current_page_url = current_page_url
        self.next_page_marker = url_quote(next_page_marker)

    def data(self):
        return {self.collection_type: self.collection,
                'links': self._links,
                }

    def _links(self):
        if not self.next_page_marker:
            return []
        app_url = AppUrl(self.current_page_url)
        next_url = app_url.change_query_params(marker=self.next_page_marker)
        next_link = {
            'rel': 'next',
            'href': str(next_url),
        }
        return [next_link]


class SimplePaginatedDataView(object):
    # In some cases, we can't create a PaginatedDataView because
    # we don't have a collection query object to create a view on.
    # In that case, we have to supply the URL and collection manually.

    def __init__(self, url, name, view, marker):
        self.url = url
        self.name = name
        self.view = view
        self.marker = url_quote(marker)

    def data(self):
        if not self.marker:
            return self.view.data()

        app_url = AppUrl(self.url)
        next_url = str(app_url.change_query_params(marker=self.marker))
        next_link = {'rel': 'next',
                     'href': next_url}
        view_data = {self.name: self.view.data()[self.name],
                     'links': [next_link]}
        return view_data


class AppUrl(object):

    def __init__(self, url):
        self.url = url

    def __str__(self):
        return self.url

    def change_query_params(self, **kwargs):
        # Seeks out the query params in a URL and changes/appends to them
        # from the kwargs given. So change_query_params(foo='bar')
        # would remove from the URL any old instance of foo=something and
        # then add &foo=bar to the URL.
        parsed_url = urlparse.urlparse(self.url)
        # Build a dictionary out of the query parameters in the URL
        query_params = dict(urlparse.parse_qsl(parsed_url.query))
        # Use kwargs to change or update any values in the query dict.
        query_params.update(kwargs)

        # Build a new query based on the updated query dict.
        new_query_params = urllib.urlencode(query_params)
        return self.__class__(
            # Force HTTPS.
            urlparse.ParseResult('https',
                                 parsed_url.netloc, parsed_url.path,
                                 parsed_url.params, new_query_params,
                                 parsed_url.fragment).geturl())

########NEW FILE########
__FILENAME__ = remote
# Copyright 2010-2012 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from trove.common import cfg
from trove.common import exception
from trove.openstack.common.importutils import import_class
from trove.openstack.common import log as logging

from cinderclient.v2 import client as CinderClient
from heatclient.v1 import client as HeatClient
from keystoneclient.service_catalog import ServiceCatalog
from novaclient.v1_1.client import Client
from swiftclient.client import Connection

CONF = cfg.CONF

PROXY_AUTH_URL = CONF.trove_auth_url
USE_SNET = CONF.backup_use_snet

LOG = logging.getLogger(__name__)


def normalize_url(url):
    """Adds trailing slash if necessary."""
    if not url.endswith('/'):
        return '%(url)s/' % {'url': url}
    else:
        return url


def get_endpoint(service_catalog, service_type=None, endpoint_region=None,
                 endpoint_type='publicURL'):
    """
    Select an endpoint from the service catalog

    We search the full service catalog for services
    matching both type and region. If the client
    supplied no region then any endpoint matching service_type
    is considered a match. There must be one -- and
    only one -- successful match in the catalog,
    otherwise we will raise an exception.

    Some parts copied from glance/common/auth.py.
    """
    if not service_catalog:
        raise exception.EmptyCatalog()

    # per IRC chat, X-Service-Catalog will be a v2 catalog regardless of token
    # format; see https://bugs.launchpad.net/python-keystoneclient/+bug/1302970
    # 'token' key necessary to get past factory validation
    sc = ServiceCatalog.factory({'token': None,
                                 'serviceCatalog': service_catalog})
    urls = sc.get_urls(service_type=service_type, region_name=endpoint_region,
                       endpoint_type=endpoint_type)

    if not urls:
        raise exception.NoServiceEndpoint(service_type=service_type,
                                          endpoint_region=endpoint_region,
                                          endpoint_type=endpoint_type)

    if len(urls) > 1:
        raise exception.RegionAmbiguity(service_type=service_type,
                                        endpoint_region=endpoint_region)

    return urls[0]


def dns_client(context):
    from trove.dns.manager import DnsManager
    return DnsManager()


def guest_client(context, id):
    from trove.guestagent.api import API
    return API(context, id)


def nova_client(context):
    if CONF.nova_compute_url:
        url = '%(nova_url)s%(tenant)s' % {
            'nova_url': normalize_url(CONF.nova_compute_url),
            'tenant': context.tenant}
    else:
        url = get_endpoint(context.service_catalog,
                           service_type=CONF.nova_compute_service_type,
                           endpoint_region=CONF.os_region_name)

    client = Client(context.user, context.auth_token,
                    project_id=context.tenant, auth_url=PROXY_AUTH_URL)
    client.client.auth_token = context.auth_token
    client.client.management_url = url
    return client


def create_admin_nova_client(context):
    """
    Creates client that uses trove admin credentials
    :return: a client for nova for the trove admin
    """
    client = create_nova_client(context)
    client.client.auth_token = None
    return client


def cinder_client(context):
    if CONF.cinder_url:
        url = '%(cinder_url)s%(tenant)s' % {
            'cinder_url': normalize_url(CONF.cinder_url),
            'tenant': context.tenant}
    else:
        url = get_endpoint(context.service_catalog,
                           service_type=CONF.cinder_service_type,
                           endpoint_region=CONF.os_region_name)

    client = CinderClient.Client(context.user, context.auth_token,
                                 project_id=context.tenant,
                                 auth_url=PROXY_AUTH_URL)
    client.client.auth_token = context.auth_token
    client.client.management_url = url
    return client


def heat_client(context):
    if CONF.heat_url:
        url = '%(heat_url)s%(tenant)s' % {
            'heat_url': normalize_url(CONF.heat_url),
            'tenant': context.tenant}
    else:
        url = get_endpoint(context.service_catalog,
                           service_type=CONF.heat_service_type,
                           endpoint_region=CONF.os_region_name)

    client = HeatClient.Client(token=context.auth_token,
                               os_no_client_auth=True,
                               endpoint=url)
    return client


def swift_client(context):
    if CONF.swift_url:
        # swift_url has a different format so doesn't need to be normalized
        url = '%(swift_url)s%(tenant)s' % {'swift_url': CONF.swift_url,
                                           'tenant': context.tenant}
    else:
        url = get_endpoint(context.service_catalog,
                           service_type=CONF.swift_service_type,
                           endpoint_region=CONF.os_region_name)

    client = Connection(preauthurl=url,
                        preauthtoken=context.auth_token,
                        tenant_name=context.tenant,
                        snet=USE_SNET)
    return client


create_dns_client = import_class(CONF.remote_dns_client)
create_guest_client = import_class(CONF.remote_guest_client)
create_nova_client = import_class(CONF.remote_nova_client)
create_swift_client = import_class(CONF.remote_swift_client)
create_cinder_client = import_class(CONF.remote_cinder_client)
create_heat_client = import_class(CONF.remote_heat_client)

########NEW FILE########
__FILENAME__ = impl_fake
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Standard openstack.common.rpc.impl_fake with nonblocking cast
"""

import eventlet

from trove.openstack.common.rpc.impl_fake import cast
from trove.openstack.common.rpc.impl_fake import create_connection


original_cast = cast


def non_blocking_cast(*args, **kwargs):
    eventlet.spawn_n(original_cast, *args, **kwargs)


cast = non_blocking_cast


# Asserting create_connection, workaround for pep8-F401 for unused import.
assert create_connection

########NEW FILE########
__FILENAME__ = service
# Copyright 2013 OpenStack Foundation
# Copyright 2013 Rackspace Hosting
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

import inspect
import os

from trove.openstack.common import importutils
from trove.openstack.common import loopingcall
from trove.openstack.common.rpc import service as rpc_service
from trove.common import cfg

CONF = cfg.CONF


class RpcService(rpc_service.Service):

    def __init__(self, host=None, binary=None, topic=None, manager=None):
        host = host or CONF.host
        binary = binary or os.path.basename(inspect.stack()[-1][1])
        topic = topic or binary.rpartition('trove-')[2]
        self.manager_impl = importutils.import_object(manager)
        self.report_interval = CONF.report_interval
        super(RpcService, self).__init__(host, topic,
                                         manager=self.manager_impl)

    def start(self):
        super(RpcService, self).start()
        # TODO(hub-cap): Currently the context is none... do we _need_ it here?
        pulse = loopingcall.FixedIntervalLoopingCall(
            self.manager_impl.run_periodic_tasks, context=None)
        pulse.start(interval=self.report_interval,
                    initial_delay=self.report_interval)
        pulse.wait()

########NEW FILE########
__FILENAME__ = template
#    Copyright 2012 OpenStack Foundation
#    Copyright 2014 Rackspace Hosting
#    All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import jinja2
from trove.common import cfg
from trove.common import configurations
from trove.common import exception
from trove.common import utils
from trove.openstack.common import log as logging

CONF = cfg.CONF
LOG = logging.getLogger(__name__)

ENV = utils.ENV

# TODO(cp16net) Maybe this should be moved to a config dict
SERVICE_PARSERS = {
    'mysql': configurations.MySQLConfParser,
    'percona': configurations.MySQLConfParser,
}


class SingleInstanceConfigTemplate(object):
    """This class selects a single configuration file by database type for
        rendering on the guest
    """

    template_name = "config.template"

    def __init__(self, datastore_version, flavor_dict, instance_id):
        """Constructor

        :param datastore_version: The datastore version.
        :type datastore_version: DatastoreVersion
        :param flavor_dict: dict containing flavor details for use in jinja.
        :type flavor_dict: dict.
        :param instance_id: trove instance id
        :type instance_id: str

        """
        self.flavor_dict = flavor_dict
        self.datastore_version = datastore_version
        #TODO(tim.simpson): The current definition of datastore_version is a
        #                   bit iffy and I believe will change soon, so I'm
        #                   creating a dictionary here for jinja to consume
        #                   rather than pass in the datastore version object.
        self.datastore_dict = {
            'name': self.datastore_version.datastore_name,
            'manager': self.datastore_version.manager,
            'version': self.datastore_version.name,
        }
        self.instance_id = instance_id

    def get_template(self):
        patterns = ['{name}/{version}/{template_name}',
                    '{name}/{template_name}',
                    '{manager}/{template_name}']
        context = self.datastore_dict.copy()
        context['template_name'] = self.template_name
        names = [name.format(**context) for name in patterns]
        return ENV.select_template(names)

    def render(self, **kwargs):
        """Renders the jinja template

        :returns: str -- The rendered configuration file

        """
        template = self.get_template()
        server_id = self._calculate_unique_id()
        self.config_contents = template.render(
            flavor=self.flavor_dict,
            datastore=self.datastore_dict,
            server_id=server_id, **kwargs)
        return self.config_contents

    def render_dict(self):
        """
        Renders the default configuration template file as a dictionary
        to apply the default configuration dynamically.
        """
        config = self.render()
        cfg_parser = SERVICE_PARSERS.get(self.datastore_version.manager)
        if not cfg_parser:
            raise exception.NoConfigParserFound(
                datastore_manager=self.datastore_version.manager)
        return cfg_parser(config).parse()

    def _calculate_unique_id(self):
        """
        Returns a positive unique id based off of the instance id

        :return: a positive integer
        """
        return abs(hash(self.instance_id) % (2 ** 31))


class OverrideConfigTemplate(SingleInstanceConfigTemplate):
    template_name = "override.config.template"


def load_heat_template(datastore_manager):
    template_filename = "%s/heat.template" % datastore_manager
    try:
        template_obj = ENV.get_template(template_filename)
        return template_obj
    except jinja2.TemplateNotFound:
        msg = "Missing heat template for %s" % datastore_manager
        LOG.error(msg)
        raise exception.TroveError(msg)

########NEW FILE########
__FILENAME__ = utils
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
"""I totally stole most of this from melange, thx guys!!!"""

import datetime
import inspect
import jinja2
import sys
import time
import six.moves.urllib.parse as urlparse
import uuid
import os
import shutil

from eventlet import event
from eventlet import greenthread
from eventlet.timeout import Timeout
from passlib import utils as passlib_utils

from trove.common import cfg
from trove.common import exception
from trove.openstack.common import importutils
from trove.openstack.common import log as logging
from trove.openstack.common import processutils
from trove.openstack.common import timeutils
from trove.openstack.common import utils as openstack_utils
from trove.openstack.common.gettextutils import _

CONF = cfg.CONF
LOG = logging.getLogger(__name__)
import_class = importutils.import_class
import_object = importutils.import_object
import_module = importutils.import_module
bool_from_string = openstack_utils.bool_from_string
execute = processutils.execute
isotime = timeutils.isotime

CONF = cfg.CONF
ENV = jinja2.Environment(loader=jinja2.ChoiceLoader([
                         jinja2.FileSystemLoader(CONF.template_path),
                         jinja2.PackageLoader("trove", "templates")
                         ]))


def create_method_args_string(*args, **kwargs):
    """Returns a string representation of args and keyword args.

    I.e. for args=1,2,3 and kwargs={'a':4, 'b':5} you'd get: "1,2,3,a=4,b=5"
    """
    # While %s turns a var into a string but in some rare cases explicit
    # repr() is less likely to raise an exception.
    arg_strs = [repr(arg) for arg in args]
    arg_strs += ['%s=%s' % (repr(key), repr(value))
                 for (key, value) in kwargs.items()]
    return ', '.join(arg_strs)


def stringify_keys(dictionary):
    if dictionary is None:
        return None
    return dict((str(key), value) for key, value in dictionary.iteritems())


def exclude(key_values, *exclude_keys):
    if key_values is None:
        return None
    return dict((key, value) for key, value in key_values.iteritems()
                if key not in exclude_keys)


def generate_uuid():
    return str(uuid.uuid4())


def utcnow():
    return datetime.datetime.utcnow()


def raise_if_process_errored(process, exception):
    try:
        err = process.stderr.read()
        if err:
            raise exception(err)
    except OSError:
        pass


def clean_out(folder):
    for root, dirs, files in os.walk(folder):
        for f in files:
            os.unlink(os.path.join(root, f))
        for d in dirs:
            shutil.rmtree(os.path.join(root, d))


class cached_property(object):
    """A decorator that converts a function into a lazy property.

    Taken from : https://github.com/nshah/python-memoize
    The function wrapped is called the first time to retrieve the result
    and than that calculated result is used the next time you access
    the value:

        class Foo(object):

            @cached_property
            def bar(self):
                # calculate something important here
                return 42

    """

    def __init__(self, func, name=None, doc=None):
        self.func = func
        self.__name__ = name or func.__name__
        self.__doc__ = doc or func.__doc__

    def __get__(self, obj, type=None):
        if obj is None:
            return self
        value = self.func(obj)
        setattr(obj, self.__name__, value)
        return value


class MethodInspector(object):

    def __init__(self, func):
        self._func = func

    @cached_property
    def required_args(self):
        return self.args[0:self.required_args_count]

    @cached_property
    def optional_args(self):
        keys = self.args[self.required_args_count: len(self.args)]
        return zip(keys, self.defaults)

    @cached_property
    def defaults(self):
        return self.argspec.defaults or ()

    @cached_property
    def required_args_count(self):
        return len(self.args) - len(self.defaults)

    @cached_property
    def args(self):
        args = self.argspec.args
        if inspect.ismethod(self._func):
            args.pop(0)
        return args

    @cached_property
    def argspec(self):
        return inspect.getargspec(self._func)

    def __str__(self):
        optionals = ["[{0}=<{0}>]".format(k) for k, v in self.optional_args]
        required = ["{0}=<{0}>".format(arg) for arg in self.required_args]
        args_str = ' '.join(required + optionals)
        return "%s %s" % (self._func.__name__, args_str)


class LoopingCallDone(Exception):
    """Exception to break out and stop a LoopingCall.

    The poll-function passed to LoopingCall can raise this exception to
    break out of the loop normally. This is somewhat analogous to
    StopIteration.

    An optional return-value can be included as the argument to the exception;
    this return-value will be returned by LoopingCall.wait()

    """

    def __init__(self, retvalue=True):
        """:param retvalue: Value that LoopingCall.wait() should return."""
        super(LoopingCallDone, self).__init__()
        self.retvalue = retvalue


class LoopingCall(object):
    """Nabbed from nova."""
    def __init__(self, f=None, *args, **kw):
        self.args = args
        self.kw = kw
        self.f = f
        self._running = False

    def start(self, interval, now=True):
        self._running = True
        done = event.Event()

        def _inner():
            if not now:
                greenthread.sleep(interval)
            try:
                while self._running:
                    self.f(*self.args, **self.kw)
                    if not self._running:
                        break
                    greenthread.sleep(interval)
            except LoopingCallDone as e:
                self.stop()
                done.send(e.retvalue)
            except Exception:
                LOG.exception(_('in looping call'))
                done.send_exception(*sys.exc_info())
                return
            else:
                done.send(True)

        self.done = done

        greenthread.spawn(_inner)
        return self.done

    def stop(self):
        self._running = False

    def wait(self):
        return self.done.wait()


def poll_until(retriever, condition=lambda value: value,
               sleep_time=1, time_out=None):
    """Retrieves object until it passes condition, then returns it.

    If time_out_limit is passed in, PollTimeOut will be raised once that
    amount of time is eclipsed.

    """
    start_time = time.time()

    def poll_and_check():
        obj = retriever()
        if condition(obj):
            raise LoopingCallDone(retvalue=obj)
        if time_out is not None and time.time() > start_time + time_out:
            raise exception.PollTimeOut
    lc = LoopingCall(f=poll_and_check).start(sleep_time, True)
    return lc.wait()


# Copied from nova.api.openstack.common in the old code.
def get_id_from_href(href):
    """Return the id or uuid portion of a url.

    Given: 'http://www.foo.com/bar/123?q=4'
    Returns: '123'

    Given: 'http://www.foo.com/bar/abc123?q=4'
    Returns: 'abc123'

    """
    return urlparse.urlsplit("%s" % href).path.split('/')[-1]


def execute_with_timeout(*args, **kwargs):
    time = kwargs.pop('timeout', 30)

    def cb_timeout():
        msg = (_("Time out after waiting"
                 " %(time)s seconds when running proc: %(args)s"
                 " %(kwargs)s") % {'time': time, 'args': args,
                                   'kwargs': kwargs})
        LOG.error(msg)
        raise exception.ProcessExecutionError(msg)

    timeout = Timeout(time)
    try:
        return execute(*args, **kwargs)
    except Timeout as t:
        if t is not timeout:
            LOG.error("Timeout reached but not from our timeout. This is bad!")
            raise
        else:
            msg = (_("Time out after waiting "
                     "%(time)s seconds when running proc: %(args)s"
                     " %(kwargs)s") % {'time': time, 'args': args,
                                       'kwargs': kwargs})
            LOG.error(msg)
            raise exception.ProcessExecutionError(msg)
    finally:
        timeout.cancel()


def correct_id_with_req(id, request):
    # Due to a shortcoming with the way Trove uses routes.mapper,
    # URL entities right of the last slash that contain at least
    # one . are routed to our service without that suffix, as
    # it was interpreted as a filetype This method looks at the
    # request, and if applicable, reattaches the suffix to the id.
    routing_args = request.environ.get('wsgiorg.routing_args', [])
    for routing_arg in routing_args:
        try:
            found = routing_arg.get('format', '')
            if found and found not in CONF.expected_filetype_suffixes:
                return "%s.%s" % (id, found)
        except (AttributeError, KeyError):
            # Not the relevant routing_args entry.
            pass
    return id


def generate_random_password(password_length=CONF.default_password_length):
    return passlib_utils.generate_password(size=password_length)


def try_recover(func):
    def _decorator(*args, **kwargs):
        recover_func = kwargs.pop("recover_func", None)
        try:
            func(*args, **kwargs)
        except Exception:
            if recover_func is not None:
                recover_func(func)
            else:
                LOG.debug("No recovery method defined for %(func)s" % {
                          'func': func.__name__})
            raise
    return _decorator


def gen_ports(portstr):
    from_port, sep, to_port = portstr.partition('-')
    if not (to_port and from_port):
        if not sep:
            to_port = from_port
    if int(from_port) > int(to_port):
        raise ValueError
    return from_port, to_port

########NEW FILE########
__FILENAME__ = views
# Copyright 2010-2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


from trove.common import wsgi


def create_links(resource_path, request, id):
    """Creates the links dictionary in the format typical of most resources."""
    context = request.environ[wsgi.CONTEXT_KEY]
    link_info = {
        'host': request.host,
        'version': request.url_version,
        'tenant_id': context.tenant,
        'resource_path': resource_path,
        'id': id,
    }
    return [
        {
            "href": "https://%(host)s/v%(version)s/%(tenant_id)s"
                    "/%(resource_path)s/%(id)s" % link_info,
            "rel": "self"
        },
        {
            "href": "https://%(host)s/%(resource_path)s/%(id)s" % link_info,
            "rel": "bookmark"
        }
    ]

########NEW FILE########
__FILENAME__ = wsgi
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
"""Wsgi helper utilities for trove"""

import eventlet.wsgi
import math
import jsonschema
import paste.urlmap
import re
import time
import traceback
import uuid
import webob
import webob.dec
import webob.exc

from trove.common import context as rd_context
from trove.common import exception
from trove.common import utils
from trove.openstack.common.gettextutils import _
from trove.openstack.common import jsonutils

from trove.openstack.common import pastedeploy
from trove.openstack.common import service
from trove.openstack.common import wsgi as openstack_wsgi
from trove.openstack.common import log as logging
from trove.common import cfg

CONTEXT_KEY = 'trove.context'
Router = openstack_wsgi.Router
Debug = openstack_wsgi.Debug
Middleware = openstack_wsgi.Middleware
JSONDictSerializer = openstack_wsgi.JSONDictSerializer
RequestDeserializer = openstack_wsgi.RequestDeserializer

CONF = cfg.CONF
# Raise the default from 8192 to accommodate large tokens
eventlet.wsgi.MAX_HEADER_LINE = CONF.max_header_line

eventlet.patcher.monkey_patch(all=False, socket=True)

LOG = logging.getLogger('trove.common.wsgi')

CONF = cfg.CONF


def versioned_urlmap(*args, **kwargs):
    urlmap = paste.urlmap.urlmap_factory(*args, **kwargs)
    return VersionedURLMap(urlmap)


def launch(app_name, port, paste_config_file, data={},
           host='0.0.0.0', backlog=128, threads=1000, workers=None):
    """Launches a wsgi server based on the passed in paste_config_file.

      Launch provides a easy way to create a paste app from the config
      file and launch it via the service launcher. It takes care of
      all of the plumbing. The only caveat is that the paste_config_file
      must be a file that paste.deploy can find and handle. There is
      a helper method in cfg.py that finds files.

      Example:
        conf_file = CONF.find_file(CONF.api_paste_config)
        launcher = wsgi.launch('myapp', CONF.bind_port, conf_file)
        launcher.wait()

    """
    app = pastedeploy.paste_deploy_app(paste_config_file, app_name, data)
    server = openstack_wsgi.Service(app, port, host=host,
                                    backlog=backlog, threads=threads)
    return service.launch(server, workers)


# Note: taken from Nova
def serializers(**serializers):
    """Attaches serializers to a method.

    This decorator associates a dictionary of serializers with a
    method.  Note that the function attributes are directly
    manipulated; the method is not wrapped.
    """

    def decorator(func):
        if not hasattr(func, 'wsgi_serializers'):
            func.wsgi_serializers = {}
        func.wsgi_serializers.update(serializers)
        return func

    return decorator


class TroveMiddleware(Middleware):
    # Note: taken from nova
    @classmethod
    def factory(cls, global_config, **local_config):
        """Used for paste app factories in paste.deploy config files.

        Any local configuration (that is, values under the [filter:APPNAME]
        section of the paste config) will be passed into the `__init__` method
        as kwargs.

        A hypothetical configuration would look like:

            [filter:analytics]
            redis_host = 127.0.0.1
            paste.filter_factory = nova.api.analytics:Analytics.factory

        which would result in a call to the `Analytics` class as

            import nova.api.analytics
            analytics.Analytics(app_from_paste, redis_host='127.0.0.1')

        You could of course re-implement the `factory` method in subclasses,
        but using the kwarg passing it shouldn't be necessary.

        """

        def _factory(app):
            return cls(app, **local_config)

        return _factory


class VersionedURLMap(object):
    def __init__(self, urlmap):
        self.urlmap = urlmap

    def __call__(self, environ, start_response):
        req = Request(environ)

        if req.url_version is None and req.accept_version is not None:
            version = "/v" + req.accept_version
            http_exc = webob.exc.HTTPNotAcceptable(_("version not supported"))
            app = self.urlmap.get(version, Fault(http_exc))
        else:
            app = self.urlmap
        return app(environ, start_response)


class Router(openstack_wsgi.Router):
    # Original router did not allow for serialization of the 404 error.
    # To fix this the _dispatch was modified to use Fault() objects.
    @staticmethod
    @webob.dec.wsgify
    def _dispatch(req):
        """
        Called by self._router after matching the incoming request to a route
        and putting the information into req.environ.  Either returns 404
        or the routed WSGI app's response.
        """

        match = req.environ['wsgiorg.routing_args'][1]
        if not match:
            return Fault(webob.exc.HTTPNotFound())
        app = match['controller']
        return app


class Request(openstack_wsgi.Request):
    @property
    def params(self):
        return utils.stringify_keys(super(Request, self).params)

    def best_match_content_type(self, supported_content_types=None):
        """Determine the most acceptable content-type.

        Based on the query extension then the Accept header.

        """
        parts = self.path.rsplit('.', 1)

        if len(parts) > 1:
            format = parts[1]
            if format in ['json']:
                return 'application/{0}'.format(parts[1])

        ctypes = {
            'application/vnd.openstack.trove+json': "application/json",
            'application/json': "application/json",
        }
        bm = self.accept.best_match(ctypes.keys())

        return ctypes.get(bm, 'application/json')

    @utils.cached_property
    def accept_version(self):
        accept_header = self.headers.get('ACCEPT', "")
        accept_version_re = re.compile(".*?application/vnd.openstack.trove"
                                       "(\+.+?)?;"
                                       "version=(?P<version_no>\d+\.?\d*)")

        match = accept_version_re.search(accept_header)
        return match.group("version_no") if match else None

    @utils.cached_property
    def url_version(self):
        versioned_url_re = re.compile("/v(?P<version_no>\d+\.?\d*)")
        match = versioned_url_re.search(self.path)
        return match.group("version_no") if match else None


class Result(object):
    """A result whose serialization is compatible with JSON."""

    def __init__(self, data, status=200):
        self._data = data
        self.status = status

    def data(self, serialization_type):
        """Return an appropriate serialized type for the body.
           serialization_type is not used presently, but may be
           in the future, so it stays.
        """

        if hasattr(self._data, "data_for_json"):
            return self._data.data_for_json()
        return self._data


class Resource(openstack_wsgi.Resource):
    def __init__(self, controller, deserializer, serializer,
                 exception_map=None):
        exception_map = exception_map or {}
        self.model_exception_map = self._invert_dict_list(exception_map)
        super(Resource, self).__init__(controller, deserializer, serializer)

    @webob.dec.wsgify(RequestClass=Request)
    def __call__(self, request):
        return super(Resource, self).__call__(request)

    def execute_action(self, action, request, **action_args):
        if getattr(self.controller, action, None) is None:
            return Fault(webob.exc.HTTPNotFound())
        try:
            self.controller.validate_request(action, action_args)
            result = super(Resource, self).execute_action(
                action,
                request,
                **action_args)
            if type(result) is dict:
                result = Result(result)
            return result

        except exception.TroveError as trove_error:
            LOG.debug(traceback.format_exc())
            LOG.debug("Caught Trove Error %s", trove_error)
            httpError = self._get_http_error(trove_error)
            LOG.debug("Mapped Error to %s", httpError)
            return Fault(httpError(str(trove_error), request=request))
        except webob.exc.HTTPError as http_error:
            LOG.debug(traceback.format_exc())
            return Fault(http_error)
        except Exception as error:
            exception_uuid = str(uuid.uuid4())
            LOG.exception(exception_uuid + ": " + str(error))
            return Fault(webob.exc.HTTPInternalServerError(
                "Internal Server Error. Please keep this ID to help us "
                "figure out what went wrong: (%s)" % exception_uuid,
                request=request))

    def _get_http_error(self, error):
        return self.model_exception_map.get(type(error),
                                            webob.exc.HTTPBadRequest)

    def _invert_dict_list(self, exception_dict):
        """Flattens values of keys and inverts keys and values.

        Example:
        {'x': [1, 2, 3], 'y': [4, 5, 6]} converted to
        {1: 'x', 2: 'x', 3: 'x', 4: 'y', 5: 'y', 6: 'y'}

        """
        inverted_dict = {}
        for key, value_list in exception_dict.items():
            for value in value_list:
                inverted_dict[value] = key
        return inverted_dict

    def serialize_response(self, action, action_result, accept):
        # If an exception is raised here in the base class, it is swallowed,
        # and the action_result is returned as-is. For us, that's bad news -
        # we never want that to happen except in the case of webob types.
        # So we override the behavior here so we can at least log it.
        try:
            return super(Resource, self).serialize_response(
                action, action_result, accept)
        except Exception:
            # execute_action either returns the results or a Fault object.
            # If action_result is not a Fault then there really was a
            # serialization error which we log. Otherwise return the Fault.
            if not isinstance(action_result, Fault):
                LOG.exception("unserializable result detected.")
                raise
            return action_result


class Controller(object):
    """Base controller that creates a Resource with default serializers."""

    exception_map = {
        webob.exc.HTTPUnprocessableEntity: [
            exception.UnprocessableEntity,
        ],
        webob.exc.HTTPUnauthorized: [
            exception.Forbidden,
            exception.SwiftAuthError,
        ],
        webob.exc.HTTPBadRequest: [
            exception.InvalidModelError,
            exception.BadRequest,
            exception.CannotResizeToSameSize,
            exception.BadValue,
            exception.DatabaseAlreadyExists,
            exception.UserAlreadyExists,
            exception.LocalStorageNotSpecified,
        ],
        webob.exc.HTTPNotFound: [
            exception.NotFound,
            exception.ComputeInstanceNotFound,
            exception.ModelNotFoundError,
            exception.UserNotFound,
            exception.DatabaseNotFound,
            exception.QuotaResourceUnknown,
            exception.BackupFileNotFound
        ],
        webob.exc.HTTPConflict: [
            exception.BackupNotCompleteError,
            exception.RestoreBackupIntegrityError,
        ],
        webob.exc.HTTPRequestEntityTooLarge: [
            exception.OverLimit,
            exception.QuotaExceeded,
            exception.VolumeQuotaExceeded,
        ],
        webob.exc.HTTPServerError: [
            exception.VolumeCreationFailure,
            exception.UpdateGuestError,
        ],
        webob.exc.HTTPNotImplemented: [
            exception.VolumeNotSupported,
            exception.LocalStorageNotSupported,
            exception.DatastoreOperationNotSupported
        ],
    }

    schemas = {}

    @classmethod
    def get_schema(cls, action, body):
        LOG.debug("Getting schema for %s:%s" %
                  (cls.__class__.__name__, action))
        if cls.schemas:
            matching_schema = cls.schemas.get(action, {})
            if matching_schema:
                LOG.debug("Found Schema: %s" % matching_schema.get("name",
                                                                   "none"))
            return matching_schema

    @staticmethod
    def format_validation_msg(errors):
        # format path like object['field1'][i]['subfield2']
        messages = []
        for error in errors:
            path = list(error.path)
            f_path = "%s%s" % (path[0],
                               ''.join(['[%r]' % i for i in path[1:]]))
            messages.append("%s %s" % (f_path, error.message))
            for suberror in sorted(error.context, key=lambda e: e.schema_path):
                messages.append(suberror.message)
        error_msg = "; ".join(messages)
        return "Validation error: %s" % error_msg

    def validate_request(self, action, action_args):
        body = action_args.get('body', {})
        schema = self.get_schema(action, body)
        if schema:
            validator = jsonschema.Draft4Validator(schema)
            if not validator.is_valid(body):
                errors = sorted(validator.iter_errors(body),
                                key=lambda e: e.path)
                error_msg = self.format_validation_msg(errors)
                LOG.info(error_msg)
                raise exception.BadRequest(message=error_msg)

    def create_resource(self):
        return Resource(
            self,
            RequestDeserializer(),
            TroveResponseSerializer(),
            self.exception_map)

    def _extract_limits(self, params):
        return dict([(key, params[key]) for key in params.keys()
                     if key in ["limit", "marker"]])


class TroveResponseSerializer(openstack_wsgi.ResponseSerializer):
    def serialize_body(self, response, data, content_type, action):
        """Overrides body serialization in openstack_wsgi.ResponseSerializer.

        If the "data" argument is the Result class, its data
        method is called and *that* is passed to the superclass implementation
        instead of the actual data.

        """
        if isinstance(data, Result):
            data = data.data(content_type)
        super(TroveResponseSerializer, self).serialize_body(
            response,
            data,
            content_type,
            action)

    def serialize_headers(self, response, data, action):
        super(TroveResponseSerializer, self).serialize_headers(
            response,
            data,
            action)
        if isinstance(data, Result):
            response.status = data.status


class Fault(webob.exc.HTTPException):
    """Error codes for API faults."""

    code_wrapper = {
        400: webob.exc.HTTPBadRequest,
        401: webob.exc.HTTPUnauthorized,
        403: webob.exc.HTTPUnauthorized,
        404: webob.exc.HTTPNotFound,
    }

    resp_codes = [int(code) for code in code_wrapper.keys()]

    def __init__(self, exception):
        """Create a Fault for the given webob.exc.exception."""

        self.wrapped_exc = exception

    @staticmethod
    def _get_error_name(exc):
        # Displays a Red Dwarf specific error name instead of a webob exc name.
        named_exceptions = {
            'HTTPBadRequest': 'badRequest',
            'HTTPUnauthorized': 'unauthorized',
            'HTTPForbidden': 'forbidden',
            'HTTPNotFound': 'itemNotFound',
            'HTTPMethodNotAllowed': 'badMethod',
            'HTTPRequestEntityTooLarge': 'overLimit',
            'HTTPUnsupportedMediaType': 'badMediaType',
            'HTTPInternalServerError': 'instanceFault',
            'HTTPNotImplemented': 'notImplemented',
            'HTTPServiceUnavailable': 'serviceUnavailable',
        }
        name = exc.__class__.__name__
        if name in named_exceptions:
            return named_exceptions[name]
            # If the exception isn't in our list, at least strip off the
        # HTTP from the name, and then drop the case on the first letter.
        name = name.split("HTTP").pop()
        name = name[:1].lower() + name[1:]
        return name

    @webob.dec.wsgify(RequestClass=Request)
    def __call__(self, req):
        """Generate a WSGI response based on the exception passed to ctor."""

        # Replace the body with fault details.
        fault_name = Fault._get_error_name(self.wrapped_exc)
        fault_data = {
            fault_name: {
                'code': self.wrapped_exc.status_int,
            }
        }
        if self.wrapped_exc.detail:
            fault_data[fault_name]['message'] = self.wrapped_exc.detail
        else:
            fault_data[fault_name]['message'] = self.wrapped_exc.explanation

        content_type = req.best_match_content_type()
        serializer = {
            'application/json': openstack_wsgi.JSONDictSerializer(),
        }[content_type]

        self.wrapped_exc.body = serializer.serialize(fault_data, content_type)
        self.wrapped_exc.content_type = content_type
        return self.wrapped_exc


class ContextMiddleware(openstack_wsgi.Middleware):
    def __init__(self, application):
        self.admin_roles = CONF.admin_roles
        super(ContextMiddleware, self).__init__(application)

    def _extract_limits(self, params):
        return dict([(key, params[key]) for key in params.keys()
                     if key in ["limit", "marker"]])

    def process_request(self, request):
        service_catalog = None
        catalog_header = request.headers.get('X-Service-Catalog', None)
        if catalog_header:
            try:
                service_catalog = jsonutils.loads(catalog_header)
            except ValueError:
                raise webob.exc.HTTPInternalServerError(
                    _('Invalid service catalog json.'))
        tenant_id = request.headers.get('X-Tenant-Id', None)
        auth_token = request.headers["X-Auth-Token"]
        user_id = request.headers.get('X-User-ID', None)
        roles = request.headers.get('X-Role', '').split(',')
        is_admin = False
        for role in roles:
            if role.lower() in self.admin_roles:
                is_admin = True
                break
        limits = self._extract_limits(request.params)
        context = rd_context.TroveContext(auth_token=auth_token,
                                          tenant=tenant_id,
                                          user=user_id,
                                          is_admin=is_admin,
                                          limit=limits.get('limit'),
                                          marker=limits.get('marker'),
                                          service_catalog=service_catalog)
        request.environ[CONTEXT_KEY] = context

    @classmethod
    def factory(cls, global_config, **local_config):
        def _factory(app):
            LOG.debug("Created context middleware with config: %s" %
                      local_config)
            return cls(app)

        return _factory


class FaultWrapper(openstack_wsgi.Middleware):
    """Calls down the middleware stack, making exceptions into faults."""

    @webob.dec.wsgify(RequestClass=openstack_wsgi.Request)
    def __call__(self, req):
        try:
            resp = req.get_response(self.application)
            if resp.status_int in Fault.resp_codes:
                for (header, value) in resp._headerlist:
                    if header == "Content-Type" and \
                            value == "text/plain; charset=UTF-8":
                        return Fault(Fault.code_wrapper[resp.status_int]())
                return resp
            return resp
        except Exception as ex:
            LOG.exception(_("Caught error: %s"), unicode(ex))
            exc = webob.exc.HTTPInternalServerError()
            return Fault(exc)

    @classmethod
    def factory(cls, global_config, **local_config):
        def _factory(app):
            return cls(app)
        return _factory


# ported from Nova
class OverLimitFault(webob.exc.HTTPException):
    """
    Rate-limited request response.
    """

    def __init__(self, message, details, retry_time):
        """
        Initialize new `OverLimitFault` with relevant information.
        """
        hdrs = OverLimitFault._retry_after(retry_time)
        self.wrapped_exc = webob.exc.HTTPRequestEntityTooLarge(headers=hdrs)
        self.content = {"overLimit": {"code": self.wrapped_exc.status_int,
                                      "message": message,
                                      "details": details,
                                      "retryAfter": hdrs['Retry-After'],
                                      },
                        }

    @staticmethod
    def _retry_after(retry_time):
        delay = int(math.ceil(retry_time - time.time()))
        retry_after = delay if delay > 0 else 0
        headers = {'Retry-After': '%d' % retry_after}
        return headers

    @webob.dec.wsgify(RequestClass=Request)
    def __call__(self, request):
        """
        Return the wrapped exception with a serialized body conforming to our
        error format.
        """
        content_type = request.best_match_content_type()

        serializer = {'application/json': JSONDictSerializer(),
                      }[content_type]

        content = serializer.serialize(self.content)
        self.wrapped_exc.body = content
        self.wrapped_exc.content_type = content_type

        return self.wrapped_exc


class ActionDispatcher(object):
    """Maps method name to local methods through action name."""

    def dispatch(self, *args, **kwargs):
        """Find and call local method."""
        action = kwargs.pop('action', 'default')
        action_method = getattr(self, str(action), self.default)
        return action_method(*args, **kwargs)

    def default(self, data):
        raise NotImplementedError()


class DictSerializer(ActionDispatcher):
    """Default request body serialization."""

    def serialize(self, data, action='default'):
        return self.dispatch(data, action=action)

    def default(self, data):
        return ""


class JSONDictSerializer(DictSerializer):
    """Default JSON request body serialization."""

    def default(self, data):
        return jsonutils.dumps(data)

########NEW FILE########
__FILENAME__ = api
#    Copyright 2013 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


from trove.common import cfg
from trove.openstack.common.rpc import proxy
from trove.openstack.common import log as logging


CONF = cfg.CONF
LOG = logging.getLogger(__name__)
RPC_API_VERSION = "1.0"


class API(proxy.RpcProxy):
    """API for interacting with trove conductor."""

    def __init__(self, context):
        self.context = context
        super(API, self).__init__(self._get_routing_key(), RPC_API_VERSION)

    def _get_routing_key(self):
        """Create the routing key for conductor."""
        return CONF.conductor_queue

    def heartbeat(self, instance_id, payload, sent=None):
        LOG.debug("Making async call to cast heartbeat for instance: %s"
                  % instance_id)
        self.cast(self.context, self.make_msg("heartbeat",
                                              instance_id=instance_id,
                                              sent=sent,
                                              payload=payload))

    def update_backup(self, instance_id, backup_id, sent=None,
                      **backup_fields):
        LOG.debug("Making async call to cast update_backup for instance: %s"
                  % instance_id)
        self.cast(self.context, self.make_msg("update_backup",
                                              instance_id=instance_id,
                                              backup_id=backup_id,
                                              sent=sent,
                                              **backup_fields))

########NEW FILE########
__FILENAME__ = manager
#    Copyright 2013 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from trove.backup import models as bkup_models
from trove.common import cfg
from trove.common import exception
from trove.common.instance import ServiceStatus
from trove.conductor.models import LastSeen
from trove.instance import models as t_models
from trove.openstack.common import log as logging
from trove.openstack.common import periodic_task
from trove.openstack.common.gettextutils import _

LOG = logging.getLogger(__name__)
RPC_API_VERSION = "1.0"
CONF = cfg.CONF


class Manager(periodic_task.PeriodicTasks):

    def __init__(self):
        super(Manager, self).__init__()

    def _message_too_old(self, instance_id, method_name, sent):
        fields = {
            "instance": instance_id,
            "method": method_name,
            "sent": sent,
        }
        LOG.debug("Instance %(instance)s sent %(method)s at %(sent)s "
                  % fields)

        if sent is None:
            LOG.error(_("[Instance %s] sent field not present. Cannot "
                        "compare.") % instance_id)
            return False

        seen = None
        try:
            seen = LastSeen.load(instance_id=instance_id,
                                 method_name=method_name)
        except exception.NotFound:
            # This is fine.
            pass

        if seen is None:
            LOG.debug("[Instance %s] Did not find any previous message. "
                      "Creating." % instance_id)
            seen = LastSeen.create(instance_id=instance_id,
                                   method_name=method_name,
                                   sent=sent)
            seen.save()
            return False

        last_sent = float(seen.sent)
        if last_sent < sent:
            LOG.debug("[Instance %s] Rec'd message is younger than last "
                      "seen. Updating." % instance_id)
            seen.sent = sent
            seen.save()
            return False

        else:
            LOG.info(_("[Instance %s] Rec'd message is older than last seen. "
                       "Discarding.") % instance_id)
            return True

    def heartbeat(self, context, instance_id, payload, sent=None):
        LOG.debug("Instance ID: %s" % str(instance_id))
        LOG.debug("Payload: %s" % str(payload))
        status = t_models.InstanceServiceStatus.find_by(
            instance_id=instance_id)
        if self._message_too_old(instance_id, 'heartbeat', sent):
            return
        if payload.get('service_status') is not None:
            status.set_status(ServiceStatus.from_description(
                payload['service_status']))
        status.save()

    def update_backup(self, context, instance_id, backup_id,
                      sent=None, **backup_fields):
        LOG.debug("Instance ID: %s" % str(instance_id))
        LOG.debug("Backup ID: %s" % str(backup_id))
        backup = bkup_models.DBBackup.find_by(id=backup_id)
        # TODO(datsun180b): use context to verify tenant matches

        if self._message_too_old(instance_id, 'update_backup', sent):
            return

        # Some verification based on IDs
        if backup_id != backup.id:
            fields = {
                'expected': backup_id,
                'found': backup.id,
                'instance': str(instance_id),
            }
            LOG.error(_("[Instance: %(instance)s] Backup IDs mismatch! "
                        "Expected %(expected)s, found %(found)s") % fields)
            return
        if instance_id != backup.instance_id:
            fields = {
                'expected': instance_id,
                'found': backup.instance_id,
                'instance': str(instance_id),
            }
            LOG.error(_("[Instance: %(instance)s] Backup instance IDs "
                        "mismatch! Expected %(expected)s, found "
                        "%(found)s") % fields)
            return

        for k, v in backup_fields.items():
            if hasattr(backup, k):
                fields = {
                    'key': k,
                    'value': v,
                }
                LOG.debug("Backup %(key)s: %(value)s" % fields)
                setattr(backup, k, v)
        backup.save()

########NEW FILE########
__FILENAME__ = models
#Copyright 2014 OpenStack Foundation

#Licensed under the Apache License, Version 2.0 (the "License");
#you may not use this file except in compliance with the License.
#You may obtain a copy of the License at
#
#http://www.apache.org/licenses/LICENSE-2.0
#
#Unless required by applicable law or agreed to in writing, software
#distributed under the License is distributed on an "AS IS" BASIS,
#WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#See the License for the specific language governing permissions and
#limitations under the License.

from trove.db import get_db_api
from trove.openstack.common import log as logging

LOG = logging.getLogger(__name__)


def persisted_models():
    return {'conductor_lastseen': LastSeen}


class LastSeen(object):
    """A table used only by Conductor to discard messages that arrive
       late and out of order.
    """
    _auto_generated_attrs = []
    _data_fields = ['instance_id', 'method_name', 'sent']
    _table_name = 'conductor_lastseen'
    preserve_on_delete = False

    def __init__(self, instance_id, method_name, sent):
        self.instance_id = instance_id
        self.method_name = method_name
        self.sent = sent

    def save(self):
        return get_db_api().save(self)

    @classmethod
    def load(cls, instance_id, method_name):
        seen = get_db_api().find_by(cls,
                                    instance_id=instance_id,
                                    method_name=method_name)
        return seen

    @classmethod
    def create(cls, instance_id, method_name, sent):
        seen = LastSeen(instance_id, method_name, sent)
        return seen.save()

########NEW FILE########
__FILENAME__ = models
# Copyright 2014 Rackspace
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from datetime import datetime

from trove.common import cfg
from trove.common import configurations
from trove.common.exception import ModelNotFoundError
from trove.datastore.models import DatastoreVersion
from trove.db import models as dbmodels
from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _
from trove.taskmanager import api as task_api


CONF = cfg.CONF
LOG = logging.getLogger(__name__)


class Configurations(object):

    DEFAULT_LIMIT = CONF.configurations_page_size

    @staticmethod
    def load(context):
        if context is None:
            raise TypeError("Argument context not defined.")
        elif id is None:
            raise TypeError("Argument is not defined.")

        if context.is_admin:
            db_info = DBConfiguration.find_all(deleted=False)
            if db_info is None:
                LOG.debug("No configurations found")
        else:
            db_info = DBConfiguration.find_all(tenant_id=context.tenant,
                                               deleted=False)
            if db_info is None:
                LOG.debug("No configurations found for tenant % s"
                          % context.tenant)

        limit = int(context.limit or Configurations.DEFAULT_LIMIT)
        if limit > Configurations.DEFAULT_LIMIT:
            limit = Configurations.DEFAULT_LIMIT

        data_view = DBConfiguration.find_by_pagination('configurations',
                                                       db_info,
                                                       "foo",
                                                       limit=limit,
                                                       marker=context.marker)
        next_marker = data_view.next_page_marker
        return data_view.collection, next_marker


class Configuration(object):

    @property
    def instances(self):
        return self.instances

    @property
    def items(self):
        return self.items

    @staticmethod
    def create(name, description, tenant_id, datastore, datastore_version):
        configurationGroup = DBConfiguration.create(
            name=name,
            description=description,
            tenant_id=tenant_id,
            datastore_version_id=datastore_version)
        return configurationGroup

    @staticmethod
    def create_items(cfg_id, values):
        LOG.debug("saving the values to the database")
        LOG.debug("cfg_id: %s" % cfg_id)
        LOG.debug("values: %s" % values)
        config_items = []
        for key, val in values.iteritems():
            config_item = ConfigurationParameter.create(
                configuration_id=cfg_id,
                configuration_key=key,
                configuration_value=val)
            config_items.append(config_item)
        return config_items

    @staticmethod
    def delete(context, group):
        deleted_at = datetime.utcnow()
        Configuration.remove_all_items(context, group.id, deleted_at)
        group.deleted = True
        group.deleted_at = deleted_at
        group.save()

    @staticmethod
    def remove_all_items(context, id, deleted_at):
        LOG.debug("removing the values from the database with configuration"
                  " %s" % id)
        items = ConfigurationParameter.find_all(configuration_id=id,
                                                deleted=False).all()
        LOG.debug("removing items: %s" % items)
        for item in items:
            item.deleted = True
            item.deleted_at = deleted_at
            item.save()

    @staticmethod
    def load_configuration_datastore_version(context, id):
        config = Configuration.load(context, id)
        datastore_version = DatastoreVersion.load_by_uuid(
            config.datastore_version_id)
        return datastore_version

    @staticmethod
    def load(context, id):
        try:
            if context.is_admin:
                config_info = DBConfiguration.find_by(id=id,
                                                      deleted=False)
            else:
                config_info = DBConfiguration.find_by(id=id,
                                                      tenant_id=context.tenant,
                                                      deleted=False)
        except ModelNotFoundError:
            msg = _("Configuration group with ID %s could not be found.") % id
            raise ModelNotFoundError(msg)
        return config_info

    @staticmethod
    def load_items(context, id):
        datastore = Configuration.load_configuration_datastore_version(context,
                                                                       id)
        config_items = ConfigurationParameter.find_all(configuration_id=id,
                                                       deleted=False).all()
        rules = configurations.get_validation_rules(
            datastore_manager=datastore.manager)

        def _get_rule(key):
            LOG.debug("finding rule with key : %s" % key)
            for rule in rules['configuration-parameters']:
                if str(rule.get('name')) == key:
                    return rule

        for item in config_items:
            rule = _get_rule(str(item.configuration_key))
            if rule.get('type') == 'boolean':
                item.configuration_value = bool(int(item.configuration_value))
            elif rule.get('type') == 'integer':
                item.configuration_value = int(item.configuration_value)
            else:
                item.configuration_value = str(item.configuration_value)
        return config_items

    @staticmethod
    def get_configuration_overrides(context, configuration_id):
        """Gets the overrides dict to apply to an instance"""
        overrides = {}
        if configuration_id:
            config_items = Configuration.load_items(context,
                                                    id=configuration_id)

            for i in config_items:
                overrides[i.configuration_key] = i.configuration_value
        return overrides

    @staticmethod
    def save(context, configuration, configuration_items, instances):
        DBConfiguration.save(configuration)
        for item in configuration_items:
            item["deleted_at"] = None
            ConfigurationParameter.save(item)

        items = Configuration.load_items(context, configuration.id)

        for instance in instances:
            LOG.debug("applying to instance: %s" % instance.id)
            overrides = {}
            for i in items:
                overrides[i.configuration_key] = i.configuration_value

            task_api.API(context).update_overrides(instance.id, overrides)


class DBConfiguration(dbmodels.DatabaseModelBase):
    _data_fields = ['name', 'description', 'tenant_id', 'datastore_version_id',
                    'deleted', 'deleted_at']


class ConfigurationParameter(dbmodels.DatabaseModelBase):
    _data_fields = ['configuration_id', 'configuration_key',
                    'configuration_value', 'deleted',
                    'deleted_at']

    def __hash__(self):
        return self.configuration_key.__hash__()


def persisted_models():
    return {
        'configurations': DBConfiguration,
        'configuration_parameters': ConfigurationParameter
    }

########NEW FILE########
__FILENAME__ = service
# Copyright 2014 Rackspace
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from datetime import datetime
from trove.common import cfg
from trove.common import configurations
from trove.common import exception
from trove.common import pagination
from trove.common import wsgi
from trove.configuration import models
from trove.configuration import views
from trove.configuration.models import ConfigurationParameter
from trove.datastore import models as ds_models
from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _
from trove.instance import models as instances_models
import trove.common.apischema as apischema


CONF = cfg.CONF
LOG = logging.getLogger(__name__)


class ConfigurationsController(wsgi.Controller):

    schemas = apischema.configuration

    def index(self, req, tenant_id):
        context = req.environ[wsgi.CONTEXT_KEY]
        configs, marker = models.Configurations.load(context)
        view = views.ConfigurationsView(configs)
        paged = pagination.SimplePaginatedDataView(req.url, 'configurations',
                                                   view, marker)
        return wsgi.Result(paged.data(), 200)

    def show(self, req, tenant_id, id):
        context = req.environ[wsgi.CONTEXT_KEY]
        configuration = models.Configuration.load(context, id)
        configuration_items = models.Configuration.load_items(context, id)

        return wsgi.Result(views.DetailedConfigurationView(
                           configuration,
                           configuration_items).data(), 200)

    def instances(self, req, tenant_id, id):
        context = req.environ[wsgi.CONTEXT_KEY]
        configuration = models.Configuration.load(context, id)
        instances = instances_models.DBInstance.find_all(
            tenant_id=context.tenant,
            configuration_id=configuration.id,
            deleted=False)
        limit = int(context.limit or CONF.instances_page_size)
        if limit > CONF.instances_page_size:
            limit = CONF.instances_page_size
        data_view = instances_models.DBInstance.find_by_pagination(
            'instances', instances, "foo",
            limit=limit,
            marker=context.marker)
        view = views.DetailedConfigurationInstancesView(data_view.collection)
        paged = pagination.SimplePaginatedDataView(req.url, 'instances', view,
                                                   data_view.next_page_marker)
        return wsgi.Result(paged.data(), 200)

    def create(self, req, body, tenant_id):
        LOG.debug("req : '%s'\n\n" % req)
        LOG.debug("body : '%s'\n\n" % req)

        name = body['configuration']['name']
        description = body['configuration'].get('description')
        values = body['configuration']['values']

        datastore_args = body['configuration'].get('datastore', {})
        datastore, datastore_version = (
            ds_models.get_datastore_version(**datastore_args))

        configItems = []
        if values:
            # validate that the values passed in are permitted by the operator.
            ConfigurationsController._validate_configuration(
                body['configuration']['values'],
                datastore_manager=datastore_version.manager)

            for k, v in values.iteritems():
                configItems.append(ConfigurationParameter(
                    configuration_key=k,
                    configuration_value=v))

        cfg_group = models.Configuration.create(name, description, tenant_id,
                                                datastore.id,
                                                datastore_version.id)
        cfg_group_items = models.Configuration.create_items(cfg_group.id,
                                                            values)
        view_data = views.DetailedConfigurationView(cfg_group,
                                                    cfg_group_items)
        return wsgi.Result(view_data.data(), 200)

    def delete(self, req, tenant_id, id):
        context = req.environ[wsgi.CONTEXT_KEY]
        group = models.Configuration.load(context, id)
        instances = instances_models.DBInstance.find_all(
            tenant_id=context.tenant,
            configuration_id=id,
            deleted=False).all()
        if instances:
            raise exception.InstanceAssignedToConfiguration()
        models.Configuration.delete(context, group)
        return wsgi.Result(None, 202)

    def update(self, req, body, tenant_id, id):
        LOG.info(_("Updating configuration for tenant id %s") % tenant_id)
        context = req.environ[wsgi.CONTEXT_KEY]
        group = models.Configuration.load(context, id)
        instances = instances_models.DBInstance.find_all(
            tenant_id=context.tenant,
            configuration_id=id,
            deleted=False).all()

        # if name/description are provided in the request body, update the
        # model with these values as well.
        if 'name' in body['configuration']:
            group.name = body['configuration']['name']

        if 'description' in body['configuration']:
            group.description = body['configuration']['description']

        items = self._configuration_items_list(group, body['configuration'])
        deleted_at = datetime.utcnow()
        models.Configuration.remove_all_items(context, group.id, deleted_at)
        LOG.info(_("loaded configuration instances: %s") % instances)
        models.Configuration.save(context, group, items, instances)
        return wsgi.Result(None, 202)

    def edit(self, req, body, tenant_id, id):
        context = req.environ[wsgi.CONTEXT_KEY]
        group = models.Configuration.load(context, id)
        instances = instances_models.DBInstance.find_all(
            tenant_id=context.tenant,
            configuration_id=id,
            deleted=False).all()
        LOG.info(_("loaded configuration instances: %s") % instances)
        items = self._configuration_items_list(group, body['configuration'])
        models.Configuration.save(context, group, items, instances)

    def _configuration_items_list(self, group, configuration):
        ds_version_id = group.datastore_version_id
        ds_version = ds_models.DatastoreVersion.load_by_uuid(ds_version_id)
        items = []
        LOG.info(_("loaded configuration group: %s") % group)
        if 'values' in configuration:
            # validate that the values passed in are permitted by the operator.
            ConfigurationsController._validate_configuration(
                configuration['values'], datastore_manager=ds_version.manager)
            for k, v in configuration['values'].iteritems():
                items.append(ConfigurationParameter(configuration_id=group.id,
                                                    configuration_key=k,
                                                    configuration_value=v,
                                                    deleted=False))
        return items

    @staticmethod
    def _validate_configuration(values, datastore_manager=None):
        rules = configurations.get_validation_rules(
            datastore_manager=datastore_manager)

        LOG.info(_("Validating configuration values"))
        for k, v in values.iteritems():
            # get the validation rule dictionary, which will ensure there is a
            # rule for the given key name. An exception will be thrown if no
            # valid rule is located.
            rule = ConfigurationsController._get_item(
                k, rules['configuration-parameters'])

            if rule.get('deleted_at'):
                raise exception.ConfigurationParameterDeleted(
                    parameter_name=rule.get('name'),
                    parameter_deleted_at=rule.get('deleted_at'))

            # type checking
            valueType = rule.get('type')

            if not isinstance(v, ConfigurationsController._find_type(
                    valueType)):
                output = {"key": k, "type": valueType}
                msg = _("The value provided for the configuration "
                        "parameter %(key)s is not of type %(type)s.") % output
                raise exception.UnprocessableEntity(message=msg)

            # integer min/max checking
            if isinstance(v, int) and not isinstance(v, bool):
                try:
                    min_value = int(rule.get('min'))
                except ValueError:
                    raise exception.TroveError(_(
                        "Invalid or unsupported min value defined in the "
                        "configuration-parameters configuration file. "
                        "Expected integer."))
                if v < min_value:
                    output = {"key": k, "min": min_value}
                    message = _("The value for the configuration parameter "
                                "%(key)s is less than the minimum allowed: "
                                "%(min)s") % output
                    raise exception.UnprocessableEntity(message=message)

                try:
                    max_value = int(rule.get('max'))
                except ValueError:
                    raise exception.TroveError(_(
                        "Invalid or unsupported max value defined in the "
                        "configuration-parameters configuration file. "
                        "Expected integer."))
                if v > max_value:
                    output = {"key": k, "max": max_value}
                    message = _("The value for the configuration parameter "
                                "%(key)s is greater than the maximum "
                                "allowed: %(max)s") % output
                    raise exception.UnprocessableEntity(message=message)

    @staticmethod
    def _find_type(valueType):
        if valueType == "boolean":
            return bool
        elif valueType == "string":
            return basestring
        elif valueType == "integer":
            return int
        else:
            raise exception.TroveError(_(
                "Invalid or unsupported type defined in the "
                "configuration-parameters configuration file."))

    @staticmethod
    def _get_item(key, dictList):
        for item in dictList:
            if key == item.get('name'):
                return item
        raise exception.UnprocessableEntity(
            message=_("%s is not a supported configuration parameter.") % key)


class ParametersController(wsgi.Controller):
    def index(self, req, tenant_id, datastore, id):
        ds, ds_version = ds_models.get_datastore_version(
            type=datastore, version=id)
        rules = configurations.get_validation_rules(
            datastore_manager=ds_version.manager)
        return wsgi.Result(views.ConfigurationParametersView(rules).data(),
                           200)

    def show(self, req, tenant_id, datastore, id, name):
        ds, ds_version = ds_models.get_datastore_version(
            type=datastore, version=id)
        rules = configurations.get_validation_rules(
            datastore_manager=ds_version.manager)
        for rule in rules['configuration-parameters']:
            if rule['name'] == name:
                return wsgi.Result(
                    views.ConfigurationParametersView(rule).data(), 200)
        raise exception.ConfigKeyNotFound(key=name)

    def index_by_version(self, req, tenant_id, version):
        ds_version = ds_models.DatastoreVersion.load_by_uuid(version)
        rules = configurations.get_validation_rules(
            datastore_manager=ds_version.manager)
        return wsgi.Result(views.ConfigurationParametersView(rules).data(),
                           200)

    def show_by_version(self, req, tenant_id, version, name):
        ds_version = ds_models.DatastoreVersion.load_by_uuid(version)
        rules = configurations.get_validation_rules(
            datastore_manager=ds_version.manager)
        for rule in rules['configuration-parameters']:
            if rule['name'] == name:
                return wsgi.Result(
                    views.ConfigurationParametersView(rule).data(), 200)
        raise exception.ConfigKeyNotFound(key=name)

########NEW FILE########
__FILENAME__ = views
# Copyright 2014 Rackspace
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from trove.openstack.common import log as logging

LOG = logging.getLogger(__name__)


class ConfigurationView(object):

    def __init__(self, configuration):
        self.configuration = configuration

    def data(self):
        configuration_dict = {
            "id": self.configuration.id,
            "name": self.configuration.name,
            "description": self.configuration.description,
            "datastore_version_id": self.configuration.datastore_version_id,
        }

        return {"configuration": configuration_dict}


class ConfigurationsView(object):

    def __init__(self, configurations):
        self.configurations = configurations

    def data(self):
        data = []

        for configuration in self.configurations:
            data.append(self.data_for_configuration(configuration))

        return {"configurations": data}

    def data_for_configuration(self, configuration):
        view = ConfigurationView(configuration)
        return view.data()['configuration']


class DetailedConfigurationInstancesView(object):

    def __init__(self, instances):
        self.instances = instances

    def instance_data(self):
        instances_list = []
        if self.instances:
            for instance in self.instances:
                instances_list.append(
                    {
                        "id": instance.id,
                        "name": instance.name
                    }
                )
        return instances_list

    def data(self):

        return {"instances": self.instance_data()}


class DetailedConfigurationView(object):

    def __init__(self, configuration, configuration_items):
        self.configuration = configuration
        self.configuration_items = configuration_items

    def data(self):
        values = {}

        for configItem in self.configuration_items:
            key = configItem.configuration_key
            value = configItem.configuration_value
            values[key] = value
        configuration_dict = {
            "id": self.configuration.id,
            "name": self.configuration.name,
            "description": self.configuration.description,
            "values": values,
            "datastore_version_id": self.configuration.datastore_version_id,
        }

        return {"configuration": configuration_dict}


class ConfigurationParametersView(object):

    def __init__(self, configuration_parameters):
        self.configuration_parameters = configuration_parameters

    def data(self):
        return self.configuration_parameters

########NEW FILE########
__FILENAME__ = models
# Copyright 2013 OpenStack Foundation
# Copyright 2013 Rackspace Hosting
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

from trove.common import cfg
from trove.common import exception
from trove.common import utils
from trove.db import models as dbmodels
from trove.db import get_db_api
from trove.openstack.common import log as logging


LOG = logging.getLogger(__name__)
CONF = cfg.CONF
db_api = get_db_api()


def persisted_models():
    return {
        'datastore': DBDatastore,
        'datastore_version': DBDatastoreVersion,
    }


class DBDatastore(dbmodels.DatabaseModelBase):

    _data_fields = ['id', 'name', 'default_version_id']


class DBDatastoreVersion(dbmodels.DatabaseModelBase):

    _data_fields = ['id', 'datastore_id', 'name', 'manager', 'image_id',
                    'packages', 'active']


class Datastore(object):

    def __init__(self, db_info):
        self.db_info = db_info

    @classmethod
    def load(cls, id_or_name):
        try:
            return cls(DBDatastore.find_by(id=id_or_name))
        except exception.ModelNotFoundError:
            try:
                return cls(DBDatastore.find_by(name=id_or_name))
            except exception.ModelNotFoundError:
                raise exception.DatastoreNotFound(datastore=id_or_name)

    @property
    def id(self):
        return self.db_info.id

    @property
    def name(self):
        return self.db_info.name

    @property
    def default_version_id(self):
        return self.db_info.default_version_id


class Datastores(object):

    def __init__(self, db_info):
        self.db_info = db_info

    @classmethod
    def load(cls, only_active=True):
        datastores = DBDatastore.find_all()
        if only_active:
            datastores = datastores.join(DBDatastoreVersion).filter(
                DBDatastoreVersion.active == 1)
        return cls(datastores)

    def __iter__(self):
        for item in self.db_info:
            yield item


class DatastoreVersion(object):

    def __init__(self, db_info):
        self.db_info = db_info
        self._datastore_name = None

    @classmethod
    def load(cls, datastore, id_or_name):
        try:
            return cls(DBDatastoreVersion.find_by(datastore_id=datastore.id,
                                                  id=id_or_name))
        except exception.ModelNotFoundError:
            versions = DBDatastoreVersion.find_all(datastore_id=datastore.id,
                                                   name=id_or_name)
            if versions.count() == 0:
                raise exception.DatastoreVersionNotFound(version=id_or_name)
            if versions.count() > 1:
                raise exception.NoUniqueMatch(name=id_or_name)
            return cls(versions.first())

    @classmethod
    def load_by_uuid(cls, uuid):
        try:
            return cls(DBDatastoreVersion.find_by(id=uuid))
        except exception.ModelNotFoundError:
            raise exception.DatastoreVersionNotFound(version=uuid)

    @property
    def id(self):
        return self.db_info.id

    @property
    def datastore_id(self):
        return self.db_info.datastore_id

    @property
    def datastore_name(self):
        if self._datastore_name is None:
            self._datastore_name = Datastore.load(self.datastore_id).name
        return self._datastore_name

    #TODO(tim.simpson): This would be less confusing if it was called "version"
    #                   and datastore_name was called "name".
    @property
    def name(self):
        return self.db_info.name

    @property
    def image_id(self):
        return self.db_info.image_id

    @property
    def packages(self):
        return self.db_info.packages

    @property
    def active(self):
        return (True if self.db_info.active else False)

    @property
    def manager(self):
        return self.db_info.manager


class DatastoreVersions(object):

    def __init__(self, db_info):
        self.db_info = db_info

    @classmethod
    def load(cls, id_or_name, only_active=True):
        datastore = Datastore.load(id_or_name)
        if only_active:
            versions = DBDatastoreVersion.find_all(datastore_id=datastore.id,
                                                   active=True)
        else:
            versions = DBDatastoreVersion.find_all(datastore_id=datastore.id)
        return cls(versions)

    def __iter__(self):
        for item in self.db_info:
            yield item


def get_datastore_version(type=None, version=None):
    datastore = type or CONF.default_datastore
    if not datastore:
        raise exception.DatastoreDefaultDatastoreNotFound()
    datastore = Datastore.load(datastore)
    version = version or datastore.default_version_id
    if not version:
        raise exception.DatastoreDefaultVersionNotFound(datastore=
                                                        datastore.name)
    datastore_version = DatastoreVersion.load(datastore, version)
    if datastore_version.datastore_id != datastore.id:
        raise exception.DatastoreNoVersion(datastore=datastore.name,
                                           version=datastore_version.name)
    if not datastore_version.active:
        raise exception.DatastoreVersionInactive(version=
                                                 datastore_version.name)
    return (datastore, datastore_version)


def update_datastore(name, default_version):
    db_api.configure_db(CONF)
    try:
        datastore = DBDatastore.find_by(name=name)
    except exception.ModelNotFoundError:
        # Create a new one
        datastore = DBDatastore()
        datastore.id = utils.generate_uuid()
        datastore.name = name

    if default_version:
        version = DatastoreVersion.load(datastore, default_version)
        if not version.active:
            raise exception.DatastoreVersionInactive(version=version.name)
        datastore.default_version_id = version.id

    db_api.save(datastore)


def update_datastore_version(datastore, name, manager, image_id, packages,
                             active):
    db_api.configure_db(CONF)
    datastore = Datastore.load(datastore)
    try:
        version = DBDatastoreVersion.find_by(datastore_id=datastore.id,
                                             name=name)
    except exception.ModelNotFoundError:
        # Create a new one
        version = DBDatastoreVersion()
        version.id = utils.generate_uuid()
        version.name = name
        version.datastore_id = datastore.id
    version.manager = manager
    version.image_id = image_id
    version.packages = packages
    version.active = active
    db_api.save(version)

########NEW FILE########
__FILENAME__ = service
# Copyright 2013 OpenStack Foundation
# Copyright 2013 Rackspace Hosting
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

from trove.common import wsgi
from trove.datastore import models, views


class DatastoreController(wsgi.Controller):

    def show(self, req, tenant_id, id):
        datastore = models.Datastore.load(id)
        return wsgi.Result(views.
                           DatastoreView(datastore, req).data(), 200)

    def index(self, req, tenant_id):
        context = req.environ[wsgi.CONTEXT_KEY]
        only_active = True
        if context.is_admin:
            only_active = False
        datastores = models.Datastores.load(only_active)
        return wsgi.Result(views.
                           DatastoresView(datastores, req).data(),
                           200)

    def version_show(self, req, tenant_id, datastore, id):
        datastore = models.Datastore.load(datastore)
        datastore_version = models.DatastoreVersion.load(datastore, id)
        return wsgi.Result(views.DatastoreVersionView(datastore_version,
                                                      req).data(), 200)

    def version_show_by_uuid(self, req, tenant_id, uuid):
        datastore_version = models.DatastoreVersion.load_by_uuid(uuid)
        return wsgi.Result(views.DatastoreVersionView(datastore_version,
                                                      req).data(), 200)

    def version_index(self, req, tenant_id, datastore):
        context = req.environ[wsgi.CONTEXT_KEY]
        only_active = True
        if context.is_admin:
            only_active = False
        datastore_versions = models.DatastoreVersions.load(datastore,
                                                           only_active)
        return wsgi.Result(views.
                           DatastoreVersionsView(datastore_versions,
                                                 req).data(), 200)

########NEW FILE########
__FILENAME__ = views
# Copyright 2013 OpenStack Foundation
# Copyright 2013 Rackspace Hosting
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

from trove.common import wsgi
from trove.common.views import create_links


class DatastoreView(object):

    def __init__(self, datastore, req=None):
        self.datastore = datastore
        self.req = req

    def data(self):
        datastore_dict = {
            "id": self.datastore.id,
            "name": self.datastore.name,
            "links": self._build_links(),
        }
        default_version = self.datastore.default_version_id
        if default_version:
            datastore_dict["default_version"] = default_version

        return {"datastore": datastore_dict}

    def _build_links(self):
        return create_links("datastores", self.req,
                            self.datastore.id)


class DatastoresView(object):

    def __init__(self, datastores, req=None):
        self.datastores = datastores
        self.req = req

    def data(self):
        data = []
        for datastore in self.datastores:
            data.append(self.data_for_datastore(datastore))
        return {'datastores': data}

    def data_for_datastore(self, datastore):
        view = DatastoreView(datastore, req=self.req)
        return view.data()['datastore']


class DatastoreVersionView(object):

    def __init__(self, datastore_version, req=None):
        self.datastore_version = datastore_version
        self.req = req
        self.context = req.environ[wsgi.CONTEXT_KEY]

    def data(self):
        datastore_version_dict = {
            "id": self.datastore_version.id,
            "name": self.datastore_version.name,
            "datastore": self.datastore_version.datastore_id,
            "links": self._build_links(),
        }
        if self.context.is_admin:
            datastore_version_dict['active'] = self.datastore_version.active
            datastore_version_dict['packages'] = (self.datastore_version.
                                                  packages)
            datastore_version_dict['image'] = self.datastore_version.image_id
        return {"version": datastore_version_dict}

    def _build_links(self):
        return create_links("datastores/versions",
                            self.req, self.datastore_version.id)


class DatastoreVersionsView(object):

    def __init__(self, datastore_versions, req=None):
        self.datastore_versions = datastore_versions
        self.req = req

    def data(self):
        data = []
        for datastore_version in self.datastore_versions:
            data.append(self.
                        data_for_datastore_version(datastore_version))
        return {'versions': data}

    def data_for_datastore_version(self, datastore_version):
        view = DatastoreVersionView(datastore_version, req=self.req)
        return view.data()['version']

########NEW FILE########
__FILENAME__ = models
#    Copyright 2011 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from trove.db import get_db_api
from trove.db import db_query
from trove.common import exception
from trove.common import models
from trove.common import pagination
from trove.common import utils
from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _

LOG = logging.getLogger(__name__)


class DatabaseModelBase(models.ModelBase):
    _auto_generated_attrs = ['id']

    @classmethod
    def create(cls, **values):
        init_vals = {
            'id': utils.generate_uuid(),
            'created': utils.utcnow(),
        }
        if hasattr(cls, 'deleted'):
            init_vals['deleted'] = False
        init_vals.update(values)
        instance = cls(**init_vals)
        if not instance.is_valid():
            raise exception.InvalidModelError(errors=instance.errors)
        return instance.save()

    @property
    def db_api(self):
        return get_db_api()

    @property
    def preserve_on_delete(self):
        return hasattr(self, 'deleted') and hasattr(self, 'deleted_at')

    @classmethod
    def query(cls):
        return get_db_api()._base_query(cls)

    def save(self):
        if not self.is_valid():
            raise exception.InvalidModelError(errors=self.errors)
        self['updated'] = utils.utcnow()
        LOG.debug("Saving %(name)s: %(dict)s" %
                  {'name': self.__class__.__name__, 'dict': self.__dict__})
        return self.db_api.save(self)

    def delete(self):
        self['updated'] = utils.utcnow()
        LOG.debug("Deleting %(name)s: %(dict)s" %
                  {'name': self.__class__.__name__, 'dict': self.__dict__})

        if self.preserve_on_delete:
            self['deleted_at'] = utils.utcnow()
            self['deleted'] = True
            return self.db_api.save(self)
        else:
            return self.db_api.delete(self)

    def update(self, **values):
        for key in values:
            if hasattr(self, key):
                setattr(self, key, values[key])
        self['updated'] = utils.utcnow()
        return self.db_api.save(self)

    def __init__(self, **kwargs):
        self.merge_attributes(kwargs)
        if not self.is_valid():
            raise exception.InvalidModelError(errors=self.errors)

    def merge_attributes(self, values):
        """dict.update() behaviour."""
        for k, v in values.iteritems():
            self[k] = v

    @classmethod
    def find_by(cls, context=None, **conditions):
        model = cls.get_by(**conditions)

        if model is None:
            raise exception.ModelNotFoundError(_("%s Not Found") %
                                               cls.__name__)

        if ((context and not context.is_admin and hasattr(model, 'tenant_id')
             and model.tenant_id != context.tenant)):
            LOG.error("Tenant %s tried to access %s, owned by %s."
                      % (context.tenant, cls.__name__, model.tenant_id))
            raise exception.ModelNotFoundError(_("%s Not Found") %
                                               cls.__name__)

        return model

    @classmethod
    def get_by(cls, **kwargs):
        return get_db_api().find_by(cls, **cls._process_conditions(kwargs))

    @classmethod
    def find_all(cls, **kwargs):
        return db_query.find_all(cls, **cls._process_conditions(kwargs))

    @classmethod
    def _process_conditions(cls, raw_conditions):
        """Override in inheritors to format/modify any conditions."""
        return raw_conditions

    @classmethod
    def find_by_pagination(cls, collection_type, collection_query,
                           paginated_url, **kwargs):
        elements, next_marker = collection_query.paginated_collection(**kwargs)

        return pagination.PaginatedDataView(collection_type,
                                            elements,
                                            paginated_url,
                                            next_marker)

########NEW FILE########
__FILENAME__ = api
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import sqlalchemy.exc

from trove.common import exception
from trove.db.sqlalchemy import migration
from trove.db.sqlalchemy import session


def list(query_func, *args, **kwargs):
    return query_func(*args, **kwargs).all()


def count(query, *args, **kwargs):
    return query(*args, **kwargs).count()


def first(query, *args, **kwargs):
    return query(*args, **kwargs).first()


def join(query, model, *args):
    return query(model).join(*args)


def find_all(model, **conditions):
    return _query_by(model, **conditions)


def find_all_by_limit(query_func, model, conditions, limit, marker=None,
                      marker_column=None):
    return _limits(query_func, model, conditions, limit, marker,
                   marker_column).all()


def find_by(model, **kwargs):
    return _query_by(model, **kwargs).first()


def save(model):
    try:
        db_session = session.get_session()
        model = db_session.merge(model)
        db_session.flush()
        return model
    except sqlalchemy.exc.IntegrityError as error:
        raise exception.DBConstraintError(model_name=model.__class__.__name__,
                                          error=str(error.orig))


def delete(model):
    db_session = session.get_session()
    model = db_session.merge(model)
    db_session.delete(model)
    db_session.flush()


def delete_all(query_func, model, **conditions):
    query_func(model, **conditions).delete()


def update(model, **values):
    for k, v in values.iteritems():
        model[k] = v


def update_all(query_func, model, conditions, values):
    query_func(model, **conditions).update(values)


def configure_db(options, *plugins):
    session.configure_db(options)
    configure_db_for_plugins(options, *plugins)


def configure_db_for_plugins(options, *plugins):
    for plugin in plugins:
        session.configure_db(options, models_mapper=plugin.mapper)


def drop_db(options):
    session.drop_db(options)


def clean_db():
    session.clean_db()


def db_sync(options, version=None, repo_path=None):
    migration.db_sync(options, version, repo_path)


def db_upgrade(options, version=None, repo_path=None):
    migration.upgrade(options, version, repo_path)


def db_downgrade(options, version, repo_path=None):
    migration.downgrade(options, version, repo_path)


def db_reset(options, *plugins):
    drop_db(options)
    db_sync(options)
    configure_db(options)


def _base_query(cls):
    return session.get_session().query(cls)


def _query_by(cls, **conditions):
    query = _base_query(cls)
    if conditions:
        query = query.filter_by(**conditions)
    return query


def _limits(query_func, model, conditions, limit, marker, marker_column=None):
    query = query_func(model, **conditions)
    marker_column = marker_column or model.id
    if marker:
        query = query.filter(marker_column > marker)
    return query.order_by(marker_column).limit(limit)

########NEW FILE########
__FILENAME__ = mappers
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from sqlalchemy import MetaData
from sqlalchemy import Table
from sqlalchemy import orm
from sqlalchemy.orm import exc as orm_exc


def map(engine, models):
    meta = MetaData()
    meta.bind = engine
    if mapping_exists(models['instance']):
        return

    orm.mapper(models['instance'], Table('instances', meta, autoload=True))
    orm.mapper(models['root_enabled_history'],
               Table('root_enabled_history', meta, autoload=True))
    orm.mapper(models['datastore'],
               Table('datastores', meta, autoload=True))
    orm.mapper(models['datastore_version'],
               Table('datastore_versions', meta, autoload=True))
    orm.mapper(models['service_statuses'],
               Table('service_statuses', meta, autoload=True))
    orm.mapper(models['dns_records'],
               Table('dns_records', meta, autoload=True))
    orm.mapper(models['agent_heartbeats'],
               Table('agent_heartbeats', meta, autoload=True))
    orm.mapper(models['quotas'],
               Table('quotas', meta, autoload=True))
    orm.mapper(models['quota_usages'],
               Table('quota_usages', meta, autoload=True))
    orm.mapper(models['reservations'],
               Table('reservations', meta, autoload=True))
    orm.mapper(models['backups'],
               Table('backups', meta, autoload=True))
    orm.mapper(models['security_group'],
               Table('security_groups', meta, autoload=True))
    orm.mapper(models['security_group_rule'],
               Table('security_group_rules', meta, autoload=True))
    orm.mapper(models['security_group_instance_association'],
               Table('security_group_instance_associations', meta,
                     autoload=True))
    orm.mapper(models['configurations'],
               Table('configurations', meta, autoload=True))
    orm.mapper(models['configuration_parameters'],
               Table('configuration_parameters', meta, autoload=True))
    orm.mapper(models['conductor_lastseen'],
               Table('conductor_lastseen', meta, autoload=True))


def mapping_exists(model):
    try:
        orm.class_mapper(model)
        return True
    except orm_exc.UnmappedClassError:
        return False

########NEW FILE########
__FILENAME__ = manage
#!/usr/bin/env python

# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from migrate.versioning.shell import main

if __name__ == "__main__":
    main(debug='False', repository='.')

########NEW FILE########
__FILENAME__ = schema
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Various conveniences used for migration scripts."""

from trove.openstack.common import log as logging
import sqlalchemy.types


logger = logging.getLogger('trove.db.sqlalchemy.migrate_repo.schema')


class String(sqlalchemy.types.String):
    def __init__(self, length, *args, **kwargs):
        super(String, self).__init__(*args, length=length, **kwargs)


class Text(sqlalchemy.types.Text):
    def __init__(self, length=None, *args, **kwargs):
        super(Text, self).__init__(*args, length=length, **kwargs)


class Boolean(sqlalchemy.types.Boolean):
    def __init__(self, create_constraint=True, name=None, *args, **kwargs):
        super(Boolean, self).__init__(*args,
                                      create_constraint=create_constraint,
                                      name=name,
                                      **kwargs)


class DateTime(sqlalchemy.types.DateTime):
    def __init__(self, timezone=False, *args, **kwargs):
        super(DateTime, self).__init__(*args,
                                       timezone=timezone,
                                       **kwargs)


class Integer(sqlalchemy.types.Integer):
    def __init__(self, *args, **kwargs):
        super(Integer, self).__init__(*args, **kwargs)


class BigInteger(sqlalchemy.types.BigInteger):
    def __init__(self, *args, **kwargs):
        super(BigInteger, self).__init__(*args, **kwargs)


class Float(sqlalchemy.types.Float):
    def __init__(self, *args, **kwargs):
        super(Float, self).__init__(*args, **kwargs)


def create_tables(tables):
    for table in tables:
        logger.info("creating table %(table)s" % {'table': table})
        table.create()


def drop_tables(tables):
    for table in tables:
        logger.info("dropping table %(table)s" % {'table': table})
        table.drop()


def Table(name, metadata, *args, **kwargs):
    return sqlalchemy.schema.Table(name, metadata, *args,
                                   mysql_engine='INNODB', **kwargs)

########NEW FILE########
__FILENAME__ = 001_base_schema
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from sqlalchemy.schema import Column
from sqlalchemy.schema import MetaData

from trove.db.sqlalchemy.migrate_repo.schema import create_tables
from trove.db.sqlalchemy.migrate_repo.schema import DateTime
from trove.db.sqlalchemy.migrate_repo.schema import drop_tables
from trove.db.sqlalchemy.migrate_repo.schema import Integer
from trove.db.sqlalchemy.migrate_repo.schema import String
from trove.db.sqlalchemy.migrate_repo.schema import Table


meta = MetaData()

instances = Table(
    'instances',
    meta,
    Column('id', String(36), primary_key=True, nullable=False),
    Column('created', DateTime()),
    Column('updated', DateTime()),
    Column('name', String(255)),
    Column('hostname', String(255)),
    Column('compute_instance_id', String(36)),
    Column('task_id', Integer()),
    Column('task_description', String(32)),
    Column('task_start_time', DateTime()),
    Column('volume_id', String(36)))


def upgrade(migrate_engine):
    meta.bind = migrate_engine
    create_tables([instances])


def downgrade(migrate_engine):
    meta.bind = migrate_engine
    drop_tables([instances])

########NEW FILE########
__FILENAME__ = 002_service_images
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from sqlalchemy.schema import Column
from sqlalchemy.schema import MetaData

from trove.db.sqlalchemy.migrate_repo.schema import create_tables
from trove.db.sqlalchemy.migrate_repo.schema import drop_tables
from trove.db.sqlalchemy.migrate_repo.schema import String
from trove.db.sqlalchemy.migrate_repo.schema import Table


meta = MetaData()

service_images = Table(
    'service_images',
    meta,
    Column('id', String(36), primary_key=True, nullable=False),
    Column('service_name', String(255)),
    Column('image_id', String(255)))


def upgrade(migrate_engine):
    meta.bind = migrate_engine
    create_tables([service_images])


def downgrade(migrate_engine):
    meta.bind = migrate_engine
    drop_tables([service_images])

########NEW FILE########
__FILENAME__ = 003_service_statuses
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from sqlalchemy.schema import Column
from sqlalchemy.schema import MetaData

from trove.db.sqlalchemy.migrate_repo.schema import create_tables
from trove.db.sqlalchemy.migrate_repo.schema import DateTime
from trove.db.sqlalchemy.migrate_repo.schema import drop_tables
from trove.db.sqlalchemy.migrate_repo.schema import Integer
from trove.db.sqlalchemy.migrate_repo.schema import String
from trove.db.sqlalchemy.migrate_repo.schema import Table


meta = MetaData()

service_statuses = Table(
    'service_statuses',
    meta,
    Column('id', String(36), primary_key=True, nullable=False),
    Column('instance_id', String(36), nullable=False),
    Column('status_id', Integer(), nullable=False),
    Column('status_description', String(64), nullable=False),
    Column('updated_at', DateTime()))


def upgrade(migrate_engine):
    meta.bind = migrate_engine
    create_tables([service_statuses])


def downgrade(migrate_engine):
    meta.bind = migrate_engine
    drop_tables([service_statuses])

########NEW FILE########
__FILENAME__ = 004_root_enabled
# Copyright 2012 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from sqlalchemy.schema import Column
from sqlalchemy.schema import MetaData

from trove.db.sqlalchemy.migrate_repo.schema import create_tables
from trove.db.sqlalchemy.migrate_repo.schema import DateTime
from trove.db.sqlalchemy.migrate_repo.schema import drop_tables
from trove.db.sqlalchemy.migrate_repo.schema import String
from trove.db.sqlalchemy.migrate_repo.schema import Table


meta = MetaData()

root_enabled_history = Table(
    'root_enabled_history',
    meta,
    Column('id', String(36), primary_key=True, nullable=False),
    Column('user', String(length=255)),
    Column('created', DateTime()),
)


def upgrade(migrate_engine):
    meta.bind = migrate_engine
    create_tables([root_enabled_history])


def downgrade(migrate_engine):
    meta.bind = migrate_engine
    drop_tables([root_enabled_history])

########NEW FILE########
__FILENAME__ = 005_heartbeat
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from sqlalchemy.schema import Column
from sqlalchemy.schema import MetaData

from trove.db.sqlalchemy.migrate_repo.schema import create_tables
from trove.db.sqlalchemy.migrate_repo.schema import DateTime
from trove.db.sqlalchemy.migrate_repo.schema import drop_tables
from trove.db.sqlalchemy.migrate_repo.schema import String
from trove.db.sqlalchemy.migrate_repo.schema import Table


meta = MetaData()

agent_heartbeats = Table(
    'agent_heartbeats',
    meta,
    Column('id', String(36), primary_key=True, nullable=False),
    Column('instance_id', String(36), nullable=False),
    Column('updated_at', DateTime()))


def upgrade(migrate_engine):
    meta.bind = migrate_engine
    create_tables([agent_heartbeats])


def downgrade(migrate_engine):
    meta.bind = migrate_engine
    drop_tables([agent_heartbeats])

########NEW FILE########
__FILENAME__ = 006_dns_records
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from sqlalchemy.schema import Column
from sqlalchemy.schema import MetaData

from trove.db.sqlalchemy.migrate_repo.schema import Table
from trove.db.sqlalchemy.migrate_repo.schema import create_tables
from trove.db.sqlalchemy.migrate_repo.schema import drop_tables
from trove.db.sqlalchemy.migrate_repo.schema import String


meta = MetaData()


dns_records = Table(
    'dns_records', meta,
    Column('name', String(length=255), primary_key=True),
    Column('record_id', String(length=64)))


def upgrade(migrate_engine):
    meta.bind = migrate_engine
    create_tables([dns_records])


def downgrade(migrate_engine):
    meta.bind = migrate_engine
    drop_tables([dns_records])

########NEW FILE########
__FILENAME__ = 007_add_volume_flavor
# Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from sqlalchemy.schema import Column
from sqlalchemy.schema import MetaData

from trove.db.sqlalchemy.migrate_repo.schema import Integer
from trove.db.sqlalchemy.migrate_repo.schema import String
from trove.db.sqlalchemy.migrate_repo.schema import Table


def upgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine

    # add column:
    instances = Table('instances', meta, autoload=True)
    volume_size = Column('volume_size', Integer())
    flavor_id = Column('flavor_id', String(36))

    instances.create_column(flavor_id)
    instances.create_column(volume_size)


def downgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine

    # drop column:
    instances = Table('instances', meta, autoload=True)

    instances.drop_column('flavor_id')
    instances.drop_column('volume_size')

########NEW FILE########
__FILENAME__ = 008_add_instance_fields
# Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from sqlalchemy.schema import Column
from sqlalchemy.schema import MetaData

from trove.db.sqlalchemy.migrate_repo.schema import String
from trove.db.sqlalchemy.migrate_repo.schema import Table


def upgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine

    # add column:
    instances = Table('instances', meta, autoload=True)
    instances.create_column(Column('tenant_id', String(36), nullable=True))
    instances.create_column(Column('server_status', String(64)))


def downgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine

    # drop column:
    instances = Table('instances', meta, autoload=True)

    instances.drop_column('tenant_id')
    instances.drop_column('server_status')

########NEW FILE########
__FILENAME__ = 009_add_deleted_flag_to_instances
# Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from sqlalchemy.schema import Column
from sqlalchemy.schema import MetaData

from trove.db.sqlalchemy.migrate_repo.schema import Boolean
from trove.db.sqlalchemy.migrate_repo.schema import DateTime
from trove.db.sqlalchemy.migrate_repo.schema import Table


def upgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine

    # add column:
    instances = Table('instances', meta, autoload=True)
    instances.create_column(Column('deleted', Boolean()))
    instances.create_column(Column('deleted_at', DateTime()))


def downgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine

    # drop column:
    instances = Table('instances', meta, autoload=True)
    instances.drop_column('deleted')
    instances.drop_column('deleted_at')

########NEW FILE########
__FILENAME__ = 010_add_usage
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from sqlalchemy.schema import Column
from sqlalchemy.schema import MetaData

from trove.db.sqlalchemy.migrate_repo.schema import create_tables
from trove.db.sqlalchemy.migrate_repo.schema import DateTime
from trove.db.sqlalchemy.migrate_repo.schema import drop_tables
from trove.db.sqlalchemy.migrate_repo.schema import Integer
from trove.db.sqlalchemy.migrate_repo.schema import String
from trove.db.sqlalchemy.migrate_repo.schema import Table


meta = MetaData()


usage_events = Table(
    'usage_events',
    meta,
    Column('id', String(36), primary_key=True, nullable=False),
    Column('instance_name', String(36)),
    Column('tenant_id', String(36)),
    Column('nova_instance_id', String(36)),
    Column('instance_size', Integer()),
    Column('nova_volume_id', String(36)),
    Column('volume_size', Integer()),
    Column('end_time', DateTime()),
    Column('updated', DateTime()))


def upgrade(migrate_engine):
    meta.bind = migrate_engine
    create_tables([usage_events])


def downgrade(migrate_engine):
    meta.bind = migrate_engine
    drop_tables([usage_events])

########NEW FILE########
__FILENAME__ = 011_quota
#Copyright [2013] Hewlett-Packard Development Company, L.P.

#Licensed under the Apache License, Version 2.0 (the "License");
#you may not use this file except in compliance with the License.
#You may obtain a copy of the License at
#
#http://www.apache.org/licenses/LICENSE-2.0
#
#Unless required by applicable law or agreed to in writing, software
#distributed under the License is distributed on an "AS IS" BASIS,
#WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#See the License for the specific language governing permissions and
#limitations under the License.

from sqlalchemy.schema import Column
from sqlalchemy.schema import MetaData
from sqlalchemy.schema import UniqueConstraint

from trove.db.sqlalchemy.migrate_repo.schema import create_tables
from trove.db.sqlalchemy.migrate_repo.schema import DateTime
from trove.db.sqlalchemy.migrate_repo.schema import drop_tables
from trove.db.sqlalchemy.migrate_repo.schema import Integer
from trove.db.sqlalchemy.migrate_repo.schema import String
from trove.db.sqlalchemy.migrate_repo.schema import Table


meta = MetaData()

quotas = Table('quotas', meta,
               Column('id', String(36),
                      primary_key=True, nullable=False),
               Column('created', DateTime()),
               Column('updated', DateTime()),
               Column('tenant_id', String(36)),
               Column('resource', String(length=255), nullable=False),
               Column('hard_limit', Integer()),
               UniqueConstraint('tenant_id', 'resource'))

quota_usages = Table('quota_usages', meta,
                     Column('id', String(36),
                            primary_key=True, nullable=False),
                     Column('created', DateTime()),
                     Column('updated', DateTime()),
                     Column('tenant_id', String(36)),
                     Column('in_use', Integer(), default=0),
                     Column('reserved', Integer(), default=0),
                     Column('resource', String(length=255), nullable=False),
                     UniqueConstraint('tenant_id', 'resource'))

reservations = Table('reservations', meta,
                     Column('created', DateTime()),
                     Column('updated', DateTime()),
                     Column('id', String(36),
                            primary_key=True, nullable=False),
                     Column('usage_id', String(36)),
                     Column('delta', Integer(), nullable=False),
                     Column('status', String(length=36)))


def upgrade(migrate_engine):
    meta.bind = migrate_engine
    create_tables([quotas, quota_usages, reservations])


def downgrade(migrate_engine):
    meta.bind = migrate_engine
    drop_tables([quotas, quota_usages, reservations])

########NEW FILE########
__FILENAME__ = 012_backup
#Copyright [2013] Hewlett-Packard Development Company, L.P.

#Licensed under the Apache License, Version 2.0 (the "License");
#you may not use this file except in compliance with the License.
#You may obtain a copy of the License at
#
#http://www.apache.org/licenses/LICENSE-2.0
#
#Unless required by applicable law or agreed to in writing, software
#distributed under the License is distributed on an "AS IS" BASIS,
#WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#See the License for the specific language governing permissions and
#limitations under the License.

from sqlalchemy.schema import Column
from sqlalchemy.schema import MetaData

from trove.db.sqlalchemy.migrate_repo.schema import create_tables
from trove.db.sqlalchemy.migrate_repo.schema import DateTime
from trove.db.sqlalchemy.migrate_repo.schema import drop_tables
from trove.db.sqlalchemy.migrate_repo.schema import Float
from trove.db.sqlalchemy.migrate_repo.schema import String
from trove.db.sqlalchemy.migrate_repo.schema import Table
from trove.db.sqlalchemy.migrate_repo.schema import Boolean

meta = MetaData()

backups = Table('backups', meta,
                Column('id', String(36), primary_key=True, nullable=False),
                Column('name', String(255), nullable=False),
                Column('description', String(512)),
                Column('location', String(1024)),
                Column('backup_type', String(32)),
                Column('size', Float()),
                Column('tenant_id', String(36)),
                Column('state', String(32), nullable=False),
                Column('instance_id', String(36)),
                Column('checksum', String(32)),
                Column('backup_timestamp', DateTime()),
                Column('deleted', Boolean()),
                Column('created', DateTime()),
                Column('updated', DateTime()),
                Column('deleted_at', DateTime()))


def upgrade(migrate_engine):
    meta.bind = migrate_engine
    create_tables([backups, ])


def downgrade(migrate_engine):
    meta.bind = migrate_engine
    drop_tables([backups, ])

########NEW FILE########
__FILENAME__ = 013_add_security_group_artifacts
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from sqlalchemy import ForeignKey
from sqlalchemy.schema import Column
from sqlalchemy.schema import MetaData

from trove.db.sqlalchemy.migrate_repo.schema import create_tables
from trove.db.sqlalchemy.migrate_repo.schema import drop_tables
from trove.db.sqlalchemy.migrate_repo.schema import Boolean
from trove.db.sqlalchemy.migrate_repo.schema import Integer
from trove.db.sqlalchemy.migrate_repo.schema import String
from trove.db.sqlalchemy.migrate_repo.schema import DateTime
from trove.db.sqlalchemy.migrate_repo.schema import Table


meta = MetaData()

security_groups = Table(
    'security_groups',
    meta,
    Column('id', String(length=36), primary_key=True, nullable=False),
    Column('name', String(length=255)),
    Column('description', String(length=255)),
    Column('user', String(length=255)),
    Column('tenant_id', String(length=255)),
    Column('created', DateTime()),
    Column('updated', DateTime()),
    Column('deleted', Boolean(), default=0),
    Column('deleted_at', DateTime()),
)

security_group_instance_associations = Table(
    'security_group_instance_associations',
    meta,
    Column('id', String(length=36), primary_key=True, nullable=False),
    Column('security_group_id', String(length=36),
           ForeignKey('security_groups.id', ondelete="CASCADE",
                      onupdate="CASCADE")),
    Column('instance_id', String(length=36),
           ForeignKey('instances.id', ondelete="CASCADE",
                      onupdate="CASCADE")),
    Column('created', DateTime()),
    Column('updated', DateTime()),
    Column('deleted', Boolean(), default=0),
    Column('deleted_at', DateTime()),
)

security_group_rules = Table(
    'security_group_rules',
    meta,
    Column('id', String(length=36), primary_key=True, nullable=False),
    Column('group_id', String(length=36),
           ForeignKey('security_groups.id', ondelete="CASCADE",
                      onupdate="CASCADE")),
    Column('parent_group_id', String(length=36),
           ForeignKey('security_groups.id', ondelete="CASCADE",
                      onupdate="CASCADE")),
    Column('protocol', String(length=255)),
    Column('from_port', Integer()),
    Column('to_port', Integer()),
    Column('cidr', String(length=255)),
    Column('created', DateTime()),
    Column('updated', DateTime()),
    Column('deleted', Boolean(), default=0),
    Column('deleted_at', DateTime()),
)


def upgrade(migrate_engine):
    meta.bind = migrate_engine
    Table(
        'instances',
        meta,
        autoload=True,
    )
    create_tables([security_groups, security_group_rules,
                   security_group_instance_associations])


def downgrade(migrate_engine):
    meta.bind = migrate_engine
    drop_tables([security_group_instance_associations,
                 security_group_rules, security_groups])

########NEW FILE########
__FILENAME__ = 014_update_instance_flavor_id
# Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from sqlalchemy.schema import MetaData

from trove.db.sqlalchemy.migrate_repo.schema import Integer
from trove.db.sqlalchemy.migrate_repo.schema import String
from trove.db.sqlalchemy.migrate_repo.schema import Table


def upgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine
    # pgsql <= 8.3 was lax about char->other casting but this was tightened up
    # in 8.4+. We now have to specify the USING clause for the cast to succeed.
    # NB: The generated sqlalchemy query doesn't support this, so this override
    # is needed.
    if migrate_engine.name == 'postgresql':
        migrate_engine.execute('ALTER TABLE instances ALTER COLUMN flavor_id '
                               'TYPE INTEGER USING flavor_id::integer')
    else:
        instances = Table('instances', meta, autoload=True)
        #modify column
        instances.c.flavor_id.alter(type=Integer())


def downgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine
    # int->char casts in pgsql still work fine without any USING clause,
    #  so downgrade is not affected.
    # modify column:
    instances = Table('instances', meta, autoload=True)
    instances.c.flavor_id.alter(type=String(36))

########NEW FILE########
__FILENAME__ = 015_add_service_type
# Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from sqlalchemy.schema import Column
from sqlalchemy.schema import MetaData

from trove.db.sqlalchemy.migrate_repo.schema import String
from trove.db.sqlalchemy.migrate_repo.schema import Table


def upgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine
    instances = Table('instances', meta, autoload=True)
    service_type = Column('service_type', String(36))
    instances.create_column(service_type)
    instances.update().values({'service_type': 'mysql'}).execute()


def downgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine
    # modify column:
    instances = Table('instances', meta, autoload=True)
    instances.drop_column('service_type')

########NEW FILE########
__FILENAME__ = 016_add_datastore_type
# Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from sqlalchemy import ForeignKey
from sqlalchemy.schema import Column
from sqlalchemy.schema import MetaData
from sqlalchemy.schema import UniqueConstraint

from trove.db.sqlalchemy.migrate_repo.schema import Boolean
from trove.db.sqlalchemy.migrate_repo.schema import create_tables
from trove.db.sqlalchemy.migrate_repo.schema import drop_tables
from trove.db.sqlalchemy.migrate_repo.schema import String
from trove.db.sqlalchemy.migrate_repo.schema import Table


meta = MetaData()


datastores = Table(
    'datastores',
    meta,
    Column('id', String(36), primary_key=True, nullable=False),
    Column('name', String(255), unique=True),
    Column('manager', String(255), nullable=False),
    Column('default_version_id', String(36)),
)


datastore_versions = Table(
    'datastore_versions',
    meta,
    Column('id', String(36), primary_key=True, nullable=False),
    Column('datastore_id', String(36), ForeignKey('datastores.id')),
    Column('name', String(255), unique=True),
    Column('image_id', String(36), nullable=False),
    Column('packages', String(511)),
    Column('active', Boolean(), nullable=False),
    UniqueConstraint('datastore_id', 'name', name='ds_versions')
)


def upgrade(migrate_engine):
    meta.bind = migrate_engine
    create_tables([datastores, datastore_versions])
    instances = Table('instances', meta, autoload=True)
    datastore_version_id = Column('datastore_version_id', String(36),
                                  ForeignKey('datastore_versions.id'))
    instances.create_column(datastore_version_id)
    instances.drop_column('service_type')
    # Table 'service_images' is deprecated since this version.
    # Leave it for few releases.
    #drop_tables([service_images])


def downgrade(migrate_engine):
    meta.bind = migrate_engine
    drop_tables([datastores, datastore_versions])
    instances = Table('instances', meta, autoload=True)
    instances.drop_column('datastore_version_id')
    service_type = Column('service_type', String(36))
    instances.create_column(service_type)
    instances.update().values({'service_type': 'mysql'}).execute()

########NEW FILE########
__FILENAME__ = 017_update_datastores
# Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from sqlalchemy.schema import Column
from sqlalchemy.schema import MetaData
from sqlalchemy.sql.expression import select

from trove.db.sqlalchemy.migrate_repo.schema import String
from trove.db.sqlalchemy.migrate_repo.schema import Table


def migrate_datastore_manager(datastores, datastore_versions):
    versions = select([datastore_versions]).execute()
    for ds_v in versions:
        ds = select([datastores]).\
            where(datastores.c.id == ds_v.datastore_id).\
            execute().fetchone()
        datastore_versions.update().\
            where(datastore_versions.c.id == ds_v.id).\
            values(manager=ds.manager).\
            execute()


def upgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine

    datastores = Table('datastores', meta, autoload=True)
    datastore_versions = Table('datastore_versions', meta, autoload=True)

    # add column to datastore_versions
    manager = Column('manager', String(255))
    datastore_versions.create_column(manager)
    migrate_datastore_manager(datastores, datastore_versions)

    # drop column from datastores
    datastores.drop_column('manager')


def downgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine

    datastores = Table('datastores', meta, autoload=True)
    datastore_versions = Table('datastore_versions', meta, autoload=True)

    # drop column from datastore_versions
    datastore_versions.drop_column('manager')

    # add column to datastores
    manager = Column('manager', String(255))
    datastores.create_column(manager)

########NEW FILE########
__FILENAME__ = 018_datastore_versions_fix
# Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from sqlalchemy.schema import MetaData

from trove.db.sqlalchemy.migrate_repo.schema import Table


def upgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine
    datastore_versions = Table('datastore_versions', meta, autoload=True)
    #modify column
    datastore_versions.c.name.alter(unique=False)


def downgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine
    # modify column:
    datastore_versions = Table('datastore_versions', meta, autoload=True)
    datastore_versions.c.name.alter(unique=True)

########NEW FILE########
__FILENAME__ = 019_datastore_fix
# Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from sqlalchemy.schema import MetaData
from sqlalchemy.sql.expression import select
from sqlalchemy.sql.expression import insert
from sqlalchemy.sql.expression import update
from sqlalchemy.sql.expression import delete
from trove.common import cfg
from trove.db.sqlalchemy.migrate_repo.schema import Table

CONF = cfg.CONF
LEGACY_IMAGE_ID = "00000000-0000-0000-0000-000000000000"
LEGACY_DATASTORE_ID = "10000000-0000-0000-0000-000000000001"
LEGACY_VERSION_ID = "20000000-0000-0000-0000-000000000002"
meta = MetaData()


def create_legacy_version(datastores_table,
                          datastore_versions_table,
                          image_id):
    insert(
        table=datastores_table,
        values=dict(id=LEGACY_DATASTORE_ID, name="Legacy MySQL")
    ).execute()

    insert(
        table=datastore_versions_table,
        values=dict(id=LEGACY_VERSION_ID,
                    datastore_id=LEGACY_DATASTORE_ID,
                    name="Unknown Legacy Version",
                    image_id=image_id,
                    packages="",
                    active=False,
                    manager="mysql")
    ).execute()

    return LEGACY_VERSION_ID


def find_image(service_name):
    image_table = Table('service_images', meta, autoload=True)
    image = select(
        columns=["id", "image_id", "service_name"],
        from_obj=image_table,
        whereclause="service_name='%s'" % service_name,
        limit=1
    ).execute().fetchone()

    if image:
        return image.id
    return LEGACY_IMAGE_ID


def has_instances_wo_datastore_version(instances_table):
    instance = select(
        columns=["id"],
        from_obj=instances_table,
        whereclause="datastore_version_id is NULL",
        limit=1
    ).execute().fetchone()

    return instance is not None


def find_all_instances_wo_datastore_version(instances_table):
    instances = select(
        columns=["id"],
        from_obj=instances_table,
        whereclause="datastore_version_id is NULL"
    ).execute()

    return instances


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    instance_table = Table('instances', meta, autoload=True)

    if has_instances_wo_datastore_version:
        instances = find_all_instances_wo_datastore_version(instance_table)
        image_id = find_image("mysql")

        datastores_table = Table('datastores',
                                 meta,
                                 autoload=True)
        datastore_versions_table = Table('datastore_versions',
                                         meta,
                                         autoload=True)

        version_id = create_legacy_version(datastores_table,
                                           datastore_versions_table,
                                           image_id)
        for instance in instances:
            update(
                table=instance_table,
                whereclause="id='%s'" % instance.id,
                values=dict(datastore_version_id=version_id)
            ).execute()

    instance_table.c.datastore_version_id.alter(nullable=False)


def downgrade(migrate_engine):
    meta.bind = migrate_engine

    instance_table = Table('instances', meta, autoload=True)

    instance_table.c.datastore_version_id.alter(nullable=True)

    update(
        table=instance_table,
        whereclause="datastore_version_id='%s'" % LEGACY_VERSION_ID,
        values=dict(datastore_version_id=None)
    ).execute()

    datastores_table = Table('datastores',
                             meta,
                             autoload=True)
    datastore_versions_table = Table('datastore_versions',
                                     meta,
                                     autoload=True)

    delete(
        table=datastore_versions_table,
        whereclause="id='%s'" % LEGACY_VERSION_ID
    ).execute()
    delete(
        table=datastores_table,
        whereclause="id='%s'" % LEGACY_DATASTORE_ID
    ).execute()

########NEW FILE########
__FILENAME__ = 020_configurations
# Copyright 2014 Rackspace
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from sqlalchemy import ForeignKey
from sqlalchemy.exc import OperationalError
from sqlalchemy.schema import Column
from sqlalchemy.schema import MetaData

from trove.db.sqlalchemy.migrate_repo.schema import create_tables
from trove.db.sqlalchemy.migrate_repo.schema import DateTime
from trove.db.sqlalchemy.migrate_repo.schema import Boolean
from trove.db.sqlalchemy.migrate_repo.schema import String
from trove.db.sqlalchemy.migrate_repo.schema import Table
from trove.openstack.common import log as logging

logger = logging.getLogger('trove.db.sqlalchemy.migrate_repo.schema')

meta = MetaData()

configurations = Table(
    'configurations',
    meta,
    Column('id', String(36), primary_key=True, nullable=False),
    Column('name', String(64), nullable=False),
    Column('description', String(256)),
    Column('tenant_id', String(36), nullable=False),
    Column('datastore_version_id', String(36), nullable=False),
    Column('deleted', Boolean(), nullable=False, default=False),
    Column('deleted_at', DateTime()),
)

configuration_parameters = Table(
    'configuration_parameters',
    meta,
    Column('configuration_id', String(36), ForeignKey("configurations.id"),
           nullable=False, primary_key=True),
    Column('configuration_key', String(128), nullable=False, primary_key=True),
    Column('configuration_value', String(128)),
    Column('deleted', Boolean(), nullable=False, default=False),
    Column('deleted_at', DateTime()),
)


def upgrade(migrate_engine):
    meta.bind = migrate_engine

    # since the downgrade is a no-op, an upgrade after a downgrade will
    # cause an exception because the tables already exist
    # we will catch that case and log an info message
    try:
        create_tables([configurations])
        create_tables([configuration_parameters])

        instances = Table('instances', meta, autoload=True)
        instances.create_column(Column('configuration_id', String(36),
                                       ForeignKey("configurations.id")))
    except OperationalError as e:
        logger.info(e)


def downgrade(migrate_engine):
    meta.bind = migrate_engine
    # Not dropping the tables for concern if rollback needed would cause
    # consumers to recreate configurations.

########NEW FILE########
__FILENAME__ = 021_conductor_last_seen
# Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from sqlalchemy.schema import Column
from sqlalchemy.schema import MetaData

from trove.db.sqlalchemy.migrate_repo.schema import Float
from trove.db.sqlalchemy.migrate_repo.schema import String
from trove.db.sqlalchemy.migrate_repo.schema import Table
from trove.db.sqlalchemy.migrate_repo.schema import create_tables
from trove.db.sqlalchemy.migrate_repo.schema import drop_tables

meta = MetaData()

conductor_lastseen = Table(
    'conductor_lastseen',
    meta,
    Column('instance_id', String(36), primary_key=True, nullable=False),
    Column('method_name', String(36), primary_key=True, nullable=False),
    Column('sent', Float(precision=32)))


def upgrade(migrate_engine):
    meta.bind = migrate_engine
    create_tables([conductor_lastseen])


def downgrade(migrate_engine):
    meta.bind = migrate_engine
    drop_tables([conductor_lastseen])

########NEW FILE########
__FILENAME__ = 022_add_backup_parent_id
# Copyright 2013 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from sqlalchemy.schema import Column
from sqlalchemy.schema import MetaData

from trove.db.sqlalchemy.migrate_repo.schema import String
from trove.db.sqlalchemy.migrate_repo.schema import Table


def upgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine

    # add column:
    backups = Table('backups', meta, autoload=True)
    backups.create_column(Column('parent_id', String(36), nullable=True))


def downgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine

    # drop column:
    backups = Table('backups', meta, autoload=True)
    backups.drop_column('parent_id')

########NEW FILE########
__FILENAME__ = 023_add_instance_indexes
# Copyright 2014 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from sqlalchemy.exc import OperationalError
from sqlalchemy.schema import MetaData
from sqlalchemy.schema import Index
from trove.openstack.common import log as logging

from trove.db.sqlalchemy.migrate_repo.schema import Table

logger = logging.getLogger('trove.db.sqlalchemy.migrate_repo.schema')


def upgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine

    instances = Table('instances', meta, autoload=True)

    tenant_id_idx = Index("instances_tenant_id", instances.c.tenant_id)

    try:
        tenant_id_idx.create()
    except OperationalError as e:
        logger.info(e)

    deleted_idx = Index("instances_deleted", instances.c.deleted)
    try:
        deleted_idx.create()
    except OperationalError as e:
        logger.info(e)


def downgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine

    instances = Table('instances', meta, autoload=True)

    tenant_id_idx = Index("instances_tenant_id", instances.c.tenant_id)
    tenant_id_idx.drop()

    deleted_idx = Index("instances_deleted", instances.c.deleted)
    deleted_idx.drop()

########NEW FILE########
__FILENAME__ = 024_add_backup_indexes
# Copyright 2014 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from sqlalchemy.exc import OperationalError
from sqlalchemy.schema import MetaData
from sqlalchemy.schema import Index
from trove.openstack.common import log as logging

from trove.db.sqlalchemy.migrate_repo.schema import Table

logger = logging.getLogger('trove.db.sqlalchemy.migrate_repo.schema')


def upgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine

    backups = Table('backups', meta, autoload=True)
    backups_instance_id_idx = Index("backups_instance_id",
                                    backups.c.instance_id)
    backups_deleted_idx = Index("backups_deleted", backups.c.deleted)

    try:
        backups_instance_id_idx.create()
    except OperationalError as e:
        logger.info(e)

    try:
        backups_deleted_idx.create()
    except OperationalError as e:
        logger.info(e)


def downgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine

    backups = Table('backups', meta, autoload=True)
    backups_instance_id_idx = Index("backups_instance_id",
                                    backups.c.instance_id)
    backups_deleted_idx = Index("backups_deleted", backups.c.deleted)

    meta.bind = migrate_engine
    backups_instance_id_idx.drop()
    backups_deleted_idx.drop()

########NEW FILE########
__FILENAME__ = 025_add_service_statuses_indexes
# Copyright 2014 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from sqlalchemy.exc import OperationalError
from sqlalchemy.schema import MetaData
from sqlalchemy.schema import Index
from trove.openstack.common import log as logging

from trove.db.sqlalchemy.migrate_repo.schema import Table

logger = logging.getLogger('trove.db.sqlalchemy.migrate_repo.schema')


def upgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine

    service_statuses = Table('service_statuses', meta, autoload=True)
    idx = Index("service_statuses_instance_id", service_statuses.c.instance_id)

    try:
        idx.create()
    except OperationalError as e:
        logger.info(e)


def downgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine

    service_statuses = Table('service_statuses', meta, autoload=True)
    idx = Index("service_statuses_instance_id", service_statuses.c.instance_id)
    idx.drop()

########NEW FILE########
__FILENAME__ = 026_datastore_versions_unique_fix
# Copyright 2014 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from migrate.changeset import UniqueConstraint
from sqlalchemy.exc import OperationalError
from sqlalchemy.schema import MetaData

from trove.db.sqlalchemy.migrate_repo.schema import Table
from trove.openstack.common import log as logging

logger = logging.getLogger('trove.db.sqlalchemy.migrate_repo.schema')


def upgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine
    datastore_versions = Table('datastore_versions', meta, autoload=True)

    # drop the unique index on the name column - unless we are
    # using sqlite - it doesn't support dropping unique constraints
    uc = None
    if migrate_engine.name == "mysql":
        uc = UniqueConstraint('name', table=datastore_versions, name='name')
    elif migrate_engine.name == "postgresql":
        uc = UniqueConstraint('name', table=datastore_versions,
                              name='datastore_versions_name_key')
    if uc:
        try:
            uc.drop()
        except OperationalError as e:
            logger.info(e)


def downgrade(migrate_engine):
    # we aren't going to recreate the index in this case for 2 reasons:
    # 1. this column being unique was a bug in the first place
    # 2. adding a unique index to a column that has duplicates will fail
    pass

########NEW FILE########
__FILENAME__ = migration
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from trove.openstack.common import log as logging
import os

from migrate.versioning import api as versioning_api
# See LP bug #719834. sqlalchemy-migrate changed location of
# exceptions.py after 0.6.0.
try:
    from migrate.versioning import exceptions as versioning_exceptions
except ImportError:
    from migrate import exceptions as versioning_exceptions

from trove.common import exception


logger = logging.getLogger('trove.db.sqlalchemy.migration')


def db_version(options, repo_path=None):
    """Return the database's current migration number.

    :param options: options dict
    :retval version number

    """
    repo_path = get_migrate_repo_path(repo_path)
    sql_connection = options['sql_connection']
    try:
        return versioning_api.db_version(sql_connection, repo_path)
    except versioning_exceptions.DatabaseNotControlledError:
        msg = ("database '%(sql_connection)s' is not under migration control"
               % {'sql_connection': sql_connection})
        raise exception.DatabaseMigrationError(msg)


def upgrade(options, version=None, repo_path=None):
    """Upgrade the database's current migration level.

    :param options: options dict
    :param version: version to upgrade (defaults to latest)
    :retval version number

    """
    db_version(options, repo_path)  # Ensure db is under migration control
    repo_path = get_migrate_repo_path(repo_path)
    sql_connection = options['sql_connection']
    version_str = version or 'latest'
    logger.info("Upgrading %(sql_connection)s to version %(version_str)s" %
                {'sql_connection': sql_connection, 'version_str': version_str})
    return versioning_api.upgrade(sql_connection, repo_path, version)


def downgrade(options, version, repo_path=None):
    """Downgrade the database's current migration level.

    :param options: options dict
    :param version: version to downgrade to
    :retval version number

    """
    db_version(options, repo_path)  # Ensure db is under migration control
    repo_path = get_migrate_repo_path(repo_path)
    sql_connection = options['sql_connection']
    logger.info("Downgrading %(sql_connection)s to version %(version)s" %
                {'sql_connection': sql_connection, 'version': version})
    return versioning_api.downgrade(sql_connection, repo_path, version)


def version_control(options, repo_path=None):
    """Place a database under migration control.

    :param options: options dict

    """
    sql_connection = options['sql_connection']
    try:
        _version_control(options)
    except versioning_exceptions.DatabaseAlreadyControlledError:
        msg = ("database '%(sql_connection)s' is already under migration "
               "control" % {'sql_connection': sql_connection})
        raise exception.DatabaseMigrationError(msg)


def _version_control(options, repo_path):
    """Place a database under migration control.

    :param options: options dict

    """
    repo_path = get_migrate_repo_path(repo_path)
    sql_connection = options['sql_connection']
    return versioning_api.version_control(sql_connection, repo_path)


def db_sync(options, version=None, repo_path=None):
    """Place a database under migration control and perform an upgrade.

    :param options: options dict
    :param repo_path: used for plugin db migrations, defaults to main repo
    :retval version number

    """
    try:
        _version_control(options, repo_path)
    except versioning_exceptions.DatabaseAlreadyControlledError:
        pass

    upgrade(options, version=version, repo_path=repo_path)


def get_migrate_repo_path(repo_path=None):
    """Get the path for the migrate repository."""

    default_path = os.path.join(
        os.path.abspath(os.path.dirname(__file__)),
        'migrate_repo')
    repo_path = repo_path or default_path
    assert os.path.exists(repo_path)
    return repo_path

########NEW FILE########
__FILENAME__ = session
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import contextlib
from sqlalchemy import create_engine
from sqlalchemy import MetaData
from sqlalchemy.orm import sessionmaker

from trove.common import cfg
from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _
from trove.db.sqlalchemy import mappers

_ENGINE = None
_MAKER = None


LOG = logging.getLogger(__name__)

CONF = cfg.CONF


def configure_db(options, models_mapper=None):
    global _ENGINE
    if not _ENGINE:
        _ENGINE = _create_engine(options)
    if models_mapper:
        models_mapper.map(_ENGINE)
    else:
        from trove.instance import models as base_models
        from trove.datastore import models as datastores_models
        from trove.dns import models as dns_models
        from trove.extensions.mysql import models as mysql_models
        from trove.guestagent import models as agent_models
        from trove.quota import models as quota_models
        from trove.backup import models as backup_models
        from trove.extensions.security_group import models as secgrp_models
        from trove.configuration import models as configurations_models
        from trove.conductor import models as conductor_models

        model_modules = [
            base_models,
            datastores_models,
            dns_models,
            mysql_models,
            agent_models,
            quota_models,
            backup_models,
            secgrp_models,
            configurations_models,
            conductor_models,
        ]

        models = {}
        for module in model_modules:
            models.update(module.persisted_models())
        mappers.map(_ENGINE, models)


def _create_engine(options):
    engine_args = {
        "pool_recycle": CONF.sql_idle_timeout,
        "echo": CONF.sql_query_log
    }
    LOG.info(_("Creating SQLAlchemy engine with args: %s") % engine_args)
    return create_engine(options['sql_connection'], **engine_args)


def get_session(autocommit=True, expire_on_commit=False):
    """Helper method to grab session."""
    global _MAKER, _ENGINE
    if not _MAKER:
        if not _ENGINE:
            msg = "***The Database has not been setup!!!***"
            LOG.exception(msg)
            raise RuntimeError(msg)
        _MAKER = sessionmaker(bind=_ENGINE,
                              autocommit=autocommit,
                              expire_on_commit=expire_on_commit)
    return _MAKER()


def raw_query(model, autocommit=True, expire_on_commit=False):
    return get_session(autocommit, expire_on_commit).query(model)


def clean_db():
    global _ENGINE
    meta = MetaData()
    meta.reflect(bind=_ENGINE)
    with contextlib.closing(_ENGINE.connect()) as con:
        trans = con.begin()
        for table in reversed(meta.sorted_tables):
            if table.name != "migrate_version":
                con.execute(table.delete())
        trans.commit()


def drop_db(options):
    meta = MetaData()
    engine = _create_engine(options)
    meta.bind = engine
    meta.reflect()
    meta.drop_all()

########NEW FILE########
__FILENAME__ = driver
# Copyright (c) 2013 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Dns Driver that uses Designate DNSaaS.
"""

from trove.common import cfg
from trove.common import exception
from trove.dns import driver
from trove.openstack.common import log as logging
from designateclient.v1 import Client
from designateclient.v1.records import Record
import base64
import hashlib


CONF = cfg.CONF

DNS_TENANT_ID = CONF.dns_account_id
DNS_AUTH_URL = CONF.dns_auth_url
DNS_ENDPOINT_URL = CONF.dns_endpoint_url
DNS_SERVICE_TYPE = CONF.dns_service_type
DNS_REGION = CONF.dns_region
DNS_USERNAME = CONF.dns_username
DNS_PASSKEY = CONF.dns_passkey
DNS_TTL = CONF.dns_ttl
DNS_DOMAIN_ID = CONF.dns_domain_id
DNS_DOMAIN_NAME = CONF.dns_domain_name


LOG = logging.getLogger(__name__)


class DesignateObjectConverter(object):

    def domain_to_zone(self, domain):
        return DesignateDnsZone(id=domain.id, name=domain.name)

    def record_to_entry(self, record, dns_zone):
        return driver.DnsEntry(name=record.name, content=record.data,
                               type=record.type, ttl=record.ttl,
                               priority=record.priority, dns_zone=dns_zone)


def create_designate_client():
    """Creates a Designate DNSaaS client."""
    client = Client(auth_url=DNS_AUTH_URL,
                    username=DNS_USERNAME,
                    password=DNS_PASSKEY,
                    tenant_id=DNS_TENANT_ID,
                    endpoint=DNS_ENDPOINT_URL,
                    service_type=DNS_SERVICE_TYPE,
                    region_name=DNS_REGION)
    return client


class DesignateDriver(driver.DnsDriver):

    def __init__(self):
        self.dns_client = create_designate_client()
        self.converter = DesignateObjectConverter()
        self.default_dns_zone = DesignateDnsZone(id=DNS_DOMAIN_ID,
                                                 name=DNS_DOMAIN_NAME)

    def create_entry(self, entry, content):
        """Creates the entry in the driver at the given dns zone."""
        dns_zone = entry.dns_zone or self.default_dns_zone
        if not dns_zone.id:
            raise TypeError("The entry's dns_zone must have an ID specified.")
        name = entry.name
        LOG.debug("Creating DNS entry %s." % name)
        client = self.dns_client
        # Record name has to end with a '.' by dns standard
        record = Record(name=entry.name + '.',
                        type=entry.type,
                        data=content,
                        ttl=entry.ttl,
                        priority=entry.priority)
        client.records.create(dns_zone.id, record)

    def delete_entry(self, name, type, dns_zone=None):
        """Deletes an entry with the given name and type from a dns zone."""
        dns_zone = dns_zone or self.default_dns_zone
        records = self._get_records(dns_zone)
        matching_record = [rec for rec in records
                           if rec.name == name + '.' and rec.type == type]
        if not matching_record:
            raise exception.DnsRecordNotFound(name)
        LOG.debug("Deleting DNS entry %s." % name)
        self.dns_client.records.delete(dns_zone.id, matching_record[0].id)

    def get_entries_by_content(self, content, dns_zone=None):
        """Retrieves all entries in a dns_zone with a matching content field"""
        records = self._get_records(dns_zone)
        return [self.converter.record_to_entry(record, dns_zone)
                for record in records if record.data == content]

    def get_entries_by_name(self, name, dns_zone):
        records = self._get_records(dns_zone)
        return [self.converter.record_to_entry(record, dns_zone)
                for record in records if record.name == name]

    def get_dns_zones(self, name=None):
        """Returns all dns zones (optionally filtered by the name argument."""
        domains = self.dns_client.domains.list()
        return [self.converter.domain_to_zone(domain)
                for domain in domains if not name or domain.name == name]

    def modify_content(self, name, content, dns_zone):
        # We dont need this in trove for now
        raise NotImplementedError("Not implemented for Designate DNS.")

    def rename_entry(self, content, name, dns_zone):
        # We dont need this in trove for now
        raise NotImplementedError("Not implemented for Designate DNS.")

    def _get_records(self, dns_zone):
        dns_zone = dns_zone or self.default_dns_zone
        if not dns_zone:
            raise TypeError('DNS domain is must be specified')
        return self.dns_client.records.list(dns_zone.id)


class DesignateInstanceEntryFactory(driver.DnsInstanceEntryFactory):
    """Defines how instance DNS entries are created for instances."""

    def create_entry(self, instance_id):
        zone = DesignateDnsZone(id=DNS_DOMAIN_ID, name=DNS_DOMAIN_NAME)
        # Constructing the hostname by hashing the instance ID.
        name = base64.b32encode(hashlib.md5(instance_id).digest())[:11].lower()
        hostname = ("%s.%s" % (name, zone.name))
        #Removing the leading dot if present
        if hostname.endswith('.'):
            hostname = hostname[:-1]

        return driver.DnsEntry(name=hostname, content=None, type="A",
                               ttl=DNS_TTL, dns_zone=zone)


class DesignateDnsZone(driver.DnsZone):

    def __init__(self, id, name):
        self._name = name
        self._id = id

    @property
    def name(self):
        return self._name

    @name.setter
    def name(self, value):
        self._name = value

    @property
    def id(self):
        return self._id

    @id.setter
    def id(self, value):
        self._id = value

    def __eq__(self, other):
        return (isinstance(other, DesignateDnsZone) and
                self.name == other.name and
                self.id == other.id)

    def __str__(self):
        return "%s:%s" % (self.id, self.name)

########NEW FILE########
__FILENAME__ = driver
# Copyright (c) 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Dns Driver base class that all DNS drivers should inherit from
"""


class DnsDriver(object):
    """The base class that all Dns drivers should inherit from."""

    def __init__(self):
        pass

    def create_entry(self, entry):
        """Creates the entry in the driver at the given dns zone."""
        pass

    def delete_entry(self, name, type, dns_zone=None):
        """Deletes an entry with the given name and type from a dns zone."""
        pass

    def get_entries_by_content(self, content, dns_zone=None):
        """Retrieves all entries in a dns_zone with a matching content field"""
        pass

    def get_entries_by_name(self, name, dns_zone=None):
        """Retrieves all entries in a dns zone with the given name field."""
        pass

    def get_dns_zones(self, name=None):
        """Returns all dns zones (optionally filtered by the name argument."""
        pass

    def modify_content(self, name, content, dns_zone):
        #TODO(tim.simpson) I've found no use for this in RS impl of DNS w/
        #                  instances. Check to see its really needed.
        pass

    def rename_entry(self, content, name, dns_zone):
        #TODO(tim.simpson) I've found no use for this in RS impl of DNS w/
        #                  instances. Check to see its really needed.
        pass


class DnsInstanceEntryFactory(object):
    """Defines how instance DNS entries are created for instances.

    By default, the DNS entry returns None meaning instances do not get entries
    associated with them. Override the create_entry method to change this
    behavior.

    """

    def create_entry(self, instance):
        return None


class DnsSimpleInstanceEntryFactory(object):
    """Creates a CNAME with the name being the instance name."""

    def create_entry(self, instance):
        return DnsEntry(name=instance.name, content=None, type="CNAME")


class DnsEntry(object):
    """Simple representation of a DNS record."""

    def __init__(self, name, content, type, ttl=None, priority=None,
                 dns_zone=None):
        self.content = content
        self.name = name
        self.type = type
        self.priority = priority
        self.dns_zone = dns_zone
        self.ttl = ttl

    def __repr__(self):
        msg = ('DnsEntry(name="%s", content="%s", type="%s", '
               'ttl=%s, priority=%s, dns_zone=%s)')
        params = (self.name, self.content, self.type, self.ttl, self.priority,
                  self.dns_zone)
        return msg % params

    def __str__(self):
        return "{ name:%s, content:%s, type:%s, zone:%s }" % \
               (self.name, self.content, self.type, self.dns_zone)


class DnsZone(object):
    """Represents a DNS Zone.

    For some APIs it is inefficient to simply represent a zone as a string
    because this would necessitate a look up on every call.  So this opaque
    object can contain additional data needed by the DNS driver.  The only
    constant is it must contain the domain name of the zone.

    """

    @property
    def name(self):
        return ""

    def __str__(self):
        return self.name

########NEW FILE########
__FILENAME__ = manager
# Copyright (c) 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Dns manager.
"""
from trove.openstack.common import log as logging

from trove.common import utils
from trove.common import cfg

LOG = logging.getLogger(__name__)

CONF = cfg.CONF


class DnsManager(object):
    """Handles associating DNS to and from IPs."""

    def __init__(self, dns_driver=None, dns_instance_entry_factory=None,
                 *args, **kwargs):
        if not dns_driver:
            dns_driver = CONF.dns_driver
        dns_driver = utils.import_class(dns_driver)
        self.driver = dns_driver()

        if not dns_instance_entry_factory:
            dns_instance_entry_factory = CONF.dns_instance_entry_factory
        entry_factory = utils.import_class(dns_instance_entry_factory)
        self.entry_factory = entry_factory()

    def create_instance_entry(self, instance_id, content):
        """Connects a new instance with a DNS entry.

        :param instance_id: The trove instance_id to associate.
        :param content: The IP content attached to the instance.

        """
        entry = self.entry_factory.create_entry(instance_id)
        if entry:
            LOG.debug("Creating entry address %s." % str(entry))
            self.driver.create_entry(entry, content)
        else:
            LOG.debug("Entry address not found for instance %s" % instance_id)

    def delete_instance_entry(self, instance_id, content=None):
        """Removes a DNS entry associated to an instance.

        :param instance_id: The trove instance id to associate.
        :param content: The IP content attached to the instance.

        """
        entry = self.entry_factory.create_entry(instance_id)
        LOG.debug("Deleting instance entry with %s" % str(entry))
        if entry:
            self.driver.delete_entry(entry.name, entry.type)

    def determine_hostname(self, instance_id):
        """
        Create the hostname field based on the instance id.
        Use instance by default.
        """
        entry = self.entry_factory.create_entry(instance_id)
        if entry:
            return entry.name
        else:
            return None

########NEW FILE########
__FILENAME__ = models
# Copyright 2010-2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Model classes that map instance Ip to dns record.
"""


from trove.db import get_db_api
from trove.common import exception
from trove.common.models import ModelBase
from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _

LOG = logging.getLogger(__name__)


def persisted_models():
    return {
        'dns_records': DnsRecord,
    }


class DnsRecord(ModelBase):

    _data_fields = ['name', 'record_id']
    _table_name = 'dns_records'

    def __init__(self, name, record_id):
        self.name = name
        self.record_id = record_id

    @classmethod
    def create(cls, **values):
        record = cls(**values).save()
        if not record.is_valid():
            raise exception.InvalidModelError(errors=record.errors)
        return record

    def save(self):
        if not self.is_valid():
            raise exception.InvalidModelError(errors=self.errors)
        LOG.debug("Saving %(name)s: %(dict)s" %
                  {'name': self.__class__.__name__, 'dict': self.__dict__})
        return get_db_api().save(self)

    def delete(self):
        LOG.debug("Deleting %(name)s: %(dict)s" %
                  {'name': self.__class__.__name__, 'dict': self.__dict__})
        return get_db_api().delete(self)

    @classmethod
    def find_by(cls, **conditions):
        model = cls.get_by(**conditions)
        if model is None:
            raise exception.ModelNotFoundError(_("%s Not Found") %
                                               cls.__name__)
        return model

    @classmethod
    def get_by(cls, **kwargs):
        return get_db_api().find_by(cls, **cls._process_conditions(kwargs))

    @classmethod
    def _process_conditions(cls, raw_conditions):
        """Override in inheritors to format/modify any conditions."""
        return raw_conditions

########NEW FILE########
__FILENAME__ = driver
# Copyright (c) 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Dns Driver that uses Rackspace DNSaaS.
"""

__version__ = '2.4'

import hashlib

from trove.openstack.common import log as logging
from trove.common import cfg
from trove.common import exception
from trove.common.exception import NotFound
from trove.dns.models import DnsRecord
from rsdns.client import DNSaas
from rsdns.client.future import RsDnsError

from trove.dns.driver import DnsEntry

CONF = cfg.CONF

DNS_ACCOUNT_ID = CONF.dns_account_id
DNS_AUTH_URL = CONF.dns_auth_url
DNS_DOMAIN_NAME = CONF.dns_domain_name
DNS_USERNAME = CONF.dns_username
DNS_PASSKEY = CONF.dns_passkey
DNS_MANAGEMENT_BASE_URL = CONF.dns_management_base_url
DNS_TTL = CONF.dns_ttl
DNS_DOMAIN_ID = CONF.dns_domain_id


LOG = logging.getLogger(__name__)


class EntryToRecordConverter(object):

    def __init__(self, default_dns_zone):
        self.default_dns_zone = default_dns_zone

    def domain_to_dns_zone(self, domain):
        return RsDnsZone(id=domain.id, name=domain.name)

    def name_to_long_name(self, name, dns_zone=None):
        dns_zone = dns_zone or self.default_dns_zone
        if name:
            long_name = name + "." + dns_zone.name
        else:
            long_name = ""
        return long_name

    def record_to_entry(self, record, dns_zone):
        entry_name = record.name
        return DnsEntry(name=entry_name, content=record.data,
                        type=record.type, ttl=record.ttl, dns_zone=dns_zone)


def create_client_with_flag_values():
    """Creates a RS DNSaaS client using the Flag values."""
    if DNS_MANAGEMENT_BASE_URL is None:
        raise RuntimeError("Missing flag value for dns_management_base_url.")
    return DNSaas(DNS_ACCOUNT_ID, DNS_USERNAME, DNS_PASSKEY,
                  auth_url=DNS_AUTH_URL,
                  management_base_url=DNS_MANAGEMENT_BASE_URL)


def find_default_zone(dns_client, raise_if_zone_missing=True):
    """Using the domain_name from the FLAG values, creates a zone.

    Because RS DNSaaS needs the ID, we need to find this value before we start.
    In testing it's difficult to keep up with it because the database keeps
    getting wiped... maybe later we could go back to storing it as a FLAG value

    """
    domain_name = DNS_DOMAIN_NAME
    try:
        domains = dns_client.domains.list(name=domain_name)
        for domain in domains:
            if domain.name == domain_name:
                return RsDnsZone(id=domain.id, name=domain_name)
    except NotFound:
        pass
    if not raise_if_zone_missing:
        return RsDnsZone(id=None, name=domain_name)
    msg = ("The dns_domain_name from the FLAG values (%s) "
           "does not exist!  account_id=%s, username=%s, LIST=%s")
    params = (domain_name, DNS_ACCOUNT_ID, DNS_USERNAME, domains)
    raise RuntimeError(msg % params)


class RsDnsDriver(object):
    """Uses RS DNSaaS"""

    def __init__(self, raise_if_zone_missing=True):
        self.dns_client = create_client_with_flag_values()
        self.dns_client.authenticate()
        self.default_dns_zone = RsDnsZone(id=DNS_DOMAIN_ID,
                                          name=DNS_DOMAIN_NAME)
        self.converter = EntryToRecordConverter(self.default_dns_zone)
        if DNS_TTL < 300:
            msg = "TTL value '--dns_ttl=%s' should be greater than 300"
            raise Exception(msg % DNS_TTL)

    def create_entry(self, entry, content):
        dns_zone = entry.dns_zone or self.default_dns_zone
        if dns_zone.id is None:
            raise TypeError("The entry's dns_zone must have an ID specified.")
        name = entry.name  # + "." + dns_zone.name
        LOG.debug("Going to create RSDNS entry %s." % name)
        try:
            future = self.dns_client.records.create(
                domain=dns_zone.id,
                record_name=name,
                record_data=content,
                record_type=entry.type,
                record_ttl=entry.ttl)
            try:
                #TODO: Bring back our good friend poll_until.
                while(future.ready is False):
                    import time
                    time.sleep(2)
                    LOG.info("Waiting for the dns record_id.. ")

                if len(future.resource) < 1:
                    raise RsDnsError("No DNS records were created.")
                elif len(future.resource) > 1:
                    LOG.error("More than one DNS record created. Ignoring.")
                actual_record = future.resource[0]
                DnsRecord.create(name=name, record_id=actual_record.id)
                LOG.debug("Added RS DNS entry.")
            except RsDnsError as rde:
                LOG.error("An error occurred creating DNS entry!")
                raise
        except Exception as ex:
            LOG.error("Error when creating a DNS record!")
            raise

    def delete_entry(self, name, type, dns_zone=None):
        dns_zone = dns_zone or self.default_dns_zone
        long_name = name
        db_record = DnsRecord.find_by(name=name)
        record = self.dns_client.records.get(
            domain_id=dns_zone.id,
            record_id=db_record.record_id)
        if record.name != name or record.type != 'A':
            LOG.error("Tried to delete DNS record with name=%s, id=%s, but the"
                      " database returned a DNS record with the name %s and "
                      "type %s." % (name, db_record.id, record.name,
                                    record.type))
            raise exception.DnsRecordNotFound(name)
        self.dns_client.records.delete(
            domain_id=dns_zone.id,
            record_id=record.id)
        db_record.delete()

    def get_entries(self, name=None, content=None, dns_zone=None):
        dns_zone = dns_zone or self.defaucreate_entrylt_dns_zone
        long_name = name  # self.converter.name_to_long_name(name)
        records = self.dns_client.records.list(
            domain_id=dns_zone.id,
            record_name=long_name,
            record_address=content)
        return [self.converter.record_to_entry(record, dns_zone)
                for record in records]

    def get_entries_by_content(self, content, dns_zone=None):
        return self.get_entries(content=content)

    def get_entries_by_name(self, name, dns_zone=None):
        return self.get_entries(name=name, dns_zone=dns_zone)

    def get_dns_zones(self, name=None):
        domains = self.dns_client.domains.list(name=name)
        return [self.converter.domain_to_dns_zone(domain)
                for domain in domains]

    def modify_content(self, *args, **kwargs):
        raise NotImplementedError("Not implemented for RS DNS.")

    def rename_entry(self, *args, **kwargs):
        raise NotImplementedError("Not implemented for RS DNS.")


class RsDnsInstanceEntryFactory(object):
    """Defines how instance DNS entries are created for instances."""

    def __init__(self, dns_domain_id=None):
        dns_domain_id = dns_domain_id or DNS_DOMAIN_ID
        self.default_dns_zone = RsDnsZone(id=dns_domain_id,
                                          name=DNS_DOMAIN_NAME)

    def create_entry(self, instance_id):
        id = instance_id
        hostname = ("%s.%s" % (hashlib.sha1(id).hexdigest(),
                               self.default_dns_zone.name))
        return DnsEntry(name=hostname, content=None, type="A", ttl=DNS_TTL,
                        dns_zone=self.default_dns_zone)


class RsDnsZone(object):

    def __init__(self, id, name):
        self.name = name
        self.id = id

    def __eq__(self, other):
        return (isinstance(other, RsDnsZone) and
                self.name == other.name and
                self.id == other.id)

    def __str__(self):
        return "%s:%s" % (self.id, self.name)

########NEW FILE########
__FILENAME__ = models
# Copyright 2010-2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from trove.openstack.common import log as logging

from trove.common.remote import create_nova_client
from trove.instance.models import DBInstance
from trove.extensions.mgmt.instances.models import MgmtInstances

LOG = logging.getLogger(__name__)


class Server(object):
    """Disguises the Nova account instance dict as a server object."""

    def __init__(self, server):
        self.id = server['id']
        self.status = server['status']
        self.name = server['name']
        self.host = server.get('host') or server['hostId']

    @staticmethod
    def list_from_account_server_list(servers):
        """Converts a list of server account dicts to this object."""
        return [Server(server) for server in servers]


class Account(object):
    """Contains all instances owned by an account."""

    def __init__(self, id, instances):
        self.id = id
        self.instances = instances

    @staticmethod
    def load(context, id):
        client = create_nova_client(context)
        account = client.accounts.get_instances(id)
        db_infos = DBInstance.find_all(tenant_id=id, deleted=False)
        servers = [Server(server) for server in account.servers]
        instances = MgmtInstances.load_status_from_existing(context, db_infos,
                                                            servers)
        return Account(id, instances)


class AccountsSummary(object):

    def __init__(self, accounts):
        self.accounts = accounts

    @classmethod
    def load(cls):
        # TODO(pdmars): This should probably be changed to a more generic
        # database filter query if one is added, however, this should suffice
        # for now.
        db_infos = DBInstance.find_all(deleted=False)
        tenant_ids_for_instances = [db_info.tenant_id for db_info in db_infos]
        tenant_ids = set(tenant_ids_for_instances)
        LOG.debug("All tenants with instances: %s" % tenant_ids)
        accounts = []
        for tenant_id in tenant_ids:
            num_instances = tenant_ids_for_instances.count(tenant_id)
            accounts.append({'id': tenant_id, 'num_instances': num_instances})
        return cls(accounts)

########NEW FILE########
__FILENAME__ = service
# Copyright 2012 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from trove.openstack.common import log as logging

from trove.common import wsgi
from trove.common.auth import admin_context
from trove.extensions.account import models
from trove.extensions.account import views
from trove.openstack.common.gettextutils import _
import trove.common.apischema as apischema

LOG = logging.getLogger(__name__)


class AccountController(wsgi.Controller):
    """Controller for account functionality"""
    schemas = apischema.account

    @admin_context
    def show(self, req, tenant_id, id):
        """Return a account and instances associated with a single account."""
        LOG.info(_("req : '%s'\n\n") % req)
        LOG.info(_("Showing account information for '%(account)s' "
                   "to '%(tenant)s'") % {'account': id, 'tenant': tenant_id})

        context = req.environ[wsgi.CONTEXT_KEY]
        account = models.Account.load(context, id)
        return wsgi.Result(views.AccountView(account).data(), 200)

    @admin_context
    def index(self, req, tenant_id):
        """Return a list of all accounts with non-deleted instances."""
        LOG.info(_("req : '%s'\n\n") % req)
        LOG.info(_("Showing all accounts with instances for '%s'") % tenant_id)
        accounts_summary = models.AccountsSummary.load()
        return wsgi.Result(views.AccountsView(accounts_summary).data(), 200)

########NEW FILE########
__FILENAME__ = views
# Copyright 2012 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


class AccountsView(object):

    def __init__(self, accounts_summary):
        self.accounts_summary = accounts_summary

    def data(self):
        return {'accounts': self.accounts_summary.accounts}


class AccountView(object):

    def __init__(self, account):
        self.account = account

    def data(self):
        instance_list = [InstanceView(instance).data()
                         for instance in self.account.instances]
        return {
            'account': {
                'id': self.account.id,
                'instances': instance_list,
            }
        }


class InstanceView(object):

    def __init__(self, instance):
        self.instance = instance

    def data(self):
        server_host = None
        if self.instance.server is not None:
            server_host = self.instance.server.host
        return {'id': self.instance.id,
                'status': self.instance.status,
                'name': self.instance.name,
                'host': server_host,
                }

########NEW FILE########
__FILENAME__ = service
# Copyright 2012 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


from trove.common import exception
from trove.common import wsgi
from trove.extensions.mgmt.host import models
from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _

LOG = logging.getLogger(__name__)


class HostInstanceController(wsgi.Controller):
    """Controller for all instances on specific hosts."""

    def action(self, req, body, tenant_id, host_id):
        LOG.info("req : '%s'\n\n" % req)
        LOG.info("Committing an ACTION against host %s for tenant '%s'"
                 % (host_id, tenant_id))
        if not body:
            raise exception.BadRequest(_("Invalid request body."))
        context = req.environ[wsgi.CONTEXT_KEY]
        host = models.DetailedHost.load(context, host_id)
        _actions = {'update': self._action_update}
        selected_action = None
        for key in body:
            if key in _actions:
                if selected_action is not None:
                    msg = _("Only one action can be specified per request.")
                    raise exception.BadRequest(msg)
                selected_action = _actions[key]
            else:
                msg = _("Invalid host action: %s") % key
                raise exception.BadRequest(msg)

        if selected_action:
            return selected_action(context, host, body)
        else:
            raise exception.BadRequest(_("Invalid request body."))

    def _action_update(self, context, host, body):
        LOG.debug("Updating all instances for host: %s" % host.name)
        host.update_all(context)
        return wsgi.Result(None, 202)

########NEW FILE########
__FILENAME__ = models
# Copyright 2012 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Model classes that extend the instances functionality for MySQL instances.
"""

from trove.openstack.common import log as logging

from trove.common import exception
from trove.instance.models import DBInstance
from trove.instance.models import InstanceServiceStatus
from trove.instance.models import SimpleInstance
from trove.common.remote import create_guest_client
from trove.common.remote import create_nova_client
from novaclient import exceptions as nova_exceptions


LOG = logging.getLogger(__name__)


class SimpleHost(object):

    def __init__(self, name, instance_count):
        self.name = name
        self.instance_count = instance_count

    @staticmethod
    def load_all(context):
        client = create_nova_client(context)
        LOG.debug("Client.rdhosts=" + str(client.rdhosts))
        rdhosts = client.rdhosts.list()
        LOG.debug("RDHOSTS=" + str(rdhosts))
        for rdhost in rdhosts:
            LOG.debug("rdhost=" + str(rdhost))
        return [SimpleHost(rdhost.name, rdhost.instanceCount)
                for rdhost in rdhosts]


class DetailedHost(object):

    def __init__(self, host_info):
        self.name = host_info.name
        self.percent_used = host_info.percentUsed
        self.total_ram = host_info.totalRAM
        self.used_ram = host_info.usedRAM
        self.instances = host_info.instances
        for instance in self.instances:
            instance['server_id'] = instance['uuid']
            del instance['uuid']
            try:
                db_info = DBInstance.find_by(
                    compute_instance_id=instance['server_id'])
                instance['id'] = db_info.id
                instance['tenant_id'] = db_info.tenant_id
                status = InstanceServiceStatus.find_by(
                    instance_id=db_info.id)
                instance_info = SimpleInstance(None, db_info, status)
                instance['status'] = instance_info.status
            except exception.TroveError as re:
                LOG.error(re)
                LOG.error("Compute Instance ID found with no associated RD "
                          "instance: %s" % instance['server_id'])
                instance['id'] = None

    def update_all(self, context):
        num_i = len(self.instances)
        LOG.debug("Host %s has %s instances to update" % (self.name, num_i))
        failed_instances = []
        for instance in self.instances:
            client = create_guest_client(context, instance['id'])
            try:
                client.update_guest()
            except exception.TroveError as re:
                LOG.error(re)
                LOG.error("Unable to update instance: %s" % instance['id'])
                failed_instances.append(instance['id'])
        if len(failed_instances) > 0:
            msg = "Failed to update instances: %s" % failed_instances
            raise exception.UpdateGuestError(msg)

    @staticmethod
    def load(context, name):
        client = create_nova_client(context)
        try:
            return DetailedHost(client.rdhosts.get(name))
        except nova_exceptions.NotFound:
            raise exception.NotFound(uuid=name)

########NEW FILE########
__FILENAME__ = service
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


from trove.common import wsgi
from trove.common.auth import admin_context
from trove.extensions.mgmt.host import models
from trove.extensions.mgmt.host import views
from trove.instance.service import InstanceController
from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _

LOG = logging.getLogger(__name__)


class HostController(InstanceController):
    """Controller for instance functionality"""

    @admin_context
    def index(self, req, tenant_id, detailed=False):
        """Return all hosts."""
        LOG.info(_("req : '%s'\n\n") % req)
        LOG.info(_("Indexing a host for tenant '%s'") % tenant_id)
        context = req.environ[wsgi.CONTEXT_KEY]
        hosts = models.SimpleHost.load_all(context)
        return wsgi.Result(views.HostsView(hosts).data(), 200)

    @admin_context
    def show(self, req, tenant_id, id):
        """Return a single host."""
        LOG.info(_("req : '%s'\n\n") % req)
        LOG.info(_("Showing a host for tenant '%s'") % tenant_id)
        LOG.info(_("id : '%s'\n\n") % id)
        context = req.environ[wsgi.CONTEXT_KEY]
        host = models.DetailedHost.load(context, id)
        return wsgi.Result(views.HostDetailedView(host).data(), 200)

########NEW FILE########
__FILENAME__ = views
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


class HostView(object):

    def __init__(self, host):
        self.host = host

    def data(self):
        return {
            'instanceCount': self.host.instance_count,
            'name': self.host.name
        }


class HostDetailedView(object):

    def __init__(self, host):
        self.host = host

    def data(self):
        return {'host': {
            'instances': self.host.instances,
            'name': self.host.name,
            'percentUsed': self.host.percent_used,
            'totalRAM': self.host.total_ram,
            'usedRAM': self.host.used_ram
        }}


class HostsView(object):

    def __init__(self, hosts):
        self.hosts = hosts

    def data(self):
        data = [HostView(host).data() for host in self.hosts]
        return {'hosts': data}

########NEW FILE########
__FILENAME__ = models
#    Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
import datetime

from trove.common import cfg
from trove.common import remote
from trove.common import utils
from trove.openstack.common import log as logging
from trove.openstack.common.notifier import api as notifier
from trove.instance import models as imodels
from trove.instance.models import load_instance, InstanceServiceStatus
from trove.instance import models as instance_models
from trove.extensions.mysql import models as mysql_models

LOG = logging.getLogger(__name__)
CONF = cfg.CONF


def load_mgmt_instances(context, deleted=None, client=None):
    if not client:
        client = remote.create_nova_client(context)
    try:
        mgmt_servers = client.rdservers.list()
    except AttributeError:
        mgmt_servers = client.servers.list(search_opts={'all_tenants': 1})
    LOG.info("Found %d servers in Nova" %
             len(mgmt_servers if mgmt_servers else []))
    if deleted is not None:
        db_infos = instance_models.DBInstance.find_all(deleted=deleted)
    else:
        db_infos = instance_models.DBInstance.find_all()
    instances = MgmtInstances.load_status_from_existing(context, db_infos,
                                                        mgmt_servers)
    return instances


def load_mgmt_instance(cls, context, id):
    try:
        instance = load_instance(cls, context, id, needs_server=True)
        client = remote.create_nova_client(context)
        try:
            server = client.rdservers.get(instance.server_id)
        except AttributeError:
            server = client.servers.get(instance.server_id)
        instance.server.host = server.host
        instance.server.deleted = server.deleted
        instance.server.deleted_at = server.deleted_at
        instance.server.local_id = server.local_id
        assert instance.server is not None
    except Exception as e:
        LOG.error(e)
        instance = load_instance(cls, context, id, needs_server=False)
    return instance


class SimpleMgmtInstance(imodels.BaseInstance):
    def __init__(self, context, db_info, server, datastore_status):
        super(SimpleMgmtInstance, self).__init__(context, db_info, server,
                                                 datastore_status)

    @property
    def status(self):
        if self.deleted:
            return imodels.InstanceStatus.SHUTDOWN
        return super(SimpleMgmtInstance, self).status

    @property
    def deleted(self):
        return self.db_info.deleted

    @property
    def deleted_at(self):
        return self.db_info.deleted_at

    @classmethod
    def load(cls, context, id):
        return load_mgmt_instance(cls, context, id)

    @property
    def task_description(self):
        return self.db_info.task_description


class DetailedMgmtInstance(SimpleMgmtInstance):
    def __init__(self, *args, **kwargs):
        super(DetailedMgmtInstance, self).__init__(*args, **kwargs)
        self.volume = None
        self.volume_used = None
        self.volume_total = None
        self.root_history = None

    @classmethod
    def load(cls, context, id):
        instance = load_mgmt_instance(cls, context, id)
        client = remote.create_cinder_client(context)
        try:
            instance.volume = client.volumes.get(instance.volume_id)
        except Exception:
            instance.volume = None
            # Populate the volume_used attribute from the guest agent.
        instance_models.load_guest_info(instance, context, id)
        instance.root_history = mysql_models.RootHistory.load(context=context,
                                                              instance_id=id)
        return instance


class MgmtInstance(imodels.Instance):
    def get_diagnostics(self):
        return self.get_guest().get_diagnostics()

    def stop_db(self):
        return self.get_guest().stop_db()

    def get_hwinfo(self):
        return self.get_guest().get_hwinfo()


class MgmtInstances(imodels.Instances):
    @staticmethod
    def load_status_from_existing(context, db_infos, servers):
        def load_instance(context, db, status, server=None):
            return SimpleMgmtInstance(context, db, server, status)

        if context is None:
            raise TypeError("Argument context not defined.")
        find_server = imodels.create_server_list_matcher(servers)
        instances = imodels.Instances._load_servers_status(load_instance,
                                                           context,
                                                           db_infos,
                                                           find_server)
        _load_servers(instances, find_server)
        return instances


def _load_servers(instances, find_server):
    for instance in instances:
        db = instance.db_info
        instance.server = None
        try:
            server = find_server(db.id, db.compute_instance_id)
            instance.server = server
        except Exception as ex:
            LOG.error(ex)
    return instances


def publish_exist_events(transformer, admin_context):
    notifications = transformer()
    # clear out admin_context.auth_token so it does not get logged
    admin_context.auth_token = None
    for notification in notifications:
        notifier.notify(admin_context,
                        CONF.host,
                        "trove.instance.exists",
                        'INFO',
                        notification)


class NotificationTransformer(object):
    def __init__(self, **kwargs):
        pass

    @staticmethod
    def _get_audit_period():
        now = datetime.datetime.now()
        audit_start = utils.isotime(now, subsecond=True)
        audit_end = utils.isotime(
            now + datetime.timedelta(
                seconds=CONF.exists_notification_ticks * CONF.report_interval),
            subsecond=True)
        return audit_start, audit_end

    def _get_service_id(self, datastore_manager, id_map):
        if datastore_manager in id_map:
            datastore_manager_id = id_map[datastore_manager]
        else:
            datastore_manager_id = cfg.UNKNOWN_SERVICE_ID
            LOG.error("Datastore ID for Manager (%s) is not configured"
                      % datastore_manager)
        return datastore_manager_id

    def transform_instance(self, instance, audit_start, audit_end):
        payload = {
            'audit_period_beginning': audit_start,
            'audit_period_ending': audit_end,
            'created_at': instance.created,
            'display_name': instance.name,
            'instance_id': instance.id,
            'instance_name': instance.name,
            'instance_type_id': instance.flavor_id,
            'launched_at': instance.created,
            'nova_instance_id': instance.server_id,
            'region': CONF.region,
            'state_description': instance.status.lower(),
            'state': instance.status.lower(),
            'tenant_id': instance.tenant_id
        }
        payload['service_id'] = self._get_service_id(
            instance.datastore_version.manager, CONF.notification_service_id)
        return payload

    def __call__(self):
        audit_start, audit_end = NotificationTransformer._get_audit_period()
        messages = []
        db_infos = instance_models.DBInstance.find_all(deleted=False)
        for db_info in db_infos:
            service_status = InstanceServiceStatus.find_by(
                instance_id=db_info.id)
            instance = SimpleMgmtInstance(None, db_info, None, service_status)
            message = self.transform_instance(instance, audit_start, audit_end)
            messages.append(message)
        return messages


class NovaNotificationTransformer(NotificationTransformer):
    def __init__(self, **kwargs):
        super(NovaNotificationTransformer, self).__init__(**kwargs)
        self.context = kwargs['context']
        self.nova_client = remote.create_admin_nova_client(self.context)
        self._flavor_cache = {}

    def _lookup_flavor(self, flavor_id):
        if flavor_id in self._flavor_cache:
            LOG.debug("Flavor cache hit for %s" % flavor_id)
            return self._flavor_cache[flavor_id]
        # fetch flavor resource from nova
        LOG.info("Flavor cache miss for %s" % flavor_id)
        flavor = self.nova_client.flavors.get(flavor_id)
        self._flavor_cache[flavor_id] = flavor.name if flavor else 'unknown'
        return self._flavor_cache[flavor_id]

    def __call__(self):
        audit_start, audit_end = NotificationTransformer._get_audit_period()
        instances = load_mgmt_instances(self.context, deleted=False,
                                        client=self.nova_client)
        messages = []
        for instance in filter(
                lambda inst: inst.status != 'SHUTDOWN' and inst.server,
                instances):
            message = {
                'instance_type': self._lookup_flavor(instance.flavor_id),
                'user_id': instance.server.user_id
            }
            message.update(self.transform_instance(instance,
                                                   audit_start,
                                                   audit_end))
            messages.append(message)
        return messages

########NEW FILE########
__FILENAME__ = service
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


from novaclient import exceptions as nova_exceptions

from trove.backup.models import Backup
from trove.common import exception
from trove.common import wsgi
from trove.common.auth import admin_context
from trove.instance import models as instance_models
from trove.extensions.mgmt.instances import models
from trove.extensions.mgmt.instances import views
from trove.extensions.mgmt.instances.views import DiagnosticsView
from trove.extensions.mgmt.instances.views import HwInfoView
from trove.extensions.mysql import models as mysql_models
from trove.instance.service import InstanceController
from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _
import trove.common.apischema as apischema


LOG = logging.getLogger(__name__)


class MgmtInstanceController(InstanceController):
    """Controller for instance functionality"""
    schemas = apischema.mgmt_instance

    @classmethod
    def get_action_schema(cls, body, action_schema):
        action_type = body.keys()[0]
        return action_schema.get(action_type, {})

    @admin_context
    def index(self, req, tenant_id, detailed=False):
        """Return all instances."""
        LOG.info(_("req : '%s'\n\n") % req)
        LOG.info(_("Indexing a database instance for tenant '%s'") % tenant_id)
        context = req.environ[wsgi.CONTEXT_KEY]
        deleted = None
        deleted_q = req.GET.get('deleted', '').lower()
        if deleted_q in ['true']:
            deleted = True
        elif deleted_q in ['false']:
            deleted = False
        try:
            instances = models.load_mgmt_instances(context, deleted=deleted)
        except nova_exceptions.ClientException as e:
            LOG.error(e)
            return wsgi.Result(str(e), 403)

        view_cls = views.MgmtInstancesView
        return wsgi.Result(view_cls(instances, req=req).data(), 200)

    @admin_context
    def show(self, req, tenant_id, id):
        """Return a single instance."""
        LOG.info(_("req : '%s'\n\n") % req)
        LOG.info(_("Showing a database instance for tenant '%s'") % tenant_id)
        LOG.info(_("id : '%s'\n\n") % id)

        context = req.environ[wsgi.CONTEXT_KEY]
        server = models.DetailedMgmtInstance.load(context, id)
        root_history = mysql_models.RootHistory.load(context=context,
                                                     instance_id=id)
        return wsgi.Result(
            views.MgmtInstanceDetailView(
                server,
                req=req,
                root_history=root_history).data(),
            200)

    @admin_context
    def action(self, req, body, tenant_id, id):
        LOG.info("req : '%s'\n\n" % req)
        LOG.info("Committing an ACTION against instance %s for tenant '%s'"
                 % (id, tenant_id))
        if not body:
            raise exception.BadRequest(_("Invalid request body."))
        context = req.environ[wsgi.CONTEXT_KEY]
        instance = models.MgmtInstance.load(context=context, id=id)
        _actions = {
            'stop': self._action_stop,
            'reboot': self._action_reboot,
            'migrate': self._action_migrate,
            'reset-task-status': self._action_reset_task_status
        }
        selected_action = None
        for key in body:
            if key in _actions:
                if selected_action is not None:
                    msg = _("Only one action can be specified per request.")
                    raise exception.BadRequest(msg)
                selected_action = _actions[key]
            else:
                msg = _("Invalid instance action: %s") % key
                raise exception.BadRequest(msg)

        if selected_action:
            return selected_action(context, instance, body)
        else:
            raise exception.BadRequest(_("Invalid request body."))

    def _action_stop(self, context, instance, body):
        LOG.debug("Stopping MySQL on instance %s." % instance.id)
        instance.stop_db()
        return wsgi.Result(None, 202)

    def _action_reboot(self, context, instance, body):
        LOG.debug("Rebooting instance %s." % instance.id)
        instance.reboot()
        return wsgi.Result(None, 202)

    def _action_migrate(self, context, instance, body):
        LOG.debug("Migrating instance %s." % instance.id)
        LOG.debug("body['migrate']= %s" % body['migrate'])
        host = body['migrate'].get('host', None)
        instance.migrate(host)
        return wsgi.Result(None, 202)

    def _action_reset_task_status(self, context, instance, body):
        LOG.debug("Setting Task-Status to NONE on instance %s." %
                  instance.id)
        instance.reset_task_status()

        LOG.debug("Failing backups for instance %s." % instance.id)
        Backup.fail_for_instance(instance.id)

        return wsgi.Result(None, 202)

    @admin_context
    def root(self, req, tenant_id, id):
        """Return the date and time root was enabled on an instance,
            if ever.
        """
        LOG.info(_("req : '%s'\n\n") % req)
        LOG.info(_("Showing root history for tenant '%s'") % tenant_id)
        LOG.info(_("id : '%s'\n\n") % id)
        context = req.environ[wsgi.CONTEXT_KEY]
        try:
            instance_models.Instance.load(context=context, id=id)
        except exception.TroveError as e:
            LOG.error(e)
            return wsgi.Result(str(e), 404)
        rhv = views.RootHistoryView(id)
        reh = mysql_models.RootHistory.load(context=context, instance_id=id)
        if reh:
            rhv = views.RootHistoryView(reh.id, enabled=reh.created,
                                        user_id=reh.user)
        return wsgi.Result(rhv.data(), 200)

    @admin_context
    def hwinfo(self, req, tenant_id, id):
        """Return a single instance hardware info."""
        LOG.info(_("req : '%s'\n\n") % req)
        LOG.info(_("Showing hardware info for instance '%s'") % id)

        context = req.environ[wsgi.CONTEXT_KEY]
        instance = models.MgmtInstance.load(context=context, id=id)

        hwinfo = instance.get_hwinfo()
        return wsgi.Result(HwInfoView(id, hwinfo).data(), 200)

    @admin_context
    def diagnostics(self, req, tenant_id, id):
        """Return a single instance diagnostics."""
        LOG.info(_("req : '%s'\n\n") % req)
        LOG.info(_("Showing a instance diagnostics for instance '%s'") % id)
        LOG.info(_("id : '%s'\n\n") % id)

        context = req.environ[wsgi.CONTEXT_KEY]
        instance = models.MgmtInstance.load(context=context, id=id)

        diagnostics = instance.get_diagnostics()
        return wsgi.Result(DiagnosticsView(id, diagnostics).data(), 200)

########NEW FILE########
__FILENAME__ = views
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


from trove.instance.views import InstanceDetailView


class MgmtInstanceView(InstanceDetailView):

    def __init__(self, instance, req=None):
        super(MgmtInstanceView, self).__init__(instance, req)

    def data(self):
        result = super(MgmtInstanceView, self).data()
        if self.instance.server is None:
            result['instance']['server'] = None
        else:
            server = self.instance.server
            result['instance']['server'] = {
                'id': server.id,
                'name': server.name,
                'status': server.status,
                'tenant_id': server.tenant_id,
            }
            if hasattr(server, 'host'):
                result['instance']['server']['host'] = server.host
            else:
                result['instance']['server']['host'] = server.hostId
            if hasattr(server, 'deleted'):
                result['instance']['server']['deleted'] = server.deleted
            if hasattr(server, 'deleted_at'):
                result['instance']['server']['deleted_at'] = server.deleted_at
            if hasattr(server, 'local_id'):
                result['instance']['server']['local_id'] = server.local_id

        try:
            service_status = self.instance.datastore_status.status.api_status
        except AttributeError:
            service_status = None
        result['instance']['service_status'] = service_status
        result['instance']['tenant_id'] = self.instance.tenant_id
        result['instance']['deleted'] = bool(self.instance.deleted)
        result['instance']['deleted_at'] = self.instance.deleted_at
        result['instance']['task_description'] = self.instance.task_description
        return result


class MgmtInstanceDetailView(MgmtInstanceView):
    """Works with a full-blown instance."""

    def __init__(self, instance, req, root_history=None):
        super(MgmtInstanceDetailView, self).__init__(instance,
                                                     req=req)
        self.root_history = root_history

    def data(self):
        result = super(MgmtInstanceDetailView, self).data()
        if self.instance.server is not None:
            server = self.instance.server
            result['instance']['server'].update(
                {'addresses': server.addresses})
        if self.root_history:
            result['instance']['root_enabled'] = self.root_history.created
            result['instance']['root_enabled_by'] = self.root_history.user
        if self.instance.volume:
            volume = self.instance.volume
            result['instance']['volume'] = {
                "attachments": volume.attachments,
                "availability_zone": volume.availability_zone,
                "created_at": volume.created_at,
                "id": volume.id,
                "size": volume.size,
                "status": volume.status,
                "used": self.instance.volume_used or None,
                "total": self.instance.volume_total or None,
            }
        else:
            result['instance']['volume'] = None
        description = self.instance.datastore_status.status.description
        result['instance']['guest_status'] = {"state_description": description}
        return result


class MgmtInstancesView(object):
    """Shows a list of MgmtInstance objects."""

    def __init__(self, instances, req=None):
        self.instances = instances
        self.req = req

    def data(self):
        data = []
        # These are model instances
        for instance in self.instances:
            data.append(self.data_for_instance(instance))
        return {'instances': data}

    def data_for_instance(self, instance):
        view = MgmtInstanceView(instance, req=self.req)
        return view.data()['instance']


class RootHistoryView(object):

    def __init__(self, instance_id, enabled='Never', user_id='Nobody'):
        self.instance_id = instance_id
        self.enabled = enabled
        self.user = user_id

    def data(self):
        return {
            'root_history': {
                'id': self.instance_id,
                'enabled': self.enabled,
                'user': self.user,
            }
        }


class HwInfoView(object):

    def __init__(self, instance_id, hwinfo):
        self.instance_id = instance_id
        self.hwinfo = hwinfo

    def data(self):
        return {
            'hwinfo': {
                'mem_total': self.hwinfo['mem_total'],
                'num_cpus': self.hwinfo['num_cpus'],
            }
        }


class DiagnosticsView(object):

    def __init__(self, instance_id, diagnostics):
        self.instance_id = instance_id
        self.diagnostics = diagnostics

    def data(self):
        return {
            'diagnostics': {
                'version': self.diagnostics['version'],
                'threads': self.diagnostics['threads'],
                'fdSize': self.diagnostics['fd_size'],
                'vmSize': self.diagnostics['vm_size'],
                'vmPeak': self.diagnostics['vm_peak'],
                'vmRss': self.diagnostics['vm_rss'],
                'vmHwm': self.diagnostics['vm_hwm'],
            }
        }

########NEW FILE########
__FILENAME__ = service
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from trove.common import wsgi
from trove.common import exception
from trove.common.auth import admin_context
from trove.extensions.mgmt.quota import views
from trove.openstack.common import log as logging
from trove.quota.quota import QUOTAS as quota_engine
from trove.quota.models import Quota

LOG = logging.getLogger(__name__)


class QuotaController(wsgi.Controller):
    """Controller for quota  functionality"""

    @admin_context
    def show(self, req, tenant_id, id):
        """Return all quotas for this tenant."""
        LOG.info(_("req : '%s'\n\n") % req)
        LOG.info(_("Indexing quota info for tenant '%s'") % id)
        quotas = quota_engine.get_all_quotas_by_tenant(id)
        return wsgi.Result(views.QuotaView(quotas).data(), 200)

    @admin_context
    def update(self, req, body, tenant_id, id):
        LOG.info("req : '%s'\n\n" % req)
        LOG.info("Updating quota limits for tenant '%s'" % id)
        if not body:
            raise exception.BadRequest(_("Invalid request body."))

        quotas = {}
        quota = None
        registered_resources = quota_engine.resources
        for resource, limit in body['quotas'].items():
            if limit is None:
                continue
            if resource == "xmlns":
                continue
            if resource not in registered_resources:
                raise exception.QuotaResourceUnknown(unknown=resource)
            try:
                quota = Quota.find_by(tenant_id=id, resource=resource)
                quota.hard_limit = limit
                quota.save()
            except exception.ModelNotFoundError:
                quota = Quota.create(tenant_id=id,
                                     resource=resource,
                                     hard_limit=limit)

            quotas[resource] = quota

        return wsgi.Result(views.QuotaView(quotas).data(), 200)

########NEW FILE########
__FILENAME__ = views
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


class QuotaView(object):

    def __init__(self, quotas):
        self.quotas = quotas

    def data(self):
        rtn = {}
        for resource_name, quota in self.quotas.items():
            rtn[resource_name] = quota.hard_limit
        return {'quotas': rtn}

########NEW FILE########
__FILENAME__ = models
# Copyright 2012 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Model classes that extend the instances functionality for volumes.
"""

from trove.openstack.common import log as logging

from trove.common.remote import create_cinder_client


LOG = logging.getLogger(__name__)


class StorageDevice(object):

    def __init__(self, storage_info):
        self.name = storage_info.name
        self.type = storage_info.type
        self.total_space = storage_info.capacity['total']
        self.total_avail = storage_info.capacity['available']
        self.prov_total = storage_info.provision['total']
        self.prov_avail = storage_info.provision['available']
        self.prov_percent = storage_info.provision['percent']
        self.used = storage_info.used


class StorageDevices(object):

    @staticmethod
    def load(context):
        client = create_cinder_client(context)
        rdstorages = client.rdstorage.list()
        for rdstorage in rdstorages:
            LOG.debug("rdstorage=" + str(rdstorage))
        return [StorageDevice(storage_info)
                for storage_info in rdstorages]

########NEW FILE########
__FILENAME__ = service
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


from trove.common import wsgi
from trove.common.auth import admin_context
from trove.extensions.mgmt.volume import models
from trove.extensions.mgmt.volume import views
from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _

LOG = logging.getLogger(__name__)


class StorageController(wsgi.Controller):
    """Controller for storage device functionality"""

    @admin_context
    def index(self, req, tenant_id):
        """Return all storage devices."""
        LOG.info(_("req : '%s'\n\n") % req)
        LOG.info(_("Indexing storage info for tenant '%s'") % tenant_id)
        context = req.environ[wsgi.CONTEXT_KEY]
        storages = models.StorageDevices.load(context)
        return wsgi.Result(views.StoragesView(storages).data(), 200)

########NEW FILE########
__FILENAME__ = views
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


class StorageView(object):

    def __init__(self, storage):
        self.storage = storage

    def data(self):
        return {'name': self.storage.name,
                'type': self.storage.type,
                'capacity': {'total': self.storage.total_space,
                             'available': self.storage.total_avail},
                'provision': {'total': self.storage.prov_total,
                              'available': self.storage.prov_avail,
                              'percent': self.storage.prov_percent},
                'used': self.storage.used}


class StoragesView(object):

    def __init__(self, storages):
        self.storages = storages

    def data(self):
        data = [StorageView(storage).data() for storage in self.storages]
        return {'devices': data}

########NEW FILE########
__FILENAME__ = common
#    Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from trove.common import exception
from trove.guestagent.db import models as guest_models
from urllib import unquote


def populate_validated_databases(dbs):
    """
    Create a serializable request with user provided data
    for creating new databases.
    """
    try:
        databases = []
        unique_identities = set()
        for database in dbs:
            mydb = guest_models.ValidatedMySQLDatabase()
            mydb.name = database.get('name', '')
            if mydb.name in unique_identities:
                raise exception.DatabaseInitialDatabaseDuplicateError()
            unique_identities.add(mydb.name)
            mydb.character_set = database.get('character_set', '')
            mydb.collate = database.get('collate', '')
            databases.append(mydb.serialize())
        return databases
    except ValueError as ve:
        # str(ve) contains user input and may include '%' which can cause a
        # format str vulnerability. Escape the '%' to avoid this. This is
        # okay to do since we're not using dict args here in any case.
        safe_string = str(ve).replace('%', '%%')
        raise exception.BadRequest(safe_string)


def populate_users(users, initial_databases=None):
    """Create a serializable request containing users"""
    users_data = []
    unique_identities = set()
    for user in users:
        u = guest_models.MySQLUser()
        u.name = user.get('name', '')
        u.host = user.get('host', '%')
        user_identity = (u.name, u.host)
        if user_identity in unique_identities:
            raise exception.DatabaseInitialUserDuplicateError()
        unique_identities.add(user_identity)
        u.password = user.get('password', '')
        user_dbs = user.get('databases', '')
        # user_db_names guaranteed unique and non-empty by apischema
        user_db_names = [user_db.get('name', '') for user_db in user_dbs]
        for user_db_name in user_db_names:
            if (initial_databases is not None and user_db_name not in
                    initial_databases):
                raise exception.DatabaseForUserNotInDatabaseListError(
                    user=u.name, database=user_db_name)
            u.databases = user_db_name
        users_data.append(u.serialize())
    return users_data


def unquote_user_host(user_hostname):
    unquoted = unquote(user_hostname)
    if '@' not in unquoted:
        return unquoted, '%'
    if unquoted.endswith('@'):
        return unquoted, '%'
    splitup = unquoted.split('@')
    host = splitup[-1]
    user = '@'.join(splitup[:-1])
    return user, host

########NEW FILE########
__FILENAME__ = models
# Copyright 2010-2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Model classes that extend the instances functionality for MySQL instances.
"""

from trove.common import cfg
from trove.common import exception
from trove.common import utils
from trove.common.remote import create_guest_client
from trove.db import get_db_api
from trove.guestagent.db import models as guest_models
from trove.instance import models as base_models
from trove.openstack.common import log as logging

CONF = cfg.CONF
LOG = logging.getLogger(__name__)


def persisted_models():
    return {'root_enabled_history': RootHistory}


def load_and_verify(context, instance_id):
    # Load InstanceServiceStatus to verify if its running
    instance = base_models.Instance.load(context, instance_id)
    if not instance.is_datastore_running:
        raise exception.UnprocessableEntity(
            "Instance %s is not ready." % instance.id)
    else:
        return instance


class User(object):

    _data_fields = ['name', 'host', 'password', 'databases']

    def __init__(self, name, host, password, databases):
        self.name = name
        self.host = host
        self.password = password
        self.databases = databases

    @classmethod
    def load(cls, context, instance_id, username, hostname):
        load_and_verify(context, instance_id)
        validate = guest_models.MySQLUser()
        validate.name = username
        validate.host = hostname
        client = create_guest_client(context, instance_id)
        found_user = client.get_user(username=username, hostname=hostname)
        if not found_user:
            return None
        database_names = [{'name': db['_name']}
                          for db in found_user['_databases']]
        return cls(found_user['_name'],
                   found_user['_host'],
                   found_user['_password'],
                   database_names)

    @classmethod
    def create(cls, context, instance_id, users):
        # Load InstanceServiceStatus to verify if it's running
        load_and_verify(context, instance_id)
        client = create_guest_client(context, instance_id)
        for user in users:
            user_name = user['_name']
            host_name = user['_host']
            userhost = "%s@%s" % (user_name, host_name)
            existing_users, _nadda = Users.load_with_client(
                client,
                limit=1,
                marker=userhost,
                include_marker=True)
            if (len(existing_users) > 0 and
                    str(existing_users[0].name) == str(user_name) and
                    str(existing_users[0].host) == str(host_name)):
                raise exception.UserAlreadyExists(name=user_name,
                                                  host=host_name)
        return client.create_user(users)

    @classmethod
    def delete(cls, context, instance_id, user):
        load_and_verify(context, instance_id)
        create_guest_client(context, instance_id).delete_user(user)

    @classmethod
    def access(cls, context, instance_id, username, hostname):
        load_and_verify(context, instance_id)
        client = create_guest_client(context, instance_id)
        databases = client.list_access(username, hostname)
        dbs = []
        for db in databases:
            dbs.append(Schema(name=db['_name'],
                              collate=db['_collate'],
                              character_set=db['_character_set']))
        return UserAccess(dbs)

    @classmethod
    def grant(cls, context, instance_id, username, hostname, databases):
        load_and_verify(context, instance_id)
        client = create_guest_client(context, instance_id)
        client.grant_access(username, hostname, databases)

    @classmethod
    def revoke(cls, context, instance_id, username, hostname, database):
        load_and_verify(context, instance_id)
        client = create_guest_client(context, instance_id)
        client.revoke_access(username, hostname, database)

    @classmethod
    def change_password(cls, context, instance_id, users):
        load_and_verify(context, instance_id)
        client = create_guest_client(context, instance_id)
        change_users = []
        for user in users:
            change_user = {'name': user.name,
                           'host': user.host,
                           'password': user.password,
                           }
            change_users.append(change_user)
        client.change_passwords(change_users)

    @classmethod
    def update_attributes(cls, context, instance_id, username, hostname,
                          user_attrs):
        load_and_verify(context, instance_id)
        client = create_guest_client(context, instance_id)

        user_changed = user_attrs.get('name')
        host_changed = user_attrs.get('host')

        validate = guest_models.MySQLUser()
        if host_changed:
            validate.host = host_changed
        if user_changed:
            validate.name = user_changed

        user = user_changed or username
        host = host_changed or hostname
        userhost = "%s@%s" % (user, host)
        if user_changed or host_changed:
            existing_users, _nadda = Users.load_with_client(
                client,
                limit=1,
                marker=userhost,
                include_marker=True)
            if (len(existing_users) > 0 and
                    existing_users[0].name == user and
                    existing_users[0].host == host):
                raise exception.UserAlreadyExists(name=user,
                                                  host=host)
        client.update_attributes(username, hostname, user_attrs)


class UserAccess(object):
    _data_fields = ['databases']

    def __init__(self, databases):
        self.databases = databases


class Root(object):

    @classmethod
    def load(cls, context, instance_id):
        load_and_verify(context, instance_id)
        # TODO(pdmars): remove the is_root_enabled call from the guest agent,
        # just check the database for this information.
        # If the root history returns null or raises an exception, the root
        # user hasn't been enabled.
        try:
            root_history = RootHistory.load(context, instance_id)
        except exception.NotFound:
            return False
        if not root_history:
            return False
        return True

    @classmethod
    def create(cls, context, instance_id, user):
        load_and_verify(context, instance_id)
        root = create_guest_client(context, instance_id).enable_root()
        root_user = guest_models.RootUser()
        root_user.deserialize(root)
        RootHistory.create(context, instance_id, user)
        return root_user


class RootHistory(object):

    _auto_generated_attrs = ['id']
    _data_fields = ['instance_id', 'user', 'created']
    _table_name = 'root_enabled_history'

    def __init__(self, instance_id, user):
        self.id = instance_id
        self.user = user
        self.created = utils.utcnow()

    def save(self):
        LOG.debug("Saving %(name)s: %(dict)s" %
                  {'name': self.__class__.__name__, 'dict': self.__dict__})
        return get_db_api().save(self)

    @classmethod
    def load(cls, context, instance_id):
        history = get_db_api().find_by(cls, id=instance_id)
        return history

    @classmethod
    def create(cls, context, instance_id, user):
        history = cls.load(context, instance_id)
        if history is not None:
            return history
        history = RootHistory(instance_id, user)
        return history.save()


def load_via_context(cls, context, instance_id):
    """Creates guest and fetches pagination arguments from the context."""
    load_and_verify(context, instance_id)
    limit = int(context.limit or cls.DEFAULT_LIMIT)
    limit = cls.DEFAULT_LIMIT if limit > cls.DEFAULT_LIMIT else limit
    client = create_guest_client(context, instance_id)
    # The REST API standard dictates that we *NEVER* include the marker.
    return cls.load_with_client(client=client, limit=limit,
                                marker=context.marker, include_marker=False)


class Users(object):

    DEFAULT_LIMIT = CONF.users_page_size

    @classmethod
    def load(cls, context, instance_id):
        return load_via_context(cls, context, instance_id)

    @classmethod
    def load_with_client(cls, client, limit, marker, include_marker):
        user_list, next_marker = client.list_users(
            limit=limit,
            marker=marker,
            include_marker=include_marker)
        model_users = []
        ignore_users = CONF.ignore_users
        for user in user_list:
            mysql_user = guest_models.MySQLUser()
            mysql_user.deserialize(user)
            if mysql_user.name in ignore_users:
                continue
            # TODO(hub-cap): databases are not being returned in the
            # reference agent
            dbs = []
            for db in mysql_user.databases:
                dbs.append({'name': db['_name']})
            model_users.append(User(mysql_user.name,
                                    mysql_user.host,
                                    mysql_user.password,
                                    dbs))
        return model_users, next_marker


class Schema(object):

    _data_fields = ['name', 'collate', 'character_set']

    def __init__(self, name, collate, character_set):
        self.name = name
        self.collate = collate
        self.character_set = character_set

    @classmethod
    def create(cls, context, instance_id, schemas):
        load_and_verify(context, instance_id)
        client = create_guest_client(context, instance_id)
        for schema in schemas:
            schema_name = schema['_name']
            existing_schema, _nadda = Schemas.load_with_client(
                client,
                limit=1,
                marker=schema_name,
                include_marker=True)
            if (len(existing_schema) > 0 and
                    str(existing_schema[0].name) == str(schema_name)):
                raise exception.DatabaseAlreadyExists(name=schema_name)
        return client.create_database(schemas)

    @classmethod
    def delete(cls, context, instance_id, schema):
        load_and_verify(context, instance_id)
        create_guest_client(context, instance_id).delete_database(schema)


class Schemas(object):

    DEFAULT_LIMIT = CONF.databases_page_size

    @classmethod
    def load(cls, context, instance_id):
        return load_via_context(cls, context, instance_id)

    @classmethod
    def load_with_client(cls, client, limit, marker, include_marker):
        schemas, next_marker = client.list_databases(
            limit=limit,
            marker=marker,
            include_marker=include_marker)
        model_schemas = []
        ignore_dbs = CONF.ignore_dbs
        for schema in schemas:
            mysql_schema = guest_models.MySQLDatabase()
            mysql_schema.deserialize(schema)
            if mysql_schema.name in ignore_dbs:
                continue
            model_schemas.append(Schema(mysql_schema.name,
                                        mysql_schema.collate,
                                        mysql_schema.character_set))
        return model_schemas, next_marker

########NEW FILE########
__FILENAME__ = service
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import webob.exc

from trove.common import exception
from trove.common import pagination
from trove.common import wsgi
from trove.common.utils import correct_id_with_req
from trove.extensions.mysql.common import populate_validated_databases
from trove.extensions.mysql.common import populate_users
from trove.extensions.mysql.common import unquote_user_host
from trove.extensions.mysql import models
from trove.extensions.mysql import views
from trove.guestagent.db import models as guest_models
from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _
import trove.common.apischema as apischema


LOG = logging.getLogger(__name__)


class RootController(wsgi.Controller):
    """Controller for instance functionality"""

    def index(self, req, tenant_id, instance_id):
        """Returns True if root is enabled for the given instance;
                    False otherwise.
        """
        LOG.info(_("Getting root enabled for instance '%s'") % instance_id)
        LOG.info(_("req : '%s'\n\n") % req)
        context = req.environ[wsgi.CONTEXT_KEY]
        is_root_enabled = models.Root.load(context, instance_id)
        return wsgi.Result(views.RootEnabledView(is_root_enabled).data(), 200)

    def create(self, req, tenant_id, instance_id):
        """Enable the root user for the db instance """
        LOG.info(_("Enabling root for instance '%s'") % instance_id)
        LOG.info(_("req : '%s'\n\n") % req)
        context = req.environ[wsgi.CONTEXT_KEY]
        user_name = context.user
        root = models.Root.create(context, instance_id, user_name)
        return wsgi.Result(views.RootCreatedView(root).data(), 200)


class UserController(wsgi.Controller):
    """Controller for instance functionality"""
    schemas = apischema.user

    @classmethod
    def get_schema(cls, action, body):
        action_schema = super(UserController, cls).get_schema(action, body)
        if 'update_all' == action:
            update_type = body.keys()[0]
            action_schema = action_schema.get(update_type, {})
        return action_schema

    def index(self, req, tenant_id, instance_id):
        """Return all users."""
        LOG.info(_("Listing users for instance '%s'") % instance_id)
        LOG.info(_("req : '%s'\n\n") % req)
        context = req.environ[wsgi.CONTEXT_KEY]
        users, next_marker = models.Users.load(context, instance_id)
        view = views.UsersView(users)
        paged = pagination.SimplePaginatedDataView(req.url, 'users', view,
                                                   next_marker)
        return wsgi.Result(paged.data(), 200)

    def create(self, req, body, tenant_id, instance_id):
        """Creates a set of users"""
        LOG.info(_("Creating users for instance '%s'") % instance_id)
        LOG.info(logging.mask_password(_("req : '%s'\n\n") % req))
        LOG.info(logging.mask_password(_("body : '%s'\n\n") % body))
        context = req.environ[wsgi.CONTEXT_KEY]
        users = body['users']
        try:
            model_users = populate_users(users)
            models.User.create(context, instance_id, model_users)
        except (ValueError, AttributeError) as e:
            raise exception.BadRequest(msg=str(e))
        return wsgi.Result(None, 202)

    def delete(self, req, tenant_id, instance_id, id):
        LOG.info(_("Deleting user for instance '%s'") % instance_id)
        LOG.info(_("req : '%s'\n\n") % req)
        context = req.environ[wsgi.CONTEXT_KEY]
        id = correct_id_with_req(id, req)
        username, host = unquote_user_host(id)
        user = None
        try:
            user = guest_models.MySQLUser()
            user.name = username
            user.host = host
            found_user = models.User.load(context, instance_id, username,
                                          host)
            if not found_user:
                user = None
        except (ValueError, AttributeError) as e:
            raise exception.BadRequest(msg=str(e))
        if not user:
            raise exception.UserNotFound(uuid=id)
        models.User.delete(context, instance_id, user.serialize())
        return wsgi.Result(None, 202)

    def show(self, req, tenant_id, instance_id, id):
        """Return a single user."""
        LOG.info(_("Showing a user for instance '%s'") % instance_id)
        LOG.info(_("req : '%s'\n\n") % req)
        context = req.environ[wsgi.CONTEXT_KEY]
        id = correct_id_with_req(id, req)
        username, host = unquote_user_host(id)
        user = None
        try:
            user = models.User.load(context, instance_id, username, host)
        except (ValueError, AttributeError) as e:
            raise exception.BadRequest(msg=str(e))
        if not user:
            raise exception.UserNotFound(uuid=id)
        view = views.UserView(user)
        return wsgi.Result(view.data(), 200)

    def update(self, req, body, tenant_id, instance_id, id):
        """Change attributes for one user."""
        LOG.info(_("Updating user attributes for instance '%s'") % instance_id)
        LOG.info(logging.mask_password(_("req : '%s'\n\n") % req))
        context = req.environ[wsgi.CONTEXT_KEY]
        id = correct_id_with_req(id, req)
        username, hostname = unquote_user_host(id)
        user = None
        user_attrs = body['user']
        try:
            user = models.User.load(context, instance_id, username, hostname)
        except (ValueError, AttributeError) as e:
            raise exception.BadRequest(msg=str(e))
        if not user:
            raise exception.UserNotFound(uuid=id)
        try:
            models.User.update_attributes(context, instance_id, username,
                                          hostname, user_attrs)
        except (ValueError, AttributeError) as e:
            raise exception.BadRequest(msg=str(e))
        return wsgi.Result(None, 202)

    def update_all(self, req, body, tenant_id, instance_id):
        """Change the password of one or more users."""
        LOG.info(_("Updating user passwords for instance '%s'") % instance_id)
        LOG.info(logging.mask_password(_("req : '%s'\n\n") % req))
        context = req.environ[wsgi.CONTEXT_KEY]
        users = body['users']
        model_users = []
        for user in users:
            try:
                mu = guest_models.MySQLUser()
                mu.name = user['name']
                mu.host = user.get('host')
                mu.password = user['password']
                found_user = models.User.load(context, instance_id,
                                              mu.name, mu.host)
                if not found_user:
                    user_and_host = mu.name
                    if mu.host:
                        user_and_host += '@' + mu.host
                    raise exception.UserNotFound(uuid=user_and_host)
                model_users.append(mu)
            except (ValueError, AttributeError) as e:
                raise exception.BadRequest(msg=str(e))
        models.User.change_password(context, instance_id, model_users)
        return wsgi.Result(None, 202)


class UserAccessController(wsgi.Controller):
    """Controller for adding and removing database access for a user."""
    schemas = apischema.user

    @classmethod
    def get_schema(cls, action, body):
        schema = {}
        if 'update_all' == action:
            schema = cls.schemas.get(action).get('databases')
        return schema

    def _get_user(self, context, instance_id, user_id):
        username, hostname = unquote_user_host(user_id)
        try:
            user = models.User.load(context, instance_id, username, hostname)
        except (ValueError, AttributeError) as e:
            raise exception.BadRequest(msg=str(e))
        if not user:
            raise exception.UserNotFound(uuid=user_id)
        return user

    def index(self, req, tenant_id, instance_id, user_id):
        """Show permissions for the given user."""
        LOG.info(_("Showing user access for instance '%s'") % instance_id)
        LOG.info(_("req : '%s'\n\n") % req)
        context = req.environ[wsgi.CONTEXT_KEY]
        # Make sure this user exists.
        user_id = correct_id_with_req(user_id, req)
        user = self._get_user(context, instance_id, user_id)
        if not user:
            LOG.error(_("No such user: %(user)s ") % {'user': user})
            raise exception.UserNotFound(uuid=user)
        username, hostname = unquote_user_host(user_id)
        access = models.User.access(context, instance_id, username, hostname)
        view = views.UserAccessView(access.databases)
        return wsgi.Result(view.data(), 200)

    def update(self, req, body, tenant_id, instance_id, user_id):
        """Grant access for a user to one or more databases."""
        LOG.info(_("Granting user access for instance '%s'") % instance_id)
        LOG.info(_("req : '%s'\n\n") % req)
        context = req.environ[wsgi.CONTEXT_KEY]
        user_id = correct_id_with_req(user_id, req)
        user = self._get_user(context, instance_id, user_id)
        if not user:
            LOG.error(_("No such user: %(user)s ") % {'user': user})
            raise exception.UserNotFound(uuid=user)
        username, hostname = unquote_user_host(user_id)
        databases = [db['name'] for db in body['databases']]
        models.User.grant(context, instance_id, username, hostname, databases)
        return wsgi.Result(None, 202)

    def delete(self, req, tenant_id, instance_id, user_id, id):
        """Revoke access for a user."""
        LOG.info(_("Revoking user access for instance '%s'") % instance_id)
        LOG.info(_("req : '%s'\n\n") % req)
        context = req.environ[wsgi.CONTEXT_KEY]
        user_id = correct_id_with_req(user_id, req)
        user = self._get_user(context, instance_id, user_id)
        if not user:
            LOG.error(_("No such user: %(user)s ") % {'user': user})
            raise exception.UserNotFound(uuid=user)
        username, hostname = unquote_user_host(user_id)
        access = models.User.access(context, instance_id, username, hostname)
        databases = [db.name for db in access.databases]
        if id not in databases:
            raise exception.DatabaseNotFound(uuid=id)
        models.User.revoke(context, instance_id, username, hostname, id)
        return wsgi.Result(None, 202)


class SchemaController(wsgi.Controller):
    """Controller for instance functionality"""
    schemas = apischema.dbschema

    def index(self, req, tenant_id, instance_id):
        """Return all schemas."""
        LOG.info(_("Listing schemas for instance '%s'") % instance_id)
        LOG.info(_("req : '%s'\n\n") % req)
        context = req.environ[wsgi.CONTEXT_KEY]
        schemas, next_marker = models.Schemas.load(context, instance_id)
        view = views.SchemasView(schemas)
        paged = pagination.SimplePaginatedDataView(req.url, 'databases', view,
                                                   next_marker)
        return wsgi.Result(paged.data(), 200)

    def create(self, req, body, tenant_id, instance_id):
        """Creates a set of schemas"""
        LOG.info(_("Creating schema for instance '%s'") % instance_id)
        LOG.info(_("req : '%s'\n\n") % req)
        LOG.info(_("body : '%s'\n\n") % body)
        context = req.environ[wsgi.CONTEXT_KEY]
        schemas = body['databases']
        model_schemas = populate_validated_databases(schemas)
        models.Schema.create(context, instance_id, model_schemas)
        return wsgi.Result(None, 202)

    def delete(self, req, tenant_id, instance_id, id):
        LOG.info(_("Deleting schema for instance '%s'") % instance_id)
        LOG.info(_("req : '%s'\n\n") % req)
        context = req.environ[wsgi.CONTEXT_KEY]
        try:
            schema = guest_models.ValidatedMySQLDatabase()
            schema.name = id
            models.Schema.delete(context, instance_id, schema.serialize())
        except (ValueError, AttributeError) as e:
            raise exception.BadRequest(msg=str(e))
        return wsgi.Result(None, 202)

    def show(self, req, tenant_id, instance_id, id):
        raise webob.exc.HTTPNotImplemented()

########NEW FILE########
__FILENAME__ = views
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


class UserView(object):

    def __init__(self, user):
        self.user = user

    def data(self):
        user_dict = {
            "name": self.user.name,
            "host": self.user.host,
            "databases": self.user.databases
        }
        return {"user": user_dict}


class UsersView(object):

    def __init__(self, users):
        self.users = users

    def data(self):
        userlist = [{"name": user.name,
                     "host": user.host,
                     "databases": user.databases}
                    for user in self.users]

        return {"users": userlist}


class UserAccessView(object):
    def __init__(self, databases):
        self.databases = databases

    def data(self):
        dbs = [{"name": db.name} for db in self.databases]
        return {"databases": dbs}


class RootCreatedView(UserView):

    def data(self):
        user_dict = {
            "name": self.user.name,
            "password": self.user.password
        }
        return {"user": user_dict}


class RootEnabledView(object):

    def __init__(self, is_root_enabled):
        self.is_root_enabled = is_root_enabled

    def data(self):
        return {'rootEnabled': self.is_root_enabled}


class SchemaView(object):

    def __init__(self, schema):
        self.schema = schema

    def data(self):
        return {"name": self.schema.name}


class SchemasView(object):

    def __init__(self, schemas):
        self.schemas = schemas

    def data(self):
        data = []
        # These are model instances
        for schema in self.schemas:
            data.append(SchemaView(schema).data())

        return {"databases": data}

########NEW FILE########
__FILENAME__ = account
# Copyright 2012 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from trove.openstack.common import log as logging

from trove.common import extensions
from trove.extensions.account import service


LOG = logging.getLogger(__name__)


class Account(extensions.ExtensionsDescriptor):

    def get_name(self):
        return "Account"

    def get_description(self):
        return "Account information with instances"

    def get_alias(self):
        return "Account"

    def get_namespace(self):
        return "http://TBD"

    def get_updated(self):
        return "2012-06-07T13:25:27-06:00"

    def get_resources(self):
        resources = []
        resource = extensions.ResourceExtension(
            '{tenant_id}/mgmt/accounts',
            service.AccountController())
        resources.append(resource)

        return resources

########NEW FILE########
__FILENAME__ = mgmt
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from trove.openstack.common import log as logging

from trove.common import extensions
from trove.extensions.mgmt.instances.service import MgmtInstanceController
from trove.extensions.mgmt.host.service import HostController
from trove.extensions.mgmt.quota.service import QuotaController
from trove.extensions.mgmt.host.instance import service as hostservice
from trove.extensions.mgmt.volume.service import StorageController


LOG = logging.getLogger(__name__)


class Mgmt(extensions.ExtensionsDescriptor):

    def get_name(self):
        return "Mgmt"

    def get_description(self):
        return "MGMT services such as details diagnostics"

    def get_alias(self):
        return "Mgmt"

    def get_namespace(self):
        return "http://TBD"

    def get_updated(self):
        return "2011-01-22T13:25:27-06:00"

    def get_resources(self):
        resources = []
        instances = extensions.ResourceExtension(
            '{tenant_id}/mgmt/instances',
            MgmtInstanceController(),
            member_actions={'root': 'GET',
                            'diagnostics': 'GET',
                            'hwinfo': 'GET',
                            'action': 'POST'})
        resources.append(instances)

        hosts = extensions.ResourceExtension(
            '{tenant_id}/mgmt/hosts',
            HostController(),
            member_actions={})
        resources.append(hosts)

        quota = extensions.ResourceExtension(
            '{tenant_id}/mgmt/quotas',
            QuotaController(),
            member_actions={})
        resources.append(quota)

        storage = extensions.ResourceExtension(
            '{tenant_id}/mgmt/storage',
            StorageController(),
            member_actions={})
        resources.append(storage)

        host_instances = extensions.ResourceExtension(
            'instances',
            hostservice.HostInstanceController(),
            parent={'member_name': 'host',
                    'collection_name': '{tenant_id}/mgmt/hosts'},
            collection_actions={'action': 'POST'})
        resources.append(host_instances)

        return resources

########NEW FILE########
__FILENAME__ = mysql
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from trove.openstack.common import log as logging

from trove.common import extensions
from trove.extensions.mysql import service


LOG = logging.getLogger(__name__)


class Mysql(extensions.ExtensionsDescriptor):

    def get_name(self):
        return "Mysql"

    def get_description(self):
        return "Non essential MySQL services such as users and schemas"

    def get_alias(self):
        return "MYSQL"

    def get_namespace(self):
        return "http://TBD"

    def get_updated(self):
        return "2011-01-22T13:25:27-06:00"

    def get_resources(self):
        resources = []

        resource = extensions.ResourceExtension(
            'databases',
            service.SchemaController(),
            parent={'member_name': 'instance',
                    'collection_name': '{tenant_id}/instances'})
        resources.append(resource)

        resource = extensions.ResourceExtension(
            'users',
            service.UserController(),
            parent={'member_name': 'instance',
                    'collection_name': '{tenant_id}/instances'},
            member_actions={'update': 'PUT'},
            collection_actions={'update_all': 'PUT'})
        resources.append(resource)

        collection_url = '{tenant_id}/instances/:instance_id/users'
        resource = extensions.ResourceExtension(
            'databases',
            service.UserAccessController(),
            parent={'member_name': 'user',
                    'collection_name': collection_url},
            collection_actions={'update': 'PUT'})
        resources.append(resource)

        resource = extensions.ResourceExtension(
            'root',
            service.RootController(),
            parent={'member_name': 'instance',
                    'collection_name': '{tenant_id}/instances'})
        resources.append(resource)

        return resources

########NEW FILE########
__FILENAME__ = security_group
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

from trove.openstack.common import log as logging

from trove.common import extensions
from trove.common import cfg
from trove.extensions.security_group import service


LOG = logging.getLogger(__name__)
CONF = cfg.CONF


# The Extensions module from openstack common expects the classname of the
# extension to be loaded to be the exact same as the filename, except with
# a capital first letter. That's the reason this class has such a funky name.
class Security_group(extensions.ExtensionsDescriptor):

    def get_name(self):
        return "SecurityGroup"

    def get_description(self):
        return "Security Group related operations such as list \
security groups and manage security group rules."

    def get_alias(self):
        return "SecurityGroup"

    def get_namespace(self):
        return "http://TBD"

    def get_updated(self):
        return "2012-02-26T17:25:27-08:00"

    def get_resources(self):
        resources = []

        if CONF.trove_security_groups_support:
            security_groups = extensions.ResourceExtension(
                '{tenant_id}/security-groups',
                service.SecurityGroupController())
            resources.append(security_groups)

            security_group_rules = extensions.ResourceExtension(
                '{tenant_id}/security-group-rules',
                service.SecurityGroupRuleController())
            resources.append(security_group_rules)

        return resources

########NEW FILE########
__FILENAME__ = models
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

"""
Model classes for Security Groups and Security Group Rules on instances.
"""
import trove.common.remote
from trove.common import cfg
from trove.common import exception
from trove.db.models import DatabaseModelBase
from trove.common.models import NovaRemoteModelBase
from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _

from novaclient import exceptions as nova_exceptions

CONF = cfg.CONF
LOG = logging.getLogger(__name__)


def persisted_models():
    return {
        'security_group': SecurityGroup,
        'security_group_rule': SecurityGroupRule,
        'security_group_instance_association':
        SecurityGroupInstanceAssociation,
    }


class SecurityGroup(DatabaseModelBase):
    _data_fields = ['id', 'name', 'description', 'user', 'tenant_id',
                    'created', 'updated', 'deleted', 'deleted_at']

    @property
    def instance_id(self):
        return SecurityGroupInstanceAssociation\
            .get_instance_id_by_security_group_id(self.id)

    @classmethod
    def create_sec_group(cls, name, description, context):
        try:
            remote_sec_group = RemoteSecurityGroup.create(name,
                                                          description,
                                                          context)

            if not remote_sec_group:
                raise exception.SecurityGroupCreationError(
                    "Failed to create Security Group")
            else:
                return cls.create(
                    id=remote_sec_group.data()['id'],
                    name=name,
                    description=description,
                    user=context.user,
                    tenant_id=context.tenant)

        except exception.SecurityGroupCreationError as e:
            LOG.exception("Failed to create remote security group")
            raise e

    @classmethod
    def create_for_instance(cls, instance_id, context):
        # Create a new security group
        name = "%s_%s" % (CONF.trove_security_group_name_prefix, instance_id)
        description = _("Security Group for %s") % instance_id
        sec_group = cls.create_sec_group(name, description, context)

        # Currently this locked down by default, since we don't create any
        # default security group rules for the security group.

        # Create security group instance association
        SecurityGroupInstanceAssociation.create(
            security_group_id=sec_group["id"],
            instance_id=instance_id)

        return sec_group

    @classmethod
    def get_security_group_by_id_or_instance_id(self, id, tenant_id):
        try:
            return SecurityGroup.find_by(id=id,
                                         tenant_id=tenant_id,
                                         deleted=False)
        except exception.ModelNotFoundError:
            return SecurityGroupInstanceAssociation.\
                get_security_group_by_instance_id(id)

    def get_rules(self):
        return SecurityGroupRule.find_all(group_id=self.id,
                                          deleted=False)

    def delete(self, context):
        try:
            sec_group_rules = self.get_rules()
            if sec_group_rules:
                for rule in sec_group_rules:
                    rule.delete(context)

            RemoteSecurityGroup.delete(self.id, context)
            super(SecurityGroup, self).delete()

        except exception.TroveError:
            LOG.exception('Failed to delete security group')
            raise exception.TroveError("Failed to delete Security Group")

    @classmethod
    def delete_for_instance(cls, instance_id, context):
        try:
            association = SecurityGroupInstanceAssociation.find_by(
                instance_id=instance_id,
                deleted=False)
            if association:
                sec_group = association.get_security_group()
                if sec_group:
                    sec_group.delete(context)
                association.delete()
        except (exception.ModelNotFoundError,
                exception.TroveError):
            LOG.info(_('Security Group with id: %(id)s '
                       'already had been deleted')
                     % {'id': instance_id})


class SecurityGroupRule(DatabaseModelBase):
    _data_fields = ['id', 'parent_group_id', 'protocol', 'from_port',
                    'to_port', 'cidr', 'group_id', 'created', 'updated',
                    'deleted', 'deleted_at']

    @classmethod
    def create_sec_group_rule(cls, sec_group, protocol, from_port,
                              to_port, cidr, context):
        try:
            remote_rule_id = RemoteSecurityGroup.add_rule(
                sec_group_id=sec_group['id'],
                protocol=protocol,
                from_port=from_port,
                to_port=to_port,
                cidr=cidr,
                context=context)

            if not remote_rule_id:
                raise exception.SecurityGroupRuleCreationError(
                    "Failed to create Security Group Rule")
            else:
                # Create db record
                return cls.create(
                    id=remote_rule_id,
                    protocol=protocol,
                    from_port=from_port,
                    to_port=to_port,
                    cidr=cidr,
                    group_id=sec_group['id'])

        except exception.SecurityGroupRuleCreationError as e:
            LOG.exception("Failed to create remote security group")
            raise e

    def get_security_group(self, tenant_id):
        return SecurityGroup.find_by(id=self.group_id,
                                     tenant_id=tenant_id,
                                     deleted=False)

    def delete(self, context):
        try:
            # Delete Remote Security Group Rule
            RemoteSecurityGroup.delete_rule(self.id, context)
            super(SecurityGroupRule, self).delete()
        except exception.TroveError:
            LOG.exception('Failed to delete security group')
            raise exception.SecurityGroupRuleDeletionError(
                "Failed to delete Security Group")


class SecurityGroupInstanceAssociation(DatabaseModelBase):
    _data_fields = ['id', 'security_group_id', 'instance_id',
                    'created', 'updated', 'deleted', 'deleted_at']

    def get_security_group(self):
        return SecurityGroup.find_by(id=self.security_group_id,
                                     deleted=False)

    @classmethod
    def get_security_group_by_instance_id(cls, id):
        association = SecurityGroupInstanceAssociation.find_by(
            instance_id=id,
            deleted=False)
        return association.get_security_group()

    @classmethod
    def get_instance_id_by_security_group_id(cls, secgroup_id):
        association = SecurityGroupInstanceAssociation.find_by(
            security_group_id=secgroup_id,
            deleted=False)
        return association.instance_id


class RemoteSecurityGroup(NovaRemoteModelBase):

    _data_fields = ['id', 'name', 'description', 'rules']

    def __init__(self, security_group=None, id=None, context=None):
        if id is None and security_group is None:
            msg = "Security Group does not have id defined!"
            raise exception.InvalidModelError(msg)
        elif security_group is None:
            try:
                client = trove.common.remote.create_nova_client(context)
                self._data_object = client.security_groups.get(id)
            except nova_exceptions.NotFound as e:
                raise exception.NotFound(id=id)
            except nova_exceptions.ClientException as e:
                raise exception.TroveError(str(e))
        else:
            self._data_object = security_group

    @classmethod
    def create(cls, name, description, context):
        """Creates a new Security Group"""
        client = trove.common.remote.create_nova_client(context)
        try:
            sec_group = client.security_groups.create(name=name,
                                                      description=description)
        except nova_exceptions.ClientException as e:
            LOG.exception('Failed to create remote security group')
            raise exception.SecurityGroupCreationError(str(e))

        return RemoteSecurityGroup(security_group=sec_group)

    @classmethod
    def delete(cls, sec_group_id, context):
        client = trove.common.remote.create_nova_client(context)

        try:
            client.security_groups.delete(sec_group_id)
        except nova_exceptions.ClientException as e:
            LOG.exception('Failed to delete remote security group')
            raise exception.SecurityGroupDeletionError(str(e))

    @classmethod
    def add_rule(cls, sec_group_id, protocol, from_port,
                 to_port, cidr, context):

        client = trove.common.remote.create_nova_client(context)

        try:
            sec_group_rule = client.security_group_rules.create(
                parent_group_id=sec_group_id,
                ip_protocol=protocol,
                from_port=from_port,
                to_port=to_port,
                cidr=cidr)

            return sec_group_rule.id
        except nova_exceptions.ClientException as e:
            LOG.exception('Failed to add rule to remote security group')
            raise exception.SecurityGroupRuleCreationError(str(e))

    @classmethod
    def delete_rule(cls, sec_group_rule_id, context):
        client = trove.common.remote.create_nova_client(context)

        try:
            client.security_group_rules.delete(sec_group_rule_id)

        except nova_exceptions.ClientException as e:
            LOG.exception('Failed to delete rule to remote security group')
            raise exception.SecurityGroupRuleDeletionError(str(e))

########NEW FILE########
__FILENAME__ = service
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
from trove.common import cfg
from trove.common import exception
from trove.common import wsgi
from trove.common import utils
from trove.datastore.models import DatastoreVersion
from trove.extensions.security_group import models
from trove.extensions.security_group import views
from trove.instance import models as instance_models
from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _

LOG = logging.getLogger(__name__)
CONF = cfg.CONF


class SecurityGroupController(wsgi.Controller):
    """Controller for security groups functionality"""

    def index(self, req, tenant_id):
        """Return all security groups tied to a particular tenant_id."""
        LOG.debug("Index() called with %s" % (tenant_id))

        sec_groups = models.SecurityGroup().find_all(tenant_id=tenant_id,
                                                     deleted=False)

        # Construct the mapping from Security Groups to Security Group Rules
        rules_map = dict([(g.id, g.get_rules()) for g in sec_groups])

        return wsgi.Result(
            views.SecurityGroupsView(sec_groups,
                                     rules_map,
                                     req, tenant_id).list(), 200)

    def show(self, req, tenant_id, id):
        """Return a single security group."""
        LOG.debug("Show() called with %s, %s" % (tenant_id, id))

        sec_group = \
            models.SecurityGroup.get_security_group_by_id_or_instance_id(
                id, tenant_id)

        return wsgi.Result(
            views.SecurityGroupView(sec_group,
                                    sec_group.get_rules(),
                                    req, tenant_id).show(), 200)


class SecurityGroupRuleController(wsgi.Controller):
    """Controller for security group rule functionality"""

    def delete(self, req, tenant_id, id):
        LOG.debug("Delete Security Group Rule called %s, %s" % (tenant_id, id))

        context = req.environ[wsgi.CONTEXT_KEY]
        sec_group_rule = models.SecurityGroupRule.find_by(id=id, deleted=False)
        sec_group = sec_group_rule.get_security_group(tenant_id)

        if sec_group is None:
            LOG.error("Attempting to delete Group Rule that does not exist or "
                      "does not belong to tenant %s" % tenant_id)
            raise exception.Forbidden("Unauthorized")

        sec_group_rule.delete(context)
        return wsgi.Result(None, 204)

    def create(self, req, body, tenant_id):
        LOG.debug("Creating a Security Group Rule for tenant '%s'" % tenant_id)

        context = req.environ[wsgi.CONTEXT_KEY]
        self._validate_create_body(body)

        sec_group_id = body['security_group_rule']['group_id']
        sec_group = models.SecurityGroup.find_by(id=sec_group_id,
                                                 tenant_id=tenant_id,
                                                 deleted=False)
        instance_id = (models.SecurityGroupInstanceAssociation.
                       get_instance_id_by_security_group_id(sec_group_id))
        db_info = instance_models.get_db_info(context, id=instance_id)
        manager = (DatastoreVersion.load_by_uuid(
            db_info.datastore_version_id).manager)
        tcp_ports = CONF.get(manager).tcp_ports
        udp_ports = CONF.get(manager).udp_ports

        def _create_rules(sec_group, ports, protocol):
            rules = []
            try:
                for port_or_range in set(ports):
                    from_, to_ = utils.gen_ports(port_or_range)
                    rule = models.SecurityGroupRule.create_sec_group_rule(
                        sec_group, protocol, int(from_), int(to_),
                        body['security_group_rule']['cidr'], context)
                    rules.append(rule)
            except (ValueError, AttributeError) as e:
                raise exception.BadRequest(msg=str(e))
            return rules

        tcp_rules = _create_rules(sec_group, tcp_ports, 'tcp')
        udp_rules = _create_rules(sec_group, udp_ports, 'udp')

        all_rules = tcp_rules + udp_rules

        view = views.SecurityGroupRulesView(
            all_rules, req, tenant_id).create()
        return wsgi.Result(view, 201)

    def _validate_create_body(self, body):
        try:
            body['security_group_rule']
            body['security_group_rule']['group_id']
            body['security_group_rule']['cidr']
        except KeyError as e:
            LOG.error(_("Create Security Group Rules Required field(s) "
                        "- %s") % e)
            raise exception.SecurityGroupRuleCreationError(
                "Required element/key - %s was not specified" % e)

    schemas = {
        "type": "object",
        "name": "security_group_rule:create",
        "required": True,
        "properties": {
            "security_group_rule": {
                "type": "object",
                "required": True,
                "properties": {
                    "cidr": {
                        "type": "string",
                        "required": True,
                        "minLength": 9,
                        "maxLength": 18
                    },
                    "group_id": {
                        "type": "string",
                        "required": True,
                        "maxLength": 255
                    },
                }
            }
        }
    }

########NEW FILE########
__FILENAME__ = views
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

from trove.openstack.common import log as logging
import os

LOG = logging.getLogger(__name__)


def _base_url(req):
    return req.application_url


class SecurityGroupView(object):

    def __init__(self, secgroup, rules, req, tenant_id):
        self.secgroup = secgroup
        self.rules = rules
        self.request = req
        self.tenant_id = tenant_id

    def _build_links(self):
        """Build the links for the secgroup"""
        base_url = _base_url(self.request)
        href = os.path.join(base_url, self.tenant_id,
                            "security-groups", str(self.secgroup['id']))
        links = [
            {
                'rel': 'self',
                'href': href
            }
        ]
        return links

    def _build_rules(self):
        rules = []

        if self.rules is None:
            return rules

        for rule in self.rules:
            rules.append({'id': str(rule['id']),
                          'protocol': rule['protocol'],
                          'from_port': rule['from_port'],
                          'to_port': rule['to_port'],
                          'cidr': rule['cidr'],
                          })
        return rules

    def data(self):
        return {"id": self.secgroup['id'],
                "name": self.secgroup['name'],
                "description": self.secgroup['description'],
                "instance_id": self.secgroup['instance_id'],
                "rules": self._build_rules(),
                "links": self._build_links(),
                "created": self.secgroup['created'],
                "updated": self.secgroup['updated']
                }

    def show(self):
        return {"security_group": self.data()}

    def create(self):
        return self.show()


class SecurityGroupsView(object):

    def __init__(self, secgroups, rules_dict, req, tenant_id):
        self.secgroups = secgroups
        self.rules = rules_dict
        self.request = req
        self.tenant_id = tenant_id

    def list(self):
        groups_data = []

        for secgroup in self.secgroups:
            rules = (self.rules[secgroup['id']]
                     if self.rules is not None else None)
            groups_data.append(SecurityGroupView(secgroup,
                                                 rules,
                                                 self.request,
                                                 self.tenant_id).data())

        return {"security_groups": groups_data}


class SecurityGroupRulesView(object):

    def __init__(self, rules, req, tenant_id):
        self.rules = rules
        self.request = req
        self.tenant_id = tenant_id

    def _build_create(self):
        views = []
        for rule in self.rules:
            to_append = {
                "id": rule.id,
                "security_group_id": rule.group_id,
                "protocol": rule.protocol,
                "from_port": rule.from_port,
                "to_port": rule.to_port,
                "cidr": rule.cidr,
                "created": rule.created
            }
            views.append(to_append)
        return {"security_group_rule": views}

    def create(self):
        return self._build_create()

########NEW FILE########
__FILENAME__ = models
# Copyright 2010-2012 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Model classes that form the core of instance flavor functionality."""


from novaclient import exceptions as nova_exceptions
from trove.common import exception
from trove.common.models import NovaRemoteModelBase
from trove.common.remote import create_nova_client


class Flavor(object):

    _data_fields = ['id', 'links', 'name', 'ram', 'vcpus', 'ephemeral']

    def __init__(self, flavor=None, context=None, flavor_id=None):
        if flavor:
            self.flavor = flavor
            return
        if flavor_id and context:
            try:
                client = create_nova_client(context)
                self.flavor = client.flavors.get(flavor_id)
            except nova_exceptions.NotFound as e:
                raise exception.NotFound(uuid=flavor_id)
            except nova_exceptions.ClientException as e:
                raise exception.TroveError(str(e))
            return
        msg = ("Flavor is not defined, and"
               " context and flavor_id were not specified.")
        raise exception.InvalidModelError(errors=msg)

    @property
    def id(self):
        return self.flavor.id

    @property
    def name(self):
        return self.flavor.name

    @property
    def ram(self):
        return self.flavor.ram

    @property
    def vcpus(self):
        return self.flavor.vcpus

    @property
    def links(self):
        return self.flavor.links

    @property
    def ephemeral(self):
        return self.flavor.ephemeral


class Flavors(NovaRemoteModelBase):

    def __init__(self, context):
        nova_flavors = create_nova_client(context).flavors.list()
        self.flavors = [Flavor(flavor=item) for item in nova_flavors]

    def __iter__(self):
        for item in self.flavors:
            yield item

########NEW FILE########
__FILENAME__ = service
# Copyright 2010-2012 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


from trove.common import exception
from trove.common import wsgi
from trove.flavor import models
from trove.flavor import views


class FlavorController(wsgi.Controller):
    """Controller for flavor functionality"""

    def show(self, req, tenant_id, id):
        """Return a single flavor."""
        context = req.environ[wsgi.CONTEXT_KEY]
        self._validate_flavor_id(id)
        flavor = models.Flavor(context=context, flavor_id=int(id))
        # Pass in the request to build accurate links.
        return wsgi.Result(views.FlavorView(flavor, req).data(), 200)

    def index(self, req, tenant_id):
        """Return all flavors."""
        context = req.environ[wsgi.CONTEXT_KEY]
        flavors = models.Flavors(context=context)
        return wsgi.Result(views.FlavorsView(flavors, req).data(), 200)

    def _validate_flavor_id(self, id):
        try:
            if int(id) != float(id):
                raise exception.NotFound(uuid=id)
        except ValueError:
            raise exception.NotFound(uuid=id)

########NEW FILE########
__FILENAME__ = views
# Copyright 2010-2012 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


from trove.common.views import create_links
from trove.common import cfg

CONF = cfg.CONF


class FlavorView(object):

    def __init__(self, flavor, req=None):
        self.flavor = flavor
        self.req = req

    def data(self):

        flavor = {
            'id': int(self.flavor.id),
            'links': self._build_links(),
            'name': self.flavor.name,
            'ram': self.flavor.ram,
        }

        if not CONF.trove_volume_support and CONF.device_path is not None:
            flavor['local_storage'] = self.flavor.ephemeral

        return {"flavor": flavor}

    def _build_links(self):
        return create_links("flavors", self.req, self.flavor.id)


class FlavorsView(object):
    view = FlavorView

    def __init__(self, flavors, req=None):
        self.flavors = flavors
        self.req = req

    def data(self):
        data = []
        for flavor in self.flavors:
            data.append(self.view(flavor, req=self.req).data()['flavor'])
        return {"flavors": data}

########NEW FILE########
__FILENAME__ = api
# Copyright (c) 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Handles all request to the Platform or Guest VM
"""

from eventlet import Timeout

from trove.common import cfg
from trove.common import exception
from trove.common import rpc as rd_rpc
from trove.guestagent import models as agent_models
from trove.openstack.common import rpc
from trove.openstack.common import log as logging
from trove.openstack.common.rpc import proxy
from trove.openstack.common.rpc import common

CONF = cfg.CONF
LOG = logging.getLogger(__name__)
AGENT_LOW_TIMEOUT = CONF.agent_call_low_timeout
AGENT_HIGH_TIMEOUT = CONF.agent_call_high_timeout
RPC_API_VERSION = "1.0"


class API(proxy.RpcProxy):
    """API for interacting with the guest manager."""

    def __init__(self, context, id):
        self.context = context
        self.id = id
        super(API, self).__init__(self._get_routing_key(),
                                  RPC_API_VERSION)

    def _call(self, method_name, timeout_sec, **kwargs):
        LOG.debug("Calling %s with timeout %s" % (method_name, timeout_sec))
        try:
            result = self.call(self.context,
                               self.make_msg(method_name, **kwargs),
                               timeout=timeout_sec)

            LOG.debug("Result is %s" % result)
            return result
        except common.RemoteError as r:
            LOG.error(r)
            raise exception.GuestError(original_message=r.value)
        except Exception as e:
            LOG.error(e)
            raise exception.GuestError(original_message=str(e))
        except Timeout as t:
            if t is not timeout:
                raise
            else:
                raise exception.GuestTimeout()

    def _cast(self, method_name, **kwargs):
        LOG.debug("Casting %s" % method_name)
        try:
            self.cast(self.context, self.make_msg(method_name, **kwargs),
                      topic=kwargs.get('topic'),
                      version=kwargs.get('version'))
        except common.RemoteError as r:
            LOG.error(r)
            raise exception.GuestError(original_message=r.value)
        except Exception as e:
            LOG.error(e)
            raise exception.GuestError(original_message=str(e))

    def _cast_with_consumer(self, method_name, **kwargs):
        conn = None
        try:
            conn = rpc.create_connection(new=True)
            conn.create_consumer(self._get_routing_key(), None, fanout=False)
        except common.RemoteError as r:
            LOG.error(r)
            raise exception.GuestError(original_message=r.value)
        except Exception as e:
            LOG.error(e)
            raise exception.GuestError(original_message=str(e))
        finally:
            if conn:
                conn.close()

        # leave the cast call out of the hackity consumer create
        self._cast(method_name, **kwargs)

    def delete_queue(self):
        """Deletes the queue."""
        topic = self._get_routing_key()
        LOG.debug("Deleting queue with name %s." % topic)
        rd_rpc.delete_queue(self.context, topic)

    def _get_routing_key(self):
        """Create the routing key based on the container id"""
        return "guestagent.%s" % self.id

    def _check_for_hearbeat(self):
        """Preemptively raise GuestTimeout if heartbeat is old."""
        try:
            agent = agent_models.AgentHeartBeat.find_by(instance_id=self.id)
            if agent_models.AgentHeartBeat.is_active(agent):
                return True
        except exception.ModelNotFoundError as mnfe:
            LOG.warn(mnfe)
        raise exception.GuestTimeout()

    def change_passwords(self, users):
        """Make an asynchronous call to change the passwords of one or more
           users.
        """
        LOG.debug("Changing passwords for users on Instance %s", self.id)
        self._cast("change_passwords", users=users)

    def update_attributes(self, username, hostname, user_attrs):
        """Update user attributes."""
        LOG.debug("Changing user attributes on Instance %s", self.id)
        self._cast("update_attributes", username=username, hostname=hostname,
                   user_attrs=user_attrs)

    def create_user(self, users):
        """Make an asynchronous call to create a new database user"""
        LOG.debug("Creating Users for Instance %s", self.id)
        self._cast("create_user", users=users)

    def get_user(self, username, hostname):
        """Make an asynchronous call to get a single database user."""
        LOG.debug("Getting a user on Instance %s", self.id)
        LOG.debug("User name is %s" % username)
        return self._call("get_user", AGENT_LOW_TIMEOUT,
                          username=username, hostname=hostname)

    def list_access(self, username, hostname):
        """Show all the databases to which a user has more than USAGE."""
        LOG.debug("Showing user grants on Instance %s", self.id)
        LOG.debug("User name is %s" % username)
        return self._call("list_access", AGENT_LOW_TIMEOUT,
                          username=username, hostname=hostname)

    def grant_access(self, username, hostname, databases):
        """Grant a user permission to use a given database."""
        return self._call("grant_access", AGENT_LOW_TIMEOUT,
                          username=username, hostname=hostname,
                          databases=databases)

    def revoke_access(self, username, hostname, database):
        """Remove a user's permission to use a given database."""
        return self._call("revoke_access", AGENT_LOW_TIMEOUT,
                          username=username, hostname=hostname,
                          database=database)

    def list_users(self, limit=None, marker=None, include_marker=False):
        """Make an asynchronous call to list database users"""
        LOG.debug("Listing Users for Instance %s", self.id)
        return self._call("list_users", AGENT_LOW_TIMEOUT, limit=limit,
                          marker=marker, include_marker=include_marker)

    def delete_user(self, user):
        """Make an asynchronous call to delete an existing database user"""
        LOG.debug("Deleting user %(user)s for Instance %(instance_id)s" %
                  {'user': user, 'instance_id': self.id})
        self._cast("delete_user", user=user)

    def create_database(self, databases):
        """Make an asynchronous call to create a new database
           within the specified container
        """
        LOG.debug("Creating databases for Instance %s", self.id)
        self._cast("create_database", databases=databases)

    def list_databases(self, limit=None, marker=None, include_marker=False):
        """Make an asynchronous call to list databases"""
        LOG.debug("Listing databases for Instance %s", self.id)
        return self._call("list_databases", AGENT_LOW_TIMEOUT, limit=limit,
                          marker=marker, include_marker=include_marker)

    def delete_database(self, database):
        """Make an asynchronous call to delete an existing database
           within the specified container
        """
        LOG.debug("Deleting database %(database)s for "
                  "Instance %(instance_id)s" % {'database': database,
                                                'instance_id': self.id})
        self._cast("delete_database", database=database)

    def enable_root(self):
        """Make a synchronous call to enable the root user for
           access from anywhere
        """
        LOG.debug("Enable root user for Instance %s", self.id)
        return self._call("enable_root", AGENT_HIGH_TIMEOUT)

    def disable_root(self):
        """Make a synchronous call to disable the root user for
           access from anywhere
        """
        LOG.debug("Disable root user for Instance %s", self.id)
        return self._call("disable_root", AGENT_LOW_TIMEOUT)

    def is_root_enabled(self):
        """Make a synchronous call to check if root access is
           available for the container
        """
        LOG.debug("Check root access for Instance %s", self.id)
        return self._call("is_root_enabled", AGENT_LOW_TIMEOUT)

    def get_hwinfo(self):
        """Make a synchronous call to get hardware info for the container"""
        LOG.debug("Check hwinfo on Instance %s", self.id)
        return self._call("get_hwinfo", AGENT_LOW_TIMEOUT)

    def get_diagnostics(self):
        """Make a synchronous call to get diagnostics for the container"""
        LOG.debug("Check diagnostics on Instance %s", self.id)
        return self._call("get_diagnostics", AGENT_LOW_TIMEOUT)

    def prepare(self, memory_mb, packages, databases, users,
                device_path='/dev/vdb', mount_point='/mnt/volume',
                backup_info=None, config_contents=None, root_password=None,
                overrides=None):
        """Make an asynchronous call to prepare the guest
           as a database container optionally includes a backup id for restores
        """
        LOG.debug("Sending the call to prepare the Guest")
        self._cast_with_consumer(
            "prepare", packages=packages, databases=databases,
            memory_mb=memory_mb, users=users, device_path=device_path,
            mount_point=mount_point, backup_info=backup_info,
            config_contents=config_contents, root_password=root_password,
            overrides=overrides)

    def restart(self):
        """Restart the MySQL server."""
        LOG.debug("Sending the call to restart MySQL on the Guest.")
        self._call("restart", AGENT_HIGH_TIMEOUT)

    def start_db_with_conf_changes(self, config_contents):
        """Start the MySQL server."""
        LOG.debug("Sending the call to start MySQL on the Guest with "
                  "a timeout of %s." % AGENT_HIGH_TIMEOUT)
        self._call("start_db_with_conf_changes", AGENT_HIGH_TIMEOUT,
                   config_contents=config_contents)

    def reset_configuration(self, configuration):
        """Ignore running state of MySQL, and just change the config file
           to a new flavor.
        """
        LOG.debug("Sending the call to change MySQL conf file on the Guest "
                  "with a timeout of %s." % AGENT_HIGH_TIMEOUT)
        self._call("reset_configuration", AGENT_HIGH_TIMEOUT,
                   configuration=configuration)

    def stop_db(self, do_not_start_on_reboot=False):
        """Stop the MySQL server."""
        LOG.debug("Sending the call to stop MySQL on the Guest.")
        self._call("stop_db", AGENT_HIGH_TIMEOUT,
                   do_not_start_on_reboot=do_not_start_on_reboot)

    def upgrade(self):
        """Make an asynchronous call to self upgrade the guest agent"""
        LOG.debug("Sending an upgrade call to nova-guest")
        self._cast_with_consumer("upgrade")

    def get_volume_info(self):
        """Make a synchronous call to get volume info for the container"""
        LOG.debug("Check Volume Info on Instance %s", self.id)
        # self._check_for_hearbeat()
        return self._call("get_filesystem_stats", AGENT_LOW_TIMEOUT,
                          fs_path=None)

    def update_guest(self):
        """Make a synchronous call to update the guest agent."""
        self._call("update_guest", AGENT_HIGH_TIMEOUT)

    def create_backup(self, backup_info):
        """Make async call to create a full backup of this instance"""
        LOG.debug("Create Backup %(backup_id)s "
                  "for Instance %(instance_id)s" %
                  {'backup_id': backup_info['id'], 'instance_id': self.id})
        self._cast("create_backup", backup_info=backup_info)

    def mount_volume(self, device_path=None, mount_point=None):
        """Mount the volume"""
        LOG.debug("Mount volume %(mount)s on instance %(id)s" % {
            'mount': mount_point, 'id': self.id})
        self._call("mount_volume", AGENT_LOW_TIMEOUT,
                   device_path=device_path, mount_point=mount_point)

    def unmount_volume(self, device_path=None, mount_point=None):
        """Unmount the volume"""
        LOG.debug("Unmount volume %(device)s on instance %(id)s" % {
            'device': device_path, 'id': self.id})
        self._call("unmount_volume", AGENT_LOW_TIMEOUT,
                   device_path=device_path, mount_point=mount_point)

    def resize_fs(self, device_path=None, mount_point=None):
        """Resize the filesystem"""
        LOG.debug("Resize device %(device)s on instance %(id)s" % {
            'device': device_path, 'id': self.id})
        self._call("resize_fs", AGENT_HIGH_TIMEOUT, device_path=device_path,
                   mount_point=mount_point)

    def update_overrides(self, overrides, remove=False):
        LOG.debug("Updating overrides on Instance %s", self.id)
        LOG.debug("Updating overrides values %s" % overrides)
        self._cast("update_overrides", overrides=overrides, remove=remove)

    def apply_overrides(self, overrides):
        LOG.debug("Applying overrides on Instance %s", self.id)
        LOG.debug("Applying overrides values %s" % overrides)
        self._cast("apply_overrides", overrides=overrides)

########NEW FILE########
__FILENAME__ = backupagent
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

import logging
from trove.backup.models import BackupState
from trove.common import cfg
from trove.common import context as trove_context
from trove.conductor import api as conductor_api
from trove.guestagent.common import timeutils
from trove.guestagent.dbaas import get_filesystem_volume_stats
from trove.guestagent.strategies.backup.base import BackupError
from trove.guestagent.strategies.backup.base import UnknownBackupType
from trove.guestagent.strategies.storage import get_storage_strategy
from trove.guestagent.strategies.backup import get_backup_strategy
from trove.guestagent.strategies.restore import get_restore_strategy
from trove.openstack.common.gettextutils import _  # noqa

LOG = logging.getLogger(__name__)
CONF = cfg.CONF

MANAGER = CONF.datastore_manager
# If datastore manager is not mentioned in guest
# configuration file, would be used mysql as datastore_manager by the default
STRATEGY = CONF.get('mysql' if not MANAGER else MANAGER).backup_strategy
NAMESPACE = CONF.backup_namespace

RUNNER = get_backup_strategy(STRATEGY, NAMESPACE)
EXTRA_OPTS = CONF.backup_runner_options.get(STRATEGY, '')

# Try to get the incremental strategy or return the default 'backup_strategy'
INCREMENTAL = CONF.backup_incremental_strategy.get(STRATEGY,
                                                   STRATEGY)

INCREMENTAL_RUNNER = get_backup_strategy(INCREMENTAL, NAMESPACE)


class BackupAgent(object):

    def _get_restore_runner(self, backup_type):
        """Returns the RestoreRunner associated with this backup type."""
        try:
            runner = get_restore_strategy(backup_type, CONF.restore_namespace)
        except ImportError:
            raise UnknownBackupType("Unknown Backup type: %s" % backup_type)
        return runner

    def execute_backup(self, context, backup_info,
                       runner=RUNNER, extra_opts=EXTRA_OPTS):
        backup_id = backup_info['id']
        ctxt = trove_context.TroveContext(
            user=CONF.nova_proxy_admin_user,
            auth_token=CONF.nova_proxy_admin_pass)
        conductor = conductor_api.API(ctxt)

        LOG.info(_("Running backup %(id)s") % backup_info)
        storage = get_storage_strategy(
            CONF.storage_strategy,
            CONF.storage_namespace)(context)

        # Check if this is an incremental backup and grab the parent metadata
        parent_metadata = {}
        if backup_info.get('parent'):
            runner = INCREMENTAL_RUNNER
            LOG.info(_("Using incremental runner: %s") % runner.__name__)
            parent = backup_info['parent']
            parent_metadata = storage.load_metadata(parent['location'],
                                                    parent['checksum'])
            # The parent could be another incremental backup so we need to
            # reset the location and checksum to *this* parents info
            parent_metadata.update({
                'parent_location': parent['location'],
                'parent_checksum': parent['checksum']
            })

        # Store the size of the filesystem before the backup.
        mount_point = CONF.get('mysql' if not CONF.datastore_manager
                               else CONF.datastore_manager).mount_point
        stats = get_filesystem_volume_stats(mount_point)
        backup = {
            'backup_id': backup_id,
            'size': stats.get('used', 0.0),
            'state': BackupState.BUILDING,
        }
        conductor.update_backup(CONF.guest_id,
                                sent=timeutils.float_utcnow(),
                                **backup)

        try:
            with runner(filename=backup_id, extra_opts=extra_opts,
                        **parent_metadata) as bkup:
                try:
                    LOG.info(_("Starting Backup %s"), backup_id)
                    success, note, checksum, location = storage.save(
                        bkup.manifest,
                        bkup)

                    backup.update({
                        'checksum': checksum,
                        'location': location,
                        'note': note,
                        'success': success,
                        'backup_type': bkup.backup_type,
                    })

                    LOG.info(_("Backup %(backup_id)s completed status: "
                               "%(success)s") % backup)
                    LOG.info(_("Backup %(backup_id)s file swift checksum: "
                               "%(checksum)s") % backup)
                    LOG.info(_("Backup %(backup_id)s location: "
                               "%(location)s") % backup)

                    if not success:
                        raise BackupError(note)

                    storage.save_metadata(location, bkup.metadata())

                except Exception:
                    LOG.exception(_("Error saving %(backup_id)s Backup") %
                                  backup)
                    backup.update({'state': BackupState.FAILED})
                    conductor.update_backup(CONF.guest_id,
                                            sent=timeutils.float_utcnow(),
                                            **backup)
                    raise

        except Exception:
            LOG.exception(_("Error running backup: %(backup_id)s") % backup)
            backup.update({'state': BackupState.FAILED})
            conductor.update_backup(CONF.guest_id,
                                    sent=timeutils.float_utcnow(),
                                    **backup)
            raise
        else:
            LOG.info(_("Saving %(backup_id)s Backup Info to model") % backup)
            backup.update({'state': BackupState.COMPLETED})
            conductor.update_backup(CONF.guest_id,
                                    sent=timeutils.float_utcnow(),
                                    **backup)

    def execute_restore(self, context, backup_info, restore_location):

        try:
            LOG.debug("Getting Restore Runner %(type)s", backup_info)
            restore_runner = self._get_restore_runner(backup_info['type'])

            LOG.debug("Getting Storage Strategy")
            storage = get_storage_strategy(
                CONF.storage_strategy,
                CONF.storage_namespace)(context)

            runner = restore_runner(storage, location=backup_info['location'],
                                    checksum=backup_info['checksum'],
                                    restore_location=restore_location)
            backup_info['restore_location'] = restore_location
            LOG.debug("Restoring instance from backup %(id)s to "
                      "%(restore_location)s" % backup_info)
            content_size = runner.restore()
            LOG.info(_("Restore from backup %(id)s completed successfully "
                       "to %(restore_location)s") % backup_info)
            LOG.info(_("Restore size: %s") % content_size)

        except Exception as e:
            LOG.error(e)
            LOG.error(_("Error restoring backup %(id)s") % backup_info)
            raise

        else:
            LOG.info(_("Restored Backup %(id)s") % backup_info)

########NEW FILE########
__FILENAME__ = operating_system
# Copyright (c) 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import os
import fcntl
import struct
import socket
from trove.common import utils

REDHAT = 'redhat'
DEBIAN = 'debian'


def get_os():
    if os.path.isfile("/etc/redhat-release"):
        return REDHAT
    else:
        return DEBIAN


def service_discovery(service_candidates):
    """
    This function discovering how to start, stop, enable, disable service
    in current environment. "service_candidates" is array with possible
    system service names. Works for upstart, systemd, sysvinit.
    """
    result = {}
    for service in service_candidates:
        # check upstart
        if os.path.isfile("/etc/init/%s.conf" % service):
            # upstart returns error code when service already started/stopped
            result['cmd_start'] = "sudo start %s || true" % service
            result['cmd_stop'] = "sudo stop %s || true" % service
            result['cmd_enable'] = ("sudo sed -i '/^manual$/d' "
                                    "/etc/init/%s.conf" % service)
            result['cmd_disable'] = ("sudo sh -c 'echo manual >> "
                                     "/etc/init/%s.conf'" % service)
            break
        # check sysvinit
        if os.path.isfile("/etc/init.d/%s" % service):
            result['cmd_start'] = "sudo service %s start" % service
            result['cmd_stop'] = "sudo service %s stop" % service
            if os.path.isfile("/usr/sbin/update-rc.d"):
                result['cmd_enable'] = "sudo update-rc.d %s defaults; sudo " \
                                       "update-rc.d %s enable" % (service,
                                                                  service)
                result['cmd_disable'] = "sudo update-rc.d %s defaults; sudo " \
                                        "update-rc.d %s disable" % (service,
                                                                    service)
            elif os.path.isfile("/sbin/chkconfig"):
                result['cmd_enable'] = "sudo chkconfig %s on" % service
                result['cmd_disable'] = "sudo chkconfig %s off" % service
            break
        # check systemd
        if os.path.isfile("/lib/systemd/system/%s.service" % service):
            result['cmd_start'] = "sudo systemctl start %s" % service
            result['cmd_stop'] = "sudo systemctl stop %s" % service
            result['cmd_enable'] = "sudo systemctl enable %s" % service
            result['cmd_disable'] = "sudo systemctl disable %s" % service
            break
    return result


#Uses the Linux SIOCGIFADDR ioctl to find the IP address associated
# with a network interface, given the name of that interface,
# e.g. "eth0". The address is returned as a string containing a dotted quad.
def get_ip_address(ifname='eth0'):
    """
    Retrieves IP address which assigned to given network interface

    @parameter ifname network interface (ethX, wlanX, etc.)
    """
    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    return socket.inet_ntoa(fcntl.ioctl(
        s.fileno(),
        0x8915,  # SIOCGIFADDR
        struct.pack('256s', ifname[:15])
    )[20:24])


def update_owner(user, group, path):
    """
       Changes the owner and group for the path (recursively)
    """
    utils.execute_with_timeout("chown", "-R", "%s:%s" % (user, group), path,
                               run_as_root=True, root_helper="sudo")

########NEW FILE########
__FILENAME__ = sql_query
# Copyright (c) 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""

Intermediary class for building SQL queries for use by the guest agent.
Do not hard-code strings into the guest agent; use this module to build
them for you.

"""


class Query(object):

    def __init__(self, columns=None, tables=None, where=None, order=None,
                 group=None, limit=None):
        self.columns = columns or []
        self.tables = tables or []
        self.where = where or []
        self.order = order or []
        self.group = group or []
        self.limit = limit

    def __repr__(self):
        return str(self)

    @property
    def _columns(self):
        if not self.columns:
            return "SELECT *"
        return "SELECT %s" % (", ".join(self.columns))

    @property
    def _tables(self):
        return "FROM %s" % (", ".join(self.tables))

    @property
    def _where(self):
        if not self.where:
            return ""
        return "WHERE %s" % (" AND ".join(self.where))

    @property
    def _order(self):
        if not self.order:
            return ""
        return "ORDER BY %s" % (", ".join(self.order))

    @property
    def _group_by(self):
        if not self.group:
            return ""
        return "GROUP BY %s" % (", ".join(self.group))

    @property
    def _limit(self):
        if not self.limit:
            return ""
        return "LIMIT %s" % str(self.limit)

    def __str__(self):
        query = [
            self._columns,
            self._tables,
            self._where,
            self._order,
            self._group_by,
            self._limit,
        ]
        query = [q for q in query if q]
        return " ".join(query) + ";"


class Grant(object):

    PERMISSIONS = ["ALL",
                   "ALL PRIVILEGES",
                   "ALTER ROUTINE",
                   "ALTER",
                   "CREATE ROUTINE",
                   "CREATE TEMPORARY TABLES",
                   "CREATE USER",
                   "CREATE VIEW",
                   "CREATE",
                   "DELETE",
                   "DROP",
                   "EVENT",
                   "EXECUTE",
                   "FILE",
                   "INDEX",
                   "INSERT",
                   "LOCK TABLES",
                   "PROCESS",
                   "REFERENCES",
                   "RELOAD",
                   "REPLICATION CLIENT",
                   "REPLICATION SLAVE",
                   "SELECT",
                   "SHOW DATABASES",
                   "SHOW VIEW",
                   "SHUTDOWN",
                   "SUPER",
                   "TRIGGER",
                   "UPDATE",
                   "USAGE",
                   ]

    def __init__(self, permissions=None, database=None, table=None, user=None,
                 host=None, clear=None, hashed=None, grant_option=False):
        self.permissions = permissions or []
        self.database = database
        self.table = table
        self.user = user
        self.host = host
        self.clear = clear
        self.hashed = hashed
        self.grant_option = grant_option

    def __repr__(self):
        return str(self)

    @property
    def _permissions(self):
        if not self.permissions:
            return "USAGE"
        if "ALL" in self.permissions:
            return "ALL PRIVILEGES"
        if "ALL PRIVILEGES" in self.permissions:
            return "ALL PRIVILEGES"
        filtered = [perm for perm in set(self.permissions)
                    if perm in self.PERMISSIONS]
        return ", ".join(sorted(filtered))

    @property
    def _database(self):
        if not self.database:
            return "*"
        return "`%s`" % self.database

    @property
    def _table(self):
        if self.table:
            return "'%s'" % self.table
        return "*"

    @property
    def _user(self):
        return self.user or ""

    @property
    def _identity(self):
        if self.clear:
            return "IDENTIFIED BY '%s'" % self.clear
        if self.hashed:
            return "IDENTIFIED BY PASSWORD '%s'" % self.hashed
        return ""

    @property
    def _host(self):
        return self.host or "%"

    @property
    def _user_host(self):
        return "`%s`@`%s`" % (self._user, self._host)

    @property
    def _what(self):
        # Permissions to be granted to the user.
        return "GRANT %s" % self._permissions

    @property
    def _where(self):
        # Database and table to which the user is granted permissions.
        return "ON %s.%s" % (self._database, self._table)

    @property
    def _whom(self):
        # User and host to be granted permission. Optionally, password, too.
        whom = [("TO %s" % self._user_host),
                self._identity,
                ]
        whom = [w for w in whom if w]
        return " ".join(whom)

    @property
    def _with(self):
        clauses = []

        if self.grant_option:
            clauses.append("GRANT OPTION")

        if not clauses:
            return ""

        return "WITH %s" % ", ".join(clauses)

    def __str__(self):
        query = [self._what,
                 self._where,
                 self._whom,
                 self._with,
                 ]
        query = [q for q in query if q]
        return " ".join(query) + ";"


class Revoke(Grant):

    def __init__(self, permissions=None, database=None, table=None, user=None,
                 host=None, clear=None, hashed=None):
        self.permissions = permissions or []
        self.database = database
        self.table = table
        self.user = user
        self.host = host
        self.clear = clear
        self.hashed = hashed

    def __str__(self):
        query = [self._what,
                 self._where,
                 self._whom,
                 ]
        query = [q for q in query if q]
        return " ".join(query) + ";"

    @property
    def _permissions(self):
        if not self.permissions:
            return "ALL"
        if "ALL" in self.permissions:
            return "ALL"
        if "ALL PRIVILEGES" in self.permissions:
            return "ALL"
        filtered = [perm for perm in self.permissions
                    if perm in self.PERMISSIONS]
        return ", ".join(sorted(filtered))

    @property
    def _what(self):
        # Permissions to be revoked from the user.
        return "REVOKE %s" % self._permissions

    @property
    def _whom(self):
        # User and host from whom to revoke permission.
        # Optionally, password, too.
        whom = [("FROM %s" % self._user_host),
                self._identity,
                ]
        whom = [w for w in whom if w]
        return " ".join(whom)


class CreateDatabase(object):

    def __init__(self, database, charset=None, collate=None):
        self.database = database
        self.charset = charset
        self.collate = collate

    def __repr__(self):
        return str(self)

    @property
    def _charset(self):
        if not self.charset:
            return ""
        return "CHARACTER SET = '%s'" % self.charset

    @property
    def _collate(self):
        if not self.collate:
            return ""
        return "COLLATE = '%s'" % self.collate

    def __str__(self):
        query = [("CREATE DATABASE IF NOT EXISTS `%s`" % self.database),
                 self._charset,
                 self._collate,
                 ]
        query = [q for q in query if q]
        return " ".join(query) + ";"


class DropDatabase(object):

    def __init__(self, database):
        self.database = database

    def __repr__(self):
        return str(self)

    def __str__(self):
        return "DROP DATABASE `%s`;" % self.database


class CreateUser(object):

    def __init__(self, user, host=None, clear=None, hashed=None):
        self.user = user
        self.host = host
        self.clear = clear  # A clear password
        self.hashed = hashed  # A hashed password

    def __repr__(self):
        return str(self)

    @property
    def keyArgs(self):
        return {'user': self.user,
                'host': self._host,
                }

    @property
    def _host(self):
        if not self.host:
            return "%"
        return self.host

    @property
    def _identity(self):
        if self.clear:
            return "IDENTIFIED BY '%s'" % self.clear
        if self.hashed:
            return "IDENTIFIED BY PASSWORD '%s'" % self.hashed
        return ""

    def __str__(self):
        query = ["CREATE USER :user@:host",
                 self._identity,
                 ]
        query = [q for q in query if q]
        return " ".join(query) + ";"


class UpdateUser(object):

    def __init__(self, user, host=None, clear=None, new_user=None,
                 new_host=None):
        self.user = user
        self.host = host
        self.clear = clear
        self.new_user = new_user
        self.new_host = new_host

    def __repr__(self):
        return str(self)

    @property
    def _set_password(self):
        if self.clear:
            return "Password=PASSWORD('%s')" % self.clear

    @property
    def _set_user(self):
        if self.new_user:
            return "User='%s'" % self.new_user

    @property
    def _set_host(self):
        if self.new_host:
            return "Host='%s'" % self.new_host

    @property
    def _host(self):
        if not self.host:
            return "%"
        return self.host

    @property
    def _set_attrs(self):
        sets = [self._set_user,
                self._set_host,
                self._set_password,
                ]
        sets = [s for s in sets if s]
        sets = ', '.join(sets)
        return 'SET %s' % sets

    @property
    def _where(self):
        clauses = []
        if self.user:
            clauses.append("User = '%s'" % self.user)
        if self.host:
            clauses.append("Host = '%s'" % self._host)
        if not clauses:
            return ""
        return "WHERE %s" % " AND ".join(clauses)

    def __str__(self):
        query = ["UPDATE mysql.user",
                 self._set_attrs,
                 self._where,
                 ]
        query = [q for q in query if q]
        return " ".join(query) + ";"


class DropUser(object):

    def __init__(self, user, host='%'):
        self.user = user
        self.host = host

    def __repr__(self):
        return str(self)

    def __str__(self):
        return "DROP USER `%s`@`%s`;" % (self.user, self.host)


class SetServerVariable(object):

    def __init__(self, key, value):
        self.key = key
        self.value = value

    def __repr__(self):
        return str(self)

    def __str__(self):
        if self.value is True:
            return "SET GLOBAL %s=%s" % (self.key, 1)
        elif self.value is False:
            return "SET GLOBAL %s=%s" % (self.key, 0)
        elif self.value is None:
            return "SET GLOBAL %s" % (self.key)
        else:
            return "SET GLOBAL %s=%s" % (self.key, self.value)

### Miscellaneous queries that need no parameters.

FLUSH = "FLUSH PRIVILEGES;"
ROOT_ENABLED = ("SELECT User FROM mysql.user "
                "WHERE User = 'root' AND Host != 'localhost';")
REMOVE_ANON = "DELETE FROM mysql.user WHERE User = '';"
REMOVE_ROOT = ("DELETE FROM mysql.user "
               "WHERE User = 'root' AND Host != 'localhost';")

########NEW FILE########
__FILENAME__ = timeutils
from trove.openstack.common import timeutils
from datetime import datetime


def float_utcnow():
    return float(datetime.strftime(timeutils.utcnow(), "%s.%f"))

########NEW FILE########
__FILENAME__ = manager
#  Copyright 2013 Mirantis Inc.
#  All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

import os
from trove.common import cfg
from trove.common import exception
from trove.guestagent import volume
from trove.guestagent.datastore.cassandra import service
from trove.openstack.common import periodic_task
from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _
from trove.guestagent import dbaas

CONF = cfg.CONF
LOG = logging.getLogger(__name__)
MANAGER = CONF.datastore_manager


class Manager(periodic_task.PeriodicTasks):

    def __init__(self):
        self.appStatus = service.CassandraAppStatus()
        self.app = service.CassandraApp(self.appStatus)

    @periodic_task.periodic_task(ticks_between_runs=3)
    def update_status(self, context):
        """Update the status of the Cassandra service"""
        self.appStatus.update()

    def restart(self, context):
        self.app.restart()

    def get_filesystem_stats(self, context, fs_path):
        """Gets the filesystem stats for the path given. """
        mount_point = CONF.get(
            'mysql' if not MANAGER else MANAGER).mount_point
        return dbaas.get_filesystem_volume_stats(mount_point)

    def start_db_with_conf_changes(self, context, config_contents):
        self.app.start_db_with_conf_changes(config_contents)

    def stop_db(self, context, do_not_start_on_reboot=False):
        self.app.stop_db(do_not_start_on_reboot=do_not_start_on_reboot)

    def reset_configuration(self, context, configuration):
        self.app.reset_configuration(configuration)

    def prepare(self, context, packages, databases, memory_mb, users,
                device_path=None, mount_point=None, backup_info=None,
                config_contents=None, root_password=None, overrides=None):
        LOG.info(_("Setting status BUILDING"))
        self.appStatus.begin_install()
        LOG.info("Installing cassandra")
        self.app.install_if_needed(packages)
        self.app.init_storage_structure(mount_point)
        if config_contents:
            LOG.info(_("Config processing"))
            self.app.write_config(config_contents)
            self.app.make_host_reachable()
        if device_path:
            device = volume.VolumeDevice(device_path)
            # unmount if device is already mounted
            device.unmount_device(device_path)
            device.format()
            if os.path.exists(mount_point):
                #rsync exiting data
                device.migrate_data(mount_point)
            #mount the volume
            device.mount(mount_point)
            LOG.debug("Mounting new volume.")
            self.app.restart()

        self.appStatus.end_install_or_restart()
        LOG.info(_('"prepare" call has finished.'))

    def change_passwords(self, context, users):
        raise exception.DatastoreOperationNotSupported(
            operation='change_passwords', datastore=MANAGER)

    def update_attributes(self, context, username, hostname, user_attrs):
        raise exception.DatastoreOperationNotSupported(
            operation='update_attributes', datastore=MANAGER)

    def create_database(self, context, databases):
        raise exception.DatastoreOperationNotSupported(
            operation='create_database', datastore=MANAGER)

    def create_user(self, context, users):
        raise exception.DatastoreOperationNotSupported(
            operation='create_user', datastore=MANAGER)

    def delete_database(self, context, database):
        raise exception.DatastoreOperationNotSupported(
            operation='delete_database', datastore=MANAGER)

    def delete_user(self, context, user):
        raise exception.DatastoreOperationNotSupported(
            operation='delete_user', datastore=MANAGER)

    def get_user(self, context, username, hostname):
        raise exception.DatastoreOperationNotSupported(
            operation='get_user', datastore=MANAGER)

    def grant_access(self, context, username, hostname, databases):
        raise exception.DatastoreOperationNotSupported(
            operation='grant_access', datastore=MANAGER)

    def revoke_access(self, context, username, hostname, database):
        raise exception.DatastoreOperationNotSupported(
            operation='revoke_access', datastore=MANAGER)

    def list_access(self, context, username, hostname):
        raise exception.DatastoreOperationNotSupported(
            operation='list_access', datastore=MANAGER)

    def list_databases(self, context, limit=None, marker=None,
                       include_marker=False):
        raise exception.DatastoreOperationNotSupported(
            operation='list_databases', datastore=MANAGER)

    def list_users(self, context, limit=None, marker=None,
                   include_marker=False):
        raise exception.DatastoreOperationNotSupported(
            operation='list_users', datastore=MANAGER)

    def enable_root(self, context):
        raise exception.DatastoreOperationNotSupported(
            operation='enable_root', datastore=MANAGER)

    def is_root_enabled(self, context):
        raise exception.DatastoreOperationNotSupported(
            operation='is_root_enabled', datastore=MANAGER)

    def _perform_restore(self, backup_info, context, restore_location, app):
        raise exception.DatastoreOperationNotSupported(
            operation='_perform_restore', datastore=MANAGER)

    def create_backup(self, context, backup_info):
        raise exception.DatastoreOperationNotSupported(
            operation='create_backup', datastore=MANAGER)

    def mount_volume(self, context, device_path=None, mount_point=None):
        device = volume.VolumeDevice(device_path)
        device.mount(mount_point, write_to_fstab=False)
        LOG.debug("Mounted the volume.")

    def unmount_volume(self, context, device_path=None, mount_point=None):
        device = volume.VolumeDevice(device_path)
        device.unmount(mount_point)
        LOG.debug("Unmounted the volume.")

    def resize_fs(self, context, device_path=None, mount_point=None):
        device = volume.VolumeDevice(device_path)
        device.resize_fs(mount_point)
        LOG.debug("Resized the filesystem")

    def update_overrides(self, context, overrides, remove=False):
        raise exception.DatastoreOperationNotSupported(
            operation='update_overrides', datastore=MANAGER)

    def apply_overrides(self, context, overrides):
        raise exception.DatastoreOperationNotSupported(
            operation='apply_overrides', datastore=MANAGER)

########NEW FILE########
__FILENAME__ = service
#  Copyright 2013 Mirantis Inc.
#  All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import yaml
from trove.common import cfg
from trove.common import utils
from trove.common import exception
from trove.common import instance as rd_instance
from trove.guestagent.common import operating_system
from trove.guestagent.datastore.cassandra import system
from trove.guestagent.datastore import service
from trove.guestagent import pkg
from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _


LOG = logging.getLogger(__name__)
CONF = cfg.CONF

packager = pkg.Package()


class CassandraApp(object):
    """Prepares DBaaS on a Guest container."""

    def __init__(self, status):
        """By default login with root no password for initial setup. """
        self.state_change_wait_time = CONF.state_change_wait_time
        self.status = status

    def install_if_needed(self, packages):
        """Prepare the guest machine with a cassandra server installation"""
        LOG.info(_("Preparing Guest as Cassandra Server"))
        if not packager.pkg_is_installed(packages):
            self._install_db(packages)
        LOG.info(_("Dbaas install_if_needed complete"))

    def complete_install_or_restart(self):
        self.status.end_install_or_restart()

    def _enable_db_on_boot(self):
        utils.execute_with_timeout(system.ENABLE_CASSANDRA_ON_BOOT,
                                   shell=True)

    def _disable_db_on_boot(self):
        utils.execute_with_timeout(system.DISABLE_CASSANDRA_ON_BOOT,
                                   shell=True)

    def init_storage_structure(self, mount_point):
        try:
            cmd = system.INIT_FS % mount_point
            utils.execute_with_timeout(cmd, shell=True)
        except exception.ProcessExecutionError as e:
            LOG.error(_("Error while initiating storage structure."))
            LOG.error(e)

    def start_db(self, update_db=False):
        self._enable_db_on_boot()
        try:
            utils.execute_with_timeout(system.START_CASSANDRA,
                                       shell=True)
        except exception.ProcessExecutionError:
            pass

        if not (self.status.
                wait_for_real_status_to_change_to(
                rd_instance.ServiceStatuses.RUNNING,
                self.state_change_wait_time,
                update_db)):
            try:
                utils.execute_with_timeout(system.CASSANDRA_KILL,
                                           shell=True)
            except exception.ProcessExecutionError as p:
                LOG.error(_("Error killing stalled Cassandra start command."))
                LOG.error(p)
            self.status.end_install_or_restart()
            raise RuntimeError(_("Could not start Cassandra"))

    def stop_db(self, update_db=False, do_not_start_on_reboot=False):
        if do_not_start_on_reboot:
            self._disable_db_on_boot()
        utils.execute_with_timeout(system.STOP_CASSANDRA,
                                   shell=True)

        if not (self.status.wait_for_real_status_to_change_to(
                rd_instance.ServiceStatuses.SHUTDOWN,
                self.state_change_wait_time, update_db)):
            LOG.error(_("Could not stop Cassandra"))
            self.status.end_install_or_restart()
            raise RuntimeError(_("Could not stop Cassandra"))

    def restart(self):
        try:
            self.status.begin_restart()
            LOG.info(_("Restarting DB"))
            self.stop_db()
            self.start_db()
        finally:
            self.status.end_install_or_restart()

    def _install_db(self, packages):
        """Install cassandra server"""
        LOG.debug("Installing cassandra server")
        packager.pkg_install(packages, None, system.TIME_OUT)
        LOG.debug("Finished installing cassandra server")

    def write_config(self, config_contents):
        LOG.info(_('Defining temp config holder at '
                 '%s') % system.CASSANDRA_TEMP_CONF)
        with open(system.CASSANDRA_TEMP_CONF, 'w+') as conf:
            conf.write(config_contents)
        LOG.info(_('Writing new config'))
        utils.execute_with_timeout("sudo", "mv",
                                   system.CASSANDRA_TEMP_CONF,
                                   system.CASSANDRA_CONF)
        LOG.info(_('Overriding old config'))

    def read_conf(self):
        """Returns cassandra.yaml in dict structure"""

        LOG.info(_("Opening cassandra.yaml"))
        with open(system.CASSANDRA_CONF, 'r') as config:
            LOG.info(_("Preparing YAML object from cassandra.yaml"))
            yamled = yaml.load(config.read())
        return yamled

    def update_config_with_single(self, key, value):
        """Updates single key:value in cassandra.yaml"""

        yamled = self.read_conf()
        yamled.update({key: value})
        LOG.info(_("Updating cassandra.yaml with %(key)s: %(value)s")
                 % {'key': key, 'value': value})
        dump = yaml.dump(yamled, default_flow_style=False)
        LOG.info(_("Dumping YAML to stream"))
        self.write_config(dump)

    def update_conf_with_group(self, group):
        """Updates group of key:value in cassandra.yaml"""

        yamled = self.read_conf()
        for key, value in group.iteritems():
            if key == 'seed':
                (yamled.get('seed_provider')[0].
                 get('parameters')[0].
                 update({'seeds': value}))
            else:
                yamled.update({key: value})
            LOG.info(_("Updating cassandra.yaml with %(key)s: %(value)s")
                     % {'key': key, 'value': value})
        dump = yaml.dump(yamled, default_flow_style=False)
        LOG.info(_("Dumping YAML to stream"))
        self.write_config(dump)

    def make_host_reachable(self):
        updates = {
            'rpc_address': "0.0.0.0",
            'listen_address': operating_system.get_ip_address(),
            'seed': operating_system.get_ip_address()
        }
        self.update_conf_with_group(updates)

    def start_db_with_conf_changes(self, config_contents):
        LOG.info(_("Starting cassandra with conf changes..."))
        LOG.info(_("inside the guest - cassandra is running %s...")
                 % self.status.is_running)
        if self.status.is_running:
            LOG.error(_("Cannot execute start_db_with_conf_changes because "
                        "cassandra state == %s!") % self.status)
            raise RuntimeError("Cassandra not stopped.")
        LOG.info(_("Initiating config."))
        self.write_config(config_contents)
        self.start_db(True)

    def reset_configuration(self, configuration):
        config_contents = configuration['config_contents']
        LOG.info(_("Resetting configuration"))
        self.write_config(config_contents)


class CassandraAppStatus(service.BaseDbStatus):

    def _get_actual_db_status(self):
        try:
            # If status check would be successful,
            # bot stdin and stdout would contain nothing
            out, err = utils.execute_with_timeout(system.CASSANDRA_STATUS,
                                                  shell=True)
            if "Connection error. Could not connect to" not in err:
                return rd_instance.ServiceStatuses.RUNNING
            else:
                return rd_instance.ServiceStatuses.SHUTDOWN
        except exception.ProcessExecutionError as e:
            LOG.error(_("Process execution %s") % e)
            return rd_instance.ServiceStatuses.SHUTDOWN
        except OSError as e:
            LOG.error(_("OS Error %s") % e)
            return rd_instance.ServiceStatuses.SHUTDOWN

########NEW FILE########
__FILENAME__ = system
#  Copyright 2013 Mirantis Inc.
#  All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from trove.common import cfg
from trove.openstack.common import log as logging

LOG = logging.getLogger(__name__)
CONF = cfg.CONF

CASSANDRA_DATA_DIR = "/var/lib/cassandra/data"
CASSANDRA_CONF = "/etc/cassandra/cassandra.yaml"
CASSANDRA_TEMP_CONF = "/tmp/cassandra.yaml"
CASSANDRA_TEMP_DIR = "/tmp/cassandra"

INIT_FS = "sudo mkdir -p %s"
ENABLE_CASSANDRA_ON_BOOT = "sudo update-rc.d cassandra enable"
DISABLE_CASSANDRA_ON_BOOT = "sudo update-rc.d cassandra disable"

# Use service command to start and stop cassandra
START_CASSANDRA = "sudo service cassandra start"
STOP_CASSANDRA = "sudo service cassandra stop"

CASSANDRA_STATUS = """echo "use system;" > /tmp/check; cqlsh -f /tmp/check"""

CASSANDRA_KILL = "sudo killall java  || true"

TIME_OUT = 10000

########NEW FILE########
__FILENAME__ = manager
# Copyright (c) 2013 eBay Software Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import os

from trove.common import cfg
from trove.common import exception
from trove.guestagent import dbaas
from trove.guestagent import volume
from trove.guestagent.datastore.couchbase import service
from trove.guestagent.datastore.couchbase import system
from trove.openstack.common import log as logging
from trove.openstack.common import periodic_task
from trove.openstack.common.gettextutils import _


LOG = logging.getLogger(__name__)
CONF = cfg.CONF
MANAGER = CONF.datastore_manager


class Manager(periodic_task.PeriodicTasks):
    """
    This is Couchbase Manager class. It is dynamically loaded
    based off of the datastore of the trove instance
    """
    def __init__(self):
        self.appStatus = service.CouchbaseAppStatus()
        self.app = service.CouchbaseApp(self.appStatus)

    @periodic_task.periodic_task(ticks_between_runs=3)
    def update_status(self, context):
        """
        Updates the couchbase trove instance. It is decorated with
        perodic task so it is automatically called every 3 ticks.
        """
        self.appStatus.update()

    def change_passwords(self, context, users):
        raise exception.DatastoreOperationNotSupported(
            operation='change_passwords', datastore=MANAGER)

    def reset_configuration(self, context, configuration):
        self.app.reset_configuration(configuration)

    def prepare(self, context, packages, databases, memory_mb, users,
                device_path=None, mount_point=None, backup_info=None,
                config_contents=None, root_password=None, overrides=None):
        """
        This is called when the trove instance first comes online.
        It is the first rpc message passed from the task manager.
        prepare handles all the base configuration of the Couchbase instance.
        """
        self.appStatus.begin_install()
        if device_path:
            device = volume.VolumeDevice(device_path)
            # unmount if device is already mounted
            device.unmount_device(device_path)
            device.format()
            device.mount(mount_point)
            LOG.debug('Mounted the volume.')
        if root_password:
            self.app.enable_root(root_password)
        self.app.install_if_needed(packages)
        LOG.info(_('Securing couchbase now.'))
        self.app.complete_install_or_restart()
        LOG.info(_('"prepare" couchbase call has finished.'))

    def restart(self, context):
        """
        Restart this couchbase instance.
        This method is called when the guest agent
        gets a restart message from the taskmanager.
        """
        self.app.restart()

    def start_db_with_conf_changes(self, context, config_contents):
        self.app.start_db_with_conf_changes(config_contents)

    def stop_db(self, context, do_not_start_on_reboot=False):
        """
        Stop this couchbase instance.
        This method is called when the guest agent
        gets a stop message from the taskmanager.
        """
        self.app.stop_db(do_not_start_on_reboot=do_not_start_on_reboot)

    def get_filesystem_stats(self, context, fs_path):
        """Gets the filesystem stats for the path given. """
        mount_point = CONF.get(
            'mysql' if not MANAGER else MANAGER).mount_point
        return dbaas.get_filesystem_volume_stats(mount_point)

    def update_attributes(self, context, username, hostname, user_attrs):
        raise exception.DatastoreOperationNotSupported(
            operation='update_attributes', datastore=MANAGER)

    def create_database(self, context, databases):
        raise exception.DatastoreOperationNotSupported(
            operation='create_database', datastore=MANAGER)

    def create_user(self, context, users):
        raise exception.DatastoreOperationNotSupported(
            operation='create_user', datastore=MANAGER)

    def delete_database(self, context, database):
        raise exception.DatastoreOperationNotSupported(
            operation='delete_database', datastore=MANAGER)

    def delete_user(self, context, user):
        raise exception.DatastoreOperationNotSupported(
            operation='delete_user', datastore=MANAGER)

    def get_user(self, context, username, hostname):
        raise exception.DatastoreOperationNotSupported(
            operation='get_user', datastore=MANAGER)

    def grant_access(self, context, username, hostname, databases):
        raise exception.DatastoreOperationNotSupported(
            operation='grant_access', datastore=MANAGER)

    def revoke_access(self, context, username, hostname, database):
        raise exception.DatastoreOperationNotSupported(
            operation='revoke_access', datastore=MANAGER)

    def list_access(self, context, username, hostname):
        raise exception.DatastoreOperationNotSupported(
            operation='list_access', datastore=MANAGER)

    def list_databases(self, context, limit=None, marker=None,
                       include_marker=False):
        raise exception.DatastoreOperationNotSupported(
            operation='list_databases', datastore=MANAGER)

    def list_users(self, context, limit=None, marker=None,
                   include_marker=False):
        raise exception.DatastoreOperationNotSupported(
            operation='list_users', datastore=MANAGER)

    def enable_root(self, context):
        return self.app.enable_root()

    def is_root_enabled(self, context):
        return os.path.exists(system.pwd_file)

    def _perform_restore(self, backup_info, context, restore_location, app):
        raise exception.DatastoreOperationNotSupported(
            operation='_perform_restore', datastore=MANAGER)

    def create_backup(self, context, backup_info):
        raise exception.DatastoreOperationNotSupported(
            operation='create_backup', datastore=MANAGER)

    def mount_volume(self, context, device_path=None, mount_point=None):
        device = volume.VolumeDevice(device_path)
        device.mount(mount_point, write_to_fstab=False)
        LOG.debug("Mounted the volume.")

    def unmount_volume(self, context, device_path=None, mount_point=None):
        device = volume.VolumeDevice(device_path)
        device.unmount(mount_point)
        LOG.debug("Unmounted the volume.")

    def resize_fs(self, context, device_path=None, mount_point=None):
        device = volume.VolumeDevice(device_path)
        device.resize_fs(mount_point)
        LOG.debug("Resized the filesystem.")

    def update_overrides(self, context, overrides, remove=False):
        raise exception.DatastoreOperationNotSupported(
            operation='update_overrides', datastore=MANAGER)

    def apply_overrides(self, context, overrides):
        raise exception.DatastoreOperationNotSupported(
            operation='apply_overrides', datastore=MANAGER)

########NEW FILE########
__FILENAME__ = service
# Copyright (c) 2013 eBay Software Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import json
import pexpect
import os

from trove.common import cfg
from trove.common import exception
from trove.common import instance as rd_instance
from trove.common import utils as utils
from trove.guestagent import pkg
from trove.guestagent.common import operating_system
from trove.guestagent.datastore import service
from trove.guestagent.datastore.couchbase import system
from trove.guestagent.db import models
from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _


LOG = logging.getLogger(__name__)
CONF = cfg.CONF
packager = pkg.Package()


class CouchbaseApp(object):
    """
    Handles installation and configuration of couchbase
    on a trove instance.
    """
    def __init__(self, status, state_change_wait_time=None):
        """
        Sets default status and state_change_wait_time
        """
        if state_change_wait_time:
            self.state_change_wait_time = state_change_wait_time
        else:
            self.state_change_wait_time = CONF.state_change_wait_time
        self.status = status

    def install_if_needed(self, packages):
        """
        Install couchbase if needed, do nothing if it is already installed.
        """
        LOG.info(_('Preparing Guest as Couchbase Server'))
        if not packager.pkg_is_installed(packages):
            LOG.info(_('Installing Couchbase'))
            self._install_couchbase(packages)
        self.initial_setup()

    def initial_setup(self):
        self.ip_address = operating_system.get_ip_address()
        mount_point = CONF.get('couchbase').mount_point
        try:
            LOG.info(_('Couchbase Server change data dir path'))
            utils.execute_with_timeout(system.cmd_own_data_dir, shell=True)
            pwd = CouchbaseRootAccess.get_password()
            utils.execute_with_timeout(
                (system.cmd_node_init
                 % {'data_path': mount_point,
                    'IP': self.ip_address,
                    'PWD': pwd}), shell=True)
            utils.execute_with_timeout(
                system.cmd_rm_old_data_dir, shell=True)
            LOG.info(_('Couchbase Server initialize cluster'))
            utils.execute_with_timeout(
                (system.cmd_cluster_init
                 % {'IP': self.ip_address, 'PWD': pwd}),
                shell=True)
            utils.execute_with_timeout(system.cmd_set_swappiness, shell=True)
            utils.execute_with_timeout(system.cmd_update_sysctl_conf,
                                       shell=True)
            LOG.info(_('Couchbase Server initial setup finished'))
        except exception.ProcessExecutionError as e:
            LOG.error(_('Process execution error %s') % e)
            raise RuntimeError("Couchbase Server initial setup failed")

    def complete_install_or_restart(self):
        """
        finalize status updates for install or restart.
        """
        self.status.end_install_or_restart()

    def _install_couchbase(self, packages):
        """
        Install the Couchbase Server.
        """
        LOG.debug('Installing Couchbase Server')
        msg = "Creating %s" % system.COUCHBASE_CONF_DIR
        LOG.debug(msg)
        utils.execute_with_timeout('mkdir',
                                   '-p',
                                   system.COUCHBASE_CONF_DIR,
                                   run_as_root=True,
                                   root_helper='sudo')
        pkg_opts = {}
        packager.pkg_install(packages, pkg_opts, system.TIME_OUT)
        self.start_db()
        LOG.debug('Finished installing Couchbase Server')

    def _enable_db_on_boot(self):
        """
        Enables Couchbase Server on boot.
        """
        LOG.info(_('Enabling Couchbase Server on boot.'))
        try:
            couchbase_service = operating_system.service_discovery(
                system.SERVICE_CANDIDATES)
            utils.execute_with_timeout(
                couchbase_service['cmd_enable'], shell=True)
        except KeyError:
            raise RuntimeError(_(
                "Command to enable Couchbase Server on boot not found."))

    def _disable_db_on_boot(self):
        LOG.info(_("Disabling Couchbase Server on boot"))
        try:
            couchbase_service = operating_system.service_discovery(
                system.SERVICE_CANDIDATES)
            utils.execute_with_timeout(
                couchbase_service['cmd_disable'], shell=True)
        except KeyError:
            raise RuntimeError(
                "Command to disable Couchbase Server on boot not found.")

    def stop_db(self, update_db=False, do_not_start_on_reboot=False):
        """
        Stops Couchbase Server on the trove instance.
        """
        LOG.info(_('Stopping Couchbase Server...'))
        if do_not_start_on_reboot:
            self._disable_db_on_boot()

        try:
            couchbase_service = operating_system.service_discovery(
                system.SERVICE_CANDIDATES)
            utils.execute_with_timeout(
                couchbase_service['cmd_stop'], shell=True)
        except KeyError:
            raise RuntimeError("Command to stop Couchbase Server not found")

        if not self.status.wait_for_real_status_to_change_to(
                rd_instance.ServiceStatuses.SHUTDOWN,
                self.state_change_wait_time, update_db):
            LOG.error(_('Could not stop Couchbase Server!'))
            self.status.end_install_or_restart()
            raise RuntimeError(_("Could not stop Couchbase Server"))

    def restart(self):
        LOG.info(_("Restarting Couchbase Server"))
        try:
            self.status.begin_restart()
            self.stop_db()
            self.start_db()
        finally:
            self.status.end_install_or_restart()

    def start_db(self, update_db=False):
        """
        Start the Couchbase Server.
        """
        LOG.info(_("Starting Couchbase Server..."))

        self._enable_db_on_boot()
        try:
            couchbase_service = operating_system.service_discovery(
                system.SERVICE_CANDIDATES)
            utils.execute_with_timeout(
                couchbase_service['cmd_start'], shell=True)
        except exception.ProcessExecutionError:
            pass
        except KeyError:
            raise RuntimeError("Command to start Couchbase Server not found.")

        if not self.status.wait_for_real_status_to_change_to(
                rd_instance.ServiceStatuses.RUNNING,
                self.state_change_wait_time, update_db):
            LOG.error(_("Start up of Couchbase Server failed!"))
            try:
                utils.execute_with_timeout(system.cmd_kill)
            except exception.ProcessExecutionError as p:
                LOG.error('Error killing stalled Couchbase start command.')
                LOG.error(p)
            self.status.end_install_or_restart()
            raise RuntimeError("Could not start Couchbase Server")

    def enable_root(self, root_password=None):
        return CouchbaseRootAccess.enable_root(root_password)

    def start_db_with_conf_changes(self, config_contents):
        LOG.info(_("Starting Couchbase with configuration changes"))
        LOG.info(_("Configuration contents:\n %s") % config_contents)
        if self.status.is_running:
            LOG.error(_("Cannot start Couchbase with configuration changes. "
                        "Couchbase state == %s!") % self.status)
            raise RuntimeError("Couchbase is not stopped.")
        self._write_config(config_contents)
        self.start_db(True)

    def reset_configuration(self, configuration):
        config_contents = configuration['config_contents']
        LOG.info(_("Resetting configuration"))
        self._write_config(config_contents)

    def _write_config(self, config_contents):
        """
        Update contents of Couchbase configuration file
        """
        LOG.info(_("Doing nothing."))


class CouchbaseAppStatus(service.BaseDbStatus):
    """
    Handles all of the status updating for the couchbase guest agent.
    """
    def _get_actual_db_status(self):
        self.ip_address = operating_system.get_ip_address()
        try:
            pwd = CouchbaseRootAccess.get_password()
            out, err = utils.execute_with_timeout(
                (system.cmd_couchbase_status %
                 {'IP': self.ip_address, 'PWD': pwd}),
                shell=True)
            server_stats = json.loads(out)
            if not err and server_stats["clusterMembership"] == "active":
                return rd_instance.ServiceStatuses.RUNNING
            else:
                return rd_instance.ServiceStatuses.SHUTDOWN
        except exception.ProcessExecutionError as e:
            LOG.error(_("Process execution %s ") % e)
            return rd_instance.ServiceStatuses.SHUTDOWN


class CouchbaseRootAccess(object):

    @classmethod
    def enable_root(cls, root_password=None):
        user = models.RootUser()
        user.name = "root"
        user.host = "%"
        user.password = root_password or utils.generate_random_password()

        if root_password:
            CouchbaseRootAccess().write_password_to_file(root_password)
        else:
            CouchbaseRootAccess().set_password(user.password)
        return user.serialize()

    def set_password(self, root_password):
        self.ip_address = operating_system.get_ip_address()
        child = pexpect.spawn(system.cmd_reset_pwd % {'IP': self.ip_address})
        try:
            child.expect('.*password.*')
            child.sendline(root_password)
            child.expect('.*(yes/no).*')
            child.sendline('yes')
            child.expect('.*successfully.*')
        except pexpect.TIMEOUT:
            child.delayafterclose = 1
            child.delayafterterminate = 1
            child.close(force=True)

        self.write_password_to_file(root_password)

    def write_password_to_file(self, root_password):
        utils.execute_with_timeout('mkdir',
                                   '-p',
                                   system.COUCHBASE_CONF_DIR,
                                   run_as_root=True,
                                   root_helper='sudo')
        utils.execute_with_timeout("sudo sh -c 'echo " +
                                   root_password +
                                   ' > ' +
                                   system.pwd_file + "'",
                                   shell=True)

    @staticmethod
    def get_password():
        pwd = "password"
        if os.path.exists(system.pwd_file):
            with open(system.pwd_file) as file:
                pwd = file.readline().strip()
        return pwd

########NEW FILE########
__FILENAME__ = system
# Copyright (c) 2013 eBay Software Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from trove.common import cfg
CONF = cfg.CONF

TIME_OUT = 1200
COUCHBASE_CONF_DIR = '/etc/couchbase'
SERVICE_CANDIDATES = ["couchbase-server"]
cmd_couchbase_status = ('sudo /opt/couchbase/bin/couchbase-cli server-info '
                        '-c %(IP)s:8091 -u root -p %(PWD)s')
cmd_node_init = ('sudo /opt/couchbase/bin/couchbase-cli node-init '
                 '-c %(IP)s:8091 --node-init-data-path=%(data_path)s '
                 '-u root -p %(PWD)s')
cmd_cluster_init = ('sudo /opt/couchbase/bin/couchbase-cli cluster-init '
                    '-c %(IP)s:8091 --cluster-init-username=root '
                    '--cluster-init-password=%(PWD)s '
                    '--cluster-init-port=8091')
cmd_kill = 'sudo pkill -u couchbase'
cmd_own_data_dir = ('sudo chown couchbase:couchbase %s' %
                    CONF.get('couchbase').mount_point)
cmd_rm_old_data_dir = 'sudo rm -rf /opt/couchbase/var/lib/couchbase/data'
""" For optimal couchbase operations, swappiness of vm should be set to 0.
Reference link: http://docs.couchbase.com/couchbase-manual-2
.5/cb-admin/#using-couchbase-in-the-cloud """
cmd_set_swappiness = 'sudo sysctl vm.swappiness=0'
cmd_update_sysctl_conf = ('echo "vm.swappiness = 0" | sudo tee -a '
                          '/etc/sysctl.conf')
cmd_reset_pwd = 'sudo /opt/couchbase/bin/cbreset_password %(IP)s:8091'
pwd_file = COUCHBASE_CONF_DIR + '/secret_key'

########NEW FILE########
__FILENAME__ = manager
#   Copyright (c) 2014 Mirantis, Inc.
#   All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import os

from trove.common import cfg
from trove.common import exception
from trove.guestagent import dbaas
from trove.guestagent import volume
from trove.guestagent.datastore.mongodb import service as mongo_service
from trove.guestagent.datastore.mongodb import system
from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _
from trove.openstack.common import periodic_task


LOG = logging.getLogger(__name__)
CONF = cfg.CONF
MANAGER = CONF.datastore_manager


class Manager(periodic_task.PeriodicTasks):

    def __init__(self):
        self.status = mongo_service.MongoDbAppStatus()
        self.app = mongo_service.MongoDBApp(self.status)

    @periodic_task.periodic_task(ticks_between_runs=3)
    def update_status(self, context):
        """Update the status of the MongoDB service"""
        self.status.update()

    def prepare(self, context, packages, databases, memory_mb, users,
                device_path=None, mount_point=None, backup_info=None,
                config_contents=None, root_password=None, overrides=None):
        """Makes ready DBAAS on a Guest container."""

        LOG.debug("Prepare MongoDB instance")

        self.status.begin_install()
        self.app.install_if_needed(packages)
        self.app.stop_db()
        self.app.clear_storage()
        mount_point = system.MONGODB_MOUNT_POINT
        if device_path:
            device = volume.VolumeDevice(device_path)
            # unmount if device is already mounted
            device.unmount_device(device_path)
            device.format()
            if os.path.exists(system.MONGODB_MOUNT_POINT):
                device.migrate_data(mount_point)
            device.mount(mount_point)
            self.app.update_owner(mount_point)

            LOG.debug("Mounted the volume %(path)s as %(mount)s" %
                      {'path': device_path, "mount": mount_point})

        if mount_point:
            config_contents = self.app.update_config_contents(
                config_contents, {
                    'dbpath': mount_point,
                })

        self.app.start_db_with_conf_changes(config_contents)
        LOG.info(_('"prepare" call has finished.'))

    def restart(self, context):
        self.app.restart()

    def start_db_with_conf_changes(self, context, config_contents):
        self.app.start_db_with_conf_changes(config_contents)

    def stop_db(self, context, do_not_start_on_reboot=False):
        self.app.stop_db(do_not_start_on_reboot=do_not_start_on_reboot)

    def reset_configuration(self, context, configuration):
        self.app.reset_configuration(configuration)

    def get_filesystem_stats(self, context, fs_path):
        """Gets the filesystem stats for the path given """
        return dbaas.get_filesystem_volume_stats(system.MONGODB_MOUNT_POINT)

    def change_passwords(self, context, users):
        raise exception.DatastoreOperationNotSupported(
            operation='change_passwords', datastore=MANAGER)

    def update_attributes(self, context, username, hostname, user_attrs):
        raise exception.DatastoreOperationNotSupported(
            operation='update_attributes', datastore=MANAGER)

    def create_database(self, context, databases):
        raise exception.DatastoreOperationNotSupported(
            operation='create_database', datastore=MANAGER)

    def create_user(self, context, users):
        raise exception.DatastoreOperationNotSupported(
            operation='create_user', datastore=MANAGER)

    def delete_database(self, context, database):
        raise exception.DatastoreOperationNotSupported(
            operation='delete_database', datastore=MANAGER)

    def delete_user(self, context, user):
        raise exception.DatastoreOperationNotSupported(
            operation='delete_user', datastore=MANAGER)

    def get_user(self, context, username, hostname):
        raise exception.DatastoreOperationNotSupported(
            operation='get_user', datastore=MANAGER)

    def grant_access(self, context, username, hostname, databases):
        raise exception.DatastoreOperationNotSupported(
            operation='grant_access', datastore=MANAGER)

    def revoke_access(self, context, username, hostname, database):
        raise exception.DatastoreOperationNotSupported(
            operation='revoke_access', datastore=MANAGER)

    def list_access(self, context, username, hostname):
        raise exception.DatastoreOperationNotSupported(
            operation='list_access', datastore=MANAGER)

    def list_databases(self, context, limit=None, marker=None,
                       include_marker=False):
        raise exception.DatastoreOperationNotSupported(
            operation='list_databases', datastore=MANAGER)

    def list_users(self, context, limit=None, marker=None,
                   include_marker=False):
        raise exception.DatastoreOperationNotSupported(
            operation='list_users', datastore=MANAGER)

    def enable_root(self, context):
        raise exception.DatastoreOperationNotSupported(
            operation='enable_root', datastore=MANAGER)

    def is_root_enabled(self, context):
        raise exception.DatastoreOperationNotSupported(
            operation='is_root_enabled', datastore=MANAGER)

    def _perform_restore(self, backup_info, context, restore_location, app):
        raise exception.DatastoreOperationNotSupported(
            operation='_perform_restore', datastore=MANAGER)

    def create_backup(self, context, backup_info):
        raise exception.DatastoreOperationNotSupported(
            operation='create_backup', datastore=MANAGER)

    def mount_volume(self, context, device_path=None, mount_point=None):
        device = volume.VolumeDevice(device_path)
        device.mount(mount_point, write_to_fstab=False)
        LOG.debug("Mounted the volume.")

    def unmount_volume(self, context, device_path=None, mount_point=None):
        device = volume.VolumeDevice(device_path)
        device.unmount(mount_point)
        LOG.debug("Unmounted the volume.")

    def resize_fs(self, context, device_path=None, mount_point=None):
        device = volume.VolumeDevice(device_path)
        device.resize_fs(mount_point)
        LOG.debug("Resized the filesystem")

    def update_overrides(self, context, overrides, remove=False):
        raise exception.DatastoreOperationNotSupported(
            operation='update_overrides', datastore=MANAGER)

    def apply_overrides(self, context, overrides):
        raise exception.DatastoreOperationNotSupported(
            operation='apply_overrides', datastore=MANAGER)

########NEW FILE########
__FILENAME__ = service
#   Copyright (c) 2014 Mirantis, Inc.
#   All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import re

from trove.common import cfg
from trove.common import utils as utils
from trove.common import exception
from trove.common import instance as rd_instance
from trove.common.exception import ProcessExecutionError
from trove.guestagent.datastore import service
from trove.guestagent.datastore.mongodb import system
from trove.openstack.common import log as logging
from trove.guestagent.common import operating_system
from trove.openstack.common.gettextutils import _

LOG = logging.getLogger(__name__)
CONF = cfg.CONF


class MongoDBApp(object):
    """Prepares DBaaS on a Guest container."""

    def __init__(self, status):
        self.state_change_wait_time = CONF.state_change_wait_time
        self.status = status

    def install_if_needed(self, packages):
        """Prepare the guest machine with a MongoDB installation"""
        LOG.info(_("Preparing Guest as MongoDB"))
        if not system.PACKAGER.pkg_is_installed(packages):
            LOG.debug("Installing packages: %s" % str(packages))
            system.PACKAGER.pkg_install(packages, {}, system.TIME_OUT)
        LOG.info(_("Finished installing MongoDB server"))

    def _enable_db_on_boot(self):
        LOG.info(_("Enabling MongoDB on boot"))
        try:
            mongodb_service = operating_system.service_discovery(
                system.SERVICE_CANDIDATES)
            utils.execute_with_timeout(mongodb_service['cmd_enable'],
                                       shell=True)
        except KeyError:
            raise RuntimeError(_("MongoDB service is not discovered."))

    def _disable_db_on_boot(self):
        LOG.info(_("Disabling MongoDB on boot"))
        try:
            mongodb_service = operating_system.service_discovery(
                system.SERVICE_CANDIDATES)
            utils.execute_with_timeout(mongodb_service['cmd_disable'],
                                       shell=True)
        except KeyError:
            raise RuntimeError("MongoDB service is not discovered.")

    def stop_db(self, update_db=False, do_not_start_on_reboot=False):
        LOG.info(_("Stopping MongoDB"))
        if do_not_start_on_reboot:
            self._disable_db_on_boot()

        try:
            mongodb_service = operating_system.service_discovery(
                system.SERVICE_CANDIDATES)
            utils.execute_with_timeout(mongodb_service['cmd_stop'],
                                       shell=True)
        except KeyError:
            raise RuntimeError(_("MongoDB service is not discovered."))

        if not self.status.wait_for_real_status_to_change_to(
                rd_instance.ServiceStatuses.SHUTDOWN,
                self.state_change_wait_time, update_db):
            LOG.error(_("Could not stop MongoDB"))
            self.status.end_install_or_restart()
            raise RuntimeError(_("Could not stop MongoDB"))

    def restart(self):
        LOG.info(_("Restarting MongoDB"))
        try:
            self.status.begin_restart()
            self.stop_db()
            self.start_db()
        finally:
            self.status.end_install_or_restart()

    def start_db(self, update_db=False):
        LOG.info(_("Starting MongoDB"))

        self._enable_db_on_boot()

        try:
            mongodb_service = operating_system.service_discovery(
                system.SERVICE_CANDIDATES)
            utils.execute_with_timeout(mongodb_service['cmd_start'],
                                       shell=True)
        except ProcessExecutionError:
            pass
        except KeyError:
            raise RuntimeError("MongoDB service is not discovered.")

        if not self.status.wait_for_real_status_to_change_to(
                rd_instance.ServiceStatuses.RUNNING,
                self.state_change_wait_time, update_db):
            LOG.error(_("Start up of MongoDB failed"))
            # If it won't start, but won't die either, kill it by hand so we
            # don't let a rouge process wander around.
            try:
                out, err = utils.execute_with_timeout(
                    system.FIND_PID, shell=True)
                pid = "".join(out.split(" ")[1:2])
                utils.execute_with_timeout(
                    system.MONGODB_KILL % pid, shell=True)
            except exception.ProcessExecutionError as p:
                LOG.error("Error killing stalled MongoDB start command.")
                LOG.error(p)
                # There's nothing more we can do...
            self.status.end_install_or_restart()
            raise RuntimeError("Could not start MongoDB")

    def start_db_with_conf_changes(self, config_contents):
        LOG.info(_("Starting MongoDB with configuration changes"))
        LOG.info(_("Configuration contents:\n %s") % config_contents)
        if self.status.is_running:
            LOG.error(_("Cannot start MongoDB with configuration changes. "
                        "MongoDB state == %s!") % self.status)
            raise RuntimeError("MongoDB is not stopped.")
        self._write_config(config_contents)
        self.start_db(True)

    def reset_configuration(self, configuration):
        config_contents = configuration['config_contents']
        LOG.info(_("Resetting configuration"))
        self._write_config(config_contents)

    def update_config_contents(self, config_contents, parameters):
        if not config_contents:
            config_contents = self._read_config()

        contents = self._delete_config_parameters(config_contents,
                                                  parameters.keys())
        for param, value in parameters.iteritems():
            if param and value:
                contents = self._add_config_parameter(contents,
                                                      param, value)

        return contents

    def _write_config(self, config_contents):
        """
        Update contents of MongoDB configuration file
        """
        LOG.info(_("Updating MongoDB config"))
        if config_contents:
            LOG.info(_("Writing %s") % system.TMP_CONFIG)
            with open(system.TMP_CONFIG, 'w') as t:
                t.write(config_contents)

            LOG.info(_("Moving %(a)s to %(b)s")
                     % {'a': system.TMP_CONFIG, 'b': system.CONFIG})
            utils.execute_with_timeout("mv", system.TMP_CONFIG, system.CONFIG,
                                       run_as_root=True, root_helper="sudo")
        else:
            LOG.info(_("Empty config_contents. Do nothing"))

    def _read_config(self):
        try:
            with open(system.CONFIG, 'r') as f:
                return f.read()
        except IOError:
            LOG.info(_("Config file %s not found") % system.CONFIG)
            return ''

    def _delete_config_parameters(self, config_contents, parameters):
        if not config_contents:
            return None

        params_as_string = '|'.join(parameters)
        p = re.compile("\\s*#?\\s*(%s)\\s*=" % params_as_string)
        contents_as_list = config_contents.splitlines()
        filtered = filter(lambda line: not p.match(line), contents_as_list)
        return '\n'.join(filtered)

    def _add_config_parameter(self, config_contents, parameter, value):
        return (config_contents or '') + "\n%s = %s" % (parameter, value)

    def update_owner(self, path):
        LOG.info(_("Set owner to 'mongodb' for %s ") % system.CONFIG)
        utils.execute_with_timeout("chown", "-R", "mongodb", path,
                                   run_as_root=True, root_helper="sudo")
        LOG.info(_("Set group to 'mongodb' for %s ") % system.CONFIG)
        utils.execute_with_timeout("chgrp", "-R", "mongodb", path,
                                   run_as_root=True, root_helper="sudo")

    def clear_storage(self):
        mount_point = "/var/lib/mongodb/*"
        try:
            cmd = "sudo rm -rf %s" % mount_point
            utils.execute_with_timeout(cmd, shell=True)
        except exception.ProcessExecutionError as e:
            LOG.error(_("Process execution %s") % e)


class MongoDbAppStatus(service.BaseDbStatus):
    def _get_actual_db_status(self):
        try:
            status_check = (system.CMD_STATUS %
                            operating_system.get_ip_address())
            out, err = utils.execute_with_timeout(status_check, shell=True)
            if not err and "connected to:" in out:
                return rd_instance.ServiceStatuses.RUNNING
            else:
                return rd_instance.ServiceStatuses.SHUTDOWN
        except exception.ProcessExecutionError as e:
            LOG.error(_("Process execution %s") % e)
            return rd_instance.ServiceStatuses.SHUTDOWN
        except OSError as e:
            LOG.error(_("OS Error %s") % e)
            return rd_instance.ServiceStatuses.SHUTDOWN

########NEW FILE########
__FILENAME__ = system
#   Copyright (c) 2014 Mirantis, Inc.
#   All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from trove.guestagent import pkg

MONGODB_MOUNT_POINT = "/var/lib/mongodb"
# After changing bind address mongodb accepts connection
# on real IP, not on the localhost
CMD_STATUS = "mongostat --host %s -n 1 | grep connected"

TMP_CONFIG = "/tmp/mongodb.conf.tmp"
CONFIG = "/etc/mongodb.conf"
SERVICE_CANDIDATES = ["mongodb", "mongod"]
MONGODB_KILL = "sudo kill %s"
FIND_PID = "ps xau | grep mongod"
TIME_OUT = 1000

PACKAGER = pkg.Package()

########NEW FILE########
__FILENAME__ = manager
# Copyright 2013 OpenStack Foundation
# Copyright 2013 Rackspace Hosting
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

import os
from trove.common import cfg
from trove.common import instance as rd_instance
from trove.guestagent import dbaas
from trove.guestagent import backup
from trove.guestagent import volume
from trove.guestagent.datastore.mysql.service import MySqlAppStatus
from trove.guestagent.datastore.mysql.service import MySqlAdmin
from trove.guestagent.datastore.mysql.service import MySqlApp
from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _
from trove.openstack.common import periodic_task


LOG = logging.getLogger(__name__)
CONF = cfg.CONF
MANAGER = CONF.datastore_manager


class Manager(periodic_task.PeriodicTasks):

    @periodic_task.periodic_task(ticks_between_runs=3)
    def update_status(self, context):
        """Update the status of the MySQL service"""
        MySqlAppStatus.get().update()

    def change_passwords(self, context, users):
        return MySqlAdmin().change_passwords(users)

    def update_attributes(self, context, username, hostname, user_attrs):
        return MySqlAdmin().update_attributes(username, hostname, user_attrs)

    def reset_configuration(self, context, configuration):
        app = MySqlApp(MySqlAppStatus.get())
        app.reset_configuration(configuration)

    def create_database(self, context, databases):
        return MySqlAdmin().create_database(databases)

    def create_user(self, context, users):
        MySqlAdmin().create_user(users)

    def delete_database(self, context, database):
        return MySqlAdmin().delete_database(database)

    def delete_user(self, context, user):
        MySqlAdmin().delete_user(user)

    def get_user(self, context, username, hostname):
        return MySqlAdmin().get_user(username, hostname)

    def grant_access(self, context, username, hostname, databases):
        return MySqlAdmin().grant_access(username, hostname, databases)

    def revoke_access(self, context, username, hostname, database):
        return MySqlAdmin().revoke_access(username, hostname, database)

    def list_access(self, context, username, hostname):
        return MySqlAdmin().list_access(username, hostname)

    def list_databases(self, context, limit=None, marker=None,
                       include_marker=False):
        return MySqlAdmin().list_databases(limit, marker,
                                           include_marker)

    def list_users(self, context, limit=None, marker=None,
                   include_marker=False):
        return MySqlAdmin().list_users(limit, marker,
                                       include_marker)

    def enable_root(self, context):
        return MySqlAdmin().enable_root()

    def is_root_enabled(self, context):
        return MySqlAdmin().is_root_enabled()

    def _perform_restore(self, backup_info, context, restore_location, app):
        LOG.info(_("Restoring database from backup %s") % backup_info['id'])
        try:
            backup.restore(context, backup_info, restore_location)
        except Exception as e:
            LOG.error(e)
            LOG.error("Error performing restore from backup %s",
                      backup_info['id'])
            app.status.set_status(rd_instance.ServiceStatuses.FAILED)
            raise
        LOG.info(_("Restored database successfully"))

    def prepare(self, context, packages, databases, memory_mb, users,
                device_path=None, mount_point=None, backup_info=None,
                config_contents=None, root_password=None, overrides=None):
        """Makes ready DBAAS on a Guest container."""
        MySqlAppStatus.get().begin_install()
        # status end_mysql_install set with secure()
        app = MySqlApp(MySqlAppStatus.get())
        app.install_if_needed(packages)
        if device_path:
            #stop and do not update database
            app.stop_db()
            device = volume.VolumeDevice(device_path)
            # unmount if device is already mounted
            device.unmount_device(device_path)
            device.format()
            if os.path.exists(mount_point):
                #rsync exiting data
                device.migrate_data(mount_point)
            #mount the volume
            device.mount(mount_point)
            LOG.debug("Mounted the volume.")
            app.start_mysql()
        if backup_info:
            self._perform_restore(backup_info, context,
                                  mount_point, app)
        LOG.info(_("Securing mysql now."))
        app.secure(config_contents, overrides)
        enable_root_on_restore = (backup_info and
                                  MySqlAdmin().is_root_enabled())
        if root_password and not backup_info:
            app.secure_root(secure_remote_root=True)
            MySqlAdmin().enable_root(root_password)
        elif enable_root_on_restore:
            app.secure_root(secure_remote_root=False)
        else:
            app.secure_root(secure_remote_root=True)

        app.complete_install_or_restart()

        if databases:
            self.create_database(context, databases)

        if users:
            self.create_user(context, users)

        LOG.info('"prepare" call has finished.')

    def restart(self, context):
        app = MySqlApp(MySqlAppStatus.get())
        app.restart()

    def start_db_with_conf_changes(self, context, config_contents):
        app = MySqlApp(MySqlAppStatus.get())
        app.start_db_with_conf_changes(config_contents)

    def stop_db(self, context, do_not_start_on_reboot=False):
        app = MySqlApp(MySqlAppStatus.get())
        app.stop_db(do_not_start_on_reboot=do_not_start_on_reboot)

    def get_filesystem_stats(self, context, fs_path):
        """Gets the filesystem stats for the path given. """
        mount_point = CONF.get(
            'mysql' if not MANAGER else MANAGER).mount_point
        return dbaas.get_filesystem_volume_stats(mount_point)

    def create_backup(self, context, backup_info):
        """
        Entry point for initiating a backup for this guest agents db instance.
        The call currently blocks until the backup is complete or errors. If
        device_path is specified, it will be mounted based to a point specified
        in configuration.

        :param backup_info: a dictionary containing the db instance id of the
                            backup task, location, type, and other data.
        """
        backup.backup(context, backup_info)

    def mount_volume(self, context, device_path=None, mount_point=None):
        device = volume.VolumeDevice(device_path)
        device.mount(mount_point, write_to_fstab=False)
        LOG.debug("Mounted the volume.")

    def unmount_volume(self, context, device_path=None, mount_point=None):
        device = volume.VolumeDevice(device_path)
        device.unmount(mount_point)
        LOG.debug("Unmounted the volume.")

    def resize_fs(self, context, device_path=None, mount_point=None):
        device = volume.VolumeDevice(device_path)
        device.resize_fs(mount_point)
        LOG.debug("Resized the filesystem")

    def update_overrides(self, context, overrides, remove=False):
        app = MySqlApp(MySqlAppStatus.get())
        app.update_overrides(overrides, remove=remove)

    def apply_overrides(self, context, overrides):
        app = MySqlApp(MySqlAppStatus.get())
        app.apply_overrides(overrides)

########NEW FILE########
__FILENAME__ = service
# Copyright 2013 OpenStack Foundation
# Copyright 2013 Rackspace Hosting
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

import os
import re
import uuid
from datetime import date
import sqlalchemy
from sqlalchemy import exc
from sqlalchemy import interfaces
from sqlalchemy.sql.expression import text

from trove.common import cfg
from trove.common import utils as utils
from trove.common import exception
from trove.common import instance as rd_instance
from trove.guestagent.common import operating_system
from trove.guestagent.common import sql_query
from trove.guestagent.db import models
from trove.guestagent import pkg
from trove.guestagent.datastore import service
from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _

ADMIN_USER_NAME = "os_admin"
LOG = logging.getLogger(__name__)
FLUSH = text(sql_query.FLUSH)
ENGINE = None
PREPARING = False
UUID = False

TMP_MYCNF = "/tmp/my.cnf.tmp"
MYSQL_BASE_DIR = "/var/lib/mysql"

CONF = cfg.CONF

INCLUDE_MARKER_OPERATORS = {
    True: ">=",
    False: ">"
}

MYSQL_CONFIG = "/etc/mysql/my.cnf"
MYSQL_SERVICE_CANDIDATES = ["mysql", "mysqld", "mysql-server"]
MYSQL_BIN_CANDIDATES = ["/usr/sbin/mysqld", "/usr/libexec/mysqld"]
MYCNF_OVERRIDES = "/etc/mysql/conf.d/overrides.cnf"
MYCNF_OVERRIDES_TMP = "/tmp/overrides.cnf.tmp"


# Create a package impl
packager = pkg.Package()


def clear_expired_password():
    """
    Some mysql installations generate random root password
    and save it in /root/.mysql_secret, this password is
    expired and should be changed by client that supports expired passwords.
    """
    LOG.debug("Removing expired password.")
    secret_file = "/root/.mysql_secret"
    try:
        out, err = utils.execute("cat", secret_file,
                                 run_as_root=True, root_helper="sudo")
    except exception.ProcessExecutionError:
        LOG.debug("/root/.mysql_secret is not exists.")
        return
    m = re.match('# The random password set for the root user at .*: (.*)',
                 out)
    if m:
        try:
            out, err = utils.execute("mysqladmin", "-p%s" % m.group(1),
                                     "password", "", run_as_root=True,
                                     root_helper="sudo")
        except exception.ProcessExecutionError:
            LOG.error("Cannot change mysql password.")
            return
        utils.execute("rm", "-f", secret_file, run_as_root=True,
                      root_helper="sudo")
        LOG.debug("Expired password removed.")


def get_auth_password():
    pwd, err = utils.execute_with_timeout(
        "sudo",
        "awk",
        "/password\\t=/{print $3; exit}",
        MYSQL_CONFIG)
    if err:
        LOG.error(err)
        raise RuntimeError("Problem reading my.cnf! : %s" % err)
    return pwd.strip()


def get_engine():
    """Create the default engine with the updated admin user."""
    #TODO(rnirmal):Based on permissions issues being resolved we may revert
    #url = URL(drivername='mysql', host='localhost',
    #          query={'read_default_file': '/etc/mysql/my.cnf'})
    global ENGINE
    if ENGINE:
        return ENGINE
    pwd = get_auth_password()
    ENGINE = sqlalchemy.create_engine("mysql://%s:%s@localhost:3306" %
                                      (ADMIN_USER_NAME, pwd.strip()),
                                      pool_recycle=7200,
                                      echo=CONF.sql_query_logging,
                                      listeners=[KeepAliveConnection()])
    return ENGINE


def load_mysqld_options():
    #find mysqld bin
    for bin in MYSQL_BIN_CANDIDATES:
        if os.path.isfile(bin):
            mysqld_bin = bin
            break
    else:
        return {}
    try:
        out, err = utils.execute(mysqld_bin, "--print-defaults",
                                 run_as_root=True, root_helper="sudo")
        arglist = re.split("\n", out)[1].split()
        args = {}
        for item in arglist:
            if "=" in item:
                key, value = item.split("=")
                args[key.lstrip("--")] = value
            else:
                args[item.lstrip("--")] = None
        return args
    except exception.ProcessExecutionError:
        return {}


class MySqlAppStatus(service.BaseDbStatus):
    @classmethod
    def get(cls):
        if not cls._instance:
            cls._instance = MySqlAppStatus()
        return cls._instance

    def _get_actual_db_status(self):
        try:
            out, err = utils.execute_with_timeout(
                "/usr/bin/mysqladmin",
                "ping", run_as_root=True, root_helper="sudo")
            LOG.info("Service Status is RUNNING.")
            return rd_instance.ServiceStatuses.RUNNING
        except exception.ProcessExecutionError:
            LOG.error("Process execution ")
            try:
                out, err = utils.execute_with_timeout("/bin/ps", "-C",
                                                      "mysqld", "h")
                pid = out.split()[0]
                # TODO(rnirmal): Need to create new statuses for instances
                # where the mysql service is up, but unresponsive
                LOG.info('MySQL pid: %(pid)s' % {'pid': pid})
                LOG.info("Service Status is BLOCKED.")
                return rd_instance.ServiceStatuses.BLOCKED
            except exception.ProcessExecutionError:
                mysql_args = load_mysqld_options()
                pid_file = mysql_args.get('pid_file',
                                          '/var/run/mysqld/mysqld.pid')
                if os.path.exists(pid_file):
                    LOG.info("Service Status is CRASHED.")
                    return rd_instance.ServiceStatuses.CRASHED
                else:
                    LOG.info("Service Status is SHUTDOWN.")
                    return rd_instance.ServiceStatuses.SHUTDOWN


class LocalSqlClient(object):
    """A sqlalchemy wrapper to manage transactions."""

    def __init__(self, engine, use_flush=True):
        self.engine = engine
        self.use_flush = use_flush

    def __enter__(self):
        self.conn = self.engine.connect()
        self.trans = self.conn.begin()
        return self.conn

    def __exit__(self, type, value, traceback):
        if self.trans:
            if type is not None:  # An error occurred
                self.trans.rollback()
            else:
                if self.use_flush:
                    self.conn.execute(FLUSH)
                self.trans.commit()
        self.conn.close()

    def execute(self, t, **kwargs):
        try:
            return self.conn.execute(t, kwargs)
        except Exception:
            self.trans.rollback()
            self.trans = None
            raise


class MySqlAdmin(object):
    """Handles administrative tasks on the MySQL database."""

    def _associate_dbs(self, user):
        """Internal. Given a MySQLUser, populate its databases attribute."""
        LOG.debug("Associating dbs to user %s at %s" % (user.name, user.host))
        with LocalSqlClient(get_engine()) as client:
            q = sql_query.Query()
            q.columns = ["grantee", "table_schema"]
            q.tables = ["information_schema.SCHEMA_PRIVILEGES"]
            q.group = ["grantee", "table_schema"]
            q.where = ["privilege_type != 'USAGE'"]
            t = text(str(q))
            db_result = client.execute(t)
            for db in db_result:
                LOG.debug("\t db: %s" % db)
                if db['grantee'] == "'%s'@'%s'" % (user.name, user.host):
                    mysql_db = models.MySQLDatabase()
                    mysql_db.name = db['table_schema']
                    user.databases.append(mysql_db.serialize())

    def change_passwords(self, users):
        """Change the passwords of one or more existing users."""
        LOG.debug("Changing the password of some users.")
        LOG.debug("Users is %s" % users)
        with LocalSqlClient(get_engine()) as client:
            for item in users:
                LOG.debug("\tUser: %s" % item)
                user_dict = {'_name': item['name'],
                             '_host': item['host'],
                             '_password': item['password']}
                user = models.MySQLUser()
                user.deserialize(user_dict)
                LOG.debug("\tDeserialized: %s" % user.__dict__)
                uu = sql_query.UpdateUser(user.name, host=user.host,
                                          clear=user.password)
                t = text(str(uu))
                client.execute(t)

    def update_attributes(self, username, hostname, user_attrs):
        """Change the attributes of an existing user."""
        LOG.debug("Changing the user attributes")
        LOG.debug("User is %s" % username)
        user = self._get_user(username, hostname)
        db_access = set()
        grantee = set()
        with LocalSqlClient(get_engine()) as client:
            q = sql_query.Query()
            q.columns = ["grantee", "table_schema"]
            q.tables = ["information_schema.SCHEMA_PRIVILEGES"]
            q.group = ["grantee", "table_schema"]
            q.where = ["privilege_type != 'USAGE'"]
            t = text(str(q))
            db_result = client.execute(t)
            for db in db_result:
                grantee.add(db['grantee'])
                if db['grantee'] == "'%s'@'%s'" % (user.name, user.host):
                    db_name = db['table_schema']
                    db_access.add(db_name)
        with LocalSqlClient(get_engine()) as client:
            uu = sql_query.UpdateUser(user.name, host=user.host,
                                      clear=user_attrs.get('password'),
                                      new_user=user_attrs.get('name'),
                                      new_host=user_attrs.get('host'))
            t = text(str(uu))
            client.execute(t)
            uname = user_attrs.get('name') or username
            host = user_attrs.get('host') or hostname
            find_user = "'%s'@'%s'" % (uname, host)
            if find_user not in grantee:
                self.grant_access(uname, host, db_access)

    def create_database(self, databases):
        """Create the list of specified databases."""
        with LocalSqlClient(get_engine()) as client:
            for item in databases:
                mydb = models.ValidatedMySQLDatabase()
                mydb.deserialize(item)
                cd = sql_query.CreateDatabase(mydb.name,
                                              mydb.character_set,
                                              mydb.collate)
                t = text(str(cd))
                client.execute(t)

    def create_user(self, users):
        """Create users and grant them privileges for the
           specified databases.
        """
        with LocalSqlClient(get_engine()) as client:
            for item in users:
                user = models.MySQLUser()
                user.deserialize(item)
                # TODO(cp16net):Should users be allowed to create users
                # 'os_admin' or 'debian-sys-maint'
                g = sql_query.Grant(user=user.name, host=user.host,
                                    clear=user.password)
                t = text(str(g))
                client.execute(t)
                for database in user.databases:
                    mydb = models.ValidatedMySQLDatabase()
                    mydb.deserialize(database)
                    g = sql_query.Grant(permissions='ALL', database=mydb.name,
                                        user=user.name, host=user.host,
                                        clear=user.password)
                    t = text(str(g))
                    client.execute(t)

    def delete_database(self, database):
        """Delete the specified database."""
        with LocalSqlClient(get_engine()) as client:
            mydb = models.ValidatedMySQLDatabase()
            mydb.deserialize(database)
            dd = sql_query.DropDatabase(mydb.name)
            t = text(str(dd))
            client.execute(t)

    def delete_user(self, user):
        """Delete the specified user."""
        with LocalSqlClient(get_engine()) as client:
            mysql_user = models.MySQLUser()
            mysql_user.deserialize(user)
            du = sql_query.DropUser(mysql_user.name, host=mysql_user.host)
            t = text(str(du))
            client.execute(t)

    def get_user(self, username, hostname):
        user = self._get_user(username, hostname)
        if not user:
            return None
        return user.serialize()

    def _get_user(self, username, hostname):
        """Return a single user matching the criteria."""
        user = models.MySQLUser()
        try:
            user.name = username  # Could possibly throw a BadRequest here.
        except exception.ValueError as ve:
            raise exception.BadRequest(_("Username %(user)s is not valid"
                                         ": %(reason)s") %
                                       {'user': username, 'reason': ve.message}
                                       )
        with LocalSqlClient(get_engine()) as client:
            q = sql_query.Query()
            q.columns = ['User', 'Host', 'Password']
            q.tables = ['mysql.user']
            q.where = ["Host != 'localhost'",
                       "User = '%s'" % username,
                       "Host = '%s'" % hostname]
            q.order = ['User', 'Host']
            t = text(str(q))
            result = client.execute(t).fetchall()
            LOG.debug("Result: %s" % result)
            if len(result) != 1:
                return None
            found_user = result[0]
            user.password = found_user['Password']
            user.host = found_user['Host']
            self._associate_dbs(user)
            return user

    def grant_access(self, username, hostname, databases):
        """Grant a user permission to use a given database."""
        user = self._get_user(username, hostname)
        mydb = models.ValidatedMySQLDatabase()
        with LocalSqlClient(get_engine()) as client:
            for database in databases:
                    try:
                        mydb.name = database
                    except ValueError:
                        raise exception.BadRequest(_(
                            "Grant access to %s is not allowed") % database)

                    g = sql_query.Grant(permissions='ALL', database=mydb.name,
                                        user=user.name, host=user.host,
                                        hashed=user.password)
                    t = text(str(g))
                    client.execute(t)

    def is_root_enabled(self):
        """Return True if root access is enabled; False otherwise."""
        return MySqlRootAccess.is_root_enabled()

    def enable_root(self, root_password=None):
        """Enable the root user global access and/or
           reset the root password.
        """
        return MySqlRootAccess.enable_root(root_password)

    def list_databases(self, limit=None, marker=None, include_marker=False):
        """List databases the user created on this mysql instance."""
        LOG.debug("---Listing Databases---")
        databases = []
        with LocalSqlClient(get_engine()) as client:
            # If you have an external volume mounted at /var/lib/mysql
            # the lost+found directory will show up in mysql as a database
            # which will create errors if you try to do any database ops
            # on it.  So we remove it here if it exists.
            q = sql_query.Query()
            q.columns = [
                'schema_name as name',
                'default_character_set_name as charset',
                'default_collation_name as collation',
            ]
            q.tables = ['information_schema.schemata']
            q.where = ["schema_name NOT IN ("
                       "'mysql', 'information_schema', "
                       "'lost+found', '#mysql50#lost+found'"
                       ")"]
            q.order = ['schema_name ASC']
            if limit:
                q.limit = limit + 1
            if marker:
                q.where.append("schema_name %s '%s'" %
                               (INCLUDE_MARKER_OPERATORS[include_marker],
                                marker))
            t = text(str(q))
            database_names = client.execute(t)
            next_marker = None
            LOG.debug("database_names = %r" % database_names)
            for count, database in enumerate(database_names):
                if count >= limit:
                    break
                LOG.debug("database = %s " % str(database))
                mysql_db = models.MySQLDatabase()
                mysql_db.name = database[0]
                next_marker = mysql_db.name
                mysql_db.character_set = database[1]
                mysql_db.collate = database[2]
                databases.append(mysql_db.serialize())
        LOG.debug("databases = " + str(databases))
        if database_names.rowcount <= limit:
            next_marker = None
        return databases, next_marker

    def list_users(self, limit=None, marker=None, include_marker=False):
        """List users that have access to the database."""
        '''
        SELECT
            User,
            Host,
            Marker
        FROM
            (SELECT
                User,
                Host,
                CONCAT(User, '@', Host) as Marker
            FROM mysql.user
            ORDER BY 1, 2) as innerquery
        WHERE
            Marker > :marker
        ORDER BY
            Marker
        LIMIT :limit;
        '''
        LOG.debug("---Listing Users---")
        users = []
        with LocalSqlClient(get_engine()) as client:
            mysql_user = models.MySQLUser()
            iq = sql_query.Query()  # Inner query.
            iq.columns = ['User', 'Host', "CONCAT(User, '@', Host) as Marker"]
            iq.tables = ['mysql.user']
            iq.order = ['User', 'Host']
            innerquery = str(iq).rstrip(';')

            oq = sql_query.Query()  # Outer query.
            oq.columns = ['User', 'Host', 'Marker']
            oq.tables = ['(%s) as innerquery' % innerquery]
            oq.where = ["Host != 'localhost'"]
            oq.order = ['Marker']
            if marker:
                oq.where.append("Marker %s '%s'" %
                                (INCLUDE_MARKER_OPERATORS[include_marker],
                                 marker))
            if limit:
                oq.limit = limit + 1
            t = text(str(oq))
            result = client.execute(t)
            next_marker = None
            LOG.debug("result = " + str(result))
            for count, row in enumerate(result):
                if count >= limit:
                    break
                LOG.debug("user = " + str(row))
                mysql_user = models.MySQLUser()
                mysql_user.name = row['User']
                mysql_user.host = row['Host']
                self._associate_dbs(mysql_user)
                next_marker = row['Marker']
                users.append(mysql_user.serialize())
        if result.rowcount <= limit:
            next_marker = None
        LOG.debug("users = " + str(users))

        return users, next_marker

    def revoke_access(self, username, hostname, database):
        """Revoke a user's permission to use a given database."""
        user = self._get_user(username, hostname)
        with LocalSqlClient(get_engine()) as client:
            r = sql_query.Revoke(database=database,
                                 user=user.name,
                                 host=user.host)
            t = text(str(r))
            client.execute(t)

    def list_access(self, username, hostname):
        """Show all the databases to which the user has more than
           USAGE granted.
        """
        user = self._get_user(username, hostname)
        return user.databases


class KeepAliveConnection(interfaces.PoolListener):
    """
    A connection pool listener that ensures live connections are returned
    from the connection pool at checkout. This alleviates the problem of
    MySQL connections timing out.
    """

    def checkout(self, dbapi_con, con_record, con_proxy):
        """Event triggered when a connection is checked out from the pool."""
        try:
            try:
                dbapi_con.ping(False)
            except TypeError:
                dbapi_con.ping()
        except dbapi_con.OperationalError as ex:
            if ex.args[0] in (2006, 2013, 2014, 2045, 2055):
                raise exc.DisconnectionError()
            else:
                raise


class MySqlApp(object):
    """Prepares DBaaS on a Guest container."""

    TIME_OUT = 1000

    def __init__(self, status):
        """By default login with root no password for initial setup."""
        self.state_change_wait_time = CONF.state_change_wait_time
        self.status = status

    def _create_admin_user(self, client, password):
        """
        Create a os_admin user with a random password
        with all privileges similar to the root user.
        """
        localhost = "localhost"
        g = sql_query.Grant(permissions='ALL', user=ADMIN_USER_NAME,
                            host=localhost, grant_option=True, clear=password)
        t = text(str(g))
        client.execute(t)

    @staticmethod
    def _generate_root_password(client):
        """Generate and set a random root password and forget about it."""
        localhost = "localhost"
        uu = sql_query.UpdateUser("root", host=localhost,
                                  clear=utils.generate_random_password())
        t = text(str(uu))
        client.execute(t)

    def install_if_needed(self, packages):
        """Prepare the guest machine with a secure
           mysql server installation.
        """
        LOG.info(_("Preparing Guest as MySQL Server"))
        if not packager.pkg_is_installed(packages):
            LOG.debug("Installing mysql server")
            self._clear_mysql_config()
            # set blank password on pkg configuration stage
            pkg_opts = {'root_password': '',
                        'root_password_again': ''}
            packager.pkg_install(packages, pkg_opts, self.TIME_OUT)
            self._create_mysql_confd_dir()
            LOG.debug("Finished installing mysql server")
        self.start_mysql()
        LOG.info(_("Dbaas install_if_needed complete"))

    def complete_install_or_restart(self):
        self.status.end_install_or_restart()

    def secure(self, config_contents, overrides):
        LOG.info(_("Generating admin password..."))
        admin_password = utils.generate_random_password()
        clear_expired_password()
        engine = sqlalchemy.create_engine("mysql://root:@localhost:3306",
                                          echo=True)
        with LocalSqlClient(engine) as client:
            self._remove_anonymous_user(client)
            self._create_admin_user(client, admin_password)

        self.stop_db()
        self._write_mycnf(admin_password, config_contents, overrides)
        self.start_mysql()

        LOG.info(_("Dbaas secure complete."))

    def secure_root(self, secure_remote_root=True):
        with LocalSqlClient(get_engine()) as client:
            LOG.info(_("Preserving root access from restore"))
            self._generate_root_password(client)
            if secure_remote_root:
                self._remove_remote_root_access(client)

    def _clear_mysql_config(self):
        """Clear old configs, which can be incompatible with new version."""
        LOG.debug("Clearing old mysql config")
        random_uuid = str(uuid.uuid4())
        configs = ["/etc/my.cnf", "/etc/mysql/conf.d", "/etc/mysql/my.cnf"]
        for config in configs:
            command = "mv %s %s_%s" % (config, config, random_uuid)
            try:
                utils.execute_with_timeout(command, shell=True,
                                           root_helper="sudo")
                LOG.debug("%s saved to %s_%s" % (config, config, random_uuid))
            except exception.ProcessExecutionError:
                pass

    def _create_mysql_confd_dir(self):
        conf_dir = "/etc/mysql/conf.d"
        LOG.debug("Creating %s" % conf_dir)
        command = "sudo mkdir -p %s" % conf_dir
        utils.execute_with_timeout(command, shell=True)

    def _enable_mysql_on_boot(self):
        LOG.info("Enabling mysql on boot.")
        try:
            mysql_service = operating_system.service_discovery(
                MYSQL_SERVICE_CANDIDATES)
            utils.execute_with_timeout(mysql_service['cmd_enable'], shell=True)
        except KeyError:
            raise RuntimeError("Service is not discovered.")

    def _disable_mysql_on_boot(self):
        try:
            mysql_service = operating_system.service_discovery(
                MYSQL_SERVICE_CANDIDATES)
            utils.execute_with_timeout(mysql_service['cmd_disable'],
                                       shell=True)
        except KeyError:
            raise RuntimeError("Service is not discovered.")

    def stop_db(self, update_db=False, do_not_start_on_reboot=False):
        LOG.info(_("Stopping mysql..."))
        if do_not_start_on_reboot:
            self._disable_mysql_on_boot()
        try:
            mysql_service = operating_system.service_discovery(
                MYSQL_SERVICE_CANDIDATES)
            utils.execute_with_timeout(mysql_service['cmd_stop'], shell=True)
        except KeyError:
            raise RuntimeError("Service is not discovered.")
        if not self.status.wait_for_real_status_to_change_to(
                rd_instance.ServiceStatuses.SHUTDOWN,
                self.state_change_wait_time, update_db):
            LOG.error(_("Could not stop MySQL!"))
            self.status.end_install_or_restart()
            raise RuntimeError("Could not stop MySQL!")

    def _remove_anonymous_user(self, client):
        t = text(sql_query.REMOVE_ANON)
        client.execute(t)

    def _remove_remote_root_access(self, client):
        t = text(sql_query.REMOVE_ROOT)
        client.execute(t)

    def restart(self):
        try:
            self.status.begin_restart()
            self.stop_db()
            self.start_mysql()
        finally:
            self.status.end_install_or_restart()

    def update_overrides(self, overrides_file, remove=False):
        """
        This function will either update or remove the MySQL overrides.cnf file
        If remove is set to True the function will remove the overrides file.

        :param overrides:
        :param remove:
        :return:
        """

        if overrides_file:
            LOG.debug("writing new overrides.cnf config file")
            self._write_config_overrides(overrides_file)
        if remove:
            LOG.debug("removing overrides.cnf config file")
            self._remove_overrides()

    def apply_overrides(self, overrides):
        LOG.debug("applying overrides to mysql")
        with LocalSqlClient(get_engine()) as client:
            LOG.debug("updating overrides values in running daemon")
            for k, v in overrides.iteritems():
                q = sql_query.SetServerVariable(key=k, value=v)
                t = text(str(q))
                try:
                    client.execute(t)
                except exc.OperationalError:
                    output = {'key': k, 'value': v}
                    LOG.exception(_("Unable to set %(key)s with value "
                                    "%(value)s") % output)

    def _replace_mycnf_with_template(self, template_path, original_path):
        LOG.debug("replacing the mycnf with template")
        LOG.debug("template_path(%(template)s) original_path(%(origin)s)"
                  % {"template": template_path, "origin": original_path})
        if os.path.isfile(template_path):
            if os.path.isfile(original_path):
                utils.execute_with_timeout(
                    "sudo", "mv", original_path,
                    "%(name)s.%(date)s" %
                    {'name': original_path, 'date':
                        date.today().isoformat()})
            utils.execute_with_timeout("sudo", "cp", template_path,
                                       original_path)

    def _write_temp_mycnf_with_admin_account(self, original_file_path,
                                             temp_file_path, password):
        mycnf_file = open(original_file_path, 'r')
        tmp_file = open(temp_file_path, 'w')
        for line in mycnf_file:
            tmp_file.write(line)
            if "[client]" in line:
                tmp_file.write("user\t\t= %s\n" % ADMIN_USER_NAME)
                tmp_file.write("password\t= %s\n" % password)
        mycnf_file.close()
        tmp_file.close()

    def wipe_ib_logfiles(self):
        """Destroys the iblogfiles.

        If for some reason the selected log size in the conf changes from the
        current size of the files MySQL will fail to start, so we delete the
        files to be safe.
        """
        LOG.info(_("Wiping ib_logfiles..."))
        for index in range(2):
            try:
                (utils.
                 execute_with_timeout("sudo", "rm", "%s/ib_logfile%d"
                                                    % (MYSQL_BASE_DIR, index)))
            except exception.ProcessExecutionError as pe:
                # On restarts, sometimes these are wiped. So it can be a race
                # to have MySQL start up before it's restarted and these have
                # to be deleted. That's why its ok if they aren't found.
                LOG.error("Could not delete logfile!")
                LOG.error(pe)
                if "No such file or directory" not in str(pe):
                    raise

    def _write_mycnf(self, admin_password, config_contents, overrides=None):
        """
        Install the set of mysql my.cnf templates.
        Update the os_admin user and password to the my.cnf
        file for direct login from localhost.
        """
        LOG.info(_("Writing my.cnf templates."))
        if admin_password is None:
            admin_password = get_auth_password()

        with open(TMP_MYCNF, 'w') as t:
            t.write(config_contents)
        utils.execute_with_timeout("sudo", "mv", TMP_MYCNF,
                                   MYSQL_CONFIG)

        self._write_temp_mycnf_with_admin_account(MYSQL_CONFIG,
                                                  TMP_MYCNF,
                                                  admin_password)
        utils.execute_with_timeout("sudo", "mv", TMP_MYCNF,
                                   MYSQL_CONFIG)

        self.wipe_ib_logfiles()

        # write configuration file overrides
        if overrides:
            self._write_config_overrides(overrides)

    def _write_config_overrides(self, overrideValues):
        LOG.info(_("Writing new temp overrides.cnf file."))

        with open(MYCNF_OVERRIDES_TMP, 'w') as overrides:
            overrides.write(overrideValues)
        LOG.info(_("Moving overrides.cnf into correct location."))
        utils.execute_with_timeout("sudo", "mv", MYCNF_OVERRIDES_TMP,
                                   MYCNF_OVERRIDES)

        LOG.info(_("Setting permissions on overrides.cnf"))
        utils.execute_with_timeout("sudo", "chmod", "0711",
                                   MYCNF_OVERRIDES)

    def _remove_overrides(self):
        LOG.info(_("Removing overrides configuration file"))
        if os.path.exists(MYCNF_OVERRIDES):
            utils.execute_with_timeout("sudo", "rm", MYCNF_OVERRIDES)

    def start_mysql(self, update_db=False):
        LOG.info(_("Starting mysql..."))
        # This is the site of all the trouble in the restart tests.
        # Essentially what happens is that mysql start fails, but does not
        # die. It is then impossible to kill the original, so

        self._enable_mysql_on_boot()

        try:
            mysql_service = operating_system.service_discovery(
                MYSQL_SERVICE_CANDIDATES)
            utils.execute_with_timeout(mysql_service['cmd_start'], shell=True)
        except KeyError:
            raise RuntimeError("Service is not discovered.")
        except exception.ProcessExecutionError:
            # it seems mysql (percona, at least) might come back with [Fail]
            # but actually come up ok. we're looking into the timing issue on
            # parallel, but for now, we'd like to give it one more chance to
            # come up. so regardless of the execute_with_timeout() response,
            # we'll assume mysql comes up and check it's status for a while.
            pass
        if not self.status.wait_for_real_status_to_change_to(
                rd_instance.ServiceStatuses.RUNNING,
                self.state_change_wait_time, update_db):
            LOG.error(_("Start up of MySQL failed!"))
            # If it won't start, but won't die either, kill it by hand so we
            # don't let a rouge process wander around.
            try:
                utils.execute_with_timeout("sudo", "pkill", "-9", "mysql")
            except exception.ProcessExecutionError as p:
                LOG.error("Error killing stalled mysql start command.")
                LOG.error(p)
                # There's nothing more we can do...
            self.status.end_install_or_restart()
            raise RuntimeError("Could not start MySQL!")

    def start_db_with_conf_changes(self, config_contents):
        LOG.info(_("Starting mysql with conf changes..."))
        LOG.info(_("inside the guest - self.status.is_mysql_running(%s)...")
                 % self.status.is_running)
        if self.status.is_running:
            LOG.error(_("Cannot execute start_db_with_conf_changes because "
                        "MySQL state == %s!") % self.status)
            raise RuntimeError("MySQL not stopped.")
        LOG.info(_("Initiating config."))
        self._write_mycnf(None, config_contents)
        self.start_mysql(True)

    def reset_configuration(self, configuration):
        config_contents = configuration['config_contents']
        LOG.info(_("Resetting configuration"))
        self._write_mycnf(None, config_contents)


class MySqlRootAccess(object):
    @classmethod
    def is_root_enabled(cls):
        """Return True if root access is enabled; False otherwise."""
        with LocalSqlClient(get_engine()) as client:
            t = text(sql_query.ROOT_ENABLED)
            result = client.execute(t)
            LOG.debug("Found %s with remote root access" % result.rowcount)
            return result.rowcount != 0

    @classmethod
    def enable_root(cls, root_password=None):
        """Enable the root user global access and/or
           reset the root password.
        """
        user = models.RootUser()
        user.name = "root"
        user.host = "%"
        user.password = root_password or utils.generate_random_password()
        with LocalSqlClient(get_engine()) as client:
            print(client)
            try:
                cu = sql_query.CreateUser(user.name, host=user.host)
                t = text(str(cu))
                client.execute(t, **cu.keyArgs)
            except exc.OperationalError as err:
                # Ignore, user is already created, just reset the password
                # TODO(rnirmal): More fine grained error checking later on
                LOG.debug(err)
        with LocalSqlClient(get_engine()) as client:
            print(client)
            uu = sql_query.UpdateUser(user.name, host=user.host,
                                      clear=user.password)
            t = text(str(uu))
            client.execute(t)

            LOG.debug("CONF.root_grant: %s CONF.root_grant_option: %s" %
                      (CONF.root_grant, CONF.root_grant_option))

            g = sql_query.Grant(permissions=CONF.root_grant,
                                user=user.name,
                                host=user.host,
                                grant_option=CONF.root_grant_option,
                                clear=user.password)

            t = text(str(g))
            client.execute(t)
            return user.serialize()

########NEW FILE########
__FILENAME__ = manager
# Copyright (c) 2013 Rackspace
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from trove.common import cfg
from trove.common import exception
from trove.guestagent import dbaas
from trove.guestagent import volume
from trove.common import instance as rd_instance
from trove.guestagent.common import operating_system
from trove.guestagent.datastore.redis.service import RedisAppStatus
from trove.guestagent.datastore.redis.service import RedisApp
from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _
from trove.openstack.common import periodic_task


LOG = logging.getLogger(__name__)
CONF = cfg.CONF
MANAGER = CONF.datastore_manager


class Manager(periodic_task.PeriodicTasks):
    """
    This is the Redis manager class. It is dynamically loaded
    based off of the service_type of the trove instance
    """

    @periodic_task.periodic_task(ticks_between_runs=3)
    def update_status(self, context):
        """
        Updates the redis trove instance. It is decorated with
        perodic task so it is automatically called every 3 ticks.
        """
        RedisAppStatus.get().update()

    def change_passwords(self, context, users):
        """
        Changes the redis instance password,
        it is currently not not implemented.
        """
        raise exception.DatastoreOperationNotSupported(
            operation='change_passwords', datastore=MANAGER)

    def reset_configuration(self, context, configuration):
        """
        Resets to the default configuration,
        currently this does nothing.
        """
        app = RedisApp(RedisAppStatus.get())
        app.reset_configuration(configuration)

    def _perform_restore(self, backup_info, context, restore_location, app):
        """
        Perform a restore on this instance,
        currently it is not implemented.
        """
        raise exception.DatastoreOperationNotSupported(
            operation='_perform_restore', datastore=MANAGER)

    def prepare(self, context, packages, databases, memory_mb, users,
                device_path=None, mount_point=None, backup_info=None,
                config_contents=None, root_password=None, overrides=None):
        """
        This is called when the trove instance first comes online.
        It is the first rpc message passed from the task manager.
        prepare handles all the base configuration of the redis instance.
        """
        try:
            app = RedisApp(RedisAppStatus.get())
            RedisAppStatus.get().begin_install()
            if device_path:
                device = volume.VolumeDevice(device_path)
                # unmount if device is already mounted
                device.unmount_device(device_path)
                device.format()
                device.mount(mount_point)
                operating_system.update_owner('redis', 'redis', mount_point)
                LOG.debug('Mounted the volume.')
            app.install_if_needed(packages)
            LOG.info(_('Securing redis now.'))
            app.write_config(config_contents)
            app.restart()
            LOG.info(_('"prepare" redis call has finished.'))
        except Exception as e:
            LOG.error(e)
            app.status.set_status(rd_instance.ServiceStatuses.FAILED)
            raise RuntimeError("prepare call has failed.")

    def restart(self, context):
        """
        Restart this redis instance.
        This method is called when the guest agent
        gets a restart message from the taskmanager.
        """
        app = RedisApp(RedisAppStatus.get())
        app.restart()

    def start_db_with_conf_changes(self, context, config_contents):
        """
        Start this redis instance with new conf changes.
        """
        app = RedisApp(RedisAppStatus.get())
        app.start_db_with_conf_changes(config_contents)

    def stop_db(self, context, do_not_start_on_reboot=False):
        """
        Stop this redis instance.
        This method is called when the guest agent
        gets a stop message from the taskmanager.
        """
        app = RedisApp(RedisAppStatus.get())
        app.stop_db(do_not_start_on_reboot=do_not_start_on_reboot)

    def get_filesystem_stats(self, context, fs_path):
        """Gets the filesystem stats for the path given. """
        mount_point = CONF.get(
            'mysql' if not MANAGER else MANAGER).mount_point
        return dbaas.get_filesystem_volume_stats(mount_point)

    def create_backup(self, context, backup_info):
        """
        This will eventually create a backup. Right now
        it does nothing.
        """
        raise exception.DatastoreOperationNotSupported(
            operation='create_backup', datastore=MANAGER)

    def mount_volume(self, context, device_path=None, mount_point=None):
        device = volume.VolumeDevice(device_path)
        device.mount(mount_point, write_to_fstab=False)
        LOG.debug("Mounted the volume.")

    def unmount_volume(self, context, device_path=None, mount_point=None):
        device = volume.VolumeDevice(device_path)
        device.unmount(mount_point)
        LOG.debug("Unmounted the volume.")

    def resize_fs(self, context, device_path=None, mount_point=None):
        device = volume.VolumeDevice(device_path)
        device.resize_fs(mount_point)
        LOG.debug("Resized the filesystem")

    def update_overrides(self, context, overrides, remove=False):
        raise exception.DatastoreOperationNotSupported(
            operation='update_overrides', datastore=MANAGER)

    def apply_overrides(self, context, overrides):
        raise exception.DatastoreOperationNotSupported(
            operation='apply_overrides', datastore=MANAGER)

    def update_attributes(self, context, username, hostname, user_attrs):
        raise exception.DatastoreOperationNotSupported(
            operation='update_attributes', datastore=MANAGER)

    def create_database(self, context, databases):
        raise exception.DatastoreOperationNotSupported(
            operation='create_database', datastore=MANAGER)

    def create_user(self, context, users):
        raise exception.DatastoreOperationNotSupported(
            operation='create_user', datastore=MANAGER)

    def delete_database(self, context, database):
        raise exception.DatastoreOperationNotSupported(
            operation='delete_database', datastore=MANAGER)

    def delete_user(self, context, user):
        raise exception.DatastoreOperationNotSupported(
            operation='delete_user', datastore=MANAGER)

    def get_user(self, context, username, hostname):
        raise exception.DatastoreOperationNotSupported(
            operation='get_user', datastore=MANAGER)

    def grant_access(self, context, username, hostname, databases):
        raise exception.DatastoreOperationNotSupported(
            operation='grant_access', datastore=MANAGER)

    def revoke_access(self, context, username, hostname, database):
        raise exception.DatastoreOperationNotSupported(
            operation='revoke_access', datastore=MANAGER)

    def list_access(self, context, username, hostname):
        raise exception.DatastoreOperationNotSupported(
            operation='list_access', datastore=MANAGER)

    def list_databases(self, context, limit=None, marker=None,
                       include_marker=False):
        raise exception.DatastoreOperationNotSupported(
            operation='list_databases', datastore=MANAGER)

    def list_users(self, context, limit=None, marker=None,
                   include_marker=False):
        raise exception.DatastoreOperationNotSupported(
            operation='list_users', datastore=MANAGER)

    def enable_root(self, context):
        raise exception.DatastoreOperationNotSupported(
            operation='enable_root', datastore=MANAGER)

    def is_root_enabled(self, context):
        raise exception.DatastoreOperationNotSupported(
            operation='is_root_enabled', datastore=MANAGER)

########NEW FILE########
__FILENAME__ = service
# Copyright (c) 2013 Rackspace
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import os

from trove.common import cfg
from trove.common import utils as utils
from trove.common import exception
from trove.common import instance as rd_instance
from trove.guestagent import pkg
from trove.guestagent.common import operating_system
from trove.guestagent.datastore import service
from trove.guestagent.datastore.redis import system
from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _

LOG = logging.getLogger(__name__)
TMP_REDIS_CONF = '/tmp/redis.conf.tmp'
TIME_OUT = 1200
CONF = cfg.CONF
packager = pkg.Package()


def _load_redis_options():
    """
    Reads the redis config file for all redis options.
    Right now this does not do any smart parsing and returns only key
    value pairs as a str, str.
    So: 'foo bar baz' becomes {'foo' : 'bar baz'}
    """
    options = {}
    with open(system.REDIS_CONFIG, 'r') as fd:
        for opt in fd.readlines():
            opt = opt.rstrip().split(' ')
            options.update({opt[0]: ' '.join(opt[1:])})
    return options


class RedisAppStatus(service.BaseDbStatus):
    """
    Handles all of the status updating for the redis guest agent.
    """
    @classmethod
    def get(cls):
        """
        Gets an instance of the RedisAppStatus class.
        """
        if not cls._instance:
            cls._instance = RedisAppStatus()
        return cls._instance

    def _get_actual_db_status(self):
        """
        Gets the actual status of the Redis instance
        First it attempts to make a connection to the redis instance
        by making a PING request.
        If PING does not return PONG we do a ps
        to see if the process is blocked or hung.
        This implementation stinks but redis-cli only returns 0
        at this time.
        http://redis.googlecode.com/svn/trunk/redis-cli.c
        If we raise another exception.ProcessExecutionError while
        running ps.
        We attempt to locate the PID file and see if the process
        is crashed or shutdown.
        Remeber by default execute_with_timeout raises this exception
        if a non 0 status code is returned from the cmd called.
        """
        options = _load_redis_options()
        out = ""
        err = ""
        try:
            if 'requirepass' in options:
                LOG.info(_('Password is set running ping with password'))
                out, err = utils.execute_with_timeout(
                    system.REDIS_CLI,
                    '-a',
                    options['requirepass'],
                    'PING',
                    run_as_root=True,
                    root_helper='sudo')
            else:
                LOG.info(_('Password not set running ping without password'))
                out, err = utils.execute_with_timeout(
                    system.REDIS_CLI,
                    'PING',
                    run_as_root=True,
                    root_helper='sudo')
            LOG.info(_('Redis is RUNNING.'))
            return rd_instance.ServiceStatuses.RUNNING
        except exception.ProcessExecutionError:
            LOG.error(_('Process execution error on redis-cli'))
        if 'PONG' not in out:
            try:
                out, err = utils.execute_with_timeout('/bin/ps', '-C',
                                                      'redis-server', 'h')
                pid = out.split()[0]
                msg = _('Redis pid: %s') % (pid)
                LOG.info(msg)
                LOG.info(_('Service Status is BLOCKED.'))
                return rd_instance.ServiceStatuses.BLOCKED
            except exception.ProcessExecutionError:
                pid_file = options.get('pidfile',
                                       '/var/run/redis/redis-server.pid')
                if os.path.exists(pid_file):
                    LOG.info(_('Service Status is CRASHED.'))
                    return rd_instance.ServiceStatuses.CRASHED
                else:
                    LOG.info(_('Service Status is SHUTDOWN.'))
                    return rd_instance.ServiceStatuses.SHUTDOWN


class RedisApp(object):
    """
    Handles installation and configuration of redis
    on a trove instance.
    """

    def __init__(self, status, state_change_wait_time=None):
        """
        Sets default status and state_change_wait_time
        """
        if state_change_wait_time:
            self.state_change_wait_time = state_change_wait_time
        else:
            self.state_change_wait_time = CONF.state_change_wait_time
        self.status = status

    def install_if_needed(self, packages):
        """
        Install redis if needed do nothing if it is already installed.
        """
        LOG.info(_('Preparing Guest as Redis Server'))
        if not packager.pkg_is_installed(packages):
            LOG.info(_('Installing Redis'))
            self._install_redis(packages)
        LOG.info(_('Dbaas install_if_needed complete'))

    def complete_install_or_restart(self):
        """
        finalize status updates for install or restart.
        """
        self.status.end_install_or_restart()

    def _install_redis(self, packages):
        """
        Install the redis server.
        """
        LOG.debug('Installing redis server')
        msg = "Creating %s" % system.REDIS_CONF_DIR
        LOG.debug(msg)
        utils.execute_with_timeout('mkdir',
                                   '-p',
                                   system.REDIS_CONF_DIR,
                                   run_as_root=True,
                                   root_helper='sudo')
        pkg_opts = {}
        packager.pkg_install(packages, pkg_opts, TIME_OUT)
        self.start_redis()
        LOG.debug('Finished installing redis server')

    def _enable_redis_on_boot(self):
        """
        Enables redis on boot.
        """
        LOG.info(_('Enabling Redis on boot.'))
        try:
            redis_service = operating_system.service_discovery(
                system.SERVICE_CANDIDATES)
            utils.execute_with_timeout(
                redis_service['cmd_enable'], shell=True)
        except KeyError:
            raise RuntimeError(_(
                "Command to enable Redis on boot not found."))

    def _disable_redis_on_boot(self):
        """
        Disables redis on boot.
        """
        LOG.info(_("Disabling Redis on boot."))
        try:
            redis_service = operating_system.service_discovery(
                system.SERVICE_CANDIDATES)
            utils.execute_with_timeout(
                redis_service['cmd_disable'], shell=True)
        except KeyError:
            raise RuntimeError(
                "Command to disable Redis on boot not found.")

    def stop_db(self, update_db=False, do_not_start_on_reboot=False):
        """
        Stops the redis application on the trove instance.
        """
        LOG.info(_('Stopping redis...'))
        if do_not_start_on_reboot:
            self._disable_redis_on_boot()
        cmd = 'sudo %s' % (system.REDIS_CMD_STOP)
        utils.execute_with_timeout(cmd,
                                   shell=True)
        if not self.status.wait_for_real_status_to_change_to(
                rd_instance.ServiceStatuses.SHUTDOWN,
                self.state_change_wait_time, update_db):
            LOG.error(_('Could not stop Redis!'))
            self.status.end_install_or_restart()

    def restart(self):
        """
        Restarts the redis daemon.
        """
        try:
            self.status.begin_restart()
            self.stop_db()
            self.start_redis()
        finally:
            self.status.end_install_or_restart()

    def write_config(self, config_contents):
        """
        Write the redis config.
        """
        with open(TMP_REDIS_CONF, 'w') as fd:
            fd.write(config_contents)
        utils.execute_with_timeout('mv',
                                   TMP_REDIS_CONF,
                                   system.REDIS_CONFIG,
                                   run_as_root=True,
                                   root_helper='sudo')

    def start_db_with_conf_changes(self, config_contents):
        LOG.info(_('Starting redis with conf changes...'))
        if self.status.is_running:
            raise RuntimeError('Cannot start_db_with_conf_changes because '
                               'status is %s' % self.status)
        LOG.info(_("Initiating config."))
        self.write_config(config_contents)
        self.start_redis(True)

    def reset_configuration(self, configuration):
        config_contents = configuration['config_contents']
        LOG.info(_("Resetting configuration"))
        self.write_config(config_contents)

    def start_redis(self, update_db=False):
        """
        Start the redis daemon.
        """
        LOG.info(_("Starting redis..."))
        self._enable_redis_on_boot()
        try:
            cmd = 'sudo %s' % (system.REDIS_CMD_START)
            utils.execute_with_timeout(cmd,
                                       shell=True)
        except exception.ProcessExecutionError:
            pass
        if not self.status.wait_for_real_status_to_change_to(
                rd_instance.ServiceStatuses.RUNNING,
                self.state_change_wait_time, update_db):
            LOG.error(_("Start up of redis failed!"))
            try:
                utils.execute_with_timeout('pkill', '-9',
                                           'redis-server',
                                           run_as_root=True,
                                           root_helper='sudo')
            except exception.ProcessExecutionError as p:
                LOG.error('Error killing stalled redis start command.')
                LOG.error(p)
            self.status.end_install_or_restart()

########NEW FILE########
__FILENAME__ = system
# Copyright (c) 2013 Rackspace
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Determines operating system version and os depended commands.
"""
from trove.guestagent.common.operating_system import get_os

OS = get_os()
REDIS_CONFIG = '/etc/redis/redis.conf'
REDIS_CONF_DIR = '/etc/redis'
REDIS_INIT = '/etc/init/redis-server.conf'
REDIS_CLI = '/usr/bin/redis-cli'
REDIS_BIN = '/usr/bin/redis-server'
REDIS_CMD_ENABLE = 'update-rc.d redis-server enable'
REDIS_CMD_DISABLE = 'update-rc.d redis-server disable'
REDIS_CMD_START = 'service redis-server start || /bin/true'
REDIS_CMD_STOP = 'service redis-server stop || /bin/true'
REDIS_PACKAGE = 'redis-server'
SERVICE_CANDIDATES = ['redis-server']

if OS is 'redhat':
    REDIS_BIN = '/usr/libexec/redis-server'
    REDIS_CMD_ENABLE = 'chkconfig redis-server on'
    REDIS_CMD_DISABLE = 'chkconfig redis-server off'
    REDIS_CMD_START = 'service redis-server start'
    REDIS_CMD_STOP = 'service redis-server stop'

########NEW FILE########
__FILENAME__ = service
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


import time

from trove.common import cfg
from trove.common import context
from trove.common import instance as rd_instance
from trove.conductor import api as conductor_api
from trove.guestagent.common import timeutils
from trove.instance import models as rd_models
from trove.openstack.common import log as logging

LOG = logging.getLogger(__name__)
CONF = cfg.CONF


class BaseDbStatus(object):
    """
    Answers the question "what is the status of the DB application on
    this box?" The answer can be that the application is not installed, or
    the state of the application is determined by calling a series of
    commands.

    This class also handles saving and load the status of the DB application
    in the database.
    The status is updated whenever the update() method is called, except
    if the state is changed to building or restart mode using the
     "begin_install" and "begin_restart" methods.
    The building mode persists in the database while restarting mode does
    not (so if there is a Python Pete crash update() will set the status to
    show a failure).
    These modes are exited and functionality to update() returns when
    end_install_or_restart() is called, at which point the status again
    reflects the actual status of the DB app.

    This is a base class, subclasses must implement real logic for
    determining current status of DB in _get_actual_db_status()
    """

    _instance = None

    def __init__(self):
        if self._instance is not None:
            raise RuntimeError("Cannot instantiate twice.")
        self.status = rd_models.InstanceServiceStatus(
            instance_id=CONF.guest_id,
            status=rd_instance.ServiceStatuses.NEW)
        self.restart_mode = False

    def begin_install(self):
        """Called right before DB is prepared."""
        self.set_status(rd_instance.ServiceStatuses.BUILDING)

    def begin_restart(self):
        """Called before restarting DB server."""
        self.restart_mode = True

    def end_install_or_restart(self):
        """Called after DB is installed or restarted.

        Updates the database with the actual DB server status.
        """
        LOG.info("Ending install_if_needed or restart.")
        self.restart_mode = False
        real_status = self._get_actual_db_status()
        LOG.info("Updating status to %s" % real_status)
        self.set_status(real_status)

    def _get_actual_db_status(self):
        raise NotImplementedError()

    @property
    def is_installed(self):
        """
        True if DB app should be installed and attempts to ascertain
        its status won't result in nonsense.
        """
        return (self.status is not None and
                self.status != rd_instance.ServiceStatuses.NEW and
                self.status != rd_instance.ServiceStatuses.BUILDING and
                self.status != rd_instance.ServiceStatuses.FAILED)

    @property
    def _is_restarting(self):
        return self.restart_mode

    @property
    def is_running(self):
        """True if DB server is running."""
        return (self.status is not None and
                self.status == rd_instance.ServiceStatuses.RUNNING)

    def set_status(self, status):
        """Use conductor to update the DB app status."""
        LOG.debug("Casting set_status message to conductor.")
        ctxt = context.TroveContext(user=CONF.nova_proxy_admin_user,
                                    auth_token=CONF.nova_proxy_admin_pass)

        heartbeat = {
            'service_status': status.description,
        }
        conductor_api.API(ctxt).heartbeat(CONF.guest_id,
                                          heartbeat,
                                          sent=timeutils.float_utcnow())
        LOG.debug("Successfully cast set_status.")
        self.status = status

    def update(self):
        """Find and report status of DB on this machine.
        The database is updated and the status is also returned.
        """
        if self.is_installed and not self._is_restarting:
            LOG.info("Determining status of DB server...")
            status = self._get_actual_db_status()
            self.set_status(status)
        else:
            LOG.info("DB server is not installed or is in restart mode, so "
                     "for now we'll skip determining the status of DB on this "
                     "box.")

    def wait_for_real_status_to_change_to(self, status, max_time,
                                          update_db=False):
        """
        Waits the given time for the real status to change to the one
        specified. Does not update the publicly viewable status Unless
        "update_db" is True.
        """
        WAIT_TIME = 3
        waited_time = 0
        while waited_time < max_time:
            time.sleep(WAIT_TIME)
            waited_time += WAIT_TIME
            LOG.info("Waiting for DB status to change to %s..." % status)
            actual_status = self._get_actual_db_status()
            LOG.info("DB status was %s after %d seconds."
                     % (actual_status, waited_time))
            if actual_status == status:
                if update_db:
                    self.set_status(actual_status)
                return True
        LOG.error("Time out while waiting for DB status to change!")
        return False

########NEW FILE########
__FILENAME__ = models
# Copyright (c) 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import re
import string

from trove.common import cfg

CONF = cfg.CONF


class Base(object):
    def serialize(self):
        return self.__dict__

    def deserialize(self, o):
        self.__dict__ = o


class MySQLDatabase(Base):
    """Represents a Database and its properties"""

    _ignore_dbs = CONF.ignore_dbs

    # Defaults
    __charset__ = "utf8"
    __collation__ = "utf8_general_ci"
    dbname = re.compile("^[A-Za-z0-9_-]+[\s\?\#\@]*[A-Za-z0-9_-]+$")

    # Complete list of acceptable values
    charset = {"big5": ["big5_chinese_ci", "big5_bin"],
               "dec8": ["dec8_swedish_ci", "dec8_bin"],
               "cp850": ["cp850_general_ci", "cp850_bin"],
               "hp8": ["hp8_english_ci", "hp8_bin"],
               "koi8r": ["koi8r_general_ci", "koi8r_bin"],
               "latin1": ["latin1_swedish_ci",
                          "latin1_german1_ci",
                          "latin1_danish_ci",
                          "latin1_german2_ci",
                          "latin1_bin",
                          "latin1_general_ci",
                          "latin1_general_cs",
                          "latin1_spanish_ci"],
               "latin2": ["latin2_general_ci",
                          "latin2_czech_cs",
                          "latin2_hungarian_ci",
                          "latin2_croatian_ci",
                          "latin2_bin"],
               "swe7": ["swe7_swedish_ci", "swe7_bin"],
               "ascii": ["ascii_general_ci", "ascii_bin"],
               "ujis": ["ujis_japanese_ci", "ujis_bin"],
               "sjis": ["sjis_japanese_ci", "sjis_bin"],
               "hebrew": ["hebrew_general_ci", "hebrew_bin"],
               "tis620": ["tis620_thai_ci", "tis620_bin"],
               "euckr": ["euckr_korean_ci", "euckr_bin"],
               "koi8u": ["koi8u_general_ci", "koi8u_bin"],
               "gb2312": ["gb2312_chinese_ci", "gb2312_bin"],
               "greek": ["greek_general_ci", "greek_bin"],
               "cp1250": ["cp1250_general_ci",
                          "cp1250_czech_cs",
                          "cp1250_croatian_ci",
                          "cp1250_bin",
                          "cp1250_polish_ci"],
               "gbk": ["gbk_chinese_ci", "gbk_bin"],
               "latin5": ["latin5_turkish_ci", "latin5_bin"],
               "armscii8": ["armscii8_general_ci", "armscii8_bin"],
               "utf8": ["utf8_general_ci",
                        "utf8_bin",
                        "utf8_unicode_ci",
                        "utf8_icelandic_ci",
                        "utf8_latvian_ci",
                        "utf8_romanian_ci",
                        "utf8_slovenian_ci",
                        "utf8_polish_ci",
                        "utf8_estonian_ci",
                        "utf8_spanish_ci",
                        "utf8_swedish_ci",
                        "utf8_turkish_ci",
                        "utf8_czech_ci",
                        "utf8_danish_ci",
                        "utf8_lithuanian_ci",
                        "utf8_slovak_ci",
                        "utf8_spanish2_ci",
                        "utf8_roman_ci",
                        "utf8_persian_ci",
                        "utf8_esperanto_ci",
                        "utf8_hungarian_ci"],
               "ucs2": ["ucs2_general_ci",
                        "ucs2_bin",
                        "ucs2_unicode_ci",
                        "ucs2_icelandic_ci",
                        "ucs2_latvian_ci",
                        "ucs2_romanian_ci",
                        "ucs2_slovenian_ci",
                        "ucs2_polish_ci",
                        "ucs2_estonian_ci",
                        "ucs2_spanish_ci",
                        "ucs2_swedish_ci",
                        "ucs2_turkish_ci",
                        "ucs2_czech_ci",
                        "ucs2_danish_ci",
                        "ucs2_lithuanian_ci",
                        "ucs2_slovak_ci",
                        "ucs2_spanish2_ci",
                        "ucs2_roman_ci",
                        "ucs2_persian_ci",
                        "ucs2_esperanto_ci",
                        "ucs2_hungarian_ci"],
               "cp866": ["cp866_general_ci", "cp866_bin"],
               "keybcs2": ["keybcs2_general_ci", "keybcs2_bin"],
               "macce": ["macce_general_ci", "macce_bin"],
               "macroman": ["macroman_general_ci", "macroman_bin"],
               "cp852": ["cp852_general_ci", "cp852_bin"],
               "latin7": ["latin7_general_ci",
                          "latin7_estonian_cs",
                          "latin7_general_cs",
                          "latin7_bin"],
               "cp1251": ["cp1251_general_ci",
                          "cp1251_bulgarian_ci",
                          "cp1251_ukrainian_ci",
                          "cp1251_bin",
                          "cp1251_general_cs"],
               "cp1256": ["cp1256_general_ci", "cp1256_bin"],
               "cp1257": ["cp1257_general_ci",
                          "cp1257_lithuanian_ci",
                          "cp1257_bin"],
               "binary": ["binary"],
               "geostd8": ["geostd8_general_ci", "geostd8_bin"],
               "cp932": ["cp932_japanese_ci", "cp932_bin"],
               "eucjpms": ["eucjpms_japanese_ci", "eucjpms_bin"]}

    collation = {"big5_chinese_ci": "big5",
                 "big5_bin": "big5",
                 "dec8_swedish_ci": "dec8",
                 "dec8_bin": "dec8",
                 "cp850_general_ci": "cp850",
                 "cp850_bin": "cp850",
                 "hp8_english_ci": "hp8",
                 "hp8_bin": "hp8",
                 "koi8r_general_ci": "koi8r",
                 "koi8r_bin": "koi8r",
                 "latin1_german1_ci": "latin1",
                 "latin1_swedish_ci": "latin1",
                 "latin1_danish_ci": "latin1",
                 "latin1_german2_ci": "latin1",
                 "latin1_bin": "latin1",
                 "latin1_general_ci": "latin1",
                 "latin1_general_cs": "latin1",
                 "latin1_spanish_ci": "latin1",
                 "latin2_czech_cs": "latin2",
                 "latin2_general_ci": "latin2",
                 "latin2_hungarian_ci": "latin2",
                 "latin2_croatian_ci": "latin2",
                 "latin2_bin": "latin2",
                 "swe7_swedish_ci": "swe7",
                 "swe7_bin": "swe7",
                 "ascii_general_ci": "ascii",
                 "ascii_bin": "ascii",
                 "ujis_japanese_ci": "ujis",
                 "ujis_bin": "ujis",
                 "sjis_japanese_ci": "sjis",
                 "sjis_bin": "sjis",
                 "hebrew_general_ci": "hebrew",
                 "hebrew_bin": "hebrew",
                 "tis620_thai_ci": "tis620",
                 "tis620_bin": "tis620",
                 "euckr_korean_ci": "euckr",
                 "euckr_bin": "euckr",
                 "koi8u_general_ci": "koi8u",
                 "koi8u_bin": "koi8u",
                 "gb2312_chinese_ci": "gb2312",
                 "gb2312_bin": "gb2312",
                 "greek_general_ci": "greek",
                 "greek_bin": "greek",
                 "cp1250_general_ci": "cp1250",
                 "cp1250_czech_cs": "cp1250",
                 "cp1250_croatian_ci": "cp1250",
                 "cp1250_bin": "cp1250",
                 "cp1250_polish_ci": "cp1250",
                 "gbk_chinese_ci": "gbk",
                 "gbk_bin": "gbk",
                 "latin5_turkish_ci": "latin5",
                 "latin5_bin": "latin5",
                 "armscii8_general_ci": "armscii8",
                 "armscii8_bin": "armscii8",
                 "utf8_general_ci": "utf8",
                 "utf8_bin": "utf8",
                 "utf8_unicode_ci": "utf8",
                 "utf8_icelandic_ci": "utf8",
                 "utf8_latvian_ci": "utf8",
                 "utf8_romanian_ci": "utf8",
                 "utf8_slovenian_ci": "utf8",
                 "utf8_polish_ci": "utf8",
                 "utf8_estonian_ci": "utf8",
                 "utf8_spanish_ci": "utf8",
                 "utf8_swedish_ci": "utf8",
                 "utf8_turkish_ci": "utf8",
                 "utf8_czech_ci": "utf8",
                 "utf8_danish_ci": "utf8",
                 "utf8_lithuanian_ci": "utf8",
                 "utf8_slovak_ci": "utf8",
                 "utf8_spanish2_ci": "utf8",
                 "utf8_roman_ci": "utf8",
                 "utf8_persian_ci": "utf8",
                 "utf8_esperanto_ci": "utf8",
                 "utf8_hungarian_ci": "utf8",
                 "ucs2_general_ci": "ucs2",
                 "ucs2_bin": "ucs2",
                 "ucs2_unicode_ci": "ucs2",
                 "ucs2_icelandic_ci": "ucs2",
                 "ucs2_latvian_ci": "ucs2",
                 "ucs2_romanian_ci": "ucs2",
                 "ucs2_slovenian_ci": "ucs2",
                 "ucs2_polish_ci": "ucs2",
                 "ucs2_estonian_ci": "ucs2",
                 "ucs2_spanish_ci": "ucs2",
                 "ucs2_swedish_ci": "ucs2",
                 "ucs2_turkish_ci": "ucs2",
                 "ucs2_czech_ci": "ucs2",
                 "ucs2_danish_ci": "ucs2",
                 "ucs2_lithuanian_ci": "ucs2",
                 "ucs2_slovak_ci": "ucs2",
                 "ucs2_spanish2_ci": "ucs2",
                 "ucs2_roman_ci": "ucs2",
                 "ucs2_persian_ci": "ucs2",
                 "ucs2_esperanto_ci": "ucs2",
                 "ucs2_hungarian_ci": "ucs2",
                 "cp866_general_ci": "cp866",
                 "cp866_bin": "cp866",
                 "keybcs2_general_ci": "keybcs2",
                 "keybcs2_bin": "keybcs2",
                 "macce_general_ci": "macce",
                 "macce_bin": "macce",
                 "macroman_general_ci": "macroman",
                 "macroman_bin": "macroman",
                 "cp852_general_ci": "cp852",
                 "cp852_bin": "cp852",
                 "latin7_estonian_cs": "latin7",
                 "latin7_general_ci": "latin7",
                 "latin7_general_cs": "latin7",
                 "latin7_bin": "latin7",
                 "cp1251_bulgarian_ci": "cp1251",
                 "cp1251_ukrainian_ci": "cp1251",
                 "cp1251_bin": "cp1251",
                 "cp1251_general_ci": "cp1251",
                 "cp1251_general_cs": "cp1251",
                 "cp1256_general_ci": "cp1256",
                 "cp1256_bin": "cp1256",
                 "cp1257_lithuanian_ci": "cp1257",
                 "cp1257_bin": "cp1257",
                 "cp1257_general_ci": "cp1257",
                 "binary": "binary",
                 "geostd8_general_ci": "geostd8",
                 "geostd8_bin": "geostd8",
                 "cp932_japanese_ci": "cp932",
                 "cp932_bin": "cp932",
                 "eucjpms_japanese_ci": "eucjpms",
                 "eucjpms_bin": "eucjpms"}

    def __init__(self):
        self._name = None
        self._collate = None
        self._character_set = None

    @property
    def name(self):
        return self._name

    def _is_valid(self, value):
        return value.lower() not in self._ignore_dbs

    @name.setter
    def name(self, value):
        self._name = value

    @property
    def collate(self):
        """Get the appropriate collate value"""
        if not self._collate and not self._character_set:
            return self.__collation__
        elif not self._collate:
            return self.charset[self._character_set][0]
        else:
            return self._collate

    @collate.setter
    def collate(self, value):
        """Validate the collation and set it"""
        if not value:
            pass
        elif self._character_set:
            if value not in self.charset[self._character_set]:
                msg = "'%s' not a valid collation for charset '%s'"
                raise ValueError(msg % (value, self._character_set))
            self._collate = value
        else:
            if value not in self.collation:
                raise ValueError("'%s' not a valid collation" % value)
            self._collate = value
            self._character_set = self.collation[value]

    @property
    def character_set(self):
        """Get the appropriate character set value"""
        if not self._character_set:
            return self.__charset__
        else:
            return self._character_set

    @character_set.setter
    def character_set(self, value):
        """Validate the character set and set it"""
        if not value:
            pass
        elif not value in self.charset:
            raise ValueError("'%s' not a valid character set" % value)
        else:
            self._character_set = value


class ValidatedMySQLDatabase(MySQLDatabase):
    @MySQLDatabase.name.setter
    def name(self, value):
        if any([not value,
                not self._is_valid(value),
                not self.dbname.match(value),
                string.find("%r" % value, "\\") != -1]):
            raise ValueError("'%s' is not a valid database name" % value)
        elif len(value) > 64:
            msg = "Database name '%s' is too long. Max length = 64"
            raise ValueError(msg % value)
        else:
            self._name = value


class MySQLUser(Base):
    """Represents a MySQL User and its associated properties"""

    not_supported_chars = re.compile("^\s|\s$|'|\"|;|`|,|/|\\\\")
    _ignore_users = CONF.ignore_users

    def __init__(self):
        self._name = None
        self._host = None
        self._password = None
        self._databases = []

    def _is_valid(self, value):
        if (not value or
                self.not_supported_chars.search(value) or
                string.find("%r" % value, "\\") != -1):
            return False
        else:
            return True

    def _is_valid_user_name(self, value):
        if (self._is_valid(value) and
                value.lower() not in self._ignore_users):
            return True
        return False

    def _is_valid_host_name(self, value):
        if value in [None, "%"]:
            # % is MySQL shorthand for "everywhere". Always permitted.
            # Null host defaults to % anyway.
            return True
        if CONF.hostname_require_ipv4:
            # Do a little legwork to determine that an address looks legit.

            if value.count('/') > 1:
                # No subnets.
                return False
            octets = value.split('.')
            if len(octets) not in range(1, 5):
                # A, A.B, A.B.C, and A.B.C.D are all valid technically.
                return False
            try:
                octets = [int(octet, 10) for octet in octets if octet != '%']
            except ValueError:
                # If these weren't decimal, there's a problem.
                return False
            return all([(octet >= 0) and (octet <= 255) for octet in octets])

        else:
            # If it wasn't required, anything else goes.
            return True

    @property
    def name(self):
        return self._name

    @name.setter
    def name(self, value):
        if not self._is_valid_user_name(value):
            raise ValueError("'%s' is not a valid user name." % value)
        elif len(value) > 16:
            raise ValueError("User name '%s' is too long. Max length = 16." %
                             value)
        else:
            self._name = value

    @property
    def password(self):
        return self._password

    @password.setter
    def password(self, value):
        if not self._is_valid(value):
            raise ValueError("'%s' is not a valid password." % value)
        else:
            self._password = value

    @property
    def databases(self):
        return self._databases

    @databases.setter
    def databases(self, value):
        mydb = ValidatedMySQLDatabase()
        mydb.name = value
        self._databases.append(mydb.serialize())

    @property
    def host(self):
        if self._host is None:
            return '%'
        return self._host

    @host.setter
    def host(self, value):
        if not self._is_valid_host_name(value):
            raise ValueError("'%s' is not a valid hostname." % value)
        else:
            self._host = value


class RootUser(MySQLUser):
    """Overrides _ignore_users from the MySQLUser class."""

    _ignore_users = []

########NEW FILE########
__FILENAME__ = dbaas
# Copyright (c) 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Handles all processes within the Guest VM, considering it as a Platform

The :py:class:`GuestManager` class is a :py:class:`nova.manager.Manager` that
handles RPC calls relating to Platform specific operations.

**Related Flags**

"""

import os

from trove.openstack.common import log
from itertools import chain
from trove.common import cfg


LOG = log.getLogger(__name__)
defaults = {
    'mysql': 'trove.guestagent.datastore.mysql.manager.Manager',
    'percona': 'trove.guestagent.datastore.mysql.manager.Manager',
    'redis': 'trove.guestagent.datastore.redis.manager.Manager',
    'cassandra': 'trove.guestagent.datastore.cassandra.manager.Manager',
    'couchbase': 'trove.guestagent.datastore.couchbase.manager.Manager',
    'mongodb': 'trove.guestagent.datastore.mongodb.manager.Manager',
}
CONF = cfg.CONF


def get_custom_managers():
    return CONF.datastore_registry_ext


def datastore_registry():
    return dict(chain(defaults.iteritems(),
                get_custom_managers().iteritems()))


def to_gb(bytes):
    if bytes == 0:
        return 0.0
    size = bytes / 1024.0 ** 3
    return round(size, 2)


def get_filesystem_volume_stats(fs_path):
    try:
        stats = os.statvfs(fs_path)
    except OSError:
        LOG.exception("Error getting volume stats.")
        raise RuntimeError("Filesystem not found (%s)" % fs_path)

    total = stats.f_blocks * stats.f_bsize
    free = stats.f_bfree * stats.f_bsize
    # return the size in GB
    used_gb = to_gb(total - free)
    total_gb = to_gb(total)

    output = {
        'block_size': stats.f_bsize,
        'total_blocks': stats.f_blocks,
        'free_blocks': stats.f_bfree,
        'total': total_gb,
        'free': free,
        'used': used_gb
    }
    return output

########NEW FILE########
__FILENAME__ = models
#    Copyright 2011 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from datetime import datetime
from datetime import timedelta

from trove.common import cfg
from trove.common import exception
from trove.common import utils
from trove.db import get_db_api
from trove.db import models as dbmodels
from trove.openstack.common import log as logging

LOG = logging.getLogger(__name__)

CONF = cfg.CONF

AGENT_HEARTBEAT = CONF.agent_heartbeat_time


def persisted_models():
    return {'agent_heartbeats': AgentHeartBeat}


class AgentHeartBeat(dbmodels.DatabaseModelBase):
    """Defines the state of a Guest Agent."""

    _data_fields = ['instance_id', 'updated_at']
    _table_name = 'agent_heartbeats'

    def __init__(self, **kwargs):
        super(AgentHeartBeat, self).__init__(**kwargs)

    @classmethod
    def create(cls, **values):
        values['id'] = utils.generate_uuid()
        heartbeat = cls(**values).save()
        if not heartbeat.is_valid():
            raise exception.InvalidModelError(errors=heartbeat.errors)
        return heartbeat

    def save(self):
        if not self.is_valid():
            raise exception.InvalidModelError(errors=self.errors)
        self['updated_at'] = utils.utcnow()
        LOG.debug("Saving %(name)s: %(dict)s" %
                  {'name': self.__class__.__name__, 'dict': self.__dict__})
        return get_db_api().save(self)

    @staticmethod
    def is_active(agent):
        return (datetime.now() - agent.updated_at <
                timedelta(seconds=AGENT_HEARTBEAT))

########NEW FILE########
__FILENAME__ = pkg
# Copyright (c) 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Manages packages on the Guest VM.
"""
import commands
import re
from tempfile import NamedTemporaryFile

import pexpect

from trove.common import exception
from trove.common import utils
from trove.common.exception import ProcessExecutionError
from trove.guestagent.common import operating_system
from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _


LOG = logging.getLogger(__name__)
OK = 0
RUN_DPKG_FIRST = 1
REINSTALL_FIRST = 2
CONFLICT_REMOVED = 3


class PkgAdminLockError(exception.TroveError):
    pass


class PkgPermissionError(exception.TroveError):
    pass


class PkgPackageStateError(exception.TroveError):
    pass


class PkgNotFoundError(exception.NotFound):
    pass


class PkgTimeout(exception.TroveError):
    pass


class PkgScriptletError(exception.TroveError):
    pass


class PkgDownloadError(exception.TroveError):
    pass


class PkgSignError(exception.TroveError):
    pass


class PkgBrokenError(exception.TroveError):
    pass


class BasePackagerMixin:

    def pexpect_kill_proc(self, child):
        child.delayafterclose = 1
        child.delayafterterminate = 1
        child.close(force=True)

    def pexpect_wait_and_close_proc(self, child):
        child.expect(pexpect.EOF)
        child.close()

    def pexpect_run(self, cmd, output_expects, time_out):
        child = pexpect.spawn(cmd, timeout=time_out)
        try:
            i = child.expect(output_expects)
            match = child.match
            self.pexpect_wait_and_close_proc(child)
        except pexpect.TIMEOUT:
            self.pexpect_kill_proc(child)
            raise PkgTimeout("Process timeout after %i seconds." % time_out)
        return (i, match)


class RedhatPackagerMixin(BasePackagerMixin):

    def _rpm_remove_nodeps(self, package_name):
        """
        Sometimes transaction errors happens, easy way is to remove
        conflicted package without dependencies and hope it will replaced
        by another package
        """
        try:
            utils.execute("rpm", "-e", "--nodeps", package_name,
                          run_as_root=True, root_helper="sudo")
        except ProcessExecutionError:
            LOG.error(_("Error removing conflict %s") % package_name)

    def _install(self, packages, time_out):
        """Attempts to install packages.

        Returns OK if the packages are installed or a result code if a
        recoverable-error occurred.
        Raises an exception if a non-recoverable error or timeout occurs.

        """
        cmd = "sudo yum --color=never -y install %s" % packages
        output_expects = ['\[sudo\] password for .*:',
                          'No package (.*) available.',
                          ('file .* from install of .* conflicts with file'
                           ' from package (.*?)\r\n'),
                          'Error: (.*?) conflicts with .*?\r\n',
                          'Processing Conflict: .* conflicts (.*?)\r\n',
                          '.*scriptlet failed*',
                          'HTTP Error',
                          'No more mirrors to try.',
                          'GPG key retrieval failed:',
                          '.*already installed and latest version',
                          'Updated:',
                          'Installed:']
        LOG.debug("Running package install command: %s" % cmd)
        i, match = self.pexpect_run(cmd, output_expects, time_out)
        if i == 0:
            raise PkgPermissionError("Invalid permissions.")
        elif i == 1:
            raise PkgNotFoundError("Could not find pkg %s" % match.group(1))
        elif i == 2 or i == 3 or i == 4:
            self._rpm_remove_nodeps(match.group(1))
            return CONFLICT_REMOVED
        elif i == 5:
            raise PkgScriptletError("Package scriptlet failed")
        elif i == 6 or i == 7:
            raise PkgDownloadError("Package download problem")
        elif i == 8:
            raise PkgSignError("GPG key retrieval failed")
        return OK

    def _remove(self, package_name, time_out):
        """Removes a package.

        Returns OK if the package is removed successfully or a result code if a
        recoverable-error occurs.
        Raises an exception if a non-recoverable error or timeout occurs.

        """
        cmd = "sudo yum --color=never -y remove %s" % package_name
        output_expects = ['\[sudo\] password for .*:',
                          'No Packages marked for removal',
                          'Removed:']
        i, match = self.pexpect_run(cmd, output_expects, time_out)
        if i == 0:
            raise PkgPermissionError("Invalid permissions.")
        elif i == 1:
            raise PkgNotFoundError("Could not find pkg %s" % package_name)
        return OK

    def pkg_install(self, packages, config_opts, time_out):
        result = self._install(packages, time_out)
        if result != OK:
            while result == CONFLICT_REMOVED:
                result = self._install(packages, time_out)
            if result != OK:
                raise PkgPackageStateError("Cannot install packages.")

    def pkg_is_installed(self, packages):
        pkg_list = packages.split()
        cmd = "rpm -qa"
        p = commands.getstatusoutput(cmd)
        std_out = p[1]
        for pkg in pkg_list:
            found = False
            for line in std_out.split("\n"):
                if line.find(pkg) != -1:
                    found = True
                    break
            if not found:
                return False
        return True

    def pkg_version(self, package_name):
        cmd_list = ["rpm", "-qa", "--qf", "'%{VERSION}-%{RELEASE}\n'",
                    package_name]
        p = commands.getstatusoutput(' '.join(cmd_list))
        # Need to capture the version string
        # check the command output
        std_out = p[1]
        for line in std_out.split("\n"):
            regex = re.compile("[0-9.]+-.*")
            matches = regex.match(line)
            if matches:
                line = matches.group()
                return line
        msg = _("version() saw unexpected output from rpm!")
        LOG.error(msg)

    def pkg_remove(self, package_name, time_out):
        """Removes a package."""
        if self.pkg_version(package_name) is None:
            return
        result = self._remove(package_name, time_out)
        if result != OK:
            raise PkgPackageStateError("Package %s is in a bad state."
                                       % package_name)


class DebianPackagerMixin(BasePackagerMixin):

    def _fix(self, time_out):
        """Sometimes you have to run this command before a
            package will install.
        """
        try:
            utils.execute("dpkg", "--configure", "-a", run_as_root=True,
                          root_helper="sudo")
        except ProcessExecutionError:
            LOG.error(_("Error fixing dpkg"))

    def _fix_package_selections(self, packages, config_opts):
        """
        Sometimes you have to run this command before a package will install.
        This command sets package selections to configure package.
        """
        selections = ""
        for package in packages:
            m = re.match('(.+)=(.+)', package)
            if m:
                package_name = m.group(1)
            else:
                package_name = package
            command = "sudo debconf-show %s" % package_name
            p = commands.getstatusoutput(command)
            std_out = p[1]
            for line in std_out.split("\n"):
                for selection, value in config_opts.items():
                    m = re.match(".* (.*/%s):.*" % selection, line)
                    if m:
                        selections += ("%s %s string '%s'\n" %
                                       (package_name, m.group(1), value))
        if selections:
            with NamedTemporaryFile(delete=False) as f:
                fname = f.name
                f.write(selections)
            utils.execute("debconf-set-selections %s && dpkg --configure -a"
                          % fname, run_as_root=True, root_helper="sudo",
                          shell=True)
            os.remove(fname)

    def _install(self, packages, time_out):
        """Attempts to install packages.

        Returns OK if the packages are installed or a result code if a
        recoverable-error occurred.
        Raises an exception if a non-recoverable error or timeout occurs.

        """
        cmd = "sudo -E DEBIAN_FRONTEND=noninteractive apt-get -y " \
              "--force-yes --allow-unauthenticated -o " \
              "DPkg::options::=--force-confmiss --reinstall " \
              "install %s" % packages
        output_expects = ['.*password*',
                          'E: Unable to locate package (.*)',
                          "Couldn't find package (.*)",
                          "E: Version '.*' for '(.*)' was not found",
                          ("dpkg was interrupted, you must manually run "
                           "'sudo dpkg --configure -a'"),
                          "Unable to lock the administration directory",
                          ("E: Unable to correct problems, you have held "
                           "broken packages."),
                          "Setting up (.*)",
                          "is already the newest version"]
        LOG.debug("Running package install command: %s" % cmd)
        i, match = self.pexpect_run(cmd, output_expects, time_out)
        if i == 0:
            raise PkgPermissionError("Invalid permissions.")
        elif i == 1 or i == 2 or i == 3:
            raise PkgNotFoundError("Could not find apt %s" % match.group(1))
        elif i == 4:
            return RUN_DPKG_FIRST
        elif i == 5:
            raise PkgAdminLockError()
        elif i == 6:
            raise PkgBrokenError()
        return OK

    def _remove(self, package_name, time_out):
        """Removes a package.

        Returns OK if the package is removed successfully or a result code if a
        recoverable-error occurs.
        Raises an exception if a non-recoverable error or timeout occurs.

        """
        cmd = "sudo -E apt-get -y --allow-unauthenticated remove %s" \
              % package_name
        output_expects = ['.*password*',
                          'E: Unable to locate package %s' % package_name,
                          'Package is in a very bad inconsistent state',
                          'Sub-process /usr/bin/dpkg returned an error code',
                          ("dpkg was interrupted, you must manually run "
                           "'sudo dpkg --configure -a'"),
                          "Unable to lock the administration directory",
                          "Removing %s*" % package_name]
        i, match = self.pexpect_run(cmd, output_expects, time_out)
        if i == 0:
            raise PkgPermissionError("Invalid permissions.")
        elif i == 1:
            raise PkgNotFoundError("Could not find pkg %s" % package_name)
        elif i == 2 or i == 3:
            return REINSTALL_FIRST
        elif i == 4:
            return RUN_DPKG_FIRST
        elif i == 5:
            raise PkgAdminLockError()
        return OK

    def pkg_install(self, packages, config_opts, time_out):
        """Installs packages."""
        try:
            utils.execute("apt-get", "update", run_as_root=True,
                          root_helper="sudo")
        except ProcessExecutionError:
            LOG.error(_("Error updating the apt sources"))

        result = self._install(packages, time_out)
        if result != OK:
            if result == RUN_DPKG_FIRST:
                self._fix(time_out)
            result = self._install(packages, time_out)
            if result != OK:
                raise PkgPackageStateError("Packages is in a bad state.")
        # even after successful install, packages can stay unconfigured
        # config_opts - is dict with name/value for questions asked by
        # interactive configure script
        if config_opts:
            self._fix_package_selections(packages, config_opts)

    def pkg_version(self, package_name):
        p = commands.getstatusoutput("apt-cache policy %s" % package_name)
        std_out = p[1]
        for line in std_out.split("\n"):
            m = re.match("\s+Installed: (.*)", line)
            if m:
                version = m.group(1)
                if version == "(none)":
                    version = None
                return version

    def pkg_is_installed(self, packages):
        pkg_list = packages.split()
        for pkg in pkg_list:
            m = re.match('(.+)=(.+)', pkg)
            if m:
                package_name = m.group(1)
                package_version = m.group(2)
            else:
                package_name = pkg
                package_version = None
            installed_version = self.pkg_version(package_name)
            if ((package_version and installed_version == package_version) or
               (installed_version and not package_version)):
                LOG.debug("Package %s already installed." % package_name)
            else:
                return False
        return True

    def pkg_remove(self, package_name, time_out):
        """Removes a package."""
        if self.pkg_version(package_name) is None:
            return
        result = self._remove(package_name, time_out)

        if result != OK:
            if result == REINSTALL_FIRST:
                self._install(package_name, time_out)
            elif result == RUN_DPKG_FIRST:
                self._fix(time_out)
            result = self._remove(package_name, time_out)
            if result != OK:
                raise PkgPackageStateError("Package %s is in a bad state."
                                           % package_name)


class BasePackage(type):

    def __new__(meta, name, bases, dct):
        if operating_system.get_os() == operating_system.REDHAT:
            bases += (RedhatPackagerMixin, )
        else:
            # The default is debian
            bases += (DebianPackagerMixin,)
        return super(BasePackage, meta).__new__(meta, name, bases, dct)


class Package(object):

    __metaclass__ = BasePackage

########NEW FILE########
__FILENAME__ = service
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from trove.openstack.common import log as logging
import routes

from trove.common import wsgi

LOG = logging.getLogger(__name__)


class Controller(wsgi.Controller):
    """Base controller class."""
    pass


class API(wsgi.Router):
    """API"""
    def __init__(self):
        mapper = routes.Mapper()
        super(API, self).__init__(mapper)
        self._instance_router(mapper)

    def _instance_router(self, mapper):
        resource = Controller().create_resource()
        path = "/guests"
        mapper.resource("guest", path, controller=resource)


def app_factory(global_conf, **local_conf):
    return API()

########NEW FILE########
__FILENAME__ = base
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

from trove.guestagent.strategy import Strategy
from trove.openstack.common import log as logging
from trove.common import cfg, utils
from eventlet.green import subprocess
import os
import signal

CONF = cfg.CONF

LOG = logging.getLogger(__name__)


class BackupError(Exception):
    """Error running the Backup Command."""


class UnknownBackupType(Exception):
    """Unknown backup type"""


class BackupRunner(Strategy):
    """Base class for Backup Strategy implementations """
    __strategy_type__ = 'backup_runner'
    __strategy_ns__ = 'trove.guestagent.strategies.backup'

    # The actual system call to run the backup
    cmd = None
    is_zipped = CONF.backup_use_gzip_compression
    is_encrypted = CONF.backup_use_openssl_encryption
    encrypt_key = CONF.backup_aes_cbc_key

    def __init__(self, filename, **kwargs):
        self.base_filename = filename
        self.process = None
        self.pid = None
        kwargs.update({'filename': filename})
        self.command = self.cmd % kwargs
        super(BackupRunner, self).__init__()

    @property
    def backup_type(self):
        return type(self).__name__

    def run(self):
        self.process = subprocess.Popen(self.command, shell=True,
                                        stdout=subprocess.PIPE,
                                        stderr=subprocess.PIPE,
                                        preexec_fn=os.setsid)
        self.pid = self.process.pid

    def __enter__(self):
        """Start up the process"""
        self._run_pre_backup()
        self.run()
        self._run_post_backup()
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        """Clean up everything."""
        if exc_type is not None:
            return False

        if hasattr(self, 'process'):
            try:
                # Send a sigterm to the session leader, so that all
                # child processes are killed and cleaned up on terminate
                # (Ensures zombie processes aren't left around on a FAILURE)
                # https://bugs.launchpad.net/trove/+bug/1253850
                os.killpg(self.process.pid, signal.SIGTERM)
                self.process.terminate()
            except OSError:
                # Already stopped
                pass
            utils.raise_if_process_errored(self.process, BackupError)
            if not self.check_process():
                raise BackupError

        return True

    def metadata(self):
        """Hook for subclasses to store metadata from the backup."""
        return {}

    @property
    def filename(self):
        """Subclasses may overwrite this to declare a format (.tar)"""
        return self.base_filename

    @property
    def manifest(self):
        return "%s%s%s" % (self.filename,
                           self.zip_manifest,
                           self.encrypt_manifest)

    @property
    def zip_cmd(self):
        return ' | gzip' if self.is_zipped else ''

    @property
    def zip_manifest(self):
        return '.gz' if self.is_zipped else ''

    @property
    def encrypt_cmd(self):
        return (' | openssl enc -aes-256-cbc -salt -pass pass:%s' %
                self.encrypt_key) if self.is_encrypted else ''

    @property
    def encrypt_manifest(self):
        return '.enc' if self.is_encrypted else ''

    def check_process(self):
        """Hook for subclasses to check process for errors."""
        return True

    def read(self, chunk_size):
        return self.process.stdout.read(chunk_size)

    def _run_pre_backup(self):
        pass

    def _run_post_backup(self):
        pass

########NEW FILE########
__FILENAME__ = mysql_impl
#    Copyright 2013 Hewlett-Packard Development Company, L.P.
#    Copyright 2014 Mirantis Inc.
#    All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

import re

from trove.guestagent.datastore.mysql.service import ADMIN_USER_NAME
from trove.guestagent.datastore.mysql.service import get_auth_password
from trove.guestagent.strategies.backup import base
from trove.openstack.common import log as logging

LOG = logging.getLogger(__name__)


class MySQLDump(base.BackupRunner):
    """Implementation of Backup Strategy for MySQLDump """
    __strategy_name__ = 'mysqldump'

    @property
    def cmd(self):
        user_and_pass = (
            ' --password=%(password)s -u %(user)s '
            '2>/tmp/mysqldump.log' %
            {'password': get_auth_password(),
             'user': ADMIN_USER_NAME})
        cmd = ('mysqldump'
               ' --all-databases'
               ' %(extra_opts)s'
               ' --opt' + user_and_pass)
        return cmd + self.zip_cmd + self.encrypt_cmd


class InnoBackupEx(base.BackupRunner):
    """Implementation of Backup Strategy for InnoBackupEx """
    __strategy_name__ = 'innobackupex'

    @property
    def cmd(self):
        cmd = ('sudo innobackupex'
               ' --stream=xbstream'
               ' %(extra_opts)s'
               ' /var/lib/mysql 2>/tmp/innobackupex.log'
               )
        return cmd + self.zip_cmd + self.encrypt_cmd

    def check_process(self):
        """Check the output from innobackupex for 'completed OK!'"""
        LOG.debug('Checking innobackupex process output')
        with open('/tmp/innobackupex.log', 'r') as backup_log:
            output = backup_log.read()
            LOG.info(output)
            if not output:
                LOG.error("Innobackupex log file empty")
                return False
            last_line = output.splitlines()[-1].strip()
            if not re.search('completed OK!', last_line):
                LOG.error("Innobackupex did not complete successfully")
                return False

        return True

    def metadata(self):
        LOG.debug('Getting metadata from backup')
        meta = {}
        lsn = re.compile("The latest check point \(for incremental\): '(\d+)'")
        with open('/tmp/innobackupex.log', 'r') as backup_log:
            output = backup_log.read()
            match = lsn.search(output)
            if match:
                meta = {'lsn': match.group(1)}
        LOG.info("Metadata for backup: %s", str(meta))
        return meta

    @property
    def filename(self):
        return '%s.xbstream' % self.base_filename


class InnoBackupExIncremental(InnoBackupEx):
    """InnoBackupEx incremental backup."""

    def __init__(self, *args, **kwargs):
        if not kwargs.get('lsn'):
            raise AttributeError('lsn attribute missing, bad parent?')
        super(InnoBackupExIncremental, self).__init__(*args, **kwargs)
        self.parent_location = kwargs.get('parent_location')
        self.parent_checksum = kwargs.get('parent_checksum')

    @property
    def cmd(self):
        cmd = ('sudo innobackupex'
               ' --stream=xbstream'
               ' --incremental'
               ' --incremental-lsn=%(lsn)s'
               ' %(extra_opts)s'
               ' /var/lib/mysql'
               ' 2>/tmp/innobackupex.log')
        return cmd + self.zip_cmd + self.encrypt_cmd

    def metadata(self):
        _meta = super(InnoBackupExIncremental, self).metadata()
        _meta.update({
            'parent_location': self.parent_location,
            'parent_checksum': self.parent_checksum,
        })
        return _meta

########NEW FILE########
__FILENAME__ = base
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
from trove.guestagent.strategy import Strategy
from trove.common import cfg
from trove.common import exception
from trove.common import utils
from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _  # noqa
from eventlet.green import subprocess

LOG = logging.getLogger(__name__)
CONF = cfg.CONF
CHUNK_SIZE = CONF.backup_chunk_size
BACKUP_USE_GZIP = CONF.backup_use_gzip_compression
BACKUP_USE_OPENSSL = CONF.backup_use_openssl_encryption
BACKUP_DECRYPT_KEY = CONF.backup_aes_cbc_key


def exec_with_root_helper(*cmd):
    try:
        out, err = utils.execute_with_timeout(
            *cmd, run_as_root=True, root_helper="sudo")
        return True
    except exception.ProcessExecutionError:
        return False


class RestoreError(Exception):
    """Error running the Backup Command."""


class RestoreRunner(Strategy):
    """Base class for Restore Strategy implementations """
    """Restore a database from a previous backup."""

    __strategy_type__ = 'restore_runner'
    __strategy_ns__ = 'trove.guestagent.strategies.restore'

    # The actual system calls to run the restore and prepare
    restore_cmd = None

    # The backup format type
    restore_type = None

    # Decryption Parameters
    is_zipped = BACKUP_USE_GZIP
    is_encrypted = BACKUP_USE_OPENSSL
    decrypt_key = BACKUP_DECRYPT_KEY

    def __init__(self, storage, **kwargs):
        self.storage = storage
        self.location = kwargs.pop('location')
        self.checksum = kwargs.pop('checksum')
        self.restore_location = kwargs.get('restore_location',
                                           '/var/lib/mysql')
        self.restore_cmd = (self.decrypt_cmd +
                            self.unzip_cmd +
                            (self.base_restore_cmd % kwargs))
        super(RestoreRunner, self).__init__()

    def pre_restore(self):
        """Hook that is called before the restore command"""
        pass

    def post_restore(self):
        """Hook that is called after the restore command"""
        pass

    def restore(self):
        self.pre_restore()
        content_length = self._run_restore()
        self.post_restore()
        return content_length

    def _run_restore(self):
        return self._unpack(self.location, self.checksum, self.restore_cmd)

    def _unpack(self, location, checksum, command):
        stream = self.storage.load(location, checksum)
        process = subprocess.Popen(command, shell=True,
                                   stdin=subprocess.PIPE,
                                   stderr=subprocess.PIPE)
        content_length = 0
        for chunk in stream:
            process.stdin.write(chunk)
            content_length += len(chunk)
        process.stdin.close()
        utils.raise_if_process_errored(process, RestoreError)
        LOG.info(_("Restored %s bytes from stream.") % content_length)

        return content_length

    @property
    def decrypt_cmd(self):
        if self.is_encrypted:
            return ('openssl enc -d -aes-256-cbc -salt -pass pass:%s | '
                    % self.decrypt_key)
        else:
            return ''

    @property
    def unzip_cmd(self):
        return 'gzip -d -c | ' if self.is_zipped else ''

########NEW FILE########
__FILENAME__ = mysql_impl
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

import glob
import os
import pexpect
import tempfile

from trove.guestagent.strategies.restore import base
from trove.openstack.common import log as logging
from trove.common import exception
from trove.common import utils
import trove.guestagent.datastore.mysql.service as dbaas
from trove.openstack.common.gettextutils import _  # noqa

LOG = logging.getLogger(__name__)


class MySQLRestoreMixin(object):
    """Common utils for restoring MySQL databases"""
    RESET_ROOT_RETRY_TIMEOUT = 100
    RESET_ROOT_SLEEP_INTERVAL = 10
    RESET_ROOT_MYSQL_COMMAND = ("SET PASSWORD FOR"
                                "'root'@'localhost'=PASSWORD('');")

    def mysql_is_running(self):
        if base.exec_with_root_helper("/usr/bin/mysqladmin", "ping"):
            LOG.info(_("The mysqld daemon is up and running."))
            return True
        else:
            LOG.info(_("The mysqld daemon is not running."))
            return False

    def mysql_is_not_running(self):
        if base.exec_with_root_helper("/usr/bin/pgrep", "mysqld"):
            LOG.info(_("The mysqld daemon is still running."))
            return False
        else:
            LOG.info(_("The mysqld daemon is not running."))
            return True

    def poll_until_then_raise(self, event, exc):
        try:
            utils.poll_until(event,
                             sleep_time=self.RESET_ROOT_SLEEP_INTERVAL,
                             time_out=self.RESET_ROOT_RETRY_TIMEOUT)
        except exception.PollTimeOut:
            raise exc

    def _spawn_with_init_file(self, temp_file):
        child = pexpect.spawn("sudo mysqld_safe --init-file=%s" %
                              temp_file.name)
        try:
            i = child.expect(['Starting mysqld daemon'])
            if i == 0:
                LOG.info(_("Starting mysqld daemon"))
        except pexpect.TIMEOUT:
            LOG.exception(_("wait_and_close_proc failed"))
        finally:
            # There is a race condition here where we kill mysqld before
            # the init file been executed. We need to ensure mysqld is up.
            self.poll_until_then_raise(
                self.mysql_is_running,
                base.RestoreError("Reset root password failed: "
                                  "mysqld did not start!"))
            LOG.info(_("Root password reset successfully!"))
            LOG.info(_("Cleaning up the temp mysqld process..."))
            child.delayafterclose = 1
            child.delayafterterminate = 1
            child.close(force=True)
            utils.execute_with_timeout("sudo", "killall", "mysqld")
            self.poll_until_then_raise(
                self.mysql_is_not_running,
                base.RestoreError("Reset root password failed: "
                                  "mysqld did not stop!"))

    def reset_root_password(self):
        #Create temp file with reset root password
        with tempfile.NamedTemporaryFile() as fp:
            fp.write(self.RESET_ROOT_MYSQL_COMMAND)
            fp.flush()
            utils.execute_with_timeout("sudo", "chmod", "a+r", fp.name)
            self._spawn_with_init_file(fp)


class MySQLDump(base.RestoreRunner, MySQLRestoreMixin):
    """Implementation of Restore Strategy for MySQLDump"""
    __strategy_name__ = 'mysqldump'
    base_restore_cmd = 'sudo mysql'


class InnoBackupEx(base.RestoreRunner, MySQLRestoreMixin):
    """Implementation of Restore Strategy for InnoBackupEx"""
    __strategy_name__ = 'innobackupex'
    base_restore_cmd = 'sudo xbstream -x -C %(restore_location)s'
    base_prepare_cmd = ('sudo innobackupex --apply-log %(restore_location)s'
                        ' --defaults-file=%(restore_location)s/backup-my.cnf'
                        ' --ibbackup xtrabackup 2>/tmp/innoprepare.log')

    def __init__(self, *args, **kwargs):
        super(InnoBackupEx, self).__init__(*args, **kwargs)
        self.prepare_cmd = self.base_prepare_cmd % kwargs
        self.prep_retcode = None

    def pre_restore(self):
        app = dbaas.MySqlApp(dbaas.MySqlAppStatus.get())
        app.stop_db()
        LOG.info(_("Cleaning out restore location: %s"), self.restore_location)
        utils.execute_with_timeout("chmod", "-R", "0777",
                                   self.restore_location,
                                   root_helper="sudo",
                                   run_as_root=True)
        utils.clean_out(self.restore_location)

    def _run_prepare(self):
        LOG.info(_("Running innobackupex prepare: %s"), self.prepare_cmd)
        self.prep_retcode = utils.execute(self.prepare_cmd, shell=True)
        LOG.info(_("Innobackupex prepare finished successfully"))

    def post_restore(self):
        self._run_prepare()
        utils.execute_with_timeout("chown", "-R", "-f", "mysql",
                                   self.restore_location,
                                   root_helper="sudo",
                                   run_as_root=True)
        self._delete_old_binlogs()
        self.reset_root_password()
        app = dbaas.MySqlApp(dbaas.MySqlAppStatus.get())
        app.start_mysql()

    def _delete_old_binlogs(self):
        files = glob.glob(os.path.join(self.restore_location, "ib_logfile*"))
        for f in files:
            os.unlink(f)


class InnoBackupExIncremental(InnoBackupEx):
    __strategy_name__ = 'innobackupexincremental'
    incremental_prep = ('sudo innobackupex'
                        ' --apply-log'
                        ' --redo-only'
                        ' %(restore_location)s'
                        ' --defaults-file=%(restore_location)s/backup-my.cnf'
                        ' --ibbackup xtrabackup'
                        ' %(incremental_args)s'
                        ' 2>/tmp/innoprepare.log')

    def __init__(self, *args, **kwargs):
        super(InnoBackupExIncremental, self).__init__(*args, **kwargs)
        self.restore_location = kwargs.get('restore_location')
        self.content_length = 0

    def _incremental_restore_cmd(self, incremental_dir):
        """Return a command for a restore with a incremental location."""
        args = {'restore_location': incremental_dir}
        return (self.decrypt_cmd +
                self.unzip_cmd +
                (self.base_restore_cmd % args))

    def _incremental_prepare_cmd(self, incremental_dir):
        if incremental_dir is not None:
            incremental_arg = '--incremental-dir=%s' % incremental_dir
        else:
            incremental_arg = ''

        args = {
            'restore_location': self.restore_location,
            'incremental_args': incremental_arg,
        }

        return self.incremental_prep % args

    def _incremental_prepare(self, incremental_dir):
        prepare_cmd = self._incremental_prepare_cmd(incremental_dir)
        LOG.info(_("Running innobackupex prepare: %s"), prepare_cmd)
        utils.execute(prepare_cmd, shell=True)
        LOG.info(_("Innobackupex prepare finished successfully"))

    def _incremental_restore(self, location, checksum):
        """Recursively apply backups from all parents.

        If we are the parent then we restore to the restore_location and
        we apply the logs to the restore_location only.

        Otherwise if we are an incremental we restore to a subfolder to
        prevent stomping on the full restore data. Then we run apply log
        with the '--incremental-dir' flag
        """
        metadata = self.storage.load_metadata(location, checksum)
        incremental_dir = None
        if 'parent_location' in metadata:
            LOG.info(_("Restoring parent: %(parent_location)s"
                       " checksum: %(parent_checksum)s") % metadata)
            parent_location = metadata['parent_location']
            parent_checksum = metadata['parent_checksum']
            # Restore parents recursively so backup are applied sequentially
            self._incremental_restore(parent_location, parent_checksum)
            # for *this* backup set the incremental_dir
            # just use the checksum for the incremental path as it is
            # sufficently unique /var/lib/mysql/<checksum>
            incremental_dir = os.path.join(self.restore_location, checksum)
            utils.execute("mkdir", "-p", incremental_dir,
                          root_helper="sudo",
                          run_as_root=True)
            command = self._incremental_restore_cmd(incremental_dir)
        else:
            # The parent (full backup) use the same command from InnobackupEx
            # super class and do not set an incremental_dir.
            command = self.restore_cmd

        self.content_length += self._unpack(location, checksum, command)
        self._incremental_prepare(incremental_dir)

    def _run_restore(self):
        """Run incremental restore.

        First grab all parents and prepare them with '--redo-only'. After
        all backups are restored the super class InnoBackupEx post_restore
        method is called to do the final prepare with '--apply-log'
        """
        self._incremental_restore(self.location, self.checksum)
        return self.content_length

########NEW FILE########
__FILENAME__ = base
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

import abc
from trove.guestagent.strategy import Strategy


class Storage(Strategy):
    """Base class for Storage Strategy implementation """
    __strategy_type__ = 'storage'
    __strategy_ns__ = 'trove.guestagent.strategies.storage'

    def __init__(self, context):
        self.context = context
        super(Storage, self).__init__()

    @abc.abstractmethod
    def save(self, filename, stream):
        """Persist information from the stream """

    @abc.abstractmethod
    def load(self, location, backup_checksum):
        """Load a stream from a persisted storage location  """

    @abc.abstractmethod
    def load_metadata(self, location, backup_checksum):
        """Load metadata for a persisted object."""

    @abc.abstractmethod
    def save_metadata(self, location, metadata={}):
        """Save metadata for a persisted object."""

########NEW FILE########
__FILENAME__ = swift
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

import hashlib

from trove.guestagent.strategies.storage import base
from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _  # noqa
from trove.common.remote import create_swift_client
from trove.common import cfg

LOG = logging.getLogger(__name__)
CONF = cfg.CONF

CHUNK_SIZE = CONF.backup_chunk_size
MAX_FILE_SIZE = CONF.backup_segment_max_size
BACKUP_CONTAINER = CONF.backup_swift_container


class DownloadError(Exception):
    """Error running the Swift Download Command."""


class SwiftDownloadIntegrityError(Exception):
    """Integrity error while running the Swift Download Command."""


class StreamReader(object):
    """Wrap the stream from the backup process and chunk it into segements."""

    def __init__(self, stream, filename, max_file_size=MAX_FILE_SIZE):
        self.stream = stream
        self.filename = filename
        self.container = BACKUP_CONTAINER
        self.max_file_size = max_file_size
        self.segment_length = 0
        self.process = None
        self.file_number = 0
        self.end_of_file = False
        self.end_of_segment = False
        self.segment_checksum = hashlib.md5()

    @property
    def base_filename(self):
        """Filename with extensions removed."""
        return self.filename.split('.')[0]

    @property
    def segment(self):
        return '%s_%08d' % (self.base_filename, self.file_number)

    @property
    def prefix(self):
        return '%s/%s_' % (self.container, self.base_filename)

    def read(self, chunk_size=CHUNK_SIZE):
        if self.end_of_segment:
            self.segment_length = 0
            self.segment_checksum = hashlib.md5()
            self.end_of_segment = False

        # Upload to a new file if we are starting or too large
        if self.segment_length > (self.max_file_size - chunk_size):
            self.file_number += 1
            self.end_of_segment = True
            return ''

        chunk = self.stream.read(chunk_size)
        if not chunk:
            self.end_of_file = True
            return ''

        self.segment_checksum.update(chunk)
        self.segment_length += len(chunk)
        return chunk


class SwiftStorage(base.Storage):
    """Implementation of Storage Strategy for Swift """
    __strategy_name__ = 'swift'

    def __init__(self, *args, **kwargs):
        super(SwiftStorage, self).__init__(*args, **kwargs)
        self.connection = create_swift_client(self.context)

    def save(self, filename, stream):
        """Persist information from the stream to swift.

        The file is saved to the location <BACKUP_CONTAINER>/<filename>.
        The filename is defined on the backup runner manifest property
        which is typically in the format '<backup_id>.<ext>.gz'
        """

        # Create the container if it doesn't already exist
        self.connection.put_container(BACKUP_CONTAINER)

        # Swift Checksum is the checksum of the concatenated segment checksums
        swift_checksum = hashlib.md5()

        # Wrap the output of the backup process to segment it for swift
        stream_reader = StreamReader(stream, filename)

        url = self.connection.url
        # Full location where the backup manifest is stored
        location = "%s/%s/%s" % (url, BACKUP_CONTAINER, filename)

        # Read from the stream and write to the container in swift
        while not stream_reader.end_of_file:
            etag = self.connection.put_object(BACKUP_CONTAINER,
                                              stream_reader.segment,
                                              stream_reader)

            segment_checksum = stream_reader.segment_checksum.hexdigest()

            # Check each segment MD5 hash against swift etag
            # Raise an error and mark backup as failed
            if etag != segment_checksum:
                LOG.error("Error saving data segment to swift. "
                          "ETAG: %s Segment MD5: %s",
                          etag, segment_checksum)
                return False, "Error saving data to Swift!", None, location

            swift_checksum.update(segment_checksum)

        # Create the manifest file
        # We create the manifest file after all the segments have been uploaded
        # so a partial swift object file can't be downloaded; if the manifest
        # file exists then all segments have been uploaded so the whole backup
        # file can be downloaded.
        headers = {'X-Object-Manifest': stream_reader.prefix}
        # The etag returned from the manifest PUT is the checksum of the
        # manifest object (which is empty); this is not the checksum we want
        self.connection.put_object(BACKUP_CONTAINER,
                                   filename,
                                   contents='',
                                   headers=headers)

        resp = self.connection.head_object(BACKUP_CONTAINER, filename)
        # swift returns etag in double quotes
        # e.g. '"dc3b0827f276d8d78312992cc60c2c3f"'
        etag = resp['etag'].strip('"')

        # Check the checksum of the concatenated segment checksums against
        # swift manifest etag.
        # Raise an error and mark backup as failed
        final_swift_checksum = swift_checksum.hexdigest()
        if etag != final_swift_checksum:
            LOG.error(
                "Error saving data to swift. Manifest ETAG: %s Swift MD5: %s",
                etag, final_swift_checksum)
            return False, "Error saving data to Swift!", None, location

        return (True, "Successfully saved data to Swift!",
                final_swift_checksum, location)

    def _explodeLocation(self, location):
        storage_url = "/".join(location.split('/')[:-2])
        container = location.split('/')[-2]
        filename = location.split('/')[-1]
        return storage_url, container, filename

    def _verify_checksum(self, etag, checksum):
        etag_checksum = etag.strip('"')
        if etag_checksum != checksum:
            msg = ("Original checksum: %(original)s does not match"
                   " the current checksum: %(current)s" %
                   {'original': etag_checksum, 'current': checksum})
            LOG.error(msg)
            raise SwiftDownloadIntegrityError(msg)
        return True

    def load(self, location, backup_checksum):
        """Restore a backup from the input stream to the restore_location"""
        storage_url, container, filename = self._explodeLocation(location)

        headers, info = self.connection.get_object(container, filename,
                                                   resp_chunk_size=CHUNK_SIZE)

        if CONF.verify_swift_checksum_on_restore:
            self._verify_checksum(headers.get('etag', ''), backup_checksum)

        return info

    def _get_attr(self, original):
        """Get a friendly name from an object header key"""
        key = original.replace('-', '_')
        key = key.replace('x_object_meta_', '')
        return key

    def _set_attr(self, original):
        """Return a swift friendly header key"""
        key = original.replace('_', '-')
        return 'X-Object-Meta-%s' % key

    def load_metadata(self, location, backup_checksum):
        """Load metadata from swift."""

        storage_url, container, filename = self._explodeLocation(location)

        headers = self.connection.head_object(container, filename)

        if CONF.verify_swift_checksum_on_restore:
            self._verify_checksum(headers.get('etag', ''), backup_checksum)

        _meta = {}
        for key, value in headers.iteritems():
            if key.startswith('x-object-meta'):
                _meta[self._get_attr(key)] = value

        return _meta

    def save_metadata(self, location, metadata={}):
        """Save metadata to a swift object."""

        storage_url, container, filename = self._explodeLocation(location)

        _headers = self.connection.head_object(container, filename)
        headers = {'X-Object-Manifest': _headers.get('x-object-manifest')}
        for key, value in metadata.iteritems():
            headers[self._set_attr(key)] = value

        LOG.info(_("Writing metadata: %s"), str(headers))
        self.connection.post_object(container, filename, headers=headers)

########NEW FILE########
__FILENAME__ = strategy
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

import abc
from trove.common import utils
from trove.openstack.common import log as logging


LOG = logging.getLogger(__name__)


class Strategy(object):
    __metaclass__ = abc.ABCMeta

    __strategy_ns__ = None

    __strategy_name__ = None
    __strategy_type__ = None

    def __init__(self):
        self.name = self.get_canonical_name()
        LOG.debug("Loaded strategy %s", self.name)

    def is_enabled(self):
        """
        Is this Strategy enabled?

        :retval: Boolean
        """
        return True

    @classmethod
    def get_strategy(cls, name, ns=None):
        """
        Load a strategy from namespace
        """
        ns = ns or cls.__strategy_ns__
        if ns is None:
            raise RuntimeError(
                'No namespace provided or __strategy_ns__ unset')

        LOG.debug('Looking for strategy %s in %s', name, ns)

        return utils.import_class(ns + "." + name)

    @classmethod
    def get_canonical_name(cls):
        """
        Return the strategy name
        """
        type_ = cls.get_strategy_type()
        name = cls.get_strategy_name()
        return "%s:%s" % (type_, name)

    @classmethod
    def get_strategy_name(cls):
        return cls.__strategy_name__

    @classmethod
    def get_strategy_type(cls):
        return cls.__strategy_type__

########NEW FILE########
__FILENAME__ = volume
# Copyright (c) 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import os
import pexpect
from tempfile import NamedTemporaryFile

from trove.common import cfg
from trove.common import utils
from trove.common.exception import GuestError
from trove.common.exception import ProcessExecutionError
from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _

TMP_MOUNT_POINT = "/mnt/volume"

LOG = logging.getLogger(__name__)
CONF = cfg.CONF


class VolumeDevice(object):

    def __init__(self, device_path):
        self.device_path = device_path

    def migrate_data(self, mysql_base):
        """Synchronize the data from the mysql directory to the new volume """
        self.mount(TMP_MOUNT_POINT, write_to_fstab=False)
        if not mysql_base[-1] == '/':
            mysql_base = "%s/" % mysql_base
        utils.execute("sudo", "rsync", "--safe-links", "--perms",
                      "--recursive", "--owner", "--group", "--xattrs",
                      "--sparse", mysql_base, TMP_MOUNT_POINT)
        self.unmount(TMP_MOUNT_POINT)

    def _check_device_exists(self):
        """Check that the device path exists.

        Verify that the device path has actually been created and can report
        it's size, only then can it be available for formatting, retry
        num_tries to account for the time lag.
        """
        try:
            num_tries = CONF.num_tries
            utils.execute('sudo', 'blockdev', '--getsize64', self.device_path,
                          attempts=num_tries)
        except ProcessExecutionError:
            raise GuestError(_("InvalidDevicePath(path=%s)") %
                             self.device_path)

    def _check_format(self):
        """Checks that an unmounted volume is formatted."""
        child = pexpect.spawn("sudo dumpe2fs %s" % self.device_path)
        try:
            i = child.expect(['has_journal', 'Wrong magic number'])
            if i == 0:
                return
            volume_fstype = CONF.volume_fstype
            raise IOError(
                _('Device path at {0} did not seem to be {1}.').format(
                    self.device_path, volume_fstype))

        except pexpect.EOF:
            raise IOError(_("Volume was not formatted."))
        child.expect(pexpect.EOF)

    def _format(self):
        """Calls mkfs to format the device at device_path."""
        volume_fstype = CONF.volume_fstype
        format_options = CONF.format_options
        cmd = "sudo mkfs -t %s %s %s" % (volume_fstype,
                                         format_options, self.device_path)
        volume_format_timeout = CONF.volume_format_timeout
        child = pexpect.spawn(cmd, timeout=volume_format_timeout)
        # child.expect("(y,n)")
        # child.sendline('y')
        child.expect(pexpect.EOF)

    def format(self):
        """Formats the device at device_path and checks the filesystem."""
        self._check_device_exists()
        self._format()
        self._check_format()

    def mount(self, mount_point, write_to_fstab=True):
        """Mounts, and writes to fstab."""
        mount_point = VolumeMountPoint(self.device_path, mount_point)
        mount_point.mount()
        if write_to_fstab:
            mount_point.write_to_fstab()

    def resize_fs(self, mount_point):
        """Resize the filesystem on the specified device"""
        self._check_device_exists()
        try:
            # check if the device is mounted at mount_point before e2fsck
            if not os.path.ismount(mount_point):
                utils.execute("e2fsck", "-f", "-p", self.device_path,
                              run_as_root=True, root_helper="sudo")
            utils.execute("resize2fs", self.device_path,
                          run_as_root=True, root_helper="sudo")
        except ProcessExecutionError as err:
            LOG.error(err)
            raise GuestError(_("Error resizing the filesystem: %s") %
                             self.device_path)

    def unmount(self, mount_point):
        if os.path.exists(mount_point):
            cmd = "sudo umount %s" % mount_point
            child = pexpect.spawn(cmd)
            child.expect(pexpect.EOF)

    def unmount_device(self, device_path):
        # unmount if device is already mounted
        mount_points = self.mount_points(device_path)
        for mnt in mount_points:
            LOG.info(_("Device %(device)s is already mounted in "
                       "%(mount_point)s") %
                     {'device': device_path, 'mount_point': mnt})
            LOG.info(_("Unmounting %s") % mnt)
            self.unmount(mnt)

    def mount_points(self, device_path):
        """Returns a list of mount points on the specified device."""
        try:
            cmd = "grep %s /etc/mtab | awk '{print $2}'" % device_path
            stdout, stderr = utils.execute(cmd, shell=True)
            return stdout.strip().split('\n')

        except ProcessExecutionError as err:
            LOG.error(err)
            raise GuestError(_("Could not obtain a list of mount points for "
                               "device: %s") % device_path)


class VolumeMountPoint(object):

    def __init__(self, device_path, mount_point):
        self.device_path = device_path
        self.mount_point = mount_point
        self.volume_fstype = CONF.volume_fstype
        self.mount_options = CONF.mount_options

    def mount(self):
        if not os.path.exists(self.mount_point):
            utils.execute("sudo", "mkdir", "-p", self.mount_point)
        LOG.debug("Mounting volume. Device path:{0}, mount_point:{1}, "
                  "volume_type:{2}, mount options:{3}".format(
                      self.device_path, self.mount_point, self.volume_fstype,
                      self.mount_options))
        cmd = ("sudo mount -t %s -o %s %s %s" %
               (self.volume_fstype, self.mount_options, self.device_path,
                self.mount_point))
        child = pexpect.spawn(cmd)
        child.expect(pexpect.EOF)

    def write_to_fstab(self):
        fstab_line = ("%s\t%s\t%s\t%s\t0\t0" %
                      (self.device_path, self.mount_point, self.volume_fstype,
                       self.mount_options))
        LOG.debug("Writing new line to fstab:%s" % fstab_line)
        with open('/etc/fstab', "r") as fstab:
            fstab_content = fstab.read()
        with NamedTemporaryFile(delete=False) as tempfstab:
            tempfstab.write(fstab_content + fstab_line)
        utils.execute("sudo", "install", "-o", "root", "-g", "root", "-m",
                      "644", tempfstab.name, "/etc/fstab")
        utils.execute("sudo", "rm", tempfstab.name)

########NEW FILE########
__FILENAME__ = models
#    Copyright 2010-2011 OpenStack Foundation
#    Copyright 2013-2014 Rackspace Hosting
#    All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Model classes that form the core of instances functionality."""

import re
from datetime import datetime
from novaclient import exceptions as nova_exceptions
from oslo.config.cfg import NoSuchOptError
from trove.common import cfg
from trove.common import exception
from trove.common import template
from trove.common.configurations import do_configs_require_restart
import trove.common.instance as tr_instance
from trove.common.remote import create_dns_client
from trove.common.remote import create_guest_client
from trove.common.remote import create_nova_client
from trove.common.remote import create_cinder_client
from trove.common import utils
from trove.configuration.models import Configuration
from trove.extensions.security_group.models import SecurityGroup
from trove.db import get_db_api
from trove.db import models as dbmodels
from trove.datastore import models as datastore_models
from trove.backup.models import Backup
from trove.quota.quota import run_with_quotas
from trove.instance.tasks import InstanceTask
from trove.instance.tasks import InstanceTasks
from trove.taskmanager import api as task_api
from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _


CONF = cfg.CONF
LOG = logging.getLogger(__name__)


def filter_ips(ips, regex):
    """Filter out IPs not matching regex."""
    return [ip for ip in ips if re.search(regex, ip)]


def load_server(context, instance_id, server_id):
    """
    Loads a server or raises an exception.
    :param context: request context used to access nova
    :param instance_id: the trove instance id corresponding to the nova server
    (informational only)
    :param server_id: the compute instance id which will be retrieved from nova
    :type context: trove.common.context.TroveContext
    :type instance_id: unicode
    :type server_id: unicode
    :rtype: novaclient.v1_1.servers.Server
    """
    client = create_nova_client(context)
    try:
        server = client.servers.get(server_id)
    except nova_exceptions.NotFound:
        LOG.debug("Could not find nova server_id(%s)" % server_id)
        raise exception.ComputeInstanceNotFound(instance_id=instance_id,
                                                server_id=server_id)
    except nova_exceptions.ClientException as e:
        raise exception.TroveError(str(e))
    return server


class InstanceStatus(object):
    ACTIVE = "ACTIVE"
    BLOCKED = "BLOCKED"
    BUILD = "BUILD"
    FAILED = "FAILED"
    REBOOT = "REBOOT"
    RESIZE = "RESIZE"
    BACKUP = "BACKUP"
    SHUTDOWN = "SHUTDOWN"
    ERROR = "ERROR"
    RESTART_REQUIRED = "RESTART_REQUIRED"


def validate_volume_size(size):
    if size is None:
        raise exception.VolumeSizeNotSpecified()
    max_size = CONF.max_accepted_volume_size
    if long(size) > max_size:
        msg = ("Volume 'size' cannot exceed maximum "
               "of %d Gb, %s cannot be accepted."
               % (max_size, size))
        raise exception.VolumeQuotaExceeded(msg)


def load_simple_instance_server_status(context, db_info):
    """Loads a server or raises an exception."""
    if 'BUILDING' == db_info.task_status.action:
        db_info.server_status = "BUILD"
        db_info.addresses = {}
    else:
        client = create_nova_client(context)
        try:
            server = client.servers.get(db_info.compute_instance_id)
            db_info.server_status = server.status
            db_info.addresses = server.addresses
        except nova_exceptions.NotFound:
            db_info.server_status = "SHUTDOWN"
            db_info.addresses = {}


# Invalid states to contact the agent
AGENT_INVALID_STATUSES = ["BUILD", "REBOOT", "RESIZE"]


class SimpleInstance(object):
    """A simple view of an instance.
    This gets loaded directly from the local database, so its cheaper than
    creating the fully loaded Instance.  As the name implies this class knows
    nothing of the underlying Nova Compute Instance (i.e. server)
    -----------
    |         |
    |    i    |
    | t  n    |
    | r  s  ---------------------
    | o  t  |  datastore/guest  |
    | v  a  ---------------------
    | e  n    |
    |    c    |
    |    e    |
    |         |
    -----------
    """

    def __init__(self, context, db_info, datastore_status, root_password=None,
                 ds_version=None, ds=None):
        """
        :type context: trove.common.context.TroveContext
        :type db_info: trove.instance.models.DBInstance
        :type datastore_status: trove.instance.models.InstanceServiceStatus
        :type root_password: str
        """
        self.context = context
        self.db_info = db_info
        self.datastore_status = datastore_status
        self.root_pass = root_password
        if ds_version is None:
            self.ds_version = (datastore_models.DatastoreVersion.
                               load_by_uuid(self.db_info.datastore_version_id))
        if ds is None:
            self.ds = (datastore_models.Datastore.
                       load(self.ds_version.datastore_id))

    @property
    def addresses(self):
        #TODO(tim.simpson): This code attaches two parts of the Nova server to
        #                   db_info: "status" and "addresses". The idea
        #                   originally was to listen to events to update this
        #                   data and store it in the Trove database.
        #                   However, it may have been unwise as a year and a
        #                   half later we still have to load the server anyway
        #                   and this makes the code confusing.
        if hasattr(self.db_info, 'addresses'):
            return self.db_info.addresses
        else:
            return None

    @property
    def created(self):
        return self.db_info.created

    @property
    def dns_ip_address(self):
        """Returns the IP address to be used with DNS."""
        ips = self.get_visible_ip_addresses()
        if ips:
            return ips[0]

    @property
    def flavor_id(self):
        # Flavor ID is a str in the 1.0 API.
        return str(self.db_info.flavor_id)

    @property
    def hostname(self):
        return self.db_info.hostname

    def get_visible_ip_addresses(self):
        """Returns IPs that will be visible to the user."""
        if self.addresses is None:
            return None
        IPs = []
        for label in self.addresses:
            if (re.search(CONF.network_label_regex, label) and
                    len(self.addresses[label]) > 0):
                IPs.extend([addr.get('addr')
                            for addr in self.addresses[label]])
        # Includes ip addresses that match the regexp pattern
        if CONF.ip_regex:
            IPs = filter_ips(IPs, CONF.ip_regex)
        return IPs

    @property
    def id(self):
        return self.db_info.id

    @property
    def tenant_id(self):
        return self.db_info.tenant_id

    @property
    def is_building(self):
        return self.status in [InstanceStatus.BUILD]

    @property
    def is_datastore_running(self):
        """True if the service status indicates datastore is up and running."""
        return self.datastore_status.status in MYSQL_RESPONSIVE_STATUSES

    def datastore_status_matches(self, service_status):
        return self.datastore_status.status == service_status

    @property
    def name(self):
        return self.db_info.name

    @property
    def server_id(self):
        return self.db_info.compute_instance_id

    @property
    def datastore_status(self):
        """
        Returns the Service Status for this instance.  For example, the status
        of the mysql datastore which is running on the server...not the server
        status itself.
        :return: the current status of the datastore
        :rtype: trove.instance.models.InstanceServiceStatus
        """
        return self.__datastore_status

    @datastore_status.setter
    def datastore_status(self, datastore_status):
        if datastore_status and not isinstance(datastore_status,
                                               InstanceServiceStatus):
            raise ValueError("datastore_status must be of type "
                             "InstanceServiceStatus. Got %s instead." %
                             datastore_status.__class__.__name__)
        self.__datastore_status = datastore_status

    @property
    def status(self):
        ### Check for taskmanager errors.
        if self.db_info.task_status.is_error:
            return InstanceStatus.ERROR

        ### Check for taskmanager status.
        action = self.db_info.task_status.action
        if 'BUILDING' == action:
            if 'ERROR' == self.db_info.server_status:
                return InstanceStatus.ERROR
            return InstanceStatus.BUILD
        if 'REBOOTING' == action:
            return InstanceStatus.REBOOT
        if 'RESIZING' == action:
            return InstanceStatus.RESIZE
        if 'RESTART_REQUIRED' == action:
            return InstanceStatus.RESTART_REQUIRED

        ### Check for server status.
        if self.db_info.server_status in ["BUILD", "ERROR", "REBOOT",
                                          "RESIZE"]:
            return self.db_info.server_status

        # As far as Trove is concerned, Nova instances in VERIFY_RESIZE should
        # still appear as though they are in RESIZE.
        if self.db_info.server_status in ["VERIFY_RESIZE"]:
            return InstanceStatus.RESIZE

        ### Check if there is a backup running for this instance
        if Backup.running(self.id):
            return InstanceStatus.BACKUP

        ### Report as Shutdown while deleting, unless there's an error.
        if 'DELETING' == action:
            if self.db_info.server_status in ["ACTIVE", "SHUTDOWN", "DELETED"]:
                return InstanceStatus.SHUTDOWN
            else:
                LOG.error(_("While shutting down instance (%(instance)s): "
                            "server had status (%(status)s).") %
                          {'instance': self.id,
                           'status': self.db_info.server_status})
                return InstanceStatus.ERROR

        ### Check against the service status.
        # The service is only paused during a reboot.
        if tr_instance.ServiceStatuses.PAUSED == self.datastore_status.status:
            return InstanceStatus.REBOOT
        # If the service status is NEW, then we are building.
        if tr_instance.ServiceStatuses.NEW == self.datastore_status.status:
            return InstanceStatus.BUILD

        # For everything else we can look at the service status mapping.
        return self.datastore_status.status.api_status

    @property
    def updated(self):
        return self.db_info.updated

    @property
    def volume_id(self):
        return self.db_info.volume_id

    @property
    def volume_size(self):
        return self.db_info.volume_size

    @property
    def datastore_version(self):
        return self.ds_version

    @property
    def datastore(self):
        return self.ds

    @property
    def root_password(self):
        return self.root_pass

    @property
    def configuration(self):
        if self.db_info.configuration_id is not None:
            return Configuration.load(self.context,
                                      self.db_info.configuration_id)


class DetailInstance(SimpleInstance):
    """A detailed view of an Instnace.

    This loads a SimpleInstance and then adds additional data for the
    instance from the guest.
    """

    def __init__(self, context, db_info, datastore_status):
        super(DetailInstance, self).__init__(context, db_info,
                                             datastore_status)
        self._volume_used = None
        self._volume_total = None

    @property
    def volume_used(self):
        return self._volume_used

    @volume_used.setter
    def volume_used(self, value):
        self._volume_used = value

    @property
    def volume_total(self):
        return self._volume_total

    @volume_total.setter
    def volume_total(self, value):
        self._volume_total = value


def get_db_info(context, id):
    """
    Retrieves an instance of the managed datastore from the persisted
    storage based on the ID and Context
    :param context: the context which owns the instance
    :type context: trove.common.context.TroveContext
    :param id: the unique ID of the instance
    :type id: unicode or str
    :return: a record of the instance as its state exists in persisted storage
    :rtype: trove.instance.models.DBInstance
    """
    if context is None:
        raise TypeError("Argument context not defined.")
    elif id is None:
        raise TypeError("Argument id not defined.")
    try:
        db_info = DBInstance.find_by(context=context, id=id, deleted=False)
    except exception.NotFound:
        raise exception.NotFound(uuid=id)
    return db_info


def load_any_instance(context, id):
    # Try to load an instance with a server.
    # If that fails, try to load it without the server.
    try:
        return load_instance(BuiltInstance, context, id, needs_server=True)
    except exception.UnprocessableEntity:
        LOG.warn("Could not load instance %s." % id)
        return load_instance(FreshInstance, context, id, needs_server=False)


def load_instance(cls, context, id, needs_server=False):
    db_info = get_db_info(context, id)
    if not needs_server:
        # TODO(tim.simpson): When we have notifications this won't be
        # necessary and instead we'll just use the server_status field from
        # the instance table.
        load_simple_instance_server_status(context, db_info)
        server = None
    else:
        try:
            server = load_server(context, db_info.id,
                                 db_info.compute_instance_id)
            #TODO(tim.simpson): Remove this hack when we have notifications!
            db_info.server_status = server.status
            db_info.addresses = server.addresses
        except exception.ComputeInstanceNotFound:
            LOG.error("COMPUTE ID = %s" % db_info.compute_instance_id)
            raise exception.UnprocessableEntity("Instance %s is not ready." %
                                                id)

    service_status = InstanceServiceStatus.find_by(instance_id=id)
    LOG.info("service status=%s" % service_status.status)
    return cls(context, db_info, server, service_status)


def load_instance_with_guest(cls, context, id):
    db_info = get_db_info(context, id)
    load_simple_instance_server_status(context, db_info)
    datastore_status = InstanceServiceStatus.find_by(instance_id=id)
    LOG.info("datastore status=%s" % datastore_status.status)
    instance = cls(context, db_info, datastore_status)
    load_guest_info(instance, context, id)
    return instance


def load_guest_info(instance, context, id):
    if instance.status not in AGENT_INVALID_STATUSES:
        guest = create_guest_client(context, id)
        try:
            volume_info = guest.get_volume_info()
            instance.volume_used = volume_info['used']
            instance.volume_total = volume_info['total']
        except Exception as e:
            LOG.error(e)
    return instance


class BaseInstance(SimpleInstance):
    """Represents an instance.
    -----------
    |         |
    |    i  ---------------------
    | t  n  |  compute instance |
    | r  s  ---------------------
    | o  t    |
    | v  a    |
    | e  n  ---------------------
    |    c  |  datastore/guest  |
    |    e  ---------------------
    |         |
    -----------
    """

    def __init__(self, context, db_info, server, datastore_status):
        """
        Creates a new initialized representation of an instance composed of its
        state in the database and its state from Nova

        :param context: the request context which contains the tenant that owns
        this instance
        :param db_info: the current state of this instance as it exists in the
        db
        :param server: the current state of this instance as it exists in the
        Nova
        :param datastore_status: the current state of the datastore on this
        instance at it exists in the db
        :type context: trove.common.context.TroveContext
        :type db_info: trove.instance.models.DBInstance
        :type server: novaclient.v1_1.servers.Server
        :typdatastore_statusus: trove.instance.models.InstanceServiceStatus
        """
        super(BaseInstance, self).__init__(context, db_info, datastore_status)
        self.server = server
        self._guest = None
        self._nova_client = None
        self._volume_client = None

    def get_guest(self):
        return create_guest_client(self.context, self.db_info.id)

    def delete(self):
        def _delete_resources():
            if self.is_building:
                raise exception.UnprocessableEntity("Instance %s is not ready."
                                                    % self.id)
            LOG.debug("  ... deleting compute id = %s" %
                      self.db_info.compute_instance_id)
            LOG.debug(" ... setting status to DELETING.")
            self.update_db(task_status=InstanceTasks.DELETING,
                           configuration_id=None)
            task_api.API(self.context).delete_instance(self.id)

        deltas = {'instances': -1}
        if CONF.trove_volume_support:
            deltas['volumes'] = -self.volume_size
        return run_with_quotas(self.tenant_id,
                               deltas,
                               _delete_resources)

    def _delete_resources(self, deleted_at):
        pass

    def delete_async(self):
        deleted_at = datetime.utcnow()
        self._delete_resources(deleted_at)
        LOG.debug("Setting instance %s to deleted..." % self.id)
        # Delete guest queue.
        try:
            guest = self.get_guest()
            guest.delete_queue()
        except Exception as ex:
            LOG.warn(ex)
        self.update_db(deleted=True, deleted_at=deleted_at,
                       task_status=InstanceTasks.NONE)
        self.set_servicestatus_deleted()
        # Delete associated security group
        if CONF.trove_security_groups_support:
            SecurityGroup.delete_for_instance(self.db_info.id,
                                              self.context)

    @property
    def guest(self):
        if not self._guest:
            self._guest = self.get_guest()
        return self._guest

    @property
    def nova_client(self):
        if not self._nova_client:
            self._nova_client = create_nova_client(self.context)
        return self._nova_client

    def update_db(self, **values):
        self.db_info = DBInstance.find_by(id=self.id, deleted=False)
        for key in values:
            setattr(self.db_info, key, values[key])
        self.db_info.save()

    def set_servicestatus_deleted(self):
        del_instance = InstanceServiceStatus.find_by(instance_id=self.id)
        del_instance.set_status(tr_instance.ServiceStatuses.DELETED)
        del_instance.save()

    @property
    def volume_client(self):
        if not self._volume_client:
            self._volume_client = create_cinder_client(self.context)
        return self._volume_client

    def reset_task_status(self):
        LOG.info(_("Settting task status to NONE on instance %s...") % self.id)
        self.update_db(task_status=InstanceTasks.NONE)


class FreshInstance(BaseInstance):
    @classmethod
    def load(cls, context, id):
        return load_instance(cls, context, id, needs_server=False)


class BuiltInstance(BaseInstance):
    @classmethod
    def load(cls, context, id):
        return load_instance(cls, context, id, needs_server=True)


class Instance(BuiltInstance):
    """Represents an instance.

    The life span of this object should be limited. Do not store them or
    pass them between threads.

    """

    @classmethod
    def get_root_on_create(cls, datastore_manager):
        try:
            root_on_create = CONF.get(datastore_manager).root_on_create
            return root_on_create
        except NoSuchOptError:
            LOG.debug("root_on_create not configured for %s"
                      " hence defaulting the value to False"
                      % datastore_manager)
            return False

    @classmethod
    def create(cls, context, name, flavor_id, image_id, databases, users,
               datastore, datastore_version, volume_size, backup_id,
               availability_zone=None, nics=None, configuration_id=None):

        client = create_nova_client(context)
        try:
            flavor = client.flavors.get(flavor_id)
        except nova_exceptions.NotFound:
            raise exception.FlavorNotFound(uuid=flavor_id)

        deltas = {'instances': 1}
        if CONF.trove_volume_support:
            validate_volume_size(volume_size)
            deltas['volumes'] = volume_size
        else:
            if volume_size is not None:
                raise exception.VolumeNotSupported()
            ephemeral_support = CONF.device_path
            if ephemeral_support and flavor.ephemeral == 0:
                raise exception.LocalStorageNotSpecified(flavor=flavor_id)

        if backup_id is not None:
            backup_info = Backup.get_by_id(context, backup_id)
            if backup_info.is_running:
                raise exception.BackupNotCompleteError(backup_id=backup_id)

            if not backup_info.check_swift_object_exist(
                    context,
                    verify_checksum=CONF.verify_swift_checksum_on_restore):
                raise exception.BackupFileNotFound(
                    location=backup_info.location)

            backup_db_info = DBInstance.find_by(
                context=context, id=backup_info.instance_id)
            if (backup_db_info.datastore_version_id
                    != datastore_version.id):
                ds_version = (datastore_models.DatastoreVersion.
                              load_by_uuid(backup_db_info.datastore_version_id)
                              )
                raise exception.BackupDatastoreVersionMismatchError(
                    version1=ds_version.name,
                    version2=datastore_version.name)

        if not nics and CONF.default_neutron_networks:
            nics = []
            for net_id in CONF.default_neutron_networks:
                nics.append({"net-id": net_id})

        def _create_resources():

            db_info = DBInstance.create(name=name, flavor_id=flavor_id,
                                        tenant_id=context.tenant,
                                        volume_size=volume_size,
                                        datastore_version_id=
                                        datastore_version.id,
                                        task_status=InstanceTasks.BUILDING,
                                        configuration_id=configuration_id)
            LOG.debug("Tenant %(tenant)s created new "
                      "Trove instance %(db)s..." %
                      {'tenant': context.tenant, 'db': db_info.id})

            # if a configuration group is associated with an instance,
            # generate an overrides dict to pass into the instance creation
            # method

            overrides = Configuration.get_configuration_overrides(
                context, configuration_id)
            datastore_status = InstanceServiceStatus.create(
                instance_id=db_info.id,
                status=tr_instance.ServiceStatuses.NEW)

            if CONF.trove_dns_support:
                dns_client = create_dns_client(context)
                hostname = dns_client.determine_hostname(db_info.id)
                db_info.hostname = hostname
                db_info.save()

            root_password = None
            if cls.get_root_on_create(
                    datastore_version.manager) and not backup_id:
                root_password = utils.generate_random_password()

            task_api.API(context).create_instance(db_info.id, name, flavor,
                                                  image_id, databases, users,
                                                  datastore_version.manager,
                                                  datastore_version.packages,
                                                  volume_size, backup_id,
                                                  availability_zone,
                                                  root_password,
                                                  nics,
                                                  overrides)

            return SimpleInstance(context, db_info, datastore_status,
                                  root_password)

        return run_with_quotas(context.tenant,
                               deltas,
                               _create_resources)

    def get_flavor(self):
        client = create_nova_client(self.context)
        return client.flavors.get(self.flavor_id)

    def get_default_configration_template(self):
        flavor = self.get_flavor()
        LOG.debug("flavor: %s" % flavor)
        config = template.SingleInstanceConfigTemplate(
            self.ds_version, flavor, id)
        return config.render_dict()

    def resize_flavor(self, new_flavor_id):
        self.validate_can_perform_action()
        LOG.debug("resizing instance %s flavor to %s"
                  % (self.id, new_flavor_id))
        # Validate that the flavor can be found and that it isn't the same size
        # as the current one.
        client = create_nova_client(self.context)
        try:
            new_flavor = client.flavors.get(new_flavor_id)
        except nova_exceptions.NotFound:
            raise exception.FlavorNotFound(uuid=new_flavor_id)
        old_flavor = client.flavors.get(self.flavor_id)
        new_flavor_size = new_flavor.ram
        old_flavor_size = old_flavor.ram
        if CONF.trove_volume_support:
            if new_flavor.ephemeral != 0:
                raise exception.LocalStorageNotSupported()
            if new_flavor_size == old_flavor_size:
                raise exception.CannotResizeToSameSize()
        elif CONF.device_path is not None:
            # ephemeral support enabled
            if new_flavor.ephemeral == 0:
                raise exception.LocalStorageNotSpecified(flavor=new_flavor_id)
            if (new_flavor_size == old_flavor_size and
                    new_flavor.ephemeral == new_flavor.ephemeral):
                raise exception.CannotResizeToSameSize()

        # Set the task to RESIZING and begin the async call before returning.
        self.update_db(task_status=InstanceTasks.RESIZING)
        LOG.debug("Instance %s set to RESIZING." % self.id)
        task_api.API(self.context).resize_flavor(self.id, old_flavor,
                                                 new_flavor)

    def resize_volume(self, new_size):
        def _resize_resources():
            self.validate_can_perform_action()
            LOG.info("Resizing volume of instance %s..." % self.id)
            if not self.volume_size:
                raise exception.BadRequest(_("Instance %s has no volume.")
                                           % self.id)
            old_size = self.volume_size
            if int(new_size) <= old_size:
                raise exception.BadRequest(_("The new volume 'size' must be "
                                             "larger than the current volume "
                                             "size of '%s'") % old_size)
            # Set the task to Resizing before sending off to the taskmanager
            self.update_db(task_status=InstanceTasks.RESIZING)
            task_api.API(self.context).resize_volume(new_size, self.id)

        new_size_l = long(new_size)
        validate_volume_size(new_size_l)
        return run_with_quotas(self.tenant_id,
                               {'volumes': new_size_l - self.volume_size},
                               _resize_resources)

    def reboot(self):
        self.validate_can_perform_action()
        LOG.info("Rebooting instance %s..." % self.id)
        self.update_db(task_status=InstanceTasks.REBOOTING)
        task_api.API(self.context).reboot(self.id)

    def restart(self):
        self.validate_can_perform_action()
        LOG.info("Restarting MySQL on instance %s..." % self.id)
        # Set our local status since Nova might not change it quick enough.
        #TODO(tim.simpson): Possible bad stuff can happen if this service
        #                   shuts down before it can set status to NONE.
        #                   We need a last updated time to mitigate this;
        #                   after some period of tolerance, we'll assume the
        #                   status is no longer in effect.
        self.update_db(task_status=InstanceTasks.REBOOTING)
        task_api.API(self.context).restart(self.id)

    def migrate(self, host=None):
        self.validate_can_perform_action()
        LOG.info("Migrating instance id = %s, to host = %s" % (self.id, host))
        self.update_db(task_status=InstanceTasks.MIGRATING)
        task_api.API(self.context).migrate(self.id, host)

    def validate_can_perform_action(self):
        """
        Raises exception if an instance action cannot currently be performed.
        """
        # cases where action cannot be performed
        if self.db_info.server_status != 'ACTIVE':
            status = self.db_info.server_status
        elif (self.db_info.task_status != InstanceTasks.NONE and
              self.db_info.task_status != InstanceTasks.RESTART_REQUIRED):
            status = self.db_info.task_status
        elif not self.datastore_status.status.action_is_allowed:
            status = self.status
        elif Backup.running(self.id):
            status = InstanceStatus.BACKUP
        else:
            # action can be performed
            return

        msg = ("Instance is not currently available for an action to be "
               "performed (status was %s)." % status)
        LOG.error(msg)
        raise exception.UnprocessableEntity(msg)

    def _validate_can_perform_assign(self):
        """
        Raises exception if a configuration assign cannot
        currently be performed
        """
        # check if the instance already has a configuration assigned
        if self.db_info.configuration_id:
            raise exception.ConfigurationAlreadyAttached(
                instance_id=self.id,
                configuration_id=self.db_info.configuration_id)

        # check if the instance is not ACTIVE or has tasks
        status = None
        if self.db_info.server_status != InstanceStatus.ACTIVE:
            status = self.db_info.server_status
        elif self.db_info.task_status != InstanceTasks.NONE:
            status = self.db_info.task_status.action

        if status:
            raise exception.InvalidInstanceState(instance_id=self.id,
                                                 status=status)

    def unassign_configuration(self):
        LOG.debug("Unassigning the configuration from the instance %s"
                  % self.id)
        if self.configuration and self.configuration.id:
            LOG.debug("Unassigning the configuration id %s"
                      % self.configuration.id)
            flavor = self.get_flavor()
            config_id = self.configuration.id
            LOG.debug("configuration being unassigned; "
                      "marking restart required")
            self.update_db(task_status=InstanceTasks.RESTART_REQUIRED)
            task_api.API(self.context).unassign_configuration(self.id,
                                                              flavor,
                                                              config_id)
        else:
            LOG.debug("no configuration found on instance skipping.")

    def assign_configuration(self, configuration_id):
        self._validate_can_perform_assign()

        try:
            configuration = Configuration.load(self.context, configuration_id)
        except exception.ModelNotFoundError:
            raise exception.NotFound(
                message='Configuration group id: %s could not be found'
                % configuration_id)

        config_ds_v = configuration.datastore_version_id
        inst_ds_v = self.db_info.datastore_version_id
        if (config_ds_v != inst_ds_v):
            raise exception.ConfigurationDatastoreNotMatchInstance(
                config_datastore_version=config_ds_v,
                instance_datastore_version=inst_ds_v)

        overrides = Configuration.get_configuration_overrides(
            self.context, configuration.id)

        LOG.info(overrides)

        self.update_overrides(overrides)
        self.update_db(configuration_id=configuration.id)

    def update_overrides(self, overrides):
        LOG.debug("Updating or removing overrides for instance %s"
                  % self.id)
        need_restart = do_configs_require_restart(
            overrides, datastore_manager=self.ds_version.manager)
        LOG.debug("config overrides has non-dynamic settings, "
                  "requires a restart: %s" % need_restart)
        if need_restart:
            self.update_db(task_status=InstanceTasks.RESTART_REQUIRED)
        task_api.API(self.context).update_overrides(self.id, overrides)


def create_server_list_matcher(server_list):
    # Returns a method which finds a server from the given list.
    def find_server(instance_id, server_id):
        matches = [server for server in server_list if server.id == server_id]
        if len(matches) == 1:
            return matches[0]
        elif len(matches) < 1:
            # The instance was not found in the list and
            # this can happen if the instance is deleted from
            # nova but still in trove database
            raise exception.ComputeInstanceNotFound(
                instance_id=instance_id, server_id=server_id)
        else:
            # Should never happen, but never say never.
            LOG.error(_("Server %(server)s for instance %(instance)s was"
                        "found twice!") % {'server': server_id,
                                           'instance': instance_id})
            raise exception.TroveError(uuid=instance_id)

    return find_server


class Instances(object):
    DEFAULT_LIMIT = CONF.instances_page_size

    @staticmethod
    def load(context):

        def load_simple_instance(context, db, status, **kwargs):
            return SimpleInstance(context, db, status)

        if context is None:
            raise TypeError("Argument context not defined.")
        client = create_nova_client(context)
        servers = client.servers.list()

        db_infos = DBInstance.find_all(tenant_id=context.tenant, deleted=False)
        limit = int(context.limit or Instances.DEFAULT_LIMIT)
        if limit > Instances.DEFAULT_LIMIT:
            limit = Instances.DEFAULT_LIMIT
        data_view = DBInstance.find_by_pagination('instances', db_infos, "foo",
                                                  limit=limit,
                                                  marker=context.marker)
        next_marker = data_view.next_page_marker

        find_server = create_server_list_matcher(servers)
        for db in db_infos:
            LOG.debug("checking for db [id=%s, compute_instance_id=%s]" %
                      (db.id, db.compute_instance_id))
        ret = Instances._load_servers_status(load_simple_instance, context,
                                             data_view.collection,
                                             find_server)
        return ret, next_marker

    @staticmethod
    def _load_servers_status(load_instance, context, db_items, find_server):
        ret = []
        for db in db_items:
            server = None
            try:
                #TODO(tim.simpson): Delete when we get notifications working!
                if InstanceTasks.BUILDING == db.task_status:
                    db.server_status = "BUILD"
                else:
                    try:
                        server = find_server(db.id, db.compute_instance_id)
                        db.server_status = server.status
                    except exception.ComputeInstanceNotFound:
                        db.server_status = "SHUTDOWN"  # Fake it...
                #TODO(tim.simpson): End of hack.

                #volumes = find_volumes(server.id)
                datastore_status = InstanceServiceStatus.find_by(
                    instance_id=db.id)
                if not datastore_status.status:  # This should never happen.
                    LOG.error(_("Server status could not be read for "
                                "instance id(%s)") % db.id)
                    continue
                LOG.info(_("Server api_status(%s)") %
                         datastore_status.status.api_status)
            except exception.ModelNotFoundError:
                LOG.error(_("Server status could not be read for "
                            "instance id(%s)") % db.id)
                continue
            ret.append(load_instance(context, db, datastore_status,
                                     server=server))
        return ret


class DBInstance(dbmodels.DatabaseModelBase):
    """Defines the task being executed plus the start time."""

    #TODO(tim.simpson): Add start time.

    _data_fields = ['name', 'created', 'compute_instance_id',
                    'task_id', 'task_description', 'task_start_time',
                    'volume_id', 'deleted', 'tenant_id',
                    'datastore_version_id', 'configuration_id']

    def __init__(self, task_status, **kwargs):
        """
        Creates a new persistable entity of the Trove Guest Instance for
        purposes recording its current state and record of modifications
        :param task_status: the current state details of any activity or error
         that is running on this guest instance (e.g. resizing, deleting)
        :type task_status: trove.instance.tasks.InstanceTask
        """
        kwargs["task_id"] = task_status.code
        kwargs["task_description"] = task_status.db_text
        kwargs["deleted"] = False
        super(DBInstance, self).__init__(**kwargs)
        self.set_task_status(task_status)

    def _validate(self, errors):
        if InstanceTask.from_code(self.task_id) is None:
            errors['task_id'] = "Not valid."
        if self.task_status is None:
            errors['task_status'] = "Cannot be none."

    def get_task_status(self):
        return InstanceTask.from_code(self.task_id)

    def set_task_status(self, value):
        self.task_id = value.code
        self.task_description = value.db_text

    task_status = property(get_task_status, set_task_status)


class InstanceServiceStatus(dbmodels.DatabaseModelBase):
    _data_fields = ['instance_id', 'status_id', 'status_description',
                    'updated_at']

    def __init__(self, status, **kwargs):
        kwargs["status_id"] = status.code
        kwargs["status_description"] = status.description
        super(InstanceServiceStatus, self).__init__(**kwargs)
        self.set_status(status)

    def _validate(self, errors):
        if self.status is None:
            errors['status'] = "Cannot be none."
        if tr_instance.ServiceStatus.from_code(self.status_id) is None:
            errors['status_id'] = "Not valid."

    def get_status(self):
        """
        Returns the current enumerated status of the Service running on the
        instance
        :return: a ServiceStatus reference indicating the currently stored
        status of the service
        :rtype: trove.common.instance.ServiceStatus
        """
        return tr_instance.ServiceStatus.from_code(self.status_id)

    def set_status(self, value):
        """
        Sets the status of the hosted service
        :param value: current state of the hosted service
        :type value: trove.common.instance.ServiceStatus
        """
        self.status_id = value.code
        self.status_description = value.description

    def save(self):
        self['updated_at'] = utils.utcnow()
        return get_db_api().save(self)

    status = property(get_status, set_status)


def persisted_models():
    return {
        'instance': DBInstance,
        'service_statuses': InstanceServiceStatus,
    }


MYSQL_RESPONSIVE_STATUSES = [tr_instance.ServiceStatuses.RUNNING]

########NEW FILE########
__FILENAME__ = service
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import webob.exc

from trove.common import cfg
from trove.common import exception
from trove.common import pagination
from trove.common import utils
from trove.common import wsgi
from trove.extensions.mysql.common import populate_validated_databases
from trove.extensions.mysql.common import populate_users
from trove.instance import models, views
from trove.datastore import models as datastore_models
from trove.backup.models import Backup as backup_model
from trove.backup import views as backup_views
from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _
import trove.common.apischema as apischema


CONF = cfg.CONF
LOG = logging.getLogger(__name__)


class InstanceController(wsgi.Controller):

    """Controller for instance functionality"""
    schemas = apischema.instance.copy()
    if not CONF.trove_volume_support:
        # see instance.models.create for further validation around this
        LOG.info("Removing volume attributes from schema")
        schemas['create']['properties']['instance']['required'].pop()

    @classmethod
    def get_action_schema(cls, body, action_schema):
        action_type = body.keys()[0]
        action_schema = action_schema.get(action_type, {})
        if action_type == 'resize':
            # volume or flavorRef
            resize_action = body[action_type].keys()[0]
            action_schema = action_schema.get(resize_action, {})
        return action_schema

    @classmethod
    def get_schema(cls, action, body):
        action_schema = super(InstanceController, cls).get_schema(action, body)
        if action == 'action':
            # resize or restart
            action_schema = cls.get_action_schema(body, action_schema)
        return action_schema

    def action(self, req, body, tenant_id, id):
        """
        Handles requests that modify existing instances in some manner. Actions
        could include 'resize', 'restart', 'reset_password'
        :param req: http request object
        :param body: deserialized body of the request as a dict
        :param tenant_id: the tenant id for whom owns the instance
        :param id: ???
        """
        LOG.info("req : '%s'\n\n" % req)
        LOG.info("Comitting an ACTION again instance %s for tenant '%s'"
                 % (id, tenant_id))
        if not body:
            raise exception.BadRequest(_("Invalid request body."))
        context = req.environ[wsgi.CONTEXT_KEY]
        instance = models.Instance.load(context, id)
        _actions = {
            'restart': self._action_restart,
            'resize': self._action_resize,
            'reset_password': self._action_reset_password
        }
        selected_action = None
        for key in body:
            if key in _actions:
                selected_action = _actions[key]
        return selected_action(instance, body)

    def _action_restart(self, instance, body):
        instance.restart()
        return wsgi.Result(None, 202)

    def _action_resize(self, instance, body):
        """
        Handles 2 cases
        1. resize volume
            body only contains {volume: {size: x}}
        2. resize instance
            body only contains {flavorRef: http.../2}

        If the body has both we will throw back an error.
        """
        options = {
            'volume': self._action_resize_volume,
            'flavorRef': self._action_resize_flavor
        }
        selected_option = None
        args = None
        for key in options:
            if key in body['resize']:
                selected_option = options[key]
                args = body['resize'][key]
                break
        return selected_option(instance, args)

    def _action_resize_volume(self, instance, volume):
        instance.resize_volume(volume['size'])
        return wsgi.Result(None, 202)

    def _action_resize_flavor(self, instance, flavorRef):
        new_flavor_id = utils.get_id_from_href(flavorRef)
        instance.resize_flavor(new_flavor_id)
        return wsgi.Result(None, 202)

    def _action_reset_password(self, instance, body):
        raise webob.exc.HTTPNotImplemented()

    def index(self, req, tenant_id):
        """Return all instances."""
        LOG.info(_("req : '%s'\n\n") % req)
        LOG.info(_("Indexing a database instance for tenant '%s'") % tenant_id)
        context = req.environ[wsgi.CONTEXT_KEY]
        servers, marker = models.Instances.load(context)
        view = views.InstancesView(servers, req=req)
        paged = pagination.SimplePaginatedDataView(req.url, 'instances', view,
                                                   marker)
        return wsgi.Result(paged.data(), 200)

    def backups(self, req, tenant_id, id):
        """Return all backups for the specified instance."""
        LOG.info(_("req : '%s'\n\n") % req)
        LOG.info(_("Indexing backups for instance '%s'") %
                 id)
        context = req.environ[wsgi.CONTEXT_KEY]
        backups, marker = backup_model.list_for_instance(context, id)
        view = backup_views.BackupViews(backups)
        paged = pagination.SimplePaginatedDataView(req.url, 'backups', view,
                                                   marker)
        return wsgi.Result(paged.data(), 200)

    def show(self, req, tenant_id, id):
        """Return a single instance."""
        LOG.info(_("req : '%s'\n\n") % req)
        LOG.info(_("Showing a database instance for tenant '%s'") % tenant_id)
        LOG.info(_("id : '%s'\n\n") % id)

        context = req.environ[wsgi.CONTEXT_KEY]
        server = models.load_instance_with_guest(models.DetailInstance,
                                                 context, id)
        return wsgi.Result(views.InstanceDetailView(server,
                                                    req=req).data(), 200)

    def delete(self, req, tenant_id, id):
        """Delete a single instance."""
        LOG.info(_("req : '%s'\n\n") % req)
        LOG.info(_("Deleting a database instance for tenant '%s'") % tenant_id)
        LOG.info(_("id : '%s'\n\n") % id)
        # TODO(hub-cap): turn this into middleware
        context = req.environ[wsgi.CONTEXT_KEY]
        instance = models.load_any_instance(context, id)
        instance.delete()
        # TODO(cp16net): need to set the return code correctly
        return wsgi.Result(None, 202)

    def create(self, req, body, tenant_id):
        # TODO(hub-cap): turn this into middleware
        LOG.info(_("Creating a database instance for tenant '%s'") % tenant_id)
        LOG.info(logging.mask_password(_("req : '%s'\n\n") % req))
        LOG.info(logging.mask_password(_("body : '%s'\n\n") % body))
        context = req.environ[wsgi.CONTEXT_KEY]
        datastore_args = body['instance'].get('datastore', {})
        datastore, datastore_version = (
            datastore_models.get_datastore_version(**datastore_args))
        image_id = datastore_version.image_id
        name = body['instance']['name']
        flavor_ref = body['instance']['flavorRef']
        flavor_id = utils.get_id_from_href(flavor_ref)

        configuration = self._configuration_parse(context, body)
        databases = populate_validated_databases(
            body['instance'].get('databases', []))
        database_names = [database.get('_name', '') for database in databases]
        users = None
        try:
            users = populate_users(body['instance'].get('users', []),
                                   database_names)
        except ValueError as ve:
            raise exception.BadRequest(msg=ve)

        if 'volume' in body['instance']:
            volume_size = int(body['instance']['volume']['size'])
        else:
            volume_size = None

        if 'restorePoint' in body['instance']:
            backupRef = body['instance']['restorePoint']['backupRef']
            backup_id = utils.get_id_from_href(backupRef)
        else:
            backup_id = None

        if 'availability_zone' in body['instance']:
            availability_zone = body['instance']['availability_zone']
        else:
            availability_zone = None

        if 'nics' in body['instance']:
            nics = body['instance']['nics']
        else:
            nics = None

        instance = models.Instance.create(context, name, flavor_id,
                                          image_id, databases, users,
                                          datastore, datastore_version,
                                          volume_size, backup_id,
                                          availability_zone, nics,
                                          configuration)

        view = views.InstanceDetailView(instance, req=req)
        return wsgi.Result(view.data(), 200)

    def _configuration_parse(self, context, body):
        if 'configuration' in body['instance']:
            configuration_ref = body['instance']['configuration']
            if configuration_ref:
                configuration_id = utils.get_id_from_href(configuration_ref)
                return configuration_id

    def update(self, req, id, body, tenant_id):
        """Updates the instance to attach/detach configuration."""
        LOG.info(_("Updating instance for tenant id %s") % tenant_id)
        LOG.info(_("req: %s") % req)
        LOG.info(_("body: %s") % body)
        context = req.environ[wsgi.CONTEXT_KEY]

        instance = models.Instance.load(context, id)

        # if configuration is set, then we will update the instance to use
        # the new configuration.  If configuration is empty, we want to
        # disassociate the instance from the configuration group and remove the
        # active overrides file.

        configuration_id = self._configuration_parse(context, body)

        if configuration_id:
            instance.assign_configuration(configuration_id)
        else:
            instance.unassign_configuration()
        return wsgi.Result(None, 202)

    def configuration(self, req, tenant_id, id):
        """
        Returns the default configuration template applied to the instance.
        """
        LOG.debug("getting default configuration for the instance(%s)" % id)
        context = req.environ[wsgi.CONTEXT_KEY]
        instance = models.Instance.load(context, id)
        LOG.debug("server: %s" % instance)
        config = instance.get_default_configration_template()
        LOG.debug("default config for instance is: %s" % config)
        return wsgi.Result(views.DefaultConfigurationView(
                           config).data(), 200)

########NEW FILE########
__FILENAME__ = tasks
# Copyright 2012 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
"""
Common instance status code used across Trove API.
"""


class InstanceTask(object):
    """
    Stores the different kind of tasks being performed by an instance.
    """
    #TODO(tim.simpson): Figure out someway to migrate this to the TaskManager
    #                   once that revs up.
    _lookup = {}

    def __init__(self, code, action, db_text, is_error=False):
        self._code = int(code)
        self._action = action
        self._db_text = db_text
        self._is_error = is_error
        InstanceTask._lookup[self._code] = self

    @property
    def action(self):
        return self._action

    @property
    def code(self):
        return self._code

    @property
    def db_text(self):
        return self._db_text

    @property
    def is_error(self):
        return self._is_error

    def __eq__(self, other):
        if not isinstance(other, InstanceTask):
            return False
        return self._db_text == other._db_text

    @classmethod
    def from_code(cls, code):
        if code not in cls._lookup:
            return None
        return cls._lookup[code]

    def __str__(self):
        return "(%d %s %s)" % (self._code, self._action, self._db_text)

    def __repr__(self):
        return "InstanceTask.%s (%s)" % (self._action, self._db_text)


class InstanceTasks(object):
    NONE = InstanceTask(0x01, 'NONE', 'No tasks for the instance.')
    DELETING = InstanceTask(0x02, 'DELETING', 'Deleting the instance.')
    REBOOTING = InstanceTask(0x03, 'REBOOTING', 'Rebooting the instance.')
    RESIZING = InstanceTask(0x04, 'RESIZING', 'Resizing the instance.')
    BUILDING = InstanceTask(0x05, 'BUILDING', 'The instance is building.')
    MIGRATING = InstanceTask(0x06, 'MIGRATING', 'Migrating the instance.')
    RESTART_REQUIRED = InstanceTask(0x07, 'RESTART_REQUIRED',
                                    'Instance requires a restart.')

    BUILDING_ERROR_DNS = InstanceTask(0x50, 'BUILDING', 'Build error: DNS.',
                                      is_error=True)
    BUILDING_ERROR_SERVER = InstanceTask(0x51, 'BUILDING',
                                         'Build error: Server.',
                                         is_error=True)
    BUILDING_ERROR_VOLUME = InstanceTask(0x52, 'BUILDING',
                                         'Build error: Volume.',
                                         is_error=True)
    BUILDING_ERROR_TIMEOUT_GA = InstanceTask(0x54, 'ERROR',
                                             'Build error: '
                                             'guestagent timeout.',
                                             is_error=True)
    BUILDING_ERROR_SEC_GROUP = InstanceTask(0x53, 'BUILDING',
                                            'Build error: Security group '
                                            'or rule.',
                                            is_error=True)

# Dissuade further additions at run-time.
InstanceTask.__init__ = None

########NEW FILE########
__FILENAME__ = views
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from trove.openstack.common import log as logging
from trove.common import cfg
from trove.common.views import create_links
from trove.instance import models

LOG = logging.getLogger(__name__)
CONF = cfg.CONF


class InstanceView(object):
    """Uses a SimpleInstance."""

    def __init__(self, instance, req=None):
        self.instance = instance
        self.req = req

    def data(self):
        instance_dict = {
            "id": self.instance.id,
            "name": self.instance.name,
            "status": self.instance.status,
            "links": self._build_links(),
            "flavor": self._build_flavor_info(),
            "datastore": {"type": self.instance.datastore.name,
                          "version": self.instance.datastore_version.name},
        }
        if CONF.trove_volume_support:
            instance_dict['volume'] = {'size': self.instance.volume_size}

        LOG.debug(instance_dict)
        return {"instance": instance_dict}

    def _build_links(self):
        return create_links("instances", self.req, self.instance.id)

    def _build_flavor_info(self):
        return {
            "id": self.instance.flavor_id,
            "links": self._build_flavor_links()
        }

    def _build_flavor_links(self):
        return create_links("flavors", self.req,
                            self.instance.flavor_id)


class InstanceDetailView(InstanceView):
    """Works with a full-blown instance."""

    def __init__(self, instance, req):
        super(InstanceDetailView, self).__init__(instance,
                                                 req=req)

    def data(self):
        result = super(InstanceDetailView, self).data()
        result['instance']['created'] = self.instance.created
        result['instance']['updated'] = self.instance.updated

        result['instance']['datastore']['version'] = (self.instance.
                                                      datastore_version.name)

        if self.instance.configuration is not None:
            result['instance']['configuration'] = (self.
                                                   _build_configuration_info())
        if self.instance.hostname:
            result['instance']['hostname'] = self.instance.hostname
        else:
            ip = self.instance.get_visible_ip_addresses()
            if ip is not None and len(ip) > 0:
                result['instance']['ip'] = ip

        if (isinstance(self.instance, models.DetailInstance) and
                self.instance.volume_used):
            used = self.instance.volume_used
            if CONF.trove_volume_support:
                result['instance']['volume']['used'] = used
            else:
                # either ephemeral or root partition
                result['instance']['local_storage'] = {'used': used}

        if self.instance.root_password:
            result['instance']['password'] = self.instance.root_password

        return result

    def _build_configuration_info(self):
        return {
            "id": self.instance.configuration.id,
            "name": self.instance.configuration.name,
            "links": create_links("configurations", self.req,
                                  self.instance.configuration.id)
        }


class InstancesView(object):
    """Shows a list of SimpleInstance objects."""

    def __init__(self, instances, req=None):
        self.instances = instances
        self.req = req

    def data(self):
        data = []
        # These are model instances
        for instance in self.instances:
            data.append(self.data_for_instance(instance))
        return {'instances': data}

    def data_for_instance(self, instance):
        view = InstanceView(instance, req=self.req)
        return view.data()['instance']


class DefaultConfigurationView(object):
    def __init__(self, config):
        self.config = config

    def data(self):
        config_dict = {}
        for key, val in self.config:
            config_dict[key] = val
        return {"instance": {"configuration": config_dict}}

########NEW FILE########
__FILENAME__ = service
# Copyright 2013 OpenStack Foundation
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from trove.common import wsgi
from trove.limits import views
from trove.quota.quota import QUOTAS


class LimitsController(wsgi.Controller):
    """
    Controller for accessing limits in the OpenStack API.
    """

    def index(self, req, tenant_id):
        """
        Return all absolute and rate limit information.
        """
        quotas = QUOTAS.get_all_quotas_by_tenant(tenant_id)
        abs_limits = dict((k, v['hard_limit']) for k, v in quotas.items())
        rate_limits = req.environ.get("trove.limits", [])

        return wsgi.Result(views.LimitViews(abs_limits,
                                            rate_limits).data(), 200)

########NEW FILE########
__FILENAME__ = views
# Copyright 2013 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import datetime
from trove.openstack.common import timeutils


class LimitView(object):

    def __init__(self, rate_limit):
        self.rate_limit = rate_limit

    def data(self):
        get_utc = datetime.datetime.utcfromtimestamp
        next_avail = get_utc(self.rate_limit.get("resetTime", 0))

        return {"limit": {
            "nextAvailable": timeutils.isotime(at=next_avail),
            "remaining": self.rate_limit.get("remaining", 0),
            "unit": self.rate_limit.get("unit", ""),
            "value": self.rate_limit.get("value", ""),
            "verb": self.rate_limit.get("verb", ""),
            "uri": self.rate_limit.get("URI", ""),
            "regex": self.rate_limit.get("regex", "")
        }
        }


class LimitViews(object):

    def __init__(self, abs_limits, rate_limits):
        self.abs_limits = abs_limits
        self.rate_limits = rate_limits

    def data(self):
        data = []
        abs_view = dict()
        abs_view["verb"] = "ABSOLUTE"
        for resource_name, abs_limit in self.abs_limits.items():
            abs_view["max_" + resource_name] = abs_limit

        data.append(abs_view)
        for l in self.rate_limits:
            data.append(LimitView(l).data()["limit"])
        return {"limits": data}

########NEW FILE########
__FILENAME__ = config
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2011 OpenStack LLC.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Routines for configuring Openstack Projects
"""

import logging
import logging.config
import logging.handlers
import optparse
import os
import sys

from paste import deploy

DEFAULT_LOG_FORMAT = "%(asctime)s %(levelname)8s [%(name)s] %(message)s"
DEFAULT_LOG_DATE_FORMAT = "%Y-%m-%d %H:%M:%S"


def parse_options(parser, cli_args=None):
    """
    Returns the parsed CLI options, command to run and its arguments, merged
    with any same-named options found in a configuration file.

    The function returns a tuple of (options, args), where options is a
    mapping of option key/str(value) pairs, and args is the set of arguments
    (not options) supplied on the command-line.

    The reason that the option values are returned as strings only is that
    ConfigParser and paste.deploy only accept string values...

    :param parser: The option parser
    :param cli_args: (Optional) Set of arguments to process. If not present,
                     sys.argv[1:] is used.
    :retval tuple of (options, args)
    """

    (options, args) = parser.parse_args(cli_args)

    return (vars(options), args)


def add_common_options(parser):
    """
    Given a supplied optparse.OptionParser, adds an OptionGroup that
    represents all common configuration options.

    :param parser: optparse.OptionParser
    """
    help_text = ("The following configuration options are common to "
                 "this app's programs.")

    group = optparse.OptionGroup(parser, "Common Options", help_text)
    group.add_option('-v', '--verbose', default=False, dest="verbose",
                     action="store_true",
                     help="Print more verbose output")
    group.add_option('-d', '--debug', default=False, dest="debug",
                     action="store_true",
                     help="Print debugging output")
    group.add_option('--config-file', default=None, metavar="PATH",
                     help="Path to the config file to use. When not specified "
                          "(the default), we generally look at the first "
                          "argument specified to be a config file, and if "
                          "that is also missing, we search standard "
                          "directories for a config file.")
    parser.add_option_group(group)


def add_log_options(parser):
    """
    Given a supplied optparse.OptionParser, adds an OptionGroup that
    represents all the configuration options around logging.

    :param parser: optparse.OptionParser
    """
    help_text = ("The following configuration options are specific to "
                 "logging functionality for this program.")

    group = optparse.OptionGroup(parser, "Logging Options", help_text)
    group.add_option('--log-config', default=None, metavar="PATH",
                     help="If this option is specified, the logging "
                          "configuration file specified is used and overrides "
                          "any other logging options specified. Please see "
                          "the Python logging module documentation for "
                          "details on logging configuration files.")
    group.add_option('--log-date-format', metavar="FORMAT",
                     default=DEFAULT_LOG_DATE_FORMAT,
                     help="Format string for %(asctime)s in log records. "
                          "Default: %default")
    group.add_option('--log-file', default=None, metavar="PATH",
                     help="(Optional) Name of log file to output to. "
                          "If not set, logging will go to stdout.")
    group.add_option("--log-dir", default=None,
                     help="(Optional) The directory to keep log files in "
                          "(will be prepended to --logfile)")
    group.add_option('--use-syslog', default=False, dest="use_syslog",
                     action="store_true",
                     help="Use syslog for logging.")
    parser.add_option_group(group)


def setup_logging(options, conf):
    """
    Sets up the logging options for a log with supplied name

    :param options: Mapping of typed option key/values
    :param conf: Mapping of untyped key/values from config file
    """

    if options.get('log_config', None):
        # Use a logging configuration file for all settings...
        if os.path.exists(options['log_config']):
            logging.config.fileConfig(options['log_config'])
            return
        else:
            raise RuntimeError("Unable to locate specified logging "
                               "config file: %s" % options['log_config'])

    # If either the CLI option or the conf value
    # is True, we set to True
    debug = (options.get('debug') or
             get_option(conf, 'debug', type='bool', default=False))
    verbose = (options.get('verbose') or
               get_option(conf, 'verbose', type='bool', default=False))
    root_logger = logging.root
    if debug:
        root_logger.setLevel(logging.DEBUG)
    elif verbose:
        root_logger.setLevel(logging.INFO)
    else:
        root_logger.setLevel(logging.WARNING)

    # Set log configuration from options...
    # Note that we use a hard-coded log format in the options
    # because of Paste.Deploy bug #379
    # http://trac.pythonpaste.org/pythonpaste/ticket/379
    log_format = options.get('log_format', DEFAULT_LOG_FORMAT)
    log_date_format = options.get('log_date_format', DEFAULT_LOG_DATE_FORMAT)
    formatter = logging.Formatter(log_format, log_date_format)

    logfile = options.get('log_file')
    if not logfile:
        logfile = conf.get('log_file')

    use_syslog = (options.get('use_syslog') or
                  get_option(conf, 'use_syslog', type='bool', default=False))

    if use_syslog:
        handler = logging.handlers.SysLogHandler(address='/dev/log')
    elif logfile:
        logdir = options.get('log_dir')
        if not logdir:
            logdir = conf.get('log_dir')
        if logdir:
            logfile = os.path.join(logdir, logfile)
        handler = logging.FileHandler(logfile)
    else:
        handler = logging.StreamHandler(sys.stdout)

    handler.setFormatter(formatter)
    root_logger.addHandler(handler)


def fix_path(path):
    """
    Return the full absolute path
    """
    return os.path.abspath(os.path.expanduser(path))


def find_config_file(app_name, options, args, config_dir=None):
    """
    Return the first config file found for an application.

    We search for the paste config file in the following order:
    * If --config-file option is used, use that
    * If args[0] is a file, use that
    * Search for $app.conf in standard directories:
        * .
        * ~.config_dir/
        * ~
        * /etc/config_dir
        * /etc

    :retval Full path to config file, or None if no config file found
    """
    config_dir = config_dir or app_name

    if options.get('config_file'):
        if os.path.exists(options['config_file']):
            return fix_path(options['config_file'])
    elif args:
        if os.path.exists(args[0]):
            return fix_path(args[0])

    # Handle standard directory search for $app_name.conf
    config_file_dirs = [fix_path(os.getcwd()),
                        fix_path(os.path.join('~', '.' + config_dir)),
                        fix_path('~'),
                        os.path.join('/etc', config_dir),
                        '/etc']

    for cfg_dir in config_file_dirs:
        cfg_file = os.path.join(cfg_dir, '%s.conf' % app_name)
        if os.path.exists(cfg_file):
            return cfg_file


def load_paste_config(app_name, options, args, config_dir=None):
    """
    Looks for a config file to use for an app and returns the
    config file path and a configuration mapping from a paste config file.

    We search for the paste config file in the following order:
    * If --config-file option is used, use that
    * If args[0] is a file, use that
    * Search for $app_name.conf in standard directories:
        * .
        * ~.config_dir/
        * ~
        * /etc/config_dir
        * /etc

    :param app_name: Name of the application to load config for, or None.
                     None signifies to only load the [DEFAULT] section of
                     the config file.
    :param options: Set of typed options returned from parse_options()
    :param args: Command line arguments from argv[1:]
    :retval Tuple of (conf_file, conf)

    :raises RuntimeError when config file cannot be located or there was a
            problem loading the configuration file.
    """
    conf_file = find_config_file(app_name, options, args, config_dir)
    if not conf_file:
        raise RuntimeError("Unable to locate any configuration file. "
                           "Cannot load application %s" % app_name)
    try:
        conf = deploy.appconfig("config:%s" % conf_file, name=app_name)
        return conf_file, conf
    except Exception, e:
        raise RuntimeError("Error trying to load config %s: %s"
                           % (conf_file, e))


def load_paste_app(app_name, options, args, config_dir=None):
    """
    Builds and returns a WSGI app from a paste config file.

    We search for the paste config file in the following order:
    * If --config-file option is used, use that
    * If args[0] is a file, use that
    * Search for $app_name.conf in standard directories:
        * .
        * ~.config_dir/
        * ~
        * /etc/config_dir
        * /etc

    :param app_name: Name of the application to load
    :param options: Set of typed options returned from parse_options()
    :param args: Command line arguments from argv[1:]

    :raises RuntimeError when config file cannot be located or application
            cannot be loaded from config file
    """
    conf_file, conf = load_paste_config(app_name, options,
                                        args, config_dir)

    try:
        # Setup logging early, supplying both the CLI options and the
        # configuration mapping from the config file
        setup_logging(options, conf)

        # We only update the conf dict for the verbose and debug
        # flags. Everything else must be set up in the conf file...
        debug = (options.get('debug') or
                 get_option(conf, 'debug', type='bool', default=False))
        verbose = (options.get('verbose') or
                   get_option(conf, 'verbose', type='bool', default=False))
        conf['debug'] = debug
        conf['verbose'] = verbose

        # Log the options used when starting if we're in debug mode...
        if debug:
            logger = logging.getLogger(app_name)
            logger.debug("*" * 80)
            logger.debug("Configuration options gathered from config file:")
            logger.debug(conf_file)
            logger.debug("================================================")
            items = dict([(k, v) for k, v in conf.items()
                          if k not in ('__file__', 'here')])
            for key, value in sorted(items.items()):
                logger.debug("%(key)-30s %(value)s" % locals())
            logger.debug("*" * 80)
        app = deploy.loadapp("config:%s" % conf_file, name=app_name)
    except (LookupError, ImportError), e:
        import traceback
        print traceback.format_exc()
        raise RuntimeError("Unable to load %(app_name)s from "
                           "configuration file %(conf_file)s."
                           "\nGot: %(e)r" % locals())
    return conf, app


def get_option(options, option, **kwargs):
    if option in options:
        value = options[option]
        type_ = kwargs.get('type', 'str')
        if type_ == 'bool':
            if hasattr(value, 'lower'):
                return value.lower() == 'true'
            else:
                return value
        elif type_ == 'int':
            return int(value)
        elif type_ == 'float':
            return float(value)
        else:
            return value
    elif 'default' in kwargs:
        return kwargs['default']
    else:
        raise KeyError("option '%s' not found" % option)

########NEW FILE########
__FILENAME__ = context
# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Simple class that stores security context information in the web request.

Projects should subclass this class if they wish to enhance the request
context or provide additional information in their specific WSGI pipeline.
"""

import itertools
import uuid


def generate_request_id():
    return 'req-%s' % str(uuid.uuid4())


class RequestContext(object):

    """Helper class to represent useful information about a request context.

    Stores information about the security context under which the user
    accesses the system, as well as additional request information.
    """

    user_idt_format = '{user} {tenant} {domain} {user_domain} {p_domain}'

    def __init__(self, auth_token=None, user=None, tenant=None, domain=None,
                 user_domain=None, project_domain=None, is_admin=False,
                 read_only=False, show_deleted=False, request_id=None,
                 instance_uuid=None):
        self.auth_token = auth_token
        self.user = user
        self.tenant = tenant
        self.domain = domain
        self.user_domain = user_domain
        self.project_domain = project_domain
        self.is_admin = is_admin
        self.read_only = read_only
        self.show_deleted = show_deleted
        self.instance_uuid = instance_uuid
        if not request_id:
            request_id = generate_request_id()
        self.request_id = request_id

    def to_dict(self):
        user_idt = (
            self.user_idt_format.format(user=self.user or '-',
                                        tenant=self.tenant or '-',
                                        domain=self.domain or '-',
                                        user_domain=self.user_domain or '-',
                                        p_domain=self.project_domain or '-'))

        return {'user': self.user,
                'tenant': self.tenant,
                'domain': self.domain,
                'user_domain': self.user_domain,
                'project_domain': self.project_domain,
                'is_admin': self.is_admin,
                'read_only': self.read_only,
                'show_deleted': self.show_deleted,
                'auth_token': self.auth_token,
                'request_id': self.request_id,
                'instance_uuid': self.instance_uuid,
                'user_identity': user_idt}


def get_admin_context(show_deleted=False):
    context = RequestContext(None,
                             tenant=None,
                             is_admin=True,
                             show_deleted=show_deleted)
    return context


def get_context_from_function_and_args(function, args, kwargs):
    """Find an arg of type RequestContext and return it.

       This is useful in a couple of decorators where we don't
       know much about the function we're wrapping.
    """

    for arg in itertools.chain(kwargs.values(), args):
        if isinstance(arg, RequestContext):
            return arg

    return None

########NEW FILE########
__FILENAME__ = utils
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2013 Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import base64

from Crypto.Hash import HMAC
from Crypto import Random

from trove.openstack.common.gettextutils import _  # noqa
from trove.openstack.common import importutils


class CryptoutilsException(Exception):
    """Generic Exception for Crypto utilities."""

    message = _("An unknown error occurred in crypto utils.")


class CipherBlockLengthTooBig(CryptoutilsException):
    """The block size is too big."""

    def __init__(self, requested, permitted):
        msg = _("Block size of %(given)d is too big, max = %(maximum)d")
        message = msg % {'given': requested, 'maximum': permitted}
        super(CryptoutilsException, self).__init__(message)


class HKDFOutputLengthTooLong(CryptoutilsException):
    """The amount of Key Material asked is too much."""

    def __init__(self, requested, permitted):
        msg = _("Length of %(given)d is too long, max = %(maximum)d")
        message = msg % {'given': requested, 'maximum': permitted}
        super(CryptoutilsException, self).__init__(message)


class HKDF(object):
    """An HMAC-based Key Derivation Function implementation (RFC5869)

    This class creates an object that allows to use HKDF to derive keys.
    """

    def __init__(self, hashtype='SHA256'):
        self.hashfn = importutils.import_module('Crypto.Hash.' + hashtype)
        self.max_okm_length = 255 * self.hashfn.digest_size

    def extract(self, ikm, salt=None):
        """An extract function that can be used to derive a robust key given
        weak Input Key Material (IKM) which could be a password.
        Returns a pseudorandom key (of HashLen octets)

        :param ikm: input keying material (ex a password)
        :param salt: optional salt value (a non-secret random value)
        """
        if salt is None:
            salt = '\x00' * self.hashfn.digest_size

        return HMAC.new(salt, ikm, self.hashfn).digest()

    def expand(self, prk, info, length):
        """An expand function that will return arbitrary length output that can
        be used as keys.
        Returns a buffer usable as key material.

        :param prk: a pseudorandom key of at least HashLen octets
        :param info: optional string (can be a zero-length string)
        :param length: length of output keying material (<= 255 * HashLen)
        """
        if length > self.max_okm_length:
            raise HKDFOutputLengthTooLong(length, self.max_okm_length)

        N = (length + self.hashfn.digest_size - 1) / self.hashfn.digest_size

        okm = ""
        tmp = ""
        for block in range(1, N + 1):
            tmp = HMAC.new(prk, tmp + info + chr(block), self.hashfn).digest()
            okm += tmp

        return okm[:length]


MAX_CB_SIZE = 256


class SymmetricCrypto(object):
    """Symmetric Key Crypto object.

    This class creates a Symmetric Key Crypto object that can be used
    to encrypt, decrypt, or sign arbitrary data.

    :param enctype: Encryption Cipher name (default: AES)
    :param hashtype: Hash/HMAC type name (default: SHA256)
    """

    def __init__(self, enctype='AES', hashtype='SHA256'):
        self.cipher = importutils.import_module('Crypto.Cipher.' + enctype)
        self.hashfn = importutils.import_module('Crypto.Hash.' + hashtype)

    def new_key(self, size):
        return Random.new().read(size)

    def encrypt(self, key, msg, b64encode=True):
        """Encrypt the provided msg and returns the cyphertext optionally
        base64 encoded.

        Uses AES-128-CBC with a Random IV by default.

        The plaintext is padded to reach blocksize length.
        The last byte of the block is the length of the padding.
        The length of the padding does not include the length byte itself.

        :param key: The Encryption key.
        :param msg: the plain text.

        :returns encblock: a block of encrypted data.
        """
        iv = Random.new().read(self.cipher.block_size)
        cipher = self.cipher.new(key, self.cipher.MODE_CBC, iv)

        # CBC mode requires a fixed block size. Append padding and length of
        # padding.
        if self.cipher.block_size > MAX_CB_SIZE:
            raise CipherBlockLengthTooBig(self.cipher.block_size, MAX_CB_SIZE)
        r = len(msg) % self.cipher.block_size
        padlen = self.cipher.block_size - r - 1
        msg += '\x00' * padlen
        msg += chr(padlen)

        enc = iv + cipher.encrypt(msg)
        if b64encode:
            enc = base64.b64encode(enc)
        return enc

    def decrypt(self, key, msg, b64decode=True):
        """Decrypts the provided ciphertext, optionally base 64 encoded, and
        returns the plaintext message, after padding is removed.

        Uses AES-128-CBC with an IV by default.

        :param key: The Encryption key.
        :param msg: the ciphetext, the first block is the IV
        """
        if b64decode:
            msg = base64.b64decode(msg)
        iv = msg[:self.cipher.block_size]
        cipher = self.cipher.new(key, self.cipher.MODE_CBC, iv)

        padded = cipher.decrypt(msg[self.cipher.block_size:])
        l = ord(padded[-1]) + 1
        plain = padded[:-l]
        return plain

    def sign(self, key, msg, b64encode=True):
        """Signs a message string and returns a base64 encoded signature.

        Uses HMAC-SHA-256 by default.

        :param key: The Signing key.
        :param msg: the message to sign.
        """
        h = HMAC.new(key, msg, self.hashfn)
        out = h.digest()
        if b64encode:
            out = base64.b64encode(out)
        return out

########NEW FILE########
__FILENAME__ = eventlet_backdoor
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright (c) 2012 OpenStack Foundation.
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from __future__ import print_function

import errno
import gc
import os
import pprint
import socket
import sys
import traceback

import eventlet
import eventlet.backdoor
import greenlet
from oslo.config import cfg

from trove.openstack.common.gettextutils import _  # noqa
from trove.openstack.common import log as logging

help_for_backdoor_port = (
    "Acceptable values are 0, <port>, and <start>:<end>, where 0 results "
    "in listening on a random tcp port number; <port> results in listening "
    "on the specified port number (and not enabling backdoor if that port "
    "is in use); and <start>:<end> results in listening on the smallest "
    "unused port number within the specified range of port numbers.  The "
    "chosen port is displayed in the service's log file.")
eventlet_backdoor_opts = [
    cfg.StrOpt('backdoor_port',
               default=None,
               help="Enable eventlet backdoor.  %s" % help_for_backdoor_port)
]

CONF = cfg.CONF
CONF.register_opts(eventlet_backdoor_opts)
LOG = logging.getLogger(__name__)


class EventletBackdoorConfigValueError(Exception):
    def __init__(self, port_range, help_msg, ex):
        msg = ('Invalid backdoor_port configuration %(range)s: %(ex)s. '
               '%(help)s' %
               {'range': port_range, 'ex': ex, 'help': help_msg})
        super(EventletBackdoorConfigValueError, self).__init__(msg)
        self.port_range = port_range


def _dont_use_this():
    print("Don't use this, just disconnect instead")


def _find_objects(t):
    return filter(lambda o: isinstance(o, t), gc.get_objects())


def _print_greenthreads():
    for i, gt in enumerate(_find_objects(greenlet.greenlet)):
        print(i, gt)
        traceback.print_stack(gt.gr_frame)
        print()


def _print_nativethreads():
    for threadId, stack in sys._current_frames().items():
        print(threadId)
        traceback.print_stack(stack)
        print()


def _parse_port_range(port_range):
    if ':' not in port_range:
        start, end = port_range, port_range
    else:
        start, end = port_range.split(':', 1)
    try:
        start, end = int(start), int(end)
        if end < start:
            raise ValueError
        return start, end
    except ValueError as ex:
        raise EventletBackdoorConfigValueError(port_range, ex,
                                               help_for_backdoor_port)


def _listen(host, start_port, end_port, listen_func):
    try_port = start_port
    while True:
        try:
            return listen_func((host, try_port))
        except socket.error as exc:
            if (exc.errno != errno.EADDRINUSE or
               try_port >= end_port):
                raise
            try_port += 1


def initialize_if_enabled():
    backdoor_locals = {
        'exit': _dont_use_this,      # So we don't exit the entire process
        'quit': _dont_use_this,      # So we don't exit the entire process
        'fo': _find_objects,
        'pgt': _print_greenthreads,
        'pnt': _print_nativethreads,
    }

    if CONF.backdoor_port is None:
        return None

    start_port, end_port = _parse_port_range(str(CONF.backdoor_port))

    # NOTE(johannes): The standard sys.displayhook will print the value of
    # the last expression and set it to __builtin__._, which overwrites
    # the __builtin__._ that gettext sets. Let's switch to using pprint
    # since it won't interact poorly with gettext, and it's easier to
    # read the output too.
    def displayhook(val):
        if val is not None:
            pprint.pprint(val)
    sys.displayhook = displayhook

    sock = _listen('localhost', start_port, end_port, eventlet.listen)

    # In the case of backdoor port being zero, a port number is assigned by
    # listen().  In any case, pull the port number out here.
    port = sock.getsockname()[1]
    LOG.info(_('Eventlet backdoor listening on %(port)s for process %(pid)d') %
             {'port': port, 'pid': os.getpid()})
    eventlet.spawn_n(eventlet.backdoor.backdoor_server, sock,
                     locals=backdoor_locals)
    return port

########NEW FILE########
__FILENAME__ = exception
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Exceptions common to OpenStack projects
"""

import logging

from trove.openstack.common.gettextutils import _

_FATAL_EXCEPTION_FORMAT_ERRORS = False


class Error(Exception):
    def __init__(self, message=None):
        super(Error, self).__init__(message)


class ApiError(Error):
    def __init__(self, message='Unknown', code='Unknown'):
        self.message = message
        self.code = code
        super(ApiError, self).__init__('%s: %s' % (code, message))


class NotFound(Error):
    pass


class UnknownScheme(Error):

    msg = "Unknown scheme '%s' found in URI"

    def __init__(self, scheme):
        msg = self.__class__.msg % scheme
        super(UnknownScheme, self).__init__(msg)


class BadStoreUri(Error):

    msg = "The Store URI %s was malformed. Reason: %s"

    def __init__(self, uri, reason):
        msg = self.__class__.msg % (uri, reason)
        super(BadStoreUri, self).__init__(msg)


class Duplicate(Error):
    pass


class NotAuthorized(Error):
    pass


class NotEmpty(Error):
    pass


class Invalid(Error):
    pass


class BadInputError(Exception):
    """Error resulting from a client sending bad input to a server"""
    pass


class MissingArgumentError(Error):
    pass


class DatabaseMigrationError(Error):
    pass


class ClientConnectionError(Exception):
    """Error resulting from a client connecting to a server"""
    pass


def wrap_exception(f):
    def _wrap(*args, **kw):
        try:
            return f(*args, **kw)
        except Exception as e:
            if not isinstance(e, Error):
                #exc_type, exc_value, exc_traceback = sys.exc_info()
                logging.exception(_('Uncaught exception'))
                #logging.error(traceback.extract_stack(exc_traceback))
                raise Error(str(e))
            raise
    _wrap.func_name = f.func_name
    return _wrap


class OpenstackException(Exception):
    """
    Base Exception

    To correctly use this class, inherit from it and define
    a 'message' property. That message will get printf'd
    with the keyword arguments provided to the constructor.
    """
    message = "An unknown exception occurred"

    def __init__(self, **kwargs):
        try:
            self._error_string = self.message % kwargs

        except Exception as e:
            if _FATAL_EXCEPTION_FORMAT_ERRORS:
                raise e
            else:
                # at least get the core message out if something happened
                self._error_string = self.message

    def __str__(self):
        return self._error_string


class MalformedRequestBody(OpenstackException):
    message = "Malformed message body: %(reason)s"


class InvalidContentType(OpenstackException):
    message = "Invalid content type %(content_type)s"

########NEW FILE########
__FILENAME__ = excutils
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2011 OpenStack Foundation.
# Copyright 2012, Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Exception related utilities.
"""

import logging
import sys
import time
import traceback

import six

from trove.openstack.common.gettextutils import _  # noqa


class save_and_reraise_exception(object):
    """Save current exception, run some code and then re-raise.

    In some cases the exception context can be cleared, resulting in None
    being attempted to be re-raised after an exception handler is run. This
    can happen when eventlet switches greenthreads or when running an
    exception handler, code raises and catches an exception. In both
    cases the exception context will be cleared.

    To work around this, we save the exception state, run handler code, and
    then re-raise the original exception. If another exception occurs, the
    saved exception is logged and the new exception is re-raised.

    In some cases the caller may not want to re-raise the exception, and
    for those circumstances this context provides a reraise flag that
    can be used to suppress the exception.  For example:

    except Exception:
        with save_and_reraise_exception() as ctxt:
            decide_if_need_reraise()
            if not should_be_reraised:
                ctxt.reraise = False
    """
    def __init__(self):
        self.reraise = True

    def __enter__(self):
        self.type_, self.value, self.tb, = sys.exc_info()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if exc_type is not None:
            logging.error(_('Original exception being dropped: %s'),
                          traceback.format_exception(self.type_,
                                                     self.value,
                                                     self.tb))
            return False
        if self.reraise:
            six.reraise(self.type_, self.value, self.tb)


def forever_retry_uncaught_exceptions(infunc):
    def inner_func(*args, **kwargs):
        last_log_time = 0
        last_exc_message = None
        exc_count = 0
        while True:
            try:
                return infunc(*args, **kwargs)
            except Exception as exc:
                this_exc_message = unicode(exc)
                if this_exc_message == last_exc_message:
                    exc_count += 1
                else:
                    exc_count = 1
                # Do not log any more frequently than once a minute unless
                # the exception message changes
                cur_time = int(time.time())
                if (cur_time - last_log_time > 60 or
                        this_exc_message != last_exc_message):
                    logging.exception(
                        _('Unexpected exception occurred %d time(s)... '
                          'retrying.') % exc_count)
                    last_log_time = cur_time
                    last_exc_message = this_exc_message
                    exc_count = 0
                # This should be a very rare event. In case it isn't, do
                # a sleep.
                time.sleep(1)
    return inner_func

########NEW FILE########
__FILENAME__ = extensions
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2011 OpenStack LLC.
# Copyright 2011 Justin Santa Barbara
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import imp
import os
import routes
import webob.dec
import webob.exc
import logging
from lxml import etree

from trove.openstack.common import exception
from trove.openstack.common import wsgi

LOG = logging.getLogger('extensions')
DEFAULT_XMLNS = "http://docs.openstack.org/"
XMLNS_ATOM = "http://www.w3.org/2005/Atom"


class ExtensionDescriptor(object):
    """Base class that defines the contract for extensions.

    Note that you don't have to derive from this class to have a valid
    extension; it is purely a convenience.

    """

    def get_name(self):
        """The name of the extension.

        e.g. 'Fox In Socks'

        """
        raise NotImplementedError()

    def get_alias(self):
        """The alias for the extension.

        e.g. 'FOXNSOX'

        """
        raise NotImplementedError()

    def get_description(self):
        """Friendly description for the extension.

        e.g. 'The Fox In Socks Extension'

        """
        raise NotImplementedError()

    def get_namespace(self):
        """The XML namespace for the extension.

        e.g. 'http://www.fox.in.socks/api/ext/pie/v1.0'

        """
        raise NotImplementedError()

    def get_updated(self):
        """The timestamp when the extension was last updated.

        e.g. '2011-01-22T13:25:27-06:00'

        """
        # NOTE(justinsb): Not sure of the purpose of this is, vs the XML NS
        raise NotImplementedError()

    def get_resources(self):
        """List of extensions.ResourceExtension extension objects.

        Resources define new nouns, and are accessible through URLs.

        """
        resources = []
        return resources

    def get_actions(self):
        """List of extensions.ActionExtension extension objects.

        Actions are verbs callable from the API.

        """
        actions = []
        return actions

    def get_request_extensions(self):
        """List of extensions.RequestException extension objects.

        Request extensions are used to handle custom request data.

        """
        request_exts = []
        return request_exts


class ActionExtensionController(object):
    def __init__(self, application):
        self.application = application
        self.action_handlers = {}

    def add_action(self, action_name, handler):
        self.action_handlers[action_name] = handler

    def action(self, req, id, body):
        for action_name, handler in self.action_handlers.iteritems():
            if action_name in body:
                return handler(body, req, id)
        # no action handler found (bump to downstream application)
        res = self.application
        return res


class ActionExtensionResource(wsgi.Resource):

    def __init__(self, application):
        controller = ActionExtensionController(application)
        wsgi.Resource.__init__(self, controller)

    def add_action(self, action_name, handler):
        self.controller.add_action(action_name, handler)


class RequestExtensionController(object):

    def __init__(self, application):
        self.application = application
        self.handlers = []

    def add_handler(self, handler):
        self.handlers.append(handler)

    def process(self, req, *args, **kwargs):
        res = req.get_response(self.application)
        # currently request handlers are un-ordered
        for handler in self.handlers:
            res = handler(req, res)
        return res


class RequestExtensionResource(wsgi.Resource):

    def __init__(self, application):
        controller = RequestExtensionController(application)
        wsgi.Resource.__init__(self, controller)

    def add_handler(self, handler):
        self.controller.add_handler(handler)


class ExtensionsResource(wsgi.Resource):

    def __init__(self, extension_manager):
        self.extension_manager = extension_manager
        body_serializers = {'application/xml': ExtensionsXMLSerializer()}
        serializer = wsgi.ResponseSerializer(body_serializers=body_serializers)
        super(ExtensionsResource, self).__init__(self, None, serializer)

    def _translate(self, ext):
        ext_data = {}
        ext_data['name'] = ext.get_name()
        ext_data['alias'] = ext.get_alias()
        ext_data['description'] = ext.get_description()
        ext_data['namespace'] = ext.get_namespace()
        ext_data['updated'] = ext.get_updated()
        ext_data['links'] = []  # TODO(dprince): implement extension links
        return ext_data

    def index(self, req):
        extensions = []
        for _alias, ext in self.extension_manager.extensions.iteritems():
            extensions.append(self._translate(ext))
        return dict(extensions=extensions)

    def show(self, req, id):
        # NOTE(dprince): the extensions alias is used as the 'id' for show
        ext = self.extension_manager.extensions.get(id, None)
        if not ext:
            raise webob.exc.HTTPNotFound(
                _("Extension with alias %s does not exist") % id)

        return dict(extension=self._translate(ext))

    def delete(self, req, id):
        raise webob.exc.HTTPNotFound()

    def create(self, req):
        raise webob.exc.HTTPNotFound()


class ExtensionMiddleware(wsgi.Middleware):
    """Extensions middleware for WSGI."""

    @classmethod
    def factory(cls, global_config, **local_config):
        """Paste factory."""
        def _factory(app):
            return cls(app, global_config, **local_config)
        return _factory

    def _action_ext_resources(self, application, ext_mgr, mapper):
        """Return a dict of ActionExtensionResource-s by collection."""
        action_resources = {}
        for action in ext_mgr.get_actions():
            if not action.collection in action_resources.keys():
                resource = ActionExtensionResource(application)
                mapper.connect("/%s/:(id)/action.:(format)" %
                               action.collection,
                               action='action',
                               controller=resource,
                               conditions=dict(method=['POST']))
                mapper.connect("/%s/:(id)/action" %
                               action.collection,
                               action='action',
                               controller=resource,
                               conditions=dict(method=['POST']))
                action_resources[action.collection] = resource

        return action_resources

    def _request_ext_resources(self, application, ext_mgr, mapper):
        """Returns a dict of RequestExtensionResource-s by collection."""
        request_ext_resources = {}
        for req_ext in ext_mgr.get_request_extensions():
            if not req_ext.key in request_ext_resources.keys():
                resource = RequestExtensionResource(application)
                mapper.connect(req_ext.url_route + '.:(format)',
                               action='process',
                               controller=resource,
                               conditions=req_ext.conditions)

                mapper.connect(req_ext.url_route,
                               action='process',
                               controller=resource,
                               conditions=req_ext.conditions)
                request_ext_resources[req_ext.key] = resource

        return request_ext_resources

    def __init__(self, application, config, ext_mgr=None):
        ext_mgr = (ext_mgr or
                   ExtensionManager(config['api_extensions_path']))
        mapper = routes.Mapper()

        # extended resources
        for resource_ext in ext_mgr.get_resources():
            LOG.debug(_('Extended resource: %s'), resource_ext.collection)
            controller_resource = wsgi.Resource(resource_ext.controller,
                                                resource_ext.deserializer,
                                                resource_ext.serializer)
            self._map_custom_collection_actions(resource_ext, mapper,
                                                controller_resource)
            kargs = dict(controller=controller_resource,
                         collection=resource_ext.collection_actions,
                         member=resource_ext.member_actions)
            if resource_ext.parent:
                kargs['parent_resource'] = resource_ext.parent
            mapper.resource(resource_ext.collection,
                            resource_ext.collection, **kargs)

        # extended actions
        action_resources = self._action_ext_resources(application, ext_mgr,
                                                      mapper)
        for action in ext_mgr.get_actions():
            LOG.debug(_('Extended action: %s'), action.action_name)
            resource = action_resources[action.collection]
            resource.add_action(action.action_name, action.handler)

        # extended requests
        req_controllers = self._request_ext_resources(application, ext_mgr,
                                                      mapper)
        for request_ext in ext_mgr.get_request_extensions():
            LOG.debug(_('Extended request: %s'), request_ext.key)
            controller = req_controllers[request_ext.key]
            controller.add_handler(request_ext.handler)

        self._router = routes.middleware.RoutesMiddleware(self._dispatch,
                                                          mapper)

        super(ExtensionMiddleware, self).__init__(application)

    def _map_custom_collection_actions(self, resource_ext, mapper,
                                       controller_resource):
        for action, method in resource_ext.collection_actions.iteritems():
            parent = resource_ext.parent
            conditions = dict(method=[method])
            path = "/%s/%s" % (resource_ext.collection, action)

            path_prefix = ""
            if parent:
                path_prefix = "/%s/{%s_id}" % (parent["collection_name"],
                                               parent["member_name"])

            with mapper.submapper(controller=controller_resource,
                                  action=action,
                                  path_prefix=path_prefix,
                                  conditions=conditions) as submap:
                submap.connect(path)
                submap.connect("%s.:(format)" % path)

    @webob.dec.wsgify(RequestClass=wsgi.Request)
    def __call__(self, req):
        """Route the incoming request with router."""
        req.environ['extended.app'] = self.application
        return self._router

    @staticmethod
    @webob.dec.wsgify(RequestClass=wsgi.Request)
    def _dispatch(req):
        """Dispatch the request.

        Returns the routed WSGI app's response or defers to the extended
        application.

        """
        match = req.environ['wsgiorg.routing_args'][1]
        if not match:
            return req.environ['extended.app']
        app = match['controller']
        return app


class ExtensionManager(object):
    """Load extensions from the configured extension path.

    See nova/tests/api/openstack/extensions/foxinsocks/extension.py for an
    example extension implementation.

    """

    def __init__(self, path):
        LOG.debug(_('Initializing extension manager.'))

        self.path = path
        self.extensions = {}
        self._load_all_extensions()

    def get_resources(self):
        """Returns a list of ResourceExtension objects."""
        resources = []
        extension_resource = ExtensionsResource(self)
        res_ext = ResourceExtension('extensions',
                                    extension_resource,
                                    serializer=extension_resource.serializer)
        resources.append(res_ext)
        for alias, ext in self.extensions.iteritems():
            try:
                resources.extend(ext.get_resources())
            except AttributeError:
                # NOTE(dprince): Extension aren't required to have resource
                # extensions
                pass
        return resources

    def get_actions(self):
        """Returns a list of ActionExtension objects."""
        actions = []
        for alias, ext in self.extensions.iteritems():
            try:
                actions.extend(ext.get_actions())
            except AttributeError:
                # NOTE(dprince): Extension aren't required to have action
                # extensions
                pass
        return actions

    def get_request_extensions(self):
        """Returns a list of RequestExtension objects."""
        request_exts = []
        for alias, ext in self.extensions.iteritems():
            try:
                request_exts.extend(ext.get_request_extensions())
            except AttributeError:
                # NOTE(dprince): Extension aren't required to have request
                # extensions
                pass
        return request_exts

    def _check_extension(self, extension):
        """Checks for required methods in extension objects."""
        try:
            LOG.debug(_('Ext name: %s'), extension.get_name())
            LOG.debug(_('Ext alias: %s'), extension.get_alias())
            LOG.debug(_('Ext description: %s'), extension.get_description())
            LOG.debug(_('Ext namespace: %s'), extension.get_namespace())
            LOG.debug(_('Ext updated: %s'), extension.get_updated())
        except AttributeError as ex:
            LOG.exception(_("Exception loading extension: %s"), unicode(ex))
            return False
        return True

    def _load_all_extensions(self):
        """Load extensions from the configured path.

        Load extensions from the configured path. The extension name is
        constructed from the module_name. If your extension module was named
        widgets.py the extension class within that module should be
        'Widgets'.

        In addition, extensions are loaded from the 'contrib' directory.

        See nova/tests/api/openstack/extensions/foxinsocks.py for an example
        extension implementation.

        """
        if os.path.exists(self.path):
            self._load_all_extensions_from_path(self.path)

        contrib_path = os.path.join(os.path.dirname(__file__), "contrib")
        if os.path.exists(contrib_path):
            self._load_all_extensions_from_path(contrib_path)

    def _load_all_extensions_from_path(self, path):
        for f in os.listdir(path):
            LOG.debug(_('Loading extension file: %s'), f)
            mod_name, file_ext = os.path.splitext(os.path.split(f)[-1])
            ext_path = os.path.join(path, f)
            if file_ext.lower() == '.py' and not mod_name.startswith('_'):
                mod = imp.load_source(mod_name, ext_path)
                ext_name = mod_name[0].upper() + mod_name[1:]
                new_ext_class = getattr(mod, ext_name, None)
                if not new_ext_class:
                    LOG.warn(_('Did not find expected name '
                               '"%(ext_name)s" in %(file)s'),
                             {'ext_name': ext_name,
                              'file': ext_path})
                    continue
                new_ext = new_ext_class()
                self.add_extension(new_ext)

    def add_extension(self, ext):
        # Do nothing if the extension doesn't check out
        if not self._check_extension(ext):
            return

        alias = ext.get_alias()
        LOG.debug(_('Loaded extension: %s'), alias)

        if alias in self.extensions:
            raise exception.Error("Found duplicate extension: %s" % alias)
        self.extensions[alias] = ext


class RequestExtension(object):
    """Extend requests and responses of core nova OpenStack API resources.

    Provide a way to add data to responses and handle custom request data
    that is sent to core nova OpenStack API controllers.

    """
    def __init__(self, method, url_route, handler):
        self.url_route = url_route
        self.handler = handler
        self.conditions = dict(method=[method])
        self.key = "%s-%s" % (method, url_route)


class ActionExtension(object):
    """Add custom actions to core nova OpenStack API resources."""

    def __init__(self, collection, action_name, handler):
        self.collection = collection
        self.action_name = action_name
        self.handler = handler


class ResourceExtension(object):
    """Add top level resources to the OpenStack API in nova."""

    def __init__(self, collection, controller, parent=None,
                 collection_actions=None, member_actions=None,
                 deserializer=None, serializer=None):
        if not collection_actions:
            collection_actions = {}
        if not member_actions:
            member_actions = {}
        self.collection = collection
        self.controller = controller
        self.parent = parent
        self.collection_actions = collection_actions
        self.member_actions = member_actions
        self.deserializer = deserializer
        self.serializer = serializer


class ExtensionsXMLSerializer(wsgi.XMLDictSerializer):

    def __init__(self):
        self.nsmap = {None: DEFAULT_XMLNS, 'atom': XMLNS_ATOM}

    def show(self, ext_dict):
        ext = etree.Element('extension', nsmap=self.nsmap)
        self._populate_ext(ext, ext_dict['extension'])
        return self._to_xml(ext)

    def index(self, exts_dict):
        exts = etree.Element('extensions', nsmap=self.nsmap)
        for ext_dict in exts_dict['extensions']:
            ext = etree.SubElement(exts, 'extension')
            self._populate_ext(ext, ext_dict)
        return self._to_xml(exts)

    def _populate_ext(self, ext_elem, ext_dict):
        """Populate an extension xml element from a dict."""

        ext_elem.set('name', ext_dict['name'])
        ext_elem.set('namespace', ext_dict['namespace'])
        ext_elem.set('alias', ext_dict['alias'])
        ext_elem.set('updated', ext_dict['updated'])
        desc = etree.Element('description')
        desc.text = ext_dict['description']
        ext_elem.append(desc)
        for link in ext_dict.get('links', []):
            elem = etree.SubElement(ext_elem, '{%s}link' % XMLNS_ATOM)
            elem.set('rel', link['rel'])
            elem.set('href', link['href'])
            elem.set('type', link['type'])
        return ext_elem

    def _to_xml(self, root):
        """Convert the xml object to an xml string."""

        return etree.tostring(root, encoding='UTF-8')

########NEW FILE########
__FILENAME__ = fileutils
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2011 OpenStack LLC.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


import errno
import os


def ensure_tree(path):
    """Create a directory (and any ancestor directories required)

    :param path: Directory to create
    """
    try:
        os.makedirs(path)
    except OSError as exc:
        if exc.errno == errno.EEXIST:
            if not os.path.isdir(path):
                raise
        else:
            raise

########NEW FILE########
__FILENAME__ = gettextutils
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2012 Red Hat, Inc.
# Copyright 2013 IBM Corp.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
gettext for openstack-common modules.

Usual usage in an openstack.common module:

    from trove.openstack.common.gettextutils import _
"""

import copy
import gettext
import logging
import os
import re
try:
    import UserString as _userString
except ImportError:
    import collections as _userString

from babel import localedata
import six

_localedir = os.environ.get('trove'.upper() + '_LOCALEDIR')
_t = gettext.translation('trove', localedir=_localedir, fallback=True)

_AVAILABLE_LANGUAGES = {}
USE_LAZY = False


def enable_lazy():
    """Convenience function for configuring _() to use lazy gettext

    Call this at the start of execution to enable the gettextutils._
    function to use lazy gettext functionality. This is useful if
    your project is importing _ directly instead of using the
    gettextutils.install() way of importing the _ function.
    """
    global USE_LAZY
    USE_LAZY = True


def _(msg):
    if USE_LAZY:
        return Message(msg, 'trove')
    else:
        return _t.ugettext(msg)


def install(domain, lazy=False):
    """Install a _() function using the given translation domain.

    Given a translation domain, install a _() function using gettext's
    install() function.

    The main difference from gettext.install() is that we allow
    overriding the default localedir (e.g. /usr/share/locale) using
    a translation-domain-specific environment variable (e.g.
    NOVA_LOCALEDIR).

    :param domain: the translation domain
    :param lazy: indicates whether or not to install the lazy _() function.
                 The lazy _() introduces a way to do deferred translation
                 of messages by installing a _ that builds Message objects,
                 instead of strings, which can then be lazily translated into
                 any available locale.
    """
    if lazy:
        # NOTE(mrodden): Lazy gettext functionality.
        #
        # The following introduces a deferred way to do translations on
        # messages in OpenStack. We override the standard _() function
        # and % (format string) operation to build Message objects that can
        # later be translated when we have more information.
        #
        # Also included below is an example LocaleHandler that translates
        # Messages to an associated locale, effectively allowing many logs,
        # each with their own locale.

        def _lazy_gettext(msg):
            """Create and return a Message object.

            Lazy gettext function for a given domain, it is a factory method
            for a project/module to get a lazy gettext function for its own
            translation domain (i.e. nova, glance, cinder, etc.)

            Message encapsulates a string so that we can translate
            it later when needed.
            """
            return Message(msg, domain)

        import __builtin__
        __builtin__.__dict__['_'] = _lazy_gettext
    else:
        localedir = '%s_LOCALEDIR' % domain.upper()
        gettext.install(domain,
                        localedir=os.environ.get(localedir),
                        unicode=True)


class Message(_userString.UserString, object):
    """Class used to encapsulate translatable messages."""
    def __init__(self, msg, domain):
        # _msg is the gettext msgid and should never change
        self._msg = msg
        self._left_extra_msg = ''
        self._right_extra_msg = ''
        self.params = None
        self.locale = None
        self.domain = domain

    @property
    def data(self):
        # NOTE(mrodden): this should always resolve to a unicode string
        # that best represents the state of the message currently

        localedir = os.environ.get(self.domain.upper() + '_LOCALEDIR')
        if self.locale:
            lang = gettext.translation(self.domain,
                                       localedir=localedir,
                                       languages=[self.locale],
                                       fallback=True)
        else:
            # use system locale for translations
            lang = gettext.translation(self.domain,
                                       localedir=localedir,
                                       fallback=True)

        full_msg = (self._left_extra_msg +
                    lang.ugettext(self._msg) +
                    self._right_extra_msg)

        if self.params is not None:
            full_msg = full_msg % self.params

        return six.text_type(full_msg)

    def _save_dictionary_parameter(self, dict_param):
        full_msg = self.data
        # look for %(blah) fields in string;
        # ignore %% and deal with the
        # case where % is first character on the line
        keys = re.findall('(?:[^%]|^)?%\((\w*)\)[a-z]', full_msg)

        # if we don't find any %(blah) blocks but have a %s
        if not keys and re.findall('(?:[^%]|^)%[a-z]', full_msg):
            # apparently the full dictionary is the parameter
            params = copy.deepcopy(dict_param)
        else:
            params = {}
            for key in keys:
                try:
                    params[key] = copy.deepcopy(dict_param[key])
                except TypeError:
                    # cast uncopyable thing to unicode string
                    params[key] = unicode(dict_param[key])

        return params

    def _save_parameters(self, other):
        # we check for None later to see if
        # we actually have parameters to inject,
        # so encapsulate if our parameter is actually None
        if other is None:
            self.params = (other, )
        elif isinstance(other, dict):
            self.params = self._save_dictionary_parameter(other)
        else:
            # fallback to casting to unicode,
            # this will handle the problematic python code-like
            # objects that cannot be deep-copied
            try:
                self.params = copy.deepcopy(other)
            except TypeError:
                self.params = unicode(other)

        return self

    # overrides to be more string-like
    def __unicode__(self):
        return self.data

    def __str__(self):
        return self.data.encode('utf-8')

    def __getstate__(self):
        to_copy = ['_msg', '_right_extra_msg', '_left_extra_msg',
                   'domain', 'params', 'locale']
        new_dict = self.__dict__.fromkeys(to_copy)
        for attr in to_copy:
            new_dict[attr] = copy.deepcopy(self.__dict__[attr])

        return new_dict

    def __setstate__(self, state):
        for (k, v) in state.items():
            setattr(self, k, v)

    # operator overloads
    def __add__(self, other):
        copied = copy.deepcopy(self)
        copied._right_extra_msg += other.__str__()
        return copied

    def __radd__(self, other):
        copied = copy.deepcopy(self)
        copied._left_extra_msg += other.__str__()
        return copied

    def __mod__(self, other):
        # do a format string to catch and raise
        # any possible KeyErrors from missing parameters
        self.data % other
        copied = copy.deepcopy(self)
        return copied._save_parameters(other)

    def __mul__(self, other):
        return self.data * other

    def __rmul__(self, other):
        return other * self.data

    def __getitem__(self, key):
        return self.data[key]

    def __getslice__(self, start, end):
        return self.data.__getslice__(start, end)

    def __getattribute__(self, name):
        # NOTE(mrodden): handle lossy operations that we can't deal with yet
        # These override the UserString implementation, since UserString
        # uses our __class__ attribute to try and build a new message
        # after running the inner data string through the operation.
        # At that point, we have lost the gettext message id and can just
        # safely resolve to a string instead.
        ops = ['capitalize', 'center', 'decode', 'encode',
               'expandtabs', 'ljust', 'lstrip', 'replace', 'rjust', 'rstrip',
               'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']
        if name in ops:
            return getattr(self.data, name)
        else:
            return _userString.UserString.__getattribute__(self, name)


def get_available_languages(domain):
    """Lists the available languages for the given translation domain.

    :param domain: the domain to get languages for
    """
    if domain in _AVAILABLE_LANGUAGES:
        return copy.copy(_AVAILABLE_LANGUAGES[domain])

    localedir = '%s_LOCALEDIR' % domain.upper()
    find = lambda x: gettext.find(domain,
                                  localedir=os.environ.get(localedir),
                                  languages=[x])

    # NOTE(mrodden): en_US should always be available (and first in case
    # order matters) since our in-line message strings are en_US
    language_list = ['en_US']
    # NOTE(luisg): Babel <1.0 used a function called list(), which was
    # renamed to locale_identifiers() in >=1.0, the requirements master list
    # requires >=0.9.6, uncapped, so defensively work with both. We can remove
    # this check when the master list updates to >=1.0, and all projects udpate
    list_identifiers = (getattr(localedata, 'list', None) or
                        getattr(localedata, 'locale_identifiers'))
    locale_identifiers = list_identifiers()
    for i in locale_identifiers:
        if find(i) is not None:
            language_list.append(i)
    _AVAILABLE_LANGUAGES[domain] = language_list
    return copy.copy(language_list)


def get_localized_message(message, user_locale):
    """Gets a localized version of the given message in the given locale."""
    if isinstance(message, Message):
        if user_locale:
            message.locale = user_locale
        return unicode(message)
    else:
        return message


class LocaleHandler(logging.Handler):
    """Handler that can have a locale associated to translate Messages.

    A quick example of how to utilize the Message class above.
    LocaleHandler takes a locale and a target logging.Handler object
    to forward LogRecord objects to after translating the internal Message.
    """

    def __init__(self, locale, target):
        """Initialize a LocaleHandler

        :param locale: locale to use for translating messages
        :param target: logging.Handler object to forward
                       LogRecord objects to after translation
        """
        logging.Handler.__init__(self)
        self.locale = locale
        self.target = target

    def emit(self, record):
        if isinstance(record.msg, Message):
            # set the locale and resolve to a string
            record.msg.locale = self.locale

        self.target.emit(record)

########NEW FILE########
__FILENAME__ = importutils
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Import related utilities and helper functions.
"""

import sys
import traceback


def import_class(import_str):
    """Returns a class from a string including module and class."""
    mod_str, _sep, class_str = import_str.rpartition('.')
    try:
        __import__(mod_str)
        return getattr(sys.modules[mod_str], class_str)
    except (ValueError, AttributeError):
        raise ImportError('Class %s cannot be found (%s)' %
                          (class_str,
                           traceback.format_exception(*sys.exc_info())))


def import_object(import_str, *args, **kwargs):
    """Import a class and return an instance of it."""
    return import_class(import_str)(*args, **kwargs)


def import_object_ns(name_space, import_str, *args, **kwargs):
    """Tries to import object from default namespace.

    Imports a class and return an instance of it, first by trying
    to find the class in a default namespace, then failing back to
    a full path if not found in the default namespace.
    """
    import_value = "%s.%s" % (name_space, import_str)
    try:
        return import_class(import_value)(*args, **kwargs)
    except ImportError:
        return import_class(import_str)(*args, **kwargs)


def import_module(import_str):
    """Import a module."""
    __import__(import_str)
    return sys.modules[import_str]


def try_import(import_str, default=None):
    """Try to import a module and if it fails return default."""
    try:
        return import_module(import_str)
    except ImportError:
        return default

########NEW FILE########
__FILENAME__ = iniparser
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2012 OpenStack LLC.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


class ParseError(Exception):
    def __init__(self, message, lineno, line):
        self.msg = message
        self.line = line
        self.lineno = lineno

    def __str__(self):
        return 'at line %d, %s: %r' % (self.lineno, self.msg, self.line)


class BaseParser(object):
    lineno = 0
    parse_exc = ParseError

    def _assignment(self, key, value):
        self.assignment(key, value)
        return None, []

    def _get_section(self, line):
        if line[-1] != ']':
            return self.error_no_section_end_bracket(line)
        if len(line) <= 2:
            return self.error_no_section_name(line)

        return line[1:-1]

    def _split_key_value(self, line):
        colon = line.find(':')
        equal = line.find('=')
        if colon < 0 and equal < 0:
            return self.error_invalid_assignment(line)

        if colon < 0 or (equal >= 0 and equal < colon):
            key, value = line[:equal], line[equal + 1:]
        else:
            key, value = line[:colon], line[colon + 1:]

        value = value.strip()
        if ((value and value[0] == value[-1]) and
            (value[0] == "\"" or value[0] == "'")):
            value = value[1:-1]
        return key.strip(), [value]

    def parse(self, lineiter):
        key = None
        value = []

        for line in lineiter:
            self.lineno += 1

            line = line.rstrip()
            if not line:
                # Blank line, ends multi-line values
                if key:
                    key, value = self._assignment(key, value)
                continue
            elif line[0] in (' ', '\t'):
                # Continuation of previous assignment
                if key is None:
                    self.error_unexpected_continuation(line)
                else:
                    value.append(line.lstrip())
                continue

            if key:
                # Flush previous assignment, if any
                key, value = self._assignment(key, value)

            if line[0] == '[':
                # Section start
                section = self._get_section(line)
                if section:
                    self.new_section(section)
            elif line[0] in '#;':
                self.comment(line[1:].lstrip())
            else:
                key, value = self._split_key_value(line)
                if not key:
                    return self.error_empty_key(line)

        if key:
            # Flush previous assignment, if any
            self._assignment(key, value)

    def assignment(self, key, value):
        """Called when a full assignment is parsed"""
        raise NotImplementedError()

    def new_section(self, section):
        """Called when a new section is started"""
        raise NotImplementedError()

    def comment(self, comment):
        """Called when a comment is parsed"""
        pass

    def error_invalid_assignment(self, line):
        raise self.parse_exc("No ':' or '=' found in assignment",
                             self.lineno, line)

    def error_empty_key(self, line):
        raise self.parse_exc('Key cannot be empty', self.lineno, line)

    def error_unexpected_continuation(self, line):
        raise self.parse_exc('Unexpected continuation line',
                             self.lineno, line)

    def error_no_section_end_bracket(self, line):
        raise self.parse_exc('Invalid section (must end with ])',
                             self.lineno, line)

    def error_no_section_name(self, line):
        raise self.parse_exc('Empty section name', self.lineno, line)

########NEW FILE########
__FILENAME__ = jsonutils
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# Copyright 2011 Justin Santa Barbara
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

'''
JSON related utilities.

This module provides a few things:

    1) A handy function for getting an object down to something that can be
    JSON serialized.  See to_primitive().

    2) Wrappers around loads() and dumps().  The dumps() wrapper will
    automatically use to_primitive() for you if needed.

    3) This sets up anyjson to use the loads() and dumps() wrappers if anyjson
    is available.
'''


import datetime
import functools
import inspect
import itertools
import json
try:
    import xmlrpclib
except ImportError:
    # NOTE(jd): xmlrpclib is not shipped with Python 3
    xmlrpclib = None

import six

from trove.openstack.common import importutils
from trove.openstack.common import timeutils

netaddr = importutils.try_import("netaddr")

_nasty_type_tests = [inspect.ismodule, inspect.isclass, inspect.ismethod,
                     inspect.isfunction, inspect.isgeneratorfunction,
                     inspect.isgenerator, inspect.istraceback, inspect.isframe,
                     inspect.iscode, inspect.isbuiltin, inspect.isroutine,
                     inspect.isabstract]

_simple_types = (six.string_types + six.integer_types
                 + (type(None), bool, float))


def to_primitive(value, convert_instances=False, convert_datetime=True,
                 level=0, max_depth=3):
    """Convert a complex object into primitives.

    Handy for JSON serialization. We can optionally handle instances,
    but since this is a recursive function, we could have cyclical
    data structures.

    To handle cyclical data structures we could track the actual objects
    visited in a set, but not all objects are hashable. Instead we just
    track the depth of the object inspections and don't go too deep.

    Therefore, convert_instances=True is lossy ... be aware.

    """
    # handle obvious types first - order of basic types determined by running
    # full tests on nova project, resulting in the following counts:
    # 572754 <type 'NoneType'>
    # 460353 <type 'int'>
    # 379632 <type 'unicode'>
    # 274610 <type 'str'>
    # 199918 <type 'dict'>
    # 114200 <type 'datetime.datetime'>
    #  51817 <type 'bool'>
    #  26164 <type 'list'>
    #   6491 <type 'float'>
    #    283 <type 'tuple'>
    #     19 <type 'long'>
    if isinstance(value, _simple_types):
        return value

    if isinstance(value, datetime.datetime):
        if convert_datetime:
            return timeutils.strtime(value)
        else:
            return value

    # value of itertools.count doesn't get caught by nasty_type_tests
    # and results in infinite loop when list(value) is called.
    if type(value) == itertools.count:
        return six.text_type(value)

    # FIXME(vish): Workaround for LP bug 852095. Without this workaround,
    #              tests that raise an exception in a mocked method that
    #              has a @wrap_exception with a notifier will fail. If
    #              we up the dependency to 0.5.4 (when it is released) we
    #              can remove this workaround.
    if getattr(value, '__module__', None) == 'mox':
        return 'mock'

    if level > max_depth:
        return '?'

    # The try block may not be necessary after the class check above,
    # but just in case ...
    try:
        recursive = functools.partial(to_primitive,
                                      convert_instances=convert_instances,
                                      convert_datetime=convert_datetime,
                                      level=level,
                                      max_depth=max_depth)
        if isinstance(value, dict):
            return dict((k, recursive(v)) for k, v in value.iteritems())
        elif isinstance(value, (list, tuple)):
            return [recursive(lv) for lv in value]

        # It's not clear why xmlrpclib created their own DateTime type, but
        # for our purposes, make it a datetime type which is explicitly
        # handled
        if xmlrpclib and isinstance(value, xmlrpclib.DateTime):
            value = datetime.datetime(*tuple(value.timetuple())[:6])

        if convert_datetime and isinstance(value, datetime.datetime):
            return timeutils.strtime(value)
        elif hasattr(value, 'iteritems'):
            return recursive(dict(value.iteritems()), level=level + 1)
        elif hasattr(value, '__iter__'):
            return recursive(list(value))
        elif convert_instances and hasattr(value, '__dict__'):
            # Likely an instance of something. Watch for cycles.
            # Ignore class member vars.
            return recursive(value.__dict__, level=level + 1)
        elif netaddr and isinstance(value, netaddr.IPAddress):
            return six.text_type(value)
        else:
            if any(test(value) for test in _nasty_type_tests):
                return six.text_type(value)
            return value
    except TypeError:
        # Class objects are tricky since they may define something like
        # __iter__ defined but it isn't callable as list().
        return six.text_type(value)


def dumps(value, default=to_primitive, **kwargs):
    return json.dumps(value, default=default, **kwargs)


def loads(s):
    return json.loads(s)


def load(s):
    return json.load(s)


try:
    import anyjson
except ImportError:
    pass
else:
    anyjson._modules.append((__name__, 'dumps', TypeError,
                                       'loads', ValueError, 'load'))
    anyjson.force_implementation(__name__)

########NEW FILE########
__FILENAME__ = local
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Local storage of variables using weak references"""

import threading
import weakref


class WeakLocal(threading.local):
    def __getattribute__(self, attr):
        rval = super(WeakLocal, self).__getattribute__(attr)
        if rval:
            # NOTE(mikal): this bit is confusing. What is stored is a weak
            # reference, not the value itself. We therefore need to lookup
            # the weak reference and return the inner value here.
            rval = rval()
        return rval

    def __setattr__(self, attr, value):
        value = weakref.ref(value)
        return super(WeakLocal, self).__setattr__(attr, value)


# NOTE(mikal): the name "store" should be deprecated in the future
store = WeakLocal()

# A "weak" store uses weak references and allows an object to fall out of scope
# when it falls out of scope in the code that uses the thread local storage. A
# "strong" store will hold a reference to the object so that it never falls out
# of scope.
weak_store = WeakLocal()
strong_store = threading.local()

########NEW FILE########
__FILENAME__ = lockutils
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


import contextlib
import errno
import functools
import os
import shutil
import subprocess
import sys
import tempfile
import threading
import time
import weakref

from oslo.config import cfg

from trove.openstack.common import fileutils
from trove.openstack.common.gettextutils import _  # noqa
from trove.openstack.common import local
from trove.openstack.common import log as logging


LOG = logging.getLogger(__name__)


util_opts = [
    cfg.BoolOpt('disable_process_locking', default=False,
                help='Whether to disable inter-process locks'),
    cfg.StrOpt('lock_path',
               default=os.environ.get("TROVE_LOCK_PATH"),
               help=('Directory to use for lock files.'))
]


CONF = cfg.CONF
CONF.register_opts(util_opts)


def set_defaults(lock_path):
    cfg.set_defaults(util_opts, lock_path=lock_path)


class _InterProcessLock(object):
    """Lock implementation which allows multiple locks, working around
    issues like bugs.debian.org/cgi-bin/bugreport.cgi?bug=632857 and does
    not require any cleanup. Since the lock is always held on a file
    descriptor rather than outside of the process, the lock gets dropped
    automatically if the process crashes, even if __exit__ is not executed.

    There are no guarantees regarding usage by multiple green threads in a
    single process here. This lock works only between processes. Exclusive
    access between local threads should be achieved using the semaphores
    in the @synchronized decorator.

    Note these locks are released when the descriptor is closed, so it's not
    safe to close the file descriptor while another green thread holds the
    lock. Just opening and closing the lock file can break synchronisation,
    so lock files must be accessed only using this abstraction.
    """

    def __init__(self, name):
        self.lockfile = None
        self.fname = name

    def __enter__(self):
        self.lockfile = open(self.fname, 'w')

        while True:
            try:
                # Using non-blocking locks since green threads are not
                # patched to deal with blocking locking calls.
                # Also upon reading the MSDN docs for locking(), it seems
                # to have a laughable 10 attempts "blocking" mechanism.
                self.trylock()
                return self
            except IOError as e:
                if e.errno in (errno.EACCES, errno.EAGAIN):
                    # external locks synchronise things like iptables
                    # updates - give it some time to prevent busy spinning
                    time.sleep(0.01)
                else:
                    raise

    def __exit__(self, exc_type, exc_val, exc_tb):
        try:
            self.unlock()
            self.lockfile.close()
        except IOError:
            LOG.exception(_("Could not release the acquired lock `%s`"),
                          self.fname)

    def trylock(self):
        raise NotImplementedError()

    def unlock(self):
        raise NotImplementedError()


class _WindowsLock(_InterProcessLock):
    def trylock(self):
        msvcrt.locking(self.lockfile.fileno(), msvcrt.LK_NBLCK, 1)

    def unlock(self):
        msvcrt.locking(self.lockfile.fileno(), msvcrt.LK_UNLCK, 1)


class _PosixLock(_InterProcessLock):
    def trylock(self):
        fcntl.lockf(self.lockfile, fcntl.LOCK_EX | fcntl.LOCK_NB)

    def unlock(self):
        fcntl.lockf(self.lockfile, fcntl.LOCK_UN)


if os.name == 'nt':
    import msvcrt
    InterProcessLock = _WindowsLock
else:
    import fcntl
    InterProcessLock = _PosixLock

_semaphores = weakref.WeakValueDictionary()
_semaphores_lock = threading.Lock()


@contextlib.contextmanager
def lock(name, lock_file_prefix=None, external=False, lock_path=None):
    """Context based lock

    This function yields a `threading.Semaphore` instance (if we don't use
    eventlet.monkey_patch(), else `semaphore.Semaphore`) unless external is
    True, in which case, it'll yield an InterProcessLock instance.

    :param lock_file_prefix: The lock_file_prefix argument is used to provide
    lock files on disk with a meaningful prefix.

    :param external: The external keyword argument denotes whether this lock
    should work across multiple processes. This means that if two different
    workers both run a a method decorated with @synchronized('mylock',
    external=True), only one of them will execute at a time.

    :param lock_path: The lock_path keyword argument is used to specify a
    special location for external lock files to live. If nothing is set, then
    CONF.lock_path is used as a default.
    """
    with _semaphores_lock:
        try:
            sem = _semaphores[name]
        except KeyError:
            sem = threading.Semaphore()
            _semaphores[name] = sem

    with sem:
        LOG.debug(_('Got semaphore "%(lock)s"'), {'lock': name})

        # NOTE(mikal): I know this looks odd
        if not hasattr(local.strong_store, 'locks_held'):
            local.strong_store.locks_held = []
        local.strong_store.locks_held.append(name)

        try:
            if external and not CONF.disable_process_locking:
                LOG.debug(_('Attempting to grab file lock "%(lock)s"'),
                          {'lock': name})

                # We need a copy of lock_path because it is non-local
                local_lock_path = lock_path or CONF.lock_path
                if not local_lock_path:
                    raise cfg.RequiredOptError('lock_path')

                if not os.path.exists(local_lock_path):
                    fileutils.ensure_tree(local_lock_path)
                    LOG.info(_('Created lock path: %s'), local_lock_path)

                def add_prefix(name, prefix):
                    if not prefix:
                        return name
                    sep = '' if prefix.endswith('-') else '-'
                    return '%s%s%s' % (prefix, sep, name)

                # NOTE(mikal): the lock name cannot contain directory
                # separators
                lock_file_name = add_prefix(name.replace(os.sep, '_'),
                                            lock_file_prefix)

                lock_file_path = os.path.join(local_lock_path, lock_file_name)

                try:
                    lock = InterProcessLock(lock_file_path)
                    with lock as lock:
                        LOG.debug(_('Got file lock "%(lock)s" at %(path)s'),
                                  {'lock': name, 'path': lock_file_path})
                        yield lock
                finally:
                    LOG.debug(_('Released file lock "%(lock)s" at %(path)s'),
                              {'lock': name, 'path': lock_file_path})
            else:
                yield sem

        finally:
            local.strong_store.locks_held.remove(name)


def synchronized(name, lock_file_prefix=None, external=False, lock_path=None):
    """Synchronization decorator.

    Decorating a method like so::

        @synchronized('mylock')
        def foo(self, *args):
           ...

    ensures that only one thread will execute the foo method at a time.

    Different methods can share the same lock::

        @synchronized('mylock')
        def foo(self, *args):
           ...

        @synchronized('mylock')
        def bar(self, *args):
           ...

    This way only one of either foo or bar can be executing at a time.
    """

    def wrap(f):
        @functools.wraps(f)
        def inner(*args, **kwargs):
            try:
                with lock(name, lock_file_prefix, external, lock_path):
                    LOG.debug(_('Got semaphore / lock "%(function)s"'),
                              {'function': f.__name__})
                    return f(*args, **kwargs)
            finally:
                LOG.debug(_('Semaphore / lock released "%(function)s"'),
                          {'function': f.__name__})
        return inner
    return wrap


def synchronized_with_prefix(lock_file_prefix):
    """Partial object generator for the synchronization decorator.

    Redefine @synchronized in each project like so::

        (in nova/utils.py)
        from nova.openstack.common import lockutils

        synchronized = lockutils.synchronized_with_prefix('nova-')


        (in nova/foo.py)
        from nova import utils

        @utils.synchronized('mylock')
        def bar(self, *args):
           ...

    The lock_file_prefix argument is used to provide lock files on disk with a
    meaningful prefix.
    """

    return functools.partial(synchronized, lock_file_prefix=lock_file_prefix)


def main(argv):
    """Create a dir for locks and pass it to command from arguments

    If you run this:
    python -m openstack.common.lockutils python setup.py testr <etc>

    a temporary directory will be created for all your locks and passed to all
    your tests in an environment variable. The temporary dir will be deleted
    afterwards and the return value will be preserved.
    """

    lock_dir = tempfile.mkdtemp()
    os.environ["TROVE_LOCK_PATH"] = lock_dir
    try:
        ret_val = subprocess.call(argv[1:])
    finally:
        shutil.rmtree(lock_dir, ignore_errors=True)
    return ret_val


if __name__ == '__main__':
    sys.exit(main(sys.argv))

########NEW FILE########
__FILENAME__ = log
# Copyright 2011 OpenStack Foundation.
# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Openstack logging handler.

This module adds to logging functionality by adding the option to specify
a context object when calling the various log methods.  If the context object
is not specified, default formatting is used. Additionally, an instance uuid
may be passed as part of the log message, which is intended to make it easier
for admins to find messages related to a specific instance.

It also allows setting of formatting information through conf.

"""

import inspect
import itertools
import logging
import logging.config
import logging.handlers
import os
import re
import sys
import traceback

from oslo.config import cfg
import six
from six import moves

from trove.openstack.common.gettextutils import _
from trove.openstack.common import importutils
from trove.openstack.common import jsonutils
from trove.openstack.common import local


_DEFAULT_LOG_DATE_FORMAT = "%Y-%m-%d %H:%M:%S"

_SANITIZE_KEYS = ['adminPass', 'admin_pass', 'password', 'admin_password']

# NOTE(ldbragst): Let's build a list of regex objects using the list of
# _SANITIZE_KEYS we already have. This way, we only have to add the new key
# to the list of _SANITIZE_KEYS and we can generate regular expressions
# for XML and JSON automatically.
_SANITIZE_PATTERNS = []
_FORMAT_PATTERNS = [r'(%(key)s\s*[=]\s*[\"\']).*?([\"\'])',
                    r'(<%(key)s>).*?(</%(key)s>)',
                    r'([\"\']%(key)s[\"\']\s*:\s*[\"\']).*?([\"\'])',
                    r'([\'"].*?%(key)s[\'"]\s*:\s*u?[\'"]).*?([\'"])']

for key in _SANITIZE_KEYS:
    for pattern in _FORMAT_PATTERNS:
        reg_ex = re.compile(pattern % {'key': key}, re.DOTALL)
        _SANITIZE_PATTERNS.append(reg_ex)


common_cli_opts = [
    cfg.BoolOpt('debug',
                short='d',
                default=False,
                help='Print debugging output (set logging level to '
                     'DEBUG instead of default WARNING level).'),
    cfg.BoolOpt('verbose',
                short='v',
                default=False,
                help='Print more verbose output (set logging level to '
                     'INFO instead of default WARNING level).'),
]

logging_cli_opts = [
    cfg.StrOpt('log-config-append',
               metavar='PATH',
               deprecated_name='log-config',
               help='The name of logging configuration file. It does not '
                    'disable existing loggers, but just appends specified '
                    'logging configuration to any other existing logging '
                    'options. Please see the Python logging module '
                    'documentation for details on logging configuration '
                    'files.'),
    cfg.StrOpt('log-format',
               default=None,
               metavar='FORMAT',
               help='DEPRECATED. '
                    'A logging.Formatter log message format string which may '
                    'use any of the available logging.LogRecord attributes. '
                    'This option is deprecated.  Please use '
                    'logging_context_format_string and '
                    'logging_default_format_string instead.'),
    cfg.StrOpt('log-date-format',
               default=_DEFAULT_LOG_DATE_FORMAT,
               metavar='DATE_FORMAT',
               help='Format string for %%(asctime)s in log records. '
                    'Default: %(default)s'),
    cfg.StrOpt('log-file',
               metavar='PATH',
               deprecated_name='logfile',
               help='(Optional) Name of log file to output to. '
                    'If no default is set, logging will go to stdout.'),
    cfg.StrOpt('log-dir',
               deprecated_name='logdir',
               help='(Optional) The base directory used for relative '
                    '--log-file paths'),
    cfg.BoolOpt('use-syslog',
                default=False,
                help='Use syslog for logging.'),
    cfg.StrOpt('syslog-log-facility',
               default='LOG_USER',
               help='syslog facility to receive log lines')
]

generic_log_opts = [
    cfg.BoolOpt('use_stderr',
                default=True,
                help='Log output to standard error')
]

log_opts = [
    cfg.StrOpt('logging_context_format_string',
               default='%(asctime)s.%(msecs)03d %(process)d %(levelname)s '
                       '%(name)s [%(request_id)s %(user_identity)s] '
                       '%(instance)s%(message)s',
               help='format string to use for log messages with context'),
    cfg.StrOpt('logging_default_format_string',
               default='%(asctime)s.%(msecs)03d %(process)d %(levelname)s '
                       '%(name)s [-] %(instance)s%(message)s',
               help='format string to use for log messages without context'),
    cfg.StrOpt('logging_debug_format_suffix',
               default='%(funcName)s %(pathname)s:%(lineno)d',
               help='data to append to log format when level is DEBUG'),
    cfg.StrOpt('logging_exception_prefix',
               default='%(asctime)s.%(msecs)03d %(process)d TRACE %(name)s '
               '%(instance)s',
               help='prefix each line of exception output with this format'),
    cfg.ListOpt('default_log_levels',
                default=[
                    'amqp=WARN',
                    'amqplib=WARN',
                    'boto=WARN',
                    'qpid=WARN',
                    'sqlalchemy=WARN',
                    'suds=INFO',
                    'iso8601=WARN',
                ],
                help='list of logger=LEVEL pairs'),
    cfg.BoolOpt('publish_errors',
                default=False,
                help='publish error events'),
    cfg.BoolOpt('fatal_deprecations',
                default=False,
                help='make deprecations fatal'),

    # NOTE(mikal): there are two options here because sometimes we are handed
    # a full instance (and could include more information), and other times we
    # are just handed a UUID for the instance.
    cfg.StrOpt('instance_format',
               default='[instance: %(uuid)s] ',
               help='If an instance is passed with the log message, format '
                    'it like this'),
    cfg.StrOpt('instance_uuid_format',
               default='[instance: %(uuid)s] ',
               help='If an instance UUID is passed with the log message, '
                    'format it like this'),
]

CONF = cfg.CONF
CONF.register_cli_opts(common_cli_opts)
CONF.register_cli_opts(logging_cli_opts)
CONF.register_opts(generic_log_opts)
CONF.register_opts(log_opts)

# our new audit level
# NOTE(jkoelker) Since we synthesized an audit level, make the logging
#                module aware of it so it acts like other levels.
logging.AUDIT = logging.INFO + 1
logging.addLevelName(logging.AUDIT, 'AUDIT')


try:
    NullHandler = logging.NullHandler
except AttributeError:  # NOTE(jkoelker) NullHandler added in Python 2.7
    class NullHandler(logging.Handler):
        def handle(self, record):
            pass

        def emit(self, record):
            pass

        def createLock(self):
            self.lock = None


def _dictify_context(context):
    if context is None:
        return None
    if not isinstance(context, dict) and getattr(context, 'to_dict', None):
        context = context.to_dict()
    return context


def _get_binary_name():
    return os.path.basename(inspect.stack()[-1][1])


def _get_log_file_path(binary=None):
    logfile = CONF.log_file
    logdir = CONF.log_dir

    if logfile and not logdir:
        return logfile

    if logfile and logdir:
        return os.path.join(logdir, logfile)

    if logdir:
        binary = binary or _get_binary_name()
        return '%s.log' % (os.path.join(logdir, binary),)

    return None


def mask_password(message, secret="***"):
    """Replace password with 'secret' in message.

    :param message: The string which includes security information.
    :param secret: value with which to replace passwords.
    :returns: The unicode value of message with the password fields masked.

    For example:

    >>> mask_password("'adminPass' : 'aaaaa'")
    "'adminPass' : '***'"
    >>> mask_password("'admin_pass' : 'aaaaa'")
    "'admin_pass' : '***'"
    >>> mask_password('"password" : "aaaaa"')
    '"password" : "***"'
    >>> mask_password("'original_password' : 'aaaaa'")
    "'original_password' : '***'"
    >>> mask_password("u'original_password' :   u'aaaaa'")
    "u'original_password' :   u'***'"
    """
    message = six.text_type(message)

    # NOTE(ldbragst): Check to see if anything in message contains any key
    # specified in _SANITIZE_KEYS, if not then just return the message since
    # we don't have to mask any passwords.
    if not any(key in message for key in _SANITIZE_KEYS):
        return message

    secret = r'\g<1>' + secret + r'\g<2>'
    for pattern in _SANITIZE_PATTERNS:
        message = re.sub(pattern, secret, message)
    return message


class BaseLoggerAdapter(logging.LoggerAdapter):

    def audit(self, msg, *args, **kwargs):
        self.log(logging.AUDIT, msg, *args, **kwargs)


class LazyAdapter(BaseLoggerAdapter):
    def __init__(self, name='unknown', version='unknown'):
        self._logger = None
        self.extra = {}
        self.name = name
        self.version = version

    @property
    def logger(self):
        if not self._logger:
            self._logger = getLogger(self.name, self.version)
        return self._logger


class ContextAdapter(BaseLoggerAdapter):
    warn = logging.LoggerAdapter.warning

    def __init__(self, logger, project_name, version_string):
        self.logger = logger
        self.project = project_name
        self.version = version_string

    @property
    def handlers(self):
        return self.logger.handlers

    def deprecated(self, msg, *args, **kwargs):
        stdmsg = _("Deprecated: %s") % msg
        if CONF.fatal_deprecations:
            self.critical(stdmsg, *args, **kwargs)
            raise DeprecatedConfig(msg=stdmsg)
        else:
            self.warn(stdmsg, *args, **kwargs)

    def process(self, msg, kwargs):
        # NOTE(mrodden): catch any Message/other object and
        #                coerce to unicode before they can get
        #                to the python logging and possibly
        #                cause string encoding trouble
        if not isinstance(msg, six.string_types):
            msg = six.text_type(msg)

        if 'extra' not in kwargs:
            kwargs['extra'] = {}
        extra = kwargs['extra']

        context = kwargs.pop('context', None)
        if not context:
            context = getattr(local.store, 'context', None)
        if context:
            extra.update(_dictify_context(context))

        instance = kwargs.pop('instance', None)
        instance_uuid = (extra.get('instance_uuid', None) or
                         kwargs.pop('instance_uuid', None))
        instance_extra = ''
        if instance:
            instance_extra = CONF.instance_format % instance
        elif instance_uuid:
            instance_extra = (CONF.instance_uuid_format
                              % {'uuid': instance_uuid})
        extra['instance'] = instance_extra

        extra.setdefault('user_identity', kwargs.pop('user_identity', None))

        extra['project'] = self.project
        extra['version'] = self.version
        extra['extra'] = extra.copy()
        return msg, kwargs


class JSONFormatter(logging.Formatter):
    def __init__(self, fmt=None, datefmt=None):
        # NOTE(jkoelker) we ignore the fmt argument, but its still there
        #                since logging.config.fileConfig passes it.
        self.datefmt = datefmt

    def formatException(self, ei, strip_newlines=True):
        lines = traceback.format_exception(*ei)
        if strip_newlines:
            lines = [moves.filter(
                lambda x: x,
                line.rstrip().splitlines()) for line in lines]
            lines = list(itertools.chain(*lines))
        return lines

    def format(self, record):
        message = {'message': record.getMessage(),
                   'asctime': self.formatTime(record, self.datefmt),
                   'name': record.name,
                   'msg': record.msg,
                   'args': record.args,
                   'levelname': record.levelname,
                   'levelno': record.levelno,
                   'pathname': record.pathname,
                   'filename': record.filename,
                   'module': record.module,
                   'lineno': record.lineno,
                   'funcname': record.funcName,
                   'created': record.created,
                   'msecs': record.msecs,
                   'relative_created': record.relativeCreated,
                   'thread': record.thread,
                   'thread_name': record.threadName,
                   'process_name': record.processName,
                   'process': record.process,
                   'traceback': None}

        if hasattr(record, 'extra'):
            message['extra'] = record.extra

        if record.exc_info:
            message['traceback'] = self.formatException(record.exc_info)

        return jsonutils.dumps(message)


def _create_logging_excepthook(product_name):
    def logging_excepthook(exc_type, value, tb):
        extra = {}
        if CONF.verbose:
            extra['exc_info'] = (exc_type, value, tb)
        getLogger(product_name).critical(str(value), **extra)
    return logging_excepthook


class LogConfigError(Exception):

    message = _('Error loading logging config %(log_config)s: %(err_msg)s')

    def __init__(self, log_config, err_msg):
        self.log_config = log_config
        self.err_msg = err_msg

    def __str__(self):
        return self.message % dict(log_config=self.log_config,
                                   err_msg=self.err_msg)


def _load_log_config(log_config_append):
    try:
        logging.config.fileConfig(log_config_append,
                                  disable_existing_loggers=False)
    except moves.configparser.Error as exc:
        raise LogConfigError(log_config_append, str(exc))


def setup(product_name):
    """Setup logging."""
    if CONF.log_config_append:
        _load_log_config(CONF.log_config_append)
    else:
        _setup_logging_from_conf()
    sys.excepthook = _create_logging_excepthook(product_name)


def set_defaults(logging_context_format_string):
    cfg.set_defaults(log_opts,
                     logging_context_format_string=
                     logging_context_format_string)


def _find_facility_from_conf():
    facility_names = logging.handlers.SysLogHandler.facility_names
    facility = getattr(logging.handlers.SysLogHandler,
                       CONF.syslog_log_facility,
                       None)

    if facility is None and CONF.syslog_log_facility in facility_names:
        facility = facility_names.get(CONF.syslog_log_facility)

    if facility is None:
        valid_facilities = facility_names.keys()
        consts = ['LOG_AUTH', 'LOG_AUTHPRIV', 'LOG_CRON', 'LOG_DAEMON',
                  'LOG_FTP', 'LOG_KERN', 'LOG_LPR', 'LOG_MAIL', 'LOG_NEWS',
                  'LOG_AUTH', 'LOG_SYSLOG', 'LOG_USER', 'LOG_UUCP',
                  'LOG_LOCAL0', 'LOG_LOCAL1', 'LOG_LOCAL2', 'LOG_LOCAL3',
                  'LOG_LOCAL4', 'LOG_LOCAL5', 'LOG_LOCAL6', 'LOG_LOCAL7']
        valid_facilities.extend(consts)
        raise TypeError(_('syslog facility must be one of: %s') %
                        ', '.join("'%s'" % fac
                                  for fac in valid_facilities))

    return facility


def _setup_logging_from_conf():
    log_root = getLogger(None).logger
    for handler in log_root.handlers:
        log_root.removeHandler(handler)

    if CONF.use_syslog:
        facility = _find_facility_from_conf()
        syslog = logging.handlers.SysLogHandler(address='/dev/log',
                                                facility=facility)
        log_root.addHandler(syslog)

    logpath = _get_log_file_path()
    if logpath:
        filelog = logging.handlers.WatchedFileHandler(logpath)
        log_root.addHandler(filelog)

    if CONF.use_stderr:
        streamlog = ColorHandler()
        log_root.addHandler(streamlog)

    elif not logpath:
        # pass sys.stdout as a positional argument
        # python2.6 calls the argument strm, in 2.7 it's stream
        streamlog = logging.StreamHandler(sys.stdout)
        log_root.addHandler(streamlog)

    if CONF.publish_errors:
        handler = importutils.import_object(
            "trove.openstack.common.log_handler.PublishErrorsHandler",
            logging.ERROR)
        log_root.addHandler(handler)

    datefmt = CONF.log_date_format
    for handler in log_root.handlers:
        # NOTE(alaski): CONF.log_format overrides everything currently.  This
        # should be deprecated in favor of context aware formatting.
        if CONF.log_format:
            handler.setFormatter(logging.Formatter(fmt=CONF.log_format,
                                                   datefmt=datefmt))
            log_root.info('Deprecated: log_format is now deprecated and will '
                          'be removed in the next release')
        else:
            handler.setFormatter(ContextFormatter(datefmt=datefmt))

    if CONF.debug:
        log_root.setLevel(logging.DEBUG)
    elif CONF.verbose:
        log_root.setLevel(logging.INFO)
    else:
        log_root.setLevel(logging.WARNING)

    for pair in CONF.default_log_levels:
        mod, _sep, level_name = pair.partition('=')
        level = logging.getLevelName(level_name)
        logger = logging.getLogger(mod)
        logger.setLevel(level)

_loggers = {}


def getLogger(name='unknown', version='unknown'):
    if name not in _loggers:
        _loggers[name] = ContextAdapter(logging.getLogger(name),
                                        name,
                                        version)
    return _loggers[name]


def getLazyLogger(name='unknown', version='unknown'):
    """Returns lazy logger.

    Creates a pass-through logger that does not create the real logger
    until it is really needed and delegates all calls to the real logger
    once it is created.
    """
    return LazyAdapter(name, version)


class WritableLogger(object):
    """A thin wrapper that responds to `write` and logs."""

    def __init__(self, logger, level=logging.INFO):
        self.logger = logger
        self.level = level

    def write(self, msg):
        self.logger.log(self.level, msg)


class ContextFormatter(logging.Formatter):
    """A context.RequestContext aware formatter configured through flags.

    The flags used to set format strings are: logging_context_format_string
    and logging_default_format_string.  You can also specify
    logging_debug_format_suffix to append extra formatting if the log level is
    debug.

    For information about what variables are available for the formatter see:
    http://docs.python.org/library/logging.html#formatter

    """

    def format(self, record):
        """Uses contextstring if request_id is set, otherwise default."""
        # NOTE(sdague): default the fancier formating params
        # to an empty string so we don't throw an exception if
        # they get used
        for key in ('instance', 'color'):
            if key not in record.__dict__:
                record.__dict__[key] = ''

        if record.__dict__.get('request_id', None):
            self._fmt = CONF.logging_context_format_string
        else:
            self._fmt = CONF.logging_default_format_string

        if (record.levelno == logging.DEBUG and
                CONF.logging_debug_format_suffix):
            self._fmt += " " + CONF.logging_debug_format_suffix

        # Cache this on the record, Logger will respect our formated copy
        if record.exc_info:
            record.exc_text = self.formatException(record.exc_info, record)
        return logging.Formatter.format(self, record)

    def formatException(self, exc_info, record=None):
        """Format exception output with CONF.logging_exception_prefix."""
        if not record:
            return logging.Formatter.formatException(self, exc_info)

        stringbuffer = moves.StringIO()
        traceback.print_exception(exc_info[0], exc_info[1], exc_info[2],
                                  None, stringbuffer)
        lines = stringbuffer.getvalue().split('\n')
        stringbuffer.close()

        if CONF.logging_exception_prefix.find('%(asctime)') != -1:
            record.asctime = self.formatTime(record, self.datefmt)

        formatted_lines = []
        for line in lines:
            pl = CONF.logging_exception_prefix % record.__dict__
            fl = '%s%s' % (pl, line)
            formatted_lines.append(fl)
        return '\n'.join(formatted_lines)


class ColorHandler(logging.StreamHandler):
    LEVEL_COLORS = {
        logging.DEBUG: '\033[00;32m',  # GREEN
        logging.INFO: '\033[00;36m',  # CYAN
        logging.AUDIT: '\033[01;36m',  # BOLD CYAN
        logging.WARN: '\033[01;33m',  # BOLD YELLOW
        logging.ERROR: '\033[01;31m',  # BOLD RED
        logging.CRITICAL: '\033[01;31m',  # BOLD RED
    }

    def format(self, record):
        record.color = self.LEVEL_COLORS[record.levelno]
        return logging.StreamHandler.format(self, record)


class DeprecatedConfig(Exception):
    message = _("Fatal call to deprecated config: %(msg)s")

    def __init__(self, msg):
        super(Exception, self).__init__(self.message % dict(msg=msg))

########NEW FILE########
__FILENAME__ = loopingcall
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# Copyright 2011 Justin Santa Barbara
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import sys

from eventlet import event
from eventlet import greenthread

from trove.openstack.common.gettextutils import _  # noqa
from trove.openstack.common import log as logging
from trove.openstack.common import timeutils

LOG = logging.getLogger(__name__)


class LoopingCallDone(Exception):
    """Exception to break out and stop a LoopingCall.

    The poll-function passed to LoopingCall can raise this exception to
    break out of the loop normally. This is somewhat analogous to
    StopIteration.

    An optional return-value can be included as the argument to the exception;
    this return-value will be returned by LoopingCall.wait()

    """

    def __init__(self, retvalue=True):
        """:param retvalue: Value that LoopingCall.wait() should return."""
        self.retvalue = retvalue


class LoopingCallBase(object):
    def __init__(self, f=None, *args, **kw):
        self.args = args
        self.kw = kw
        self.f = f
        self._running = False
        self.done = None

    def stop(self):
        self._running = False

    def wait(self):
        return self.done.wait()


class FixedIntervalLoopingCall(LoopingCallBase):
    """A fixed interval looping call."""

    def start(self, interval, initial_delay=None):
        self._running = True
        done = event.Event()

        def _inner():
            if initial_delay:
                greenthread.sleep(initial_delay)

            try:
                while self._running:
                    start = timeutils.utcnow()
                    self.f(*self.args, **self.kw)
                    end = timeutils.utcnow()
                    if not self._running:
                        break
                    delay = interval - timeutils.delta_seconds(start, end)
                    if delay <= 0:
                        LOG.warn(_('task run outlasted interval by %s sec') %
                                 -delay)
                    greenthread.sleep(delay if delay > 0 else 0)
            except LoopingCallDone as e:
                self.stop()
                done.send(e.retvalue)
            except Exception:
                LOG.exception(_('in fixed duration looping call'))
                done.send_exception(*sys.exc_info())
                return
            else:
                done.send(True)

        self.done = done

        greenthread.spawn_n(_inner)
        return self.done


# TODO(mikal): this class name is deprecated in Havana and should be removed
# in the I release
LoopingCall = FixedIntervalLoopingCall


class DynamicLoopingCall(LoopingCallBase):
    """A looping call which sleeps until the next known event.

    The function called should return how long to sleep for before being
    called again.
    """

    def start(self, initial_delay=None, periodic_interval_max=None):
        self._running = True
        done = event.Event()

        def _inner():
            if initial_delay:
                greenthread.sleep(initial_delay)

            try:
                while self._running:
                    idle = self.f(*self.args, **self.kw)
                    if not self._running:
                        break

                    if periodic_interval_max is not None:
                        idle = min(idle, periodic_interval_max)
                    LOG.debug(_('Dynamic looping call sleeping for %.02f '
                                'seconds'), idle)
                    greenthread.sleep(idle)
            except LoopingCallDone as e:
                self.stop()
                done.send(e.retvalue)
            except Exception:
                LOG.exception(_('in dynamic looping call'))
                done.send_exception(*sys.exc_info())
                return
            else:
                done.send(True)

        self.done = done

        greenthread.spawn(_inner)
        return self.done

########NEW FILE########
__FILENAME__ = context
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Middleware that attaches a context to the WSGI request
"""

from trove.openstack.common import context
from trove.openstack.common import importutils
from trove.openstack.common import wsgi


class ContextMiddleware(wsgi.Middleware):
    def __init__(self, app, options):
        self.options = options
        super(ContextMiddleware, self).__init__(app)

    def make_context(self, *args, **kwargs):
        """
        Create a context with the given arguments.
        """

        # Determine the context class to use
        ctxcls = context.RequestContext
        if 'context_class' in self.options:
            ctxcls = importutils.import_class(self.options['context_class'])

        return ctxcls(*args, **kwargs)

    def process_request(self, req):
        """
        Extract any authentication information in the request and
        construct an appropriate context from it.
        """
        # Use the default empty context, with admin turned on for
        # backwards compatibility
        req.context = self.make_context(is_admin=True)


def filter_factory(global_conf, **local_conf):
    """
    Factory method for paste.deploy
    """
    conf = global_conf.copy()
    conf.update(local_conf)

    def filter(app):
        return ContextMiddleware(app, conf)

    return filter

########NEW FILE########
__FILENAME__ = network_utils
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2012 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Network-related utilities and helper functions.
"""

import six.moves.urllib.parse as urlparse


def parse_host_port(address, default_port=None):
    """Interpret a string as a host:port pair.

    An IPv6 address MUST be escaped if accompanied by a port,
    because otherwise ambiguity ensues: 2001:db8:85a3::8a2e:370:7334
    means both [2001:db8:85a3::8a2e:370:7334] and
    [2001:db8:85a3::8a2e:370]:7334.

    >>> parse_host_port('server01:80')
    ('server01', 80)
    >>> parse_host_port('server01')
    ('server01', None)
    >>> parse_host_port('server01', default_port=1234)
    ('server01', 1234)
    >>> parse_host_port('[::1]:80')
    ('::1', 80)
    >>> parse_host_port('[::1]')
    ('::1', None)
    >>> parse_host_port('[::1]', default_port=1234)
    ('::1', 1234)
    >>> parse_host_port('2001:db8:85a3::8a2e:370:7334', default_port=1234)
    ('2001:db8:85a3::8a2e:370:7334', 1234)

    """
    if address[0] == '[':
        # Escaped ipv6
        _host, _port = address[1:].split(']')
        host = _host
        if ':' in _port:
            port = _port.split(':')[1]
        else:
            port = default_port
    else:
        if address.count(':') == 1:
            host, port = address.split(':')
        else:
            # 0 means ipv4, >1 means ipv6.
            # We prohibit unescaped ipv6 addresses with port.
            host = address
            port = default_port

    return (host, None if port is None else int(port))


def urlsplit(url, scheme='', allow_fragments=True):
    """Parse a URL using urlparse.urlsplit(), splitting query and fragments.
    This function papers over Python issue9374 when needed.

    The parameters are the same as urlparse.urlsplit.
    """
    scheme, netloc, path, query, fragment = urlparse.urlsplit(
        url, scheme, allow_fragments)
    if allow_fragments and '#' in path:
        path, fragment = path.split('#', 1)
    if '?' in path:
        path, query = path.split('?', 1)
    return urlparse.SplitResult(scheme, netloc, path, query, fragment)

########NEW FILE########
__FILENAME__ = api
# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import uuid

from oslo.config import cfg

from trove.openstack.common import context
from trove.openstack.common.gettextutils import _
from trove.openstack.common import importutils
from trove.openstack.common import jsonutils
from trove.openstack.common import log as logging
from trove.openstack.common import timeutils


LOG = logging.getLogger(__name__)

notifier_opts = [
    cfg.MultiStrOpt('notification_driver',
                    default=[],
                    help='Driver or drivers to handle sending notifications'),
    cfg.StrOpt('default_notification_level',
               default='INFO',
               help='Default notification level for outgoing notifications'),
    cfg.StrOpt('default_publisher_id',
               default='$host',
               help='Default publisher_id for outgoing notifications'),
]

CONF = cfg.CONF
CONF.register_opts(notifier_opts)

WARN = 'WARN'
INFO = 'INFO'
ERROR = 'ERROR'
CRITICAL = 'CRITICAL'
DEBUG = 'DEBUG'

log_levels = (DEBUG, WARN, INFO, ERROR, CRITICAL)


class BadPriorityException(Exception):
    pass


def notify_decorator(name, fn):
    """ decorator for notify which is used from utils.monkey_patch()

        :param name: name of the function
        :param function: - object of the function
        :returns: function -- decorated function

    """
    def wrapped_func(*args, **kwarg):
        body = {}
        body['args'] = []
        body['kwarg'] = {}
        for arg in args:
            body['args'].append(arg)
        for key in kwarg:
            body['kwarg'][key] = kwarg[key]

        ctxt = context.get_context_from_function_and_args(fn, args, kwarg)
        notify(ctxt,
               CONF.default_publisher_id,
               name,
               CONF.default_notification_level,
               body)
        return fn(*args, **kwarg)
    return wrapped_func


def publisher_id(service, host=None):
    if not host:
        host = CONF.host
    return "%s.%s" % (service, host)


def notify(context, publisher_id, event_type, priority, payload):
    """Sends a notification using the specified driver

    :param publisher_id: the source worker_type.host of the message
    :param event_type:   the literal type of event (ex. Instance Creation)
    :param priority:     patterned after the enumeration of Python logging
                         levels in the set (DEBUG, WARN, INFO, ERROR, CRITICAL)
    :param payload:       A python dictionary of attributes

    Outgoing message format includes the above parameters, and appends the
    following:

    message_id
      a UUID representing the id for this notification

    timestamp
      the GMT timestamp the notification was sent at

    The composite message will be constructed as a dictionary of the above
    attributes, which will then be sent via the transport mechanism defined
    by the driver.

    Message example::

        {'message_id': str(uuid.uuid4()),
         'publisher_id': 'compute.host1',
         'timestamp': timeutils.utcnow(),
         'priority': 'WARN',
         'event_type': 'compute.create_instance',
         'payload': {'instance_id': 12, ... }}

    """
    if priority not in log_levels:
        raise BadPriorityException(
            _('%s not in valid priorities') % priority)

    # Ensure everything is JSON serializable.
    payload = jsonutils.to_primitive(payload, convert_instances=True)

    msg = dict(message_id=str(uuid.uuid4()),
               publisher_id=publisher_id,
               event_type=event_type,
               priority=priority,
               payload=payload,
               timestamp=str(timeutils.utcnow()))

    for driver in _get_drivers():
        try:
            driver.notify(context, msg)
        except Exception as e:
            LOG.exception(_("Problem '%(e)s' attempting to "
                            "send to notification system. "
                            "Payload=%(payload)s")
                          % dict(e=e, payload=payload))


_drivers = None


def _get_drivers():
    """Instantiate, cache, and return drivers based on the CONF."""
    global _drivers
    if _drivers is None:
        _drivers = {}
        for notification_driver in CONF.notification_driver:
            add_driver(notification_driver)

    return _drivers.values()


def add_driver(notification_driver):
    """Add a notification driver at runtime."""
    # Make sure the driver list is initialized.
    _get_drivers()
    if isinstance(notification_driver, basestring):
        # Load and add
        try:
            driver = importutils.import_module(notification_driver)
            _drivers[notification_driver] = driver
        except ImportError:
            LOG.exception(_("Failed to load notifier %s. "
                            "These notifications will not be sent.") %
                          notification_driver)
    else:
        # Driver is already loaded; just add the object.
        _drivers[notification_driver] = notification_driver


def _reset_drivers():
    """Used by unit tests to reset the drivers."""
    global _drivers
    _drivers = None

########NEW FILE########
__FILENAME__ = log_notifier
# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from oslo.config import cfg

from trove.openstack.common import jsonutils
from trove.openstack.common import log as logging


CONF = cfg.CONF


def notify(_context, message):
    """Notifies the recipient of the desired event given the model.
    Log notifications using openstack's default logging system"""

    priority = message.get('priority',
                           CONF.default_notification_level)
    priority = priority.lower()
    logger = logging.getLogger(
        'trove.openstack.common.notification.%s' %
        message['event_type'])
    getattr(logger, priority)(jsonutils.dumps(message))

########NEW FILE########
__FILENAME__ = no_op_notifier
# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


def notify(_context, message):
    """Notifies the recipient of the desired event given the model"""
    pass

########NEW FILE########
__FILENAME__ = rabbit_notifier
# Copyright 2012 Red Hat, Inc.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


from trove.openstack.common.gettextutils import _
from trove.openstack.common import log as logging
from trove.openstack.common.notifier import rpc_notifier

LOG = logging.getLogger(__name__)


def notify(context, message):
    """Deprecated in Grizzly. Please use rpc_notifier instead."""

    LOG.deprecated(_("The rabbit_notifier is now deprecated."
                     " Please use rpc_notifier instead."))
    rpc_notifier.notify(context, message)

########NEW FILE########
__FILENAME__ = rpc_notifier
# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from oslo.config import cfg

from trove.openstack.common import context as req_context
from trove.openstack.common.gettextutils import _
from trove.openstack.common import log as logging
from trove.openstack.common import rpc

LOG = logging.getLogger(__name__)

notification_topic_opt = cfg.ListOpt(
    'notification_topics', default=['notifications', ],
    help='AMQP topic used for openstack notifications')

CONF = cfg.CONF
CONF.register_opt(notification_topic_opt)


def notify(context, message):
    """Sends a notification via RPC"""
    if not context:
        context = req_context.get_admin_context()
    priority = message.get('priority',
                           CONF.default_notification_level)
    priority = priority.lower()
    for topic in CONF.notification_topics:
        topic = '%s.%s' % (topic, priority)
        try:
            rpc.notify(context, topic, message)
        except Exception:
            LOG.exception(_("Could not send notification to %(topic)s. "
                            "Payload=%(message)s"), locals())

########NEW FILE########
__FILENAME__ = rpc_notifier2
# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

'''messaging based notification driver, with message envelopes'''

from oslo.config import cfg

from trove.openstack.common import context as req_context
from trove.openstack.common.gettextutils import _
from trove.openstack.common import log as logging
from trove.openstack.common import rpc

LOG = logging.getLogger(__name__)

notification_topic_opt = cfg.ListOpt(
    'topics', default=['notifications', ],
    help='AMQP topic(s) used for openstack notifications')

opt_group = cfg.OptGroup(name='rpc_notifier2',
                         title='Options for rpc_notifier2')

CONF = cfg.CONF
CONF.register_group(opt_group)
CONF.register_opt(notification_topic_opt, opt_group)


def notify(context, message):
    """Sends a notification via RPC"""
    if not context:
        context = req_context.get_admin_context()
    priority = message.get('priority',
                           CONF.default_notification_level)
    priority = priority.lower()
    for topic in CONF.rpc_notifier2.topics:
        topic = '%s.%s' % (topic, priority)
        try:
            rpc.notify(context, topic, message, envelope=True)
        except Exception:
            LOG.exception(_("Could not send notification to %(topic)s. "
                            "Payload=%(message)s"), locals())

########NEW FILE########
__FILENAME__ = test_notifier
# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


NOTIFICATIONS = []


def notify(_context, message):
    """Test notifier, stores notifications in memory for unittests."""
    NOTIFICATIONS.append(message)

########NEW FILE########
__FILENAME__ = pastedeploy
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2012 Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import sys

from paste import deploy

from trove.openstack.common import local


class BasePasteFactory(object):

    """A base class for paste app and filter factories.

    Sub-classes must override the KEY class attribute and provide
    a __call__ method.
    """

    KEY = None

    def __init__(self, data):
        self.data = data

    def _import_factory(self, local_conf):
        """Import an app/filter class.

        Lookup the KEY from the PasteDeploy local conf and import the
        class named there. This class can then be used as an app or
        filter factory.

        Note we support the <module>:<class> format.

        Note also that if you do e.g.

          key =
              value

        then ConfigParser returns a value with a leading newline, so
        we strip() the value before using it.
        """
        mod_str, _sep, class_str = local_conf[self.KEY].strip().rpartition(':')
        del local_conf[self.KEY]

        __import__(mod_str)
        return getattr(sys.modules[mod_str], class_str)


class AppFactory(BasePasteFactory):

    """A Generic paste.deploy app factory.

    This requires openstack.app_factory to be set to a callable which returns a
    WSGI app when invoked. The format of the name is <module>:<callable> e.g.

      [app:myfooapp]
      paste.app_factory = openstack.common.pastedeploy:app_factory
      openstack.app_factory = myapp:Foo

    The WSGI app constructor must accept a data object and a local config
    dict as its two arguments.
    """

    KEY = 'openstack.app_factory'

    def __call__(self, global_conf, **local_conf):
        """The actual paste.app_factory protocol method."""
        factory = self._import_factory(local_conf)
        return factory(self.data, **local_conf)


class FilterFactory(AppFactory):

    """A Generic paste.deploy filter factory.

    This requires openstack.filter_factory to be set to a callable which
    returns a  WSGI filter when invoked. The format is <module>:<callable> e.g.

      [filter:myfoofilter]
      paste.filter_factory = openstack.common.pastedeploy:filter_factory
      openstack.filter_factory = myfilter:Foo

    The WSGI filter constructor must accept a WSGI app, a data object and
    a local config dict as its three arguments.
    """

    KEY = 'openstack.filter_factory'

    def __call__(self, global_conf, **local_conf):
        """The actual paste.filter_factory protocol method."""
        factory = self._import_factory(local_conf)

        def filter(app):
            return factory(app, self.data, **local_conf)

        return filter


def app_factory(global_conf, **local_conf):
    """A paste app factory used with paste_deploy_app()."""
    return local.store.app_factory(global_conf, **local_conf)


def filter_factory(global_conf, **local_conf):
    """A paste filter factory used with paste_deploy_app()."""
    return local.store.filter_factory(global_conf, **local_conf)


def paste_deploy_app(paste_config_file, app_name, data):
    """Load a WSGI app from a PasteDeploy configuration.

    Use deploy.loadapp() to load the app from the PasteDeploy configuration,
    ensuring that the supplied data object is passed to the app and filter
    factories defined in this module.

    To use these factories and the data object, the configuration should look
    like this:

      [app:myapp]
      paste.app_factory = openstack.common.pastedeploy:app_factory
      openstack.app_factory = myapp:App
      ...
      [filter:myfilter]
      paste.filter_factory = openstack.common.pastedeploy:filter_factory
      openstack.filter_factory = myapp:Filter

    and then:

      myapp.py:

        class App(object):
            def __init__(self, data):
                ...

        class Filter(object):
            def __init__(self, app, data):
                ...

    :param paste_config_file: a PasteDeploy config file
    :param app_name: the name of the app/pipeline to load from the file
    :param data: a data object to supply to the app and its filters
    :returns: the WSGI app
    """
    (af, ff) = (AppFactory(data), FilterFactory(data))

    local.store.app_factory = af
    local.store.filter_factory = ff
    try:
        return deploy.loadapp("config:%s" % paste_config_file, name=app_name)
    finally:
        del local.store.app_factory
        del local.store.filter_factory

########NEW FILE########
__FILENAME__ = periodic_task
# vim: tabstop=4 shiftwidth=4 softtabstop=4

#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from trove.openstack.common.gettextutils import _
from trove.openstack.common import log as logging

LOG = logging.getLogger(__name__)


def periodic_task(*args, **kwargs):
    """Decorator to indicate that a method is a periodic task.

    This decorator can be used in two ways:

        1. Without arguments '@periodic_task', this will be run on every tick
           of the periodic scheduler.

        2. With arguments, @periodic_task(ticks_between_runs=N), this will be
           run on every N ticks of the periodic scheduler.
    """
    def decorator(f):
        f._periodic_task = True
        f._ticks_between_runs = kwargs.pop('ticks_between_runs', 0)
        return f

    # NOTE(sirp): The `if` is necessary to allow the decorator to be used with
    # and without parens.
    #
    # In the 'with-parens' case (with kwargs present), this function needs to
    # return a decorator function since the interpreter will invoke it like:
    #
    #   periodic_task(*args, **kwargs)(f)
    #
    # In the 'without-parens' case, the original function will be passed
    # in as the first argument, like:
    #
    #   periodic_task(f)
    if kwargs:
        return decorator
    else:
        return decorator(args[0])


class _PeriodicTasksMeta(type):
    def __init__(cls, names, bases, dict_):
        """Metaclass that allows us to collect decorated periodic tasks."""
        super(_PeriodicTasksMeta, cls).__init__(names, bases, dict_)

        # NOTE(sirp): if the attribute is not present then we must be the base
        # class, so, go ahead and initialize it. If the attribute is present,
        # then we're a subclass so make a copy of it so we don't step on our
        # parent's toes.
        try:
            cls._periodic_tasks = cls._periodic_tasks[:]
        except AttributeError:
            cls._periodic_tasks = []

        try:
            cls._ticks_to_skip = cls._ticks_to_skip.copy()
        except AttributeError:
            cls._ticks_to_skip = {}

        # This uses __dict__ instead of
        # inspect.getmembers(cls, inspect.ismethod) so only the methods of the
        # current class are added when this class is scanned, and base classes
        # are not added redundantly.
        for value in cls.__dict__.values():
            if getattr(value, '_periodic_task', False):
                task = value
                name = task.__name__
                cls._periodic_tasks.append((name, task))
                cls._ticks_to_skip[name] = task._ticks_between_runs


class PeriodicTasks(object):
    __metaclass__ = _PeriodicTasksMeta

    def run_periodic_tasks(self, context, raise_on_error=False):
        """Tasks to be run at a periodic interval."""
        for task_name, task in self._periodic_tasks:
            full_task_name = '.'.join([self.__class__.__name__, task_name])

            ticks_to_skip = self._ticks_to_skip[task_name]
            if ticks_to_skip > 0:
                LOG.debug(_("Skipping %(full_task_name)s, %(ticks_to_skip)s"
                            " ticks left until next run"),
                          dict(full_task_name=full_task_name,
                               ticks_to_skip=ticks_to_skip))
                self._ticks_to_skip[task_name] -= 1
                continue

            self._ticks_to_skip[task_name] = task._ticks_between_runs
            LOG.debug(_("Running periodic task %(full_task_name)s"),
                      dict(full_task_name=full_task_name))

            try:
                task(self, context)
            except Exception as e:
                if raise_on_error:
                    raise
                LOG.exception(_("Error during %(full_task_name)s:"
                                " %(e)s"),
                              dict(e=e, full_task_name=full_task_name))

########NEW FILE########
__FILENAME__ = policy
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright (c) 2012 OpenStack, LLC.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Common Policy Engine Implementation

Policies can be expressed in one of two forms: A list of lists, or a
string written in the new policy language.

In the list-of-lists representation, each check inside the innermost
list is combined as with an "and" conjunction--for that check to pass,
all the specified checks must pass.  These innermost lists are then
combined as with an "or" conjunction.  This is the original way of
expressing policies, but there now exists a new way: the policy
language.

In the policy language, each check is specified the same way as in the
list-of-lists representation: a simple "a:b" pair that is matched to
the correct code to perform that check.  However, conjunction
operators are available, allowing for more expressiveness in crafting
policies.

As an example, take the following rule, expressed in the list-of-lists
representation::

    [["role:admin"], ["project_id:%(project_id)s", "role:projectadmin"]]

In the policy language, this becomes::

    role:admin or (project_id:%(project_id)s and role:projectadmin)

The policy language also has the "not" operator, allowing a richer
policy rule::

    project_id:%(project_id)s and not role:dunce

Finally, two special policy checks should be mentioned; the policy
check "@" will always accept an access, and the policy check "!" will
always reject an access.  (Note that if a rule is either the empty
list ("[]") or the empty string, this is equivalent to the "@" policy
check.)  Of these, the "!" policy check is probably the most useful,
as it allows particular rules to be explicitly disabled.
"""

import abc
import logging
import re
import urllib

import urllib2

from trove.openstack.common.gettextutils import _
from trove.openstack.common import jsonutils


LOG = logging.getLogger(__name__)


_rules = None
_checks = {}


class Rules(dict):
    """
    A store for rules.  Handles the default_rule setting directly.
    """

    @classmethod
    def load_json(cls, data, default_rule=None):
        """
        Allow loading of JSON rule data.
        """

        # Suck in the JSON data and parse the rules
        rules = dict((k, parse_rule(v)) for k, v in
                     jsonutils.loads(data).items())

        return cls(rules, default_rule)

    def __init__(self, rules=None, default_rule=None):
        """Initialize the Rules store."""

        super(Rules, self).__init__(rules or {})
        self.default_rule = default_rule

    def __missing__(self, key):
        """Implements the default rule handling."""

        # If the default rule isn't actually defined, do something
        # reasonably intelligent
        if not self.default_rule or self.default_rule not in self:
            raise KeyError(key)

        return self[self.default_rule]

    def __str__(self):
        """Dumps a string representation of the rules."""

        # Start by building the canonical strings for the rules
        out_rules = {}
        for key, value in self.items():
            # Use empty string for singleton TrueCheck instances
            if isinstance(value, TrueCheck):
                out_rules[key] = ''
            else:
                out_rules[key] = str(value)

        # Dump a pretty-printed JSON representation
        return jsonutils.dumps(out_rules, indent=4)


# Really have to figure out a way to deprecate this
def set_rules(rules):
    """Set the rules in use for policy checks."""

    global _rules

    _rules = rules


# Ditto
def reset():
    """Clear the rules used for policy checks."""

    global _rules

    _rules = None


def check(rule, target, creds, exc=None, *args, **kwargs):
    """
    Checks authorization of a rule against the target and credentials.

    :param rule: The rule to evaluate.
    :param target: As much information about the object being operated
                   on as possible, as a dictionary.
    :param creds: As much information about the user performing the
                  action as possible, as a dictionary.
    :param exc: Class of the exception to raise if the check fails.
                Any remaining arguments passed to check() (both
                positional and keyword arguments) will be passed to
                the exception class.  If exc is not provided, returns
                False.

    :return: Returns False if the policy does not allow the action and
             exc is not provided; otherwise, returns a value that
             evaluates to True.  Note: for rules using the "case"
             expression, this True value will be the specified string
             from the expression.
    """

    # Allow the rule to be a Check tree
    if isinstance(rule, BaseCheck):
        result = rule(target, creds)
    elif not _rules:
        # No rules to reference means we're going to fail closed
        result = False
    else:
        try:
            # Evaluate the rule
            result = _rules[rule](target, creds)
        except KeyError:
            # If the rule doesn't exist, fail closed
            result = False

    # If it is False, raise the exception if requested
    if exc and result is False:
        raise exc(*args, **kwargs)

    return result


class BaseCheck(object):
    """
    Abstract base class for Check classes.
    """

    __metaclass__ = abc.ABCMeta

    @abc.abstractmethod
    def __str__(self):
        """
        Retrieve a string representation of the Check tree rooted at
        this node.
        """

        pass

    @abc.abstractmethod
    def __call__(self, target, cred):
        """
        Perform the check.  Returns False to reject the access or a
        true value (not necessary True) to accept the access.
        """

        pass


class FalseCheck(BaseCheck):
    """
    A policy check that always returns False (disallow).
    """

    def __str__(self):
        """Return a string representation of this check."""

        return "!"

    def __call__(self, target, cred):
        """Check the policy."""

        return False


class TrueCheck(BaseCheck):
    """
    A policy check that always returns True (allow).
    """

    def __str__(self):
        """Return a string representation of this check."""

        return "@"

    def __call__(self, target, cred):
        """Check the policy."""

        return True


class Check(BaseCheck):
    """
    A base class to allow for user-defined policy checks.
    """

    def __init__(self, kind, match):
        """
        :param kind: The kind of the check, i.e., the field before the
                     ':'.
        :param match: The match of the check, i.e., the field after
                      the ':'.
        """

        self.kind = kind
        self.match = match

    def __str__(self):
        """Return a string representation of this check."""

        return "%s:%s" % (self.kind, self.match)


class NotCheck(BaseCheck):
    """
    A policy check that inverts the result of another policy check.
    Implements the "not" operator.
    """

    def __init__(self, rule):
        """
        Initialize the 'not' check.

        :param rule: The rule to negate.  Must be a Check.
        """

        self.rule = rule

    def __str__(self):
        """Return a string representation of this check."""

        return "not %s" % self.rule

    def __call__(self, target, cred):
        """
        Check the policy.  Returns the logical inverse of the wrapped
        check.
        """

        return not self.rule(target, cred)


class AndCheck(BaseCheck):
    """
    A policy check that requires that a list of other checks all
    return True.  Implements the "and" operator.
    """

    def __init__(self, rules):
        """
        Initialize the 'and' check.

        :param rules: A list of rules that will be tested.
        """

        self.rules = rules

    def __str__(self):
        """Return a string representation of this check."""

        return "(%s)" % ' and '.join(str(r) for r in self.rules)

    def __call__(self, target, cred):
        """
        Check the policy.  Requires that all rules accept in order to
        return True.
        """

        for rule in self.rules:
            if not rule(target, cred):
                return False

        return True

    def add_check(self, rule):
        """
        Allows addition of another rule to the list of rules that will
        be tested.  Returns the AndCheck object for convenience.
        """

        self.rules.append(rule)
        return self


class OrCheck(BaseCheck):
    """
    A policy check that requires that at least one of a list of other
    checks returns True.  Implements the "or" operator.
    """

    def __init__(self, rules):
        """
        Initialize the 'or' check.

        :param rules: A list of rules that will be tested.
        """

        self.rules = rules

    def __str__(self):
        """Return a string representation of this check."""

        return "(%s)" % ' or '.join(str(r) for r in self.rules)

    def __call__(self, target, cred):
        """
        Check the policy.  Requires that at least one rule accept in
        order to return True.
        """

        for rule in self.rules:
            if rule(target, cred):
                return True

        return False

    def add_check(self, rule):
        """
        Allows addition of another rule to the list of rules that will
        be tested.  Returns the OrCheck object for convenience.
        """

        self.rules.append(rule)
        return self


def _parse_check(rule):
    """
    Parse a single base check rule into an appropriate Check object.
    """

    # Handle the special checks
    if rule == '!':
        return FalseCheck()
    elif rule == '@':
        return TrueCheck()

    try:
        kind, match = rule.split(':', 1)
    except Exception:
        LOG.exception(_("Failed to understand rule %(rule)s") % locals())
        # If the rule is invalid, we'll fail closed
        return FalseCheck()

    # Find what implements the check
    if kind in _checks:
        return _checks[kind](kind, match)
    elif None in _checks:
        return _checks[None](kind, match)
    else:
        LOG.error(_("No handler for matches of kind %s") % kind)
        return FalseCheck()


def _parse_list_rule(rule):
    """
    Provided for backwards compatibility.  Translates the old
    list-of-lists syntax into a tree of Check objects.
    """

    # Empty rule defaults to True
    if not rule:
        return TrueCheck()

    # Outer list is joined by "or"; inner list by "and"
    or_list = []
    for inner_rule in rule:
        # Elide empty inner lists
        if not inner_rule:
            continue

        # Handle bare strings
        if isinstance(inner_rule, basestring):
            inner_rule = [inner_rule]

        # Parse the inner rules into Check objects
        and_list = [_parse_check(r) for r in inner_rule]

        # Append the appropriate check to the or_list
        if len(and_list) == 1:
            or_list.append(and_list[0])
        else:
            or_list.append(AndCheck(and_list))

    # If we have only one check, omit the "or"
    if len(or_list) == 0:
        return FalseCheck()
    elif len(or_list) == 1:
        return or_list[0]

    return OrCheck(or_list)


# Used for tokenizing the policy language
_tokenize_re = re.compile(r'\s+')


def _parse_tokenize(rule):
    """
    Tokenizer for the policy language.

    Most of the single-character tokens are specified in the
    _tokenize_re; however, parentheses need to be handled specially,
    because they can appear inside a check string.  Thankfully, those
    parentheses that appear inside a check string can never occur at
    the very beginning or end ("%(variable)s" is the correct syntax).
    """

    for tok in _tokenize_re.split(rule):
        # Skip empty tokens
        if not tok or tok.isspace():
            continue

        # Handle leading parens on the token
        clean = tok.lstrip('(')
        for i in range(len(tok) - len(clean)):
            yield '(', '('

        # If it was only parentheses, continue
        if not clean:
            continue
        else:
            tok = clean

        # Handle trailing parens on the token
        clean = tok.rstrip(')')
        trail = len(tok) - len(clean)

        # Yield the cleaned token
        lowered = clean.lower()
        if lowered in ('and', 'or', 'not'):
            # Special tokens
            yield lowered, clean
        elif clean:
            # Not a special token, but not composed solely of ')'
            if len(tok) >= 2 and ((tok[0], tok[-1]) in
                                  [('"', '"'), ("'", "'")]):
                # It's a quoted string
                yield 'string', tok[1:-1]
            else:
                yield 'check', _parse_check(clean)

        # Yield the trailing parens
        for i in range(trail):
            yield ')', ')'


class ParseStateMeta(type):
    """
    Metaclass for the ParseState class.  Facilitates identifying
    reduction methods.
    """

    def __new__(mcs, name, bases, cls_dict):
        """
        Create the class.  Injects the 'reducers' list, a list of
        tuples matching token sequences to the names of the
        corresponding reduction methods.
        """

        reducers = []

        for key, value in cls_dict.items():
            if not hasattr(value, 'reducers'):
                continue
            for reduction in value.reducers:
                reducers.append((reduction, key))

        cls_dict['reducers'] = reducers

        return super(ParseStateMeta, mcs).__new__(mcs, name, bases, cls_dict)


def reducer(*tokens):
    """
    Decorator for reduction methods.  Arguments are a sequence of
    tokens, in order, which should trigger running this reduction
    method.
    """

    def decorator(func):
        # Make sure we have a list of reducer sequences
        if not hasattr(func, 'reducers'):
            func.reducers = []

        # Add the tokens to the list of reducer sequences
        func.reducers.append(list(tokens))

        return func

    return decorator


class ParseState(object):
    """
    Implement the core of parsing the policy language.  Uses a greedy
    reduction algorithm to reduce a sequence of tokens into a single
    terminal, the value of which will be the root of the Check tree.

    Note: error reporting is rather lacking.  The best we can get with
    this parser formulation is an overall "parse failed" error.
    Fortunately, the policy language is simple enough that this
    shouldn't be that big a problem.
    """

    __metaclass__ = ParseStateMeta

    def __init__(self):
        """Initialize the ParseState."""

        self.tokens = []
        self.values = []

    def reduce(self):
        """
        Perform a greedy reduction of the token stream.  If a reducer
        method matches, it will be executed, then the reduce() method
        will be called recursively to search for any more possible
        reductions.
        """

        for reduction, methname in self.reducers:
            if (len(self.tokens) >= len(reduction) and
                self.tokens[-len(reduction):] == reduction):
                    # Get the reduction method
                    meth = getattr(self, methname)

                    # Reduce the token stream
                    results = meth(*self.values[-len(reduction):])

                    # Update the tokens and values
                    self.tokens[-len(reduction):] = [r[0] for r in results]
                    self.values[-len(reduction):] = [r[1] for r in results]

                    # Check for any more reductions
                    return self.reduce()

    def shift(self, tok, value):
        """Adds one more token to the state.  Calls reduce()."""

        self.tokens.append(tok)
        self.values.append(value)

        # Do a greedy reduce...
        self.reduce()

    @property
    def result(self):
        """
        Obtain the final result of the parse.  Raises ValueError if
        the parse failed to reduce to a single result.
        """

        if len(self.values) != 1:
            raise ValueError("Could not parse rule")
        return self.values[0]

    @reducer('(', 'check', ')')
    @reducer('(', 'and_expr', ')')
    @reducer('(', 'or_expr', ')')
    def _wrap_check(self, _p1, check, _p2):
        """Turn parenthesized expressions into a 'check' token."""

        return [('check', check)]

    @reducer('check', 'and', 'check')
    def _make_and_expr(self, check1, _and, check2):
        """
        Create an 'and_expr' from two checks joined by the 'and'
        operator.
        """

        return [('and_expr', AndCheck([check1, check2]))]

    @reducer('and_expr', 'and', 'check')
    def _extend_and_expr(self, and_expr, _and, check):
        """
        Extend an 'and_expr' by adding one more check.
        """

        return [('and_expr', and_expr.add_check(check))]

    @reducer('check', 'or', 'check')
    def _make_or_expr(self, check1, _or, check2):
        """
        Create an 'or_expr' from two checks joined by the 'or'
        operator.
        """

        return [('or_expr', OrCheck([check1, check2]))]

    @reducer('or_expr', 'or', 'check')
    def _extend_or_expr(self, or_expr, _or, check):
        """
        Extend an 'or_expr' by adding one more check.
        """

        return [('or_expr', or_expr.add_check(check))]

    @reducer('not', 'check')
    def _make_not_expr(self, _not, check):
        """Invert the result of another check."""

        return [('check', NotCheck(check))]


def _parse_text_rule(rule):
    """
    Translates a policy written in the policy language into a tree of
    Check objects.
    """

    # Empty rule means always accept
    if not rule:
        return TrueCheck()

    # Parse the token stream
    state = ParseState()
    for tok, value in _parse_tokenize(rule):
        state.shift(tok, value)

    try:
        return state.result
    except ValueError:
        # Couldn't parse the rule
        LOG.exception(_("Failed to understand rule %(rule)r") % locals())

        # Fail closed
        return FalseCheck()


def parse_rule(rule):
    """
    Parses a policy rule into a tree of Check objects.
    """

    # If the rule is a string, it's in the policy language
    if isinstance(rule, basestring):
        return _parse_text_rule(rule)
    return _parse_list_rule(rule)


def register(name, func=None):
    """
    Register a function or Check class as a policy check.

    :param name: Gives the name of the check type, e.g., 'rule',
                 'role', etc.  If name is None, a default check type
                 will be registered.
    :param func: If given, provides the function or class to register.
                 If not given, returns a function taking one argument
                 to specify the function or class to register,
                 allowing use as a decorator.
    """

    # Perform the actual decoration by registering the function or
    # class.  Returns the function or class for compliance with the
    # decorator interface.
    def decorator(func):
        _checks[name] = func
        return func

    # If the function or class is given, do the registration
    if func:
        return decorator(func)

    return decorator


@register("rule")
class RuleCheck(Check):
    def __call__(self, target, creds):
        """
        Recursively checks credentials based on the defined rules.
        """

        try:
            return _rules[self.match](target, creds)
        except KeyError:
            # We don't have any matching rule; fail closed
            return False


@register("role")
class RoleCheck(Check):
    def __call__(self, target, creds):
        """Check that there is a matching role in the cred dict."""

        return self.match.lower() in [x.lower() for x in creds['roles']]


@register('http')
class HttpCheck(Check):
    def __call__(self, target, creds):
        """
        Check http: rules by calling to a remote server.

        This example implementation simply verifies that the response
        is exactly 'True'.
        """

        url = ('http:' + self.match) % target
        data = {'target': jsonutils.dumps(target),
                'credentials': jsonutils.dumps(creds)}
        post_data = urllib.urlencode(data)
        f = urllib2.urlopen(url, post_data)
        return f.read() == "True"


@register(None)
class GenericCheck(Check):
    def __call__(self, target, creds):
        """
        Check an individual match.

        Matches look like:

            tenant:%(tenant_id)s
            role:compute:admin
        """

        # TODO(termie): do dict inspection via dot syntax
        match = self.match % target
        if self.kind in creds:
            return match == unicode(creds[self.kind])
        return False

########NEW FILE########
__FILENAME__ = processutils
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
System-level utilities and helper functions.
"""

import os
import random
import shlex
import signal

from eventlet.green import subprocess
from eventlet import greenthread

from trove.openstack.common.gettextutils import _
from trove.openstack.common import log as logging


LOG = logging.getLogger(__name__)


class UnknownArgumentError(Exception):
    def __init__(self, message=None):
        super(UnknownArgumentError, self).__init__(message)


class ProcessExecutionError(Exception):
    def __init__(self, stdout=None, stderr=None, exit_code=None, cmd=None,
                 description=None):
        self.exit_code = exit_code
        self.stderr = stderr
        self.stdout = stdout
        self.cmd = cmd
        self.description = description

        if description is None:
            description = "Unexpected error while running command."
        if exit_code is None:
            exit_code = '-'
        message = ("%s\nCommand: %s\nExit code: %s\nStdout: %r\nStderr: %r"
                   % (description, cmd, exit_code, stdout, stderr))
        super(ProcessExecutionError, self).__init__(message)


class NoRootWrapSpecified(Exception):
    def __init__(self, message=None):
        super(NoRootWrapSpecified, self).__init__(message)


def _subprocess_setup():
    # Python installs a SIGPIPE handler by default. This is usually not what
    # non-Python subprocesses expect.
    signal.signal(signal.SIGPIPE, signal.SIG_DFL)


def execute(*cmd, **kwargs):
    """
    Helper method to shell out and execute a command through subprocess with
    optional retry.

    :param cmd:             Passed to subprocess.Popen.
    :type cmd:              string
    :param process_input:   Send to opened process.
    :type proces_input:     string
    :param check_exit_code: Single bool, int, or list of allowed exit
                            codes.  Defaults to [0].  Raise
                            :class:`ProcessExecutionError` unless
                            program exits with one of these code.
    :type check_exit_code:  boolean, int, or [int]
    :param delay_on_retry:  True | False. Defaults to True. If set to True,
                            wait a short amount of time before retrying.
    :type delay_on_retry:   boolean
    :param attempts:        How many times to retry cmd.
    :type attempts:         int
    :param run_as_root:     True | False. Defaults to False. If set to True,
                            the command is prefixed by the command specified
                            in the root_helper kwarg.
    :type run_as_root:      boolean
    :param root_helper:     command to prefix to commands called with
                            run_as_root=True
    :type root_helper:      string
    :param shell:           whether or not there should be a shell used to
                            execute this command. Defaults to false.
    :type shell:            boolean
    :returns:               (stdout, stderr) from process execution
    :raises:                :class:`UnknownArgumentError` on
                            receiving unknown arguments
    :raises:                :class:`ProcessExecutionError`
    """

    process_input = kwargs.pop('process_input', None)
    check_exit_code = kwargs.pop('check_exit_code', [0])
    ignore_exit_code = False
    delay_on_retry = kwargs.pop('delay_on_retry', True)
    attempts = kwargs.pop('attempts', 1)
    run_as_root = kwargs.pop('run_as_root', False)
    root_helper = kwargs.pop('root_helper', '')
    shell = kwargs.pop('shell', False)

    if isinstance(check_exit_code, bool):
        ignore_exit_code = not check_exit_code
        check_exit_code = [0]
    elif isinstance(check_exit_code, int):
        check_exit_code = [check_exit_code]

    if len(kwargs):
        raise UnknownArgumentError(_('Got unknown keyword args '
                                     'to utils.execute: %r') % kwargs)

    if run_as_root and os.geteuid() != 0:
        if not root_helper:
            raise NoRootWrapSpecified(
                message=('Command requested root, but did not specify a root '
                         'helper.'))
        cmd = shlex.split(root_helper) + list(cmd)

    cmd = map(str, cmd)

    while attempts > 0:
        attempts -= 1
        try:
            LOG.debug(_('Running cmd (subprocess): %s'), ' '.join(cmd))
            _PIPE = subprocess.PIPE  # pylint: disable=E1101

            if os.name == 'nt':
                preexec_fn = None
                close_fds = False
            else:
                preexec_fn = _subprocess_setup
                close_fds = True

            obj = subprocess.Popen(cmd,
                                   stdin=_PIPE,
                                   stdout=_PIPE,
                                   stderr=_PIPE,
                                   close_fds=close_fds,
                                   preexec_fn=preexec_fn,
                                   shell=shell)
            result = None
            if process_input is not None:
                result = obj.communicate(process_input)
            else:
                result = obj.communicate()
            obj.stdin.close()  # pylint: disable=E1101
            _returncode = obj.returncode  # pylint: disable=E1101
            if _returncode:
                LOG.debug(_('Result was %s') % _returncode)
                if not ignore_exit_code and _returncode not in check_exit_code:
                    (stdout, stderr) = result
                    raise ProcessExecutionError(exit_code=_returncode,
                                                stdout=stdout,
                                                stderr=stderr,
                                                cmd=' '.join(cmd))
            return result
        except ProcessExecutionError:
            if not attempts:
                raise
            else:
                LOG.debug(_('%r failed. Retrying.'), cmd)
                if delay_on_retry:
                    greenthread.sleep(random.randint(20, 200) / 100.0)
        finally:
            # NOTE(termie): this appears to be necessary to let the subprocess
            #               call clean something up in between calls, without
            #               it two execute calls in a row hangs the second one
            greenthread.sleep(0)

########NEW FILE########
__FILENAME__ = amqp
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
# Copyright 2011 - 2012, Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Shared code between AMQP based openstack.common.rpc implementations.

The code in this module is shared between the rpc implemenations based on AMQP.
Specifically, this includes impl_kombu and impl_qpid.  impl_carrot also uses
AMQP, but is deprecated and predates this code.
"""

import collections
import inspect
import sys
import uuid

from eventlet import greenpool
from eventlet import pools
from eventlet import queue
from eventlet import semaphore
from oslo.config import cfg

from trove.openstack.common import excutils
from trove.openstack.common.gettextutils import _  # noqa
from trove.openstack.common import local
from trove.openstack.common import log as logging
from trove.openstack.common.rpc import common as rpc_common


amqp_opts = [
    cfg.BoolOpt('amqp_durable_queues',
                default=False,
                deprecated_name='rabbit_durable_queues',
                deprecated_group='DEFAULT',
                help='Use durable queues in amqp.'),
    cfg.BoolOpt('amqp_auto_delete',
                default=False,
                help='Auto-delete queues in amqp.'),
]

cfg.CONF.register_opts(amqp_opts)

UNIQUE_ID = '_unique_id'
LOG = logging.getLogger(__name__)


class Pool(pools.Pool):
    """Class that implements a Pool of Connections."""
    def __init__(self, conf, connection_cls, *args, **kwargs):
        self.connection_cls = connection_cls
        self.conf = conf
        kwargs.setdefault("max_size", self.conf.rpc_conn_pool_size)
        kwargs.setdefault("order_as_stack", True)
        super(Pool, self).__init__(*args, **kwargs)
        self.reply_proxy = None

    # TODO(comstud): Timeout connections not used in a while
    def create(self):
        LOG.debug(_('Pool creating new connection'))
        return self.connection_cls(self.conf)

    def empty(self):
        while self.free_items:
            self.get().close()
        # Force a new connection pool to be created.
        # Note that this was added due to failing unit test cases. The issue
        # is the above "while loop" gets all the cached connections from the
        # pool and closes them, but never returns them to the pool, a pool
        # leak. The unit tests hang waiting for an item to be returned to the
        # pool. The unit tests get here via the tearDown() method. In the run
        # time code, it gets here via cleanup() and only appears in service.py
        # just before doing a sys.exit(), so cleanup() only happens once and
        # the leakage is not a problem.
        self.connection_cls.pool = None


_pool_create_sem = semaphore.Semaphore()


def get_connection_pool(conf, connection_cls):
    with _pool_create_sem:
        # Make sure only one thread tries to create the connection pool.
        if not connection_cls.pool:
            connection_cls.pool = Pool(conf, connection_cls)
    return connection_cls.pool


class ConnectionContext(rpc_common.Connection):
    """The class that is actually returned to the create_connection() caller.

    This is essentially a wrapper around Connection that supports 'with'.
    It can also return a new Connection, or one from a pool.

    The function will also catch when an instance of this class is to be
    deleted.  With that we can return Connections to the pool on exceptions
    and so forth without making the caller be responsible for catching them.
    If possible the function makes sure to return a connection to the pool.
    """

    def __init__(self, conf, connection_pool, pooled=True, server_params=None):
        """Create a new connection, or get one from the pool."""
        self.connection = None
        self.conf = conf
        self.connection_pool = connection_pool
        if pooled:
            self.connection = connection_pool.get()
        else:
            self.connection = connection_pool.connection_cls(
                conf,
                server_params=server_params)
        self.pooled = pooled

    def __enter__(self):
        """When with ConnectionContext() is used, return self."""
        return self

    def _done(self):
        """If the connection came from a pool, clean it up and put it back.
        If it did not come from a pool, close it.
        """
        if self.connection:
            if self.pooled:
                # Reset the connection so it's ready for the next caller
                # to grab from the pool
                self.connection.reset()
                self.connection_pool.put(self.connection)
            else:
                try:
                    self.connection.close()
                except Exception:
                    pass
            self.connection = None

    def __exit__(self, exc_type, exc_value, tb):
        """End of 'with' statement.  We're done here."""
        self._done()

    def __del__(self):
        """Caller is done with this connection.  Make sure we cleaned up."""
        self._done()

    def close(self):
        """Caller is done with this connection."""
        self._done()

    def create_consumer(self, topic, proxy, fanout=False):
        self.connection.create_consumer(topic, proxy, fanout)

    def create_worker(self, topic, proxy, pool_name):
        self.connection.create_worker(topic, proxy, pool_name)

    def join_consumer_pool(self, callback, pool_name, topic, exchange_name,
                           ack_on_error=True):
        self.connection.join_consumer_pool(callback,
                                           pool_name,
                                           topic,
                                           exchange_name,
                                           ack_on_error)

    def consume_in_thread(self):
        self.connection.consume_in_thread()

    def __getattr__(self, key):
        """Proxy all other calls to the Connection instance."""
        if self.connection:
            return getattr(self.connection, key)
        else:
            raise rpc_common.InvalidRPCConnectionReuse()


class ReplyProxy(ConnectionContext):
    """Connection class for RPC replies / callbacks."""
    def __init__(self, conf, connection_pool):
        self._call_waiters = {}
        self._num_call_waiters = 0
        self._num_call_waiters_wrn_threshhold = 10
        self._reply_q = 'reply_' + uuid.uuid4().hex
        super(ReplyProxy, self).__init__(conf, connection_pool, pooled=False)
        self.declare_direct_consumer(self._reply_q, self._process_data)
        self.consume_in_thread()

    def _process_data(self, message_data):
        msg_id = message_data.pop('_msg_id', None)
        waiter = self._call_waiters.get(msg_id)
        if not waiter:
            LOG.warn(_('No calling threads waiting for msg_id : %(msg_id)s'
                       ', message : %(data)s'), {'msg_id': msg_id,
                                                 'data': message_data})
            LOG.warn(_('_call_waiters: %s') % str(self._call_waiters))
        else:
            waiter.put(message_data)

    def add_call_waiter(self, waiter, msg_id):
        self._num_call_waiters += 1
        if self._num_call_waiters > self._num_call_waiters_wrn_threshhold:
            LOG.warn(_('Number of call waiters is greater than warning '
                       'threshhold: %d. There could be a MulticallProxyWaiter '
                       'leak.') % self._num_call_waiters_wrn_threshhold)
            self._num_call_waiters_wrn_threshhold *= 2
        self._call_waiters[msg_id] = waiter

    def del_call_waiter(self, msg_id):
        self._num_call_waiters -= 1
        del self._call_waiters[msg_id]

    def get_reply_q(self):
        return self._reply_q


def msg_reply(conf, msg_id, reply_q, connection_pool, reply=None,
              failure=None, ending=False, log_failure=True):
    """Sends a reply or an error on the channel signified by msg_id.

    Failure should be a sys.exc_info() tuple.

    """
    with ConnectionContext(conf, connection_pool) as conn:
        if failure:
            failure = rpc_common.serialize_remote_exception(failure,
                                                            log_failure)

        msg = {'result': reply, 'failure': failure}
        if ending:
            msg['ending'] = True
        _add_unique_id(msg)
        # If a reply_q exists, add the msg_id to the reply and pass the
        # reply_q to direct_send() to use it as the response queue.
        # Otherwise use the msg_id for backward compatibilty.
        if reply_q:
            msg['_msg_id'] = msg_id
            conn.direct_send(reply_q, rpc_common.serialize_msg(msg))
        else:
            conn.direct_send(msg_id, rpc_common.serialize_msg(msg))


class RpcContext(rpc_common.CommonRpcContext):
    """Context that supports replying to a rpc.call."""
    def __init__(self, **kwargs):
        self.msg_id = kwargs.pop('msg_id', None)
        self.reply_q = kwargs.pop('reply_q', None)
        self.conf = kwargs.pop('conf')
        super(RpcContext, self).__init__(**kwargs)

    def deepcopy(self):
        values = self.to_dict()
        values['conf'] = self.conf
        values['msg_id'] = self.msg_id
        values['reply_q'] = self.reply_q
        return self.__class__(**values)

    def reply(self, reply=None, failure=None, ending=False,
              connection_pool=None, log_failure=True):
        if self.msg_id:
            msg_reply(self.conf, self.msg_id, self.reply_q, connection_pool,
                      reply, failure, ending, log_failure)
            if ending:
                self.msg_id = None


def unpack_context(conf, msg):
    """Unpack context from msg."""
    context_dict = {}
    for key in list(msg.keys()):
        # NOTE(vish): Some versions of python don't like unicode keys
        #             in kwargs.
        key = str(key)
        if key.startswith('_context_'):
            value = msg.pop(key)
            context_dict[key[9:]] = value
    context_dict['msg_id'] = msg.pop('_msg_id', None)
    context_dict['reply_q'] = msg.pop('_reply_q', None)
    context_dict['conf'] = conf
    ctx = RpcContext.from_dict(context_dict)
    rpc_common._safe_log(LOG.debug, _('unpacked context: %s'), ctx.to_dict())
    return ctx


def pack_context(msg, context):
    """Pack context into msg.

    Values for message keys need to be less than 255 chars, so we pull
    context out into a bunch of separate keys. If we want to support
    more arguments in rabbit messages, we may want to do the same
    for args at some point.

    """
    if isinstance(context, dict):
        context_d = dict([('_context_%s' % key, value)
                          for (key, value) in context.iteritems()])
    else:
        context_d = dict([('_context_%s' % key, value)
                          for (key, value) in context.to_dict().iteritems()])

    msg.update(context_d)


class _MsgIdCache(object):
    """This class checks any duplicate messages."""

    # NOTE: This value is considered can be a configuration item, but
    #       it is not necessary to change its value in most cases,
    #       so let this value as static for now.
    DUP_MSG_CHECK_SIZE = 16

    def __init__(self, **kwargs):
        self.prev_msgids = collections.deque([],
                                             maxlen=self.DUP_MSG_CHECK_SIZE)

    def check_duplicate_message(self, message_data):
        """AMQP consumers may read same message twice when exceptions occur
           before ack is returned. This method prevents doing it.
        """
        if UNIQUE_ID in message_data:
            msg_id = message_data[UNIQUE_ID]
            if msg_id not in self.prev_msgids:
                self.prev_msgids.append(msg_id)
            else:
                raise rpc_common.DuplicateMessageError(msg_id=msg_id)


def _add_unique_id(msg):
    """Add unique_id for checking duplicate messages."""
    unique_id = uuid.uuid4().hex
    msg.update({UNIQUE_ID: unique_id})
    LOG.debug(_('UNIQUE_ID is %s.') % (unique_id))


class _ThreadPoolWithWait(object):
    """Base class for a delayed invocation manager.

    Used by the Connection class to start up green threads
    to handle incoming messages.
    """

    def __init__(self, conf, connection_pool):
        self.pool = greenpool.GreenPool(conf.rpc_thread_pool_size)
        self.connection_pool = connection_pool
        self.conf = conf

    def wait(self):
        """Wait for all callback threads to exit."""
        self.pool.waitall()


class CallbackWrapper(_ThreadPoolWithWait):
    """Wraps a straight callback.

    Allows it to be invoked in a green thread.
    """

    def __init__(self, conf, callback, connection_pool):
        """Initiates CallbackWrapper object.

        :param conf: cfg.CONF instance
        :param callback: a callable (probably a function)
        :param connection_pool: connection pool as returned by
                                get_connection_pool()
        """
        super(CallbackWrapper, self).__init__(
            conf=conf,
            connection_pool=connection_pool,
        )
        self.callback = callback

    def __call__(self, message_data):
        self.pool.spawn_n(self.callback, message_data)


class ProxyCallback(_ThreadPoolWithWait):
    """Calls methods on a proxy object based on method and args."""

    def __init__(self, conf, proxy, connection_pool):
        super(ProxyCallback, self).__init__(
            conf=conf,
            connection_pool=connection_pool,
        )
        self.proxy = proxy
        self.msg_id_cache = _MsgIdCache()

    def __call__(self, message_data):
        """Consumer callback to call a method on a proxy object.

        Parses the message for validity and fires off a thread to call the
        proxy object method.

        Message data should be a dictionary with two keys:
            method: string representing the method to call
            args: dictionary of arg: value

        Example: {'method': 'echo', 'args': {'value': 42}}

        """
        # It is important to clear the context here, because at this point
        # the previous context is stored in local.store.context
        if hasattr(local.store, 'context'):
            del local.store.context
        rpc_common._safe_log(LOG.debug, _('received %s'), message_data)
        self.msg_id_cache.check_duplicate_message(message_data)
        ctxt = unpack_context(self.conf, message_data)
        method = message_data.get('method')
        args = message_data.get('args', {})
        version = message_data.get('version')
        namespace = message_data.get('namespace')
        if not method:
            LOG.warn(_('no method for message: %s') % message_data)
            ctxt.reply(_('No method for message: %s') % message_data,
                       connection_pool=self.connection_pool)
            return
        self.pool.spawn_n(self._process_data, ctxt, version, method,
                          namespace, args)

    def _process_data(self, ctxt, version, method, namespace, args):
        """Process a message in a new thread.

        If the proxy object we have has a dispatch method
        (see rpc.dispatcher.RpcDispatcher), pass it the version,
        method, and args and let it dispatch as appropriate.  If not, use
        the old behavior of magically calling the specified method on the
        proxy we have here.
        """
        ctxt.update_store()
        try:
            rval = self.proxy.dispatch(ctxt, version, method, namespace,
                                       **args)
            # Check if the result was a generator
            if inspect.isgenerator(rval):
                for x in rval:
                    ctxt.reply(x, None, connection_pool=self.connection_pool)
            else:
                ctxt.reply(rval, None, connection_pool=self.connection_pool)
            # This final None tells multicall that it is done.
            ctxt.reply(ending=True, connection_pool=self.connection_pool)
        except rpc_common.ClientException as e:
            LOG.debug(_('Expected exception during message handling (%s)') %
                      e._exc_info[1])
            ctxt.reply(None, e._exc_info,
                       connection_pool=self.connection_pool,
                       log_failure=False)
        except Exception:
            # sys.exc_info() is deleted by LOG.exception().
            exc_info = sys.exc_info()
            LOG.error(_('Exception during message handling'),
                      exc_info=exc_info)
            ctxt.reply(None, exc_info, connection_pool=self.connection_pool)


class MulticallProxyWaiter(object):
    def __init__(self, conf, msg_id, timeout, connection_pool):
        self._msg_id = msg_id
        self._timeout = timeout or conf.rpc_response_timeout
        self._reply_proxy = connection_pool.reply_proxy
        self._done = False
        self._got_ending = False
        self._conf = conf
        self._dataqueue = queue.LightQueue()
        # Add this caller to the reply proxy's call_waiters
        self._reply_proxy.add_call_waiter(self, self._msg_id)
        self.msg_id_cache = _MsgIdCache()

    def put(self, data):
        self._dataqueue.put(data)

    def done(self):
        if self._done:
            return
        self._done = True
        # Remove this caller from reply proxy's call_waiters
        self._reply_proxy.del_call_waiter(self._msg_id)

    def _process_data(self, data):
        result = None
        self.msg_id_cache.check_duplicate_message(data)
        if data['failure']:
            failure = data['failure']
            result = rpc_common.deserialize_remote_exception(self._conf,
                                                             failure)
        elif data.get('ending', False):
            self._got_ending = True
        else:
            result = data['result']
        return result

    def __iter__(self):
        """Return a result until we get a reply with an 'ending' flag."""
        if self._done:
            raise StopIteration
        while True:
            try:
                data = self._dataqueue.get(timeout=self._timeout)
                result = self._process_data(data)
            except queue.Empty:
                self.done()
                raise rpc_common.Timeout()
            except Exception:
                with excutils.save_and_reraise_exception():
                    self.done()
            if self._got_ending:
                self.done()
                raise StopIteration
            if isinstance(result, Exception):
                self.done()
                raise result
            yield result


def create_connection(conf, new, connection_pool):
    """Create a connection."""
    return ConnectionContext(conf, connection_pool, pooled=not new)


_reply_proxy_create_sem = semaphore.Semaphore()


def multicall(conf, context, topic, msg, timeout, connection_pool):
    """Make a call that returns multiple times."""
    LOG.debug(_('Making synchronous call on %s ...'), topic)
    msg_id = uuid.uuid4().hex
    msg.update({'_msg_id': msg_id})
    LOG.debug(_('MSG_ID is %s') % (msg_id))
    _add_unique_id(msg)
    pack_context(msg, context)

    with _reply_proxy_create_sem:
        if not connection_pool.reply_proxy:
            connection_pool.reply_proxy = ReplyProxy(conf, connection_pool)
    msg.update({'_reply_q': connection_pool.reply_proxy.get_reply_q()})
    wait_msg = MulticallProxyWaiter(conf, msg_id, timeout, connection_pool)
    with ConnectionContext(conf, connection_pool) as conn:
        conn.topic_send(topic, rpc_common.serialize_msg(msg), timeout)
    return wait_msg


def call(conf, context, topic, msg, timeout, connection_pool):
    """Sends a message on a topic and wait for a response."""
    rv = multicall(conf, context, topic, msg, timeout, connection_pool)
    # NOTE(vish): return the last result from the multicall
    rv = list(rv)
    if not rv:
        return
    return rv[-1]


def cast(conf, context, topic, msg, connection_pool):
    """Sends a message on a topic without waiting for a response."""
    LOG.debug(_('Making asynchronous cast on %s...'), topic)
    _add_unique_id(msg)
    pack_context(msg, context)
    with ConnectionContext(conf, connection_pool) as conn:
        conn.topic_send(topic, rpc_common.serialize_msg(msg))


def fanout_cast(conf, context, topic, msg, connection_pool):
    """Sends a message on a fanout exchange without waiting for a response."""
    LOG.debug(_('Making asynchronous fanout cast...'))
    _add_unique_id(msg)
    pack_context(msg, context)
    with ConnectionContext(conf, connection_pool) as conn:
        conn.fanout_send(topic, rpc_common.serialize_msg(msg))


def cast_to_server(conf, context, server_params, topic, msg, connection_pool):
    """Sends a message on a topic to a specific server."""
    _add_unique_id(msg)
    pack_context(msg, context)
    with ConnectionContext(conf, connection_pool, pooled=False,
                           server_params=server_params) as conn:
        conn.topic_send(topic, rpc_common.serialize_msg(msg))


def fanout_cast_to_server(conf, context, server_params, topic, msg,
                          connection_pool):
    """Sends a message on a fanout exchange to a specific server."""
    _add_unique_id(msg)
    pack_context(msg, context)
    with ConnectionContext(conf, connection_pool, pooled=False,
                           server_params=server_params) as conn:
        conn.fanout_send(topic, rpc_common.serialize_msg(msg))


def notify(conf, context, topic, msg, connection_pool, envelope):
    """Sends a notification event on a topic."""
    LOG.debug(_('Sending %(event_type)s on %(topic)s'),
              dict(event_type=msg.get('event_type'),
                   topic=topic))
    _add_unique_id(msg)
    pack_context(msg, context)
    with ConnectionContext(conf, connection_pool) as conn:
        if envelope:
            msg = rpc_common.serialize_msg(msg)
        conn.notify_send(topic, msg)


def cleanup(connection_pool):
    if connection_pool:
        connection_pool.empty()


def get_control_exchange(conf):
    return conf.control_exchange

########NEW FILE########
__FILENAME__ = common
# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
# Copyright 2011 Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import copy
import sys
import traceback

from oslo.config import cfg
import six

from trove.openstack.common.gettextutils import _
from trove.openstack.common import importutils
from trove.openstack.common import jsonutils
from trove.openstack.common import local
from trove.openstack.common import log as logging
from trove.openstack.common import versionutils


CONF = cfg.CONF
LOG = logging.getLogger(__name__)


_RPC_ENVELOPE_VERSION = '2.0'
'''RPC Envelope Version.

This version number applies to the top level structure of messages sent out.
It does *not* apply to the message payload, which must be versioned
independently.  For example, when using rpc APIs, a version number is applied
for changes to the API being exposed over rpc.  This version number is handled
in the rpc proxy and dispatcher modules.

This version number applies to the message envelope that is used in the
serialization done inside the rpc layer.  See serialize_msg() and
deserialize_msg().

The current message format (version 2.0) is very simple.  It is::

    {
        'oslo.version': <RPC Envelope Version as a String>,
        'oslo.message': <Application Message Payload, JSON encoded>
    }

Message format version '1.0' is just considered to be the messages we sent
without a message envelope.

So, the current message envelope just includes the envelope version.  It may
eventually contain additional information, such as a signature for the message
payload.

We will JSON encode the application message payload.  The message envelope,
which includes the JSON encoded application message body, will be passed down
to the messaging libraries as a dict.
'''

_VERSION_KEY = 'oslo.version'
_MESSAGE_KEY = 'oslo.message'

_REMOTE_POSTFIX = '_Remote'


class RPCException(Exception):
    msg_fmt = _("An unknown RPC related exception occurred.")

    def __init__(self, message=None, **kwargs):
        self.kwargs = kwargs

        if not message:
            try:
                message = self.msg_fmt % kwargs

            except Exception:
                # kwargs doesn't match a variable in the message
                # log the issue and the kwargs
                LOG.exception(_('Exception in string format operation'))
                for name, value in six.iteritems(kwargs):
                    LOG.error("%s: %s" % (name, value))
                # at least get the core message out if something happened
                message = self.msg_fmt

        super(RPCException, self).__init__(message)


class RemoteError(RPCException):
    """Signifies that a remote class has raised an exception.

    Contains a string representation of the type of the original exception,
    the value of the original exception, and the traceback.  These are
    sent to the parent as a joined string so printing the exception
    contains all of the relevant info.

    """
    msg_fmt = _("Remote error: %(exc_type)s %(value)s\n%(traceback)s.")

    def __init__(self, exc_type=None, value=None, traceback=None):
        self.exc_type = exc_type
        self.value = value
        self.traceback = traceback
        super(RemoteError, self).__init__(exc_type=exc_type,
                                          value=value,
                                          traceback=traceback)


class Timeout(RPCException):
    """Signifies that a timeout has occurred.

    This exception is raised if the rpc_response_timeout is reached while
    waiting for a response from the remote side.
    """
    msg_fmt = _('Timeout while waiting on RPC response - '
                'topic: "%(topic)s", RPC method: "%(method)s" '
                'info: "%(info)s"')

    def __init__(self, info=None, topic=None, method=None):
        """Initiates Timeout object.

        :param info: Extra info to convey to the user
        :param topic: The topic that the rpc call was sent to
        :param rpc_method_name: The name of the rpc method being
                                called
        """
        self.info = info
        self.topic = topic
        self.method = method
        super(Timeout, self).__init__(
            None,
            info=info or _('<unknown>'),
            topic=topic or _('<unknown>'),
            method=method or _('<unknown>'))


class DuplicateMessageError(RPCException):
    msg_fmt = _("Found duplicate message(%(msg_id)s). Skipping it.")


class InvalidRPCConnectionReuse(RPCException):
    msg_fmt = _("Invalid reuse of an RPC connection.")


class UnsupportedRpcVersion(RPCException):
    msg_fmt = _("Specified RPC version, %(version)s, not supported by "
                "this endpoint.")


class UnsupportedRpcEnvelopeVersion(RPCException):
    msg_fmt = _("Specified RPC envelope version, %(version)s, "
                "not supported by this endpoint.")


class RpcVersionCapError(RPCException):
    msg_fmt = _("Specified RPC version cap, %(version_cap)s, is too low")


class Connection(object):
    """A connection, returned by rpc.create_connection().

    This class represents a connection to the message bus used for rpc.
    An instance of this class should never be created by users of the rpc API.
    Use rpc.create_connection() instead.
    """
    def close(self):
        """Close the connection.

        This method must be called when the connection will no longer be used.
        It will ensure that any resources associated with the connection, such
        as a network connection, and cleaned up.
        """
        raise NotImplementedError()

    def create_consumer(self, topic, proxy, fanout=False):
        """Create a consumer on this connection.

        A consumer is associated with a message queue on the backend message
        bus.  The consumer will read messages from the queue, unpack them, and
        dispatch them to the proxy object.  The contents of the message pulled
        off of the queue will determine which method gets called on the proxy
        object.

        :param topic: This is a name associated with what to consume from.
                      Multiple instances of a service may consume from the same
                      topic. For example, all instances of nova-compute consume
                      from a queue called "compute".  In that case, the
                      messages will get distributed amongst the consumers in a
                      round-robin fashion if fanout=False.  If fanout=True,
                      every consumer associated with this topic will get a
                      copy of every message.
        :param proxy: The object that will handle all incoming messages.
        :param fanout: Whether or not this is a fanout topic.  See the
                       documentation for the topic parameter for some
                       additional comments on this.
        """
        raise NotImplementedError()

    def create_worker(self, topic, proxy, pool_name):
        """Create a worker on this connection.

        A worker is like a regular consumer of messages directed to a
        topic, except that it is part of a set of such consumers (the
        "pool") which may run in parallel. Every pool of workers will
        receive a given message, but only one worker in the pool will
        be asked to process it. Load is distributed across the members
        of the pool in round-robin fashion.

        :param topic: This is a name associated with what to consume from.
                      Multiple instances of a service may consume from the same
                      topic.
        :param proxy: The object that will handle all incoming messages.
        :param pool_name: String containing the name of the pool of workers
        """
        raise NotImplementedError()

    def join_consumer_pool(self, callback, pool_name, topic, exchange_name):
        """Register as a member of a group of consumers.

        Uses given topic from the specified exchange.
        Exactly one member of a given pool will receive each message.

        A message will be delivered to multiple pools, if more than
        one is created.

        :param callback: Callable to be invoked for each message.
        :type callback: callable accepting one argument
        :param pool_name: The name of the consumer pool.
        :type pool_name: str
        :param topic: The routing topic for desired messages.
        :type topic: str
        :param exchange_name: The name of the message exchange where
                              the client should attach. Defaults to
                              the configured exchange.
        :type exchange_name: str
        """
        raise NotImplementedError()

    def consume_in_thread(self):
        """Spawn a thread to handle incoming messages.

        Spawn a thread that will be responsible for handling all incoming
        messages for consumers that were set up on this connection.

        Message dispatching inside of this is expected to be implemented in a
        non-blocking manner.  An example implementation would be having this
        thread pull messages in for all of the consumers, but utilize a thread
        pool for dispatching the messages to the proxy objects.
        """
        raise NotImplementedError()


def _safe_log(log_func, msg, msg_data):
    """Sanitizes the msg_data field before logging."""
    SANITIZE = ['_context_auth_token', 'auth_token', 'new_pass']

    def _fix_passwords(d):
        """Sanitizes the password fields in the dictionary."""
        for k in six.iterkeys(d):
            if k.lower().find('password') != -1:
                d[k] = '<SANITIZED>'
            elif k.lower() in SANITIZE:
                d[k] = '<SANITIZED>'
            elif isinstance(d[k], list):
                for e in d[k]:
                    if isinstance(e, dict):
                        _fix_passwords(e)
            elif isinstance(d[k], dict):
                _fix_passwords(d[k])
        return d

    return log_func(msg, _fix_passwords(copy.deepcopy(msg_data)))


def serialize_remote_exception(failure_info, log_failure=True):
    """Prepares exception data to be sent over rpc.

    Failure_info should be a sys.exc_info() tuple.

    """
    tb = traceback.format_exception(*failure_info)
    failure = failure_info[1]
    if log_failure:
        LOG.error(_("Returning exception %s to caller"),
                  six.text_type(failure))
        LOG.error(tb)

    kwargs = {}
    if hasattr(failure, 'kwargs'):
        kwargs = failure.kwargs

    # NOTE(matiu): With cells, it's possible to re-raise remote, remote
    # exceptions. Lets turn it back into the original exception type.
    cls_name = str(failure.__class__.__name__)
    mod_name = str(failure.__class__.__module__)
    if (cls_name.endswith(_REMOTE_POSTFIX) and
            mod_name.endswith(_REMOTE_POSTFIX)):
        cls_name = cls_name[:-len(_REMOTE_POSTFIX)]
        mod_name = mod_name[:-len(_REMOTE_POSTFIX)]

    data = {
        'class': cls_name,
        'module': mod_name,
        'message': six.text_type(failure),
        'tb': tb,
        'args': failure.args,
        'kwargs': kwargs
    }

    json_data = jsonutils.dumps(data)

    return json_data


def deserialize_remote_exception(conf, data):
    failure = jsonutils.loads(str(data))

    trace = failure.get('tb', [])
    message = failure.get('message', "") + "\n" + "\n".join(trace)
    name = failure.get('class')
    module = failure.get('module')

    # NOTE(ameade): We DO NOT want to allow just any module to be imported, in
    # order to prevent arbitrary code execution.
    if module not in conf.allowed_rpc_exception_modules:
        return RemoteError(name, failure.get('message'), trace)

    try:
        mod = importutils.import_module(module)
        klass = getattr(mod, name)
        if not issubclass(klass, Exception):
            raise TypeError("Can only deserialize Exceptions")

        failure = klass(*failure.get('args', []), **failure.get('kwargs', {}))
    except (AttributeError, TypeError, ImportError):
        return RemoteError(name, failure.get('message'), trace)

    ex_type = type(failure)
    str_override = lambda self: message
    new_ex_type = type(ex_type.__name__ + _REMOTE_POSTFIX, (ex_type,),
                       {'__str__': str_override, '__unicode__': str_override})
    new_ex_type.__module__ = '%s%s' % (module, _REMOTE_POSTFIX)
    try:
        # NOTE(ameade): Dynamically create a new exception type and swap it in
        # as the new type for the exception. This only works on user defined
        # Exceptions and not core python exceptions. This is important because
        # we cannot necessarily change an exception message so we must override
        # the __str__ method.
        failure.__class__ = new_ex_type
    except TypeError:
        # NOTE(ameade): If a core exception then just add the traceback to the
        # first exception argument.
        failure.args = (message,) + failure.args[1:]
    return failure


class CommonRpcContext(object):
    def __init__(self, **kwargs):
        self.values = kwargs

    def __getattr__(self, key):
        try:
            return self.values[key]
        except KeyError:
            raise AttributeError(key)

    def to_dict(self):
        return copy.deepcopy(self.values)

    @classmethod
    def from_dict(cls, values):
        return cls(**values)

    def deepcopy(self):
        return self.from_dict(self.to_dict())

    def update_store(self):
        local.store.context = self

    def elevated(self, read_deleted=None, overwrite=False):
        """Return a version of this context with admin flag set."""
        # TODO(russellb) This method is a bit of a nova-ism.  It makes
        # some assumptions about the data in the request context sent
        # across rpc, while the rest of this class does not.  We could get
        # rid of this if we changed the nova code that uses this to
        # convert the RpcContext back to its native RequestContext doing
        # something like nova.context.RequestContext.from_dict(ctxt.to_dict())

        context = self.deepcopy()
        context.values['is_admin'] = True

        context.values.setdefault('roles', [])

        if 'admin' not in context.values['roles']:
            context.values['roles'].append('admin')

        if read_deleted is not None:
            context.values['read_deleted'] = read_deleted

        return context


class ClientException(Exception):
    """Encapsulates actual exception expected to be hit by a RPC proxy object.

    Merely instantiating it records the current exception information, which
    will be passed back to the RPC client without exceptional logging.
    """
    def __init__(self):
        self._exc_info = sys.exc_info()


def catch_client_exception(exceptions, func, *args, **kwargs):
    try:
        return func(*args, **kwargs)
    except Exception as e:
        if type(e) in exceptions:
            raise ClientException()
        else:
            raise


def client_exceptions(*exceptions):
    """Decorator for manager methods that raise expected exceptions.

    Marking a Manager method with this decorator allows the declaration
    of expected exceptions that the RPC layer should not consider fatal,
    and not log as if they were generated in a real error scenario. Note
    that this will cause listed exceptions to be wrapped in a
    ClientException, which is used internally by the RPC layer.
    """
    def outer(func):
        def inner(*args, **kwargs):
            return catch_client_exception(exceptions, func, *args, **kwargs)
        return inner
    return outer


# TODO(sirp): we should deprecate this in favor of
# using `versionutils.is_compatible` directly
def version_is_compatible(imp_version, version):
    """Determine whether versions are compatible.

    :param imp_version: The version implemented
    :param version: The version requested by an incoming message.
    """
    return versionutils.is_compatible(version, imp_version)


def serialize_msg(raw_msg):
    # NOTE(russellb) See the docstring for _RPC_ENVELOPE_VERSION for more
    # information about this format.
    msg = {_VERSION_KEY: _RPC_ENVELOPE_VERSION,
           _MESSAGE_KEY: jsonutils.dumps(raw_msg)}

    return msg


def deserialize_msg(msg):
    # NOTE(russellb): Hang on to your hats, this road is about to
    # get a little bumpy.
    #
    # Robustness Principle:
    #    "Be strict in what you send, liberal in what you accept."
    #
    # At this point we have to do a bit of guessing about what it
    # is we just received.  Here is the set of possibilities:
    #
    # 1) We received a dict.  This could be 2 things:
    #
    #   a) Inspect it to see if it looks like a standard message envelope.
    #      If so, great!
    #
    #   b) If it doesn't look like a standard message envelope, it could either
    #      be a notification, or a message from before we added a message
    #      envelope (referred to as version 1.0).
    #      Just return the message as-is.
    #
    # 2) It's any other non-dict type.  Just return it and hope for the best.
    #    This case covers return values from rpc.call() from before message
    #    envelopes were used.  (messages to call a method were always a dict)

    if not isinstance(msg, dict):
        # See #2 above.
        return msg

    base_envelope_keys = (_VERSION_KEY, _MESSAGE_KEY)
    if not all(map(lambda key: key in msg, base_envelope_keys)):
        #  See #1.b above.
        return msg

    # At this point we think we have the message envelope
    # format we were expecting. (#1.a above)

    if not version_is_compatible(_RPC_ENVELOPE_VERSION, msg[_VERSION_KEY]):
        raise UnsupportedRpcEnvelopeVersion(version=msg[_VERSION_KEY])

    raw_msg = jsonutils.loads(msg[_MESSAGE_KEY])

    return raw_msg

########NEW FILE########
__FILENAME__ = dispatcher
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2012 Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Code for rpc message dispatching.

Messages that come in have a version number associated with them.  RPC API
version numbers are in the form:

    Major.Minor

For a given message with version X.Y, the receiver must be marked as able to
handle messages of version A.B, where:

    A = X

    B >= Y

The Major version number would be incremented for an almost completely new API.
The Minor version number would be incremented for backwards compatible changes
to an existing API.  A backwards compatible change could be something like
adding a new method, adding an argument to an existing method (but not
requiring it), or changing the type for an existing argument (but still
handling the old type as well).

The conversion over to a versioned API must be done on both the client side and
server side of the API at the same time.  However, as the code stands today,
there can be both versioned and unversioned APIs implemented in the same code
base.

EXAMPLES
========

Nova was the first project to use versioned rpc APIs.  Consider the compute rpc
API as an example.  The client side is in nova/compute/rpcapi.py and the server
side is in nova/compute/manager.py.


Example 1) Adding a new method.
-------------------------------

Adding a new method is a backwards compatible change.  It should be added to
nova/compute/manager.py, and RPC_API_VERSION should be bumped from X.Y to
X.Y+1.  On the client side, the new method in nova/compute/rpcapi.py should
have a specific version specified to indicate the minimum API version that must
be implemented for the method to be supported.  For example::

    def get_host_uptime(self, ctxt, host):
        topic = _compute_topic(self.topic, ctxt, host, None)
        return self.call(ctxt, self.make_msg('get_host_uptime'), topic,
                version='1.1')

In this case, version '1.1' is the first version that supported the
get_host_uptime() method.


Example 2) Adding a new parameter.
----------------------------------

Adding a new parameter to an rpc method can be made backwards compatible.  The
RPC_API_VERSION on the server side (nova/compute/manager.py) should be bumped.
The implementation of the method must not expect the parameter to be present.::

    def some_remote_method(self, arg1, arg2, newarg=None):
        # The code needs to deal with newarg=None for cases
        # where an older client sends a message without it.
        pass

On the client side, the same changes should be made as in example 1.  The
minimum version that supports the new parameter should be specified.
"""

from trove.openstack.common.rpc import common as rpc_common
from trove.openstack.common.rpc import serializer as rpc_serializer


class RpcDispatcher(object):
    """Dispatch rpc messages according to the requested API version.

    This class can be used as the top level 'manager' for a service.  It
    contains a list of underlying managers that have an API_VERSION attribute.
    """

    def __init__(self, callbacks, serializer=None):
        """Initialize the rpc dispatcher.

        :param callbacks: List of proxy objects that are an instance
                          of a class with rpc methods exposed.  Each proxy
                          object should have an RPC_API_VERSION attribute.
        :param serializer: The Serializer object that will be used to
                           deserialize arguments before the method call and
                           to serialize the result after it returns.
        """
        self.callbacks = callbacks
        if serializer is None:
            serializer = rpc_serializer.NoOpSerializer()
        self.serializer = serializer
        super(RpcDispatcher, self).__init__()

    def _deserialize_args(self, context, kwargs):
        """Helper method called to deserialize args before dispatch.

        This calls our serializer on each argument, returning a new set of
        args that have been deserialized.

        :param context: The request context
        :param kwargs: The arguments to be deserialized
        :returns: A new set of deserialized args
        """
        new_kwargs = dict()
        for argname, arg in kwargs.iteritems():
            new_kwargs[argname] = self.serializer.deserialize_entity(context,
                                                                     arg)
        return new_kwargs

    def dispatch(self, ctxt, version, method, namespace, **kwargs):
        """Dispatch a message based on a requested version.

        :param ctxt: The request context
        :param version: The requested API version from the incoming message
        :param method: The method requested to be called by the incoming
                       message.
        :param namespace: The namespace for the requested method.  If None,
                          the dispatcher will look for a method on a callback
                          object with no namespace set.
        :param kwargs: A dict of keyword arguments to be passed to the method.

        :returns: Whatever is returned by the underlying method that gets
                  called.
        """
        if not version:
            version = '1.0'

        had_compatible = False
        for proxyobj in self.callbacks:
            # Check for namespace compatibility
            try:
                cb_namespace = proxyobj.RPC_API_NAMESPACE
            except AttributeError:
                cb_namespace = None

            if namespace != cb_namespace:
                continue

            # Check for version compatibility
            try:
                rpc_api_version = proxyobj.RPC_API_VERSION
            except AttributeError:
                rpc_api_version = '1.0'

            is_compatible = rpc_common.version_is_compatible(rpc_api_version,
                                                             version)
            had_compatible = had_compatible or is_compatible

            if not hasattr(proxyobj, method):
                continue
            if is_compatible:
                kwargs = self._deserialize_args(ctxt, kwargs)
                result = getattr(proxyobj, method)(ctxt, **kwargs)
                return self.serializer.serialize_entity(ctxt, result)

        if had_compatible:
            raise AttributeError("No such RPC function '%s'" % method)
        else:
            raise rpc_common.UnsupportedRpcVersion(version=version)

########NEW FILE########
__FILENAME__ = impl_fake
# vim: tabstop=4 shiftwidth=4 softtabstop=4

#    Copyright 2011 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
"""Fake RPC implementation which calls proxy methods directly with no
queues.  Casts will block, but this is very useful for tests.
"""

import inspect
# NOTE(russellb): We specifically want to use json, not our own jsonutils.
# jsonutils has some extra logic to automatically convert objects to primitive
# types so that they can be serialized.  We want to catch all cases where
# non-primitive types make it into this code and treat it as an error.
import json
import time

import eventlet

from trove.openstack.common.rpc import common as rpc_common

CONSUMERS = {}


class RpcContext(rpc_common.CommonRpcContext):
    def __init__(self, **kwargs):
        super(RpcContext, self).__init__(**kwargs)
        self._response = []
        self._done = False

    def deepcopy(self):
        values = self.to_dict()
        new_inst = self.__class__(**values)
        new_inst._response = self._response
        new_inst._done = self._done
        return new_inst

    def reply(self, reply=None, failure=None, ending=False):
        if ending:
            self._done = True
        if not self._done:
            self._response.append((reply, failure))


class Consumer(object):
    def __init__(self, topic, proxy):
        self.topic = topic
        self.proxy = proxy

    def call(self, context, version, method, namespace, args, timeout):
        done = eventlet.event.Event()

        def _inner():
            ctxt = RpcContext.from_dict(context.to_dict())
            try:
                rval = self.proxy.dispatch(context, version, method,
                                           namespace, **args)
                res = []
                # Caller might have called ctxt.reply() manually
                for (reply, failure) in ctxt._response:
                    if failure:
                        raise failure[0], failure[1], failure[2]
                    res.append(reply)
                # if ending not 'sent'...we might have more data to
                # return from the function itself
                if not ctxt._done:
                    if inspect.isgenerator(rval):
                        for val in rval:
                            res.append(val)
                    else:
                        res.append(rval)
                done.send(res)
            except rpc_common.ClientException as e:
                done.send_exception(e._exc_info[1])
            except Exception as e:
                done.send_exception(e)

        thread = eventlet.greenthread.spawn(_inner)

        if timeout:
            start_time = time.time()
            while not done.ready():
                eventlet.greenthread.sleep(1)
                cur_time = time.time()
                if (cur_time - start_time) > timeout:
                    thread.kill()
                    raise rpc_common.Timeout()

        return done.wait()


class Connection(object):
    """Connection object."""

    def __init__(self):
        self.consumers = []

    def create_consumer(self, topic, proxy, fanout=False):
        consumer = Consumer(topic, proxy)
        self.consumers.append(consumer)
        if topic not in CONSUMERS:
            CONSUMERS[topic] = []
        CONSUMERS[topic].append(consumer)

    def close(self):
        for consumer in self.consumers:
            CONSUMERS[consumer.topic].remove(consumer)
        self.consumers = []

    def consume_in_thread(self):
        pass


def create_connection(conf, new=True):
    """Create a connection."""
    return Connection()


def check_serialize(msg):
    """Make sure a message intended for rpc can be serialized."""
    json.dumps(msg)


def multicall(conf, context, topic, msg, timeout=None):
    """Make a call that returns multiple times."""

    check_serialize(msg)

    method = msg.get('method')
    if not method:
        return
    args = msg.get('args', {})
    version = msg.get('version', None)
    namespace = msg.get('namespace', None)

    try:
        consumer = CONSUMERS[topic][0]
    except (KeyError, IndexError):
        raise rpc_common.Timeout("No consumers available")
    else:
        return consumer.call(context, version, method, namespace, args,
                             timeout)


def call(conf, context, topic, msg, timeout=None):
    """Sends a message on a topic and wait for a response."""
    rv = multicall(conf, context, topic, msg, timeout)
    # NOTE(vish): return the last result from the multicall
    rv = list(rv)
    if not rv:
        return
    return rv[-1]


def cast(conf, context, topic, msg):
    check_serialize(msg)
    try:
        call(conf, context, topic, msg)
    except Exception:
        pass


def notify(conf, context, topic, msg, envelope):
    check_serialize(msg)


def cleanup():
    pass


def fanout_cast(conf, context, topic, msg):
    """Cast to all consumers of a topic."""
    check_serialize(msg)
    method = msg.get('method')
    if not method:
        return
    args = msg.get('args', {})
    version = msg.get('version', None)
    namespace = msg.get('namespace', None)

    for consumer in CONSUMERS.get(topic, []):
        try:
            consumer.call(context, version, method, namespace, args, None)
        except Exception:
            pass

########NEW FILE########
__FILENAME__ = impl_kombu
# vim: tabstop=4 shiftwidth=4 softtabstop=4

#    Copyright 2011 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import functools
import itertools
import socket
import ssl
import time
import uuid

import eventlet
import greenlet
import kombu
import kombu.connection
import kombu.entity
import kombu.messaging
from oslo.config import cfg

from trove.openstack.common import excutils
from trove.openstack.common.gettextutils import _  # noqa
from trove.openstack.common import network_utils
from trove.openstack.common.rpc import amqp as rpc_amqp
from trove.openstack.common.rpc import common as rpc_common
from trove.openstack.common import sslutils

kombu_opts = [
    cfg.StrOpt('kombu_ssl_version',
               default='',
               help='SSL version to use (valid only if SSL enabled). '
                    'valid values are TLSv1, SSLv23 and SSLv3. SSLv2 may '
                    'be available on some distributions'
               ),
    cfg.StrOpt('kombu_ssl_keyfile',
               default='',
               help='SSL key file (valid only if SSL enabled)'),
    cfg.StrOpt('kombu_ssl_certfile',
               default='',
               help='SSL cert file (valid only if SSL enabled)'),
    cfg.StrOpt('kombu_ssl_ca_certs',
               default='',
               help=('SSL certification authority file '
                     '(valid only if SSL enabled)')),
    cfg.StrOpt('rabbit_host',
               default='localhost',
               help='The RabbitMQ broker address where a single node is used'),
    cfg.IntOpt('rabbit_port',
               default=5672,
               help='The RabbitMQ broker port where a single node is used'),
    cfg.ListOpt('rabbit_hosts',
                default=['$rabbit_host:$rabbit_port'],
                help='RabbitMQ HA cluster host:port pairs'),
    cfg.BoolOpt('rabbit_use_ssl',
                default=False,
                help='connect over SSL for RabbitMQ'),
    cfg.StrOpt('rabbit_userid',
               default='guest',
               help='the RabbitMQ userid'),
    cfg.StrOpt('rabbit_password',
               default='guest',
               help='the RabbitMQ password',
               secret=True),
    cfg.StrOpt('rabbit_virtual_host',
               default='/',
               help='the RabbitMQ virtual host'),
    cfg.IntOpt('rabbit_retry_interval',
               default=1,
               help='how frequently to retry connecting with RabbitMQ'),
    cfg.IntOpt('rabbit_retry_backoff',
               default=2,
               help='how long to backoff for between retries when connecting '
                    'to RabbitMQ'),
    cfg.IntOpt('rabbit_max_retries',
               default=0,
               help='maximum retries with trying to connect to RabbitMQ '
                    '(the default of 0 implies an infinite retry count)'),
    cfg.BoolOpt('rabbit_ha_queues',
                default=False,
                help='use H/A queues in RabbitMQ (x-ha-policy: all).'
                     'You need to wipe RabbitMQ database when '
                     'changing this option.'),

]

cfg.CONF.register_opts(kombu_opts)

LOG = rpc_common.LOG


def _get_queue_arguments(conf):
    """Construct the arguments for declaring a queue.

    If the rabbit_ha_queues option is set, we declare a mirrored queue
    as described here:

      http://www.rabbitmq.com/ha.html

    Setting x-ha-policy to all means that the queue will be mirrored
    to all nodes in the cluster.
    """
    return {'x-ha-policy': 'all'} if conf.rabbit_ha_queues else {}


class ConsumerBase(object):
    """Consumer base class."""

    def __init__(self, channel, callback, tag, **kwargs):
        """Declare a queue on an amqp channel.

        'channel' is the amqp channel to use
        'callback' is the callback to call when messages are received
        'tag' is a unique ID for the consumer on the channel

        queue name, exchange name, and other kombu options are
        passed in here as a dictionary.
        """
        self.callback = callback
        self.tag = str(tag)
        self.kwargs = kwargs
        self.queue = None
        self.ack_on_error = kwargs.get('ack_on_error', True)
        self.reconnect(channel)

    def reconnect(self, channel):
        """Re-declare the queue after a rabbit reconnect."""
        self.channel = channel
        self.kwargs['channel'] = channel
        self.queue = kombu.entity.Queue(**self.kwargs)
        self.queue.declare()

    def _callback_handler(self, message, callback):
        """Call callback with deserialized message.

        Messages that are processed without exception are ack'ed.

        If the message processing generates an exception, it will be
        ack'ed if ack_on_error=True. Otherwise it will be .requeue()'ed.
        """

        try:
            msg = rpc_common.deserialize_msg(message.payload)
            callback(msg)
        except Exception:
            if self.ack_on_error:
                LOG.exception(_("Failed to process message"
                                " ... skipping it."))
                message.ack()
            else:
                LOG.exception(_("Failed to process message"
                                " ... will requeue."))
                message.requeue()
        else:
            message.ack()

    def consume(self, *args, **kwargs):
        """Actually declare the consumer on the amqp channel.  This will
        start the flow of messages from the queue.  Using the
        Connection.iterconsume() iterator will process the messages,
        calling the appropriate callback.

        If a callback is specified in kwargs, use that.  Otherwise,
        use the callback passed during __init__()

        If kwargs['nowait'] is True, then this call will block until
        a message is read.

        """

        options = {'consumer_tag': self.tag}
        options['nowait'] = kwargs.get('nowait', False)
        callback = kwargs.get('callback', self.callback)
        if not callback:
            raise ValueError("No callback defined")

        def _callback(raw_message):
            message = self.channel.message_to_python(raw_message)
            self._callback_handler(message, callback)

        self.queue.consume(*args, callback=_callback, **options)

    def cancel(self):
        """Cancel the consuming from the queue, if it has started."""
        try:
            self.queue.cancel(self.tag)
        except KeyError as e:
            # NOTE(comstud): Kludge to get around a amqplib bug
            if str(e) != "u'%s'" % self.tag:
                raise
        self.queue = None


class DirectConsumer(ConsumerBase):
    """Queue/consumer class for 'direct'."""

    def __init__(self, conf, channel, msg_id, callback, tag, **kwargs):
        """Init a 'direct' queue.

        'channel' is the amqp channel to use
        'msg_id' is the msg_id to listen on
        'callback' is the callback to call when messages are received
        'tag' is a unique ID for the consumer on the channel

        Other kombu options may be passed
        """
        # Default options
        options = {'durable': False,
                   'queue_arguments': _get_queue_arguments(conf),
                   'auto_delete': True,
                   'exclusive': False}
        options.update(kwargs)
        exchange = kombu.entity.Exchange(name=msg_id,
                                         type='direct',
                                         durable=options['durable'],
                                         auto_delete=options['auto_delete'])
        super(DirectConsumer, self).__init__(channel,
                                             callback,
                                             tag,
                                             name=msg_id,
                                             exchange=exchange,
                                             routing_key=msg_id,
                                             **options)


class TopicConsumer(ConsumerBase):
    """Consumer class for 'topic'."""

    def __init__(self, conf, channel, topic, callback, tag, name=None,
                 exchange_name=None, **kwargs):
        """Init a 'topic' queue.

        :param channel: the amqp channel to use
        :param topic: the topic to listen on
        :paramtype topic: str
        :param callback: the callback to call when messages are received
        :param tag: a unique ID for the consumer on the channel
        :param name: optional queue name, defaults to topic
        :paramtype name: str

        Other kombu options may be passed as keyword arguments
        """
        # Default options
        options = {'durable': conf.amqp_durable_queues,
                   'queue_arguments': _get_queue_arguments(conf),
                   'auto_delete': conf.amqp_auto_delete,
                   'exclusive': False}
        options.update(kwargs)
        exchange_name = exchange_name or rpc_amqp.get_control_exchange(conf)
        exchange = kombu.entity.Exchange(name=exchange_name,
                                         type='topic',
                                         durable=options['durable'],
                                         auto_delete=options['auto_delete'])
        super(TopicConsumer, self).__init__(channel,
                                            callback,
                                            tag,
                                            name=name or topic,
                                            exchange=exchange,
                                            routing_key=topic,
                                            **options)


class FanoutConsumer(ConsumerBase):
    """Consumer class for 'fanout'."""

    def __init__(self, conf, channel, topic, callback, tag, **kwargs):
        """Init a 'fanout' queue.

        'channel' is the amqp channel to use
        'topic' is the topic to listen on
        'callback' is the callback to call when messages are received
        'tag' is a unique ID for the consumer on the channel

        Other kombu options may be passed
        """
        unique = uuid.uuid4().hex
        exchange_name = '%s_fanout' % topic
        queue_name = '%s_fanout_%s' % (topic, unique)

        # Default options
        options = {'durable': False,
                   'queue_arguments': _get_queue_arguments(conf),
                   'auto_delete': True,
                   'exclusive': False}
        options.update(kwargs)
        exchange = kombu.entity.Exchange(name=exchange_name, type='fanout',
                                         durable=options['durable'],
                                         auto_delete=options['auto_delete'])
        super(FanoutConsumer, self).__init__(channel, callback, tag,
                                             name=queue_name,
                                             exchange=exchange,
                                             routing_key=topic,
                                             **options)


class Publisher(object):
    """Base Publisher class."""

    def __init__(self, channel, exchange_name, routing_key, **kwargs):
        """Init the Publisher class with the exchange_name, routing_key,
        and other options
        """
        self.exchange_name = exchange_name
        self.routing_key = routing_key
        self.kwargs = kwargs
        self.reconnect(channel)

    def reconnect(self, channel):
        """Re-establish the Producer after a rabbit reconnection."""
        self.exchange = kombu.entity.Exchange(name=self.exchange_name,
                                              **self.kwargs)
        self.producer = kombu.messaging.Producer(exchange=self.exchange,
                                                 channel=channel,
                                                 routing_key=self.routing_key)

    def send(self, msg, timeout=None):
        """Send a message."""
        if timeout:
            #
            # AMQP TTL is in milliseconds when set in the header.
            #
            self.producer.publish(msg, headers={'ttl': (timeout * 1000)})
        else:
            self.producer.publish(msg)


class DirectPublisher(Publisher):
    """Publisher class for 'direct'."""
    def __init__(self, conf, channel, msg_id, **kwargs):
        """init a 'direct' publisher.

        Kombu options may be passed as keyword args to override defaults
        """

        options = {'durable': False,
                   'auto_delete': True,
                   'exclusive': False}
        options.update(kwargs)
        super(DirectPublisher, self).__init__(channel, msg_id, msg_id,
                                              type='direct', **options)


class TopicPublisher(Publisher):
    """Publisher class for 'topic'."""
    def __init__(self, conf, channel, topic, **kwargs):
        """init a 'topic' publisher.

        Kombu options may be passed as keyword args to override defaults
        """
        options = {'durable': conf.amqp_durable_queues,
                   'auto_delete': conf.amqp_auto_delete,
                   'exclusive': False}
        options.update(kwargs)
        exchange_name = rpc_amqp.get_control_exchange(conf)
        super(TopicPublisher, self).__init__(channel,
                                             exchange_name,
                                             topic,
                                             type='topic',
                                             **options)


class FanoutPublisher(Publisher):
    """Publisher class for 'fanout'."""
    def __init__(self, conf, channel, topic, **kwargs):
        """init a 'fanout' publisher.

        Kombu options may be passed as keyword args to override defaults
        """
        options = {'durable': False,
                   'auto_delete': True,
                   'exclusive': False}
        options.update(kwargs)
        super(FanoutPublisher, self).__init__(channel, '%s_fanout' % topic,
                                              None, type='fanout', **options)


class NotifyPublisher(TopicPublisher):
    """Publisher class for 'notify'."""

    def __init__(self, conf, channel, topic, **kwargs):
        self.durable = kwargs.pop('durable', conf.amqp_durable_queues)
        self.queue_arguments = _get_queue_arguments(conf)
        super(NotifyPublisher, self).__init__(conf, channel, topic, **kwargs)

    def reconnect(self, channel):
        super(NotifyPublisher, self).reconnect(channel)

        # NOTE(jerdfelt): Normally the consumer would create the queue, but
        # we do this to ensure that messages don't get dropped if the
        # consumer is started after we do
        queue = kombu.entity.Queue(channel=channel,
                                   exchange=self.exchange,
                                   durable=self.durable,
                                   name=self.routing_key,
                                   routing_key=self.routing_key,
                                   queue_arguments=self.queue_arguments)
        queue.declare()


class Connection(object):
    """Connection object."""

    pool = None

    def __init__(self, conf, server_params=None):
        self.consumers = []
        self.consumer_thread = None
        self.proxy_callbacks = []
        self.conf = conf
        self.max_retries = self.conf.rabbit_max_retries
        # Try forever?
        if self.max_retries <= 0:
            self.max_retries = None
        self.interval_start = self.conf.rabbit_retry_interval
        self.interval_stepping = self.conf.rabbit_retry_backoff
        # max retry-interval = 30 seconds
        self.interval_max = 30
        self.memory_transport = False

        if server_params is None:
            server_params = {}
        # Keys to translate from server_params to kombu params
        server_params_to_kombu_params = {'username': 'userid'}

        ssl_params = self._fetch_ssl_params()
        params_list = []
        for adr in self.conf.rabbit_hosts:
            hostname, port = network_utils.parse_host_port(
                adr, default_port=self.conf.rabbit_port)

            params = {
                'hostname': hostname,
                'port': port,
                'userid': self.conf.rabbit_userid,
                'password': self.conf.rabbit_password,
                'virtual_host': self.conf.rabbit_virtual_host,
            }

            for sp_key, value in server_params.iteritems():
                p_key = server_params_to_kombu_params.get(sp_key, sp_key)
                params[p_key] = value

            if self.conf.fake_rabbit:
                params['transport'] = 'memory'
            if self.conf.rabbit_use_ssl:
                params['ssl'] = ssl_params

            params_list.append(params)

        self.params_list = params_list

        self.memory_transport = self.conf.fake_rabbit

        self.connection = None
        self.reconnect()

    def _fetch_ssl_params(self):
        """Handles fetching what ssl params should be used for the connection
        (if any).
        """
        ssl_params = dict()

        # http://docs.python.org/library/ssl.html - ssl.wrap_socket
        if self.conf.kombu_ssl_version:
            ssl_params['ssl_version'] = sslutils.validate_ssl_version(
                self.conf.kombu_ssl_version)
        if self.conf.kombu_ssl_keyfile:
            ssl_params['keyfile'] = self.conf.kombu_ssl_keyfile
        if self.conf.kombu_ssl_certfile:
            ssl_params['certfile'] = self.conf.kombu_ssl_certfile
        if self.conf.kombu_ssl_ca_certs:
            ssl_params['ca_certs'] = self.conf.kombu_ssl_ca_certs
            # We might want to allow variations in the
            # future with this?
            ssl_params['cert_reqs'] = ssl.CERT_REQUIRED

        # Return the extended behavior or just have the default behavior
        return ssl_params or True

    def _connect(self, params):
        """Connect to rabbit.  Re-establish any queues that may have
        been declared before if we are reconnecting.  Exceptions should
        be handled by the caller.
        """
        if self.connection:
            LOG.info(_("Reconnecting to AMQP server on "
                     "%(hostname)s:%(port)d") % params)
            try:
                self.connection.release()
            except self.connection_errors:
                pass
            # Setting this in case the next statement fails, though
            # it shouldn't be doing any network operations, yet.
            self.connection = None
        self.connection = kombu.connection.BrokerConnection(**params)
        self.connection_errors = self.connection.connection_errors
        if self.memory_transport:
            # Kludge to speed up tests.
            self.connection.transport.polling_interval = 0.0
        self.consumer_num = itertools.count(1)
        self.connection.connect()
        self.channel = self.connection.channel()
        # work around 'memory' transport bug in 1.1.3
        if self.memory_transport:
            self.channel._new_queue('ae.undeliver')
        for consumer in self.consumers:
            consumer.reconnect(self.channel)
        LOG.info(_('Connected to AMQP server on %(hostname)s:%(port)d') %
                 params)

    def reconnect(self):
        """Handles reconnecting and re-establishing queues.
        Will retry up to self.max_retries number of times.
        self.max_retries = 0 means to retry forever.
        Sleep between tries, starting at self.interval_start
        seconds, backing off self.interval_stepping number of seconds
        each attempt.
        """

        attempt = 0
        while True:
            params = self.params_list[attempt % len(self.params_list)]
            attempt += 1
            try:
                self._connect(params)
                return
            except (IOError, self.connection_errors) as e:
                pass
            except Exception as e:
                # NOTE(comstud): Unfortunately it's possible for amqplib
                # to return an error not covered by its transport
                # connection_errors in the case of a timeout waiting for
                # a protocol response.  (See paste link in LP888621)
                # So, we check all exceptions for 'timeout' in them
                # and try to reconnect in this case.
                if 'timeout' not in str(e):
                    raise

            log_info = {}
            log_info['err_str'] = str(e)
            log_info['max_retries'] = self.max_retries
            log_info.update(params)

            if self.max_retries and attempt == self.max_retries:
                msg = _('Unable to connect to AMQP server on '
                        '%(hostname)s:%(port)d after %(max_retries)d '
                        'tries: %(err_str)s') % log_info
                LOG.error(msg)
                raise rpc_common.RPCException(msg)

            if attempt == 1:
                sleep_time = self.interval_start or 1
            elif attempt > 1:
                sleep_time += self.interval_stepping
            if self.interval_max:
                sleep_time = min(sleep_time, self.interval_max)

            log_info['sleep_time'] = sleep_time
            LOG.error(_('AMQP server on %(hostname)s:%(port)d is '
                        'unreachable: %(err_str)s. Trying again in '
                        '%(sleep_time)d seconds.') % log_info)
            time.sleep(sleep_time)

    def ensure(self, error_callback, method, *args, **kwargs):
        while True:
            try:
                return method(*args, **kwargs)
            except (self.connection_errors, socket.timeout, IOError) as e:
                if error_callback:
                    error_callback(e)
            except Exception as e:
                # NOTE(comstud): Unfortunately it's possible for amqplib
                # to return an error not covered by its transport
                # connection_errors in the case of a timeout waiting for
                # a protocol response.  (See paste link in LP888621)
                # So, we check all exceptions for 'timeout' in them
                # and try to reconnect in this case.
                if 'timeout' not in str(e):
                    raise
                if error_callback:
                    error_callback(e)
            self.reconnect()

    def get_channel(self):
        """Convenience call for bin/clear_rabbit_queues."""
        return self.channel

    def close(self):
        """Close/release this connection."""
        self.cancel_consumer_thread()
        self.wait_on_proxy_callbacks()
        self.connection.release()
        self.connection = None

    def reset(self):
        """Reset a connection so it can be used again."""
        self.cancel_consumer_thread()
        self.wait_on_proxy_callbacks()
        self.channel.close()
        self.channel = self.connection.channel()
        # work around 'memory' transport bug in 1.1.3
        if self.memory_transport:
            self.channel._new_queue('ae.undeliver')
        self.consumers = []

    def declare_consumer(self, consumer_cls, topic, callback):
        """Create a Consumer using the class that was passed in and
        add it to our list of consumers
        """

        def _connect_error(exc):
            log_info = {'topic': topic, 'err_str': str(exc)}
            LOG.error(_("Failed to declare consumer for topic '%(topic)s': "
                      "%(err_str)s") % log_info)

        def _declare_consumer():
            consumer = consumer_cls(self.conf, self.channel, topic, callback,
                                    self.consumer_num.next())
            self.consumers.append(consumer)
            return consumer

        return self.ensure(_connect_error, _declare_consumer)

    def iterconsume(self, limit=None, timeout=None):
        """Return an iterator that will consume from all queues/consumers."""

        info = {'do_consume': True}

        def _error_callback(exc):
            if isinstance(exc, socket.timeout):
                LOG.debug(_('Timed out waiting for RPC response: %s') %
                          str(exc))
                raise rpc_common.Timeout()
            else:
                LOG.exception(_('Failed to consume message from queue: %s') %
                              str(exc))
                info['do_consume'] = True

        def _consume():
            if info['do_consume']:
                queues_head = self.consumers[:-1]  # not fanout.
                queues_tail = self.consumers[-1]  # fanout
                for queue in queues_head:
                    queue.consume(nowait=True)
                queues_tail.consume(nowait=False)
                info['do_consume'] = False
            return self.connection.drain_events(timeout=timeout)

        for iteration in itertools.count(0):
            if limit and iteration >= limit:
                raise StopIteration
            yield self.ensure(_error_callback, _consume)

    def cancel_consumer_thread(self):
        """Cancel a consumer thread."""
        if self.consumer_thread is not None:
            self.consumer_thread.kill()
            try:
                self.consumer_thread.wait()
            except greenlet.GreenletExit:
                pass
            self.consumer_thread = None

    def wait_on_proxy_callbacks(self):
        """Wait for all proxy callback threads to exit."""
        for proxy_cb in self.proxy_callbacks:
            proxy_cb.wait()

    def publisher_send(self, cls, topic, msg, timeout=None, **kwargs):
        """Send to a publisher based on the publisher class."""

        def _error_callback(exc):
            log_info = {'topic': topic, 'err_str': str(exc)}
            LOG.exception(_("Failed to publish message to topic "
                          "'%(topic)s': %(err_str)s") % log_info)

        def _publish():
            publisher = cls(self.conf, self.channel, topic, **kwargs)
            publisher.send(msg, timeout)

        self.ensure(_error_callback, _publish)

    def declare_direct_consumer(self, topic, callback):
        """Create a 'direct' queue.
        In nova's use, this is generally a msg_id queue used for
        responses for call/multicall
        """
        self.declare_consumer(DirectConsumer, topic, callback)

    def declare_topic_consumer(self, topic, callback=None, queue_name=None,
                               exchange_name=None, ack_on_error=True):
        """Create a 'topic' consumer."""
        self.declare_consumer(functools.partial(TopicConsumer,
                                                name=queue_name,
                                                exchange_name=exchange_name,
                                                ack_on_error=ack_on_error,
                                                ),
                              topic, callback)

    def declare_fanout_consumer(self, topic, callback):
        """Create a 'fanout' consumer."""
        self.declare_consumer(FanoutConsumer, topic, callback)

    def direct_send(self, msg_id, msg):
        """Send a 'direct' message."""
        self.publisher_send(DirectPublisher, msg_id, msg)

    def topic_send(self, topic, msg, timeout=None):
        """Send a 'topic' message."""
        self.publisher_send(TopicPublisher, topic, msg, timeout)

    def fanout_send(self, topic, msg):
        """Send a 'fanout' message."""
        self.publisher_send(FanoutPublisher, topic, msg)

    def notify_send(self, topic, msg, **kwargs):
        """Send a notify message on a topic."""
        self.publisher_send(NotifyPublisher, topic, msg, None, **kwargs)

    def consume(self, limit=None):
        """Consume from all queues/consumers."""
        it = self.iterconsume(limit=limit)
        while True:
            try:
                it.next()
            except StopIteration:
                return

    def consume_in_thread(self):
        """Consumer from all queues/consumers in a greenthread."""
        @excutils.forever_retry_uncaught_exceptions
        def _consumer_thread():
            try:
                self.consume()
            except greenlet.GreenletExit:
                return
        if self.consumer_thread is None:
            self.consumer_thread = eventlet.spawn(_consumer_thread)
        return self.consumer_thread

    def create_consumer(self, topic, proxy, fanout=False):
        """Create a consumer that calls a method in a proxy object."""
        proxy_cb = rpc_amqp.ProxyCallback(
            self.conf, proxy,
            rpc_amqp.get_connection_pool(self.conf, Connection))
        self.proxy_callbacks.append(proxy_cb)

        if fanout:
            self.declare_fanout_consumer(topic, proxy_cb)
        else:
            self.declare_topic_consumer(topic, proxy_cb)

    def create_worker(self, topic, proxy, pool_name):
        """Create a worker that calls a method in a proxy object."""
        proxy_cb = rpc_amqp.ProxyCallback(
            self.conf, proxy,
            rpc_amqp.get_connection_pool(self.conf, Connection))
        self.proxy_callbacks.append(proxy_cb)
        self.declare_topic_consumer(topic, proxy_cb, pool_name)

    def join_consumer_pool(self, callback, pool_name, topic,
                           exchange_name=None, ack_on_error=True):
        """Register as a member of a group of consumers for a given topic from
        the specified exchange.

        Exactly one member of a given pool will receive each message.

        A message will be delivered to multiple pools, if more than
        one is created.
        """
        callback_wrapper = rpc_amqp.CallbackWrapper(
            conf=self.conf,
            callback=callback,
            connection_pool=rpc_amqp.get_connection_pool(self.conf,
                                                         Connection),
        )
        self.proxy_callbacks.append(callback_wrapper)
        self.declare_topic_consumer(
            queue_name=pool_name,
            topic=topic,
            exchange_name=exchange_name,
            callback=callback_wrapper,
            ack_on_error=ack_on_error,
        )


def create_connection(conf, new=True):
    """Create a connection."""
    return rpc_amqp.create_connection(
        conf, new,
        rpc_amqp.get_connection_pool(conf, Connection))


def multicall(conf, context, topic, msg, timeout=None):
    """Make a call that returns multiple times."""
    return rpc_amqp.multicall(
        conf, context, topic, msg, timeout,
        rpc_amqp.get_connection_pool(conf, Connection))


def call(conf, context, topic, msg, timeout=None):
    """Sends a message on a topic and wait for a response."""
    return rpc_amqp.call(
        conf, context, topic, msg, timeout,
        rpc_amqp.get_connection_pool(conf, Connection))


def cast(conf, context, topic, msg):
    """Sends a message on a topic without waiting for a response."""
    return rpc_amqp.cast(
        conf, context, topic, msg,
        rpc_amqp.get_connection_pool(conf, Connection))


def fanout_cast(conf, context, topic, msg):
    """Sends a message on a fanout exchange without waiting for a response."""
    return rpc_amqp.fanout_cast(
        conf, context, topic, msg,
        rpc_amqp.get_connection_pool(conf, Connection))


def cast_to_server(conf, context, server_params, topic, msg):
    """Sends a message on a topic to a specific server."""
    return rpc_amqp.cast_to_server(
        conf, context, server_params, topic, msg,
        rpc_amqp.get_connection_pool(conf, Connection))


def fanout_cast_to_server(conf, context, server_params, topic, msg):
    """Sends a message on a fanout exchange to a specific server."""
    return rpc_amqp.fanout_cast_to_server(
        conf, context, server_params, topic, msg,
        rpc_amqp.get_connection_pool(conf, Connection))


def notify(conf, context, topic, msg, envelope):
    """Sends a notification event on a topic."""
    return rpc_amqp.notify(
        conf, context, topic, msg,
        rpc_amqp.get_connection_pool(conf, Connection),
        envelope)


def cleanup():
    return rpc_amqp.cleanup(Connection.pool)

########NEW FILE########
__FILENAME__ = impl_qpid
# vim: tabstop=4 shiftwidth=4 softtabstop=4

#    Copyright 2011 OpenStack Foundation
#    Copyright 2011 - 2012, Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import functools
import itertools
import time
import uuid

import eventlet
import greenlet
from oslo.config import cfg

from trove.openstack.common import excutils
from trove.openstack.common.gettextutils import _  # noqa
from trove.openstack.common import importutils
from trove.openstack.common import jsonutils
from trove.openstack.common import log as logging
from trove.openstack.common.rpc import amqp as rpc_amqp
from trove.openstack.common.rpc import common as rpc_common

qpid_codec = importutils.try_import("qpid.codec010")
qpid_messaging = importutils.try_import("qpid.messaging")
qpid_exceptions = importutils.try_import("qpid.messaging.exceptions")

LOG = logging.getLogger(__name__)

qpid_opts = [
    cfg.StrOpt('qpid_hostname',
               default='localhost',
               help='Qpid broker hostname'),
    cfg.IntOpt('qpid_port',
               default=5672,
               help='Qpid broker port'),
    cfg.ListOpt('qpid_hosts',
                default=['$qpid_hostname:$qpid_port'],
                help='Qpid HA cluster host:port pairs'),
    cfg.StrOpt('qpid_username',
               default='',
               help='Username for qpid connection'),
    cfg.StrOpt('qpid_password',
               default='',
               help='Password for qpid connection',
               secret=True),
    cfg.StrOpt('qpid_sasl_mechanisms',
               default='',
               help='Space separated list of SASL mechanisms to use for auth'),
    cfg.IntOpt('qpid_heartbeat',
               default=60,
               help='Seconds between connection keepalive heartbeats'),
    cfg.StrOpt('qpid_protocol',
               default='tcp',
               help="Transport to use, either 'tcp' or 'ssl'"),
    cfg.BoolOpt('qpid_tcp_nodelay',
                default=True,
                help='Disable Nagle algorithm'),
]

cfg.CONF.register_opts(qpid_opts)

JSON_CONTENT_TYPE = 'application/json; charset=utf8'


class ConsumerBase(object):
    """Consumer base class."""

    def __init__(self, session, callback, node_name, node_opts,
                 link_name, link_opts):
        """Declare a queue on an amqp session.

        'session' is the amqp session to use
        'callback' is the callback to call when messages are received
        'node_name' is the first part of the Qpid address string, before ';'
        'node_opts' will be applied to the "x-declare" section of "node"
                    in the address string.
        'link_name' goes into the "name" field of the "link" in the address
                    string
        'link_opts' will be applied to the "x-declare" section of "link"
                    in the address string.
        """
        self.callback = callback
        self.receiver = None
        self.session = None

        addr_opts = {
            "create": "always",
            "node": {
                "type": "topic",
                "x-declare": {
                    "durable": True,
                    "auto-delete": True,
                },
            },
            "link": {
                "name": link_name,
                "durable": True,
                "x-declare": {
                    "durable": False,
                    "auto-delete": True,
                    "exclusive": False,
                },
            },
        }
        addr_opts["node"]["x-declare"].update(node_opts)
        addr_opts["link"]["x-declare"].update(link_opts)

        self.address = "%s ; %s" % (node_name, jsonutils.dumps(addr_opts))

        self.connect(session)

    def connect(self, session):
        """Declare the reciever on connect."""
        self._declare_receiver(session)

    def reconnect(self, session):
        """Re-declare the receiver after a qpid reconnect."""
        self._declare_receiver(session)

    def _declare_receiver(self, session):
        self.session = session
        self.receiver = session.receiver(self.address)
        self.receiver.capacity = 1

    def _unpack_json_msg(self, msg):
        """Load the JSON data in msg if msg.content_type indicates that it
           is necessary.  Put the loaded data back into msg.content and
           update msg.content_type appropriately.

        A Qpid Message containing a dict will have a content_type of
        'amqp/map', whereas one containing a string that needs to be converted
        back from JSON will have a content_type of JSON_CONTENT_TYPE.

        :param msg: a Qpid Message object
        :returns: None
        """
        if msg.content_type == JSON_CONTENT_TYPE:
            msg.content = jsonutils.loads(msg.content)
            msg.content_type = 'amqp/map'

    def consume(self):
        """Fetch the message and pass it to the callback object."""
        message = self.receiver.fetch()
        try:
            self._unpack_json_msg(message)
            msg = rpc_common.deserialize_msg(message.content)
            self.callback(msg)
        except Exception:
            LOG.exception(_("Failed to process message... skipping it."))
        finally:
            # TODO(sandy): Need support for optional ack_on_error.
            self.session.acknowledge(message)

    def get_receiver(self):
        return self.receiver

    def get_node_name(self):
        return self.address.split(';')[0]


class DirectConsumer(ConsumerBase):
    """Queue/consumer class for 'direct'."""

    def __init__(self, conf, session, msg_id, callback):
        """Init a 'direct' queue.

        'session' is the amqp session to use
        'msg_id' is the msg_id to listen on
        'callback' is the callback to call when messages are received
        """

        super(DirectConsumer, self).__init__(
            session, callback,
            "%s/%s" % (msg_id, msg_id),
            {"type": "direct"},
            msg_id,
            {
                "auto-delete": conf.amqp_auto_delete,
                "exclusive": True,
                "durable": conf.amqp_durable_queues,
            })


class TopicConsumer(ConsumerBase):
    """Consumer class for 'topic'."""

    def __init__(self, conf, session, topic, callback, name=None,
                 exchange_name=None):
        """Init a 'topic' queue.

        :param session: the amqp session to use
        :param topic: is the topic to listen on
        :paramtype topic: str
        :param callback: the callback to call when messages are received
        :param name: optional queue name, defaults to topic
        """

        exchange_name = exchange_name or rpc_amqp.get_control_exchange(conf)
        super(TopicConsumer, self).__init__(
            session, callback,
            "%s/%s" % (exchange_name, topic),
            {}, name or topic,
            {
                "auto-delete": conf.amqp_auto_delete,
                "durable": conf.amqp_durable_queues,
            })


class FanoutConsumer(ConsumerBase):
    """Consumer class for 'fanout'."""

    def __init__(self, conf, session, topic, callback):
        """Init a 'fanout' queue.

        'session' is the amqp session to use
        'topic' is the topic to listen on
        'callback' is the callback to call when messages are received
        """
        self.conf = conf

        super(FanoutConsumer, self).__init__(
            session, callback,
            "%s_fanout" % topic,
            {"durable": False, "type": "fanout"},
            "%s_fanout_%s" % (topic, uuid.uuid4().hex),
            {"exclusive": True})

    def reconnect(self, session):
        topic = self.get_node_name().rpartition('_fanout')[0]
        params = {
            'session': session,
            'topic': topic,
            'callback': self.callback,
        }

        self.__init__(conf=self.conf, **params)

        super(FanoutConsumer, self).reconnect(session)


class Publisher(object):
    """Base Publisher class."""

    def __init__(self, session, node_name, node_opts=None):
        """Init the Publisher class with the exchange_name, routing_key,
        and other options
        """
        self.sender = None
        self.session = session

        addr_opts = {
            "create": "always",
            "node": {
                "type": "topic",
                "x-declare": {
                    "durable": False,
                    # auto-delete isn't implemented for exchanges in qpid,
                    # but put in here anyway
                    "auto-delete": True,
                },
            },
        }
        if node_opts:
            addr_opts["node"]["x-declare"].update(node_opts)

        self.address = "%s ; %s" % (node_name, jsonutils.dumps(addr_opts))

        self.reconnect(session)

    def reconnect(self, session):
        """Re-establish the Sender after a reconnection."""
        self.sender = session.sender(self.address)

    def _pack_json_msg(self, msg):
        """Qpid cannot serialize dicts containing strings longer than 65535
           characters.  This function dumps the message content to a JSON
           string, which Qpid is able to handle.

        :param msg: May be either a Qpid Message object or a bare dict.
        :returns: A Qpid Message with its content field JSON encoded.
        """
        try:
            msg.content = jsonutils.dumps(msg.content)
        except AttributeError:
            # Need to have a Qpid message so we can set the content_type.
            msg = qpid_messaging.Message(jsonutils.dumps(msg))
        msg.content_type = JSON_CONTENT_TYPE
        return msg

    def send(self, msg):
        """Send a message."""
        try:
            # Check if Qpid can encode the message
            check_msg = msg
            if not hasattr(check_msg, 'content_type'):
                check_msg = qpid_messaging.Message(msg)
            content_type = check_msg.content_type
            enc, dec = qpid_messaging.message.get_codec(content_type)
            enc(check_msg.content)
        except qpid_codec.CodecException:
            # This means the message couldn't be serialized as a dict.
            msg = self._pack_json_msg(msg)
        self.sender.send(msg)


class DirectPublisher(Publisher):
    """Publisher class for 'direct'."""
    def __init__(self, conf, session, msg_id):
        """Init a 'direct' publisher."""
        super(DirectPublisher, self).__init__(session, msg_id,
                                              {"type": "direct"})


class TopicPublisher(Publisher):
    """Publisher class for 'topic'."""
    def __init__(self, conf, session, topic):
        """init a 'topic' publisher.
        """
        exchange_name = rpc_amqp.get_control_exchange(conf)
        super(TopicPublisher, self).__init__(session,
                                             "%s/%s" % (exchange_name, topic))


class FanoutPublisher(Publisher):
    """Publisher class for 'fanout'."""
    def __init__(self, conf, session, topic):
        """init a 'fanout' publisher.
        """
        super(FanoutPublisher, self).__init__(
            session,
            "%s_fanout" % topic, {"type": "fanout"})


class NotifyPublisher(Publisher):
    """Publisher class for notifications."""
    def __init__(self, conf, session, topic):
        """init a 'topic' publisher.
        """
        exchange_name = rpc_amqp.get_control_exchange(conf)
        super(NotifyPublisher, self).__init__(session,
                                              "%s/%s" % (exchange_name, topic),
                                              {"durable": True})


class Connection(object):
    """Connection object."""

    pool = None

    def __init__(self, conf, server_params=None):
        if not qpid_messaging:
            raise ImportError("Failed to import qpid.messaging")

        self.session = None
        self.consumers = {}
        self.consumer_thread = None
        self.proxy_callbacks = []
        self.conf = conf

        if server_params and 'hostname' in server_params:
            # NOTE(russellb) This enables support for cast_to_server.
            server_params['qpid_hosts'] = [
                '%s:%d' % (server_params['hostname'],
                           server_params.get('port', 5672))
            ]

        params = {
            'qpid_hosts': self.conf.qpid_hosts,
            'username': self.conf.qpid_username,
            'password': self.conf.qpid_password,
        }
        params.update(server_params or {})

        self.brokers = params['qpid_hosts']
        self.username = params['username']
        self.password = params['password']
        self.connection_create(self.brokers[0])
        self.reconnect()

    def connection_create(self, broker):
        # Create the connection - this does not open the connection
        self.connection = qpid_messaging.Connection(broker)

        # Check if flags are set and if so set them for the connection
        # before we call open
        self.connection.username = self.username
        self.connection.password = self.password

        self.connection.sasl_mechanisms = self.conf.qpid_sasl_mechanisms
        # Reconnection is done by self.reconnect()
        self.connection.reconnect = False
        self.connection.heartbeat = self.conf.qpid_heartbeat
        self.connection.transport = self.conf.qpid_protocol
        self.connection.tcp_nodelay = self.conf.qpid_tcp_nodelay

    def _register_consumer(self, consumer):
        self.consumers[str(consumer.get_receiver())] = consumer

    def _lookup_consumer(self, receiver):
        return self.consumers[str(receiver)]

    def reconnect(self):
        """Handles reconnecting and re-establishing sessions and queues."""
        attempt = 0
        delay = 1
        while True:
            # Close the session if necessary
            if self.connection.opened():
                try:
                    self.connection.close()
                except qpid_exceptions.ConnectionError:
                    pass

            broker = self.brokers[attempt % len(self.brokers)]
            attempt += 1

            try:
                self.connection_create(broker)
                self.connection.open()
            except qpid_exceptions.ConnectionError as e:
                msg_dict = dict(e=e, delay=delay)
                msg = _("Unable to connect to AMQP server: %(e)s. "
                        "Sleeping %(delay)s seconds") % msg_dict
                LOG.error(msg)
                time.sleep(delay)
                delay = min(2 * delay, 60)
            else:
                LOG.info(_('Connected to AMQP server on %s'), broker)
                break

        self.session = self.connection.session()

        if self.consumers:
            consumers = self.consumers
            self.consumers = {}

            for consumer in consumers.itervalues():
                consumer.reconnect(self.session)
                self._register_consumer(consumer)

            LOG.debug(_("Re-established AMQP queues"))

    def ensure(self, error_callback, method, *args, **kwargs):
        while True:
            try:
                return method(*args, **kwargs)
            except (qpid_exceptions.Empty,
                    qpid_exceptions.ConnectionError) as e:
                if error_callback:
                    error_callback(e)
                self.reconnect()

    def close(self):
        """Close/release this connection."""
        self.cancel_consumer_thread()
        self.wait_on_proxy_callbacks()
        try:
            self.connection.close()
        except Exception:
            # NOTE(dripton) Logging exceptions that happen during cleanup just
            # causes confusion; there's really nothing useful we can do with
            # them.
            pass
        self.connection = None

    def reset(self):
        """Reset a connection so it can be used again."""
        self.cancel_consumer_thread()
        self.wait_on_proxy_callbacks()
        self.session.close()
        self.session = self.connection.session()
        self.consumers = {}

    def declare_consumer(self, consumer_cls, topic, callback):
        """Create a Consumer using the class that was passed in and
        add it to our list of consumers
        """
        def _connect_error(exc):
            log_info = {'topic': topic, 'err_str': str(exc)}
            LOG.error(_("Failed to declare consumer for topic '%(topic)s': "
                      "%(err_str)s") % log_info)

        def _declare_consumer():
            consumer = consumer_cls(self.conf, self.session, topic, callback)
            self._register_consumer(consumer)
            return consumer

        return self.ensure(_connect_error, _declare_consumer)

    def iterconsume(self, limit=None, timeout=None):
        """Return an iterator that will consume from all queues/consumers."""

        def _error_callback(exc):
            if isinstance(exc, qpid_exceptions.Empty):
                LOG.debug(_('Timed out waiting for RPC response: %s') %
                          str(exc))
                raise rpc_common.Timeout()
            else:
                LOG.exception(_('Failed to consume message from queue: %s') %
                              str(exc))

        def _consume():
            nxt_receiver = self.session.next_receiver(timeout=timeout)
            try:
                self._lookup_consumer(nxt_receiver).consume()
            except Exception:
                LOG.exception(_("Error processing message.  Skipping it."))

        for iteration in itertools.count(0):
            if limit and iteration >= limit:
                raise StopIteration
            yield self.ensure(_error_callback, _consume)

    def cancel_consumer_thread(self):
        """Cancel a consumer thread."""
        if self.consumer_thread is not None:
            self.consumer_thread.kill()
            try:
                self.consumer_thread.wait()
            except greenlet.GreenletExit:
                pass
            self.consumer_thread = None

    def wait_on_proxy_callbacks(self):
        """Wait for all proxy callback threads to exit."""
        for proxy_cb in self.proxy_callbacks:
            proxy_cb.wait()

    def publisher_send(self, cls, topic, msg):
        """Send to a publisher based on the publisher class."""

        def _connect_error(exc):
            log_info = {'topic': topic, 'err_str': str(exc)}
            LOG.exception(_("Failed to publish message to topic "
                          "'%(topic)s': %(err_str)s") % log_info)

        def _publisher_send():
            publisher = cls(self.conf, self.session, topic)
            publisher.send(msg)

        return self.ensure(_connect_error, _publisher_send)

    def declare_direct_consumer(self, topic, callback):
        """Create a 'direct' queue.
        In nova's use, this is generally a msg_id queue used for
        responses for call/multicall
        """
        self.declare_consumer(DirectConsumer, topic, callback)

    def declare_topic_consumer(self, topic, callback=None, queue_name=None,
                               exchange_name=None):
        """Create a 'topic' consumer."""
        self.declare_consumer(functools.partial(TopicConsumer,
                                                name=queue_name,
                                                exchange_name=exchange_name,
                                                ),
                              topic, callback)

    def declare_fanout_consumer(self, topic, callback):
        """Create a 'fanout' consumer."""
        self.declare_consumer(FanoutConsumer, topic, callback)

    def direct_send(self, msg_id, msg):
        """Send a 'direct' message."""
        self.publisher_send(DirectPublisher, msg_id, msg)

    def topic_send(self, topic, msg, timeout=None):
        """Send a 'topic' message."""
        #
        # We want to create a message with attributes, e.g. a TTL. We
        # don't really need to keep 'msg' in its JSON format any longer
        # so let's create an actual qpid message here and get some
        # value-add on the go.
        #
        # WARNING: Request timeout happens to be in the same units as
        # qpid's TTL (seconds). If this changes in the future, then this
        # will need to be altered accordingly.
        #
        qpid_message = qpid_messaging.Message(content=msg, ttl=timeout)
        self.publisher_send(TopicPublisher, topic, qpid_message)

    def fanout_send(self, topic, msg):
        """Send a 'fanout' message."""
        self.publisher_send(FanoutPublisher, topic, msg)

    def notify_send(self, topic, msg, **kwargs):
        """Send a notify message on a topic."""
        self.publisher_send(NotifyPublisher, topic, msg)

    def consume(self, limit=None):
        """Consume from all queues/consumers."""
        it = self.iterconsume(limit=limit)
        while True:
            try:
                it.next()
            except StopIteration:
                return

    def consume_in_thread(self):
        """Consumer from all queues/consumers in a greenthread."""
        @excutils.forever_retry_uncaught_exceptions
        def _consumer_thread():
            try:
                self.consume()
            except greenlet.GreenletExit:
                return
        if self.consumer_thread is None:
            self.consumer_thread = eventlet.spawn(_consumer_thread)
        return self.consumer_thread

    def create_consumer(self, topic, proxy, fanout=False):
        """Create a consumer that calls a method in a proxy object."""
        proxy_cb = rpc_amqp.ProxyCallback(
            self.conf, proxy,
            rpc_amqp.get_connection_pool(self.conf, Connection))
        self.proxy_callbacks.append(proxy_cb)

        if fanout:
            consumer = FanoutConsumer(self.conf, self.session, topic, proxy_cb)
        else:
            consumer = TopicConsumer(self.conf, self.session, topic, proxy_cb)

        self._register_consumer(consumer)

        return consumer

    def create_worker(self, topic, proxy, pool_name):
        """Create a worker that calls a method in a proxy object."""
        proxy_cb = rpc_amqp.ProxyCallback(
            self.conf, proxy,
            rpc_amqp.get_connection_pool(self.conf, Connection))
        self.proxy_callbacks.append(proxy_cb)

        consumer = TopicConsumer(self.conf, self.session, topic, proxy_cb,
                                 name=pool_name)

        self._register_consumer(consumer)

        return consumer

    def join_consumer_pool(self, callback, pool_name, topic,
                           exchange_name=None, ack_on_error=True):
        """Register as a member of a group of consumers for a given topic from
        the specified exchange.

        Exactly one member of a given pool will receive each message.

        A message will be delivered to multiple pools, if more than
        one is created.
        """
        callback_wrapper = rpc_amqp.CallbackWrapper(
            conf=self.conf,
            callback=callback,
            connection_pool=rpc_amqp.get_connection_pool(self.conf,
                                                         Connection),
        )
        self.proxy_callbacks.append(callback_wrapper)

        consumer = TopicConsumer(conf=self.conf,
                                 session=self.session,
                                 topic=topic,
                                 callback=callback_wrapper,
                                 name=pool_name,
                                 exchange_name=exchange_name)

        self._register_consumer(consumer)
        return consumer


def create_connection(conf, new=True):
    """Create a connection."""
    return rpc_amqp.create_connection(
        conf, new,
        rpc_amqp.get_connection_pool(conf, Connection))


def multicall(conf, context, topic, msg, timeout=None):
    """Make a call that returns multiple times."""
    return rpc_amqp.multicall(
        conf, context, topic, msg, timeout,
        rpc_amqp.get_connection_pool(conf, Connection))


def call(conf, context, topic, msg, timeout=None):
    """Sends a message on a topic and wait for a response."""
    return rpc_amqp.call(
        conf, context, topic, msg, timeout,
        rpc_amqp.get_connection_pool(conf, Connection))


def cast(conf, context, topic, msg):
    """Sends a message on a topic without waiting for a response."""
    return rpc_amqp.cast(
        conf, context, topic, msg,
        rpc_amqp.get_connection_pool(conf, Connection))


def fanout_cast(conf, context, topic, msg):
    """Sends a message on a fanout exchange without waiting for a response."""
    return rpc_amqp.fanout_cast(
        conf, context, topic, msg,
        rpc_amqp.get_connection_pool(conf, Connection))


def cast_to_server(conf, context, server_params, topic, msg):
    """Sends a message on a topic to a specific server."""
    return rpc_amqp.cast_to_server(
        conf, context, server_params, topic, msg,
        rpc_amqp.get_connection_pool(conf, Connection))


def fanout_cast_to_server(conf, context, server_params, topic, msg):
    """Sends a message on a fanout exchange to a specific server."""
    return rpc_amqp.fanout_cast_to_server(
        conf, context, server_params, topic, msg,
        rpc_amqp.get_connection_pool(conf, Connection))


def notify(conf, context, topic, msg, envelope):
    """Sends a notification event on a topic."""
    return rpc_amqp.notify(conf, context, topic, msg,
                           rpc_amqp.get_connection_pool(conf, Connection),
                           envelope)


def cleanup():
    return rpc_amqp.cleanup(Connection.pool)

########NEW FILE########
__FILENAME__ = impl_zmq
# vim: tabstop=4 shiftwidth=4 softtabstop=4

#    Copyright 2011 Cloudscaling Group, Inc
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import os
import pprint
import re
import socket
import sys
import types
import uuid

import eventlet
import greenlet
from oslo.config import cfg

from trove.openstack.common import excutils
from trove.openstack.common.gettextutils import _  # noqa
from trove.openstack.common import importutils
from trove.openstack.common import jsonutils
from trove.openstack.common.rpc import common as rpc_common

zmq = importutils.try_import('eventlet.green.zmq')

# for convenience, are not modified.
pformat = pprint.pformat
Timeout = eventlet.timeout.Timeout
LOG = rpc_common.LOG
RemoteError = rpc_common.RemoteError
RPCException = rpc_common.RPCException

zmq_opts = [
    cfg.StrOpt('rpc_zmq_bind_address', default='*',
               help='ZeroMQ bind address. Should be a wildcard (*), '
                    'an ethernet interface, or IP. '
                    'The "host" option should point or resolve to this '
                    'address.'),

    # The module.Class to use for matchmaking.
    cfg.StrOpt(
        'rpc_zmq_matchmaker',
        default=('trove.openstack.common.rpc.'
                 'matchmaker.MatchMakerLocalhost'),
        help='MatchMaker driver',
    ),

    # The following port is unassigned by IANA as of 2012-05-21
    cfg.IntOpt('rpc_zmq_port', default=9501,
               help='ZeroMQ receiver listening port'),

    cfg.IntOpt('rpc_zmq_contexts', default=1,
               help='Number of ZeroMQ contexts, defaults to 1'),

    cfg.IntOpt('rpc_zmq_topic_backlog', default=None,
               help='Maximum number of ingress messages to locally buffer '
                    'per topic. Default is unlimited.'),

    cfg.StrOpt('rpc_zmq_ipc_dir', default='/var/run/openstack',
               help='Directory for holding IPC sockets'),

    cfg.StrOpt('rpc_zmq_host', default=socket.gethostname(),
               help='Name of this node. Must be a valid hostname, FQDN, or '
                    'IP address. Must match "host" option, if running Nova.')
]


CONF = cfg.CONF
CONF.register_opts(zmq_opts)

ZMQ_CTX = None  # ZeroMQ Context, must be global.
matchmaker = None  # memoized matchmaker object


def _serialize(data):
    """Serialization wrapper.

    We prefer using JSON, but it cannot encode all types.
    Error if a developer passes us bad data.
    """
    try:
        return jsonutils.dumps(data, ensure_ascii=True)
    except TypeError:
        with excutils.save_and_reraise_exception():
            LOG.error(_("JSON serialization failed."))


def _deserialize(data):
    """Deserialization wrapper."""
    LOG.debug(_("Deserializing: %s"), data)
    return jsonutils.loads(data)


class ZmqSocket(object):
    """A tiny wrapper around ZeroMQ.

    Simplifies the send/recv protocol and connection management.
    Can be used as a Context (supports the 'with' statement).
    """

    def __init__(self, addr, zmq_type, bind=True, subscribe=None):
        self.sock = _get_ctxt().socket(zmq_type)
        self.addr = addr
        self.type = zmq_type
        self.subscriptions = []

        # Support failures on sending/receiving on wrong socket type.
        self.can_recv = zmq_type in (zmq.PULL, zmq.SUB)
        self.can_send = zmq_type in (zmq.PUSH, zmq.PUB)
        self.can_sub = zmq_type in (zmq.SUB, )

        # Support list, str, & None for subscribe arg (cast to list)
        do_sub = {
            list: subscribe,
            str: [subscribe],
            type(None): []
        }[type(subscribe)]

        for f in do_sub:
            self.subscribe(f)

        str_data = {'addr': addr, 'type': self.socket_s(),
                    'subscribe': subscribe, 'bind': bind}

        LOG.debug(_("Connecting to %(addr)s with %(type)s"), str_data)
        LOG.debug(_("-> Subscribed to %(subscribe)s"), str_data)
        LOG.debug(_("-> bind: %(bind)s"), str_data)

        try:
            if bind:
                self.sock.bind(addr)
            else:
                self.sock.connect(addr)
        except Exception:
            raise RPCException(_("Could not open socket."))

    def socket_s(self):
        """Get socket type as string."""
        t_enum = ('PUSH', 'PULL', 'PUB', 'SUB', 'REP', 'REQ', 'ROUTER',
                  'DEALER')
        return dict(map(lambda t: (getattr(zmq, t), t), t_enum))[self.type]

    def subscribe(self, msg_filter):
        """Subscribe."""
        if not self.can_sub:
            raise RPCException("Cannot subscribe on this socket.")
        LOG.debug(_("Subscribing to %s"), msg_filter)

        try:
            self.sock.setsockopt(zmq.SUBSCRIBE, msg_filter)
        except Exception:
            return

        self.subscriptions.append(msg_filter)

    def unsubscribe(self, msg_filter):
        """Unsubscribe."""
        if msg_filter not in self.subscriptions:
            return
        self.sock.setsockopt(zmq.UNSUBSCRIBE, msg_filter)
        self.subscriptions.remove(msg_filter)

    def close(self):
        if self.sock is None or self.sock.closed:
            return

        # We must unsubscribe, or we'll leak descriptors.
        if self.subscriptions:
            for f in self.subscriptions:
                try:
                    self.sock.setsockopt(zmq.UNSUBSCRIBE, f)
                except Exception:
                    pass
            self.subscriptions = []

        try:
            # Default is to linger
            self.sock.close()
        except Exception:
            # While this is a bad thing to happen,
            # it would be much worse if some of the code calling this
            # were to fail. For now, lets log, and later evaluate
            # if we can safely raise here.
            LOG.error("ZeroMQ socket could not be closed.")
        self.sock = None

    def recv(self, **kwargs):
        if not self.can_recv:
            raise RPCException(_("You cannot recv on this socket."))
        return self.sock.recv_multipart(**kwargs)

    def send(self, data, **kwargs):
        if not self.can_send:
            raise RPCException(_("You cannot send on this socket."))
        self.sock.send_multipart(data, **kwargs)


class ZmqClient(object):
    """Client for ZMQ sockets."""

    def __init__(self, addr):
        self.outq = ZmqSocket(addr, zmq.PUSH, bind=False)

    def cast(self, msg_id, topic, data, envelope):
        msg_id = msg_id or 0

        if not envelope:
            self.outq.send(map(bytes,
                           (msg_id, topic, 'cast', _serialize(data))))
            return

        rpc_envelope = rpc_common.serialize_msg(data[1], envelope)
        zmq_msg = reduce(lambda x, y: x + y, rpc_envelope.items())
        self.outq.send(map(bytes,
                       (msg_id, topic, 'impl_zmq_v2', data[0]) + zmq_msg))

    def close(self):
        self.outq.close()


class RpcContext(rpc_common.CommonRpcContext):
    """Context that supports replying to a rpc.call."""
    def __init__(self, **kwargs):
        self.replies = []
        super(RpcContext, self).__init__(**kwargs)

    def deepcopy(self):
        values = self.to_dict()
        values['replies'] = self.replies
        return self.__class__(**values)

    def reply(self, reply=None, failure=None, ending=False):
        if ending:
            return
        self.replies.append(reply)

    @classmethod
    def marshal(self, ctx):
        ctx_data = ctx.to_dict()
        return _serialize(ctx_data)

    @classmethod
    def unmarshal(self, data):
        return RpcContext.from_dict(_deserialize(data))


class InternalContext(object):
    """Used by ConsumerBase as a private context for - methods."""

    def __init__(self, proxy):
        self.proxy = proxy
        self.msg_waiter = None

    def _get_response(self, ctx, proxy, topic, data):
        """Process a curried message and cast the result to topic."""
        LOG.debug(_("Running func with context: %s"), ctx.to_dict())
        data.setdefault('version', None)
        data.setdefault('args', {})

        try:
            result = proxy.dispatch(
                ctx, data['version'], data['method'],
                data.get('namespace'), **data['args'])
            return ConsumerBase.normalize_reply(result, ctx.replies)
        except greenlet.GreenletExit:
            # ignore these since they are just from shutdowns
            pass
        except rpc_common.ClientException as e:
            LOG.debug(_("Expected exception during message handling (%s)") %
                      e._exc_info[1])
            return {'exc':
                    rpc_common.serialize_remote_exception(e._exc_info,
                                                          log_failure=False)}
        except Exception:
            LOG.error(_("Exception during message handling"))
            return {'exc':
                    rpc_common.serialize_remote_exception(sys.exc_info())}

    def reply(self, ctx, proxy,
              msg_id=None, context=None, topic=None, msg=None):
        """Reply to a casted call."""
        # NOTE(ewindisch): context kwarg exists for Grizzly compat.
        #                  this may be able to be removed earlier than
        #                  'I' if ConsumerBase.process were refactored.
        if type(msg) is list:
            payload = msg[-1]
        else:
            payload = msg

        response = ConsumerBase.normalize_reply(
            self._get_response(ctx, proxy, topic, payload),
            ctx.replies)

        LOG.debug(_("Sending reply"))
        _multi_send(_cast, ctx, topic, {
            'method': '-process_reply',
            'args': {
                'msg_id': msg_id,  # Include for Folsom compat.
                'response': response
            }
        }, _msg_id=msg_id)


class ConsumerBase(object):
    """Base Consumer."""

    def __init__(self):
        self.private_ctx = InternalContext(None)

    @classmethod
    def normalize_reply(self, result, replies):
        #TODO(ewindisch): re-evaluate and document this method.
        if isinstance(result, types.GeneratorType):
            return list(result)
        elif replies:
            return replies
        else:
            return [result]

    def process(self, proxy, ctx, data):
        data.setdefault('version', None)
        data.setdefault('args', {})

        # Method starting with - are
        # processed internally. (non-valid method name)
        method = data.get('method')
        if not method:
            LOG.error(_("RPC message did not include method."))
            return

        # Internal method
        # uses internal context for safety.
        if method == '-reply':
            self.private_ctx.reply(ctx, proxy, **data['args'])
            return

        proxy.dispatch(ctx, data['version'],
                       data['method'], data.get('namespace'), **data['args'])


class ZmqBaseReactor(ConsumerBase):
    """A consumer class implementing a centralized casting broker (PULL-PUSH).

    Used for RoundRobin requests.
    """

    def __init__(self, conf):
        super(ZmqBaseReactor, self).__init__()

        self.proxies = {}
        self.threads = []
        self.sockets = []
        self.subscribe = {}

        self.pool = eventlet.greenpool.GreenPool(conf.rpc_thread_pool_size)

    def register(self, proxy, in_addr, zmq_type_in,
                 in_bind=True, subscribe=None):

        LOG.info(_("Registering reactor"))

        if zmq_type_in not in (zmq.PULL, zmq.SUB):
            raise RPCException("Bad input socktype")

        # Items push in.
        inq = ZmqSocket(in_addr, zmq_type_in, bind=in_bind,
                        subscribe=subscribe)

        self.proxies[inq] = proxy
        self.sockets.append(inq)

        LOG.info(_("In reactor registered"))

    def consume_in_thread(self):
        @excutils.forever_retry_uncaught_exceptions
        def _consume(sock):
            LOG.info(_("Consuming socket"))
            while True:
                self.consume(sock)

        for k in self.proxies.keys():
            self.threads.append(
                self.pool.spawn(_consume, k)
            )

    def wait(self):
        for t in self.threads:
            t.wait()

    def close(self):
        for s in self.sockets:
            s.close()

        for t in self.threads:
            t.kill()


class ZmqProxy(ZmqBaseReactor):
    """A consumer class implementing a topic-based proxy.

    Forwards to IPC sockets.
    """

    def __init__(self, conf):
        super(ZmqProxy, self).__init__(conf)
        pathsep = set((os.path.sep or '', os.path.altsep or '', '/', '\\'))
        self.badchars = re.compile(r'[%s]' % re.escape(''.join(pathsep)))

        self.topic_proxy = {}

    def consume(self, sock):
        ipc_dir = CONF.rpc_zmq_ipc_dir

        data = sock.recv(copy=False)
        topic = data[1].bytes

        if topic.startswith('fanout~'):
            sock_type = zmq.PUB
            topic = topic.split('.', 1)[0]
        elif topic.startswith('zmq_replies'):
            sock_type = zmq.PUB
        else:
            sock_type = zmq.PUSH

        if topic not in self.topic_proxy:
            def publisher(waiter):
                LOG.info(_("Creating proxy for topic: %s"), topic)

                try:
                    # The topic is received over the network,
                    # don't trust this input.
                    if self.badchars.search(topic) is not None:
                        emsg = _("Topic contained dangerous characters.")
                        LOG.warn(emsg)
                        raise RPCException(emsg)

                    out_sock = ZmqSocket("ipc://%s/zmq_topic_%s" %
                                         (ipc_dir, topic),
                                         sock_type, bind=True)
                except RPCException:
                    waiter.send_exception(*sys.exc_info())
                    return

                self.topic_proxy[topic] = eventlet.queue.LightQueue(
                    CONF.rpc_zmq_topic_backlog)
                self.sockets.append(out_sock)

                # It takes some time for a pub socket to open,
                # before we can have any faith in doing a send() to it.
                if sock_type == zmq.PUB:
                    eventlet.sleep(.5)

                waiter.send(True)

                while(True):
                    data = self.topic_proxy[topic].get()
                    out_sock.send(data, copy=False)

            wait_sock_creation = eventlet.event.Event()
            eventlet.spawn(publisher, wait_sock_creation)

            try:
                wait_sock_creation.wait()
            except RPCException:
                LOG.error(_("Topic socket file creation failed."))
                return

        try:
            self.topic_proxy[topic].put_nowait(data)
        except eventlet.queue.Full:
            LOG.error(_("Local per-topic backlog buffer full for topic "
                        "%(topic)s. Dropping message.") % {'topic': topic})

    def consume_in_thread(self):
        """Runs the ZmqProxy service."""
        ipc_dir = CONF.rpc_zmq_ipc_dir
        consume_in = "tcp://%s:%s" % \
            (CONF.rpc_zmq_bind_address,
             CONF.rpc_zmq_port)
        consumption_proxy = InternalContext(None)

        try:
            os.makedirs(ipc_dir)
        except os.error:
            if not os.path.isdir(ipc_dir):
                with excutils.save_and_reraise_exception():
                    LOG.error(_("Required IPC directory does not exist at"
                                " %s") % (ipc_dir, ))
        try:
            self.register(consumption_proxy,
                          consume_in,
                          zmq.PULL)
        except zmq.ZMQError:
            if os.access(ipc_dir, os.X_OK):
                with excutils.save_and_reraise_exception():
                    LOG.error(_("Permission denied to IPC directory at"
                                " %s") % (ipc_dir, ))
            with excutils.save_and_reraise_exception():
                LOG.error(_("Could not create ZeroMQ receiver daemon. "
                            "Socket may already be in use."))

        super(ZmqProxy, self).consume_in_thread()


def unflatten_envelope(packenv):
    """Unflattens the RPC envelope.

    Takes a list and returns a dictionary.
    i.e. [1,2,3,4] => {1: 2, 3: 4}
    """
    i = iter(packenv)
    h = {}
    try:
        while True:
            k = i.next()
            h[k] = i.next()
    except StopIteration:
        return h


class ZmqReactor(ZmqBaseReactor):
    """A consumer class implementing a consumer for messages.

    Can also be used as a 1:1 proxy
    """

    def __init__(self, conf):
        super(ZmqReactor, self).__init__(conf)

    def consume(self, sock):
        #TODO(ewindisch): use zero-copy (i.e. references, not copying)
        data = sock.recv()
        LOG.debug(_("CONSUMER RECEIVED DATA: %s"), data)

        proxy = self.proxies[sock]

        if data[2] == 'cast':  # Legacy protocol
            packenv = data[3]

            ctx, msg = _deserialize(packenv)
            request = rpc_common.deserialize_msg(msg)
            ctx = RpcContext.unmarshal(ctx)
        elif data[2] == 'impl_zmq_v2':
            packenv = data[4:]

            msg = unflatten_envelope(packenv)
            request = rpc_common.deserialize_msg(msg)

            # Unmarshal only after verifying the message.
            ctx = RpcContext.unmarshal(data[3])
        else:
            LOG.error(_("ZMQ Envelope version unsupported or unknown."))
            return

        self.pool.spawn_n(self.process, proxy, ctx, request)


class Connection(rpc_common.Connection):
    """Manages connections and threads."""

    def __init__(self, conf):
        self.topics = []
        self.reactor = ZmqReactor(conf)

    def create_consumer(self, topic, proxy, fanout=False):
        # Register with matchmaker.
        _get_matchmaker().register(topic, CONF.rpc_zmq_host)

        # Subscription scenarios
        if fanout:
            sock_type = zmq.SUB
            subscribe = ('', fanout)[type(fanout) == str]
            topic = 'fanout~' + topic.split('.', 1)[0]
        else:
            sock_type = zmq.PULL
            subscribe = None
            topic = '.'.join((topic.split('.', 1)[0], CONF.rpc_zmq_host))

        if topic in self.topics:
            LOG.info(_("Skipping topic registration. Already registered."))
            return

        # Receive messages from (local) proxy
        inaddr = "ipc://%s/zmq_topic_%s" % \
            (CONF.rpc_zmq_ipc_dir, topic)

        LOG.debug(_("Consumer is a zmq.%s"),
                  ['PULL', 'SUB'][sock_type == zmq.SUB])

        self.reactor.register(proxy, inaddr, sock_type,
                              subscribe=subscribe, in_bind=False)
        self.topics.append(topic)

    def close(self):
        _get_matchmaker().stop_heartbeat()
        for topic in self.topics:
            _get_matchmaker().unregister(topic, CONF.rpc_zmq_host)

        self.reactor.close()
        self.topics = []

    def wait(self):
        self.reactor.wait()

    def consume_in_thread(self):
        _get_matchmaker().start_heartbeat()
        self.reactor.consume_in_thread()


def _cast(addr, context, topic, msg, timeout=None, envelope=False,
          _msg_id=None):
    timeout_cast = timeout or CONF.rpc_cast_timeout
    payload = [RpcContext.marshal(context), msg]

    with Timeout(timeout_cast, exception=rpc_common.Timeout):
        try:
            conn = ZmqClient(addr)

            # assumes cast can't return an exception
            conn.cast(_msg_id, topic, payload, envelope)
        except zmq.ZMQError:
            raise RPCException("Cast failed. ZMQ Socket Exception")
        finally:
            if 'conn' in vars():
                conn.close()


def _call(addr, context, topic, msg, timeout=None,
          envelope=False):
    # timeout_response is how long we wait for a response
    timeout = timeout or CONF.rpc_response_timeout

    # The msg_id is used to track replies.
    msg_id = uuid.uuid4().hex

    # Replies always come into the reply service.
    reply_topic = "zmq_replies.%s" % CONF.rpc_zmq_host

    LOG.debug(_("Creating payload"))
    # Curry the original request into a reply method.
    mcontext = RpcContext.marshal(context)
    payload = {
        'method': '-reply',
        'args': {
            'msg_id': msg_id,
            'topic': reply_topic,
            # TODO(ewindisch): safe to remove mcontext in I.
            'msg': [mcontext, msg]
        }
    }

    LOG.debug(_("Creating queue socket for reply waiter"))

    # Messages arriving async.
    # TODO(ewindisch): have reply consumer with dynamic subscription mgmt
    with Timeout(timeout, exception=rpc_common.Timeout):
        try:
            msg_waiter = ZmqSocket(
                "ipc://%s/zmq_topic_zmq_replies.%s" %
                (CONF.rpc_zmq_ipc_dir,
                 CONF.rpc_zmq_host),
                zmq.SUB, subscribe=msg_id, bind=False
            )

            LOG.debug(_("Sending cast"))
            _cast(addr, context, topic, payload, envelope)

            LOG.debug(_("Cast sent; Waiting reply"))
            # Blocks until receives reply
            msg = msg_waiter.recv()
            LOG.debug(_("Received message: %s"), msg)
            LOG.debug(_("Unpacking response"))

            if msg[2] == 'cast':  # Legacy version
                raw_msg = _deserialize(msg[-1])[-1]
            elif msg[2] == 'impl_zmq_v2':
                rpc_envelope = unflatten_envelope(msg[4:])
                raw_msg = rpc_common.deserialize_msg(rpc_envelope)
            else:
                raise rpc_common.UnsupportedRpcEnvelopeVersion(
                    _("Unsupported or unknown ZMQ envelope returned."))

            responses = raw_msg['args']['response']
        # ZMQError trumps the Timeout error.
        except zmq.ZMQError:
            raise RPCException("ZMQ Socket Error")
        except (IndexError, KeyError):
            raise RPCException(_("RPC Message Invalid."))
        finally:
            if 'msg_waiter' in vars():
                msg_waiter.close()

    # It seems we don't need to do all of the following,
    # but perhaps it would be useful for multicall?
    # One effect of this is that we're checking all
    # responses for Exceptions.
    for resp in responses:
        if isinstance(resp, types.DictType) and 'exc' in resp:
            raise rpc_common.deserialize_remote_exception(CONF, resp['exc'])

    return responses[-1]


def _multi_send(method, context, topic, msg, timeout=None,
                envelope=False, _msg_id=None):
    """Wraps the sending of messages.

    Dispatches to the matchmaker and sends message to all relevant hosts.
    """
    conf = CONF
    LOG.debug(_("%(msg)s") % {'msg': ' '.join(map(pformat, (topic, msg)))})

    queues = _get_matchmaker().queues(topic)
    LOG.debug(_("Sending message(s) to: %s"), queues)

    # Don't stack if we have no matchmaker results
    if not queues:
        LOG.warn(_("No matchmaker results. Not casting."))
        # While not strictly a timeout, callers know how to handle
        # this exception and a timeout isn't too big a lie.
        raise rpc_common.Timeout(_("No match from matchmaker."))

    # This supports brokerless fanout (addresses > 1)
    for queue in queues:
        (_topic, ip_addr) = queue
        _addr = "tcp://%s:%s" % (ip_addr, conf.rpc_zmq_port)

        if method.__name__ == '_cast':
            eventlet.spawn_n(method, _addr, context,
                             _topic, msg, timeout, envelope,
                             _msg_id)
            return
        return method(_addr, context, _topic, msg, timeout,
                      envelope)


def create_connection(conf, new=True):
    return Connection(conf)


def multicall(conf, *args, **kwargs):
    """Multiple calls."""
    return _multi_send(_call, *args, **kwargs)


def call(conf, *args, **kwargs):
    """Send a message, expect a response."""
    data = _multi_send(_call, *args, **kwargs)
    return data[-1]


def cast(conf, *args, **kwargs):
    """Send a message expecting no reply."""
    _multi_send(_cast, *args, **kwargs)


def fanout_cast(conf, context, topic, msg, **kwargs):
    """Send a message to all listening and expect no reply."""
    # NOTE(ewindisch): fanout~ is used because it avoid splitting on .
    # and acts as a non-subtle hint to the matchmaker and ZmqProxy.
    _multi_send(_cast, context, 'fanout~' + str(topic), msg, **kwargs)


def notify(conf, context, topic, msg, envelope):
    """Send notification event.

    Notifications are sent to topic-priority.
    This differs from the AMQP drivers which send to topic.priority.
    """
    # NOTE(ewindisch): dot-priority in rpc notifier does not
    # work with our assumptions.
    topic = topic.replace('.', '-')
    cast(conf, context, topic, msg, envelope=envelope)


def cleanup():
    """Clean up resources in use by implementation."""
    global ZMQ_CTX
    if ZMQ_CTX:
        ZMQ_CTX.term()
    ZMQ_CTX = None

    global matchmaker
    matchmaker = None


def _get_ctxt():
    if not zmq:
        raise ImportError("Failed to import eventlet.green.zmq")

    global ZMQ_CTX
    if not ZMQ_CTX:
        ZMQ_CTX = zmq.Context(CONF.rpc_zmq_contexts)
    return ZMQ_CTX


def _get_matchmaker(*args, **kwargs):
    global matchmaker
    if not matchmaker:
        mm = CONF.rpc_zmq_matchmaker
        if mm.endswith('matchmaker.MatchMakerRing'):
            mm.replace('matchmaker', 'matchmaker_ring')
            LOG.warn(_('rpc_zmq_matchmaker = %(orig)s is deprecated; use'
                       ' %(new)s instead') % dict(
                     orig=CONF.rpc_zmq_matchmaker, new=mm))
        matchmaker = importutils.import_object(mm, *args, **kwargs)
    return matchmaker

########NEW FILE########
__FILENAME__ = matchmaker
# vim: tabstop=4 shiftwidth=4 softtabstop=4

#    Copyright 2011 Cloudscaling Group, Inc
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
"""
The MatchMaker classes should except a Topic or Fanout exchange key and
return keys for direct exchanges, per (approximate) AMQP parlance.
"""

import contextlib

import eventlet
from oslo.config import cfg

from trove.openstack.common.gettextutils import _  # noqa
from trove.openstack.common import log as logging


matchmaker_opts = [
    cfg.IntOpt('matchmaker_heartbeat_freq',
               default=300,
               help='Heartbeat frequency'),
    cfg.IntOpt('matchmaker_heartbeat_ttl',
               default=600,
               help='Heartbeat time-to-live.'),
]

CONF = cfg.CONF
CONF.register_opts(matchmaker_opts)
LOG = logging.getLogger(__name__)
contextmanager = contextlib.contextmanager


class MatchMakerException(Exception):
    """Signified a match could not be found."""
    message = _("Match not found by MatchMaker.")


class Exchange(object):
    """Implements lookups.

    Subclass this to support hashtables, dns, etc.
    """
    def __init__(self):
        pass

    def run(self, key):
        raise NotImplementedError()


class Binding(object):
    """A binding on which to perform a lookup."""
    def __init__(self):
        pass

    def test(self, key):
        raise NotImplementedError()


class MatchMakerBase(object):
    """Match Maker Base Class.

    Build off HeartbeatMatchMakerBase if building a heartbeat-capable
    MatchMaker.
    """
    def __init__(self):
        # Array of tuples. Index [2] toggles negation, [3] is last-if-true
        self.bindings = []

        self.no_heartbeat_msg = _('Matchmaker does not implement '
                                  'registration or heartbeat.')

    def register(self, key, host):
        """Register a host on a backend.

        Heartbeats, if applicable, may keepalive registration.
        """
        pass

    def ack_alive(self, key, host):
        """Acknowledge that a key.host is alive.

        Used internally for updating heartbeats, but may also be used
        publically to acknowledge a system is alive (i.e. rpc message
        successfully sent to host)
        """
        pass

    def is_alive(self, topic, host):
        """Checks if a host is alive."""
        pass

    def expire(self, topic, host):
        """Explicitly expire a host's registration."""
        pass

    def send_heartbeats(self):
        """Send all heartbeats.

        Use start_heartbeat to spawn a heartbeat greenthread,
        which loops this method.
        """
        pass

    def unregister(self, key, host):
        """Unregister a topic."""
        pass

    def start_heartbeat(self):
        """Spawn heartbeat greenthread."""
        pass

    def stop_heartbeat(self):
        """Destroys the heartbeat greenthread."""
        pass

    def add_binding(self, binding, rule, last=True):
        self.bindings.append((binding, rule, False, last))

    #NOTE(ewindisch): kept the following method in case we implement the
    #                 underlying support.
    #def add_negate_binding(self, binding, rule, last=True):
    #    self.bindings.append((binding, rule, True, last))

    def queues(self, key):
        workers = []

        # bit is for negate bindings - if we choose to implement it.
        # last stops processing rules if this matches.
        for (binding, exchange, bit, last) in self.bindings:
            if binding.test(key):
                workers.extend(exchange.run(key))

                # Support last.
                if last:
                    return workers
        return workers


class HeartbeatMatchMakerBase(MatchMakerBase):
    """Base for a heart-beat capable MatchMaker.

    Provides common methods for registering, unregistering, and maintaining
    heartbeats.
    """
    def __init__(self):
        self.hosts = set()
        self._heart = None
        self.host_topic = {}

        super(HeartbeatMatchMakerBase, self).__init__()

    def send_heartbeats(self):
        """Send all heartbeats.

        Use start_heartbeat to spawn a heartbeat greenthread,
        which loops this method.
        """
        for key, host in self.host_topic:
            self.ack_alive(key, host)

    def ack_alive(self, key, host):
        """Acknowledge that a host.topic is alive.

        Used internally for updating heartbeats, but may also be used
        publically to acknowledge a system is alive (i.e. rpc message
        successfully sent to host)
        """
        raise NotImplementedError("Must implement ack_alive")

    def backend_register(self, key, host):
        """Implements registration logic.

        Called by register(self,key,host)
        """
        raise NotImplementedError("Must implement backend_register")

    def backend_unregister(self, key, key_host):
        """Implements de-registration logic.

        Called by unregister(self,key,host)
        """
        raise NotImplementedError("Must implement backend_unregister")

    def register(self, key, host):
        """Register a host on a backend.

        Heartbeats, if applicable, may keepalive registration.
        """
        self.hosts.add(host)
        self.host_topic[(key, host)] = host
        key_host = '.'.join((key, host))

        self.backend_register(key, key_host)

        self.ack_alive(key, host)

    def unregister(self, key, host):
        """Unregister a topic."""
        if (key, host) in self.host_topic:
            del self.host_topic[(key, host)]

        self.hosts.discard(host)
        self.backend_unregister(key, '.'.join((key, host)))

        LOG.info(_("Matchmaker unregistered: %(key)s, %(host)s"),
                 {'key': key, 'host': host})

    def start_heartbeat(self):
        """Implementation of MatchMakerBase.start_heartbeat.

        Launches greenthread looping send_heartbeats(),
        yielding for CONF.matchmaker_heartbeat_freq seconds
        between iterations.
        """
        if not self.hosts:
            raise MatchMakerException(
                _("Register before starting heartbeat."))

        def do_heartbeat():
            while True:
                self.send_heartbeats()
                eventlet.sleep(CONF.matchmaker_heartbeat_freq)

        self._heart = eventlet.spawn(do_heartbeat)

    def stop_heartbeat(self):
        """Destroys the heartbeat greenthread."""
        if self._heart:
            self._heart.kill()


class DirectBinding(Binding):
    """Specifies a host in the key via a '.' character.

    Although dots are used in the key, the behavior here is
    that it maps directly to a host, thus direct.
    """
    def test(self, key):
        return '.' in key


class TopicBinding(Binding):
    """Where a 'bare' key without dots.

    AMQP generally considers topic exchanges to be those *with* dots,
    but we deviate here in terminology as the behavior here matches
    that of a topic exchange (whereas where there are dots, behavior
    matches that of a direct exchange.
    """
    def test(self, key):
        return '.' not in key


class FanoutBinding(Binding):
    """Match on fanout keys, where key starts with 'fanout.' string."""
    def test(self, key):
        return key.startswith('fanout~')


class StubExchange(Exchange):
    """Exchange that does nothing."""
    def run(self, key):
        return [(key, None)]


class LocalhostExchange(Exchange):
    """Exchange where all direct topics are local."""
    def __init__(self, host='localhost'):
        self.host = host
        super(Exchange, self).__init__()

    def run(self, key):
        return [('.'.join((key.split('.')[0], self.host)), self.host)]


class DirectExchange(Exchange):
    """Exchange where all topic keys are split, sending to second half.

    i.e. "compute.host" sends a message to "compute.host" running on "host"
    """
    def __init__(self):
        super(Exchange, self).__init__()

    def run(self, key):
        e = key.split('.', 1)[1]
        return [(key, e)]


class MatchMakerLocalhost(MatchMakerBase):
    """Match Maker where all bare topics resolve to localhost.

    Useful for testing.
    """
    def __init__(self, host='localhost'):
        super(MatchMakerLocalhost, self).__init__()
        self.add_binding(FanoutBinding(), LocalhostExchange(host))
        self.add_binding(DirectBinding(), DirectExchange())
        self.add_binding(TopicBinding(), LocalhostExchange(host))


class MatchMakerStub(MatchMakerBase):
    """Match Maker where topics are untouched.

    Useful for testing, or for AMQP/brokered queues.
    Will not work where knowledge of hosts is known (i.e. zeromq)
    """
    def __init__(self):
        super(MatchMakerStub, self).__init__()

        self.add_binding(FanoutBinding(), StubExchange())
        self.add_binding(DirectBinding(), StubExchange())
        self.add_binding(TopicBinding(), StubExchange())

########NEW FILE########
__FILENAME__ = matchmaker_redis
# vim: tabstop=4 shiftwidth=4 softtabstop=4

#    Copyright 2013 Cloudscaling Group, Inc
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
"""
The MatchMaker classes should accept a Topic or Fanout exchange key and
return keys for direct exchanges, per (approximate) AMQP parlance.
"""

from oslo.config import cfg

from trove.openstack.common import importutils
from trove.openstack.common import log as logging
from trove.openstack.common.rpc import matchmaker as mm_common

redis = importutils.try_import('redis')


matchmaker_redis_opts = [
    cfg.StrOpt('host',
               default='127.0.0.1',
               help='Host to locate redis'),
    cfg.IntOpt('port',
               default=6379,
               help='Use this port to connect to redis host.'),
    cfg.StrOpt('password',
               default=None,
               help='Password for Redis server. (optional)'),
]

CONF = cfg.CONF
opt_group = cfg.OptGroup(name='matchmaker_redis',
                         title='Options for Redis-based MatchMaker')
CONF.register_group(opt_group)
CONF.register_opts(matchmaker_redis_opts, opt_group)
LOG = logging.getLogger(__name__)


class RedisExchange(mm_common.Exchange):
    def __init__(self, matchmaker):
        self.matchmaker = matchmaker
        self.redis = matchmaker.redis
        super(RedisExchange, self).__init__()


class RedisTopicExchange(RedisExchange):
    """Exchange where all topic keys are split, sending to second half.

    i.e. "compute.host" sends a message to "compute" running on "host"
    """
    def run(self, topic):
        while True:
            member_name = self.redis.srandmember(topic)

            if not member_name:
                # If this happens, there are no
                # longer any members.
                break

            if not self.matchmaker.is_alive(topic, member_name):
                continue

            host = member_name.split('.', 1)[1]
            return [(member_name, host)]
        return []


class RedisFanoutExchange(RedisExchange):
    """Return a list of all hosts."""
    def run(self, topic):
        topic = topic.split('~', 1)[1]
        hosts = self.redis.smembers(topic)
        good_hosts = filter(
            lambda host: self.matchmaker.is_alive(topic, host), hosts)

        return [(x, x.split('.', 1)[1]) for x in good_hosts]


class MatchMakerRedis(mm_common.HeartbeatMatchMakerBase):
    """MatchMaker registering and looking-up hosts with a Redis server."""
    def __init__(self):
        super(MatchMakerRedis, self).__init__()

        if not redis:
            raise ImportError("Failed to import module redis.")

        self.redis = redis.StrictRedis(
            host=CONF.matchmaker_redis.host,
            port=CONF.matchmaker_redis.port,
            password=CONF.matchmaker_redis.password)

        self.add_binding(mm_common.FanoutBinding(), RedisFanoutExchange(self))
        self.add_binding(mm_common.DirectBinding(), mm_common.DirectExchange())
        self.add_binding(mm_common.TopicBinding(), RedisTopicExchange(self))

    def ack_alive(self, key, host):
        topic = "%s.%s" % (key, host)
        if not self.redis.expire(topic, CONF.matchmaker_heartbeat_ttl):
            # If we could not update the expiration, the key
            # might have been pruned. Re-register, creating a new
            # key in Redis.
            self.register(self.topic_host[host], host)

    def is_alive(self, topic, host):
        if self.redis.ttl(host) == -1:
            self.expire(topic, host)
            return False
        return True

    def expire(self, topic, host):
        with self.redis.pipeline() as pipe:
            pipe.multi()
            pipe.delete(host)
            pipe.srem(topic, host)
            pipe.execute()

    def backend_register(self, key, key_host):
        with self.redis.pipeline() as pipe:
            pipe.multi()
            pipe.sadd(key, key_host)

            # No value is needed, we just
            # care if it exists. Sets aren't viable
            # because only keys can expire.
            pipe.set(key_host, '')

            pipe.execute()

    def backend_unregister(self, key, key_host):
        with self.redis.pipeline() as pipe:
            pipe.multi()
            pipe.srem(key, key_host)
            pipe.delete(key_host)
            pipe.execute()

########NEW FILE########
__FILENAME__ = matchmaker_ring
# vim: tabstop=4 shiftwidth=4 softtabstop=4

#    Copyright 2011-2013 Cloudscaling Group, Inc
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
"""
The MatchMaker classes should except a Topic or Fanout exchange key and
return keys for direct exchanges, per (approximate) AMQP parlance.
"""

import itertools
import json

from oslo.config import cfg

from trove.openstack.common.gettextutils import _  # noqa
from trove.openstack.common import log as logging
from trove.openstack.common.rpc import matchmaker as mm


matchmaker_opts = [
    # Matchmaker ring file
    cfg.StrOpt('ringfile',
               deprecated_name='matchmaker_ringfile',
               deprecated_group='DEFAULT',
               default='/etc/oslo/matchmaker_ring.json',
               help='Matchmaker ring file (JSON)'),
]

CONF = cfg.CONF
CONF.register_opts(matchmaker_opts, 'matchmaker_ring')
LOG = logging.getLogger(__name__)


class RingExchange(mm.Exchange):
    """Match Maker where hosts are loaded from a static JSON formatted file.

    __init__ takes optional ring dictionary argument, otherwise
    loads the ringfile from CONF.mathcmaker_ringfile.
    """
    def __init__(self, ring=None):
        super(RingExchange, self).__init__()

        if ring:
            self.ring = ring
        else:
            fh = open(CONF.matchmaker_ring.ringfile, 'r')
            self.ring = json.load(fh)
            fh.close()

        self.ring0 = {}
        for k in self.ring.keys():
            self.ring0[k] = itertools.cycle(self.ring[k])

    def _ring_has(self, key):
        return key in self.ring0


class RoundRobinRingExchange(RingExchange):
    """A Topic Exchange based on a hashmap."""
    def __init__(self, ring=None):
        super(RoundRobinRingExchange, self).__init__(ring)

    def run(self, key):
        if not self._ring_has(key):
            LOG.warn(
                _("No key defining hosts for topic '%s', "
                  "see ringfile") % (key, )
            )
            return []
        host = next(self.ring0[key])
        return [(key + '.' + host, host)]


class FanoutRingExchange(RingExchange):
    """Fanout Exchange based on a hashmap."""
    def __init__(self, ring=None):
        super(FanoutRingExchange, self).__init__(ring)

    def run(self, key):
        # Assume starts with "fanout~", strip it for lookup.
        nkey = key.split('fanout~')[1:][0]
        if not self._ring_has(nkey):
            LOG.warn(
                _("No key defining hosts for topic '%s', "
                  "see ringfile") % (nkey, )
            )
            return []
        return map(lambda x: (key + '.' + x, x), self.ring[nkey])


class MatchMakerRing(mm.MatchMakerBase):
    """Match Maker where hosts are loaded from a static hashmap."""
    def __init__(self, ring=None):
        super(MatchMakerRing, self).__init__()
        self.add_binding(mm.FanoutBinding(), FanoutRingExchange(ring))
        self.add_binding(mm.DirectBinding(), mm.DirectExchange())
        self.add_binding(mm.TopicBinding(), RoundRobinRingExchange(ring))

########NEW FILE########
__FILENAME__ = proxy
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2012-2013 Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
A helper class for proxy objects to remote APIs.

For more information about rpc API version numbers, see:
    rpc/dispatcher.py
"""


from trove.openstack.common import rpc
from trove.openstack.common.rpc import common as rpc_common
from trove.openstack.common.rpc import serializer as rpc_serializer


class RpcProxy(object):
    """A helper class for rpc clients.

    This class is a wrapper around the RPC client API.  It allows you to
    specify the topic and API version in a single place.  This is intended to
    be used as a base class for a class that implements the client side of an
    rpc API.
    """

    # The default namespace, which can be overriden in a subclass.
    RPC_API_NAMESPACE = None

    def __init__(self, topic, default_version, version_cap=None,
                 serializer=None):
        """Initialize an RpcProxy.

        :param topic: The topic to use for all messages.
        :param default_version: The default API version to request in all
               outgoing messages.  This can be overridden on a per-message
               basis.
        :param version_cap: Optionally cap the maximum version used for sent
               messages.
        :param serializer: Optionaly (de-)serialize entities with a
               provided helper.
        """
        self.topic = topic
        self.default_version = default_version
        self.version_cap = version_cap
        if serializer is None:
            serializer = rpc_serializer.NoOpSerializer()
        self.serializer = serializer
        super(RpcProxy, self).__init__()

    def _set_version(self, msg, vers):
        """Helper method to set the version in a message.

        :param msg: The message having a version added to it.
        :param vers: The version number to add to the message.
        """
        v = vers if vers else self.default_version
        if (self.version_cap and not
                rpc_common.version_is_compatible(self.version_cap, v)):
            raise rpc_common.RpcVersionCapError(version_cap=self.version_cap)
        msg['version'] = v

    def _get_topic(self, topic):
        """Return the topic to use for a message."""
        return topic if topic else self.topic

    def can_send_version(self, version):
        """Check to see if a version is compatible with the version cap."""
        return (not self.version_cap or
                rpc_common.version_is_compatible(self.version_cap, version))

    @staticmethod
    def make_namespaced_msg(method, namespace, **kwargs):
        return {'method': method, 'namespace': namespace, 'args': kwargs}

    def make_msg(self, method, **kwargs):
        return self.make_namespaced_msg(method, self.RPC_API_NAMESPACE,
                                        **kwargs)

    def _serialize_msg_args(self, context, kwargs):
        """Helper method called to serialize message arguments.

        This calls our serializer on each argument, returning a new
        set of args that have been serialized.

        :param context: The request context
        :param kwargs: The arguments to serialize
        :returns: A new set of serialized arguments
        """
        new_kwargs = dict()
        for argname, arg in kwargs.iteritems():
            new_kwargs[argname] = self.serializer.serialize_entity(context,
                                                                   arg)
        return new_kwargs

    def call(self, context, msg, topic=None, version=None, timeout=None):
        """rpc.call() a remote method.

        :param context: The request context
        :param msg: The message to send, including the method and args.
        :param topic: Override the topic for this message.
        :param version: (Optional) Override the requested API version in this
               message.
        :param timeout: (Optional) A timeout to use when waiting for the
               response.  If no timeout is specified, a default timeout will be
               used that is usually sufficient.

        :returns: The return value from the remote method.
        """
        self._set_version(msg, version)
        msg['args'] = self._serialize_msg_args(context, msg['args'])
        real_topic = self._get_topic(topic)
        try:
            result = rpc.call(context, real_topic, msg, timeout)
            return self.serializer.deserialize_entity(context, result)
        except rpc.common.Timeout as exc:
            raise rpc.common.Timeout(
                exc.info, real_topic, msg.get('method'))

    def multicall(self, context, msg, topic=None, version=None, timeout=None):
        """rpc.multicall() a remote method.

        :param context: The request context
        :param msg: The message to send, including the method and args.
        :param topic: Override the topic for this message.
        :param version: (Optional) Override the requested API version in this
               message.
        :param timeout: (Optional) A timeout to use when waiting for the
               response.  If no timeout is specified, a default timeout will be
               used that is usually sufficient.

        :returns: An iterator that lets you process each of the returned values
                  from the remote method as they arrive.
        """
        self._set_version(msg, version)
        msg['args'] = self._serialize_msg_args(context, msg['args'])
        real_topic = self._get_topic(topic)
        try:
            result = rpc.multicall(context, real_topic, msg, timeout)
            return self.serializer.deserialize_entity(context, result)
        except rpc.common.Timeout as exc:
            raise rpc.common.Timeout(
                exc.info, real_topic, msg.get('method'))

    def cast(self, context, msg, topic=None, version=None):
        """rpc.cast() a remote method.

        :param context: The request context
        :param msg: The message to send, including the method and args.
        :param topic: Override the topic for this message.
        :param version: (Optional) Override the requested API version in this
               message.

        :returns: None.  rpc.cast() does not wait on any return value from the
                  remote method.
        """
        self._set_version(msg, version)
        msg['args'] = self._serialize_msg_args(context, msg['args'])
        rpc.cast(context, self._get_topic(topic), msg)

    def fanout_cast(self, context, msg, topic=None, version=None):
        """rpc.fanout_cast() a remote method.

        :param context: The request context
        :param msg: The message to send, including the method and args.
        :param topic: Override the topic for this message.
        :param version: (Optional) Override the requested API version in this
               message.

        :returns: None.  rpc.fanout_cast() does not wait on any return value
                  from the remote method.
        """
        self._set_version(msg, version)
        msg['args'] = self._serialize_msg_args(context, msg['args'])
        rpc.fanout_cast(context, self._get_topic(topic), msg)

    def cast_to_server(self, context, server_params, msg, topic=None,
                       version=None):
        """rpc.cast_to_server() a remote method.

        :param context: The request context
        :param server_params: Server parameters.  See rpc.cast_to_server() for
               details.
        :param msg: The message to send, including the method and args.
        :param topic: Override the topic for this message.
        :param version: (Optional) Override the requested API version in this
               message.

        :returns: None.  rpc.cast_to_server() does not wait on any
                  return values.
        """
        self._set_version(msg, version)
        msg['args'] = self._serialize_msg_args(context, msg['args'])
        rpc.cast_to_server(context, server_params, self._get_topic(topic), msg)

    def fanout_cast_to_server(self, context, server_params, msg, topic=None,
                              version=None):
        """rpc.fanout_cast_to_server() a remote method.

        :param context: The request context
        :param server_params: Server parameters.  See rpc.cast_to_server() for
               details.
        :param msg: The message to send, including the method and args.
        :param topic: Override the topic for this message.
        :param version: (Optional) Override the requested API version in this
               message.

        :returns: None.  rpc.fanout_cast_to_server() does not wait on any
                  return values.
        """
        self._set_version(msg, version)
        msg['args'] = self._serialize_msg_args(context, msg['args'])
        rpc.fanout_cast_to_server(context, server_params,
                                  self._get_topic(topic), msg)

########NEW FILE########
__FILENAME__ = securemessage
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2013 Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import base64
import collections
import os
import struct
import time

import requests

from oslo.config import cfg

from trove.openstack.common.crypto import utils as cryptoutils
from trove.openstack.common import jsonutils
from trove.openstack.common import log as logging

secure_message_opts = [
    cfg.BoolOpt('enabled', default=True,
                help='Whether Secure Messaging (Signing) is enabled,'
                     ' defaults to enabled'),
    cfg.BoolOpt('enforced', default=False,
                help='Whether Secure Messaging (Signing) is enforced,'
                     ' defaults to not enforced'),
    cfg.BoolOpt('encrypt', default=False,
                help='Whether Secure Messaging (Encryption) is enabled,'
                     ' defaults to not enabled'),
    cfg.StrOpt('secret_keys_file',
               help='Path to the file containing the keys, takes precedence'
                    ' over secret_key'),
    cfg.MultiStrOpt('secret_key',
                    help='A list of keys: (ex: name:<base64 encoded key>),'
                         ' ignored if secret_keys_file is set'),
    cfg.StrOpt('kds_endpoint',
               help='KDS endpoint (ex: http://kds.example.com:35357/v3)'),
]
secure_message_group = cfg.OptGroup('secure_messages',
                                    title='Secure Messaging options')

LOG = logging.getLogger(__name__)


class SecureMessageException(Exception):
    """Generic Exception for Secure Messages."""

    msg = "An unknown Secure Message related exception occurred."

    def __init__(self, msg=None):
        if msg is None:
            msg = self.msg
        super(SecureMessageException, self).__init__(msg)


class SharedKeyNotFound(SecureMessageException):
    """No shared key was found and no other external authentication mechanism
    is available.
    """

    msg = "Shared Key for [%s] Not Found. (%s)"

    def __init__(self, name, errmsg):
        super(SharedKeyNotFound, self).__init__(self.msg % (name, errmsg))


class InvalidMetadata(SecureMessageException):
    """The metadata is invalid."""

    msg = "Invalid metadata: %s"

    def __init__(self, err):
        super(InvalidMetadata, self).__init__(self.msg % err)


class InvalidSignature(SecureMessageException):
    """Signature validation failed."""

    msg = "Failed to validate signature (source=%s, destination=%s)"

    def __init__(self, src, dst):
        super(InvalidSignature, self).__init__(self.msg % (src, dst))


class UnknownDestinationName(SecureMessageException):
    """The Destination name is unknown to us."""

    msg = "Invalid destination name (%s)"

    def __init__(self, name):
        super(UnknownDestinationName, self).__init__(self.msg % name)


class InvalidEncryptedTicket(SecureMessageException):
    """The Encrypted Ticket could not be successfully handled."""

    msg = "Invalid Ticket (source=%s, destination=%s)"

    def __init__(self, src, dst):
        super(InvalidEncryptedTicket, self).__init__(self.msg % (src, dst))


class InvalidExpiredTicket(SecureMessageException):
    """The ticket received is already expired."""

    msg = "Expired ticket (source=%s, destination=%s)"

    def __init__(self, src, dst):
        super(InvalidExpiredTicket, self).__init__(self.msg % (src, dst))


class CommunicationError(SecureMessageException):
    """The Communication with the KDS failed."""

    msg = "Communication Error (target=%s): %s"

    def __init__(self, target, errmsg):
        super(CommunicationError, self).__init__(self.msg % (target, errmsg))


class InvalidArgument(SecureMessageException):
    """Bad initialization argument."""

    msg = "Invalid argument: %s"

    def __init__(self, errmsg):
        super(InvalidArgument, self).__init__(self.msg % errmsg)


Ticket = collections.namedtuple('Ticket', ['skey', 'ekey', 'esek'])


class KeyStore(object):
    """A storage class for Signing and Encryption Keys.

    This class creates an object that holds Generic Keys like Signing
    Keys, Encryption Keys, Encrypted SEK Tickets ...
    """

    def __init__(self):
        self._kvps = dict()

    def _get_key_name(self, source, target, ktype):
        return (source, target, ktype)

    def _put(self, src, dst, ktype, expiration, data):
        name = self._get_key_name(src, dst, ktype)
        self._kvps[name] = (expiration, data)

    def _get(self, src, dst, ktype):
        name = self._get_key_name(src, dst, ktype)
        if name in self._kvps:
            expiration, data = self._kvps[name]
            if expiration > time.time():
                return data
            else:
                del self._kvps[name]

        return None

    def clear(self):
        """Wipes the store clear of all data."""
        self._kvps.clear()

    def put_ticket(self, source, target, skey, ekey, esek, expiration):
        """Puts a sek pair in the cache.

        :param source: Client name
        :param target: Target name
        :param skey: The Signing Key
        :param ekey: The Encription Key
        :param esek: The token encrypted with the target key
        :param expiration: Expiration time in seconds since Epoch
        """
        keys = Ticket(skey, ekey, esek)
        self._put(source, target, 'ticket', expiration, keys)

    def get_ticket(self, source, target):
        """Returns a Ticket (skey, ekey, esek) namedtuple for the
           source/target pair.
        """
        return self._get(source, target, 'ticket')


_KEY_STORE = KeyStore()


class _KDSClient(object):

    USER_AGENT = 'oslo-incubator/rpc'

    def __init__(self, endpoint=None, timeout=None):
        """A KDS Client class."""

        self._endpoint = endpoint
        if timeout is not None:
            self.timeout = float(timeout)
        else:
            self.timeout = None

    def _do_get(self, url, request):
        req_kwargs = dict()
        req_kwargs['headers'] = dict()
        req_kwargs['headers']['User-Agent'] = self.USER_AGENT
        req_kwargs['headers']['Content-Type'] = 'application/json'
        req_kwargs['data'] = jsonutils.dumps({'request': request})
        if self.timeout is not None:
            req_kwargs['timeout'] = self.timeout

        try:
            resp = requests.get(url, **req_kwargs)
        except requests.ConnectionError as e:
            err = "Unable to establish connection. %s" % e
            raise CommunicationError(url, err)

        return resp

    def _get_reply(self, url, resp):
        if resp.text:
            try:
                body = jsonutils.loads(resp.text)
                reply = body['reply']
            except (KeyError, TypeError, ValueError):
                msg = "Failed to decode reply: %s" % resp.text
                raise CommunicationError(url, msg)
        else:
            msg = "No reply data was returned."
            raise CommunicationError(url, msg)

        return reply

    def _get_ticket(self, request, url=None, redirects=10):
        """Send an HTTP request.

        Wraps around 'requests' to handle redirects and common errors.
        """
        if url is None:
            if not self._endpoint:
                raise CommunicationError(url, 'Endpoint not configured')
            url = self._endpoint + '/kds/ticket'

        while redirects:
            resp = self._do_get(url, request)
            if resp.status_code in (301, 302, 305):
                # Redirected. Reissue the request to the new location.
                url = resp.headers['location']
                redirects -= 1
                continue
            elif resp.status_code != 200:
                msg = "Request returned failure status: %s (%s)"
                err = msg % (resp.status_code, resp.text)
                raise CommunicationError(url, err)

            return self._get_reply(url, resp)

        raise CommunicationError(url, "Too many redirections, giving up!")

    def get_ticket(self, source, target, crypto, key):

        # prepare metadata
        md = {'requestor': source,
              'target': target,
              'timestamp': time.time(),
              'nonce': struct.unpack('Q', os.urandom(8))[0]}
        metadata = base64.b64encode(jsonutils.dumps(md))

        # sign metadata
        signature = crypto.sign(key, metadata)

        # HTTP request
        reply = self._get_ticket({'metadata': metadata,
                                  'signature': signature})

        # verify reply
        signature = crypto.sign(key, (reply['metadata'] + reply['ticket']))
        if signature != reply['signature']:
            raise InvalidEncryptedTicket(md['source'], md['destination'])
        md = jsonutils.loads(base64.b64decode(reply['metadata']))
        if ((md['source'] != source or
             md['destination'] != target or
             md['expiration'] < time.time())):
            raise InvalidEncryptedTicket(md['source'], md['destination'])

        # return ticket data
        tkt = jsonutils.loads(crypto.decrypt(key, reply['ticket']))

        return tkt, md['expiration']


# we need to keep a global nonce, as this value should never repeat non
# matter how many SecureMessage objects we create
_NONCE = None


def _get_nonce():
    """We keep a single counter per instance, as it is so huge we can't
    possibly cycle through within 1/100 of a second anyway.
    """

    global _NONCE
    # Lazy initialize, for now get a random value, multiply by 2^32 and
    # use it as the nonce base. The counter itself will rotate after
    # 2^32 increments.
    if _NONCE is None:
        _NONCE = [struct.unpack('I', os.urandom(4))[0], 0]

    # Increment counter and wrap at 2^32
    _NONCE[1] += 1
    if _NONCE[1] > 0xffffffff:
        _NONCE[1] = 0

    # Return base + counter
    return long((_NONCE[0] * 0xffffffff)) + _NONCE[1]


class SecureMessage(object):
    """A Secure Message object.

    This class creates a signing/encryption facility for RPC messages.
    It encapsulates all the necessary crypto primitives to insulate
    regular code from the intricacies of message authentication, validation
    and optionally encryption.

    :param topic: The topic name of the queue
    :param host: The server name, together with the topic it forms a unique
                 name that is used to source signing keys, and verify
                 incoming messages.
    :param conf: a ConfigOpts object
    :param key: (optional) explicitly pass in endpoint private key.
                  If not provided it will be sourced from the service config
    :param key_store: (optional) Storage class for local caching
    :param encrypt: (defaults to False) Whether to encrypt messages
    :param enctype: (defaults to AES) Cipher to use
    :param hashtype: (defaults to SHA256) Hash function to use for signatures
    """

    def __init__(self, topic, host, conf, key=None, key_store=None,
                 encrypt=None, enctype='AES', hashtype='SHA256'):

        conf.register_group(secure_message_group)
        conf.register_opts(secure_message_opts, group='secure_messages')

        self._name = '%s.%s' % (topic, host)
        self._key = key
        self._conf = conf.secure_messages
        self._encrypt = self._conf.encrypt if (encrypt is None) else encrypt
        self._crypto = cryptoutils.SymmetricCrypto(enctype, hashtype)
        self._hkdf = cryptoutils.HKDF(hashtype)
        self._kds = _KDSClient(self._conf.kds_endpoint)

        if self._key is None:
            self._key = self._init_key(topic, self._name)
        if self._key is None:
            err = "Secret Key (or key file) is missing or malformed"
            raise SharedKeyNotFound(self._name, err)

        self._key_store = key_store or _KEY_STORE

    def _init_key(self, topic, name):
        keys = None
        if self._conf.secret_keys_file:
            with open(self._conf.secret_keys_file, 'r') as f:
                keys = f.readlines()
        elif self._conf.secret_key:
            keys = self._conf.secret_key

        if keys is None:
            return None

        for k in keys:
            if k[0] == '#':
                continue
            if ':' not in k:
                break
            svc, key = k.split(':', 1)
            if svc == topic or svc == name:
                return base64.b64decode(key)

        return None

    def _split_key(self, key, size):
        sig_key = key[:size]
        enc_key = key[size:]
        return sig_key, enc_key

    def _decode_esek(self, key, source, target, timestamp, esek):
        """This function decrypts the esek buffer passed in and returns a
        KeyStore to be used to check and decrypt the received message.

        :param key: The key to use to decrypt the ticket (esek)
        :param source: The name of the source service
        :param traget: The name of the target service
        :param timestamp: The incoming message timestamp
        :param esek: a base64 encoded encrypted block containing a JSON string
        """
        rkey = None

        try:
            s = self._crypto.decrypt(key, esek)
            j = jsonutils.loads(s)

            rkey = base64.b64decode(j['key'])
            expiration = j['timestamp'] + j['ttl']
            if j['timestamp'] > timestamp or timestamp > expiration:
                raise InvalidExpiredTicket(source, target)

        except Exception:
            raise InvalidEncryptedTicket(source, target)

        info = '%s,%s,%s' % (source, target, str(j['timestamp']))

        sek = self._hkdf.expand(rkey, info, len(key) * 2)

        return self._split_key(sek, len(key))

    def _get_ticket(self, target):
        """This function will check if we already have a SEK for the specified
        target in the cache, or will go and try to fetch a new SEK from the key
        server.

        :param target: The name of the target service
        """
        ticket = self._key_store.get_ticket(self._name, target)

        if ticket is not None:
            return ticket

        tkt, expiration = self._kds.get_ticket(self._name, target,
                                               self._crypto, self._key)

        self._key_store.put_ticket(self._name, target,
                                   base64.b64decode(tkt['skey']),
                                   base64.b64decode(tkt['ekey']),
                                   tkt['esek'], expiration)
        return self._key_store.get_ticket(self._name, target)

    def encode(self, version, target, json_msg):
        """This is the main encoding function.

        It takes a target and a message and returns a tuple consisting of a
        JSON serialized metadata object, a JSON serialized (and optionally
        encrypted) message, and a signature.

        :param version: the current envelope version
        :param target: The name of the target service (usually with hostname)
        :param json_msg: a serialized json message object
        """
        ticket = self._get_ticket(target)

        metadata = jsonutils.dumps({'source': self._name,
                                    'destination': target,
                                    'timestamp': time.time(),
                                    'nonce': _get_nonce(),
                                    'esek': ticket.esek,
                                    'encryption': self._encrypt})

        message = json_msg
        if self._encrypt:
            message = self._crypto.encrypt(ticket.ekey, message)

        signature = self._crypto.sign(ticket.skey,
                                      version + metadata + message)

        return (metadata, message, signature)

    def decode(self, version, metadata, message, signature):
        """This is the main decoding function.

        It takes a version, metadata, message and signature strings and
        returns a tuple with a (decrypted) message and metadata or raises
        an exception in case of error.

        :param version: the current envelope version
        :param metadata: a JSON serialized object with metadata for validation
        :param message: a JSON serialized (base64 encoded encrypted) message
        :param signature: a base64 encoded signature
        """
        md = jsonutils.loads(metadata)

        check_args = ('source', 'destination', 'timestamp',
                      'nonce', 'esek', 'encryption')
        for arg in check_args:
            if arg not in md:
                raise InvalidMetadata('Missing metadata "%s"' % arg)

        if md['destination'] != self._name:
            # TODO(simo) handle group keys by checking target
            raise UnknownDestinationName(md['destination'])

        try:
            skey, ekey = self._decode_esek(self._key,
                                           md['source'], md['destination'],
                                           md['timestamp'], md['esek'])
        except InvalidExpiredTicket:
            raise
        except Exception:
            raise InvalidMetadata('Failed to decode ESEK for %s/%s' % (
                                  md['source'], md['destination']))

        sig = self._crypto.sign(skey, version + metadata + message)

        if sig != signature:
            raise InvalidSignature(md['source'], md['destination'])

        if md['encryption'] is True:
            msg = self._crypto.decrypt(ekey, message)
        else:
            msg = message

        return (md, msg)

########NEW FILE########
__FILENAME__ = serializer
#    Copyright 2013 IBM Corp.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Provides the definition of an RPC serialization handler"""

import abc


class Serializer(object):
    """Generic (de-)serialization definition base class."""
    __metaclass__ = abc.ABCMeta

    @abc.abstractmethod
    def serialize_entity(self, context, entity):
        """Serialize something to primitive form.

        :param context: Security context
        :param entity: Entity to be serialized
        :returns: Serialized form of entity
        """
        pass

    @abc.abstractmethod
    def deserialize_entity(self, context, entity):
        """Deserialize something from primitive form.

        :param context: Security context
        :param entity: Primitive to be deserialized
        :returns: Deserialized form of entity
        """
        pass


class NoOpSerializer(Serializer):
    """A serializer that does nothing."""

    def serialize_entity(self, context, entity):
        return entity

    def deserialize_entity(self, context, entity):
        return entity

########NEW FILE########
__FILENAME__ = service
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
# Copyright 2011 Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from trove.openstack.common.gettextutils import _  # noqa
from trove.openstack.common import log as logging
from trove.openstack.common import rpc
from trove.openstack.common.rpc import dispatcher as rpc_dispatcher
from trove.openstack.common import service


LOG = logging.getLogger(__name__)


class Service(service.Service):
    """Service object for binaries running on hosts.

    A service enables rpc by listening to queues based on topic and host.
    """
    def __init__(self, host, topic, manager=None, serializer=None):
        super(Service, self).__init__()
        self.host = host
        self.topic = topic
        self.serializer = serializer
        if manager is None:
            self.manager = self
        else:
            self.manager = manager

    def start(self):
        super(Service, self).start()

        self.conn = rpc.create_connection(new=True)
        LOG.debug(_("Creating Consumer connection for Service %s") %
                  self.topic)

        dispatcher = rpc_dispatcher.RpcDispatcher([self.manager],
                                                  self.serializer)

        # Share this same connection for these Consumers
        self.conn.create_consumer(self.topic, dispatcher, fanout=False)

        node_topic = '%s.%s' % (self.topic, self.host)
        self.conn.create_consumer(node_topic, dispatcher, fanout=False)

        self.conn.create_consumer(self.topic, dispatcher, fanout=True)

        # Hook to allow the manager to do other initializations after
        # the rpc connection is created.
        if callable(getattr(self.manager, 'initialize_service_hook', None)):
            self.manager.initialize_service_hook(self)

        # Consume from all consumers in a thread
        self.conn.consume_in_thread()

    def stop(self):
        # Try to shut the connection down, but if we get any sort of
        # errors, go ahead and ignore them.. as we're shutting down anyway
        try:
            self.conn.close()
        except Exception:
            pass
        super(Service, self).stop()

########NEW FILE########
__FILENAME__ = zmq_receiver
# vim: tabstop=4 shiftwidth=4 softtabstop=4

#    Copyright 2011 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import eventlet
eventlet.monkey_patch()

import contextlib
import sys

from oslo.config import cfg

from trove.openstack.common import log as logging
from trove.openstack.common import rpc
from trove.openstack.common.rpc import impl_zmq

CONF = cfg.CONF
CONF.register_opts(rpc.rpc_opts)
CONF.register_opts(impl_zmq.zmq_opts)


def main():
    CONF(sys.argv[1:], project='oslo')
    logging.setup("oslo")

    with contextlib.closing(impl_zmq.ZmqProxy(CONF)) as reactor:
        reactor.consume_in_thread()
        reactor.wait()

########NEW FILE########
__FILENAME__ = service
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# Copyright 2011 Justin Santa Barbara
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Generic Node base class for all workers that run on hosts."""

import errno
import os
import random
import signal
import sys
import time

import eventlet
from eventlet import event
import logging as std_logging
from oslo.config import cfg

from trove.openstack.common import eventlet_backdoor
from trove.openstack.common.gettextutils import _  # noqa
from trove.openstack.common import importutils
from trove.openstack.common import log as logging
from trove.openstack.common import threadgroup


rpc = importutils.try_import('trove.openstack.common.rpc')
CONF = cfg.CONF
LOG = logging.getLogger(__name__)


class Launcher(object):
    """Launch one or more services and wait for them to complete."""

    def __init__(self):
        """Initialize the service launcher.

        :returns: None

        """
        self.services = Services()
        self.backdoor_port = eventlet_backdoor.initialize_if_enabled()

    def launch_service(self, service):
        """Load and start the given service.

        :param service: The service you would like to start.
        :returns: None

        """
        service.backdoor_port = self.backdoor_port
        self.services.add(service)

    def stop(self):
        """Stop all services which are currently running.

        :returns: None

        """
        self.services.stop()

    def wait(self):
        """Waits until all services have been stopped, and then returns.

        :returns: None

        """
        self.services.wait()

    def restart(self):
        """Reload config files and restart service.

        :returns: None

        """
        cfg.CONF.reload_config_files()
        self.services.restart()


class SignalExit(SystemExit):
    def __init__(self, signo, exccode=1):
        super(SignalExit, self).__init__(exccode)
        self.signo = signo


class ServiceLauncher(Launcher):
    def _handle_signal(self, signo, frame):
        # Allow the process to be killed again and die from natural causes
        signal.signal(signal.SIGTERM, signal.SIG_DFL)
        signal.signal(signal.SIGINT, signal.SIG_DFL)
        signal.signal(signal.SIGHUP, signal.SIG_DFL)

        raise SignalExit(signo)

    def handle_signal(self):
        signal.signal(signal.SIGTERM, self._handle_signal)
        signal.signal(signal.SIGINT, self._handle_signal)
        signal.signal(signal.SIGHUP, self._handle_signal)

    def _wait_for_exit_or_signal(self):
        status = None
        signo = 0

        LOG.debug(_('Full set of CONF:'))
        CONF.log_opt_values(LOG, std_logging.DEBUG)

        try:
            super(ServiceLauncher, self).wait()
        except SignalExit as exc:
            signame = {signal.SIGTERM: 'SIGTERM',
                       signal.SIGINT: 'SIGINT',
                       signal.SIGHUP: 'SIGHUP'}[exc.signo]
            LOG.info(_('Caught %s, exiting'), signame)
            status = exc.code
            signo = exc.signo
        except SystemExit as exc:
            status = exc.code
        finally:
            self.stop()
            if rpc:
                try:
                    rpc.cleanup()
                except Exception:
                    # We're shutting down, so it doesn't matter at this point.
                    LOG.exception(_('Exception during rpc cleanup.'))

        return status, signo

    def wait(self):
        while True:
            self.handle_signal()
            status, signo = self._wait_for_exit_or_signal()
            if signo != signal.SIGHUP:
                return status
            self.restart()


class ServiceWrapper(object):
    def __init__(self, service, workers):
        self.service = service
        self.workers = workers
        self.children = set()
        self.forktimes = []


class ProcessLauncher(object):
    def __init__(self):
        self.children = {}
        self.sigcaught = None
        self.running = True
        rfd, self.writepipe = os.pipe()
        self.readpipe = eventlet.greenio.GreenPipe(rfd, 'r')
        self.handle_signal()

    def handle_signal(self):
        signal.signal(signal.SIGTERM, self._handle_signal)
        signal.signal(signal.SIGINT, self._handle_signal)
        signal.signal(signal.SIGHUP, self._handle_signal)

    def _handle_signal(self, signo, frame):
        self.sigcaught = signo
        self.running = False

        # Allow the process to be killed again and die from natural causes
        signal.signal(signal.SIGTERM, signal.SIG_DFL)
        signal.signal(signal.SIGINT, signal.SIG_DFL)
        signal.signal(signal.SIGHUP, signal.SIG_DFL)

    def _pipe_watcher(self):
        # This will block until the write end is closed when the parent
        # dies unexpectedly
        self.readpipe.read()

        LOG.info(_('Parent process has died unexpectedly, exiting'))

        sys.exit(1)

    def _child_process_handle_signal(self):
        # Setup child signal handlers differently
        def _sigterm(*args):
            signal.signal(signal.SIGTERM, signal.SIG_DFL)
            raise SignalExit(signal.SIGTERM)

        def _sighup(*args):
            signal.signal(signal.SIGHUP, signal.SIG_DFL)
            raise SignalExit(signal.SIGHUP)

        signal.signal(signal.SIGTERM, _sigterm)
        signal.signal(signal.SIGHUP, _sighup)
        # Block SIGINT and let the parent send us a SIGTERM
        signal.signal(signal.SIGINT, signal.SIG_IGN)

    def _child_wait_for_exit_or_signal(self, launcher):
        status = None
        signo = 0

        try:
            launcher.wait()
        except SignalExit as exc:
            signame = {signal.SIGTERM: 'SIGTERM',
                       signal.SIGINT: 'SIGINT',
                       signal.SIGHUP: 'SIGHUP'}[exc.signo]
            LOG.info(_('Caught %s, exiting'), signame)
            status = exc.code
            signo = exc.signo
        except SystemExit as exc:
            status = exc.code
        except BaseException:
            LOG.exception(_('Unhandled exception'))
            status = 2
        finally:
            launcher.stop()

        return status, signo

    def _child_process(self, service):
        self._child_process_handle_signal()

        # Reopen the eventlet hub to make sure we don't share an epoll
        # fd with parent and/or siblings, which would be bad
        eventlet.hubs.use_hub()

        # Close write to ensure only parent has it open
        os.close(self.writepipe)
        # Create greenthread to watch for parent to close pipe
        eventlet.spawn_n(self._pipe_watcher)

        # Reseed random number generator
        random.seed()

        launcher = Launcher()
        launcher.launch_service(service)
        return launcher

    def _start_child(self, wrap):
        if len(wrap.forktimes) > wrap.workers:
            # Limit ourselves to one process a second (over the period of
            # number of workers * 1 second). This will allow workers to
            # start up quickly but ensure we don't fork off children that
            # die instantly too quickly.
            if time.time() - wrap.forktimes[0] < wrap.workers:
                LOG.info(_('Forking too fast, sleeping'))
                time.sleep(1)

            wrap.forktimes.pop(0)

        wrap.forktimes.append(time.time())

        pid = os.fork()
        if pid == 0:
            # NOTE(johannes): All exceptions are caught to ensure this
            # doesn't fallback into the loop spawning children. It would
            # be bad for a child to spawn more children.
            launcher = self._child_process(wrap.service)
            while True:
                self._child_process_handle_signal()
                status, signo = self._child_wait_for_exit_or_signal(launcher)
                if signo != signal.SIGHUP:
                    break
                launcher.restart()

            os._exit(status)

        LOG.info(_('Started child %d'), pid)

        wrap.children.add(pid)
        self.children[pid] = wrap

        return pid

    def launch_service(self, service, workers=1):
        wrap = ServiceWrapper(service, workers)

        LOG.info(_('Starting %d workers'), wrap.workers)
        while self.running and len(wrap.children) < wrap.workers:
            self._start_child(wrap)

    def _wait_child(self):
        try:
            # Don't block if no child processes have exited
            pid, status = os.waitpid(0, os.WNOHANG)
            if not pid:
                return None
        except OSError as exc:
            if exc.errno not in (errno.EINTR, errno.ECHILD):
                raise
            return None

        if os.WIFSIGNALED(status):
            sig = os.WTERMSIG(status)
            LOG.info(_('Child %(pid)d killed by signal %(sig)d'),
                     dict(pid=pid, sig=sig))
        else:
            code = os.WEXITSTATUS(status)
            LOG.info(_('Child %(pid)s exited with status %(code)d'),
                     dict(pid=pid, code=code))

        if pid not in self.children:
            LOG.warning(_('pid %d not in child list'), pid)
            return None

        wrap = self.children.pop(pid)
        wrap.children.remove(pid)
        return wrap

    def _respawn_children(self):
        while self.running:
            wrap = self._wait_child()
            if not wrap:
                # Yield to other threads if no children have exited
                # Sleep for a short time to avoid excessive CPU usage
                # (see bug #1095346)
                eventlet.greenthread.sleep(.01)
                continue
            while self.running and len(wrap.children) < wrap.workers:
                self._start_child(wrap)

    def wait(self):
        """Loop waiting on children to die and respawning as necessary."""

        LOG.debug(_('Full set of CONF:'))
        CONF.log_opt_values(LOG, std_logging.DEBUG)

        while True:
            self.handle_signal()
            self._respawn_children()
            if self.sigcaught:
                signame = {signal.SIGTERM: 'SIGTERM',
                           signal.SIGINT: 'SIGINT',
                           signal.SIGHUP: 'SIGHUP'}[self.sigcaught]
                LOG.info(_('Caught %s, stopping children'), signame)
            if self.sigcaught != signal.SIGHUP:
                break

            for pid in self.children:
                os.kill(pid, signal.SIGHUP)
            self.running = True
            self.sigcaught = None

        for pid in self.children:
            try:
                os.kill(pid, signal.SIGTERM)
            except OSError as exc:
                if exc.errno != errno.ESRCH:
                    raise

        # Wait for children to die
        if self.children:
            LOG.info(_('Waiting on %d children to exit'), len(self.children))
            while self.children:
                self._wait_child()


class Service(object):
    """Service object for binaries running on hosts."""

    def __init__(self, threads=1000):
        self.tg = threadgroup.ThreadGroup(threads)

        # signal that the service is done shutting itself down:
        self._done = event.Event()

    def reset(self):
        # NOTE(Fengqian): docs for Event.reset() recommend against using it
        self._done = event.Event()

    def start(self):
        pass

    def stop(self):
        self.tg.stop()
        self.tg.wait()
        # Signal that service cleanup is done:
        if not self._done.ready():
            self._done.send()

    def wait(self):
        self._done.wait()


class Services(object):

    def __init__(self):
        self.services = []
        self.tg = threadgroup.ThreadGroup()
        self.done = event.Event()

    def add(self, service):
        self.services.append(service)
        self.tg.add_thread(self.run_service, service, self.done)

    def stop(self):
        # wait for graceful shutdown of services:
        for service in self.services:
            service.stop()
            service.wait()

        # Each service has performed cleanup, now signal that the run_service
        # wrapper threads can now die:
        if not self.done.ready():
            self.done.send()

        # reap threads:
        self.tg.stop()

    def wait(self):
        self.tg.wait()

    def restart(self):
        self.stop()
        self.done = event.Event()
        for restart_service in self.services:
            restart_service.reset()
            self.tg.add_thread(self.run_service, restart_service, self.done)

    @staticmethod
    def run_service(service, done):
        """Service start wrapper.

        :param service: service to run
        :param done: event to wait on until a shutdown is triggered
        :returns: None

        """
        service.start()
        done.wait()


def launch(service, workers=None):
    if workers:
        launcher = ProcessLauncher()
        launcher.launch_service(service, workers=workers)
    else:
        launcher = ServiceLauncher()
        launcher.launch_service(service)
    return launcher

########NEW FILE########
__FILENAME__ = sslutils
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2013 IBM Corp.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import os
import ssl

from oslo.config import cfg

from trove.openstack.common.gettextutils import _  # noqa


ssl_opts = [
    cfg.StrOpt('ca_file',
               default=None,
               help="CA certificate file to use to verify "
                    "connecting clients"),
    cfg.StrOpt('cert_file',
               default=None,
               help="Certificate file to use when starting "
                    "the server securely"),
    cfg.StrOpt('key_file',
               default=None,
               help="Private key file to use when starting "
                    "the server securely"),
]


CONF = cfg.CONF
CONF.register_opts(ssl_opts, "ssl")


def is_enabled():
    cert_file = CONF.ssl.cert_file
    key_file = CONF.ssl.key_file
    ca_file = CONF.ssl.ca_file
    use_ssl = cert_file or key_file

    if cert_file and not os.path.exists(cert_file):
        raise RuntimeError(_("Unable to find cert_file : %s") % cert_file)

    if ca_file and not os.path.exists(ca_file):
        raise RuntimeError(_("Unable to find ca_file : %s") % ca_file)

    if key_file and not os.path.exists(key_file):
        raise RuntimeError(_("Unable to find key_file : %s") % key_file)

    if use_ssl and (not cert_file or not key_file):
        raise RuntimeError(_("When running server in SSL mode, you must "
                             "specify both a cert_file and key_file "
                             "option value in your configuration file"))

    return use_ssl


def wrap(sock):
    ssl_kwargs = {
        'server_side': True,
        'certfile': CONF.ssl.cert_file,
        'keyfile': CONF.ssl.key_file,
        'cert_reqs': ssl.CERT_NONE,
    }

    if CONF.ssl.ca_file:
        ssl_kwargs['ca_certs'] = CONF.ssl.ca_file
        ssl_kwargs['cert_reqs'] = ssl.CERT_REQUIRED

    return ssl.wrap_socket(sock, **ssl_kwargs)


_SSL_PROTOCOLS = {
    "tlsv1": ssl.PROTOCOL_TLSv1,
    "sslv23": ssl.PROTOCOL_SSLv23,
    "sslv3": ssl.PROTOCOL_SSLv3
}

try:
    _SSL_PROTOCOLS["sslv2"] = ssl.PROTOCOL_SSLv2
except AttributeError:
    pass


def validate_ssl_version(version):
    key = version.lower()
    try:
        return _SSL_PROTOCOLS[key]
    except KeyError:
        raise RuntimeError(_("Invalid SSL version : %s") % version)

########NEW FILE########
__FILENAME__ = testutils
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Utilities for unit tests."""

import functools
import nose


class skip_test(object):
    """Decorator that skips a test."""
    # TODO(tr3buchet): remember forever what comstud did here
    def __init__(self, msg):
        self.message = msg

    def __call__(self, func):
        @functools.wraps(func)
        def _skipper(*args, **kw):
            """Wrapped skipper function."""
            raise nose.SkipTest(self.message)
        return _skipper


class skip_if(object):
    """Decorator that skips a test if condition is true."""
    def __init__(self, condition, msg):
        self.condition = condition
        self.message = msg

    def __call__(self, func):
        @functools.wraps(func)
        def _skipper(*args, **kw):
            """Wrapped skipper function."""
            if self.condition:
                raise nose.SkipTest(self.message)
            func(*args, **kw)
        return _skipper


class skip_unless(object):
    """Decorator that skips a test if condition is not true."""
    def __init__(self, condition, msg):
        self.condition = condition
        self.message = msg

    def __call__(self, func):
        @functools.wraps(func)
        def _skipper(*args, **kw):
            """Wrapped skipper function."""
            if not self.condition:
                raise nose.SkipTest(self.message)
            func(*args, **kw)
        return _skipper

########NEW FILE########
__FILENAME__ = threadgroup
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2012 Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import eventlet
from eventlet import greenpool
from eventlet import greenthread

from trove.openstack.common import log as logging
from trove.openstack.common import loopingcall


LOG = logging.getLogger(__name__)


def _thread_done(gt, *args, **kwargs):
    """Callback function to be passed to GreenThread.link() when we spawn()
    Calls the :class:`ThreadGroup` to notify if.

    """
    kwargs['group'].thread_done(kwargs['thread'])


class Thread(object):
    """Wrapper around a greenthread, that holds a reference to the
    :class:`ThreadGroup`. The Thread will notify the :class:`ThreadGroup` when
    it has done so it can be removed from the threads list.
    """
    def __init__(self, thread, group):
        self.thread = thread
        self.thread.link(_thread_done, group=group, thread=self)

    def stop(self):
        self.thread.kill()

    def wait(self):
        return self.thread.wait()


class ThreadGroup(object):
    """The point of the ThreadGroup classis to:

    * keep track of timers and greenthreads (making it easier to stop them
      when need be).
    * provide an easy API to add timers.
    """
    def __init__(self, thread_pool_size=10):
        self.pool = greenpool.GreenPool(thread_pool_size)
        self.threads = []
        self.timers = []

    def add_dynamic_timer(self, callback, initial_delay=None,
                          periodic_interval_max=None, *args, **kwargs):
        timer = loopingcall.DynamicLoopingCall(callback, *args, **kwargs)
        timer.start(initial_delay=initial_delay,
                    periodic_interval_max=periodic_interval_max)
        self.timers.append(timer)

    def add_timer(self, interval, callback, initial_delay=None,
                  *args, **kwargs):
        pulse = loopingcall.FixedIntervalLoopingCall(callback, *args, **kwargs)
        pulse.start(interval=interval,
                    initial_delay=initial_delay)
        self.timers.append(pulse)

    def add_thread(self, callback, *args, **kwargs):
        gt = self.pool.spawn(callback, *args, **kwargs)
        th = Thread(gt, self)
        self.threads.append(th)

    def thread_done(self, thread):
        self.threads.remove(thread)

    def stop(self):
        current = greenthread.getcurrent()
        for x in self.threads:
            if x is current:
                # don't kill the current thread.
                continue
            try:
                x.stop()
            except Exception as ex:
                LOG.exception(ex)

        for x in self.timers:
            try:
                x.stop()
            except Exception as ex:
                LOG.exception(ex)
        self.timers = []

    def wait(self):
        for x in self.timers:
            try:
                x.wait()
            except eventlet.greenlet.GreenletExit:
                pass
            except Exception as ex:
                LOG.exception(ex)
        current = greenthread.getcurrent()
        for x in self.threads:
            if x is current:
                continue
            try:
                x.wait()
            except eventlet.greenlet.GreenletExit:
                pass
            except Exception as ex:
                LOG.exception(ex)

########NEW FILE########
__FILENAME__ = timeutils
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Time related utilities and helper functions.
"""

import calendar
import datetime

import iso8601
import six


# ISO 8601 extended time format with microseconds
_ISO8601_TIME_FORMAT_SUBSECOND = '%Y-%m-%dT%H:%M:%S.%f'
_ISO8601_TIME_FORMAT = '%Y-%m-%dT%H:%M:%S'
PERFECT_TIME_FORMAT = _ISO8601_TIME_FORMAT_SUBSECOND


def isotime(at=None, subsecond=False):
    """Stringify time in ISO 8601 format."""
    if not at:
        at = utcnow()
    st = at.strftime(_ISO8601_TIME_FORMAT
                     if not subsecond
                     else _ISO8601_TIME_FORMAT_SUBSECOND)
    tz = at.tzinfo.tzname(None) if at.tzinfo else 'UTC'
    st += ('Z' if tz == 'UTC' else tz)
    return st


def parse_isotime(timestr):
    """Parse time from ISO 8601 format."""
    try:
        return iso8601.parse_date(timestr)
    except iso8601.ParseError as e:
        raise ValueError(unicode(e))
    except TypeError as e:
        raise ValueError(unicode(e))


def strtime(at=None, fmt=PERFECT_TIME_FORMAT):
    """Returns formatted utcnow."""
    if not at:
        at = utcnow()
    return at.strftime(fmt)


def parse_strtime(timestr, fmt=PERFECT_TIME_FORMAT):
    """Turn a formatted time back into a datetime."""
    return datetime.datetime.strptime(timestr, fmt)


def normalize_time(timestamp):
    """Normalize time in arbitrary timezone to UTC naive object."""
    offset = timestamp.utcoffset()
    if offset is None:
        return timestamp
    return timestamp.replace(tzinfo=None) - offset


def is_older_than(before, seconds):
    """Return True if before is older than seconds."""
    if isinstance(before, six.string_types):
        before = parse_strtime(before).replace(tzinfo=None)
    return utcnow() - before > datetime.timedelta(seconds=seconds)


def is_newer_than(after, seconds):
    """Return True if after is newer than seconds."""
    if isinstance(after, six.string_types):
        after = parse_strtime(after).replace(tzinfo=None)
    return after - utcnow() > datetime.timedelta(seconds=seconds)


def utcnow_ts():
    """Timestamp version of our utcnow function."""
    return calendar.timegm(utcnow().timetuple())


def utcnow():
    """Overridable version of utils.utcnow."""
    if utcnow.override_time:
        try:
            return utcnow.override_time.pop(0)
        except AttributeError:
            return utcnow.override_time
    return datetime.datetime.utcnow()


def iso8601_from_timestamp(timestamp):
    """Returns a iso8601 formated date from timestamp."""
    return isotime(datetime.datetime.utcfromtimestamp(timestamp))


utcnow.override_time = None


def set_time_override(override_time=datetime.datetime.utcnow()):
    """Overrides utils.utcnow.

    Make it return a constant time or a list thereof, one at a time.
    """
    utcnow.override_time = override_time


def advance_time_delta(timedelta):
    """Advance overridden time using a datetime.timedelta."""
    assert(not utcnow.override_time is None)
    try:
        for dt in utcnow.override_time:
            dt += timedelta
    except TypeError:
        utcnow.override_time += timedelta


def advance_time_seconds(seconds):
    """Advance overridden time by seconds."""
    advance_time_delta(datetime.timedelta(0, seconds))


def clear_time_override():
    """Remove the overridden time."""
    utcnow.override_time = None


def marshall_now(now=None):
    """Make an rpc-safe datetime with microseconds.

    Note: tzinfo is stripped, but not required for relative times.
    """
    if not now:
        now = utcnow()
    return dict(day=now.day, month=now.month, year=now.year, hour=now.hour,
                minute=now.minute, second=now.second,
                microsecond=now.microsecond)


def unmarshall_time(tyme):
    """Unmarshall a datetime dict."""
    return datetime.datetime(day=tyme['day'],
                             month=tyme['month'],
                             year=tyme['year'],
                             hour=tyme['hour'],
                             minute=tyme['minute'],
                             second=tyme['second'],
                             microsecond=tyme['microsecond'])


def delta_seconds(before, after):
    """Return the difference between two timing objects.

    Compute the difference in seconds between two date, time, or
    datetime objects (as a float, to microsecond resolution).
    """
    delta = after - before
    try:
        return delta.total_seconds()
    except AttributeError:
        return ((delta.days * 24 * 3600) + delta.seconds +
                float(delta.microseconds) / (10 ** 6))


def is_soon(dt, window):
    """Determines if time is going to happen in the next window seconds.

    :params dt: the time
    :params window: minimum seconds to remain to consider the time not soon

    :return: True if expiration is within the given duration
    """
    soon = (utcnow() + datetime.timedelta(seconds=window))
    return normalize_time(dt) <= soon

########NEW FILE########
__FILENAME__ = utils
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2011 OpenStack LLC.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
System-level utilities and helper functions.
"""

import logging

LOG = logging.getLogger(__name__)


def int_from_bool_as_string(subject):
    """
    Interpret a string as a boolean and return either 1 or 0.

    Any string value in:

        ('True', 'true', 'On', 'on', '1')

    is interpreted as a boolean True.

    Useful for JSON-decoded stuff and config file parsing
    """
    return bool_from_string(subject) and 1 or 0


def bool_from_string(subject):
    """
    Interpret a string as a boolean.

    Any string value in:

        ('True', 'true', 'On', 'on', 'Yes', 'yes', '1')

    is interpreted as a boolean True.

    Useful for JSON-decoded stuff and config file parsing
    """
    if isinstance(subject, bool):
        return subject
    if isinstance(subject, basestring):
        if subject.strip().lower() in ('true', 'on', 'yes', '1'):
            return True
    return False

########NEW FILE########
__FILENAME__ = versionutils
# Copyright (c) 2013 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Helpers for comparing version strings.
"""

import functools
import pkg_resources

from trove.openstack.common.gettextutils import _
from trove.openstack.common import log as logging


LOG = logging.getLogger(__name__)


class deprecated(object):
    """A decorator to mark callables as deprecated.

    This decorator logs a deprecation message when the callable it decorates is
    used. The message will include the release where the callable was
    deprecated, the release where it may be removed and possibly an optional
    replacement.

    Examples:

    1. Specifying the required deprecated release

    >>> @deprecated(as_of=deprecated.ICEHOUSE)
    ... def a(): pass

    2. Specifying a replacement:

    >>> @deprecated(as_of=deprecated.ICEHOUSE, in_favor_of='f()')
    ... def b(): pass

    3. Specifying the release where the functionality may be removed:

    >>> @deprecated(as_of=deprecated.ICEHOUSE, remove_in=+1)
    ... def c(): pass

    """

    FOLSOM = 'F'
    GRIZZLY = 'G'
    HAVANA = 'H'
    ICEHOUSE = 'I'

    _RELEASES = {
        'F': 'Folsom',
        'G': 'Grizzly',
        'H': 'Havana',
        'I': 'Icehouse',
    }

    _deprecated_msg_with_alternative = _(
        '%(what)s is deprecated as of %(as_of)s in favor of '
        '%(in_favor_of)s and may be removed in %(remove_in)s.')

    _deprecated_msg_no_alternative = _(
        '%(what)s is deprecated as of %(as_of)s and may be '
        'removed in %(remove_in)s. It will not be superseded.')

    def __init__(self, as_of, in_favor_of=None, remove_in=2, what=None):
        """Initialize decorator

        :param as_of: the release deprecating the callable. Constants
            are define in this class for convenience.
        :param in_favor_of: the replacement for the callable (optional)
        :param remove_in: an integer specifying how many releases to wait
            before removing (default: 2)
        :param what: name of the thing being deprecated (default: the
            callable's name)

        """
        self.as_of = as_of
        self.in_favor_of = in_favor_of
        self.remove_in = remove_in
        self.what = what

    def __call__(self, func):
        if not self.what:
            self.what = func.__name__ + '()'

        @functools.wraps(func)
        def wrapped(*args, **kwargs):
            msg, details = self._build_message()
            LOG.deprecated(msg, details)
            return func(*args, **kwargs)
        return wrapped

    def _get_safe_to_remove_release(self, release):
        # TODO(dstanek): this method will have to be reimplemented once
        #    when we get to the X release because once we get to the Y
        #    release, what is Y+2?
        new_release = chr(ord(release) + self.remove_in)
        if new_release in self._RELEASES:
            return self._RELEASES[new_release]
        else:
            return new_release

    def _build_message(self):
        details = dict(what=self.what,
                       as_of=self._RELEASES[self.as_of],
                       remove_in=self._get_safe_to_remove_release(self.as_of))

        if self.in_favor_of:
            details['in_favor_of'] = self.in_favor_of
            msg = self._deprecated_msg_with_alternative
        else:
            msg = self._deprecated_msg_no_alternative
        return msg, details


def is_compatible(requested_version, current_version, same_major=True):
    """Determine whether `requested_version` is satisfied by
    `current_version`; in other words, `current_version` is >=
    `requested_version`.

    :param requested_version: version to check for compatibility
    :param current_version: version to check against
    :param same_major: if True, the major version must be identical between
        `requested_version` and `current_version`. This is used when a
        major-version difference indicates incompatibility between the two
        versions. Since this is the common-case in practice, the default is
        True.
    :returns: True if compatible, False if not
    """
    requested_parts = pkg_resources.parse_version(requested_version)
    current_parts = pkg_resources.parse_version(current_version)

    if same_major and (requested_parts[0] != current_parts[0]):
        return False

    return current_parts >= requested_parts

########NEW FILE########
__FILENAME__ = wsgi
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2011 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Utility methods for working with WSGI servers."""

from __future__ import print_function

import eventlet
eventlet.patcher.monkey_patch(all=False, socket=True)

import datetime
import errno
import socket
import sys
import time

import eventlet.wsgi
from oslo.config import cfg
import routes
import routes.middleware
#import six
import webob.dec
import webob.exc
from xml.dom import minidom
from xml.parsers import expat

from trove.openstack.common import exception
from trove.openstack.common.gettextutils import _
from trove.openstack.common import jsonutils
from trove.openstack.common import log as logging
from trove.openstack.common import service
from trove.openstack.common import sslutils
from trove.openstack.common import xmlutils

socket_opts = [
    cfg.IntOpt('backlog',
               default=4096,
               help="Number of backlog requests to configure the socket with"),
    cfg.IntOpt('tcp_keepidle',
               default=600,
               help="Sets the value of TCP_KEEPIDLE in seconds for each "
                    "server socket. Not supported on OS X."),
]

CONF = cfg.CONF
CONF.register_opts(socket_opts)

LOG = logging.getLogger(__name__)


def run_server(application, port, **kwargs):
    """Run a WSGI server with the given application."""
    sock = eventlet.listen(('0.0.0.0', port))
    eventlet.wsgi.server(sock, application, **kwargs)


class Service(service.Service):
    """
    Provides a Service API for wsgi servers.

    This gives us the ability to launch wsgi servers with the
    Launcher classes in service.py.
    """

    def __init__(self, application, port,
                 host='0.0.0.0', backlog=4096, threads=1000):
        self.application = application
        self._port = port
        self._host = host
        self._backlog = backlog if backlog else CONF.backlog
        self._socket = self._get_socket(host, port, self._backlog)
        super(Service, self).__init__(threads)

    def _get_socket(self, host, port, backlog):
        # TODO(dims): eventlet's green dns/socket module does not actually
        # support IPv6 in getaddrinfo(). We need to get around this in the
        # future or monitor upstream for a fix
        info = socket.getaddrinfo(host,
                                  port,
                                  socket.AF_UNSPEC,
                                  socket.SOCK_STREAM)[0]
        family = info[0]
        bind_addr = info[-1]

        sock = None
        retry_until = time.time() + 30
        while not sock and time.time() < retry_until:
            try:
                sock = eventlet.listen(bind_addr,
                                       backlog=backlog,
                                       family=family)
                if sslutils.is_enabled():
                    sock = sslutils.wrap(sock)

            except socket.error as err:
                if err.args[0] != errno.EADDRINUSE:
                    raise
                eventlet.sleep(0.1)
        if not sock:
            raise RuntimeError(_("Could not bind to %(host)s:%(port)s "
                               "after trying for 30 seconds") %
                               {'host': host, 'port': port})
        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        # sockets can hang around forever without keepalive
        sock.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)

        # This option isn't available in the OS X version of eventlet
        if hasattr(socket, 'TCP_KEEPIDLE'):
            sock.setsockopt(socket.IPPROTO_TCP,
                            socket.TCP_KEEPIDLE,
                            CONF.tcp_keepidle)

        return sock

    def start(self):
        """Start serving this service using the provided server instance.

        :returns: None

        """
        super(Service, self).start()
        self.tg.add_thread(self._run, self.application, self._socket)

    @property
    def backlog(self):
        return self._backlog

    @property
    def host(self):
        return self._socket.getsockname()[0] if self._socket else self._host

    @property
    def port(self):
        return self._socket.getsockname()[1] if self._socket else self._port

    def stop(self):
        """Stop serving this API.

        :returns: None

        """
        super(Service, self).stop()

    def _run(self, application, socket):
        """Start a WSGI server in a new green thread."""
        logger = logging.getLogger('eventlet.wsgi')
        eventlet.wsgi.server(socket,
                             application,
                             custom_pool=self.tg.pool,
                             log=logging.WritableLogger(logger))


class Middleware(object):
    """
    Base WSGI middleware wrapper. These classes require an application to be
    initialized that will be called next.  By default the middleware will
    simply call its wrapped app, or you can override __call__ to customize its
    behavior.
    """

    def __init__(self, application):
        self.application = application

    def process_request(self, req):
        """
        Called on each request.

        If this returns None, the next application down the stack will be
        executed. If it returns a response then that response will be returned
        and execution will stop here.
        """
        return None

    def process_response(self, response):
        """Do whatever you'd like to the response."""
        return response

    @webob.dec.wsgify
    def __call__(self, req):
        response = self.process_request(req)
        if response:
            return response
        response = req.get_response(self.application)
        return self.process_response(response)


class Debug(Middleware):
    """
    Helper class that can be inserted into any WSGI application chain
    to get information about the request and response.
    """

    @webob.dec.wsgify
    def __call__(self, req):
        print(("*" * 40) + " REQUEST ENVIRON")
        for key, value in req.environ.items():
            print(key, "=", value)
        print()
        resp = req.get_response(self.application)

        print(("*" * 40) + " RESPONSE HEADERS")
        for (key, value) in resp.headers.iteritems():
            print(key, "=", value)
        print()

        resp.app_iter = self.print_generator(resp.app_iter)

        return resp

    @staticmethod
    def print_generator(app_iter):
        """
        Iterator that prints the contents of a wrapper string iterator
        when iterated.
        """
        print(("*" * 40) + " BODY")
        for part in app_iter:
            sys.stdout.write(part)
            sys.stdout.flush()
            yield part
        print()


class Router(object):

    """
    WSGI middleware that maps incoming requests to WSGI apps.
    """

    def __init__(self, mapper):
        """
        Create a router for the given routes.Mapper.

        Each route in `mapper` must specify a 'controller', which is a
        WSGI app to call.  You'll probably want to specify an 'action' as
        well and have your controller be a wsgi.Controller, who will route
        the request to the action method.

        Examples:
          mapper = routes.Mapper()
          sc = ServerController()

          # Explicit mapping of one route to a controller+action
          mapper.connect(None, "/svrlist", controller=sc, action="list")

          # Actions are all implicitly defined
          mapper.resource("server", "servers", controller=sc)

          # Pointing to an arbitrary WSGI app.  You can specify the
          # {path_info:.*} parameter so the target app can be handed just that
          # section of the URL.
          mapper.connect(None, "/v1.0/{path_info:.*}", controller=BlogApp())
        """
        self.map = mapper
        self._router = routes.middleware.RoutesMiddleware(self._dispatch,
                                                          self.map)

    @webob.dec.wsgify
    def __call__(self, req):
        """
        Route the incoming request to a controller based on self.map.
        If no match, return a 404.
        """
        return self._router

    @staticmethod
    @webob.dec.wsgify
    def _dispatch(req):
        """
        Called by self._router after matching the incoming request to a route
        and putting the information into req.environ.  Either returns 404
        or the routed WSGI app's response.
        """
        match = req.environ['wsgiorg.routing_args'][1]
        if not match:
            return webob.exc.HTTPNotFound()
        app = match['controller']
        return app


class Request(webob.Request):
    """Add some Openstack API-specific logic to the base webob.Request."""

    default_request_content_types = ('application/json', 'application/xml')
    default_accept_types = ('application/json', 'application/xml')
    default_accept_type = 'application/json'

    def best_match_content_type(self, supported_content_types=None):
        """Determine the requested response content-type.

        Based on the query extension then the Accept header.
        Defaults to default_accept_type if we don't find a preference

        """
        supported_content_types = (supported_content_types or
                                   self.default_accept_types)

        parts = self.path.rsplit('.', 1)
        if len(parts) > 1:
            ctype = 'application/{0}'.format(parts[1])
            if ctype in supported_content_types:
                return ctype

        bm = self.accept.best_match(supported_content_types)
        return bm or self.default_accept_type

    def get_content_type(self, allowed_content_types=None):
        """Determine content type of the request body.

        Does not do any body introspection, only checks header

        """
        if "Content-Type" not in self.headers:
            return None

        content_type = self.content_type
        allowed_content_types = (allowed_content_types or
                                 self.default_request_content_types)

        if content_type not in allowed_content_types:
            raise exception.InvalidContentType(content_type=content_type)
        return content_type


class Resource(object):
    """
    WSGI app that handles (de)serialization and controller dispatch.

    Reads routing information supplied by RoutesMiddleware and calls
    the requested action method upon its deserializer, controller,
    and serializer. Those three objects may implement any of the basic
    controller action methods (create, update, show, index, delete)
    along with any that may be specified in the api router. A 'default'
    method may also be implemented to be used in place of any
    non-implemented actions. Deserializer methods must accept a request
    argument and return a dictionary. Controller methods must accept a
    request argument. Additionally, they must also accept keyword
    arguments that represent the keys returned by the Deserializer. They
    may raise a webob.exc exception or return a dict, which will be
    serialized by requested content type.
    """
    def __init__(self, controller, deserializer=None, serializer=None):
        """
        :param controller: object that implement methods created by routes lib
        :param deserializer: object that supports webob request deserialization
                             through controller-like actions
        :param serializer: object that supports webob response serialization
                           through controller-like actions
        """
        self.controller = controller
        self.serializer = serializer or ResponseSerializer()
        self.deserializer = deserializer or RequestDeserializer()

    @webob.dec.wsgify(RequestClass=Request)
    def __call__(self, request):
        """WSGI method that controls (de)serialization and method dispatch."""

        try:
            action, action_args, accept = self.deserialize_request(request)
        except exception.InvalidContentType:
            msg = _("Unsupported Content-Type")
            return webob.exc.HTTPUnsupportedMediaType(explanation=msg)
        except exception.MalformedRequestBody:
            msg = _("Malformed request body")
            return webob.exc.HTTPBadRequest(explanation=msg)

        action_result = self.execute_action(action, request, **action_args)
        try:
            return self.serialize_response(action, action_result, accept)
        # return unserializable result (typically a webob exc)
        except Exception:
            return action_result

    def deserialize_request(self, request):
        return self.deserializer.deserialize(request)

    def serialize_response(self, action, action_result, accept):
        return self.serializer.serialize(action_result, accept, action)

    def execute_action(self, action, request, **action_args):
        return self.dispatch(self.controller, action, request, **action_args)

    def dispatch(self, obj, action, *args, **kwargs):
        """Find action-specific method on self and call it."""
        try:
            method = getattr(obj, action)
        except AttributeError:
            method = getattr(obj, 'default')

        return method(*args, **kwargs)

    def get_action_args(self, request_environment):
        """Parse dictionary created by routes library."""
        try:
            args = request_environment['wsgiorg.routing_args'][1].copy()
        except Exception:
            return {}

        try:
            del args['controller']
        except KeyError:
            pass

        try:
            del args['format']
        except KeyError:
            pass

        return args


class ActionDispatcher(object):
    """Maps method name to local methods through action name."""

    def dispatch(self, *args, **kwargs):
        """Find and call local method."""
        action = kwargs.pop('action', 'default')
        action_method = getattr(self, str(action), self.default)
        return action_method(*args, **kwargs)

    def default(self, data):
        raise NotImplementedError()


class DictSerializer(ActionDispatcher):
    """Default request body serialization"""

    def serialize(self, data, action='default'):
        return self.dispatch(data, action=action)

    def default(self, data):
        return ""


class JSONDictSerializer(DictSerializer):
    """Default JSON request body serialization"""

    def default(self, data):
        def sanitizer(obj):
            if isinstance(obj, datetime.datetime):
                _dtime = obj - datetime.timedelta(microseconds=obj.microsecond)
                return _dtime.isoformat()
            return obj
#            return six.text_type(obj)
        return jsonutils.dumps(data, default=sanitizer)


class XMLDictSerializer(DictSerializer):

    def __init__(self, metadata=None, xmlns=None):
        """
        :param metadata: information needed to deserialize xml into
                         a dictionary.
        :param xmlns: XML namespace to include with serialized xml
        """
        super(XMLDictSerializer, self).__init__()
        self.metadata = metadata or {}
        self.xmlns = xmlns

    def default(self, data):
        # We expect data to contain a single key which is the XML root.
        root_key = data.keys()[0]
        doc = minidom.Document()
        node = self._to_xml_node(doc, self.metadata, root_key, data[root_key])

        return self.to_xml_string(node)

    def to_xml_string(self, node, has_atom=False):
        self._add_xmlns(node, has_atom)
        return node.toprettyxml(indent='    ', encoding='UTF-8')

    #NOTE (ameade): the has_atom should be removed after all of the
    # xml serializers and view builders have been updated to the current
    # spec that required all responses include the xmlns:atom, the has_atom
    # flag is to prevent current tests from breaking
    def _add_xmlns(self, node, has_atom=False):
        if self.xmlns is not None:
            node.setAttribute('xmlns', self.xmlns)
        if has_atom:
            node.setAttribute('xmlns:atom', "http://www.w3.org/2005/Atom")

    def _to_xml_node(self, doc, metadata, nodename, data):
        """Recursive method to convert data members to XML nodes."""
        result = doc.createElement(nodename)

        # Set the xml namespace if one is specified
        # TODO(justinsb): We could also use prefixes on the keys
        xmlns = metadata.get('xmlns', None)
        if xmlns:
            result.setAttribute('xmlns', xmlns)

        #TODO(bcwaldon): accomplish this without a type-check
        if type(data) is list:
            collections = metadata.get('list_collections', {})
            if nodename in collections:
                metadata = collections[nodename]
                for item in data:
                    node = doc.createElement(metadata['item_name'])
                    node.setAttribute(metadata['item_key'], str(item))
                    result.appendChild(node)
                return result
            singular = metadata.get('plurals', {}).get(nodename, None)
            if singular is None:
                if nodename.endswith('s'):
                    singular = nodename[:-1]
                else:
                    singular = 'item'
            for item in data:
                node = self._to_xml_node(doc, metadata, singular, item)
                result.appendChild(node)
        #TODO(bcwaldon): accomplish this without a type-check
        elif type(data) is dict:
            collections = metadata.get('dict_collections', {})
            if nodename in collections:
                metadata = collections[nodename]
                for k, v in data.items():
                    node = doc.createElement(metadata['item_name'])
                    node.setAttribute(metadata['item_key'], str(k))
                    text = doc.createTextNode(str(v))
                    node.appendChild(text)
                    result.appendChild(node)
                return result
            attrs = metadata.get('attributes', {}).get(nodename, {})
            for k, v in data.items():
                if k in attrs:
                    result.setAttribute(k, str(v))
                else:
                    node = self._to_xml_node(doc, metadata, k, v)
                    result.appendChild(node)
        else:
            # Type is atom
            node = doc.createTextNode(str(data))
            result.appendChild(node)
        return result

    def _create_link_nodes(self, xml_doc, links):
        link_nodes = []
        for link in links:
            link_node = xml_doc.createElement('atom:link')
            link_node.setAttribute('rel', link['rel'])
            link_node.setAttribute('href', link['href'])
            if 'type' in link:
                link_node.setAttribute('type', link['type'])
            link_nodes.append(link_node)
        return link_nodes


class ResponseHeadersSerializer(ActionDispatcher):
    """Default response headers serialization"""

    def serialize(self, response, data, action):
        self.dispatch(response, data, action=action)

    def default(self, response, data):
        response.status_int = 200


class ResponseSerializer(object):
    """Encode the necessary pieces into a response object"""

    def __init__(self, body_serializers=None, headers_serializer=None):
        self.body_serializers = {
            'application/xml': XMLDictSerializer(),
            'application/json': JSONDictSerializer(),
        }
        self.body_serializers.update(body_serializers or {})

        self.headers_serializer = (headers_serializer or
                                   ResponseHeadersSerializer())

    def serialize(self, response_data, content_type, action='default'):
        """Serialize a dict into a string and wrap in a wsgi.Request object.

        :param response_data: dict produced by the Controller
        :param content_type: expected mimetype of serialized response body

        """
        response = webob.Response()
        self.serialize_headers(response, response_data, action)
        self.serialize_body(response, response_data, content_type, action)
        return response

    def serialize_headers(self, response, data, action):
        self.headers_serializer.serialize(response, data, action)

    def serialize_body(self, response, data, content_type, action):
        response.headers['Content-Type'] = content_type
        if data is not None:
            serializer = self.get_body_serializer(content_type)
            response.body = serializer.serialize(data, action)

    def get_body_serializer(self, content_type):
        try:
            return self.body_serializers[content_type]
        except (KeyError, TypeError):
            raise exception.InvalidContentType(content_type=content_type)


class RequestHeadersDeserializer(ActionDispatcher):
    """Default request headers deserializer"""

    def deserialize(self, request, action):
        return self.dispatch(request, action=action)

    def default(self, request):
        return {}


class RequestDeserializer(object):
    """Break up a Request object into more useful pieces."""

    def __init__(self, body_deserializers=None, headers_deserializer=None,
                 supported_content_types=None):

        self.supported_content_types = supported_content_types

        self.body_deserializers = {
            'application/xml': XMLDeserializer(),
            'application/json': JSONDeserializer(),
        }
        self.body_deserializers.update(body_deserializers or {})

        self.headers_deserializer = (headers_deserializer or
                                     RequestHeadersDeserializer())

    def deserialize(self, request):
        """Extract necessary pieces of the request.

        :param request: Request object
        :returns: tuple of (expected controller action name, dictionary of
                  keyword arguments to pass to the controller, the expected
                  content type of the response)

        """
        action_args = self.get_action_args(request.environ)
        action = action_args.pop('action', None)

        action_args.update(self.deserialize_headers(request, action))
        action_args.update(self.deserialize_body(request, action))

        accept = self.get_expected_content_type(request)

        return (action, action_args, accept)

    def deserialize_headers(self, request, action):
        return self.headers_deserializer.deserialize(request, action)

    def deserialize_body(self, request, action):
        if not len(request.body) > 0:
            LOG.debug(_("Empty body provided in request"))
            return {}

        try:
            content_type = request.get_content_type()
        except exception.InvalidContentType:
            LOG.debug(_("Unrecognized Content-Type provided in request"))
            raise

        if content_type is None:
            LOG.debug(_("No Content-Type provided in request"))
            return {}

        try:
            deserializer = self.get_body_deserializer(content_type)
        except exception.InvalidContentType:
            LOG.debug(_("Unable to deserialize body as provided Content-Type"))
            raise

        return deserializer.deserialize(request.body, action)

    def get_body_deserializer(self, content_type):
        try:
            return self.body_deserializers[content_type]
        except (KeyError, TypeError):
            raise exception.InvalidContentType(content_type=content_type)

    def get_expected_content_type(self, request):
        return request.best_match_content_type(self.supported_content_types)

    def get_action_args(self, request_environment):
        """Parse dictionary created by routes library."""
        try:
            args = request_environment['wsgiorg.routing_args'][1].copy()
        except Exception:
            return {}

        try:
            del args['controller']
        except KeyError:
            pass

        try:
            del args['format']
        except KeyError:
            pass

        return args


class TextDeserializer(ActionDispatcher):
    """Default request body deserialization"""

    def deserialize(self, datastring, action='default'):
        return self.dispatch(datastring, action=action)

    def default(self, datastring):
        return {}


class JSONDeserializer(TextDeserializer):

    def _from_json(self, datastring):
        try:
            return jsonutils.loads(datastring)
        except ValueError:
            msg = _("cannot understand JSON")
            raise exception.MalformedRequestBody(reason=msg)

    def default(self, datastring):
        return {'body': self._from_json(datastring)}


class XMLDeserializer(TextDeserializer):

    def __init__(self, metadata=None):
        """
        :param metadata: information needed to deserialize xml into
                         a dictionary.
        """
        super(XMLDeserializer, self).__init__()
        self.metadata = metadata or {}

    def _from_xml(self, datastring):
        plurals = set(self.metadata.get('plurals', {}))

        try:
            node = xmlutils.safe_minidom_parse_string(datastring).childNodes[0]
            return {node.nodeName: self._from_xml_node(node, plurals)}
        except expat.ExpatError:
            msg = _("cannot understand XML")
            raise exception.MalformedRequestBody(reason=msg)

    def _from_xml_node(self, node, listnames):
        """Convert a minidom node to a simple Python type.

        :param listnames: list of XML node names whose subnodes should
                          be considered list items.

        """

        if len(node.childNodes) == 1 and node.childNodes[0].nodeType == 3:
            return node.childNodes[0].nodeValue
        elif node.nodeName in listnames:
            return [self._from_xml_node(n, listnames) for n in node.childNodes]
        else:
            result = dict()
            for attr in node.attributes.keys():
                result[attr] = node.attributes[attr].nodeValue
            for child in node.childNodes:
                if child.nodeType != node.TEXT_NODE:
                    result[child.nodeName] = self._from_xml_node(child,
                                                                 listnames)
            return result

    def find_first_child_named(self, parent, name):
        """Search a nodes children for the first child with a given name"""
        for node in parent.childNodes:
            if node.nodeName == name:
                return node
        return None

    def find_children_named(self, parent, name):
        """Return all of a nodes children who have the given name"""
        for node in parent.childNodes:
            if node.nodeName == name:
                yield node

    def extract_text(self, node):
        """Get the text field contained by the given node"""
        if len(node.childNodes) == 1:
            child = node.childNodes[0]
            if child.nodeType == child.TEXT_NODE:
                return child.nodeValue
        return ""

    def default(self, datastring):
        return {'body': self._from_xml(datastring)}

########NEW FILE########
__FILENAME__ = xmlutils
# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2013 IBM Corp.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from xml.dom import minidom
from xml.parsers import expat
from xml import sax
from xml.sax import expatreader


class ProtectedExpatParser(expatreader.ExpatParser):
    """An expat parser which disables DTD's and entities by default."""

    def __init__(self, forbid_dtd=True, forbid_entities=True,
                 *args, **kwargs):
        # Python 2.x old style class
        expatreader.ExpatParser.__init__(self, *args, **kwargs)
        self.forbid_dtd = forbid_dtd
        self.forbid_entities = forbid_entities

    def start_doctype_decl(self, name, sysid, pubid, has_internal_subset):
        raise ValueError("Inline DTD forbidden")

    def entity_decl(self, entityName, is_parameter_entity, value, base,
                    systemId, publicId, notationName):
        raise ValueError("<!ENTITY> entity declaration forbidden")

    def unparsed_entity_decl(self, name, base, sysid, pubid, notation_name):
        # expat 1.2
        raise ValueError("<!ENTITY> unparsed entity forbidden")

    def external_entity_ref(self, context, base, systemId, publicId):
        raise ValueError("<!ENTITY> external entity forbidden")

    def notation_decl(self, name, base, sysid, pubid):
        raise ValueError("<!ENTITY> notation forbidden")

    def reset(self):
        expatreader.ExpatParser.reset(self)
        if self.forbid_dtd:
            self._parser.StartDoctypeDeclHandler = self.start_doctype_decl
            self._parser.EndDoctypeDeclHandler = None
        if self.forbid_entities:
            self._parser.EntityDeclHandler = self.entity_decl
            self._parser.UnparsedEntityDeclHandler = self.unparsed_entity_decl
            self._parser.ExternalEntityRefHandler = self.external_entity_ref
            self._parser.NotationDeclHandler = self.notation_decl
            try:
                self._parser.SkippedEntityHandler = None
            except AttributeError:
                # some pyexpat versions do not support SkippedEntity
                pass


def safe_minidom_parse_string(xml_string):
    """Parse an XML string using minidom safely.

    """
    try:
        return minidom.parseString(xml_string, parser=ProtectedExpatParser())
    except sax.SAXParseException:
        raise expat.ExpatError()

########NEW FILE########
__FILENAME__ = models
#    Copyright 2011 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


from trove.common import cfg
from trove.common import utils
from trove.db import models as dbmodels
from trove.openstack.common import log as logging

LOG = logging.getLogger(__name__)

CONF = cfg.CONF


def enum(**enums):
    return type('Enum', (), enums)


class Quota(dbmodels.DatabaseModelBase):
    """Defines the base model class for a quota."""

    _data_fields = ['created', 'updated', 'tenant_id', 'resource',
                    'hard_limit', 'id']

    def __init__(self, tenant_id, resource, hard_limit,
                 id=utils.generate_uuid(), created=utils.utcnow(),
                 update=utils.utcnow()):
        self.tenant_id = tenant_id
        self.resource = resource
        self.hard_limit = hard_limit
        self.id = id
        self.created = created
        self.update = update


class QuotaUsage(dbmodels.DatabaseModelBase):
    """Defines the quota usage for a tenant."""

    _data_fields = ['created', 'updated', 'tenant_id', 'resource',
                    'in_use', 'reserved', 'id']


class Reservation(dbmodels.DatabaseModelBase):
    """Defines the reservation for a quota."""

    _data_fields = ['created', 'updated', 'usage_id',
                    'id', 'delta', 'status']

    Statuses = enum(NEW='New',
                    RESERVED='Reserved',
                    COMMITTED='Committed',
                    ROLLEDBACK='Rolled Back')


def persisted_models():
    return {
        'quotas': Quota,
        'quota_usages': QuotaUsage,
        'reservations': Reservation,
    }


class Resource(object):
    """Describe a single resource for quota checking."""

    INSTANCES = 'instances'
    VOLUMES = 'volumes'
    BACKUPS = 'backups'

    def __init__(self, name, flag=None):
        """
        Initializes a Resource.

        :param name: The name of the resource, i.e., "volumes".
        :param flag: The name of the flag or configuration option
                     which specifies the default value of the quota
                     for this resource.
        """

        self.name = name
        self.flag = flag

    def __str__(self):
        return self.name

    def __hash__(self):
        return hash(self.name)

    def __eq__(self, other):
        return (isinstance(other, Resource) and
                self.name == other.name and
                self.flag == other.flag)

    @property
    def default(self):
        """Return the default value of the quota."""

        return CONF[self.flag] if self.flag is not None else -1

########NEW FILE########
__FILENAME__ = quota
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Quotas for DB instances and resources."""

from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _
from oslo.config import cfg
from trove.common import exception
from trove.openstack.common import importutils
from trove.quota.models import Quota
from trove.quota.models import QuotaUsage
from trove.quota.models import Reservation
from trove.quota.models import Resource

LOG = logging.getLogger(__name__)
CONF = cfg.CONF


class DbQuotaDriver(object):
    """
    Driver to perform necessary checks to enforce quotas and obtain
    quota information.  The default driver utilizes the local
    database.
    """

    def __init__(self, resources):
        self.resources = resources

    def get_quota_by_tenant(self, tenant_id, resource):
        """Get a specific quota by tenant."""

        quotas = Quota.find_all(tenant_id=tenant_id, resource=resource).all()
        if len(quotas) == 0:
            return Quota(tenant_id, resource, self.resources[resource].default)

        return quotas[0]

    def get_all_quotas_by_tenant(self, tenant_id, resources):
        """
        Retrieve the quotas for the given tenant.

        :param resources: A list of the registered resource to get.
        :param tenant_id: The ID of the tenant to return quotas for.
        """

        all_quotas = Quota.find_all(tenant_id=tenant_id).all()
        result_quotas = dict((quota.resource, quota)
                             for quota in all_quotas
                             if quota.resource in resources)

        if len(result_quotas) != len(resources):
            for resource in resources:
                # Not in the DB, return default value
                if resource not in result_quotas:
                    quota = Quota(tenant_id,
                                  resource,
                                  self.resources[resource].default)
                    result_quotas[resource] = quota

        return result_quotas

    def get_quota_usage_by_tenant(self, tenant_id, resource):
        """Get a specific quota usage by tenant."""

        quotas = QuotaUsage.find_all(tenant_id=tenant_id,
                                     resource=resource).all()
        if len(quotas) == 0:
            return QuotaUsage.create(tenant_id=tenant_id,
                                     in_use=0,
                                     reserved=0,
                                     resource=resource)
        return quotas[0]

    def get_all_quota_usages_by_tenant(self, tenant_id, resources):
        """
        Retrieve the quota usagess for the given tenant.

        :param tenant_id: The ID of the tenant to return quotas for.
        :param resources: A list of the registered resources to get.
        """

        all_usages = QuotaUsage.find_all(tenant_id=tenant_id).all()
        result_usages = dict((usage.resource, usage)
                             for usage in all_usages
                             if usage.resource in resources)
        if len(result_usages) != len(resources):
            for resource in resources:
                # Not in the DB, return default value
                if resource not in result_usages:
                    usage = QuotaUsage.create(tenant_id=tenant_id,
                                              in_use=0,
                                              reserved=0,
                                              resource=resource)
                    result_usages[resource] = usage

        return result_usages

    def get_defaults(self, resources):
        """Given a list of resources, retrieve the default quotas.

        :param resources: A list of the registered resources.
        """

        quotas = {}
        for resource in resources.values():
            quotas[resource.name] = resource.default

        return quotas

    def reserve(self, tenant_id, resources, deltas):
        """Check quotas and reserve resources for a tenant.

        This method checks quotas against current usage,
        reserved resources and the desired deltas.

        If any of the proposed values is over the defined quota, an
        QuotaExceeded exception will be raised with the sorted list of the
        resources which are too high.  Otherwise, the method returns a
        list of reservation objects which were created.

        :param tenant_id: The ID of the tenant reserving the resources.
        :param resources: A dictionary of the registered resources.
        :param deltas: A dictionary of the proposed delta changes.
        """

        unregistered_resources = [delta for delta in deltas
                                  if delta not in resources]
        if unregistered_resources:
            raise exception.QuotaResourceUnknown(unknown=
                                                 unregistered_resources)

        quotas = self.get_all_quotas_by_tenant(tenant_id, deltas.keys())
        quota_usages = self.get_all_quota_usages_by_tenant(tenant_id,
                                                           deltas.keys())

        overs = [resource for resource in deltas
                 if (int(deltas[resource]) > 0 and
                    (quota_usages[resource].in_use +
                     quota_usages[resource].reserved +
                     int(deltas[resource])) > quotas[resource].hard_limit)]

        if overs:
            raise exception.QuotaExceeded(overs=sorted(overs))

        reservations = []
        for resource in deltas:
            reserved = deltas[resource]
            usage = quota_usages[resource]
            usage.reserved += reserved
            usage.save()

            resv = Reservation.create(usage_id=usage.id,
                                      delta=reserved,
                                      status=Reservation.Statuses.RESERVED)
            reservations.append(resv)

        return reservations

    def commit(self, reservations):
        """Commit reservations.

        :param reservations: A list of the reservation UUIDs, as
                             returned by the reserve() method.
        """

        for reservation in reservations:
            usage = QuotaUsage.find_by(id=reservation.usage_id)
            usage.in_use += reservation.delta
            if usage.in_use < 0:
                usage.in_use = 0
            usage.reserved -= reservation.delta
            reservation.status = Reservation.Statuses.COMMITTED
            usage.save()
            reservation.save()

    def rollback(self, reservations):
        """Roll back reservations.

        :param reservations: A list of the reservation UUIDs, as
                             returned by the reserve() method.
        """

        for reservation in reservations:
            usage = QuotaUsage.find_by(id=reservation.usage_id)
            usage.reserved -= reservation.delta
            reservation.status = Reservation.Statuses.ROLLEDBACK
            usage.save()
            reservation.save()


class QuotaEngine(object):
    """Represent the set of recognized quotas."""

    def __init__(self, quota_driver_class=None):
        """Initialize a Quota object."""

        self._resources = {}

        if not quota_driver_class:
            quota_driver_class = CONF.quota_driver
        if isinstance(quota_driver_class, basestring):
            quota_driver_class = importutils.import_object(quota_driver_class,
                                                           self._resources)
        self._driver = quota_driver_class

    def __contains__(self, resource):
        return resource in self._resources

    def register_resource(self, resource):
        """Register a resource."""

        self._resources[resource.name] = resource

    def register_resources(self, resources):
        """Register a dictionary of resources."""

        for resource in resources:
            self.register_resource(resource)

    def get_quota_by_tenant(self, tenant_id, resource):
        """Get a specific quota by tenant."""

        return self._driver.get_quota_by_tenant(tenant_id, resource)

    def get_defaults(self):
        """Retrieve the default quotas."""

        return self._driver.get_defaults(self._resources)

    def get_all_quotas_by_tenant(self, tenant_id):
        """Retrieve the quotas for the given tenant.

        :param tenant_id: The ID of the tenant to return quotas for.
        """

        return self._driver.get_all_quotas_by_tenant(tenant_id,
                                                     self._resources)

    def reserve(self, tenant_id, **deltas):
        """Check quotas and reserve resources.

        For counting quotas--those quotas for which there is a usage
        synchronization function--this method checks quotas against
        current usage and the desired deltas.  The deltas are given as
        keyword arguments, and current usage and other reservations
        are factored into the quota check.

        This method will raise a QuotaResourceUnknown exception if a
        given resource is unknown or if it does not have a usage
        synchronization function.

        If any of the proposed values is over the defined quota, an
        QuotaExceeded exception will be raised with the sorted list of the
        resources which are too high.  Otherwise, the method returns a
        list of reservation UUIDs which were created.

        :param tenant_id: The ID of the tenant to reserve quotas for.
        """

        reservations = self._driver.reserve(tenant_id, self._resources, deltas)

        LOG.debug("Created reservations %(reservations)s" %
                  {'reservations': reservations})

        return reservations

    def commit(self, reservations):
        """Commit reservations.

        :param reservations: A list of the reservation UUIDs, as
                             returned by the reserve() method.
        """

        try:
            self._driver.commit(reservations)
        except Exception:
            LOG.exception(_("Failed to commit reservations "
                          "%(reservations)s") % {'reservations': reservations})

    def rollback(self, reservations):
        """Roll back reservations.

        :param reservations: A list of the reservation UUIDs, as
                             returned by the reserve() method.
        """

        try:
            self._driver.rollback(reservations)
        except Exception:
            LOG.exception(_("Failed to roll back reservations "
                          "%(reservations)s") % {'reservations': reservations})

    @property
    def resources(self):
        return sorted(self._resources.keys())


QUOTAS = QuotaEngine()

''' Define all kind of resources here '''
resources = [Resource(Resource.INSTANCES, 'max_instances_per_user'),
             Resource(Resource.BACKUPS, 'max_backups_per_user')]
if CONF.trove_volume_support:
    resources.append(Resource(Resource.VOLUMES, 'max_volumes_per_user'))

QUOTAS.register_resources(resources)


def run_with_quotas(tenant_id, deltas, f):
    """Quota wrapper """

    reservations = QUOTAS.reserve(tenant_id, **deltas)
    result = None
    try:
        result = f()
    except Exception:
        QUOTAS.rollback(reservations)
        raise
    else:
        QUOTAS.commit(reservations)
    return result

########NEW FILE########
__FILENAME__ = api
# Copyright 2012 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


"""
Routes all the requests to the task manager.
"""


from trove.common import cfg
from trove.openstack.common.rpc import proxy
from trove.openstack.common import log as logging


CONF = cfg.CONF
LOG = logging.getLogger(__name__)
RPC_API_VERSION = "1.0"


class API(proxy.RpcProxy):
    """API for interacting with the task manager."""

    def __init__(self, context):
        self.context = context
        super(API, self).__init__(self._get_routing_key(),
                                  RPC_API_VERSION)

    def _transform_obj(self, obj_ref):
        # Turn the object into a dictionary and remove the mgr
        if "__dict__" in dir(obj_ref):
            obj_dict = obj_ref.__dict__
            # We assume manager contains a object due to the *clients
            if obj_dict.get('manager'):
                del obj_dict['manager']
            return obj_dict
        raise ValueError("Could not transform %s" % obj_ref)

    def _get_routing_key(self):
        """Create the routing key for the taskmanager"""
        return CONF.taskmanager_queue

    def resize_volume(self, new_size, instance_id):
        LOG.debug("Making async call to resize volume for instance: %s"
                  % instance_id)
        self.cast(self.context, self.make_msg("resize_volume",
                                              new_size=new_size,
                                              instance_id=instance_id))

    def resize_flavor(self, instance_id, old_flavor, new_flavor):
        LOG.debug("Making async call to resize flavor for instance: %s" %
                  instance_id)
        self.cast(self.context,
                  self.make_msg("resize_flavor",
                                instance_id=instance_id,
                                old_flavor=self._transform_obj(old_flavor),
                                new_flavor=self._transform_obj(new_flavor)))

    def reboot(self, instance_id):
        LOG.debug("Making async call to reboot instance: %s" % instance_id)
        self.cast(self.context,
                  self.make_msg("reboot", instance_id=instance_id))

    def restart(self, instance_id):
        LOG.debug("Making async call to restart instance: %s" % instance_id)
        self.cast(self.context,
                  self.make_msg("restart", instance_id=instance_id))

    def migrate(self, instance_id, host):
        LOG.debug("Making async call to migrate instance: %s" % instance_id)
        self.cast(self.context,
                  self.make_msg("migrate", instance_id=instance_id, host=host))

    def delete_instance(self, instance_id):
        LOG.debug("Making async call to delete instance: %s" % instance_id)
        self.cast(self.context,
                  self.make_msg("delete_instance", instance_id=instance_id))

    def create_backup(self, backup_info, instance_id):
        LOG.debug("Making async call to create a backup for instance: %s" %
                  instance_id)
        self.cast(self.context, self.make_msg("create_backup",
                                              backup_info=backup_info,
                                              instance_id=instance_id))

    def delete_backup(self, backup_id):
        LOG.debug("Making async call to delete backup: %s" % backup_id)
        self.cast(self.context, self.make_msg("delete_backup",
                                              backup_id=backup_id))

    def create_instance(self, instance_id, name, flavor,
                        image_id, databases, users, datastore_manager,
                        packages, volume_size, backup_id=None,
                        availability_zone=None, root_password=None,
                        nics=None, overrides=None):
        LOG.debug("Making async call to create instance %s " % instance_id)
        self.cast(self.context,
                  self.make_msg("create_instance",
                                instance_id=instance_id, name=name,
                                flavor=self._transform_obj(flavor),
                                image_id=image_id,
                                databases=databases,
                                users=users,
                                datastore_manager=
                                datastore_manager,
                                packages=packages,
                                volume_size=volume_size,
                                backup_id=backup_id,
                                availability_zone=availability_zone,
                                root_password=root_password,
                                nics=nics,
                                overrides=overrides))

    def update_overrides(self, instance_id, overrides=None):
        LOG.debug("Making async call to update configuration overrides for "
                  "instance %s" % instance_id)

        self.cast(self.context,
                  self.make_msg("update_overrides",
                                instance_id=instance_id,
                                overrides=overrides))

    def unassign_configuration(self, instance_id, flavor, configuration_id):
        LOG.debug("Making async call to unassign configuration for "
                  "instance %s" % instance_id)

        self.cast(self.context,
                  self.make_msg("unassign_configuration",
                                instance_id=instance_id,
                                flavor=self._transform_obj(flavor),
                                configuration_id=configuration_id))

########NEW FILE########
__FILENAME__ = manager
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from trove.common.context import TroveContext

import trove.extensions.mgmt.instances.models as mgmtmodels
import trove.common.cfg as cfg
from trove.common import exception
from trove.openstack.common import log as logging
from trove.openstack.common import importutils
from trove.openstack.common import periodic_task
from trove.taskmanager import models
from trove.taskmanager.models import FreshInstanceTasks

LOG = logging.getLogger(__name__)
RPC_API_VERSION = "1.0"
CONF = cfg.CONF


class Manager(periodic_task.PeriodicTasks):

    def __init__(self):
        super(Manager, self).__init__()
        self.admin_context = TroveContext(
            user=CONF.nova_proxy_admin_user,
            auth_token=CONF.nova_proxy_admin_pass,
            tenant=CONF.nova_proxy_admin_tenant_name)
        if CONF.exists_notification_transformer:
            self.exists_transformer = importutils.import_object(
                CONF.exists_notification_transformer,
                context=self.admin_context)

    def resize_volume(self, context, instance_id, new_size):
        instance_tasks = models.BuiltInstanceTasks.load(context, instance_id)
        instance_tasks.resize_volume(new_size)

    def resize_flavor(self, context, instance_id, old_flavor, new_flavor):
        instance_tasks = models.BuiltInstanceTasks.load(context, instance_id)
        instance_tasks.resize_flavor(old_flavor, new_flavor)

    def reboot(self, context, instance_id):
        instance_tasks = models.BuiltInstanceTasks.load(context, instance_id)
        instance_tasks.reboot()

    def restart(self, context, instance_id):
        instance_tasks = models.BuiltInstanceTasks.load(context, instance_id)
        instance_tasks.restart()

    def migrate(self, context, instance_id, host):
        instance_tasks = models.BuiltInstanceTasks.load(context, instance_id)
        instance_tasks.migrate(host)

    def delete_instance(self, context, instance_id):
        try:
            instance_tasks = models.BuiltInstanceTasks.load(context,
                                                            instance_id)
            instance_tasks.delete_async()
        except exception.UnprocessableEntity:
            instance_tasks = models.FreshInstanceTasks.load(context,
                                                            instance_id)
            instance_tasks.delete_async()

    def delete_backup(self, context, backup_id):
        models.BackupTasks.delete_backup(context, backup_id)

    def create_backup(self, context, backup_info, instance_id):
        instance_tasks = models.BuiltInstanceTasks.load(context, instance_id)
        instance_tasks.create_backup(backup_info)

    def create_instance(self, context, instance_id, name, flavor,
                        image_id, databases, users, datastore_manager,
                        packages, volume_size, backup_id, availability_zone,
                        root_password, nics, overrides):
        instance_tasks = FreshInstanceTasks.load(context, instance_id)
        instance_tasks.create_instance(flavor, image_id, databases, users,
                                       datastore_manager, packages,
                                       volume_size, backup_id,
                                       availability_zone, root_password, nics,
                                       overrides)

    def update_overrides(self, context, instance_id, overrides):
        instance_tasks = models.BuiltInstanceTasks.load(context, instance_id)
        instance_tasks.update_overrides(overrides)

    def unassign_configuration(self, context, instance_id, flavor,
                               configuration_id):
        instance_tasks = models.BuiltInstanceTasks.load(context, instance_id)
        instance_tasks.unassign_configuration(flavor, configuration_id)

    if CONF.exists_notification_transformer:
        @periodic_task.periodic_task(
            ticks_between_runs=CONF.exists_notification_ticks)
        def publish_exists_event(self, context):
            """
            Push this in Instance Tasks to fetch a report/collection
            :param context: currently None as specied in bin script
            """
            mgmtmodels.publish_exist_events(self.exists_transformer,
                                            self.admin_context)

########NEW FILE########
__FILENAME__ = models
#    Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import re
import traceback
import os.path

from heatclient import exc as heat_exceptions
from cinderclient import exceptions as cinder_exceptions
from eventlet import greenthread
from novaclient import exceptions as nova_exceptions
from trove.backup import models as bkup_models
from trove.common import cfg
from trove.common import template
from trove.common import utils
from trove.common.utils import try_recover
from trove.common.configurations import do_configs_require_restart
from trove.common.exception import GuestError
from trove.common.exception import GuestTimeout
from trove.common.exception import PollTimeOut
from trove.common.exception import VolumeCreationFailure
from trove.common.exception import TroveError
from trove.common.exception import MalformedSecurityGroupRuleError
from trove.common.instance import ServiceStatuses
from trove.common import instance as rd_instance
from trove.common.remote import create_dns_client
from trove.common.remote import create_heat_client
from trove.common.remote import create_cinder_client
from trove.extensions.mysql import models as mysql_models
from trove.configuration.models import Configuration
from trove.extensions.security_group.models import SecurityGroup
from trove.extensions.security_group.models import SecurityGroupRule
from swiftclient.client import ClientException
from trove.instance import models as inst_models
from trove.instance.models import BuiltInstance
from trove.instance.models import DBInstance
from trove.instance.models import FreshInstance
from trove.instance.tasks import InstanceTasks
from trove.instance.models import InstanceStatus
from trove.instance.models import InstanceServiceStatus
from trove.openstack.common import log as logging
from trove.openstack.common.gettextutils import _
from trove.openstack.common.notifier import api as notifier
from trove.openstack.common import timeutils
import trove.common.remote as remote

LOG = logging.getLogger(__name__)
CONF = cfg.CONF
VOLUME_TIME_OUT = CONF.volume_time_out  # seconds.
DNS_TIME_OUT = CONF.dns_time_out  # seconds.
RESIZE_TIME_OUT = CONF.resize_time_out  # seconds.
REVERT_TIME_OUT = CONF.revert_time_out  # seconds.
HEAT_TIME_OUT = CONF.heat_time_out  # seconds.
USAGE_SLEEP_TIME = CONF.usage_sleep_time  # seconds.
HEAT_STACK_SUCCESSFUL_STATUSES = [('CREATE', 'CREATE_COMPLETE')]
HEAT_RESOURCE_SUCCESSFUL_STATE = 'CREATE_COMPLETE'

use_nova_server_volume = CONF.use_nova_server_volume
use_heat = CONF.use_heat


class NotifyMixin(object):
    """Notification Mixin

    This adds the ability to send usage events to an Instance object.
    """

    def _get_service_id(self, datastore_manager, id_map):
        if datastore_manager in id_map:
            datastore_manager_id = id_map[datastore_manager]
        else:
            datastore_manager_id = cfg.UNKNOWN_SERVICE_ID
            LOG.error("Datastore ID for Manager (%s) is not configured"
                      % datastore_manager)
        return datastore_manager_id

    def send_usage_event(self, event_type, **kwargs):
        event_type = 'trove.instance.%s' % event_type
        publisher_id = CONF.host
        # Grab the instance size from the kwargs or from the nova client
        instance_size = kwargs.pop('instance_size', None)
        flavor = self.nova_client.flavors.get(self.flavor_id)
        server = kwargs.pop('server', None)
        if server is None:
            server = self.nova_client.servers.get(self.server_id)
        az = getattr(server, 'OS-EXT-AZ:availability_zone', None)

        # Default payload
        created_time = timeutils.isotime(self.db_info.created)
        payload = {
            'availability_zone': az,
            'created_at': created_time,
            'name': self.name,
            'instance_id': self.id,
            'instance_name': self.name,
            'instance_size': instance_size or flavor.ram,
            'instance_type': flavor.name,
            'instance_type_id': flavor.id,
            'launched_at': created_time,
            'nova_instance_id': self.server_id,
            'region': CONF.region,
            'state_description': self.status,
            'state': self.status,
            'tenant_id': self.tenant_id,
            'user_id': self.context.user,
        }

        if CONF.trove_volume_support:
            payload.update({
                'volume_size': self.volume_size,
                'nova_volume_id': self.volume_id
            })

        payload['service_id'] = self._get_service_id(
            self.datastore_version.manager, CONF.notification_service_id)

        # Update payload with all other kwargs
        payload.update(kwargs)
        LOG.debug('Sending event: %(event_type)s, %(payload)s' %
                  {'event_type': event_type, 'payload': payload})
        notifier.notify(self.context, publisher_id, event_type, 'INFO',
                        payload)


class ConfigurationMixin(object):
    """Configuration Mixin

    Configuration related tasks for instances and resizes.
    """

    def _render_config(self, flavor):
        config = template.SingleInstanceConfigTemplate(
            self.datastore_version, flavor, self.id)
        config.render()
        return config

    def _render_override_config(self, flavor, overrides=None):
        config = template.OverrideConfigTemplate(
            self.datastore_version, flavor, self.id)
        config.render(overrides=overrides)
        return config

    def _render_config_dict(self, flavor):
        config = template.SingleInstanceConfigTemplate(
            self.datastore_version, flavor, self.id)
        ret = config.render_dict()
        LOG.debug("the default template dict of mysqld section: %s" % ret)
        return ret


class FreshInstanceTasks(FreshInstance, NotifyMixin, ConfigurationMixin):
    def create_instance(self, flavor, image_id, databases, users,
                        datastore_manager, packages, volume_size,
                        backup_id, availability_zone, root_password, nics,
                        overrides):

        LOG.debug("begin create_instance for id: %s" % self.id)
        security_groups = None

        # If security group support is enabled and heat based instance
        # orchestration is disabled, create a security group.
        #
        # Heat based orchestration handles security group(resource)
        # in the template definition.
        if CONF.trove_security_groups_support and not use_heat:
            try:
                security_groups = self._create_secgroup(datastore_manager)
            except Exception as e:
                msg = (_("Error creating security group for instance: %s") %
                       self.id)
                err = inst_models.InstanceTasks.BUILDING_ERROR_SEC_GROUP
                self._log_and_raise(e, msg, err)
            else:
                LOG.debug("Successfully created security group for "
                          "instance: %s" % self.id)

        if use_heat:
            volume_info = self._create_server_volume_heat(
                flavor,
                image_id,
                datastore_manager,
                volume_size,
                availability_zone,
                nics)
        elif use_nova_server_volume:
            volume_info = self._create_server_volume(
                flavor['id'],
                image_id,
                security_groups,
                datastore_manager,
                volume_size,
                availability_zone,
                nics)
        else:
            volume_info = self._create_server_volume_individually(
                flavor['id'],
                image_id,
                security_groups,
                datastore_manager,
                volume_size,
                availability_zone,
                nics)

        config = self._render_config(flavor)
        config_overrides = self._render_override_config(flavor,
                                                        overrides=overrides)

        backup_info = None
        if backup_id is not None:
                backup = bkup_models.Backup.get_by_id(self.context, backup_id)
                backup_info = {'id': backup_id,
                               'location': backup.location,
                               'type': backup.backup_type,
                               'checksum': backup.checksum,
                               }
        self._guest_prepare(flavor['ram'], volume_info,
                            packages, databases, users, backup_info,
                            config.config_contents, root_password,
                            config_overrides.config_contents)

        if root_password:
            self.report_root_enabled()

        if not self.db_info.task_status.is_error:
            self.reset_task_status()

        # when DNS is supported, we attempt to add this after the
        # instance is prepared.  Otherwise, if DNS fails, instances
        # end up in a poorer state and there's no tooling around
        # re-sending the prepare call; retrying DNS is much easier.
        try:
            self._create_dns_entry()
        except Exception as e:
            msg = _("Error creating DNS entry for instance: %s") % self.id
            err = inst_models.InstanceTasks.BUILDING_ERROR_DNS
            self._log_and_raise(e, msg, err)
        else:
            LOG.debug("Successfully created DNS entry for instance: %s" %
                      self.id)

        # Make sure the service becomes active before sending a usage
        # record to avoid over billing a customer for an instance that
        # fails to build properly.
        try:
            usage_timeout = CONF.get(datastore_manager).usage_timeout
            utils.poll_until(self._service_is_active,
                             sleep_time=USAGE_SLEEP_TIME,
                             time_out=usage_timeout)
            self.send_usage_event('create', instance_size=flavor['ram'])
        except PollTimeOut:
            LOG.error(_("Timeout for service changing to active. "
                      "No usage create-event sent."))
            self.update_statuses_on_time_out()

        except Exception:
            LOG.exception(_("Error during create-event call."))

        LOG.debug("end create_instance for id: %s" % self.id)

    def report_root_enabled(self):
        mysql_models.RootHistory.create(self.context, self.id, 'root')

    def update_statuses_on_time_out(self):

        if CONF.update_status_on_fail:
            #Updating service status
            service = InstanceServiceStatus.find_by(instance_id=self.id)
            service.set_status(ServiceStatuses.
                               FAILED_TIMEOUT_GUESTAGENT)
            service.save()
            LOG.error(_("Service status: %(status)s") %
                      {'status': ServiceStatuses.
                       FAILED_TIMEOUT_GUESTAGENT.api_status})
            LOG.error(_("Service error description: %(desc)s") %
                      {'desc': ServiceStatuses.
                       FAILED_TIMEOUT_GUESTAGENT.description})
            #Updating instance status
            db_info = DBInstance.find_by(name=self.name)
            db_info.set_task_status(InstanceTasks.
                                    BUILDING_ERROR_TIMEOUT_GA)
            db_info.save()
            LOG.error(_("Trove instance status: %(action)s") %
                      {'action': InstanceTasks.
                       BUILDING_ERROR_TIMEOUT_GA.action})
            LOG.error(_("Trove instance status description: %(text)s") %
                      {'text': InstanceTasks.
                       BUILDING_ERROR_TIMEOUT_GA.db_text})

    def _service_is_active(self):
        """
        Check that the database guest is active.

        This function is meant to be called with poll_until to check that
        the guest is alive before sending a 'create' message. This prevents
        over billing a customer for a instance that they can never use.

        Returns: boolean if the service is active.
        Raises: TroveError if the service is in a failure state.
        """
        service = InstanceServiceStatus.find_by(instance_id=self.id)
        status = service.get_status()
        if status == rd_instance.ServiceStatuses.RUNNING:
            return True
        elif status not in [rd_instance.ServiceStatuses.NEW,
                            rd_instance.ServiceStatuses.BUILDING]:
            raise TroveError(_("Service not active, status: %s") % status)

        c_id = self.db_info.compute_instance_id
        nova_status = self.nova_client.servers.get(c_id).status
        if nova_status in [InstanceStatus.ERROR,
                           InstanceStatus.FAILED]:
            raise TroveError(_("Server not active, status: %s") % nova_status)
        return False

    def _create_server_volume(self, flavor_id, image_id, security_groups,
                              datastore_manager, volume_size,
                              availability_zone, nics):
        LOG.debug("begin _create_server_volume for id: %s" % self.id)
        try:
            files = {"/etc/guest_info": ("[DEFAULT]\n--guest_id="
                                         "%s\n--datastore_manager=%s\n"
                                         "--tenant_id=%s\n" %
                                         (self.id, datastore_manager,
                                          self.tenant_id))}
            name = self.hostname or self.name
            volume_desc = ("datastore volume for %s" % self.id)
            volume_name = ("datastore-%s" % self.id)
            volume_ref = {'size': volume_size, 'name': volume_name,
                          'description': volume_desc}

            server = self.nova_client.servers.create(
                name, image_id, flavor_id,
                files=files, volume=volume_ref,
                security_groups=security_groups,
                availability_zone=availability_zone, nics=nics)
            LOG.debug("Created new compute instance %(server_id)s "
                      "for id: %(id)s" %
                      {'server_id': server.id, 'id': self.id})

            server_dict = server._info
            LOG.debug("Server response: %s" % server_dict)
            volume_id = None
            for volume in server_dict.get('os:volumes', []):
                volume_id = volume.get('id')

            # Record the server ID and volume ID in case something goes wrong.
            self.update_db(compute_instance_id=server.id, volume_id=volume_id)
        except Exception as e:
            msg = _("Error creating server and volume for "
                    "instance %s") % self.id
            LOG.debug("end _create_server_volume for id: %s" % self.id)
            err = inst_models.InstanceTasks.BUILDING_ERROR_SERVER
            self._log_and_raise(e, msg, err)

        device_path = CONF.device_path
        mount_point = CONF.get(datastore_manager).mount_point
        volume_info = {'device_path': device_path, 'mount_point': mount_point}
        LOG.debug("end _create_server_volume for id: %s" % self.id)
        return volume_info

    def _create_server_volume_heat(self, flavor, image_id,
                                   datastore_manager,
                                   volume_size, availability_zone, nics):
        LOG.debug("begin _create_server_volume_heat for id: %s" % self.id)
        try:
            client = create_heat_client(self.context)

            ifaces, ports = self._build_heat_nics(nics)
            template_obj = template.load_heat_template(datastore_manager)
            heat_template_unicode = template_obj.render(
                volume_support=CONF.trove_volume_support,
                ifaces=ifaces, ports=ports)
            try:
                heat_template = heat_template_unicode.encode('utf-8')
            except UnicodeEncodeError:
                LOG.error(_("heat template ascii encode issue"))
                raise TroveError("heat template ascii encode issue")

            parameters = {"Flavor": flavor["name"],
                          "VolumeSize": volume_size,
                          "InstanceId": self.id,
                          "ImageId": image_id,
                          "DatastoreManager": datastore_manager,
                          "AvailabilityZone": availability_zone,
                          "TenantId": self.tenant_id}
            stack_name = 'trove-%s' % self.id
            client.stacks.create(stack_name=stack_name,
                                 template=heat_template,
                                 parameters=parameters)
            try:
                utils.poll_until(
                    lambda: client.stacks.get(stack_name),
                    lambda stack: stack.stack_status in ['CREATE_COMPLETE',
                                                         'CREATE_FAILED'],
                    sleep_time=USAGE_SLEEP_TIME,
                    time_out=HEAT_TIME_OUT)
            except PollTimeOut:
                LOG.error(_("Timeout during stack status tracing"))
                raise TroveError("Timeout occured in tracking stack status")

            stack = client.stacks.get(stack_name)
            if ((stack.action, stack.stack_status)
                    not in HEAT_STACK_SUCCESSFUL_STATUSES):
                raise TroveError("Heat Stack Create Failed.")

            resource = client.resources.get(stack.id, 'BaseInstance')
            if resource.resource_status != HEAT_RESOURCE_SUCCESSFUL_STATE:
                raise TroveError("Heat Resource Provisioning Failed.")
            instance_id = resource.physical_resource_id

            if CONF.trove_volume_support:
                resource = client.resources.get(stack.id, 'DataVolume')
                if resource.resource_status != HEAT_RESOURCE_SUCCESSFUL_STATE:
                    raise TroveError("Heat Resource Provisioning Failed.")
                volume_id = resource.physical_resource_id
                self.update_db(compute_instance_id=instance_id,
                               volume_id=volume_id)
            else:
                self.update_db(compute_instance_id=instance_id)

        except (TroveError, heat_exceptions.HTTPNotFound) as e:
            msg = "Error during creating stack for instance %s" % self.id
            LOG.debug(msg)
            err = inst_models.InstanceTasks.BUILDING_ERROR_SERVER
            self._log_and_raise(e, msg, err)

        device_path = CONF.device_path
        mount_point = CONF.get(datastore_manager).mount_point
        volume_info = {'device_path': device_path, 'mount_point': mount_point}

        LOG.debug("end _create_server_volume_heat for id: %s" % self.id)
        return volume_info

    def _create_server_volume_individually(self, flavor_id, image_id,
                                           security_groups, datastore_manager,
                                           volume_size,
                                           availability_zone, nics):
        LOG.debug("begin _create_server_volume_individually for id: %s" %
                  self.id)
        server = None
        volume_info = self._build_volume_info(datastore_manager,
                                              volume_size=volume_size)
        block_device_mapping = volume_info['block_device']
        try:
            server = self._create_server(flavor_id, image_id, security_groups,
                                         datastore_manager,
                                         block_device_mapping,
                                         availability_zone, nics)
            server_id = server.id
            # Save server ID.
            self.update_db(compute_instance_id=server_id)
        except Exception as e:
            msg = _("Error creating server for instance %s") % self.id
            err = inst_models.InstanceTasks.BUILDING_ERROR_SERVER
            self._log_and_raise(e, msg, err)
        LOG.debug("end _create_server_volume_individually for id: %s" %
                  self.id)
        return volume_info

    def _build_volume_info(self, datastore_manager, volume_size=None):
        volume_info = None
        volume_support = CONF.trove_volume_support
        LOG.debug("trove volume support = %s" % volume_support)
        if volume_support:
            try:
                volume_info = self._create_volume(
                    volume_size, datastore_manager)
            except Exception as e:
                msg = _("Error provisioning volume for instance: %s") % self.id
                err = inst_models.InstanceTasks.BUILDING_ERROR_VOLUME
                self._log_and_raise(e, msg, err)
        else:
            LOG.debug("device_path = %s" % CONF.device_path)
            LOG.debug("mount_point = %s" %
                      CONF.get(datastore_manager).mount_point)
            volume_info = {
                'block_device': None,
                'device_path': CONF.device_path,
                'mount_point': CONF.get(datastore_manager).mount_point,
                'volumes': None,
            }
        return volume_info

    def _log_and_raise(self, exc, message, task_status):
        LOG.error(message)
        LOG.error(exc)
        LOG.error(traceback.format_exc())
        self.update_db(task_status=task_status)
        raise TroveError(message=message)

    def _create_volume(self, volume_size, datastore_manager):
        LOG.info("Entering create_volume")
        LOG.debug("begin _create_volume for id: %s" % self.id)
        volume_client = create_cinder_client(self.context)
        volume_desc = ("datastore volume for %s" % self.id)
        volume_ref = volume_client.volumes.create(
            volume_size, name="datastore-%s" % self.id,
            description=volume_desc)

        # Record the volume ID in case something goes wrong.
        self.update_db(volume_id=volume_ref.id)

        utils.poll_until(
            lambda: volume_client.volumes.get(volume_ref.id),
            lambda v_ref: v_ref.status in ['available', 'error'],
            sleep_time=2,
            time_out=VOLUME_TIME_OUT)

        v_ref = volume_client.volumes.get(volume_ref.id)
        if v_ref.status in ['error']:
            raise VolumeCreationFailure()
        LOG.debug("end _create_volume for id: %s" % self.id)
        return self._build_volume(v_ref, datastore_manager)

    def _build_volume(self, v_ref, datastore_manager):
        LOG.debug("Created volume %s" % v_ref)
        # The mapping is in the format:
        # <id>:[<type>]:[<size(GB)>]:[<delete_on_terminate>]
        # setting the delete_on_terminate instance to true=1
        mapping = "%s:%s:%s:%s" % (v_ref.id, '', v_ref.size, 1)
        bdm = CONF.block_device_mapping
        block_device = {bdm: mapping}
        created_volumes = [{'id': v_ref.id,
                            'size': v_ref.size}]
        LOG.debug("block_device = %s" % block_device)
        LOG.debug("volume = %s" % created_volumes)

        device_path = CONF.device_path
        mount_point = CONF.get(datastore_manager).mount_point
        LOG.debug("device_path = %s" % device_path)
        LOG.debug("mount_point = %s" % mount_point)

        volume_info = {'block_device': block_device,
                       'device_path': device_path,
                       'mount_point': mount_point,
                       'volumes': created_volumes}
        return volume_info

    def _create_server(self, flavor_id, image_id, security_groups,
                       datastore_manager, block_device_mapping,
                       availability_zone, nics):
        files = {"/etc/guest_info": ("[DEFAULT]\nguest_id=%s\n"
                                     "datastore_manager=%s\n"
                                     "tenant_id=%s\n" %
                                     (self.id, datastore_manager,
                                      self.tenant_id))}
        if os.path.isfile(CONF.get('guest_config')):
            with open(CONF.get('guest_config'), "r") as f:
                files["/etc/trove-guestagent.conf"] = f.read()
        userdata = None
        cloudinit = os.path.join(CONF.get('cloudinit_location'),
                                 "%s.cloudinit" % datastore_manager)
        if os.path.isfile(cloudinit):
            with open(cloudinit, "r") as f:
                userdata = f.read()
        name = self.hostname or self.name
        bdmap = block_device_mapping
        server = self.nova_client.servers.create(
            name, image_id, flavor_id, files=files, userdata=userdata,
            security_groups=security_groups, block_device_mapping=bdmap,
            availability_zone=availability_zone, nics=nics)
        LOG.debug("Created new compute instance %(server_id)s "
                  "for id: %(id)s" %
                  {'server_id': server.id, 'id': self.id})
        return server

    def _guest_prepare(self, flavor_ram, volume_info,
                       packages, databases, users, backup_info=None,
                       config_contents=None, root_password=None,
                       overrides=None):
        LOG.info(_("Entering guest_prepare"))
        # Now wait for the response from the create to do additional work
        self.guest.prepare(flavor_ram, packages, databases, users,
                           device_path=volume_info['device_path'],
                           mount_point=volume_info['mount_point'],
                           backup_info=backup_info,
                           config_contents=config_contents,
                           root_password=root_password,
                           overrides=overrides)

    def _create_dns_entry(self):
        LOG.debug("%(gt)s: Creating dns entry for instance: %(id)s" %
                  {'gt': greenthread.getcurrent(), 'id': self.id})
        dns_support = CONF.trove_dns_support
        LOG.debug("trove dns support = %s" % dns_support)

        if dns_support:
            dns_client = create_dns_client(self.context)

            def get_server():
                c_id = self.db_info.compute_instance_id
                return self.nova_client.servers.get(c_id)

            def ip_is_available(server):
                LOG.info(_("Polling for ip addresses: $%s ") %
                         server.addresses)
                if server.addresses != {}:
                    return True
                elif (server.addresses == {} and
                      server.status != InstanceStatus.ERROR):
                    return False
                elif (server.addresses == {} and
                      server.status == InstanceStatus.ERROR):
                    LOG.error(_("Instance IP not available, "
                                "instance (%(instance)s): "
                                "server had status (%(status)s).") %
                              {'instance': self.id, 'status': server.status})
                    raise TroveError(status=server.status)

            utils.poll_until(get_server, ip_is_available,
                             sleep_time=1, time_out=DNS_TIME_OUT)
            server = self.nova_client.servers.get(
                self.db_info.compute_instance_id)
            self.db_info.addresses = server.addresses
            LOG.info(_("Creating dns entry..."))
            ip = self.dns_ip_address
            if not ip:
                raise TroveError('Error creating DNS. No IP available.')
            dns_client.create_instance_entry(self.id, ip)
        else:
            LOG.debug("%(gt)s: DNS not enabled for instance: %(id)s" %
                      {'gt': greenthread.getcurrent(), 'id': self.id})

    def _create_secgroup(self, datastore_manager):
        security_group = SecurityGroup.create_for_instance(
            self.id, self.context)
        tcp_ports = CONF.get(datastore_manager).tcp_ports
        udp_ports = CONF.get(datastore_manager).udp_ports
        self._create_rules(security_group, tcp_ports, 'tcp')
        self._create_rules(security_group, udp_ports, 'udp')
        return [security_group["name"]]

    def _create_rules(self, s_group, ports, protocol):
        err = inst_models.InstanceTasks.BUILDING_ERROR_SEC_GROUP
        err_msg = _("Error creating security group rules."
                    " Invalid port format. "
                    "FromPort = %(from)s, ToPort = %(to)s")

        def set_error_and_raise(port_or_range):
            from_port, to_port = port_or_range
            self.update_db(task_status=err)
            msg = err_msg % {'from': from_port, 'to': to_port}
            raise MalformedSecurityGroupRuleError(message=msg)

        for port_or_range in set(ports):
            try:
                from_, to_ = (None, None)
                from_, to_ = utils.gen_ports(port_or_range)
                cidr = CONF.trove_security_group_rule_cidr
                SecurityGroupRule.create_sec_group_rule(
                    s_group, protocol, int(from_), int(to_),
                    cidr, self.context)
            except (ValueError, TroveError):
                set_error_and_raise([from_, to_])

    def _build_heat_nics(self, nics):
        ifaces = []
        ports = []
        if nics:
            for idx, nic in enumerate(nics):
                iface_id = nic.get('port-id')
                if iface_id:
                    ifaces.append(iface_id)
                    continue
                net_id = nic.get('net-id')
                if net_id:
                    port = {}
                    port['name'] = "Port%s" % idx
                    port['net_id'] = net_id
                    fixed_ip = nic.get('v4-fixed-ip')
                    if fixed_ip:
                        port['fixed_ip'] = fixed_ip
                    ports.append(port)
                    ifaces.append("{Ref: Port%s}" % idx)
        return ifaces, ports


class BuiltInstanceTasks(BuiltInstance, NotifyMixin, ConfigurationMixin):
    """
    Performs the various asynchronous instance related tasks.
    """

    def _delete_resources(self, deleted_at):
        LOG.debug("begin _delete_resources for id: %s" % self.id)
        server_id = self.db_info.compute_instance_id
        old_server = self.nova_client.servers.get(server_id)
        try:
            if use_heat:
                # Delete the server via heat
                heatclient = create_heat_client(self.context)
                name = 'trove-%s' % self.id
                heatclient.stacks.delete(name)
            else:
                self.server.delete()
        except Exception as ex:
            LOG.exception(_("Error during delete compute server %s")
                          % self.server.id)
        try:
            dns_support = CONF.trove_dns_support
            LOG.debug("trove dns support = %s" % dns_support)
            if dns_support:
                dns_api = create_dns_client(self.context)
                dns_api.delete_instance_entry(instance_id=self.db_info.id)
        except Exception as ex:
            LOG.exception(_("Error during dns entry of instance %(id)s: "
                            "%(ex)s") % {'id': self.db_info.id, 'ex': ex})

            # Poll until the server is gone.
        def server_is_finished():
            try:
                server = self.nova_client.servers.get(server_id)
                if not self.server_status_matches(['SHUTDOWN', 'ACTIVE'],
                                                  server=server):
                    LOG.error(_("Server %(server_id)s got into ERROR status "
                                "during delete of instance %(instance_id)s!") %
                              {'server_id': server.id, 'instance_id': self.id})
                return False
            except nova_exceptions.NotFound:
                return True

        try:
            utils.poll_until(server_is_finished, sleep_time=2,
                             time_out=CONF.server_delete_time_out)
        except PollTimeOut:
            LOG.exception(_("Timout during nova server delete of server: %s") %
                          server_id)
        self.send_usage_event('delete',
                              deleted_at=timeutils.isotime(deleted_at),
                              server=old_server)
        LOG.debug("end _delete_resources for id: %s" % self.id)

    def server_status_matches(self, expected_status, server=None):
        if not server:
            server = self.server
        return server.status.upper() in (
            status.upper() for status in expected_status)

    def resize_volume(self, new_size):
        LOG.debug("begin resize_volume for instance: %s" % self.id)
        action = ResizeVolumeAction(self, self.volume_size, new_size)
        action.execute()
        LOG.debug("end resize_volume for instance: %s" % self.id)

    def resize_flavor(self, old_flavor, new_flavor):
        action = ResizeAction(self, old_flavor, new_flavor)
        action.execute()

    def migrate(self, host):
        LOG.debug("Calling migrate with host(%s)..." % host)
        action = MigrateAction(self, host)
        action.execute()

    def create_backup(self, backup_info):
        LOG.debug("Calling create_backup  %s " % self.id)
        self.guest.create_backup(backup_info)

    def reboot(self):
        try:
            LOG.debug("Instance %s calling stop_db..." % self.id)
            self.guest.stop_db()
            LOG.debug("Rebooting instance %s" % self.id)
            self.server.reboot()

            # Poll nova until instance is active
            reboot_time_out = CONF.reboot_time_out

            def update_server_info():
                self.refresh_compute_server_info()
                return self.server_status_matches(['ACTIVE'])

            utils.poll_until(
                update_server_info,
                sleep_time=2,
                time_out=reboot_time_out)

            # Set the status to PAUSED. The guest agent will reset the status
            # when the reboot completes and MySQL is running.
            self.set_datastore_status_to_paused()
            LOG.debug("Successfully rebooted instance %s" % self.id)
        except Exception as e:
            LOG.error(_("Failed to reboot instance %(id)s: %(e)s") %
                      {'id': self.id, 'e': str(e)})
        finally:
            LOG.debug("Rebooting FINALLY  %s" % self.id)
            self.reset_task_status()

    def restart(self):
        LOG.debug("Restarting datastore on instance %s " % self.id)
        try:
            self.guest.restart()
            LOG.debug("Restarting datastore successful  %s " % self.id)
        except GuestError:
            LOG.error(_("Failure to restart datastore for instance %s.") %
                      self.id)
        finally:
            LOG.debug("Restarting complete on instance  %s " % self.id)
            self.reset_task_status()

    def update_overrides(self, overrides, remove=False):
        LOG.debug("Updating configuration overrides on instance %s"
                  % self.id)
        LOG.debug("overrides: %s" % overrides)
        LOG.debug("self.ds_version: %s" % self.ds_version.__dict__)
        # todo(cp16net) How do we know what datastore type we have?
        need_restart = do_configs_require_restart(
            overrides, datastore_manager=self.ds_version.manager)
        LOG.debug("do we need a restart?: %s" % need_restart)
        if need_restart:
            status = inst_models.InstanceTasks.RESTART_REQUIRED
            self.update_db(task_status=status)

        flavor = self.nova_client.flavors.get(self.flavor_id)

        config_overrides = self._render_override_config(
            flavor,
            overrides=overrides)
        try:
            self.guest.update_overrides(config_overrides.config_contents,
                                        remove=remove)
            self.guest.apply_overrides(overrides)
            LOG.debug("Configuration overrides update successful.")
        except GuestError:
            LOG.error(_("Failed to update configuration overrides."))

    def unassign_configuration(self, flavor, configuration_id):
        LOG.debug("Unassigning the configuration from the instance %s"
                  % self.id)
        LOG.debug("Unassigning the configuration id %s"
                  % self.configuration.id)

        def _find_item(items, item_name):
            LOG.debug("items: %s" % items)
            LOG.debug("item_name: %s" % item_name)
            # find the item in the list
            for i in items:
                if i[0] == item_name:
                    return i

        def _convert_value(value):
            # split the value and the size e.g. 512M=['512','M']
            pattern = re.compile('(\d+)(\w+)')
            split = pattern.findall(value)
            if len(split) < 2:
                return value
            digits, size = split
            conversions = {
                'K': 1024,
                'M': 1024 ** 2,
                'G': 1024 ** 3,
            }
            return str(int(digits) * conversions[size])

        default_config = self._render_config_dict(flavor)
        args = {
            "ds_manager": self.ds_version.manager,
            "config": default_config,
        }
        LOG.debug("default %(ds_manager)s section: %(config)s" % args)
        LOG.debug("self.configuration: %s" % self.configuration.__dict__)

        overrides = {}
        config_items = Configuration.load_items(self.context, configuration_id)
        for item in config_items:
            LOG.debug("finding item(%s)" % item.__dict__)
            try:
                key, val = _find_item(default_config, item.configuration_key)
            except TypeError:
                val = None
                restart_required = inst_models.InstanceTasks.RESTART_REQUIRED
                self.update_db(task_status=restart_required)
            if val:
                overrides[item.configuration_key] = _convert_value(val)
        LOG.debug("setting the default variables in dict: %s" % overrides)
        self.update_overrides(overrides, remove=True)
        self.update_db(configuration_id=None)

    def refresh_compute_server_info(self):
        """Refreshes the compute server field."""
        server = self.nova_client.servers.get(self.server.id)
        self.server = server

    def _refresh_datastore_status(self):
        """
        Gets the latest instance service status from datastore and updates
        the reference on this BuiltInstanceTask reference
        """
        self.datastore_status = InstanceServiceStatus.find_by(
            instance_id=self.id)

    def set_datastore_status_to_paused(self):
        """
        Updates the InstanceServiceStatus for this BuiltInstance to PAUSED.
        This does not change the reference for this BuiltInstanceTask
        """
        datastore_status = InstanceServiceStatus.find_by(instance_id=self.id)
        datastore_status.status = rd_instance.ServiceStatuses.PAUSED
        datastore_status.save()


class BackupTasks(object):
    @classmethod
    def _parse_manifest(cls, manifest):
        # manifest is in the format 'container/prefix'
        # where prefix can be 'path' or 'lots/of/paths'
        try:
            container_index = manifest.index('/')
            prefix_index = container_index + 1
        except ValueError:
            return None, None
        container = manifest[:container_index]
        prefix = manifest[prefix_index:]
        return container, prefix

    @classmethod
    def delete_files_from_swift(cls, context, filename):
        container = CONF.backup_swift_container
        client = remote.create_swift_client(context)
        obj = client.head_object(container, filename)
        manifest = obj.get('x-object-manifest', '')
        cont, prefix = cls._parse_manifest(manifest)
        if all([cont, prefix]):
            # This is a manifest file, first delete all segments.
            LOG.info(_("Deleting files with prefix: %(cont)s/%(prefix)s") %
                     {'cont': cont, 'prefix': prefix})
            # list files from container/prefix specified by manifest
            headers, segments = client.get_container(cont, prefix=prefix)
            LOG.debug(headers)
            for segment in segments:
                name = segment.get('name')
                if name:
                    LOG.info(_("Deleting file: %(cont)s/%(name)s") %
                             {'cont': cont, 'name': name})
                    client.delete_object(cont, name)
        # Delete the manifest file
        LOG.info(_("Deleting file: %(cont)s/%(filename)s") %
                 {'cont': cont, 'filename': filename})
        client.delete_object(container, filename)

    @classmethod
    def delete_backup(cls, context, backup_id):
        #delete backup from swift
        backup = bkup_models.Backup.get_by_id(context, backup_id)
        try:
            filename = backup.filename
            if filename:
                BackupTasks.delete_files_from_swift(context, filename)
        except ValueError:
            backup.delete()
        except ClientException as e:
            if e.http_status == 404:
                # Backup already deleted in swift
                backup.delete()
            else:
                LOG.exception(_("Exception deleting from swift. "
                                "Details: %s") % e)
                backup.state = bkup_models.BackupState.DELETE_FAILED
                backup.save()
                raise TroveError("Failed to delete swift objects")
        else:
            backup.delete()


class ResizeVolumeAction(object):
    """Performs volume resize action."""

    def __init__(self, instance, old_size, new_size):
        self.instance = instance
        self.old_size = int(old_size)
        self.new_size = int(new_size)

    def get_mount_point(self):
        mount_point = CONF.get(
            self.instance.datastore_version.manager).mount_point
        return mount_point

    def _fail(self, orig_func):
        LOG.exception(_("%(func)s encountered an error when attempting to "
                      "resize the volume for instance %(id)s. Setting service "
                      "status to failed.") % {'func': orig_func.__name__,
                      'id': self.instance.id})
        service = InstanceServiceStatus.find_by(instance_id=self.instance.id)
        service.set_status(ServiceStatuses.FAILED)
        service.save()

    def _recover_restart(self, orig_func):
        LOG.exception(_("%(func)s encountered an error when attempting to "
                      "resize the volume for instance %(id)s. Trying to "
                      "recover by restarting the guest.") % {
                      'func': orig_func.__name__,
                      'id': self.instance.id})
        self.instance.restart()

    def _recover_mount_restart(self, orig_func):
        LOG.exception(_("%(func)s encountered an error when attempting to "
                      "resize the volume for instance %(id)s. Trying to "
                      "recover by mounting the volume and then restarting the "
                      "guest.") % {'func': orig_func.__name__,
                      'id': self.instance.id})
        self._mount_volume()
        self.instance.restart()

    def _recover_full(self, orig_func):
        LOG.exception(_("%(func)s encountered an error when attempting to "
                      "resize the volume for instance %(id)s. Trying to "
                      "recover by attaching and mounting the volume and then "
                      "restarting the guest.") % {'func': orig_func.__name__,
                      'id': self.instance.id})
        self._attach_volume()
        self._mount_volume()
        self.instance.restart()

    def _stop_db(self):
        LOG.debug("Instance %s calling stop_db." % self.instance.id)
        self.instance.guest.stop_db()

    @try_recover
    def _unmount_volume(self):
        LOG.debug("Unmounting the volume on instance %(id)s" % {
                  'id': self.instance.id})
        mount_point = self.get_mount_point()
        self.instance.guest.unmount_volume(device_path=CONF.device_path,
                                           mount_point=mount_point)
        LOG.debug("Successfully unmounted the volume %(vol_id)s for "
                  "instance %(id)s" % {'vol_id': self.instance.volume_id,
                  'id': self.instance.id})

    @try_recover
    def _detach_volume(self):
        LOG.debug("Detach volume %(vol_id)s from instance %(id)s" % {
                  'vol_id': self.instance.volume_id,
                  'id': self.instance.id})
        self.instance.nova_client.volumes.delete_server_volume(
            self.instance.server.id, self.instance.volume_id)

        def volume_available():
            volume = self.instance.volume_client.volumes.get(
                self.instance.volume_id)
            return volume.status == 'available'
        utils.poll_until(volume_available,
                         sleep_time=2,
                         time_out=CONF.volume_time_out)

        LOG.debug("Successfully detached volume %(vol_id)s from instance "
                  "%(id)s" % {'vol_id': self.instance.volume_id,
                              'id': self.instance.id})

    @try_recover
    def _attach_volume(self):
        LOG.debug("Attach volume %(vol_id)s to instance %(id)s at "
                  "%(dev)s" % {'vol_id': self.instance.volume_id,
                  'id': self.instance.id, 'dev': CONF.device_path})
        self.instance.nova_client.volumes.create_server_volume(
            self.instance.server.id, self.instance.volume_id, CONF.device_path)

        def volume_in_use():
            volume = self.instance.volume_client.volumes.get(
                self.instance.volume_id)
            return volume.status == 'in-use'
        utils.poll_until(volume_in_use,
                         sleep_time=2,
                         time_out=CONF.volume_time_out)

        LOG.debug("Successfully attached volume %(vol_id)s to instance "
                  "%(id)s" % {'vol_id': self.instance.volume_id,
                  'id': self.instance.id})

    @try_recover
    def _resize_fs(self):
        LOG.debug("Resizing the filesystem for instance %(id)s" % {
                  'id': self.instance.id})
        mount_point = self.get_mount_point()
        self.instance.guest.resize_fs(device_path=CONF.device_path,
                                      mount_point=mount_point)
        LOG.debug("Successfully resized volume %(vol_id)s filesystem for "
                  "instance %(id)s" % {'vol_id': self.instance.volume_id,
                  'id': self.instance.id})

    @try_recover
    def _mount_volume(self):
        LOG.debug("Mount the volume on instance %(id)s" % {
                  'id': self.instance.id})
        mount_point = self.get_mount_point()
        self.instance.guest.mount_volume(device_path=CONF.device_path,
                                         mount_point=mount_point)
        LOG.debug("Successfully mounted the volume %(vol_id)s on instance "
                  "%(id)s" % {'vol_id': self.instance.volume_id,
                  'id': self.instance.id})

    @try_recover
    def _extend(self):
        LOG.debug("Extending volume %(vol_id)s for instance %(id)s to "
                  "size %(size)s" % {'vol_id': self.instance.volume_id,
                  'id': self.instance.id, 'size': self.new_size})
        self.instance.volume_client.volumes.extend(self.instance.volume_id,
                                                   self.new_size)
        LOG.debug("Successfully extended the volume %(vol_id)s for instance "
                  "%(id)s" % {'vol_id': self.instance.volume_id,
                  'id': self.instance.id})

    def _verify_extend(self):
        try:
            volume = self.instance.volume_client.volumes.get(
                self.instance.volume_id)
            if not volume:
                msg = (_('Failed to get volume %(vol_id)s') % {
                       'vol_id': self.instance.volume_id})
                raise cinder_exceptions.ClientException(msg)

            def volume_is_new_size():
                volume = self.instance.volume_client.volumes.get(
                    self.instance.volume_id)
                return volume.size == self.new_size
            utils.poll_until(volume_is_new_size,
                             sleep_time=2,
                             time_out=CONF.volume_time_out)

            self.instance.update_db(volume_size=self.new_size)
        except PollTimeOut:
            LOG.exception(_("Timeout trying to extend the volume %(vol_id)s "
                          "for instance %(id)s") % {
                          'vol_id': self.instance.volume_id,
                          'id': self.instance.id})
            volume = self.instance.volume_client.volumes.get(
                self.instance.volume_id)
            if volume.status == 'extending':
                self._fail(self._verify_extend)
            elif volume.size != self.new_size:
                self.instance.update_db(volume_size=volume.size)
                self._recover_full(self._verify_extend)
            raise
        except Exception:
            LOG.exception(_("Error encountered trying to verify extend for "
                          "the volume %(vol_id)s for instance %(id)s") % {
                          'vol_id': self.instance.volume_id,
                          'id': self.instance.id})
            self._recover_full(self._verify_extend)
            raise

    def _resize_active_volume(self):
        LOG.debug("begin _resize_active_volume for id: %(id)s" % {
                  'id': self.instance.id})
        self._stop_db()
        self._unmount_volume(recover_func=self._recover_restart)
        self._detach_volume(recover_func=self._recover_mount_restart)
        self._extend(recover_func=self._recover_full)
        self._verify_extend()
        # if anything fails after this point, recovery is futile
        self._attach_volume(recover_func=self._fail)
        self._resize_fs(recover_func=self._fail)
        self._mount_volume(recover_func=self._fail)
        self.instance.restart()
        LOG.debug("end _resize_active_volume for id: %(id)s" % {
                  'id': self.instance.id})

    def execute(self):
        LOG.debug("%(gt)s: Resizing instance %(id)s volume for server "
                  "%(server_id)s from %(old_volume_size)s to "
                  "%(new_size)r GB" % {'gt': greenthread.getcurrent(),
                  'id': self.instance.id,
                  'server_id': self.instance.server.id,
                  'old_volume_size': self.old_size,
                  'new_size': self.new_size})

        if self.instance.server.status == InstanceStatus.ACTIVE:
            self._resize_active_volume()
            self.instance.reset_task_status()
            # send usage event for size reported by cinder
            volume = self.instance.volume_client.volumes.get(
                self.instance.volume_id)
            launched_time = timeutils.isotime(self.instance.updated)
            modified_time = timeutils.isotime(self.instance.updated)
            self.instance.send_usage_event('modify_volume',
                                           old_volume_size=self.old_size,
                                           launched_at=launched_time,
                                           modify_at=modified_time,
                                           volume_size=volume.size)
        else:
            self.instance.reset_task_status()
            msg = _("Volume resize failed for instance %(id)s. The instance "
                    "must be in state %(state)s not %(inst_state)s.") % {
                        'id': self.instance.id,
                        'state': InstanceStatus.ACTIVE,
                        'inst_state': self.instance.server.status}
            raise TroveError(msg)


class ResizeActionBase(object):
    """Base class for executing a resize action."""

    def __init__(self, instance):
        """
        Creates a new resize action for a given instance
        :param instance: reference to existing instance that will be resized
        :type instance: trove.taskmanager.models.BuiltInstanceTasks
        """
        self.instance = instance

    def _assert_guest_is_ok(self):
        # The guest will never set the status to PAUSED.
        self.instance.set_datastore_status_to_paused()
        # Now we wait until it sets it to anything at all,
        # so we know it's alive.
        utils.poll_until(
            self._guest_is_awake,
            sleep_time=2,
            time_out=RESIZE_TIME_OUT)

    def _assert_nova_status_is_ok(self):
        # Make sure Nova thinks things went well.
        if not self.instance.server_status_matches(["VERIFY_RESIZE"]):
            msg = "Migration failed! status=%(act_status)s and " \
                  "not %(exp_status)s" % {
                      "act_status": self.instance.server.status,
                      "exp_status": 'VERIFY_RESIZE'}
            raise TroveError(msg)

    def _assert_datastore_is_ok(self):
        # Tell the guest to turn on datastore, and ensure the status becomes
        # RUNNING.
        self._start_datastore()
        utils.poll_until(
            self._datastore_is_online,
            sleep_time=2,
            time_out=RESIZE_TIME_OUT)

    def _assert_datastore_is_offline(self):
        # Tell the guest to turn off MySQL, and ensure the status becomes
        # SHUTDOWN.
        self.instance.guest.stop_db(do_not_start_on_reboot=True)
        utils.poll_until(
            self._datastore_is_offline,
            sleep_time=2,
            time_out=RESIZE_TIME_OUT)

    def _assert_processes_are_ok(self):
        """Checks the procs; if anything is wrong, reverts the operation."""
        # Tell the guest to turn back on, and make sure it can start.
        self._assert_guest_is_ok()
        LOG.debug("Nova guest is ok.")
        self._assert_datastore_is_ok()
        LOG.debug("Datastore is ok.")

    def _confirm_nova_action(self):
        LOG.debug("Instance %s calling Compute confirm resize..."
                  % self.instance.id)
        self.instance.server.confirm_resize()

    def _datastore_is_online(self):
        self.instance._refresh_datastore_status()
        return self.instance.is_datastore_running

    def _datastore_is_offline(self):
        self.instance._refresh_datastore_status()
        return (self.instance.datastore_status_matches(
                rd_instance.ServiceStatuses.SHUTDOWN))

    def _revert_nova_action(self):
        LOG.debug("Instance %s calling Compute revert resize..."
                  % self.instance.id)
        self.instance.server.revert_resize()

    def execute(self):
        """Initiates the action."""
        try:
            LOG.debug("Instance %s calling stop_db..."
                      % self.instance.id)
            self._assert_datastore_is_offline()
            self._perform_nova_action()
        finally:
            if self.instance.db_info.task_status != (
                    inst_models.InstanceTasks.NONE):
                self.instance.reset_task_status()

    def _guest_is_awake(self):
        self.instance._refresh_datastore_status()
        return not self.instance.datastore_status_matches(
            rd_instance.ServiceStatuses.PAUSED)

    def _perform_nova_action(self):
        """Calls Nova to resize or migrate an instance, and confirms."""
        LOG.debug("begin resize method _perform_nova_action instance: %s" %
                  self.instance.id)
        need_to_revert = False
        try:
            LOG.debug("Initiating nova action")
            self._initiate_nova_action()
            LOG.debug("Waiting for nova action")
            self._wait_for_nova_action()
            LOG.debug("Asserting nova status is ok")
            self._assert_nova_status_is_ok()
            need_to_revert = True
            LOG.debug("* * * REVERT BARRIER PASSED * * *")
            LOG.debug("Asserting nova action success")
            self._assert_nova_action_was_successful()
            LOG.debug("Asserting processes are OK")
            self._assert_processes_are_ok()
            LOG.debug("Confirming nova action")
            self._confirm_nova_action()
        except Exception as ex:
            LOG.exception(_("Exception during nova action."))
            if need_to_revert:
                LOG.error(_("Reverting action for instance %s") %
                          self.instance.id)
                self._revert_nova_action()
                self._wait_for_revert_nova_action()

            if self.instance.server_status_matches(['ACTIVE']):
                LOG.error(_("Restarting datastore."))
                self.instance.guest.restart()
            else:
                LOG.error(_("Cannot restart datastore because "
                            "Nova server status is not ACTIVE"))

            LOG.error(_("Error resizing instance %s.") % self.instance.id)
            raise ex

        LOG.debug("Recording success")
        self._record_action_success()
        LOG.debug("end resize method _perform_nova_action instance: %s" %
                  self.instance.id)

    def _wait_for_nova_action(self):
        # Wait for the flavor to change.
        def update_server_info():
            self.instance.refresh_compute_server_info()
            return not self.instance.server_status_matches(['RESIZE'])

        utils.poll_until(
            update_server_info,
            sleep_time=2,
            time_out=RESIZE_TIME_OUT)

    def _wait_for_revert_nova_action(self):
        # Wait for the server to return to ACTIVE after revert.
        def update_server_info():
            self.instance.refresh_compute_server_info()
            return self.instance.server_status_matches(['ACTIVE'])

        utils.poll_until(
            update_server_info,
            sleep_time=2,
            time_out=REVERT_TIME_OUT)


class ResizeAction(ResizeActionBase):
    def __init__(self, instance, old_flavor, new_flavor):
        """
        :type instance: trove.taskmanager.models.BuiltInstanceTasks
        :type old_flavor: dict
        :type new_flavor: dict
        """
        super(ResizeAction, self).__init__(instance)
        self.old_flavor = old_flavor
        self.new_flavor = new_flavor
        self.new_flavor_id = new_flavor['id']

    def _assert_nova_action_was_successful(self):
        # Do check to make sure the status and flavor id are correct.
        if str(self.instance.server.flavor['id']) != str(self.new_flavor_id):
            msg = "Assertion failed! flavor_id=%s and not %s" \
                  % (self.instance.server.flavor['id'], self.new_flavor_id)
            raise TroveError(msg)

    def _initiate_nova_action(self):
        self.instance.server.resize(self.new_flavor_id)

    def _revert_nova_action(self):
        LOG.debug("Instance %s calling Compute revert resize..."
                  % self.instance.id)
        LOG.debug("Repairing config.")
        try:
            config = self.instance._render_config(self.old_flavor)
            config = {'config_contents': config.config_contents}
            self.instance.guest.reset_configuration(config)
        except GuestTimeout:
            LOG.exception(_("Error sending reset_configuration call."))
        LOG.debug("Reverting resize.")
        super(ResizeAction, self)._revert_nova_action()

    def _record_action_success(self):
        LOG.debug("Updating instance %(id)s to flavor_id %(flavor_id)s."
                  % {'id': self.instance.id, 'flavor_id': self.new_flavor_id})
        self.instance.update_db(flavor_id=self.new_flavor_id,
                                task_status=inst_models.InstanceTasks.NONE)
        self.instance.send_usage_event(
            'modify_flavor',
            old_instance_size=self.old_flavor['ram'],
            instance_size=self.new_flavor['ram'],
            launched_at=timeutils.isotime(self.instance.updated),
            modify_at=timeutils.isotime(self.instance.updated),
            server=self.instance.server)

    def _start_datastore(self):
        config = self.instance._render_config(self.new_flavor)
        self.instance.guest.start_db_with_conf_changes(config.config_contents)


class MigrateAction(ResizeActionBase):
    def __init__(self, instance, host=None):
        super(MigrateAction, self).__init__(instance)
        self.instance = instance
        self.host = host

    def _assert_nova_action_was_successful(self):
        LOG.debug("Currently no assertions for a Migrate Action")

    def _initiate_nova_action(self):
        LOG.debug("Migrating instance %s without flavor change ..."
                  % self.instance.id)
        LOG.debug("Forcing migration to host(%s)" % self.host)
        self.instance.server.migrate(force_host=self.host)

    def _record_action_success(self):
        LOG.debug("Successfully finished Migration to "
                  "%(hostname)s: %(id)s" %
                  {'hostname': self.instance.hostname,
                   'id': self.instance.id})

    def _start_datastore(self):
        self.instance.guest.restart()

########NEW FILE########
__FILENAME__ = service
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from trove.openstack.common import log as logging


LOG = logging.getLogger(__name__)


class TaskService(object):
    """Task Manager interface."""


def app_factory(global_conf, **local_conf):
    return TaskService()

########NEW FILE########
__FILENAME__ = backups
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from proboscis.asserts import assert_equal
from proboscis.asserts import assert_not_equal
from proboscis.asserts import assert_raises
from proboscis.asserts import fail
from proboscis import test
from proboscis import SkipTest
from proboscis.decorators import time_out
from trove.common.utils import poll_until
from trove.common.utils import generate_uuid
from trove.common import exception
from trove.tests.util import create_dbaas_client
from trove.tests.util.users import Requirements
from trove.tests.config import CONFIG
from troveclient.compat import exceptions
from trove.tests.api.instances import WaitForGuestInstallationToFinish
from trove.tests.api.instances import instance_info
from trove.tests.api.instances import TIMEOUT_INSTANCE_CREATE
from trove.tests.api.instances import TIMEOUT_INSTANCE_DELETE
from trove.tests.api.instances import assert_unprocessable
from trove import tests


GROUP = "dbaas.api.backups"
BACKUP_NAME = 'backup_test'
BACKUP_DESC = 'test description'

TIMEOUT_BACKUP_CREATE = 60 * 30
TIMEOUT_BACKUP_DELETE = 120

backup_info = None
incremental_info = None
incremental_db = generate_uuid()
restore_instance_id = None
backup_count_prior_to_create = 0
backup_count_for_instance_prior_to_create = 0


@test(depends_on_classes=[WaitForGuestInstallationToFinish],
      groups=[GROUP, tests.INSTANCES])
class CreateBackups(object):

    @test
    def test_backup_create_instance_invalid(self):
        """test create backup with unknown instance"""
        invalid_inst_id = 'invalid-inst-id'
        try:
            instance_info.dbaas.backups.create(BACKUP_NAME, invalid_inst_id,
                                               BACKUP_DESC)
        except exceptions.BadRequest as e:
            resp, body = instance_info.dbaas.client.last_response
            assert_equal(resp.status, 400)
            assert_equal(e.message,
                         "Validation error: "
                         "backup['instance'] u'%s' does not match "
                         "'^([0-9a-fA-F]){8}-([0-9a-fA-F]){4}-"
                         "([0-9a-fA-F]){4}-([0-9a-fA-F]){4}-"
                         "([0-9a-fA-F]){12}$'" %
                         invalid_inst_id)

    @test
    def test_backup_create_instance_not_found(self):
        """test create backup with unknown instance"""
        assert_raises(exceptions.NotFound, instance_info.dbaas.backups.create,
                      BACKUP_NAME, generate_uuid(), BACKUP_DESC)

    @test
    def test_backup_create_instance(self):
        """test create backup for a given instance"""
        # Necessary to test that the count increases.
        global backup_count_prior_to_create
        backup_count_prior_to_create = len(instance_info.dbaas.backups.list())
        global backup_count_for_instance_prior_to_create
        backup_count_for_instance_prior_to_create = len(
            instance_info.dbaas.instances.backups(instance_info.id))

        result = instance_info.dbaas.backups.create(BACKUP_NAME,
                                                    instance_info.id,
                                                    BACKUP_DESC)
        global backup_info
        backup_info = result
        assert_equal(BACKUP_NAME, result.name)
        assert_equal(BACKUP_DESC, result.description)
        assert_equal(instance_info.id, result.instance_id)
        assert_equal('NEW', result.status)
        instance = instance_info.dbaas.instances.get(instance_info.id)
        assert_equal('BACKUP', instance.status)


@test(runs_after=[CreateBackups],
      groups=[GROUP, tests.INSTANCES])
class AfterBackupCreation(object):

    @test
    def test_instance_action_right_after_backup_create(self):
        """test any instance action while backup is running"""
        assert_unprocessable(instance_info.dbaas.instances.resize_instance,
                             instance_info.id, 1)

    @test
    def test_backup_create_another_backup_running(self):
        """test create backup when another backup is running"""
        assert_unprocessable(instance_info.dbaas.backups.create,
                             'backup_test2', instance_info.id,
                             'test description2')

    @test
    def test_backup_delete_still_running(self):
        """test delete backup when it is running"""
        result = instance_info.dbaas.backups.list()
        backup = result[0]
        assert_unprocessable(instance_info.dbaas.backups.delete, backup.id)


@test(runs_after=[AfterBackupCreation],
      groups=[GROUP, tests.INSTANCES])
class WaitForBackupCreateToFinish(object):
    """
        Wait until the backup create is finished.
    """

    @test
    @time_out(TIMEOUT_BACKUP_CREATE)
    def test_backup_created(self):
        # This version just checks the REST API status.
        def result_is_active():
            backup = instance_info.dbaas.backups.get(backup_info.id)
            if backup.status == "COMPLETED":
                return True
            else:
                assert_not_equal("FAILED", backup.status)
                return False

        poll_until(result_is_active)


@test(depends_on=[WaitForBackupCreateToFinish],
      groups=[GROUP, tests.INSTANCES])
class ListBackups(object):

    @test
    def test_backup_list(self):
        """test list backups"""
        result = instance_info.dbaas.backups.list()
        assert_equal(backup_count_prior_to_create + 1, len(result))
        backup = result[0]
        assert_equal(BACKUP_NAME, backup.name)
        assert_equal(BACKUP_DESC, backup.description)
        assert_not_equal(0.0, backup.size)
        assert_equal(instance_info.id, backup.instance_id)
        assert_equal('COMPLETED', backup.status)

    @test
    def test_backup_list_for_instance(self):
        """test backup list for instance"""
        result = instance_info.dbaas.instances.backups(instance_info.id)
        assert_equal(backup_count_for_instance_prior_to_create + 1,
                     len(result))
        backup = result[0]
        assert_equal(BACKUP_NAME, backup.name)
        assert_equal(BACKUP_DESC, backup.description)
        assert_not_equal(0.0, backup.size)
        assert_equal(instance_info.id, backup.instance_id)
        assert_equal('COMPLETED', backup.status)

    @test
    def test_backup_get(self):
        """test get backup"""
        backup = instance_info.dbaas.backups.get(backup_info.id)
        assert_equal(backup_info.id, backup.id)
        assert_equal(backup_info.name, backup.name)
        assert_equal(backup_info.description, backup.description)
        assert_equal(instance_info.id, backup.instance_id)
        assert_not_equal(0.0, backup.size)
        assert_equal('COMPLETED', backup.status)

        # Test to make sure that user in other tenant is not able
        # to GET this backup
        reqs = Requirements(is_admin=False)
        other_user = CONFIG.users.find_user(
            reqs,
            black_list=[instance_info.user.auth_user])
        other_client = create_dbaas_client(other_user)
        assert_raises(exceptions.NotFound, other_client.backups.get,
                      backup_info.id)


@test(runs_after=[ListBackups],
      depends_on=[WaitForBackupCreateToFinish],
      groups=[GROUP, tests.INSTANCES])
class IncrementalBackups(object):

    @test
    def test_create_db(self):
        databases = [{'name': incremental_db}]
        instance_info.dbaas.databases.create(instance_info.id, databases)
        assert_equal(202, instance_info.dbaas.last_http_code)

    @test(runs_after=['test_create_db'])
    def test_create_incremental_backup(self):
        result = instance_info.dbaas.backups.create("incremental-backup",
                                                    backup_info.instance_id,
                                                    parent_id=backup_info.id)
        global incremental_info
        incremental_info = result
        assert_equal(202, instance_info.dbaas.last_http_code)

        # Wait for the backup to finish
        def result_is_active():
            backup = instance_info.dbaas.backups.get(incremental_info.id)
            if backup.status == "COMPLETED":
                return True
            else:
                assert_not_equal("FAILED", backup.status)
                return False

        poll_until(result_is_active, time_out=TIMEOUT_BACKUP_CREATE)
        assert_equal(backup_info.id, incremental_info.parent_id)


@test(groups=[GROUP, tests.INSTANCES])
class RestoreUsingBackup(object):

    @classmethod
    def _restore(cls, backup_ref):
        restorePoint = {"backupRef": backup_ref}
        result = instance_info.dbaas.instances.create(
            instance_info.name + "_restore",
            instance_info.dbaas_flavor_href,
            instance_info.volume,
            restorePoint=restorePoint)
        assert_equal(200, instance_info.dbaas.last_http_code)
        assert_equal("BUILD", result.status)
        return result.id

    @test(depends_on=[WaitForBackupCreateToFinish])
    def test_restore(self):
        global restore_instance_id
        restore_instance_id = self._restore(backup_info.id)

    @test(depends_on=[IncrementalBackups])
    def test_restore_incremental(self):
        global incremental_restore_instance_id
        incremental_restore_instance_id = self._restore(incremental_info.id)


@test(groups=[GROUP, tests.INSTANCES])
class WaitForRestoreToFinish(object):

    @classmethod
    def _poll(cls, instance_id_to_poll):
        """Shared "instance restored" test logic."""
        # This version just checks the REST API status.
        def result_is_active():
            instance = instance_info.dbaas.instances.get(instance_id_to_poll)
            if instance.status == "ACTIVE":
                return True
            else:
                # If its not ACTIVE, anything but BUILD must be
                # an error.
                assert_equal("BUILD", instance.status)
                if instance_info.volume is not None:
                    assert_equal(instance.volume.get('used', None), None)
                return False

        poll_until(result_is_active, time_out=TIMEOUT_INSTANCE_CREATE,
                   sleep_time=10)

    """
        Wait until the instance is finished restoring from full backup.
    """
    @test(depends_on=[RestoreUsingBackup.test_restore])
    def test_instance_restored(self):
        try:
            self._poll(restore_instance_id)
        except exception.PollTimeOut:
            fail('Timed out')

    """
        Wait until the instance is finished restoring from incremental backup.
    """
    @test(depends_on=[RestoreUsingBackup.test_restore_incremental])
    def test_instance_restored_incremental(self):
        try:
            self._poll(incremental_restore_instance_id)
        except exception.PollTimeOut:
            fail('Timed out')


@test(enabled=(not CONFIG.fake_mode),
      groups=[GROUP, tests.INSTANCES])
class VerifyRestore(object):

    @classmethod
    def _poll(cls, instance_id, db):
        def db_is_found():
            databases = instance_info.dbaas.databases.list(instance_id)
            if db in [d.name for d in databases]:
                return True
            else:
                return False

        poll_until(db_is_found, time_out=60 * 10, sleep_time=10)

    @test(depends_on=[WaitForRestoreToFinish.
          test_instance_restored_incremental])
    def test_database_restored_incremental(self):
        try:
            self._poll(incremental_restore_instance_id, incremental_db)
        except exception.PollTimeOut:
            fail('Timed out')


@test(groups=[GROUP, tests.INSTANCES])
class DeleteRestoreInstance(object):

    @classmethod
    def _delete(cls, instance_id):
        """test delete restored instance"""
        instance_info.dbaas.instances.delete(instance_id)
        assert_equal(202, instance_info.dbaas.last_http_code)

        def instance_is_gone():
            try:
                instance_info.dbaas.instances.get(instance_id)
                return False
            except exceptions.NotFound:
                return True

        poll_until(instance_is_gone, time_out=TIMEOUT_INSTANCE_DELETE)
        assert_raises(exceptions.NotFound, instance_info.dbaas.instances.get,
                      instance_id)

    @test(runs_after=[WaitForRestoreToFinish.test_instance_restored])
    def test_delete_restored_instance(self):
        try:
            self._delete(restore_instance_id)
        except exception.PollTimeOut:
            fail('Timed out')

    @test(runs_after=[VerifyRestore.test_database_restored_incremental])
    def test_delete_restored_instance_incremental(self):
        try:
            self._delete(incremental_restore_instance_id)
        except exception.PollTimeOut:
            fail('Timed out')


@test(runs_after=[DeleteRestoreInstance],
      groups=[GROUP, tests.INSTANCES])
class DeleteBackups(object):

    @test
    def test_backup_delete_not_found(self):
        """test delete unknown backup"""
        assert_raises(exceptions.NotFound, instance_info.dbaas.backups.delete,
                      'nonexistent_backup')

    @test
    def test_backup_delete_other(self):
        """Test another user cannot delete backup"""
        # Test to make sure that user in other tenant is not able
        # to DELETE this backup
        reqs = Requirements(is_admin=False)
        other_user = CONFIG.users.find_user(
            reqs,
            black_list=[instance_info.user.auth_user])
        other_client = create_dbaas_client(other_user)
        assert_raises(exceptions.NotFound, other_client.backups.delete,
                      backup_info.id)

    @test(runs_after=[test_backup_delete_other])
    def test_backup_delete(self):
        """test backup deletion"""
        instance_info.dbaas.backups.delete(backup_info.id)
        assert_equal(202, instance_info.dbaas.last_http_code)

        def backup_is_gone():
            try:
                instance_info.dbaas.backups.get(backup_info.id)
                return False
            except exceptions.NotFound:
                return True

        poll_until(backup_is_gone, time_out=TIMEOUT_BACKUP_DELETE)

    @test(runs_after=[test_backup_delete])
    def test_incremental_deleted(self):
        """test backup children are deleted"""
        if incremental_info is None:
            raise SkipTest("Incremental Backup not created")
        assert_raises(exceptions.NotFound, instance_info.dbaas.backups.get,
                      incremental_info.id)

########NEW FILE########
__FILENAME__ = configurations
#    Copyright 2014 Rackspace Hosting
#    All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


import json
from datetime import datetime
from proboscis import SkipTest
from proboscis import test
from proboscis.asserts import assert_equal
from proboscis.asserts import assert_raises
from proboscis.asserts import assert_true
from proboscis.asserts import assert_not_equal
from proboscis.decorators import time_out
from trove.common.utils import poll_until
from trove.tests.api.instances import assert_unprocessable
from trove.tests.api.instances import InstanceTestInfo
from trove.tests.api.instances import instance_info
from trove.tests.api.instances import TIMEOUT_INSTANCE_CREATE
from trove.tests.api.instances import TIMEOUT_INSTANCE_DELETE
from trove.tests.api.instances import WaitForGuestInstallationToFinish
from trove.tests.config import CONFIG
from trove.tests.util import create_dbaas_client
from trove.tests.util import test_config
from trove.tests.util.check import AttrCheck
from trove.tests.util.check import CollectionCheck
from trove.tests.util.check import TypeCheck
from trove.tests.util.mysql import create_mysql_connection
from trove.tests.util.users import Requirements
from troveclient.compat import exceptions


GROUP = "dbaas.api.configurations"
CONFIG_NAME = "test_configuration"
CONFIG_DESC = "configuration description"

configuration_default = None
configuration_info = None
configuration_href = None
configuration_instance = InstanceTestInfo()
configuration_instance_id = None
sql_variables = [
    'key_buffer_size',
    'connect_timeout',
    'join_buffer_size',
]


# helper methods to validate configuration is applied to instance
def _execute_query(host, user_name, password, query):
    print(host, user_name, password, query)
    with create_mysql_connection(host, user_name, password) as db:
        result = db.execute(query)
        return result
    assert_true(False, "something went wrong in the sql connection")


def _get_address(instance_id):
    result = instance_info.dbaas_admin.mgmt.instances.show(instance_id)
    return result.ip[0]


def _test_configuration_is_applied_to_instance(instance, configuration_id):
    if CONFIG.fake_mode:
        raise SkipTest("configuration from sql does not work in fake mode")
    instance_test = instance_info.dbaas.instances.get(instance.id)
    assert_equal(configuration_id, instance_test.configuration['id'])
    if configuration_id:
        testconfig_info = instance_info.dbaas.configurations.get(
            configuration_id)
    else:
        testconfig_info = instance_info.dbaas.instance.configuration(
            instance.id)
        testconfig_info['configuration']
    conf_instances = instance_info.dbaas.configurations.instances(
        configuration_id)
    config_instance_ids = [inst.id for inst in conf_instances]
    assert_true(instance_test.id in config_instance_ids)
    cfg_names = testconfig_info.values.keys()

    host = _get_address(instance.id)
    for user in instance.users:
        username = user['name']
        password = user['password']
        concat_variables = "','".join(cfg_names)
        query = ("show variables where Variable_name "
                 "in ('%s');" % concat_variables)
        actual_values = _execute_query(host, username, password, query)
    print("actual_values %s" % actual_values)
    print("testconfig_info.values %s" % testconfig_info.values)
    assert_true(len(actual_values) == len(cfg_names))

    # check the configs exist
    attrcheck = AttrCheck()
    expected_attrs = [actual_key for actual_key, actual_value in actual_values]
    attrcheck.attrs_exist(testconfig_info.values, expected_attrs,
                          msg="Configurations parameters")

    def _get_parameter_type(name):
        instance_info.dbaas.configuration_parameters.get_parameter(
            instance_info.dbaas_datastore,
            instance_info.dbaas_datastore_version,
            name)
        resp, body = instance_info.dbaas.client.last_response
        print(resp)
        print(body)
        return json.loads(body)['type']

    # check the config values are correct
    for key, value in actual_values:
        key_type = _get_parameter_type(key)
        # mysql returns 'ON' and 'OFF' for True and False respectively
        if value == 'ON':
            converted_key_value = (str(key), 1)
        elif value == 'OFF':
            converted_key_value = (str(key), 0)
        else:
            if key_type == 'integer':
                value = int(value)
            converted_key_value = (str(key), value)
        print("converted_key_value: %s" % str(converted_key_value))
        assert_true(converted_key_value in testconfig_info.values.items())


@test(depends_on_classes=[WaitForGuestInstallationToFinish], groups=[GROUP])
class CreateConfigurations(object):

    @test
    def test_expected_configurations_parameters(self):
        """test get expected configurations parameters"""
        expected_attrs = ["configuration-parameters"]
        instance_info.dbaas.configuration_parameters.parameters(
            instance_info.dbaas_datastore,
            instance_info.dbaas_datastore_version)
        resp, body = instance_info.dbaas.client.last_response
        attrcheck = AttrCheck()
        config_parameters_dict = json.loads(body)
        attrcheck.attrs_exist(config_parameters_dict, expected_attrs,
                              msg="Configurations parameters")
        # sanity check that a few options are in the list
        config_params_list = config_parameters_dict['configuration-parameters']
        config_param_keys = []
        for param in config_params_list:
            config_param_keys.append(param['name'])
        expected_config_params = ['key_buffer_size', 'connect_timeout']
        # check for duplicate configuration parameters
        msg = "check for duplicate configuration parameters"
        assert_equal(len(config_param_keys), len(set(config_param_keys)), msg)
        for expected_config_item in expected_config_params:
            assert_true(expected_config_item in config_param_keys)

    @test
    def test_expected_get_configuration_parameter(self):
        # tests get on a single parameter to verify it has expected attributes
        param = 'key_buffer_size'
        expected_config_params = ['name', 'restart_required', 'max',
                                  'min', 'type']
        instance_info.dbaas.configuration_parameters.get_parameter(
            instance_info.dbaas_datastore,
            instance_info.dbaas_datastore_version,
            param)
        resp, body = instance_info.dbaas.client.last_response
        print(resp)
        print(body)
        attrcheck = AttrCheck()
        config_parameter_dict = json.loads(body)
        print(config_parameter_dict)
        attrcheck.attrs_exist(config_parameter_dict, expected_config_params,
                              msg="Get Configuration parameter")
        assert_equal(param, config_parameter_dict['name'])

    @test
    def test_configurations_create_invalid_values(self):
        """test create configurations with invalid values"""
        values = '{"this_is_invalid": 123}'
        assert_unprocessable(instance_info.dbaas.configurations.create,
                             CONFIG_NAME, values, CONFIG_DESC)

    @test
    def test_configurations_create_invalid_value_type(self):
        """test create configuration with invalild value type"""
        values = '{"key_buffer_size": "this is a string not int"}'
        assert_unprocessable(instance_info.dbaas.configurations.create,
                             CONFIG_NAME, values, CONFIG_DESC)

    @test
    def test_configurations_create_value_out_of_bounds(self):
        """test create configuration with value out of bounds"""
        values = '{"connect_timeout": 1000000}'
        assert_unprocessable(instance_info.dbaas.configurations.create,
                             CONFIG_NAME, values, CONFIG_DESC)
        values = '{"connect_timeout": -10}'
        assert_unprocessable(instance_info.dbaas.configurations.create,
                             CONFIG_NAME, values, CONFIG_DESC)

    @test
    def test_valid_configurations_create(self):
        # create a configuration with valid parameters
        values = ('{"connect_timeout": 120, "local_infile": true, '
                  '"collation_server": "latin1_swedish_ci"}')
        expected_values = json.loads(values)
        result = instance_info.dbaas.configurations.create(CONFIG_NAME,
                                                           values,
                                                           CONFIG_DESC)
        resp, body = instance_info.dbaas.client.last_response
        assert_equal(resp.status, 200)
        global configuration_info
        configuration_info = result
        assert_equal(configuration_info.name, CONFIG_NAME)
        assert_equal(configuration_info.description, CONFIG_DESC)
        assert_equal(configuration_info.values, expected_values)

    @test(runs_after=[test_valid_configurations_create])
    def test_appending_to_existing_configuration(self):
        # test being able to update and insert new parameter name and values
        # to an existing configuration
        values = '{"join_buffer_size": 1048576, "connect_timeout": 60}'
        instance_info.dbaas.configurations.edit(configuration_info.id,
                                                values)
        resp, body = instance_info.dbaas.client.last_response
        assert_equal(resp.status, 200)


@test(runs_after=[CreateConfigurations], groups=[GROUP])
class AfterConfigurationsCreation(object):

    @test
    def test_assign_configuration_to_invalid_instance(self):
        # test assigning to an instance that does not exist
        invalid_id = "invalid-inst-id"
        try:
            instance_info.dbaas.instances.modify(invalid_id,
                                                 configuration_info.id)
        except exceptions.NotFound:
            resp, body = instance_info.dbaas.client.last_response
            assert_equal(resp.status, 404)

    @test
    def test_assign_configuration_to_valid_instance(self):
        # test assigning a configuration to an instance
        print("instance_info.id: %s" % instance_info.id)
        print("configuration_info: %s" % configuration_info)
        print("configuration_info.id: %s" % configuration_info.id)
        config_id = configuration_info.id
        instance_info.dbaas.instances.modify(instance_info.id,
                                             configuration=config_id)
        resp, body = instance_info.dbaas.client.last_response
        assert_equal(resp.status, 202)

    @test(depends_on=[test_assign_configuration_to_valid_instance])
    def test_assign_configuration_to_instance_with_config(self):
        # test assigning a configuration to an instance that
        # already has an assigned configuration
        config_id = configuration_info.id
        assert_raises(exceptions.BadRequest,
                      instance_info.dbaas.instances.modify, instance_info.id,
                      configuration=config_id)

    @test(depends_on=[test_assign_configuration_to_valid_instance])
    @time_out(10)
    def test_get_configuration_details_from_instance_validation(self):
        # validate that the configuraiton was applied correctly to the instance
        inst = instance_info.dbaas.instances.get(instance_info.id)
        configuration_id = inst.configuration['id']
        assert_not_equal(None, inst.configuration['id'])
        _test_configuration_is_applied_to_instance(instance_info,
                                                   configuration_id)

    @test
    def test_configurations_get(self):
        # test that the instance shows up on the assigned configuration
        result = instance_info.dbaas.configurations.get(configuration_info.id)
        assert_equal(configuration_info.id, result.id)
        assert_equal(configuration_info.name, result.name)
        assert_equal(configuration_info.description, result.description)

        # check the result field types
        with TypeCheck("configuration", result) as check:
            check.has_field("id", basestring)
            check.has_field("name", basestring)
            check.has_field("description", basestring)
            check.has_field("values", dict)

        print(result.values)
        with CollectionCheck("configuration_values", result.values) as check:
            # check each item has the correct type according to the rules
            for (item_key, item_val) in result.values.iteritems():
                print("item_key: %s" % item_key)
                print("item_val: %s" % item_val)
                dbaas = instance_info.dbaas
                param = dbaas.configuration_parameters.get_parameter(
                    instance_info.dbaas_datastore,
                    instance_info.dbaas_datastore_version,
                    item_key)
                if param.type == 'integer':
                    check.has_element(item_key, int)
                if param.type == 'string':
                    check.has_element(item_key, basestring)
                if param.type == 'boolean':
                    check.has_element(item_key, bool)

        # Test to make sure that another user is not able to GET this config
        reqs = Requirements(is_admin=False)
        test_auth_user = instance_info.user.auth_user
        other_user = CONFIG.users.find_user(reqs, black_list=[test_auth_user])
        other_user_tenant_id = other_user.tenant_id
        client_tenant_id = instance_info.user.tenant_id
        if other_user_tenant_id == client_tenant_id:
            other_user = CONFIG.users.find_user(reqs,
                                                black_list=[
                                                instance_info.user.auth_user,
                                                other_user])
        print(other_user)
        print(other_user.__dict__)
        other_client = create_dbaas_client(other_user)
        assert_raises(exceptions.NotFound, other_client.configurations.get,
                      configuration_info.id)


@test(runs_after=[AfterConfigurationsCreation], groups=[GROUP])
class ListConfigurations(object):

    @test
    def test_configurations_list(self):
        # test listing configurations show up
        result = instance_info.dbaas.configurations.list()
        exists = [config for config in result if
                  config.id == configuration_info.id]
        assert_equal(1, len(exists))
        configuration = exists[0]
        assert_equal(configuration.id, configuration_info.id)
        assert_equal(configuration.name, configuration_info.name)
        assert_equal(configuration.description, configuration_info.description)

    @test
    def test_configurations_list_for_instance(self):
        # test getting an instance shows the configuration assigned shows up
        instance = instance_info.dbaas.instances.get(instance_info.id)
        assert_equal(instance.configuration['id'], configuration_info.id)
        assert_equal(instance.configuration['name'], configuration_info.name)
        # expecting two things in links, href and bookmark
        assert_equal(2, len(instance.configuration['links']))
        link = instance.configuration['links'][0]
        global configuration_href
        configuration_href = link['href']

    @test
    def test_get_default_configuration_on_instance(self):
        # test the api call to get the default template of an instance exists
        result = instance_info.dbaas.instances.configuration(instance_info.id)
        global configuration_default
        configuration_default = result
        assert_not_equal(None, result.configuration)

    @test
    def test_changing_configuration_with_nondynamic_parameter(self):
        # test that changing a non-dynamic parameter is applied to instance
        # and show that the instance requires a restart
        values = ('{"join_buffer_size":1048576,'
                  '"innodb_buffer_pool_size":57671680}')
        instance_info.dbaas.configurations.update(configuration_info.id,
                                                  values)
        resp, body = instance_info.dbaas.client.last_response
        assert_equal(resp.status, 202)

        instance_info.dbaas.configurations.get(configuration_info.id)
        resp, body = instance_info.dbaas.client.last_response
        assert_equal(resp.status, 200)

    @test(depends_on=[test_changing_configuration_with_nondynamic_parameter])
    @time_out(20)
    def test_waiting_for_instance_in_restart_required(self):
        def result_is_not_active():
            instance = instance_info.dbaas.instances.get(
                instance_info.id)
            if instance.status == "ACTIVE":
                return False
            else:
                return True
        poll_until(result_is_not_active)

        instance = instance_info.dbaas.instances.get(instance_info.id)
        resp, body = instance_info.dbaas.client.last_response
        assert_equal(resp.status, 200)
        print(instance.status)
        assert_equal('RESTART_REQUIRED', instance.status)

    @test(depends_on=[test_waiting_for_instance_in_restart_required])
    def test_restart_service_should_return_active(self):
        # test that after restarting the instance it becomes active
        instance_info.dbaas.instances.restart(instance_info.id)
        resp, body = instance_info.dbaas.client.last_response
        assert_equal(resp.status, 202)

        def result_is_active():
            instance = instance_info.dbaas.instances.get(
                instance_info.id)
            if instance.status == "ACTIVE":
                return True
            else:
                assert_equal("REBOOT", instance.status)
                return False
        poll_until(result_is_active)

    @test(depends_on=[test_restart_service_should_return_active])
    @time_out(10)
    def test_get_configuration_details_from_instance_validation(self):
        # validate that the configuraiton was applied correctly to the instance
        inst = instance_info.dbaas.instances.get(instance_info.id)
        configuration_id = inst.configuration['id']
        assert_not_equal(None, inst.configuration['id'])
        _test_configuration_is_applied_to_instance(instance_info,
                                                   configuration_id)


@test(runs_after=[ListConfigurations], groups=[GROUP])
class StartInstanceWithConfiguration(object):

    @test
    def test_start_instance_with_configuration(self):
        # test that a new instance will apply the configuration on create
        if test_config.auth_strategy == "fake":
            raise SkipTest("Skipping instance start with configuration "
                           "test for fake mode.")
        global configuration_instance
        databases = []
        databases.append({"name": "firstdbconfig", "character_set": "latin2",
                          "collate": "latin2_general_ci"})
        databases.append({"name": "db2"})
        configuration_instance.databases = databases
        users = []
        users.append({"name": "liteconf", "password": "liteconfpass",
                      "databases": [{"name": "firstdbconfig"}]})
        configuration_instance.users = users
        configuration_instance.name = "TEST_" + str(datetime.now()) + "_config"
        flavor_href = instance_info.dbaas_flavor_href
        configuration_instance.dbaas_flavor_href = flavor_href
        configuration_instance.volume = instance_info.volume

        result = instance_info.dbaas.instances.create(
            configuration_instance.name,
            configuration_instance.dbaas_flavor_href,
            configuration_instance.volume,
            configuration_instance.databases,
            configuration_instance.users,
            availability_zone="nova",
            configuration=configuration_href)
        assert_equal(200, instance_info.dbaas.last_http_code)
        assert_equal("BUILD", result.status)
        configuration_instance.id = result.id


@test(runs_after=[StartInstanceWithConfiguration], groups=[GROUP])
class WaitForConfigurationInstanceToFinish(object):

    @test
    @time_out(TIMEOUT_INSTANCE_CREATE)
    def test_instance_with_configuration_active(self):
        # wait for the instance to become active
        if test_config.auth_strategy == "fake":
            raise SkipTest("Skipping instance start with configuration "
                           "test for fake mode.")

        def result_is_active():
            instance = instance_info.dbaas.instances.get(
                configuration_instance.id)
            if instance.status == "ACTIVE":
                return True
            else:
                assert_equal("BUILD", instance.status)
                return False

        poll_until(result_is_active)

    @test(depends_on=[test_instance_with_configuration_active])
    @time_out(10)
    def test_get_configuration_details_from_instance_validation(self):
        # validate that the configuraiton was applied correctly to the instance
        inst = instance_info.dbaas.instances.get(configuration_instance.id)
        configuration_id = inst.configuration['id']
        assert_not_equal(None, inst.configuration['id'])
        _test_configuration_is_applied_to_instance(configuration_instance,
                                                   configuration_id)


@test(runs_after=[WaitForConfigurationInstanceToFinish], groups=[GROUP])
class DeleteConfigurations(object):

    @test
    def test_delete_invalid_configuration_not_found(self):
        # test deleting a configuration that does not exist throws exception
        invalid_configuration_id = "invalid-config-id"
        assert_raises(exceptions.NotFound,
                      instance_info.dbaas.configurations.delete,
                      invalid_configuration_id)

    @test
    def test_unable_delete_instance_configurations(self):
        # test deleting a configuration that is assigned to
        # an instance is not allowed.
        assert_raises(exceptions.BadRequest,
                      instance_info.dbaas.configurations.delete,
                      configuration_info.id)

    @test(depends_on=[test_unable_delete_instance_configurations])
    @time_out(30)
    def test_unassign_configuration_from_instances(self):
        # test to unassign configuration from instance
        instance_info.dbaas.instances.modify(configuration_instance.id,
                                             configuration="")
        resp, body = instance_info.dbaas.client.last_response
        assert_equal(resp.status, 202)
        instance_info.dbaas.instances.get(configuration_instance.id)
        #test that config group is not removed
        instance_info.dbaas.instances.modify(instance_info.id,
                                             configuration=None)
        resp, body = instance_info.dbaas.client.last_response
        assert_equal(resp.status, 202)
        instance_info.dbaas.instances.get(instance_info.id)

        def result_has_no_configuration():
            instance = instance_info.dbaas.instances.get(inst_info.id)
            if hasattr(instance, 'configuration'):
                return False
            else:
                return True
        inst_info = instance_info
        poll_until(result_has_no_configuration)
        inst_info = configuration_instance
        poll_until(result_has_no_configuration)
        instance = instance_info.dbaas.instances.get(instance_info.id)
        assert_equal('RESTART_REQUIRED', instance.status)

    @test(depends_on=[test_unassign_configuration_from_instances])
    def test_assign_in_wrong_state(self):
        # test assigning a config to an instance in RESTART state
        assert_raises(exceptions.BadRequest,
                      instance_info.dbaas.instances.modify,
                      configuration_instance.id,
                      configuration=configuration_info.id)

    @test(depends_on=[test_assign_in_wrong_state])
    def test_no_instances_on_configuration(self):
        # test there is no configuration on the instance after unassigning
        result = instance_info.dbaas.configurations.get(configuration_info.id)
        assert_equal(configuration_info.id, result.id)
        assert_equal(configuration_info.name, result.name)
        assert_equal(configuration_info.description, result.description)
        print(configuration_instance.id)
        print(instance_info.id)

    @test(depends_on=[test_no_instances_on_configuration])
    def test_delete_unassigned_configuration(self):
        # test that we can delete the configuration after no instances are
        # assigned to it any longer
        instance_info.dbaas.configurations.delete(configuration_info.id)
        resp, body = instance_info.dbaas.client.last_response
        assert_equal(resp.status, 202)

    @test(depends_on=[test_unassign_configuration_from_instances])
    @time_out(120)
    def test_restart_service_after_unassign_return_active(self):
        def result_is_not_active():
            instance = instance_info.dbaas.instances.get(
                instance_info.id)
            if instance.status == "ACTIVE":
                return False
            else:
                return True
        poll_until(result_is_not_active)

        config = instance_info.dbaas.configurations.list()
        print(config)
        instance = instance_info.dbaas.instances.get(instance_info.id)
        print(instance.__dict__)
        resp, body = instance_info.dbaas.client.last_response
        assert_equal(resp.status, 200)
        print(instance.status)
        assert_equal('RESTART_REQUIRED', instance.status)

    @test(depends_on=[test_restart_service_after_unassign_return_active])
    @time_out(120)
    def test_restart_service_should_return_active(self):
        # test that after restarting the instance it becomes active
        instance_info.dbaas.instances.restart(instance_info.id)
        resp, body = instance_info.dbaas.client.last_response
        assert_equal(resp.status, 202)

        def result_is_active():
            instance = instance_info.dbaas.instances.get(
                instance_info.id)
            if instance.status == "ACTIVE":
                return True
            else:
                assert_equal("REBOOT", instance.status)
                return False
        poll_until(result_is_active)

    @test(depends_on=[test_delete_unassigned_configuration])
    @time_out(TIMEOUT_INSTANCE_DELETE)
    def test_delete_configuration_instance(self):
        # test that we can delete the instance even though there is a
        # configuration applied to the instance
        instance_info.dbaas.instances.delete(configuration_instance.id)
        assert_equal(202, instance_info.dbaas.last_http_code)

        def instance_is_gone():
            try:
                instance_info.dbaas.instances.get(configuration_instance.id)
                return False
            except exceptions.NotFound:
                return True

        poll_until(instance_is_gone)
        assert_raises(exceptions.NotFound, instance_info.dbaas.instances.get,
                      configuration_instance.id)

########NEW FILE########
__FILENAME__ = databases
#    Copyright 2011 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import time

from troveclient.compat import exceptions

from proboscis import before_class
from proboscis import test
from proboscis.asserts import assert_equal
from proboscis.asserts import assert_false
from proboscis.asserts import assert_raises
from proboscis.asserts import assert_true
from proboscis.decorators import time_out

from trove import tests

from trove.tests import util
from trove.tests.api.instances import GROUP_START
from trove.tests.api.instances import instance_info
from trove.tests.util import test_config

GROUP = "dbaas.api.databases"
FAKE = test_config.values['fake_mode']


@test(depends_on_groups=[GROUP_START],
      groups=[tests.INSTANCES, "dbaas.guest.mysql"],
      enabled=not test_config.values['fake_mode'])
class TestMysqlAccess(object):
    """
        Make sure that MySQL server was secured.
    """

    @time_out(60 * 2)
    @test
    def test_mysql_admin(self):
        """Ensure we aren't allowed access with os_admin and wrong password."""
        util.mysql_connection().assert_fails(
            instance_info.get_address(), "os_admin", "asdfd-asdf234")

    @test
    def test_mysql_root(self):
        """Ensure we aren't allowed access with root and wrong password."""
        util.mysql_connection().assert_fails(
            instance_info.get_address(), "root", "dsfgnear")


@test(depends_on_groups=[GROUP_START],
      depends_on_classes=[TestMysqlAccess],
      groups=[tests.DBAAS_API, GROUP, tests.INSTANCES])
class TestDatabases(object):
    """
    Test the creation and deletion of additional MySQL databases
    """

    dbname = "third #?@some_-"
    dbname_urlencoded = "third%20%23%3F%40some_-"

    dbname2 = "seconddb"
    created_dbs = [dbname, dbname2]
    system_dbs = ['information_schema', 'mysql', 'lost+found']

    @before_class
    def setUp(self):
        self.dbaas = util.create_dbaas_client(instance_info.user)
        self.dbaas_admin = util.create_dbaas_client(instance_info.admin_user)

    @test
    def test_cannot_create_taboo_database_names(self):
        for name in self.system_dbs:
            databases = [{"name": name, "character_set": "latin2",
                          "collate": "latin2_general_ci"}]
            assert_raises(exceptions.BadRequest, self.dbaas.databases.create,
                          instance_info.id, databases)
            assert_equal(400, self.dbaas.last_http_code)

    @test
    def test_create_database(self):
        databases = []
        databases.append({"name": self.dbname, "character_set": "latin2",
                          "collate": "latin2_general_ci"})
        databases.append({"name": self.dbname2})

        self.dbaas.databases.create(instance_info.id, databases)
        assert_equal(202, self.dbaas.last_http_code)
        if not FAKE:
            time.sleep(5)

    @test(depends_on=[test_create_database])
    def test_create_database_list(self):
        databases = self.dbaas.databases.list(instance_info.id)
        assert_equal(200, self.dbaas.last_http_code)
        found = False
        for db in self.created_dbs:
            for result in databases:
                if result.name == db:
                    found = True
            assert_true(found, "Database '%s' not found in result" % db)
            found = False

    @test(depends_on=[test_create_database])
    def test_fails_when_creating_a_db_twice(self):
        databases = []
        databases.append({"name": self.dbname, "character_set": "latin2",
                          "collate": "latin2_general_ci"})
        databases.append({"name": self.dbname2})

        assert_raises(exceptions.BadRequest, self.dbaas.databases.create,
                      instance_info.id, databases)
        assert_equal(400, self.dbaas.last_http_code)

    @test
    def test_create_database_list_system(self):
        #Databases that should not be returned in the list
        databases = self.dbaas.databases.list(instance_info.id)
        assert_equal(200, self.dbaas.last_http_code)
        found = False
        for db in self.system_dbs:
            found = any(result.name == db for result in databases)
            msg = "Database '%s' SHOULD NOT be found in result" % db
            assert_false(found, msg)
            found = False

    @test
    def test_create_database_on_missing_instance(self):
        databases = [{"name": "invalid_db", "character_set": "latin2",
                      "collate": "latin2_general_ci"}]
        assert_raises(exceptions.NotFound, self.dbaas.databases.create,
                      -1, databases)
        assert_equal(404, self.dbaas.last_http_code)

    @test(runs_after=[test_create_database])
    def test_delete_database(self):
        self.dbaas.databases.delete(instance_info.id, self.dbname_urlencoded)
        assert_equal(202, self.dbaas.last_http_code)
        if not FAKE:
            time.sleep(5)
        dbs = self.dbaas.databases.list(instance_info.id)
        assert_equal(200, self.dbaas.last_http_code)
        found = any(result.name == self.dbname_urlencoded for result in dbs)
        assert_false(found, "Database '%s' SHOULD NOT be found in result" %
                     self.dbname_urlencoded)

    @test(runs_after=[test_delete_database])
    def test_cannot_delete_taboo_database_names(self):
        for name in self.system_dbs:
            assert_raises(exceptions.BadRequest, self.dbaas.databases.delete,
                          instance_info.id, name)
            assert_equal(400, self.dbaas.last_http_code)

    @test(runs_after=[test_delete_database])
    def test_delete_database_on_missing_instance(self):
        assert_raises(exceptions.NotFound, self.dbaas.databases.delete,
                      -1, self.dbname_urlencoded)
        assert_equal(404, self.dbaas.last_http_code)

    @test
    def test_database_name_too_long(self):
        databases = []
        name = ("aasdlkhaglkjhakjdkjgfakjgadgfkajsg"
                "34523dfkljgasldkjfglkjadsgflkjagsdd")
        databases.append({"name": name})
        assert_raises(exceptions.BadRequest, self.dbaas.databases.create,
                      instance_info.id, databases)
        assert_equal(400, self.dbaas.last_http_code)

    @test
    def test_invalid_database_name(self):
        databases = []
        databases.append({"name": "sdfsd,"})
        assert_raises(exceptions.BadRequest, self.dbaas.databases.create,
                      instance_info.id, databases)
        assert_equal(400, self.dbaas.last_http_code)

    @test
    def test_pagination(self):
        databases = []
        databases.append({"name": "Sprockets", "character_set": "latin2",
                          "collate": "latin2_general_ci"})
        databases.append({"name": "Cogs"})
        databases.append({"name": "Widgets"})

        self.dbaas.databases.create(instance_info.id, databases)
        assert_equal(202, self.dbaas.last_http_code)
        if not FAKE:
            time.sleep(5)
        limit = 2
        databases = self.dbaas.databases.list(instance_info.id, limit=limit)
        assert_equal(200, self.dbaas.last_http_code)
        marker = databases.next

        # Better get only as many as we asked for
        assert_true(len(databases) <= limit)
        assert_true(databases.next is not None)
        assert_equal(marker, databases[-1].name)
        marker = databases.next

        # I better get new databases if I use the marker I was handed.
        databases = self.dbaas.databases.list(instance_info.id, limit=limit,
                                              marker=marker)
        assert_equal(200, self.dbaas.last_http_code)
        assert_true(marker not in [database.name for database in databases])

        # Now fetch again with a larger limit.
        databases = self.dbaas.databases.list(instance_info.id)
        assert_equal(200, self.dbaas.last_http_code)
        assert_true(databases.next is None)

########NEW FILE########
__FILENAME__ = datastores
# Copyright (c) 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


from nose.tools import assert_equal
from troveclient.compat import exceptions

from proboscis import before_class
from proboscis import test
from proboscis.asserts import assert_raises
from proboscis.asserts import assert_true

from trove import tests
from trove.tests.util import create_dbaas_client
from trove.tests.util import test_config
from trove.tests.util.users import Requirements
from trove.tests.util.check import TypeCheck

GROUP = "dbaas.api.datastores"
NAME = "nonexistent"


@test(groups=[tests.DBAAS_API, GROUP, tests.PRE_INSTANCES],
      depends_on_groups=["services.initialize"])
class Datastores(object):

    @before_class
    def setUp(self):
        rd_user = test_config.users.find_user(
            Requirements(is_admin=False, services=["trove"]))
        rd_admin = test_config.users.find_user(
            Requirements(is_admin=True, services=["trove"]))
        self.rd_client = create_dbaas_client(rd_user)
        self.rd_admin = create_dbaas_client(rd_admin)

    @test
    def test_datastore_list_attrs(self):
        datastores = self.rd_client.datastores.list()
        for datastore in datastores:
            with TypeCheck('Datastore', datastore) as check:
                check.has_field("id", basestring)
                check.has_field("name", basestring)
                check.has_field("links", list)

    @test
    def test_datastore_get(self):
        # Test get by name
        datastore_by_name = self.rd_client.datastores.get(
            test_config.dbaas_datastore)
        with TypeCheck('Datastore', datastore_by_name) as check:
            check.has_field("id", basestring)
            check.has_field("name", basestring)
            check.has_field("links", list)
        assert_equal(datastore_by_name.name, test_config.dbaas_datastore)

        # test get by id
        datastore_by_id = self.rd_client.datastores.get(
            datastore_by_name.id)
        with TypeCheck('Datastore', datastore_by_id) as check:
            check.has_field("id", basestring)
            check.has_field("name", basestring)
            check.has_field("links", list)
        assert_equal(datastore_by_id.id, datastore_by_name.id)

    @test
    def test_datastore_not_found(self):
        try:
            assert_raises(exceptions.NotFound,
                          self.rd_client.datastores.get, NAME)
        except exceptions.BadRequest as e:
            assert_equal(e.message,
                         "Datastore '%s' cannot be found." % NAME)

    @test
    def test_datastore_with_no_active_versions_is_hidden(self):
        datastores = self.rd_client.datastores.list()
        id_list = [datastore.id for datastore in datastores]
        id_no_versions = test_config.dbaas_datastore_id_no_versions
        assert_true(id_no_versions not in id_list)

    @test
    def test_datastore_with_no_active_versions_is_visible_for_admin(self):
        datastores = self.rd_admin.datastores.list()
        id_list = [datastore.id for datastore in datastores]
        id_no_versions = test_config.dbaas_datastore_id_no_versions
        assert_true(id_no_versions in id_list)


@test(groups=[tests.DBAAS_API, GROUP, tests.PRE_INSTANCES],
      depends_on_groups=["services.initialize"])
class DatastoreVersions(object):

    @before_class
    def setUp(self):
        rd_user = test_config.users.find_user(
            Requirements(is_admin=False, services=["trove"]))
        self.rd_client = create_dbaas_client(rd_user)
        self.datastore_active = self.rd_client.datastores.get(
            test_config.dbaas_datastore)
        self.datastore_version_active = self.rd_client.datastore_versions.list(
            self.datastore_active.id)[0]

    @test
    def test_datastore_version_list_attrs(self):
        versions = self.rd_client.datastore_versions.list(
            self.datastore_active.name)
        for version in versions:
            with TypeCheck('DatastoreVersion', version) as check:
                check.has_field("id", basestring)
                check.has_field("name", basestring)
                check.has_field("links", list)

    @test
    def test_datastore_version_get_attrs(self):
        version = self.rd_client.datastore_versions.get(
            self.datastore_active.name, self.datastore_version_active.name)
        with TypeCheck('DatastoreVersion', version) as check:
            check.has_field("id", basestring)
            check.has_field("name", basestring)
            check.has_field("datastore", basestring)
            check.has_field("links", list)
        assert_equal(version.name, self.datastore_version_active.name)

    @test
    def test_datastore_version_get_by_uuid_attrs(self):
        version = self.rd_client.datastore_versions.get_by_uuid(
            self.datastore_version_active.id)
        with TypeCheck('DatastoreVersion', version) as check:
            check.has_field("id", basestring)
            check.has_field("name", basestring)
            check.has_field("datastore", basestring)
            check.has_field("links", list)
        assert_equal(version.name, self.datastore_version_active.name)

    @test
    def test_datastore_version_not_found(self):
        try:
            assert_raises(exceptions.NotFound,
                          self.rd_client.datastore_versions.get,
                          self.datastore_active.name, NAME)
        except exceptions.BadRequest as e:
            assert_equal(e.message,
                         "Datastore version '%s' cannot be found." % NAME)

    @test
    def test_datastore_version_list_by_uuid(self):
        versions = self.rd_client.datastore_versions.list(
            self.datastore_active.id)
        for version in versions:
            with TypeCheck('DatastoreVersion', version) as check:
                check.has_field("id", basestring)
                check.has_field("name", basestring)
                check.has_field("links", list)

    @test
    def test_datastore_version_get_by_uuid(self):
        version = self.rd_client.datastore_versions.get(
            self.datastore_active.id, self.datastore_version_active.id)
        with TypeCheck('DatastoreVersion', version) as check:
            check.has_field("id", basestring)
            check.has_field("name", basestring)
            check.has_field("datastore", basestring)
            check.has_field("links", list)
        assert_equal(version.name, self.datastore_version_active.name)

    @test
    def test_datastore_version_invalid_uuid(self):
        try:
            self.rd_client.datastore_versions.get_by_uuid(
                self.datastore_version_active.id)
        except exceptions.BadRequest as e:
            assert_equal(e.message,
                         "Datastore version '%s' cannot be found." %
                         test_config.dbaas_datastore_version)

########NEW FILE########
__FILENAME__ = flavors
# Copyright (c) 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import os

from nose.tools import assert_equal
from nose.tools import assert_false
from nose.tools import assert_true
from troveclient.compat import exceptions
from troveclient.v1.flavors import Flavor

from proboscis import before_class
from proboscis import test
from proboscis.asserts import assert_raises

from trove import tests
from trove.tests.util import create_dbaas_client
from trove.tests.util import create_nova_client
from trove.tests.util import test_config
from trove.tests.util.users import Requirements
from trove.tests.util.check import AttrCheck

GROUP = "dbaas.api.flavors"


servers_flavors = None
dbaas_flavors = None
user = None


def assert_attributes_equal(name, os_flavor, dbaas_flavor):
    """Given an attribute name and two objects,
        ensures the attribute is equal.
    """
    assert_true(hasattr(os_flavor, name),
                "open stack flavor did not have attribute %s" % name)
    assert_true(hasattr(dbaas_flavor, name),
                "dbaas flavor did not have attribute %s" % name)
    expected = getattr(os_flavor, name)
    actual = getattr(dbaas_flavor, name)
    assert_equal(expected, actual,
                 'DBaas flavor differs from Open Stack on attribute ' + name)


def assert_flavors_roughly_equivalent(os_flavor, dbaas_flavor):
    assert_attributes_equal('name', os_flavor, dbaas_flavor)
    assert_attributes_equal('ram', os_flavor, dbaas_flavor)
    assert_false(hasattr(dbaas_flavor, 'disk'),
                 "The attribute 'disk' s/b absent from the dbaas API.")


def assert_link_list_is_equal(flavor):
    assert_true(hasattr(flavor, 'links'))
    assert_true(flavor.links)

    for link in flavor.links:
        href = link['href']
        if "self" in link['rel']:
            expected_href = os.path.join(test_config.dbaas_url, "flavors",
                                         str(flavor.id))
            url = test_config.dbaas_url.replace('http:', 'https:', 1)
            msg = ("REL HREF %s doesn't start with %s" %
                   (href, test_config.dbaas_url))
            assert_true(href.startswith(url), msg)
            url = os.path.join("flavors", str(flavor.id))
            msg = "REL HREF %s doesn't end in 'flavors/id'" % href
            assert_true(href.endswith(url), msg)
        elif "bookmark" in link['rel']:
            base_url = test_config.version_url.replace('http:', 'https:', 1)
            expected_href = os.path.join(base_url, "flavors", str(flavor.id))
            msg = 'bookmark "href" must be %s, not %s' % (expected_href, href)
            assert_equal(href, expected_href, msg)
        else:
            assert_false(True, "Unexpected rel - %s" % link['rel'])


@test(groups=[tests.DBAAS_API, GROUP, tests.PRE_INSTANCES],
      depends_on_groups=["services.initialize"])
class Flavors(object):

    @before_class
    def setUp(self):
        rd_user = test_config.users.find_user(
            Requirements(is_admin=False, services=["trove"]))
        self.rd_client = create_dbaas_client(rd_user)

        if test_config.nova_client is not None:
            nova_user = test_config.users.find_user(
                Requirements(services=["nova"]))
            self.nova_client = create_nova_client(nova_user)

    def get_expected_flavors(self):
        # If we have access to the client, great! Let's use that as the flavors
        # returned by Trove should be identical.
        if test_config.nova_client is not None:
            return self.nova_client.flavors.list()
        # If we don't have access to the client the flavors need to be spelled
        # out in the config file.
        flavors = [Flavor(Flavors, flavor_dict, loaded=True)
                   for flavor_dict in test_config.flavors]
        return flavors

    @test
    def confirm_flavors_lists_nearly_identical(self):
        os_flavors = self.get_expected_flavors()
        dbaas_flavors = self.rd_client.flavors.list()

        print("Open Stack Flavors:")
        print(os_flavors)
        print("DBaaS Flavors:")
        print(dbaas_flavors)
        #Length of both flavors list should be identical.
        assert_equal(len(os_flavors), len(dbaas_flavors))
        for os_flavor in os_flavors:
            found_index = None
            for index, dbaas_flavor in enumerate(dbaas_flavors):
                if os_flavor.name == dbaas_flavor.name:
                    msg = ("Flavor ID '%s' appears in elements #%s and #%d." %
                           (dbaas_flavor.id, str(found_index), index))
                    assert_true(found_index is None, msg)
                    assert_flavors_roughly_equivalent(os_flavor, dbaas_flavor)
                    found_index = index
            msg = "Some flavors from OS list were missing in DBAAS list."
            assert_false(found_index is None, msg)
        for flavor in dbaas_flavors:
            assert_link_list_is_equal(flavor)

    @test
    def test_flavor_list_attrs(self):
        expected_attrs = ['id', 'name', 'ram', 'links', 'local_storage']
        flavors = self.rd_client.flavors.list()
        attrcheck = AttrCheck()
        for flavor in flavors:
            flavor_dict = flavor._info
            attrcheck.attrs_exist(flavor_dict, expected_attrs,
                                  msg="Flavors list")
            attrcheck.links(flavor_dict['links'])

    @test
    def test_flavor_get_attrs(self):
        expected_attrs = ['id', 'name', 'ram', 'links', 'local_storage']
        flavor = self.rd_client.flavors.get(1)
        attrcheck = AttrCheck()
        flavor_dict = flavor._info
        attrcheck.attrs_exist(flavor_dict, expected_attrs,
                              msg="Flavor Get 1")
        attrcheck.links(flavor_dict['links'])

    @test
    def test_flavor_not_found(self):
        assert_raises(exceptions.NotFound,
                      self.rd_client.flavors.get, "detail")

########NEW FILE########
__FILENAME__ = header
# Copyright 2013 OpenStack Foundation
# Copyright 2013 Rackspace Hosting
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

from proboscis import test
from proboscis import SkipTest
from functools import wraps

from troveclient.compat.client import TroveHTTPClient
from trove.tests.api.versions import Versions


@test(groups=['dbaas.api.headers'])
def must_work_with_blank_accept_headers():
    """Test to make sure that trove works without the headers"""
    versions = Versions()
    versions.setUp()
    client = versions.client

    if type(client.client).morph_request != TroveHTTPClient.morph_request:
        raise SkipTest("Not using the JSON client so can't execute this test.")

    original_morph_request = client.client.morph_request

    def morph_content_type_to(content_type):
        @wraps(original_morph_request)
        def _morph_request(kwargs):
            original_morph_request(kwargs)
            kwargs['headers']['Accept'] = content_type
            kwargs['headers']['Content-Type'] = content_type

        client.client.morph_request = _morph_request

    try:
        morph_content_type_to('')
        # run versions to make sure the API still returns JSON even though the
        # header type is blank
        versions.test_list_versions_index()
    finally:
        client.client.morph_request = original_morph_request

########NEW FILE########
__FILENAME__ = instances
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import os
import re
import time
import unittest


GROUP = "dbaas.guest"
GROUP_START = "dbaas.guest.initialize"
GROUP_START_SIMPLE = "dbaas.guest.initialize.simple"
GROUP_TEST = "dbaas.guest.test"
GROUP_STOP = "dbaas.guest.shutdown"
GROUP_USERS = "dbaas.api.users"
GROUP_ROOT = "dbaas.api.root"
GROUP_DATABASES = "dbaas.api.databases"
GROUP_SECURITY_GROUPS = "dbaas.api.security_groups"
GROUP_CREATE_INSTANCE_FAILURE = "dbaas.api.failures"

TIMEOUT_INSTANCE_CREATE = 60 * 32
TIMEOUT_INSTANCE_DELETE = 120

from datetime import datetime
from time import sleep

from trove.datastore import models as datastore_models
from trove.common import exception as rd_exceptions
from troveclient.compat import exceptions

from proboscis.decorators import time_out
from proboscis import before_class
from proboscis import after_class
from proboscis import test
from proboscis import SkipTest
from proboscis.asserts import assert_equal
from proboscis.asserts import assert_not_equal
from proboscis.asserts import assert_raises
from proboscis.asserts import assert_is_not_none
from proboscis.asserts import assert_true
from proboscis.asserts import fail

from trove import tests
from trove.tests.config import CONFIG
from trove.tests.util import create_dbaas_client
from trove.tests.util.usage import create_usage_verifier
from trove.tests.util import dns_checker
from trove.tests.util import iso_time
from trove.tests.util.users import Requirements
from trove.common.utils import poll_until
from trove.tests.util.check import AttrCheck
from trove.tests.util.check import TypeCheck
from trove.tests.util import test_config

FAKE = test_config.values['fake_mode']


class InstanceTestInfo(object):
    """Stores new instance information used by dependent tests."""

    def __init__(self):
        self.dbaas = None  # The rich client instance used by these tests.
        self.dbaas_admin = None  # The rich client with admin access.
        self.dbaas_flavor = None  # The flavor object of the instance.
        self.dbaas_flavor_href = None  # The flavor of the instance.
        self.dbaas_datastore = None  # The datastore id
        self.dbaas_datastore_version = None  # The datastore version id
        self.dbaas_inactive_datastore_version = None  # The DS inactive id
        self.id = None  # The ID of the instance in the database.
        self.local_id = None
        self.address = None
        self.initial_result = None  # The initial result from the create call.
        self.user_ip = None  # The IP address of the instance, given to user.
        self.infra_ip = None  # The infrastructure network IP address.
        self.result = None  # The instance info returned by the API
        self.nova_client = None  # The instance of novaclient.
        self.volume_client = None  # The instance of the volume client.
        self.name = None  # Test name, generated each test run.
        self.pid = None  # The process ID of the instance.
        self.user = None  # The user instance who owns the instance.
        self.admin_user = None  # The admin user for the management interfaces.
        self.volume = None  # The volume the instance will have.
        self.volume_id = None  # Id for the attached vo186lume
        self.storage = None  # The storage device info for the volumes.
        self.databases = None  # The databases created on the instance.
        self.host_info = None  # Host Info before creating instances
        self.user_context = None  # A regular user context
        self.users = None  # The users created on the instance.
        self.consumer = create_usage_verifier()

    def find_default_flavor(self):
        if EPHEMERAL_SUPPORT:
            flavor_name = CONFIG.values.get('instance_eph_flavor_name',
                                            'eph.rd-tiny')
        else:
            flavor_name = CONFIG.values.get('instance_flavor_name', 'm1.tiny')
        flavors = self.dbaas.find_flavors_by_name(flavor_name)
        assert_equal(len(flavors), 1,
                     "Number of flavors with name '%s' "
                     "found was '%d'." % (flavor_name, len(flavors)))
        flavor = flavors[0]
        assert_true(flavor is not None, "Flavor '%s' not found!" % flavor_name)
        flavor_href = self.dbaas.find_flavor_self_href(flavor)
        assert_true(flavor_href is not None,
                    "Flavor href '%s' not found!" % flavor_name)
        return flavor, flavor_href

    def get_address(self):
        result = self.dbaas_admin.mgmt.instances.show(self.id)
        if not hasattr(result, 'hostname'):
            return result.ip[0]
        else:
            return result.server['addresses']

    def get_local_id(self):
        mgmt_instance = self.dbaas_admin.management.show(self.id)
        return mgmt_instance.server["local_id"]

    def get_volume_filesystem_size(self):
        mgmt_instance = self.dbaas_admin.management.show(self.id)
        return mgmt_instance.volume["total"]


# The two variables are used below by tests which depend on an instance
# existing.
instance_info = InstanceTestInfo()
dbaas = None  # Rich client used throughout this test.
dbaas_admin = None  # Same as above, with admin privs.
ROOT_ON_CREATE = CONFIG.get('root_on_create', False)
VOLUME_SUPPORT = CONFIG.get('trove_volume_support', False)
EPHEMERAL_SUPPORT = not VOLUME_SUPPORT and CONFIG.get('device_path',
                                                      '/dev/vdb') is not None
ROOT_PARTITION = not VOLUME_SUPPORT and CONFIG.get('device_path',
                                                   None) is None


# This is like a cheat code which allows the tests to skip creating a new
# instance and use an old one.
def existing_instance():
    return os.environ.get("TESTS_USE_INSTANCE_ID", None)


def do_not_delete_instance():
    return os.environ.get("TESTS_DO_NOT_DELETE_INSTANCE", None) is not None


def create_new_instance():
    return existing_instance() is None


@test(groups=['dbaas.usage', 'dbaas.usage.init'])
def clear_messages_off_queue():
    instance_info.consumer.clear_events()


@test(groups=[GROUP, GROUP_START, GROUP_START_SIMPLE, 'dbaas.setup'],
      depends_on_groups=["services.initialize"])
class InstanceSetup(object):
    """Makes sure the client can hit the ReST service.

    This test also uses the API to find the flavor to use.

    """

    @before_class
    def setUp(self):
        """Sets up the client."""

        reqs = Requirements(is_admin=True)
        instance_info.admin_user = CONFIG.users.find_user(reqs)
        instance_info.dbaas_admin = create_dbaas_client(
            instance_info.admin_user)
        global dbaas_admin
        dbaas_admin = instance_info.dbaas_admin

        # Make sure we create the client as the correct user if we're using
        # a pre-built instance.
        if existing_instance():
            mgmt_inst = dbaas_admin.mgmt.instances.show(existing_instance())
            t_id = mgmt_inst.tenant_id
            instance_info.user = CONFIG.users.find_user_by_tenant_id(t_id)
        else:
            reqs = Requirements(is_admin=False)
            instance_info.user = CONFIG.users.find_user(reqs)

        instance_info.dbaas = create_dbaas_client(instance_info.user)
        global dbaas
        dbaas = instance_info.dbaas

    @test
    def test_find_flavor(self):
        flavor, flavor_href = instance_info.find_default_flavor()
        instance_info.dbaas_flavor = flavor
        instance_info.dbaas_flavor_href = flavor_href

    @test
    def create_instance_name(self):
        id = existing_instance()
        if id is None:
            instance_info.name = "TEST_" + str(datetime.now())
        else:
            instance_info.name = dbaas.instances.get(id).name


@test(depends_on_classes=[InstanceSetup], groups=[GROUP])
def test_delete_instance_not_found():
    """Deletes an instance that does not exist."""
    # Looks for a random UUID that (most probably) does not exist.
    assert_raises(exceptions.NotFound, dbaas.instances.delete,
                  "7016efb6-c02c-403e-9628-f6f57d0920d0")


@test(depends_on_classes=[InstanceSetup],
      groups=[GROUP, 'dbaas_quotas'],
      runs_after_groups=[tests.PRE_INSTANCES])
class CreateInstanceQuotaTest(unittest.TestCase):

    def setUp(self):
        import copy

        self.test_info = copy.deepcopy(instance_info)
        self.test_info.dbaas_datastore = CONFIG.dbaas_datastore

    def tearDown(self):
        quota_dict = {'instances': CONFIG.trove_max_instances_per_user}
        if VOLUME_SUPPORT:
            quota_dict['volumes'] = CONFIG.trove_max_volumes_per_user
        dbaas_admin.quota.update(self.test_info.user.tenant_id,
                                 quota_dict)

    def test_instance_size_too_big(self):
        if ('trove_max_accepted_volume_size' in CONFIG.values and
                VOLUME_SUPPORT):
            too_big = CONFIG.trove_max_accepted_volume_size

            self.test_info.volume = {'size': too_big + 1}
            self.test_info.name = "way_too_large"
            assert_raises(exceptions.OverLimit,
                          dbaas.instances.create,
                          self.test_info.name,
                          self.test_info.dbaas_flavor_href,
                          self.test_info.volume)

    def test_update_quota_invalid_resource_should_fail(self):
        quota_dict = {'invalid_resource': 100}
        assert_raises(exceptions.NotFound, dbaas_admin.quota.update,
                      self.test_info.user.tenant_id, quota_dict)

    def test_update_quota_volume_should_fail_volume_not_supported(self):
        if VOLUME_SUPPORT:
            raise SkipTest("Volume support needs to be disabled")
        quota_dict = {'volumes': 100}
        assert_raises(exceptions.NotFound, dbaas_admin.quota.update,
                      self.test_info.user.tenant_id, quota_dict)

    def test_create_too_many_instances(self):
        instance_quota = 0
        quota_dict = {'instances': instance_quota}
        new_quotas = dbaas_admin.quota.update(self.test_info.user.tenant_id,
                                              quota_dict)

        verify_quota = dbaas_admin.quota.show(self.test_info.user.tenant_id)

        assert_equal(new_quotas['instances'], quota_dict['instances'])
        assert_equal(0, verify_quota['instances'])
        self.test_info.volume = None

        if VOLUME_SUPPORT:
            assert_equal(CONFIG.trove_max_volumes_per_user,
                         verify_quota['volumes'])
            self.test_info.volume = {'size': 1}

        self.test_info.name = "too_many_instances"
        assert_raises(exceptions.OverLimit,
                      dbaas.instances.create,
                      self.test_info.name,
                      self.test_info.dbaas_flavor_href,
                      self.test_info.volume)

        assert_equal(413, dbaas.last_http_code)

    def test_create_instances_total_volume_exceeded(self):
        if not VOLUME_SUPPORT:
            raise SkipTest("Volume support not enabled")
        volume_quota = 3
        quota_dict = {'volumes': volume_quota}
        self.test_info.volume = {'size': volume_quota + 1}
        new_quotas = dbaas_admin.quota.update(self.test_info.user.tenant_id,
                                              quota_dict)
        assert_equal(volume_quota, new_quotas['volumes'])

        self.test_info.name = "too_large_volume"
        assert_raises(exceptions.OverLimit,
                      dbaas.instances.create,
                      self.test_info.name,
                      self.test_info.dbaas_flavor_href,
                      self.test_info.volume)

        assert_equal(413, dbaas.last_http_code)


@test(depends_on_classes=[InstanceSetup],
      groups=[GROUP, GROUP_CREATE_INSTANCE_FAILURE],
      runs_after_groups=[tests.PRE_INSTANCES, 'dbaas_quotas'])
class CreateInstanceFail(object):

    def instance_in_error(self, instance_id):
        def check_if_error():
            instance = dbaas.instances.get(instance_id)
            if instance.status == "ERROR":
                return True
            else:
                # The status should still be BUILD
                assert_equal("BUILD", instance.status)
                return False
        return check_if_error

    def delete_async(self, instance_id):
        dbaas.instances.delete(instance_id)
        while True:
            try:
                dbaas.instances.get(instance_id)
            except exceptions.NotFound:
                return True
            time.sleep(1)

    @test
    @time_out(30)
    def test_create_with_bad_availability_zone(self):
        instance_name = "instance-failure-with-bad-ephemeral"
        if VOLUME_SUPPORT:
            volume = {'size': 1}
        else:
            volume = None
        databases = []
        result = dbaas.instances.create(instance_name,
                                        instance_info.dbaas_flavor_href,
                                        volume, databases,
                                        availability_zone="BAD_ZONE")

        poll_until(self.instance_in_error(result.id))
        instance = dbaas.instances.get(result.id)
        assert_equal("ERROR", instance.status)

        self.delete_async(result.id)

    @test
    def test_create_with_bad_nics(self):
        instance_name = "instance-failure-with-bad-nics"
        if VOLUME_SUPPORT:
            volume = {'size': 1}
        else:
            volume = None
        databases = []
        bad_nic = [{"port-id": "UNKNOWN", "net-id": "1234",
                    "v4-fixed-ip": "1.2.3.4"}]
        result = dbaas.instances.create(instance_name,
                                        instance_info.dbaas_flavor_href,
                                        volume, databases, nics=bad_nic)

        poll_until(self.instance_in_error(result.id))
        instance = dbaas.instances.get(result.id)
        assert_equal("ERROR", instance.status)

        self.delete_async(result.id)

    @test(enabled=VOLUME_SUPPORT)
    def test_create_failure_with_empty_volume(self):
        instance_name = "instance-failure-with-no-volume-size"
        databases = []
        volume = {}
        assert_raises(exceptions.BadRequest, dbaas.instances.create,
                      instance_name, instance_info.dbaas_flavor_href,
                      volume, databases)
        assert_equal(400, dbaas.last_http_code)

    @test(enabled=VOLUME_SUPPORT)
    def test_create_failure_with_no_volume_size(self):
        instance_name = "instance-failure-with-no-volume-size"
        databases = []
        volume = {'size': None}
        assert_raises(exceptions.BadRequest, dbaas.instances.create,
                      instance_name, instance_info.dbaas_flavor_href,
                      volume, databases)
        assert_equal(400, dbaas.last_http_code)

    @test(enabled=not VOLUME_SUPPORT)
    def test_create_failure_with_volume_size_and_volume_disabled(self):
        instance_name = "instance-failure-volume-size_and_volume_disabled"
        databases = []
        volume = {'size': 2}
        assert_raises(exceptions.HTTPNotImplemented, dbaas.instances.create,
                      instance_name, instance_info.dbaas_flavor_href,
                      volume, databases)
        assert_equal(501, dbaas.last_http_code)

    @test(enabled=EPHEMERAL_SUPPORT)
    def test_create_failure_with_no_ephemeral_flavor(self):
        instance_name = "instance-failure-with-no-ephemeral-flavor"
        databases = []
        flavor_name = CONFIG.values.get('instance_flavor_name', 'm1.tiny')
        flavors = dbaas.find_flavors_by_name(flavor_name)
        assert_raises(exceptions.BadRequest, dbaas.instances.create,
                      instance_name, flavors[0].id, None, databases)
        assert_equal(400, dbaas.last_http_code)

    @test
    def test_create_failure_with_no_name(self):
        if VOLUME_SUPPORT:
            volume = {'size': 1}
        else:
            volume = None
        instance_name = ""
        databases = []
        assert_raises(exceptions.BadRequest, dbaas.instances.create,
                      instance_name, instance_info.dbaas_flavor_href,
                      volume, databases)
        assert_equal(400, dbaas.last_http_code)

    @test
    def test_create_failure_with_spaces_for_name(self):
        if VOLUME_SUPPORT:
            volume = {'size': 1}
        else:
            volume = None
        instance_name = "      "
        databases = []
        assert_raises(exceptions.BadRequest, dbaas.instances.create,
                      instance_name, instance_info.dbaas_flavor_href,
                      volume, databases)
        assert_equal(400, dbaas.last_http_code)

    @test
    def test_mgmt_get_instance_on_create(self):
        if CONFIG.test_mgmt:
            result = dbaas_admin.management.show(instance_info.id)
            expected_attrs = ['account_id', 'addresses', 'created',
                              'databases', 'flavor', 'guest_status', 'host',
                              'hostname', 'id', 'name', 'datastore',
                              'server_state_description', 'status', 'updated',
                              'users', 'volume', 'root_enabled_at',
                              'root_enabled_by']
            with CheckInstance(result._info) as check:
                check.attrs_exist(result._info, expected_attrs,
                                  msg="Mgmt get instance")
                check.flavor()
                check.datastore()
                check.guest_status()

    @test
    def test_create_failure_with_datastore_default_notfound(self):
        if not FAKE:
            raise SkipTest("This test only for fake mode.")
        if VOLUME_SUPPORT:
            volume = {'size': 1}
        else:
            volume = None
        instance_name = "datastore_default_notfound"
        databases = []
        users = []
        origin_default_datastore = (datastore_models.CONF.
                                    default_datastore)
        datastore_models.CONF.default_datastore = ""
        try:
            assert_raises(exceptions.NotFound,
                          dbaas.instances.create, instance_name,
                          instance_info.dbaas_flavor_href,
                          volume, databases, users)
        except exceptions.BadRequest as e:
            assert_equal(e.message,
                         "Please specify datastore.")
        datastore_models.CONF.default_datastore = \
            origin_default_datastore

    @test
    def test_create_failure_with_datastore_default_version_notfound(self):
        if VOLUME_SUPPORT:
            volume = {'size': 1}
        else:
            volume = None
        instance_name = "datastore_default_version_notfound"
        databases = []
        users = []
        datastore = "Test_Datastore_1"
        try:
            assert_raises(exceptions.NotFound,
                          dbaas.instances.create, instance_name,
                          instance_info.dbaas_flavor_href,
                          volume, databases, users,
                          datastore=datastore)
        except exceptions.BadRequest as e:
            assert_equal(e.message,
                         "Default version for datastore '%s' not found." %
                         datastore)

    @test
    def test_create_failure_with_datastore_notfound(self):
        if VOLUME_SUPPORT:
            volume = {'size': 1}
        else:
            volume = None
        instance_name = "datastore_notfound"
        databases = []
        users = []
        datastore = "nonexistent"
        try:
            assert_raises(exceptions.NotFound,
                          dbaas.instances.create, instance_name,
                          instance_info.dbaas_flavor_href,
                          volume, databases, users,
                          datastore=datastore)
        except exceptions.BadRequest as e:
            assert_equal(e.message,
                         "Datastore '%s' cannot be found." %
                         datastore)

    @test
    def test_create_failure_with_datastore_version_notfound(self):
        if VOLUME_SUPPORT:
            volume = {'size': 1}
        else:
            volume = None
        instance_name = "datastore_version_notfound"
        databases = []
        users = []
        datastore = CONFIG.dbaas_datastore
        datastore_version = "nonexistent"
        try:
            assert_raises(exceptions.NotFound,
                          dbaas.instances.create, instance_name,
                          instance_info.dbaas_flavor_href,
                          volume, databases, users,
                          datastore=datastore,
                          datastore_version=datastore_version)
        except exceptions.BadRequest as e:
            assert_equal(e.message,
                         "Datastore version '%s' cannot be found." %
                         datastore_version)

    @test
    def test_create_failure_with_datastore_version_inactive(self):
        if VOLUME_SUPPORT:
            volume = {'size': 1}
        else:
            volume = None
        instance_name = "datastore_version_inactive"
        databases = []
        users = []
        datastore = CONFIG.dbaas_datastore
        datastore_version = CONFIG.dbaas_inactive_datastore_version
        try:
            assert_raises(exceptions.NotFound,
                          dbaas.instances.create, instance_name,
                          instance_info.dbaas_flavor_href,
                          volume, databases, users,
                          datastore=datastore,
                          datastore_version=datastore_version)
        except exceptions.BadRequest as e:
            assert_equal(e.message,
                         "Datastore version '%s' is not active." %
                         datastore_version)


def assert_unprocessable(func, *args):
    try:
        func(*args)
        # If the exception didn't get raised, but the instance is still in
        # the BUILDING state, that's a bug.
        result = dbaas.instances.get(instance_info.id)
        if result.status == "BUILD":
            fail("When an instance is being built, this function should "
                 "always raise UnprocessableEntity.")
    except exceptions.UnprocessableEntity:
        assert_equal(422, dbaas.last_http_code)
        pass  # Good

    @test
    def test_deep_list_security_group_with_rules(self):
        securityGroupList = dbaas.security_groups.list()
        assert_is_not_none(securityGroupList)
        securityGroup = [x for x in securityGroupList
                         if x.name in self.secGroupName]
        assert_is_not_none(securityGroup[0])
        assert_not_equal(len(securityGroup[0].rules), 0)


@test(depends_on_classes=[InstanceSetup],
      run_after_class=[CreateInstanceFail],
      groups=[GROUP, GROUP_START, GROUP_START_SIMPLE, tests.INSTANCES],
      runs_after_groups=[tests.PRE_INSTANCES, 'dbaas_quotas'])
class CreateInstance(object):

    """Test to create a Database Instance

    If the call returns without raising an exception this test passes.

    """

    @test
    def test_create(self):
        databases = []
        databases.append({"name": "firstdb", "character_set": "latin2",
                          "collate": "latin2_general_ci"})
        databases.append({"name": "db2"})
        instance_info.databases = databases
        users = []
        users.append({"name": "lite", "password": "litepass",
                      "databases": [{"name": "firstdb"}]})
        instance_info.users = users
        instance_info.dbaas_datastore = CONFIG.dbaas_datastore
        instance_info.dbaas_datastore_version = CONFIG.dbaas_datastore_version
        if VOLUME_SUPPORT:
            instance_info.volume = {'size': 1}
        else:
            instance_info.volume = None

        if create_new_instance():
            instance_info.initial_result = dbaas.instances.create(
                instance_info.name,
                instance_info.dbaas_flavor_href,
                instance_info.volume,
                databases,
                users,
                availability_zone="nova",
                datastore=instance_info.dbaas_datastore,
                datastore_version=instance_info.dbaas_datastore_version)
            assert_equal(200, dbaas.last_http_code)
        else:
            id = existing_instance()
            instance_info.initial_result = dbaas.instances.get(id)

        result = instance_info.initial_result
        instance_info.id = result.id
        instance_info.dbaas_datastore_version = result.datastore['version']

        report = CONFIG.get_report()
        report.log("Instance UUID = %s" % instance_info.id)
        if create_new_instance():
            assert_equal("BUILD", instance_info.initial_result.status)

        else:
            report.log("Test was invoked with TESTS_USE_INSTANCE_ID=%s, so no "
                       "instance was actually created." % id)

        # Check these attrs only are returned in create response
        expected_attrs = ['created', 'flavor', 'addresses', 'id', 'links',
                          'name', 'status', 'updated', 'datastore']
        if ROOT_ON_CREATE:
            expected_attrs.append('password')
        if VOLUME_SUPPORT:
            expected_attrs.append('volume')
        if CONFIG.trove_dns_support:
            expected_attrs.append('hostname')

        with CheckInstance(result._info) as check:
            if create_new_instance():
                check.attrs_exist(result._info, expected_attrs,
                                  msg="Create response")
            # Don't CheckInstance if the instance already exists.
            check.flavor()
            check.datastore()
            check.links(result._info['links'])
            if VOLUME_SUPPORT:
                check.volume()


@test(depends_on_classes=[CreateInstance],
      groups=[GROUP,
              GROUP_START,
              GROUP_START_SIMPLE,
              'dbaas.mgmt.hosts_post_install'],
      enabled=create_new_instance())
class AfterInstanceCreation(unittest.TestCase):

    # instance calls
    def test_instance_delete_right_after_create(self):
        assert_unprocessable(dbaas.instances.delete, instance_info.id)

    # root calls
    def test_root_create_root_user_after_create(self):
        assert_unprocessable(dbaas.root.create, instance_info.id)

    def test_root_is_root_enabled_after_create(self):
        assert_unprocessable(dbaas.root.is_root_enabled, instance_info.id)

    # database calls
    def test_database_index_after_create(self):
        assert_unprocessable(dbaas.databases.list, instance_info.id)

    def test_database_delete_after_create(self):
        assert_unprocessable(dbaas.databases.delete, instance_info.id,
                             "testdb")

    def test_database_create_after_create(self):
        assert_unprocessable(dbaas.databases.create, instance_info.id,
                             instance_info.databases)

    # user calls
    def test_users_index_after_create(self):
        assert_unprocessable(dbaas.users.list, instance_info.id)

    def test_users_delete_after_create(self):
        assert_unprocessable(dbaas.users.delete, instance_info.id,
                             "testuser")

    def test_users_create_after_create(self):
        users = list()
        users.append({"name": "testuser", "password": "password",
                      "databases": [{"name": "testdb"}]})
        assert_unprocessable(dbaas.users.create, instance_info.id, users)

    def test_resize_instance_after_create(self):
        assert_unprocessable(dbaas.instances.resize_instance,
                             instance_info.id, 8)

    def test_resize_volume_after_create(self):
        assert_unprocessable(dbaas.instances.resize_volume,
                             instance_info.id, 2)


@test(depends_on_classes=[CreateInstance],
      runs_after=[AfterInstanceCreation],
      groups=[GROUP, GROUP_START, GROUP_START_SIMPLE],
      enabled=create_new_instance())
class WaitForGuestInstallationToFinish(object):
    """
        Wait until the Guest is finished installing.  It takes quite a while...
    """

    @test
    @time_out(TIMEOUT_INSTANCE_CREATE)
    def test_instance_created(self):
        # This version just checks the REST API status.
        def result_is_active():
            instance = dbaas.instances.get(instance_info.id)
            if instance.status == "ACTIVE":
                return True
            else:
                # If its not ACTIVE, anything but BUILD must be
                # an error.
                assert_equal("BUILD", instance.status)
                if instance_info.volume is not None:
                    assert_equal(instance.volume.get('used', None), None)
                return False

        poll_until(result_is_active)
        dbaas.instances.get(instance_info.id)

        report = CONFIG.get_report()
        report.log("Created an instance, ID = %s." % instance_info.id)
        report.log("TIP:")
        report.log("Rerun the tests with TESTS_USE_INSTANCE_ID=%s "
                   "to skip ahead to this point." % instance_info.id)
        report.log("Add TESTS_DO_NOT_DELETE_INSTANCE=True to avoid deleting "
                   "the instance at the end of the tests.")


@test(depends_on_classes=[WaitForGuestInstallationToFinish],
      groups=[GROUP, GROUP_SECURITY_GROUPS])
class SecurityGroupsTest(object):

    @before_class
    def setUp(self):
        self.testSecurityGroup = dbaas.security_groups.get(
            instance_info.id)
        self.secGroupName = "SecGroup_%s" % instance_info.id
        self.secGroupDescription = "Security Group for %s" % instance_info.id

    @test
    def test_created_security_group(self):
        assert_is_not_none(self.testSecurityGroup)
        with TypeCheck('SecurityGroup', self.testSecurityGroup) as secGrp:
            secGrp.has_field('id', basestring)
            secGrp.has_field('name', basestring)
            secGrp.has_field('description', basestring)
            secGrp.has_field('created', basestring)
            secGrp.has_field('updated', basestring)
        assert_equal(self.testSecurityGroup.name, self.secGroupName)
        assert_equal(self.testSecurityGroup.description,
                     self.secGroupDescription)

    @test
    def test_list_security_group(self):
        securityGroupList = dbaas.security_groups.list()
        assert_is_not_none(securityGroupList)
        securityGroup = [x for x in securityGroupList
                         if x.name in self.secGroupName]
        assert_is_not_none(securityGroup)

    @test
    def test_get_security_group(self):
        securityGroup = dbaas.security_groups.get(self.testSecurityGroup.id)
        assert_is_not_none(securityGroup)
        assert_equal(securityGroup.name, self.secGroupName)
        assert_equal(securityGroup.description, self.secGroupDescription)
        assert_equal(securityGroup.instance_id, instance_info.id)


@test(depends_on_classes=[SecurityGroupsTest],
      groups=[GROUP, GROUP_SECURITY_GROUPS])
class SecurityGroupsRulesTest(object):

    # Security group already have default rule
    # that is why 'delete'-test is not needed anymore

    @before_class
    def setUp(self):
        self.testSecurityGroup = dbaas.security_groups.get(
            instance_info.id)
        self.secGroupName = "SecGroup_%s" % instance_info.id
        self.secGroupDescription = "Security Group for %s" % instance_info.id

    @test
    def test_create_security_group_rule(self):
        cidr = "1.2.3.4/16"
        self.testSecurityGroupRules = (
            dbaas.security_group_rules.create(
                group_id=self.testSecurityGroup.id,
                cidr=cidr))
        assert_not_equal(len(self.testSecurityGroupRules), 0)
        assert_is_not_none(self.testSecurityGroupRules)
        for rule in self.testSecurityGroupRules:
            assert_is_not_none(rule)
            assert_equal(rule['security_group_id'],
                         self.testSecurityGroup.id)
            assert_is_not_none(rule['id'])
            assert_equal(rule['cidr'], cidr)
            assert_equal(rule['from_port'], 3306)
            assert_equal(rule['to_port'], 3306)
            assert_is_not_none(rule['created'])


@test(depends_on_classes=[WaitForGuestInstallationToFinish],
      groups=[GROUP, GROUP_START], enabled=create_new_instance())
class TestGuestProcess(object):
    """
        Test that the guest process is started with all the right parameters
    """

    @test
    def check_hwinfo_before_tests(self):
        if CONFIG.test_mgmt:
            hwinfo = dbaas_admin.hwinfo.get(instance_info.id)
            print("hwinfo : %r" % hwinfo._info)
            expected_attrs = ['hwinfo']
            CheckInstance(None).attrs_exist(hwinfo._info, expected_attrs,
                                            msg="Hardware information")
            # TODO(pdmars): instead of just checking that these are int's, get
            # the instance flavor and verify that the values are correct for
            # the flavor
            assert_true(isinstance(hwinfo.hwinfo['mem_total'], int))
            assert_true(isinstance(hwinfo.hwinfo['num_cpus'], int))

    @test
    def grab_diagnostics_before_tests(self):
        if CONFIG.test_mgmt:
            diagnostics = dbaas_admin.diagnostics.get(instance_info.id)
            diagnostic_tests_helper(diagnostics)


@test(depends_on_classes=[WaitForGuestInstallationToFinish],
      groups=[GROUP, GROUP_TEST, "dbaas.dns"])
class DnsTests(object):

    @test
    def test_dns_entries_are_found(self):
        """Talk to DNS system to ensure entries were created."""
        print("Instance name=%s" % instance_info.name)
        client = instance_info.dbaas_admin
        mgmt_instance = client.mgmt.instances.show(instance_info.id)
        dns_checker(mgmt_instance)


@test(depends_on_classes=[WaitForGuestInstallationToFinish],
      groups=[GROUP, GROUP_TEST, "dbaas.guest.start.test"])
class TestAfterInstanceCreatedGuestData(object):
    """
    Test the optional parameters (databases and users) passed in to create
    instance call were created.
    """

    @test
    def test_databases(self):
        databases = dbaas.databases.list(instance_info.id)
        dbs = [database.name for database in databases]
        for db in instance_info.databases:
            assert_true(db["name"] in dbs)

    @test
    def test_users(self):
        users = dbaas.users.list(instance_info.id)
        usernames = [user.name for user in users]
        for user in instance_info.users:
            assert_true(user["name"] in usernames)


@test(depends_on_classes=[WaitForGuestInstallationToFinish],
      groups=[GROUP, GROUP_START, GROUP_START_SIMPLE, "dbaas.listing"])
class TestInstanceListing(object):
    """Test the listing of the instance information """

    @before_class
    def setUp(self):
        reqs = Requirements(is_admin=False)
        self.other_user = CONFIG.users.find_user(
            reqs,
            black_list=[instance_info.user.auth_user])
        self.other_client = create_dbaas_client(self.other_user)

    @test
    def test_index_list(self):
        expected_attrs = ['id', 'links', 'name', 'status', 'flavor',
                          'datastore']
        if VOLUME_SUPPORT:
            expected_attrs.append('volume')
        instances = dbaas.instances.list()
        assert_equal(200, dbaas.last_http_code)
        for instance in instances:
            instance_dict = instance._info
            with CheckInstance(instance_dict) as check:
                print("testing instance_dict=%s" % instance_dict)
                check.attrs_exist(instance_dict, expected_attrs,
                                  msg="Instance Index")
                check.links(instance_dict['links'])
                check.flavor()
                check.datastore()
                check.volume()

    @test
    def test_get_instance(self):
        expected_attrs = ['created', 'databases', 'flavor', 'hostname', 'id',
                          'links', 'name', 'status', 'updated', 'ip',
                          'datastore']
        if VOLUME_SUPPORT:
            expected_attrs.append('volume')
        else:
            expected_attrs.append('local_storage')
        instance = dbaas.instances.get(instance_info.id)
        assert_equal(200, dbaas.last_http_code)
        instance_dict = instance._info
        print("instance_dict=%s" % instance_dict)
        with CheckInstance(instance_dict) as check:
            check.attrs_exist(instance_dict, expected_attrs,
                              msg="Get Instance")
            check.flavor()
            check.datastore()
            check.links(instance_dict['links'])
            check.used_volume()

    @test
    def test_get_instance_status(self):
        result = dbaas.instances.get(instance_info.id)
        assert_equal(200, dbaas.last_http_code)
        assert_equal("ACTIVE", result.status)

    @test
    def test_get_legacy_status(self):
        result = dbaas.instances.get(instance_info.id)
        assert_equal(200, dbaas.last_http_code)
        assert_true(result is not None)

    @test
    def test_get_legacy_status_notfound(self):
        assert_raises(exceptions.NotFound, dbaas.instances.get, -2)

    @test(enabled=VOLUME_SUPPORT)
    def test_volume_found(self):
        instance = dbaas.instances.get(instance_info.id)
        if create_new_instance():
            assert_equal(instance_info.volume['size'], instance.volume['size'])
        else:
            assert_true(isinstance(instance_info.volume['size'], float))
        if create_new_instance():
            assert_true(0.0 < instance.volume['used']
                        < instance.volume['size'])

    @test(enabled=EPHEMERAL_SUPPORT)
    def test_ephemeral_mount(self):
        instance = dbaas.instances.get(instance_info.id)
        assert_true(isinstance(instance.local_storage['used'], float))

    @test(enabled=ROOT_PARTITION)
    def test_root_partition(self):
        instance = dbaas.instances.get(instance_info.id)
        assert_true(isinstance(instance.local_storage['used'], float))

    @test(enabled=do_not_delete_instance())
    def test_instance_not_shown_to_other_user(self):
        daffy_ids = [instance.id for instance in
                     self.other_client.instances.list()]
        assert_equal(200, self.other_client.last_http_code)
        admin_ids = [instance.id for instance in dbaas.instances.list()]
        assert_equal(200, dbaas.last_http_code)
        assert_equal(len(daffy_ids), 0)
        assert_not_equal(sorted(admin_ids), sorted(daffy_ids))
        assert_raises(exceptions.NotFound,
                      self.other_client.instances.get, instance_info.id)
        for id in admin_ids:
            assert_equal(daffy_ids.count(id), 0)

    @test(enabled=do_not_delete_instance())
    def test_instance_not_deleted_by_other_user(self):
        assert_raises(exceptions.NotFound,
                      self.other_client.instances.get, instance_info.id)
        assert_raises(exceptions.NotFound,
                      self.other_client.instances.delete, instance_info.id)

    @test(enabled=CONFIG.test_mgmt)
    def test_mgmt_get_instance_after_started(self):
        result = dbaas_admin.management.show(instance_info.id)
        expected_attrs = ['account_id', 'addresses', 'created', 'databases',
                          'flavor', 'guest_status', 'host', 'hostname', 'id',
                          'name', 'root_enabled_at', 'root_enabled_by',
                          'server_state_description', 'status', 'datastore',
                          'updated', 'users', 'volume']
        with CheckInstance(result._info) as check:
            check.attrs_exist(result._info, expected_attrs,
                              msg="Mgmt get instance")
            check.flavor()
            check.datastore()
            check.guest_status()
            check.addresses()
            check.volume_mgmt()


@test(depends_on_classes=[WaitForGuestInstallationToFinish],
      groups=[GROUP, 'dbaas.usage'])
class TestCreateNotification(object):
    """
    Test that the create notification has been sent correctly.
    """

    @test
    def test_create_notification(self):
        expected = {
            'instance_size': instance_info.dbaas_flavor.ram,
            'tenant_id': instance_info.user.tenant_id,
            'instance_id': instance_info.id,
            'instance_name': instance_info.name,
            'created_at': iso_time(instance_info.initial_result.created),
            'launched_at': iso_time(instance_info.initial_result.created),
            'region': 'LOCAL_DEV',
            'availability_zone': 'nova',
        }
        instance_info.consumer.check_message(instance_info.id,
                                             'trove.instance.create',
                                             **expected)


@test(depends_on_groups=['dbaas.api.instances.actions'],
      groups=[GROUP, tests.INSTANCES, "dbaas.diagnostics"])
class CheckDiagnosticsAfterTests(object):
    """Check the diagnostics after running api commands on an instance. """
    @test
    def test_check_diagnostics_on_instance_after_tests(self):
        diagnostics = dbaas_admin.diagnostics.get(instance_info.id)
        assert_equal(200, dbaas.last_http_code)
        diagnostic_tests_helper(diagnostics)
        msg = "Fat Pete has emerged. size (%s > 30MB)" % diagnostics.vmPeak
        assert_true(diagnostics.vmPeak < (30 * 1024), msg)


@test(depends_on=[WaitForGuestInstallationToFinish],
      depends_on_groups=[GROUP_USERS, GROUP_DATABASES, GROUP_ROOT],
      groups=[GROUP, GROUP_STOP],
      runs_after_groups=[GROUP_START,
                         GROUP_START_SIMPLE, GROUP_TEST, tests.INSTANCES])
class DeleteInstance(object):
    """Delete the created instance """

    @time_out(3 * 60)
    @test
    def test_delete(self):
        if do_not_delete_instance():
            CONFIG.get_report().log("TESTS_DO_NOT_DELETE_INSTANCE=True was "
                                    "specified, skipping delete...")
            raise SkipTest("TESTS_DO_NOT_DELETE_INSTANCE was specified.")
        global dbaas
        if not hasattr(instance_info, "initial_result"):
            raise SkipTest("Instance was never created, skipping test...")
        # Update the report so the logs inside the instance will be saved.
        CONFIG.get_report().update()
        dbaas.instances.delete(instance_info.id)

        attempts = 0
        try:
            time.sleep(1)
            result = True
            while result is not None:
                attempts += 1
                result = dbaas.instances.get(instance_info.id)
                assert_equal(200, dbaas.last_http_code)
                assert_equal("SHUTDOWN", result.status)
                time.sleep(1)
        except exceptions.NotFound:
            pass
        except Exception as ex:
            fail("A failure occurred when trying to GET instance %s for the %d"
                 " time: %s" % (str(instance_info.id), attempts, str(ex)))

    @time_out(30)
    @test(enabled=VOLUME_SUPPORT,
          depends_on=[test_delete])
    def test_volume_is_deleted(self):
        raise SkipTest("Cannot test volume is deleted from db.")
        try:
            while True:
                db.volume_get(instance_info.user_context,
                              instance_info.volume_id)
                time.sleep(1)
        except backend_exception.VolumeNotFound:
            pass

    #TODO(tim-simpson): make sure that the actual instance, volume,
    # guest status, and DNS entries are deleted.


@test(depends_on=[WaitForGuestInstallationToFinish],
      runs_after=[DeleteInstance],
      groups=[GROUP, GROUP_STOP, 'dbaas.usage'])
class AfterDeleteChecks(object):
    @test
    def test_instance_delete_event_sent(self):
        deleted_at = None
        mgmt_details = dbaas_admin.management.index(deleted=True)
        for instance in mgmt_details:
            if instance.id == instance_info.id:
                deleted_at = instance.deleted_at
        expected = {
            'instance_size': instance_info.dbaas_flavor.ram,
            'tenant_id': instance_info.user.tenant_id,
            'instance_id': instance_info.id,
            'instance_name': instance_info.name,
            'created_at': iso_time(instance_info.initial_result.created),
            'launched_at': iso_time(instance_info.initial_result.created),
            'deleted_at': iso_time(deleted_at),
        }
        instance_info.consumer.check_message(instance_info.id,
                                             'trove.instance.delete',
                                             **expected)

    @test
    def test_instance_status_deleted_in_db(self):
        mgmt_details = dbaas_admin.management.index(deleted=True)
        for instance in mgmt_details:
            if instance.id == instance_info.id:
                assert_equal(instance.service_status, 'DELETED')
                break
        else:
            fail("Could not find instance %s" % instance_info.id)


@test(depends_on_classes=[CreateInstance,
                          WaitForGuestInstallationToFinish],
      groups=[GROUP, GROUP_START, GROUP_START_SIMPLE],
      enabled=CONFIG.test_mgmt)
class VerifyInstanceMgmtInfo(object):

    @before_class
    def set_up(self):
        self.mgmt_details = dbaas_admin.management.show(instance_info.id)

    def _assert_key(self, k, expected):
        v = getattr(self.mgmt_details, k)
        err = "Key %r does not match expected value of %r (was %r)." \
              % (k, expected, v)
        assert_equal(str(v), str(expected), err)

    @test
    def test_id_matches(self):
        self._assert_key('id', instance_info.id)

    @test
    def test_bogus_instance_mgmt_data(self):
        # Make sure that a management call to a bogus API 500s.
        # The client reshapes the exception into just an OpenStackException.
        assert_raises(exceptions.NotFound,
                      dbaas_admin.management.show, -1)

    @test
    def test_mgmt_ips_associated(self):
        # Test that the management index properly associates an instances with
        # ONLY its IPs.
        mgmt_index = dbaas_admin.management.index()
        # Every instances has exactly one address.
        for instance in mgmt_index:
            assert_equal(1, len(instance.ips))

    @test
    def test_mgmt_data(self):
        # Test that the management API returns all the values we expect it to.
        info = instance_info
        ir = info.initial_result
        cid = ir.id
        expected = {
            'id': cid,
            'name': ir.name,
            'account_id': info.user.auth_user,
            # TODO(hub-cap): fix this since its a flavor object now
            #'flavorRef': info.dbaas_flavor_href,
            'databases': [
                {
                    'name': 'db2',
                    'character_set': 'utf8',
                    'collate': 'utf8_general_ci',
                },
                {
                    'name': 'firstdb',
                    'character_set': 'latin2',
                    'collate': 'latin2_general_ci',
                }
            ],
        }

        expected_entry = info.expected_dns_entry()
        if expected_entry:
            expected['hostname'] = expected_entry.name

        assert_true(self.mgmt_details is not None)
        for (k, v) in expected.items():
            msg = "Attr %r is missing." % k
            assert_true(hasattr(self.mgmt_details, k), msg)
            msg = ("Attr %r expected to be %r but was %r." %
                   (k, v, getattr(self.mgmt_details, k)))
            assert_equal(getattr(self.mgmt_details, k), v, msg)
        print(self.mgmt_details.users)
        for user in self.mgmt_details.users:
            assert_true('name' in user, "'name' not in users element.")


class CheckInstance(AttrCheck):
    """Class to check various attributes of Instance details"""

    def __init__(self, instance):
        super(CheckInstance, self).__init__()
        self.instance = instance

    def flavor(self):
        if 'flavor' not in self.instance:
            self.fail("'flavor' not found in instance.")
        else:
            expected_attrs = ['id', 'links']
            self.attrs_exist(self.instance['flavor'], expected_attrs,
                             msg="Flavor")
            self.links(self.instance['flavor']['links'])

    def datastore(self):
        if 'datastore' not in self.instance:
            self.fail("'datastore' not found in instance.")
        else:
            expected_attrs = ['type', 'version']
            self.attrs_exist(self.instance['datastore'], expected_attrs,
                             msg="datastore")

    def volume_key_exists(self):
        if 'volume' not in self.instance:
            self.fail("'volume' not found in instance.")
            return False
        return True

    def volume(self):
        if not VOLUME_SUPPORT:
            return
        if self.volume_key_exists():
            expected_attrs = ['size']
            if not create_new_instance():
                expected_attrs.append('used')
            self.attrs_exist(self.instance['volume'], expected_attrs,
                             msg="Volumes")

    def used_volume(self):
        if not VOLUME_SUPPORT:
            return
        if self.volume_key_exists():
            expected_attrs = ['size', 'used']
            print(self.instance)
            self.attrs_exist(self.instance['volume'], expected_attrs,
                             msg="Volumes")

    def volume_mgmt(self):
        if not VOLUME_SUPPORT:
            return
        if self.volume_key_exists():
            expected_attrs = ['description', 'id', 'name', 'size']
            self.attrs_exist(self.instance['volume'], expected_attrs,
                             msg="Volumes")

    def addresses(self):
        expected_attrs = ['addr', 'version']
        print(self.instance)
        networks = ['usernet']
        for network in networks:
            for address in self.instance['addresses'][network]:
                self.attrs_exist(address, expected_attrs,
                                 msg="Address")

    def guest_status(self):
        expected_attrs = ['created_at', 'deleted', 'deleted_at', 'instance_id',
                          'state', 'state_description', 'updated_at']
        self.attrs_exist(self.instance['guest_status'], expected_attrs,
                         msg="Guest status")

    def mgmt_volume(self):
        if not VOLUME_SUPPORT:
            return
        expected_attrs = ['description', 'id', 'name', 'size']
        self.attrs_exist(self.instance['volume'], expected_attrs,
                         msg="Volume")


@test(groups=[GROUP])
class BadInstanceStatusBug():
    @before_class()
    def setUp(self):
        self.instances = []
        reqs = Requirements(is_admin=True)
        self.user = CONFIG.users.find_user(
            reqs, black_list=[])
        self.client = create_dbaas_client(self.user)
        self.mgmt = self.client.management

    @test
    def test_instance_status_after_double_migrate(self):
        """
        This test is to verify that instance status returned is more
        informative than 'Status is {}'.  There are several ways to
        replicate this error.  A double migration is just one of them but
        since this is a known way to recreate that error we will use it
        here to be sure that the error is fixed.  The actual code lives
        in trove/instance/models.py in _validate_can_perform_action()
        """
        # TODO(imsplitbit): test other instances where this issue could be
        # replicated.  Resizing a resized instance awaiting confirmation
        # can be used as another case.  This all boils back to the same
        # piece of code so I'm not sure if it's relevant or not but could
        # be done.
        size = None
        if VOLUME_SUPPORT:
            size = {'size': 5}

        result = self.client.instances.create('testbox',
                                              instance_info.dbaas_flavor_href,
                                              size)
        id = result.id
        self.instances.append(id)

        def verify_instance_is_active():
            result = self.client.instances.get(id)
            print(result.status)
            return result.status == 'ACTIVE'

        def attempt_migrate():
            print('attempting migration')
            try:
                self.mgmt.migrate(id)
            except exceptions.UnprocessableEntity:
                return False
            return True

        # Timing necessary to make the error occur
        poll_until(verify_instance_is_active, time_out=120, sleep_time=1)

        try:
            poll_until(attempt_migrate, time_out=10, sleep_time=1)
        except rd_exceptions.PollTimeOut:
            fail('Initial migration timed out')

        try:
            self.mgmt.migrate(id)
        except exceptions.UnprocessableEntity as err:
            assert('status was {}' not in err.message)
        else:
            # If we are trying to test what status is returned when an
            # instance is in a confirm_resize state and another
            # migration is attempted then we also need to
            # assert that an exception is raised when running migrate.
            # If one is not then we aren't able to test what the
            # returned status is in the exception message.
            fail('UnprocessableEntity was not thrown')

    @after_class(always_run=True)
    def tearDown(self):
        while len(self.instances) > 0:
            for id in self.instances:
                try:
                    self.client.instances.delete(id)
                    self.instances.remove(id)
                except exceptions.UnprocessableEntity:
                    sleep(1.0)


def diagnostic_tests_helper(diagnostics):
    print("diagnostics : %r" % diagnostics._info)
    expected_attrs = ['version', 'fdSize', 'vmSize', 'vmHwm', 'vmRss',
                      'vmPeak', 'threads']
    CheckInstance(None).attrs_exist(diagnostics._info, expected_attrs,
                                    msg="Diagnostics")
    assert_true(isinstance(diagnostics.fdSize, int))
    assert_true(isinstance(diagnostics.threads, int))
    assert_true(isinstance(diagnostics.vmHwm, int))
    assert_true(isinstance(diagnostics.vmPeak, int))
    assert_true(isinstance(diagnostics.vmRss, int))
    assert_true(isinstance(diagnostics.vmSize, int))
    actual_version = diagnostics.version
    update_test_conf = CONFIG.values.get("guest-update-test", None)
    if update_test_conf is not None:
        if actual_version == update_test_conf['next-version']:
            return  # This is acceptable but may not match the regex.
    version_pattern = re.compile(r'[a-f0-9]+')
    msg = "Version %s does not match pattern %s." % (actual_version,
                                                     version_pattern)
    assert_true(version_pattern.match(actual_version), msg)

########NEW FILE########
__FILENAME__ = instances_actions
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import time

from proboscis import after_class
from proboscis import before_class
from proboscis import test
from proboscis import asserts
from proboscis.decorators import time_out
from proboscis import SkipTest

from trove import tests
from trove.tests.util.check import Checker
from troveclient.compat.exceptions import BadRequest
from troveclient.compat.exceptions import HTTPNotImplemented
from trove.tests.api.instances import GROUP as INSTANCE_GROUP
from trove.tests.api.instances import instance_info
from trove.tests.api.instances import GROUP_START
from trove.tests.api.instances import assert_unprocessable
from trove.tests.api.instances import VOLUME_SUPPORT
from trove.tests.api.instances import EPHEMERAL_SUPPORT
from trove.tests.util.server_connection import create_server_connection
from trove.common.utils import poll_until
import trove.tests.util as testsutil
from trove.tests.config import CONFIG
from trove.tests.util import LocalSqlClient
from sqlalchemy import exc as sqlalchemy_exc
from trove.tests.util.check import TypeCheck
from sqlalchemy.sql.expression import text

GROUP = "dbaas.api.instances.actions"
GROUP_REBOOT = "dbaas.api.instances.actions.reboot"
GROUP_RESTART = "dbaas.api.instances.actions.restart"
GROUP_STOP_MYSQL = "dbaas.api.instances.actions.stop"
MYSQL_USERNAME = "test_user"
MYSQL_PASSWORD = "abcde"
# stored in test conf
SERVICE_ID = '123'
FAKE_MODE = CONFIG.fake_mode
# If true, then we will actually log into the database.
USE_IP = not FAKE_MODE
# If true, then we will actually search for the process
USE_LOCAL_OVZ = CONFIG.use_local_ovz


class MySqlConnection(object):

    def __init__(self, host):
        self.host = host

    def connect(self):
        """Connect to MySQL database."""
        print("Connecting to MySQL, mysql --host %s -u %s -p%s"
              % (self.host, MYSQL_USERNAME, MYSQL_PASSWORD))
        sql_engine = LocalSqlClient.init_engine(MYSQL_USERNAME, MYSQL_PASSWORD,
                                                self.host)
        self.client = LocalSqlClient(sql_engine, use_flush=False)

    def is_connected(self):
        try:
            with self.client:
                self.client.execute(text("""SELECT "Hello.";"""))
            return True
        except (sqlalchemy_exc.OperationalError,
                sqlalchemy_exc.DisconnectionError,
                sqlalchemy_exc.TimeoutError):
            return False
        except Exception as ex:
            print("EX WAS:")
            print(type(ex))
            print(ex)
            raise ex


TIME_OUT_TIME = 15 * 60
USER_WAS_DELETED = False


class ActionTestBase(object):
    """Has some helpful functions for testing actions.

    The test user must be created for some of these functions to work.

    """

    def set_up(self):
        """If you're using this as a base class, call this method first."""
        self.dbaas = instance_info.dbaas
        if USE_IP:
            address = instance_info.get_address()
            self.connection = MySqlConnection(address)

    @property
    def instance(self):
        return self.dbaas.instances.get(self.instance_id)

    @property
    def instance_address(self):
        return instance_info.get_address()

    @property
    def instance_id(self):
        return instance_info.id

    def create_user(self):
        """Create a MySQL user we can use for this test."""

        users = [{"name": MYSQL_USERNAME, "password": MYSQL_PASSWORD,
                  "databases": [{"name": MYSQL_USERNAME}]}]
        self.dbaas.users.create(instance_info.id, users)

        def has_user():
            users = self.dbaas.users.list(instance_info.id)
            return any([user.name == MYSQL_USERNAME for user in users])

        poll_until(has_user, time_out=30)
        if not FAKE_MODE:
            time.sleep(5)

    def ensure_mysql_is_running(self):
        """Make sure MySQL is accessible before restarting."""
        with Checker() as check:
            if USE_IP:
                self.connection.connect()
                check.true(self.connection.is_connected(),
                           "Able to connect to MySQL.")
                self.proc_id = self.find_mysql_proc_on_instance()
                check.true(self.proc_id is not None,
                           "MySQL process can not be found.")
            instance = self.instance
            check.false(instance is None)
            check.equal(instance.status, "ACTIVE")

    def find_mysql_proc_on_instance(self):
        server = create_server_connection(self.instance_id)
        cmd = "ps aux | grep /usr/sbin/mysqld " \
              "| awk '{print $2}'"
        stdout, stderr = server.execute(cmd)
        try:
            return int(stdout)
        except ValueError:
            return None

    def log_current_users(self):
        users = self.dbaas.users.list(self.instance_id)
        CONFIG.get_report().log("Current user count = %d" % len(users))
        for user in users:
            CONFIG.get_report().log("\t" + str(user))

    def _build_expected_msg(self):
        expected = {
            'instance_size': instance_info.dbaas_flavor.ram,
            'tenant_id': instance_info.user.tenant_id,
            'instance_id': instance_info.id,
            'instance_name': instance_info.name,
            'created_at': testsutil.iso_time(
                instance_info.initial_result.created),
            'launched_at': testsutil.iso_time(self.instance.updated),
            'modify_at': testsutil.iso_time(self.instance.updated)
        }
        return expected


@test(depends_on_groups=[GROUP_START])
def create_user():
    """Create a test user so that subsequent tests can log in."""
    helper = ActionTestBase()
    helper.set_up()
    if USE_IP:
        try:
            helper.create_user()
        except BadRequest:
            pass  # Ignore this if the user already exists.
        helper.connection.connect()
        asserts.assert_true(helper.connection.is_connected(),
                            "Test user must be able to connect to MySQL.")


class RebootTestBase(ActionTestBase):
    """Tests restarting MySQL."""

    def call_reboot(self):
        raise NotImplementedError()

    def wait_for_broken_connection(self):
        """Wait until our connection breaks."""
        if not USE_IP:
            return
        if not hasattr(self, "connection"):
            return
        poll_until(self.connection.is_connected,
                   lambda connected: not connected,
                   time_out=TIME_OUT_TIME)

    def wait_for_successful_restart(self):
        """Wait until status becomes running."""
        def is_finished_rebooting():
            instance = self.instance
            if instance.status == "REBOOT":
                return False
            asserts.assert_equal("ACTIVE", instance.status)
            return True

        poll_until(is_finished_rebooting, time_out=TIME_OUT_TIME)

    def assert_mysql_proc_is_different(self):
        if not USE_IP:
            return
        new_proc_id = self.find_mysql_proc_on_instance()
        asserts.assert_not_equal(new_proc_id, self.proc_id,
                                 "MySQL process ID should be different!")

    def successful_restart(self):
        """Restart MySQL via the REST API successfully."""
        self.fix_mysql()
        self.call_reboot()
        self.wait_for_broken_connection()
        self.wait_for_successful_restart()
        self.assert_mysql_proc_is_different()

    def mess_up_mysql(self):
        """Ruin MySQL's ability to restart."""
        server = create_server_connection(self.instance_id)
        cmd = "sudo cp /dev/null /var/lib/mysql/ib_logfile%d"
        instance_info.dbaas_admin.management.stop(self.instance_id)
        for index in range(2):
            server.execute(cmd % index)

    def fix_mysql(self):
        """Fix MySQL's ability to restart."""
        if not FAKE_MODE:
            server = create_server_connection(self.instance_id)
            cmd = "sudo rm /var/lib/mysql/ib_logfile%d"
            # We want to stop mysql so that upstart does not keep trying to
            # respawn it and block the guest agent from accessing the logs.
            instance_info.dbaas_admin.management.stop(self.instance_id)
            for index in range(2):
                server.execute(cmd % index)

    def wait_for_failure_status(self):
        """Wait until status becomes running."""
        def is_finished_rebooting():
            instance = self.instance
            if instance.status == "REBOOT" or instance.status == "ACTIVE":
                return False
            # The reason we check for BLOCKED as well as SHUTDOWN is because
            # Upstart might try to bring mysql back up after the borked
            # connection and the guest status can be either
            asserts.assert_true(instance.status in ("SHUTDOWN", "BLOCKED"))
            return True

        poll_until(is_finished_rebooting, time_out=TIME_OUT_TIME)

    def unsuccessful_restart(self):
        """Restart MySQL via the REST when it should fail, assert it does."""
        assert not FAKE_MODE
        self.mess_up_mysql()
        self.call_reboot()
        self.wait_for_broken_connection()
        self.wait_for_failure_status()

    def restart_normally(self):
        """Fix iblogs and reboot normally."""
        self.fix_mysql()
        self.test_successful_restart()


@test(groups=[tests.INSTANCES, INSTANCE_GROUP, GROUP, GROUP_RESTART],
      depends_on_groups=[GROUP_START], depends_on=[create_user])
class RestartTests(RebootTestBase):
    """Tests restarting MySQL."""

    def call_reboot(self):
        self.instance.restart()
        asserts.assert_equal(202, self.dbaas.last_http_code)

    @before_class
    def test_set_up(self):
        self.set_up()

    @test
    def test_ensure_mysql_is_running(self):
        """Make sure MySQL is accessible before restarting."""
        self.ensure_mysql_is_running()

    @test(depends_on=[test_ensure_mysql_is_running], enabled=not FAKE_MODE)
    def test_unsuccessful_restart(self):
        """Restart MySQL via the REST when it should fail, assert it does."""
        if FAKE_MODE:
            raise SkipTest("Cannot run this in fake mode.")
        self.unsuccessful_restart()

    @test(depends_on=[test_set_up],
          runs_after=[test_ensure_mysql_is_running, test_unsuccessful_restart])
    def test_successful_restart(self):
        """Restart MySQL via the REST API successfully."""
        self.successful_restart()


@test(groups=[tests.INSTANCES, INSTANCE_GROUP, GROUP, GROUP_STOP_MYSQL],
      depends_on_groups=[GROUP_START], depends_on=[create_user])
class StopTests(RebootTestBase):
    """Tests which involve stopping MySQL."""

    def call_reboot(self):
        self.instance.restart()

    @before_class
    def test_set_up(self):
        self.set_up()

    @test
    def test_ensure_mysql_is_running(self):
        """Make sure MySQL is accessible before restarting."""
        self.ensure_mysql_is_running()

    @test(depends_on=[test_ensure_mysql_is_running])
    def test_stop_mysql(self):
        """Stops MySQL."""
        instance_info.dbaas_admin.management.stop(self.instance_id)
        self.wait_for_broken_connection()
        self.wait_for_failure_status()

    @test(depends_on=[test_stop_mysql])
    def test_instance_get_shows_volume_info_while_mysql_is_down(self):
        """
        Confirms the get call behaves appropriately while an instance is
        down.
        """
        if not VOLUME_SUPPORT:
            raise SkipTest("Not testing volumes.")
        instance = self.dbaas.instances.get(self.instance_id)
        with TypeCheck("instance", instance) as check:
            check.has_field("volume", dict)
            check.true('size' in instance.volume)
            check.true('used' in instance.volume)
            check.true(isinstance(instance.volume.get('size', None), int))
            check.true(isinstance(instance.volume.get('used', None), float))

    @test(depends_on=[test_set_up],
          runs_after=[test_instance_get_shows_volume_info_while_mysql_is_down])
    def test_successful_restart_when_in_shutdown_state(self):
        """Restart MySQL via the REST API successfully when MySQL is down."""
        self.successful_restart()


@test(groups=[tests.INSTANCES, INSTANCE_GROUP, GROUP, GROUP_REBOOT],
      depends_on_groups=[GROUP_START], depends_on=[RestartTests, create_user])
class RebootTests(RebootTestBase):
    """Tests restarting instance."""

    def call_reboot(self):
        instance_info.dbaas_admin.management.reboot(self.instance_id)

    @before_class
    def test_set_up(self):
        self.set_up()
        asserts.assert_true(hasattr(self, 'dbaas'))
        asserts.assert_true(self.dbaas is not None)

    @test
    def test_ensure_mysql_is_running(self):
        """Make sure MySQL is accessible before restarting."""
        self.ensure_mysql_is_running()

    @test(depends_on=[test_ensure_mysql_is_running])
    def test_unsuccessful_restart(self):
        """Restart MySQL via the REST when it should fail, assert it does."""
        if FAKE_MODE:
            raise SkipTest("Cannot run this in fake mode.")
        self.unsuccessful_restart()

    @after_class(depends_on=[test_set_up])
    def test_successful_restart(self):
        """Restart MySQL via the REST API successfully."""
        self.successful_restart()


@test(groups=[tests.INSTANCES, INSTANCE_GROUP, GROUP,
              GROUP + ".resize.instance"],
      depends_on_groups=[GROUP_START], depends_on=[create_user],
      runs_after=[RebootTests])
class ResizeInstanceTest(ActionTestBase):

    """
    Integration Test cases for resize instance
    """
    @property
    def flavor_id(self):
        return instance_info.dbaas_flavor_href

    def get_flavor_href(self, flavor_id=2):
        res = instance_info.dbaas.find_flavor_and_self_href(flavor_id)
        dbaas_flavor, dbaas_flavor_href = res
        return dbaas_flavor_href

    def wait_for_resize(self):
        def is_finished_resizing():
            instance = self.instance
            if instance.status == "RESIZE":
                return False
            asserts.assert_equal("ACTIVE", instance.status)
            return True
        poll_until(is_finished_resizing, time_out=TIME_OUT_TIME)

    @before_class
    def setup(self):
        self.set_up()
        if USE_IP:
            self.connection.connect()
            asserts.assert_true(self.connection.is_connected(),
                                "Should be able to connect before resize.")
        self.user_was_deleted = False

    @test
    def test_instance_resize_same_size_should_fail(self):
        asserts.assert_raises(BadRequest, self.dbaas.instances.resize_instance,
                              self.instance_id, self.flavor_id)

    @test(enabled=VOLUME_SUPPORT)
    def test_instance_resize_to_ephemeral_in_volume_support_should_fail(self):
        flavor_name = CONFIG.values.get('instance_bigger_eph_flavor_name',
                                        'eph.rd-smaller')
        flavors = self.dbaas.find_flavors_by_name(flavor_name)

        def is_active():
            return self.instance.status == 'ACTIVE'
        poll_until(is_active, time_out=TIME_OUT_TIME)
        asserts.assert_equal(self.instance.status, 'ACTIVE')

        self.get_flavor_href(
            flavor_id=self.expected_old_flavor_id)
        asserts.assert_raises(HTTPNotImplemented,
                              self.dbaas.instances.resize_instance,
                              self.instance_id, flavors[0].id)

    @test(enabled=EPHEMERAL_SUPPORT)
    def test_instance_resize_to_non_ephemeral_flavor_should_fail(self):
        flavor_name = CONFIG.values.get('instance_bigger_flavor_name',
                                        'm1-small')
        flavors = self.dbaas.find_flavors_by_name(flavor_name)
        asserts.assert_raises(BadRequest, self.dbaas.instances.resize_instance,
                              self.instance_id, flavors[0].id)

    def obtain_flavor_ids(self):
        old_id = self.instance.flavor['id']
        self.expected_old_flavor_id = old_id
        res = instance_info.dbaas.find_flavor_and_self_href(old_id)
        self.expected_dbaas_flavor, _dontcare_ = res
        if EPHEMERAL_SUPPORT:
            flavor_name = CONFIG.values.get('instance_bigger_eph_flavor_name',
                                            'eph.rd-smaller')
        else:
            flavor_name = CONFIG.values.get('instance_bigger_flavor_name',
                                            'm1.small')
        flavors = self.dbaas.find_flavors_by_name(flavor_name)
        asserts.assert_equal(len(flavors), 1,
                             "Number of flavors with name '%s' "
                             "found was '%d'." % (flavor_name,
                                                  len(flavors)))
        flavor = flavors[0]
        self.old_dbaas_flavor = instance_info.dbaas_flavor
        instance_info.dbaas_flavor = flavor
        asserts.assert_true(flavor is not None,
                            "Flavor '%s' not found!" % flavor_name)
        flavor_href = self.dbaas.find_flavor_self_href(flavor)
        asserts.assert_true(flavor_href is not None,
                            "Flavor href '%s' not found!" % flavor_name)
        self.expected_new_flavor_id = flavor.id

    @test(depends_on=[test_instance_resize_same_size_should_fail])
    def test_status_changed_to_resize(self):
        self.log_current_users()
        self.obtain_flavor_ids()
        self.dbaas.instances.resize_instance(
            self.instance_id,
            self.get_flavor_href(flavor_id=self.expected_new_flavor_id))
        asserts.assert_equal(202, self.dbaas.last_http_code)

        #(WARNING) IF THE RESIZE IS WAY TOO FAST THIS WILL FAIL
        assert_unprocessable(
            self.dbaas.instances.resize_instance,
            self.instance_id,
            self.get_flavor_href(flavor_id=self.expected_new_flavor_id))

    @test(depends_on=[test_status_changed_to_resize])
    @time_out(TIME_OUT_TIME)
    def test_instance_returns_to_active_after_resize(self):
        self.wait_for_resize()

    @test(depends_on=[test_instance_returns_to_active_after_resize,
                      test_status_changed_to_resize],
          groups=["dbaas.usage"])
    def test_resize_instance_usage_event_sent(self):
        expected = self._build_expected_msg()
        expected['old_instance_size'] = self.old_dbaas_flavor.ram
        instance_info.consumer.check_message(instance_info.id,
                                             'trove.instance.modify_flavor',
                                             **expected)

    @test(depends_on=[test_instance_returns_to_active_after_resize],
          runs_after=[test_resize_instance_usage_event_sent])
    def resize_should_not_delete_users(self):
        """Resize should not delete users."""
        # Resize has an incredibly weird bug where users are deleted after
        # a resize. The code below is an attempt to catch this while proceeding
        # with the rest of the test (note the use of runs_after).
        if USE_IP:
            self.connection.connect()
            if not self.connection.is_connected():
                # Ok, this is def. a failure, but before we toss up an error
                # lets recreate to see how far we can get.
                CONFIG.get_report().log(
                    "Having to recreate the test_user! Resizing killed it!")
                self.log_current_users()
                self.create_user()
                fail("Somehow, the resize made the test user disappear.")

    @test(depends_on=[test_instance_returns_to_active_after_resize],
          runs_after=[resize_should_not_delete_users])
    def test_make_sure_mysql_is_running_after_resize(self):
        self.ensure_mysql_is_running()

    @test(depends_on=[test_instance_returns_to_active_after_resize],
          runs_after=[test_make_sure_mysql_is_running_after_resize])
    def test_instance_has_new_flavor_after_resize(self):
        actual = self.get_flavor_href(self.instance.flavor['id'])
        expected = self.get_flavor_href(flavor_id=self.expected_new_flavor_id)
        asserts.assert_equal(actual, expected)

    @test(depends_on=[test_instance_has_new_flavor_after_resize])
    @time_out(TIME_OUT_TIME)
    def test_resize_down(self):
        expected_dbaas_flavor = self.expected_dbaas_flavor

        def is_active():
            return self.instance.status == 'ACTIVE'
        poll_until(is_active, time_out=TIME_OUT_TIME)
        asserts.assert_equal(self.instance.status, 'ACTIVE')

        old_flavor_href = self.get_flavor_href(
            flavor_id=self.expected_old_flavor_id)

        self.dbaas.instances.resize_instance(self.instance_id, old_flavor_href)
        asserts.assert_equal(202, self.dbaas.last_http_code)
        self.old_dbaas_flavor = instance_info.dbaas_flavor
        instance_info.dbaas_flavor = expected_dbaas_flavor
        self.wait_for_resize()
        asserts.assert_equal(str(self.instance.flavor['id']),
                             str(self.expected_old_flavor_id))

    @test(depends_on=[test_resize_down],
          groups=["dbaas.usage"])
    def test_resize_instance_down_usage_event_sent(self):
        expected = self._build_expected_msg()
        expected['old_instance_size'] = self.old_dbaas_flavor.ram
        instance_info.consumer.check_message(instance_info.id,
                                             'trove.instance.modify_flavor',
                                             **expected)


@test(groups=[tests.INSTANCES, INSTANCE_GROUP, GROUP,
              GROUP + ".resize.instance"],
      depends_on_groups=[GROUP_START], depends_on=[create_user],
      runs_after=[RebootTests, ResizeInstanceTest])
def resize_should_not_delete_users():
    if USER_WAS_DELETED:
        fail("Somehow, the resize made the test user disappear.")


@test(runs_after=[ResizeInstanceTest], depends_on=[create_user],
      groups=[GROUP, tests.INSTANCES],
      enabled=VOLUME_SUPPORT)
class ResizeInstanceVolume(ActionTestBase):
    """Resize the volume of the instance """

    @before_class
    def setUp(self):
        self.set_up()
        self.old_volume_size = int(instance_info.volume['size'])
        self.new_volume_size = self.old_volume_size + 1
        self.old_volume_fs_size = instance_info.get_volume_filesystem_size()

        # Create some databases to check they still exist after the resize
        self.expected_dbs = ['salmon', 'halibut']
        databases = []
        for name in self.expected_dbs:
            databases.append({"name": name})
        instance_info.dbaas.databases.create(instance_info.id, databases)

    @test
    @time_out(60)
    def test_volume_resize(self):
        instance_info.dbaas.instances.resize_volume(instance_info.id,
                                                    self.new_volume_size)

    @test(depends_on=[test_volume_resize])
    @time_out(300)
    def test_volume_resize_success(self):

        def check_resize_status():
            instance = instance_info.dbaas.instances.get(instance_info.id)
            if instance.status == "ACTIVE":
                return True
            elif instance.status == "RESIZE":
                return False
            else:
                fail("Status should not be %s" % instance.status)

        poll_until(check_resize_status, sleep_time=2, time_out=300)
        instance = instance_info.dbaas.instances.get(instance_info.id)
        asserts.assert_equal(instance.volume['size'], self.new_volume_size)

    @test(depends_on=[test_volume_resize_success])
    def test_volume_filesystem_resize_success(self):
        # The get_volume_filesystem_size is a mgmt call through the guestagent
        # and the volume resize occurs through the fake nova-volume.
        # Currently the guestagent fakes don't have access to the nova fakes so
        # it doesn't know that a volume resize happened and to what size so
        # we can't fake the filesystem size.
        if FAKE_MODE:
            raise SkipTest("Cannot run this in fake mode.")
        new_volume_fs_size = instance_info.get_volume_filesystem_size()
        asserts.assert_true(self.old_volume_fs_size < new_volume_fs_size)
        # The total filesystem size is not going to be exactly the same size of
        # cinder volume but it should round to it. (e.g. round(1.9) == 2)
        asserts.assert_equal(round(new_volume_fs_size), self.new_volume_size)

    @test(depends_on=[test_volume_resize_success], groups=["dbaas.usage"])
    def test_resize_volume_usage_event_sent(self):
        expected = self._build_expected_msg()
        expected['volume_size'] = self.new_volume_size
        expected['old_volume_size'] = self.old_volume_size
        instance_info.consumer.check_message(instance_info.id,
                                             'trove.instance.modify_volume',
                                             **expected)

    @test
    @time_out(300)
    def test_volume_resize_success_databases(self):
        databases = instance_info.dbaas.databases.list(instance_info.id)
        db_list = []
        for database in databases:
            db_list.append(database.name)
        for name in self.expected_dbs:
            if name not in db_list:
                fail("Database %s was not found after the volume resize. "
                     "Returned list: %s" % (name, databases))


# This tests the ability of the guest to upgrade itself.
# It is necessarily tricky because we need to be able to upload a new copy of
# the guest into an apt-repo in the middle of the test.
# "guest-update-test" is where the knowledge of how to do this is set in the
# test conf. If it is not specified this test never runs.
UPDATE_GUEST_CONF = CONFIG.values.get("guest-update-test", None)


@test(groups=[tests.INSTANCES, INSTANCE_GROUP, GROUP, GROUP + ".update_guest"],
      depends_on=[create_user],
      depends_on_groups=[GROUP_START])
class UpdateGuest(object):

    def get_version(self):
        info = instance_info.dbaas_admin.diagnostics.get(instance_info.id)
        return info.version

    @before_class(enabled=UPDATE_GUEST_CONF is not None)
    def check_version_is_old(self):
        """Make sure we have the old version before proceeding."""
        self.old_version = self.get_version()
        self.next_version = UPDATE_GUEST_CONF["next-version"]
        asserts.assert_not_equal(self.old_version, self.next_version)

    @test(enabled=UPDATE_GUEST_CONF is not None)
    def upload_update_to_repo(self):
        cmds = UPDATE_GUEST_CONF["install-repo-cmd"]
        testsutil.execute(*cmds, run_as_root=True, root_helper="sudo")

    @test(enabled=UPDATE_GUEST_CONF is not None,
          depends_on=[upload_update_to_repo])
    def update_and_wait_to_finish(self):
        instance_info.dbaas_admin.management.update(instance_info.id)

        def finished():
            current_version = self.get_version()
            if current_version == self.next_version:
                return True
            # The only valid thing for it to be aside from next_version is
            # old version.
            assert_equal(current_version, self.old_version)
        poll_until(finished, sleep_time=1, time_out=3 * 60)

    @test(enabled=UPDATE_GUEST_CONF is not None,
          depends_on=[upload_update_to_repo])
    @time_out(30)
    def update_again(self):
        """Test the wait time of a pointless update."""
        instance_info.dbaas_admin.management.update(instance_info.id)
        # Make sure this isn't taking too long.
        instance_info.dbaas_admin.diagnostics.get(instance_info.id)

########NEW FILE########
__FILENAME__ = instances_delete
# Copyright 2013 OpenStack Foundation
# Copyright 2013 Rackspace Hosting
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

import time

from proboscis import after_class
from proboscis import before_class
from proboscis import test
from proboscis import asserts
from proboscis.decorators import time_out

from trove.common import cfg
from troveclient.compat import exceptions
from trove.tests.util import create_dbaas_client
from trove.common.utils import poll_until
from trove.tests.util import test_config
from trove.tests.util.users import Requirements
from trove.tests.api.instances import instance_info
from trove.tests.api.instances import VOLUME_SUPPORT

CONF = cfg.CONF


class TestBase(object):

    def set_up(self):
        reqs = Requirements(is_admin=True)
        self.user = test_config.users.find_user(reqs)
        self.dbaas = create_dbaas_client(self.user)

    def create_instance(self, name, size=1):
        volume = None
        if VOLUME_SUPPORT:
            volume = {'size': size}
        result = self.dbaas.instances.create(name,
                                             instance_info.dbaas_flavor_href,
                                             volume, [], [])
        return result.id

    def wait_for_instance_status(self, instance_id, status="ACTIVE"):
        poll_until(lambda: self.dbaas.instances.get(instance_id),
                   lambda instance: instance.status == status,
                   time_out=3, sleep_time=1)

    def wait_for_instance_task_status(self, instance_id, description):
        poll_until(lambda: self.dbaas.management.show(instance_id),
                   lambda instance: instance.task_description == description,
                   time_out=3, sleep_time=1)

    def is_instance_deleted(self, instance_id):
        while True:
            try:
                self.dbaas.instances.get(instance_id)
            except exceptions.NotFound:
                return True
            time.sleep(.5)

    def get_task_info(self, instance_id):
        instance = self.dbaas.management.show(instance_id)
        return instance.status, instance.task_description

    def delete_instance(self, instance_id, assert_deleted=True):
        instance = self.dbaas.instances.get(instance_id)
        instance.delete()
        if assert_deleted:
            asserts.assert_true(self.is_instance_deleted(instance_id))

    def delete_errored_instance(self, instance_id):
        self.wait_for_instance_status(instance_id, 'ERROR')
        status, desc = self.get_task_info(instance_id)
        asserts.assert_equal(status, "ERROR")
        self.delete_instance(instance_id)


@test(runs_after_groups=["services.initialize", "dbaas.guest.shutdown"],
      groups=['dbaas.api.instances.delete'])
class ErroredInstanceDelete(TestBase):
    """
    Test that an instance in an ERROR state is actually deleted when delete
    is called.
    """

    @before_class
    def set_up_err(self):
        """Create some flawed instances."""
        from trove.taskmanager.models import CONF
        self.old_dns_support = CONF.trove_dns_support
        CONF.trove_dns_support = False

        super(ErroredInstanceDelete, self).set_up()
        # Create an instance that fails during server prov.
        self.server_error = self.create_instance('test_SERVER_ERROR')
        if VOLUME_SUPPORT:
            # Create an instance that fails during volume prov.
            self.volume_error = self.create_instance('test_VOLUME_ERROR',
                                                     size=9)
        else:
            self.volume_error = None
        # Create an instance that fails during DNS prov.
        #self.dns_error = self.create_instance('test_DNS_ERROR')
        # Create an instance that fails while it's been deleted the first time.
        self.delete_error = self.create_instance('test_ERROR_ON_DELETE')

    @after_class(always_run=True)
    def clean_up(self):
        from trove.taskmanager.models import CONF
        CONF.trove_dns_support = self.old_dns_support

    @test
    @time_out(30)
    def delete_server_error(self):
        self.delete_errored_instance(self.server_error)

    @test(enabled=VOLUME_SUPPORT)
    @time_out(30)
    def delete_volume_error(self):
        self.delete_errored_instance(self.volume_error)

    @test(enabled=False)
    @time_out(30)
    def delete_dns_error(self):
        self.delete_errored_instance(self.dns_error)

    @test
    @time_out(30)
    def delete_error_on_delete_instance(self):
        id = self.delete_error
        self.wait_for_instance_status(id, 'ACTIVE')
        self.wait_for_instance_task_status(id, 'No tasks for the instance.')
        instance = self.dbaas.management.show(id)
        asserts.assert_equal(instance.status, "ACTIVE")
        asserts.assert_equal(instance.task_description,
                             'No tasks for the instance.')
        # Try to delete the instance. This fails the first time due to how
        # the test fake  is setup.
        self.delete_instance(id, assert_deleted=False)
        instance = self.dbaas.management.show(id)
        asserts.assert_equal(instance.status, "SHUTDOWN")
        asserts.assert_equal(instance.task_description,
                             "Deleting the instance.")
        # Try a second time. This will succeed.
        self.delete_instance(id)

########NEW FILE########
__FILENAME__ = instances_mysql_down
# Copyright 2012 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
"""
Extra tests to create an instance, shut down MySQL, and delete it.
"""

from proboscis.decorators import time_out
from proboscis import before_class
from proboscis import test
from proboscis import asserts
import time

from datetime import datetime
from troveclient.compat import exceptions
from trove.tests.util import create_client
from trove.common.utils import poll_until
from trove.tests.util import test_config
from trove.tests.api.instances import VOLUME_SUPPORT
from trove.tests.api.instances import EPHEMERAL_SUPPORT


@test(groups=["dbaas.api.instances.down"])
class TestBase(object):
    """Base class for instance-down tests."""

    @before_class
    def set_up(self):
        self.client = create_client(is_admin=False)
        self.mgmt_client = create_client(is_admin=True)

        if EPHEMERAL_SUPPORT:
            flavor_name = test_config.values.get('instance_eph_flavor_name',
                                                 'eph.rd-tiny')
            flavor2_name = test_config.values.get(
                'instance_bigger_eph_flavor_name', 'eph.rd-smaller')
        else:
            flavor_name = test_config.values.get('instance_flavor_name',
                                                 'm1.tiny')
            flavor2_name = test_config.values.get(
                'instance_bigger_flavor_name', 'm1.small')
        flavors = self.client.find_flavors_by_name(flavor_name)
        self.flavor_id = flavors[0].id
        self.name = "TEST_" + str(datetime.now())
        # Get the resize to flavor.
        flavors2 = self.client.find_flavors_by_name(flavor2_name)
        self.new_flavor_id = flavors2[0].id
        asserts.assert_not_equal(self.flavor_id, self.new_flavor_id)

    def _wait_for_active(self):
        poll_until(lambda: self.client.instances.get(self.id),
                   lambda instance: instance.status == "ACTIVE",
                   time_out=(60 * 8))

    @test
    def create_instance(self):
        volume = None
        if VOLUME_SUPPORT:
            volume = {'size': 1}
        initial = self.client.instances.create(self.name, self.flavor_id,
                                               volume, [], [])
        self.id = initial.id
        self._wait_for_active()

    def _shutdown_instance(self):
        self.client.instances.get(self.id)
        self.mgmt_client.management.stop(self.id)

    @test(depends_on=[create_instance])
    def put_into_shutdown_state(self):
        self._shutdown_instance()

    @test(depends_on=[put_into_shutdown_state])
    @time_out(60 * 5)
    def resize_instance_in_shutdown_state(self):
        self.client.instances.resize_instance(self.id, self.new_flavor_id)
        self._wait_for_active()

    @test(depends_on=[create_instance],
          runs_after=[resize_instance_in_shutdown_state])
    def put_into_shutdown_state_2(self):
        self._shutdown_instance()

    @test(depends_on=[put_into_shutdown_state_2],
          enabled=VOLUME_SUPPORT)
    @time_out(60 * 5)
    def resize_volume_in_shutdown_state(self):
        self.client.instances.resize_volume(self.id, 2)
        poll_until(lambda: self.client.instances.get(self.id),
                   lambda instance: instance.volume['size'] == 2,
                   time_out=(60 * 8))

    @test(depends_on=[create_instance],
          runs_after=[resize_volume_in_shutdown_state])
    def put_into_shutdown_state_3(self):
        self._shutdown_instance()

    @test(depends_on=[create_instance],
          runs_after=[put_into_shutdown_state_3])
    @time_out(2 * 60)
    def delete_instances(self):
        instance = self.client.instances.get(self.id)
        instance.delete()
        while True:
            try:
                instance = self.client.instances.get(self.id)
                asserts.assert_equal("SHUTDOWN", instance.status)
            except exceptions.NotFound:
                break
            time.sleep(0.25)

########NEW FILE########
__FILENAME__ = instances_resize
# Copyright 2013 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import mox
from mock import Mock
from testtools import TestCase
from proboscis import test

from novaclient.exceptions import BadRequest
from novaclient.v1_1.servers import Server

from trove.common.exception import PollTimeOut
from trove.common import template
from trove.common import utils
from trove.common.context import TroveContext
from trove.common import instance as rd_instance
from trove.datastore.models import DatastoreVersion
from trove.guestagent import api as guest
from trove.instance.models import DBInstance
from trove.instance.models import InstanceServiceStatus
from trove.instance.tasks import InstanceTasks
from trove.openstack.common.rpc.common import RPCException
from trove.taskmanager import models as models
from trove.tests.fakes import nova
from trove.tests.util import test_config

GROUP = 'dbaas.api.instances.resize'

OLD_FLAVOR_ID = 1
NEW_FLAVOR_ID = 2
OLD_FLAVOR = nova.FLAVORS.get(OLD_FLAVOR_ID)
NEW_FLAVOR = nova.FLAVORS.get(NEW_FLAVOR_ID)


class ResizeTestBase(TestCase):

    def _init(self):
        self.mock = mox.Mox()
        self.instance_id = 500
        context = TroveContext()
        self.db_info = DBInstance.create(
            name="instance",
            flavor_id=OLD_FLAVOR_ID,
            tenant_id=999,
            volume_size=None,
            datastore_version_id=test_config.dbaas_datastore_version_id,
            task_status=InstanceTasks.RESIZING)
        self.server = self.mock.CreateMock(Server)
        self.instance = models.BuiltInstanceTasks(
            context,
            self.db_info,
            self.server,
            datastore_status=InstanceServiceStatus.create(
                instance_id=self.db_info.id,
                status=rd_instance.ServiceStatuses.RUNNING))
        self.instance.server.flavor = {'id': OLD_FLAVOR_ID}
        self.guest = self.mock.CreateMock(guest.API)
        self.instance._guest = self.guest
        self.instance.refresh_compute_server_info = lambda: None
        self.instance._refresh_datastore_status = lambda: None
        self.mock.StubOutWithMock(self.instance, 'update_db')
        self.mock.StubOutWithMock(self.instance,
                                  'set_datastore_status_to_paused')
        self.poll_until_mocked = False
        self.action = None

    def tearDown(self):
        super(ResizeTestBase, self).tearDown()
        self.mock.UnsetStubs()
        self.db_info.delete()

    def _execute_action(self):
        self.instance.update_db(task_status=InstanceTasks.NONE)
        self.mock.ReplayAll()
        excs = (Exception)
        self.assertRaises(excs, self.action.execute)
        self.mock.VerifyAll()

    def _stop_db(self, reboot=True):
        self.guest.stop_db(do_not_start_on_reboot=reboot)
        self.instance.datastore_status.status = (
            rd_instance.ServiceStatuses.SHUTDOWN)

    def _server_changes_to(self, new_status, new_flavor_id):
        def change():
            self.server.status = new_status
            self.instance.server.flavor['id'] = new_flavor_id

        if not self.poll_until_mocked:
            self.mock.StubOutWithMock(utils, "poll_until")
            self.poll_until_mocked = True
        utils.poll_until(mox.IgnoreArg(), sleep_time=2, time_out=120)\
            .WithSideEffects(lambda ignore, sleep_time, time_out: change())

    def _nova_resizes_successfully(self):
        self.server.resize(NEW_FLAVOR_ID)
        self._server_changes_to("VERIFY_RESIZE", NEW_FLAVOR_ID)


@test(groups=[GROUP, GROUP + '.resize'])
class ResizeTests(ResizeTestBase):

    def setUp(self):
        super(ResizeTests, self).setUp()
        self._init()
        # By the time flavor objects pass over amqp to the
        # resize action they have been turned into dicts
        self.action = models.ResizeAction(self.instance,
                                          OLD_FLAVOR.__dict__,
                                          NEW_FLAVOR.__dict__)

    def _start_mysql(self):
        datastore = Mock(spec=DatastoreVersion)
        datastore.datastore_name = 'mysql'
        datastore.name = 'mysql-5.6'
        datastore.manager = 'mysql'
        config = template.SingleInstanceConfigTemplate(
            datastore, NEW_FLAVOR.__dict__, self.instance.id)
        self.instance.guest.start_db_with_conf_changes(config.render())

    def test_guest_wont_stop_mysql(self):
        self.guest.stop_db(do_not_start_on_reboot=True)\
            .AndRaise(RPCException("Could not stop MySQL!"))

    def test_nova_wont_resize(self):
        self._stop_db()
        self.server.resize(NEW_FLAVOR_ID).AndRaise(BadRequest)
        self.server.status = "ACTIVE"
        self.guest.restart()
        self._execute_action()

    def test_nova_resize_timeout(self):
        self._stop_db()
        self.server.resize(NEW_FLAVOR_ID)

        self.mock.StubOutWithMock(utils, 'poll_until')
        utils.poll_until(mox.IgnoreArg(), sleep_time=2, time_out=120)
        utils.poll_until(mox.IgnoreArg(), sleep_time=2, time_out=120)\
            .AndRaise(PollTimeOut)
        self._execute_action()

    def test_nova_doesnt_change_flavor(self):
        self._stop_db()
        self.server.resize(NEW_FLAVOR_ID)
        self._server_changes_to("VERIFY_RESIZE", OLD_FLAVOR_ID)
        utils.poll_until(mox.IgnoreArg(), sleep_time=2, time_out=120)
        self.instance.guest.reset_configuration(mox.IgnoreArg())
        self.instance.server.revert_resize()
        self._server_changes_to("ACTIVE", OLD_FLAVOR_ID)
        self.guest.restart()
        self._execute_action()

    def test_nova_resize_fails(self):
        self._stop_db()
        self.server.resize(NEW_FLAVOR_ID)
        self._server_changes_to("ERROR", OLD_FLAVOR_ID)
        self._execute_action()

    def test_nova_resizes_in_weird_state(self):
        self._stop_db()
        self.server.resize(NEW_FLAVOR_ID)
        self._server_changes_to("ACTIVE", NEW_FLAVOR_ID)
        self.guest.restart()
        self._execute_action()

    def test_guest_is_not_okay(self):
        self._stop_db()
        self._nova_resizes_successfully()
        utils.poll_until(mox.IgnoreArg(), sleep_time=2, time_out=120)
        self.instance.set_datastore_status_to_paused()
        self.instance.datastore_status.status = (
            rd_instance.ServiceStatuses.PAUSED)
        utils.poll_until(mox.IgnoreArg(), sleep_time=2, time_out=120)\
            .AndRaise(PollTimeOut)
        self.instance.guest.reset_configuration(mox.IgnoreArg())
        self.instance.server.revert_resize()
        self._server_changes_to("ACTIVE", OLD_FLAVOR_ID)
        self.guest.restart()
        self._execute_action()

    def test_mysql_is_not_okay(self):
        self._stop_db()
        self._nova_resizes_successfully()
        utils.poll_until(mox.IgnoreArg(), sleep_time=2, time_out=120)
        self.instance.set_datastore_status_to_paused()
        self.instance.datastore_status.status = (
            rd_instance.ServiceStatuses.SHUTDOWN)
        utils.poll_until(mox.IgnoreArg(), sleep_time=2, time_out=120)
        self._start_mysql()
        utils.poll_until(mox.IgnoreArg(), sleep_time=2,
                         time_out=120).AndRaise(PollTimeOut)
        self.instance.guest.reset_configuration(mox.IgnoreArg())
        self.instance.server.revert_resize()
        self._server_changes_to("ACTIVE", OLD_FLAVOR_ID)
        self.guest.restart()
        self._execute_action()

    def test_confirm_resize_fails(self):
        self._stop_db()
        self._nova_resizes_successfully()
        utils.poll_until(mox.IgnoreArg(), sleep_time=2, time_out=120)
        self.instance.set_datastore_status_to_paused()
        self.instance.datastore_status.status = (
            rd_instance.ServiceStatuses.RUNNING)
        utils.poll_until(mox.IgnoreArg(), sleep_time=2, time_out=120)
        self._start_mysql()
        utils.poll_until(mox.IgnoreArg(), sleep_time=2, time_out=120)
        self.server.status = "SHUTDOWN"
        self.instance.server.confirm_resize()
        self._execute_action()

    def test_revert_nova_fails(self):
        self._stop_db()
        self._nova_resizes_successfully()
        utils.poll_until(mox.IgnoreArg(), sleep_time=2, time_out=120)
        self.instance.set_datastore_status_to_paused()
        self.instance.datastore_status.status = (
            rd_instance.ServiceStatuses.PAUSED)
        utils.poll_until(mox.IgnoreArg(),
                         sleep_time=2,
                         time_out=120).AndRaise(PollTimeOut)
        self.instance.guest.reset_configuration(mox.IgnoreArg())
        self.instance.server.revert_resize()
        self._server_changes_to("ERROR", OLD_FLAVOR_ID)
        self._execute_action()


@test(groups=[GROUP, GROUP + '.migrate'])
class MigrateTests(ResizeTestBase):

    def setUp(self):
        super(MigrateTests, self).setUp()
        self._init()
        self.action = models.MigrateAction(self.instance)

    def _execute_action(self):
        self.instance.update_db(task_status=InstanceTasks.NONE)
        self.mock.ReplayAll()
        self.assertEqual(None, self.action.execute())
        self.mock.VerifyAll()

    def _start_mysql(self):
        self.guest.restart()

    def test_successful_migrate(self):
        self.mock.StubOutWithMock(self.instance.server, 'migrate')
        self._stop_db()
        self.server.migrate(force_host=None)
        self._server_changes_to("VERIFY_RESIZE", NEW_FLAVOR_ID)
        utils.poll_until(mox.IgnoreArg(), sleep_time=2, time_out=120)
        self.instance.set_datastore_status_to_paused()
        self.instance.datastore_status.status = (
            rd_instance.ServiceStatuses.RUNNING)
        utils.poll_until(mox.IgnoreArg(), sleep_time=2, time_out=120)
        self._start_mysql()
        utils.poll_until(mox.IgnoreArg(), sleep_time=2, time_out=120)
        self.instance.server.confirm_resize()
        self._execute_action()

########NEW FILE########
__FILENAME__ = limits
# Copyright 2013 OpenStack Foundation
# Copyright 2013 Rackspace Hosting
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

from nose.tools import assert_equal
from nose.tools import assert_true

from proboscis import before_class
from proboscis import test

from trove.openstack.common import timeutils
from trove.tests.util import create_dbaas_client
from troveclient.compat import exceptions
from datetime import datetime
from trove.tests.util.users import Users
from trove.tests.config import CONFIG

GROUP = "dbaas.api.limits"
DEFAULT_RATE = 200
DEFAULT_MAX_VOLUMES = 100
DEFAULT_MAX_INSTANCES = 55
DEFAULT_MAX_BACKUPS = 5


@test(groups=[GROUP])
class Limits(object):

    @before_class
    def setUp(self):

        users = [
            {
                "auth_user": "rate_limit",
                "auth_key": "password",
                "tenant": "4000",
                "requirements": {
                    "is_admin": False,
                    "services": ["trove"]
                }
            },
            {
                "auth_user": "rate_limit_exceeded",
                "auth_key": "password",
                "tenant": "4050",
                "requirements": {
                    "is_admin": False,
                    "services": ["trove"]
                }
            }]

        self._users = Users(users)

        rate_user = self._get_user('rate_limit')
        self.rd_client = create_dbaas_client(rate_user)

    def _get_user(self, name):
        return self._users.find_user_by_name(name)

    def __is_available(self, next_available):
        dt_next = timeutils.parse_isotime(next_available)
        dt_now = datetime.now()
        return dt_next.time() < dt_now.time()

    def _get_limits_as_dict(self, limits):
        d = {}
        for l in limits:
            d[l.verb] = l
        return d

    @test
    def test_limits_index(self):
        """test_limits_index"""

        limits = self.rd_client.limits.list()
        d = self._get_limits_as_dict(limits)

        # remove the abs_limits from the rate limits
        abs_limits = d.pop("ABSOLUTE", None)
        assert_equal(abs_limits.verb, "ABSOLUTE")
        assert_equal(int(abs_limits.max_instances), DEFAULT_MAX_INSTANCES)
        assert_equal(int(abs_limits.max_backups), DEFAULT_MAX_BACKUPS)
        if CONFIG.trove_volume_support:
            assert_equal(int(abs_limits.max_volumes), DEFAULT_MAX_VOLUMES)

        for k in d:
            assert_equal(d[k].verb, k)
            assert_equal(d[k].unit, "MINUTE")
            assert_true(int(d[k].remaining) <= DEFAULT_RATE)
            assert_true(d[k].nextAvailable is not None)

    @test
    def test_limits_get_remaining(self):
        """test_limits_get_remaining"""

        limits = ()
        for i in xrange(5):
            limits = self.rd_client.limits.list()

        d = self._get_limits_as_dict(limits)
        abs_limits = d["ABSOLUTE"]
        get = d["GET"]

        assert_equal(int(abs_limits.max_instances), DEFAULT_MAX_INSTANCES)
        assert_equal(int(abs_limits.max_backups), DEFAULT_MAX_BACKUPS)
        if CONFIG.trove_volume_support:
            assert_equal(int(abs_limits.max_volumes), DEFAULT_MAX_VOLUMES)
        assert_equal(get.verb, "GET")
        assert_equal(get.unit, "MINUTE")
        assert_true(int(get.remaining) <= DEFAULT_RATE - 5)
        assert_true(get.nextAvailable is not None)

    @test
    def test_limits_exception(self):
        """test_limits_exception"""

        # use a different user to avoid throttling tests run out of order
        rate_user_exceeded = self._get_user('rate_limit_exceeded')
        rd_client = create_dbaas_client(rate_user_exceeded)

        get = None
        encountered = False
        for i in xrange(DEFAULT_RATE + 50):
            try:
                limits = rd_client.limits.list()
                d = self._get_limits_as_dict(limits)
                get = d["GET"]
                abs_limits = d["ABSOLUTE"]

                assert_equal(get.verb, "GET")
                assert_equal(get.unit, "MINUTE")
                assert_equal(int(abs_limits.max_instances),
                             DEFAULT_MAX_INSTANCES)
                assert_equal(int(abs_limits.max_backups),
                             DEFAULT_MAX_BACKUPS)
                if CONFIG.trove_volume_support:
                    assert_equal(int(abs_limits.max_volumes),
                                 DEFAULT_MAX_VOLUMES)

            except exceptions.OverLimit:
                encountered = True

        assert_true(encountered)
        assert_true(int(get.remaining) <= 50)

########NEW FILE########
__FILENAME__ = accounts
#    Copyright 2011 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from troveclient.compat import exceptions

from nose.plugins.skip import SkipTest

from proboscis import after_class
from proboscis import before_class
from proboscis import test
from proboscis import asserts
from proboscis.decorators import time_out

from trove import tests
from trove.tests.api.instances import instance_info
from trove.tests.util import test_config
from trove.tests.util import create_dbaas_client
from trove.common.utils import poll_until
from trove.tests.config import CONFIG
from trove.tests.util.users import Requirements
from trove.tests.api.instances import existing_instance

GROUP = "dbaas.api.mgmt.accounts"


@test(groups=[tests.DBAAS_API, GROUP, tests.PRE_INSTANCES],
      depends_on_groups=["services.initialize"])
class AccountsBeforeInstanceCreation(object):

    @before_class
    def setUp(self):
        self.user = test_config.users.find_user(Requirements(is_admin=True))
        self.client = create_dbaas_client(self.user)

    @test
    def test_invalid_account(self):
        raise SkipTest("Don't have a good way to know if accounts are valid.")
        asserts.assert_raises(exceptions.NotFound, self.client.accounts.show,
                              "asd#4#@fasdf")

    @test
    def test_invalid_account_fails(self):
        account_info = self.client.accounts.show("badaccount")
        asserts.assert_not_equal(self.user.tenant_id, account_info.id)

    @test
    def test_account_zero_instances(self):
        account_info = self.client.accounts.show(self.user.tenant_id)
        expected_instances = 0 if not existing_instance() else 1
        asserts.assert_equal(expected_instances, len(account_info.instances))
        expected = self.user.tenant_id
        if expected is None:
            expected = "None"
        asserts.assert_equal(expected, account_info.id)

    @test
    def test_list_empty_accounts(self):
        accounts_info = self.client.accounts.index()
        expected_accounts = 0 if not existing_instance() else 1
        asserts.assert_equal(expected_accounts, len(accounts_info.accounts))


@test(groups=[tests.INSTANCES, GROUP], depends_on_groups=["dbaas.listing"])
class AccountsAfterInstanceCreation(object):

    @before_class
    def setUp(self):
        self.user = test_config.users.find_user(Requirements(is_admin=True))
        self.client = create_dbaas_client(self.user)

    @test
    def test_account_details_available(self):
        if test_config.auth_strategy == "fake":
            raise SkipTest("Skipping this as auth is faked anyway.")
        account_info = self.client.accounts.show(instance_info.user.tenant_id)
        # Now check the results.
        expected = instance_info.user.tenant_id
        if expected is None:
            expected = "None"
        print("account_id.id = '%s'" % account_info.id)
        print("expected = '%s'" % expected)
        asserts.assert_equal(account_info.id, expected)
        # Instances: Here we know we've only created one instance.
        asserts.assert_equal(1, len(account_info.instances))
        asserts.assert_is_not_none(account_info.instances[0]['host'])
        # We know the there's only 1 instance
        instance = account_info.instances[0]
        print("instances in account: %s" % instance)
        asserts.assert_equal(instance['id'], instance_info.id)
        asserts.assert_equal(instance['name'], instance_info.name)
        asserts.assert_equal(instance['status'], "ACTIVE")
        asserts.assert_is_not_none(instance['host'])

    @test
    def test_list_accounts(self):
        if test_config.auth_strategy == "fake":
            raise SkipTest("Skipping this as auth is faked anyway.")
        accounts_info = self.client.accounts.index()
        asserts.assert_equal(1, len(accounts_info.accounts))
        account = accounts_info.accounts[0]
        asserts.assert_equal(1, account['num_instances'])
        asserts.assert_equal(instance_info.user.tenant_id, account['id'])


@test(groups=[tests.POST_INSTANCES, GROUP],
      depends_on_groups=["dbaas.guest.shutdown"])
class AccountsAfterInstanceDeletion(object):

    @before_class
    def setUp(self):
        self.user = test_config.users.find_user(Requirements(is_admin=True))
        self.client = create_dbaas_client(self.user)

    @test
    def test_no_details_empty_account(self):
        account_info = self.client.accounts.show(instance_info.user.tenant_id)
        asserts.assert_equal(0, len(account_info.instances))


@test(groups=["fake.dbaas.api.mgmt.allaccounts"],
      depends_on_groups=["services.initialize"])
class AllAccounts(object):
    max = 5

    def _delete_instances_for_users(self):
        for user in self.users:
            user_client = create_dbaas_client(user)
            while True:
                deleted_count = 0
                user_instances = user_client.instances.list()
                for instance in user_instances:
                    try:
                        instance.delete()
                    except exceptions.NotFound:
                        deleted_count += 1
                    except Exception:
                        print("Failed to delete instance")
                if deleted_count == len(user_instances):
                    break

    def _create_instances_for_users(self):
        for user in self.users:
            user_client = create_dbaas_client(user)
            for index in range(self.max):
                name = "instance-%s-%03d" % (user.auth_user, index)
                user_client.instances.create(name, 1, {'size': 1}, [], [])

    @before_class
    def setUp(self):
        admin_req = Requirements(is_admin=True)
        self.admin_user = test_config.users.find_user(admin_req)
        self.admin_client = create_dbaas_client(self.admin_user)
        user_req = Requirements(is_admin=False)
        self.users = test_config.users.find_all_users_who_satisfy(user_req)
        self.user_tenant_ids = [user.tenant_id for user in self.users]
        self._create_instances_for_users()

    @test
    def test_list_accounts_with_multiple_users(self):
        accounts_info = self.admin_client.accounts.index()
        for account in accounts_info.accounts:
            asserts.assert_true(account['id'] in self.user_tenant_ids)
            asserts.assert_equal(self.max, account['num_instances'])

    @after_class(always_run=True)
    @time_out(60)
    def tear_down(self):
        self._delete_instances_for_users()


@test(groups=["fake.%s.broken" % GROUP],
      depends_on_groups=["services.initialize"],
      runs_after_groups=["dbaas.guest.shutdown"])
class AccountWithBrokenInstance(object):

    @before_class
    def setUpACCR(self):
        from trove.taskmanager.models import CONF
        self.old_dns_support = CONF.trove_dns_support
        CONF.trove_dns_support = False

        self.user = test_config.users.find_user(Requirements(is_admin=True))
        self.client = create_dbaas_client(self.user)
        self.name = 'test_SERVER_ERROR'
        # Create an instance with a broken compute instance.
        volume = None
        if CONFIG.trove_volume_support:
            volume = {'size': 1}
        self.response = self.client.instances.create(
            self.name,
            instance_info.dbaas_flavor_href,
            volume,
            [])
        poll_until(lambda: self.client.instances.get(self.response.id),
                   lambda instance: instance.status == 'ERROR',
                   time_out=10)
        self.instance = self.client.instances.get(self.response.id)
        print("Status: %s" % self.instance.status)
        msg = "Instance did not drop to error after server prov failure."
        asserts.assert_equal(self.instance.status, "ERROR", msg)

    @test
    def no_compute_instance_no_problem(self):
        '''Get account by ID shows even instances lacking computes'''
        if test_config.auth_strategy == "fake":
            raise SkipTest("Skipping this as auth is faked anyway.")
        account_info = self.client.accounts.show(self.user.tenant_id)
        # All we care about is that accounts.show doesn't 500 on us
        # for having a broken instance in the roster.
        asserts.assert_equal(len(account_info.instances), 1)
        instance = account_info.instances[0]
        asserts.assert_true(isinstance(instance['id'], basestring))
        asserts.assert_equal(len(instance['id']), 36)
        asserts.assert_equal(instance['name'], self.name)
        asserts.assert_equal(instance['status'], "ERROR")
        assert_is_none(instance['host'])

    @after_class
    def tear_down(self):
        self.client.instances.delete(self.response.id)

    @after_class
    def restore_dns(self):
        from trove.taskmanager.models import CONF
        CONF.trove_dns_support = self.old_dns_support

########NEW FILE########
__FILENAME__ = admin_required
#    Copyright 2011 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from troveclient.compat.exceptions import Unauthorized

from proboscis import before_class
from proboscis import test
from proboscis.asserts import assert_raises

from trove import tests
from trove.tests.util import create_dbaas_client
from trove.tests.util import test_config
from trove.tests.util.users import Requirements

GROUP = "dbaas.api.mgmt.admin"


@test(groups=[tests.DBAAS_API, GROUP, tests.PRE_INSTANCES],
      depends_on_groups=["services.initialize"])
class TestAdminRequired(object):
    """
    These tests verify that admin privileges are checked
    when calling management level functions.
    """

    @before_class
    def setUp(self):
        """Create the user and client for use in the subsequent tests."""
        self.user = test_config.users.find_user(Requirements(is_admin=False))
        self.dbaas = create_dbaas_client(self.user)

    @test
    def test_accounts_show(self):
        """A regular user may not view the details of any account. """
        assert_raises(Unauthorized, self.dbaas.accounts.show, 0)

    @test
    def test_hosts_index(self):
        """A regular user may not view the list of hosts. """
        assert_raises(Unauthorized, self.dbaas.hosts.index)

    @test
    def test_hosts_get(self):
        """A regular user may not view the details of any host. """
        assert_raises(Unauthorized, self.dbaas.hosts.get, 0)

    @test
    def test_mgmt_show(self):
        """
        A regular user may not view the management details
        of any instance.
        """
        assert_raises(Unauthorized, self.dbaas.management.show, 0)

    @test
    def test_mgmt_root_history(self):
        """
        A regular user may not view the root access history of
        any instance.
        """
        assert_raises(Unauthorized,
                      self.dbaas.management.root_enabled_history, 0)

    @test
    def test_mgmt_instance_reboot(self):
        """A regular user may not perform an instance reboot. """
        assert_raises(Unauthorized, self.dbaas.management.reboot, 0)

    @test
    def test_mgmt_instance_reset_task_status(self):
        """A regular user may not perform an instance task status reset. """
        assert_raises(Unauthorized, self.dbaas.management.reset_task_status, 0)

    @test
    def test_storage_index(self):
        """A regular user may not view the list of storage available. """
        assert_raises(Unauthorized, self.dbaas.storage.index)

    @test
    def test_diagnostics_get(self):
        """A regular user may not view the diagnostics. """
        assert_raises(Unauthorized, self.dbaas.diagnostics.get, 0)

    @test
    def test_hwinfo_get(self):
        """A regular user may not view the hardware info. """
        assert_raises(Unauthorized, self.dbaas.hwinfo.get, 0)

########NEW FILE########
__FILENAME__ = hosts
#    Copyright 2013 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from troveclient.compat import exceptions

from proboscis import before_class
from proboscis import test
from proboscis.asserts import assert_not_equal
from proboscis.asserts import assert_raises
from proboscis.asserts import assert_true
from proboscis.check import Check

from trove.tests.api.instances import create_new_instance
from trove.tests.api.instances import CreateInstance
from trove.tests.config import CONFIG
from trove.tests.util import create_dbaas_client
from trove.tests.util.users import Requirements
from trove.tests import DBAAS_API
from trove.tests import PRE_INSTANCES
from trove.tests import INSTANCES

GROUP = "dbaas.api.mgmt.hosts"


def percent_boundary(used_ram, total_ram):
    """Return a upper and lower bound for percent ram used."""
    calc = int((1.0 * used_ram / total_ram) * 100)
    # return calculated percent +/- 2 to account for rounding errors
    lower_boundary = calc - 2
    upper_boundary = calc + 2
    return lower_boundary, upper_boundary


@test(groups=[DBAAS_API, GROUP, PRE_INSTANCES],
      depends_on_groups=["services.initialize"],
      enabled=create_new_instance())
class HostsBeforeInstanceCreation(object):

    @before_class
    def setUp(self):
        self.user = CONFIG.users.find_user(Requirements(is_admin=True))
        self.client = create_dbaas_client(self.user)
        self.host = None

    @test
    def test_empty_index_host_list(self):
        host_index_result = self.client.hosts.index()
        assert_not_equal(host_index_result, None,
                         "list hosts call should not be empty: %s" %
                         str(host_index_result))
        assert_true(len(host_index_result) > 0,
                    "list hosts length should be greater than zero: %r" %
                    host_index_result)

        self.host = host_index_result[0]
        assert_true(self.host is not None, "Expected to find a host.")

    @test(depends_on=[test_empty_index_host_list])
    def test_empty_index_host_list_single(self):
        self.host.name = self.host.name.replace(".", "\.")
        result = self.client.hosts.get(self.host)
        assert_not_equal(result, None,
                         "Get host should not be empty for: %s" % self.host)
        with Check() as check:
            used_ram = int(result.usedRAM)
            total_ram = int(result.totalRAM)
            percent_used = int(result.percentUsed)
            lower, upper = percent_boundary(used_ram, total_ram)
            check.true(percent_used > lower,
                       "percentUsed %r is below the lower boundary %r"
                       % (percent_used, lower))
            check.true(percent_used < upper,
                       "percentUsed %r is above the upper boundary %r"
                       % (percent_used, upper))
            check.true(used_ram < total_ram,
                       "usedRAM %r should be less than totalRAM %r"
                       % (used_ram, total_ram))
            check.true(percent_used < 100,
                       "percentUsed should be less than 100 but was %r"
                       % percent_used)
            check.true(total_ram > 0,
                       "totalRAM should be greater than 0 but was %r"
                       % total_ram)
            check.true(used_ram < total_ram,
                       "usedRAM %r should be less than totalRAM %r"
                       % (used_ram, total_ram))


@test(groups=[INSTANCES, GROUP],
      depends_on=[CreateInstance],
      enabled=create_new_instance())
class HostsMgmtCommands(object):

    @before_class
    def setUp(self):
        self.user = CONFIG.users.find_user(Requirements(is_admin=True))
        self.client = create_dbaas_client(self.user)
        self.host = None

    @test
    def test_index_host_list(self):
        result = self.client.hosts.index()
        assert_not_equal(len(result), 0,
                         "list hosts should not be empty: %s" % str(result))
        hosts = []
        # Find a host with a instanceCount > 0
        for host in result:
            msg = 'Host: %s, Count: %s' % (host.name, host.instanceCount)
            hosts.append(msg)
            if int(host.instanceCount) > 0:
                self.host = host
                break

        msg = "Unable to find a host with instances: %r" % hosts
        assert_not_equal(self.host, None, msg)

    @test(depends_on=[test_index_host_list])
    def test_index_host_list_single(self):
        self.host.name = self.host.name.replace(".", "\.")
        result = self.client.hosts.get(self.host)
        assert_not_equal(result, None,
                         "list hosts should not be empty: %s" % str(result))
        assert_true(len(result.instances) > 0,
                    "instance list on the host should not be empty: %r"
                    % result.instances)
        with Check() as check:
            used_ram = int(result.usedRAM)
            total_ram = int(result.totalRAM)
            percent_used = int(result.percentUsed)
            lower, upper = percent_boundary(used_ram, total_ram)
            check.true(percent_used > lower,
                       "percentUsed %r is below the lower boundary %r"
                       % (percent_used, lower))
            check.true(percent_used < upper,
                       "percentUsed %r is above the upper boundary %r"
                       % (percent_used, upper))
            check.true(used_ram < total_ram,
                       "usedRAM %r should be less than totalRAM %r"
                       % (used_ram, total_ram))
            check.true(percent_used < 100,
                       "percentUsed should be less than 100 but was %r"
                       % percent_used)
            check.true(total_ram > 0,
                       "totalRAM should be greater than 0 but was %r"
                       % total_ram)
            check.true(used_ram < total_ram,
                       "usedRAM %r should be less than totalRAM %r"
                       % (used_ram, total_ram))

            # Check all active instances and validate all the fields exist
            active_instance = None
            for instance in result.instances:
                print("instance: %s" % instance)
                if instance['status'] != 'ACTIVE':
                    continue
                active_instance = instance
                check.is_not_none(instance['id'])
                check.is_not_none(instance['name'])
                check.is_not_none(instance['status'])
                check.is_not_none(instance['server_id'])
                check.is_not_none(instance['tenant_id'])
            check.true(active_instance is not None, "No active instances")

    def _get_ids(self):
        """Get all the ids of instances that are ACTIVE."""
        ids = []
        results = self.client.hosts.index()
        for host in results:
            result = self.client.hosts.get(host)
            for instance in result.instances:
                if instance['status'] == 'ACTIVE':
                    ids.append(instance['id'])
        return ids

    @test
    def test_update_hosts(self):
        ids = self._get_ids()
        assert_not_equal(ids, [], "No active instances found")
        before_versions = {}
        for _id in ids:
            diagnostics = self.client.diagnostics.get(_id)
            before_versions[_id] = diagnostics.version

        hosts = self.client.hosts.index()
        for host in hosts:
            self.client.hosts.update_all(host.name)

        after_versions = {}
        for _id in ids:
            diagnostics = self.client.diagnostics.get(_id)
            after_versions[_id] = diagnostics.version

        assert_not_equal(before_versions, {},
                         "No versions found before update")
        assert_not_equal(after_versions, {},
                         "No versions found after update")
        if CONFIG.fake_mode:
            for _id in after_versions:
                assert_not_equal(before_versions[_id], after_versions[_id])

    @test
    def test_host_not_found(self):
        hostname = "host@$%3dne"
        assert_raises(exceptions.NotFound, self.client.hosts.get, hostname)

########NEW FILE########
__FILENAME__ = instances
#    Copyright 2011 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from troveclient.compat import exceptions

from proboscis import before_class
from proboscis import test
from proboscis.asserts import assert_equal
from proboscis.asserts import assert_raises
from proboscis.check import Check
from proboscis import SkipTest

from trove.tests.config import CONFIG
from trove.tests.util import create_client
from trove.tests.util import create_dbaas_client
from trove.tests.util.users import Requirements
from trove.tests.util.check import CollectionCheck
from trove.tests.util.check import TypeCheck

from trove.tests.api.instances import CreateInstance
from trove.tests.api.instances import instance_info
from trove.tests.api.instances import GROUP_START
from trove.tests.api.instances import GROUP_TEST
from trove.common.utils import poll_until

GROUP = "dbaas.api.mgmt.instances"


@test(groups=[GROUP])
def mgmt_index_requires_admin_account():
    """Verify that an admin context is required to call this function. """
    client = create_client(is_admin=False)
    assert_raises(exceptions.Unauthorized, client.management.index)


# These functions check some dictionaries in the returned response.
def flavor_check(flavor):
    with CollectionCheck("flavor", flavor) as check:
        check.has_element("id", basestring)
        check.has_element("links", list)


def datastore_check(datastore):
    with CollectionCheck("datastore", datastore) as check:
        check.has_element("type", basestring)
        check.has_element("version", basestring)


def guest_status_check(guest_status):
    with CollectionCheck("guest_status", guest_status) as check:
        check.has_element("state_description", basestring)


def volume_check(volume):
    with CollectionCheck("volume", volume) as check:
        check.has_element("id", basestring)
        check.has_element("size", int)
        check.has_element("used", float)
        check.has_element("total", float)


@test(depends_on_groups=[GROUP_START], groups=[GROUP, GROUP_TEST])
def mgmt_instance_get():
    """Tests the mgmt instances index method. """
    reqs = Requirements(is_admin=True)
    user = CONFIG.users.find_user(reqs)
    client = create_dbaas_client(user)
    mgmt = client.management
    # Grab the info.id created by the main instance test which is stored in
    # a global.
    id = instance_info.id
    api_instance = mgmt.show(id)

    # Print out all fields for extra info if the test fails.
    for name in dir(api_instance):
        print(str(name) + "=" + str(getattr(api_instance, name)))
    with TypeCheck("instance", api_instance) as instance:
        instance.has_field('created', basestring)
        instance.has_field('deleted', bool)
        # If the instance hasn't been deleted, this should be false... but
        # lets avoid creating more ordering work.
        instance.has_field('deleted_at', (basestring, None))
        instance.has_field('flavor', dict, flavor_check)
        instance.has_field('datastore', dict, datastore_check)
        instance.has_field('guest_status', dict, guest_status_check)
        instance.has_field('id', basestring)
        instance.has_field('links', list)
        instance.has_field('name', basestring)
        #instance.has_field('server_status', basestring)
        instance.has_field('status', basestring)
        instance.has_field('tenant_id', basestring)
        instance.has_field('updated', basestring)
        # Can be None if no volume is given on this instance.
        if CONFIG.trove_volume_support:
            instance.has_field('volume', dict, volume_check)
        else:
            instance.has_field('volume', None)
        #TODO(tim-simpson): Validate additional fields, assert
        # no extra fields exist.
    if api_instance.server is not None:
        print("the real content of server: %s" % dir(api_instance.server))
        print("the type of server: %s" % type(api_instance.server))
        print("the real content of api_instance: %s" % dir(api_instance))
        print("the type of api_instance: %s" % type(api_instance))
        print(hasattr(api_instance, "server"))

        with CollectionCheck("server", api_instance.server) as server:
            server.has_element("addresses", dict)
            server.has_element("deleted", bool)
            server.has_element("deleted_at", (basestring, None))
            server.has_element("host", basestring)
            server.has_element("id", basestring)
            server.has_element("local_id", int)
            server.has_element("name", basestring)
            server.has_element("status", basestring)
            server.has_element("tenant_id", basestring)

    if (CONFIG.trove_volume_support and
            CONFIG.trove_main_instance_has_volume):
        with CollectionCheck("volume", api_instance.volume) as volume:
            volume.has_element("attachments", list)
            volume.has_element("availability_zone", basestring)
            volume.has_element("created_at", (basestring, None))
            volume.has_element("id", basestring)
            volume.has_element("size", int)
            volume.has_element("status", basestring)


@test(groups=["fake." + GROUP])
class WhenMgmtInstanceGetIsCalledButServerIsNotReady(object):

    @before_class
    def set_up(self):
        """Create client for mgmt instance test (2)."""
        if not CONFIG.fake_mode:
            raise SkipTest("This test only works in fake mode.")
        self.client = create_client(is_admin=True)
        self.mgmt = self.client.management
        # Fake nova will fail a server ending with 'test_SERVER_ERROR'."
        # Fake volume will fail if the size is 13.
        # TODO(tim.simpson): This would be a lot nicer looking if we used a
        #                    traditional mock framework.
        body = None
        if CONFIG.trove_volume_support:
            body = {'size': 13}
        response = self.client.instances.create(
            'test_SERVER_ERROR',
            instance_info.dbaas_flavor_href,
            body,
            [])
        poll_until(lambda: self.client.instances.get(response.id),
                   lambda instance: instance.status == 'ERROR',
                   time_out=10)
        self.id = response.id

    @test
    def mgmt_instance_get(self):
        """Tests the mgmt get call works when the Nova server isn't ready."""
        api_instance = self.mgmt.show(self.id)
        # Print out all fields for extra info if the test fails.
        for name in dir(api_instance):
            print(str(name) + "=" + str(getattr(api_instance, name)))
        # Print out all fields for extra info if the test fails.
        for name in dir(api_instance):
            print(str(name) + "=" + str(getattr(api_instance, name)))
        with TypeCheck("instance", api_instance) as instance:
            instance.has_field('created', basestring)
            instance.has_field('deleted', bool)
            # If the instance hasn't been deleted, this should be false... but
            # lets avoid creating more ordering work.
            instance.has_field('deleted_at', (basestring, None))
            instance.has_field('flavor', dict, flavor_check)
            instance.has_field('datastore', dict, datastore_check)
            instance.has_field('guest_status', dict, guest_status_check)
            instance.has_field('id', basestring)
            instance.has_field('links', list)
            instance.has_field('name', basestring)
            #instance.has_field('server_status', basestring)
            instance.has_field('status', basestring)
            instance.has_field('tenant_id', basestring)
            instance.has_field('updated', basestring)
            # Can be None if no volume is given on this instance.
            instance.has_field('server', None)
            instance.has_field('volume', None)
            #TODO(tim-simpson): Validate additional fields,
            # assert no extra fields exist.


@test(depends_on_classes=[CreateInstance], groups=[GROUP])
class MgmtInstancesIndex(object):
    """Tests the mgmt instances index method. """

    @before_class
    def setUp(self):
        """Create client for mgmt instance test."""
        reqs = Requirements(is_admin=True)
        self.user = CONFIG.users.find_user(reqs)
        self.client = create_dbaas_client(self.user)

    @test
    def test_mgmt_instance_index_fields_present(self):
        """
        Verify that all the expected fields are returned by the index method.
        """
        expected_fields = [
            'created',
            'deleted',
            'deleted_at',
            'flavor',
            'datastore',
            'id',
            'links',
            'name',
            'server',
            'status',
            'task_description',
            'tenant_id',
            'updated',
        ]
        if CONFIG.trove_volume_support:
            expected_fields.append('volume')

        index = self.client.management.index()

        if not hasattr(index, "deleted"):
            raise SkipTest("instance index must have a "
                           "deleted label for this test")

        for instance in index:
            with Check() as check:
                for field in expected_fields:
                    check.true(hasattr(instance, field),
                               "Index lacks field %s" % field)

    @test
    def test_mgmt_instance_index_check_filter(self):
        """
        Make sure that the deleted= filter works as expected, and no instances
        are excluded.
        """
        if not hasattr(self.client.management.index, 'deleted'):
            raise SkipTest("instance index must have a deleted "
                           "label for this test")
        instance_counts = []
        for deleted_filter in (True, False):
            filtered_index = self.client.management.index(
                deleted=deleted_filter)
            instance_counts.append(len(filtered_index))
        for instance in filtered_index:
                # Every instance listed here should have the proper value
                # for 'deleted'.
                assert_equal(deleted_filter, instance.deleted)
        full_index = self.client.management.index()
        # There should be no instances that are neither deleted or not-deleted.
        assert_equal(len(full_index), sum(instance_counts))

########NEW FILE########
__FILENAME__ = instances_actions
#    Copyright 2013 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import mox
import trove.common.instance as tr_instance
from trove.backup import models as backup_models
from trove.common.context import TroveContext
from trove.instance.tasks import InstanceTasks
from trove.instance import models as imodels
from trove.instance.models import DBInstance
from trove.extensions.mgmt.instances.models import MgmtInstance
from trove.tests.util import create_dbaas_client
from trove.tests.util import test_config
from trove.tests.util.users import Requirements

from novaclient.v1_1.servers import Server

from proboscis import test
from proboscis import before_class
from proboscis import after_class
from proboscis.asserts import assert_equal
from proboscis.asserts import assert_raises
from trove.common import exception
from trove.extensions.mgmt.instances.service import MgmtInstanceController

GROUP = "dbaas.api.mgmt.action.reset-task-status"


class MgmtInstanceBase(object):

    def setUp(self):
        self.mock = mox.Mox()
        self._create_instance()
        self.controller = MgmtInstanceController()

    def tearDown(self):
        self.db_info.delete()

    def _create_instance(self):
        self.context = TroveContext(is_admin=True)
        self.tenant_id = 999
        self.db_info = DBInstance.create(
            id="inst-id-1",
            name="instance",
            flavor_id=1,
            datastore_version_id=test_config.dbaas_datastore_version_id,
            tenant_id=self.tenant_id,
            volume_size=None,
            task_status=InstanceTasks.NONE)
        self.server = self.mock.CreateMock(Server)
        self.instance = imodels.Instance(
            self.context,
            self.db_info,
            self.server,
            datastore_status=imodels.InstanceServiceStatus(
                tr_instance.ServiceStatuses.RUNNING))

    def _make_request(self, path='/', context=None, **kwargs):
        from webob import Request
        path = '/'
        print("path: %s" % path)
        return Request.blank(path=path, environ={'trove.context': context},
                             **kwargs)

    def _reload_db_info(self):
        self.db_info = DBInstance.find_by(id=self.db_info.id, deleted=False)


@test(groups=[GROUP])
class RestartTaskStatusTests(MgmtInstanceBase):
    @before_class
    def setUp(self):
        super(RestartTaskStatusTests, self).setUp()
        self.backups_to_clear = []

    @after_class
    def tearDown(self):
        super(RestartTaskStatusTests, self).tearDown()

    def _change_task_status_to(self, new_task_status):
        self.db_info.task_status = new_task_status
        self.db_info.save()

    def _make_request(self, path='/', context=None, **kwargs):
        req = super(RestartTaskStatusTests, self)._make_request(path, context,
                                                                **kwargs)
        req.method = 'POST'
        body = {'reset-task-status': {}}
        return req, body

    def reset_task_status(self):
        self.mock.StubOutWithMock(MgmtInstance, 'load')
        MgmtInstance.load(context=self.context,
                          id=self.db_info.id).AndReturn(self.instance)
        self.mock.ReplayAll()

        req, body = self._make_request(context=self.context)
        self.controller = MgmtInstanceController()
        resp = self.controller.action(req, body, self.tenant_id,
                                      self.db_info.id)

        self.mock.UnsetStubs()
        self.mock.VerifyAll()
        return resp

    @test
    def mgmt_restart_task_requires_admin_account(self):
        context = TroveContext(is_admin=False)
        req, body = self._make_request(context=context)
        self.controller = MgmtInstanceController()
        assert_raises(exception.Forbidden, self.controller.action,
                      req, body, self.tenant_id, self.db_info.id)

    @test
    def mgmt_restart_task_returns_json(self):
        resp = self.reset_task_status()
        out = resp.data("application/json")
        assert_equal(out, None)

    @test
    def mgmt_restart_task_changes_status_to_none(self):
        self._change_task_status_to(InstanceTasks.BUILDING)
        self.reset_task_status()
        self._reload_db_info()
        assert_equal(self.db_info.task_status, InstanceTasks.NONE)

    @test
    def mgmt_reset_task_status_clears_backups(self):
        self.reset_task_status()
        self._reload_db_info()
        assert_equal(self.db_info.task_status, InstanceTasks.NONE)

        user = test_config.users.find_user(Requirements(is_admin=False))
        dbaas = create_dbaas_client(user)
        result = dbaas.instances.backups(self.db_info.id)
        assert_equal(0, len(result))

        # Create some backups.
        backup_models.DBBackup.create(
            name="forever_new",
            description="forever new",
            tenant_id=self.tenant_id,
            state=backup_models.BackupState.NEW,
            instance_id=self.db_info.id,
            deleted=False)

        backup_models.DBBackup.create(
            name="forever_build",
            description="forever build",
            tenant_id=self.tenant_id,
            state=backup_models.BackupState.BUILDING,
            instance_id=self.db_info.id,
            deleted=False)

        backup_models.DBBackup.create(
            name="forever_completed",
            description="forever completed",
            tenant_id=self.tenant_id,
            state=backup_models.BackupState.COMPLETED,
            instance_id=self.db_info.id,
            deleted=False)

        # List the backups for this instance. There ought to be three!
        result = dbaas.instances.backups(self.db_info.id)
        assert_equal(3, len(result))
        self.backups_to_clear = result

        # Reset the task status.
        self.reset_task_status()
        self._reload_db_info()
        result = dbaas.instances.backups(self.db_info.id)
        assert_equal(3, len(result))
        for backup in result:
            if backup.name == 'forever_completed':
                assert_equal(backup.status,
                             backup_models.BackupState.COMPLETED)
            else:
                assert_equal(backup.status, backup_models.BackupState.FAILED)

    @test(runs_after=[mgmt_reset_task_status_clears_backups])
    def clear_test_backups(self):
        for backup in self.backups_to_clear:
            found_backup = backup_models.DBBackup.find_by(id=backup.id)
            found_backup.delete()
        user = test_config.users.find_user(Requirements(is_admin=False))
        dbaas = create_dbaas_client(user)
        result = dbaas.instances.backups(self.db_info.id)
        assert_equal(0, len(result))

########NEW FILE########
__FILENAME__ = malformed_json
# Copyright 2013 OpenStack Foundation
# Copyright 2013 Rackspace Hosting
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

from collections import deque
from proboscis import test
from proboscis import asserts
from proboscis import after_class
from proboscis import before_class
from trove.tests.config import CONFIG
from trove.tests.api.instances import instance_info
from trove.tests.api.instances import VOLUME_SUPPORT

from trove.tests.util.users import Requirements
from trove.tests.util import assert_contains
from trove.tests.util import create_dbaas_client
from trove.common.utils import poll_until


@test(groups=["dbaas.api.mgmt.malformed_json"])
class MalformedJson(object):
    @before_class
    def setUp(self):
        self.reqs = Requirements(is_admin=False)
        self.user = CONFIG.users.find_user(self.reqs)
        self.dbaas = create_dbaas_client(self.user)
        volume = None
        if VOLUME_SUPPORT:
            volume = {"size": 1}
        self.instance = self.dbaas.instances.create(
            name="qe_instance",
            flavor_id=instance_info.dbaas_flavor_href,
            volume=volume,
            databases=[{"name": "firstdb", "character_set": "latin2",
                        "collate": "latin2_general_ci"}])

    @after_class
    def tearDown(self):
        self.dbaas.instances.delete(self.instance)

    @test
    def test_bad_instance_data(self):
        databases = "foo"
        users = "bar"
        try:
            self.dbaas.instances.create("bad_instance", 3, 3,
                                        databases=databases, users=users)
        except Exception as e:
            resp, body = self.dbaas.client.last_response
            httpCode = resp.status
            asserts.assert_equal(httpCode, 400,
                                 "Create instance failed with code %s,"
                                 " exception %s" % (httpCode, e))
            databases = "u'foo'"
            users = "u'bar'"
            assert_contains(
                e.message,
                ["Validation error:",
                 "instance['databases'] %s is not of type 'array'" % databases,
                 "instance['users'] %s is not of type 'array'" % users,
                 "instance['volume'] 3 is not of type 'object'"])

    @test
    def test_bad_database_data(self):
        _bad_db_data = "{foo}"
        try:
            self.dbaas.databases.create(self.instance.id, _bad_db_data)
        except Exception as e:
            resp, body = self.dbaas.client.last_response
            httpCode = resp.status
            asserts.assert_equal(httpCode, 400,
                                 "Create database failed with code %s, "
                                 "exception %s" % (httpCode, e))
            _bad_db_data = "u'{foo}'"
            asserts.assert_equal(e.message,
                                 "Validation error: "
                                 "databases %s is not of type 'array'" %
                                 _bad_db_data)

    @test
    def test_bad_user_data(self):

        def format_path(values):
            values = list(values)
            msg = "%s%s" % (values[0],
                            ''.join(['[%r]' % i for i in values[1:]]))
            return msg

        _user = []
        _user_name = "F343jasdf"
        _user.append({"name12": _user_name,
                      "password12": "password"})
        try:
            self.dbaas.users.create(self.instance.id, _user)
        except Exception as e:
            resp, body = self.dbaas.client.last_response
            httpCode = resp.status
            asserts.assert_equal(httpCode, 400,
                                 "Create user failed with code %s, "
                                 "exception %s" % (httpCode, e))
            err_1 = format_path(deque(('users', 0)))
            assert_contains(
                e.message,
                ["Validation error:",
                 "%(err_1)s 'name' is a required property" % {'err_1': err_1},
                 "%(err_1)s 'password' is a required property"
                 % {'err_1': err_1}])

    @test
    def test_bad_resize_instance_data(self):
        def _check_instance_status():
            inst = self.dbaas.instances.get(self.instance)
            if inst.status == "ACTIVE":
                return True
            else:
                return False

        poll_until(_check_instance_status)
        try:
            self.dbaas.instances.resize_instance(self.instance.id, "bad data")
        except Exception as e:
            resp, body = self.dbaas.client.last_response
            httpCode = resp.status
            asserts.assert_equal(httpCode, 400,
                                 "Resize instance failed with code %s, "
                                 "exception %s" % (httpCode, e))

    @test
    def test_bad_resize_vol_data(self):
        def _check_instance_status():
            inst = self.dbaas.instances.get(self.instance)
            if inst.status == "ACTIVE":
                return True
            else:
                return False

        poll_until(_check_instance_status)
        data = "bad data"
        try:
            self.dbaas.instances.resize_volume(self.instance.id, data)
        except Exception as e:
            resp, body = self.dbaas.client.last_response
            httpCode = resp.status
            asserts.assert_equal(httpCode, 400,
                                 "Resize instance failed with code %s, "
                                 "exception %s" % (httpCode, e))
            data = "u'bad data'"
            assert_contains(
                e.message,
                ["Validation error:",
                 "resize['volume']['size'] %s is not valid under "
                 "any of the given schemas" % data,
                 "%s is not of type 'integer'" % data,
                 "%s does not match '[0-9]+'" % data])

    @test
    def test_bad_change_user_password(self):
        password = ""
        users = [{"name": password}]

        def _check_instance_status():
            inst = self.dbaas.instances.get(self.instance)
            if inst.status == "ACTIVE":
                return True
            else:
                return False

        poll_until(_check_instance_status)
        try:
            self.dbaas.users.change_passwords(self.instance, users)
        except Exception as e:
            resp, body = self.dbaas.client.last_response
            httpCode = resp.status
            asserts.assert_equal(httpCode, 400,
                                 "Change usr/passwd failed with code %s, "
                                 "exception %s" % (httpCode, e))
            password = "u''"
            assert_contains(
                e.message,
                ["Validation error: users[0] 'password' "
                 "is a required property",
                 "users[0]['name'] %s is too short" % password,
                 "users[0]['name'] %s does not match "
                 "'^.*[0-9a-zA-Z]+.*$'" % password])

    @test
    def test_bad_grant_user_access(self):
        dbs = []

        def _check_instance_status():
            inst = self.dbaas.instances.get(self.instance)
            if inst.status == "ACTIVE":
                return True
            else:
                return False

        poll_until(_check_instance_status)
        try:
            self.dbaas.users.grant(self.instance, self.user, dbs)
        except Exception as e:
            resp, body = self.dbaas.client.last_response
            httpCode = resp.status
            asserts.assert_equal(httpCode, 400,
                                 "Grant user access failed with code %s, "
                                 "exception %s" % (httpCode, e))

    @test
    def test_bad_revoke_user_access(self):
        db = ""

        def _check_instance_status():
            inst = self.dbaas.instances.get(self.instance)
            if inst.status == "ACTIVE":
                return True
            else:
                return False

        poll_until(_check_instance_status)
        try:
            self.dbaas.users.revoke(self.instance, self.user, db)
        except Exception as e:
            resp, body = self.dbaas.client.last_response
            httpCode = resp.status
            asserts.assert_equal(httpCode, 404,
                                 "Revoke user access failed w/code %s, "
                                 "exception %s" % (httpCode, e))
            asserts.assert_equal(e.message, "The resource could not be found.")

    @test
    def test_bad_body_flavorid_create_instance(self):

        flavorId = ["?"]
        try:
            self.dbaas.instances.create("test_instance",
                                        flavorId,
                                        2)
        except Exception as e:
            resp, body = self.dbaas.client.last_response
            httpCode = resp.status
            asserts.assert_equal(httpCode, 400,
                                 "Create instance failed with code %s, "
                                 "exception %s" % (httpCode, e))
            flavorId = [u'?']
            assert_contains(
                e.message,
                ["Validation error:",
                 "instance['flavorRef'] %s is not valid "
                 "under any of the given schemas" % flavorId,
                 "%s is not of type 'string'" % flavorId,
                 "%s is not of type 'string'" % flavorId,
                 "%s is not of type 'integer'" % flavorId,
                 "instance['volume'] 2 is not of type 'object'"])

    @test
    def test_bad_body_datastore_create_instance(self):

        datastore = "*"
        datastore_version = "*"
        try:
            self.dbaas.instances.create("test_instance",
                                        3, {"size": 2},
                                        datastore=datastore,
                                        datastore_version=datastore_version)
        except Exception as e:
            resp, body = self.dbaas.client.last_response
            httpCode = resp.status
            asserts.assert_equal(httpCode, 400,
                                 "Create instance failed with code %s, "
                                 "exception %s" % (httpCode, e))
            assert_contains(
                e.message,
                ["Validation error:",
                 "instance['datastore']['type']"
                 " u'%s' does not match"
                 " '^.*[0-9a-zA-Z]+.*$'" % datastore,
                 "instance['datastore']['version'] u'%s' "
                 "does not match '^.*[0-9a-zA-Z]+.*$'" % datastore_version])

    @test
    def test_bad_body_volsize_create_instance(self):
        volsize = "h3ll0"
        try:
            self.dbaas.instances.create("test_instance",
                                        "1",
                                        volsize)
        except Exception as e:
            resp, body = self.dbaas.client.last_response
            httpCode = resp.status
            asserts.assert_equal(httpCode, 400,
                                 "Create instance failed with code %s, "
                                 "exception %s" % (httpCode, e))
            volsize = "u'h3ll0'"
            asserts.assert_equal(e.message,
                                 "Validation error: "
                                 "instance['volume'] %s is not of "
                                 "type 'object'" % volsize)

########NEW FILE########
__FILENAME__ = quotas
# Copyright 2013 OpenStack Foundation
# Copyright 2013 Rackspace Hosting
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

from proboscis import test
from proboscis import asserts
from proboscis import after_class
from proboscis import before_class
from proboscis.asserts import Check
from trove.tests.util import create_dbaas_client
from trove.tests.util import create_client
from trove.tests.util import get_standby_instance_flavor
from trove.tests.util.users import Requirements
from trove.tests.config import CONFIG
from troveclient.compat import exceptions


class QuotasBase(object):

    def setUp(self):
        self.user1 = CONFIG.users.find_user(Requirements(is_admin=False))
        self.user2 = CONFIG.users.find_user(Requirements(is_admin=False))
        asserts.assert_not_equal(self.user1.tenant, self.user2.tenant,
                                 "Not enough users to run QuotasTest."
                                 + " Needs >=2.")
        self.client1 = create_dbaas_client(self.user1)
        self.client2 = create_dbaas_client(self.user2)
        self.mgmt_client = create_client(is_admin=True)
        ''' Orig quotas from config
            "trove_max_instances_per_user": 55,
            "trove_max_volumes_per_user": 100,    '''
        self.original_quotas1 = self.mgmt_client.quota.show(self.user1.tenant)
        self.original_quotas2 = self.mgmt_client.quota.show(self.user2.tenant)

    def tearDown(self):
        self.mgmt_client.quota.update(self.user1.tenant,
                                      self.original_quotas1)
        self.mgmt_client.quota.update(self.user2.tenant,
                                      self.original_quotas2)


@test(groups=["dbaas.api.mgmt.quotas"])
class DefaultQuotasTest(QuotasBase):

    @before_class
    def setUp(self):
        super(DefaultQuotasTest, self).setUp()

    @after_class
    def tearDown(self):
        super(DefaultQuotasTest, self).tearDown()

    @test
    def check_quotas_are_set_to_defaults(self):
        quotas = self.mgmt_client.quota.show(self.user1.tenant)
        with Check() as check:
            check.equal(CONFIG.trove_max_instances_per_user,
                        quotas["instances"])
            check.equal(CONFIG.trove_max_volumes_per_user,
                        quotas["volumes"])
        asserts.assert_equal(len(quotas), 2)


@test(groups=["dbaas.api.mgmt.quotas"])
class ChangeInstancesQuota(QuotasBase):

    @before_class
    def setUp(self):
        super(ChangeInstancesQuota, self).setUp()
        self.mgmt_client.quota.update(self.user1.tenant, {"instances": 0})
        asserts.assert_equal(200, self.mgmt_client.last_http_code)

    @after_class
    def tearDown(self):
        super(ChangeInstancesQuota, self).tearDown()

    @test
    def check_user2_is_not_affected_on_instances_quota_change(self):
        user2_current_quota = self.mgmt_client.quota.show(self.user2.tenant)
        asserts.assert_equal(self.original_quotas2, user2_current_quota,
                             "Changing one user's quota affected another"
                             + "user's quota."
                             + " Original: %s. After Quota Change: %s" %
                             (self.original_quotas2, user2_current_quota))

    @test
    def verify_correct_update(self):
        quotas = self.mgmt_client.quota.show(self.user1.tenant)
        with Check() as check:
            check.equal(0, quotas["instances"])
            check.equal(CONFIG.trove_max_volumes_per_user,
                        quotas["volumes"])
        asserts.assert_equal(len(quotas), 2)

    @test
    def create_too_many_instances(self):
        flavor, flavor_href = get_standby_instance_flavor(self.client1)
        asserts.assert_raises(exceptions.OverLimit,
                              self.client1.instances.create,
                              "too_many_instances",
                              flavor_href, {'size': 1})
        asserts.assert_equal(413, self.client1.last_http_code)


@test(groups=["dbaas.api.mgmt.quotas"])
class ChangeVolumesQuota(QuotasBase):

    @before_class
    def setUp(self):
        super(ChangeVolumesQuota, self).setUp()
        self.mgmt_client.quota.update(self.user1.tenant, {"volumes": 0})
        asserts.assert_equal(200, self.mgmt_client.last_http_code)

    @after_class
    def tearDown(self):
        super(ChangeVolumesQuota, self).tearDown()

    @test
    def check_volumes_overlimit(self):
        flavor, flavor_href = get_standby_instance_flavor(self.client1)
        asserts.assert_raises(exceptions.OverLimit,
                              self.client1.instances.create,
                              "too_large_volume",
                              flavor_href,
                              {'size': CONFIG.trove_max_accepted_volume_size
                               + 1})
        asserts.assert_equal(413, self.client1.last_http_code)

    @test
    def check_user2_is_not_affected_on_volumes_quota_change(self):
        user2_current_quota = self.mgmt_client.quota.show(self.user2.tenant)
        asserts.assert_equal(self.original_quotas2, user2_current_quota,
                             "Changing one user's quota affected another"
                             + "user's quota."
                             + " Original: %s. After Quota Change: %s" %
                             (self.original_quotas2, user2_current_quota))

    @test
    def verify_correct_update(self):
        quotas = self.mgmt_client.quota.show(self.user1.tenant)
        with Check() as check:
            check.equal(CONFIG.trove_max_instances_per_user,
                        quotas["instances"])
            check.equal(0, quotas["volumes"])
        asserts.assert_equal(len(quotas), 2)

    @test
    def create_too_large_volume(self):
        flavor, flavor_href = get_standby_instance_flavor(self.client1)
        asserts.assert_raises(exceptions.OverLimit,
                              self.client1.instances.create,
                              "too_large_volume",
                              flavor_href,
                              {'size': CONFIG.trove_max_accepted_volume_size
                               + 1})
        asserts.assert_equal(413, self.client1.last_http_code)

    #create an instance when I set the limit back to
    #multiple updates to the quota and it should do what you expect

########NEW FILE########
__FILENAME__ = storage
#    Copyright 2011 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


from nose.plugins.skip import SkipTest

from proboscis import before_class
from proboscis import test
from proboscis import asserts

from trove import tests
from trove.tests.api.instances import CheckInstance
from trove.tests.api.instances import instance_info
from trove.tests.util import test_config
from trove.tests.util import create_dbaas_client
from trove.tests.util.users import Requirements

FAKE_MODE = test_config.values['fake_mode']
GROUP = "dbaas.api.mgmt.storage"


@test(groups=[tests.DBAAS_API, GROUP, tests.PRE_INSTANCES],
      depends_on_groups=["services.initialize"])
class StorageBeforeInstanceCreation(object):

    @before_class
    def setUp(self):
        self.user = test_config.users.find_user(Requirements(is_admin=True))
        self.client = create_dbaas_client(self.user)

    @test
    def test_storage_on_host(self):
        if not FAKE_MODE:
            raise SkipTest("Volume driver currently not working.")
        storage = self.client.storage.index()
        print("storage : %r" % storage)
        for device in storage:
            asserts.assert_true(hasattr(device, 'name'),
                                "device.name: %r" % device.name)
            asserts.assert_true(hasattr(device, 'type'),
                                "device.type: %r" % device.name)
            asserts.assert_true(hasattr(device, 'used'),
                                "device.used: %r" % device.used)

            asserts.assert_true(hasattr(device, 'provision'),
                                "device.provision: %r" % device.provision)
            provision = device.provision
            asserts.assert_true('available' in provision,
                                "provision.available: "
                                + "%r" % provision['available'])
            asserts.assert_true('percent' in provision,
                                "provision.percent: %r" % provision['percent'])
            asserts.assert_true('total' in provision,
                                "provision.total: %r" % provision['total'])

            asserts.assert_true(hasattr(device, 'capacity'),
                                "device.capacity: %r" % device.capacity)
            capacity = device.capacity
            asserts.assert_true('available' in capacity,
                                "capacity.available: "
                                + "%r" % capacity['available'])
            asserts.assert_true('total' in capacity,
                                "capacity.total: %r" % capacity['total'])
        instance_info.storage = storage


@test(groups=[tests.INSTANCES, GROUP],
      depends_on_groups=["dbaas.listing"])
class StorageAfterInstanceCreation(object):

    @before_class
    def setUp(self):
        self.user = test_config.users.find_user(Requirements(is_admin=True))
        self.client = create_dbaas_client(self.user)

    @test
    def test_storage_on_host(self):
        if not FAKE_MODE:
            raise SkipTest("Volume driver currently not working.")
        storage = self.client.storage.index()
        print("storage : %r" % storage)
        print("instance_info.storage : %r" % instance_info.storage)
        expected_attrs = ['name', 'type', 'used', 'provision', 'capacity']
        for index, device in enumerate(storage):
            CheckInstance(None).attrs_exist(device._info, expected_attrs,
                                            msg="Storage")
            asserts.assert_equal(device.name,
                                 instance_info.storage[index].name)
            asserts.assert_equal(device.used,
                                 instance_info.storage[index].used)
            asserts.assert_equal(device.type,
                                 instance_info.storage[index].type)

            provision = instance_info.storage[index].provision
            asserts.assert_equal(device.provision['available'],
                                 provision['available'])
            asserts.assert_equal(device.provision['percent'],
                                 provision['percent'])
            asserts.assert_equal(device.provision['total'], provision['total'])

            capacity = instance_info.storage[index].capacity
            asserts.assert_equal(device.capacity['available'],
                                 capacity['available'])
            asserts.assert_equal(device.capacity['total'], capacity['total'])

########NEW FILE########
__FILENAME__ = root
#    Copyright 2011 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


from troveclient.compat import exceptions

from nose.plugins.skip import SkipTest
from proboscis import before_class
from proboscis import test
from proboscis.asserts import assert_equal
from proboscis.asserts import assert_false
from proboscis.asserts import assert_not_equal
from proboscis.asserts import assert_raises
from proboscis.asserts import assert_true

from trove import tests
from trove.tests.api.users import TestUsers
from trove.tests.api.instances import instance_info
from trove.tests import util
from trove.tests.util import test_config
from trove.tests.api.databases import TestMysqlAccess


GROUP = "dbaas.api.root"


@test(depends_on_classes=[TestMysqlAccess],
      runs_after=[TestUsers],
      groups=[tests.DBAAS_API, GROUP, tests.INSTANCES])
class TestRoot(object):
    """
    Test the root operations
    """

    root_enabled_timestamp = 'Never'
    system_users = ['root', 'debian_sys_maint']

    @before_class
    def setUp(self):
        self.dbaas = util.create_dbaas_client(instance_info.user)
        self.dbaas_admin = util.create_dbaas_client(instance_info.admin_user)

    def _verify_root_timestamp(self, id):
        reh = self.dbaas_admin.management.root_enabled_history(id)
        timestamp = reh.enabled
        assert_equal(self.root_enabled_timestamp, timestamp)
        assert_equal(id, reh.id)

    def _root(self):
        global root_password
        self.dbaas.root.create(instance_info.id)
        assert_equal(200, self.dbaas.last_http_code)
        reh = self.dbaas_admin.management.root_enabled_history
        self.root_enabled_timestamp = reh(instance_info.id).enabled

    @test
    def test_root_initially_disabled(self):
        """Test that root is disabled"""
        enabled = self.dbaas.root.is_root_enabled(instance_info.id)
        assert_equal(200, self.dbaas.last_http_code)

        is_enabled = enabled
        if hasattr(enabled, 'rootEnabled'):
            is_enabled = enabled.rootEnabled
        assert_false(is_enabled, "Root SHOULD NOT be enabled.")

    @test
    def test_create_user_os_admin_failure(self):
        users = [{"name": "os_admin", "password": "12345"}]
        assert_raises(exceptions.BadRequest, self.dbaas.users.create,
                      instance_info.id, users)

    @test
    def test_delete_user_os_admin_failure(self):
        assert_raises(exceptions.BadRequest, self.dbaas.users.delete,
                      instance_info.id, "os_admin")

    @test(depends_on=[test_root_initially_disabled],
          enabled=not test_config.values['root_removed_from_instance_api'])
    def test_root_initially_disabled_details(self):
        """Use instance details to test that root is disabled."""
        instance = self.dbaas.instances.get(instance_info.id)
        assert_true(hasattr(instance, 'rootEnabled'),
                    "Instance has no rootEnabled property.")
        assert_false(instance.rootEnabled, "Root SHOULD NOT be enabled.")
        assert_equal(self.root_enabled_timestamp, 'Never')

    @test(depends_on=[test_root_initially_disabled_details])
    def test_root_disabled_in_mgmt_api(self):
        """Verifies in the management api that the timestamp exists"""
        self._verify_root_timestamp(instance_info.id)

    @test(depends_on=[test_root_initially_disabled_details])
    def test_enable_root(self):
        self._root()

    @test(depends_on=[test_enable_root])
    def test_enabled_timestamp(self):
        assert_not_equal(self.root_enabled_timestamp, 'Never')

    @test(depends_on=[test_enable_root])
    def test_root_not_in_users_list(self):
        """
        Tests that despite having enabled root, user root doesn't appear
        in the users list for the instance.
        """
        users = self.dbaas.users.list(instance_info.id)
        usernames = [user.name for user in users]
        assert_true('root' not in usernames)

    @test(depends_on=[test_enable_root])
    def test_root_now_enabled(self):
        """Test that root is now enabled."""
        enabled = self.dbaas.root.is_root_enabled(instance_info.id)
        assert_equal(200, self.dbaas.last_http_code)
        assert_true(enabled, "Root SHOULD be enabled.")

    @test(depends_on=[test_root_now_enabled],
          enabled=not test_config.values['root_removed_from_instance_api'])
    def test_root_now_enabled_details(self):
        """Use instance details to test that root is now enabled."""
        instance = self.dbaas.instances.get(instance_info.id)
        assert_true(hasattr(instance, 'rootEnabled'),
                    "Instance has no rootEnabled property.")
        assert_true(instance.rootEnabled, "Root SHOULD be enabled.")
        assert_not_equal(self.root_enabled_timestamp, 'Never')
        self._verify_root_timestamp(instance_info.id)

    @test(depends_on=[test_root_now_enabled_details])
    def test_reset_root(self):
        if test_config.values['root_timestamp_disabled']:
            raise SkipTest("Enabled timestamp not enabled yet")
        old_ts = self.root_enabled_timestamp
        self._root()
        assert_not_equal(self.root_enabled_timestamp, 'Never')
        assert_equal(self.root_enabled_timestamp, old_ts)

    @test(depends_on=[test_reset_root])
    def test_root_still_enabled(self):
        """Test that after root was reset it's still enabled."""
        enabled = self.dbaas.root.is_root_enabled(instance_info.id)
        assert_equal(200, self.dbaas.last_http_code)
        assert_true(enabled, "Root SHOULD still be enabled.")

    @test(depends_on=[test_root_still_enabled],
          enabled=not test_config.values['root_removed_from_instance_api'])
    def test_root_still_enabled_details(self):
        """Use instance details to test that after root was reset,
            it's still enabled.
        """
        instance = self.dbaas.instances.get(instance_info.id)
        assert_true(hasattr(instance, 'rootEnabled'),
                    "Instance has no rootEnabled property.")
        assert_true(instance.rootEnabled, "Root SHOULD still be enabled.")
        assert_not_equal(self.root_enabled_timestamp, 'Never')
        self._verify_root_timestamp(instance_info.id)

    @test(depends_on=[test_enable_root])
    def test_root_cannot_be_deleted(self):
        """Even if root was enabled, the user root cannot be deleted."""
        assert_raises(exceptions.BadRequest, self.dbaas.users.delete,
                      instance_info.id, "root")

########NEW FILE########
__FILENAME__ = root_on_create
#    Copyright 2013 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from proboscis import before_class
from proboscis import after_class
from proboscis import test
from proboscis.asserts import assert_equal
from proboscis.asserts import assert_not_equal
from proboscis.asserts import assert_true

from trove import tests
from trove.common import cfg
from trove.common.utils import poll_until
from trove.tests import util
from trove.tests.api.users import TestUsers
from trove.tests.api.instances import instance_info
from trove.tests.api.databases import TestMysqlAccess

CONF = cfg.CONF
GROUP = "dbaas.api.root.oncreate"


@test(depends_on_classes=[TestMysqlAccess],
      runs_after=[TestUsers],
      groups=[tests.DBAAS_API, GROUP, tests.INSTANCES])
class TestRootOnCreate(object):
    """
    Test 'CONF.root_on_create', which if True, creates the root user upon
    database instance initialization.
    """

    root_enabled_timestamp = 'Never'
    instance_id = None

    def create_instance(self):
        result = self.dbaas.instances.create(
            instance_info.name,
            instance_info.dbaas_flavor_href,
            instance_info.volume,
            instance_info.databases,
            instance_info.users,
            availability_zone="nova",
            datastore=instance_info.dbaas_datastore,
            datastore_version=instance_info.dbaas_datastore_version)
        assert_equal(200, self.dbaas.last_http_code)
        new_id = result.id

        def result_is_active():
            instance = self.dbaas.instances.get(new_id)
            if instance.status == "ACTIVE":
                return True
            else:
                assert_equal("BUILD", instance.status)
        poll_until(result_is_active)
        if 'password' in result._info:
            self.dbaas.root.create(new_id)
        return new_id

    @before_class
    def setUp(self):
        self.orig_conf_value = CONF.get(
            instance_info.dbaas_datastore).root_on_create
        CONF.get(instance_info.dbaas_datastore).root_on_create = True
        self.dbaas = util.create_dbaas_client(instance_info.user)
        self.dbaas_admin = util.create_dbaas_client(instance_info.admin_user)
        self.history = self.dbaas_admin.management.root_enabled_history
        self.enabled = self.dbaas.root.is_root_enabled
        self.instance_id = self.create_instance()

    @after_class
    def tearDown(self):
        CONF.get(instance_info.
                 dbaas_datastore).root_on_create = self.orig_conf_value
        instance = self.dbaas.instances.get(self.instance_id)
        instance.delete()

    @test
    def test_root_on_create(self):
        """Test that root is enabled after instance creation"""
        enabled = self.enabled(self.instance_id).rootEnabled
        assert_equal(200, self.dbaas.last_http_code)
        assert_true(enabled)

    @test(depends_on=[test_root_on_create])
    def test_history_after_root_on_create(self):
        """Test that the timestamp in the root enabled history is set"""
        self.root_enabled_timestamp = self.history(self.instance_id).enabled
        assert_equal(200, self.dbaas.last_http_code)
        assert_not_equal(self.root_enabled_timestamp, 'Never')

    @test(depends_on=[test_history_after_root_on_create])
    def test_reset_root(self):
        """Test that root reset does not alter the timestamp"""
        orig_timestamp = self.root_enabled_timestamp
        self.dbaas.root.create(self.instance_id)
        assert_equal(200, self.dbaas.last_http_code)
        self.root_enabled_timestamp = self.history(self.instance_id).enabled
        assert_equal(200, self.dbaas.last_http_code)
        assert_equal(orig_timestamp, self.root_enabled_timestamp)

    @test(depends_on=[test_reset_root])
    def test_root_still_enabled(self):
        """Test that after root was reset, it's still enabled."""
        enabled = self.enabled(self.instance_id).rootEnabled
        assert_equal(200, self.dbaas.last_http_code)
        assert_true(enabled)

########NEW FILE########
__FILENAME__ = users
#    Copyright 2011 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import time

from troveclient.compat import exceptions

from proboscis import after_class
from proboscis import before_class
from proboscis import test
from proboscis.asserts import assert_equal
from proboscis.asserts import assert_false
from proboscis.asserts import assert_raises
from proboscis.asserts import assert_true
from proboscis.asserts import fail

from trove import tests
from trove.tests.api.databases import TestDatabases
from trove.tests.api.instances import instance_info
from trove.tests import util
from trove.tests.util import test_config
from trove.tests.api.databases import TestMysqlAccess

import urllib


GROUP = "dbaas.api.users"
FAKE = test_config.values['fake_mode']


@test(depends_on_classes=[TestMysqlAccess],
      groups=[tests.DBAAS_API, GROUP, tests.INSTANCES],
      runs_after=[TestDatabases])
class TestUsers(object):
    """
    Test the creation and deletion of users
    """

    username = "tes!@#tuser"
    password = "testpa$^%ssword"
    username1 = "anous*&^er"
    password1 = "anopas*?.sword"
    db1 = "usersfirstdb"
    db2 = "usersseconddb"

    created_users = [username, username1]
    system_users = ['root', 'debian_sys_maint']

    def __init__(self):
        self.dbaas = util.create_dbaas_client(instance_info.user)
        self.dbaas_admin = util.create_dbaas_client(instance_info.admin_user)

    @before_class
    def setUp(self):
        databases = [{"name": self.db1, "character_set": "latin2",
                      "collate": "latin2_general_ci"},
                     {"name": self.db2}]
        try:
            self.dbaas.databases.create(instance_info.id, databases)
        except exceptions.BadRequest as e:
            if "Validation error" in e.message:
                raise e
        if not FAKE:
            time.sleep(5)

    @after_class
    def tearDown(self):
        self.dbaas.databases.delete(instance_info.id, self.db1)
        self.dbaas.databases.delete(instance_info.id, self.db2)

    @test()
    def test_delete_nonexistent_user(self):
        assert_raises(exceptions.NotFound, self.dbaas.users.delete,
                      instance_info.id, "thisuserDNE")
        assert_equal(404, self.dbaas.last_http_code)

    @test()
    def test_create_users(self):
        users = []
        users.append({"name": self.username, "password": self.password,
                      "databases": [{"name": self.db1}]})
        users.append({"name": self.username1, "password": self.password1,
                     "databases": [{"name": self.db1}, {"name": self.db2}]})
        self.dbaas.users.create(instance_info.id, users)
        assert_equal(202, self.dbaas.last_http_code)

        # Do we need this?
        if not FAKE:
            time.sleep(5)

        self.check_database_for_user(self.username, self.password,
                                     [self.db1])
        self.check_database_for_user(self.username1, self.password1,
                                     [self.db1, self.db2])

    @test(depends_on=[test_create_users])
    def test_create_users_list(self):
        #tests for users that should be listed
        users = self.dbaas.users.list(instance_info.id)
        assert_equal(200, self.dbaas.last_http_code)
        found = False
        for user in self.created_users:
            for result in users:
                if user == result.name:
                    found = True
            assert_true(found, "User '%s' not found in result" % user)
            found = False

    @test(depends_on=[test_create_users])
    def test_fails_when_creating_user_twice(self):
        users = []
        users.append({"name": self.username, "password": self.password,
                      "databases": [{"name": self.db1}]})
        users.append({"name": self.username1, "password": self.password1,
                     "databases": [{"name": self.db1}, {"name": self.db2}]})
        assert_raises(exceptions.BadRequest, self.dbaas.users.create,
                      instance_info.id, users)
        assert_equal(400, self.dbaas.last_http_code)

    @test(depends_on=[test_create_users_list])
    def test_cannot_create_root_user(self):
        # Tests that the user root (in Config:ignore_users) cannot be created.
        users = [{"name": "root", "password": "12345",
                  "databases": [{"name": self.db1}]}]
        assert_raises(exceptions.BadRequest, self.dbaas.users.create,
                      instance_info.id, users)

    @test(depends_on=[test_create_users_list])
    def test_get_one_user(self):
        user = self.dbaas.users.get(instance_info.id, username=self.username,
                                    hostname='%')
        assert_equal(200, self.dbaas.last_http_code)
        assert_equal(user.name, self.username)
        assert_equal(1, len(user.databases))
        for db in user.databases:
            assert_equal(db["name"], self.db1)
        self.check_database_for_user(self.username, self.password, [self.db1])

    @test(depends_on=[test_create_users_list])
    def test_create_users_list_system(self):
        #tests for users that should not be listed
        users = self.dbaas.users.list(instance_info.id)
        assert_equal(200, self.dbaas.last_http_code)
        for user in self.system_users:
            found = any(result.name == user for result in users)
            msg = "User '%s' SHOULD NOT BE found in result" % user
            assert_false(found, msg)

    @test(depends_on=[test_create_users_list],
          runs_after=[test_fails_when_creating_user_twice])
    def test_delete_users(self):
        self.dbaas.users.delete(instance_info.id, self.username, hostname='%')
        assert_equal(202, self.dbaas.last_http_code)
        self.dbaas.users.delete(instance_info.id, self.username1, hostname='%')
        assert_equal(202, self.dbaas.last_http_code)
        if not FAKE:
            time.sleep(5)

        self._check_connection(self.username, self.password)
        self._check_connection(self.username1, self.password1)

    @test(depends_on=[test_create_users_list, test_delete_users])
    def test_hostnames_default_if_not_present(self):
        # These tests rely on test_delete_users as they create users only
        # they use.
        username = "testuser_nohost"
        user = {"name": username, "password": "password", "databases": []}

        self.dbaas.users.create(instance_info.id, [user])

        user["host"] = "%"
        # Can't create the user a second time if it already exists.
        assert_raises(exceptions.BadRequest, self.dbaas.users.create,
                      instance_info.id, [user])

        self.dbaas.users.delete(instance_info.id, username)

    @test(depends_on=[test_create_users_list, test_delete_users])
    def test_hostnames_make_users_unique(self):
        # These tests rely on test_delete_users as they create users only
        # they use.
        username = "testuser_unique"
        hostnames = ["192.168.0.1", "192.168.0.2"]
        users = [{"name": username, "password": "password", "databases": [],
                  "host": hostname}
                 for hostname in hostnames]

        # Nothing wrong with creating two users with the same name, so long
        # as their hosts are different.
        self.dbaas.users.create(instance_info.id, users)
        for hostname in hostnames:
            self.dbaas.users.delete(instance_info.id, username,
                                    hostname=hostname)

    @test()
    def test_updateduser_newname_host_unique(self):
        #The updated_username@hostname should not exist already
        users = []
        old_name = "testuser1"
        hostname = "192.168.0.1"
        users.append({"name": old_name, "password": "password",
                      "host": hostname, "databases": []})
        users.append({"name": "testuser2", "password": "password",
                      "host": hostname, "databases": []})
        self.dbaas.users.create(instance_info.id, users)
        user_new = {"name": "testuser2"}
        assert_raises(exceptions.BadRequest,
                      self.dbaas.users.update_attributes, instance_info.id,
                      old_name, user_new, hostname)
        assert_equal(400, self.dbaas.last_http_code)
        self.dbaas.users.delete(instance_info.id, old_name, hostname=hostname)
        self.dbaas.users.delete(instance_info.id, "testuser2",
                                hostname=hostname)

    @test()
    def test_updateduser_name_newhost_unique(self):
        # The username@updated_hostname should not exist already
        users = []
        username = "testuser"
        hostname1 = "192.168.0.1"
        hostname2 = "192.168.0.2"
        users.append({"name": username, "password": "password",
                      "host": hostname1, "databases": []})
        users.append({"name": username, "password": "password",
                      "host": hostname2, "databases": []})
        self.dbaas.users.create(instance_info.id, users)
        user_new = {"host": "192.168.0.2"}
        assert_raises(exceptions.BadRequest,
                      self.dbaas.users.update_attributes, instance_info.id,
                      username, user_new, hostname1)
        assert_equal(400, self.dbaas.last_http_code)
        self.dbaas.users.delete(instance_info.id, username, hostname=hostname1)
        self.dbaas.users.delete(instance_info.id, username, hostname=hostname2)

    @test()
    def test_updateduser_newname_newhost_unique(self):
        # The updated_username@updated_hostname should not exist already
        users = []
        username = "testuser1"
        hostname1 = "192.168.0.1"
        hostname2 = "192.168.0.2"
        users.append({"name": username, "password": "password",
                      "host": hostname1, "databases": []})
        users.append({"name": "testuser2", "password": "password",
                      "host": hostname2, "databases": []})
        self.dbaas.users.create(instance_info.id, users)
        user_new = {"name": "testuser2", "host": "192.168.0.2"}
        assert_raises(exceptions.BadRequest,
                      self.dbaas.users.update_attributes, instance_info.id,
                      username, user_new, hostname1)
        assert_equal(400, self.dbaas.last_http_code)
        self.dbaas.users.delete(instance_info.id, username, hostname=hostname1)
        self.dbaas.users.delete(instance_info.id, "testuser2",
                                hostname=hostname2)

    @test()
    def test_updateduser_newhost_invalid(self):
        # Ensure invalid hostnames/usernames aren't allowed to enter the system
        users = []
        username = "testuser1"
        hostname1 = "192.168.0.1"
        users.append({"name": username, "password": "password",
                      "host": hostname1, "databases": []})
        self.dbaas.users.create(instance_info.id, users)
        hostname1 = hostname1.replace('.', '%2e')
        assert_raises(exceptions.BadRequest,
                      self.dbaas.users.update_attributes, instance_info.id,
                      username, {"host": "badjuju"}, hostname1)
        assert_equal(400, self.dbaas.last_http_code)

        assert_raises(exceptions.BadRequest,
                      self.dbaas.users.update_attributes, instance_info.id,
                      username, {"name": " bad username   "}, hostname1)
        assert_equal(400, self.dbaas.last_http_code)

        self.dbaas.users.delete(instance_info.id, username, hostname=hostname1)

    @test()
    def test_cannot_change_rootpassword(self):
        # Cannot change password for a root user
        user_new = {"password": "12345"}
        assert_raises(exceptions.BadRequest,
                      self.dbaas.users.update_attributes, instance_info.id,
                      "root", user_new)

    @test()
    def test_updateuser_emptyhost(self):
        # Cannot update the user hostname with an empty string
        users = []
        username = "testuser1"
        hostname = "192.168.0.1"
        users.append({"name": username, "password": "password",
                      "host": hostname, "databases": []})
        self.dbaas.users.create(instance_info.id, users)
        user_new = {"host": ""}
        assert_raises(exceptions.BadRequest,
                      self.dbaas.users.update_attributes, instance_info.id,
                      username, user_new, hostname)
        assert_equal(400, self.dbaas.last_http_code)
        self.dbaas.users.delete(instance_info.id, username, hostname=hostname)

    @test(depends_on=[test_create_users])
    def test_hostname_ipv4_restriction(self):
        # By default, user hostnames are required to be % or IPv4 addresses.
        user = {"name": "ipv4_nodice", "password": "password",
                "databases": [], "host": "disallowed_host"}

        assert_raises(exceptions.BadRequest, self.dbaas.users.create,
                      instance_info.id, [user])

    def show_databases(self, user, password):
        print("Going to connect to %s, %s, %s"
              % (instance_info.get_address(), user, password))
        with util.mysql_connection().create(instance_info.get_address(),
                                            user, password) as db:
            print(db)
            dbs = db.execute("show databases")
            return [row['Database'] for row in dbs]

    def check_database_for_user(self, user, password, dbs):
        if not FAKE:
            # Make the real call to the database to check things.
            actual_list = self.show_databases(user, password)
            for db in dbs:
                assert_true(
                    db in actual_list,
                    "No match for db %s in dblist. %s :(" % (db, actual_list))
        # Confirm via API list.
        result = self.dbaas.users.list(instance_info.id)
        assert_equal(200, self.dbaas.last_http_code)
        for item in result:
            if item.name == user:
                break
        else:
            fail("User %s not added to collection." % user)

        # Confirm via API get.
        result = self.dbaas.users.get(instance_info.id, user, '%')
        assert_equal(200, self.dbaas.last_http_code)
        if result.name != user:
            fail("User %s not found via get." % user)

    @test
    def test_username_too_long(self):
        users = [{"name": "1233asdwer345tyg56", "password": self.password,
                  "database": self.db1}]
        assert_raises(exceptions.BadRequest, self.dbaas.users.create,
                      instance_info.id, users)
        assert_equal(400, self.dbaas.last_http_code)

    @test
    def test_invalid_username(self):
        users = []
        users.append({"name": "user,", "password": self.password,
                      "database": self.db1})
        assert_raises(exceptions.BadRequest, self.dbaas.users.create,
                      instance_info.id, users)
        assert_equal(400, self.dbaas.last_http_code)

    @test(enabled=False)
    #TODO(hub_cap): Make this test work once python-routes is updated, if ever.
    def test_delete_user_with_period_in_name(self):
        """Attempt to create/destroy a user with a period in its name"""
        users = []
        username_with_period = "user.name"
        users.append({"name": username_with_period, "password": self.password,
                      "databases": [{"name": self.db1}]})
        self.dbaas.users.create(instance_info.id, users)
        assert_equal(202, self.dbaas.last_http_code)
        if not FAKE:
            time.sleep(5)

        self.check_database_for_user(username_with_period, self.password,
                                     [self.db1])
        self.dbaas.users.delete(instance_info.id, username_with_period)
        assert_equal(202, self.dbaas.last_http_code)

    @test
    def test_invalid_password(self):
        users = [{"name": "anouser", "password": "sdf,;",
                  "database": self.db1}]
        assert_raises(exceptions.BadRequest, self.dbaas.users.create,
                      instance_info.id, users)
        assert_equal(400, self.dbaas.last_http_code)

    @test
    def test_pagination(self):
        users = []
        users.append({"name": "Jetson", "password": "george",
                      "databases": [{"name": "Sprockets"}]})
        users.append({"name": "Jetson", "password": "george",
                      "host": "127.0.0.1",
                      "databases": [{"name": "Sprockets"}]})
        users.append({"name": "Spacely", "password": "cosmo",
                      "databases": [{"name": "Sprockets"}]})
        users.append({"name": "Spacely", "password": "cosmo",
                      "host": "127.0.0.1",
                      "databases": [{"name": "Sprockets"}]})
        users.append({"name": "Uniblab", "password": "fired",
                      "databases": [{"name": "Sprockets"}]})
        users.append({"name": "Uniblab", "password": "fired",
                      "host": "192.168.0.10",
                      "databases": [{"name": "Sprockets"}]})

        self.dbaas.users.create(instance_info.id, users)
        assert_equal(202, self.dbaas.last_http_code)
        if not FAKE:
            time.sleep(5)
        limit = 2
        users = self.dbaas.users.list(instance_info.id, limit=limit)
        assert_equal(200, self.dbaas.last_http_code)
        marker = users.next

        # Better get only as many as we asked for
        assert_true(len(users) <= limit)
        assert_true(users.next is not None)
        expected_marker = "%s@%s" % (users[-1].name, users[-1].host)
        expected_marker = urllib.quote(expected_marker)
        assert_equal(marker, expected_marker)
        marker = users.next

        # I better get new users if I use the marker I was handed.
        users = self.dbaas.users.list(instance_info.id, limit=limit,
                                      marker=marker)
        assert_equal(200, self.dbaas.last_http_code)
        assert_true(marker not in [user.name for user in users])

        # Now fetch again with a larger limit.
        users = self.dbaas.users.list(instance_info.id)
        assert_equal(200, self.dbaas.last_http_code)
        assert_true(users.next is None)

    def _check_connection(self, username, password):
        if not FAKE:
            util.mysql_connection().assert_fails(instance_info.get_address(),
                                                 username, password)
        # Also determine the db is gone via API.
        result = self.dbaas.users.list(instance_info.id)
        assert_equal(200, self.dbaas.last_http_code)
        for item in result:
            if item.name == username:
                fail("User %s was not deleted." % username)

########NEW FILE########
__FILENAME__ = user_access
#    Copyright 2013 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from random import choice

from troveclient.compat import exceptions

from proboscis import after_class
from proboscis import before_class
from proboscis import test
from proboscis import asserts

from trove import tests
from trove.tests.api.instances import instance_info
from trove.tests import util
from trove.tests.util import test_config
from trove.tests.api.users import TestUsers

GROUP = "dbaas.api.useraccess"
GROUP_POSITIVE = GROUP + ".positive"
GROUP_NEGATIVE = GROUP + ".negative"

FAKE = test_config.values['fake_mode']


class UserAccessBase(object):
    """
    Base class for Positive and Negative TestUserAccess classes
    """
    users = []
    databases = []

    def set_up(self):
        self.dbaas = util.create_dbaas_client(instance_info.user)
        self.users = ["test_access_user"]
        self.databases = [("test_access_db%02i" % i) for i in range(4)]

    def _user_list_from_names(self, usernames):
        return [{"name": name,
                 "password": "password",
                 "databases": []} for name in usernames]

    def _grant_access_singular(self, user, databases, expected_response=202):
        """Grant a single user access to the databases listed.
            Potentially, expect an exception in the process.
        """
        try:
            self.dbaas.users.grant(instance_info.id, user, databases)
        except exceptions.BadRequest:
            asserts.assert_equal(400, expected_response)
        except exceptions.NotFound:
            asserts.assert_equal(404, expected_response)
        except exceptions.ClientException:
            asserts.assert_equal(500, expected_response)
        finally:
            asserts.assert_equal(expected_response, self.dbaas.last_http_code)

    def _grant_access_plural(self, users, databases, expected_response=202):
        """Grant each user in the list access to all the databases listed.
            Potentially, expect an exception in the process.
        """
        for user in users:
            self._grant_access_singular(user, databases, expected_response)

    def _revoke_access_singular(self, user, database, expected_response=202):
        """Revoke from a user access to the given database .
            Potentially, expect an exception in the process.
        """
        try:
            self.dbaas.users.revoke(instance_info.id, user, database)
            asserts.assert_true(expected_response, self.dbaas.last_http_code)
        except exceptions.BadRequest:
            asserts.assert_equal(400, self.dbaas.last_http_code)
        except exceptions.NotFound:
            asserts.assert_equal(404, self.dbaas.last_http_code)

    def _revoke_access_plural(self, users, databases, expected_response=202):
        """Revoke from each user access to each database.
            Potentially, expect an exception in the process.
        """
        for user in users:
            for database in databases:
                self._revoke_access_singular(user,
                                             database,
                                             expected_response)

    def _test_access(self, users, databases, expected_response=200):
        """Verify that each user in the list has access to each database in
            the list.
        """
        for user in users:
            access = self.dbaas.users.list_access(instance_info.id, user)
            asserts.assert_equal(expected_response, self.dbaas.last_http_code)
            access = [db.name for db in access]
            asserts.assert_equal(set(access), set(databases))

    def _test_ignore_access(self, users, databases, expected_response=200):
        databases = [d for d in databases if d not in ['lost+found',
                                                       'mysql',
                                                       'information_schema']]
        self._test_access(users, databases, expected_response)

    def _reset_access(self):
        for user in self.users:
            for database in self.databases + self.ghostdbs:
                try:
                    self.dbaas.users.revoke(instance_info.id, user, database)
                    asserts.assert_true(self.dbaas.last_http_code in [202, 404]
                                        )
                except exceptions.NotFound:
                    # This is all right here, since we're resetting.
                    pass
        self._test_access(self.users, [])


@test(depends_on_classes=[TestUsers],
      groups=[tests.DBAAS_API, GROUP, tests.INSTANCES],
      runs_after=[TestUsers])
class TestUserAccessPasswordChange(UserAccessBase):
    """
    Test that change_password works.
    """

    @before_class
    def setUp(self):
        super(TestUserAccessPasswordChange, self).set_up()

    def _check_mysql_connection(self, username, password, success=True):
        # This can only test connections for users with the host %.
        # Much more difficult to simulate connection attempts from other hosts.
        if FAKE:
            # "Fake mode; cannot test mysql connection."
            return

        conn = util.mysql_connection()
        if success:
            conn.create(instance_info.get_address(), username, password)
        else:
            conn.assert_fails(instance_info.get_address(), username, password)

    def _pick_a_user(self):
        users = self._user_list_from_names(self.users)
        return choice(users)  # Pick one, it doesn't matter.

    @test()
    def test_change_password_bogus_user(self):
        user = self._pick_a_user()
        user["name"] = "thisuserhasanamethatstoolong"
        asserts.assert_raises(exceptions.BadRequest,
                              self.dbaas.users.change_passwords,
                              instance_info.id, [user])
        asserts.assert_equal(400, self.dbaas.last_http_code)

    @test()
    def test_change_password_nonexistent_user(self):
        user = self._pick_a_user()
        user["name"] = "thisuserDNE"
        asserts.assert_raises(exceptions.NotFound,
                              self.dbaas.users.change_passwords,
                              instance_info.id, [user])
        asserts.assert_equal(404, self.dbaas.last_http_code)

    @test()
    def test_create_user_and_dbs(self):
        users = self._user_list_from_names(self.users)
        # Default password for everyone is 'password'.
        self.dbaas.users.create(instance_info.id, users)
        asserts.assert_equal(202, self.dbaas.last_http_code)

        databases = [{"name": db}
                     for db in self.databases]
        self.dbaas.databases.create(instance_info.id, databases)
        asserts.assert_equal(202, self.dbaas.last_http_code)

    @test(depends_on=[test_create_user_and_dbs])
    def test_initial_connection(self):
        user = self._pick_a_user()
        self._check_mysql_connection(user["name"], "password")

    @test(depends_on=[test_initial_connection])
    def test_change_password(self):
        # Doesn't actually change anything, just tests that the call doesn't
        # have any problems. As an aside, also checks that a user can
        # change its password to the same thing again.
        user = self._pick_a_user()
        password = user["password"]
        self.dbaas.users.change_passwords(instance_info.id, [user])
        self._check_mysql_connection(user["name"], password)

    @test(depends_on=[test_change_password])
    def test_change_password_back(self):
        user = self._pick_a_user()
        old_password = user["password"]
        new_password = "NEWPASSWORD"

        user["password"] = new_password
        self.dbaas.users.change_passwords(instance_info.id, [user])
        self._check_mysql_connection(user["name"], new_password)

        user["password"] = old_password
        self.dbaas.users.change_passwords(instance_info.id, [user])
        self._check_mysql_connection(user["name"], old_password)

    @test(depends_on=[test_change_password_back])
    def test_change_password_twice(self):
        # Changing the password twice isn't a problem.
        user = self._pick_a_user()
        password = "NEWPASSWORD"
        user["password"] = password
        self.dbaas.users.change_passwords(instance_info.id, [user])
        self.dbaas.users.change_passwords(instance_info.id, [user])
        self._check_mysql_connection(user["name"], password)

    @after_class(always_run=True)
    def tearDown(self):
        for database in self.databases:
            self.dbaas.databases.delete(instance_info.id, database)
            asserts.assert_equal(202, self.dbaas.last_http_code)
        for username in self.users:
            self.dbaas.users.delete(instance_info.id, username)


@test(depends_on_classes=[TestUsers],
      groups=[tests.DBAAS_API, GROUP, GROUP_POSITIVE, tests.INSTANCES],
      runs_after=[TestUsers])
class TestUserAccessPositive(UserAccessBase):
    """
    Test the creation and deletion of user grants.
    """

    @before_class
    def setUp(self):
        super(TestUserAccessPositive, self).set_up()
        # None of the ghosts are real databases or users.
        self.ghostdbs = ["test_user_access_ghost_db"]
        self.ghostusers = ["test_ghostuser"]
        self.revokedbs = self.databases[:1]
        self.remainingdbs = self.databases[1:]

    def _ensure_nothing_else_created(self):
        # Make sure grants and revokes do not create users or databases.
        databases = self.dbaas.databases.list(instance_info.id)
        database_names = [db.name for db in databases]
        for ghost in self.ghostdbs:
            asserts.assert_true(ghost not in database_names)
        users = self.dbaas.users.list(instance_info.id)
        user_names = [user.name for user in users]
        for ghost in self.ghostusers:
            asserts.assert_true(ghost not in user_names)

    @test()
    def test_create_user_and_dbs(self):
        users = self._user_list_from_names(self.users)
        self.dbaas.users.create(instance_info.id, users)
        asserts.assert_equal(202, self.dbaas.last_http_code)

        databases = [{"name": db}
                     for db in self.databases]
        self.dbaas.databases.create(instance_info.id, databases)
        asserts.assert_equal(202, self.dbaas.last_http_code)

    @test(depends_on=[test_create_user_and_dbs])
    def test_no_access(self):
        # No users have any access to any database.
        self._reset_access()
        self._test_access(self.users, [])

    @test(depends_on=[test_no_access])
    def test_grant_full_access(self):
        # The users are granted access to all test databases.
        self._reset_access()
        self._grant_access_plural(self.users, self.databases)
        self._test_access(self.users, self.databases)

    @test(depends_on=[test_no_access])
    def test_grant_full_access_ignore_databases(self):
        # The users are granted access to all test databases.
        all_dbs = []
        all_dbs.extend(self.databases)
        all_dbs.extend(['lost+found', 'mysql', 'information_schema'])
        self._reset_access()
        self._grant_access_plural(self.users, self.databases)
        self._test_ignore_access(self.users, self.databases)

    @test(depends_on=[test_grant_full_access])
    def test_grant_idempotence(self):
        # Grant operations can be repeated with no ill effects.
        self._reset_access()
        for repeat in range(3):
            self._grant_access_plural(self.users, self.databases)
        self._test_access(self.users, self.databases)

    @test(depends_on=[test_grant_full_access])
    def test_revoke_one_database(self):
        # Revoking permission removes that database from a user's list.
        self._reset_access()
        self._grant_access_plural(self.users, self.databases)
        self._test_access(self.users, self.databases)
        self._revoke_access_plural(self.users, self.revokedbs)
        self._test_access(self.users, self.remainingdbs)

    @test(depends_on=[test_grant_full_access])
    def test_revoke_non_idempotence(self):
        # Revoking access cannot be repeated.
        self._reset_access()
        self._grant_access_plural(self.users, self.databases)
        self._revoke_access_plural(self.users, self.revokedbs)
        self._revoke_access_plural(self.users, self.revokedbs, 404)
        self._test_access(self.users, self.remainingdbs)

    @test(depends_on=[test_grant_full_access])
    def test_revoke_all_access(self):
        # Revoking access to all databases will leave their access empty.
        self._reset_access()
        self._grant_access_plural(self.users, self.databases)
        self._revoke_access_plural(self.users, self.revokedbs)
        self._test_access(self.users, self.remainingdbs)

    @test(depends_on=[test_grant_full_access])
    def test_grant_ghostdbs(self):
        # Grants to imaginary databases are acceptable, and are honored.
        self._reset_access()
        self._ensure_nothing_else_created()
        self._grant_access_plural(self.users, self.ghostdbs)
        self._ensure_nothing_else_created()

    @test(depends_on=[test_grant_full_access])
    def test_revoke_ghostdbs(self):
        # Revokes to imaginary databases are acceptable, and are honored.
        self._reset_access()
        self._ensure_nothing_else_created()
        self._grant_access_plural(self.users, self.ghostdbs)
        self._revoke_access_plural(self.users, self.ghostdbs)
        self._ensure_nothing_else_created()

    @test(depends_on=[test_grant_full_access])
    def test_grant_ghostusers(self):
        # You cannot grant permissions to imaginary users, as imaginary users
        # don't have passwords we can pull from mysql.users
        self._reset_access()
        self._grant_access_plural(self.ghostusers, self.databases, 404)

    @test(depends_on=[test_grant_full_access])
    def test_revoke_ghostusers(self):
        # You cannot revoke permissions from imaginary users, as imaginary
        # users don't have passwords we can pull from mysql.users
        self._reset_access()
        self._revoke_access_plural(self.ghostusers, self.databases, 404)

    @after_class(always_run=True)
    def tearDown(self):
        self._reset_access()
        for database in self.databases:
            self.dbaas.databases.delete(instance_info.id, database)
            asserts.assert_equal(202, self.dbaas.last_http_code)
        for username in self.users:
            self.dbaas.users.delete(instance_info.id, username)


@test(depends_on_classes=[TestUserAccessPositive],
      groups=[tests.DBAAS_API, GROUP, GROUP_NEGATIVE, tests.INSTANCES],
      depends_on=[TestUserAccessPositive])
class TestUserAccessNegative(UserAccessBase):
    """
    Negative tests for the creation and deletion of user grants.
    """

    @before_class
    def setUp(self):
        super(TestUserAccessNegative, self).set_up()
        self.users = ["qe_user?neg3F", "qe_user#neg23"]
        self.databases = [("qe_user_neg_db%02i" % i) for i in range(2)]
        self.ghostdbs = []

    def _add_users(self, users, expected_response=202):
        user_list = self._user_list_from_names(users)
        try:
            self.dbaas.users.create(instance_info.id, user_list)
            asserts.assert_equal(self.dbaas.last_http_code, 202)
        except exceptions.BadRequest:
            asserts.assert_equal(self.dbaas.last_http_code, 400)
        asserts.assert_equal(expected_response, self.dbaas.last_http_code)

    @test()
    def test_create_duplicate_user_and_dbs(self):
        """
        Create the same user to the first DB - allowed, not part of change
        """
        users = self._user_list_from_names(self.users)
        self.dbaas.users.create(instance_info.id, users)
        asserts.assert_equal(202, self.dbaas.last_http_code)
        databases = [{"name": db} for db in self.databases]
        self.dbaas.databases.create(instance_info.id, databases)
        asserts.assert_equal(202, self.dbaas.last_http_code)

    @test(depends_on=[test_create_duplicate_user_and_dbs])
    def test_neg_duplicate_useraccess(self):
        """
        Grant duplicate users access to all database.
        """
        username = "qe_user.neg2E"
        self._add_users([username])
        self._add_users([username], 400)
        for repeat in range(3):
            self._grant_access_plural(self.users, self.databases)
        self._test_access(self.users, self.databases)

    @test()
    def test_re_create_user(self):
        user_list = ["re_create_user"]
        # create, grant, then check a new user
        self._add_users(user_list)
        self._test_access(user_list, [])
        self._grant_access_singular(user_list[0], self.databases)
        self._test_access(user_list, self.databases)
        # drop the user temporarily
        self.dbaas.users.delete(instance_info.id, user_list[0])
        # check his access - user should not be found
        asserts.assert_raises(exceptions.NotFound,
                              self.dbaas.users.list_access,
                              instance_info.id,
                              user_list[0])
        # re-create the user
        self._add_users(user_list)
        # check his access - should not exist
        self._test_access(user_list, [])
        # grant user access to all database.
        self._grant_access_singular(user_list[0], self.databases)
        # check his access - user should exist
        self._test_access(user_list, self.databases)
        # revoke users access
        self._revoke_access_plural(user_list, self.databases)

    def _negative_user_test(self, username, databases,
                            create_response=202, grant_response=202,
                            access_response=200, revoke_response=202):
        # Try and fail to create the user.
        self._add_users([username], create_response)
        self._grant_access_singular(username, databases, grant_response)
        access = None
        try:
            access = self.dbaas.users.list_access(instance_info.id, username)
            asserts.assert_equal(200, self.dbaas.last_http_code)
        except exceptions.BadRequest:
            asserts.assert_equal(400, self.dbaas.last_http_code)
        except exceptions.NotFound:
            asserts.assert_equal(404, self.dbaas.last_http_code)
        finally:
            asserts.assert_equal(access_response, self.dbaas.last_http_code)
        if access is not None:
            access = [db.name for db in access]
            asserts.assert_equal(set(access), set(self.databases))

        self._revoke_access_plural([username], databases, revoke_response)

    @test
    def test_user_withperiod(self):
        # This is actually fine; we escape dots in the user-host pairing.
        self._negative_user_test("test.user", self.databases)

    @test
    def test_user_empty_no_host(self):
        # This creates a request to .../<instance-id>/users//databases,
        # which is parsed to mean "show me user 'databases', which in this
        # case is a valid username, but not one of an extant user.
        self._negative_user_test("", self.databases, 400, 500, 404, 404)

    @test
    def test_user_empty_with_host(self):
        #self._negative_user_test("", self.databases, 400, 400, 400, 400)
        # Try and fail to create the user.
        empty_user = {"name": "", "host": "%",
                      "password": "password", "databases": []}
        asserts.assert_raises(exceptions.BadRequest,
                              self.dbaas.users.create,
                              instance_info.id,
                              [empty_user])
        asserts.assert_equal(400, self.dbaas.last_http_code)

        asserts.assert_raises(exceptions.BadRequest, self.dbaas.users.grant,
                              instance_info.id, "", [], "%")
        asserts.assert_equal(400, self.dbaas.last_http_code)

        asserts.assert_raises(exceptions.BadRequest,
                              self.dbaas.users.list_access,
                              instance_info.id, "", "%")
        asserts.assert_equal(400, self.dbaas.last_http_code)

        asserts.assert_raises(exceptions.BadRequest, self.dbaas.users.revoke,
                              instance_info.id, "", "db", "%")
        asserts.assert_equal(400, self.dbaas.last_http_code)

    @test
    def test_user_nametoolong(self):
        # You cannot create a user with this name.
        # Grant revoke, and access filter this username as invalid.
        self._negative_user_test("exceed_limit_user", self.databases,
                                 400, 400, 400, 400)

    @test
    def test_user_allspaces(self):
        self._negative_user_test("     ", self.databases, 400, 400, 400, 400)

    @after_class(always_run=True)
    def tearDown(self):
        self._reset_access()

        for database in self.databases:
            self.dbaas.databases.delete(instance_info.id, database)
            asserts.assert_equal(202, self.dbaas.last_http_code)
        for username in self.users:
            self.dbaas.users.delete(instance_info.id, username)

########NEW FILE########
__FILENAME__ = versions
#    Copyright 2011 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from proboscis import before_class
from proboscis import test
from proboscis.asserts import assert_equal
from proboscis import SkipTest

from troveclient.compat.exceptions import ClientException
from trove import tests
from trove.tests.util import test_config
from trove.tests.util import create_dbaas_client
from trove.tests.util.users import Requirements

GROUP = "dbaas.api.versions"


@test(groups=[tests.DBAAS_API, GROUP, tests.PRE_INSTANCES, 'DBAAS_VERSIONS'],
      depends_on_groups=["services.initialize"])
class Versions(object):
    """Test listing all versions and verify the current version"""

    @before_class
    def setUp(self):
        """Sets up the client."""
        user = test_config.users.find_user(Requirements(is_admin=False))
        self.client = create_dbaas_client(user)

    @test
    def test_list_versions_index(self):
        versions = self.client.versions.index(test_config.version_url)
        assert_equal(1, len(versions))
        assert_equal("CURRENT", versions[0].status,
                     message="Version status: %s" % versions[0].status)
        expected_version = test_config.values['trove_version']
        assert_equal(expected_version, versions[0].id,
                     message="Version ID: %s" % versions[0].id)
        expected_api_updated = test_config.values['trove_api_updated']
        assert_equal(expected_api_updated, versions[0].updated,
                     message="Version updated: %s" % versions[0].updated)

    def _request(self, url, method='GET', response='200'):
        resp, body = None, None
        full_url = test_config.version_url + url
        try:
            resp, body = self.client.client.request(full_url, method)
            assert_equal(resp.get('status', ''), response)
        except ClientException as ce:
            assert_equal(str(ce.http_status), response)
        return body

    @test
    def test_no_slash_no_version(self):
        self._request('')

    @test
    def test_no_slash_with_version(self):
        if test_config.auth_strategy == "fake":
            raise SkipTest("Skipping this test since auth is faked.")
        self._request('/v1.0', response='401')

    @test
    def test_with_slash_no_version(self):
        self._request('/')

    @test
    def test_with_slash_with_version(self):
        if test_config.auth_strategy == "fake":
            raise SkipTest("Skipping this test since auth is faked.")
        self._request('/v1.0/', response='401')

    @test
    def test_request_no_version(self):
        self._request('/dbaas/instances', response='404')

    @test
    def test_request_bogus_version(self):
        self._request('/0.0/', response='404')

########NEW FILE########
__FILENAME__ = config
# Copyright (c) 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Handles configuration options for the tests.

The tests are capable of running in other contexts, such as in a VM or against
a real deployment. Using this configuration ensures we can run them in other
environments if we choose to.

"""

import json
import os
from collections import Mapping


#TODO(tim.simpson): I feel like this class already exists somewhere in core
#                   Python.
class FrozenDict(Mapping):

    def __init__(self, original):
        self.original = original

    def __len__(self):
        return self.original.__len__()

    def __iter__(self, *args, **kwargs):
        return self.original.__iter__(self, *args, **kwargs)

    def __getitem__(self, *args, **kwargs):
        return self.original.__getitem__(*args, **kwargs)

    def __str__(self):
        return self.original.__str__()


USAGE_ENDPOINT = os.environ.get("USAGE_ENDPOINT",
                                "trove.tests.util.usage.UsageVerifier")


class TestConfig(object):
    """
    Holds test configuration values which can be accessed as attributes
    or using the values dictionary.
    """

    def __init__(self):
        """
        Create TestConfig, and set default values. These will be overwritten by
        the "load_from" methods below.
        """
        self._loaded_files = []
        self._values = {
            'clean_slate': os.environ.get("CLEAN_SLATE", "False") == "True",
            'fake_mode': os.environ.get("FAKE_MODE", "False") == "True",
            'nova_auth_url': "http://localhost:5000/v2.0",
            'trove_auth_url': "http://localhost:5000/v2.0/tokens",
            'dbaas_url': "http://localhost:8775/v1.0/dbaas",
            'version_url': "http://localhost:8775/",
            'nova_url': "http://localhost:8774/v1.1",
            'dbaas_datastore': "mysql",
            'dbaas_datastore_id': "a00000a0-00a0-0a00-00a0-000a000000aa",
            'dbaas_datastore_id_no_versions': "10000000-0000-0000-0000-"
                                              "000000000001",
            'dbaas_datastore_version': "5.5",
            'dbaas_datastore_version_id': "b00000b0-00b0-0b00-00b0-"
                                          "000b000000bb",
            'dbaas_inactive_datastore_version': "mysql_inactive_version",
            'instance_create_time': 16 * 60,
            'mysql_connection_method': {"type": "direct"},
            'typical_nova_image_name': None,
            'white_box': os.environ.get("WHITE_BOX", "False") == "True",
            'test_mgmt': False,
            'use_local_ovz': False,
            "known_bugs": {},
            "in_proc_server": True,
            "report_directory": os.environ.get("REPORT_DIRECTORY", None),
            "trove_volume_support": True,
            "trove_max_volumes_per_user": 100,
            "usage_endpoint": USAGE_ENDPOINT,
            "root_on_create": False
        }
        self._frozen_values = FrozenDict(self._values)
        self._users = None

    def get(self, name, default_value):
        return self.values.get(name, default_value)

    def get_report(self):
        return PrintReporter()

    def load_from_line(self, line):
        index = line.find("=")
        if index >= 0:
            key = line[:index]
            value = line[index + 1:]
            self._values[key] = value

    def load_include_files(self, original_file, files):
        directory = os.path.dirname(original_file)
        for file_sub_path in files:
            file_full_path = os.path.join(directory, file_sub_path)
            self.load_from_file(file_full_path)

    def load_from_file(self, file_path):
        if file_path in self._loaded_files:
            return
        file_contents = open(file_path, "r").read()
        try:
            contents = json.loads(file_contents)
        except Exception as exception:
            raise RuntimeError("Error loading conf file \"%s\"." % file_path,
                               exception)
        finally:
            self._loaded_files.append(file_path)

        if "include-files" in contents:
            self.load_include_files(file_path, contents['include-files'])
            del contents['include-files']
        self._values.update(contents)

    def __getattr__(self, name):
        if name not in self._values:
            raise AttributeError('Configuration value "%s" not found.' % name)
        else:
            return self._values[name]

    def python_cmd_list(self):
        """The start of a command list to use when running Python scripts."""
        commands = []
        if self.use_venv:
            commands.append("%s/tools/with_venv.sh" % self.nova_code_root)
            return list
        commands.append("python")
        return commands

    @property
    def users(self):
        if self._users is None:
            from trove.tests.util.users import Users
            self._users = Users(self.values['users'])
        return self._users

    @property
    def values(self):
        return self._frozen_values


class PrintReporter(object):

    def log(self, msg):
        print("[REPORT] %s" % msg)

    def update(self):
        pass  # Ignore. This is used in other reporters.


CONFIG = TestConfig()
del TestConfig.__init__

########NEW FILE########
__FILENAME__ = common
# Copyright 2010-2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Common code to help in faking the models."""

from novaclient import exceptions as nova_exceptions
from trove.common import cfg
from trove.openstack.common import log as logging


CONF = cfg.CONF
LOG = logging.getLogger(__name__)


def authorize(context):
    if not context.is_admin:
        raise nova_exceptions.Forbidden(403, "Forbidden")

########NEW FILE########
__FILENAME__ = dns
#    Copyright 2014 Rackspace
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from trove.dns import driver
from proboscis.asserts import fail
from proboscis.asserts import assert_equal
from proboscis.asserts import assert_true
from trove.openstack.common import log as logging


LOG = logging.getLogger(__name__)
ENTRIES = {}


class FakeDnsDriver(driver.DnsDriver):

    def create_entry(self, entry, content):
        """Pretend to create a DNS entry somewhere.

        Since nothing else tests that this works, there's nothing more to do
        here.

        """
        entry.content = content
        assert_true(entry.name not in ENTRIES)
        LOG.debug("Adding fake DNS entry for hostname %s." % entry.name)
        ENTRIES[entry.name] = entry

    def delete_entry(self, name, type, dns_zone=None):
        LOG.debug("Deleting fake DNS entry for hostname %s" % name)
        ENTRIES.pop(name, None)


class FakeDnsInstanceEntryFactory(driver.DnsInstanceEntryFactory):

    def create_entry(self, instance_id):
        # Construct hostname using pig-latin.
        hostname = "%s-lay" % instance_id
        LOG.debug("Mapping instance_id %s to hostname %s"
                  % (instance_id, hostname))
        return driver.DnsEntry(name=hostname, content=None,
                               type="A", ttl=42, dns_zone=None)


class FakeDnsChecker(object):
    """Used by tests to make sure a DNS record was written in fake mode."""

    def __call__(self, mgmt_instance):
        """
        Given an instance ID and ip address, confirm that the proper DNS
        record was stored in Designate or some other DNS system.
        """
        entry = FakeDnsInstanceEntryFactory().create_entry(mgmt_instance.id)
        # Confirm DNS entry shown to user is what we expect.
        assert_equal(entry.name, mgmt_instance.hostname)
        hostname = entry.name
        for i in ENTRIES:
            print(i)
            print("\t%s" % ENTRIES[i])
        assert_true(hostname in ENTRIES,
                    "Hostname %s not found in DNS entries!" % hostname)
        entry = ENTRIES[hostname]
        # See if the ip address assigned to the record is what we expect.
        # This isn't perfect, but for Fake Mode its good enough. If we
        # really want to know exactly what it should be then we should restore
        # the ability to return the IP from the API as well as a hostname,
        # since that lines up to the DnsEntry's content field.
        ip_addresses = mgmt_instance.server['addresses']
        for network_name, ip_list in ip_addresses.items():
            for ip in ip_list:
                if entry.content == ip['addr']:
                    return
        fail("Couldn't find IP address %s among these values: %s"
             % (entry.content, ip_addresses))

########NEW FILE########
__FILENAME__ = guestagent
# Copyright 2010-2012 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from trove.openstack.common import log as logging
import time
import re

import eventlet
from trove.common import exception as rd_exception
from trove.common import instance as rd_instance
from trove.tests.util import unquote_user_host

DB = {}
LOG = logging.getLogger(__name__)


class FakeGuest(object):

    def __init__(self, id):
        self.id = id
        self.users = {}
        self.dbs = {}
        self.root_was_enabled = False
        self.version = 1
        self.grants = {}
        self.overrides = {}

        # Our default admin user.
        self._create_user({
            "_name": "os_admin",
            "_host": "%",
            "_password": "12345",
            "_databases": [],
        })

    def get_hwinfo(self):
        return {'mem_total': 524288, 'num_cpus': 1}

    def get_diagnostics(self):
        return {
            'version': str(self.version),
            'fd_size': 64,
            'vm_size': 29096,
            'vm_peak': 29160,
            'vm_rss': 2872,
            'vm_hwm': 2872,
            'threads': 2
        }

    def update_guest(self):
        LOG.debug("Updating guest %s" % self.id)
        self.version += 1

    def _check_username(self, username):
        unsupported_chars = re.compile("^\s|\s$|'|\"|;|`|,|/|\\\\")
        if (not username or
                unsupported_chars.search(username) or
                ("%r" % username).find("\\") != -1):
            raise ValueError("'%s' is not a valid user name." % username)
        if len(username) > 16:
            raise ValueError("User name '%s' is too long. Max length = 16" %
                             username)

    def change_passwords(self, users):
        for user in users:
            # Use the model to check validity.
            username = user['name']
            self._check_username(username)
            hostname = user['host']
            password = user['password']
            if (username, hostname) not in self.users:
                raise rd_exception.UserNotFound(
                    "User %s@%s cannot be found on the instance."
                    % (username, hostname))
            self.users[(username, hostname)]['password'] = password

    def update_attributes(self, username, hostname, user_attrs):
        LOG.debug("Updating attributes")
        self._check_username(username)
        if (username, hostname) not in self.users:
                raise rd_exception.UserNotFound(
                    "User %s@%s cannot be found on the instance."
                    % (username, hostname))
        new_name = user_attrs.get('name')
        new_host = user_attrs.get('host')
        new_password = user_attrs.get('password')
        old_name = username
        old_host = hostname
        name = new_name or old_name
        host = new_host or old_host
        if new_name or new_host:
            old_grants = self.grants.get((old_name, old_host), set())
            self._create_user({
                "_name": name,
                "_host": host,
                "_password": self.users[(old_name, host)]['_password'],
                "_databases": [],
            })
            self.grants[(name, host)] = old_grants
            del self.users[(old_name, old_host)]
        if new_password:
            self.users[(name, host)]['_password'] = new_password

    def create_database(self, databases):
        for db in databases:
            self.dbs[db['_name']] = db

    def create_user(self, users):
        for user in users:
            self._create_user(user)

    def _create_user(self, user):
        username = user['_name']
        self._check_username(username)
        hostname = user['_host']
        if hostname is None:
            hostname = '%'
        self.users[(username, hostname)] = user
        print("CREATING %s @ %s" % (username, hostname))
        databases = [db['_name'] for db in user['_databases']]
        self.grant_access(username, hostname, databases)
        return user

    def delete_database(self, database):
        if database['_name'] in self.dbs:
            del self.dbs[database['_name']]

    def delete_queue(self):
        pass

    def enable_root(self):
        self.root_was_enabled = True
        return self._create_user({
            "_name": "root",
            "_host": "%",
            "_password": "12345",
            "_databases": [],
        })

    def delete_user(self, user):
        username = user['_name']
        self._check_username(username)
        hostname = user['_host']
        self.grants[(username, hostname)] = set()
        if (username, hostname) in self.users:
            del self.users[(username, hostname)]

    def is_root_enabled(self):
        return self.root_was_enabled

    def _list_resource(self, resource, limit=None, marker=None,
                       include_marker=False):
        names = sorted([name for name in resource])
        if marker in names:
            if not include_marker:
                # Cut off everything left of and including the marker item.
                names = names[names.index(marker) + 1:]
            else:
                names = names[names.index(marker):]
        next_marker = None
        if limit:
            if len(names) > limit:
                next_marker = names[limit - 1]
            names = names[:limit]
        return [resource[name] for name in names], next_marker

    def list_databases(self, limit=None, marker=None, include_marker=False):
        return self._list_resource(self.dbs, limit, marker, include_marker)

    def list_users(self, limit=None, marker=None, include_marker=False):
        # The markers for users are a composite of the username and hostname.
        names = sorted(["%s@%s" % (name, host) for (name, host) in self.users])
        if marker in names:
            if not include_marker:
                # Cut off everything left of and including the marker item.
                names = names[names.index(marker) + 1:]
            else:
                names = names[names.index(marker):]
        next_marker = None
        if limit:
            if len(names) > limit:
                next_marker = names[limit - 1]
            names = names[:limit]
        return ([self.users[unquote_user_host(userhost)]
                 for userhost in names], next_marker)

    def get_user(self, username, hostname):
        self._check_username(username)
        for (u, h) in self.users:
            print("%r @ %r" % (u, h))
        if (username, hostname) not in self.users:
            raise rd_exception.UserNotFound(
                "User %s@%s cannot be found on the instance."
                % (username, hostname))
        return self.users.get((username, hostname), None)

    def prepare(self, memory_mb, packages, databases, users, device_path=None,
                mount_point=None, backup_info=None, config_contents=None,
                root_password=None, overrides=None):
        from trove.instance.models import DBInstance
        from trove.instance.models import InstanceServiceStatus
        from trove.guestagent.models import AgentHeartBeat
        LOG.debug("users... %s" % users)
        LOG.debug("databases... %s" % databases)
        instance_name = DBInstance.find_by(id=self.id).name
        self.create_user(users)
        self.create_database(databases)
        self.overrides = overrides or {}

        def update_db():
            status = InstanceServiceStatus.find_by(instance_id=self.id)
            if instance_name.endswith('GUEST_ERROR'):
                status.status = rd_instance.ServiceStatuses.FAILED
            else:
                status.status = rd_instance.ServiceStatuses.RUNNING
            status.save()
            AgentHeartBeat.create(instance_id=self.id)
        eventlet.spawn_after(1.0, update_db)

    def _set_status(self, new_status='RUNNING'):
        from trove.instance.models import InstanceServiceStatus
        print("Setting status to %s" % new_status)
        states = {'RUNNING': rd_instance.ServiceStatuses.RUNNING,
                  'SHUTDOWN': rd_instance.ServiceStatuses.SHUTDOWN,
                  }
        status = InstanceServiceStatus.find_by(instance_id=self.id)
        status.status = states[new_status]
        status.save()

    def restart(self):
        # All this does is restart, and shut off the status updates while it
        # does so. So there's actually nothing to do to fake this out except
        # take a nap.
        print("Sleeping for a second.")
        time.sleep(1)
        self._set_status('RUNNING')

    def reset_configuration(self, config):
        # There's nothing to do here, since there is no config to update.
        pass

    def start_db_with_conf_changes(self, config_contents):
        time.sleep(2)
        self._set_status('RUNNING')

    def stop_db(self, do_not_start_on_reboot=False):
        self._set_status('SHUTDOWN')

    def get_volume_info(self):
        """Return used and total volume filesystem information in GB."""
        return {'used': 0.16, 'total': 4.0}

    def grant_access(self, username, hostname, databases):
        """Add a database to a users's grant list."""
        if (username, hostname) not in self.users:
            raise rd_exception.UserNotFound(
                "User %s cannot be found on the instance." % username)
        current_grants = self.grants.get((username, hostname), set())
        for db in databases:
            current_grants.add(db)
        self.grants[(username, hostname)] = current_grants

    def revoke_access(self, username, hostname, database):
        """Remove a database from a users's grant list."""
        if (username, hostname) not in self.users:
            raise rd_exception.UserNotFound(
                "User %s cannot be found on the instance." % username)
        if database not in self.grants.get((username, hostname), set()):
            raise rd_exception.DatabaseNotFound(
                "Database %s cannot be found on the instance." % database)
        current_grants = self.grants.get((username, hostname), set())
        if database in current_grants:
            current_grants.remove(database)
        self.grants[(username, hostname)] = current_grants

    def list_access(self, username, hostname):
        if (username, hostname) not in self.users:
            raise rd_exception.UserNotFound(
                "User %s cannot be found on the instance." % username)
        current_grants = self.grants.get((username, hostname), set())
        dbs = [{'_name': db,
                '_collate': '',
                '_character_set': '',
                } for db in current_grants]
        return dbs

    def create_backup(self, backup_info):
        from trove.backup.models import Backup, BackupState
        backup = Backup.get_by_id(context=None,
                                  backup_id=backup_info['id'])

        def finish_create_backup():
            backup.state = BackupState.COMPLETED
            backup.location = 'http://localhost/path/to/backup'
            backup.checksum = 'fake-md5-sum'
            backup.save()
        eventlet.spawn_after(1.0, finish_create_backup)

    def mount_volume(self, device_path=None, mount_point=None):
        pass

    def unmount_volume(self, device_path=None, mount_point=None):
        pass

    def resize_fs(self, device_path=None, mount_point=None):
        pass

    def update_overrides(self, overrides, remove=False):
        self.overrides = overrides

    def apply_overrides(self, overrides):
        self.overrides = overrides


def get_or_create(id):
    if id not in DB:
        DB[id] = FakeGuest(id)
    return DB[id]


def fake_create_guest_client(context, id):
    return get_or_create(id)

########NEW FILE########
__FILENAME__ = keystone
# Copyright 2010-2012 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


class AuthProtocol(object):

    def __init__(self, app, conf):
        self.conf = conf
        self.app = app

    def __call__(self, env, start_response):
        token = self._get_user_token_from_header(env)
        user_headers = self._get_info_from_token(token)
        self._add_headers(env, user_headers)
        return self.app(env, start_response)

    def _header_to_env_var(self, key):
        """Convert header to wsgi env variable.

        :param key: http header name (ex. 'X-Auth-Token')
        :return wsgi env variable name (ex. 'HTTP_X_AUTH_TOKEN')

        """
        return 'HTTP_%s' % key.replace('-', '_').upper()

    def _add_headers(self, env, headers):
        """Add http headers to environment."""
        for (k, v) in headers.iteritems():
            env_key = self._header_to_env_var(k)
            env[env_key] = v

    def get_admin_token(self):
        return "ABCDEF0123456789"

    def _get_info_from_token(self, token):
        if token.startswith("admin"):
            role = "admin,%s" % token
        else:
            role = token
        return {
            'X_IDENTITY_STATUS': 'Confirmed',
            'X_TENANT_ID': token,
            'X_TENANT_NAME': token,
            'X_USER_ID': token,
            'X_USER_NAME': token,
            'X_ROLE': role,
        }

    def _get_header(self, env, key, default=None):
        # Copied from keystone.
        env_key = self._header_to_env_var(key)
        return env.get(env_key, default)

    def _get_user_token_from_header(self, env):
        token = self._get_header(env, 'X-Auth-Token',
                                 self._get_header(env, 'X-Storage-Token'))
        if token:
            return token
        else:
            raise RuntimeError('Unable to find token in headers')


def filter_factory(global_conf, **local_conf):
    """Fakes a keystone filter."""
    conf = global_conf.copy()
    conf.update(local_conf)

    def auth_filter(app):
        return AuthProtocol(app, conf)
    return auth_filter

########NEW FILE########
__FILENAME__ = nova
# Copyright 2010-2012 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from novaclient import exceptions as nova_exceptions
from trove.common.exception import PollTimeOut
from trove.common import instance as rd_instance
from trove.openstack.common import log as logging
from trove.tests.fakes.common import authorize

import eventlet
import uuid

LOG = logging.getLogger(__name__)
FAKE_HOSTS = ["fake_host_1", "fake_host_2"]


class FakeFlavor(object):

    def __init__(self, id, disk, name, ram, ephemeral=0, vcpus=10):
        self.id = id
        self.disk = disk
        self.name = name
        self.ram = ram
        self.vcpus = vcpus
        self.ephemeral = ephemeral

    @property
    def links(self):
        url = ("http://localhost:8774/v2/5064d71eb09c47e1956cf579822bae9a/"
               "flavors/%s") % self.id
        return [{"href": url, "rel": link_type}
                for link_type in ['self', 'bookmark']]

    @property
    def href_suffix(self):
        return "flavors/%s" % self.id


class FakeFlavors(object):

    def __init__(self):
        self.db = {}
        self._add(1, 0, "m1.tiny", 512)
        self._add(2, 20, "m1.small", 2048)
        self._add(3, 40, "m1.medium", 4096)
        self._add(4, 80, "m1.large", 8192)
        self._add(5, 160, "m1.xlarge", 16384)
        self._add(6, 0, "m1.nano", 64)
        self._add(7, 0, "m1.micro", 128)
        self._add(8, 2, "m1.rd-smaller", 768)
        self._add(9, 10, "tinier", 506)
        self._add(10, 2, "m1.rd-tiny", 512)
        self._add(11, 0, "eph.rd-tiny", 512, 1)
        self._add(12, 20, "eph.rd-smaller", 768, 2)

    def _add(self, *args, **kwargs):
        new_flavor = FakeFlavor(*args, **kwargs)
        self.db[new_flavor.id] = new_flavor

    def get(self, id):
        id = int(id)
        if id not in self.db:
            raise nova_exceptions.NotFound(404, "Flavor id not found %s" % id)
        return self.db[id]

    def get_by_href(self, href):
        for id in self.db:
            value = self.db[id]
            # Use inexact match since faking the exact endpoints would be
            # difficult.
            if href.endswith(value.href_suffix):
                return value
        raise nova_exceptions.NotFound(404, "Flavor href not found %s" % href)

    def list(self):
        return [self.get(id) for id in self.db]


class FakeServer(object):

    next_local_id = 0

    def __init__(self, parent, owner, id, name, image_id, flavor_ref,
                 block_device_mapping, volumes):
        self.owner = owner  # This is a context.
        self.id = id
        self.parent = parent
        self.name = name
        self.image_id = image_id
        self.flavor_ref = flavor_ref
        self.old_flavor_ref = None
        self._current_status = "BUILD"
        self.volumes = volumes
        # This is used by "RdServers". Its easier to compute the
        # fake value in this class's initializer.
        self._local_id = self.next_local_id
        self.next_local_id += 1
        info_vols = []
        for volume in self.volumes:
            info_vols.append({'id': volume.id})
            volume.set_attachment(id)
            volume.schedule_status("in-use", 1)
        self.host = FAKE_HOSTS[0]
        self.old_host = None
        setattr(self, 'OS-EXT-AZ:availability_zone', 'nova')

        self._info = {'os:volumes': info_vols}

    @property
    def addresses(self):
        return {"private": [{"addr": "123.123.123.123"}]}

    def confirm_resize(self):
        if self.status != "VERIFY_RESIZE":
            raise RuntimeError("Not in resize confirm mode.")
        self._current_status = "ACTIVE"

    def revert_resize(self):
        if self.status != "VERIFY_RESIZE":
            raise RuntimeError("Not in resize confirm mode.")
        self.host = self.old_host
        self.old_host = None
        self.flavor_ref = self.old_flavor_ref
        self.old_flavor_ref = None
        self._current_status = "ACTIVE"

    def reboot(self):
        LOG.debug("Rebooting server %s" % (self.id))

        def set_to_active():
            self._current_status = "ACTIVE"
            self.parent.schedule_simulate_running_server(self.id, 1.5)

        self._current_status = "REBOOT"
        eventlet.spawn_after(1, set_to_active)

    def delete(self):
        self.schedule_status = []
        # TODO(pdmars): This is less than ideal, but a quick way to force it
        # into the error state before scheduling the delete.
        if (self.name.endswith("_ERROR_ON_DELETE") and
                self._current_status != "SHUTDOWN"):
            # Fail to delete properly the first time, just set the status
            # to SHUTDOWN and break. It's important that we only fail to delete
            # once in fake mode.
            self._current_status = "SHUTDOWN"
            return
        self._current_status = "SHUTDOWN"
        self.parent.schedule_delete(self.id, 1.5)

    @property
    def flavor(self):
        return FLAVORS.get_by_href(self.flavor_ref).__dict__

    @property
    def links(self):
        url = "https://localhost:9999/v1.0/1234/instances/%s" % self.id
        return [{"href": url, "rel": link_type}
                for link_type in ['self', 'bookmark']]

    def migrate(self, force_host=None):
        self.resize(None, force_host)

    def resize(self, new_flavor_id=None, force_host=None):
        self._current_status = "RESIZE"
        if self.name.endswith("_RESIZE_TIMEOUT"):
            raise PollTimeOut()

        def set_to_confirm_mode():
            self._current_status = "VERIFY_RESIZE"

            def set_to_active():
                self.parent.schedule_simulate_running_server(self.id, 1.5)
            eventlet.spawn_after(1, set_to_active)

        def change_host():
            self.old_host = self.host
            if not force_host:
                self.host = [host for host in FAKE_HOSTS
                             if host != self.host][0]
            else:
                self.host = force_host

        def set_flavor():
            if self.name.endswith("_RESIZE_ERROR"):
                self._current_status = "ACTIVE"
                return
            if new_flavor_id is None:
                # Migrations are flavorless flavor resizes.
                # A resize MIGHT change the host, but a migrate
                # deliberately does.
                LOG.debug("Migrating fake instance.")
                eventlet.spawn_after(0.75, change_host)
            else:
                LOG.debug("Resizing fake instance.")
                self.old_flavor_ref = self.flavor_ref
                flavor = self.parent.flavors.get(new_flavor_id)
                self.flavor_ref = flavor.links[0]['href']
            eventlet.spawn_after(1, set_to_confirm_mode)

        eventlet.spawn_after(0.8, set_flavor)

    def schedule_status(self, new_status, time_from_now):
        """Makes a new status take effect at the given time."""
        def set_status():
            self._current_status = new_status
        eventlet.spawn_after(time_from_now, set_status)

    @property
    def status(self):
        return self._current_status

    @property
    def created(self):
        return "2012-01-25T21:55:51Z"

    @property
    def updated(self):
        return "2012-01-25T21:55:51Z"

    @property
    def tenant(self):   # This is on the RdServer extension type.
        return self.owner.tenant

    @property
    def tenant_id(self):
        return self.owner.tenant


# The global var contains the servers dictionary in use for the life of these
# tests.
FAKE_SERVERS_DB = {}


class FakeServers(object):

    def __init__(self, context, flavors):
        self.context = context
        self.db = FAKE_SERVERS_DB
        self.flavors = flavors

    def can_see(self, id):
        """Can this FakeServers, with its context, see some resource?"""
        server = self.db[id]
        return (self.context.is_admin or
                server.owner.tenant == self.context.tenant)

    def create(self, name, image_id, flavor_ref, files=None, userdata=None,
               block_device_mapping=None, volume=None, security_groups=None,
               availability_zone=None, nics=None):
        id = "FAKE_%s" % uuid.uuid4()
        if volume:
            volume = self.volumes.create(volume['size'], volume['name'],
                                         volume['description'])
            while volume.status == "BUILD":
                eventlet.sleep(0.1)
            if volume.status != "available":
                LOG.info(_("volume status = %s") % volume.status)
                raise nova_exceptions.ClientException("Volume was bad!")
            mapping = "%s::%s:%s" % (volume.id, volume.size, 1)
            block_device_mapping = {'vdb': mapping}
            volumes = [volume]
            LOG.debug("Fake Volume Create %(volumeid)s with "
                      "status %(volumestatus)s" %
                      {'volumeid': volume.id, 'volumestatus': volume.status})
        else:
            volumes = self._get_volumes_from_bdm(block_device_mapping)
            for volume in volumes:
                volume.schedule_status('in-use', 1)
        server = FakeServer(self, self.context, id, name, image_id, flavor_ref,
                            block_device_mapping, volumes)
        self.db[id] = server
        if name.endswith('SERVER_ERROR'):
            raise nova_exceptions.ClientException("Fake server create error.")

        if availability_zone == 'BAD_ZONE':
            raise nova_exceptions.ClientException("The requested availability "
                                                  "zone is not available.")

        if nics is not None and nics.port_id == 'UNKNOWN':
            raise nova_exceptions.ClientException("The requested availability "
                                                  "zone is not available.")

        server.schedule_status("ACTIVE", 1)
        LOG.info(_("FAKE_SERVERS_DB : %s") % str(FAKE_SERVERS_DB))
        return server

    def _get_volumes_from_bdm(self, block_device_mapping):
        volumes = []
        if block_device_mapping is not None:
            # block_device_mapping is a dictionary, where the key is the
            # device name on the compute instance and the mapping info is a
            # set of fields in a string, separated by colons.
            # For each device, find the volume, and record the mapping info
            # to another fake object and attach it to the volume
            # so that the fake API can later retrieve this.
            for device in block_device_mapping:
                mapping = block_device_mapping[device]
                (id, _type, size, delete_on_terminate) = mapping.split(":")
                volume = self.volumes.get(id)
                volume.mapping = FakeBlockDeviceMappingInfo(
                    id, device, _type, size, delete_on_terminate)
                volumes.append(volume)
        return volumes

    def get(self, id):
        if id not in self.db:
            LOG.error(_("Couldn't find server id %(id)s, collection=%(db)s") %
                      {'id': id, 'db': self.db})
            raise nova_exceptions.NotFound(404, "Not found")
        else:
            if self.can_see(id):
                return self.db[id]
            else:
                raise nova_exceptions.NotFound(404, "Bad permissions")

    def get_server_volumes(self, server_id):
        """Fake method we've added to grab servers from the volume."""
        return [volume.mapping
                for volume in self.get(server_id).volumes
                if volume.mapping is not None]

    def list(self):
        return [v for (k, v) in self.db.items() if self.can_see(v.id)]

    def schedule_delete(self, id, time_from_now):
        def delete_server():
            LOG.info(_("Simulated event ended, deleting server %s.") % id)
            del self.db[id]
        eventlet.spawn_after(time_from_now, delete_server)

    def schedule_simulate_running_server(self, id, time_from_now):
        from trove.instance.models import DBInstance
        from trove.instance.models import InstanceServiceStatus

        def set_server_running():
            instance = DBInstance.find_by(compute_instance_id=id)
            LOG.debug("Setting server %s to running" % instance.id)
            status = InstanceServiceStatus.find_by(instance_id=instance.id)
            status.status = rd_instance.ServiceStatuses.RUNNING
            status.save()
        eventlet.spawn_after(time_from_now, set_server_running)


class FakeRdServer(object):

    def __init__(self, server):
        self.server = server
        self.deleted = False
        self.deleted_at = None  # Not sure how to simulate "True" for this.
        self.local_id = server._local_id

    def __getattr__(self, name):
        return getattr(self.server, name)


class FakeRdServers(object):

    def __init__(self, servers):
        self.servers = servers

    def get(self, id):
        return FakeRdServer(self.servers.get(id))

    def list(self):
        # Attach the extra Rd Server stuff to the normal server.
        return [FakeRdServer(server) for server in self.servers.list()]


class FakeServerVolumes(object):

    def __init__(self, context):
        self.context = context

    def get_server_volumes(self, server_id):
        class ServerVolumes(object):
            def __init__(self, block_device_mapping):
                LOG.debug("block_device_mapping = %s" %
                          block_device_mapping)
                device = block_device_mapping['vdb']
                (self.volumeId,
                    self.type,
                    self.size,
                    self.delete_on_terminate) = device.split(":")
        fake_servers = FakeServers(self.context, FLAVORS)
        server = fake_servers.get(server_id)
        return [ServerVolumes(server.block_device_mapping)]


class FakeVolume(object):

    def __init__(self, parent, owner, id, size, name,
                 description):
        self.attachments = []
        self.parent = parent
        self.owner = owner  # This is a context.
        self.id = id
        self.size = size
        self.name = name
        self.description = description
        self._current_status = "BUILD"
        # For some reason we grab this thing from device then call it mount
        # point.
        self.device = "vdb"

    def __repr__(self):
        msg = ("FakeVolume(id=%s, size=%s, name=%s, "
               "description=%s, _current_status=%s)")
        params = (self.id, self.size, self.name,
                  self.description, self._current_status)
        return (msg % params)

    @property
    def availability_zone(self):
        return "fake-availability-zone"

    @property
    def created_at(self):
        return "2001-01-01-12:30:30"

    def get(self, key):
        return getattr(self, key)

    def schedule_status(self, new_status, time_from_now):
        """Makes a new status take effect at the given time."""
        def set_status():
            self._current_status = new_status
        eventlet.spawn_after(time_from_now, set_status)

    def set_attachment(self, server_id):
        """Fake method we've added to set attachments. Idempotent."""
        for attachment in self.attachments:
            if attachment['server_id'] == server_id:
                return  # Do nothing
        self.attachments.append({'server_id': server_id,
                                 'device': self.device})

    @property
    def status(self):
        return self._current_status


class FakeBlockDeviceMappingInfo(object):

    def __init__(self, id, device, _type, size, delete_on_terminate):
        self.volumeId = id
        self.device = device
        self.type = _type
        self.size = size
        self.delete_on_terminate = delete_on_terminate


FAKE_VOLUMES_DB = {}


class FakeVolumes(object):

    def __init__(self, context):
        self.context = context
        self.db = FAKE_VOLUMES_DB

    def can_see(self, id):
        """Can this FakeVolumes, with its context, see some resource?"""
        server = self.db[id]
        return (self.context.is_admin or
                server.owner.tenant == self.context.tenant)

    def get(self, id):
        if id not in self.db:
            LOG.error(_("Couldn't find volume id %(id)s, collection=%(db)s") %
                      {'id': id, 'db': self.db})
            raise nova_exceptions.NotFound(404, "Not found")
        else:
            if self.can_see(id):
                return self.db[id]
            else:
                raise nova_exceptions.NotFound(404, "Bad permissions")

    def create(self, size, name=None, description=None):
        id = "FAKE_VOL_%s" % uuid.uuid4()
        volume = FakeVolume(self, self.context, id, size, name,
                            description)
        self.db[id] = volume
        if size == 9:
            volume.schedule_status("error", 2)
        elif size == 13:
            raise Exception("No volume for you!")
        else:
            volume.schedule_status("available", 2)
        LOG.debug("Fake volume created %(volumeid)s with "
                  "status %(volumestatus)s" %
                  {'volumeid': volume.id, 'volumestatus': volume.status})
        LOG.info("FAKE_VOLUMES_DB : %s" % FAKE_VOLUMES_DB)
        return volume

    def list(self, detailed=True):
        return [self.db[key] for key in self.db]

    def extend(self, volume_id, new_size):
        LOG.debug("Resize volume id (%(volumeid)s) to size (%(size)s)" %
                  {'volumeid': volume_id, 'size': new_size})
        volume = self.get(volume_id)

        if volume._current_status != 'available':
            raise Exception("Invalid volume status: "
                            "expected 'in-use' but was '%s'" %
                            volume._current_status)

        def finish_resize():
            volume.size = new_size
        eventlet.spawn_after(1.0, finish_resize)

    def delete_server_volume(self, server_id, volume_id):
        volume = self.get(volume_id)

        if volume._current_status != 'in-use':
            raise Exception("Invalid volume status: "
                            "expected 'in-use' but was '%s'" %
                            volume._current_status)

        def finish_detach():
            volume._current_status = "available"
        eventlet.spawn_after(1.0, finish_detach)

    def create_server_volume(self, server_id, volume_id, device_path):
        volume = self.get(volume_id)

        if volume._current_status != "available":
            raise Exception("Invalid volume status: "
                            "expected 'available' but was '%s'" %
                            volume._current_status)

        def finish_attach():
            volume._current_status = "in-use"
        eventlet.spawn_after(1.0, finish_attach)


class FakeAccount(object):

    def __init__(self, id, servers):
        self.id = id
        self.servers = self._servers_to_dict(servers)

    def _servers_to_dict(self, servers):
        ret = []
        for server in servers:
            server_dict = {}
            server_dict['id'] = server.id
            server_dict['name'] = server.name
            server_dict['status'] = server.status
            server_dict['host'] = server.host
            ret.append(server_dict)
        return ret


class FakeAccounts(object):

    def __init__(self, context, servers):

        self.context = context
        self.db = FAKE_SERVERS_DB
        self.servers = servers

    def _belongs_to_tenant(self, tenant, id):
        server = self.db[id]
        return server.tenant == tenant

    def get_instances(self, id):
        authorize(self.context)
        servers = [v for (k, v) in self.db.items()
                   if self._belongs_to_tenant(id, v.id)]
        return FakeAccount(id, servers)


FLAVORS = FakeFlavors()


class FakeHost(object):

    def __init__(self, name, servers):
        self.name = name
        self.servers = servers
        self.instances = []
        self.percentUsed = 0
        self.totalRAM = 0
        self.usedRAM = 0

    @property
    def instanceCount(self):
        return len(self.instances)

    def recalc(self):
        """
        This fake-mode exclusive method recalculates the fake data this
        object passes back.
        """
        self.instances = []
        self.percentUsed = 0
        self.totalRAM = 2004  # 16384
        self.usedRAM = 0
        for server in self.servers.list():
            print(server)
            if server.host != self.name:
                print("\t...not on this host.")
                continue
            self.instances.append({
                'uuid': server.id,
                'name': server.name,
                'status': server.status
            })
            try:
                flavor = FLAVORS.get(server.flavor_ref)
            except ValueError:
                # Maybe flavor_ref isn't an int?
                flavor = FLAVORS.get_by_href(server.flavor_ref)
            ram = flavor.ram
            self.usedRAM += ram
        decimal = float(self.usedRAM) / float(self.totalRAM)
        self.percentUsed = int(decimal * 100)


class FakeHosts(object):

    def __init__(self, servers):
        self.hosts = {}
        for host in FAKE_HOSTS:
            self.add_host(FakeHost(host, servers))

    def add_host(self, host):
        self.hosts[host.name] = host
        return host

    def get(self, name):
        try:
            self.hosts[name].recalc()
            return self.hosts[name]
        except KeyError:
            raise nova_exceptions.NotFound(404, "Host not found %s" % name)

    def list(self):
        for name in self.hosts:
            self.hosts[name].recalc()
        return [self.hosts[name] for name in self.hosts]


class FakeRdStorage(object):

    def __init__(self, name):
        self.name = name
        self.type = ""
        self.used = 0
        self.capacity = {}
        self.provision = {}

    def recalc(self):
        self.type = "test_type"
        self.used = 10
        self.capacity['total'] = 100
        self.capacity['available'] = 90
        self.provision['total'] = 50
        self.provision['available'] = 40
        self.provision['percent'] = 10


class FakeRdStorages(object):

    def __init__(self):
        self.storages = {}
        self.add_storage(FakeRdStorage("fake_storage"))

    def add_storage(self, storage):
        self.storages[storage.name] = storage
        return storage

    def list(self):
        for name in self.storages:
            self.storages[name].recalc()
        return [self.storages[name] for name in self.storages]


class FakeSecurityGroup(object):

    def __init__(self, name=None, description=None, context=None):
        self.name = name
        self.description = description
        self.id = "FAKE_SECGRP_%s" % uuid.uuid4()
        self.rules = {}

    def get_id(self):
        return self.id

    def add_rule(self, fakeSecGroupRule):
        self.rules.append(fakeSecGroupRule)
        return self.rules

    def get_rules(self):
        result = ""
        for rule in self.rules:
            result = result + rule.data()
        return result

    def data(self):
        return {
            'id': self.id,
            'name': self.name,
            'description': self.description
        }


class FakeSecurityGroups(object):

    def __init__(self, context=None):
        self.context = context
        self.securityGroups = {}

    def create(self, name=None, description=None):
        secGrp = FakeSecurityGroup(name, description)
        self.securityGroups[secGrp.get_id()] = secGrp
        return secGrp

    def list(self):
        pass


class FakeSecurityGroupRule(object):

    def __init__(self, ip_protocol=None, from_port=None, to_port=None,
                 cidr=None, parent_group_id=None, context=None):
        self.group_id = parent_group_id
        self.protocol = ip_protocol
        self.from_port = from_port
        self.to_port = to_port
        self.cidr = cidr
        self.context = context
        self.id = "FAKE_SECGRP_RULE_%s" % uuid.uuid4()

    def get_id(self):
        return self.id

    def data(self):
        return {
            'id': self.id,
            'group_id': self.group_id,
            'protocol': self.protocol,
            'from_port': self.from_port,
            'to_port': self.to_port,
            'cidr': self.cidr
        }


class FakeSecurityGroupRules(object):

    def __init__(self, context=None):
        self.context = context
        self.securityGroupRules = {}

    def create(self, parent_group_id, ip_protocol, from_port, to_port, cidr):
        secGrpRule = FakeSecurityGroupRule(ip_protocol, from_port, to_port,
                                           cidr, parent_group_id)
        self.securityGroupRules[secGrpRule.get_id()] = secGrpRule
        return secGrpRule

    def delete(self, id):
        if id in self.securityGroupRules:
            del self.securityGroupRules[id]


class FakeClient(object):

    def __init__(self, context):
        self.context = context
        self.flavors = FLAVORS
        self.servers = FakeServers(context, self.flavors)
        self.volumes = FakeVolumes(context)
        self.servers.volumes = self.volumes
        self.accounts = FakeAccounts(context, self.servers)
        self.rdhosts = FakeHosts(self.servers)
        self.rdstorage = FakeRdStorages()
        self.rdservers = FakeRdServers(self.servers)
        self.security_groups = FakeSecurityGroups(context)
        self.security_group_rules = FakeSecurityGroupRules(context)

    def get_server_volumes(self, server_id):
        return self.servers.get_server_volumes(server_id)

    def rescan_server_volume(self, server, volume_id):
        LOG.info("FAKE rescanning volume.")


CLIENT_DATA = {}


def get_client_data(context):
    if context not in CLIENT_DATA:
        nova_client = FakeClient(context)
        volume_client = FakeClient(context)
        volume_client.servers = nova_client
        CLIENT_DATA[context] = {
            'nova': nova_client,
            'volume': volume_client
        }
    return CLIENT_DATA[context]


def fake_create_nova_client(context):
    return get_client_data(context)['nova']


def fake_create_nova_volume_client(context):
    return get_client_data(context)['volume']


def fake_create_cinder_client(context):
    return get_client_data(context)['volume']

########NEW FILE########
__FILENAME__ = swift
# Copyright (C) 2012 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import httplib
import json
import os
import socket
import uuid
import logging
import swiftclient.client as swift_client
import swiftclient
from hashlib import md5
from mock import MagicMock

from swiftclient import client as swift

from trove.openstack.common.gettextutils import _  # noqa

LOG = logging.getLogger(__name__)


class FakeSwiftClient(object):
    """Logs calls instead of executing."""
    def __init__(self, *args, **kwargs):
        pass

    @classmethod
    def Connection(self, *args, **kargs):
        LOG.debug("fake FakeSwiftClient Connection")
        return FakeSwiftConnection()


class FakeSwiftConnection(object):
    """Logging calls instead of executing"""
    MANIFEST_HEADER_KEY = 'X-Object-Manifest'
    url = 'http://mockswift/v1'

    def __init__(self, *args, **kwargs):
        self.manifest_prefix = None
        self.manifest_name = None
        self.container_objects = {}

    def get_auth(self):
        return (
            u"http://127.0.0.1:8080/v1/AUTH_c7b038976df24d96bf1980f5da17bd89",
            u'MIINrwYJKoZIhvcNAQcCoIINoDCCDZwCAQExCTAHBgUrDgMCGjCCDIgGCSqGSIb3'
            u'DQEHAaCCDHkEggx1eyJhY2Nlc3MiOiB7InRva2VuIjogeyJpc3N1ZWRfYXQiOiAi'
            u'MjAxMy0wMy0xOFQxODoxMzoyMC41OTMyNzYiLCAiZXhwaXJlcyI6ICIyMDEzLTAz'
            u'LTE5VDE4OjEzOjIwWiIsICJpZCI6ICJwbGFjZWhvbGRlciIsICJ0ZW5hbnQiOiB7'
            u'ImVuYWJsZWQiOiB0cnVlLCAiZGVzY3JpcHRpb24iOiBudWxsLCAibmFtZSI6ICJy'
            u'ZWRkd2FyZiIsICJpZCI6ICJjN2IwMzg5NzZkZjI0ZDk2YmYxOTgwZjVkYTE3YmQ4'
            u'OSJ9fSwgInNlcnZpY2VDYXRhbG9nIjogW3siZW5kcG9pbnRzIjogW3siYWRtaW5')

    def get_account(self):
        return ({'content-length': '2', 'accept-ranges': 'bytes',
                 'x-timestamp': '1363049003.92304',
                 'x-trans-id': 'tx9e5da02c49ed496395008309c8032a53',
                 'date': 'Tue, 10 Mar 2013 00:43:23 GMT',
                 'x-account-bytes-used': '0',
                 'x-account-container-count': '0',
                 'content-type': 'application/json; charset=utf-8',
                 'x-account-object-count': '0'}, [])

    def head_container(self, container):
        LOG.debug("fake head_container(%s)" % container)
        if container == 'missing_container':
            raise swift.ClientException('fake exception',
                                        http_status=httplib.NOT_FOUND)
        elif container == 'unauthorized_container':
            raise swift.ClientException('fake exception',
                                        http_status=httplib.UNAUTHORIZED)
        elif container == 'socket_error_on_head':
            raise socket.error(111, 'ECONNREFUSED')
        pass

    def put_container(self, container):
        LOG.debug("fake put_container(%s)" % container)
        pass

    def get_container(self, container, **kwargs):
        LOG.debug("fake get_container(%s)" % container)
        fake_header = None
        fake_body = [{'name': 'backup_001'},
                     {'name': 'backup_002'},
                     {'name': 'backup_003'}]
        return fake_header, fake_body

    def head_object(self, container, name):
        LOG.debug("fake put_container(%(container)s, %(name)s)" %
                  {'container': container, 'name': name})
        checksum = md5()
        if self.manifest_prefix and self.manifest_name == name:
            for object_name in sorted(self.container_objects.iterkeys()):
                object_checksum = md5(self.container_objects[object_name])
                # The manifest file etag for a HEAD or GET is the checksum of
                # the concatenated checksums.
                checksum.update(object_checksum.hexdigest())
            # this is included to test bad swift segment etags
            if name.startswith("bad_manifest_etag_"):
                return {'etag': '"this_is_an_intentional_bad_manifest_etag"'}
        else:
            if name in self.container_objects:
                checksum.update(self.container_objects[name])
            else:
                return {'etag': 'fake-md5-sum'}

        # Currently a swift HEAD object returns etag with double quotes
        return {'etag': '"%s"' % checksum.hexdigest()}

    def get_object(self, container, name, resp_chunk_size=None):
        LOG.debug("fake get_object(%(container)s, %(name)s)" %
                  {'container': container, 'name': name})
        if container == 'socket_error_on_get':
            raise socket.error(111, 'ECONNREFUSED')
        if 'metadata' in name:
            fake_object_header = None
            metadata = {}
            if container == 'unsupported_version':
                metadata['version'] = '9.9.9'
            else:
                metadata['version'] = '1.0.0'
            metadata['backup_id'] = 123
            metadata['volume_id'] = 123
            metadata['backup_name'] = 'fake backup'
            metadata['backup_description'] = 'fake backup description'
            metadata['created_at'] = '2013-02-19 11:20:54,805'
            metadata['objects'] = [{
                'backup_001': {'compression': 'zlib', 'length': 10},
                'backup_002': {'compression': 'zlib', 'length': 10},
                'backup_003': {'compression': 'zlib', 'length': 10}
            }]
            metadata_json = json.dumps(metadata, sort_keys=True, indent=2)
            fake_object_body = metadata_json
            return (fake_object_header, fake_object_body)

        fake_header = {'etag': '"fake-md5-sum"'}
        if resp_chunk_size:
            def _object_info():
                length = 0
                while length < (1024 * 1024):
                    yield os.urandom(resp_chunk_size)
                    length += resp_chunk_size
            fake_object_body = _object_info()
        else:
            fake_object_body = os.urandom(1024 * 1024)
        return (fake_header, fake_object_body)

    def put_object(self, container, name, contents, **kwargs):
        LOG.debug("fake put_object(%(container)s, %(name)s)" %
                  {'container': container, 'name': name})
        if container == 'socket_error_on_put':
            raise socket.error(111, 'ECONNREFUSED')
        headers = kwargs.get('headers', {})
        object_checksum = md5()
        if self.MANIFEST_HEADER_KEY in headers:
            # the manifest prefix format is <container>/<prefix> where
            # container is where the object segments are in and prefix is the
            # common prefix for all segments.
            self.manifest_prefix = headers.get(self.MANIFEST_HEADER_KEY)
            self.manifest_name = name
            object_checksum.update(contents)
        else:
            if hasattr(contents, 'read'):
                chunk_size = 128
                object_content = ""
                chunk = contents.read(chunk_size)
                while chunk:
                    object_content += chunk
                    object_checksum.update(chunk)
                    chunk = contents.read(chunk_size)

                self.container_objects[name] = object_content
            else:
                object_checksum.update(contents)
                self.container_objects[name] = contents

            # this is included to test bad swift segment etags
            if name.startswith("bad_segment_etag_"):
                return "this_is_an_intentional_bad_segment_etag"
        return object_checksum.hexdigest()

    def post_object(self, container, name, headers={}):
        LOG.debug("fake post_object(%(container)s, %(name)s, %(head)s)" %
                  {'container': container, 'name': name, 'head': str(headers)})

    def delete_object(self, container, name):
        LOG.debug("fake delete_object(%(container)s, %(name)s)" %
                  {'container': container, 'name': name})
        if container == 'socket_error_on_delete':
            raise socket.error(111, 'ECONNREFUSED')
        pass


class SwiftClientStub(object):
    """
    Component for controlling behavior of Swift Client Stub.  Instantiated
    before tests are invoked in "fake" mode.  Invoke methods to control
    behavior so that systems under test can interact with this as it is a
    real swift client with a real backend

    example:

    if FAKE:
        swift_stub = SwiftClientStub()
        swift_stub.with_account('xyz')

    # returns swift account info and auth token
    component_using_swift.get_swift_account()

    if FAKE:
        swift_stub.with_container('test-container-name')

    # returns swift container information - mostly faked
    component_using.swift.create_container('test-container-name')
    component_using_swift.get_container_info('test-container-name')

    if FAKE:
        swift_stub.with_object('test-container-name', 'test-object-name',
            'test-object-contents')

    # returns swift object info and contents
    component_using_swift.create_object('test-container-name',
        'test-object-name', 'test-contents')
    component_using_swift.get_object('test-container-name', 'test-object-name')

    if FAKE:
        swift_stub.without_object('test-container-name', 'test-object-name')

    # allows object to be removed ONCE
    component_using_swift.remove_object('test-container-name',
        'test-object-name')
    # throws ClientException - 404
    component_using_swift.get_object('test-container-name', 'test-object-name')
    component_using_swift.remove_object('test-container-name',
        'test-object-name')

    if FAKE:
        swift_stub.without_object('test-container-name', 'test-object-name')

    # allows container to be removed ONCE
    component_using_swift.remove_container('test-container-name')
    # throws ClientException - 404
    component_using_swift.get_container('test-container-name')
    component_using_swift.remove_container('test-container-name')
    """

    def __init__(self):
        self._connection = swift_client.Connection()
        self._containers = {}
        self._containers_list = []
        self._objects = {}

    def _remove_object(self, name, some_list):
        idx = [i for i, obj in enumerate(some_list) if obj['name'] == name]
        if len(idx) == 1:
            del some_list[idx[0]]

    def _ensure_object_exists(self, container, name):
        self._connection.get_object(container, name)

    def with_account(self, account_id):
        """
        setups up account headers

        example:

        if FAKE:
            swift_stub = SwiftClientStub()
            swift_stub.with_account('xyz')

        # returns swift account info and auth token
        component_using_swift.get_swift_account()

        :param account_id: account id
        """

        def account_resp():
            return ({'content-length': '2', 'accept-ranges': 'bytes',
                     'x-timestamp': '1363049003.92304',
                     'x-trans-id': 'tx9e5da02c49ed496395008309c8032a53',
                     'date': 'Tue, 10 Mar 2013 00:43:23 GMT',
                     'x-account-bytes-used': '0',
                     'x-account-container-count': '0',
                     'content-type': 'application/json; charset=utf-8',
                     'x-account-object-count': '0'}, self._containers_list)

        swift_client.Connection.get_auth = MagicMock(return_value=(
            u"http://127.0.0.1:8080/v1/AUTH_c7b038976df24d96bf1980f5da17bd89",
            u'MIINrwYJKoZIhvcNAQcCoIINoDCCDZwCAQExCTAHBgUrDgMCGjCCDIgGCSqGSIb3'
            u'DQEHAaCCDHkEggx1eyJhY2Nlc3MiOiB7InRva2VuIjogeyJpc3N1ZWRfYXQiOiAi'
            u'MjAxMy0wMy0xOFQxODoxMzoyMC41OTMyNzYiLCAiZXhwaXJlcyI6ICIyMDEzLTAz'
            u'LTE5VDE4OjEzOjIwWiIsICJpZCI6ICJwbGFjZWhvbGRlciIsICJ0ZW5hbnQiOiB7'
            u'ImVuYWJsZWQiOiB0cnVlLCAiZGVzY3JpcHRpb24iOiBudWxsLCAibmFtZSI6ICJy'
            u'ZWRkd2FyZiIsICJpZCI6ICJjN2IwMzg5NzZkZjI0ZDk2YmYxOTgwZjVkYTE3YmQ4'
            u'OSJ9fSwgInNlcnZpY2VDYXRhbG9nIjogW3siZW5kcG9pbnRzIjogW3siYWRtaW5')
        )
        swift_client.Connection.get_account = MagicMock(
            return_value=account_resp())
        return self

    def _create_container(self, container_name):
        container = {'count': 0, 'bytes': 0, 'name': container_name}
        self._containers[container_name] = container
        self._containers_list.append(container)
        self._objects[container_name] = []

    def _ensure_container_exists(self, container):
        self._connection.get_container(container)

    def _delete_container(self, container):
        self._remove_object(container, self._containers_list)
        del self._containers[container]
        del self._objects[container]

    def with_container(self, container_name):
        """
        sets expectations for creating a container and subsequently getting its
        information

        example:

        if FAKE:
            swift_stub.with_container('test-container-name')

        # returns swift container information - mostly faked
        component_using.swift.create_container('test-container-name')
        component_using_swift.get_container_info('test-container-name')

        :param container_name: container name that is expected to be created
        """

        def container_resp(container):
            return ({'content-length': '2', 'x-container-object-count': '0',
                     'accept-ranges': 'bytes', 'x-container-bytes-used': '0',
                     'x-timestamp': '1363370869.72356',
                     'x-trans-id': 'tx7731801ac6ec4e5f8f7da61cde46bed7',
                     'date': 'Fri, 10 Mar 2013 18:07:58 GMT',
                     'content-type': 'application/json; charset=utf-8'},
                    self._objects[container])

        # if this is called multiple times then nothing happens
        swift_client.Connection.put_container = MagicMock(return_value=None)

        def side_effect_func(*args, **kwargs):
            if args[0] in self._containers:
                return container_resp(args[0])
            else:
                raise swiftclient.ClientException('Resource Not Found',
                                                  http_status=404)

        self._create_container(container_name)
        # return container headers
        swift_client.Connection.get_container = MagicMock(
            side_effect=side_effect_func)

        return self

    def without_container(self, container):
        """
        sets expectations for removing a container and subsequently throwing an
        exception for further interactions

        example:

        if FAKE:
            swift_stub.without_container('test-container-name')

        # returns swift container information - mostly faked
        component_using.swift.remove_container('test-container-name')
        # throws exception "Resource Not Found - 404"
        component_using_swift.get_container_info('test-container-name')

        :param container: container name that is expected to be removed
        """
        # first ensure container
        self._ensure_container_exists(container)
        self._delete_container(container)
        return self

    def with_object(self, container, name, contents):
        """
        sets expectations for creating an object and subsequently getting its
        contents

        example:

        if FAKE:
        swift_stub.with_object('test-container-name', 'test-object-name',
            'test-object-contents')

        # returns swift object info and contents
        component_using_swift.create_object('test-container-name',
            'test-object-name', 'test-contents')
        component_using_swift.get_object('test-container-name',
            'test-object-name')

        :param container: container name that is the object belongs
        :param name: the name of the object expected to be created
        :param contents: the contents of the object
        """

        swift_client.Connection.put_object = MagicMock(
            return_value=uuid.uuid1())

        def side_effect_func(*args, **kwargs):
            if (args[0] in self._containers and
                args[1] in map(lambda x: x['name'],
                               self._objects[args[0]])):
                return (
                    {'content-length': len(contents), 'accept-ranges': 'bytes',
                     'last-modified': 'Mon, 10 Mar 2013 01:06:34 GMT',
                     'etag': 'eb15a6874ce265e2c3eb1b4891567bab',
                     'x-timestamp': '1363568794.67584',
                     'x-trans-id': 'txef3aaf26c897420c8e77c9750ce6a501',
                     'date': 'Mon, 10 Mar 2013 05:35:14 GMT',
                     'content-type': 'application/octet-stream'},
                    [obj for obj in self._objects[args[0]]
                     if obj['name'] == args[1]][0]['contents'])
            else:
                raise swiftclient.ClientException('Resource Not Found',
                                                  http_status=404)

        swift_client.Connection.get_object = MagicMock(
            side_effect=side_effect_func)

        self._remove_object(name, self._objects[container])
        self._objects[container].append(
            {'bytes': 13, 'last_modified': '2013-03-15T22:10:49.361950',
             'hash': 'ccc55aefbf92aa66f42b638802c5e7f6', 'name': name,
             'content_type': 'application/octet-stream', 'contents': contents})
        return self

    def without_object(self, container, name):
        """
        sets expectations for deleting an object

        example:

        if FAKE:
        swift_stub.without_object('test-container-name', 'test-object-name')

        # allows container to be removed ONCE
        component_using_swift.remove_container('test-container-name')
        # throws ClientException - 404
        component_using_swift.get_container('test-container-name')
        component_using_swift.remove_container('test-container-name')

        :param container: container name that is the object belongs
        :param name: the name of the object expected to be removed
        """
        self._ensure_container_exists(container)
        self._ensure_object_exists(container, name)

        def side_effect_func(*args, **kwargs):
            if not [obj for obj in self._objects[args[0]]
                    if obj['name'] == [args[1]]]:
                raise swiftclient.ClientException('Resource Not found',
                                                  http_status=404)
            else:
                return None

        swift_client.Connection.delete_object = MagicMock(
            side_effect=side_effect_func)

        self._remove_object(name, self._objects[container])
        return self


def fake_create_swift_client(calculate_etag=False, *args):
    return FakeSwiftClient.Connection(*args)

########NEW FILE########
__FILENAME__ = test_limits
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Tests dealing with HTTP rate-limiting.
"""

import httplib
from trove.quota.models import Quota
import testtools
import webob

from mock import Mock, MagicMock
import six

from trove.common import limits
from trove.common.limits import Limit
from trove.limits import views
from trove.limits.service import LimitsController
from trove.openstack.common import jsonutils
from trove.quota.quota import QUOTAS

TEST_LIMITS = [
    Limit("GET", "/delayed", "^/delayed", 1, limits.PER_MINUTE),
    Limit("POST", "*", ".*", 7, limits.PER_MINUTE),
    Limit("POST", "/mgmt", "^/mgmt", 3, limits.PER_MINUTE),
    Limit("PUT", "*", "", 10, limits.PER_MINUTE),
]


class BaseLimitTestSuite(testtools.TestCase):
    """Base test suite which provides relevant stubs and time abstraction."""

    def setUp(self):
        super(BaseLimitTestSuite, self).setUp()
        self.absolute_limits = {"max_instances": 55,
                                "max_volumes": 100,
                                "max_backups": 40}


class LimitsControllerTest(BaseLimitTestSuite):
    def setUp(self):
        super(LimitsControllerTest, self).setUp()

    def test_limit_index_empty(self):
        limit_controller = LimitsController()

        req = MagicMock()
        req.environ = {}

        QUOTAS.get_all_quotas_by_tenant = MagicMock(return_value={})

        view = limit_controller.index(req, "test_tenant_id")
        expected = {'limits': [{'verb': 'ABSOLUTE'}]}
        self.assertEqual(expected, view._data)

    def test_limit_index(self):
        tenant_id = "test_tenant_id"
        limit_controller = LimitsController()

        limits = [
            {
                "URI": "*",
                "regex": ".*",
                "value": 10,
                "verb": "POST",
                "remaining": 2,
                "unit": "MINUTE",
                "resetTime": 1311272226
            },
            {
                "URI": "*",
                "regex": ".*",
                "value": 10,
                "verb": "PUT",
                "remaining": 2,
                "unit": "MINUTE",
                "resetTime": 1311272226
            },
            {
                "URI": "*",
                "regex": ".*",
                "value": 10,
                "verb": "DELETE",
                "remaining": 2,
                "unit": "MINUTE",
                "resetTime": 1311272226
            },
            {
                "URI": "*",
                "regex": ".*",
                "value": 10,
                "verb": "GET",
                "remaining": 2,
                "unit": "MINUTE",
                "resetTime": 1311272226
            }
        ]

        abs_limits = {"instances": Quota(tenant_id=tenant_id,
                                         resource="instances",
                                         hard_limit=100),

                      "backups": Quota(tenant_id=tenant_id,
                                       resource="backups",
                                       hard_limit=40),

                      "volumes": Quota(tenant_id=tenant_id,
                                       resource="volumes",
                                       hard_limit=55)}

        req = MagicMock()
        req.environ = {"trove.limits": limits}

        QUOTAS.get_all_quotas_by_tenant = MagicMock(return_value=abs_limits)

        view = limit_controller.index(req, tenant_id)

        expected = {
            'limits': [
                {
                    'max_instances': 100,
                    'max_backups': 40,
                    'verb': 'ABSOLUTE',
                    'max_volumes': 55
                },
                {
                    'regex': '.*',
                    'nextAvailable': '2011-07-21T18:17:06Z',
                    'uri': '*',
                    'value': 10,
                    'verb': 'POST',
                    'remaining': 2,
                    'unit': 'MINUTE'
                },
                {
                    'regex': '.*',
                    'nextAvailable': '2011-07-21T18:17:06Z',
                    'uri': '*',
                    'value': 10,
                    'verb': 'PUT',
                    'remaining': 2,
                    'unit': 'MINUTE'
                },
                {
                    'regex': '.*',
                    'nextAvailable': '2011-07-21T18:17:06Z',
                    'uri': '*',
                    'value': 10,
                    'verb': 'DELETE',
                    'remaining': 2,
                    'unit': 'MINUTE'
                },
                {
                    'regex': '.*',
                    'nextAvailable': '2011-07-21T18:17:06Z',
                    'uri': '*',
                    'value': 10,
                    'verb': 'GET',
                    'remaining': 2,
                    'unit': 'MINUTE'
                }
            ]
        }

        self.assertEqual(expected, view._data)


class TestLimiter(limits.Limiter):
    """Note: This was taken from Nova"""
    pass


class LimitMiddlewareTest(BaseLimitTestSuite):
    """
    Tests for the `limits.RateLimitingMiddleware` class.
    """

    @webob.dec.wsgify
    def _empty_app(self, request):
        """Do-nothing WSGI app."""
        pass

    def setUp(self):
        """Prepare middleware for use through fake WSGI app."""
        super(LimitMiddlewareTest, self).setUp()
        _limits = '(GET, *, .*, 1, MINUTE)'
        self.app = limits.RateLimitingMiddleware(self._empty_app, _limits,
                                                 "%s.TestLimiter" %
                                                 self.__class__.__module__)

    def test_limit_class(self):
        # Test that middleware selected correct limiter class.
        assert isinstance(self.app._limiter, TestLimiter)

    def test_good_request(self):
        # Test successful GET request through middleware.
        request = webob.Request.blank("/")
        response = request.get_response(self.app)
        self.assertEqual(200, response.status_int)

    def test_limited_request_json(self):
        # Test a rate-limited (413) GET request through middleware.
        request = webob.Request.blank("/")
        response = request.get_response(self.app)
        self.assertEqual(200, response.status_int)

        request = webob.Request.blank("/")
        response = request.get_response(self.app)
        self.assertEqual(response.status_int, 413)

        self.assertTrue('Retry-After' in response.headers)
        retry_after = int(response.headers['Retry-After'])
        self.assertAlmostEqual(retry_after, 60, 1)

        body = jsonutils.loads(response.body)
        expected = "Only 1 GET request(s) can be made to * every minute."
        value = body["overLimit"]["details"].strip()
        self.assertEqual(value, expected)

        self.assertTrue("retryAfter" in body["overLimit"])
        retryAfter = body["overLimit"]["retryAfter"]
        self.assertEqual(retryAfter, "60")


class LimitTest(BaseLimitTestSuite):
    """
    Tests for the `limits.Limit` class.
    """

    def test_GET_no_delay(self):
        # Test a limit handles 1 GET per second.
        limit = Limit("GET", "*", ".*", 1, 1)

        limit._get_time = MagicMock(return_value=0.0)
        delay = limit("GET", "/anything")
        self.assertEqual(None, delay)
        self.assertEqual(0, limit.next_request)
        self.assertEqual(0, limit.last_request)

    def test_GET_delay(self):
        # Test two calls to 1 GET per second limit.
        limit = Limit("GET", "*", ".*", 1, 1)
        limit._get_time = MagicMock(return_value=0.0)

        delay = limit("GET", "/anything")
        self.assertEqual(None, delay)

        delay = limit("GET", "/anything")
        self.assertEqual(1, delay)
        self.assertEqual(1, limit.next_request)
        self.assertEqual(0, limit.last_request)

        limit._get_time = MagicMock(return_value=4.0)

        delay = limit("GET", "/anything")
        self.assertEqual(None, delay)
        self.assertEqual(4, limit.next_request)
        self.assertEqual(4, limit.last_request)


class ParseLimitsTest(BaseLimitTestSuite):
    """
    Tests for the default limits parser in the in-memory
    `limits.Limiter` class.
    """

    def test_invalid(self):
        # Test that parse_limits() handles invalid input correctly.
        self.assertRaises(ValueError, limits.Limiter.parse_limits,
                          ';;;;;')

    def test_bad_rule(self):
        # Test that parse_limits() handles bad rules correctly.
        self.assertRaises(ValueError, limits.Limiter.parse_limits,
                          'GET, *, .*, 20, minute')

    def test_missing_arg(self):
        # Test that parse_limits() handles missing args correctly.
        self.assertRaises(ValueError, limits.Limiter.parse_limits,
                          '(GET, *, .*, 20)')

    def test_bad_value(self):
        # Test that parse_limits() handles bad values correctly.
        self.assertRaises(ValueError, limits.Limiter.parse_limits,
                          '(GET, *, .*, foo, minute)')

    def test_bad_unit(self):
        # Test that parse_limits() handles bad units correctly.
        self.assertRaises(ValueError, limits.Limiter.parse_limits,
                          '(GET, *, .*, 20, lightyears)')

    def test_multiple_rules(self):
        # Test that parse_limits() handles multiple rules correctly.
        try:
            l = limits.Limiter.parse_limits('(get, *, .*, 20, minute);'
                                            '(PUT, /foo*, /foo.*, 10, hour);'
                                            '(POST, /bar*, /bar.*, 5, second);'
                                            '(Say, /derp*, /derp.*, 1, day)')
        except ValueError as e:
            assert False, str(e)

        # Make sure the number of returned limits are correct
        self.assertEqual(len(l), 4)

        # Check all the verbs...
        expected = ['GET', 'PUT', 'POST', 'SAY']
        self.assertEqual([t.verb for t in l], expected)

        # ...the URIs...
        expected = ['*', '/foo*', '/bar*', '/derp*']
        self.assertEqual([t.uri for t in l], expected)

        # ...the regexes...
        expected = ['.*', '/foo.*', '/bar.*', '/derp.*']
        self.assertEqual([t.regex for t in l], expected)

        # ...the values...
        expected = [20, 10, 5, 1]
        self.assertEqual([t.value for t in l], expected)

        # ...and the units...
        expected = [limits.PER_MINUTE, limits.PER_HOUR,
                    limits.PER_SECOND, limits.PER_DAY]
        self.assertEqual([t.unit for t in l], expected)


class LimiterTest(BaseLimitTestSuite):
    """
    Tests for the in-memory `limits.Limiter` class.
    """

    def update_limits(self, delay, limit_list):
        for ln in limit_list:
            ln._get_time = Mock(return_value=delay)

    def setUp(self):
        """Run before each test."""
        super(LimiterTest, self).setUp()
        userlimits = {'user:user3': ''}

        self.update_limits(0.0, TEST_LIMITS)
        self.limiter = limits.Limiter(TEST_LIMITS, **userlimits)

    def _check(self, num, verb, url, username=None):
        """Check and yield results from checks."""
        for x in xrange(num):
            yield self.limiter.check_for_delay(verb, url, username)[0]

    def _check_sum(self, num, verb, url, username=None):
        """Check and sum results from checks."""
        results = self._check(num, verb, url, username)
        return sum(item for item in results if item)

    def test_no_delay_GET(self):
        """
        Simple test to ensure no delay on a single call for a limit verb we
        didn"t set.
        """
        delay = self.limiter.check_for_delay("GET", "/anything")
        self.assertEqual(delay, (None, None))

    def test_no_delay_PUT(self):
        # Simple test to ensure no delay on a single call for a known limit.
        delay = self.limiter.check_for_delay("PUT", "/anything")
        self.assertEqual(delay, (None, None))

    def test_delay_PUT(self):
        """
        Ensure the 11th PUT will result in a delay of 6.0 seconds until
        the next request will be granted.
        """
        expected = [None] * 10 + [6.0]
        results = list(self._check(11, "PUT", "/anything"))

        self.assertEqual(expected, results)

    def test_delay_POST(self):
        """
        Ensure the 8th POST will result in a delay of 6.0 seconds until
        the next request will be granted.
        """
        expected = [None] * 7
        results = list(self._check(7, "POST", "/anything"))
        self.assertEqual(expected, results)

        expected = 60.0 / 7.0
        results = self._check_sum(1, "POST", "/anything")
        self.assertAlmostEqual(expected, results, 8)

    def test_delay_POST_mgmt(self):
        """
        Ensure the 4th mgmt POST will result in a delay of 6.0 seconds until
        the next request will be granted.
        """
        expected = [None] * 3
        results = list(self._check(3, "POST", "/mgmt"))
        self.assertEqual(expected, results)

        expected = 60.0 / 3.0
        results = self._check_sum(1, "POST", "/mgmt")
        self.assertAlmostEqual(expected, results, 4)

    def test_delay_GET(self):
        # Ensure the 11th GET will result in NO delay.
        expected = [None] * 11
        results = list(self._check(11, "GET", "/mgmt"))

        self.assertEqual(expected, results)

    def test_delay_PUT_wait(self):
        """
        Ensure after hitting the limit and then waiting for the correct
        amount of time, the limit will be lifted.
        """
        expected = [None] * 10 + [6.0]
        results = list(self._check(11, "PUT", "/anything"))
        self.assertEqual(expected, results)

        # Advance time
        self.update_limits(6.0, self.limiter.levels[None])

        expected = [None, 6.0]
        results = list(self._check(2, "PUT", "/anything"))
        self.assertEqual(expected, results)

    def test_multiple_delays(self):
        # Ensure multiple requests still get a delay.
        expected = [None] * 10 + [6.0] * 10
        results = list(self._check(20, "PUT", "/anything"))
        self.assertEqual(expected, results)

        self.update_limits(1.0, self.limiter.levels[None])

        expected = [5.0] * 10
        results = list(self._check(10, "PUT", "/anything"))
        self.assertEqual(expected, results)

    def test_user_limit(self):
        # Test user-specific limits.
        self.assertEqual(self.limiter.levels['user3'], [])

    def test_multiple_users(self):
        # Tests involving multiple users.
        # User1
        self.update_limits(0.0, self.limiter.levels["user1"])
        expected = [None] * 10 + [6.0] * 10
        results = list(self._check(20, "PUT", "/anything", "user1"))
        self.assertEqual(expected, results)

        # User2
        expected = [None] * 10 + [6.0] * 5
        results = list(self._check(15, "PUT", "/anything", "user2"))
        self.assertEqual(expected, results)

        # User3
        expected = [None] * 20
        results = list(self._check(20, "PUT", "/anything", "user3"))
        self.assertEqual(expected, results)

        # User1 again
        self.update_limits(1.0, self.limiter.levels["user1"])
        expected = [5.0] * 10
        results = list(self._check(10, "PUT", "/anything", "user1"))
        self.assertEqual(expected, results)

        # User2 again
        self.update_limits(2.0, self.limiter.levels["user2"])
        expected = [4.0] * 5
        results = list(self._check(5, "PUT", "/anything", "user2"))
        self.assertEqual(expected, results)


class WsgiLimiterTest(BaseLimitTestSuite):
    """
    Tests for `limits.WsgiLimiter` class.
    """

    def setUp(self):
        """Run before each test."""
        super(WsgiLimiterTest, self).setUp()
        self.app = limits.WsgiLimiter(TEST_LIMITS)

    def _request_data(self, verb, path):
        """Get data describing a limit request verb/path."""
        return jsonutils.dumps({"verb": verb, "path": path})

    def _request(self, verb, url, username=None):
        """Make sure that POSTing to the given url causes the given username
        to perform the given action.  Make the internal rate limiter return
        delay and make sure that the WSGI app returns the correct response.
        """
        if username:
            request = webob.Request.blank("/%s" % username)
        else:
            request = webob.Request.blank("/")

        request.method = "POST"
        request.body = self._request_data(verb, url)
        response = request.get_response(self.app)

        if "X-Wait-Seconds" in response.headers:
            self.assertEqual(response.status_int, 403)
            return response.headers["X-Wait-Seconds"]

        self.assertEqual(response.status_int, 204)

    def test_invalid_methods(self):
        # Only POSTs should work.
        for method in ["GET", "PUT", "DELETE", "HEAD", "OPTIONS"]:
            request = webob.Request.blank("/", method=method)
            response = request.get_response(self.app)
            self.assertEqual(response.status_int, 405)

    def test_good_url(self):
        delay = self._request("GET", "/something")
        self.assertEqual(delay, None)

    def test_escaping(self):
        delay = self._request("GET", "/something/jump%20up")
        self.assertEqual(delay, None)

    def test_response_to_delays(self):
        delay = self._request("GET", "/delayed")
        self.assertEqual(delay, None)

        delay = self._request("GET", "/delayed")
        self.assertAlmostEqual(float(delay), 60, 1)

    def test_response_to_delays_usernames(self):
        delay = self._request("GET", "/delayed", "user1")
        self.assertEqual(delay, None)

        delay = self._request("GET", "/delayed", "user2")
        self.assertEqual(delay, None)

        delay = self._request("GET", "/delayed", "user1")
        self.assertAlmostEqual(float(delay), 60, 1)

        delay = self._request("GET", "/delayed", "user2")
        self.assertAlmostEqual(float(delay), 60, 1)


class FakeHttplibSocket(object):
    """
    Fake `httplib.HTTPResponse` replacement.
    """

    def __init__(self, response_string):
        """Initialize new `FakeHttplibSocket`."""
        self._buffer = six.StringIO(response_string)

    def makefile(self, _mode, _other):
        """Returns the socket's internal buffer."""
        return self._buffer


class FakeHttplibConnection(object):
    """
    Fake `httplib.HTTPConnection`.
    """

    def __init__(self, app, host):
        """
        Initialize `FakeHttplibConnection`.
        """
        self.app = app
        self.host = host

    def request(self, method, path, body="", headers=None):
        """
        Requests made via this connection actually get translated and routed
        into our WSGI app, we then wait for the response and turn it back into
        an `httplib.HTTPResponse`.
        """
        if not headers:
            headers = {}

        req = webob.Request.blank(path)
        req.method = method
        req.headers = headers
        req.host = self.host
        req.body = body

        resp = str(req.get_response(self.app))
        resp = "HTTP/1.0 %s" % resp
        sock = FakeHttplibSocket(resp)
        self.http_response = httplib.HTTPResponse(sock)
        self.http_response.begin()

    def getresponse(self):
        """Return our generated response from the request."""
        return self.http_response


def wire_HTTPConnection_to_WSGI(host, app):
    """Monkeypatches HTTPConnection so that if you try to connect to host, you
    are instead routed straight to the given WSGI app.

    After calling this method, when any code calls

    httplib.HTTPConnection(host)

    the connection object will be a fake.  Its requests will be sent directly
    to the given WSGI app rather than through a socket.

    Code connecting to hosts other than host will not be affected.

    This method may be called multiple times to map different hosts to
    different apps.

    This method returns the original HTTPConnection object, so that the caller
    can restore the default HTTPConnection interface (for all hosts).
    """

    class HTTPConnectionDecorator(object):
        """Wraps the real HTTPConnection class so that when you instantiate
            the class you might instead get a fake instance.
        """

        def __init__(self, wrapped):
            self.wrapped = wrapped

        def __call__(self, connection_host, *args, **kwargs):
            if connection_host == host:
                return FakeHttplibConnection(app, host)
            else:
                return self.wrapped(connection_host, *args, **kwargs)

    oldHTTPConnection = httplib.HTTPConnection
    httplib.HTTPConnection = HTTPConnectionDecorator(httplib.HTTPConnection)
    return oldHTTPConnection


class WsgiLimiterProxyTest(BaseLimitTestSuite):
    """
    Tests for the `limits.WsgiLimiterProxy` class.
    """

    def setUp(self):
        """
        Do some nifty HTTP/WSGI magic which allows for WSGI to be called
        directly by something like the `httplib` library.
        """
        super(WsgiLimiterProxyTest, self).setUp()
        self.app = limits.WsgiLimiter(TEST_LIMITS)
        self.oldHTTPConnection = (
            wire_HTTPConnection_to_WSGI("169.254.0.1:80", self.app))
        self.proxy = limits.WsgiLimiterProxy("169.254.0.1:80")

    def test_200(self):
        # Successful request test.
        delay = self.proxy.check_for_delay("GET", "/anything")
        self.assertEqual(delay, (None, None))

    def test_403(self):
        # Forbidden request test.
        delay = self.proxy.check_for_delay("GET", "/delayed")
        self.assertEqual(delay, (None, None))

        delay, error = self.proxy.check_for_delay("GET", "/delayed")
        error = error.strip()

        self.assertAlmostEqual(float(delay), 60, 1)
        self.assertEqual(error, "403 Forbidden\n\nOnly 1 GET request(s) can be"
                                " made to /delayed every minute.")

    def tearDown(self):
        # restore original HTTPConnection object
        httplib.HTTPConnection = self.oldHTTPConnection
        super(WsgiLimiterProxyTest, self).tearDown()


class LimitsViewTest(testtools.TestCase):
    def setUp(self):
        super(LimitsViewTest, self).setUp()

    def test_empty_data(self):
        """
        Test the default returned results if an empty dictionary is given
        """
        rate_limit = {}
        view = views.LimitView(rate_limit)
        self.assertIsNotNone(view)

        data = view.data()
        expected = {'limit': {'regex': '',
                              'nextAvailable': '1970-01-01T00:00:00Z',
                              'uri': '',
                              'value': '',
                              'verb': '',
                              'remaining': 0,
                              'unit': ''}}

        self.assertEqual(expected, data)

    def test_data(self):
        """
        Test the returned results for a fully populated dictionary
        """
        rate_limit = {
            "URI": "*",
            "regex": ".*",
            "value": 10,
            "verb": "POST",
            "remaining": 2,
            "unit": "MINUTE",
            "resetTime": 1311272226
        }

        view = views.LimitView(rate_limit)
        self.assertIsNotNone(view)

        data = view.data()
        expected = {'limit': {'regex': '.*',
                              'nextAvailable': '2011-07-21T18:17:06Z',
                              'uri': '*',
                              'value': 10,
                              'verb': 'POST',
                              'remaining': 2,
                              'unit': 'MINUTE'}}

        self.assertEqual(expected, data)


class LimitsViewsTest(testtools.TestCase):
    def setUp(self):
        super(LimitsViewsTest, self).setUp()

    def test_empty_data(self):
        rate_limits = []
        abs_view = dict()

        view_data = views.LimitViews(abs_view, rate_limits)
        self.assertIsNotNone(view_data)

        data = view_data.data()
        expected = {'limits': [{'verb': 'ABSOLUTE'}]}

        self.assertEqual(expected, data)

    def test_data(self):
        rate_limits = [
            {
                "URI": "*",
                "regex": ".*",
                "value": 10,
                "verb": "POST",
                "remaining": 2,
                "unit": "MINUTE",
                "resetTime": 1311272226
            },
            {
                "URI": "*",
                "regex": ".*",
                "value": 10,
                "verb": "PUT",
                "remaining": 2,
                "unit": "MINUTE",
                "resetTime": 1311272226
            },
            {
                "URI": "*",
                "regex": ".*",
                "value": 10,
                "verb": "DELETE",
                "remaining": 2,
                "unit": "MINUTE",
                "resetTime": 1311272226
            },
            {
                "URI": "*",
                "regex": ".*",
                "value": 10,
                "verb": "GET",
                "remaining": 2,
                "unit": "MINUTE",
                "resetTime": 1311272226
            }
        ]
        abs_view = {"instances": 55, "volumes": 100, "backups": 40}

        view_data = views.LimitViews(abs_view, rate_limits)
        self.assertIsNotNone(view_data)

        data = view_data.data()
        expected = {'limits': [{'max_instances': 55,
                                'max_backups': 40,
                                'verb': 'ABSOLUTE',
                                'max_volumes': 100},
                               {'regex': '.*',
                                'nextAvailable': '2011-07-21T18:17:06Z',
                                'uri': '*',
                                'value': 10,
                                'verb': 'POST',
                                'remaining': 2, 'unit': 'MINUTE'},
                               {'regex': '.*',
                                'nextAvailable': '2011-07-21T18:17:06Z',
                                'uri': '*',
                                'value': 10,
                                'verb': 'PUT',
                                'remaining': 2,
                                'unit': 'MINUTE'},
                               {'regex': '.*',
                                'nextAvailable': '2011-07-21T18:17:06Z',
                                'uri': '*',
                                'value': 10,
                                'verb': 'DELETE',
                                'remaining': 2,
                                'unit': 'MINUTE'},
                               {'regex': '.*',
                                'nextAvailable': '2011-07-21T18:17:06Z',
                                'uri': '*',
                                'value': 10,
                                'verb': 'GET',
                                'remaining': 2, 'unit': 'MINUTE'}]}

        self.assertEqual(expected, data)

########NEW FILE########
__FILENAME__ = test_versions
# Copyright 2013 OpenStack Foundation
# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from mock import Mock
from trove.versions import BaseVersion
from trove.versions import Version
from trove.versions import VersionDataView
from trove.versions import VersionsAPI
from trove.versions import VersionsController
from trove.versions import VersionsDataView
from trove.versions import VERSIONS

import testtools

BASE_URL = 'http://localhost'


class VersionsControllerTest(testtools.TestCase):

    def setUp(self):
        super(VersionsControllerTest, self).setUp()
        self.controller = VersionsController()
        self.assertIsNotNone(self.controller,
                             "VersionsController instance was None")

    def test_index_json(self):
        request = Mock()
        result = self.controller.index(request)
        self.assertIsNotNone(result,
                             'Result was None')
        result._data = Mock()
        result._data.data_for_json = \
            lambda: {'status': 'CURRENT',
                     'updated': '2012-08-01T00:00:00Z',
                     'id': 'v1.0',
                     'links': [{'href': 'http://localhost/v1.0/',
                                'rel': 'self'}]}

        # can be anything but xml
        json_data = result.data("application/json")

        self.assertIsNotNone(json_data,
                             'Result json_data was None')
        self.assertEqual('v1.0', json_data['id'],
                         'Version id is incorrect')
        self.assertEqual('CURRENT', json_data['status'],
                         'Version status is incorrect')
        self.assertEqual('2012-08-01T00:00:00Z', json_data['updated'],
                         'Version updated value is incorrect')

    def test_show_json(self):
        request = Mock()
        request.url_version = '1.0'
        result = self.controller.show(request)
        self.assertIsNotNone(result,
                             'Result was None')
        json_data = result.data("application/json")
        self.assertIsNotNone(json_data, "JSON data was None")

        version = json_data.get('version', None)
        self.assertIsNotNone(version, "Version was None")

        self.assertEqual('CURRENT', version['status'],
                         "Version status was not 'CURRENT'")
        self.assertEqual('2012-08-01T00:00:00Z', version['updated'],
                         "Version updated was not '2012-08-01T00:00:00Z'")
        self.assertEqual('v1.0', version['id'], "Version id was not 'v1.0'")


class BaseVersionTestCase(testtools.TestCase):

    def setUp(self):
        super(BaseVersionTestCase, self).setUp()

        id = VERSIONS['1.0']['id']
        status = VERSIONS['1.0']['status']
        base_url = BASE_URL
        updated = VERSIONS['1.0']['updated']

        self.base_version = BaseVersion(id, status, base_url, updated)
        self.assertIsNotNone(self.base_version,
                             'BaseVersion instance was None')

    def test_data(self):
        data = self.base_version.data()
        self.assertIsNotNone(data, 'Base Version data was None')

        self.assertTrue(type(data) is dict,
                        "Base Version data is not a dict")
        self.assertEqual('CURRENT', data['status'],
                         "Data status was not 'CURRENT'")
        self.assertEqual('2012-08-01T00:00:00Z', data['updated'],
                         "Data updated was not '2012-08-01T00:00:00Z'")
        self.assertEqual('v1.0', data['id'],
                         "Data status was not 'v1.0'")

    def test_url(self):
        url = self.base_version.url()
        self.assertIsNotNone(url, 'Url was None')
        self.assertEqual('http://localhost/v1.0/', url,
                         "Base Version url is incorrect")


class VersionTestCase(testtools.TestCase):

    def setUp(self):
        super(VersionTestCase, self).setUp()

        id = VERSIONS['1.0']['id']
        status = VERSIONS['1.0']['status']
        base_url = BASE_URL
        updated = VERSIONS['1.0']['updated']

        self.version = Version(id, status, base_url, updated)
        self.assertIsNotNone(self.version,
                             'Version instance was None')

    def test_url_no_trailing_slash(self):
        url = self.version.url()
        self.assertIsNotNone(url, 'Version url was None')
        self.assertEqual(BASE_URL + '/', url,
                         'Base url value was incorrect')

    def test_url_with_trailing_slash(self):
        self.version.base_url = 'http://localhost/'
        url = self.version.url()
        self.assertEqual(BASE_URL + '/', url,
                         'Base url value was incorrect')


class VersionDataViewTestCase(testtools.TestCase):

    def setUp(self):
        super(VersionDataViewTestCase, self).setUp()

        # get a version object first
        id = VERSIONS['1.0']['id']
        status = VERSIONS['1.0']['status']
        base_url = BASE_URL
        updated = VERSIONS['1.0']['updated']

        self.version = Version(id, status, base_url, updated)
        self.assertIsNotNone(self.version,
                             'Version instance was None')

        # then create an instance of VersionDataView
        self.version_data_view = VersionDataView(self.version)
        self.assertIsNotNone(self.version_data_view,
                             'Version Data view instance was None')

    def test_data_for_json(self):
        json_data = self.version_data_view.data_for_json()
        self.assertIsNotNone(json_data, "JSON data was None")
        self.assertTrue(type(json_data) is dict,
                        "JSON version data is not a dict")
        self.assertIsNotNone(json_data.get('version'),
                             "Dict json_data has no key 'version'")
        data = json_data['version']
        self.assertIsNotNone(data, "JSON data version was None")
        self.assertEqual('CURRENT', data['status'],
                         "Data status was not 'CURRENT'")
        self.assertEqual('2012-08-01T00:00:00Z', data['updated'],
                         "Data updated was not '2012-08-01T00:00:00Z'")
        self.assertEqual('v1.0', data['id'],
                         "Data status was not 'v1.0'")


class VersionsDataViewTestCase(testtools.TestCase):

    def setUp(self):
        super(VersionsDataViewTestCase, self).setUp()

        # get a version object, put it in a list
        self.versions = []

        id = VERSIONS['1.0']['id']
        status = VERSIONS['1.0']['status']
        base_url = BASE_URL
        updated = VERSIONS['1.0']['updated']

        self.version = Version(id, status, base_url, updated)
        self.assertIsNotNone(self.version,
                             'Version instance was None')
        self.versions.append(self.version)

        # then create an instance of VersionsDataView
        self.versions_data_view = VersionsDataView(self.versions)
        self.assertIsNotNone(self.versions_data_view,
                             'Versions Data view instance was None')

    def test_data_for_json(self):
        json_data = self.versions_data_view.data_for_json()
        self.assertIsNotNone(json_data, "JSON data was None")
        self.assertTrue(type(json_data) is dict,
                        "JSON versions data is not a dict")
        self.assertIsNotNone(json_data.get('versions', None),
                             "Dict json_data has no key 'versions'")
        versions = json_data['versions']
        self.assertIsNotNone(versions, "Versions was None")
        self.assertTrue(len(versions) == 1, "Versions length != 1")

        # explode the version object
        versions_data = [v.data() for v in self.versions]

        d1 = versions_data.pop()
        d2 = versions.pop()
        self.assertEqual(d1['id'], d2['id'],
                         "Version ids are not equal")


class VersionAPITestCase(testtools.TestCase):

    def setUp(self):
        super(VersionAPITestCase, self).setUp()

    def test_instance(self):
        self.versions_api = VersionsAPI()
        self.assertIsNotNone(self.versions_api,
                             "VersionsAPI instance was None")

########NEW FILE########
__FILENAME__ = test_backupagent
#Copyright 2013 Hewlett-Packard Development Company, L.P.

#Licensed under the Apache License, Version 2.0 (the "License");
#you may not use this file except in compliance with the License.
#You may obtain a copy of the License at
#
#http://www.apache.org/licenses/LICENSE-2.0
#
#Unless required by applicable law or agreed to in writing, software
#distributed under the License is distributed on an "AS IS" BASIS,
#WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#See the License for the specific language governing permissions and
#limitations under the License.

from mock import Mock, MagicMock, patch, ANY
from webob.exc import HTTPNotFound

import hashlib
import os
import testtools

from trove.common.context import TroveContext
from trove.conductor import api as conductor_api
from trove.guestagent.strategies.backup import mysql_impl
from trove.guestagent.strategies.restore.base import RestoreRunner
from trove.backup.models import BackupState
from trove.guestagent.backup import backupagent
from trove.guestagent.strategies.backup.base import BackupRunner
from trove.guestagent.strategies.backup.base import UnknownBackupType
from trove.guestagent.strategies.storage.base import Storage

conductor_api.API.update_backup = Mock()


def create_fake_data():
    from random import choice
    from string import ascii_letters

    return ''.join([choice(ascii_letters) for _ in xrange(1024)])


class MockBackup(BackupRunner):
    """Create a large temporary file to 'backup' with subprocess."""

    backup_type = 'mock_backup'

    def __init__(self, *args, **kwargs):
        self.data = create_fake_data()
        self.cmd = 'echo %s' % self.data
        super(MockBackup, self).__init__(*args, **kwargs)

    def cmd(self):
        return self.cmd


class MockCheckProcessBackup(MockBackup):
    """Backup runner that fails confirming the process."""

    def check_process(self):
        return False


class MockLossyBackup(MockBackup):
    """Fake Incomplete writes to swift"""

    def read(self, *args):
        results = super(MockLossyBackup, self).read(*args)
        if results:
            # strip a few chars from the stream
            return results[20:]


class MockSwift(object):
    """Store files in String"""

    def __init__(self, *args, **kwargs):
        self.store = ''
        self.containers = []
        self.container = "database_backups"
        self.url = 'http://mockswift/v1'
        self.etag = hashlib.md5()

    def put_container(self, container):
        if container not in self.containers:
            self.containers.append(container)
        return None

    def put_object(self, container, obj, contents, **kwargs):
        if container not in self.containers:
            raise HTTPNotFound
        while True:
            if not hasattr(contents, 'read'):
                break
            content = contents.read(2 ** 16)
            if not content:
                break
            self.store += content
        self.etag.update(self.store)
        return self.etag.hexdigest()

    def save(self, filename, stream):
        location = '%s/%s/%s' % (self.url, self.container, filename)
        return True, 'w00t', 'fake-checksum', location

    def load(self, context, storage_url, container, filename, backup_checksum):
        pass

    def load_metadata(self, location, checksum):
        return {}

    def save_metadata(self, location, metadata):
        pass


class MockStorage(Storage):

    def __call__(self, *args, **kwargs):
        return self

    def load(self, location, backup_checksum):
        pass

    def save(self, filename, stream):
        pass

    def load_metadata(self, location, checksum):
        return {}

    def save_metadata(self, location, metadata={}):
        pass

    def is_enabled(self):
        return True


class MockRestoreRunner(RestoreRunner):
    def __init__(self, storage, **kwargs):
        pass

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        pass

    def restore(self):
        pass

    def is_zipped(self):
        return False


class MockStats:
    f_blocks = 1024 ** 2
    f_bsize = 4096
    f_bfree = 512 * 1024


class BackupAgentTest(testtools.TestCase):
    def setUp(self):
        super(BackupAgentTest, self).setUp()
        mysql_impl.get_auth_password = MagicMock(return_value='123')
        backupagent.get_storage_strategy = MagicMock(return_value=MockSwift)
        os.statvfs = MagicMock(return_value=MockStats)

    def tearDown(self):
        super(BackupAgentTest, self).tearDown()

    def test_backup_impl_MySQLDump(self):
        """This test is for
           guestagent/strategies/backup/impl
        """
        mysql_dump = mysql_impl.MySQLDump(
            'abc', extra_opts='')
        self.assertIsNotNone(mysql_dump.cmd)
        str_mysql_dump_cmd = ('mysqldump'
                              ' --all-databases'
                              ' %(extra_opts)s'
                              ' --opt'
                              ' --password=123'
                              ' -u os_admin'
                              ' 2>/tmp/mysqldump.log'
                              ' | gzip |'
                              ' openssl enc -aes-256-cbc -salt '
                              '-pass pass:default_aes_cbc_key')
        self.assertEqual(mysql_dump.cmd, str_mysql_dump_cmd)
        self.assertIsNotNone(mysql_dump.manifest)
        self.assertEqual(mysql_dump.manifest, 'abc.gz.enc')

    def test_backup_impl_InnoBackupEx(self):
        """This test is for
           guestagent/strategies/backup/impl
        """
        inno_backup_ex = mysql_impl.InnoBackupEx('innobackupex', extra_opts='')
        self.assertIsNotNone(inno_backup_ex.cmd)
        str_innobackup_cmd = ('sudo innobackupex'
                              ' --stream=xbstream'
                              ' %(extra_opts)s'
                              ' /var/lib/mysql 2>/tmp/innobackupex.log'
                              ' | gzip |'
                              ' openssl enc -aes-256-cbc -salt '
                              '-pass pass:default_aes_cbc_key')
        self.assertEqual(inno_backup_ex.cmd, str_innobackup_cmd)
        self.assertIsNotNone(inno_backup_ex.manifest)
        str_innobackup_manifest = 'innobackupex.xbstream.gz.enc'
        self.assertEqual(inno_backup_ex.manifest, str_innobackup_manifest)

    def test_backup_base(self):
        """This test is for
           guestagent/strategies/backup/base
        """
        BackupRunner.cmd = "%s"
        backup_runner = BackupRunner('sample', cmd='echo command')
        if backup_runner.is_zipped:
            self.assertEqual(backup_runner.zip_manifest, '.gz')
            self.assertIsNotNone(backup_runner.zip_manifest)
            self.assertIsNotNone(backup_runner.zip_cmd)
            self.assertEqual(backup_runner.zip_cmd, ' | gzip')
        else:
            self.assertIsNone(backup_runner.zip_manifest)
            self.assertIsNone(backup_runner.zip_cmd)
        self.assertEqual(backup_runner.backup_type, 'BackupRunner')

    def test_execute_backup(self):
        """This test should ensure backup agent
                ensures that backup and storage is not running
                resolves backup instance
                starts backup
                starts storage
                reports status
        """
        agent = backupagent.BackupAgent()
        backup_info = {'id': '123',
                       'location': 'fake-location',
                       'type': 'InnoBackupEx',
                       'checksum': 'fake-checksum',
                       }
        agent.execute_backup(context=None, backup_info=backup_info,
                             runner=MockBackup)

        self.assertTrue(
            conductor_api.API.update_backup.called_once_with(
                ANY,
                backup_id=backup_info['id'],
                state=BackupState.NEW))

        self.assertTrue(
            conductor_api.API.update_backup.called_once_with(
                ANY,
                backup_id=backup_info['id'],
                size=ANY,
                state=BackupState.BUILDING))

        self.assertTrue(
            conductor_api.API.update_backup.called_once_with(
                ANY,
                backup_id=backup_info['id'],
                checksum=ANY,
                location=ANY,
                note=ANY,
                backup_type=backup_info['type'],
                state=BackupState.COMPLETED))

    def test_execute_bad_process_backup(self):
        agent = backupagent.BackupAgent()
        backup_info = {'id': '123',
                       'location': 'fake-location',
                       'type': 'InnoBackupEx',
                       'checksum': 'fake-checksum',
                       }

        self.assertRaises(backupagent.BackupError, agent.execute_backup,
                          context=None, backup_info=backup_info,
                          runner=MockCheckProcessBackup)

        self.assertTrue(
            conductor_api.API.update_backup.called_once_with(
                ANY,
                backup_id=backup_info['id'],
                state=BackupState.NEW))

        self.assertTrue(
            conductor_api.API.update_backup.called_once_with(
                ANY,
                backup_id=backup_info['id'],
                size=ANY,
                state=BackupState.BUILDING))

        self.assertTrue(
            conductor_api.API.update_backup.called_once_with(
                ANY,
                backup_id=backup_info['id'],
                checksum=ANY,
                location=ANY,
                note=ANY,
                backup_type=backup_info['type'],
                state=BackupState.FAILED))

    def test_execute_lossy_backup(self):
        """This test verifies that incomplete writes to swift will fail."""
        with patch.object(MockSwift, 'save',
                          return_value=(False, 'Error', 'y', 'z')):

            agent = backupagent.BackupAgent()

            backup_info = {'id': '123',
                           'location': 'fake-location',
                           'type': 'InnoBackupEx',
                           'checksum': 'fake-checksum',
                           }

            self.assertRaises(backupagent.BackupError, agent.execute_backup,
                              context=None, backup_info=backup_info,
                              runner=MockLossyBackup)

            self.assertTrue(
                conductor_api.API.update_backup.called_once_with(
                    ANY,
                    backup_id=backup_info['id'],
                    state=BackupState.FAILED))

    def test_execute_restore(self):
        """This test should ensure backup agent
                resolves backup instance
                determines backup/restore type
                transfers/downloads data and invokes the restore module
                reports status
        """
        with patch.object(backupagent, 'get_storage_strategy',
                          return_value=MockStorage):

            with patch.object(backupagent, 'get_restore_strategy',
                              return_value=MockRestoreRunner):

                agent = backupagent.BackupAgent()

                bkup_info = {'id': '123',
                             'location': 'fake-location',
                             'type': 'InnoBackupEx',
                             'checksum': 'fake-checksum',
                             }
                agent.execute_restore(TroveContext(),
                                      bkup_info,
                                      '/var/lib/mysql')

    def test_restore_unknown(self):
        with patch.object(backupagent, 'get_restore_strategy',
                          side_effect=ImportError):

            agent = backupagent.BackupAgent()

            bkup_info = {'id': '123',
                         'location': 'fake-location',
                         'type': 'foo',
                         'checksum': 'fake-checksum',
                         }

            self.assertRaises(UnknownBackupType, agent.execute_restore,
                              context=None, backup_info=bkup_info,
                              restore_location='/var/lib/mysql')

    def test_backup_incremental_metadata(self):
        with patch.object(backupagent, 'get_storage_strategy',
                          return_value=MockSwift):
            MockStorage.save_metadata = Mock()
            with patch.object(MockSwift, 'load_metadata',
                              return_value={'lsn': '54321'}):
                meta = {
                    'lsn': '12345',
                    'parent_location': 'fake',
                    'parent_checksum': 'md5',
                }

                mysql_impl.InnoBackupExIncremental.metadata = MagicMock(
                    return_value=meta)
                mysql_impl.InnoBackupExIncremental.run = MagicMock(
                    return_value=True)
                mysql_impl.InnoBackupExIncremental.__exit__ = MagicMock(
                    return_value=True)

                agent = backupagent.BackupAgent()

                bkup_info = {'id': '123',
                             'location': 'fake-location',
                             'type': 'InnoBackupEx',
                             'checksum': 'fake-checksum',
                             'parent': {'location': 'fake', 'checksum': 'md5'}
                             }

                agent.execute_backup(TroveContext(),
                                     bkup_info,
                                     '/var/lib/mysql')

                self.assertTrue(MockStorage.save_metadata.called_once_with(
                                ANY,
                                meta))

    def test_backup_incremental_bad_metadata(self):
        with patch.object(backupagent, 'get_storage_strategy',
                          return_value=MockSwift):

            agent = backupagent.BackupAgent()

            bkup_info = {'id': '123',
                         'location': 'fake-location',
                         'type': 'InnoBackupEx',
                         'checksum': 'fake-checksum',
                         'parent': {'location': 'fake', 'checksum': 'md5'}
                         }

            self.assertRaises(
                AttributeError,
                agent.execute_backup, TroveContext(), bkup_info, 'location')

########NEW FILE########
__FILENAME__ = test_backup_controller
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
import jsonschema
from testtools import TestCase
from testtools.matchers import Equals
from trove.backup.service import BackupController
from trove.common import apischema


class TestBackupController(TestCase):

    def setUp(self):
        super(TestBackupController, self).setUp()
        self.uuid = "d6338c9c-3cc8-4313-b98f-13cc0684cf15"
        self.invalid_uuid = "ead-edsa-e23-sdf-23"
        self.controller = BackupController()

    def test_validate_create_complete(self):
        body = {"backup": {"instance": self.uuid,
                           "name": "testback-backup"}}
        schema = self.controller.get_schema('create', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertTrue(validator.is_valid(body))

    def test_validate_create_invalid_uuid(self):
        body = {"backup": {"instance": self.invalid_uuid,
                           "name": "testback-backup"}}
        schema = self.controller.get_schema('create', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertFalse(validator.is_valid(body))
        errors = sorted(validator.iter_errors(body), key=lambda e: e.path)
        self.assertThat(errors[0].message,
                        Equals("'%s' does not match '%s'" %
                               (self.invalid_uuid, apischema.uuid['pattern'])))

    def test_validate_create_incremental(self):
        body = {"backup": {"instance": self.uuid,
                           "name": "testback-backup",
                           "parent_id": self.uuid}}
        schema = self.controller.get_schema('create', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertTrue(validator.is_valid(body))

    def test_invalid_parent_id(self):
        body = {"backup": {"instance": self.uuid,
                           "name": "testback-backup",
                           "parent_id": self.invalid_uuid}}
        schema = self.controller.get_schema('create', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertFalse(validator.is_valid(body))
        errors = sorted(validator.iter_errors(body), key=lambda e: e.path)
        self.assertThat(errors[0].message,
                        Equals("'%s' does not match '%s'" %
                               (self.invalid_uuid, apischema.uuid['pattern'])))

########NEW FILE########
__FILENAME__ = test_backup_models
#Copyright 2013 Hewlett-Packard Development Company, L.P.
#Licensed under the Apache License, Version 2.0 (the "License");
#you may not use this file except in compliance with the License.
#You may obtain a copy of the License at
#
#http://www.apache.org/licenses/LICENSE-2.0
#
#Unless required by applicable law or agreed to in writing, software
#distributed under the License is distributed on an "AS IS" BASIS,
#WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#See the License for the specific language governing permissions and
#limitations under the License.


import datetime
from mock import MagicMock, patch
import testtools

from trove.backup import models
from trove.common import context
from trove.common import exception
from trove.common import utils
from trove.instance import models as instance_models
from trove.taskmanager import api
from trove.tests.unittests.util import util


def _prep_conf(current_time):
    current_time = str(current_time)
    _context = context.TroveContext(tenant='TENANT-' + current_time)
    instance_id = 'INSTANCE-' + current_time
    return _context, instance_id

BACKUP_NAME = 'WORKS'
BACKUP_NAME_2 = 'IT-WORKS'
BACKUP_STATE = "NEW"
BACKUP_DESC = 'Backup test'
BACKUP_FILENAME = '45a3d8cb-ade8-484c-a8a5-0c3c7286fb2f.xbstream.gz'
BACKUP_LOCATION = 'https://hpcs.com/tenant/database_backups/' + BACKUP_FILENAME


class BackupCreateTest(testtools.TestCase):
    def setUp(self):
        super(BackupCreateTest, self).setUp()
        util.init_db()
        self.context, self.instance_id = _prep_conf(utils.utcnow())
        self.created = False

    def tearDown(self):
        super(BackupCreateTest, self).tearDown()
        if self.created:
            models.DBBackup.find_by(
                tenant_id=self.context.tenant).delete()

    def test_create(self):
        instance = MagicMock()
        with patch.object(instance_models.BuiltInstance, 'load',
                          return_value=instance):
            instance.validate_can_perform_action = MagicMock(
                return_value=None)
            with patch.object(models.Backup, 'validate_can_perform_action',
                              return_value=None):
                with patch.object(models.Backup, 'verify_swift_auth_token',
                                  return_value=None):
                    api.API.create_backup = MagicMock(return_value=None)
                    bu = models.Backup.create(self.context, self.instance_id,
                                              BACKUP_NAME, BACKUP_DESC)
                    self.created = True

                    self.assertEqual(BACKUP_NAME, bu.name)
                    self.assertEqual(BACKUP_DESC, bu.description)
                    self.assertEqual(self.instance_id, bu.instance_id)
                    self.assertEqual(models.BackupState.NEW, bu.state)

                    db_record = models.DBBackup.find_by(id=bu.id)
                    self.assertEqual(bu.id, db_record['id'])
                    self.assertEqual(BACKUP_NAME, db_record['name'])
                    self.assertEqual(BACKUP_DESC, db_record['description'])
                    self.assertEqual(self.instance_id,
                                     db_record['instance_id'])
                    self.assertEqual(models.BackupState.NEW,
                                     db_record['state'])

    def test_create_incremental(self):
        instance = MagicMock()
        parent = MagicMock(spec=models.DBBackup)
        with patch.object(instance_models.BuiltInstance, 'load',
                          return_value=instance):
            instance.validate_can_perform_action = MagicMock(
                return_value=None)
            with patch.object(models.Backup, 'validate_can_perform_action',
                              return_value=None):
                with patch.object(models.Backup, 'verify_swift_auth_token',
                                  return_value=None):
                    api.API.create_backup = MagicMock(return_value=None)
                    with patch.object(models.Backup, 'get_by_id',
                                      return_value=parent):

                        incremental = models.Backup.create(
                            self.context,
                            self.instance_id,
                            BACKUP_NAME,
                            BACKUP_DESC,
                            parent_id='parent_uuid')

                        self.created = True

                        db_record = models.DBBackup.find_by(id=incremental.id)
                        self.assertEqual(incremental.id,
                                         db_record['id'])
                        self.assertEqual(BACKUP_NAME,
                                         db_record['name'])
                        self.assertEqual(BACKUP_DESC,
                                         db_record['description'])
                        self.assertEqual(self.instance_id,
                                         db_record['instance_id'])
                        self.assertEqual(models.BackupState.NEW,
                                         db_record['state'])
                        self.assertEqual('parent_uuid',
                                         db_record['parent_id'])

    def test_create_instance_not_found(self):
        self.assertRaises(exception.NotFound, models.Backup.create,
                          self.context, self.instance_id,
                          BACKUP_NAME, BACKUP_DESC)

    def test_create_incremental_not_found(self):
        instance = MagicMock()
        with patch.object(instance_models.BuiltInstance, 'load',
                          return_value=instance):
            instance.validate_can_perform_action = MagicMock(
                return_value=None)
            with patch.object(models.Backup, 'validate_can_perform_action',
                              return_value=None):
                with patch.object(models.Backup, 'verify_swift_auth_token',
                                  return_value=None):
                    self.assertRaises(exception.NotFound, models.Backup.create,
                                      self.context, self.instance_id,
                                      BACKUP_NAME, BACKUP_DESC,
                                      parent_id='BAD')

    def test_create_instance_not_active(self):
        instance = MagicMock()
        with patch.object(instance_models.BuiltInstance, 'load',
                          return_value=instance):
            instance.validate_can_perform_action = MagicMock(
                side_effect=exception.UnprocessableEntity)
            self.assertRaises(exception.UnprocessableEntity,
                              models.Backup.create,
                              self.context, self.instance_id,
                              BACKUP_NAME, BACKUP_DESC)

    def test_create_backup_swift_token_invalid(self):
        instance = MagicMock()
        with patch.object(instance_models.BuiltInstance, 'load',
                          return_value=instance):
            instance.validate_can_perform_action = MagicMock(
                return_value=None)
            with patch.object(models.Backup, 'validate_can_perform_action',
                              return_value=None):
                with patch.object(models.Backup, 'verify_swift_auth_token',
                                  side_effect=exception.SwiftAuthError):
                    self.assertRaises(exception.SwiftAuthError,
                                      models.Backup.create,
                                      self.context, self.instance_id,
                                      BACKUP_NAME, BACKUP_DESC)

    def test_create_backup_datastore_operation_not_supported(self):
        instance = MagicMock()
        with patch.object(instance_models.BuiltInstance, 'load',
                          return_value=instance):
            instance.validate_can_perform_action = MagicMock(
                return_value=None)
            with patch.object(
                models.Backup, 'validate_can_perform_action',
                side_effect=exception.DatastoreOperationNotSupported
            ):
                self.assertRaises(exception.DatastoreOperationNotSupported,
                                  models.Backup.create,
                                  self.context, self.instance_id,
                                  BACKUP_NAME, BACKUP_DESC)


class BackupDeleteTest(testtools.TestCase):
    def setUp(self):
        super(BackupDeleteTest, self).setUp()
        util.init_db()
        self.context, self.instance_id = _prep_conf(utils.utcnow())

    def tearDown(self):
        super(BackupDeleteTest, self).tearDown()

    def test_delete_backup_not_found(self):
        self.assertRaises(exception.NotFound, models.Backup.delete,
                          self.context, 'backup-id')

    def test_delete_backup_is_running(self):
        backup = MagicMock()
        backup.is_running = True
        with patch.object(models.Backup, 'get_by_id', return_value=backup):
            self.assertRaises(exception.UnprocessableEntity,
                              models.Backup.delete, self.context, 'backup_id')

    def test_delete_backup_swift_token_invalid(self):
        backup = MagicMock()
        backup.is_running = False
        with patch.object(models.Backup, 'get_by_id', return_value=backup):
            with patch.object(models.Backup, 'verify_swift_auth_token',
                              side_effect=exception.SwiftAuthError):
                self.assertRaises(exception.SwiftAuthError,
                                  models.Backup.delete,
                                  self.context, 'backup_id')


class BackupORMTest(testtools.TestCase):
    def setUp(self):
        super(BackupORMTest, self).setUp()
        util.init_db()
        self.context, self.instance_id = _prep_conf(utils.utcnow())
        self.backup = models.DBBackup.create(tenant_id=self.context.tenant,
                                             name=BACKUP_NAME,
                                             state=BACKUP_STATE,
                                             instance_id=self.instance_id,
                                             deleted=False,
                                             size=2.0,
                                             location=BACKUP_LOCATION)
        self.deleted = False

    def tearDown(self):
        super(BackupORMTest, self).tearDown()
        if not self.deleted:
            models.DBBackup.find_by(tenant_id=self.context.tenant).delete()

    def test_list(self):
        backups, marker = models.Backup.list(self.context)
        self.assertIsNone(marker)
        self.assertEqual(1, len(backups))

    def test_list_for_instance(self):
        models.DBBackup.create(tenant_id=self.context.tenant,
                               name=BACKUP_NAME_2,
                               state=BACKUP_STATE,
                               instance_id=self.instance_id,
                               size=2.0,
                               deleted=False)
        backups, marker = models.Backup.list_for_instance(self.context,
                                                          self.instance_id)
        self.assertIsNone(marker)
        self.assertEqual(2, len(backups))

    def test_running(self):
        running = models.Backup.running(instance_id=self.instance_id)
        self.assertTrue(running)

    def test_not_running(self):
        not_running = models.Backup.running(instance_id='non-existent')
        self.assertFalse(not_running)

    def test_running_exclude(self):
        not_running = models.Backup.running(instance_id=self.instance_id,
                                            exclude=self.backup.id)
        self.assertFalse(not_running)

    def test_is_running(self):
        self.assertTrue(self.backup.is_running)

    def test_is_done(self):
        self.backup.state = models.BackupState.COMPLETED
        self.backup.save()
        self.assertTrue(self.backup.is_done)

    def test_not_is_running(self):
        self.backup.state = models.BackupState.COMPLETED
        self.backup.save()
        self.assertFalse(self.backup.is_running)

    def test_not_is_done(self):
        self.assertFalse(self.backup.is_done)

    def test_backup_size(self):
        db_record = models.DBBackup.find_by(id=self.backup.id)
        self.assertEqual(db_record.size, self.backup.size)

    def test_backup_delete(self):
        backup = models.DBBackup.find_by(id=self.backup.id)
        backup.delete()
        backups, marker = models.Backup.list_for_instance(self.context,
                                                          self.instance_id)
        self.assertIsNone(marker)
        self.assertEqual(0, len(backups))

    def test_delete(self):
        self.backup.delete()
        db_record = models.DBBackup.find_by(id=self.backup.id, deleted=True)
        self.assertEqual(self.instance_id, db_record['instance_id'])

    def test_deleted_not_running(self):
        self.backup.delete()
        self.assertFalse(models.Backup.running(self.instance_id))

    def test_filename(self):
        self.assertEqual(BACKUP_FILENAME, self.backup.filename)


class PaginationTests(testtools.TestCase):

    def setUp(self):
        super(PaginationTests, self).setUp()
        util.init_db()
        self.context, self.instance_id = _prep_conf(utils.utcnow())
        # Create a bunch of backups
        bkup_info = {
            'tenant_id': self.context.tenant,
            'state': BACKUP_STATE,
            'instance_id': self.instance_id,
            'size': 2.0,
            'deleted': False
        }
        for backup in xrange(50):
            bkup_info.update({'name': 'Backup-%s' % backup})
            models.DBBackup.create(**bkup_info)

    def tearDown(self):
        super(PaginationTests, self).tearDown()
        query = models.DBBackup.query()
        query.filter_by(instance_id=self.instance_id).delete()

    def test_pagination_list(self):
        # page one
        backups, marker = models.Backup.list(self.context)
        self.assertEqual(20, marker)
        self.assertEqual(20, len(backups))
        # page two
        self.context.marker = 20
        backups, marker = models.Backup.list(self.context)
        self.assertEqual(40, marker)
        self.assertEqual(20, len(backups))
        # page three
        self.context.marker = 40
        backups, marker = models.Backup.list(self.context)
        self.assertIsNone(marker)
        self.assertEqual(10, len(backups))

    def test_pagination_list_for_instance(self):
        # page one
        backups, marker = models.Backup.list_for_instance(self.context,
                                                          self.instance_id)
        self.assertEqual(20, marker)
        self.assertEqual(20, len(backups))
        # page two
        self.context.marker = 20
        backups, marker = models.Backup.list(self.context)
        self.assertEqual(40, marker)
        self.assertEqual(20, len(backups))
        # page three
        self.context.marker = 40
        backups, marker = models.Backup.list_for_instance(self.context,
                                                          self.instance_id)
        self.assertIsNone(marker)
        self.assertEqual(10, len(backups))


class OrderingTests(testtools.TestCase):

    def setUp(self):
        super(OrderingTests, self).setUp()
        util.init_db()
        now = utils.utcnow()
        self.context, self.instance_id = _prep_conf(now)
        info = {
            'tenant_id': self.context.tenant,
            'state': BACKUP_STATE,
            'instance_id': self.instance_id,
            'size': 2.0,
            'deleted': False
        }
        four = now - datetime.timedelta(days=4)
        one = now - datetime.timedelta(days=1)
        three = now - datetime.timedelta(days=3)
        two = now - datetime.timedelta(days=2)
        # Create backups out of order, save/create set the 'updated' field,
        # so we need to use the db_api directly.
        models.DBBackup().db_api.save(
            models.DBBackup(name='four', updated=four,
                            id=utils.generate_uuid(), **info))
        models.DBBackup().db_api.save(
            models.DBBackup(name='one', updated=one,
                            id=utils.generate_uuid(), **info))
        models.DBBackup().db_api.save(
            models.DBBackup(name='three', updated=three,
                            id=utils.generate_uuid(), **info))
        models.DBBackup().db_api.save(
            models.DBBackup(name='two', updated=two,
                            id=utils.generate_uuid(), **info))

    def tearDown(self):
        super(OrderingTests, self).tearDown()
        query = models.DBBackup.query()
        query.filter_by(instance_id=self.instance_id).delete()

    def test_list(self):
        backups, marker = models.Backup.list(self.context)
        self.assertIsNone(marker)
        actual = [b.name for b in backups]
        expected = [u'one', u'two', u'three', u'four']
        self.assertEqual(expected, actual)

    def test_list_for_instance(self):
        backups, marker = models.Backup.list_for_instance(self.context,
                                                          self.instance_id)
        self.assertIsNone(marker)
        actual = [b.name for b in backups]
        expected = [u'one', u'two', u'three', u'four']
        self.assertEqual(expected, actual)

########NEW FILE########
__FILENAME__ = test_storage
#Copyright 2013 Rackspace Development Company, L.P.

#Licensed under the Apache License, Version 2.0 (the "License");
#you may not use this file except in compliance with the License.
#You may obtain a copy of the License at
#
#http://www.apache.org/licenses/LICENSE-2.0
#
#Unless required by applicable law or agreed to in writing, software
#distributed under the License is distributed on an "AS IS" BASIS,
#WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#See the License for the specific language governing permissions and
#limitations under the License.

import testtools
from mock import Mock, MagicMock, patch
import hashlib

from trove.common.context import TroveContext
from trove.tests.fakes.swift import FakeSwiftConnection
from trove.tests.unittests.backup.test_backupagent \
    import MockBackup as MockBackupRunner
from trove.guestagent.strategies.storage.swift \
    import SwiftDownloadIntegrityError
from trove.guestagent.strategies.storage import swift
from trove.guestagent.strategies.storage.swift import SwiftStorage
from trove.guestagent.strategies.storage.swift import StreamReader


class SwiftStorageSaveChecksumTests(testtools.TestCase):
    """SwiftStorage.save is used to save a backup to Swift"""

    def setUp(self):
        super(SwiftStorageSaveChecksumTests, self).setUp()

    def tearDown(self):
        super(SwiftStorageSaveChecksumTests, self).tearDown()

    def test_swift_checksum_save(self):
        """This tests that SwiftStorage.save returns the swift checksum"""
        context = TroveContext()
        backup_id = '123'
        user = 'user'
        password = 'password'

        swift_client = FakeSwiftConnection()
        with patch.object(swift, 'create_swift_client',
                          return_value=swift_client):
            storage_strategy = SwiftStorage(context)

            with MockBackupRunner(filename=backup_id,
                                  user=user,
                                  password=password) as runner:
                (success,
                 note,
                 checksum,
                 location) = storage_strategy.save(runner.manifest, runner)

        self.assertTrue(success, "The backup should have been successful.")
        self.assertIsNotNone(note, "A note should have been returned.")
        self.assertEqual('http://mockswift/v1/database_backups/123.gz.enc',
                         location,
                         "Incorrect swift location was returned.")

    def test_swift_segment_checksum_etag_mismatch(self):
        """This tests that when etag doesn't match segment uploaded checksum
            False is returned and None for checksum and location
        """
        context = TroveContext()
        # this backup_id will trigger fake swift client with calculate_etag
        # enabled to spit out a bad etag when a segment object is uploaded
        backup_id = 'bad_segment_etag_123'
        user = 'user'
        password = 'password'

        swift_client = FakeSwiftConnection()
        with patch.object(swift, 'create_swift_client',
                          return_value=swift_client):
            storage_strategy = SwiftStorage(context)

            with MockBackupRunner(filename=backup_id,
                                  user=user,
                                  password=password) as runner:
                (success,
                 note,
                 checksum,
                 location) = storage_strategy.save(runner.manifest, runner)

        self.assertFalse(success, "The backup should have failed!")
        self.assertTrue(note.startswith("Error saving data to Swift!"))
        self.assertIsNone(checksum,
                          "Swift checksum should be None for failed backup.")
        self.assertEqual('http://mockswift/v1/database_backups/'
                         'bad_segment_etag_123.gz.enc',
                         location,
                         "Incorrect swift location was returned.")

    def test_swift_checksum_etag_mismatch(self):
        """This tests that when etag doesn't match swift checksum False is
            returned and None for checksum and location
        """
        context = TroveContext()
        # this backup_id will trigger fake swift client with calculate_etag
        # enabled to spit out a bad etag when a segment object is uploaded
        backup_id = 'bad_manifest_etag_123'
        user = 'user'
        password = 'password'

        swift_client = FakeSwiftConnection()
        with patch.object(swift, 'create_swift_client',
                          return_value=swift_client):
            storage_strategy = SwiftStorage(context)

            with MockBackupRunner(filename=backup_id,
                                  user=user,
                                  password=password) as runner:
                (success,
                 note,
                 checksum,
                 location) = storage_strategy.save(runner.manifest, runner)

        self.assertFalse(success, "The backup should have failed!")
        self.assertTrue(note.startswith("Error saving data to Swift!"))
        self.assertIsNone(checksum,
                          "Swift checksum should be None for failed backup.")
        self.assertEqual('http://mockswift/v1/database_backups/'
                         'bad_manifest_etag_123.gz.enc',
                         location,
                         "Incorrect swift location was returned.")


class SwiftStorageUtils(testtools.TestCase):

    def setUp(self):
        super(SwiftStorageUtils, self).setUp()
        context = TroveContext()
        swift_client = FakeSwiftConnection()
        swift.create_swift_client = MagicMock(return_value=swift_client)
        self.swift = SwiftStorage(context)

    def tearDown(self):
        super(SwiftStorageUtils, self).tearDown()

    def test_explode_location(self):
        location = 'http://mockswift.com/v1/545433/backups/mybackup.tar'
        url, container, filename = self.swift._explodeLocation(location)
        self.assertEqual(url, 'http://mockswift.com/v1/545433')
        self.assertEqual(container, 'backups')
        self.assertEqual(filename, 'mybackup.tar')

    def test_validate_checksum_good(self):
        match = self.swift._verify_checksum('"my-good-etag"', 'my-good-etag')
        self.assertTrue(match)

    def test_verify_checksum_bad(self):
        self.assertRaises(SwiftDownloadIntegrityError,
                          self.swift._verify_checksum,
                          '"THE-GOOD-THE-BAD"',
                          'AND-THE-UGLY')


class SwiftStorageLoad(testtools.TestCase):
    """SwiftStorage.load is used to return SwiftDownloadStream which is used
        to download a backup object from Swift
    """

    def setUp(self):
        super(SwiftStorageLoad, self).setUp()

    def tearDown(self):
        super(SwiftStorageLoad, self).tearDown()

    def test_run_verify_checksum(self):
        """This tests that swift download cmd runs if original backup checksum
            matches swift object etag
        """

        context = TroveContext()
        location = "/backup/location/123"
        backup_checksum = "fake-md5-sum"

        swift_client = FakeSwiftConnection()
        with patch.object(swift, 'create_swift_client',
                          return_value=swift_client):

            storage_strategy = SwiftStorage(context)
            download_stream = storage_strategy.load(location, backup_checksum)
        self.assertIsNotNone(download_stream)

    def test_run_verify_checksum_mismatch(self):
        """This tests that SwiftDownloadIntegrityError is raised and swift
            download cmd does not run when original backup checksum
            does not match swift object etag
        """

        context = TroveContext()
        location = "/backup/location/123"
        backup_checksum = "checksum_different_then_fake_swift_etag"

        swift_client = FakeSwiftConnection()
        with patch.object(swift, 'create_swift_client',
                          return_value=swift_client):
            storage_strategy = SwiftStorage(context)

        self.assertRaises(SwiftDownloadIntegrityError,
                          storage_strategy.load,
                          location,
                          backup_checksum)


class MockBackupStream(MockBackupRunner):

    def read(self, chunk_size):
        return 'X' * chunk_size


class StreamReaderTests(testtools.TestCase):

    def setUp(self):
        super(StreamReaderTests, self).setUp()
        self.runner = MockBackupStream(filename='123.xbstream.enc.gz',
                                       user='user',
                                       password='password')
        self.stream = StreamReader(self.runner,
                                   self.runner.manifest,
                                   max_file_size=100)

    def test_base_filename(self):
        self.assertEqual('123', self.stream.base_filename)

    def test_base_filename_no_extension(self):
        stream_reader = StreamReader(self.runner, 'foo')
        self.assertEqual('foo', stream_reader.base_filename)

    def test_prefix(self):
        self.assertEqual('database_backups/123_', self.stream.prefix)

    def test_segment(self):
        self.assertEqual('123_00000000', self.stream.segment)

    def test_end_of_file(self):
        self.assertFalse(self.stream.end_of_file)

    def test_end_of_segment(self):
        self.assertFalse(self.stream.end_of_segment)

    def test_segment_almost_complete(self):
        self.stream.segment_length = 98
        results = self.stream.read(2)
        self.assertEqual('XX', results)
        self.assertEqual('123_00000000', self.stream.segment,
                         "The Segment should still be the same")
        self.assertEqual(self.stream.segment_length, 100)
        checksum = hashlib.md5('XX')
        checksum = checksum.hexdigest()
        segment_checksum = self.stream.segment_checksum.hexdigest()
        self.assertEqual(checksum, segment_checksum,
                         "Segment checksum did not match")

    def test_segment_complete(self):
        self.stream.segment_length = 99
        results = self.stream.read(2)
        self.assertEqual('', results, "Results should be empty.")
        self.assertEqual('123_00000001', self.stream.segment)

    def test_stream_complete(self):
        results = self.stream.read(0)
        self.assertEqual('', results, "Results should be empty.")
        self.assertTrue(self.stream.end_of_file)


class SwiftMetadataTests(testtools.TestCase):

    def setUp(self):
        super(SwiftMetadataTests, self).setUp()
        self.swift_client = FakeSwiftConnection()
        self.context = TroveContext()
        swift.create_swift_client = MagicMock(return_value=self.swift_client)
        self.swift = SwiftStorage(self.context)

    def tearDown(self):
        super(SwiftMetadataTests, self).tearDown()

    def test__get_attr(self):
        normal_header = self.swift._get_attr('content-type')
        self.assertEqual('content_type', normal_header)
        meta_header = self.swift._get_attr('x-object-meta-foo')
        self.assertEqual('foo', meta_header)
        meta_header_two = self.swift._get_attr('x-object-meta-foo-bar')
        self.assertEqual('foo_bar', meta_header_two)

    def test__set_attr(self):
        meta_header = self.swift._set_attr('foo')
        self.assertEqual('X-Object-Meta-foo', meta_header)
        meta_header_two = self.swift._set_attr('foo_bar')
        self.assertEqual('X-Object-Meta-foo-bar', meta_header_two)

    def test_load_metadata(self):
        location = 'http://mockswift.com/v1/545433/backups/mybackup.tar'
        headers = {
            'etag': '"fake-md5-sum"',
            'x-object-meta-lsn': '1234567'
        }
        with patch.object(self.swift_client, 'head_object',
                          return_value=headers):
            metadata = self.swift.load_metadata(location, 'fake-md5-sum')
        self.assertEqual({'lsn': '1234567'}, metadata)

    def test_save_metadata(self):
        location = 'http://mockswift.com/v1/545433/backups/mybackup.tar'
        metadata = {'lsn': '1234567'}
        self.swift_client.post_object = Mock()

        self.swift.save_metadata(location, metadata=metadata)

        headers = {
            'X-Object-Meta-lsn': '1234567',
            'X-Object-Manifest': None
        }
        self.swift_client.post_object.assert_called_with(
            'backups', 'mybackup.tar', headers=headers)

########NEW FILE########
__FILENAME__ = test_context
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
import trove.common.context as context

import testtools
from testtools.matchers import Equals, Is


class TestTroveContext(testtools.TestCase):
    def test_create_with_extended_args(self):
        expected_service_catalog = {'key': 'value'}
        ctx = context.TroveContext(user="test_user_id",
                                   request_id="test_req_id",
                                   limit="500",
                                   marker="x",
                                   service_catalog=expected_service_catalog)
        self.assertThat(ctx.limit, Equals("500"))
        self.assertThat(ctx.marker, Equals("x"))
        self.assertThat(ctx.service_catalog, Equals(expected_service_catalog))

    def test_create(self):
        ctx = context.TroveContext(user='test_user_id',
                                   request_id='test_req_id')
        self.assertThat(ctx.user, Equals('test_user_id'))
        self.assertThat(ctx.request_id, Equals('test_req_id'))
        self.assertThat(ctx.limit, Is(None))
        self.assertThat(ctx.marker, Is(None))
        self.assertThat(ctx.service_catalog, Is(None))

    def test_to_dict(self):
        ctx = context.TroveContext(user='test_user_id',
                                   request_id='test_req_id')
        ctx_dict = ctx.to_dict()
        self.assertThat(ctx_dict.get('user'), Equals('test_user_id'))
        self.assertThat(ctx_dict.get('request_id'), Equals('test_req_id'))

########NEW FILE########
__FILENAME__ = test_exception
#    Copyright 2013 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


from testtools import TestCase
from trove.common.exception import TroveError


class TroveErrorTest(TestCase):

    def test_valid_error_message_format(self):
        error = TroveError("%02d" % 1)
        self.assertEqual(error.message, "01")

    def test_invalid_error_message_format(self):
        error = TroveError("test%999999sdb")
        self.assertEqual(error.message, "test999999sdb")

########NEW FILE########
__FILENAME__ = test_pagination
# Copyright 2013 OpenStack Foundation
# Copyright 2013 Rackspace Hosting
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

import testtools
from trove.common.pagination import PaginatedDataView


class TestPaginatedDataView(testtools.TestCase):

    def test_creation_with_string_marker(self):
        view = PaginatedDataView("TestType", [], "http://current_page",
                                 next_page_marker="marker")
        self.assertEqual("marker", view.next_page_marker)

    def test_creation_with_none_marker(self):
        view = PaginatedDataView("TestType", [], "http://current_page",
                                 next_page_marker=None)
        self.assertIsNone(view.next_page_marker)

    def test_creation_with_none_string_marker(self):
        view = PaginatedDataView("TestType", [], "http://current_page",
                                 next_page_marker=52)
        self.assertEqual("52", view.next_page_marker)

########NEW FILE########
__FILENAME__ = test_remote
# Copyright 2013 OpenStack Foundation
# Copyright 2013 Rackspace Hosting
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

from mock import MagicMock
import testtools
from testtools import matchers

import swiftclient.client

from trove.tests.fakes.swift import SwiftClientStub
from trove.common.context import TroveContext
from trove.common import remote
from trove.common import exception
from trove.common import cfg


class TestRemote(testtools.TestCase):
    def setUp(self):
        super(TestRemote, self).setUp()

    def tearDown(self):
        super(TestRemote, self).tearDown()

    def test_creation(self):
        swiftclient.client.Connection.get_auth = MagicMock(return_value=None)
        conn = swiftclient.client.Connection()
        self.assertIsNone(conn.get_auth())

    def test_create_swift_client(self):
        mock_resp = MagicMock()
        swiftclient.client.Connection.get_container = MagicMock(
            return_value=["text", mock_resp])
        service_catalog = [{'endpoints': [{'publicURL': 'example.com'}],
                            'type': 'object-store'}]
        client = remote.create_swift_client(TroveContext(
            tenant='123',
            service_catalog=service_catalog))
        headers, container = client.get_container('bob')
        self.assertIs(headers, "text")
        self.assertIs(container, mock_resp)

    def test_empty_account(self):
        """
        this is an account with no containers and no objects
        """
        # setup expectation
        swift_stub = SwiftClientStub()
        swift_stub.with_account('123223')
        # interact
        conn = swiftclient.client.Connection()
        account_info = conn.get_account()
        self.assertThat(account_info, matchers.Not(matchers.Is(None)))
        self.assertThat(len(account_info), matchers.Is(2))
        self.assertThat(account_info, matchers.IsInstance(tuple))
        self.assertThat(account_info[0], matchers.IsInstance(dict))
        self.assertThat(account_info[0],
                        matchers.KeysEqual('content-length', 'accept-ranges',
                                           'x-timestamp', 'x-trans-id', 'date',
                                           'x-account-bytes-used',
                                           'x-account-container-count',
                                           'content-type',
                                           'x-account-object-count'))
        self.assertThat(account_info[1], matchers.IsInstance(list))
        self.assertThat(len(account_info[1]), matchers.Is(0))

    def test_one_container(self):
        """
        tests to ensure behavior is normal with one container
        """
        # setup expectation
        swift_stub = SwiftClientStub()
        swift_stub.with_account('123223')
        cont_name = 'a-container-name'
        swift_stub.with_container(cont_name)
        # interact
        conn = swiftclient.client.Connection()
        conn.get_auth()
        conn.put_container(cont_name)
        # get headers plus container metadata
        self.assertThat(len(conn.get_account()), matchers.Is(2))
        # verify container details
        account_containers = conn.get_account()[1]
        self.assertThat(len(account_containers), matchers.Is(1))
        self.assertThat(account_containers[0],
                        matchers.KeysEqual('count', 'bytes', 'name'))
        self.assertThat(account_containers[0]['name'], matchers.Is(cont_name))
        # get container details
        cont_info = conn.get_container(cont_name)
        self.assertIsNotNone(cont_info)
        self.assertThat(cont_info[0], matchers.KeysEqual('content-length',
                        'x-container-object-count', 'accept-ranges',
                        'x-container-bytes-used', 'x-timestamp',
                        'x-trans-id', 'date', 'content-type'))
        self.assertThat(len(cont_info[1]), matchers.Equals(0))
        # remove container
        swift_stub.without_container(cont_name)
        with testtools.ExpectedException(swiftclient.ClientException):
            conn.get_container(cont_name)
            # ensure there are no more containers in account
        self.assertThat(len(conn.get_account()[1]), matchers.Is(0))

    def test_one_object(self):
        swift_stub = SwiftClientStub()
        swift_stub.with_account('123223')
        swift_stub.with_container('bob')
        swift_stub.with_object('bob', 'test', 'test_contents')
        # create connection
        conn = swiftclient.client.Connection()
        # test container lightly
        cont_info = conn.get_container('bob')
        self.assertIsNotNone(cont_info)
        self.assertThat(cont_info[0],
                        matchers.KeysEqual('content-length',
                                           'x-container-object-count',
                                           'accept-ranges',
                                           'x-container-bytes-used',
                                           'x-timestamp', 'x-trans-id', 'date',
                                           'content-type'))
        cont_objects = cont_info[1]
        self.assertThat(len(cont_objects), matchers.Equals(1))
        obj_1 = cont_objects[0]
        self.assertThat(obj_1, matchers.Equals(
            {'bytes': 13, 'last_modified': '2013-03-15T22:10:49.361950',
             'hash': 'ccc55aefbf92aa66f42b638802c5e7f6', 'name': 'test',
             'content_type': 'application/octet-stream',
             'contents': 'test_contents'}))
        # test object api - not much to do here
        self.assertThat(conn.get_object('bob', 'test')[1],
                        matchers.Is('test_contents'))

        # test remove object
        swift_stub.without_object('bob', 'test')
        # interact
        with testtools.ExpectedException(swiftclient.ClientException):
            conn.delete_object('bob', 'test')
        self.assertThat(len(conn.get_container('bob')[1]), matchers.Is(0))

    def test_two_objects(self):
        swift_stub = SwiftClientStub()
        swift_stub.with_account('123223')
        swift_stub.with_container('bob')
        swift_stub.with_container('bob2')
        swift_stub.with_object('bob', 'test', 'test_contents')
        swift_stub.with_object('bob', 'test2', 'test_contents2')

        conn = swiftclient.client.Connection()

        self.assertIs(len(conn.get_account()), 2)
        cont_info = conn.get_container('bob')
        self.assertIsNotNone(cont_info)
        self.assertThat(cont_info[0],
                        matchers.KeysEqual('content-length',
                                           'x-container-object-count',
                                           'accept-ranges',
                                           'x-container-bytes-used',
                                           'x-timestamp', 'x-trans-id', 'date',
                                           'content-type'))
        self.assertThat(len(cont_info[1]), matchers.Equals(2))
        self.assertThat(cont_info[1][0], matchers.Equals(
            {'bytes': 13, 'last_modified': '2013-03-15T22:10:49.361950',
             'hash': 'ccc55aefbf92aa66f42b638802c5e7f6', 'name': 'test',
             'content_type': 'application/octet-stream',
             'contents': 'test_contents'}))
        self.assertThat(conn.get_object('bob', 'test')[1],
                        matchers.Is('test_contents'))
        self.assertThat(conn.get_object('bob', 'test2')[1],
                        matchers.Is('test_contents2'))

        swift_stub.without_object('bob', 'test')
        with testtools.ExpectedException(swiftclient.ClientException):
            conn.delete_object('bob', 'test')
        self.assertThat(len(conn.get_container('bob')[1]), matchers.Is(1))

        swift_stub.without_container('bob')
        with testtools.ExpectedException(swiftclient.ClientException):
            conn.get_container('bob')

        self.assertThat(len(conn.get_account()), matchers.Is(2))

    def test_nonexisting_container(self):
        """
        when a container does not exist and is accessed then a 404 is returned
        """
        from trove.tests.fakes.swift import SwiftClientStub

        swift_stub = SwiftClientStub()
        swift_stub.with_account('123223')
        swift_stub.with_container('existing')

        conn = swiftclient.client.Connection()

        with testtools.ExpectedException(swiftclient.ClientException):
            conn.get_container('nonexisting')

    def test_replace_object(self):
        """
        Test to ensure that if an object is updated the container object
        count is the same and the contents of the object are updated
        """
        swift_stub = SwiftClientStub()
        swift_stub.with_account('1223df2')
        swift_stub.with_container('new-container')
        swift_stub.with_object('new-container', 'new-object',
                               'new-object-contents')

        conn = swiftclient.client.Connection()

        conn.put_object('new-container', 'new-object', 'new-object-contents')
        obj_resp = conn.get_object('new-container', 'new-object')
        self.assertThat(obj_resp, matchers.Not(matchers.Is(None)))
        self.assertThat(len(obj_resp), matchers.Is(2))
        self.assertThat(obj_resp[1], matchers.Is('new-object-contents'))

        # set expected behavior - trivial here since it is the intended
        # behavior however keep in mind this is just to support testing of
        # trove components
        swift_stub.with_object('new-container', 'new-object',
                               'updated-object-contents')

        conn.put_object('new-container', 'new-object',
                        'updated-object-contents')
        obj_resp = conn.get_object('new-container', 'new-object')
        self.assertThat(obj_resp, matchers.Not(matchers.Is(None)))
        self.assertThat(len(obj_resp), matchers.Is(2))
        self.assertThat(obj_resp[1], matchers.Is('updated-object-contents'))
        # ensure object count has not increased
        self.assertThat(len(conn.get_container('new-container')[1]),
                        matchers.Is(1))


class TestCreateCinderClient(testtools.TestCase):
    def setUp(self):
        super(TestCreateCinderClient, self).setUp()
        self.volumev2_public_url = 'http://publicURL/v2'
        self.volume_public_url_region_two = 'http://publicURL-r2/v1'
        self.service_catalog = [
            {
                'endpoints': [
                    {
                        'region': 'RegionOne',
                        'publicURL': self.volumev2_public_url,
                    }
                ],
                'type': 'volumev2'
            },
            {
                'endpoints': [
                    {
                        'region': 'RegionOne',
                        'publicURL': 'http://publicURL-r1/v1',
                    },
                    {
                        'region': 'RegionTwo',
                        'publicURL': self.volume_public_url_region_two,
                    }
                ],
                'type': 'volume'
            }
        ]

    def tearDown(self):
        super(TestCreateCinderClient, self).tearDown()
        cfg.CONF.clear_override('cinder_url')
        cfg.CONF.clear_override('cinder_service_type')
        cfg.CONF.clear_override('os_region_name')

    def test_create_with_no_conf_no_catalog(self):
        self.assertRaises(exception.EmptyCatalog,
                          remote.create_cinder_client,
                          TroveContext())

    def test_create_with_conf_override(self):
        cinder_url_from_conf = 'http://example.com'
        tenant_from_ctx = 'abc'
        cfg.CONF.set_override('cinder_url', cinder_url_from_conf)

        client = remote.create_cinder_client(
            TroveContext(tenant=tenant_from_ctx))
        self.assertEqual('%s/%s' % (cinder_url_from_conf, tenant_from_ctx),
                         client.client.management_url)

    def test_create_with_conf_override_trailing_slash(self):
        cinder_url_from_conf = 'http://example.com/'
        tenant_from_ctx = 'abc'
        cfg.CONF.set_override('cinder_url', cinder_url_from_conf)
        client = remote.create_cinder_client(
            TroveContext(tenant=tenant_from_ctx))
        self.assertEqual('%s%s' % (cinder_url_from_conf, tenant_from_ctx),
                         client.client.management_url)

    def test_create_with_catalog_and_default_service_type(self):
        client = remote.create_cinder_client(
            TroveContext(service_catalog=self.service_catalog))
        self.assertEqual(self.volumev2_public_url,
                         client.client.management_url)

    def test_create_with_catalog_all_opts(self):
        cfg.CONF.set_override('cinder_service_type', 'volume')
        cfg.CONF.set_override('os_region_name', 'RegionTwo')
        client = remote.create_cinder_client(
            TroveContext(service_catalog=self.service_catalog))
        self.assertEqual(self.volume_public_url_region_two,
                         client.client.management_url)


class TestCreateNovaClient(testtools.TestCase):
    def setUp(self):
        super(TestCreateNovaClient, self).setUp()
        self.compute_public_url = 'http://publicURL/v2'
        self.computev3_public_url_region_two = 'http://publicURL-r2/v3'
        self.service_catalog = [
            {
                'endpoints': [
                    {
                        'region': 'RegionOne',
                        'publicURL': self.compute_public_url,
                    }
                ],
                'type': 'compute'
            },
            {
                'endpoints': [
                    {
                        'region': 'RegionOne',
                        'publicURL': 'http://publicURL-r1/v1',
                    },
                    {
                        'region': 'RegionTwo',
                        'publicURL': self.computev3_public_url_region_two,
                    }
                ],
                'type': 'computev3'
            }
        ]

    def tearDown(self):
        super(TestCreateNovaClient, self).tearDown()
        cfg.CONF.clear_override('nova_compute_url')
        cfg.CONF.clear_override('nova_compute_service_type')
        cfg.CONF.clear_override('os_region_name')

    def test_create_with_no_conf_no_catalog(self):
        self.assertRaises(exception.EmptyCatalog,
                          remote.create_nova_client,
                          TroveContext())

    def test_create_with_conf_override(self):
        nova_url_from_conf = 'http://example.com'
        tenant_from_ctx = 'abc'
        cfg.CONF.set_override('nova_compute_url', nova_url_from_conf)

        client = remote.create_nova_client(
            TroveContext(tenant=tenant_from_ctx))
        self.assertEqual('%s/%s' % (nova_url_from_conf, tenant_from_ctx),
                         client.client.management_url)

    def test_create_with_conf_override_trailing_slash(self):
        nova_url_from_conf = 'http://example.com/'
        tenant_from_ctx = 'abc'
        cfg.CONF.set_override('nova_compute_url', nova_url_from_conf)
        client = remote.create_nova_client(
            TroveContext(tenant=tenant_from_ctx))
        self.assertEqual('%s%s' % (nova_url_from_conf, tenant_from_ctx),
                         client.client.management_url)

    def test_create_with_catalog_and_default_service_type(self):
        client = remote.create_nova_client(
            TroveContext(service_catalog=self.service_catalog))
        self.assertEqual(self.compute_public_url,
                         client.client.management_url)

    def test_create_with_catalog_all_opts(self):
        cfg.CONF.set_override('nova_compute_service_type', 'computev3')
        cfg.CONF.set_override('os_region_name', 'RegionTwo')
        client = remote.create_nova_client(
            TroveContext(service_catalog=self.service_catalog))
        self.assertEqual(self.computev3_public_url_region_two,
                         client.client.management_url)


class TestCreateHeatClient(testtools.TestCase):
    def setUp(self):
        super(TestCreateHeatClient, self).setUp()
        self.heat_public_url = 'http://publicURL/v2'
        self.heatv3_public_url_region_two = 'http://publicURL-r2/v3'
        self.service_catalog = [
            {
                'endpoints': [
                    {
                        'region': 'RegionOne',
                        'publicURL': self.heat_public_url,
                    }
                ],
                'type': 'orchestration'
            },
            {
                'endpoints': [
                    {
                        'region': 'RegionOne',
                        'publicURL': 'http://publicURL-r1/v1',
                    },
                    {
                        'region': 'RegionTwo',
                        'publicURL': self.heatv3_public_url_region_two,
                    }
                ],
                'type': 'orchestrationv3'
            }
        ]

    def tearDown(self):
        super(TestCreateHeatClient, self).tearDown()
        cfg.CONF.clear_override('heat_url')
        cfg.CONF.clear_override('heat_service_type')
        cfg.CONF.clear_override('os_region_name')

    def test_create_with_no_conf_no_catalog(self):
        self.assertRaises(exception.EmptyCatalog,
                          remote.create_heat_client,
                          TroveContext())

    def test_create_with_conf_override(self):
        heat_url_from_conf = 'http://example.com'
        tenant_from_ctx = 'abc'
        cfg.CONF.set_override('heat_url', heat_url_from_conf)

        client = remote.create_heat_client(
            TroveContext(tenant=tenant_from_ctx))
        self.assertEqual('%s/%s' % (heat_url_from_conf, tenant_from_ctx),
                         client.http_client.endpoint)

    def test_create_with_conf_override_trailing_slash(self):
        heat_url_from_conf = 'http://example.com/'
        tenant_from_ctx = 'abc'
        cfg.CONF.set_override('heat_url', heat_url_from_conf)
        client = remote.create_heat_client(
            TroveContext(tenant=tenant_from_ctx))
        self.assertEqual('%s%s' % (heat_url_from_conf, tenant_from_ctx),
                         client.http_client.endpoint)

    def test_create_with_catalog_and_default_service_type(self):
        client = remote.create_heat_client(
            TroveContext(service_catalog=self.service_catalog))
        self.assertEqual(self.heat_public_url,
                         client.http_client.endpoint)

    def test_create_with_catalog_all_opts(self):
        cfg.CONF.set_override('heat_service_type', 'orchestrationv3')
        cfg.CONF.set_override('os_region_name', 'RegionTwo')
        client = remote.create_heat_client(
            TroveContext(service_catalog=self.service_catalog))
        self.assertEqual(self.heatv3_public_url_region_two,
                         client.http_client.endpoint)


class TestCreateSwiftClient(testtools.TestCase):
    def setUp(self):
        super(TestCreateSwiftClient, self).setUp()
        self.swift_public_url = 'http://publicURL/v2'
        self.swiftv3_public_url_region_two = 'http://publicURL-r2/v3'
        self.service_catalog = [
            {
                'endpoints': [
                    {
                        'region': 'RegionOne',
                        'publicURL': self.swift_public_url,
                    }
                ],
                'type': 'object-store'
            },
            {
                'endpoints': [
                    {
                        'region': 'RegionOne',
                        'publicURL': 'http://publicURL-r1/v1',
                    },
                    {
                        'region': 'RegionTwo',
                        'publicURL': self.swiftv3_public_url_region_two,
                    }
                ],
                'type': 'object-storev3'
            }
        ]

    def tearDown(self):
        super(TestCreateSwiftClient, self).tearDown()
        cfg.CONF.clear_override('swift_url')
        cfg.CONF.clear_override('swift_service_type')
        cfg.CONF.clear_override('os_region_name')

    def test_create_with_no_conf_no_catalog(self):
        self.assertRaises(exception.EmptyCatalog,
                          remote.create_swift_client,
                          TroveContext())

    def test_create_with_conf_override(self):
        swift_url_from_conf = 'http://example.com/AUTH_'
        tenant_from_ctx = 'abc'
        cfg.CONF.set_override('swift_url', swift_url_from_conf)

        client = remote.create_swift_client(
            TroveContext(tenant=tenant_from_ctx))
        self.assertEqual('%s%s' % (swift_url_from_conf, tenant_from_ctx),
                         client.url)

    def test_create_with_catalog_and_default_service_type(self):
        client = remote.create_swift_client(
            TroveContext(service_catalog=self.service_catalog))
        self.assertEqual(self.swift_public_url,
                         client.url)

    def test_create_with_catalog_all_opts(self):
        cfg.CONF.set_override('swift_service_type', 'object-storev3')
        cfg.CONF.set_override('os_region_name', 'RegionTwo')
        client = remote.create_swift_client(
            TroveContext(service_catalog=self.service_catalog))
        self.assertEqual(self.swiftv3_public_url_region_two,
                         client.url)


class TestEndpoints(testtools.TestCase):
    """
    Copied from glance/tests/unit/test_auth.py.
    """
    def setUp(self):
        super(TestEndpoints, self).setUp()

        self.service_catalog = [
            {
                'endpoint_links': [],
                'endpoints': [
                    {
                        'adminURL': 'http://localhost:8080/',
                        'region': 'RegionOne',
                        'internalURL': 'http://internalURL/',
                        'publicURL': 'http://publicURL/',
                    },
                    {
                        'adminURL': 'http://localhost:8081/',
                        'region': 'RegionTwo',
                        'internalURL': 'http://internalURL2/',
                        'publicURL': 'http://publicURL2/',
                    },
                ],
                'type': 'object-store',
                'name': 'Object Storage Service',
            }
        ]

    def test_get_endpoint_empty_catalog(self):
        self.assertRaises(exception.EmptyCatalog,
                          remote.get_endpoint,
                          None)

    def test_get_endpoint_with_custom_server_type(self):
        endpoint = remote.get_endpoint(self.service_catalog,
                                       service_type='object-store',
                                       endpoint_region='RegionOne')
        self.assertEqual('http://publicURL/', endpoint)

    def test_get_endpoint_with_custom_endpoint_type(self):
        endpoint = remote.get_endpoint(self.service_catalog,
                                       service_type='object-store',
                                       endpoint_type='internalURL',
                                       endpoint_region='RegionOne')
        self.assertEqual('http://internalURL/', endpoint)

    def test_get_endpoint_raises_with_ambiguous_endpoint_region(self):
        self.assertRaises(exception.RegionAmbiguity,
                          remote.get_endpoint,
                          self.service_catalog,
                          service_type='object-store')

    def test_get_endpoint_raises_with_invalid_service_type(self):
        self.assertRaises(exception.NoServiceEndpoint,
                          remote.get_endpoint,
                          self.service_catalog,
                          service_type='foo')

    def test_get_endpoint_raises_with_invalid_endpoint_type(self):
        self.assertRaises(exception.NoServiceEndpoint,
                          remote.get_endpoint,
                          self.service_catalog,
                          service_type='object-store',
                          endpoint_type='foo',
                          endpoint_region='RegionOne')

    def test_get_endpoint_raises_with_invalid_endpoint_region(self):
        self.assertRaises(exception.NoServiceEndpoint,
                          remote.get_endpoint,
                          self.service_catalog,
                          service_type='object-store',
                          endpoint_region='foo',
                          endpoint_type='internalURL')

    def test_get_endpoint_ignores_missing_type(self):
        service_catalog = [
            {
                'name': 'Other Service',
            },
            {
                'endpoint_links': [],
                'endpoints': [
                    {
                        'adminURL': 'http://localhost:8080/',
                        'region': 'RegionOne',
                        'internalURL': 'http://internalURL/',
                        'publicURL': 'http://publicURL/',
                    },
                    {
                        'adminURL': 'http://localhost:8081/',
                        'region': 'RegionTwo',
                        'internalURL': 'http://internalURL2/',
                        'publicURL': 'http://publicURL2/',
                    },
                ],
                'type': 'object-store',
                'name': 'Object Storage Service',
            }
        ]
        endpoint = remote.get_endpoint(service_catalog,
                                       service_type='object-store',
                                       endpoint_region='RegionOne')
        self.assertEqual('http://publicURL/', endpoint)

########NEW FILE########
__FILENAME__ = test_template
#Licensed under the Apache License, Version 2.0 (the "License");
#you may not use this file except in compliance with the License.
#You may obtain a copy of the License at
#
#http://www.apache.org/licenses/LICENSE-2.0
#
#Unless required by applicable law or agreed to in writing, software
#distributed under the License is distributed on an "AS IS" BASIS,
#WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#See the License for the specific language governing permissions and
#limitations under the License.


from mock import Mock
import testtools
import re

from trove.common import template
from trove.common import exception
from trove.datastore.models import DatastoreVersion
from trove.tests.unittests.util import util


class TemplateTest(testtools.TestCase):
    def setUp(self):
        super(TemplateTest, self).setUp()
        util.init_db()
        self.env = template.ENV
        self.template = self.env.get_template("mysql/config.template")
        self.flavor_dict = {'ram': 1024, 'name': 'small', 'id': '55'}
        self.server_id = "180b5ed1-3e57-4459-b7a3-2aeee4ac012a"

    def tearDown(self):
        super(TemplateTest, self).tearDown()

    def validate_template(self, contents, teststr, test_flavor, server_id):
        # expected query_cache_size = {{ 8 * flavor_multiplier }}M
        flavor_multiplier = test_flavor['ram'] / 512
        found_group = None
        for line in contents.split('\n'):
            m = re.search('^%s.*' % teststr, line)
            if m:
                found_group = m.group(0)
        if not found_group:
            raise "Could not find text in template"
        # Check that the last group has been rendered
        memsize = found_group.split(" ")[2]
        self.assertEqual(memsize, "%sM" % (8 * flavor_multiplier))
        self.assertIsNotNone(server_id)
        self.assertTrue(server_id > 1)

    def test_rendering(self):
        rendered = self.template.render(flavor=self.flavor_dict,
                                        server_id=self.server_id)
        self.validate_template(rendered,
                               "query_cache_size",
                               self.flavor_dict,
                               self.server_id)

    def test_single_instance_config_rendering(self):
        datastore = Mock(spec=DatastoreVersion)
        datastore.datastore_name = 'MySql'
        datastore.name = 'mysql-5.6'
        datastore.manager = 'mysql'
        config = template.SingleInstanceConfigTemplate(datastore,
                                                       self.flavor_dict,
                                                       self.server_id)
        self.validate_template(config.render(), "query_cache_size",
                               self.flavor_dict, self.server_id)

    def test_renderer_discovers_special_config(self):
        """Finds our special config file for the version 'mysql-test'."""
        datastore = Mock(spec=DatastoreVersion)
        datastore.datastore_name = 'mysql'
        datastore.name = 'mysql-test'
        datastore.manager = 'mysql'
        config = template.SingleInstanceConfigTemplate(datastore,
                                                       self.flavor_dict,
                                                       self.server_id)
        self.validate_template(config.render(), "hyper",
                               {'ram': 0}, self.server_id)


class HeatTemplateLoadTest(testtools.TestCase):

    def setUp(self):
        super(HeatTemplateLoadTest, self).setUp()

    def tearDown(self):
        super(HeatTemplateLoadTest, self).tearDown()

    def test_heat_template_load_fail(self):
        self.assertRaises(exception.TroveError,
                          template.load_heat_template,
                          'mysql-blah')

    def test_heat_template_load_success(self):
        mysql_tmpl = template.load_heat_template('mysql')
        redis_tmplt = template.load_heat_template('redis')
        cassandra_tmpl = template.load_heat_template('cassandra')
        mongo_tmpl = template.load_heat_template('mongodb')
        self.assertIsNotNone(mysql_tmpl)
        self.assertIsNotNone(redis_tmplt)
        self.assertIsNotNone(cassandra_tmpl)
        self.assertIsNotNone(mongo_tmpl)

########NEW FILE########
__FILENAME__ = test_wsgi
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
import trove.common.wsgi as wsgi
import webob

import testtools
from testtools.matchers import Equals, Is, Not


class TestWsgi(testtools.TestCase):
    def test_process_request(self):
        middleware = wsgi.ContextMiddleware("test_trove")
        req = webob.BaseRequest({})
        token = 'MI23fdf2defg123'
        user_id = 'test_user_id'
        req.headers = {
            'X-User': 'do not use - deprecated',
            'X-User-ID': user_id,
            'X-Auth-Token': token,
            'X-Service-Catalog': '[]'
        }
        req.environ = {}
        # invocation
        middleware.process_request(req)
        # assertions
        ctx = req.environ[wsgi.CONTEXT_KEY]
        self.assertThat(ctx, Not(Is(None)))
        self.assertThat(ctx.user, Equals(user_id))
        self.assertThat(ctx.auth_token, Equals(token))
        self.assertEqual(0, len(ctx.service_catalog))

########NEW FILE########
__FILENAME__ = test_methods
#    Copyright 2013 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import testtools
from trove.backup import models as bkup_models
from trove.common import exception as t_exception
from trove.common import utils
from trove.common.instance import ServiceStatuses
from trove.conductor import manager as conductor_manager
from trove.guestagent.common import timeutils
from trove.instance import models as t_models
from trove.tests.unittests.util import util


# See LP bug #1255178
OLD_DBB_SAVE = bkup_models.DBBackup.save


class ConductorMethodTests(testtools.TestCase):
    def setUp(self):
        # See LP bug #1255178
        bkup_models.DBBackup.save = OLD_DBB_SAVE
        super(ConductorMethodTests, self).setUp()
        util.init_db()
        self.cond_mgr = conductor_manager.Manager()
        self.instance_id = utils.generate_uuid()

    def tearDown(self):
        super(ConductorMethodTests, self).tearDown()

    def _create_iss(self):
        new_id = utils.generate_uuid()
        iss = t_models.InstanceServiceStatus(
            id=new_id,
            instance_id=self.instance_id,
            status=ServiceStatuses.NEW)
        iss.save()
        return new_id

    def _get_iss(self, id):
        return t_models.InstanceServiceStatus.find_by(id=id)

    def _create_backup(self, name='fake backup'):
        new_id = utils.generate_uuid()
        backup = bkup_models.DBBackup.create(
            id=new_id,
            name=name,
            description='This is a fake backup object.',
            tenant_id=utils.generate_uuid(),
            state=bkup_models.BackupState.NEW,
            instance_id=self.instance_id)
        backup.save()
        return new_id

    def _get_backup(self, id):
        return bkup_models.DBBackup.find_by(id=id)

    # --- Tests for heartbeat ---

    def test_heartbeat_instance_not_found(self):
        new_id = utils.generate_uuid()
        self.assertRaises(t_exception.ModelNotFoundError,
                          self.cond_mgr.heartbeat, None, new_id, {})

    def test_heartbeat_instance_no_changes(self):
        iss_id = self._create_iss()
        old_iss = self._get_iss(iss_id)
        self.cond_mgr.heartbeat(None, self.instance_id, {})
        new_iss = self._get_iss(iss_id)
        self.assertEqual(old_iss.status_id, new_iss.status_id)
        self.assertEqual(old_iss.status_description,
                         new_iss.status_description)

    def test_heartbeat_instance_status_bogus_change(self):
        iss_id = self._create_iss()
        old_iss = self._get_iss(iss_id)
        new_status = 'potato salad'
        payload = {
            'service_status': new_status,
        }
        self.assertRaises(ValueError, self.cond_mgr.heartbeat,
                          None, self.instance_id, payload)
        new_iss = self._get_iss(iss_id)
        self.assertEqual(old_iss.status_id, new_iss.status_id)
        self.assertEqual(old_iss.status_description,
                         new_iss.status_description)

    def test_heartbeat_instance_status_changed(self):
        iss_id = self._create_iss()
        payload = {'service_status': ServiceStatuses.BUILDING.description}
        self.cond_mgr.heartbeat(None, self.instance_id, payload)
        iss = self._get_iss(iss_id)
        self.assertEqual(ServiceStatuses.BUILDING, iss.status)

    # --- Tests for update_backup ---

    def test_backup_not_found(self):
        new_bkup_id = utils.generate_uuid()
        self.assertRaises(t_exception.ModelNotFoundError,
                          self.cond_mgr.update_backup,
                          None, self.instance_id, new_bkup_id)

    def test_backup_instance_id_nomatch(self):
        new_iid = utils.generate_uuid()
        bkup_id = self._create_backup('nomatch')
        old_name = self._get_backup(bkup_id).name
        self.cond_mgr.update_backup(None, new_iid, bkup_id,
                                    name="remains unchanged")
        bkup = self._get_backup(bkup_id)
        self.assertEqual(old_name, bkup.name)

    def test_backup_bogus_fields_not_changed(self):
        bkup_id = self._create_backup('bogus')
        self.cond_mgr.update_backup(None, self.instance_id, bkup_id,
                                    not_a_valid_field="INVALID")
        bkup = self._get_backup(bkup_id)
        self.assertFalse(hasattr(bkup, 'not_a_valid_field'))

    def test_backup_real_fields_changed(self):
        bkup_id = self._create_backup('realrenamed')
        new_name = "recently renamed"
        self.cond_mgr.update_backup(None, self.instance_id, bkup_id,
                                    name=new_name)
        bkup = self._get_backup(bkup_id)
        self.assertEqual(new_name, bkup.name)

    # --- Tests for discarding old messages ---

    def test_heartbeat_newer_timestamp_accepted(self):
        new_p = {'service_status': ServiceStatuses.NEW.description}
        build_p = {'service_status': ServiceStatuses.BUILDING.description}
        iss_id = self._create_iss()
        iss = self._get_iss(iss_id)
        now = timeutils.float_utcnow()
        future = now + 60
        self.cond_mgr.heartbeat(None, self.instance_id, new_p, sent=now)
        self.cond_mgr.heartbeat(None, self.instance_id, build_p, sent=future)
        iss = self._get_iss(iss_id)
        self.assertEqual(ServiceStatuses.BUILDING, iss.status)

    def test_heartbeat_older_timestamp_discarded(self):
        new_p = {'service_status': ServiceStatuses.NEW.description}
        build_p = {'service_status': ServiceStatuses.BUILDING.description}
        iss_id = self._create_iss()
        iss = self._get_iss(iss_id)
        now = timeutils.float_utcnow()
        past = now - 60
        self.cond_mgr.heartbeat(None, self.instance_id, new_p, sent=past)
        self.cond_mgr.heartbeat(None, self.instance_id, build_p, sent=past)
        iss = self._get_iss(iss_id)
        self.assertEqual(ServiceStatuses.NEW, iss.status)

    def test_backup_newer_timestamp_accepted(self):
        old_name = "oldname"
        new_name = "renamed"
        bkup_id = self._create_backup(old_name)
        bkup = self._get_backup(bkup_id)
        now = timeutils.float_utcnow()
        future = now + 60
        self.cond_mgr.update_backup(None, self.instance_id, bkup_id,
                                    sent=now, name=old_name)
        self.cond_mgr.update_backup(None, self.instance_id, bkup_id,
                                    sent=future, name=new_name)
        bkup = self._get_backup(bkup_id)
        self.assertEqual(new_name, bkup.name)

    def test_backup_older_timestamp_discarded(self):
        old_name = "oldname"
        new_name = "renamed"
        bkup_id = self._create_backup(old_name)
        bkup = self._get_backup(bkup_id)
        now = timeutils.float_utcnow()
        past = now - 60
        self.cond_mgr.update_backup(None, self.instance_id, bkup_id,
                                    sent=now, name=old_name)
        self.cond_mgr.update_backup(None, self.instance_id, bkup_id,
                                    sent=past, name=new_name)
        bkup = self._get_backup(bkup_id)
        self.assertEqual(old_name, bkup.name)

########NEW FILE########
__FILENAME__ = test_configuration_controller
# Copyright 2014 Rackspace
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

import jsonschema
from testtools import TestCase
from trove.configuration.service import ConfigurationsController
from trove.common import configurations


class TestConfigurationParser(TestCase):
    def setUp(self):
        super(TestConfigurationParser, self).setUp()

    def test_parse_my_cnf_correctly(self):
        config = """
        [mysqld]
        pid-file = /var/run/mysqld/mysqld.pid
        connect_timeout = 15
        # we need to test no value params
        skip-external-locking
        ;another comment
        !includedir /etc/mysql/conf.d/
        """
        cfg_parser = configurations.MySQLConfParser(config)
        parsed = cfg_parser.parse()
        d_parsed = dict(parsed)
        self.assertIsNotNone(d_parsed)
        self.assertEqual(d_parsed["pid-file"], "/var/run/mysqld/mysqld.pid")
        self.assertEqual(d_parsed["connect_timeout"], '15')
        self.assertEqual(d_parsed["skip-external-locking"], '1')


class TestConfigurationController(TestCase):
    def setUp(self):
        super(TestConfigurationController, self).setUp()
        self.controller = ConfigurationsController()

    def test_validate_create_configuration(self):
        body = {
            "configuration": {
                "values": {},
                "name": "test",
                "datastore": {
                    "type": "test_type",
                    "version": "test_version"
                }
            }
        }
        schema = self.controller.get_schema('create', body)
        self.assertIsNotNone(schema)
        validator = jsonschema.Draft4Validator(schema)
        self.assertTrue(validator.is_valid(body))

    def test_validate_create_configuration_no_datastore(self):
        body = {
            "configuration": {
                "values": {},
                "name": "test"
            }
        }
        schema = self.controller.get_schema('create', body)
        self.assertIsNotNone(schema)
        validator = jsonschema.Draft4Validator(schema)
        self.assertTrue(validator.is_valid(body))

    def test_validate_create_invalid_values_param(self):
        body = {
            "configuration": {
                "values": '',
                "name": "test",
                "datastore": {
                    "type": "test_type",
                    "version": "test_version"
                }
            }
        }
        schema = self.controller.get_schema('create', body)
        self.assertIsNotNone(schema)
        validator = jsonschema.Draft4Validator(schema)
        self.assertFalse(validator.is_valid(body))
        errors = sorted(validator.iter_errors(body), key=lambda e: e.path)
        error_messages = [error.message for error in errors]
        self.assertIn("'' is not of type 'object'", error_messages)

    def test_validate_create_invalid_name_param(self):
        body = {
            "configuration": {
                "values": {},
                "name": "",
                "datastore": {
                    "type": "test_type",
                    "version": "test_version"
                }
            }
        }
        schema = self.controller.get_schema('create', body)
        self.assertIsNotNone(schema)
        validator = jsonschema.Draft4Validator(schema)
        self.assertFalse(validator.is_valid(body))
        errors = sorted(validator.iter_errors(body), key=lambda e: e.path)
        error_messages = [error.message for error in errors]
        self.assertIn("'' is too short", error_messages)

    def test_validate_edit_configuration(self):
        body = {
            "configuration": {
                "values": {}
            }
        }
        schema = self.controller.get_schema('edit', body)
        self.assertIsNotNone(schema)
        validator = jsonschema.Draft4Validator(schema)
        self.assertTrue(validator.is_valid(body))

########NEW FILE########
__FILENAME__ = test_datastore
# Copyright 2014 Rackspace
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from testtools import TestCase
from trove.datastore import models as datastore_models
from trove.common.exception import DatastoreDefaultDatastoreNotFound


class TestDatastore(TestCase):
    def setUp(self):
        super(TestDatastore, self).setUp()

    def test_create_failure_with_datastore_default_notfound(self):
        self.assertRaises(
            DatastoreDefaultDatastoreNotFound,
            datastore_models.get_datastore_version)

########NEW FILE########
__FILENAME__ = test_designate_driver
#    Copyright 2013 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
import testtools
from designateclient.v1.domains import Domain
from designateclient.v1.records import Record
from trove.dns.designate import driver
from mock import MagicMock
import base64
import hashlib


class DesignateObjectConverterTest(testtools.TestCase):

    def setUp(self):
        super(DesignateObjectConverterTest, self).setUp()

    def tearDown(self):
        super(DesignateObjectConverterTest, self).tearDown()

    def test_convert_domain_to_zone(self):
        name = 'www.example.com'
        id = '39413651-3b9e-41f1-a4df-e47d5e9f67be'
        email = 'john.smith@openstack.com'
        domain = Domain(name=name, id=id, email=email)
        converter = driver.DesignateObjectConverter()
        converted_domain = converter.domain_to_zone(domain)
        self.assertEqual(name, converted_domain.name)
        self.assertEqual(id, converted_domain.id)

    def test_convert_record_to_entry(self):
        name = 'test.example.com'
        id = '4f3439ef-fc8b-4098-a1aa-a66ed01102b9'
        domain_id = '39413651-3b9e-41f1-a4df-e47d5e9f67be'
        domain_name = 'example.com'
        type = 'CNAME'
        data = '127.0.0.1'
        ttl = 3600
        priority = 1
        zone = driver.DesignateDnsZone(domain_id, domain_name)
        record = Record(name=name, id=id, domain_id=domain_id, type=type,
                        data=data, priority=priority, ttl=ttl)
        converter = driver.DesignateObjectConverter()
        converted_record = converter.record_to_entry(record, zone)
        self.assertEqual(name, converted_record.name)
        self.assertEqual(data, converted_record.content)
        self.assertEqual(type, converted_record.type)
        self.assertEqual(priority, converted_record.priority)
        self.assertEqual(ttl, converted_record.ttl)
        self.assertEqual(zone, converted_record.dns_zone)


class DesignateDriverTest(testtools.TestCase):

    def setUp(self):
        super(DesignateDriverTest, self).setUp()
        self.domains = [Domain(name='www.example.com',
                               id='11111111-1111-1111-1111-111111111111',
                               email='test@example.com'),
                        Domain(name='www.trove.com',
                               id='22222222-2222-2222-2222-222222222222',
                               email='test@trove.com'),
                        Domain(name='www.openstack.com',
                               id='33333333-3333-3333-3333-333333333333',
                               email='test@openstack.com')]
        self.records = [Record(name='record1', type='A', data='10.0.0.1',
                               ttl=3600, priority=1),
                        Record(name='record2', type='CNAME', data='10.0.0.2',
                               ttl=1800, priority=2),
                        Record(name='record3', type='A', data='10.0.0.3',
                               ttl=3600, priority=1)]

    def tearDown(self):
        super(DesignateDriverTest, self).tearDown()

    def test_get_entries_by_name(self):
        zone = driver.DesignateDnsZone('123', 'www.example.com')
        driver.create_designate_client = MagicMock(return_value=None)
        driver.DesignateDriver._get_records = MagicMock(
            return_value=self.records)
        dns_driver = driver.DesignateDriver()
        entries = dns_driver.get_entries_by_name('record2', zone)
        self.assertTrue(len(entries) == 1, 'More than one record found')
        entry = entries[0]
        self.assertEqual('record2', entry.name)
        self.assertEqual('CNAME', entry.type)
        self.assertEqual('10.0.0.2', entry.content)
        self.assertEqual(1800, entry.ttl)
        self.assertEqual(2, entry.priority)
        zone = entry.dns_zone
        self.assertEqual('123', zone.id)
        self.assertEqual('www.example.com', zone.name)

    def test_get_entries_by_name_not_found(self):
        zone = driver.DesignateDnsZone('123', 'www.example.com')
        driver.create_designate_client = MagicMock(return_value=None)
        driver.DesignateDriver._get_records = MagicMock(
            return_value=self.records)
        dns_driver = driver.DesignateDriver()
        entries = dns_driver.get_entries_by_name('record_not_found', zone)
        self.assertTrue(len(entries) == 0, 'Some records were returned')

    def test_get_entries_by_content(self):
        zone = driver.DesignateDnsZone('123', 'www.example.com')
        driver.create_designate_client = MagicMock(return_value=None)
        driver.DesignateDriver._get_records = MagicMock(
            return_value=self.records)
        dns_driver = driver.DesignateDriver()
        entries = dns_driver.get_entries_by_content('10.0.0.1', zone)
        self.assertTrue(len(entries) == 1, 'More than one record found')
        entry = entries[0]
        self.assertEqual('record1', entry.name)
        self.assertEqual('A', entry.type)
        self.assertEqual('10.0.0.1', entry.content)
        self.assertEqual(3600, entry.ttl)
        self.assertEqual(1, entry.priority)
        zone = entry.dns_zone
        self.assertEqual('123', zone.id)
        self.assertEqual('www.example.com', zone.name)

    def test_get_entries_by_content_not_found(self):
        zone = driver.DesignateDnsZone('123', 'www.example.com')
        driver.create_designate_client = MagicMock(return_value=None)
        driver.DesignateDriver._get_records = MagicMock(
            return_value=self.records)
        dns_driver = driver.DesignateDriver()
        entries = dns_driver.get_entries_by_content('127.0.0.1', zone)
        self.assertTrue(len(entries) == 0, 'Some records were returned')

    def test_get_dnz_zones(self):
        client = MagicMock()
        driver.create_designate_client = MagicMock(return_value=client)
        client.domains.list = MagicMock(return_value=self.domains)
        dns_driver = driver.DesignateDriver()
        zones = dns_driver.get_dns_zones()
        self.assertTrue(len(zones) == 3)
        for x in range(0, 3):
            self.assertDomainsAreEqual(self.domains[x], zones[x])

    def test_get_dnz_zones_by_name(self):
        client = MagicMock()
        driver.create_designate_client = MagicMock(return_value=client)
        client.domains.list = MagicMock(return_value=self.domains)
        dns_driver = driver.DesignateDriver()
        zones = dns_driver.get_dns_zones('www.trove.com')
        self.assertTrue(len(zones) == 1)
        self.assertDomainsAreEqual(self.domains[1], zones[0])

    def test_get_dnz_zones_not_found(self):
        client = MagicMock()
        driver.create_designate_client = MagicMock(return_value=client)
        client.domains.list = MagicMock(return_value=self.domains)
        dns_driver = driver.DesignateDriver()
        zones = dns_driver.get_dns_zones('www.notfound.com')
        self.assertTrue(len(zones) == 0)

    def assertDomainsAreEqual(self, expected, actual):
        self.assertEqual(expected.name, actual.name)
        self.assertEqual(expected.id, actual.id)


class DesignateInstanceEntryFactoryTest(testtools.TestCase):

    def setUp(self):
        super(DesignateInstanceEntryFactoryTest, self).setUp()

    def tearDown(self):
        super(DesignateInstanceEntryFactoryTest, self).tearDown()

    def test_create_entry(self):
        instance_id = '11111111-2222-3333-4444-555555555555'
        driver.DNS_DOMAIN_ID = '00000000-0000-0000-0000-000000000000'
        driver.DNS_DOMAIN_NAME = 'trove.com'
        driver.DNS_TTL = 3600
        hashed_id = base64.b32encode(hashlib.md5(instance_id).digest())
        hashed_id_concat = hashed_id[:11].lower()
        exp_hostname = ("%s.%s" % (hashed_id_concat, driver.DNS_DOMAIN_NAME))
        factory = driver.DesignateInstanceEntryFactory()
        entry = factory.create_entry(instance_id)
        self.assertEqual(exp_hostname, entry.name)
        self.assertEqual('A', entry.type)
        self.assertEqual(3600, entry.ttl)
        zone = entry.dns_zone
        self.assertEqual(driver.DNS_DOMAIN_NAME, zone.name)
        self.assertEqual(driver.DNS_DOMAIN_ID, zone.id)

    def test_create_entry_ends_with_dot(self):
        instance_id = '11111111-2222-3333-4444-555555555555'
        driver.DNS_DOMAIN_ID = '00000000-0000-0000-0000-000000000000'
        driver.DNS_DOMAIN_NAME = 'trove.com.'
        driver.DNS_TTL = 3600
        hashed_id = base64.b32encode(hashlib.md5(instance_id).digest())
        hashed_id_concat = hashed_id[:11].lower()
        exp_hostname = ("%s.%s" %
                        (hashed_id_concat, driver.DNS_DOMAIN_NAME))[:-1]
        factory = driver.DesignateInstanceEntryFactory()
        entry = factory.create_entry(instance_id)
        self.assertEqual(exp_hostname, entry.name)

########NEW FILE########
__FILENAME__ = test_api
#    Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
import mock
import testtools
from testtools.matchers import KeysEqual, Is

import trove.common.context as context
from trove.common import exception
from trove.guestagent import api
import trove.openstack.common.rpc as rpc
import trove.common.rpc as trove_rpc


def _mock_call_pwd_change(cmd, users=None):
    if users == 'dummy':
        return True
    else:
        raise BaseException("Test Failed")


def _mock_call(cmd, timerout, username=None, hostname=None,
               database=None, databases=None):
    #To check get_user, list_access, grant_access, revoke_access in cmd.
    if cmd in ('get_user', 'list_access', 'grant_access', 'revoke_access'):
        return True
    else:
        raise BaseException("Test Failed")


class ApiTest(testtools.TestCase):
    def setUp(self):
        super(ApiTest, self).setUp()
        self.context = context.TroveContext()
        self.guest = api.API(self.context, 0)
        self.guest._cast = _mock_call_pwd_change
        self.guest._call = _mock_call
        self.FAKE_ID = 'instance-id-x23d2d'
        self.api = api.API(self.context, self.FAKE_ID)

    def test_change_passwords(self):
        self.assertIsNone(self.guest.change_passwords("dummy"))

    def test_get_user(self):
        self.assertTrue(self.guest.get_user("dummyname", "dummyhost"))

    def test_list_access(self):
        self.assertTrue(self.guest.list_access("dummyname", "dummyhost"))

    def test_grant_access(self):
        self.assertTrue(self.guest.grant_access("dumname", "dumhost", "dumdb"))

    def test_revoke_access(self):
        self.assertTrue(self.guest.revoke_access("dumname", "dumhost",
                                                 "dumdb"))

    def test_get_routing_key(self):
        self.assertEqual('guestagent.' + self.FAKE_ID,
                         self.api._get_routing_key())

    @mock.patch('trove.guestagent.models.AgentHeartBeat')
    def test_check_for_heartbeat_positive(self, mock_agent):
        self.assertTrue(self.api._check_for_hearbeat())

    @mock.patch('trove.guestagent.models.AgentHeartBeat')
    def test_check_for_heartbeat_exception(self, mock_agent):
        # TODO(juice): maybe it would be ok to extend the test to validate
        # the is_active method on the heartbeat
        mock_agent.find_by.side_effect = exception.ModelNotFoundError("Uh Oh!")
        # execute
        self.assertRaises(exception.GuestTimeout, self.api._check_for_hearbeat)
        # validate
        self.assertEqual(mock_agent.is_active.call_count, 0)

    @mock.patch('trove.guestagent.models.AgentHeartBeat')
    def test_check_for_heartbeat_negative(self, mock_agent):
        # TODO(juice): maybe it would be ok to extend the test to validate
        # the is_active method on the heartbeat
        mock_agent.is_active.return_value = False
        self.assertRaises(exception.GuestTimeout, self.api._check_for_hearbeat)

    def test_delete_queue(self):
        trove_rpc.delete_queue = mock.Mock()
        # execute
        self.api.delete_queue()
        # verify
        trove_rpc.delete_queue.assert_called_with(self.context, mock.ANY)

    def test_create_user(self):
        rpc.cast = mock.Mock()
        exp_msg = RpcMsgMatcher('create_user', 'users')
        self.api.create_user('test_user')
        self._verify_rpc_cast(exp_msg, rpc.cast)

    def test_rpc_cast_exception(self):
        rpc.cast = mock.Mock(side_effect=IOError('host down'))
        exp_msg = RpcMsgMatcher('create_user', 'users')
        # execute
        with testtools.ExpectedException(exception.GuestError, '.* host down'):
            self.api.create_user('test_user')
        # verify
        self._verify_rpc_cast(exp_msg, rpc.cast)

    def test_list_users(self):
        exp_resp = ['user1', 'user2', 'user3']
        rpc.call = mock.Mock(return_value=exp_resp)
        exp_msg = RpcMsgMatcher('list_users', 'limit', 'marker',
                                'include_marker')
        # execute
        act_resp = self.api.list_users()
        # verify
        self.assertThat(act_resp, Is(exp_resp))
        self._verify_rpc_call(exp_msg, rpc.call)

    def test_rpc_call_exception(self):
        rpc.call = mock.Mock(side_effect=IOError('host_down'))
        exp_msg = RpcMsgMatcher('list_users', 'limit', 'marker',
                                'include_marker')
        # execute
        with testtools.ExpectedException(exception.GuestError,
                                         'An error occurred.*'):
            self.api.list_users()
        # verify
        self._verify_rpc_call(exp_msg, rpc.call)

    def test_delete_user(self):
        rpc.cast = mock.Mock()
        exp_msg = RpcMsgMatcher('delete_user', 'user')
        # execute
        self.api.delete_user('test_user')
        # verify
        self._verify_rpc_cast(exp_msg, rpc.cast)

    def test_create_database(self):
        rpc.cast = mock.Mock()
        exp_msg = RpcMsgMatcher('create_database', 'databases')
        # execute
        self.api.create_database(['db1', 'db2', 'db3'])
        # verify
        self._verify_rpc_cast(exp_msg, rpc.cast)

    def test_list_databases(self):
        exp_resp = ['db1', 'db2', 'db3']
        rpc.call = mock.Mock(return_value=exp_resp)
        exp_msg = RpcMsgMatcher('list_databases', 'limit', 'marker',
                                'include_marker')
        # execute
        resp = self.api.list_databases(limit=1, marker=2,
                                       include_marker=False)
        # verify
        self.assertThat(resp, Is(exp_resp))
        self._verify_rpc_call(exp_msg, rpc.call)

    def test_delete_database(self):
        rpc.cast = mock.Mock()
        exp_msg = RpcMsgMatcher('delete_database', 'database')
        # execute
        self.api.delete_database('test_database_name')
        # verify
        self._verify_rpc_cast(exp_msg, rpc.cast)

    def test_enable_root(self):
        rpc.call = mock.Mock(return_value=True)
        exp_msg = RpcMsgMatcher('enable_root')
        # execute
        self.assertThat(self.api.enable_root(), Is(True))
        # verify
        self._verify_rpc_call(exp_msg, rpc.call)

    def test_disable_root(self):
        rpc.call = mock.Mock(return_value=True)
        exp_msg = RpcMsgMatcher('disable_root')
        # execute
        self.assertThat(self.api.disable_root(), Is(True))
        # verify
        self._verify_rpc_call(exp_msg, rpc.call)

    def test_is_root_enabled(self):
        rpc.call = mock.Mock(return_value=False)
        exp_msg = RpcMsgMatcher('is_root_enabled')
        # execute
        self.assertThat(self.api.is_root_enabled(), Is(False))
        # verify
        self._verify_rpc_call(exp_msg, rpc.call)

    def test_get_hwinfo(self):
        rpc.call = mock.Mock(return_value='[blah]')
        exp_msg = RpcMsgMatcher('get_hwinfo')
        # execute
        self.assertThat(self.api.get_hwinfo(), Is('[blah]'))
        # verify
        self._verify_rpc_call(exp_msg, rpc.call)

    def test_get_diagnostics(self):
        rpc.call = mock.Mock(spec=rpc, return_value='[all good]')
        exp_msg = RpcMsgMatcher('get_diagnostics')
        # execute
        self.assertThat(self.api.get_diagnostics(), Is('[all good]'))
        # verify
        self._verify_rpc_call(exp_msg, rpc.call)

    def test_restart(self):
        rpc.call = mock.Mock()
        exp_msg = RpcMsgMatcher('restart')
        # execute
        self.api.restart()
        # verify
        self._verify_rpc_call(exp_msg, rpc.call)

    def test_start_db_with_conf_changes(self):
        rpc.call = mock.Mock()
        exp_msg = RpcMsgMatcher('start_db_with_conf_changes',
                                'config_contents')
        # execute
        self.api.start_db_with_conf_changes(None)
        # verify
        self._verify_rpc_call(exp_msg, rpc.call)

    def test_stop_db(self):
        rpc.call = mock.Mock()
        exp_msg = RpcMsgMatcher('stop_db', 'do_not_start_on_reboot')
        # execute
        self.api.stop_db(do_not_start_on_reboot=False)
        # verify
        self._verify_rpc_call(exp_msg, rpc.call)

    def test_get_volume_info(self):
        fake_resp = {'fake': 'resp'}
        rpc.call = mock.Mock(return_value=fake_resp)
        exp_msg = RpcMsgMatcher('get_filesystem_stats', 'fs_path')
        # execute
        self.assertThat(self.api.get_volume_info(), Is(fake_resp))
        # verify
        self._verify_rpc_call(exp_msg, rpc.call)

    def test_update_guest(self):
        rpc.call = mock.Mock()
        exp_msg = RpcMsgMatcher('update_guest')
        # execute
        self.api.update_guest()
        # verify
        self._verify_rpc_call(exp_msg, rpc.call)

    def test_create_backup(self):
        rpc.cast = mock.Mock()
        exp_msg = RpcMsgMatcher('create_backup', 'backup_info')
        # execute
        self.api.create_backup({'id': '123'})
        # verify
        self._verify_rpc_cast(exp_msg, rpc.cast)

    def test_update_overrides(self):
        rpc.cast = mock.Mock()
        exp_msg = RpcMsgMatcher('update_overrides', 'overrides', 'remove')
        # execute
        self.api.update_overrides('123')
        # verify
        self._verify_rpc_cast(exp_msg, rpc.cast)

    def test_apply_overrides(self):
        rpc.cast = mock.Mock()
        exp_msg = RpcMsgMatcher('apply_overrides', 'overrides')
        # execute
        self.api.apply_overrides('123')
        # verify
        self._verify_rpc_cast(exp_msg, rpc.cast)

    def _verify_rpc_connection_and_cast(self, rpc, mock_conn, exp_msg):
        rpc.create_connection.assert_called_with(new=True)
        mock_conn.create_consumer.assert_called_with(
            self.api._get_routing_key(), None, fanout=False)
        rpc.cast.assert_called_with(mock.ANY, mock.ANY, exp_msg)

    def test_prepare(self):
        mock_conn = mock.Mock()
        rpc.create_connection = mock.Mock(return_value=mock_conn)
        rpc.cast = mock.Mock()
        exp_msg = RpcMsgMatcher('prepare', 'memory_mb', 'packages',
                                'databases', 'users', 'device_path',
                                'mount_point', 'backup_info',
                                'config_contents', 'root_password',
                                'overrides')
        # execute
        self.api.prepare('2048', 'package1', 'db1', 'user1', '/dev/vdt',
                         '/mnt/opt', 'bkup-1232', 'cont', '1-2-3-4',
                         'override')
        # verify
        self._verify_rpc_connection_and_cast(rpc, mock_conn, exp_msg)

    def test_prepare_with_backup(self):
        mock_conn = mock.Mock()
        rpc.create_connection = mock.Mock(return_value=mock_conn)
        rpc.cast = mock.Mock()
        exp_msg = RpcMsgMatcher('prepare', 'memory_mb', 'packages',
                                'databases', 'users', 'device_path',
                                'mount_point', 'backup_info',
                                'config_contents', 'root_password',
                                'overrides')
        bkup = {'id': 'backup_id_123'}
        # execute
        self.api.prepare('2048', 'package1', 'db1', 'user1', '/dev/vdt',
                         '/mnt/opt', bkup, 'cont', '1-2-3-4',
                         'overrides')
        # verify
        self._verify_rpc_connection_and_cast(rpc, mock_conn, exp_msg)

    def test_upgrade(self):
        mock_conn = mock.Mock()
        rpc.create_connection = mock.Mock(return_value=mock_conn)
        rpc.cast = mock.Mock()
        exp_msg = RpcMsgMatcher('upgrade')
        # execute
        self.api.upgrade()
        # verify
        self._verify_rpc_connection_and_cast(rpc, mock_conn, exp_msg)

    def test_rpc_cast_with_consumer_exception(self):
        mock_conn = mock.Mock()
        rpc.create_connection = mock.Mock(side_effect=IOError('host down'))
        rpc.cast = mock.Mock()
        # execute
        with testtools.ExpectedException(exception.GuestError, '.* host down'):
            self.api.prepare('2048', 'package1', 'db1', 'user1', '/dev/vdt',
                             '/mnt/opt')
        # verify
        rpc.create_connection.assert_called_with(new=True)
        self.assertThat(mock_conn.call_count, Is(0))
        self.assertThat(rpc.cast.call_count, Is(0))

    def _verify_rpc_call(self, exp_msg, mock_call=None):
        mock_call.assert_called_with(self.context, mock.ANY, exp_msg,
                                     mock.ANY)

    def _verify_rpc_cast(self, exp_msg, mock_cast=None):
        mock_cast.assert_called_with(mock.ANY,
                                     mock.ANY, exp_msg)


class CastWithConsumerTest(testtools.TestCase):
    def setUp(self):
        super(CastWithConsumerTest, self).setUp()
        self.context = context.TroveContext()
        self.api = api.API(self.context, 'instance-id-x23d2d')

    def test_cast_with_consumer(self):
        mock_conn = mock.Mock()
        rpc.create_connection = mock.Mock(return_value=mock_conn)
        rpc.cast = mock.Mock()
        # execute
        self.api._cast_with_consumer('fake_method_name', fake_param=1)
        # verify
        rpc.create_connection.assert_called_with(new=True)
        mock_conn.create_consumer.assert_called_with(mock.ANY, None,
                                                     fanout=False)
        rpc.cast.assert_called_with(self.context, mock.ANY, mock.ANY)


class RpcMsgMatcher(object):
    def __init__(self, method, *args_dict):
        self.wanted_method = method
        self.wanted_dict = KeysEqual('version', 'method', 'args', 'namespace')
        args_dict = args_dict or [{}]
        self.args_dict = KeysEqual(*args_dict)

    def __eq__(self, arg):
        if self.wanted_method != arg['method']:
            raise Exception("Method does not match: %s != %s" %
                            (self.wanted_method, arg['method']))
            #return False
        if self.wanted_dict.match(arg) or self.args_dict.match(arg['args']):
            raise Exception("Args do not match: %s != %s" %
                            (self.args_dict, arg['args']))
            #return False
        return True

    def __repr__(self):
        return "<Dict: %s>" % self.wanted_dict

########NEW FILE########
__FILENAME__ = test_backups
#    Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import testtools
import mock

import trove.guestagent.strategies.backup.base as backupBase
import trove.guestagent.strategies.restore.base as restoreBase

from trove.guestagent.strategies.backup import mysql_impl
from trove.common import utils

BACKUP_XTRA_CLS = ("trove.guestagent.strategies.backup."
                   "mysql_impl.InnoBackupEx")
RESTORE_XTRA_CLS = ("trove.guestagent.strategies.restore."
                    "mysql_impl.InnoBackupEx")
BACKUP_XTRA_INCR_CLS = ("trove.guestagent.strategies.backup."
                        "mysql_impl.InnoBackupExIncremental")
RESTORE_XTRA_INCR_CLS = ("trove.guestagent.strategies.restore."
                         "mysql_impl.InnoBackupExIncremental")
BACKUP_SQLDUMP_CLS = ("trove.guestagent.strategies.backup."
                      "mysql_impl.MySQLDump")
RESTORE_SQLDUMP_CLS = ("trove.guestagent.strategies.restore."
                       "mysql_impl.MySQLDump")
PIPE = " | "
ZIP = "gzip"
UNZIP = "gzip -d -c"
ENCRYPT = "openssl enc -aes-256-cbc -salt -pass pass:default_aes_cbc_key"
DECRYPT = "openssl enc -d -aes-256-cbc -salt -pass pass:default_aes_cbc_key"
XTRA_BACKUP_RAW = ("sudo innobackupex --stream=xbstream %(extra_opts)s"
                   " /var/lib/mysql 2>/tmp/innobackupex.log")
XTRA_BACKUP = XTRA_BACKUP_RAW % {'extra_opts': ''}
XTRA_BACKUP_EXTRA_OPTS = XTRA_BACKUP_RAW % {'extra_opts': '--no-lock'}
XTRA_BACKUP_INCR = ('sudo innobackupex --stream=xbstream'
                    ' --incremental --incremental-lsn=%(lsn)s'
                    ' %(extra_opts)s /var/lib/mysql 2>/tmp/innobackupex.log')
SQLDUMP_BACKUP_RAW = ("mysqldump --all-databases %(extra_opts)s "
                      "--opt --password=password -u os_admin"
                      " 2>/tmp/mysqldump.log")
SQLDUMP_BACKUP = SQLDUMP_BACKUP_RAW % {'extra_opts': ''}
SQLDUMP_BACKUP_EXTRA_OPTS = (SQLDUMP_BACKUP_RAW %
                             {'extra_opts': '--events --routines --triggers'})
XTRA_RESTORE_RAW = "sudo xbstream -x -C %(restore_location)s"
XTRA_RESTORE = XTRA_RESTORE_RAW % {'restore_location': '/var/lib/mysql'}
XTRA_INCR_PREPARE = ("sudo innobackupex --apply-log"
                     " --redo-only /var/lib/mysql"
                     " --defaults-file=/var/lib/mysql/backup-my.cnf"
                     " --ibbackup xtrabackup %(incr)s"
                     " 2>/tmp/innoprepare.log")
SQLDUMP_RESTORE = "sudo mysql"
PREPARE = ("sudo innobackupex --apply-log /var/lib/mysql "
           "--defaults-file=/var/lib/mysql/backup-my.cnf "
           "--ibbackup xtrabackup 2>/tmp/innoprepare.log")
CRYPTO_KEY = "default_aes_cbc_key"


class GuestAgentBackupTest(testtools.TestCase):

    def setUp(self):
        super(GuestAgentBackupTest, self).setUp()
        self.orig = mysql_impl.get_auth_password
        mysql_impl.get_auth_password = mock.Mock(
            return_value='password')

    def tearDown(self):
        super(GuestAgentBackupTest, self).tearDown()
        mysql_impl.get_auth_password = self.orig

    def test_backup_decrypted_xtrabackup_command(self):
        backupBase.BackupRunner.is_zipped = True
        backupBase.BackupRunner.is_encrypted = False
        RunnerClass = utils.import_class(BACKUP_XTRA_CLS)
        bkup = RunnerClass(12345, extra_opts="")
        self.assertEqual(bkup.command, XTRA_BACKUP + PIPE + ZIP)
        self.assertEqual(bkup.manifest, "12345.xbstream.gz")

    def test_backup_decrypted_xtrabackup_with_extra_opts_command(self):
        backupBase.BackupRunner.is_zipped = True
        backupBase.BackupRunner.is_encrypted = False
        RunnerClass = utils.import_class(BACKUP_XTRA_CLS)
        bkup = RunnerClass(12345, extra_opts="--no-lock")
        self.assertEqual(bkup.command, XTRA_BACKUP_EXTRA_OPTS + PIPE + ZIP)
        self.assertEqual(bkup.manifest, "12345.xbstream.gz")

    def test_backup_encrypted_xtrabackup_command(self):
        backupBase.BackupRunner.is_zipped = True
        backupBase.BackupRunner.is_encrypted = True
        backupBase.BackupRunner.encrypt_key = CRYPTO_KEY
        RunnerClass = utils.import_class(BACKUP_XTRA_CLS)
        bkup = RunnerClass(12345, extra_opts="")
        self.assertEqual(bkup.command,
                         XTRA_BACKUP + PIPE + ZIP + PIPE + ENCRYPT)
        self.assertEqual(bkup.manifest, "12345.xbstream.gz.enc")

    def test_backup_xtrabackup_incremental(self):
        backupBase.BackupRunner.is_zipped = True
        backupBase.BackupRunner.is_encrypted = False
        RunnerClass = utils.import_class(BACKUP_XTRA_INCR_CLS)
        opts = {'lsn': '54321', 'extra_opts': ''}
        expected = (XTRA_BACKUP_INCR % opts) + PIPE + ZIP
        bkup = RunnerClass(12345, extra_opts="", lsn="54321")
        self.assertEqual(expected, bkup.command)
        self.assertEqual("12345.xbstream.gz", bkup.manifest)

    def test_backup_xtrabackup_incremental_with_extra_opts_command(self):
        backupBase.BackupRunner.is_zipped = True
        backupBase.BackupRunner.is_encrypted = False
        RunnerClass = utils.import_class(BACKUP_XTRA_INCR_CLS)
        opts = {'lsn': '54321', 'extra_opts': '--no-lock'}
        expected = (XTRA_BACKUP_INCR % opts) + PIPE + ZIP
        bkup = RunnerClass(12345, extra_opts="--no-lock", lsn="54321")
        self.assertEqual(expected, bkup.command)
        self.assertEqual("12345.xbstream.gz", bkup.manifest)

    def test_backup_xtrabackup_incremental_encrypted(self):
        backupBase.BackupRunner.is_zipped = True
        backupBase.BackupRunner.is_encrypted = True
        backupBase.BackupRunner.encrypt_key = CRYPTO_KEY
        RunnerClass = utils.import_class(BACKUP_XTRA_INCR_CLS)
        opts = {'lsn': '54321', 'extra_opts': ''}
        expected = (XTRA_BACKUP_INCR % opts) + PIPE + ZIP + PIPE + ENCRYPT
        bkup = RunnerClass(12345, extra_opts="", lsn="54321")
        self.assertEqual(expected, bkup.command)
        self.assertEqual("12345.xbstream.gz.enc", bkup.manifest)

    def test_backup_decrypted_mysqldump_command(self):
        backupBase.BackupRunner.is_zipped = True
        backupBase.BackupRunner.is_encrypted = False
        RunnerClass = utils.import_class(BACKUP_SQLDUMP_CLS)
        bkup = RunnerClass(12345, extra_opts="")
        self.assertEqual(bkup.command, SQLDUMP_BACKUP + PIPE + ZIP)
        self.assertEqual(bkup.manifest, "12345.gz")

    def test_backup_decrypted_mysqldump_with_extra_opts_command(self):
        backupBase.BackupRunner.is_zipped = True
        backupBase.BackupRunner.is_encrypted = False
        RunnerClass = utils.import_class(BACKUP_SQLDUMP_CLS)
        bkup = RunnerClass(12345, extra_opts="--events --routines --triggers")
        self.assertEqual(bkup.command, SQLDUMP_BACKUP_EXTRA_OPTS + PIPE + ZIP)
        self.assertEqual(bkup.manifest, "12345.gz")

    def test_backup_encrypted_mysqldump_command(self):
        backupBase.BackupRunner.is_zipped = True
        backupBase.BackupRunner.is_encrypted = True
        backupBase.BackupRunner.encrypt_key = CRYPTO_KEY
        RunnerClass = utils.import_class(BACKUP_SQLDUMP_CLS)
        bkup = RunnerClass(12345, user="user",
                           password="password", extra_opts="")
        self.assertEqual(bkup.command,
                         SQLDUMP_BACKUP + PIPE + ZIP + PIPE + ENCRYPT)
        self.assertEqual(bkup.manifest, "12345.gz.enc")

    def test_restore_decrypted_xtrabackup_command(self):
        restoreBase.RestoreRunner.is_zipped = True
        restoreBase.RestoreRunner.is_encrypted = False
        RunnerClass = utils.import_class(RESTORE_XTRA_CLS)
        restr = RunnerClass(None, restore_location="/var/lib/mysql",
                            location="filename", checksum="md5")
        self.assertEqual(restr.restore_cmd, UNZIP + PIPE + XTRA_RESTORE)
        self.assertEqual(restr.prepare_cmd, PREPARE)

    def test_restore_encrypted_xtrabackup_command(self):
        restoreBase.RestoreRunner.is_zipped = True
        restoreBase.RestoreRunner.is_encrypted = True
        restoreBase.RestoreRunner.decrypt_key = CRYPTO_KEY
        RunnerClass = utils.import_class(RESTORE_XTRA_CLS)
        restr = RunnerClass(None, restore_location="/var/lib/mysql",
                            location="filename", checksum="md5")
        self.assertEqual(restr.restore_cmd,
                         DECRYPT + PIPE + UNZIP + PIPE + XTRA_RESTORE)
        self.assertEqual(restr.prepare_cmd, PREPARE)

    def test_restore_xtrabackup_incremental_prepare_command(self):
        RunnerClass = utils.import_class(RESTORE_XTRA_INCR_CLS)
        restr = RunnerClass(None, restore_location="/var/lib/mysql",
                            location="filename", checksum="m5d")
        # Final prepare command (same as normal xtrabackup)
        self.assertEqual(PREPARE, restr.prepare_cmd)
        # Incremental backup prepare command
        expected = XTRA_INCR_PREPARE % {'incr': '--incremental-dir=/foo/bar/'}
        observed = restr._incremental_prepare_cmd('/foo/bar/')
        self.assertEqual(expected, observed)
        # Full backup prepare command
        expected = XTRA_INCR_PREPARE % {'incr': ''}
        observed = restr._incremental_prepare_cmd(None)
        self.assertEqual(expected, observed)

    def test_restore_decrypted_xtrabackup_incremental_command(self):
        restoreBase.RestoreRunner.is_zipped = True
        restoreBase.RestoreRunner.is_encrypted = False
        RunnerClass = utils.import_class(RESTORE_XTRA_INCR_CLS)
        restr = RunnerClass(None, restore_location="/var/lib/mysql",
                            location="filename", checksum="m5d")
        # Full restore command
        expected = UNZIP + PIPE + XTRA_RESTORE
        self.assertEqual(expected, restr.restore_cmd)
        # Incremental backup restore command
        opts = {'restore_location': '/foo/bar/'}
        expected = UNZIP + PIPE + (XTRA_RESTORE_RAW % opts)
        observed = restr._incremental_restore_cmd('/foo/bar/')
        self.assertEqual(expected, observed)

    def test_restore_encrypted_xtrabackup_incremental_command(self):
        restoreBase.RestoreRunner.is_zipped = True
        restoreBase.RestoreRunner.is_encrypted = True
        restoreBase.RestoreRunner.decrypt_key = CRYPTO_KEY
        RunnerClass = utils.import_class(RESTORE_XTRA_INCR_CLS)
        restr = RunnerClass(None, restore_location="/var/lib/mysql",
                            location="filename", checksum="md5")
        # Full restore command
        expected = DECRYPT + PIPE + UNZIP + PIPE + XTRA_RESTORE
        self.assertEqual(expected, restr.restore_cmd)
        # Incremental backup restore command
        opts = {'restore_location': '/foo/bar/'}
        expected = DECRYPT + PIPE + UNZIP + PIPE + (XTRA_RESTORE_RAW % opts)
        observed = restr._incremental_restore_cmd('/foo/bar/')
        self.assertEqual(expected, observed)

    def test_restore_decrypted_mysqldump_command(self):
        restoreBase.RestoreRunner.is_zipped = True
        restoreBase.RestoreRunner.is_encrypted = False
        RunnerClass = utils.import_class(RESTORE_SQLDUMP_CLS)
        restr = RunnerClass(None, restore_location="/var/lib/mysql",
                            location="filename", checksum="md5")
        self.assertEqual(restr.restore_cmd, UNZIP + PIPE + SQLDUMP_RESTORE)

    def test_restore_encrypted_mysqldump_command(self):
        restoreBase.RestoreRunner.is_zipped = True
        restoreBase.RestoreRunner.is_encrypted = True
        restoreBase.RestoreRunner.decrypt_key = CRYPTO_KEY
        RunnerClass = utils.import_class(RESTORE_SQLDUMP_CLS)
        restr = RunnerClass(None, restore_location="/var/lib/mysql",
                            location="filename", checksum="md5")
        self.assertEqual(restr.restore_cmd,
                         DECRYPT + PIPE + UNZIP + PIPE + SQLDUMP_RESTORE)

########NEW FILE########
__FILENAME__ = test_cassandra_manager
#    Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import os

import testtools
from mock import MagicMock
from trove.common.context import TroveContext
from trove.common.instance import ServiceStatuses
from trove.guestagent import volume
from trove.guestagent.common import operating_system
from trove.guestagent.datastore.cassandra import service as cass_service
from trove.guestagent.datastore.cassandra import manager as cass_manager
from trove.guestagent import pkg as pkg


class GuestAgentCassandraDBManagerTest(testtools.TestCase):

    def setUp(self):
        super(GuestAgentCassandraDBManagerTest, self).setUp()
        self.real_status = cass_service.CassandraAppStatus.set_status

        class FakeInstanceServiceStatus(object):
            status = ServiceStatuses.NEW

            def save(self):
                pass

        cass_service.CassandraAppStatus.set_status = MagicMock(
            return_value=FakeInstanceServiceStatus())
        self.context = TroveContext()
        self.manager = cass_manager.Manager()
        self.pkg = cass_service.packager
        self.real_db_app_status = cass_service.CassandraAppStatus
        self.origin_os_path_exists = os.path.exists
        self.origin_format = volume.VolumeDevice.format
        self.origin_migrate_data = volume.VolumeDevice.migrate_data
        self.origin_mount = volume.VolumeDevice.mount
        self.origin_mount_points = volume.VolumeDevice.mount_points
        self.origin_stop_db = cass_service.CassandraApp.stop_db
        self.origin_start_db = cass_service.CassandraApp.start_db
        self.origin_install_db = cass_service.CassandraApp._install_db
        self.original_get_ip = operating_system.get_ip_address
        self.orig_make_host_reachable = (
            cass_service.CassandraApp.make_host_reachable)

    def tearDown(self):
        super(GuestAgentCassandraDBManagerTest, self).tearDown()
        cass_service.packager = self.pkg
        cass_service.CassandraAppStatus.set_status = self.real_db_app_status
        os.path.exists = self.origin_os_path_exists
        volume.VolumeDevice.format = self.origin_format
        volume.VolumeDevice.migrate_data = self.origin_migrate_data
        volume.VolumeDevice.mount = self.origin_mount
        volume.VolumeDevice.mount_points = self.origin_mount_points
        cass_service.CassandraApp.stop_db = self.origin_stop_db
        cass_service.CassandraApp.start_db = self.origin_start_db
        cass_service.CassandraApp._install_db = self.origin_install_db
        operating_system.get_ip_address = self.original_get_ip
        cass_service.CassandraApp.make_host_reachable = (
            self.orig_make_host_reachable)

    def test_update_status(self):
        mock_status = MagicMock()
        self.manager.appStatus = mock_status
        self.manager.update_status(self.context)
        mock_status.update.assert_any_call()

    def test_prepare_pkg(self):
        self._prepare_dynamic(['cassandra'])

    def test_prepare_no_pkg(self):
        self._prepare_dynamic([])

    def test_prepare_db_not_installed(self):
        self._prepare_dynamic([], is_db_installed=False)

    def test_prepare_db_not_installed_no_package(self):
        self._prepare_dynamic([],
                              is_db_installed=True)

    def _prepare_dynamic(self, packages,
                         config_content='MockContent', device_path='/dev/vdb',
                         is_db_installed=True, backup_id=None,
                         is_root_enabled=False,
                         overrides=None):
        # covering all outcomes is starting to cause trouble here
        if not backup_id:
            backup_info = {'id': backup_id,
                           'location': 'fake-location',
                           'type': 'InnoBackupEx',
                           'checksum': 'fake-checksum',
                           }

        mock_status = MagicMock()
        mock_app = MagicMock()
        self.manager.appStatus = mock_status
        self.manager.app = mock_app

        mock_status.begin_install = MagicMock(return_value=None)
        mock_app.install_if_needed = MagicMock(return_value=None)
        pkg.Package.pkg_is_installed = MagicMock(return_value=is_db_installed)
        mock_app.init_storage_structure = MagicMock(return_value=None)
        mock_app.write_config = MagicMock(return_value=None)
        mock_app.make_host_reachable = MagicMock(return_value=None)
        mock_app.restart = MagicMock(return_value=None)
        os.path.exists = MagicMock(return_value=True)
        volume.VolumeDevice.format = MagicMock(return_value=None)
        volume.VolumeDevice.migrate_data = MagicMock(return_value=None)
        volume.VolumeDevice.mount = MagicMock(return_value=None)
        volume.VolumeDevice.mount_points = MagicMock(return_value=[])

        # invocation
        self.manager.prepare(context=self.context, packages=packages,
                             config_contents=config_content,
                             databases=None,
                             memory_mb='2048', users=None,
                             device_path=device_path,
                             mount_point="/var/lib/cassandra",
                             backup_info=backup_info,
                             overrides=None)

        # verification/assertion
        mock_status.begin_install.assert_any_call()
        mock_app.install_if_needed.assert_any_call(packages)
        mock_app.init_storage_structure.assert_any_call('/var/lib/cassandra')
        mock_app.make_host_reachable.assert_any_call()
        mock_app.restart.assert_any_call()

########NEW FILE########
__FILENAME__ = test_couchbase_manager
#    Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import testtools
from mock import MagicMock
from trove.common.context import TroveContext
from trove.guestagent import volume
from trove.guestagent.common import operating_system
from trove.guestagent.datastore.couchbase import service as couch_service
from trove.guestagent.datastore.couchbase import manager as couch_manager


class GuestAgentCouchbaseManagerTest(testtools.TestCase):

    def setUp(self):
        super(GuestAgentCouchbaseManagerTest, self).setUp()
        self.context = TroveContext()
        self.manager = couch_manager.Manager()
        self.packages = 'couchbase-server'
        self.origin_CouchbaseAppStatus = couch_service.CouchbaseAppStatus
        self.origin_format = volume.VolumeDevice.format
        self.origin_mount = volume.VolumeDevice.mount
        self.origin_mount_points = volume.VolumeDevice.mount_points
        self.origin_stop_db = couch_service.CouchbaseApp.stop_db
        self.origin_start_db = couch_service.CouchbaseApp.start_db
        self.origin_restart = couch_service.CouchbaseApp.restart
        self.origin_install_if = couch_service.CouchbaseApp.install_if_needed
        self.origin_complete_install = \
            couch_service.CouchbaseApp.complete_install_or_restart
        operating_system.get_ip_address = MagicMock()

    def tearDown(self):
        super(GuestAgentCouchbaseManagerTest, self).tearDown()
        couch_service.CouchbaseAppStatus = self.origin_CouchbaseAppStatus
        volume.VolumeDevice.format = self.origin_format
        volume.VolumeDevice.mount = self.origin_mount
        volume.VolumeDevice.mount_points = self.origin_mount_points
        couch_service.CouchbaseApp.stop_db = self.origin_stop_db
        couch_service.CouchbaseApp.start_db = self.origin_start_db
        couch_service.CouchbaseApp.restart = self.origin_restart
        couch_service.CouchbaseApp.install_if_needed = self.origin_install_if
        couch_service.CouchbaseApp.complete_install_or_restart = \
            self.origin_complete_install

    def test_update_status(self):
        mock_status = MagicMock()
        self.manager.appStatus = mock_status
        self.manager.update_status(self.context)
        mock_status.update.assert_any_call()

    def test_prepare_device_path_true(self):
        self._prepare_dynamic()

    def _prepare_dynamic(self, device_path='/dev/vdb', is_db_installed=True,
                         backup_info=None):
        mock_status = MagicMock()
        self.manager.appStatus = mock_status

        mock_status.begin_install = MagicMock(return_value=None)
        volume.VolumeDevice.format = MagicMock(return_value=None)
        volume.VolumeDevice.mount = MagicMock(return_value=None)
        volume.VolumeDevice.mount_points = MagicMock(return_value=[])
        couch_service.CouchbaseApp.install_if_needed = MagicMock(
            return_value=None)
        couch_service.CouchbaseApp.complete_install_or_restart = MagicMock(
            return_value=None)

        #invocation
        self.manager.prepare(self.context, self.packages, None, 2048,
                             None, device_path=device_path,
                             mount_point='/var/lib/couchbase',
                             backup_info=backup_info)
        #verification/assertion
        mock_status.begin_install.assert_any_call()
        couch_service.CouchbaseApp.install_if_needed.assert_any_call(
            self.packages)
        couch_service.CouchbaseApp.complete_install_or_restart.\
            assert_any_call()

    def test_restart(self):
        mock_status = MagicMock()
        self.manager.appStatus = mock_status
        couch_service.CouchbaseApp.restart = MagicMock(return_value=None)
        #invocation
        self.manager.restart(self.context)
        #verification/assertion
        couch_service.CouchbaseApp.restart.assert_any_call()

    def test_stop_db(self):
        mock_status = MagicMock()
        self.manager.appStatus = mock_status
        couch_service.CouchbaseApp.stop_db = MagicMock(return_value=None)
        #invocation
        self.manager.stop_db(self.context)
        #verification/assertion
        couch_service.CouchbaseApp.stop_db.assert_any_call(
            do_not_start_on_reboot=False)

########NEW FILE########
__FILENAME__ = test_dbaas
#    Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import os
from uuid import uuid4
import time
from mock import Mock
from mock import MagicMock
from mock import patch
from mock import ANY
import sqlalchemy
import testtools
from testtools.matchers import Is
from testtools.matchers import Equals
from testtools.matchers import Not
from trove.common.exception import ProcessExecutionError
from trove.common import utils
from trove.common import instance as rd_instance
from trove.conductor import api as conductor_api
import trove.guestagent.datastore.mysql.service as dbaas
from trove.guestagent import dbaas as dbaas_sr
from trove.guestagent import pkg
from trove.guestagent.common import operating_system
from trove.guestagent.dbaas import to_gb
from trove.guestagent.dbaas import get_filesystem_volume_stats
from trove.guestagent.datastore.service import BaseDbStatus
from trove.guestagent.datastore.redis import service as rservice
from trove.guestagent.datastore.redis.service import RedisApp
from trove.guestagent.datastore.redis import system as RedisSystem
from trove.guestagent.datastore.cassandra import service as cass_service
from trove.guestagent.datastore.mysql.service import MySqlAdmin
from trove.guestagent.datastore.mysql.service import MySqlRootAccess
from trove.guestagent.datastore.mysql.service import MySqlApp
from trove.guestagent.datastore.mysql.service import MySqlAppStatus
from trove.guestagent.datastore.mysql.service import KeepAliveConnection
from trove.guestagent.datastore.couchbase import service as couchservice
from trove.guestagent.datastore.mongodb import service as mongo_service
from trove.guestagent.datastore.mongodb import system as mongo_system
from trove.guestagent.db import models
from trove.instance.models import InstanceServiceStatus
from trove.tests.unittests.util import util


"""
Unit tests for the classes and functions in dbaas.py.
"""

FAKE_DB = {"_name": "testDB", "_character_set": "latin2",
           "_collate": "latin2_general_ci"}
FAKE_DB_2 = {"_name": "testDB2", "_character_set": "latin2",
             "_collate": "latin2_general_ci"}
FAKE_USER = [{"_name": "random", "_password": "guesswhat",
              "_databases": [FAKE_DB]}]


conductor_api.API.heartbeat = Mock()


class FakeAppStatus(BaseDbStatus):

    def __init__(self, id, status):
        self.id = id
        self.next_fake_status = status

    def _get_actual_db_status(self):
        return self.next_fake_status

    def set_next_status(self, next_status):
        self.next_fake_status = next_status


class DbaasTest(testtools.TestCase):

    def setUp(self):
        super(DbaasTest, self).setUp()
        self.orig_utils_execute_with_timeout = dbaas.utils.execute_with_timeout
        self.orig_utils_execute = dbaas.utils.execute

    def tearDown(self):
        super(DbaasTest, self).tearDown()
        dbaas.utils.execute_with_timeout = self.orig_utils_execute_with_timeout
        dbaas.utils.execute = self.orig_utils_execute

    def test_get_auth_password(self):

        dbaas.utils.execute_with_timeout = Mock(
            return_value=("password    ", None))

        password = dbaas.get_auth_password()

        self.assertEqual("password", password)

    def test_get_auth_password_error(self):

        dbaas.utils.execute_with_timeout = Mock(
            return_value=("password", "Error"))

        self.assertRaises(RuntimeError, dbaas.get_auth_password)

    def test_service_discovery(self):
        with patch.object(os.path, 'isfile', return_value=True):
            mysql_service = dbaas.operating_system.service_discovery(["mysql"])
        self.assertIsNotNone(mysql_service['cmd_start'])
        self.assertIsNotNone(mysql_service['cmd_enable'])

    def test_load_mysqld_options(self):

        output = "mysqld would've been started with the these args:\n"\
                 "--user=mysql --port=3306 --basedir=/usr "\
                 "--tmpdir=/tmp --skip-external-locking"

        with patch.object(os.path, 'isfile', return_value=True):
            dbaas.utils.execute = Mock(return_value=(output, None))
            options = dbaas.load_mysqld_options()

        self.assertEqual(5, len(options))
        self.assertEqual(options["user"], "mysql")
        self.assertEqual(options["port"], "3306")
        self.assertEqual(options["basedir"], "/usr")
        self.assertEqual(options["tmpdir"], "/tmp")
        self.assertTrue("skip-external-locking" in options)

    def test_load_mysqld_options_error(self):

        dbaas.utils.execute = Mock(side_effect=ProcessExecutionError())

        self.assertFalse(dbaas.load_mysqld_options())


class ResultSetStub(object):

    def __init__(self, rows):
        self._rows = rows

    def __iter__(self):
        return self._rows.__iter__()

    @property
    def rowcount(self):
        return len(self._rows)

    def __repr__(self):
        return self._rows.__repr__()


class MySqlAdminMockTest(testtools.TestCase):

    def tearDown(self):
        super(MySqlAdminMockTest, self).tearDown()

    def test_list_databases(self):
        mock_conn = mock_sql_connection()

        with patch.object(mock_conn, 'execute',
                          return_value=ResultSetStub(
                [('db1', 'utf8', 'utf8_bin'),
                 ('db2', 'utf8', 'utf8_bin'),
                 ('db3', 'utf8', 'utf8_bin')])):
            databases, next_marker = MySqlAdmin().list_databases(limit=10)

        self.assertThat(next_marker, Is(None))
        self.assertThat(len(databases), Is(3))


class MySqlAdminTest(testtools.TestCase):

    def setUp(self):

        super(MySqlAdminTest, self).setUp()

        self.orig_get_engine = dbaas.get_engine
        self.orig_LocalSqlClient = dbaas.LocalSqlClient
        self.orig_LocalSqlClient_enter = dbaas.LocalSqlClient.__enter__
        self.orig_LocalSqlClient_exit = dbaas.LocalSqlClient.__exit__
        self.orig_LocalSqlClient_execute = dbaas.LocalSqlClient.execute
        self.orig_MySQLUser_is_valid_user_name = (
            models.MySQLUser._is_valid_user_name)
        dbaas.get_engine = MagicMock(name='get_engine')
        dbaas.LocalSqlClient = Mock
        dbaas.LocalSqlClient.__enter__ = Mock()
        dbaas.LocalSqlClient.__exit__ = Mock()
        dbaas.LocalSqlClient.execute = Mock()
        self.mySqlAdmin = MySqlAdmin()

    def tearDown(self):

        super(MySqlAdminTest, self).tearDown()
        dbaas.get_engine = self.orig_get_engine
        dbaas.LocalSqlClient = self.orig_LocalSqlClient
        dbaas.LocalSqlClient.__enter__ = self.orig_LocalSqlClient_enter
        dbaas.LocalSqlClient.__exit__ = self.orig_LocalSqlClient_exit
        dbaas.LocalSqlClient.execute = self.orig_LocalSqlClient_execute
        models.MySQLUser._is_valid_user_name = (
            self.orig_MySQLUser_is_valid_user_name)

    def test_create_database(self):

        databases = []
        databases.append(FAKE_DB)

        self.mySqlAdmin.create_database(databases)

        args, _ = dbaas.LocalSqlClient.execute.call_args_list[0]
        expected = ("CREATE DATABASE IF NOT EXISTS "
                    "`testDB` CHARACTER SET = 'latin2' "
                    "COLLATE = 'latin2_general_ci';")
        self.assertEqual(args[0].text, expected,
                         "Create database queries are not the same")

        self.assertEqual(1, dbaas.LocalSqlClient.execute.call_count,
                         "The client object was not called exactly once, " +
                         "it was called %d times"
                         % dbaas.LocalSqlClient.execute.call_count)

    def test_create_database_more_than_1(self):

        databases = []
        databases.append(FAKE_DB)
        databases.append(FAKE_DB_2)

        self.mySqlAdmin.create_database(databases)

        args, _ = dbaas.LocalSqlClient.execute.call_args_list[0]
        expected = ("CREATE DATABASE IF NOT EXISTS "
                    "`testDB` CHARACTER SET = 'latin2' "
                    "COLLATE = 'latin2_general_ci';")
        self.assertEqual(args[0].text, expected,
                         "Create database queries are not the same")

        args, _ = dbaas.LocalSqlClient.execute.call_args_list[1]
        expected = ("CREATE DATABASE IF NOT EXISTS "
                    "`testDB2` CHARACTER SET = 'latin2' "
                    "COLLATE = 'latin2_general_ci';")
        self.assertEqual(args[0].text, expected,
                         "Create database queries are not the same")

        self.assertEqual(2, dbaas.LocalSqlClient.execute.call_count,
                         "The client object was not called exactly twice, " +
                         "it was called %d times"
                         % dbaas.LocalSqlClient.execute.call_count)

    def test_create_database_no_db(self):

        databases = []

        self.mySqlAdmin.create_database(databases)

        self.assertFalse(dbaas.LocalSqlClient.execute.called,
                         "The client object was called when it wasn't " +
                         "supposed to")

    def test_delete_database(self):

        database = {"_name": "testDB"}

        self.mySqlAdmin.delete_database(database)

        args, _ = dbaas.LocalSqlClient.execute.call_args
        expected = "DROP DATABASE `testDB`;"
        self.assertEqual(args[0].text, expected,
                         "Delete database queries are not the same")

        self.assertTrue(dbaas.LocalSqlClient.execute.called,
                        "The client object was not called")

    def test_delete_user(self):

        user = {"_name": "testUser"}

        self.mySqlAdmin.delete_user(user)

        # For some reason, call_args is None.
        call_args = dbaas.LocalSqlClient.execute.call_args
        if call_args is not None:
            args, _ = call_args
            expected = "DROP USER `testUser`;"
            self.assertEqual(args[0].text, expected,
                             "Delete user queries are not the same")

            self.assertTrue(dbaas.LocalSqlClient.execute.called,
                            "The client object was not called")

    def test_create_user(self):
        self.mySqlAdmin.create_user(FAKE_USER)
        expected = ("GRANT ALL PRIVILEGES ON `testDB`.* TO `random`@`%` "
                    "IDENTIFIED BY 'guesswhat' "
                    "WITH GRANT OPTION;")
        # For some reason, call_args is None.
        call_args = dbaas.LocalSqlClient.execute.call_args
        if call_args is not None:
            args, _ = call_args
            self.assertEqual(args[0].text.strip(), expected,
                             "Create user queries are not the same")
            self.assertEqual(2, dbaas.LocalSqlClient.execute.call_count)

    def test_list_databases(self):
        self.mySqlAdmin.list_databases()
        args, _ = dbaas.LocalSqlClient.execute.call_args
        expected = ["SELECT schema_name as name,",
                    "default_character_set_name as charset,",
                    "default_collation_name as collation",
                    "FROM information_schema.schemata",
                    ("schema_name NOT IN ("
                     "'mysql', 'information_schema', "
                     "'lost+found', '#mysql50#lost+found'"
                     ")"),
                    "ORDER BY schema_name ASC",
                    ]
        for text in expected:
            self.assertTrue(text in args[0].text, "%s not in query." % text)
        self.assertFalse("LIMIT " in args[0].text)

    def test_list_databases_with_limit(self):
        limit = 2
        self.mySqlAdmin.list_databases(limit)
        args, _ = dbaas.LocalSqlClient.execute.call_args
        expected = ["SELECT schema_name as name,",
                    "default_character_set_name as charset,",
                    "default_collation_name as collation",
                    "FROM information_schema.schemata",
                    ("schema_name NOT IN ("
                     "'mysql', 'information_schema', "
                     "'lost+found', '#mysql50#lost+found'"
                     ")"),
                    "ORDER BY schema_name ASC",
                    ]
        for text in expected:
            self.assertTrue(text in args[0].text, "%s not in query." % text)

        self.assertTrue("LIMIT " + str(limit + 1) in args[0].text)

    def test_list_databases_with_marker(self):
        marker = "aMarker"
        self.mySqlAdmin.list_databases(marker=marker)
        args, _ = dbaas.LocalSqlClient.execute.call_args
        expected = ["SELECT schema_name as name,",
                    "default_character_set_name as charset,",
                    "default_collation_name as collation",
                    "FROM information_schema.schemata",
                    ("schema_name NOT IN ("
                     "'mysql', 'information_schema', "
                     "'lost+found', '#mysql50#lost+found'"
                     ")"),
                    "ORDER BY schema_name ASC",
                    ]

        for text in expected:
            self.assertTrue(text in args[0].text, "%s not in query." % text)

        self.assertFalse("LIMIT " in args[0].text)

        self.assertTrue("AND schema_name > '" + marker + "'" in args[0].text)

    def test_list_databases_with_include_marker(self):
        marker = "aMarker"
        self.mySqlAdmin.list_databases(marker=marker, include_marker=True)
        args, _ = dbaas.LocalSqlClient.execute.call_args
        expected = ["SELECT schema_name as name,",
                    "default_character_set_name as charset,",
                    "default_collation_name as collation",
                    "FROM information_schema.schemata",
                    ("schema_name NOT IN ("
                     "'mysql', 'information_schema', "
                     "'lost+found', '#mysql50#lost+found'"
                     ")"),
                    "ORDER BY schema_name ASC",
                    ]
        for text in expected:
            self.assertTrue(text in args[0].text, "%s not in query." % text)

        self.assertFalse("LIMIT " in args[0].text)

        self.assertTrue(("AND schema_name >= '%s'" % marker) in args[0].text)

    def test_list_users(self):
        self.mySqlAdmin.list_users()
        args, _ = dbaas.LocalSqlClient.execute.call_args

        expected = ["SELECT User, Host",
                    "FROM mysql.user",
                    "WHERE Host != 'localhost'",
                    "ORDER BY User",
                    ]
        for text in expected:
            self.assertTrue(text in args[0].text, "%s not in query." % text)

        self.assertFalse("LIMIT " in args[0].text)
        self.assertFalse("AND Marker > '" in args[0].text)

    def test_list_users_with_limit(self):
        limit = 2
        self.mySqlAdmin.list_users(limit)
        args, _ = dbaas.LocalSqlClient.execute.call_args

        expected = ["SELECT User, Host",
                    "FROM mysql.user",
                    "WHERE Host != 'localhost'",
                    "ORDER BY User",
                    ("LIMIT " + str(limit + 1)),
                    ]
        for text in expected:
            self.assertTrue(text in args[0].text, "%s not in query." % text)

    def test_list_users_with_marker(self):
        marker = "aMarker"
        self.mySqlAdmin.list_users(marker=marker)
        args, _ = dbaas.LocalSqlClient.execute.call_args

        expected = ["SELECT User, Host, Marker",
                    "FROM mysql.user",
                    "WHERE Host != 'localhost'",
                    "ORDER BY User",
                    ]

        for text in expected:
            self.assertTrue(text in args[0].text, "%s not in query." % text)

        self.assertFalse("LIMIT " in args[0].text)
        self.assertTrue("AND Marker > '" + marker + "'" in args[0].text)

    def test_list_users_with_include_marker(self):
        marker = "aMarker"
        self.mySqlAdmin.list_users(marker=marker, include_marker=True)
        args, _ = dbaas.LocalSqlClient.execute.call_args

        expected = ["SELECT User, Host",
                    "FROM mysql.user",
                    "WHERE Host != 'localhost'",
                    "ORDER BY User",
                    ]

        for text in expected:
            self.assertTrue(text in args[0].text, "%s not in query." % text)

        self.assertFalse("LIMIT " in args[0].text)

        self.assertTrue("AND Marker >= '" + marker + "'" in args[0].text)

    def test_get_user(self):
        """
        Unit tests for mySqlAdmin.get_user.
        This test case checks if the sql query formed by the get_user method
        is correct or not by checking with expected query.
        """
        username = "user1"
        hostname = "host"
        self.mySqlAdmin.get_user(username, hostname)
        args, _ = dbaas.LocalSqlClient.execute.call_args
        expected = ["SELECT User, Host",
                    "FROM mysql.user",
                    "WHERE Host != 'localhost' AND User = 'user1'",
                    "ORDER BY User, Host",
                    ]

        for text in expected:
            self.assertTrue(text in args[0].text, "%s not in query." % text)


class MySqlAppTest(testtools.TestCase):

    def setUp(self):
        super(MySqlAppTest, self).setUp()
        self.orig_utils_execute_with_timeout = dbaas.utils.execute_with_timeout
        self.orig_time_sleep = time.sleep
        util.init_db()
        self.FAKE_ID = str(uuid4())
        InstanceServiceStatus.create(instance_id=self.FAKE_ID,
                                     status=rd_instance.ServiceStatuses.NEW)
        self.appStatus = FakeAppStatus(self.FAKE_ID,
                                       rd_instance.ServiceStatuses.NEW)
        self.mySqlApp = MySqlApp(self.appStatus)
        mysql_service = {'cmd_start': Mock(),
                         'cmd_stop': Mock(),
                         'cmd_enable': Mock(),
                         'cmd_disable': Mock(),
                         'bin': Mock()}
        dbaas.operating_system.service_discovery = Mock(return_value=
                                                        mysql_service)
        time.sleep = Mock()

    def tearDown(self):
        super(MySqlAppTest, self).tearDown()
        dbaas.utils.execute_with_timeout = self.orig_utils_execute_with_timeout
        time.sleep = self.orig_time_sleep
        InstanceServiceStatus.find_by(instance_id=self.FAKE_ID).delete()

    def assert_reported_status(self, expected_status):
        service_status = InstanceServiceStatus.find_by(
            instance_id=self.FAKE_ID)
        self.assertEqual(expected_status, service_status.status)

    def mysql_starts_successfully(self):
        def start(update_db=False):
            self.appStatus.set_next_status(
                rd_instance.ServiceStatuses.RUNNING)

        self.mySqlApp.start_mysql.side_effect = start

    def mysql_starts_unsuccessfully(self):
        def start():
            raise RuntimeError("MySQL failed to start!")

        self.mySqlApp.start_mysql.side_effect = start

    def mysql_stops_successfully(self):
        def stop():
            self.appStatus.set_next_status(
                rd_instance.ServiceStatuses.SHUTDOWN)

        self.mySqlApp.stop_db.side_effect = stop

    def mysql_stops_unsuccessfully(self):
        def stop():
            raise RuntimeError("MySQL failed to stop!")

        self.mySqlApp.stop_db.side_effect = stop

    def test_stop_mysql(self):

        dbaas.utils.execute_with_timeout = Mock()
        self.appStatus.set_next_status(
            rd_instance.ServiceStatuses.SHUTDOWN)

        self.mySqlApp.stop_db()
        self.assert_reported_status(rd_instance.ServiceStatuses.NEW)

    def test_stop_mysql_with_db_update(self):

        dbaas.utils.execute_with_timeout = Mock()
        self.appStatus.set_next_status(
            rd_instance.ServiceStatuses.SHUTDOWN)

        self.mySqlApp.stop_db(True)
        self.assertTrue(conductor_api.API.heartbeat.called_once_with(
            self.FAKE_ID,
            {'service_status':
             rd_instance.ServiceStatuses.SHUTDOWN.description}))

    def test_stop_mysql_error(self):

        dbaas.utils.execute_with_timeout = Mock()
        self.appStatus.set_next_status(rd_instance.ServiceStatuses.RUNNING)
        self.mySqlApp.state_change_wait_time = 1
        self.assertRaises(RuntimeError, self.mySqlApp.stop_db)

    def test_restart_is_successful(self):

        self.mySqlApp.start_mysql = Mock()
        self.mySqlApp.stop_db = Mock()
        self.mysql_stops_successfully()
        self.mysql_starts_successfully()

        self.mySqlApp.restart()

        self.assertTrue(self.mySqlApp.stop_db.called)
        self.assertTrue(self.mySqlApp.start_mysql.called)
        self.assertTrue(conductor_api.API.heartbeat.called_once_with(
            self.FAKE_ID,
            {'service_status':
             rd_instance.ServiceStatuses.RUNNING.description}))

    def test_restart_mysql_wont_start_up(self):

        self.mySqlApp.start_mysql = Mock()
        self.mySqlApp.stop_db = Mock()
        self.mysql_stops_unsuccessfully()
        self.mysql_starts_unsuccessfully()

        self.assertRaises(RuntimeError, self.mySqlApp.restart)

        self.assertTrue(self.mySqlApp.stop_db.called)
        self.assertFalse(self.mySqlApp.start_mysql.called)
        self.assert_reported_status(rd_instance.ServiceStatuses.NEW)

    def test_wipe_ib_logfiles_no_file(self):

        processexecerror = ProcessExecutionError('No such file or directory')
        dbaas.utils.execute_with_timeout = Mock(side_effect=processexecerror)

        self.mySqlApp.wipe_ib_logfiles()

    def test_wipe_ib_logfiles_error(self):

        mocked = Mock(side_effect=ProcessExecutionError('Error'))
        dbaas.utils.execute_with_timeout = mocked

        self.assertRaises(ProcessExecutionError,
                          self.mySqlApp.wipe_ib_logfiles)

    def test_start_mysql(self):

        dbaas.utils.execute_with_timeout = Mock()
        self.appStatus.set_next_status(rd_instance.ServiceStatuses.RUNNING)
        self.mySqlApp._enable_mysql_on_boot = Mock()
        self.mySqlApp.start_mysql()
        self.assert_reported_status(rd_instance.ServiceStatuses.NEW)

    def test_start_mysql_with_db_update(self):

        dbaas.utils.execute_with_timeout = Mock()
        self.mySqlApp._enable_mysql_on_boot = Mock()
        self.appStatus.set_next_status(rd_instance.ServiceStatuses.RUNNING)

        self.mySqlApp.start_mysql(update_db=True)
        self.assertTrue(conductor_api.API.heartbeat.called_once_with(
            self.FAKE_ID,
            {'service_status':
             rd_instance.ServiceStatuses.RUNNING.description}))

    def test_start_mysql_runs_forever(self):

        dbaas.utils.execute_with_timeout = Mock()
        self.mySqlApp._enable_mysql_on_boot = Mock()
        self.mySqlApp.state_change_wait_time = 1
        self.appStatus.set_next_status(rd_instance.ServiceStatuses.SHUTDOWN)

        self.assertRaises(RuntimeError, self.mySqlApp.start_mysql)
        self.assertTrue(conductor_api.API.heartbeat.called_once_with(
            self.FAKE_ID,
            {'service_status':
             rd_instance.ServiceStatuses.SHUTDOWN.description}))

    def test_start_mysql_error(self):

        self.mySqlApp._enable_mysql_on_boot = Mock()
        mocked = Mock(side_effect=ProcessExecutionError('Error'))
        dbaas.utils.execute_with_timeout = mocked

        self.assertRaises(RuntimeError, self.mySqlApp.start_mysql)

    def test_start_db_with_conf_changes(self):

        self.mySqlApp.start_mysql = Mock()
        self.mySqlApp._write_mycnf = Mock()
        self.mysql_starts_successfully()

        self.appStatus.status = rd_instance.ServiceStatuses.SHUTDOWN
        self.mySqlApp.start_db_with_conf_changes(Mock())

        self.assertTrue(self.mySqlApp._write_mycnf.called)
        self.assertTrue(self.mySqlApp.start_mysql.called)
        self.assertEqual(self.appStatus._get_actual_db_status(),
                         rd_instance.ServiceStatuses.RUNNING)

    def test_start_db_with_conf_changes_mysql_is_running(self):

        self.mySqlApp.start_mysql = Mock()
        self.mySqlApp._write_mycnf = Mock()

        self.appStatus.status = rd_instance.ServiceStatuses.RUNNING
        self.assertRaises(RuntimeError,
                          self.mySqlApp.start_db_with_conf_changes,
                          Mock())

    def test_remove_overrides(self):

        from trove.common.exception import ProcessExecutionError
        mocked = Mock(side_effect=ProcessExecutionError('Error'))
        dbaas.utils.execute_with_timeout = mocked
        self.assertRaises(ProcessExecutionError, self.mySqlApp.start_mysql)


class MySqlAppInstallTest(MySqlAppTest):

    def setUp(self):
        super(MySqlAppInstallTest, self).setUp()
        self.orig_create_engine = sqlalchemy.create_engine
        self.orig_pkg_version = dbaas.packager.pkg_version
        self.orig_utils_execute_with_timeout = utils.execute_with_timeout

    def tearDown(self):
        super(MySqlAppInstallTest, self).tearDown()
        sqlalchemy.create_engine = self.orig_create_engine
        dbaas.packager.pkg_version = self.orig_pkg_version
        utils.execute_with_timeout = self.orig_utils_execute_with_timeout

    def test_install(self):

        self.mySqlApp._install_mysql = Mock()
        pkg.Package.pkg_is_installed = Mock(return_value=False)
        utils.execute_with_timeout = Mock()
        pkg.Package.pkg_install = Mock()
        self.mySqlApp._clear_mysql_config = Mock()
        self.mySqlApp._create_mysql_confd_dir = Mock()
        self.mySqlApp.start_mysql = Mock()
        self.mySqlApp.install_if_needed(["package"])
        self.assertTrue(pkg.Package.pkg_install.called)
        self.assert_reported_status(rd_instance.ServiceStatuses.NEW)

    def test_secure(self):

        dbaas.clear_expired_password = Mock()
        self.mySqlApp.start_mysql = Mock()
        self.mySqlApp.stop_db = Mock()
        self.mySqlApp._write_mycnf = Mock()
        self.mysql_stops_successfully()
        self.mysql_starts_successfully()
        sqlalchemy.create_engine = Mock()

        self.mySqlApp.secure('contents', None)

        self.assertTrue(self.mySqlApp.stop_db.called)
        self.assertTrue(self.mySqlApp._write_mycnf.called)
        self.assertTrue(self.mySqlApp.start_mysql.called)
        self.assert_reported_status(rd_instance.ServiceStatuses.NEW)

    def test_install_install_error(self):

        from trove.guestagent import pkg
        self.mySqlApp.start_mysql = Mock()
        self.mySqlApp.stop_db = Mock()
        pkg.Package.pkg_is_installed = Mock(return_value=False)
        self.mySqlApp._clear_mysql_config = Mock()
        self.mySqlApp._create_mysql_confd_dir = Mock()
        pkg.Package.pkg_install = \
            Mock(side_effect=pkg.PkgPackageStateError("Install error"))

        self.assertRaises(pkg.PkgPackageStateError,
                          self.mySqlApp.install_if_needed, ["package"])

        self.assert_reported_status(rd_instance.ServiceStatuses.NEW)

    def test_secure_write_conf_error(self):

        dbaas.clear_expired_password = Mock()
        self.mySqlApp.start_mysql = Mock()
        self.mySqlApp.stop_db = Mock()
        self.mySqlApp._write_mycnf = Mock(
            side_effect=IOError("Could not write file"))
        self.mysql_stops_successfully()
        self.mysql_starts_successfully()
        sqlalchemy.create_engine = Mock()

        self.assertRaises(IOError, self.mySqlApp.secure, "foo", None)

        self.assertTrue(self.mySqlApp.stop_db.called)
        self.assertTrue(self.mySqlApp._write_mycnf.called)
        self.assertFalse(self.mySqlApp.start_mysql.called)
        self.assert_reported_status(rd_instance.ServiceStatuses.NEW)


class TextClauseMatcher(object):
    def __init__(self, text):
        self.text = text

    def __repr__(self):
        return "TextClause(%s)" % self.text

    def __eq__(self, arg):
        print("Matching %s" % arg.text)
        return self.text in arg.text


def mock_sql_connection():
    utils.execute_with_timeout = MagicMock(return_value=['fake_password',
                                                         None])
    mock_engine = MagicMock()
    sqlalchemy.create_engine = MagicMock(return_value=mock_engine)
    mock_conn = MagicMock()
    dbaas.LocalSqlClient.__enter__ = MagicMock(return_value=mock_conn)
    dbaas.LocalSqlClient.__exit__ = MagicMock(return_value=None)
    return mock_conn


class MySqlAppMockTest(testtools.TestCase):

    def setUp(self):
        super(MySqlAppMockTest, self).setUp()
        self.orig_utils_execute_with_timeout = utils.execute_with_timeout

    def tearDown(self):
        super(MySqlAppMockTest, self).tearDown()
        utils.execute_with_timeout = self.orig_utils_execute_with_timeout

    def test_secure_keep_root(self):
        mock_conn = mock_sql_connection()

        with patch.object(mock_conn, 'execute', return_value=None):
            utils.execute_with_timeout = MagicMock(return_value=None)
            # skip writing the file for now
            with patch.object(os.path, 'isfile', return_value=False):
                mock_status = MagicMock()
                mock_status.wait_for_real_status_to_change_to = MagicMock(
                    return_value=True)
                dbaas.clear_expired_password = MagicMock(return_value=None)
                app = MySqlApp(mock_status)
                app._write_mycnf = MagicMock(return_value=True)
                app.start_mysql = MagicMock(return_value=None)
                app.stop_db = MagicMock(return_value=None)
                app.secure('foo', None)
                self.assertTrue(mock_conn.execute.called)


class MySqlRootStatusTest(testtools.TestCase):

    def setUp(self):
        super(MySqlRootStatusTest, self).setUp()
        self.orig_utils_execute_with_timeout = utils.execute_with_timeout

    def tearDown(self):
        super(MySqlRootStatusTest, self).tearDown()
        utils.execute_with_timeout = self.orig_utils_execute_with_timeout

    def test_root_is_enabled(self):
        mock_conn = mock_sql_connection()

        mock_rs = MagicMock()
        mock_rs.rowcount = 1
        with patch.object(mock_conn, 'execute', return_value=mock_rs):
            self.assertThat(MySqlRootAccess().is_root_enabled(), Is(True))

    def test_root_is_not_enabled(self):
        mock_conn = mock_sql_connection()

        mock_rs = MagicMock()
        mock_rs.rowcount = 0
        with patch.object(mock_conn, 'execute', return_value=mock_rs):
            self.assertThat(MySqlRootAccess.is_root_enabled(), Equals(False))

    def test_enable_root(self):
        mock_conn = mock_sql_connection()

        with patch.object(mock_conn, 'execute', return_value=None):
            # invocation
            user_ser = MySqlRootAccess.enable_root()
            # verification
            self.assertThat(user_ser, Not(Is(None)))
            mock_conn.execute.assert_any_call(TextClauseMatcher('CREATE USER'),
                                              user='root', host='%')
            mock_conn.execute.assert_any_call(TextClauseMatcher(
                'GRANT ALL PRIVILEGES ON *.*'))
            mock_conn.execute.assert_any_call(TextClauseMatcher(
                'UPDATE mysql.user'))

    def test_enable_root_failed(self):
        with patch.object(models.MySQLUser, '_is_valid_user_name',
                          return_value=False):
            self.assertRaises(ValueError, MySqlAdmin().enable_root)


class MockStats:
    f_blocks = 1024 ** 2
    f_bsize = 4096
    f_bfree = 512 * 1024


class InterrogatorTest(testtools.TestCase):

    def tearDown(self):
        super(InterrogatorTest, self).tearDown()

    def test_to_gb(self):
        result = to_gb(123456789)
        self.assertEqual(result, 0.11)

    def test_to_gb_zero(self):
        result = to_gb(0)
        self.assertEqual(result, 0.0)

    def test_get_filesystem_volume_stats(self):
        with patch.object(os, 'statvfs', return_value=MockStats):
            result = get_filesystem_volume_stats('/some/path/')

        self.assertEqual(result['block_size'], 4096)
        self.assertEqual(result['total_blocks'], 1048576)
        self.assertEqual(result['free_blocks'], 524288)
        self.assertEqual(result['total'], 4.0)
        self.assertEqual(result['free'], 2147483648)
        self.assertEqual(result['used'], 2.0)

    def test_get_filesystem_volume_stats_error(self):
        with patch.object(os, 'statvfs', side_effect=OSError):
            self.assertRaises(
                RuntimeError,
                get_filesystem_volume_stats, '/nonexistent/path')


class ServiceRegistryTest(testtools.TestCase):

    def setUp(self):
        super(ServiceRegistryTest, self).setUp()

    def tearDown(self):
        super(ServiceRegistryTest, self).tearDown()

    def test_datastore_registry_with_extra_manager(self):
        datastore_registry_ext_test = {
            'test': 'trove.guestagent.datastore.test.manager.Manager',
        }
        dbaas_sr.get_custom_managers = Mock(return_value=
                                            datastore_registry_ext_test)
        test_dict = dbaas_sr.datastore_registry()
        self.assertEqual(test_dict.get('test'),
                         datastore_registry_ext_test.get('test', None))
        self.assertEqual(test_dict.get('mysql'),
                         'trove.guestagent.datastore.mysql.'
                         'manager.Manager')
        self.assertEqual(test_dict.get('percona'),
                         'trove.guestagent.datastore.mysql.'
                         'manager.Manager')
        self.assertEqual(test_dict.get('redis'),
                         'trove.guestagent.datastore.redis.'
                         'manager.Manager')
        self.assertEqual(test_dict.get('cassandra'),
                         'trove.guestagent.datastore.cassandra.'
                         'manager.Manager')
        self.assertEqual(test_dict.get('couchbase'),
                         'trove.guestagent.datastore.couchbase.manager'
                         '.Manager')
        self.assertEqual('trove.guestagent.datastore.mongodb.'
                         'manager.Manager',
                         test_dict.get('mongodb'))

    def test_datastore_registry_with_existing_manager(self):
        datastore_registry_ext_test = {
            'mysql': 'trove.guestagent.datastore.mysql.'
                     'manager.Manager123',
        }
        dbaas_sr.get_custom_managers = Mock(return_value=
                                            datastore_registry_ext_test)
        test_dict = dbaas_sr.datastore_registry()
        self.assertEqual(test_dict.get('mysql'),
                         'trove.guestagent.datastore.mysql.'
                         'manager.Manager123')
        self.assertEqual(test_dict.get('percona'),
                         'trove.guestagent.datastore.mysql.'
                         'manager.Manager')
        self.assertEqual(test_dict.get('redis'),
                         'trove.guestagent.datastore.redis.manager.Manager')
        self.assertEqual(test_dict.get('cassandra'),
                         'trove.guestagent.datastore.cassandra.'
                         'manager.Manager')
        self.assertEqual(test_dict.get('couchbase'),
                         'trove.guestagent.datastore.couchbase.manager'
                         '.Manager')
        self.assertEqual('trove.guestagent.datastore.mongodb.manager.Manager',
                         test_dict.get('mongodb'))

    def test_datastore_registry_with_blank_dict(self):
        datastore_registry_ext_test = dict()
        dbaas_sr.get_custom_managers = Mock(return_value=
                                            datastore_registry_ext_test)
        test_dict = dbaas_sr.datastore_registry()
        self.assertEqual(test_dict.get('mysql'),
                         'trove.guestagent.datastore.mysql.'
                         'manager.Manager')
        self.assertEqual(test_dict.get('percona'),
                         'trove.guestagent.datastore.mysql.'
                         'manager.Manager')
        self.assertEqual(test_dict.get('redis'),
                         'trove.guestagent.datastore.redis.manager.Manager')
        self.assertEqual(test_dict.get('cassandra'),
                         'trove.guestagent.datastore.cassandra.'
                         'manager.Manager')
        self.assertEqual(test_dict.get('couchbase'),
                         'trove.guestagent.datastore.couchbase.manager'
                         '.Manager')
        self.assertEqual('trove.guestagent.datastore.mongodb.manager.Manager',
                         test_dict.get('mongodb'))


class KeepAliveConnectionTest(testtools.TestCase):

    class OperationalError(Exception):
        def __init__(self, value):
            self.args = [value]

        def __str__(self):
            return repr(self.value)

    def setUp(self):
        super(KeepAliveConnectionTest, self).setUp()
        self.orig_utils_execute_with_timeout = dbaas.utils.execute_with_timeout
        self.orig_LOG_err = dbaas.LOG

    def tearDown(self):
        super(KeepAliveConnectionTest, self).tearDown()
        dbaas.utils.execute_with_timeout = self.orig_utils_execute_with_timeout
        dbaas.LOG = self.orig_LOG_err

    def test_checkout_type_error(self):

        dbapi_con = Mock()
        dbapi_con.ping = Mock(side_effect=TypeError("Type Error"))

        self.keepAliveConn = KeepAliveConnection()
        self.assertRaises(TypeError, self.keepAliveConn.checkout,
                          dbapi_con, Mock(), Mock())

    def test_checkout_disconnection_error(self):

        from sqlalchemy import exc
        dbapi_con = Mock()
        dbapi_con.OperationalError = self.OperationalError
        dbapi_con.ping = Mock(side_effect=dbapi_con.OperationalError(2013))

        self.keepAliveConn = KeepAliveConnection()
        self.assertRaises(exc.DisconnectionError, self.keepAliveConn.checkout,
                          dbapi_con, Mock(), Mock())

    def test_checkout_operation_error(self):

        dbapi_con = Mock()
        dbapi_con.OperationalError = self.OperationalError
        dbapi_con.ping = Mock(side_effect=dbapi_con.OperationalError(1234))

        self.keepAliveConn = KeepAliveConnection()
        self.assertRaises(self.OperationalError, self.keepAliveConn.checkout,
                          dbapi_con, Mock(), Mock())


class BaseDbStatusTest(testtools.TestCase):

    def setUp(self):
        super(BaseDbStatusTest, self).setUp()
        util.init_db()
        self.orig_dbaas_time_sleep = time.sleep
        self.FAKE_ID = str(uuid4())
        InstanceServiceStatus.create(instance_id=self.FAKE_ID,
                                     status=rd_instance.ServiceStatuses.NEW)
        dbaas.CONF.guest_id = self.FAKE_ID

    def tearDown(self):
        super(BaseDbStatusTest, self).tearDown()
        time.sleep = self.orig_dbaas_time_sleep
        InstanceServiceStatus.find_by(instance_id=self.FAKE_ID).delete()
        dbaas.CONF.guest_id = None

    def test_begin_install(self):

        self.baseDbStatus = BaseDbStatus()

        self.baseDbStatus.begin_install()

        self.assertEqual(self.baseDbStatus.status,
                         rd_instance.ServiceStatuses.BUILDING)

    def test_begin_restart(self):

        self.baseDbStatus = BaseDbStatus()
        self.baseDbStatus.restart_mode = False

        self.baseDbStatus.begin_restart()

        self.assertTrue(self.baseDbStatus.restart_mode)

    def test_end_install_or_restart(self):

        self.baseDbStatus = BaseDbStatus()
        self.baseDbStatus._get_actual_db_status = Mock(
            return_value=rd_instance.ServiceStatuses.SHUTDOWN)

        self.baseDbStatus.end_install_or_restart()

        self.assertEqual(rd_instance.ServiceStatuses.SHUTDOWN,
                         self.baseDbStatus.status)
        self.assertFalse(self.baseDbStatus.restart_mode)

    def test_is_installed(self):
        self.baseDbStatus = BaseDbStatus()
        self.baseDbStatus.status = rd_instance.ServiceStatuses.RUNNING

        self.assertTrue(self.baseDbStatus.is_installed)

    def test_is_installed_none(self):
        self.baseDbStatus = BaseDbStatus()
        self.baseDbStatus.status = None

        self.assertFalse(self.baseDbStatus.is_installed)

    def test_is_installed_building(self):
        self.baseDbStatus = BaseDbStatus()
        self.baseDbStatus.status = rd_instance.ServiceStatuses.BUILDING

        self.assertFalse(self.baseDbStatus.is_installed)

    def test_is_installed_new(self):
        self.baseDbStatus = BaseDbStatus()
        self.baseDbStatus.status = rd_instance.ServiceStatuses.NEW

        self.assertFalse(self.baseDbStatus.is_installed)

    def test_is_installed_failed(self):
        self.baseDbStatus = BaseDbStatus()
        self.baseDbStatus.status = rd_instance.ServiceStatuses.FAILED

        self.assertFalse(self.baseDbStatus.is_installed)

    def test_is_restarting(self):
        self.baseDbStatus = BaseDbStatus()
        self.baseDbStatus.restart_mode = True

        self.assertTrue(self.baseDbStatus._is_restarting)

    def test_is_running(self):
        self.baseDbStatus = BaseDbStatus()
        self.baseDbStatus.status = rd_instance.ServiceStatuses.RUNNING

        self.assertTrue(self.baseDbStatus.is_running)

    def test_is_running_not(self):
        self.baseDbStatus = BaseDbStatus()
        self.baseDbStatus.status = rd_instance.ServiceStatuses.SHUTDOWN

        self.assertFalse(self.baseDbStatus.is_running)

    def test_wait_for_real_status_to_change_to(self):
        self.baseDbStatus = BaseDbStatus()
        self.baseDbStatus._get_actual_db_status = Mock(
            return_value=rd_instance.ServiceStatuses.RUNNING)
        time.sleep = Mock()

        self.assertTrue(self.baseDbStatus.
                        wait_for_real_status_to_change_to
                        (rd_instance.ServiceStatuses.RUNNING, 10))

    def test_wait_for_real_status_to_change_to_timeout(self):
        self.baseDbStatus = BaseDbStatus()
        self.baseDbStatus._get_actual_db_status = Mock(
            return_value=rd_instance.ServiceStatuses.RUNNING)
        time.sleep = Mock()

        self.assertFalse(self.baseDbStatus.
                         wait_for_real_status_to_change_to
                         (rd_instance.ServiceStatuses.SHUTDOWN, 10))


class MySqlAppStatusTest(testtools.TestCase):

    def setUp(self):
        super(MySqlAppStatusTest, self).setUp()
        util.init_db()
        self.orig_utils_execute_with_timeout = dbaas.utils.execute_with_timeout
        self.orig_load_mysqld_options = dbaas.load_mysqld_options
        self.orig_dbaas_os_path_exists = dbaas.os.path.exists
        self.orig_dbaas_time_sleep = time.sleep
        self.FAKE_ID = str(uuid4())
        InstanceServiceStatus.create(instance_id=self.FAKE_ID,
                                     status=rd_instance.ServiceStatuses.NEW)
        dbaas.CONF.guest_id = self.FAKE_ID

    def tearDown(self):
        super(MySqlAppStatusTest, self).tearDown()
        dbaas.utils.execute_with_timeout = self.orig_utils_execute_with_timeout
        dbaas.load_mysqld_options = self.orig_load_mysqld_options
        dbaas.os.path.exists = self.orig_dbaas_os_path_exists
        time.sleep = self.orig_dbaas_time_sleep
        InstanceServiceStatus.find_by(instance_id=self.FAKE_ID).delete()
        dbaas.CONF.guest_id = None

    def test_get_actual_db_status(self):

        dbaas.utils.execute_with_timeout = Mock(return_value=(None, None))

        self.mySqlAppStatus = MySqlAppStatus()
        status = self.mySqlAppStatus._get_actual_db_status()

        self.assertEqual(rd_instance.ServiceStatuses.RUNNING, status)

    def test_get_actual_db_status_error_shutdown(self):

        mocked = Mock(side_effect=ProcessExecutionError())
        dbaas.utils.execute_with_timeout = mocked
        dbaas.load_mysqld_options = Mock()
        dbaas.os.path.exists = Mock(return_value=False)

        self.mySqlAppStatus = MySqlAppStatus()
        status = self.mySqlAppStatus._get_actual_db_status()

        self.assertEqual(rd_instance.ServiceStatuses.SHUTDOWN, status)

    def test_get_actual_db_status_error_crashed(self):

        dbaas.utils.execute_with_timeout = MagicMock(
            side_effect=[ProcessExecutionError(), ("some output", None)])
        dbaas.load_mysqld_options = Mock()
        dbaas.os.path.exists = Mock(return_value=True)

        self.mySqlAppStatus = MySqlAppStatus()
        status = self.mySqlAppStatus._get_actual_db_status()

        self.assertEqual(rd_instance.ServiceStatuses.BLOCKED, status)


class TestRedisApp(testtools.TestCase):

    def setUp(self):
        super(TestRedisApp, self).setUp()
        self.FAKE_ID = 1000
        self.appStatus = FakeAppStatus(self.FAKE_ID,
                                       rd_instance.ServiceStatuses.NEW)
        self.app = RedisApp(self.appStatus)
        self.orig_os_path_isfile = os.path.isfile
        self.orig_utils_execute_with_timeout = utils.execute_with_timeout
        utils.execute_with_timeout = Mock()
        rservice.utils.execute_with_timeout = Mock()

    def tearDown(self):
        super(TestRedisApp, self).tearDown()
        self.app = None
        os.path.isfile = self.orig_os_path_isfile
        utils.execute_with_timeout = self.orig_utils_execute_with_timeout
        rservice.utils.execute_with_timeout = \
            self.orig_utils_execute_with_timeout

    def test_install_if_needed_installed(self):
        with patch.object(pkg.Package, 'pkg_is_installed', return_value=True):
            with patch.object(RedisApp, '_install_redis', return_value=None):
                self.app.install_if_needed('bar')
                pkg.Package.pkg_is_installed.assert_any_call('bar')
                self.assertEqual(RedisApp._install_redis.call_count, 0)

    def test_install_if_needed_not_installed(self):
        with patch.object(pkg.Package, 'pkg_is_installed', return_value=False):
            with patch.object(RedisApp, '_install_redis', return_value=None):
                self.app.install_if_needed('asdf')
                pkg.Package.pkg_is_installed.assert_any_call('asdf')
                RedisApp._install_redis.assert_any_call('asdf')

    def test_install_redis(self):
        with patch.object(utils, 'execute_with_timeout'):
            with patch.object(pkg.Package, 'pkg_install', return_value=None):
                with patch.object(RedisApp, 'start_redis', return_value=None):
                    self.app._install_redis('redis')
                    pkg.Package.pkg_install.assert_any_call('redis', {}, 1200)
                    RedisApp.start_redis.assert_any_call()
                    self.assertTrue(utils.execute_with_timeout.called)

    def test_enable_redis_on_boot_without_upstart(self):
        cmd = '123'
        with patch.object(operating_system, 'service_discovery',
                          return_value={'cmd_enable': cmd}):
            with patch.object(utils, 'execute_with_timeout',
                              return_value=None):
                self.app._enable_redis_on_boot()
                operating_system.service_discovery.assert_any_call(
                    RedisSystem.SERVICE_CANDIDATES)
                utils.execute_with_timeout.assert_any_call(
                    cmd, shell=True)

    def test_enable_redis_on_boot_with_upstart(self):
        cmd = '123'
        with patch.object(operating_system, 'service_discovery',
                          return_value={'cmd_enable': cmd}):
            with patch.object(utils, 'execute_with_timeout',
                              return_value=None):
                self.app._enable_redis_on_boot()
                operating_system.service_discovery.assert_any_call(
                    RedisSystem.SERVICE_CANDIDATES)
                utils.execute_with_timeout.assert_any_call(
                    cmd, shell=True)

    def test_disable_redis_on_boot_with_upstart(self):
        cmd = '123'
        with patch.object(operating_system, 'service_discovery',
                          return_value={'cmd_disable': cmd}):
            with patch.object(utils, 'execute_with_timeout',
                              return_value=None):
                self.app._disable_redis_on_boot()
                operating_system.service_discovery.assert_any_call(
                    RedisSystem.SERVICE_CANDIDATES)
                utils.execute_with_timeout.assert_any_call(
                    cmd, shell=True)

    def test_disable_redis_on_boot_without_upstart(self):
        cmd = '123'
        with patch.object(operating_system, 'service_discovery',
                          return_value={'cmd_disable': cmd}):
            with patch.object(utils, 'execute_with_timeout',
                              return_value=None):
                self.app._disable_redis_on_boot()
                operating_system.service_discovery.assert_any_call(
                    RedisSystem.SERVICE_CANDIDATES)
                utils.execute_with_timeout.assert_any_call(
                    cmd, shell=True)

    def test_stop_db_without_fail(self):
        mock_status = MagicMock()
        mock_status.wait_for_real_status_to_change_to = MagicMock(
            return_value=True)
        app = RedisApp(mock_status, state_change_wait_time=0)
        RedisApp._disable_redis_on_boot = MagicMock(
            return_value=None)

        with patch.object(utils, 'execute_with_timeout', return_value=None):
            mock_status.wait_for_real_status_to_change_to = MagicMock(
                return_value=True)
            app.stop_db(do_not_start_on_reboot=True)

            utils.execute_with_timeout.assert_any_call(
                'sudo ' + RedisSystem.REDIS_CMD_STOP,
                shell=True)
            self.assertTrue(RedisApp._disable_redis_on_boot.called)
            self.assertTrue(
                mock_status.wait_for_real_status_to_change_to.called)

    def test_stop_db_with_failure(self):
        mock_status = MagicMock()
        mock_status.wait_for_real_status_to_change_to = MagicMock(
            return_value=True)
        app = RedisApp(mock_status, state_change_wait_time=0)
        RedisApp._disable_redis_on_boot = MagicMock(
            return_value=None)

        with patch.object(utils, 'execute_with_timeout', return_value=None):
            mock_status.wait_for_real_status_to_change_to = MagicMock(
                return_value=False)
            app.stop_db(do_not_start_on_reboot=True)

            utils.execute_with_timeout.assert_any_call(
                'sudo ' + RedisSystem.REDIS_CMD_STOP,
                shell=True)
            self.assertTrue(RedisApp._disable_redis_on_boot.called)
            self.assertTrue(mock_status.end_install_or_restart.called)
            self.assertTrue(
                mock_status.wait_for_real_status_to_change_to.called)

    def test_restart(self):
        mock_status = MagicMock()
        app = RedisApp(mock_status, state_change_wait_time=0)
        mock_status.begin_restart = MagicMock(return_value=None)
        with patch.object(RedisApp, 'stop_db', return_value=None):
            with patch.object(RedisApp, 'start_redis', return_value=None):
                mock_status.end_install_or_restart = MagicMock(
                    return_value=None)
                app.restart()
                mock_status.begin_restart.assert_any_call()
                RedisApp.stop_db.assert_any_call()
                RedisApp.start_redis.assert_any_call()
                mock_status.end_install_or_restart.assert_any_call()

    def test_start_redis(self):
        mock_status = MagicMock()
        app = RedisApp(mock_status, state_change_wait_time=0)
        with patch.object(RedisApp, '_enable_redis_on_boot',
                          return_value=None):
            with patch.object(utils, 'execute_with_timeout',
                              return_value=None):
                mock_status.wait_for_real_status_to_change_to = MagicMock(
                    return_value=None)
                mock_status.end_install_or_restart = MagicMock(
                    return_value=None)
                app.start_redis()

                utils.execute_with_timeout.assert_any_call(
                    'sudo ' + RedisSystem.REDIS_CMD_START,
                    shell=True)
                utils.execute_with_timeout.assert_any_call('pkill', '-9',
                                                           'redis-server',
                                                           run_as_root=True,
                                                           root_helper='sudo')
                self.assertTrue(RedisApp._enable_redis_on_boot.called)
                self.assertTrue(mock_status.end_install_or_restart.called)
                self.assertTrue(
                    mock_status.wait_for_real_status_to_change_to.callled)


class CassandraDBAppTest(testtools.TestCase):

    def setUp(self):
        super(CassandraDBAppTest, self).setUp()
        self.utils_execute_with_timeout = (
            cass_service.utils.execute_with_timeout)
        self.sleep = time.sleep
        self.pkg_version = cass_service.packager.pkg_version
        self.pkg = cass_service.packager
        util.init_db()
        self.FAKE_ID = str(uuid4())
        InstanceServiceStatus.create(instance_id=self.FAKE_ID,
                                     status=rd_instance.ServiceStatuses.NEW)
        self.appStatus = FakeAppStatus(self.FAKE_ID,
                                       rd_instance.ServiceStatuses.NEW)
        self.cassandra = cass_service.CassandraApp(self.appStatus)

    def tearDown(self):

        super(CassandraDBAppTest, self).tearDown()
        cass_service.utils.execute_with_timeout = (self.
                                                   utils_execute_with_timeout)
        time.sleep = self.sleep
        cass_service.packager.pkg_version = self.pkg_version
        cass_service.packager = self.pkg
        InstanceServiceStatus.find_by(instance_id=self.FAKE_ID).delete()

    def assert_reported_status(self, expected_status):
        service_status = InstanceServiceStatus.find_by(
            instance_id=self.FAKE_ID)
        self.assertEqual(expected_status, service_status.status)

    def test_stop_db(self):

        cass_service.utils.execute_with_timeout = Mock()
        self.appStatus.set_next_status(
            rd_instance.ServiceStatuses.SHUTDOWN)

        self.cassandra.stop_db()
        self.assert_reported_status(rd_instance.ServiceStatuses.NEW)

    def test_stop_db_with_db_update(self):

        cass_service.utils.execute_with_timeout = Mock()
        self.appStatus.set_next_status(
            rd_instance.ServiceStatuses.SHUTDOWN)

        self.cassandra.stop_db(True)
        self.assertTrue(conductor_api.API.heartbeat.called_once_with(
            self.FAKE_ID,
            {'service_status':
             rd_instance.ServiceStatuses.SHUTDOWN.description}))

    def test_stop_db_error(self):

        cass_service.utils.execute_with_timeout = Mock()
        self.appStatus.set_next_status(rd_instance.ServiceStatuses.RUNNING)
        self.cassandra.state_change_wait_time = 1
        self.assertRaises(RuntimeError, self.cassandra.stop_db)

    def test_restart(self):

        self.cassandra.stop_db = Mock()
        self.cassandra.start_db = Mock()
        self.appStatus.set_next_status(rd_instance.ServiceStatuses.RUNNING)

        self.cassandra.restart()

        self.assertTrue(conductor_api.API.heartbeat.called_once_with(
            self.FAKE_ID,
            {'service_status':
             rd_instance.ServiceStatuses.RUNNING.description}))
        self.assert_reported_status(rd_instance.ServiceStatuses.NEW)

    def test_start_cassandra(self):

        cass_service.utils.execute_with_timeout = Mock()
        self.appStatus.set_next_status(rd_instance.ServiceStatuses.RUNNING)

        self.cassandra.start_db()
        self.assert_reported_status(rd_instance.ServiceStatuses.NEW)

    def test_start_cassandra_runs_forever(self):

        cass_service.utils.execute_with_timeout = Mock()
        (self.cassandra.status.
         wait_for_real_status_to_change_to) = Mock(return_value=False)
        self.appStatus.set_next_status(rd_instance.ServiceStatuses.SHUTDOWN)

        self.assertRaises(RuntimeError, self.cassandra.stop_db)
        self.assertTrue(conductor_api.API.heartbeat.called_once_with(
            self.FAKE_ID,
            {'service_status':
             rd_instance.ServiceStatuses.SHUTDOWN.description}))

    def test_start_db_with_db_update(self):

        cass_service.utils.execute_with_timeout = Mock()
        self.appStatus.set_next_status(
            rd_instance.ServiceStatuses.RUNNING)

        self.cassandra.start_db(True)
        self.assertTrue(conductor_api.API.heartbeat.called_once_with(
            self.FAKE_ID,
            {'service_status':
             rd_instance.ServiceStatuses.RUNNING.description}))
        self.assert_reported_status(rd_instance.ServiceStatuses.NEW)

    def test_start_cassandra_error(self):
        self.cassandra._enable_db_on_boot = Mock()
        self.cassandra.state_change_wait_time = 1
        cass_service.utils.execute_with_timeout = Mock(
            side_effect=ProcessExecutionError('Error'))

        self.assertRaises(RuntimeError, self.cassandra.start_db)

    def test_install(self):

        self.cassandra._install_db = Mock()
        self.pkg.pkg_is_installed = Mock(return_value=False)
        self.cassandra.install_if_needed(['cassandra'])
        self.assertTrue(self.cassandra._install_db.called)
        self.assert_reported_status(rd_instance.ServiceStatuses.NEW)

    def test_install_install_error(self):

        from trove.guestagent import pkg
        self.cassandra.start_db = Mock()
        self.cassandra.stop_db = Mock()
        self.pkg.pkg_is_installed = Mock(return_value=False)
        self.cassandra._install_db = Mock(
            side_effect=pkg.PkgPackageStateError("Install error"))

        self.assertRaises(pkg.PkgPackageStateError,
                          self.cassandra.install_if_needed,
                          ['cassandra=1.2.10'])

        self.assert_reported_status(rd_instance.ServiceStatuses.NEW)


class CouchbaseAppTest(testtools.TestCase):

    def fake_couchbase_service_discovery(self, candidates):
        return {
            'cmd_start': 'start',
            'cmd_stop': 'stop',
            'cmd_enable': 'enable',
            'cmd_disable': 'disable'
        }

    def setUp(self):
        super(CouchbaseAppTest, self).setUp()
        self.orig_utils_execute_with_timeout = (
            couchservice.utils.execute_with_timeout)
        self.orig_time_sleep = time.sleep
        time.sleep = Mock()
        self.orig_service_discovery = operating_system.service_discovery
        operating_system.service_discovery = (
            self.fake_couchbase_service_discovery)
        operating_system.get_ip_address = Mock()
        self.FAKE_ID = str(uuid4())
        InstanceServiceStatus.create(instance_id=self.FAKE_ID,
                                     status=rd_instance.ServiceStatuses.NEW)
        self.appStatus = FakeAppStatus(self.FAKE_ID,
                                       rd_instance.ServiceStatuses.NEW)
        self.couchbaseApp = couchservice.CouchbaseApp(self.appStatus)
        dbaas.CONF.guest_id = self.FAKE_ID

    def tearDown(self):
        super(CouchbaseAppTest, self).tearDown()
        couchservice.utils.execute_with_timeout = (
            self.orig_utils_execute_with_timeout)
        operating_system.service_discovery = self.orig_service_discovery
        time.sleep = self.orig_time_sleep
        InstanceServiceStatus.find_by(instance_id=self.FAKE_ID).delete()
        dbaas.CONF.guest_id = None

    def assert_reported_status(self, expected_status):
        service_status = InstanceServiceStatus.find_by(
            instance_id=self.FAKE_ID)
        self.assertEqual(expected_status, service_status.status)

    def test_stop_db(self):
        couchservice.utils.execute_with_timeout = Mock()
        self.appStatus.set_next_status(rd_instance.ServiceStatuses.SHUTDOWN)

        self.couchbaseApp.stop_db()
        self.assert_reported_status(rd_instance.ServiceStatuses.NEW)

    def test_stop_db_error(self):
        couchservice.utils.execute_with_timeout = Mock()
        self.appStatus.set_next_status(rd_instance.ServiceStatuses.RUNNING)
        self.couchbaseApp.state_change_wait_time = 1

        self.assertRaises(RuntimeError, self.couchbaseApp.stop_db)

    def test_restart(self):
        self.appStatus.set_next_status(rd_instance.ServiceStatuses.RUNNING)
        self.couchbaseApp.stop_db = Mock()
        self.couchbaseApp.start_db = Mock()

        self.couchbaseApp.restart()

        self.assertTrue(self.couchbaseApp.stop_db.called)
        self.assertTrue(self.couchbaseApp.start_db.called)
        self.assertTrue(conductor_api.API.heartbeat.called)

    def test_start_db(self):
        couchservice.utils.execute_with_timeout = Mock()
        self.appStatus.set_next_status(rd_instance.ServiceStatuses.RUNNING)
        self.couchbaseApp._enable_db_on_boot = Mock()

        self.couchbaseApp.start_db()
        self.assert_reported_status(rd_instance.ServiceStatuses.NEW)

    def test_start_db_error(self):
        from trove.common.exception import ProcessExecutionError
        mocked = Mock(side_effect=ProcessExecutionError('Error'))
        couchservice.utils.execute_with_timeout = mocked
        self.couchbaseApp._enable_db_on_boot = Mock()

        self.assertRaises(RuntimeError, self.couchbaseApp.start_db)

    def test_start_db_runs_forever(self):
        couchservice.utils.execute_with_timeout = Mock()
        self.couchbaseApp._enable_db_on_boot = Mock()
        self.couchbaseApp.state_change_wait_time = 1
        self.appStatus.set_next_status(rd_instance.ServiceStatuses.SHUTDOWN)

        self.assertRaises(RuntimeError, self.couchbaseApp.start_db)
        self.assertTrue(conductor_api.API.heartbeat.called)

    def test_install_when_couchbase_installed(self):
        self.couchbaseApp.initial_setup = Mock()
        couchservice.packager.pkg_is_installed = Mock(return_value=True)
        couchservice.utils.execute_with_timeout = Mock()

        self.couchbaseApp.install_if_needed(["package"])
        self.assertTrue(couchservice.packager.pkg_is_installed.called)
        self.assertTrue(self.couchbaseApp.initial_setup.called)
        self.assert_reported_status(rd_instance.ServiceStatuses.NEW)


class MongoDBAppTest(testtools.TestCase):

    def fake_mongodb_service_discovery(self, candidates):
        return {
            'cmd_start': 'start',
            'cmd_stop': 'stop',
            'cmd_enable': 'enable',
            'cmd_disable': 'disable'
        }

    def setUp(self):
        super(MongoDBAppTest, self).setUp()
        self.orig_utils_execute_with_timeout = (mongo_service.
                                                utils.execute_with_timeout)
        self.orig_time_sleep = time.sleep
        self.orig_packager = mongo_system.PACKAGER
        self.orig_service_discovery = operating_system.service_discovery

        operating_system.service_discovery = (
            self.fake_mongodb_service_discovery)
        util.init_db()
        self.FAKE_ID = str(uuid4())
        InstanceServiceStatus.create(instance_id=self.FAKE_ID,
                                     status=rd_instance.ServiceStatuses.NEW)
        self.appStatus = FakeAppStatus(self.FAKE_ID,
                                       rd_instance.ServiceStatuses.NEW)
        self.mongoDbApp = mongo_service.MongoDBApp(self.appStatus)
        time.sleep = Mock()

    def tearDown(self):
        super(MongoDBAppTest, self).tearDown()
        mongo_service.utils.execute_with_timeout = (
            self.orig_utils_execute_with_timeout)
        time.sleep = self.orig_time_sleep
        mongo_system.PACKAGER = self.orig_packager
        operating_system.service_discovery = self.orig_service_discovery
        InstanceServiceStatus.find_by(instance_id=self.FAKE_ID).delete()

    def assert_reported_status(self, expected_status):
        service_status = InstanceServiceStatus.find_by(
            instance_id=self.FAKE_ID)
        self.assertEqual(expected_status, service_status.status)

    def test_stopdb(self):
        mongo_service.utils.execute_with_timeout = Mock()
        self.appStatus.set_next_status(
            rd_instance.ServiceStatuses.SHUTDOWN)

        self.mongoDbApp.stop_db()
        self.assert_reported_status(rd_instance.ServiceStatuses.NEW)

    def test_stop_db_with_db_update(self):

        mongo_service.utils.execute_with_timeout = Mock()
        self.appStatus.set_next_status(
            rd_instance.ServiceStatuses.SHUTDOWN)

        self.mongoDbApp.stop_db(True)
        self.assertTrue(conductor_api.API.heartbeat.called_once_with(
            self.FAKE_ID, {'service_status': 'shutdown'}))

    def test_stop_db_error(self):

        mongo_service.utils.execute_with_timeout = Mock()
        self.appStatus.set_next_status(rd_instance.ServiceStatuses.RUNNING)
        self.mongoDbApp.state_change_wait_time = 1
        self.assertRaises(RuntimeError, self.mongoDbApp.stop_db)

    def test_restart(self):

        self.appStatus.set_next_status(rd_instance.ServiceStatuses.RUNNING)
        self.mongoDbApp.stop_db = Mock()
        self.mongoDbApp.start_db = Mock()

        self.mongoDbApp.restart()

        self.assertTrue(self.mongoDbApp.stop_db.called)
        self.assertTrue(self.mongoDbApp.start_db.called)

        self.assertTrue(conductor_api.API.heartbeat.called_once_with(
            self.FAKE_ID, {'service_status': 'shutdown'}))

        self.assertTrue(conductor_api.API.heartbeat.called_once_with(
            self.FAKE_ID, {'service_status': 'running'}))

    def test_start_db(self):

        mongo_service.utils.execute_with_timeout = Mock()
        self.appStatus.set_next_status(rd_instance.ServiceStatuses.RUNNING)

        self.mongoDbApp.start_db()
        self.assert_reported_status(rd_instance.ServiceStatuses.NEW)

    def test_start_db_with_update(self):

        mongo_service.utils.execute_with_timeout = Mock()
        self.appStatus.set_next_status(rd_instance.ServiceStatuses.RUNNING)

        self.mongoDbApp.start_db(True)
        self.assertTrue(conductor_api.API.heartbeat.called_once_with(
            self.FAKE_ID, {'service_status': 'running'}))

    def test_start_db_runs_forever(self):

        mongo_service.utils.execute_with_timeout = Mock(
            return_value=["ubuntu 17036  0.0  0.1 618960 "
                          "29232 pts/8    Sl+  Jan29   0:07 mongod", ""])
        self.mongoDbApp.state_change_wait_time = 1
        self.appStatus.set_next_status(rd_instance.ServiceStatuses.SHUTDOWN)

        self.assertRaises(RuntimeError, self.mongoDbApp.start_db)
        self.assertTrue(conductor_api.API.heartbeat.called_once_with(
            self.FAKE_ID, {'service_status': 'shutdown'}))

    def test_start_db_error(self):

        self.mongoDbApp._enable_db_on_boot = Mock()
        from trove.common.exception import ProcessExecutionError
        mocked = Mock(side_effect=ProcessExecutionError('Error'))
        mongo_service.utils.execute_with_timeout = mocked

        self.assertRaises(RuntimeError, self.mongoDbApp.start_db)

    def test_start_db_with_conf_changes_db_is_running(self):

        self.mongoDbApp.start_db = Mock()

        self.appStatus.status = rd_instance.ServiceStatuses.RUNNING
        self.assertRaises(RuntimeError,
                          self.mongoDbApp.start_db_with_conf_changes,
                          Mock())

    def test_install_when_db_installed(self):
        packager_mock = MagicMock()
        packager_mock.pkg_is_installed = MagicMock(return_value=True)
        mongo_system.PACKAGER = packager_mock
        self.mongoDbApp.install_if_needed(['package'])
        self.assert_reported_status(rd_instance.ServiceStatuses.NEW)

    def test_install_when_db_not_installed(self):
        packager_mock = MagicMock()
        packager_mock.pkg_is_installed = MagicMock(return_value=False)
        mongo_system.PACKAGER = packager_mock
        self.mongoDbApp.install_if_needed(['package'])
        packager_mock.pkg_install.assert_any_call(ANY, {}, ANY)
        self.assert_reported_status(rd_instance.ServiceStatuses.NEW)

########NEW FILE########
__FILENAME__ = test_dbmodels
#    Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
import testtools
from trove.guestagent.db import models as dbmodels
from mock import MagicMock


class MySQLDatabaseTest(testtools.TestCase):

    def setUp(self):
        super(MySQLDatabaseTest, self).setUp()

        self.mysqlDb = dbmodels.ValidatedMySQLDatabase()
        self.origin_ignore_db = self.mysqlDb._ignore_dbs
        self.mysqlDb._ignore_dbs = ['mysql']

    def tearDown(self):
        super(MySQLDatabaseTest, self).tearDown()
        self.mysqlDb._ignore_dbs = self.origin_ignore_db

    def test_name(self):
        self.assertEqual(self.mysqlDb.name, None)

    def test_name_setter(self):
        test_name = "Anna"
        self.mysqlDb.name = test_name
        self.assertEqual(test_name, self.mysqlDb.name)

    def test_is_valid_positive(self):
        self.assertTrue(self.mysqlDb._is_valid('mysqldb'))

    def test_is_valid_negative(self):
        self.assertFalse(self.mysqlDb._is_valid('mysql'))


class MySQLUserTest(testtools.TestCase):
    def setUp(self):
        super(MySQLUserTest, self).setUp()
        self.mysqlUser = dbmodels.MySQLUser()

    def tearDown(self):
        super(MySQLUserTest, self).tearDown()

    def test_is_valid_negative(self):
        self.assertFalse(self.mysqlUser._is_valid(None))
        self.assertFalse(self.mysqlUser._is_valid("|;"))
        self.assertFalse(self.mysqlUser._is_valid("\\"))

    def test_is_valid_positive(self):
        self.assertTrue(self.mysqlUser._is_valid("real_name"))


class IsValidUsernameTest(testtools.TestCase):
    def setUp(self):
        super(IsValidUsernameTest, self).setUp()
        self.mysqlUser = dbmodels.MySQLUser()
        self.origin_is_valid = self.mysqlUser._is_valid
        self.origin_ignore_users = self.mysqlUser._ignore_users
        self.mysqlUser._ignore_users = ["king"]

    def tearDown(self):
        super(IsValidUsernameTest, self).tearDown()
        self.mysqlUser._is_valid = self.origin_is_valid
        self.mysqlUser._ignore_users = self.origin_ignore_users

    def test_is_valid_user_name(self):
        value = "trove"
        self.assertTrue(self.mysqlUser._is_valid_user_name(value))

    def test_is_valid_user_name_negative(self):
        self.mysqlUser._is_valid = MagicMock(return_value=False)
        self.assertFalse(self.mysqlUser._is_valid_user_name("trove"))

        self.mysqlUser._is_valid = MagicMock(return_value=True)
        self.assertFalse(self.mysqlUser._is_valid_user_name("king"))


class IsValidHostnameTest(testtools.TestCase):
    def setUp(self):
        super(IsValidHostnameTest, self).setUp()
        self.mysqlUser = dbmodels.MySQLUser()

    def tearDown(self):
        super(IsValidHostnameTest, self).tearDown()

    def test_is_valid_octet(self):
        self.assertTrue(self.mysqlUser._is_valid_host_name('192.168.1.1'))

    def test_is_valid_bad_octet(self):
        self.assertFalse(self.mysqlUser._is_valid_host_name('999.168.1.1'))

    def test_is_valid_global_wildcard(self):
        self.assertTrue(self.mysqlUser._is_valid_host_name('%'))

    def test_is_valid_prefix_wildcard(self):
        self.assertTrue(self.mysqlUser._is_valid_host_name('%.168.1.1'))

    def test_is_valid_suffix_wildcard(self):
        self.assertTrue(self.mysqlUser._is_valid_host_name('192.168.1.%'))

########NEW FILE########
__FILENAME__ = test_models
#    Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import testtools
from mock import Mock, MagicMock
from trove.guestagent import models
from trove.common import utils
from trove.db.sqlalchemy import api as dbapi
from trove.db import models as dbmodels
from datetime import datetime


class AgentHeartBeatTest(testtools.TestCase):
    def setUp(self):
        super(AgentHeartBeatTest, self).setUp()
        self.origin_get_db_api = dbmodels.get_db_api
        self.origin_utcnow = utils.utcnow
        self.origin_DatabaseModelBase = dbmodels.DatabaseModelBase
        self.origin_db_api_save = dbapi.save
        self.origin_is_valid = dbmodels.DatabaseModelBase.is_valid
        self.origin_generate_uuid = utils.generate_uuid

    def tearDown(self):
        super(AgentHeartBeatTest, self).tearDown()
        dbmodels.get_db_api = self.origin_get_db_api
        utils.utcnow = self.origin_utcnow
        dbmodels.DatabaseModelBase = self.origin_DatabaseModelBase
        dbapi.save = self.origin_db_api_save
        dbmodels.DatabaseModelBase.is_valid = self.origin_is_valid
        utils.generate_uuid = self.origin_generate_uuid

    def test_create(self):
        utils.generate_uuid = Mock()
        dbapi.save = MagicMock(
            return_value=dbmodels.DatabaseModelBase)
        dbmodels.DatabaseModelBase.is_valid = Mock(return_value=True)
        models.AgentHeartBeat.create()
        self.assertEqual(1, utils.generate_uuid.call_count)
        self.assertEqual(3,
                         dbmodels.DatabaseModelBase.is_valid.call_count)

    def test_save(self):
        utils.utcnow = Mock()
        dbmodels.DatabaseModelBase = Mock
        dbmodels.get_db_api = MagicMock(
            return_value=dbmodels.DatabaseModelBase)
        dbapi.save = Mock()
        dbmodels.DatabaseModelBase.is_valid = Mock(return_value=True)
        self.heartBeat = models.AgentHeartBeat()
        self.heartBeat.save()
        self.assertEqual(1, utils.utcnow.call_count)

    def test_is_active(self):
        models.AGENT_HEARTBEAT = 10000000000
        mock = models.AgentHeartBeat()
        models.AgentHeartBeat.__setitem__(mock, 'updated_at', datetime.now())
        self.assertTrue(models.AgentHeartBeat.is_active(mock))

########NEW FILE########
__FILENAME__ = test_mongodb_manager
#    Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import os

import testtools
from mock import MagicMock
from trove.common.context import TroveContext
from trove.guestagent import volume
from trove.guestagent.datastore.mongodb import service as mongo_service
from trove.guestagent.datastore.mongodb import manager as mongo_manager
from trove.guestagent.volume import VolumeDevice


class GuestAgentMongoDBManagerTest(testtools.TestCase):

    def setUp(self):
        super(GuestAgentMongoDBManagerTest, self).setUp()
        self.context = TroveContext()
        self.manager = mongo_manager.Manager()
        self.origin_MongoDbAppStatus = mongo_service.MongoDbAppStatus
        self.origin_os_path_exists = os.path.exists
        self.origin_format = volume.VolumeDevice.format
        self.origin_migrate_data = volume.VolumeDevice.migrate_data
        self.origin_mount = volume.VolumeDevice.mount
        self.origin_mount_points = volume.VolumeDevice.mount_points
        self.origin_stop_db = mongo_service.MongoDBApp.stop_db
        self.origin_start_db = mongo_service.MongoDBApp.start_db

    def tearDown(self):
        super(GuestAgentMongoDBManagerTest, self).tearDown()
        mongo_service.MongoDbAppStatus = self.origin_MongoDbAppStatus
        os.path.exists = self.origin_os_path_exists
        volume.VolumeDevice.format = self.origin_format
        volume.VolumeDevice.migrate_data = self.origin_migrate_data
        volume.VolumeDevice.mount = self.origin_mount
        volume.VolumeDevice.mount_points = self.origin_mount_points
        mongo_service.MongoDBApp.stop_db = self.origin_stop_db
        mongo_service.MongoDBApp.start_db = self.origin_start_db

    def test_update_status(self):
        self.manager.status = MagicMock()
        self.manager.update_status(self.context)
        self.manager.status.update.assert_any_call()

    def test_prepare_from_backup(self):
        self._prepare_dynamic(backup_id='backup_id_123abc')

    def _prepare_dynamic(self, device_path='/dev/vdb', is_db_installed=True,
                         backup_id=None):

        # covering all outcomes is starting to cause trouble here
        backup_info = {'id': backup_id,
                       'location': 'fake-location',
                       'type': 'MongoDBDump',
                       'checksum': 'fake-checksum'} if backup_id else None

        mock_status = MagicMock()
        mock_app = MagicMock()
        self.manager.status = mock_status
        self.manager.app = mock_app

        mock_status.begin_install = MagicMock(return_value=None)
        volume.VolumeDevice.format = MagicMock(return_value=None)
        volume.VolumeDevice.migrate_data = MagicMock(return_value=None)
        volume.VolumeDevice.mount = MagicMock(return_value=None)
        volume.VolumeDevice.mount_points = MagicMock(return_value=[])

        mock_app.stop_db = MagicMock(return_value=None)
        mock_app.start_db = MagicMock(return_value=None)
        mock_app.clear_storage = MagicMock(return_value=None)
        os.path.exists = MagicMock(return_value=is_db_installed)

        # invocation
        self.manager.prepare(context=self.context, databases=None,
                             packages=['package'],
                             memory_mb='2048', users=None,
                             device_path=device_path,
                             mount_point='/var/lib/mongodb',
                             backup_info=backup_info)

        # verification/assertion
        mock_status.begin_install.assert_any_call()
        mock_app.install_if_needed.assert_any_call(['package'])
        mock_app.stop_db.assert_any_call()
        VolumeDevice.format.assert_any_call()
        VolumeDevice.migrate_data.assert_any_call('/var/lib/mongodb')

########NEW FILE########
__FILENAME__ = test_mysql_manager
#    Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import os

import testtools
from mock import MagicMock
from testtools.matchers import Is, Equals, Not
from trove.common.context import TroveContext
from trove.guestagent import volume
from trove.guestagent.datastore.mysql.manager import Manager
import trove.guestagent.datastore.mysql.service as dbaas
from trove.guestagent import backup
from trove.guestagent.volume import VolumeDevice
from trove.guestagent import pkg as pkg


class GuestAgentManagerTest(testtools.TestCase):
    def setUp(self):
        super(GuestAgentManagerTest, self).setUp()
        self.context = TroveContext()
        self.manager = Manager()
        self.origin_MySqlAppStatus = dbaas.MySqlAppStatus
        self.origin_os_path_exists = os.path.exists
        self.origin_format = volume.VolumeDevice.format
        self.origin_migrate_data = volume.VolumeDevice.migrate_data
        self.origin_mount = volume.VolumeDevice.mount
        self.origin_unmount = volume.VolumeDevice.unmount
        self.origin_mount_points = volume.VolumeDevice.mount_points
        self.origin_stop_mysql = dbaas.MySqlApp.stop_db
        self.origin_start_mysql = dbaas.MySqlApp.start_mysql
        self.origin_pkg_is_installed = pkg.Package.pkg_is_installed
        self.origin_os_path_exists = os.path.exists

    def tearDown(self):
        super(GuestAgentManagerTest, self).tearDown()
        dbaas.MySqlAppStatus = self.origin_MySqlAppStatus
        os.path.exists = self.origin_os_path_exists
        volume.VolumeDevice.format = self.origin_format
        volume.VolumeDevice.migrate_data = self.origin_migrate_data
        volume.VolumeDevice.mount = self.origin_mount
        volume.VolumeDevice.unmount = self.origin_unmount
        volume.VolumeDevice.mount_points = self.origin_mount_points
        dbaas.MySqlApp.stop_db = self.origin_stop_mysql
        dbaas.MySqlApp.start_mysql = self.origin_start_mysql
        pkg.Package.pkg_is_installed = self.origin_pkg_is_installed
        os.path.exists = self.origin_os_path_exists

    def test_update_status(self):
        mock_status = MagicMock()
        dbaas.MySqlAppStatus.get = MagicMock(return_value=mock_status)
        self.manager.update_status(self.context)
        dbaas.MySqlAppStatus.get.assert_any_call()
        mock_status.update.assert_any_call()

    def test_create_database(self):
        dbaas.MySqlAdmin.create_database = MagicMock(return_value=None)
        self.manager.create_database(self.context, ['db1'])
        dbaas.MySqlAdmin.create_database.assert_any_call(['db1'])

    def test_create_user(self):
        dbaas.MySqlAdmin.create_user = MagicMock(return_value=None)
        self.manager.create_user(self.context, ['user1'])
        dbaas.MySqlAdmin.create_user.assert_any_call(['user1'])

    def test_delete_database(self):
        databases = ['db1']
        dbaas.MySqlAdmin.delete_database = MagicMock(return_value=None)
        self.manager.delete_database(self.context, databases)
        dbaas.MySqlAdmin.delete_database.assert_any_call(databases)

    def test_delete_user(self):
        user = ['user1']
        dbaas.MySqlAdmin.delete_user = MagicMock(return_value=None)
        self.manager.delete_user(self.context, user)
        dbaas.MySqlAdmin.delete_user.assert_any_call(user)

    def test_grant_access(self):
        username = "test_user"
        hostname = "test_host"
        databases = ["test_database"]
        dbaas.MySqlAdmin.grant_access = MagicMock(return_value=None)
        self.manager.grant_access(self.context,
                                  username,
                                  hostname,
                                  databases)

        dbaas.MySqlAdmin.grant_access.assert_any_call(username,
                                                      hostname,
                                                      databases)

    def test_list_databases(self):
        dbaas.MySqlAdmin.list_databases = MagicMock(return_value=['database1'])
        databases = self.manager.list_databases(self.context)
        self.assertThat(databases, Not(Is(None)))
        self.assertThat(databases, Equals(['database1']))
        dbaas.MySqlAdmin.list_databases.assert_any_call(None, None, False)

    def test_list_users(self):
        dbaas.MySqlAdmin.list_users = MagicMock(return_value=['user1'])
        users = self.manager.list_users(self.context)
        self.assertThat(users, Equals(['user1']))
        dbaas.MySqlAdmin.list_users.assert_any_call(None, None, False)

    def test_get_users(self):
        username = ['user1']
        hostname = ['host']
        dbaas.MySqlAdmin.get_user = MagicMock(return_value=['user1'])
        users = self.manager.get_user(self.context, username, hostname)
        self.assertThat(users, Equals(['user1']))
        dbaas.MySqlAdmin.get_user.assert_any_call(username, hostname)

    def test_enable_root(self):
        dbaas.MySqlAdmin.enable_root = MagicMock(return_value='user_id_stuff')
        user_id = self.manager.enable_root(self.context)
        self.assertThat(user_id, Is('user_id_stuff'))
        dbaas.MySqlAdmin.enable_root.assert_any_call()

    def test_is_root_enabled(self):
        dbaas.MySqlAdmin.is_root_enabled = MagicMock(return_value=True)
        is_enabled = self.manager.is_root_enabled(self.context)
        self.assertThat(is_enabled, Is(True))
        dbaas.MySqlAdmin.is_root_enabled.assert_any_call()

    def test_create_backup(self):
        backup.backup = MagicMock(return_value=None)
        # entry point
        Manager().create_backup(self.context, 'backup_id_123')
        # assertions
        backup.backup.assert_any_call(self.context, 'backup_id_123')

    def test_prepare_device_path_true(self):
        self._prepare_dynamic()

    def test_prepare_device_path_false(self):
        self._prepare_dynamic(device_path=None)

    def test_prepare_device_path_mounted(self):
        self._prepare_dynamic(is_mounted=True)

    def test_prepare_mysql_not_installed(self):
        self._prepare_dynamic(is_mysql_installed=False)

    def test_prepare_mysql_from_backup(self):
        self._prepare_dynamic(backup_id='backup_id_123abc')

    def test_prepare_mysql_from_backup_with_root(self):
        self._prepare_dynamic(backup_id='backup_id_123abc',
                              is_root_enabled=True)

    def _prepare_dynamic(self, device_path='/dev/vdb', is_mysql_installed=True,
                         backup_id=None, is_root_enabled=False,
                         overrides=None, is_mounted=False):
        # covering all outcomes is starting to cause trouble here
        COUNT = 1 if device_path else 0
        backup_info = None
        if backup_id is not None:
            backup_info = {'id': backup_id,
                           'location': 'fake-location',
                           'type': 'InnoBackupEx',
                           'checksum': 'fake-checksum',
                           }

        # TODO(juice): this should stub an instance of the MySqlAppStatus
        mock_status = MagicMock()

        dbaas.MySqlAppStatus.get = MagicMock(return_value=mock_status)
        mock_status.begin_install = MagicMock(return_value=None)
        VolumeDevice.format = MagicMock(return_value=None)
        VolumeDevice.migrate_data = MagicMock(return_value=None)
        VolumeDevice.mount = MagicMock(return_value=None)
        mount_points = []
        if is_mounted:
            mount_points = ['/mnt']
        VolumeDevice.mount_points = MagicMock(return_value=mount_points)
        VolumeDevice.unmount = MagicMock(return_value=None)
        dbaas.MySqlApp.stop_db = MagicMock(return_value=None)
        dbaas.MySqlApp.start_mysql = MagicMock(return_value=None)
        dbaas.MySqlApp.install_if_needed = MagicMock(return_value=None)
        backup.restore = MagicMock(return_value=None)
        dbaas.MySqlApp.secure = MagicMock(return_value=None)
        dbaas.MySqlApp.secure_root = MagicMock(return_value=None)
        pkg.Package.pkg_is_installed = MagicMock(
            return_value=is_mysql_installed)
        dbaas.MySqlAdmin.is_root_enabled = MagicMock(
            return_value=is_root_enabled)
        dbaas.MySqlAdmin.create_user = MagicMock(return_value=None)
        dbaas.MySqlAdmin.create_database = MagicMock(return_value=None)

        os.path.exists = MagicMock(return_value=True)
        # invocation
        self.manager.prepare(context=self.context,
                             packages=None,
                             memory_mb='2048',
                             databases=None,
                             users=None,
                             device_path=device_path,
                             mount_point='/var/lib/mysql',
                             backup_info=backup_info,
                             overrides=overrides)

        # verification/assertion
        mock_status.begin_install.assert_any_call()

        self.assertEqual(VolumeDevice.format.call_count, COUNT)
        self.assertEqual(VolumeDevice.migrate_data.call_count, COUNT)
        self.assertEqual(VolumeDevice.mount_points.call_count, COUNT)
        self.assertEqual(dbaas.MySqlApp.stop_db.call_count, COUNT)
        if is_mounted:
            self.assertEqual(VolumeDevice.unmount.call_count, 1)
        else:
            self.assertEqual(VolumeDevice.unmount.call_count, 0)
        if backup_info:
            backup.restore.assert_any_call(self.context,
                                           backup_info,
                                           '/var/lib/mysql')
        dbaas.MySqlApp.install_if_needed.assert_any_call(None)
        # We dont need to make sure the exact contents are there
        dbaas.MySqlApp.secure.assert_any_call(None, None)
        self.assertFalse(dbaas.MySqlAdmin.create_database.called)
        self.assertFalse(dbaas.MySqlAdmin.create_user.called)
        dbaas.MySqlApp.secure_root.assert_any_call(
            secure_remote_root=not is_root_enabled)

########NEW FILE########
__FILENAME__ = test_pkg
# Copyright (c) 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import testtools
from mock import Mock, MagicMock
import pexpect
from trove.common import utils
from trove.guestagent import pkg
import commands
import re

"""
Unit tests for the classes and functions in pkg.py.
"""


class PkgDEBInstallTestCase(testtools.TestCase):

    def setUp(self):
        super(PkgDEBInstallTestCase, self).setUp()
        self.utils_execute = utils.execute
        self.pexpect_spawn_init = pexpect.spawn.__init__
        self.pexpect_spawn_closed = pexpect.spawn.close
        self.pkg = pkg.DebianPackagerMixin()
        self.pkg_fix = self.pkg._fix
        self.pkg_fix_package_selections = self.pkg._fix_package_selections
        utils.execute = Mock()
        pexpect.spawn.__init__ = Mock(return_value=None)
        pexpect.spawn.closed = Mock(return_value=None)
        self.pkg._fix = Mock(return_value=None)
        self.pkg._fix_package_selections = Mock(return_value=None)
        self.pkgName = 'packageName'

    def tearDown(self):
        super(PkgDEBInstallTestCase, self).tearDown()
        utils.execute = self.utils_execute
        pexpect.spawn.__init__ = self.pexpect_spawn_init
        pexpect.spawn.close = self.pexpect_spawn_closed
        self.pkg._fix = self.pkg_fix
        self.pkg._fix_package_selections = self.pkg_fix_package_selections

    def test_pkg_is_instaled_no_packages(self):
        packages = ""
        self.assertTrue(self.pkg.pkg_is_installed(packages))

    def test_pkg_is_instaled_yes(self):
        packages = "package1=1.0 package2"
        self.pkg.pkg_version = MagicMock(side_effect=["1.0", "2.0"])
        self.assertTrue(self.pkg.pkg_is_installed(packages))

    def test_pkg_is_instaled_no(self):
        packages = "package1=1.0 package2 package3=3.1"
        self.pkg.pkg_version = MagicMock(side_effect=["1.0", "2.0", "3.0"])
        self.assertFalse(self.pkg.pkg_is_installed(packages))

    def test_success_install(self):
        # test
        pexpect.spawn.expect = Mock(return_value=7)
        pexpect.spawn.match = False
        self.assertTrue(self.pkg.pkg_install(self.pkgName, {}, 5000) is None)

    def test_permission_error(self):
        # test
        pexpect.spawn.expect = Mock(return_value=0)
        pexpect.spawn.match = False
        # test and verify
        self.assertRaises(pkg.PkgPermissionError, self.pkg.pkg_install,
                          self.pkgName, {}, 5000)

    def test_package_not_found_1(self):
        # test
        pexpect.spawn.expect = Mock(return_value=1)
        pexpect.spawn.match = re.match('(.*)', self.pkgName)
        # test and verify
        self.assertRaises(pkg.PkgNotFoundError, self.pkg.pkg_install,
                          self.pkgName, {}, 5000)

    def test_package_not_found_2(self):
        # test
        pexpect.spawn.expect = Mock(return_value=2)
        pexpect.spawn.match = re.match('(.*)', self.pkgName)
        # test and verify
        self.assertRaises(pkg.PkgNotFoundError, self.pkg.pkg_install,
                          self.pkgName, {}, 5000)

    def test_run_DPKG_bad_State(self):
        # test _fix method is called and PackageStateError is thrown
        pexpect.spawn.expect = Mock(return_value=4)
        pexpect.spawn.match = False
        # test and verify
        self.assertRaises(pkg.PkgPackageStateError, self.pkg.pkg_install,
                          self.pkgName, {}, 5000)
        self.assertTrue(self.pkg._fix.called)

    def test_admin_lock_error(self):
        # test 'Unable to lock the administration directory' error
        pexpect.spawn.expect = Mock(return_value=5)
        pexpect.spawn.match = False
        # test and verify
        self.assertRaises(pkg.PkgAdminLockError, self.pkg.pkg_install,
                          self.pkgName, {}, 5000)

    def test_package_broken_error(self):
        pexpect.spawn.expect = Mock(return_value=6)
        pexpect.spawn.match = False
        # test and verify
        self.assertRaises(pkg.PkgBrokenError, self.pkg.pkg_install,
                          self.pkgName, {}, 5000)

    def test_timeout_error(self):
        # test timeout error
        pexpect.spawn.expect = Mock(side_effect=pexpect.
                                    TIMEOUT('timeout error'))
        # test and verify
        self.assertRaises(pkg.PkgTimeout, self.pkg.pkg_install,
                          self.pkgName, {}, 5000)


class PkgDEBRemoveTestCase(testtools.TestCase):

    def setUp(self):
        super(PkgDEBRemoveTestCase, self).setUp()
        self.utils_execute = utils.execute
        self.pexpect_spawn_init = pexpect.spawn.__init__
        self.pexpect_spawn_closed = pexpect.spawn.close
        self.pkg = pkg.DebianPackagerMixin()
        self.pkg_version = self.pkg.pkg_version
        self.pkg_install = self.pkg._install
        self.pkg_fix = self.pkg._fix

        utils.execute = Mock()
        pexpect.spawn.__init__ = Mock(return_value=None)
        pexpect.spawn.closed = Mock(return_value=None)
        self.pkg.pkg_version = Mock(return_value="OK")
        self.pkg._install = Mock(return_value=None)
        self.pkg._fix = Mock(return_value=None)

        self.pkgName = 'packageName'

    def tearDown(self):
        super(PkgDEBRemoveTestCase, self).tearDown()
        utils.execute = self.utils_execute
        pexpect.spawn.__init__ = self.pexpect_spawn_init
        pexpect.spawn.close = self.pexpect_spawn_closed
        self.pkg.pkg_version = self.pkg_version
        self.pkg._install = self.pkg_install
        self.pkg._fix = self.pkg_fix

    def test_success_remove(self):
        # test
        pexpect.spawn.expect = Mock(return_value=6)
        pexpect.spawn.match = False
        self.assertTrue(self.pkg.pkg_remove(self.pkgName, 5000) is None)

    def test_permission_error(self):
        # test
        pexpect.spawn.expect = Mock(return_value=0)
        pexpect.spawn.match = False
        # test and verify
        self.assertRaises(pkg.PkgPermissionError, self.pkg.pkg_remove,
                          self.pkgName, 5000)

    def test_package_not_found(self):
        # test
        pexpect.spawn.expect = Mock(return_value=1)
        pexpect.spawn.match = False
        # test and verify
        self.assertRaises(pkg.PkgNotFoundError, self.pkg.pkg_remove,
                          self.pkgName, 5000)

    def test_package_reinstall_first_1(self):
        # test
        pexpect.spawn.expect = Mock(return_value=2)
        pexpect.spawn.match = False
        # test and verify
        self.assertRaises(pkg.PkgPackageStateError, self.pkg.pkg_remove,
                          self.pkgName, 5000)
        self.assertTrue(self.pkg._install.called)
        self.assertFalse(self.pkg._fix.called)

    def test_package_reinstall_first_2(self):
        # test
        pexpect.spawn.expect = Mock(return_value=3)
        pexpect.spawn.match = False
        # test and verify
        self.assertRaises(pkg.PkgPackageStateError, self.pkg.pkg_remove,
                          self.pkgName, 5000)
        self.assertTrue(self.pkg._install.called)
        self.assertFalse(self.pkg._fix.called)

    def test_package_DPKG_first(self):
        # test
        pexpect.spawn.expect = Mock(return_value=4)
        pexpect.spawn.match = False
        # test and verify
        self.assertRaises(pkg.PkgPackageStateError, self.pkg.pkg_remove,
                          self.pkgName, 5000)
        self.assertFalse(self.pkg._install.called)
        self.assertTrue(self.pkg._fix.called)

    def test_admin_lock_error(self):
        # test 'Unable to lock the administration directory' error
        pexpect.spawn.expect = Mock(return_value=5)
        pexpect.spawn.match = False
        # test and verify
        self.assertRaises(pkg.PkgAdminLockError, self.pkg.pkg_remove,
                          self.pkgName, 5000)

    def test_timeout_error(self):
        # test timeout error
        pexpect.spawn.expect = Mock(side_effect=pexpect.
                                    TIMEOUT('timeout error'))
        # test and verify
        self.assertRaises(pkg.PkgTimeout, self.pkg.pkg_remove,
                          self.pkgName, 5000)


class PkgDEBVersionTestCase(testtools.TestCase):

    def setUp(self):
        super(PkgDEBVersionTestCase, self).setUp()
        self.pkgName = 'mysql-server-5.5'
        self.pkgVersion = '5.5.28-0'
        self.commands_output = commands.getstatusoutput

    def tearDown(self):
        super(PkgDEBVersionTestCase, self).tearDown()
        commands.getstatusoutput = self.commands_output

    def test_version_success(self):
        cmd_out = "%s:\n  Installed: %s\n" % (self.pkgName, self.pkgVersion)
        commands.getstatusoutput = Mock(return_value=(0, cmd_out))
        version = pkg.DebianPackagerMixin().pkg_version(self.pkgName)
        self.assertTrue(version)
        self.assertEqual(self.pkgVersion, version)

    def test_version_unknown_package(self):
        cmd_out = "N: Unable to locate package %s" % self.pkgName
        commands.getstatusoutput = Mock(return_value=(0, cmd_out))
        self.assertFalse(pkg.DebianPackagerMixin().pkg_version(self.pkgName))

    def test_version_no_version(self):
        cmd_out = "%s:\n  Installed: %s\n" % (self.pkgName, "(none)")
        commands.getstatusoutput = Mock(return_value=(0, cmd_out))
        self.assertFalse(pkg.DebianPackagerMixin().pkg_version(self.pkgName))


class PkgRPMVersionTestCase(testtools.TestCase):

    def setUp(self):
        super(PkgRPMVersionTestCase, self).setUp()
        self.pkgName = 'python-requests'
        self.pkgVersion = '0.14.2-1.el6'
        self.commands_output = commands.getstatusoutput

    def tearDown(self):
        super(PkgRPMVersionTestCase, self).tearDown()
        commands.getstatusoutput = self.commands_output

    def test_version_no_output(self):
        cmd_out = ''
        commands.getstatusoutput = Mock(return_value=(0, cmd_out))
        self.assertIsNone(pkg.RedhatPackagerMixin().pkg_version(self.pkgName))

    def test_version_success(self):
        cmd_out = self.pkgVersion
        commands.getstatusoutput = Mock(return_value=(0, cmd_out))
        version = pkg.RedhatPackagerMixin().pkg_version(self.pkgName)
        self.assertTrue(version)
        self.assertEqual(self.pkgVersion, version)


class PkgRPMInstallTestCase(testtools.TestCase):

    def setUp(self):
        super(PkgRPMInstallTestCase, self).setUp()
        self.utils_execute = utils.execute
        self.pexpect_spawn_init = pexpect.spawn.__init__
        self.pexpect_spawn_closed = pexpect.spawn.close
        self.pkg = pkg.RedhatPackagerMixin()
        utils.execute = Mock()
        pexpect.spawn.__init__ = Mock(return_value=None)
        pexpect.spawn.closed = Mock(return_value=None)
        self.pkgName = 'packageName'

    def tearDown(self):
        super(PkgRPMInstallTestCase, self).tearDown()
        utils.execute = self.utils_execute
        pexpect.spawn.__init__ = self.pexpect_spawn_init
        pexpect.spawn.close = self.pexpect_spawn_closed

    def test_pkg_is_instaled_no_packages(self):
        packages = ""
        self.assertTrue(self.pkg.pkg_is_installed(packages))

    def test_pkg_is_instaled_yes(self):
        packages = "package1=1.0 package2"
        commands.getstatusoutput = MagicMock(return_value={1: "package1=1.0\n"
                                                           "package2=2.0"})
        self.assertTrue(self.pkg.pkg_is_installed(packages))

    def test_pkg_is_instaled_no(self):
        packages = "package1=1.0 package2 package3=3.0"
        commands.getstatusoutput = MagicMock(return_value={1: "package1=1.0\n"
                                                           "package2=2.0"})
        self.assertFalse(self.pkg.pkg_is_installed(packages))

    def test_permission_error(self):
        # test
        pexpect.spawn.expect = Mock(return_value=0)
        pexpect.spawn.match = False
        # test and verify
        self.assertRaises(pkg.PkgPermissionError, self.pkg.pkg_install,
                          self.pkgName, {}, 5000)

    def test_package_not_found(self):
        # test
        pexpect.spawn.expect = Mock(return_value=1)
        pexpect.spawn.match = re.match('(.*)', self.pkgName)
        # test and verify
        self.assertRaises(pkg.PkgNotFoundError, self.pkg.pkg_install,
                          self.pkgName, {}, 5000)

    def test_package_conflict_remove(self):
        # test
        pexpect.spawn.expect = Mock(return_value=2)
        pexpect.spawn.match = re.match('(.*)', self.pkgName)
        self.pkg._rpm_remove_nodeps = Mock()
        # test and verify
        self.pkg._install(self.pkgName, 5000)
        self.assertTrue(self.pkg._rpm_remove_nodeps.called)

    def test_package_scriptlet_error(self):
        # test
        pexpect.spawn.expect = Mock(return_value=5)
        pexpect.spawn.match = False
        # test and verify
        self.assertRaises(pkg.PkgScriptletError, self.pkg.pkg_install,
                          self.pkgName, {}, 5000)

    def test_package_http_error(self):
        # test
        pexpect.spawn.expect = Mock(return_value=6)
        pexpect.spawn.match = False
        # test and verify
        self.assertRaises(pkg.PkgDownloadError, self.pkg.pkg_install,
                          self.pkgName, {}, 5000)

    def test_package_nomirrors_error(self):
        # test
        pexpect.spawn.expect = Mock(return_value=7)
        pexpect.spawn.match = False
        # test and verify
        self.assertRaises(pkg.PkgDownloadError, self.pkg.pkg_install,
                          self.pkgName, {}, 5000)

    def test_package_sign_error(self):
        # test
        pexpect.spawn.expect = Mock(return_value=8)
        pexpect.spawn.match = False
        # test and verify
        self.assertRaises(pkg.PkgSignError, self.pkg.pkg_install,
                          self.pkgName, {}, 5000)

    def test_package_already_installed(self):
        # test
        pexpect.spawn.expect = Mock(return_value=9)
        pexpect.spawn.match = False
        # test and verify
        self.assertTrue(self.pkg.pkg_install(self.pkgName, {}, 5000) is None)

    def test_package_success_updated(self):
        # test
        pexpect.spawn.expect = Mock(return_value=10)
        pexpect.spawn.match = False
        # test and verify
        self.assertTrue(self.pkg.pkg_install(self.pkgName, {}, 5000) is None)

    def test_package_success_installed(self):
        # test
        pexpect.spawn.expect = Mock(return_value=11)
        pexpect.spawn.match = False
        # test and verify
        self.assertTrue(self.pkg.pkg_install(self.pkgName, {}, 5000) is None)

    def test_timeout_error(self):
        # test timeout error
        pexpect.spawn.expect = Mock(side_effect=pexpect.
                                    TIMEOUT('timeout error'))
        pexpect.spawn.match = False
        # test and verify
        self.assertRaises(pkg.PkgTimeout, self.pkg.pkg_install,
                          self.pkgName, {}, 5000)


class PkgRPMRemoveTestCase(testtools.TestCase):

    def setUp(self):
        super(PkgRPMRemoveTestCase, self).setUp()
        self.utils_execute = utils.execute
        self.pexpect_spawn_init = pexpect.spawn.__init__
        self.pexpect_spawn_closed = pexpect.spawn.close
        self.pkg = pkg.RedhatPackagerMixin()
        self.pkg_version = self.pkg.pkg_version
        self.pkg_install = self.pkg._install
        utils.execute = Mock()
        pexpect.spawn.__init__ = Mock(return_value=None)
        pexpect.spawn.closed = Mock(return_value=None)
        self.pkg.pkg_version = Mock(return_value="OK")
        self.pkg._install = Mock(return_value=None)
        self.pkgName = 'packageName'

    def tearDown(self):
        super(PkgRPMRemoveTestCase, self).tearDown()
        utils.execute = self.utils_execute
        pexpect.spawn.__init__ = self.pexpect_spawn_init
        pexpect.spawn.close = self.pexpect_spawn_closed
        self.pkg.pkg_version = self.pkg_version
        self.pkg._install = self.pkg_install

    def test_permission_error(self):
        # test
        pexpect.spawn.expect = Mock(return_value=0)
        # test and verify
        self.assertRaises(pkg.PkgPermissionError, self.pkg.pkg_remove,
                          self.pkgName, 5000)

    def test_package_not_found(self):
        # test
        pexpect.spawn.expect = Mock(return_value=1)
        # test and verify
        self.assertRaises(pkg.PkgNotFoundError, self.pkg.pkg_remove,
                          self.pkgName, 5000)

    def test_timeout_error(self):
        # test timeout error
        pexpect.spawn.expect = Mock(side_effect=pexpect.
                                    TIMEOUT('timeout error'))
        # test and verify
        self.assertRaises(pkg.PkgTimeout, self.pkg.pkg_remove,
                          self.pkgName, 5000)

########NEW FILE########
__FILENAME__ = test_query
#    Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import testtools
from trove.guestagent.common import sql_query


class QueryTestBase(testtools.TestCase):
    def setUp(self):
        super(QueryTestBase, self).setUp()

    def tearDown(self):
        super(QueryTestBase, self).tearDown()


class QueryTest(QueryTestBase):
    def setUp(self):
        super(QueryTest, self).setUp()

    def tearDown(self):
        super(QueryTest, self).tearDown()

    def test_columns(self):
        myQuery = sql_query.Query(columns=None)
        self.assertEqual("SELECT *", myQuery._columns)

    def test_columns_2(self):
        columns = ["col_A", "col_B"]
        myQuery = sql_query.Query(columns=columns)
        self.assertEqual("SELECT col_A, col_B", myQuery._columns)

    def test_tables(self):
        tables = ['table_A', 'table_B']
        myQuery = sql_query.Query(tables=tables)
        self.assertEqual("FROM table_A, table_B", myQuery._tables)

    def test_where(self):
        myQuery = sql_query.Query(where=None)
        self.assertEqual("", myQuery._where)

    def test_where_2(self):
        conditions = ['cond_A', 'cond_B']
        myQuery = sql_query.Query(where=conditions)
        self.assertEqual("WHERE cond_A AND cond_B", myQuery._where)

    def test_order(self):
        myQuery = sql_query.Query(order=None)
        self.assertEqual('', myQuery._order)

    def test_order_2(self):
        orders = ['deleted_at', 'updated_at']
        myQuery = sql_query.Query(order=orders)
        self.assertEqual('ORDER BY deleted_at, updated_at', myQuery._order)

    def test_group_by(self):
        myQuery = sql_query.Query(group=None)
        self.assertEqual('', myQuery._group_by)

    def test_group_by_2(self):
        groups = ['deleted=1']
        myQuery = sql_query.Query(group=groups)
        self.assertEqual('GROUP BY deleted=1', myQuery._group_by)

    def test_limit(self):
        myQuery = sql_query.Query(limit=None)
        self.assertEqual('', myQuery._limit)

    def test_limit_2(self):
        limit_count = 20
        myQuery = sql_query.Query(limit=limit_count)
        self.assertEqual('LIMIT 20', myQuery._limit)


class GrantTest(QueryTestBase):
    def setUp(self):
        super(GrantTest, self).setUp()

    def tearDown(self):
        super(GrantTest, self).tearDown()

    def test_grant_no_arg_constr(self):
        grant = sql_query.Grant()
        self.assertIsNotNone(grant)
        self.assertEqual("GRANT USAGE ON *.* "
                         "TO ``@`%`;",
                         str(grant))

    def test_grant_all_with_grant_option(self):
        permissions = ['ALL']
        user_name = 'root'
        user_password = 'password123'
        host = 'localhost'

        # grant_option defaults to True
        grant = sql_query.Grant(permissions=permissions,
                                user=user_name,
                                host=host,
                                clear=user_password,
                                grant_option=True)

        self.assertEqual("GRANT ALL PRIVILEGES ON *.* TO "
                         "`root`@`localhost` "
                         "IDENTIFIED BY 'password123' "
                         "WITH GRANT OPTION;",
                         str(grant))

    def test_grant_all_with_explicit_grant_option(self):
        permissions = ['ALL', 'GRANT OPTION']
        user_name = 'root'
        user_password = 'password123'
        host = 'localhost'
        grant = sql_query.Grant(permissions=permissions,
                                user=user_name,
                                host=host,
                                clear=user_password,
                                grant_option=True)

        self.assertEqual("GRANT ALL PRIVILEGES ON *.* TO "
                         "`root`@`localhost` "
                         "IDENTIFIED BY 'password123' "
                         "WITH GRANT OPTION;",
                         str(grant))

    def test_grant_specify_permissions(self):
        permissions = ['ALTER ROUTINE',
                       'CREATE',
                       'ALTER',
                       'CREATE ROUTINE',
                       'CREATE TEMPORARY TABLES',
                       'CREATE VIEW',
                       'CREATE USER',
                       'DELETE',
                       'DROP',
                       'EVENT',
                       'EXECUTE',
                       'INDEX',
                       'INSERT',
                       'LOCK TABLES',
                       'PROCESS',
                       'REFERENCES',
                       'SELECT',
                       'SHOW DATABASES',
                       'SHOW VIEW',
                       'TRIGGER',
                       'UPDATE',
                       'USAGE']

        user_name = 'root'
        user_password = 'password123'
        host = 'localhost'
        grant = sql_query.Grant(permissions=permissions,
                                user=user_name,
                                host=host,
                                clear=user_password)

        self.assertEqual("GRANT ALTER, "
                         "ALTER ROUTINE, "
                         "CREATE, "
                         "CREATE ROUTINE, "
                         "CREATE TEMPORARY TABLES, "
                         "CREATE USER, "
                         "CREATE VIEW, "
                         "DELETE, "
                         "DROP, "
                         "EVENT, "
                         "EXECUTE, "
                         "INDEX, "
                         "INSERT, "
                         "LOCK TABLES, "
                         "PROCESS, "
                         "REFERENCES, "
                         "SELECT, "
                         "SHOW DATABASES, "
                         "SHOW VIEW, "
                         "TRIGGER, "
                         "UPDATE, "
                         "USAGE ON *.* TO "
                         "`root`@`localhost` "
                         "IDENTIFIED BY "
                         "'password123';",
                         str(grant))

    def test_grant_specify_duplicate_permissions(self):
        permissions = ['ALTER ROUTINE',
                       'CREATE',
                       'CREATE',
                       'DROP',
                       'DELETE',
                       'DELETE',
                       'ALTER',
                       'CREATE ROUTINE',
                       'CREATE TEMPORARY TABLES',
                       'CREATE VIEW',
                       'CREATE USER',
                       'DELETE',
                       'DROP',
                       'EVENT',
                       'EXECUTE',
                       'INDEX',
                       'INSERT',
                       'LOCK TABLES',
                       'PROCESS',
                       'REFERENCES',
                       'SELECT',
                       'SHOW DATABASES',
                       'SHOW VIEW',
                       'TRIGGER',
                       'UPDATE',
                       'USAGE']

        user_name = 'root'
        user_password = 'password123'
        host = 'localhost'
        grant = sql_query.Grant(permissions=permissions,
                                user=user_name,
                                host=host,
                                clear=user_password)

        self.assertEqual("GRANT ALTER, "
                         "ALTER ROUTINE, "
                         "CREATE, "
                         "CREATE ROUTINE, "
                         "CREATE TEMPORARY TABLES, "
                         "CREATE USER, "
                         "CREATE VIEW, "
                         "DELETE, "
                         "DROP, "
                         "EVENT, "
                         "EXECUTE, "
                         "INDEX, "
                         "INSERT, "
                         "LOCK TABLES, "
                         "PROCESS, "
                         "REFERENCES, "
                         "SELECT, "
                         "SHOW DATABASES, "
                         "SHOW VIEW, "
                         "TRIGGER, "
                         "UPDATE, "
                         "USAGE ON *.* TO "
                         "`root`@`localhost` "
                         "IDENTIFIED BY "
                         "'password123';",
                         str(grant))


class RevokeTest(QueryTestBase):
    def setUp(self):
        super(RevokeTest, self).setUp()

    def tearDown(self):
        super(RevokeTest, self).tearDown()

    def test_defaults(self):
        r = sql_query.Revoke()
        # Technically, this isn't valid for MySQL.
        self.assertEqual(str(r), "REVOKE ALL ON *.* FROM ``@`%`;")

    def test_permissions(self):
        r = sql_query.Revoke()
        r.user = 'x'
        r.permissions = ['CREATE', 'DELETE', 'DROP']
        self.assertEqual(str(r),
                         "REVOKE CREATE, DELETE, DROP ON *.* FROM `x`@`%`;")

    def test_database(self):
        r = sql_query.Revoke()
        r.user = 'x'
        r.database = 'foo'
        self.assertEqual(str(r), "REVOKE ALL ON `foo`.* FROM `x`@`%`;")

    def test_table(self):
        r = sql_query.Revoke()
        r.user = 'x'
        r.database = 'foo'
        r.table = 'bar'
        self.assertEqual(str(r), "REVOKE ALL ON `foo`.'bar' FROM `x`@`%`;")

    def test_user(self):
        r = sql_query.Revoke()
        r.user = 'x'
        self.assertEqual(str(r), "REVOKE ALL ON *.* FROM `x`@`%`;")

    def test_user_host(self):
        r = sql_query.Revoke()
        r.user = 'x'
        r.host = 'y'
        self.assertEqual(str(r), "REVOKE ALL ON *.* FROM `x`@`y`;")


class CreateDatabaseTest(QueryTestBase):
    def setUp(self):
        super(CreateDatabaseTest, self).setUp()

    def tearDown(self):
        super(CreateDatabaseTest, self).tearDown()

    def test_defaults(self):
        cd = sql_query.CreateDatabase('foo')
        self.assertEqual(str(cd), "CREATE DATABASE IF NOT EXISTS `foo`;")

    def test_charset(self):
        cd = sql_query.CreateDatabase('foo')
        cd.charset = "foo"
        self.assertEqual(str(cd), ("CREATE DATABASE IF NOT EXISTS `foo` "
                                   "CHARACTER SET = 'foo';"))

    def test_collate(self):
        cd = sql_query.CreateDatabase('foo')
        cd.collate = "bar"
        self.assertEqual(str(cd), ("CREATE DATABASE IF NOT EXISTS `foo` "
                                   "COLLATE = 'bar';"))


class DropDatabaseTest(QueryTestBase):
    def setUp(self):
        super(DropDatabaseTest, self).setUp()

    def tearDown(self):
        super(DropDatabaseTest, self).tearDown()

    def test_defaults(self):
        dd = sql_query.DropDatabase('foo')
        self.assertEqual(str(dd), "DROP DATABASE `foo`;")


class CreateUserTest(QueryTestBase):
    def setUp(self):
        super(CreateUserTest, self).setUp()

    def tearDown(self):
        super(CreateUserTest, self).tearDown()

    def test_defaults(self):
        username = 'root'
        hostname = 'localhost'
        password = 'password123'
        cu = sql_query.CreateUser(user=username, host=hostname, clear=password)
        self.assertEqual(str(cu), "CREATE USER :user@:host "
                                  "IDENTIFIED BY 'password123';")


class UpdateUserTest(QueryTestBase):
    def setUp(self):
        super(UpdateUserTest, self).setUp()

    def tearDown(self):
        super(UpdateUserTest, self).tearDown()

    def test_rename_user(self):
        username = 'root'
        hostname = 'localhost'
        new_user = 'root123'
        uu = sql_query.UpdateUser(user=username, host=hostname,
                                  new_user=new_user)
        self.assertEqual(str(uu), "UPDATE mysql.user SET User='root123' "
                                  "WHERE User = 'root' "
                                  "AND Host = 'localhost';")

    def test_change_password(self):
        username = 'root'
        hostname = 'localhost'
        new_password = 'password123'
        uu = sql_query.UpdateUser(user=username, host=hostname,
                                  clear=new_password)
        self.assertEqual(str(uu), "UPDATE mysql.user SET "
                                  "Password=PASSWORD('password123') "
                                  "WHERE User = 'root' "
                                  "AND Host = 'localhost';")

    def test_change_host(self):
        username = 'root'
        hostname = 'localhost'
        new_host = '%'
        uu = sql_query.UpdateUser(user=username, host=hostname,
                                  new_host=new_host)
        self.assertEqual(str(uu), "UPDATE mysql.user SET Host='%' "
                                  "WHERE User = 'root' "
                                  "AND Host = 'localhost';")

    def test_change_password_and_username(self):
        username = 'root'
        hostname = 'localhost'
        new_user = 'root123'
        new_password = 'password123'
        uu = sql_query.UpdateUser(user=username, host=hostname,
                                  clear=new_password, new_user=new_user)
        self.assertEqual(str(uu), "UPDATE mysql.user SET User='root123', "
                                  "Password=PASSWORD('password123') "
                                  "WHERE User = 'root' "
                                  "AND Host = 'localhost';")

    def test_change_username_password_hostname(self):
        username = 'root'
        hostname = 'localhost'
        new_user = 'root123'
        new_password = 'password123'
        new_host = '%'
        uu = sql_query.UpdateUser(user=username, host=hostname,
                                  clear=new_password, new_user=new_user,
                                  new_host=new_host)
        self.assertEqual(str(uu), "UPDATE mysql.user SET User='root123', "
                                  "Host='%', "
                                  "Password=PASSWORD('password123') "
                                  "WHERE User = 'root' "
                                  "AND Host = 'localhost';")

    def test_change_username_and_hostname(self):
        username = 'root'
        hostname = 'localhost'
        new_user = 'root123'
        new_host = '%'
        uu = sql_query.UpdateUser(user=username, host=hostname,
                                  new_host=new_host, new_user=new_user)
        self.assertEqual(str(uu), "UPDATE mysql.user SET User='root123', "
                                  "Host='%' "
                                  "WHERE User = 'root' "
                                  "AND Host = 'localhost';")


class DropUserTest(QueryTestBase):
    def setUp(self):
        super(DropUserTest, self).setUp()

    def tearDown(self):
        super(DropUserTest, self).tearDown()

    def test_defaults(self):
        username = 'root'
        hostname = 'localhost'
        du = sql_query.DropUser(user=username, host=hostname)
        self.assertEqual(str(du), "DROP USER `root`@`localhost`;")

########NEW FILE########
__FILENAME__ = test_redis_manager
#    Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import testtools
from mock import MagicMock
from trove.common.context import TroveContext
from trove.guestagent.datastore.redis.manager import Manager as RedisManager
import trove.guestagent.datastore.redis.service as redis_service
from trove.guestagent import backup
from trove.guestagent.common import operating_system
from trove.guestagent.volume import VolumeDevice


class RedisGuestAgentManagerTest(testtools.TestCase):

    def setUp(self):
        super(RedisGuestAgentManagerTest, self).setUp()
        self.context = TroveContext()
        self.manager = RedisManager()
        self.packages = 'redis-server'
        self.origin_RedisAppStatus = redis_service.RedisAppStatus
        self.origin_start_redis = redis_service.RedisApp.start_redis
        self.origin_stop_redis = redis_service.RedisApp.stop_db
        self.origin_install_redis = redis_service.RedisApp._install_redis
        self.origin_write_config = redis_service.RedisApp.write_config
        self.origin_install_if_needed = \
            redis_service.RedisApp.install_if_needed
        self.origin_complete_install_or_restart = \
            redis_service.RedisApp.complete_install_or_restart
        self.origin_format = VolumeDevice.format
        self.origin_mount = VolumeDevice.mount
        self.origin_mount_points = VolumeDevice.mount_points
        self.origin_restore = backup.restore

    def tearDown(self):
        super(RedisGuestAgentManagerTest, self).tearDown()
        redis_service.RedisAppStatus = self.origin_RedisAppStatus
        redis_service.RedisApp.stop_db = self.origin_stop_redis
        redis_service.RedisApp.start_redis = self.origin_start_redis
        redis_service.RedisApp._install_redis = self.origin_install_redis
        redis_service.RedisApp.write_config = self.origin_write_config
        redis_service.RedisApp.install_if_needed = \
            self.origin_install_if_needed
        redis_service.RedisApp.complete_install_or_restart = \
            self.origin_complete_install_or_restart
        VolumeDevice.format = self.origin_format
        VolumeDevice.mount = self.origin_mount
        VolumeDevice.mount_points = self.origin_mount_points
        backup.restore = self.origin_restore

    def test_update_status(self):
        mock_status = MagicMock()
        self.manager.appStatus = mock_status
        redis_service.RedisAppStatus.get = MagicMock(return_value=mock_status)
        self.manager.update_status(self.context)
        redis_service.RedisAppStatus.get.assert_any_call()
        mock_status.update.assert_any_call()

    def test_prepare_redis_not_installed(self):
        self._prepare_dynamic(is_redis_installed=False)

    def _prepare_dynamic(self, device_path='/dev/vdb', is_redis_installed=True,
                         backup_info=None, is_root_enabled=False,
                         mount_point='var/lib/redis'):

        # covering all outcomes is starting to cause trouble here
        mock_status = MagicMock()
        redis_service.RedisAppStatus.get = MagicMock(return_value=mock_status)
        redis_service.RedisApp.start_redis = MagicMock(return_value=None)
        redis_service.RedisApp.install_if_needed = MagicMock(return_value=None)
        redis_service.RedisApp.write_config = MagicMock(return_value=None)
        operating_system.update_owner = MagicMock(return_value=None)
        redis_service.RedisApp.restart = MagicMock(return_value=None)
        mock_status.begin_install = MagicMock(return_value=None)
        VolumeDevice.format = MagicMock(return_value=None)
        VolumeDevice.mount = MagicMock(return_value=None)
        VolumeDevice.mount_points = MagicMock(return_value=[])
        backup.restore = MagicMock(return_value=None)

        self.manager.prepare(self.context, self.packages,
                             None, '2048',
                             None, device_path=device_path,
                             mount_point=mount_point,
                             backup_info=backup_info)

        self.assertEqual(redis_service.RedisAppStatus.get.call_count, 2)
        mock_status.begin_install.assert_any_call()
        VolumeDevice.format.assert_any_call()
        redis_service.RedisApp.install_if_needed.assert_any_call(self.packages)
        redis_service.RedisApp.write_config.assert_any_call(None)
        operating_system.update_owner.assert_any_call(
            'redis', 'redis', mount_point)
        redis_service.RedisApp.restart.assert_any_call()

    def test_restart(self):
        mock_status = MagicMock()
        self.manager.appStatus = mock_status
        redis_service.RedisAppStatus.get = MagicMock(return_value=mock_status)
        redis_service.RedisApp.restart = MagicMock(return_value=None)
        self.manager.restart(self.context)
        redis_service.RedisAppStatus.get.assert_any_call()
        redis_service.RedisApp.restart.assert_any_call()

    def test_stop_db(self):
        mock_status = MagicMock()
        self.manager.appStatus = mock_status
        redis_service.RedisAppStatus.get = MagicMock(return_value=mock_status)
        redis_service.RedisApp.stop_db = MagicMock(return_value=None)
        self.manager.stop_db(self.context)
        redis_service.RedisAppStatus.get.assert_any_call()
        redis_service.RedisApp.stop_db.assert_any_call(
            do_not_start_on_reboot=False)

########NEW FILE########
__FILENAME__ = test_service
#    Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import testtools
from mock import Mock, MagicMock
from trove.guestagent import service


class ServiceTest(testtools.TestCase):
    def setUp(self):
        super(ServiceTest, self).setUp()

    def tearDown(self):
        super(ServiceTest, self).tearDown()

    def test_app_factory(self):
        service.API._instance_router = MagicMock()
        service.app_factory(Mock)
        self.assertEqual(1, service.API._instance_router.call_count)

########NEW FILE########
__FILENAME__ = test_volume
#    Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
import os
import testtools
import pexpect
from mock import Mock, MagicMock, patch, mock_open
from trove.guestagent import volume
from trove.common import utils


def _setUp_fake_spawn(return_val=0):
    fake_spawn = pexpect.spawn('echo')
    fake_spawn.expect = Mock(return_value=return_val)
    pexpect.spawn = Mock(return_value=fake_spawn)
    return fake_spawn


class VolumeDeviceTest(testtools.TestCase):

    def setUp(self):
        super(VolumeDeviceTest, self).setUp()
        self.volumeDevice = volume.VolumeDevice('/dev/vdb')

    def tearDown(self):
        super(VolumeDeviceTest, self).tearDown()

    def test_migrate_data(self):
        origin_execute = utils.execute
        utils.execute = Mock()
        origin_os_path_exists = os.path.exists
        os.path.exists = Mock()
        fake_spawn = _setUp_fake_spawn()

        origin_unmount = self.volumeDevice.unmount
        self.volumeDevice.unmount = MagicMock()
        self.volumeDevice.migrate_data('/')
        self.assertEqual(1, fake_spawn.expect.call_count)
        self.assertEqual(1, utils.execute.call_count)
        self.assertEqual(1, self.volumeDevice.unmount.call_count)
        utils.execute = origin_execute
        self.volumeDevice.unmount = origin_unmount
        os.path.exists = origin_os_path_exists

    def test__check_device_exists(self):
        origin_execute = utils.execute
        utils.execute = Mock()
        self.volumeDevice._check_device_exists()
        self.assertEqual(1, utils.execute.call_count)
        utils.execute = origin_execute

    def test__check_format(self):
        fake_spawn = _setUp_fake_spawn()

        self.volumeDevice._check_format()
        self.assertEqual(1, fake_spawn.expect.call_count)

    def test__check_format_2(self):
        fake_spawn = _setUp_fake_spawn(return_val=1)

        self.assertEqual(0, fake_spawn.expect.call_count)
        self.assertRaises(IOError, self.volumeDevice._check_format)

    def test__format(self):
        fake_spawn = _setUp_fake_spawn()

        self.volumeDevice._format()

        self.assertEqual(1, fake_spawn.expect.call_count)
        self.assertEqual(1, pexpect.spawn.call_count)

    def test_format(self):
        origin_check_device_exists = self.volumeDevice._check_device_exists
        origin_format = self.volumeDevice._format
        origin_check_format = self.volumeDevice._check_format
        self.volumeDevice._check_device_exists = MagicMock()
        self.volumeDevice._check_format = MagicMock()
        self.volumeDevice._format = MagicMock()

        self.volumeDevice.format()
        self.assertEqual(1, self.volumeDevice._check_device_exists.call_count)
        self.assertEqual(1, self.volumeDevice._format.call_count)
        self.assertEqual(1, self.volumeDevice._check_format.call_count)

        self.volumeDevice._check_device_exists = origin_check_device_exists
        self.volumeDevice._format = origin_format
        self.volumeDevice._check_format = origin_check_format

    def test_mount(self):
        origin_ = volume.VolumeMountPoint.mount
        volume.VolumeMountPoint.mount = Mock()
        origin_os_path_exists = os.path.exists
        os.path.exists = Mock()
        origin_write_to_fstab = volume.VolumeMountPoint.write_to_fstab
        volume.VolumeMountPoint.write_to_fstab = Mock()

        self.volumeDevice.mount(Mock)
        self.assertEqual(1, volume.VolumeMountPoint.mount.call_count)
        self.assertEqual(1, volume.VolumeMountPoint.write_to_fstab.call_count)
        volume.VolumeMountPoint.mount = origin_
        volume.VolumeMountPoint.write_to_fstab = origin_write_to_fstab
        os.path.exists = origin_os_path_exists

    def test_resize_fs(self):
        origin_check_device_exists = self.volumeDevice._check_device_exists
        origin_execute = utils.execute
        utils.execute = Mock()
        self.volumeDevice._check_device_exists = MagicMock()
        origin_os_path_exists = os.path.exists
        os.path.exists = Mock()

        self.volumeDevice.resize_fs('/mnt/volume')

        self.assertEqual(1, self.volumeDevice._check_device_exists.call_count)
        self.assertEqual(2, utils.execute.call_count)
        self.volumeDevice._check_device_exists = origin_check_device_exists
        os.path.exists = origin_os_path_exists
        utils.execute = origin_execute

    def test_unmount_positive(self):
        self._test_unmount()

    def test_unmount_negative(self):
        self._test_unmount(False)

    def _test_unmount(self, positive=True):
        origin_ = os.path.exists
        os.path.exists = MagicMock(return_value=positive)
        fake_spawn = _setUp_fake_spawn()

        self.volumeDevice.unmount('/mnt/volume')
        COUNT = 1
        if not positive:
            COUNT = 0
        self.assertEqual(COUNT, fake_spawn.expect.call_count)
        os.path.exists = origin_


class VolumeMountPointTest(testtools.TestCase):
    def setUp(self):
        super(VolumeMountPointTest, self).setUp()
        self.volumeMountPoint = volume.VolumeMountPoint('/mnt/device',
                                                        '/dev/vdb')

    def tearDown(self):
        super(VolumeMountPointTest, self).tearDown()

    def test_mount(self):
        origin_ = os.path.exists
        os.path.exists = MagicMock(return_value=False)
        fake_spawn = _setUp_fake_spawn()

        utils.execute = Mock()

        self.volumeMountPoint.mount()

        self.assertEqual(1, os.path.exists.call_count)
        self.assertEqual(1, utils.execute.call_count)
        self.assertEqual(1, fake_spawn.expect.call_count)

        os.path.exists = origin_

    def test_write_to_fstab(self):
        origin_execute = utils.execute
        utils.execute = Mock()
        m = mock_open()
        with patch('%s.open' % volume.__name__, m, create=True):
            self.volumeMountPoint.write_to_fstab()

        self.assertEqual(2, utils.execute.call_count)
        utils.execute = origin_execute

########NEW FILE########
__FILENAME__ = test_instance_controller
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
import jsonschema
from testtools import TestCase
from testtools.matchers import Is, Equals
from testtools.testcase import skip
from trove.common import apischema
from trove.instance.service import InstanceController


class TestInstanceController(TestCase):
    def setUp(self):
        super(TestInstanceController, self).setUp()
        self.controller = InstanceController()
        self.instance = {
            "instance": {
                "volume": {"size": "1"},
                "users": [
                    {"name": "user1",
                     "password": "litepass",
                     "databases": [{"name": "firstdb"}]}
                ],
                "flavorRef": "https://localhost:8779/v1.0/2500/1",
                "name": "TEST-XYS2d2fe2kl;zx;jkl2l;sjdcma239(E)@(D",
                "databases": [
                    {
                        "name": "firstdb",
                        "collate": "latin2_general_ci",
                        "character_set": "latin2"
                    },
                    {
                        "name": "db2"
                    }
                ]
            }
        }

    def verify_errors(self, errors, msg=None, properties=None, path=None):
        msg = msg or []
        properties = properties or []
        self.assertThat(len(errors), Is(len(msg)))
        i = 0
        while i < len(msg):
            self.assertIn(errors[i].message, msg)
            if path:
                self.assertThat(path, Equals(properties[i]))
            else:
                self.assertThat(errors[i].path.pop(), Equals(properties[i]))
            i += 1

    def test_get_schema_create(self):
        schema = self.controller.get_schema('create', {'instance': {}})
        self.assertIsNotNone(schema)
        self.assertTrue('instance' in schema['properties'])

    def test_get_schema_action_restart(self):
        schema = self.controller.get_schema('action', {'restart': {}})
        self.assertIsNotNone(schema)
        self.assertTrue('restart' in schema['properties'])

    def test_get_schema_action_resize_volume(self):
        schema = self.controller.get_schema(
            'action', {'resize': {'volume': {}}})
        self.assertIsNotNone(schema)
        self.assertTrue('resize' in schema['properties'])
        self.assertTrue(
            'volume' in schema['properties']['resize']['properties'])

    def test_get_schema_action_resize_flavorRef(self):
        schema = self.controller.get_schema(
            'action', {'resize': {'flavorRef': {}}})
        self.assertIsNotNone(schema)
        self.assertTrue('resize' in schema['properties'])
        self.assertTrue(
            'flavorRef' in schema['properties']['resize']['properties'])

    def test_get_schema_action_other(self):
        schema = self.controller.get_schema(
            'action', {'supersized': {'flavorRef': {}}})
        self.assertIsNotNone(schema)
        self.assertThat(len(schema.keys()), Is(0))

    def test_validate_create_complete(self):
        body = self.instance
        schema = self.controller.get_schema('create', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertTrue(validator.is_valid(body))

    def test_validate_create_complete_with_restore(self):
        body = self.instance
        body['instance']['restorePoint'] = {
            "backupRef": "d761edd8-0771-46ff-9743-688b9e297a3b"
        }
        schema = self.controller.get_schema('create', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertTrue(validator.is_valid(body))

    def test_validate_create_complete_with_restore_error(self):
        body = self.instance
        backup_id_ref = "invalid-backup-id-ref"
        body['instance']['restorePoint'] = {
            "backupRef": backup_id_ref
        }
        schema = self.controller.get_schema('create', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertFalse(validator.is_valid(body))
        errors = sorted(validator.iter_errors(body), key=lambda e: e.path)
        self.assertThat(len(errors), Is(1))
        self.assertThat(errors[0].message,
                        Equals("'%s' does not match '%s'" %
                               (backup_id_ref, apischema.uuid['pattern'])))

    def test_validate_create_blankname(self):
        body = self.instance
        body['instance']['name'] = "     "
        schema = self.controller.get_schema('create', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertFalse(validator.is_valid(body))
        errors = sorted(validator.iter_errors(body), key=lambda e: e.path)
        self.assertThat(len(errors), Is(1))
        self.assertThat(errors[0].message,
                        Equals("'     ' does not match '^.*[0-9a-zA-Z]+.*$'"))

    def test_validate_restart(self):
        body = {"restart": {}}
        schema = self.controller.get_schema('action', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertTrue(validator.is_valid(body))

    def test_validate_invalid_action(self):
        # TODO(juice) perhaps we should validate the schema not recognized
        body = {"restarted": {}}
        schema = self.controller.get_schema('action', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertTrue(validator.is_valid(body))

    def test_validate_resize_volume(self):
        body = {"resize": {"volume": {"size": 4}}}
        schema = self.controller.get_schema('action', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertTrue(validator.is_valid(body))

    def test_validate_resize_volume_string(self):
        body = {"resize": {"volume": {"size": '-44.0'}}}
        schema = self.controller.get_schema('action', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertTrue(validator.is_valid(body))

    def test_validate_resize_volume_invalid(self):
        body = {"resize": {"volume": {"size": 'x'}}}
        schema = self.controller.get_schema('action', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertFalse(validator.is_valid(body))
        errors = sorted(validator.iter_errors(body), key=lambda e: e.path)
        self.assertThat(errors[0].context[0].message,
                        Equals("'x' is not of type 'integer'"))
        self.assertThat(errors[0].context[1].message,
                        Equals("'x' does not match '[0-9]+'"))
        self.assertThat(errors[0].path.pop(), Equals('size'))

    def test_validate_resize_instance(self):
        body = {"resize": {"flavorRef": "https://endpoint/v1.0/123/flavors/2"}}
        schema = self.controller.get_schema('action', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertTrue(validator.is_valid(body))

    def test_validate_resize_instance_int(self):
        body = {"resize": {"flavorRef": 2}}
        schema = self.controller.get_schema('action', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertTrue(validator.is_valid(body))

    def test_validate_resize_instance_empty_url(self):
        body = {"resize": {"flavorRef": ""}}
        schema = self.controller.get_schema('action', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertFalse(validator.is_valid(body))
        errors = sorted(validator.iter_errors(body), key=lambda e: e.path)
        self.verify_errors(errors[0].context,
                           ["'' is too short",
                            "'' does not match 'http[s]?://(?:[a-zA-Z]"
                            "|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]"
                            "|(?:%[0-9a-fA-F][0-9a-fA-F]))+'",
                            "'' does not match '[0-9]+'",
                            "'' is not of type 'integer'"],
                           ["flavorRef", "flavorRef", "flavorRef",
                            "flavorRef"],
                           errors[0].path.pop())

    @skip("This damn URI validator allows just about any garbage you give it")
    def test_validate_resize_instance_invalid_url(self):
        body = {"resize": {"flavorRef": "xyz-re1f2-daze329d-f23901"}}
        schema = self.controller.get_schema('action', body)
        self.assertIsNotNone(schema)
        validator = jsonschema.Draft4Validator(schema)
        self.assertFalse(validator.is_valid(body))
        errors = sorted(validator.iter_errors(body), key=lambda e: e.path)
        self.verify_errors(errors, ["'' is too short"], ["flavorRef"])

########NEW FILE########
__FILENAME__ = test_instance_models
#    Copyright 2014 Rackspace Hosting
#    Copyright 2014 Hewlett-Packard Development Company, L.P.
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from mock import Mock
from testtools import TestCase
from trove.common import cfg
from trove.common.instance import ServiceStatuses
from trove.instance.models import filter_ips
from trove.instance.models import InstanceServiceStatus
from trove.instance.models import DBInstance
from trove.instance.models import Instance
from trove.instance.models import SimpleInstance
from trove.instance.tasks import InstanceTasks

CONF = cfg.CONF


class SimpleInstanceTest(TestCase):

    def setUp(self):
        super(SimpleInstanceTest, self).setUp()
        db_info = DBInstance(InstanceTasks.BUILDING, name="TestInstance")
        self.instance = SimpleInstance(None, db_info,
                                       InstanceServiceStatus(
                                           ServiceStatuses.BUILDING),
                                       ds_version=Mock(), ds=Mock())
        db_info.addresses = {"private": [{"addr": "123.123.123.123"}],
                             "internal": [{"addr": "10.123.123.123"}],
                             "public": [{"addr": "15.123.123.123"}]}
        self.orig_conf = CONF.network_label_regex
        self.orig_ip_regex = CONF.ip_regex

    def tearDown(self):
        super(SimpleInstanceTest, self).tearDown()
        CONF.network_label_regex = self.orig_conf
        CONF.ip_start = None
        CONF.ip_regex = self.orig_ip_regex

    def test_get_root_on_create(self):
        root_on_create_val = Instance.get_root_on_create('redis')
        self.assertFalse(root_on_create_val)

    def test_filter_ips(self):
        CONF.network_label_regex = '.*'
        CONF.ip_regex = '^(15.|123.)'
        ip = self.instance.get_visible_ip_addresses()
        ip = filter_ips(ip, CONF.ip_regex)
        self.assertTrue(len(ip) == 2)
        self.assertTrue('123.123.123.123' in ip)
        self.assertTrue('15.123.123.123' in ip)

    def test_one_network_label_exact(self):
        CONF.network_label_regex = '^internal$'
        ip = self.instance.get_visible_ip_addresses()
        self.assertEqual(['10.123.123.123'], ip)

    def test_one_network_label(self):
        CONF.network_label_regex = 'public'
        ip = self.instance.get_visible_ip_addresses()
        self.assertEqual(['15.123.123.123'], ip)

    def test_two_network_labels(self):
        CONF.network_label_regex = '^(private|public)$'
        ip = self.instance.get_visible_ip_addresses()
        self.assertTrue(len(ip) == 2)
        self.assertTrue('123.123.123.123' in ip)
        self.assertTrue('15.123.123.123' in ip)

    def test_all_network_labels(self):
        CONF.network_label_regex = '.*'
        ip = self.instance.get_visible_ip_addresses()
        self.assertTrue(len(ip) == 3)
        self.assertTrue('10.123.123.123' in ip)
        self.assertTrue('123.123.123.123' in ip)
        self.assertTrue('15.123.123.123' in ip)

########NEW FILE########
__FILENAME__ = test_instance_status
# Copyright 2013 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
from testtools import TestCase
from trove.common.instance import ServiceStatuses
from trove.instance.models import InstanceStatus
from trove.instance.models import InstanceServiceStatus
from trove.instance.models import SimpleInstance
from trove.tests.util import test_config


class FakeInstanceTask(object):

    def __init__(self):
        self.is_error = False
        self.action = None


class FakeDBInstance(object):

    def __init__(self):
        self.id = None
        self.deleted = False
        self.datastore_version_id = test_config.dbaas_datastore_version_id
        self.server_status = "ACTIVE"
        self.task_status = FakeInstanceTask()


class InstanceStatusTest(TestCase):

    def setUp(self):
        super(InstanceStatusTest, self).setUp()

    def tearDown(self):
        super(InstanceStatusTest, self).tearDown()

    def test_task_status_error_reports_error(self):
        fake_db_info = FakeDBInstance()
        fake_status = InstanceServiceStatus(ServiceStatuses.RUNNING)
        fake_db_info.task_status.is_error = True
        instance = SimpleInstance('dummy context', fake_db_info, fake_status)
        self.assertEqual(InstanceStatus.ERROR, instance.status)

    def test_task_status_action_building_reports_build(self):
        fake_db_info = FakeDBInstance()
        fake_status = InstanceServiceStatus(ServiceStatuses.RUNNING)
        fake_db_info.task_status.action = "BUILDING"
        instance = SimpleInstance('dummy context', fake_db_info, fake_status)
        self.assertEqual(InstanceStatus.BUILD, instance.status)

    def test_task_status_action_rebooting_reports_reboot(self):
        fake_db_info = FakeDBInstance()
        fake_status = InstanceServiceStatus(ServiceStatuses.RUNNING)
        fake_db_info.task_status.action = "REBOOTING"
        instance = SimpleInstance('dummy context', fake_db_info, fake_status)
        self.assertEqual(InstanceStatus.REBOOT, instance.status)

    def test_task_status_action_resizing_reports_resize(self):
        fake_db_info = FakeDBInstance()
        fake_status = InstanceServiceStatus(ServiceStatuses.RUNNING)
        fake_db_info.task_status.action = "RESIZING"
        instance = SimpleInstance('dummy context', fake_db_info, fake_status)
        self.assertEqual(InstanceStatus.RESIZE, instance.status)

    def test_task_status_action_deleting_reports_shutdown(self):
        fake_db_info = FakeDBInstance()
        fake_status = InstanceServiceStatus(ServiceStatuses.RUNNING)
        fake_db_info.task_status.action = "DELETING"
        instance = SimpleInstance('dummy context', fake_db_info, fake_status)
        self.assertEqual(InstanceStatus.SHUTDOWN, instance.status)

    def test_nova_server_build_reports_build(self):
        fake_db_info = FakeDBInstance()
        fake_status = InstanceServiceStatus(ServiceStatuses.RUNNING)
        fake_db_info.server_status = "BUILD"
        instance = SimpleInstance('dummy context', fake_db_info, fake_status)
        self.assertEqual(InstanceStatus.BUILD, instance.status)

    def test_nova_server_error_reports_error(self):
        fake_db_info = FakeDBInstance()
        fake_status = InstanceServiceStatus(ServiceStatuses.RUNNING)
        fake_db_info.server_status = "ERROR"
        instance = SimpleInstance('dummy context', fake_db_info, fake_status)
        self.assertEqual(InstanceStatus.ERROR, instance.status)

    def test_nova_server_reboot_reports_reboot(self):
        fake_db_info = FakeDBInstance()
        fake_status = InstanceServiceStatus(ServiceStatuses.RUNNING)
        fake_db_info.server_status = "REBOOT"
        instance = SimpleInstance('dummy context', fake_db_info, fake_status)
        self.assertEqual(InstanceStatus.REBOOT, instance.status)

    def test_nova_server_resize_reports_resize(self):
        fake_db_info = FakeDBInstance()
        fake_status = InstanceServiceStatus(ServiceStatuses.RUNNING)
        fake_db_info.server_status = "RESIZE"
        instance = SimpleInstance('dummy context', fake_db_info, fake_status)
        self.assertEqual(InstanceStatus.RESIZE, instance.status)

    def test_nova_server_verify_resize_reports_resize(self):
        fake_db_info = FakeDBInstance()
        fake_status = InstanceServiceStatus(ServiceStatuses.RUNNING)
        fake_db_info.server_status = "VERIFY_RESIZE"
        instance = SimpleInstance('dummy context', fake_db_info, fake_status)
        self.assertEqual(InstanceStatus.RESIZE, instance.status)

    def test_service_status_paused_reports_reboot(self):
        fake_db_info = FakeDBInstance()
        fake_status = InstanceServiceStatus(ServiceStatuses.RUNNING)
        fake_status.set_status(ServiceStatuses.PAUSED)
        instance = SimpleInstance('dummy context', fake_db_info, fake_status)
        self.assertEqual(InstanceStatus.REBOOT, instance.status)

    def test_service_status_new_reports_build(self):
        fake_db_info = FakeDBInstance()
        fake_status = InstanceServiceStatus(ServiceStatuses.RUNNING)
        fake_status.set_status(ServiceStatuses.NEW)
        instance = SimpleInstance('dummy context', fake_db_info, fake_status)
        self.assertEqual(InstanceStatus.BUILD, instance.status)

    def test_service_status_running_reports_active(self):
        fake_db_info = FakeDBInstance()
        fake_status = InstanceServiceStatus(ServiceStatuses.RUNNING)
        fake_status.set_status(ServiceStatuses.RUNNING)
        instance = SimpleInstance('dummy context', fake_db_info, fake_status)
        self.assertEqual(InstanceStatus.ACTIVE, instance.status)

########NEW FILE########
__FILENAME__ = test_instance_views
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
from mock import Mock
from testtools import TestCase
from trove.common import cfg
from trove.instance.views import InstanceView
from trove.instance.views import InstanceDetailView

CONF = cfg.CONF


class InstanceViewsTest(TestCase):

    def setUp(self):
        super(InstanceViewsTest, self).setUp()
        self.addresses = {"private": [{"addr": "123.123.123.123"}],
                          "internal": [{"addr": "10.123.123.123"}],
                          "public": [{"addr": "15.123.123.123"}]}
        self.orig_label_regex = CONF.network_label_regex
        self.orig_ip_regex = CONF.ip_regex

    def tearDown(self):
        super(InstanceViewsTest, self).tearDown()
        CONF.network_label_regex = self.orig_label_regex
        CONF.ip_regex = self.orig_ip_regex


class InstanceDetailViewTest(TestCase):

    def setUp(self):
        super(InstanceDetailViewTest, self).setUp()
        self.build_links_method = InstanceView._build_links
        self.build_flavor_links_method = InstanceView._build_flavor_links
        self.build_config_method = InstanceDetailView._build_configuration_info
        InstanceView._build_links = Mock()
        InstanceView._build_flavor_links = Mock()
        InstanceDetailView._build_configuration_info = Mock()
        self.instance = Mock()
        self.instance.created = 'Yesterday'
        self.instance.updated = 'Now'
        self.instance.datastore_version = Mock()
        self.instance.datastore_version.name = 'mysql_test_version'
        self.instance.hostname = 'test.trove.com'
        self.ip = "1.2.3.4"
        self.instance.addresses = {"private": [{"addr": self.ip}]}
        self.instance.volume_used = '3'
        self.instance.root_password = 'iloveyou'
        self.instance.get_visible_ip_addresses = lambda: ["1.2.3.4"]

    def tearDown(self):
        super(InstanceDetailViewTest, self).tearDown()
        InstanceView._build_links = self.build_links_method
        InstanceView._build_flavor_links = self.build_flavor_links_method
        InstanceDetailView._build_configuration_info = self.build_config_method

    def test_data_hostname(self):
        view = InstanceDetailView(self.instance, Mock())
        result = view.data()
        self.assertEqual(self.instance.created, result['instance']['created'])
        self.assertEqual(self.instance.updated, result['instance']['updated'])
        self.assertEqual(self.instance.datastore_version.name,
                         result['instance']['datastore']['version'])
        self.assertEqual(self.instance.hostname,
                         result['instance']['hostname'])
        self.assertNotIn('ip', result['instance'])

    def test_data_ip(self):
        self.instance.hostname = None
        view = InstanceDetailView(self.instance, Mock())
        result = view.data()
        self.assertEqual(self.instance.created, result['instance']['created'])
        self.assertEqual(self.instance.updated, result['instance']['updated'])
        self.assertEqual(self.instance.datastore_version.name,
                         result['instance']['datastore']['version'])
        self.assertNotIn('hostname', result['instance'])
        self.assertEqual([self.ip], result['instance']['ip'])

########NEW FILE########
__FILENAME__ = test_models
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
from mock import MagicMock, patch, ANY
from testtools import TestCase
from testtools.matchers import Equals, Is, Not

from novaclient.v1_1 import Client
from novaclient.v1_1.flavors import FlavorManager, Flavor
from novaclient.v1_1.servers import Server, ServerManager
from oslo.config import cfg
from trove.backup.models import Backup
from trove.common.context import TroveContext
from trove.common import instance as rd_instance
from trove.datastore import models as datastore_models
from trove.db.models import DatabaseModelBase
from trove.instance.models import DBInstance
from trove.instance.models import InstanceServiceStatus
from trove.instance.tasks import InstanceTasks
import trove.extensions.mgmt.instances.models as mgmtmodels
from trove.openstack.common.notifier import api as notifier
from trove.common import remote
from trove.tests.util import test_config

CONF = cfg.CONF


class MockMgmtInstanceTest(TestCase):
    def setUp(self):
        super(MockMgmtInstanceTest, self).setUp()
        self.context = TroveContext()
        self.context.auth_token = 'some_secret_password'
        self.client = MagicMock(spec=Client)
        self.server_mgr = MagicMock(spec=ServerManager)
        self.client.servers = self.server_mgr
        self.flavor_mgr = MagicMock(spec=FlavorManager)
        self.client.flavors = self.flavor_mgr
        remote.create_admin_nova_client = MagicMock(return_value=self.client)
        CONF.set_override('host', 'test_host')
        CONF.set_override('exists_notification_ticks', 1)
        CONF.set_override('report_interval', 20)
        CONF.set_override('notification_service_id', {'mysql': '123'})

    def tearDown(self):
        super(MockMgmtInstanceTest, self).tearDown()

    @staticmethod
    def build_db_instance(status, task_status=InstanceTasks.DELETING):
        return DBInstance(task_status,
                          created='xyz',
                          name='test_name',
                          id='1',
                          flavor_id='flavor_1',
                          datastore_version_id=
                          test_config.dbaas_datastore_version_id,
                          compute_instance_id='compute_id_1',
                          server_id='server_id_1',
                          tenant_id='tenant_id_1',
                          server_status=status)


class TestNotificationTransformer(MockMgmtInstanceTest):

    def test_tranformer(self):
        transformer = mgmtmodels.NotificationTransformer(context=self.context)
        status = rd_instance.ServiceStatuses.BUILDING.api_status
        db_instance = MockMgmtInstanceTest.build_db_instance(
            status, InstanceTasks.BUILDING)

        with patch.object(DatabaseModelBase, 'find_all',
                          return_value=[db_instance]):
            stub_dsv_db_info = MagicMock(
                spec=datastore_models.DBDatastoreVersion)
            stub_dsv_db_info.id = "test_datastore_version"
            stub_dsv_db_info.datastore_id = "mysql_test_version"
            stub_dsv_db_info.name = "test_datastore_name"
            stub_dsv_db_info.image_id = "test_datastore_image_id"
            stub_dsv_db_info.packages = "test_datastore_pacakges"
            stub_dsv_db_info.active = 1
            stub_dsv_db_info.manager = "mysql"
            stub_datastore_version = datastore_models.DatastoreVersion(
                stub_dsv_db_info)

            def side_effect_func(*args, **kwargs):
                if 'instance_id' in kwargs:
                    return InstanceServiceStatus(
                        rd_instance.ServiceStatuses.BUILDING)
                else:
                    return stub_datastore_version

            with patch.object(DatabaseModelBase, 'find_by',
                              side_effect=side_effect_func):
                payloads = transformer()
                self.assertIsNotNone(payloads)
                self.assertThat(len(payloads), Equals(1))
                payload = payloads[0]
                self.assertThat(payload['audit_period_beginning'],
                                Not(Is(None)))
                self.assertThat(payload['audit_period_ending'], Not(Is(None)))
                self.assertThat(payload['state'], Equals(status.lower()))

    def test_get_service_id(self):
        id_map = {
            'mysql': '123',
            'percona': 'abc'
        }
        transformer = mgmtmodels.NotificationTransformer(context=self.context)
        self.assertThat(transformer._get_service_id('mysql', id_map),
                        Equals('123'))

    def test_get_service_id_unknown(self):
        id_map = {
            'mysql': '123',
            'percona': 'abc'
        }
        transformer = mgmtmodels.NotificationTransformer(context=self.context)
        self.assertThat(transformer._get_service_id('m0ng0', id_map),
                        Equals('unknown-service-id-error'))


class TestNovaNotificationTransformer(MockMgmtInstanceTest):
    def test_transformer_cache(self):
        flavor = MagicMock(spec=Flavor)
        flavor.name = 'db.small'
        with patch.object(self.flavor_mgr, 'get', return_value=flavor):
            transformer = mgmtmodels.NovaNotificationTransformer(
                context=self.context)
            transformer2 = mgmtmodels.NovaNotificationTransformer(
                context=self.context)
            self.assertThat(transformer._flavor_cache,
                            Not(Is(transformer2._flavor_cache)))

    def test_lookup_flavor(self):
        flavor = MagicMock(spec=Flavor)
        flavor.name = 'flav_1'
        with patch.object(self.flavor_mgr, 'get', side_effect=[flavor, None]):
            transformer = mgmtmodels.NovaNotificationTransformer(
                context=self.context)
            self.assertThat(transformer._lookup_flavor('1'),
                            Equals(flavor.name))
            self.assertThat(transformer._lookup_flavor('2'),
                            Equals('unknown'))

    def test_tranformer(self):
        status = rd_instance.ServiceStatuses.BUILDING.api_status
        db_instance = MockMgmtInstanceTest.build_db_instance(
            status, task_status=InstanceTasks.BUILDING)

        stub_dsv_db_info = MagicMock(spec=datastore_models.DBDatastoreVersion)
        stub_dsv_db_info.id = "test_datastore_version"
        stub_dsv_db_info.datastore_id = "mysql_test_version"
        stub_dsv_db_info.name = "test_datastore_name"
        stub_dsv_db_info.image_id = "test_datastore_image_id"
        stub_dsv_db_info.packages = "test_datastore_pacakges"
        stub_dsv_db_info.active = 1
        stub_dsv_db_info.manager = "mysql"
        stub_datastore_version = datastore_models.DatastoreVersion(
            stub_dsv_db_info)

        flavor = MagicMock(spec=Flavor)
        flavor.name = 'db.small'

        server = MagicMock(spec=Server)
        server.user_id = 'test_user_id'
        mgmt_instance = mgmtmodels.SimpleMgmtInstance(self.context,
                                                      db_instance,
                                                      server,
                                                      None)

        with patch.object(DatabaseModelBase, 'find_by',
                          return_value=stub_datastore_version):

            with patch.object(mgmtmodels, 'load_mgmt_instances',
                              return_value=[mgmt_instance]):

                with patch.object(self.flavor_mgr, 'get', return_value=flavor):

                    # invocation
                    transformer = mgmtmodels.NovaNotificationTransformer(
                        context=self.context)
                    payloads = transformer()

                    # assertions
                    self.assertIsNotNone(payloads)
                    self.assertThat(len(payloads), Equals(1))
                    payload = payloads[0]
                    self.assertThat(payload['audit_period_beginning'],
                                    Not(Is(None)))
                    self.assertThat(payload['audit_period_ending'],
                                    Not(Is(None)))
                    self.assertThat(payload['state'], Equals(status.lower()))
                    self.assertThat(payload['instance_type'],
                                    Equals('db.small'))
                    self.assertThat(payload['instance_type_id'],
                                    Equals('flavor_1'))
                    self.assertThat(payload['user_id'], Equals('test_user_id'))
                    self.assertThat(payload['service_id'], Equals('123'))

    def test_tranformer_invalid_datastore_manager(self):
        status = rd_instance.ServiceStatuses.BUILDING.api_status
        db_instance = MockMgmtInstanceTest.build_db_instance(
            status, task_status=InstanceTasks.BUILDING)

        server = MagicMock(spec=Server)
        server.user_id = 'test_user_id'
        stub_datastore_version = MagicMock()
        stub_datastore_version.id = "stub_datastore_version"
        stub_datastore_version.manager = "m0ng0"
        stub_datastore = MagicMock()
        stub_datastore.default_datastore_version = "stub_datastore_version"

        flavor = MagicMock(spec=Flavor)
        flavor.name = 'db.small'

        with patch.object(datastore_models.DatastoreVersion, 'load',
                          return_value=stub_datastore_version):
            with patch.object(datastore_models.DatastoreVersion,
                              'load_by_uuid',
                              return_value=stub_datastore_version):
                with patch.object(datastore_models.Datastore, 'load',
                                  return_value=stub_datastore):
                    mgmt_instance = mgmtmodels.SimpleMgmtInstance(self.context,
                                                                  db_instance,
                                                                  server,
                                                                  None)
                    with patch.object(mgmtmodels, 'load_mgmt_instances',
                                      return_value=[mgmt_instance]):
                        with patch.object(self.flavor_mgr,
                                          'get', return_value=flavor):

                            # invocation
                            transformer = (
                                mgmtmodels.NovaNotificationTransformer(
                                    context=self.context)
                            )

                            payloads = transformer()
                            # assertions
                            self.assertIsNotNone(payloads)
                            self.assertThat(len(payloads), Equals(1))
                            payload = payloads[0]
                            self.assertThat(payload['audit_period_beginning'],
                                            Not(Is(None)))
                            self.assertThat(payload['audit_period_ending'],
                                            Not(Is(None)))
                            self.assertThat(payload['state'],
                                            Equals(status.lower()))
                            self.assertThat(payload['instance_type'],
                                            Equals('db.small'))
                            self.assertThat(payload['instance_type_id'],
                                            Equals('flavor_1'))
                            self.assertThat(payload['user_id'],
                                            Equals('test_user_id'))
                            self.assertThat(payload['service_id'],
                                            Equals('unknown-service-id-error'))

    def test_tranformer_shutdown_instance(self):
        status = rd_instance.ServiceStatuses.SHUTDOWN.api_status
        db_instance = self.build_db_instance(status)

        server = MagicMock(spec=Server)
        server.user_id = 'test_user_id'
        mgmt_instance = mgmtmodels.SimpleMgmtInstance(self.context,
                                                      db_instance,
                                                      server,
                                                      None)
        flavor = MagicMock(spec=Flavor)
        flavor.name = 'db.small'

        with patch.object(Backup, 'running', return_value=None):
            self.assertThat(mgmt_instance.status, Equals('SHUTDOWN'))
            with patch.object(mgmtmodels, 'load_mgmt_instances',
                              return_value=[mgmt_instance]):
                with patch.object(self.flavor_mgr, 'get', return_value=flavor):
                    # invocation
                    transformer = mgmtmodels.NovaNotificationTransformer(
                        context=self.context)
                    payloads = transformer()
                    # assertion that SHUTDOWN instances are not reported
                    self.assertIsNotNone(payloads)
                    self.assertThat(len(payloads), Equals(0))

    def test_tranformer_no_nova_instance(self):
        status = rd_instance.ServiceStatuses.SHUTDOWN.api_status
        db_instance = MockMgmtInstanceTest.build_db_instance(status)

        mgmt_instance = mgmtmodels.SimpleMgmtInstance(self.context,
                                                      db_instance,
                                                      None,
                                                      None)
        flavor = MagicMock(spec=Flavor)
        flavor.name = 'db.small'

        with patch.object(Backup, 'running', return_value=None):
            self.assertThat(mgmt_instance.status, Equals('SHUTDOWN'))
            with patch.object(mgmtmodels, 'load_mgmt_instances',
                              return_value=[mgmt_instance]):
                with patch.object(self.flavor_mgr, 'get', return_value=flavor):
                    # invocation
                    transformer = mgmtmodels.NovaNotificationTransformer(
                        context=self.context)
                    payloads = transformer()
                    # assertion that SHUTDOWN instances are not reported
                    self.assertIsNotNone(payloads)
                    self.assertThat(len(payloads), Equals(0))

    def test_tranformer_flavor_cache(self):
        status = rd_instance.ServiceStatuses.BUILDING.api_status
        db_instance = MockMgmtInstanceTest.build_db_instance(
            status, InstanceTasks.BUILDING)

        server = MagicMock(spec=Server)
        server.user_id = 'test_user_id'
        mgmt_instance = mgmtmodels.SimpleMgmtInstance(self.context,
                                                      db_instance,
                                                      server,
                                                      None)
        flavor = MagicMock(spec=Flavor)
        flavor.name = 'db.small'

        with patch.object(mgmtmodels, 'load_mgmt_instances',
                          return_value=[mgmt_instance]):
            with patch.object(self.flavor_mgr, 'get', return_value=flavor):
                transformer = mgmtmodels.NovaNotificationTransformer(
                    context=self.context)
                transformer()
                # call twice ensure client.flavor invoked once
                payloads = transformer()
                self.assertIsNotNone(payloads)
                self.assertThat(len(payloads), Equals(1))
                payload = payloads[0]
                self.assertThat(payload['audit_period_beginning'],
                                Not(Is(None)))
                self.assertThat(payload['audit_period_ending'], Not(Is(None)))
                self.assertThat(payload['state'], Equals(status.lower()))
                self.assertThat(payload['instance_type'], Equals('db.small'))
                self.assertThat(payload['instance_type_id'],
                                Equals('flavor_1'))
                self.assertThat(payload['user_id'], Equals('test_user_id'))
                # ensure cache was used to get flavor second time
                self.flavor_mgr.get.assert_any_call('flavor_1')


class TestMgmtInstanceTasks(MockMgmtInstanceTest):
    def test_public_exists_events(self):
        status = rd_instance.ServiceStatuses.BUILDING.api_status
        db_instance = MockMgmtInstanceTest.build_db_instance(
            status, task_status=InstanceTasks.BUILDING)

        server = MagicMock(spec=Server)
        server.user_id = 'test_user_id'
        mgmt_instance = mgmtmodels.SimpleMgmtInstance(self.context,
                                                      db_instance,
                                                      server,
                                                      None)

        flavor = MagicMock(spec=Flavor)
        flavor.name = 'db.small'

        with patch.object(mgmtmodels, 'load_mgmt_instances',
                          return_value=[mgmt_instance]):
            with patch.object(self.flavor_mgr, 'get', return_value=flavor):
                self.assertThat(self.context.auth_token,
                                Is('some_secret_password'))
                with patch.object(notifier, 'notify', return_value=None):
                    # invocation
                    mgmtmodels.publish_exist_events(
                        mgmtmodels.NovaNotificationTransformer(
                            context=self.context),
                        self.context)
                    # assertion
                    notifier.notify.assert_any_call(self.context,
                                                    'test_host',
                                                    'trove.instance.exists',
                                                    'INFO',
                                                    ANY)
                    self.assertThat(self.context.auth_token, Is(None))

########NEW FILE########
__FILENAME__ = test_common
#    Copyright 2013 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
from testtools import TestCase
from testtools.matchers import Is
from testtools.matchers import Equals
from trove.common.exception import DatabaseInitialDatabaseDuplicateError
from trove.common.exception import DatabaseForUserNotInDatabaseListError
from trove.common.exception import DatabaseInitialUserDuplicateError
from trove.extensions.mysql.common import populate_validated_databases
from trove.extensions.mysql.common import populate_users


class MySqlCommonTest(TestCase):

    def setUp(self):
        super(MySqlCommonTest, self).setUp()

    def tearDown(self):
        super(MySqlCommonTest, self).tearDown()

    def test_initial_databases_none(self):
        databases = []
        result = populate_validated_databases(databases)
        self.assertThat(len(result), Is(0))

    def test_initial_databases_single(self):
        databases = [{'name': 'one_db'}]
        result = populate_validated_databases(databases)
        self.assertThat(len(result), Is(1))
        self.assertThat(result[0]['_name'], Equals('one_db'))

    def test_initial_databases_unique(self):
        databases = [{'name': 'one_db'}, {'name': 'diff_db'}]
        result = populate_validated_databases(databases)
        self.assertThat(len(result), Is(2))

    def test_initial_databases_duplicate(self):
        databases = [{'name': 'same_db'}, {'name': 'same_db'}]
        self.assertRaises(DatabaseInitialDatabaseDuplicateError,
                          populate_validated_databases, databases)

    def test_initial_databases_intermingled(self):
        databases = [{'name': 'a_db'}, {'name': 'b_db'}, {'name': 'a_db'}]
        self.assertRaises(DatabaseInitialDatabaseDuplicateError,
                          populate_validated_databases, databases)

    def test_populate_users_single(self):
        users = [{'name': 'bob', 'password': 'x'}]
        result = populate_users(users)
        self.assertThat(len(result), Is(1))
        self.assertThat(result[0]['_name'], Equals('bob'))
        self.assertThat(result[0]['_password'], Equals('x'))

    def test_populate_users_unique_host(self):
        users = [{'name': 'bob', 'password': 'x', 'host': '127.0.0.1'},
                 {'name': 'bob', 'password': 'x', 'host': '128.0.0.1'}]
        result = populate_users(users)
        self.assertThat(len(result), Is(2))

    def test_populate_users_unique_name(self):
        users = [{'name': 'bob', 'password': 'x', 'host': '127.0.0.1'},
                 {'name': 'tom', 'password': 'x', 'host': '127.0.0.1'}]
        result = populate_users(users)
        self.assertThat(len(result), Is(2))

    def test_populate_users_duplicate(self):
        users = [{'name': 'bob', 'password': 'x', 'host': '127.0.0.1'},
                 {'name': 'bob', 'password': 'y', 'host': '127.0.0.1'}]
        self.assertRaises(DatabaseInitialUserDuplicateError,
                          populate_users, users)

    def test_populate_users_intermingled(self):
        users = [{'name': 'bob', 'password': 'x', 'host': '127.0.0.1'},
                 {'name': 'tom', 'password': 'y', 'host': '128.0.0.1'},
                 {'name': 'bob', 'password': 'z', 'host': '127.0.0.1'}]
        self.assertRaises(DatabaseInitialUserDuplicateError,
                          populate_users, users)

    def test_populate_users_both_db_list_empty(self):
        initial_databases = []
        users = [{"name": "bob", "password": "x"}]
        result = populate_users(users, initial_databases)
        self.assertThat(len(result), Is(1))

    def test_populate_users_initial_db_list_empty(self):
        initial_databases = []
        users = [{"name": "bob", "password": "x",
                  "databases": [{"name": "my_db"}]}]
        self.assertRaises(DatabaseForUserNotInDatabaseListError,
                          populate_users, users, initial_databases)

    def test_populate_users_user_db_list_empty(self):
        initial_databases = ['my_db']
        users = [{"name": "bob", "password": "x"}]
        result = populate_users(users, initial_databases)
        self.assertThat(len(result), Is(1))

    def test_populate_users_db_in_list(self):
        initial_databases = ['my_db']
        users = [{"name": "bob", "password": "x",
                  "databases": [{"name": "my_db"}]}]
        result = populate_users(users, initial_databases)
        self.assertThat(len(result), Is(1))

    def test_populate_users_db_multi_in_list(self):
        initial_databases = ['a_db', 'b_db', 'c_db', 'd_db']
        users = [{"name": "bob", "password": "x",
                  "databases": [{"name": "a_db"}]},
                 {"name": "tom", "password": "y",
                  "databases": [{"name": "c_db"}]},
                 {"name": "sue", "password": "z",
                  "databases": [{"name": "c_db"}]}]
        result = populate_users(users, initial_databases)
        self.assertThat(len(result), Is(3))

    def test_populate_users_db_not_in_list(self):
        initial_databases = ['a_db', 'b_db', 'c_db', 'd_db']
        users = [{"name": "bob", "password": "x",
                  "databases": [{"name": "fake_db"}]}]
        self.assertRaises(DatabaseForUserNotInDatabaseListError,
                          populate_users, users, initial_databases)

    def test_populate_users_db_multi_not_in_list(self):
        initial_databases = ['a_db', 'b_db', 'c_db', 'd_db']
        users = [{"name": "bob", "password": "x",
                  "databases": [{"name": "a_db"}]},
                 {"name": "tom", "password": "y",
                  "databases": [{"name": "fake_db"}]},
                 {"name": "sue", "password": "z",
                  "databases": [{"name": "d_db"}]}]
        self.assertRaises(DatabaseForUserNotInDatabaseListError,
                          populate_users, users, initial_databases)

########NEW FILE########
__FILENAME__ = test_user_controller
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
import jsonschema

from testtools import TestCase
from testtools.matchers import Is
from trove.extensions.mysql.service import UserController
from trove.extensions.mysql.service import UserAccessController
from trove.extensions.mysql.service import SchemaController


class TestUserController(TestCase):
    def setUp(self):
        super(TestUserController, self).setUp()
        self.controller = UserController()

    def test_get_create_schema(self):
        body = {'users': [{'name': 'test', 'password': 'test'}]}
        schema = self.controller.get_schema('create', body)
        self.assertTrue('users' in schema['properties'])

    def test_get_update_user_pw(self):
        body = {'users': [{'name': 'test', 'password': 'test'}]}
        schema = self.controller.get_schema('update_all', body)
        self.assertTrue('users' in schema['properties'])

    def test_get_update_user_db(self):
        body = {'databases': [{'name': 'test'}, {'name': 'test'}]}
        schema = self.controller.get_schema('update_all', body)
        self.assertTrue('databases' in schema['properties'])

    def test_validate_create_empty(self):
        body = {"users": []}
        schema = self.controller.get_schema('create', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertTrue(validator.is_valid(body))
        #TODO(zed): Restore after API version increment
        #errors = sorted(validator.iter_errors(body), key=lambda e: e.path)
        #self.assertThat(len(errors), Is(1))
        #self.assertThat(errors[0].message, Equals("[] is too short"))
        #self.assertThat(errors[0].path.pop(), Equals("users"))

    def test_validate_create_short_password(self):
        body = {"users": [{"name": "joe", "password": ""}]}
        schema = self.controller.get_schema('create', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertFalse(validator.is_valid(body))
        errors = sorted(validator.iter_errors(body), key=lambda e: e.path)
        error_messages = [error.message for error in errors]
        error_paths = [error.path.pop() for error in errors]
        self.assertThat(len(errors), Is(2))
        self.assertIn("'' is too short", error_messages)
        self.assertIn("'' does not match '^.*[0-9a-zA-Z]+.*$'", error_messages)
        self.assertIn("password", error_paths)

    def test_validate_create_no_password(self):
        body = {"users": [{"name": "joe"}]}
        schema = self.controller.get_schema('create', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertFalse(validator.is_valid(body))
        errors = sorted(validator.iter_errors(body), key=lambda e: e.path)
        error_messages = [error.message for error in errors]
        self.assertThat(len(errors), Is(1))
        self.assertIn("'password' is a required property", error_messages)

    def test_validate_create_short_name(self):
        body = {"users": [{"name": ""}]}
        schema = self.controller.get_schema('create', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertFalse(validator.is_valid(body))
        errors = sorted(validator.iter_errors(body), key=lambda e: e.path)
        error_messages = [error.message for error in errors]
        error_paths = [error.path.pop() for error in errors]
        self.assertThat(len(errors), Is(3))
        self.assertIn("'password' is a required property", error_messages)
        self.assertIn("'' is too short", error_messages)
        self.assertIn("'' does not match '^.*[0-9a-zA-Z]+.*$'", error_messages)
        self.assertIn("name", error_paths)

    def test_validate_create_complete_db_empty(self):
        body = {"users": [{"databases": [], "name": "joe", "password": "123"}]}
        schema = self.controller.get_schema('create', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertTrue(validator.is_valid(body))
        errors = sorted(validator.iter_errors(body), key=lambda e: e.path)
        self.assertThat(len(errors), Is(0))

    def test_validate_create_complete_db_no_name(self):
        body = {"users": [{"databases": [{}], "name": "joe",
                           "password": "123"}]}
        schema = self.controller.get_schema('create', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertFalse(validator.is_valid(body))
        errors = sorted(validator.iter_errors(body), key=lambda e: e.path)
        error_messages = [error.message for error in errors]
        self.assertThat(len(errors), Is(1))
        self.assertIn("'name' is a required property", error_messages)

    def test_validate_create_bogus_attr(self):
        body = {"users": [{"databases": [{"name": "x"}], "name": "joe",
                           "bogosity": 100,
                           "password": "123"}]}
        schema = self.controller.get_schema('create', body)
        validator = jsonschema.Draft4Validator(schema)
        #TODO(zed): After API increment, this will NOT be valid.
        self.assertTrue(validator.is_valid(body))

    def test_validate_create_complete_db(self):
        body = {"users": [{"databases": [{"name": "x"}], "name": "joe",
                           "password": "123"}]}
        schema = self.controller.get_schema('create', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertTrue(validator.is_valid(body))

    def test_validate_create_host_no_wildcard(self):
        body = {"users": [{"databases": [{"name": "x"}], "name": "joe",
                           "password": "123", "host": "192.168.1.1"}]}
        schema = self.controller.get_schema('create', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertTrue(validator.is_valid(body))

    def test_validate_create_host_wildcard(self):
        body = {"users": [{"databases": [{"name": "x"}], "name": "joe",
                           "password": "123", "host": "%"}]}
        schema = self.controller.get_schema('create', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertTrue(validator.is_valid(body))

    def test_validate_create_host_wildcard_prefix(self):
        body = {"users": [{"databases": [{"name": "x"}], "name": "joe",
                           "password": "123", "host": "%.168.1.1"}]}
        schema = self.controller.get_schema('create', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertTrue(validator.is_valid(body))

    def test_validate_create_host_wildcard_middle(self):
        body = {"users": [{"databases": [{"name": "x"}], "name": "joe",
                           "password": "123", "host": "192.%.1.1"}]}
        schema = self.controller.get_schema('create', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertFalse(validator.is_valid(body))
        errors = sorted(validator.iter_errors(body), key=lambda e: e.path)
        error_messages = [error.message for error in errors]
        self.assertThat(len(errors), Is(1))
        self.assertIn("'192.%.1.1' does not match '^[%]?[\\\\w(-).]*[%]?$'",
                      error_messages)

    def test_validate_create_host_wildcard_suffix(self):
        body = {"users": [{"databases": [{"name": "x"}], "name": "joe",
                           "password": "123", "host": "192.168.1.%"}]}
        schema = self.controller.get_schema('create', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertTrue(validator.is_valid(body))

    def test_validate_update_empty(self):
        body = {"users": []}
        schema = self.controller.get_schema('update_all', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertTrue(validator.is_valid(body))
        #TODO(zed): Restore after API version increment
        #errors = sorted(validator.iter_errors(body), key=lambda e: e.path)
        #self.assertThat(len(errors), Is(1))
        #self.assertThat(errors[0].message, Equals("[] is too short"))
        #self.assertThat(errors[0].path.pop(), Equals("users"))

    def test_validate_update_short_password(self):
        body = {"users": [{"name": "joe", "password": ""}]}
        schema = self.controller.get_schema('update_all', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertFalse(validator.is_valid(body))
        errors = sorted(validator.iter_errors(body), key=lambda e: e.path)
        error_messages = [error.message for error in errors]
        error_paths = [error.path.pop() for error in errors]
        self.assertThat(len(errors), Is(2))
        self.assertIn("'' is too short", error_messages)
        self.assertIn("'' does not match '^.*[0-9a-zA-Z]+.*$'", error_messages)
        self.assertIn("password", error_paths)

    def test_validate_update_user_complete(self):
        body = {"users": [{"name": "joe", "password": "",
                           "databases": [{"name": "testdb"}]}]}
        schema = self.controller.get_schema('update_all', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertFalse(validator.is_valid(body))
        errors = sorted(validator.iter_errors(body), key=lambda e: e.path)
        error_messages = [error.message for error in errors]
        error_paths = [error.path.pop() for error in errors]
        self.assertThat(len(errors), Is(2))
        self.assertIn("'' is too short", error_messages)
        self.assertIn("'' does not match '^.*[0-9a-zA-Z]+.*$'", error_messages)
        self.assertIn("password", error_paths)

    def test_validate_update_user_with_db_short_password(self):
        body = {"users": [{"name": "joe", "password": "",
                           "databases": [{"name": "testdb"}]}]}
        schema = self.controller.get_schema('update_all', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertFalse(validator.is_valid(body))
        errors = sorted(validator.iter_errors(body), key=lambda e: e.path)
        error_messages = [error.message for error in errors]
        error_paths = [error.path.pop() for error in errors]
        self.assertThat(len(errors), Is(2))
        self.assertIn("'' is too short", error_messages)
        self.assertIn("password", error_paths)

    def test_validate_update_no_password(self):
        body = {"users": [{"name": "joe"}]}
        schema = self.controller.get_schema('update_all', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertFalse(validator.is_valid(body))
        errors = sorted(validator.iter_errors(body), key=lambda e: e.path)
        error_messages = [error.message for error in errors]
        self.assertThat(len(errors), Is(1))
        self.assertIn("'password' is a required property", error_messages)

    def test_validate_update_database_complete(self):
        body = {"databases": [{"name": "test1"}, {"name": "test2"}]}
        schema = self.controller.get_schema('update_all', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertTrue(validator.is_valid(body))

    def test_validate_update_database_empty(self):
        body = {"databases": []}
        schema = self.controller.get_schema('update_all', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertTrue(validator.is_valid(body))
        #TODO(zed): Restore after API version increment
        #errors = sorted(validator.iter_errors(body), key=lambda e: e.path)
        #self.assertThat(len(errors), Is(1))
        #self.assertThat(errors[0].message, Equals('[] is too short'))

    def test_validate_update_short_name(self):
        body = {"users": [{"name": ""}]}
        schema = self.controller.get_schema('update_all', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertFalse(validator.is_valid(body))
        errors = sorted(validator.iter_errors(body), key=lambda e: e.path)
        error_messages = [error.message for error in errors]
        error_paths = [error.path.pop() for error in errors]
        self.assertThat(len(errors), Is(3))
        self.assertIn("'password' is a required property", error_messages)
        self.assertIn("'' is too short", error_messages)
        self.assertIn("'' does not match '^.*[0-9a-zA-Z]+.*$'", error_messages)
        self.assertIn("name", error_paths)

    def test_get_update_user_attributes(self):
        body = {'user': {'name': 'test'}}
        schema = self.controller.get_schema('update', body)
        self.assertTrue('user' in schema['properties'])

    def test_validate_update_user_attributes(self):
        body = {'user': {'name': 'test', 'password': 'test', 'host': '%'}}
        schema = self.controller.get_schema('update', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertTrue(validator.is_valid(body))

    def test_validate_update_user_attributes_empty(self):
        body = {"user": {}}
        schema = self.controller.get_schema('update', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertFalse(validator.is_valid(body))

    def test_validate_host_in_user_attributes(self):
        body_empty_host = {'user': {
            'name': 'test',
            'password': 'test',
            'host': '%'
        }}
        body_with_host = {'user': {
            'name': 'test',
            'password': 'test',
            'host': '1.1.1.1'
        }}
        body_none_host = {'user': {
            'name': 'test',
            'password': 'test',
            'host': ""
        }}

        schema_empty_host = self.controller.get_schema('update',
                                                       body_empty_host)
        schema_with_host = self.controller.get_schema('update',
                                                      body_with_host)
        schema_none_host = self.controller.get_schema('update', body_none_host)

        validator_empty_host = jsonschema.Draft4Validator(schema_empty_host)
        validator_with_host = jsonschema.Draft4Validator(schema_with_host)
        validator_none_host = jsonschema.Draft4Validator(schema_none_host)

        self.assertTrue(validator_empty_host.is_valid(body_empty_host))
        self.assertTrue(validator_with_host.is_valid(body_with_host))
        self.assertFalse(validator_none_host.is_valid(body_none_host))


class TestUserAccessController(TestCase):
    def test_validate_update_db(self):
        body = {"databases": []}
        schema = (UserAccessController()).get_schema('update_all', body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertTrue(validator.is_valid(body))
        #TODO(zed): Restore after API version increment
        #errors = sorted(validator.iter_errors(body), key=lambda e: e.path)
        #self.assertThat(len(errors), Is(1))
        #self.assertThat(errors[0].message, Equals("[] is too short"))
        #self.assertThat(errors[0].path.pop(), Equals("databases"))


class TestSchemaController(TestCase):
    def setUp(self):
        super(TestSchemaController, self).setUp()
        self.controller = SchemaController()
        self.body = {
            "databases": [
                {
                    "name": "first_db",
                    "collate": "latin2_general_ci",
                    "character_set": "latin2"
                },
                {
                    "name": "second_db"
                }
            ]
        }

    def test_validate_mixed(self):
        schema = self.controller.get_schema('create', self.body)
        self.assertNotEqual(schema, None)
        validator = jsonschema.Draft4Validator(schema)
        self.assertTrue(validator.is_valid(self.body))

    def test_validate_mixed_with_no_name(self):
        body = self.body.copy()
        body['databases'].append({"collate": "some_collation"})
        schema = self.controller.get_schema('create', body)
        self.assertNotEqual(schema, None)
        validator = jsonschema.Draft4Validator(schema)
        self.assertFalse(validator.is_valid(body))

    def test_validate_empty(self):
        body = {"databases": []}
        schema = self.controller.get_schema('create', body)
        self.assertNotEqual(schema, None)
        self.assertTrue('databases' in body)
        validator = jsonschema.Draft4Validator(schema)
        self.assertTrue(validator.is_valid(body))

########NEW FILE########
__FILENAME__ = test_quota
#    Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
import testtools
from mock import Mock, MagicMock, patch
from trove.quota.quota import DbQuotaDriver
from trove.quota.models import Resource
from trove.quota.models import Quota
from trove.quota.models import QuotaUsage
from trove.quota.models import Reservation
from trove.db.models import DatabaseModelBase
from trove.extensions.mgmt.quota.service import QuotaController
from trove.common import exception
from trove.common import cfg
from trove.quota.quota import run_with_quotas
from trove.quota.quota import QUOTAS
"""
Unit tests for the classes and functions in DbQuotaDriver.py.
"""

CONF = cfg.CONF
resources = {
    Resource.INSTANCES: Resource(Resource.INSTANCES, 'max_instances_per_user'),
    Resource.VOLUMES: Resource(Resource.VOLUMES, 'max_volumes_per_user'),
}

FAKE_TENANT1 = "123456"
FAKE_TENANT2 = "654321"


class Run_with_quotasTest(testtools.TestCase):

    def setUp(self):
        super(Run_with_quotasTest, self).setUp()
        self.quota_reserve_orig = QUOTAS.reserve
        self.quota_rollback_orig = QUOTAS.rollback
        self.quota_commit_orig = QUOTAS.commit
        QUOTAS.reserve = Mock()
        QUOTAS.rollback = Mock()
        QUOTAS.commit = Mock()

    def tearDown(self):
        super(Run_with_quotasTest, self).tearDown()
        QUOTAS.reserve = self.quota_reserve_orig
        QUOTAS.rollback = self.quota_rollback_orig
        QUOTAS.commit = self.quota_commit_orig

    def test_run_with_quotas(self):

        f = Mock()
        run_with_quotas(FAKE_TENANT1, {'instances': 1, 'volumes': 5}, f)

        self.assertTrue(QUOTAS.reserve.called)
        self.assertTrue(QUOTAS.commit.called)
        self.assertFalse(QUOTAS.rollback.called)
        self.assertTrue(f.called)

    def test_run_with_quotas_error(self):

        f = Mock(side_effect=exception.TroveError())

        self.assertRaises(exception.TroveError, run_with_quotas, FAKE_TENANT1,
                          {'instances': 1, 'volumes': 5}, f)
        self.assertTrue(QUOTAS.reserve.called)
        self.assertTrue(QUOTAS.rollback.called)
        self.assertFalse(QUOTAS.commit.called)
        self.assertTrue(f.called)


class QuotaControllerTest(testtools.TestCase):

    def setUp(self):
        super(QuotaControllerTest, self).setUp()
        context = MagicMock()
        context.is_admin = True
        req = MagicMock()
        req.environ = MagicMock()
        req.environ.get = MagicMock(return_value=context)
        self.req = req
        self.controller = QuotaController()

    def tearDown(self):
        super(QuotaControllerTest, self).tearDown()

    def test_update_unknown_resource(self):
        body = {'quotas': {'unknown_resource': 5}}
        self.assertRaises(exception.QuotaResourceUnknown,
                          self.controller.update, self.req, body,
                          FAKE_TENANT1, FAKE_TENANT2)

    def test_update_resource_no_value(self):
        quota = MagicMock(spec=Quota)
        with patch.object(DatabaseModelBase, 'find_by', return_value=quota):
            body = {'quotas': {'instances': None}}
            result = self.controller.update(self.req, body, FAKE_TENANT1,
                                            FAKE_TENANT2)
            self.assertEqual(quota.save.call_count, 0)
            self.assertEqual(200, result.status)

    def test_update_resource_instance(self):
        instance_quota = MagicMock(spec=Quota)
        with patch.object(DatabaseModelBase, 'find_by',
                          return_value=instance_quota):
            body = {'quotas': {'instances': 2}}
            result = self.controller.update(self.req, body, FAKE_TENANT1,
                                            FAKE_TENANT2)
            self.assertEqual(instance_quota.save.call_count, 1)
            self.assertTrue('instances' in result._data['quotas'])
            self.assertEqual(200, result.status)
            self.assertEqual(2, result._data['quotas']['instances'])

    @testtools.skipIf(not CONF.trove_volume_support,
                      'Volume support is not enabled')
    def test_update_resource_volume(self):
        instance_quota = MagicMock(spec=Quota)
        volume_quota = MagicMock(spec=Quota)

        def side_effect_func(*args, **kwargs):
            return (instance_quota if kwargs['resource'] == 'instances'
                    else volume_quota)

        with patch.object(DatabaseModelBase, 'find_by',
                          side_effect=side_effect_func):
            body = {'quotas': {'instances': None, 'volumes': 10}}
            result = self.controller.update(self.req, body, FAKE_TENANT1,
                                            FAKE_TENANT2)
            self.assertEqual(instance_quota.save.call_count, 0)
            self.assertFalse('instances' in result._data['quotas'])
            self.assertEqual(volume_quota.save.call_count, 1)
            self.assertEqual(200, result.status)
            self.assertEqual(10, result._data['quotas']['volumes'])


class DbQuotaDriverTest(testtools.TestCase):

    def setUp(self):

        super(DbQuotaDriverTest, self).setUp()
        self.driver = DbQuotaDriver(resources)
        self.orig_Quota_find_all = Quota.find_all
        self.orig_QuotaUsage_find_all = QuotaUsage.find_all
        self.orig_QuotaUsage_find_by = QuotaUsage.find_by
        self.orig_Reservation_create = Reservation.create
        self.orig_QuotaUsage_create = QuotaUsage.create
        self.orig_QuotaUsage_save = QuotaUsage.save
        self.orig_Reservation_save = Reservation.save
        self.mock_quota_result = Mock()
        self.mock_usage_result = Mock()
        Quota.find_all = Mock(return_value=self.mock_quota_result)
        QuotaUsage.find_all = Mock(return_value=self.mock_usage_result)

    def tearDown(self):
        super(DbQuotaDriverTest, self).tearDown()
        Quota.find_all = self.orig_Quota_find_all
        QuotaUsage.find_all = self.orig_QuotaUsage_find_all
        QuotaUsage.find_by = self.orig_QuotaUsage_find_by
        Reservation.create = self.orig_Reservation_create
        QuotaUsage.create = self.orig_QuotaUsage_create
        QuotaUsage.save = self.orig_QuotaUsage_save
        Reservation.save = self.orig_Reservation_save

    def test_get_defaults(self):
        defaults = self.driver.get_defaults(resources)
        self.assertEqual(CONF.max_instances_per_user,
                         defaults[Resource.INSTANCES])
        self.assertEqual(CONF.max_volumes_per_user,
                         defaults[Resource.VOLUMES])

    def test_get_quota_by_tenant(self):

        FAKE_QUOTAS = [Quota(tenant_id=FAKE_TENANT1,
                             resource=Resource.INSTANCES,
                             hard_limit=12)]

        self.mock_quota_result.all = Mock(return_value=FAKE_QUOTAS)

        quota = self.driver.get_quota_by_tenant(FAKE_TENANT1,
                                                Resource.VOLUMES)

        self.assertEqual(FAKE_TENANT1, quota.tenant_id)
        self.assertEqual(Resource.INSTANCES, quota.resource)
        self.assertEqual(12, quota.hard_limit)

    def test_get_quota_by_tenant_default(self):

        self.mock_quota_result.all = Mock(return_value=[])

        quota = self.driver.get_quota_by_tenant(FAKE_TENANT1,
                                                Resource.VOLUMES)

        self.assertEqual(FAKE_TENANT1, quota.tenant_id)
        self.assertEqual(Resource.VOLUMES, quota.resource)
        self.assertEqual(CONF.max_volumes_per_user, quota.hard_limit)

    def test_get_all_quotas_by_tenant(self):

        FAKE_QUOTAS = [Quota(tenant_id=FAKE_TENANT1,
                             resource=Resource.INSTANCES,
                             hard_limit=22),
                       Quota(tenant_id=FAKE_TENANT1,
                             resource=Resource.VOLUMES,
                             hard_limit=15)]

        self.mock_quota_result.all = Mock(return_value=FAKE_QUOTAS)

        quotas = self.driver.get_all_quotas_by_tenant(FAKE_TENANT1,
                                                      resources.keys())

        self.assertEqual(FAKE_TENANT1, quotas[Resource.INSTANCES].tenant_id)
        self.assertEqual(Resource.INSTANCES,
                         quotas[Resource.INSTANCES].resource)
        self.assertEqual(22, quotas[Resource.INSTANCES].hard_limit)
        self.assertEqual(FAKE_TENANT1, quotas[Resource.VOLUMES].tenant_id)
        self.assertEqual(Resource.VOLUMES, quotas[Resource.VOLUMES].resource)
        self.assertEqual(15, quotas[Resource.VOLUMES].hard_limit)

    def test_get_all_quotas_by_tenant_with_all_default(self):

        self.mock_quota_result.all = Mock(return_value=[])

        quotas = self.driver.get_all_quotas_by_tenant(FAKE_TENANT1,
                                                      resources.keys())

        self.assertEqual(FAKE_TENANT1, quotas[Resource.INSTANCES].tenant_id)
        self.assertEqual(Resource.INSTANCES,
                         quotas[Resource.INSTANCES].resource)
        self.assertEqual(CONF.max_instances_per_user,
                         quotas[Resource.INSTANCES].hard_limit)
        self.assertEqual(FAKE_TENANT1, quotas[Resource.VOLUMES].tenant_id)
        self.assertEqual(Resource.VOLUMES, quotas[Resource.VOLUMES].resource)
        self.assertEqual(CONF.max_volumes_per_user,
                         quotas[Resource.VOLUMES].hard_limit)

    def test_get_all_quotas_by_tenant_with_one_default(self):

        FAKE_QUOTAS = [Quota(tenant_id=FAKE_TENANT1,
                             resource=Resource.INSTANCES,
                             hard_limit=22)]

        self.mock_quota_result.all = Mock(return_value=FAKE_QUOTAS)

        quotas = self.driver.get_all_quotas_by_tenant(FAKE_TENANT1,
                                                      resources.keys())

        self.assertEqual(FAKE_TENANT1, quotas[Resource.INSTANCES].tenant_id)
        self.assertEqual(Resource.INSTANCES,
                         quotas[Resource.INSTANCES].resource)
        self.assertEqual(22, quotas[Resource.INSTANCES].hard_limit)
        self.assertEqual(FAKE_TENANT1, quotas[Resource.VOLUMES].tenant_id)
        self.assertEqual(Resource.VOLUMES, quotas[Resource.VOLUMES].resource)
        self.assertEqual(CONF.max_volumes_per_user,
                         quotas[Resource.VOLUMES].hard_limit)

    def test_get_quota_usage_by_tenant(self):

        FAKE_QUOTAS = [QuotaUsage(tenant_id=FAKE_TENANT1,
                                  resource=Resource.VOLUMES,
                                  in_use=3,
                                  reserved=1)]

        self.mock_usage_result.all = Mock(return_value=FAKE_QUOTAS)

        usage = self.driver.get_quota_usage_by_tenant(FAKE_TENANT1,
                                                      Resource.VOLUMES)

        self.assertEqual(FAKE_TENANT1, usage.tenant_id)
        self.assertEqual(Resource.VOLUMES, usage.resource)
        self.assertEqual(3, usage.in_use)
        self.assertEqual(1, usage.reserved)

    def test_get_quota_usage_by_tenant_default(self):

        FAKE_QUOTA = QuotaUsage(tenant_id=FAKE_TENANT1,
                                resource=Resource.VOLUMES,
                                in_use=0,
                                reserved=0)

        self.mock_usage_result.all = Mock(return_value=[])
        QuotaUsage.create = Mock(return_value=FAKE_QUOTA)

        usage = self.driver.get_quota_usage_by_tenant(FAKE_TENANT1,
                                                      Resource.VOLUMES)

        self.assertEqual(FAKE_TENANT1, usage.tenant_id)
        self.assertEqual(Resource.VOLUMES, usage.resource)
        self.assertEqual(0, usage.in_use)
        self.assertEqual(0, usage.reserved)

    def test_get_all_quota_usages_by_tenant(self):

        FAKE_QUOTAS = [QuotaUsage(tenant_id=FAKE_TENANT1,
                                  resource=Resource.INSTANCES,
                                  in_use=2,
                                  reserved=1),
                       QuotaUsage(tenant_id=FAKE_TENANT1,
                                  resource=Resource.VOLUMES,
                                  in_use=1,
                                  reserved=1)]

        self.mock_usage_result.all = Mock(return_value=FAKE_QUOTAS)

        usages = self.driver.get_all_quota_usages_by_tenant(FAKE_TENANT1,
                                                            resources.keys())

        self.assertEqual(FAKE_TENANT1, usages[Resource.INSTANCES].tenant_id)
        self.assertEqual(Resource.INSTANCES,
                         usages[Resource.INSTANCES].resource)
        self.assertEqual(2, usages[Resource.INSTANCES].in_use)
        self.assertEqual(1, usages[Resource.INSTANCES].reserved)
        self.assertEqual(FAKE_TENANT1, usages[Resource.VOLUMES].tenant_id)
        self.assertEqual(Resource.VOLUMES, usages[Resource.VOLUMES].resource)
        self.assertEqual(1, usages[Resource.VOLUMES].in_use)
        self.assertEqual(1, usages[Resource.VOLUMES].reserved)

    def test_get_all_quota_usages_by_tenant_with_all_default(self):

        FAKE_QUOTAS = [QuotaUsage(tenant_id=FAKE_TENANT1,
                                  resource=Resource.INSTANCES,
                                  in_use=0,
                                  reserved=0),
                       QuotaUsage(tenant_id=FAKE_TENANT1,
                                  resource=Resource.VOLUMES,
                                  in_use=0,
                                  reserved=0)]

        def side_effect_func(*args, **kwargs):
            return (FAKE_QUOTAS[0] if kwargs['resource'] == 'instances'
                    else FAKE_QUOTAS[1])

        self.mock_usage_result.all = Mock(return_value=[])
        QuotaUsage.create = Mock(side_effect=side_effect_func)

        usages = self.driver.get_all_quota_usages_by_tenant(FAKE_TENANT1,
                                                            resources.keys())

        self.assertEqual(FAKE_TENANT1, usages[Resource.INSTANCES].tenant_id)
        self.assertEqual(Resource.INSTANCES,
                         usages[Resource.INSTANCES].resource)
        self.assertEqual(0, usages[Resource.INSTANCES].in_use)
        self.assertEqual(0, usages[Resource.INSTANCES].reserved)
        self.assertEqual(FAKE_TENANT1, usages[Resource.VOLUMES].tenant_id)
        self.assertEqual(Resource.VOLUMES, usages[Resource.VOLUMES].resource)
        self.assertEqual(0, usages[Resource.VOLUMES].in_use)
        self.assertEqual(0, usages[Resource.VOLUMES].reserved)

    def test_get_all_quota_usages_by_tenant_with_one_default(self):

        FAKE_QUOTAS = [QuotaUsage(tenant_id=FAKE_TENANT1,
                                  resource=Resource.INSTANCES,
                                  in_use=0,
                                  reserved=0)]

        NEW_FAKE_QUOTA = QuotaUsage(tenant_id=FAKE_TENANT1,
                                    resource=Resource.VOLUMES,
                                    in_use=0,
                                    reserved=0)
        self.mock_usage_result.all = Mock(return_value=FAKE_QUOTAS)
        QuotaUsage.create = Mock(return_value=NEW_FAKE_QUOTA)

        usages = self.driver.get_all_quota_usages_by_tenant(FAKE_TENANT1,
                                                            resources.keys())

        self.assertEqual(FAKE_TENANT1, usages[Resource.INSTANCES].tenant_id)
        self.assertEqual(Resource.INSTANCES,
                         usages[Resource.INSTANCES].resource)
        self.assertEqual(0, usages[Resource.INSTANCES].in_use)
        self.assertEqual(0, usages[Resource.INSTANCES].reserved)
        self.assertEqual(FAKE_TENANT1, usages[Resource.VOLUMES].tenant_id)
        self.assertEqual(Resource.VOLUMES, usages[Resource.VOLUMES].resource)
        self.assertEqual(0, usages[Resource.VOLUMES].in_use)
        self.assertEqual(0, usages[Resource.VOLUMES].reserved)

    def test_reserve(self):

        FAKE_QUOTAS = [QuotaUsage(id=1,
                                  tenant_id=FAKE_TENANT1,
                                  resource=Resource.INSTANCES,
                                  in_use=1,
                                  reserved=2),
                       QuotaUsage(id=2,
                                  tenant_id=FAKE_TENANT1,
                                  resource=Resource.VOLUMES,
                                  in_use=1,
                                  reserved=1)]

        self.mock_quota_result.all = Mock(return_value=[])
        self.mock_usage_result.all = Mock(return_value=FAKE_QUOTAS)
        QuotaUsage.save = Mock()
        Reservation.create = Mock()

        delta = {'instances': 2, 'volumes': 3}
        self.driver.reserve(FAKE_TENANT1, resources, delta)
        _, kw = Reservation.create.call_args_list[0]
        self.assertEqual(1, kw['usage_id'])
        self.assertEqual(2, kw['delta'])
        self.assertEqual(Reservation.Statuses.RESERVED, kw['status'])
        _, kw = Reservation.create.call_args_list[1]
        self.assertEqual(2, kw['usage_id'])
        self.assertEqual(3, kw['delta'])
        self.assertEqual(Reservation.Statuses.RESERVED, kw['status'])

    def test_reserve_resource_unknown(self):

        delta = {'instances': 10, 'volumes': 2000, 'Fake_resource': 123}
        self.assertRaises(exception.QuotaResourceUnknown,
                          self.driver.reserve,
                          FAKE_TENANT1,
                          resources,
                          delta)

    def test_reserve_over_quota(self):

        FAKE_QUOTAS = [QuotaUsage(id=1,
                                  tenant_id=FAKE_TENANT1,
                                  resource=Resource.INSTANCES,
                                  in_use=0,
                                  reserved=0),
                       QuotaUsage(id=2,
                                  tenant_id=FAKE_TENANT1,
                                  resource=Resource.VOLUMES,
                                  in_use=0,
                                  reserved=0)]

        self.mock_quota_result.all = Mock(return_value=[])
        self.mock_usage_result.all = Mock(return_value=FAKE_QUOTAS)

        delta = {'instances': 1, 'volumes': CONF.max_volumes_per_user + 1}
        self.assertRaises(exception.QuotaExceeded,
                          self.driver.reserve,
                          FAKE_TENANT1,
                          resources,
                          delta)

    def test_reserve_over_quota_with_usage(self):

        FAKE_QUOTAS = [QuotaUsage(id=1,
                                  tenant_id=FAKE_TENANT1,
                                  resource=Resource.INSTANCES,
                                  in_use=1,
                                  reserved=0),
                       QuotaUsage(id=2,
                                  tenant_id=FAKE_TENANT1,
                                  resource=Resource.VOLUMES,
                                  in_use=0,
                                  reserved=0)]

        self.mock_quota_result.all = Mock(return_value=[])
        self.mock_usage_result.all = Mock(return_value=FAKE_QUOTAS)

        delta = {'instances': 5, 'volumes': 3}
        self.assertRaises(exception.QuotaExceeded,
                          self.driver.reserve,
                          FAKE_TENANT1,
                          resources,
                          delta)

    def test_reserve_over_quota_with_reserved(self):

        FAKE_QUOTAS = [QuotaUsage(id=1,
                                  tenant_id=FAKE_TENANT1,
                                  resource=Resource.INSTANCES,
                                  in_use=1,
                                  reserved=2),
                       QuotaUsage(id=2,
                                  tenant_id=FAKE_TENANT1,
                                  resource=Resource.VOLUMES,
                                  in_use=0,
                                  reserved=0)]

        self.mock_quota_result.all = Mock(return_value=[])
        self.mock_usage_result.all = Mock(return_value=FAKE_QUOTAS)

        delta = {'instances': 4, 'volumes': 2}
        self.assertRaises(exception.QuotaExceeded,
                          self.driver.reserve,
                          FAKE_TENANT1,
                          resources,
                          delta)

    def test_reserve_over_quota_but_can_apply_negative_deltas(self):

        FAKE_QUOTAS = [QuotaUsage(id=1,
                                  tenant_id=FAKE_TENANT1,
                                  resource=Resource.INSTANCES,
                                  in_use=10,
                                  reserved=0),
                       QuotaUsage(id=2,
                                  tenant_id=FAKE_TENANT1,
                                  resource=Resource.VOLUMES,
                                  in_use=50,
                                  reserved=0)]

        self.mock_quota_result.all = Mock(return_value=[])
        self.mock_usage_result.all = Mock(return_value=FAKE_QUOTAS)

        QuotaUsage.save = Mock()
        Reservation.create = Mock()

        delta = {'instances': -1, 'volumes': -3}
        self.driver.reserve(FAKE_TENANT1, resources, delta)
        _, kw = Reservation.create.call_args_list[0]
        self.assertEqual(1, kw['usage_id'])
        self.assertEqual(-1, kw['delta'])
        self.assertEqual(Reservation.Statuses.RESERVED, kw['status'])
        _, kw = Reservation.create.call_args_list[1]
        self.assertEqual(2, kw['usage_id'])
        self.assertEqual(-3, kw['delta'])
        self.assertEqual(Reservation.Statuses.RESERVED, kw['status'])

    def test_commit(self):

        Reservation.save = Mock()
        QuotaUsage.save = Mock()

        FAKE_QUOTAS = [QuotaUsage(id=1,
                                  tenant_id=FAKE_TENANT1,
                                  resource=Resource.INSTANCES,
                                  in_use=5,
                                  reserved=2),
                       QuotaUsage(id=2,
                                  tenant_id=FAKE_TENANT1,
                                  resource=Resource.VOLUMES,
                                  in_use=1,
                                  reserved=2)]

        FAKE_RESERVATIONS = [Reservation(usage_id=1,
                                         delta=1,
                                         status=Reservation.Statuses.RESERVED),
                             Reservation(usage_id=2,
                                         delta=2,
                                         status=Reservation.Statuses.RESERVED)]

        QuotaUsage.find_by = Mock(side_effect=FAKE_QUOTAS)
        self.driver.commit(FAKE_RESERVATIONS)

        self.assertEqual(6, FAKE_QUOTAS[0].in_use)
        self.assertEqual(1, FAKE_QUOTAS[0].reserved)
        self.assertEqual(Reservation.Statuses.COMMITTED,
                         FAKE_RESERVATIONS[0].status)

        self.assertEqual(3, FAKE_QUOTAS[1].in_use)
        self.assertEqual(0, FAKE_QUOTAS[1].reserved)
        self.assertEqual(Reservation.Statuses.COMMITTED,
                         FAKE_RESERVATIONS[1].status)

    def test_commit_cannot_be_less_than_zero(self):

        Reservation.save = Mock()
        QuotaUsage.save = Mock()

        FAKE_QUOTAS = [QuotaUsage(id=1,
                                  tenant_id=FAKE_TENANT1,
                                  resource=Resource.INSTANCES,
                                  in_use=0,
                                  reserved=-1)]

        FAKE_RESERVATIONS = [Reservation(usage_id=1,
                                         delta=-1,
                                         status=Reservation.Statuses.RESERVED)]

        QuotaUsage.find_by = Mock(side_effect=FAKE_QUOTAS)
        self.driver.commit(FAKE_RESERVATIONS)

        self.assertEqual(0, FAKE_QUOTAS[0].in_use)
        self.assertEqual(0, FAKE_QUOTAS[0].reserved)
        self.assertEqual(Reservation.Statuses.COMMITTED,
                         FAKE_RESERVATIONS[0].status)

    def test_rollback(self):

        Reservation.save = Mock()
        QuotaUsage.save = Mock()

        FAKE_QUOTAS = [QuotaUsage(id=1,
                                  tenant_id=FAKE_TENANT1,
                                  resource=Resource.INSTANCES,
                                  in_use=5,
                                  reserved=2),
                       QuotaUsage(id=2,
                                  tenant_id=FAKE_TENANT1,
                                  resource=Resource.VOLUMES,
                                  in_use=1,
                                  reserved=2)]

        FAKE_RESERVATIONS = [Reservation(usage_id=1,
                                         delta=1,
                                         status=Reservation.Statuses.RESERVED),
                             Reservation(usage_id=2,
                                         delta=2,
                                         status=Reservation.Statuses.RESERVED)]

        QuotaUsage.find_by = Mock(side_effect=FAKE_QUOTAS)
        self.driver.rollback(FAKE_RESERVATIONS)

        self.assertEqual(5, FAKE_QUOTAS[0].in_use)
        self.assertEqual(1, FAKE_QUOTAS[0].reserved)
        self.assertEqual(Reservation.Statuses.ROLLEDBACK,
                         FAKE_RESERVATIONS[0].status)

        self.assertEqual(1, FAKE_QUOTAS[1].in_use)
        self.assertEqual(0, FAKE_QUOTAS[1].reserved)
        self.assertEqual(Reservation.Statuses.ROLLEDBACK,
                         FAKE_RESERVATIONS[1].status)

########NEW FILE########
__FILENAME__ = test_router
#    Copyright 2013 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import testtools

from trove.common.wsgi import Router, Fault

from routes import Mapper


class FakeRequst(object):
    """A fake webob request object designed to cause 404.

    The dispatcher actually checks if the given request is a dict and throws
    an error if it is. This object wrapper tricks the dispatcher into
    handling the request like a regular request.
    """

    environ = {
        "wsgiorg.routing_args": [
            False,
            False
        ]
    }


class TestRouter(testtools.TestCase):
    """Test case for trove `Router` extensions."""

    def setUp(self):
        super(TestRouter, self).setUp()
        self.mapper = Mapper()

    def test_404_is_fault(self):
        """Test that the dispatcher wraps 404's in a `Fault`."""

        fake_request = FakeRequst()

        response = Router._dispatch(fake_request)

        assert isinstance(response, Fault)

########NEW FILE########
__FILENAME__ = test_security_group
#    Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import testtools
import uuid
import trove.common.remote
from mock import Mock
from trove.common import exception
from trove.tests.fakes import nova
from trove.extensions.security_group import models as sec_mod
from trove.instance import models as inst_model
from novaclient import exceptions as nova_exceptions


"""
Unit tests for testing the exceptions raised by Security Groups
"""


class Security_Group_Exceptions_Test(testtools.TestCase):

    def setUp(self):
        super(Security_Group_Exceptions_Test, self).setUp()
        self.createNovaClient = trove.common.remote.create_nova_client
        self.context = Mock()
        self.FakeClient = nova.fake_create_nova_client(self.context)

        fException = Mock(side_effect=
                          lambda *args, **kwargs:
                          self._raise(nova_exceptions.ClientException("Test")))

        self.FakeClient.security_groups.create = fException
        self.FakeClient.security_groups.delete = fException
        self.FakeClient.security_group_rules.create = fException
        self.FakeClient.security_group_rules.delete = fException

        trove.common.remote.create_nova_client = (
            lambda c: self._return_mocked_nova_client(c))

    def tearDown(self):
        super(Security_Group_Exceptions_Test, self).tearDown()
        trove.common.remote.create_nova_client = self.createNovaClient

    def _return_mocked_nova_client(self, context):
        return self.FakeClient

    def _raise(self, ex):
        raise ex

    def test_failed_to_create_security_group(self):
        self.assertRaises(exception.SecurityGroupCreationError,
                          sec_mod.RemoteSecurityGroup.create,
                          "TestName",
                          "TestDescription",
                          self.context)

    def test_failed_to_delete_security_group(self):
        self.assertRaises(exception.SecurityGroupDeletionError,
                          sec_mod.RemoteSecurityGroup.delete,
                          1, self.context)

    def test_failed_to_create_security_group_rule(self):
        self.assertRaises(exception.SecurityGroupRuleCreationError,
                          sec_mod.RemoteSecurityGroup.add_rule,
                          1, "tcp", 3306, 3306, "0.0.0.0/0", self.context)

    def test_failed_to_delete_security_group_rule(self):
        self.assertRaises(exception.SecurityGroupRuleDeletionError,
                          sec_mod.RemoteSecurityGroup.delete_rule,
                          1, self.context)


class fake_RemoteSecGr(object):
    def data(self):
        self.id = uuid.uuid4()
        return {'id': self.id}

    def delete(self, context):
        pass


class fake_SecGr_Association(object):
    def get_security_group(self):
        return fake_RemoteSecGr()

    def delete(self):
        pass


class SecurityGroupDeleteTest(testtools.TestCase):

    def setUp(self):
        super(SecurityGroupDeleteTest, self).setUp()
        inst_model.CONF = Mock()
        self.context = Mock()
        self. original_find_by = (sec_mod.
                                  SecurityGroupInstanceAssociation.
                                  find_by)
        self.original_delete = sec_mod.SecurityGroupInstanceAssociation.delete
        self.fException = Mock(side_effect=
                               lambda *args, **kwargs:
                               self._raise(exception.ModelNotFoundError()))

    def tearDown(self):
        super(SecurityGroupDeleteTest, self).tearDown()
        (sec_mod.SecurityGroupInstanceAssociation.
         find_by) = self.original_find_by
        (sec_mod.SecurityGroupInstanceAssociation.
         delete) = self.original_delete

    def _raise(self, ex):
        raise ex

    def test_failed_to_get_assoc_on_delete(self):

        sec_mod.SecurityGroupInstanceAssociation.find_by = self.fException
        self.assertEqual(None,
                         sec_mod.SecurityGroup.delete_for_instance(
                             uuid.uuid4(), self.context))

    def test_get_security_group_from_assoc_with_db_exception(self):

        fException = Mock(side_effect=
                          lambda *args, **kwargs:
                          self._raise(nova_exceptions.
                                      ClientException('TEST')))
        i_id = uuid.uuid4()

        class new_fake_RemoteSecGrAssoc(object):

            def get_security_group(self):
                return None

            def delete(self):
                return fException

        sec_mod.SecurityGroupInstanceAssociation.find_by = Mock(
            return_value=new_fake_RemoteSecGrAssoc())
        self.assertEqual(sec_mod.SecurityGroup.delete_for_instance(
            i_id, self.context), None)

    def test_delete_secgr_assoc_with_db_exception(self):

        i_id = uuid.uuid4()
        sec_mod.SecurityGroupInstanceAssociation.find_by = Mock(
            return_value=fake_SecGr_Association())
        sec_mod.SecurityGroupInstanceAssociation.delete = self.fException
        self.assertNotEqual(sec_mod.SecurityGroupInstanceAssociation.find_by(
            i_id, deleted=False).get_security_group(), None)
        self.assertTrue(hasattr(sec_mod.SecurityGroupInstanceAssociation.
                                find_by(i_id, deleted=False).
                                get_security_group(), 'delete'))
        self.assertEqual(None,
                         sec_mod.SecurityGroup.delete_for_instance(
                             i_id, self.context))

########NEW FILE########
__FILENAME__ = test_models
#    Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
import datetime

import testtools
from mock import Mock, MagicMock, patch
from testtools.matchers import Equals, Is
from cinderclient import exceptions as cinder_exceptions
from novaclient import exceptions as nova_exceptions
import novaclient.v1_1.servers
import novaclient.v1_1.flavors
import cinderclient.v2.client as cinderclient
import trove.backup.models
import trove.common.context
from trove.datastore import models as datastore_models
import trove.db.models
from trove.taskmanager import models as taskmanager_models
import trove.guestagent.api
from trove.backup import models as backup_models
from trove.common import remote
from trove.common.exception import GuestError
from trove.common.exception import PollTimeOut
from trove.common.exception import TroveError
from trove.common.exception import MalformedSecurityGroupRuleError
from trove.common.instance import ServiceStatuses
from trove.extensions.mysql import models as mysql_models
from trove.instance.models import InstanceServiceStatus
from trove.instance.models import InstanceStatus
from trove.instance.models import DBInstance
from trove.instance.tasks import InstanceTasks

from trove.tests.unittests.util import util
from trove.common import utils
from trove.openstack.common import timeutils
from swiftclient.client import ClientException
from tempfile import NamedTemporaryFile
import os
import trove.common.template as template
import uuid

INST_ID = 'dbinst-id-1'
VOLUME_ID = 'volume-id-1'


class FakeOptGroup(object):
    def __init__(self, tcp_ports=['3306', '3301-3307'],
                 udp_ports=[]):
        self.tcp_ports = tcp_ports
        self.udp_ports = udp_ports


class fake_Server:
    def __init__(self):
        self.id = None
        self.name = None
        self.image_id = None
        self.flavor_id = None
        self.files = None
        self.userdata = None
        self.security_groups = None
        self.block_device_mapping = None
        self.status = 'ACTIVE'


class fake_ServerManager:
    def create(self, name, image_id, flavor_id, files, userdata,
               security_groups, block_device_mapping, availability_zone=None,
               nics=None):
        server = fake_Server()
        server.id = "server_id"
        server.name = name
        server.image_id = image_id
        server.flavor_id = flavor_id
        server.files = files
        server.userdata = userdata
        server.security_groups = security_groups
        server.block_device_mapping = block_device_mapping
        server.availability_zone = availability_zone
        server.nics = nics
        return server


class fake_nova_client:
    def __init__(self):
        self.servers = fake_ServerManager()


class fake_InstanceServiceStatus(object):

    _instance = None

    def __init__(self):
        self.deleted = False
        self.status = None
        pass

    def set_status(self, status):
        self.status = status
        pass

    def get_status(self):
        return self.status

    @classmethod
    def find_by(cls, **kwargs):
        if not cls._instance:
            cls._instance = fake_InstanceServiceStatus()
        return cls._instance

    def save(self):
        pass

    def delete(self):
        self.deleted = True
        pass

    def is_deleted(self):
        return self.deleted


class fake_DBInstance(object):

    _instance = None

    def __init__(self):
        self.deleted = False
        pass

    @classmethod
    def find_by(cls, **kwargs):
        if not cls._instance:
            cls._instance = fake_DBInstance()
        return cls._instance

    def set_task_status(self, status):
        self.status = status
        pass

    def get_task_status(self):
        return self.status

    def save(self):
        pass

    def delete(self):
        self.deleted = True
        pass

    def is_deleted(self):
        return self.deleted


class FreshInstanceTasksTest(testtools.TestCase):
    def setUp(self):
        super(FreshInstanceTasksTest, self).setUp()
        mock_instance = patch('trove.instance.models.FreshInstance')
        mock_instance.start()
        self.addCleanup(mock_instance.stop)
        mock_instance.id = Mock(return_value='instance_id')
        mock_instance.tenant_id = Mock(return_value="tenant_id")
        mock_instance.hostname = Mock(return_value="hostname")
        mock_instance.name = Mock(return_value='name')
        mock_instance.nova_client = Mock(
            return_value=fake_nova_client())
        mock_datastore_v = patch(
            'trove.datastore.models.DatastoreVersion')
        mock_datastore_v.start()
        self.addCleanup(mock_datastore_v.stop)
        mock_datastore = patch(
            'trove.datastore.models.Datastore')
        mock_datastore.start()
        self.addCleanup(mock_datastore.stop)

        taskmanager_models.FreshInstanceTasks.nova_client = fake_nova_client()
        self.orig_ISS_find_by = InstanceServiceStatus.find_by
        self.orig_DBI_find_by = DBInstance.find_by
        self.userdata = "hello moto"
        self.guestconfig_content = "guest config"
        with NamedTemporaryFile(suffix=".cloudinit", delete=False) as f:
            self.cloudinit = f.name
            f.write(self.userdata)
        with NamedTemporaryFile(delete=False) as f:
            self.guestconfig = f.name
            f.write(self.guestconfig_content)
        self.freshinstancetasks = taskmanager_models.FreshInstanceTasks(
            None, Mock(), None, None)

    def tearDown(self):
        super(FreshInstanceTasksTest, self).tearDown()
        os.remove(self.cloudinit)
        os.remove(self.guestconfig)
        InstanceServiceStatus.find_by = self.orig_ISS_find_by
        DBInstance.find_by = self.orig_DBI_find_by

    @patch('trove.taskmanager.models.CONF')
    def test_create_instance_userdata(self, mock_conf):
        cloudinit_location = os.path.dirname(self.cloudinit)
        datastore_manager = os.path.splitext(os.path.basename(self.
                                                              cloudinit))[0]

        def fake_conf_getter(*args, **kwargs):
            if args[0] == 'cloudinit_location':
                return cloudinit_location
            else:
                return ''
        mock_conf.get.side_effect = fake_conf_getter

        server = self.freshinstancetasks._create_server(
            None, None, None, datastore_manager, None, None, None)
        self.assertEqual(server.userdata, self.userdata)

    @patch('trove.taskmanager.models.CONF')
    def test_create_instance_guestconfig(self, mock_conf):
        def fake_conf_getter(*args, **kwargs):
            if args[0] == 'guest_config':
                return self.guestconfig
            else:
                return ''
        mock_conf.get.side_effect = fake_conf_getter
        # execute
        server = self.freshinstancetasks._create_server(
            None, None, None, "test", None, None, None)
        # verify
        self.assertTrue('/etc/trove-guestagent.conf' in server.files)
        self.assertEqual(server.files['/etc/trove-guestagent.conf'],
                         self.guestconfig_content)

    @patch('trove.taskmanager.models.CONF')
    def test_create_instance_with_az_kwarg(self, mock_conf):
        mock_conf.get.return_value = ''
        # execute
        server = self.freshinstancetasks._create_server(
            None, None, None, None, None, availability_zone='nova', nics=None)
        # verify
        self.assertIsNotNone(server)

    @patch('trove.taskmanager.models.CONF')
    def test_create_instance_with_az(self, mock_conf):
        mock_conf.get.return_value = ''
        # execute
        server = self.freshinstancetasks._create_server(
            None, None, None, None, None, 'nova', None)
        # verify
        self.assertIsNotNone(server)

    @patch('trove.taskmanager.models.CONF')
    def test_create_instance_with_az_none(self, mock_conf):
        mock_conf.get.return_value = ''
        # execute
        server = self.freshinstancetasks._create_server(
            None, None, None, None, None, None, None)
        # verify
        self.assertIsNotNone(server)

    @patch('trove.taskmanager.models.CONF')
    def test_update_status_of_intance_failure(self, mock_conf):
        mock_conf.get.return_value = ''
        InstanceServiceStatus.find_by = Mock(
            return_value=fake_InstanceServiceStatus.find_by())
        DBInstance.find_by = Mock(
            return_value=fake_DBInstance.find_by())
        self.freshinstancetasks.update_statuses_on_time_out()
        self.assertEqual(fake_InstanceServiceStatus.find_by().get_status(),
                         ServiceStatuses.FAILED_TIMEOUT_GUESTAGENT)
        self.assertEqual(fake_DBInstance.find_by().get_task_status(),
                         InstanceTasks.BUILDING_ERROR_TIMEOUT_GA)

    def test_create_sg_rules_success(self):
        datastore_manager = 'mysql'
        taskmanager_models.SecurityGroup.create_for_instance = (
            Mock(return_value={'id': uuid.uuid4(),
                               'name': uuid.uuid4()}))
        taskmanager_models.CONF.get = Mock(return_value=FakeOptGroup())
        taskmanager_models.SecurityGroupRule.create_sec_group_rule = (
            Mock())
        self.freshinstancetasks._create_secgroup(datastore_manager)
        self.assertEqual(2, taskmanager_models.SecurityGroupRule.
                         create_sec_group_rule.call_count)

    def test_create_sg_rules_format_exception_raised(self):
        datastore_manager = 'mysql'
        taskmanager_models.SecurityGroup.create_for_instance = (
            Mock(return_value={'id': uuid.uuid4(),
                               'name': uuid.uuid4()}))
        taskmanager_models.CONF.get = Mock(
            return_value=FakeOptGroup(tcp_ports=['3306', '-3306']))
        self.freshinstancetasks.update_db = Mock()
        taskmanager_models.SecurityGroupRule.create_sec_group_rule = (
            Mock())
        self.assertRaises(MalformedSecurityGroupRuleError,
                          self.freshinstancetasks._create_secgroup,
                          datastore_manager)

    def test_create_sg_rules_greater_than_exception_raised(self):
        datastore_manager = 'mysql'
        taskmanager_models.SecurityGroup.create_for_instance = (
            Mock(return_value={'id': uuid.uuid4(),
                               'name': uuid.uuid4()}))
        taskmanager_models.CONF.get = Mock(
            return_value=FakeOptGroup(tcp_ports=['3306', '33060-3306']))
        self.freshinstancetasks.update_db = Mock()
        taskmanager_models.SecurityGroupRule.create_sec_group_rule = (
            Mock())
        self.assertRaises(MalformedSecurityGroupRuleError,
                          self.freshinstancetasks._create_secgroup,
                          datastore_manager)

    def test_create_sg_rules_success_with_duplicated_port_or_range(self):
        datastore_manager = 'mysql'
        taskmanager_models.SecurityGroup.create_for_instance = (
            Mock(return_value={'id': uuid.uuid4(),
                               'name': uuid.uuid4()}))
        taskmanager_models.CONF.get = Mock(
            return_value=FakeOptGroup(
                tcp_ports=['3306', '3306', '3306-3307', '3306-3307']))
        taskmanager_models.SecurityGroupRule.create_sec_group_rule = (
            Mock())
        self.freshinstancetasks.update_db = Mock()
        self.freshinstancetasks._create_secgroup(datastore_manager)
        self.assertEqual(2, taskmanager_models.SecurityGroupRule.
                         create_sec_group_rule.call_count)

    def test_create_sg_rules_exception_with_malformed_ports_or_range(self):
        datastore_manager = 'mysql'
        taskmanager_models.SecurityGroup.create_for_instance = (
            Mock(return_value={'id': uuid.uuid4(),
                               'name': uuid.uuid4()}))
        taskmanager_models.CONF.get = Mock(
            return_value=FakeOptGroup(tcp_ports=['A', 'B-C']))
        self.freshinstancetasks.update_db = Mock()
        self.assertRaises(MalformedSecurityGroupRuleError,
                          self.freshinstancetasks._create_secgroup,
                          datastore_manager)


class ResizeVolumeTest(testtools.TestCase):
    def setUp(self):
        super(ResizeVolumeTest, self).setUp()
        utils.poll_until = Mock()
        timeutils.isotime = Mock()
        self.instance = Mock()
        self.old_vol_size = 1
        self.new_vol_size = 2
        self.action = taskmanager_models.ResizeVolumeAction(self.instance,
                                                            self.old_vol_size,
                                                            self.new_vol_size)

        class FakeGroup():
            def __init__(self):
                self.mount_point = 'var/lib/mysql'
        taskmanager_models.CONF.get = Mock(return_value=FakeGroup())

    def tearDown(self):
        super(ResizeVolumeTest, self).tearDown()

    def test_resize_volume_unmount_exception(self):
        self.instance.guest.unmount_volume = Mock(
            side_effect=GuestError("test exception"))
        self.assertRaises(GuestError,
                          self.action._unmount_volume,
                          recover_func=self.action._recover_restart)
        self.assertEqual(1, self.instance.restart.call_count)
        self.instance.guest.unmount_volume.side_effect = None
        self.instance.reset_mock()

    def test_resize_volume_detach_exception(self):
        self.instance.nova_client.volumes.delete_server_volume = Mock(
            side_effect=nova_exceptions.ClientException("test exception"))
        self.assertRaises(nova_exceptions.ClientException,
                          self.action._detach_volume,
                          recover_func=self.action._recover_mount_restart)
        self.assertEqual(1, self.instance.guest.mount_volume.call_count)
        self.assertEqual(1, self.instance.restart.call_count)
        self.instance.nova_client.volumes.delete_server_volume.side_effect = (
            None)
        self.instance.reset_mock()

    def test_resize_volume_extend_exception(self):
        self.instance.volume_client.volumes.extend = Mock(
            side_effect=cinder_exceptions.ClientException("test exception"))
        self.assertRaises(cinder_exceptions.ClientException,
                          self.action._extend,
                          recover_func=self.action._recover_full)
        attach_count = (
            self.instance.nova_client.volumes.create_server_volume.call_count)
        self.assertEqual(1, attach_count)
        self.assertEqual(1, self.instance.guest.mount_volume.call_count)
        self.assertEqual(1, self.instance.restart.call_count)
        self.instance.volume_client.volumes.extend.side_effect = None
        self.instance.reset_mock()

    def test_resize_volume_verify_extend_no_volume(self):
        self.instance.volume_client.volumes.get = Mock(
            return_value=None)
        self.assertRaises(cinder_exceptions.ClientException,
                          self.action._verify_extend)
        self.instance.reset_mock()

    def test_resize_volume_poll_timeout(self):
        utils.poll_until = Mock(side_effect=PollTimeOut)
        self.assertRaises(PollTimeOut, self.action._verify_extend)
        self.assertEqual(2, self.instance.volume_client.volumes.get.call_count)
        utils.poll_until.side_effect = None
        self.instance.reset_mock()

    def test_resize_volume_active_server_succeeds(self):
        server = Mock(status=InstanceStatus.ACTIVE)
        self.instance.attach_mock(server, 'server')
        self.action.execute()
        self.assertEqual(1, self.instance.guest.stop_db.call_count)
        self.assertEqual(1, self.instance.guest.unmount_volume.call_count)
        detach_count = (
            self.instance.nova_client.volumes.delete_server_volume.call_count)
        self.assertEqual(1, detach_count)
        extend_count = self.instance.volume_client.volumes.extend.call_count
        self.assertEqual(1, extend_count)
        attach_count = (
            self.instance.nova_client.volumes.create_server_volume.call_count)
        self.assertEqual(1, attach_count)
        self.assertEqual(1, self.instance.guest.resize_fs.call_count)
        self.assertEqual(1, self.instance.guest.mount_volume.call_count)
        self.assertEqual(1, self.instance.restart.call_count)
        self.instance.reset_mock()

    def test_resize_volume_server_error_fails(self):
        server = Mock(status=InstanceStatus.ERROR)
        self.instance.attach_mock(server, 'server')
        self.assertRaises(TroveError, self.action.execute)
        self.instance.reset_mock()


class BuiltInstanceTasksTest(testtools.TestCase):

    def get_inst_service_status(self, status_id, statuses):
        answers = []
        for i, status in enumerate(statuses):
            inst_svc_status = InstanceServiceStatus(status,
                                                    id="%s-%s" % (status_id,
                                                                  i))
            inst_svc_status.save = MagicMock(return_value=None)
            answers.append(inst_svc_status)
        return answers

    def _stub_volume_client(self):
        self.instance_task._volume_client = MagicMock(spec=cinderclient.Client)
        stub_volume_mgr = MagicMock(spec=cinderclient.volumes.VolumeManager)
        self.instance_task.volume_client.volumes = stub_volume_mgr
        stub_volume_mgr.extend = MagicMock(return_value=None)
        stub_new_volume = cinderclient.volumes.Volume(
            stub_volume_mgr, {'status': 'available', 'size': 2}, True)
        stub_volume_mgr.get = MagicMock(return_value=stub_new_volume)
        stub_volume_mgr.attach = MagicMock(return_value=None)

    def setUp(self):
        super(BuiltInstanceTasksTest, self).setUp()
        self.new_flavor = {'id': 8, 'ram': 768, 'name': 'bigger_flavor'}
        stub_nova_server = MagicMock()
        db_instance = DBInstance(InstanceTasks.NONE,
                                 id=INST_ID,
                                 name='resize-inst-name',
                                 datastore_version_id='1',
                                 datastore_id='id-1',
                                 flavor_id='6',
                                 manager='mysql',
                                 created=datetime.datetime.utcnow(),
                                 updated=datetime.datetime.utcnow(),
                                 compute_instance_id='computeinst-id-1',
                                 tenant_id='testresize-tenant-id',
                                 volume_size='1',
                                 volume_id=VOLUME_ID)

        # this is used during the final check of whether the resize successful
        db_instance.server_status = 'ACTIVE'
        self.db_instance = db_instance
        datastore_models.DatastoreVersion.load_by_uuid = MagicMock(
            return_value=datastore_models.DatastoreVersion(db_instance))
        datastore_models.Datastore.load = MagicMock(
            return_value=datastore_models.Datastore(db_instance))

        self.instance_task = taskmanager_models.BuiltInstanceTasks(
            trove.common.context.TroveContext(),
            db_instance,
            stub_nova_server,
            InstanceServiceStatus(ServiceStatuses.RUNNING,
                                  id='inst-stat-id-0'))

        self.instance_task._guest = MagicMock(spec=trove.guestagent.api.API)
        self.instance_task._nova_client = MagicMock(
            spec=novaclient.v1_1.Client)
        self.stub_server_mgr = MagicMock(
            spec=novaclient.v1_1.servers.ServerManager)
        self.stub_running_server = MagicMock(
            spec=novaclient.v1_1.servers.Server)
        self.stub_running_server.status = 'ACTIVE'
        self.stub_running_server.flavor = {'id': 6, 'ram': 512}
        self.stub_verifying_server = MagicMock(
            spec=novaclient.v1_1.servers.Server)
        self.stub_verifying_server.status = 'VERIFY_RESIZE'
        self.stub_verifying_server.flavor = {'id': 8, 'ram': 768}
        self.stub_server_mgr.get = MagicMock(
            return_value=self.stub_verifying_server)
        self.instance_task._nova_client.servers = self.stub_server_mgr
        stub_flavor_manager = MagicMock(
            spec=novaclient.v1_1.flavors.FlavorManager)
        self.instance_task._nova_client.flavors = stub_flavor_manager

        nova_flavor = novaclient.v1_1.flavors.Flavor(stub_flavor_manager,
                                                     self.new_flavor,
                                                     True)
        stub_flavor_manager.get = MagicMock(return_value=nova_flavor)

        answers = (status for status in
                   self.get_inst_service_status('inst_stat-id',
                                                [ServiceStatuses.SHUTDOWN,
                                                 ServiceStatuses.RUNNING,
                                                 ServiceStatuses.RUNNING,
                                                 ServiceStatuses.RUNNING]))

        def side_effect_func(*args, **kwargs):
            if 'instance_id' in kwargs:
                return answers.next()
            elif ('id' in kwargs and 'deleted' in kwargs
                  and not kwargs['deleted']):
                return db_instance
            else:
                return MagicMock()
        trove.db.models.DatabaseModelBase.find_by = MagicMock(
            side_effect=side_effect_func)

        template.SingleInstanceConfigTemplate = MagicMock(
            spec=template.SingleInstanceConfigTemplate)
        db_instance.save = MagicMock(return_value=None)
        trove.backup.models.Backup.running = MagicMock(return_value=None)

        if 'volume' in self._testMethodName:
            self._stub_volume_client()

    def tearDown(self):
        super(BuiltInstanceTasksTest, self).tearDown()

    def test_resize_flavor(self):
        orig_server = self.instance_task.server
        self.instance_task.resize_flavor({'id': 1, 'ram': 512},
                                         self.new_flavor)
        # verify
        self.assertIsNot(self.instance_task.server, orig_server)
        self.instance_task._guest.stop_db.assert_any_call(
            do_not_start_on_reboot=True)
        orig_server.resize.assert_any_call(self.new_flavor['id'])
        self.assertThat(self.db_instance.task_status, Is(InstanceTasks.NONE))
        self.assertEqual(self.stub_server_mgr.get.call_count, 1)
        self.assertThat(self.db_instance.flavor_id, Is(self.new_flavor['id']))

    def test_resize_flavor_resize_failure(self):
        orig_server = self.instance_task.server
        self.stub_verifying_server.status = 'ERROR'
        with patch.object(self.instance_task._nova_client.servers, 'get',
                          return_value=self.stub_verifying_server):
            # execute
            self.assertRaises(TroveError, self.instance_task.resize_flavor,
                              {'id': 1, 'ram': 512}, self.new_flavor)
            # verify
            self.assertTrue(self.stub_server_mgr.get.called)
            self.assertIs(self.instance_task.server,
                          self.stub_verifying_server)
            self.instance_task._guest.stop_db.assert_any_call(
                do_not_start_on_reboot=True)
            orig_server.resize.assert_any_call(self.new_flavor['id'])
            self.assertThat(self.db_instance.task_status,
                            Is(InstanceTasks.NONE))
            self.assertThat(self.db_instance.flavor_id, Is('6'))


class BackupTasksTest(testtools.TestCase):
    def setUp(self):
        super(BackupTasksTest, self).setUp()
        self.backup = backup_models.DBBackup()
        self.backup.id = 'backup_id'
        self.backup.name = 'backup_test',
        self.backup.description = 'test desc'
        self.backup.location = 'http://xxx/z_CLOUD/12e48.xbstream.gz'
        self.backup.instance_id = 'instance id'
        self.backup.created = 'yesterday'
        self.backup.updated = 'today'
        self.backup.size = 2.0
        self.backup.state = backup_models.BackupState.NEW
        self.container_content = (None,
                                  [{'name': 'first'},
                                   {'name': 'second'},
                                   {'name': 'third'}])
        backup_models.Backup.delete = MagicMock(return_value=None)
        backup_models.Backup.get_by_id = MagicMock(return_value=self.backup)
        backup_models.DBBackup.save = MagicMock(return_value=self.backup)
        self.backup.delete = MagicMock(return_value=None)
        self.swift_client = MagicMock()
        remote.create_swift_client = MagicMock(return_value=self.swift_client)

        self.swift_client.head_container = MagicMock(
            side_effect=ClientException("foo"))
        self.swift_client.head_object = MagicMock(
            side_effect=ClientException("foo"))
        self.swift_client.get_container = MagicMock(
            return_value=self.container_content)
        self.swift_client.delete_object = MagicMock(return_value=None)
        self.swift_client.delete_container = MagicMock(return_value=None)

    def tearDown(self):
        super(BackupTasksTest, self).tearDown()

    def test_delete_backup_nolocation(self):
        self.backup.location = ''
        taskmanager_models.BackupTasks.delete_backup('dummy context',
                                                     self.backup.id)
        self.backup.delete.assert_any_call()

    def test_delete_backup_fail_delete_manifest(self):
        with patch.object(self.swift_client, 'delete_object',
                          side_effect=ClientException("foo")):
            with patch.object(self.swift_client, 'head_object',
                              return_value={}):
                self.assertRaises(
                    TroveError,
                    taskmanager_models.BackupTasks.delete_backup,
                    'dummy context', self.backup.id)
                self.assertFalse(backup_models.Backup.delete.called)
                self.assertEqual(
                    backup_models.BackupState.DELETE_FAILED,
                    self.backup.state,
                    "backup should be in DELETE_FAILED status")

    def test_delete_backup_fail_delete_segment(self):
        with patch.object(self.swift_client, 'delete_object',
                          side_effect=ClientException("foo")):
            self.assertRaises(
                TroveError,
                taskmanager_models.BackupTasks.delete_backup,
                'dummy context', self.backup.id)
            self.assertFalse(backup_models.Backup.delete.called)
            self.assertEqual(
                backup_models.BackupState.DELETE_FAILED,
                self.backup.state,
                "backup should be in DELETE_FAILED status")

    def test_parse_manifest(self):
        manifest = 'container/prefix'
        cont, prefix = taskmanager_models.BackupTasks._parse_manifest(manifest)
        self.assertEqual(cont, 'container')
        self.assertEqual(prefix, 'prefix')

    def test_parse_manifest_bad(self):
        manifest = 'bad_prefix'
        cont, prefix = taskmanager_models.BackupTasks._parse_manifest(manifest)
        self.assertEqual(cont, None)
        self.assertEqual(prefix, None)

    def test_parse_manifest_long(self):
        manifest = 'container/long/path/to/prefix'
        cont, prefix = taskmanager_models.BackupTasks._parse_manifest(manifest)
        self.assertEqual(cont, 'container')
        self.assertEqual(prefix, 'long/path/to/prefix')

    def test_parse_manifest_short(self):
        manifest = 'container/'
        cont, prefix = taskmanager_models.BackupTasks._parse_manifest(manifest)
        self.assertEqual(cont, 'container')
        self.assertEqual(prefix, '')


class NotifyMixinTest(testtools.TestCase):
    def test_get_service_id(self):
        id_map = {
            'mysql': '123',
            'percona': 'abc'
        }
        mixin = taskmanager_models.NotifyMixin()
        self.assertThat(mixin._get_service_id('mysql', id_map), Equals('123'))

    def test_get_service_id_unknown(self):
        id_map = {
            'mysql': '123',
            'percona': 'abc'
        }
        transformer = taskmanager_models.NotifyMixin()
        self.assertThat(transformer._get_service_id('m0ng0', id_map),
                        Equals('unknown-service-id-error'))


class RootReportTest(testtools.TestCase):

    def setUp(self):
        super(RootReportTest, self).setUp()
        util.init_db()

    def tearDown(self):
        super(RootReportTest, self).tearDown()

    def test_report_root_first_time(self):
        report = mysql_models.RootHistory.create(
            None, utils.generate_uuid(), 'root')
        self.assertIsNotNone(report)

    def test_report_root_double_create(self):
        uuid = utils.generate_uuid()
        history = mysql_models.RootHistory(uuid, 'root').save()
        mysql_models.RootHistory.load = Mock(return_value=history)
        report = mysql_models.RootHistory.create(
            None, uuid, 'root')
        self.assertTrue(mysql_models.RootHistory.load.called)
        self.assertEqual(history.user, report.user)
        self.assertEqual(history.id, report.id)

########NEW FILE########
__FILENAME__ = matchers
# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# Copyright 2012 Hewlett-Packard Development Company, L.P.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

"""Matcher classes to be used inside of the testtools assertThat framework."""

import pprint


class DictKeysMismatch(object):
    def __init__(self, d1only, d2only):
        self.d1only = d1only
        self.d2only = d2only

    def describe(self):
        return ('Keys in d1 and not d2: %(d1only)s.'
                ' Keys in d2 and not d1: %(d2only)s' % self.__dict__)

    def get_details(self):
        return {}


class DictMismatch(object):
    def __init__(self, key, d1_value, d2_value):
        self.key = key
        self.d1_value = d1_value
        self.d2_value = d2_value

    def describe(self):
        return ("Dictionaries do not match at %(key)s."
                " d1: %(d1_value)s d2: %(d2_value)s" % self.__dict__)

    def get_details(self):
        return {}


class DictMatches(object):
    def __init__(self, d1, approx_equal=False, tolerance=0.001):
        self.d1 = d1
        self.approx_equal = approx_equal
        self.tolerance = tolerance

    def __str__(self):
        return 'DictMatches(%s)' % (pprint.pformat(self.d1))

    # Useful assertions
    def match(self, d2):
        """Assert two dicts are equivalent.

        This is a 'deep' match in the sense that it handles nested
        dictionaries appropriately.

        NOTE:

            If you don't care (or don't know) a given value, you can specify
            the string DONTCARE as the value. This will cause that dict-item
            to be skipped.

        """

        d1keys = set(self.d1.keys())
        d2keys = set(d2.keys())
        if d1keys != d2keys:
            d1only = d1keys - d2keys
            d2only = d2keys - d1keys
            return DictKeysMismatch(d1only, d2only)

        for key in d1keys:
            d1value = self.d1[key]
            d2value = d2[key]
            try:
                error = abs(float(d1value) - float(d2value))
                within_tolerance = error <= self.tolerance
            except (ValueError, TypeError):
                # If both values aren't convertible to float, just ignore
                # ValueError if arg is a str, TypeError if it's something else
                # (like None)
                within_tolerance = False

            if hasattr(d1value, 'keys') and hasattr(d2value, 'keys'):
                matcher = DictMatches(d1value)
                did_match = matcher.match(d2value)
                if did_match is not None:
                    return did_match
            elif 'DONTCARE' in (d1value, d2value):
                continue
            elif self.approx_equal and within_tolerance:
                continue
            elif d1value != d2value:
                return DictMismatch(key, d1value, d2value)


class ListLengthMismatch(object):
    def __init__(self, len1, len2):
        self.len1 = len1
        self.len2 = len2

    def describe(self):
        return ('Length mismatch: len(L1)=%(len1)d != '
                'len(L2)=%(len2)d' % self.__dict__)

    def get_details(self):
        return {}


class DictListMatches(object):
    def __init__(self, l1, approx_equal=False, tolerance=0.001):
        self.l1 = l1
        self.approx_equal = approx_equal
        self.tolerance = tolerance

    def __str__(self):
        return 'DictListMatches(%s)' % (pprint.pformat(self.l1))

    # Useful assertions
    def match(self, l2):
        """Assert a list of dicts are equivalent."""

        l1count = len(self.l1)
        l2count = len(l2)
        if l1count != l2count:
            return ListLengthMismatch(l1count, l2count)

        for d1, d2 in zip(self.l1, l2):
            matcher = DictMatches(d2,
                                  approx_equal=self.approx_equal,
                                  tolerance=self.tolerance)
            did_match = matcher.match(d1)
            if did_match:
                return did_match


class SubDictMismatch(object):
    def __init__(self,
                 key=None,
                 sub_value=None,
                 super_value=None,
                 keys=False):
        self.key = key
        self.sub_value = sub_value
        self.super_value = super_value
        self.keys = keys

    def describe(self):
        if self.keys:
            return "Keys between dictionaries did not match"
        else:
            return ("Dictionaries do not match at %s. d1: %s d2: %s"
                    % (self.key,
                       self.super_value,
                       self.sub_value))

    def get_details(self):
        return {}


class IsSubDictOf(object):
    def __init__(self, super_dict):
        self.super_dict = super_dict

    def __str__(self):
        return 'IsSubDictOf(%s)' % (self.super_dict)

    def match(self, sub_dict):
        """Assert a sub_dict is subset of super_dict."""
        if not set(sub_dict.keys()).issubset(set(self.super_dict.keys())):
            return SubDictMismatch(keys=True)
        for k, sub_value in sub_dict.items():
            super_value = self.super_dict[k]
            if isinstance(sub_value, dict):
                matcher = IsSubDictOf(super_value)
                did_match = matcher.match(sub_value)
                if did_match is not None:
                    return did_match
            elif 'DONTCARE' in (sub_value, super_value):
                continue
            else:
                if sub_value != super_value:
                    return SubDictMismatch(k, sub_value, super_value)


class FunctionCallMatcher(object):
    def __init__(self, expected_func_calls):
        self.expected_func_calls = expected_func_calls
        self.actual_func_calls = []

    def call(self, *args, **kwargs):
        func_call = {'args': args, 'kwargs': kwargs}
        self.actual_func_calls.append(func_call)

    def match(self):
        dict_list_matcher = DictListMatches(self.expected_func_calls)
        return dict_list_matcher.match(self.actual_func_calls)

########NEW FILE########
__FILENAME__ = util
#    Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


def init_db():
    from trove.common import cfg
    from trove.db import get_db_api
    from trove.db.sqlalchemy import session
    CONF = cfg.CONF
    db_api = get_db_api()
    db_api.db_sync(CONF)
    session.configure_db(CONF)

########NEW FILE########
__FILENAME__ = check
# Copyright (c) 2012 OpenStack
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Like asserts, but does not raise an exception until the end of a block."""

import traceback
from proboscis.asserts import ASSERTION_ERROR
from proboscis.asserts import assert_equal
from proboscis.asserts import assert_false
from proboscis.asserts import assert_not_equal
from proboscis.asserts import assert_true
from proboscis.asserts import Check


def get_stack_trace_of_caller(level_up):
    """Gets the stack trace at the point of the caller."""
    level_up += 1
    st = traceback.extract_stack()
    caller_index = len(st) - level_up
    if caller_index < 0:
        caller_index = 0
    new_st = st[0:caller_index]
    return new_st


def raise_blame_caller(level_up, ex):
    """Raises an exception, changing the stack trace to point to the caller."""
    new_st = get_stack_trace_of_caller(level_up + 2)
    raise type(ex), ex, new_st


class Checker(object):

    def __init__(self):
        self.messages = []
        self.odd = True
        self.protected = False

    def _add_exception(self, _type, value, tb):
        """Takes an exception, and adds it as a string."""
        if self.odd:
            prefix = "* "
        else:
            prefix = "- "
        start = "Check failure! Traceback:"
        middle = prefix.join(traceback.format_list(tb))
        end = '\n'.join(traceback.format_exception_only(_type, value))
        msg = '\n'.join([start, middle, end])
        self.messages.append(msg)
        self.odd = not self.odd

    def equal(self, *args, **kwargs):
        self._run_assertion(assert_equal, *args, **kwargs)

    def false(self, *args, **kwargs):
        self._run_assertion(assert_false, *args, **kwargs)

    def not_equal(self, *args, **kwargs):
        _run_assertion(assert_not_equal, *args, **kwargs)

    def _run_assertion(self, assert_func, *args, **kwargs):
        """
        Runs an assertion method, but catches any failure and adds it as a
        string to the messages list.
        """
        if self.protected:
            try:
                assert_func(*args, **kwargs)
            except ASSERTION_ERROR as ae:
                st = get_stack_trace_of_caller(2)
                self._add_exception(ASSERTION_ERROR, ae, st)
        else:
            assert_func(*args, **kwargs)

    def __enter__(self):
        self.protected = True
        return self

    def __exit__(self, _type, value, tb):
        self.protected = False
        if _type is not None:
            # An error occurred other than an assertion failure.
            # Return False to allow the Exception to be raised
            return False
        if len(self.messages) != 0:
            final_message = '\n'.join(self.messages)
            raise ASSERTION_ERROR(final_message)

    def true(self, *args, **kwargs):
        self._run_assertion(assert_true, *args, **kwargs)


class AttrCheck(Check):
    """Class for attr checks, links and other common items."""

    def __init__(self):
        super(AttrCheck, self).__init__()

    def fail(self, msg):
        self.true(False, msg)

    def attrs_exist(self, list, expected_attrs, msg=None):
        # Check these attrs only are returned in create response
        for attr in list:
            if attr not in expected_attrs:
                self.fail("%s should not contain '%s'" % (msg, attr))

    def links(self, links):
        expected_attrs = ['href', 'rel']
        for link in links:
            self.attrs_exist(link, expected_attrs, msg="Links")


class CollectionCheck(Check):
    """Checks for elements in a dictionary."""

    def __init__(self, name, collection):
        self.name = name
        self.collection = collection
        super(CollectionCheck, self).__init__()

    def element_equals(self, key, expected_value):
        if key not in self.collection:
            message = 'Element "%s.%s" does not exist.' % (self.name, key)
            self.fail(message)
        else:
            value = self.collection[key]
            self.equal(value, expected_value)

    def has_element(self, key, element_type):
        if key not in self.collection:
            message = 'Element "%s.%s" does not exist.' % (self.name, key)
            self.fail(message)
        else:
            value = self.collection[key]
            match = False
            if not isinstance(element_type, tuple):
                type_list = [element_type]
            else:
                type_list = element_type
            for possible_type in type_list:
                if possible_type is None:
                    if value is None:
                        match = True
                else:
                    if isinstance(value, possible_type):
                        match = True
            if not match:
                self.fail('Element "%s.%s" does not match any of these '
                          'expected types: %s' % (self.name, key, type_list))


class TypeCheck(Check):
    """Checks for attributes in an object."""

    def __init__(self, name, instance):
        self.name = name
        self.instance = instance
        super(TypeCheck, self).__init__()

    def _check_type(value, attribute_type):
        if not isinstance(value, attribute_type):
            self.fail("%s attribute %s is of type %s (expected %s)."
                      % (self.name, attribute_name, type(value),
                         attribute_type))

    def has_field(self, attribute_name, attribute_type,
                  additional_checks=None):
        if not hasattr(self.instance, attribute_name):
            self.fail("%s missing attribute %s." % (self.name, attribute_name))
        else:
            value = getattr(self.instance, attribute_name)
            match = False
            if isinstance(attribute_type, tuple):
                type_list = attribute_type
            else:
                type_list = [attribute_type]
            for possible_type in type_list:
                if possible_type is None:
                    if value is None:
                        match = True
                else:
                    if isinstance(value, possible_type):
                        match = True
            if not match:
                self.fail("%s attribute %s is of type %s (expected one of "
                          "the following: %s)." % (self.name, attribute_name,
                                                   type(value),
                                                   attribute_type))
            if match and additional_checks:
                additional_checks(value)

########NEW FILE########
__FILENAME__ = client
# Copyright (c) 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
:mod:`tests` -- Utility methods for tests.
===================================

.. automodule:: utils
   :platform: Unix
   :synopsis: Tests for Nova.
.. moduleauthor:: Nirmal Ranganathan <nirmal.ranganathan@rackspace.com>
.. moduleauthor:: Tim Simpson <tim.simpson@rackspace.com>
"""


from proboscis import asserts

from trove.tests.config import CONFIG


def add_report_event_to(home, name):
    """Takes a module, class, etc, and an attribute name to decorate."""
    func = getattr(home, name)

    def __cb(*args, **kwargs):
        # While %s turns a var into a string but in some rare cases explicit
        # str() is less likely to raise an exception.
        arg_strs = [repr(arg) for arg in args]
        arg_strs += ['%s=%s' % (repr(key), repr(value))
                     for (key, value) in kwargs.items()]
        CONFIG.get_reporter().log("[RDC] Calling : %s(%s)..."
                                  % (name, ','.join(arg_strs)))
        value = func(*args, **kwargs)
        CONFIG.get_reporter.log("[RDC]     returned %s." % str(value))
        return value
    setattr(home, name, __cb)


class TestClient(object):
    """Decorates the rich clients with some extra methods.

    These methods are filled with test asserts, meaning if you use this you
    get the tests for free.

    """

    def __init__(self, real_client):
        """Accepts a normal client."""
        self.real_client = real_client

    def assert_http_code(self, expected_http_code):
        resp, body = self.real_client.client.last_response
        asserts.assert_equal(resp.status, expected_http_code)

    @property
    def last_http_code(self):
        resp, body = self.real_client.client.last_response
        return resp.status

    @staticmethod
    def find_flavor_self_href(flavor):
        self_links = [link for link in flavor.links if link['rel'] == 'self']
        asserts.assert_true(len(self_links) > 0, "Flavor had no self href!")
        flavor_href = self_links[0]['href']
        asserts.assert_false(flavor_href is None,
                             "Flavor link self href missing.")
        return flavor_href

    def find_flavors_by(self, condition, flavor_manager=None):
        flavor_manager = flavor_manager or self.flavors
        flavors = flavor_manager.list()
        return [flavor for flavor in flavors if condition(flavor)]

    def find_flavors_by_name(self, name, flavor_manager=None):
        return self.find_flavors_by(lambda flavor: flavor.name == name,
                                    flavor_manager)

    def find_flavors_by_ram(self, ram, flavor_manager=None):
        return self.find_flavors_by(lambda flavor: flavor.ram == ram,
                                    flavor_manager)

    def find_flavor_and_self_href(self, flavor_id, flavor_manager=None):
        """Given an ID, returns flavor and its self href."""
        flavor_manager = flavor_manager or self.flavors
        asserts.assert_false(flavor_id is None)
        flavor = flavor_manager.get(flavor_id)
        asserts.assert_false(flavor is None)
        flavor_href = self.find_flavor_self_href(flavor)
        return flavor, flavor_href

    def __getattr__(self, item):
        return getattr(self.real_client, item)

########NEW FILE########
__FILENAME__ = event_simulator
# Copyright 2013 OpenStack Foundation
# Copyright 2013 Rackspace Hosting
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

"""
Simulates time itself to make the fake mode tests run even faster.
"""

from proboscis.asserts import fail
from trove.openstack.common import log as logging
from trove.common import exception


LOG = logging.getLogger(__name__)

allowable_empty_sleeps = 0
pending_events = []
sleep_entrance_count = 0


def event_simulator_spawn_after(time_from_now_in_seconds, func, *args, **kw):
    """Fakes events without doing any actual waiting."""
    def __cb():
        func(*args, **kw)
    pending_events.append({"time": time_from_now_in_seconds, "func": __cb})


def event_simulator_spawn(func, *args, **kw):
    event_simulator_spawn_after(0, func, *args, **kw)


def event_simulator_sleep(time_to_sleep):
    """Simulates waiting for an event.

    This is used to monkey patch the sleep methods, so that no actually waiting
    occurs but functions which would have run as threads are executed.

    This function will also raise an assertion failure if there were no pending
    events ready to run. If this happens there are two possibilities:
        1. The test code (or potentially code in Trove task manager) is
           sleeping even though no action is taking place in
           another thread.
        2. The test code (or task manager code) is sleeping waiting for a
           condition that will never be met because the thread it was waiting
           on experienced an error or did not finish successfully.

    A good example of this second case is when a bug in task manager causes the
    create instance method to fail right away, but the test code tries to poll
    the instance's status until it gets rate limited. That makes finding the
    real error a real hassle. Thus it makes more sense to raise an exception
    whenever the app seems to be napping for no reason.

    """
    global pending_events
    global allowable_empty_sleeps
    if len(pending_events) == 0:
        allowable_empty_sleeps -= 1
        if allowable_empty_sleeps < 0:
            fail("Trying to sleep when no events are pending.")

    global sleep_entrance_count
    sleep_entrance_count += 1
    time_to_sleep = float(time_to_sleep)

    run_once = False  # Ensure simulator runs even if the sleep time is zero.
    while not run_once or time_to_sleep > 0:
        run_once = True
        itr_sleep = 0.5
        for i in range(len(pending_events)):
            event = pending_events[i]
            event["time"] = event["time"] - itr_sleep
            if event["func"] is not None and event["time"] < 0:
                # Call event, but first delete it so this function can be
                # reentrant.
                func = event["func"]
                event["func"] = None
                try:
                    func()
                except Exception:
                    LOG.exception("Simulated event error.")
        time_to_sleep -= itr_sleep
    sleep_entrance_count -= 1
    if sleep_entrance_count < 1:
        # Clear out old events
        pending_events = [event for event in pending_events
                          if event["func"] is not None]


def fake_poll_until(retriever, condition=lambda value: value,
                    sleep_time=1, time_out=None):
    """Retrieves object until it passes condition, then returns it.

    If time_out_limit is passed in, PollTimeOut will be raised once that
    amount of time is eclipsed.

    """
    slept_time = 0
    while True:
        resource = retriever()
        if condition(resource):
            return resource
        event_simulator_sleep(sleep_time)
        slept_time += sleep_time
        if time_out and slept_time >= time_out:
                raise exception.PollTimeOut()


def monkey_patch():
    import time
    time.sleep = event_simulator_sleep
    import eventlet
    from eventlet import greenthread
    eventlet.sleep = event_simulator_sleep
    greenthread.sleep = event_simulator_sleep
    eventlet.spawn_after = event_simulator_spawn_after
    eventlet.spawn_n = event_simulator_spawn
    eventlet.spawn = NotImplementedError
    from trove.common import utils
    utils.poll_until = fake_poll_until

########NEW FILE########
__FILENAME__ = mysql
# Copyright 2013 OpenStack Foundation
# Copyright 2013 Rackspace Hosting
# Copyright 2013 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#

import pexpect
import re
from sqlalchemy import create_engine
from trove import tests
from trove.tests.config import CONFIG
from sqlalchemy.exc import OperationalError
try:
    from sqlalchemy.exc import ResourceClosedError
except ImportError:
    ResourceClosedError = Exception


def create_mysql_connection(host, user, password):
    connection = CONFIG.mysql_connection_method
    if connection['type'] == "direct":
        return SqlAlchemyConnection(host, user, password)
    elif connection['type'] == "tunnel":
        if 'ssh' not in connection:
            raise RuntimeError("If connection type is 'tunnel' then a "
                               "property 'ssh' is expected.")
        return PexpectMySqlConnection(connection['ssh'], host, user, password)
    else:
        raise RuntimeError("Unknown Bad test configuration for "
                           "mysql_connection_method")


class MySqlConnectionFailure(RuntimeError):

    def __init__(self, msg):
        super(MySqlConnectionFailure, self).__init__(msg)


class MySqlPermissionsFailure(RuntimeError):

    def __init__(self, msg):
        super(MySqlPermissionsFailure, self).__init__(msg)


class SqlAlchemyConnection(object):

    def __init__(self, host, user, password):
        self.host = host
        self.user = user
        self.password = password
        try:
            self.engine = self._init_engine(user, password, host)
        except OperationalError as oe:
            if self._exception_is_permissions_issue(str(oe)):
                raise MySqlPermissionsFailure(oe)
            else:
                raise MySqlConnectionFailure(oe)

    @staticmethod
    def _exception_is_permissions_issue(msg):
        """Assert message cited a permissions issue and not something else."""
        pos_error = re.compile(".*Host '[\w\.]*' is not allowed to connect to "
                               "this MySQL server.*")
        pos_error1 = re.compile(".*Access denied for user "
                                "'[\w\*\!\@\#\^\&]*'@'[\w\.]*'.*")
        if (pos_error.match(msg) or pos_error1.match(msg)):
            return True

    def __enter__(self):
        try:
            self.conn = self.engine.connect()
        except OperationalError as oe:
            if self._exception_is_permissions_issue(str(oe)):
                raise MySqlPermissionsFailure(oe)
            else:
                raise MySqlConnectionFailure(oe)
        self.trans = self.conn.begin()
        return self

    def execute(self, cmd):
        """Execute some code."""
        cmd = cmd.replace("%", "%%")
        try:
            return self.conn.execute(cmd).fetchall()
        except Exception:
            self.trans.rollback()
            self.trans = None
            try:
                raise
            except ResourceClosedError:
                return []

    def __exit__(self, type, value, traceback):
        if self.trans:
            if type is not None:  # An error occurred
                self.trans.rollback()
            else:
                self.trans.commit()
        self.conn.close()

    @staticmethod
    def _init_engine(user, password, host):
        return create_engine("mysql://%s:%s@%s:3306" % (user, password, host),
                             pool_recycle=1800, echo=True)


class PexpectMySqlConnection(object):

    TIME_OUT = 30

    def __init__(self, ssh_args, host, user, password):
        self.host = host
        self.user = user
        self.password = password
        cmd = '%s %s' % (tests.SSH_CMD, ssh_args)
        self.proc = pexpect.spawn(cmd)
        print(cmd)
        self.proc.expect(":~\$", timeout=self.TIME_OUT)
        cmd2 = "mysql --host '%s' -u '%s' '-p%s'\n" % \
               (self.host, self.user, self.password)
        print(cmd2)
        self.proc.send(cmd2)
        result = self.proc.expect([
            'mysql>',
            'Access denied',
            "Can't connect to MySQL server"],
            timeout=self.TIME_OUT)
        if result == 1:
            raise MySqlPermissionsFailure(self.proc.before)
        elif result == 2:
            raise MySqlConnectionFailure(self.proc.before)

    def __enter__(self):
        return self

    def __exit__(self, type, value, traceback):
        self.proc.close()

    def execute(self, cmd):
        self.proc.send(cmd + "\G\n")
        outcome = self.proc.expect(['Empty set', 'mysql>'],
                                   timeout=self.TIME_OUT)
        if outcome == 0:
            return []
        else:
            # This next line might be invaluable for long test runs.
            print("Interpreting output: %s" % self.proc.before)
            lines = self.proc.before.split("\r\n")
            result = []
            row = None
            for line in lines:
                plural_s = "s" if len(result) != 0 else ""
                end_line = "%d row%s in set" % ((len(result) + 1), plural_s)
                if len(result) == 0:
                    end_line = "1 row in set"
                if (line.startswith("***************************") or
                        line.startswith(end_line)):
                    if row is not None:
                        result.append(row)
                    row = {}
                elif row is not None:
                    colon = line.find(": ")
                    field = line[:colon]
                    value = line[colon + 2:]
                    row[field] = value
            return result

########NEW FILE########
__FILENAME__ = server_connection
# Copyright 2013 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from trove import tests
from trove.tests import util
from trove.tests.util.users import Requirements


def create_server_connection(instance_id):
    if util.test_config.use_local_ovz:
        return OpenVZServerConnection(instance_id)
    return ServerSSHConnection(instance_id)


class ServerSSHConnection(object):
    def __init__(self, instance_id):
        self.instance_id = instance_id
        req_admin = Requirements(is_admin=True)
        self.user = util.test_config.users.find_user(req_admin)
        self.dbaas_admin = util.create_dbaas_client(self.user)
        self.instance = self.dbaas_admin.management.show(self.instance_id)
        self.ip_address = self.instance.ip[0]

    def execute(self, cmd):
        exe_cmd = "%s %s %s" % (tests.SSH_CMD, self.ip_address, cmd)
        print("RUNNING COMMAND: %s" % exe_cmd)
        return util.process(exe_cmd)


class OpenVZServerConnection(object):
    def __init__(self, instance_id):
        self.instance_id = instance_id
        req_admin = Requirements(is_admin=True)
        self.user = util.test_config.users.find_user(req_admin)
        self.dbaas_admin = util.create_dbaas_client(self.user)
        self.instance = self.dbaas_admin.management.show(self.instance_id)
        self.instance_local_id = self.instance.server["local_id"]

    def execute(self, cmd):
        exe_cmd = "sudo vzctl exec %s %s" % (self.instance_local_id, cmd)
        print("RUNNING COMMAND: %s" % exe_cmd)
        return util.process(exe_cmd)

########NEW FILE########
__FILENAME__ = usage
# Copyright (c) 2013 Rackspace Hosting
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from collections import defaultdict

import trove.openstack.common.log as logging
from trove.common import utils
from trove.tests.config import CONFIG
import proboscis.asserts as asserts
from proboscis.dependencies import SkipTest

LOG = logging.getLogger(__name__)
MESSAGE_QUEUE = defaultdict(list)


def create_usage_verifier():
    return utils.import_object(CONFIG.usage_endpoint)


class UsageVerifier(object):

    def clear_events(self):
        """Hook that is called to allow endpoints to clean up."""
        pass

    def check_message(self, resource_id, event_type, **attrs):
        messages = utils.poll_until(lambda: self.get_messages(resource_id),
                                    lambda x: len(x) > 0, time_out=30)
        found = None
        for message in messages:
            if message['event_type'] == event_type:
                found = message
        asserts.assert_is_not_none(found,
                                   "No message type %s for resource %s" %
                                   (event_type, resource_id))
        with asserts.Check() as check:
            for key, value in attrs.iteritems():
                check.equal(found[key], value)

    def get_messages(self, resource_id, expected_messages=None):
        global MESSAGE_QUEUE
        msgs = MESSAGE_QUEUE.get(resource_id, [])
        if expected_messages is not None:
            asserts.assert_equal(len(msgs), expected_messages)
        return msgs


class FakeVerifier(object):
    """This is the default handler in fake mode, it is basically a no-op."""

    def clear_events(self):
        pass

    def check_message(self, *args, **kwargs):
        raise SkipTest("Notifications not available")

    def get_messages(self, *args, **kwargs):
        pass


def notify(context, message):
    """Simple test notify function which saves the messages to global list."""
    LOG.debug('Received Usage Notification: %s' % message)
    payload = message.get('payload', None)
    payload['event_type'] = message['event_type']
    resource_id = payload['instance_id']
    global MESSAGE_QUEUE
    MESSAGE_QUEUE[resource_id].append(payload)
    LOG.debug('Message Queue for %(id)s now has %(msg_count)d messages' %
              {'id': resource_id,
               'msg_count': len(MESSAGE_QUEUE[resource_id])})

########NEW FILE########
__FILENAME__ = users
# Copyright (c) 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Information on users / identities we can hit the services on behalf of.

This code allows tests to grab from a set of users based on the features they
possess instead of specifying exact identities in the test code.

"""


class Requirements(object):
    """Defines requirements a test has of a user."""

    def __init__(self, is_admin=None, services=None):
        self.is_admin = is_admin
        self.services = services or ["trove"]
        # Make sure they're all the same kind of string.
        self.services = [str(service) for service in self.services]

    def satisfies(self, reqs):
        """True if these requirements conform to the given requirements."""
        if reqs.is_admin is not None:  # Only check if it was specified.
            if reqs.is_admin != self.is_admin:
                return False
        for service in reqs.services:
            if service not in self.services:
                return False
        return True

    def __str__(self):
        return "is_admin=%s, services=%s" % (self.is_admin, self.services)


class ServiceUser(object):
    """Represents a user who uses a service.

    Importantly, this represents general information, such that a test can be
    written to state the general information about a user it needs (for
    example, if the user is an admin or not) rather than explicitly list
    users.

    """

    def __init__(self, auth_user=None, auth_key=None, services=None,
                 tenant=None, tenant_id=None, requirements=None):
        """Creates info on a user."""
        self.auth_user = auth_user
        self.auth_key = auth_key
        self.tenant = tenant
        self.tenant_id = tenant_id
        self.requirements = requirements
        self.test_count = 0
        if self.requirements.is_admin is None:
            raise ValueError("'is_admin' must be specified for a user.")

    def __str__(self):
        return ("{ user_name=%s, tenant=%s, tenant_id=%s, reqs=%s, tests=%d }"
                % (self.auth_user, self.tenant, self.tenant_id,
                   self.requirements, self.test_count))


class Users(object):
    """Collection of users with methods to find them via requirements."""

    def __init__(self, user_list):
        self.users = []
        for user_dict in user_list:
            reqs = Requirements(**user_dict["requirements"])
            user = ServiceUser(auth_user=user_dict["auth_user"],
                               auth_key=user_dict["auth_key"],
                               tenant=user_dict["tenant"],
                               tenant_id=user_dict.get("tenant_id", None),
                               requirements=reqs)
            self.users.append(user)

    def find_all_users_who_satisfy(self, requirements, black_list=None):
        """Returns a list of all users who satisfy the given requirements."""
        black_list = black_list or []
        print("Searching for a user who meets requirements %s in our list..."
              % requirements)
        print("Users:")
        for user in self.users:
            print("\t" + str(user))
        print("Black list")
        for item in black_list:
            print("\t" + str(item))
        black_list = black_list or []
        return (user for user in self.users
                if user.auth_user not in black_list and
                user.requirements.satisfies(requirements))

    def find_user(self, requirements, black_list=None):
        """Finds a user who meets the requirements and has been used least."""
        users = self.find_all_users_who_satisfy(requirements, black_list)
        try:
            user = min(users, key=lambda user: user.test_count)
        except ValueError:  # Raised when "users" is empty.
            raise RuntimeError("The test configuration data lacks a user "
                               "who meets these requirements: %s"
                               % requirements)
        user.test_count += 1
        return user

    def _find_user_by_condition(self, condition):
        users = (user for user in self.users if condition(user))
        try:
            user = min(users, key=lambda user: user.test_count)
        except ValueError:
            raise RuntimeError('Did not find a user with name "%s".' % name)
        user.test_count += 1
        return user

    def find_user_by_name(self, name):
        """Finds a user who meets the requirements and has been used least."""
        condition = lambda user: user.auth_user == name
        return self._find_user_by_condition(condition)

    def find_user_by_tenant_id(self, tenant_id):
        condition = lambda user: user.tenant_id == tenant_id
        return self._find_user_by_condition(condition)

########NEW FILE########
__FILENAME__ = version
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

try:
    from trove.vcsversion import version_info
except ImportError:
    version_info = {'branch_nick': u'LOCALBRANCH',
                    'revision_id': 'LOCALREVISION',
                    'revno': 0}

TROVE_VERSION = ['2012', '1']
YEAR, COUNT = TROVE_VERSION

FINAL = False   # This becomes true at Release Candidate time


def canonical_version_string():
    return '.'.join([YEAR, COUNT])


def version_string():
    if FINAL:
        return canonical_version_string()
    else:
        return '%s-dev' % (canonical_version_string(),)


def vcs_version_string():
    return "%s:%s" % (version_info['branch_nick'], version_info['revision_id'])


def version_string_with_vcs():
    return "%s-%s" % (canonical_version_string(), vcs_version_string())

########NEW FILE########
__FILENAME__ = versions
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import os
import routes

from trove.common import wsgi


VERSIONS = {
    "1.0": {
        "id": "v1.0",
        "status": "CURRENT",
        "updated": "2012-08-01T00:00:00Z",
        "links": [],
    },
}


class VersionsController(wsgi.Controller):

    def index(self, request):
        """Respond to a request for API versions."""
        versions = []
        for key, data in VERSIONS.items():
            v = BaseVersion(
                data["id"],
                data["status"],
                request.application_url,
                data["updated"])
            versions.append(v)
        return wsgi.Result(VersionsDataView(versions))

    def show(self, request):
        """Respond to a request for a specific API version."""
        data = VERSIONS[request.url_version]
        v = Version(data["id"], data["status"],
                    request.application_url, data["updated"])
        return wsgi.Result(VersionDataView(v))


class BaseVersion(object):

    def __init__(self, id, status, base_url, updated):
        self.id = id
        self.status = status
        self.base_url = base_url
        self.updated = updated

    def data(self):
        return {
            "id": self.id,
            "status": self.status,
            "updated": self.updated,
            "links": [{"rel": "self", "href": self.url()}],
        }

    def url(self):
        url = os.path.join(self.base_url, self.id)
        if not url.endswith("/"):
            return url + "/"
        return url


class Version(BaseVersion):

    def url(self):
        if not self.base_url.endswith("/"):
            return self.base_url + "/"
        return self.base_url


class VersionDataView(object):

    def __init__(self, version):
        self.version = version

    def data_for_json(self):
        return {'version': self.version.data()}


class VersionsDataView(object):

    def __init__(self, versions):
        self.versions = versions

    def data_for_json(self):
        return {'versions': [version.data() for version in self.versions]}


class VersionsAPI(wsgi.Router):
    def __init__(self):
        mapper = routes.Mapper()
        versions_resource = VersionsController().create_resource()
        mapper.connect("/", controller=versions_resource, action="index")
        super(VersionsAPI, self).__init__(mapper)


def app_factory(global_conf, **local_conf):
    conf = global_conf.copy()
    conf.update(local_conf)
    return VersionsAPI()

########NEW FILE########
